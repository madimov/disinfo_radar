,title,url,date,cleaning
0,Meta AI & Columbia U ‘Squeeze the Juice’ to Turn Bad Responses into Good Labels and Boost Dialogue Model Performance,https://syncedreview.com/2022/11/01/meta-ai-columbia-u-squeeze-the-juice-to-turn-bad-responses-into-good-labels-and-boost-dialogue-model-performance/,2022-11-01,"There is growing interest in the machine learning community on how to best employ human feedback to improve the responses and performance of chatbots and other dialogue models. Human feedback in the wild is however sparse; and typically comes in the form of up/down votes and free-form textual comments and “gold” corrections which are not always reliable or explicit. In the new paper When Life Gives You Lemons, Make Cherryade: Converting Feedback from Bad Responses into Good Labels, a research team from Meta AI and Columbia University proposes JUICER, a framework that effectively utilizes binary and textual human feedback to improve the conversational responses of dialogue models. The team summarizes their main contributions as follows: The proposed JUICER model consists of three modules: a satisfaction classifier, a reply corrector, and the dialogue model itself. The satisfaction classifier is trained to detect good and bad feedback and predict binary satisfaction labels for all unannotated bot responses. The reply corrector subsequently converts the bad responses into good responses. The final dialogue model is then re-trained using the refined feedback and predictions from the previous steps. In their empirical study, the team applied their approach to the baseline 3B parameter BlenderBot2 (BB2 3B) dialogue model on the FITS and DEMO datasets. In the experiments, JUICER boosted the F1 accuracy score from 15.3 to 18.5 on an unseen test set, and improved good responses from 33.2 to 41.9 percent in human evaluations results, confirming its ability to leverage both good and bad human feedback to improve the overall performance of dialogue models. The paper When Life Gives You Lemons, Make Cherryade: Converting Feedback from Bad Responses into Good Labels is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
1,CMU Takes a Big Step Toward Real-Time Realistic Video Generation Based on Language Descriptions,https://syncedreview.com/2022/10/26/cmu-takes-a-big-step-toward-real-time-realistic-video-generation-based-on-language-descriptions/,2022-10-26,"There are now dozens of AI-powered text-to-image models on the Internet, with DALL.E Mini alone generating more than 50,000 images daily from users’ natural language prompts. The next challenging step for such generative AI models is text-to-video — which brings the potential for creating animated scenes based on users’ storytelling inputs. While current text-to-video approaches guided by Open AI’s CLIP network can translate text into highly-realistic imagery, they are slow — requiring from 17 seconds to five minutes to generate a single frame of video. In the new paper Towards Real-Time Text2Video via CLIP-Guided, Pixel-Level Optimization, researchers from Carnegie Mellon University leverage CLIP-guided, pixel-level optimization to generate 720p resolution videos from natural language descriptions at a rate of one-to-two frames per second — taking a big step towards a real-time text-to-video system. Existing CLIP-Guided text-to-video approaches generate their highly realistic imagery by optimizing through large pretrained image generator diffusion models, a process that is both time-consuming and computationally heavy. The CMU team employs a novel two-step approach to approach real-time text-to-video generation: 1) Generating noisy semantic content at a fast speed; and 2) Refining the generated image textures in a post-processing step. The proposed approach generates each frame sequentially while iterating through the input language to guide the content. CLIP-Guided techniques are used to compare the frame and the language description and evolve the frame toward consistency with the content. A trained CycleGAN model smooths and denoises the generated images. In their empirical study, the team demonstrated their approach’s ability to generate realistic videos at up to 720p resolution at speeds 20-300 times faster than existing methods. The code and sample videos are available on the team’s website. In future work, the researchers plan to add priors to enable smoother motion in the videos and improve user control over their style and appearance. The paper Towards Real-Time Text2Video via CLIP-Guided, Pixel-Level Optimization is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
2,DeepMind Study Shows That Language Models Can Learn From Explanations in Context Even Without Tuning,https://syncedreview.com/2022/10/25/deepmind-study-shows-that-language-models-can-learn-from-explanations-in-context-even-without-tuning/,2022-10-25,"If you have ever considered the answer to a question and asked “…but why?” you are not alone. Humans have an innate ability to improve their learning and broaden their understanding via explanations that relate examples to principles. The machine learning community in recent years has witnessed the rapid growth of few-shot prompting language models (LMs) that exhibit impressive transfer learning capability, enabling them to successfully perform new tasks by adapting to a few in-context examples. Might these LMs benefit, as humans do, from explanations of these few-shot examples? In the new paper Can Language Models Learn From Explanations in Context?, DeepMind researchers investigate how different types of explanations, instructions, and controls affect language models’ zero- and few-shot performance and how such explanations can support in-context learning for large language models on challenging tasks. The team highlights their main contributions as follows: The team considered a set of decoder-only transformer models ranging from 1 billion to 280 billion parameters and crafted a variety of control explanations that match different aspects of the semantics and word- or sentence-level content, including scrambled explanations, true non-explanations, and other item explanations. They tested model performance under each prompt condition on all task dataset items (except those in the prompt) and calculated the model’s likelihood of returning each answer option. They then chose the highest-likelihood answer from the set and evaluated model accuracy based on the answer scores defined by the task. In their empirical experiments, the team examined the benefits of different prompt components for the largest (280B parameter) LM and the relative distribution of benefits from different explanation types. They also provided raw summaries of the average effects of untuned explanations across model scales. Their findings can be summarized as follows: The researchers believe their work can contribute to improved prompt engineering and scientific understanding of the in-context learning abilities of large LMs. The paper Can Language Models Learn From Explanations in Context? is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
3,Google & Stanford Team Applies Chain-of-Thought Prompting to Surpass Human Performance on Challenging BIG-Bench Tasks,https://syncedreview.com/2022/10/24/google-stanford-team-applies-chain-of-thought-prompting-to-surpass-human-performance-on-challenging-big-bench-tasks/,2022-10-24,"Today’s large language models (LLMs) have demonstrated game-changing performance across a wide range of tasks and domains, but they have their limits. These weaknesses can be identified by the Beyond the Imitation Game benchmark (BIG-Bench, Srivastava et al., 2022), which evaluates LLM capabilities on a diverse suite of especially challenging tasks. A 540B parameter PaLM language model surpasses average human-rater performance on 65 percent of the BIG-Bench tasks, but what about the remainder — are they simply unsolvable by LLMs? A Google Research and Stanford University team addresses this question in the new paper Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them. The team applies chain-of-thought (CoT) prompting — a series of intermediate reasoning steps inspired by the paper Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (Wei et al., 2022b) — to 23 BIG-Bench tasks on which LLMs have failed to match the average human rater. Their strongest resulting model outperforms the human baseline on 17 of the 23 tasks. The team selected their 23 evaluation tasks — a subset they dub BIG-Bench Hard (BBH) — from BIG-Bench tasks where state-of-the-art LLMs perform worse than the average human rater, tasks fundamentally not solvable by scaling existing LLMs, and tasks that require prompting techniques beyond the standard few-shot prompting setup. In their experiments, the team applied the standard BIG-Bench answer-only prompting setup and the proposed CoT prompting approach on three language model families — Codex, InstructGPT and PaLM — to explore whether and to what extent CoT prompting can improve performance on the 23 BBH tasks. The results show that conventional answer-only prompting underestimates LLM performance and capabilities on challenging tasks that require multiple reasoning steps; as CoT prompting achieves double-digit improvements for all three models, surpassing the average human-rater score on 10 of the 23 tasks on PaLM, on 15/23 tasks on InstructGPT, and on 17/23 tasks on Codex. The paper also details the effects of CoT prompting on four BBH task categories: algorithmic and multi-step arithmetic reasoning, natural language understanding, use of world knowledge, and multilingual knowledge and reasoning.The data, prompts, and Codex model outputs are available on the project’s GitHub. The paper Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
4,"Wider, Not Deeper: Cambridge, Oxford & ICL Challenge Conventional Transformer Design Approaches",https://syncedreview.com/2022/10/20/wider-not-deeper-cambridge-oxford-icl-challenge-conventional-transformer-design-approaches/,2022-10-20,"Transformers have become a preferred architecture in the machine learning community, and building deeper models is the common approach for improving their performance. But is deeper necessarily better? In the new paper Wide Attention Is The Way Forward For Transformers, a research team from the University of Cambridge, Imperial College London, and the University of Oxford challenges the commonly held belief that deeper is better for transformer architectures, demonstrating that wider layers result in superior performance on natural language processing (NLP) tasks. The team summarizes their main contributions as follows: The paper first evaluates the impact of model aspect ratio — the ratio of layers to heads — on model accuracy, runtime performance, model size, and interpretability. Unlike conventional approaches, which focus on finding more efficient attention styles or using network architecture search (NAS) to obtain optimal combination operators, the team considers a more coarse-grained design space by changing the model aspect ratio. This enables them to evaluate novel architectures, such as a single-layer model with many parallel heads. The researchers performed experiments on four text classification tasks: sentiment analysis on the IMDb dataset at both the token and byte level, Listops 10-way classification, and byte-level document matching. They also investigated how widening the attention layer would affect ten different types of transformer attention mechanisms. The researchers summarize the empirical results as follows: Overall, this work shows that the proposed wide transformer networks can achieve performance comparable to or better than deep transformers. The researchers conclude that wider and shallower models are thus a “viable and desirable alternative” for transformers when there is no pretraining of weights or embeddings.The paper Wide Attention Is The Way Forward For Transformers is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
5,"Embedding Training With 1% GPU Memory and 100 Times Less Budget, an Open Source Solution for Super-Large Recommendation Model Training on a Single GPU",https://syncedreview.com/2022/10/19/embedding-training-with-1-gpu-memory-and-100-times-less-budget-an-open-source-solution-for-super-large-recommendation-model-training-on-a-single-gpu/,2022-10-19,"Deep recommendation models (DLRMs) have become critical for deep learning applications in IT companies. DLRMs can be used to improve user experience for video recommendations, shopping searches, and companies’ advertisements. However, DLRMs have several limitations, including difficulties managing too much user data, frequent model updates, and high training costs. DLRMs first search an embedding bag (EmbeddingBags), and then go through a dense DNN. Embedded tables usually hold more than 99% of the memory in the DLRM, and only 1% of the computation requirements. With the help of GPU’s on-chip high-speed memory (High Bandwidth Memory) and increased computing power, GPU has become the mainstream hardware for DLRM training. However, with the increasing research depth of recommendation systems, the embedding tables are also growing in size, and the limited GPU memory is not able to keep up. The question remains: How can we use GPU to efficiently train super-large DLRM models despite the limitation of GPU memory? Colossal-AI has successfully used a heterogeneous training strategy to increase the number of NLP model training parameters capacity by hundreds of times at the same hardware. Recently, it has used a software cache method which dynamically stores the embedding table in the CPU and GPU memory to extend the parameters to the recommendation system. In relation to the software cache design, Colossal-AI also incorporates pipeline prefetching which reduces software cache retrieval and data movement overhead by observing future training data. At the same time, it trains the entire DLRM model on the GPU in a synchronized update manner, which can be scaled to multiple GPUs with the widely used hybrid parallel training method. Experiments show that Colossal-AI only needs to keep 1~5% of the embedding parameters in the GPU, and is still able to maintain excellent end-to-end training speed. Compared with other PyTorch solutions, the memory requirements are reduced by an immense magnitude, with a single GPU being able to train a terabyte-level recommendation model. As a result, the cost advantage is significant. For example, only 5GB of GPU memory can be used to train a DLRM that occupies a 91GB Embedding Bag. The training hardware cost is reduced from two NVIDIA A100s totaling about 30,000 USD, to an entry-level graphics card like RTX 3050 which only costs about 300 USD. Open Source Repo：https://github.com/hpcaitech/ColossalAI The purpose of the embedding table is to map categorical variables into floating-point variables. The following figure shows the training process of the embedding table in DLRMs. First, it identifies the corresponding records for each feature in the embedding table, outputs a feature vector through reduction operations (i.e. max, mean, and sum operations) and then inputs them to the subsequent dense neural network. The embedding table DLRM training process is comprised mainly of irregular memory access operations, so it is severely limited by the hardware memory access bandwidth. In real applications, the embedding table of a DLRM may reach hundreds of GB, or even TB levels, far exceeding the single GPU capacity of only a few tens of GB. There are many ways to increase the size of DLRM’s embedded table. Taking the memory hierarchy diagram of the GPU cluster shown in the figure below as an example, we can analyze the advantages and disadvantages of several common solutions. GPU model parallelism: During this method, the embedding table is sharded and distributed in the memory of multiple GPUs, and the intermediate results are synchronized via the interconnection network between GPUs. The disadvantage of this method is that the workload of the embedded table may not be equal, and it is difficult to scale. Furthermore, the initial investment of adding GPUs is high, and the computing power of GPU is not fully utilized. DLRM mainly utilizes the HBM bandwidth of GPUs, while computing units are not utilized well. Hybrid Training: This method starts by splitting the embedding table into two parts, one trained on the GPU and the other trained on the CPU. By using long-tail input data distribution, we can minimize the CPU computing and maximize GPU computing. However, as the batch size increases, it becomes difficult to ensure that all the data in the mini-batch hits the CPU or GPU. Additionally, since the DDR bandwidth and HBM differ by magnitude, even if just 10% of the input data is trained on the CPU, the entire system will slow down by at least two times. The CPU and GPU also need to transmit intermediate results, which requires lots of overhead communication, further slowing down the training speed. Consequently, researchers have designed methods like asynchronous updates to combat these issues, but asynchronous methods can cause uncertainty in training accuracy, and are not ideal for algorithm engineers. Software Cache: All training is performed on the GPU with this method, and the embedding tables are kept in the heterogeneous memory space composed of the CPU and GPU. Each time the software cache is used, the used part is exchanged into the GPU. In this way, storage resources are expanded inexpensively while meeting the increased demand for embedded tables. Compared to using the CPU to calculate, the entire training process is completed on the GPU, making full use of the HBM bandwidth advantage. On the contrary, cache query and data movement of this method will bring additional performance loss. Currently, there are some excellent software cache solutions for embedding tables, but they are often implemented using customized EmbeddingBags Kernel, such as fbgemm, or with the help of third-party deep learning frameworks. With native PyTorch, Colossal-AI can implement a unique set of software Cache EmbeddingBags, further optimize the DLRM training process, and propose a prefetch pipeline to further reduce Cache overhead. Colossal-AI implements a class of CachedEmbedding which works as a subclass of the nn.Module in PyTorch and can replace the native PyTorch EmbeddingBag. It consists of software which manages the CPU and GPU memory. It maintains EmbeddingBag parameters as CPU Weight. A small part of the EmbeddingBag called the CUDA Cached Weight is stored as GPU memory, which will be used for future training. During DLRM training, records of the embedding table are first identified by the current mini-batch. If some records are not in the GPU yet, they are transmitted from the CPU Weight to the CUDA Cached Weight. If there is not enough space in the GPU, the LFU algorithm will be used to discard the least frequently used embedding records. In order to query the cache efficiently, some auxiliary data structures are needed: the cached_idx_map is a 1-D array mapping the indices of records in the CPU Weight to the indices of CUDA Cached Weight, as well as the GPU access frequency. The ratio of the CUDA Cached Weight size to CPU Weight size is named cache_ratio and defaults to 1.0%.The cache operates before each forward iteration to adjust the data in the CUDA Weight in three steps. Step 1:Query CPU records: Query record indices of CPU Weight that need to be cached. This requires intersecting the cached_idx_map and the input of the current mini-batch. Step 2: Query GPU records: Identify CUDA Cached weight that should be evicted according to frequency. This requires performing a top-k operation on the different sets of cache_idx_map and input of the current mini-batch. Step 3: Data transmission: Free enough space on the CUDA Cached Weight for CPU Weight, which may lead to queried GPU records being transferred from GPU to CPU. Then move the to-be queried CPU records from the CPU Weight into CUDA Cached Weight. The processes in Step 1 and Step 2 of the Cache are memory demanding. In order to take advantage of the bandwidth on the GPU’s HBM, they are run on the GPU and implemented using drop-in API provided by PyTorch. The overhead of Cache operations is particularly prominent compared to the training operations of the embedding table on the GPU. For example, for a training task that takes 199 seconds, the overhead of the cache operation is 99 seconds, which accounts for nearly 50% of the overall computing time. The main overhead of the Cache is mainly caused by Step 1 and Step 2 in the cache operation, and the base in the figure below shows the total time decomposition of the cache operation. The red and orange stages (Step 1, 2) account for 70% of the total cache overhead. The problem above arose because the traditional Cache strategy is somewhat “short-sighted”, so the Cache is adjusted according to the input of the current mini-batch, and most of the time is wasted on query operations. In order to reduce the overhead time of the Cache, Colossal-AI has designed a “far-sighted” Cache mechanism. Instead of only performing Cache operations on the first mini-batch, Colossal-AI fetches several mini-batches that will be used later, and performs Cache query operations together. As shown in the figure below, Colossal-AI uses prefetching to merge multiple mini-batches of data and conduct one cache operation after merging. It also uses a pipeline method to overlap the overhead of data loading and model training. As shown in the following figure, the number of mini-batches prefetched in the example is 2. Before starting training, it loads mini-batch 0 and 1’s data from disk to GPU memory, conducts Cache operation, and then performs forward and back propagation and a parameter update of these two mini-batches. This can simultaneously be read with the initial data of mini-batch 2 & 3, and this part of the overhead can overlap with the calculation. Compared with the execution mode of baseline cache, Figure [Time decomposition of Cache operation] compares the time decomposition of cache operation using 8 mini-batches prefetching with a baseline cache without prefetching. The total training time dropped from 201 seconds to 120 seconds, and the proportion of cache queries shown in the figure also dropped significantly. To sum up, Cache pipeline prefetching brings two benefits. The most obvious benefit of prefetching is reducing cache operation’s Step 1 and Step 2 overhead, so that this two-step operation accounts for less than 5% of the total training process. As shown in Fig. [Time Decomposition of Cache Operations], by pre-fetching 8 mini-batches of data, the overhead of cache queries is significantly reduced compared to the baseline. By concentrating more data and improving the granularity of data transmission, the CPU-GPU transmission bandwidth can be fully utilized. For the example above, the CUDA→CPU bandwidth is increased from 860MB/s to 1477MB/s, and the CPU→CUDA bandwidth is increased from 1257MB/s to 2415MB/s, almost double the performance gain. Our CachedEmbeddingBag is consistent with the basic usage of the PyTorch EmbeddingBag. When building a recommendation model, only a few lines of code can significantly increase the capacity of the embedding table and complete TB super-large recommendation model training at a low cost. The testbed is a GPU cluster with 8x NVIDIA A100 GPU (80GB) and AMD EPYC 7543 32-Core Processor (512GB) CPU. Colossal-AI also uses Meta’s DLRM model implementation, and evaluates on a Cretio 1TB dataset, as well as a synthetic dataset. The PyTorch training with the entire embedding table on the GPU is used as the baseline in the experiments. ### Cretio 1TB embedding table has 177,944,275 records, and its memory allocation takes up 91.10GB, with embedding dim equal to 128. To accommodate EmbeddingBags on one single GPU is impossible, even with top-end NVIDIA A100 with 80GB GPU memory. Hopefully, Colossal-AI will make it possible to accomplish the training task on one GPU, with memory consumption dropping to 5.01 GB (lowering approx. 18 times), and show the possibility of training super large (terabyte-level) recommendation system models on just one GPU. In terms of training speed, the following figure shows the latency of training 100M samples with different batch sizes. Prefetch1 (shown in dark green) is the latency without pre-fetching, and Prefetch8 (shown in blue) is the latency with prefetching (prefetch mini-batch=8). This shows that prefetch flow optimization plays an important role in overall performance improvement. Each bar colored with darker colors in the figure is part of the Cache overhead, being controlled within 15% of the total training time after pre-fetching. In our experiment, DLRM is trained with 100M samples on 8 GPUs, using table-wise sharding as EmbeddingBags in a parallel manner (global batch size = 8192, prefetch size = 4). The following figure shows the training latency for cases with a different number of GPUs. ColossalAI-cr-0.05 in the figures indicates the cache ratio is 0.05, while Colossal-cr-0.5 is 0.5. The PyTorch and Colossal-AI training times are mostly similar, but PyTorch encounters OOM issues when training on 1 GPU. It can be observed that adding GPUs (increasing to 4 or 8) does not bring significant performance benefits as synchronizing results requires huge communication overhead and table-wise sharding, leading to an unbalanced slice load. In other words, using multiple GPUs to scale embedding table training does not have significant advantages. The graph below shows the maximum memory usage, varying across different numbers of GPUs. When using one GPU, only a software Cache method from Colossal-AI works, with the memory assumption of multiple cards in parallel showing a significant reduction. The synthetic dlrm_datasets from Meta Research mimic the training access behavior of embedding tables. As a result, it is usually used as a reference for testing hardware and software designs that relate to recommendation systems. Subsequently, 500 million rows of these embedding table items are selected as sub-datasets, and two EmbeddingBags of 256GB and 128GB are constructed for testing. With GPU memory limitations, PyTorch has poor performance when training on one NVIDIA A100 GPU. In contrast, Colossal-AI’s software cache significantly eases GPU memory requirements, is capable of training embedding tables as large as 256GB, and also shows the potential to scale to terabytes. The acceleration is also demonstrated by running prefetching, where total training time decreases by 60% (#prefetches = 32) and GPU memory demand does not increase. Colossal-AI is a user-friendly deep learning system that allows companies to maximize AI deployment efficiency while drastically reducing costs. Since becoming open source to the public, Colossal-AI has reached №1 in trending projects on GitHub and Papers With Code multiple times, amidst other projects that have as many as 10K stars. Colossal-AI values open source community construction, providing English and Chinese tutorials, while supporting the latest cutting-edge applications such as PaLM and AlphaFold. Ultimately, Colossal-AI is constantly increasing the availability of AI solutions across a variety of fields, including medicine, autonomous vehicles, cloud computing, retail, chip production, etc. PortalProject address: https://github.com/hpcaitech/ColossalAI Reference Embedding Training With 1% GPU Memory and 100 Times Less Budget, an Open Source Solution for Super-Large Recommendation Model Training on a Single GPU | by Yang You | Oct, 2022 | Medium https://ai.facebook.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/ We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
6,Meet Magneto: Microsoft’s Foundation Transformer for General-Purpose Modelling Across Tasks and Modalities,https://syncedreview.com/2022/10/18/meet-magneto-microsofts-foundation-transformer-for-general-purpose-modelling-across-tasks-and-modalities/,2022-10-18,"The machine learning community has seen a trend in recent years, with researchers working to converge their model architectures across language, vision, speech, and multimodal classes. While transformer architectures have become the de facto standard for building such highly desirable general-purpose foundation models, the optimal transformer variants still differ for different input modalities. In the new paper Foundation Transformers, a Microsoft team proposes a method for true general-purpose modelling. Their Foundation Transformer is a single unified transformer that provides guaranteed training stability and is capable of handling diverse tasks and modalities without performance degradation. The team first identifies the properties a foundation model should possess for true general-purpose modelling: 1) The desired modelling should be able to serve as a go-to architecture for various tasks and modalities, so that we can use the same backbone without trial and error, and 2) The architecture should provide guaranteed training stability. The proposed Magneto is a Foundation Transformer implementation designed to achieve the abovementioned goals. Magento uses Sub-LayerNorm (Sub-LN), which adds another LayerNorm inside each sublayer. The team also introduces a novel initialization method theoretically proven to guarantee training stability, enabling the model to be scaled relatively easily. In their empirical studies, the team compared Magneto with popular transformer variants such as BERT, GPT, and BEiT-3 on a wide range of tasks and modalities, including natural language processing, speech recognition, vision tasks, etc. Magneto significantly surpassed its baseline counterparts in the experiments. Moreover, it was shown to be more stable in terms of optimization, indicating its potential for effectively scaling up all manner of transformer models.The paper Foundation Transformers is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
7,Stanford U & Google Brain’s Classifier-Free Guidance Model Diffusion Technique Reduces Sampling Steps by 256x,https://syncedreview.com/2022/10/17/stanford-u-google-brains-classifier-free-guidance-model-diffusion-technique-reduces-sampling-steps-by-256x/,2022-10-17,"Denoising diffusion probabilistic models (DDPMs) with classifier-free guidance such as DALL·E 2, GLIDE, and Imagen have achieved state-of-the-art results in high-resolution image generation. The downside to such models is that their inference process requires evaluating both a class-conditional model and an unconditional model hundreds of times, rendering them prohibitively compute-expensive for many real-world applications. In the new paper On Distillation of Guided Diffusion Models, researchers from Google Brain and Stanford University propose a novel approach for distilling classifier-free guided diffusion models with high sampling efficiency. The resulting models achieve performance comparable to the original model but with sampling steps reduced by up to 256 times. The researchers’ distillation approach comprises two steps: Given a trained guided teacher model, a single student model first matches the combined output of the teacher’s two diffusion models, and this learned student model is then progressively distilled to a fewer-step model. The resulting single distilled model can handle a wide range of different guidance strengths and enable efficient tradeoffs between sample quality and diversity. The proposed sampling method employs a deterministic sampler and a novel stochastic sampling process. One deterministic sampling step is first applied with two times the original step length, and one stochastic step is then performed backward (i.e., perturb with noise) using the original step length. This approach was inspired by Karras et al.’s paperElucidating the Design Space of Diffusion-Based Generative Models, published earlier this year. In their empirical study, the team applied their method to classifier-free guidance DDPMs and performed image generation experiments on the ImageNet 64×64 and CIFAR-10 datasets. The results show that the proposed approach can achieve “visually decent” samples using as few as one step and obtain FID/IS (Frechet Inception Distance/Inception) scores comparable to that of the original baseline models while being up to 256 times faster to sample from. Overall, this work demonstrates the effectiveness of the proposed approach in addressing the high computational costs that have limited the deployment of denoising diffusion probabilistic models.The paper On Distillation of Guided Diffusion Models is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
9,"‘Ask Me Anything’: Stanford U, Numbers Station & UW Madison’s Novel Prompting Strategy Enables LLMs With 30x Fewer Parameters to Outperform Few-Shot GPT3-175B",https://syncedreview.com/2022/10/12/ask-me-anything-stanford-u-numbers-station-uw-madisons-novel-prompting-strategy-enables-llms-with-30x-fewer-parameters-to-outperform-few-shot-gpt3-175b/,2022-10-12,"Large language models (LLMs) have taken a step toward task-agnostic machine learning by leveraging user prompts — instructions written in natural language — to help them target specific tasks without additional training or fine-tuning. Prompts can significantly boost model performance, but designing the perfect prompt, aka “prompt engineering,” remains a time-consuming, hands-on process that often comes down to trial and error. In the new paper Ask Me Anything: A Simple Strategy for Prompting Language Models, a research team from Stanford University, Numbers Station, and the University of Wisconsin-Madison presents Ask Me Anything Prompting (AMA), a simple LLM prompting strategy that aggregates multiple “effective yet imperfect” prompts to enable a 30x smaller language model to outperform few-shot GPT3-175B. The team summarizes their main contributions as follows: The researchers first explore different prompt formats, concluding that open-ended question-answering (QA) prompts (e.g. “Who went to the park?”) outperform prompts that restrict the model to particular tokens (e.g. “John went to the park. Output True or False”). They recursively use the LLM to transform task inputs to the effective open-ended question-answering format noted above, collecting multiple candidate prompts with different accuracies and complex dependencies. Finally, they apply a weak supervision (WS) technique to aggregate the outputs and produce final predictions that demonstrably improve the prompting reliability and performance of off-the-shelf LLMs without further training. In their empirical study, the team evaluated AMA’s impact on the out-of-the-box few-shot performance of four open-source LLMs (EleutherAI, OPT, BLOOM, and T0) on seven tasks. In the experiments, AMA achieved an average improvement of 10.2 percent over the few-shot baselines; and also enabled a 30x smaller LLM to outperform few-shot GPT3-175B on 15 of 20 popular benchmarks. Overall, this work validates the effectiveness of the proposed AMA prompting strategy. The team believes AMA could also benefit LLM applications that involve private data or require operating over large amounts of data. The AMA code is available on the project GitHub. The paper Ask Me Anything: A Simple Strategy for Prompting Language Models is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
10,Maximizing FLOPS Utilization: DeepMind & NYU Propose Efficiency Evaluations for Visual Pretraining Methods,https://syncedreview.com/2022/10/11/maximizing-flops-utilization-deepmind-nyu-propose-efficiency-evaluations-for-visual-pretraining-methods/,2022-10-11,"While self-supervised learning (SSL) has achieved impressive results in recent years thanks to complex data augmentation techniques and lengthy training schedules, these approaches also lead to extremely high computation costs. Given a fixed FLOPS budget, is it possible to identify the best datasets, models, and self-supervised training strategies for obtaining high accuracy on visual tasks? In the new paper Where Should I Spend My FLOPS? Efficiency Evaluations of Visual Pre-training Methods, a research team from DeepMind and the NYU Center for Neural Systems introduces evaluation approaches designed to measure the computational efficiency of various visual pretraining strategies across multiple datasets and model sizes and aid in the selection of optimal methods, datasets and models for pretraining visual tasks on a fixed FLOP budget. Previous studies on SSL have mainly focused on improving performance with little regard for the associated computational costs. This work takes the first steps toward identifying computationally optimal pretraining methods, datasets and models. The team analyzes four common self-supervised methods (BYOL, SimCLR, DINO, and MAE) and two supervised methods (CLIP and standard softmax classification). The methods’ per gradient-step FLOP costs are computed and used for comparisons across three axes: pretraining method, model size, and dataset. Downstream task performance is measured by finetuning the pretrained encoders on semantic segmentation tasks on the ADE20K dataset. Based on the evaluations, the team concludes that: 1) Self-supervised methods are generally less FLOP efficient and supervised representations dominate the efficiency Pareto-front; 2) For most methods, the small and large model curves intersect, indicating the point at which it is better to switch to larger model sizes for a given FLOP budget; 3) Dataset quality and curation level significantly affect model accuracy. The team sees their work as a first step towards more rigorously measuring the computational efficiency of contemporary supervised and self-supervised pretraining approaches in terms of pretraining method, dataset and model size. They hope their results will spark future research into visual SSL methods that learn more effectively and scalably on uncurated data. The paper Where Should I Spend My FLOPS? Efficiency Evaluations of Visual Pre-training Methods is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
12,Google & TUAT’s WaveFit Neural Vocoder Achieves Inference Speeds 240x Faster Than WaveRNN,https://syncedreview.com/2022/10/05/google-tuats-wavefit-neural-vocoder-achieves-inference-speeds-240x-faster-than-wavernn/,2022-10-05,"A neural vocoder is a neural network designed to generate speech waveforms given acoustic features — often used as a backbone module for speech recognition tasks such as text-to-speech (TTS), speech-to-speech translation (S2ST), etc. Current neural vocoders however can struggle to maintain high sound quality without incurring high computational costs. In the new paper WaveFit: An Iterative and Non-autoregressive Neural Vocoder based on Fixed-Point Iteration, a team from Google Research and the Tokyo University of Agriculture and Technology presents WaveFit, a fast and high-quality neural vocoder that achieves natural human speech with inference speeds that are 240 times faster than WaveRNN. The first breakthrough in neural vocoder development was the introduction of autoregressive (AR) models such as WaveNet (van den Oord et al., 2016), which revolutionized the quality of speech generation but proved inefficient as they required a huge number of sequential operations for signal generation. Non-AR models were subsequently proposed to speed up inference speeds, with denoising diffusion probabilistic models (DDPMs) and generative adversarial networks (GANs) among the most popular. Generating human-comparable speech waveforms in a few iterations however remains challenging, and typically involves an undesirable trade-off between sound quality and computational cost. The proposed WaveFit non-AR neural vocoder is inspired by the theory of fixed-point iteration and introduces a novel method for combining DDPMs and GANs to boost the performance of conventional non-AR models. WaveFit iteratively applies a DNN as a denoising mapping that eliminates noise components from an input signal. A GAN-based and a short-time Fourier transform (STFT)- based loss are combined to produce a loss function that is insensitive to imperceptible phase differences and to encourage the intermediate output signals to approach the target speech along with the iterations. In their empirical study, the team evaluated WaveFit on subjective listening experiments and compared it with baselines that included WaveRNN, DDPM-based models and GAN-based models. The results show that WaveFit with five iterations can generate synthetic speech with audio quality comparable to that of WaveRNN and natural human speech while achieving inference speeds more than 240 times faster than WaveRNN. Audio demos are available at google.github.io/df-conformer/wavefit/. The paper WaveFit: An Iterative and Non-autoregressive Neural Vocoder based on Fixed-Point Iteration is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
13,UNC Chapel Hill’s Textless Vision-Language Transformer: Comparable Performance to Text-Based Approaches but 28x Faster,https://syncedreview.com/2022/10/04/unc-chapel-hills-textless-vision-language-transformer-comparable-performance-to-text-based-approaches-but-28x-faster/,2022-10-04,"Transformer architectures have achieved impressive performance in vision-language (VL) representation learning when trained on text-annotated images or videos. It remains challenging, however, for transformers to learn VL representations without relying on text, i.e. using only low-level visual and acoustic inputs. In the new paper TVLT: Textless Vision-Language Transformer, researchers from UNC Chapel Hill present the Textless Vision-Language Transformer (TVLT) for vision-and-language representation learning. TVLT uses only raw visual and audio inputs and performs comparably to its text-based counterparts but requires only 1/3 the parameters and achieves 28x faster inference speeds. The TVLT’s main architecture is a transformer comprising a 12-layer encoder and an 8-layer decoder. It takes its inputs as a list of embeddings obtained directly from perception-level video and audio and does not include any text-specific modules for automatic speech recognition (ASR) or tokenization. The input embeddings are a combination of 1) modality embedding, 2) temporal/spatial embeddings for video, 3) temporal/frequency embeddings for audio, and 4) vision/audio patch embeddings. The TVLT is pretrained with two objectives: vision-audio matching (VAM) and masked autoencoding (MAE). VAM is employed to learn the global cross-modal representations, and a linear layer with sigmoid activation is then applied to the encoder to obtain a matching probability. Finally, the binary cross-entropy loss is computed. MAE is used to improve unimodal representations by masking random patches of visual frames and the audio spectrogram and reconstructing missing inputs. The novel approach slices the audio and video parts of the encoder output and feeds them to the decoder independently instead of jointly, which saves compute costs and boosts finetuning performance. In their empirical study, the team compared TVLT with text-based counterparts on audio-to-video retrieval, video-based multimodal sentiment analysis, and visual question-answering benchmarks. In the experiments, TVLT achieved performance competitive with state-of-the-art audio-based vision-and-language models on visual question answering, image retrieval, video retrieval and multimodal sentiment analysis. Moreover, it required only 1/3 of the parameters, and its inference speed was 28x faster than the text-based methods. Overall, this paper showcases the powerful performance of TVLT and advances the possibility of learning compact and efficient visual-linguistic representations from low-level visual and audio signals without the need for traditional but computationally expensive text modelling. The code and checkpoints are available on the project’s GitHub. The paper TVLT: Textless Vision-Language Transformer is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
14,Google & DeepMind Propose Geometric Complexity for DNN Analysis and Evaluation,https://syncedreview.com/2022/10/03/google-deepmind-propose-geometric-complexity-for-dnn-analysis-and-evaluation/,2022-10-03,"Bigger is not always better. While large language models and complex deep neural networks (DNNs) have resulted in huge performance gains across a variety of AI-related tasks, lighter and simpler models are often preferable in industrial applications. It is thus crucial for continued efficient DNN development and deployment that the machine learning research community improves its understanding of fundamental model complexity control methods such as regularization. In the new paper Why Neural Networks Find Simple Solutions: The Many Regularizers of Geometric Complexity, a research team from Google and DeepMind proposes Geometric Complexity (GC), a measure of DNN model complexity that serves as a useful tool for understanding the underlying mechanisms of complexity control. The team summarizes their main contributions as follows: Previous studies have proposed numerous complexity measures — naive parameter count, data-driven approaches, VC dimension and Rademacher complexity — but most of these fail to clarify the properties of regularizers and the connections between implicit and explicit regularizers. The proposed GC aims at solving these issues. The researchers provide a clear definition of GC and how it relates to important aspects of deep learning, including linear models, ReLU networks, Lipschitz smoothness, arc length, and harmonic maps. The paper explores the impacts of initialization, explicit regularization and implicit regularization on geometric complexity; examining different parameter initialization choices, L2 regularization, Lipschitz regularization via spectral norm regularization, noise regularization, flatness regularization, explicit GC regularization and Jacobian regularization. The researchers conclude that common training heuristics such as parameter norm regularization, spectral norm regularization, flatness regularization, implicit gradient regularization, noise regularization and the choice of parameter initialization can all play a part in reducing geometric complexity. Finally, the team demonstrates that GC can also capture double-descent behaviour in the test loss when a model’s parameter count increases. Overall, this paper validates GC as an effective tool for understanding DNN models and sheds light on how DNNs achieve low test errors with highly expressive models. The team hopes their work will encourage further research in this area and lead to a better understanding of current best practices and the discovery of new methods for efficient model training. The paper Why Neural Networks Find Simple Solutions: The Many Regularizers of Geometric Complexity has been accepted by the 36th Conference on Neural Information Processing Systems (NeurIPS 2022), which runs from November 28 to December 9 in New Orleans, USA; and is available on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
15,Hindering Adversarial Attacks with Implicit Neural Representations,"[{'href': 'http://arxiv.org/abs/2210.13982v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2210.13982v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-10-22 13:10:24,"Bootstrap Advantage Estimation for Policy Optimization in Reinforcement Learning Md Masudur Rahman, Yexiang Xue Department of Computer Science Purdue University, West Lafayette, Indiana, USA {rahman64, yexiang}@purdue.edu 2 2 0 2 t c O 3 1 ] G L . s c [ 1 v 2 1 3 7 0 . 0 1 2 2 : v i X r a Abstract—This paper proposes an advantage estimation ap- proach based on data augmentation for policy optimization. Unlike using data augmentation on the input to learn value and policy function as existing methods use, our method uses data augmentation to compute a bootstrap advantage estimation. This Bootstrap Advantage Estimation (BAE) is then used for learning and updating the gradient of policy and value function. To demonstrate the effectiveness of our approach, we conducted ex- periments on several environments. These environments are from three benchmarks: Procgen, Deepmind Control, and Pybullet, which include both image and vector-based observations; discrete and continuous action spaces. We observe that our method reduces the policy and the value loss better than the Generalized advantage estimation (GAE) method and eventually improves cumulative return. Furthermore, our method performs better than two recently proposed data augmentation techniques (RAD and DRAC). Overall, our method performs better empirically than baselines in sample efﬁciency and generalization, where the agent is tested in unseen environments. Index Terms—Deep Reinforcement Learning, Advantage Esti- mation, Generalization in Reinforcement Learning I. INTRODUCTION The policy gradient method directly involves learning policy function, which enjoys performance improvement in function approximation settings [1]. The policy gradient theorem gives a rather simple formulation of the gradient estimation, which gives an unbias estimation [2]. However, it requires the re- turn estimation of the entire trajectory, leading to very high variance. A commonly used technique to reduce variance is to use a baseline, which can help reduce variance without introducing bias. Several effective methods originated from this concept [3], [4]. An effective way is to use the value function as a baseline to indicate whether the action taken by the current policy is better than the average action taken in that state, which can be formulated as an advantage estimation. However, based on a single trajectory, the estimate can be local and have high variance. Thus we can add a truncated scenario where the value function can potentially give a global estimate. Combining these two is the Generalized advantage estimation [5] which shows strong empirical results [6]. However, due to procedural style content generation, the value estimation can be erroneous and give different advantage estimates even when the observation context changes. At the same time, the semantic meaning remains the same. As the procedural scenario can exist in a real-world scenario and might cause agent to perform sup-optimally [7], [8], thus this advantage estimation can be problematic in those scenarios, resulting in poor sample efﬁciency. This issue might exist partly due to difﬁculty in reducing policy estimation (policy loss) and value function estimation (value loss). the reward semantic remains the same, but This paper proposes Bootstrap Advantage Estimation (BAE), which calculates the advantage estimation by comput- ing advantage estimation on the original and its transformed observations. We assume the transformation to be a semantic invariant; the contextual information can be changed. For example, if the then changing the background of a game is not relevant, background color from red to green can be a semantic invariant transformation. The ultimate goal is to train an agent to be robust against any such background change, which performs well in the blue background in this example. The transformed observation can be of any form; we experimented with data augmentation-based observation transformation (e.g., random crop, amplitude scaling). The intuition is that taking advantage of estimate over augmented data forces the advantage estimate to consider the error over many variations of the observations. We demonstrate our BAE on the policy gradient method (i.e., PPO [3]) and show a comparison over GAE-based estimation. We observed that our method BAE achieved better sample ef- ﬁciency and zero-shot generalization in Procgen environments (starpilot and miner) with image-based observation. In recent times, data augmentation demonstrated an effective choice in improving sample efﬁciency in high-dimensional ob- servation space and improving generalization [9]–[11]. Though this process sometimes generates empirical success [10], such methods might lead to detrimental performance [11] as we observed in our experiments. To mitigate this issue, the DRAC [11] method suggests regularizing the policy network and value network by augmented observation and not using augmented data for policy training. In contrast, we propose a novel way to leverage data augmentation. Our method augmented observations for advantage estimation, one of the core components of many policy optimization algorithms (e.g., PPO). We conducted extensive experiments on six environ- ments consisting of image and vector-based observation; and discrete and continuous action spaces. Our method falls in the general model-free on-policy category, and we experimented with Proximal Policy Optimization (PPO) [3] in this paper. In particular, our experiments on Procgen Starpilot and Miner environments demonstrate that our method can be beneﬁcial in the zero-shot generalization setup compared to baseline GAE [5], and two data augmentation techniques: RAD [10], and DRAC [11]. We further evaluated our method on several robotic lo- comotion tasks with the high-dimensional observation from Deepmind Control Suite [12]: Quadruped Run, and Cartpole - Three Poles; and PyBullet [13]: Minitaur and HalfCheetah. In experiments, we observe that our method BAE performs better than baseline agents, including base PPO, RAD, and DRAC. Our method achieves a much lower loss for policy and value function estimation. Eventually, it performs better in sample efﬁciency and zero-shot generalization than baseline agents, including data augmentation. We observe that the base- line data augmentation methods (RAD and DRAC) sometimes worsen the base model performance. In contrast, our BAE method improves the performance in most tested environments and performs consistently with the base algorithm in other cases. These results show that our method BAE is more robust in performance compared to baseline data augmentation methods. The source code of our method is available at https://github. com/masud99r/bae. II. PRELIMINARIES AND PROBLEM SETTINGS Reinforcement Learning We assume the task/environment is a Markov Decision Process (MDP) which is denoted by M = (S, A, P, R, γ), where S is state space, A is that action space, P is the transition probability between states resulted by an action. In this setup, at every timestep t, the agent take an action at ∈ A in a state st ∈ S and the environment transition to next state st+1 ∈ S determined by the transition probability P (st+1|st, at). In a reinforcement learning framework, the goal of the agent is to learn a policy π ∈ Π by maximizing the expected return in an MDP, where Π is a set of all possible policies and π is a single policy which is a mapping from state to action. The policy which achieves the highest expected return is optimal π∗ ∈ Π. Policy Gradient Proximal Policy Optimization (PPO) [3], a type of policy gradient method which achieved tremendous success and is popularly used in many setups because of its effective learning and simple implementation. However, the choice of implementation details might impact the perfor- mance of such algorithms in signiﬁcant ways [6], [14], [15]. These implementation details consist of training individual components of the algorithms, such as learning value function, policy function, and advantage estimation. The following is the objective of the PPO [3]. Lπ = −Et[ πθ(at|st) πθold(at|st) At] (1) , where πθ(at|st) is the probability of choosing action at give state st at timestep t using the current policy parameterized by θ. On the other hand the πθold(at|st) refer to the probabilities using an old policy parameterized by previous parameter values θold. The advantage At is an estimation which is the advantage of taking action at at st. A popular and effective choice of estimating advantage using a value function is as follows (equation 2): At = −V (st) + rt + γrr+1 + ... + γT −t+1rT −1 + γT −tV (sT ). (2) Here V (s) is a value function that gives the average future return under the underlying policy. The ﬁrst term V (st) is the value prediction at timestep t, and the rest of the terms except the last term in the equation is the discounted Monte Carlo Estimation which can be computed for a given episode from t to T −1 (T > t). The last term V (sT ) is the value prediction at state sT . Thus overall, this At represents how much the current action at is doing compared to the current value prediction. The more the advantage of action, the more the policy should weigh that action. This is done by multiplying πθ(at|st) with At. The πθold(at|st) in Equation 2 introduced due to importance sampling which allows estimating the advantage from the old policy samples. For details discussion, we refer the reader to [3], [4]. Value Function Estimation An effective value function esti- mation [3] is to regress value prediction with an advantage- based return estimation. Here the Verror = |Vprediction − VReturn|, where Vprediction is the predicted value and the VReturn is value computed from the rewards R = (cid:80) t rt of sampled trajectories and advantage A. Thus, VReturn = A+R. Note that in this way, both the policy (in equation 1 and value function is dependent on the advantage estimation. Thus, an accurate advantage estimation should give us lower policy and value losses and thus a better performing policy. Generalization in RL Now we turn attention to the scenarios where different episode varies by confounding features in these confounders (also called the observation. Note that context)impact the reward in the environment; however, they might misguide the agent to think otherwise. Due to nature, the agent might overﬁt the confounding features and fails to generalize to slightly modiﬁed test environments [7], [8]. For a details overview of generalization in reinforcement learning, we refer the reader to the servery papers [16]. Data augmentation in various forms has been leveraged [10], idea is to transform the observation so [11]. The general that the observation’s semantic meaning remains the same but the contextual information changes. However, the context information is readily not available, and thus we need to impose various assumptions that certain transformations on the observation s(cid:48) = f (s) keep the reward semantic. For image- based observation, various image manipulation can be used, such as cropping, rotation, and color-jitter. Advantage Estimation An essential component in policy training is to estimate the advantage. Generalized Advantage Estimation (GAE) [5] is a useful way to compute advan- tage, which combines the value function estimate and the Monte Carlo method. However, the data augmentation-based regularization approaches [10], [11] do not handle the case of advantage estimation. The advantage is estimated using a single trajectory; thus, the computed advantage has a high variance due to the systematic noise in the advantage esti- mation. Given two similar states (observation), the advantage estimation should be the same. For example, an observation with the same semantic but a different background (red and blue) should have the same advantage if the background is not essential and thus confounded. III. BOOSTRAP ADVANTAGE ESTIMATION (BAE) We proposed to bootstrap the advantage estimation using observation transformation to mitigate the abovementioned issue. Formally, we generate m additional estimation with m transformation. For each such transformation i, we compute an estimate as in equation 3. A(k,i) t = −V (f (st+1, vi))+rt+γrr+1+...+γT −tV (f (st+k, vi)), (3) where v0 refer to no augmentation. Furthermore, ﬁnally, we take the average of all estimates as in equation 4 to estimate the ﬁnal advantage estimation for k-step return. A(k,b) t = 1 m + 1 (A(k,0) t + A(k,1) t + A(k,2) t + .. + A(k,m) t ) (4) Finally, we can achieve bootstrap advantage estimation of a trajectory of length T by combining several k-step returns using exponential-weighted average as in 5 following the GAE method [5]. ABAE(γ,λ) t +...+λT −1A(T,b) = (1−λ)(A(1,b) t +λA(2,b) ) (5) t t This ABAE(γ,λ) is used to compute the advantage at state st t of timestep t in an episode. Note that, our BAE differs from the GAE [5] in computing the k-step return as in equation 4. Algorithm 1 shows the details step of using our BAE with the PPO-based policy optimization method. In this paper, we Algorithm 1 BAE for Policy Optimization 1: Get transformation function f (s, v) with augmentation type v for each environment step do 2: Get PPO for policy optimization RL agent 3: for each iteration do 4: 5: 6: 7: 8: 9: at ∼ πθ(at|st) st+1 ∼ P (st+1|st, at) rt ∼ R(st, at) B ←− B ∪ {(st, at, rt, st+1)} 10: 11: 12: end for Transform all s ∈ B to get B(cid:48) using v augmentation with function f (s, v). Compute Bootstrap Advantage Estimate (BAE) from data B and B(cid:48) using equation 5. Perform PPO updates with BAE to optimize for Lπ as in equation 1 13: end for leverage PPO [3] as the base RL algorithm, which uses gen- eralized advantage estimation (GAE) as the default estimator. In contrast, our method BAE-PPO uses bootstrap advantage estimation (BAE) instead of GAE. The data augmentation baselines RAD and DRAC use base PPO with GAE advantage estimation. In the experiments we use m = 1 in equation 4. This means we use one data augmentation approach and combine it with original advantage estimation as in equation 5. Note that we do not apply any observation transformation in other parts of the agent objective, and thus equation 2 remains theoretically and practically sound. Furthermore, we empirically show how our Bootstrap Advantage Estimation leads to a smaller value, policy loss, and performance boost. Finally, we also compared the baseline RAD [10] and DRAC [11] and show that our method performs better in many setups. IV. EXPERIMENTS A. Setup Environments We experimented with image-based observa- tions with discrete action space and vector-based observations with continuous action space. Procgen We use Procgen [17]: Starpilot and Miner (Figure 1) which use image-based observation and procedural generation to produce challenging game logic that changes episode by episode. This benchmark allows for evaluating both sample efﬁciency and generalization capacity of RL agents. Each environment has around 100K levels. A subset of levels can be used to train the agent, and then the full distribution, that is, 100K levels, can be used to test the agent’s generalization capacity. For our experiment, we use the standard evaluation protocol from [17]; 200 levels of each environment are used for training in the difﬁculty level easy. All the environments have discrete action space of dimension 16. Intuitively, during training, the agent has access to a limited number of envi- ronment variability (e.g., 200 levels). The trained agent is tested on all the available variabilities, which consist of unseen scenarios. Thus, to master the game, the agent must focus on essential aspects of the state and ignore irrelevant information such as background color. Fig. 1. Procgen: Some snapshots of Starpilot and Miner. The environments are generated procedurally, which results in different observations (e.g., background) in each episode. two environments Deepmind Control We use from dm control [12]: Quadruped run, and Cartpole with three poles (Figure 2). The Quadruped Run has high-dimensional vector observation, and the task is to run as far as possible. On the other hand, the Cartpole variation consists of three procedurally generated poles. The complexity of these environ- ments is suitable for evaluating the data augmentation-based approaches. [Left] Deepmind Control: Some snapshots of Deepmind Control Fig. 2. tasks. [Right] Pybullet: Some snapshots of Pybullet Minitaur quadruped and HalfCheetah environments. These environments contain vector-based state space, and the action space is continuous. Fig. 3. Starpilot Env. Training time policy and value loss [lower is better]. Our method achieves lowest value and policy losses than the base algorithm (GAE-PPO) and data augmentation baselines (RAD and DRAC). Pybullet We use Pybullet [13]: Minitaur quadruped and HalfCheetah environments with vector-based observation. Each observation consists of raw sensory inputs. The Minitaur quadruped is a 4-legged robot, and the task is to travel as long as possible on ﬂat ground. Furthermore, the HalfCheetah is a two-legged robot that can control its movement in 2D, and the task is to travel as much distance as possible. The action spaces are continuous in these environments. Snippets of these environments are in Figure 2. Baselines All agents usage on-policy PPO [3] as the base policy. We compare our method with Generalized Advantage Estimation (GAE) [5] which is referred to as GAE-PPO in our experiments. GAE is shown to perform better compared to other advantage estimation techniques [6]. Moreover, we compare with the data augmentation-based approach uses data augmentation to transform the observation and then uses the transformed observation to train the base policy. In particular, we compare our method with existing baselines RAD [10] and DRAC [11]. RAD, referred to as RAD-PPO, proposes various data augmentation techniques to improve learning from pixel-based observation. DRAC, referred to as DRAC- PPO leverages the data augmentation to regularize the policy and value learning, showing improved performance in policy learning. In our method, we replaced the GAE estimation with our proposed Bootstrap Advantage Estimation (BAE), which is referred to as BAE-PPO. the agents, including our BAE-PPO and baseline, using the implementation available in [15]. In a PPO-based scenario, many factors have been identiﬁed as key in implementing algorithms that impact the performance [6], [14]. Thus, we use the same implementation logic for all the baselines and our method for a fair comparison. Data augmentation We evaluate Cutout Color data augmen- tation for image-based observation, which performs best in our setup compared to another popularly used Random Crop. Thus, we report Cutout Color data augmentation results for RAD, DRAC, and our BAE. We use the implementation avail- able in RAD [10] for data augmentation. For the vector-based observation robotic task, we use a random amplitude scale proposed in RAD [10]. This method multiply the observation with a number generated randomly between a range α to β. We used best performing range α = 0.8 to β = 1.4 for our We build all BAE method, and a range α = 0.6 to β = 1.2 for RAD, and DRAC (suggested in RAD [10]). Implementation and Hyper-parameters For the Procgen Starpilot and Miner, we report mean and standard deviation across 3 seeds run following the setup of Procgen paper [17]. We used an Nvidia A100 GPU to run agents with the IMPALA CNN model [18] on the image observation-based Procgen environments. We use neural networks to represent policy and value functions for vector-based observations. For Deepmind control environments, we report results with 10 random seed runs, and for Pybullet environments, we report results over 5 seeds. For all experiments, we keep the common hyperparam- eters the same for a fair comparison. The implementation and hyperparameters are based on [15], [19]. For all results, we report the mean (showed in solid line) and standard deviation (showed in shaded areas) across runs. B. Results The PPO-based agent’s objective consists of value loss and policy loss. The objective is to reduce them and potentially improve the expected return. We show that our method BAE reduces the losses and thus learns a better value function and policy than the baselines. We then show how our method performs in the expected return. Procgen Results Figure 3 shows value and policy loss during policy training on Procgen Starpilot environments. As the training progresses, the loss of our method BAE reduces drastically compared to the baselines. These results show the sign of the effectiveness of our method in reducing the agent’s losses. Note that the advantage estimation is used to train both the value function and the policy; thus, better estimation of advantages should generally give better value and policy. In this sense, our method shows empirical evidence that it can help better advantage estimation. We observe that in the Starpilot environment, the success in policy and value loss translates to the ﬁnal return. In Figure 4 we see that our method (BAE-PPO) shows improved sample efﬁciency (Train Return) and generalization capacity compared to the baseline GAE-PPO, RAD-PPO, and DRAC-PPO. Note that the RAD-PPO performance worsens the perfor- mance of the base GAE-PPO algorithms. This result is con- sistent with the ﬁndings in [11], and it shows that naive data Starpilot Env. Sample efﬁciency performance measured in train Fig. 4. time return. We see our method BAE-PPO achieves higher returns where DRAC does not improve the base agent’s (GAE-PPO) performance. RAD slightly worsens the performance of the base agent. [Right] Generalization performance measured in test time return. We see a similar trend and observe that our method performs the best. [Left] Performance on Quadruped run environments. Our method Fig. 7. BAE-PPO shows higher mean returns compared to all other agents. Data augmentation baseline RAD and DRAC worsen the performance of the base agent (GAE-PPO). [Right] Our method BAE consistently achieves a higher mean where DRAC fails to improve upon the base agent, and RAD worsens the base performance. Fig. 5. Miner Env. Training time policy and value loss [lower is better]. The value loss of BAE eventually become lower. Losses of RAD increases compared to base PPO. Fig. 6. Miner Env. [Left] Sample efﬁciency performance measured in train time return and [Right] Generalization performance measured in test time return. Overall, we see our method BAE-PPO shows consistence improvement over baselines. augmentation can be detrimental to performance. Furthermore, we observe a similar trend for the value and policy loss results. We observe a similar performance trend in the Procgen Miner environment. In Figure 5, we see that in our method, BAE shows a smaller value loss eventually despite being higher at the beginning compared to the baselines. GAE and DRAC show slightly lower values in the policy loss plot than BAE. However, both show smaller policy losses in general. In performance measure (Figure 6), we see that our method shows better performance throughout the training than the baseline in both sample efﬁciency and generalization. The performance difference is consistent across timestep. Similar to Starpilot, RAD also performs worse compared to base GAE- PPO. We further evaluate our method on vector-based state space and continuous robotic tasks: Deepmind control and Pybullet. Note that the setup of these benchmarks are different from Procgen’s train-test setup, and here we evaluate how our agents can perform in high-dimensional states and procedurally gen- erated task. Thus we can only report returns during training. Deepmind Control Results Figure 7 shows performance compariosn on Quadruped run and Cartpole - Three Poles environments. We observe that our method BAE achieves the best performance in both environments. On the other hand, the baseline RAD severely worsens the base agent’s (GAE-PPO) performance in both environments. Another baseline, DRAC, worsens the base agents’ performance in Quadruped Run and fails to improve performance in the Cartpole Three Poles envi- ronment. These results show that properly using the augmenta- tion in policy learning can lead to strong performance. In this case, all data augmentation agents (RAD, DRAC, and BAE) use the same random amplitude modulation augmentation. However, using this augmentation in advantage computation, our method BAE shows a substantial performance boost. On the other hand, other baselines, RAD and DRAC, worsen the performance (Quadruped Run). Pybullet Results In Figure 8, for the HalfCheetah environ- ment, we see that our method BAE performs better than base agent GAE-PPO and other data augmentation baselines RAD and DRAC. On the other hand, in Minitaur, the data augmentation baseline DRAC and RAD worsen the base agent’s (GAE-PPO) performance where BAE can maintain the base performance. Overall, these results show the robustness of our method in performance compared to baseline data augmentation methods. Therefore, the proposed augmented observation is expected not to worsen the base performance. However, in our exper- iments, we observe that the RAD and DRAC barely match the base agent’s (GAE-PPO) results and sometimes worsen the performance. These variabilities in performance hinder the widespread adaptation of these methods. In contrast, our method BAE shows a consistent performance across various tasks without reducing the base agent’s performance. [2] R. J. Williams, “Simple statistical gradient-following algorithms for connectionist reinforcement learning,” Machine learning, vol. 8, no. 3, pp. 229–256, 1992. [3] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox- imal policy optimization algorithms,” arXiv preprint arXiv:1707.06347, 2017. [4] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust region policy optimization,” in International conference on machine learning. PMLR, 2015, pp. 1889–1897. [5] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High- dimensional continuous control using generalized advantage estimation,” in Proceedings of the International Conference on Learning Represen- tations (ICLR), 2016. [6] M. Andrychowicz, A. Raichuk, P. Sta´nczyk, M. Orsini, S. Girgin, R. Marinier, L. Hussenot, M. Geist, O. Pietquin, M. Michalski, S. Gelly, and O. Bachem, “What matters for on-policy deep actor-critic methods? a large-scale study,” in International Conference on Learning Represen- tations, 2021. [7] X. Song, Y. Jiang, S. Tu, Y. Du, and B. Neyshabur, “Observational overﬁtting in reinforcement learning,” in International Conference on Learning Representations, 2020. [8] C. Zhang, O. Vinyals, R. Munos, and S. Bengio, “A study on overﬁtting in deep reinforcement learning,” arXiv preprint arXiv:1804.06893, 2018. [9] K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman, “Quantifying generalization in reinforcement learning,” in International Conference on Machine Learning. PMLR, 2019, pp. 1282–1289. [10] M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas, “Reinforcement learning with augmented data,” in Advances in neural information processing systems, 2020. [11] R. Raileanu, M. Goldstein, D. Yarats, I. Kostrikov, and R. Fergus, “Automatic data augmentation for generalization in deep reinforcement learning,” arXiv preprint arXiv:2006.12862, 2020. [12] S. Tunyasuvunakool, A. Muldal, Y. Doron, S. Liu, S. Bohez, J. Merel, T. Erez, T. Lillicrap, N. Heess, and Y. Tassa, “dm control: Software and tasks for continuous control,” Software Impacts, vol. 6, p. 100022, 2020. [13] E. Coumans and Y. Bai, “Pybullet, a python module for physics simulation for games, robotics and machine learning,” http://pybullet.org, 2016–2021. [14] L. Engstrom, A. Ilyas, S. Santurkar, D. Tsipras, F. Janoos, L. Rudolph, and A. Madry, “Implementation matters in deep rl: A case study on ppo and trpo,” in International Conference on Learning Representations, 2020. [15] S. Huang, R. F. J. Dossa, A. Rafﬁn, A. Kanervisto, and W. Wang, “The 37 implementation details of proximal policy optimization,” in ICLR Blog Track, 2022. [Online]. Available: https://iclr-blog-track.github.io/ 2022/03/25/ppo-implementation-details/ [16] R. Kirk, A. Zhang, E. Grefenstette, and T. Rockt¨aschel, “A sur- vey of generalisation in deep reinforcement learning,” arXiv preprint arXiv:2111.09794, 2021. [17] K. Cobbe, C. Hesse, J. Hilton, and J. Schulman, “Leveraging procedu- ral generation to benchmark reinforcement learning,” in International conference on machine learning. PMLR, 2020, pp. 2048–2056. [18] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning et al., “Impala: Scalable dis- tributed deep-rl with importance weighted actor-learner architectures,” arXiv preprint arXiv:1802.01561, 2018. [19] S. Huang, R. F. J. Dossa, C. Ye, and J. Braga, “Cleanrl: High-quality single-ﬁle implementations of deep reinforcement learning algorithms,” 2021. [20] J. Peters and S. Schaal, “Reinforcement learning of motor skills with policy gradients,” Neural networks, vol. 21, no. 4, pp. 682–697, 2008. [21] C. Wu, A. Rajeswaran, Y. Duan, V. Kumar, A. M. Bayen, S. Kakade, I. Mordatch, and P. Abbeel, “Variance reduction for policy gradient with action-dependent factorized baselines,” in International Conference on Learning Representations, 2018. [22] M. Igl, K. Ciosek, Y. Li, S. Tschiatschek, C. Zhang, S. Devlin, and K. Hofmann, “Generalization in reinforcement learning with selective noise injection and information bottleneck,” in Advances in neural information processing systems, 2019, pp. 13 978–13 990. [23] M. M. Rahman and Y. Xue, “Bootstrap state representation using style transfer for better generalization in deep reinforcement learning,” in European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD 2022), 2022. Performance on Minitaur [Left] and HalfCheetah [Right] environ- Fig. 8. ments. Our method shows better or similar performance compared to the base agent (GAE-PPO), where the data augmentation baselines sometimes worsen the base performance. V. RELATED WORK Advantage Estimation. The baseline has been leveraged to reduce variance in policy gradient update [2], [20], [21]. Furthermore, effective modiﬁcation of such methods is the use of advantage estimation. This method is commonly used in policy optimization and enjoys strong empirical success [6], especially the Generalized Advantage Estimation (GAE) [5] method. In contrast to GAE, our method BAE leverages data augmentation and incorporates advantage computation across various semantically similar states. Empirically we observe that our method of computing advantage can be beneﬁcial over GAE, especially in high-dimensional and procedural generated environments. Data augmentation. Data augmentation has been demon- strated to be an effective and efﬁcient approaches to improve performance [9]–[11]. Other methods proposed to improve generalization which includes regularization [22], and style- transfer [23]. Depending on how the augmented observation is used, the method can be different; for example, RAD [10] and DRAC [11]. In contrast to these methods, our method incorporates data augmentation into advantage estimation, which shows better empirical performance compared to these methods (RAD and DRAC). VI. CONCLUSION In this paper, we propose a data augmentation-based advan- tage estimation method for policy optimization. Our Bootstrap advantage estimation (BAE) method replaces the GAE method in policy gradient-based algorithms. We demonstrated the effectiveness of our method on PPO algorithms. Furthermore, we evaluated our methods on both image-based observation space with discrete action space and vector-based observation with continuous action space (Procgen, Deepmind Control, and Pybullet). Our BAE method showed better performance in various environment setups than GAE. Furthermore, our method performs better than two existing data augmentation techniques (RAD and DRAC). REFERENCES [1] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour, “Policy gradi- ent methods for reinforcement learning with function approximation,” Advances in neural information processing systems, vol. 12, 1999."
19,"Google's 2019 ""Quantum Supremacy'' Claims: Data, Documentation, and   Discussion","[{'href': 'http://arxiv.org/abs/2210.12753v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2210.12753v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-10-23 15:36:25,"2 2 0 2 t c O 8 1 ] G L . s c [ 1 v 8 9 5 9 0 . 0 1 2 2 : v i X r a Planning for Sample Efﬁcient Imitation Learning Zhao-Heng Yin∗ Weirui Ye†† Qifeng Chen∗ Yang Gao†‡§ ∗HKUST †Tsinghua University ‡Shanghai Qi Zhi Institute Abstract Imitation learning is a class of promising policy learning algorithms that is free from many practical issues with reinforcement learning, such as the reward design issue and the exploration hardness. However, the current imitation algorithm strug- gles to achieve both high performance and high in-environment sample efﬁciency simultaneously. Behavioral Cloning (BC) does not need in-environment interac- tions, but it suffers from the covariate shift problem which harms its performance. Adversarial Imitation Learning (AIL) turns imitation learning into a distribution matching problem. It can achieve better performance on some tasks but it requires a large number of in-environment interactions. Inspired by the recent success of EfﬁcientZero in RL, we propose EfﬁcientImitate (EI), a planning-based imitation learning method that can achieve high in-environment sample efﬁciency and per- formance simultaneously. Our algorithmic contribution in this paper is two-fold. First, we extend AIL into the MCTS-based RL. Second, we show the seemingly incompatible two classes of imitation algorithms (BC and AIL) can be naturally uniﬁed under our framework, enjoying the beneﬁts of both. We benchmark our method not only on the state-based DeepMind Control Suite, but also on the image version which many previous works ﬁnd highly challenging. Experimental results show that EI achieves state-of-the-art results in performance and sample efﬁciency. EI shows over 4x gain in performance in the limited sample setting on state-based and image-based tasks and can solve challenging problems like Humanoid, where previous methods fail with small amount of interactions. Our code is available at https://github.com/zhaohengyin/EfficientImitate. 1 Introduction The real-world sequential decision process in robotics is highly challenging. Robots have to handle high dimensional input such as images, need to solve long horizon problems, some critical timesteps need highly accurate maneuver, and the learning process on the real robot has to be sample efﬁcient. Imitation learning is a promising approach to solving those problems, given a small dataset of expert demonstrations. However, current imitation algorithms struggle to achieve these goals simultaneously. There are two kinds of popular imitation learning algorithms, Behavior Cloning (BC) and Adversarial Imitation Learning (AIL). BC formulates imitation learning as a supervised learning problem. It needs no in-environment samples, but it suffers from the covariate shift issue [40], often leading to test time performance degradation. Adversarial Imitation Learning (AIL) [16, 6] casts imitation learning as a distribution matching problem. Though AIL suffers less from the covariate shift problem and can perform better than BC on some domains, it requires impractical number of online interactions [20, 23] and can perform badly on image inputs [37]. These drawbacks heavily limit its application in ﬁelds like robotics, where physical robot time matters. In summary, current imitation ∗zhaoheng.yin@connect.ust.hk, cqf@ust.hk †ywr20@mails.tsinghua.edu.cn, gaoyangiiis@tsinghua.edu.cn §Corresponding author. 36th Conference on Neural Information Processing Systems (NeurIPS 2022). Figure 1: Left: The system-level overview of EfﬁcientImitate. The agent (yellow area) takes actions in the environment and stores the data in the replay buffer. The data in the replay buffer and expert buffer are then used to train a model and AIL reward. The planning module then searches for improved policy and value in each state in the replay buffer, based on which the policy and value networks are optimized. Right: The planning procedure. We use a continuous version EfﬁcientZero as our planner. We ﬁnd that MCTS uniquely beneﬁts from BC and can unify BC and AIL. For the expansion of each node, we sample actions from both the current policy (black arrow) and a BC policy (blue arrow). We use AIL reward (yellow cube) to encourage long-term distribution matching. MCTS searches (pink area) for best actions to maximize the cumulative AIL reward and update the estimated value and policy of the root node. The output of the planning procedure is the value estimate and the policy estimate of s, and the value and policy networks are optimized to ﬁt them. learning algorithms fail to achieve high online testing performance and high in-environment sample efﬁciency at the same time. Further, the two types of imitation algorithms seem to be incompatible since they are based on two completely different training objectives. Previous work ﬁnds that it is hard to unify them naively[34]. Inspired by the recent success in sample efﬁcient RL, such as EfﬁcientZero [52], we propose a planning-based imitation algorithm named EfﬁcientImitate (EI) that achieves high test performance and high in-environment sample efﬁciency at the same time. Our method extends AIL to a model- based setting with multi-step losses under the MCTS-based RL framework. Our algorithm also uniﬁes the two types of the previous imitation algorithms (BC and AIL) naturally, thanks to the planning component of our algorithm. Intuitively, BC gives a coarse solution that is correct most of the time but fails to match the expert’s behavior in the long term. On the other hand, AIL knows the goal of the learning, i.e., matching state action distribution, but doesn’t give the solution directly. Our method’s planning component uniﬁes those two methods and shows a signiﬁcant performance boost, especially in the harder tasks such as Humanoid. We illustrate the detailed procedure in Figure 1. We validate our idea not only on state-based tasks but also on image-based tasks, which relatively few previous algorithms can handle them [37]. EI achieves state-of-the-art results in sample efﬁciency and performance. It shows over 4x gain in performance in the limited sample setting on state-based and image-based tasks. On harder tasks such as Humanoid, the gain is even larger. A by-product of this paper is that we extend the recent EfﬁcientZero algorithm to the continuous action space. We open-source the code at https://github.com/zhaohengyin/EfficientImitate to facilitate future research. Our contributions in this paper are summarized as follows. • We present EfﬁcientImitate, a sample efﬁcient imitation learning algorithm based on MCTS. • We identify that MCTS can beneﬁt from BC by using BC actions during the search, which is crucial for challenging tasks. EfﬁcientImitate suggests a natural way to unify BC and AIL. • We conduct experiments in both the state and the image domain to evaluate EI. Experimental results show that EI can achieve state-of-the-art sample efﬁciency and performance. 2 2 Related Work 2.1 Imitation Learning Imitation learning (IL) aims to solve the sequential decision-making problems with expert demon- stration data. It has wide applications in games and robotics [4]. Compared with RL, one major beneﬁt of IL is that it can avoid the notorious reward design problem. IL can be divided into two branches: BC [1] and IRL [33]. To solve the covariate shift problem of BC, researchers propose methods like dataset aggregation [40] and noise injection [25]. But these methods either require extra expert queries or exert constraints over the learning process. A recent variant branch of IRL is the Adversarial Imitation Learning (AIL) [16, 6]. AIL models IL as a state-action distribution matching problem. Many works extend AIL by using better distribution metrics such as f -divergence [54] and Wasserstein distance [5]. Though these methods can learn better reward functions and somewhat speed up the training, they do not directly focus on AIL’s sample efﬁciency problem. Some works have drawn attention to sample efﬁciency, for example [23, 41, 55] propose to use off-policy training in AIL training to reduce sample complexity. ValueDICE [24] reformulates AIL objective in an ofﬂine min-max optimization process, but recent work points out that it is an improved form of BC [29]. VMAIL [37] uses a model-based approach to improve sample efﬁciency. It collects latent rollout with a variational model and reduces online interactions. MoBILE [22] also shows a provably sample efﬁcient model-based imitation approach. Compared with these methods, our method introduces MCTS planning to the off-policy imitation learning and uses it to unify BC and AIL to take advantage of both. The idea of combining BC and AIL helps to improve the sample efﬁciency can be traced back to GAIL[16]. GAIL suggests using BC to initialize the policy network, but [34] ﬁnds that this does not work well because the initialized BC knowledge will be corrupted in AIL training. Then, [20] proposes to use an annealed (decaying) BC loss to regularize policy training to solve this problem. But in this work, we ﬁnd that such a BC regularizer can be harmful to exploration when BC is incorrect. One concurrent work mitigates this issue by using adpative BC loss and replacing AIL reward with optimal-transport-based imitation reward [13]. We also notice that AlphaGo [46] involves BC in their method, but they focus on RL rather than IL, and BC is only used to initialize the policy. Different from these methods, EI uses BC actions as candidates in MCTS. 2.2 Sample Efﬁciency in RL The sample efﬁciency problem of imitation learning is closely related to that in RL. One line of work ﬁnds that the reward signal is not a good data source for representation learning in RL and is one reason for sample inefﬁciency. Then they utilize self-supervised representation learning to accelerate representation learning and improve the sample efﬁciency. Researchers propose to use contrastive learning [27], consistency-based learning [44, 53, 52], or pretrained representation [45] for this purpose. Some works also explore the possibility of applying self-supervised representation learning to imitation learning [4]. Another line of work focuses on RL with a learned model, which is promising for sample efﬁcient learning [8, 10–12, 19, 28, 31, 52, 14]. These approaches usually imagine additional rollouts with the learned model or use it as a more compact environment representation for RL. Besides, some works also ﬁnd that data augmentation can effectively improve sample efﬁciency [26, 50, 49]. EI also beneﬁts from these approaches. It includes a model and applies representation learning to boost sample efﬁciency. In application, people also consider to augment RL with the demonstration [15, 38, 32, 21] to improve the sample efﬁciency. This can be viewed as a combination of RL and imitation learning and favor RL on real robots. We believe that our method can also be extended to this setting. 3 Background 3.1 Setting We formalize the sequential decising making problem as Markov Decision Process M = (S, A, R, T ). Here, S is the state space, A is the action space, R is the reward function, and T is the transition dynamics. The agent’s state at timestep t is st ∈ S. The agent takes action at and 3 t=0 γtrt, where γ is a discount factor. receives reward rt = R(st, at). Its state at timestep t + 1 is then st+1 ∼ T (st, at). The objective of the agent is to maximize the return (cid:80)T In the imitation learning problem studied here, the agent has no access to the reward function R and transition dynamics T . It is provided with a ﬁxed expert demonstration dataset D = {τi}. Here, each τi = (sE T ) is an expert trajectory that can achieve high performance in M. The agent can not solicit extra expert demonstrations but can interact with the MDP, observing new states and actions, but not rewards. In this work, we deﬁne (in-environment) sample efﬁciency as the number of online interactions during training. We expect the agent to achieve high performance within a ﬁxed online sample budget. 1 , ...sE T , aE 0 , aE 1 , aE 0 , sE 3.2 BC and AIL BC considers imitation learning as a supervised learning problem. It trains a policy network π to minimize the following loss function: L = −E (sE i ,aE i )∼D log π(aE i |sE i ). (1) AIL treats imitation learning as a distribution matching problem. One typical AIL algorithm is GAIL. It trains a discriminator D to distinguish the agent generated state-action tuple (st, at) from those (sE i ) in the demonstration dataset by minimizing i , aE L = −E (st,at)∼ρ,(sE i ,aE i )∼D (cid:2)log(D(st, at)) + log(1 − D(sE i , aE i ))(cid:3) , (2) where ρ is the state-action distribution induced by the agent. Meanwhile, it trains the agent to maximize the return with respect to the adversarial reward rt = − log(1 − D(st, at)) using any on-policy RL algorithm. 3.3 MuZero and its Extensions Our planning method is based on MuZero [42] and its extensions. MuZero learns an environment model for MCTS. The model consists of an encoder network f , a dynamics network g, and a reward network R. It operates on abstract states [52]. Concretely, it gets the abstract state ht of the current state st by ht = f (st). It can then predicts the future abstract states recursively by ht+1 = g(ht, at), and the rewards by R(ht, at). Besides the model, MuZero also contains a policy network and a value network. The policy network provides a prior over the actions at each node, and the value network calculates the expected return of the node. MuZero uses the model, the policy network, and the value network to search for improved policy and value for each state with MCTS. We refer the readers to the original MuZero paper for details. Sampled MuZero Sampled MuZero [17] extends MuZero from the discrete action domain to the continuous action domain, which is of our interest in this paper. At each node s to expand, Sampled MuZero samples K actions {ai}K i=1 from current policy π(a|s). During the search, it selects action a∗ from the sampled actions that maximize the probabilistic upper conﬁdence bound a∗ = arg max a∈{ai} Q(s, a) + c(s)ˆπ(a|s) (cid:112)(cid:80) b N (s, b) 1 + N (s, a) , (3) (cid:80) where ˆπ(a|s) = 1 i δ(a, ai). Q(s, a) is the current Q-estimation of the pair (s, a). N (s, a) K denotes the times that this pair is visited in MCTS. c(s) is a weighting coeffcient. During policy optimization, MuZero minimizes the Kullback-Leibler divergence between the current policy π and the MCTS statistics πMCTS at the root node DKL(πMCTS||π). EfﬁcientZero We also apply EfﬁcientZero [52] in this paper. EfﬁcientZero improves the sample efﬁciency of MuZero by using a self-supervised representation learning method to regularize the hidden representation. It uses a SimSiam-style structure [3] to enforce the similarity between the predicted future representation and the real future representation. 4 EfﬁcientImitate In this section, we present our EfﬁcientImitate algorithm. We ﬁrst present an MCTS-based approach to solving the AIL problem in Section 4.1. Then we show a simple yet effective method to unify BC 4 Figure 2: Computation ﬂow of loss functions. Left: Multi-step Discriminator Loss. We do not distinguish between the calculation for expert and agent here, and use a superscript (E) to indicate that the computation applies to both. Right: Multi-step BC Loss. It applies to the expert sequences. and AIL with MCTS in Section 4.2. We brieﬂy discuss the implementation in Section 4.3, and the full details can be found in the Appendix. 4.1 Extending AIL to MCTS-based RL Traditionally, the adversarial imitation learning (AIL) algorithm trains a discriminator D between the policy samples and the expert samples and uses some form of D, such as − log(1 − D), as the reward function. Then some model-free RL algorithms are used to maximize the cumulative reward. In MCTS-based RL algorithms, such as MuZero [42] and EfﬁcientZero [52], the reward function is used in the value target computation and the MCTS search. The use in value target computation is similar to prior model-free RL algorithms, where the value target is computed with n-step value bootstrapping on the actual observations. However, during the MCTS search, the rewards are computed on the abstract state obtained by running the forward dynamics function ht+1 = g(ht, at) multiple times. If we were training the discriminator only on actual observations of the expert and the policy rollouts, the discriminator might not generalize well to abstract states outputted by the forward dynamics functions. Therefore, we train the discriminator with the model-based rollout. Speciﬁcally, we sample sequence (st, at+1, ..., at+n) in replay buffer B and expert sequence (sE t(cid:48)+n) in demonstration dataset D and minimizes following multi-step discriminator loss function: t(cid:48)+1, ..., aE t(cid:48) , aE t(cid:48) , aE LD = −E (st,at:t+n)∼B,(sE t(cid:48) ,aE t(cid:48) :t(cid:48)+n (cid:34) n (cid:88) i=0 )∼D log(D(ht+i, at+i)) + log(1 − D(hE t(cid:48)+i, aE (cid:35) t(cid:48)+i)) . (4) Here, ht+i (and ht(cid:48)+i) terms are produced by the forward dynamics in EfﬁcientZero (Figure 2). We use the GAIL transition reward R(h, a) = − log(1 − D(h, a)), and then the MCTS planner searches for action that can maximize cumulative GAIL reward to guarantee long-term distribution matching. Note that V-MAIL also propose a similar discriminator training technique, but under the Dreamer [11] model. Besides, since the discriminator’s input is based on the representation rather than raw input, the discriminator should be trained with the encoder jointly. However, this can lead to a highly non- stationary reward during the bootstrap value calculation. To mitigate this issue, we also propose to use a target discriminator for bootstrap value calculation. This can make the training more stable. Though we use the GAIL reward here, one can also use other kinds of AIL and IRL reward functions proposed in recent research. Using the GAIL reward can already achieve satisfactory performance in our experiments. When the real reward presents, one may also combine this into planning [21]. This may favor application scenarios where handcrafting a reward function is not hard. We do not study this case here and leave it to future work. 4.2 Unifying BC and AIL in MCTS As discussed in related work, researchers realize that using BC can improve AIL’s sample efﬁciency by providing a good initial BC policy or constraining the policy to BC. However, these existing solutions are not good enough in practice. The main pitfall in these methods is that BC knowledge in the policy network is destined to be forgotten if the policy network is trained with the AIL objective, and then BC will no longer be helpful [34]. 5 We observe that MCTS can naturally unify the BC and AIL methods, enjoying the beneﬁt of both and being free from this pitfall. We propose to plug BC actions into MCTS as candidates at each node and use a planning process to search for an improved solution. This time, the BC actions are consistently considered throughout the entire training procedure without being forgotten. Concretely, we train a BC policy πBC and use a mixture policy ˜π for the sampling at each node in MCTS: ˜π = απBC + (1 − α)π. (5) α is a mixture factor, which is ﬁxed during training and π is the current policy. We use α = 0.25 in this paper. This ensures that a small fraction of action samples are from the BC policy. During planning, the BC actions are evaluated and will be frequently visited and selected as output if they can lead to long-term distribution matching. This can then reduce the effort of ﬁnding good expert-like actions from scratch as desired. Moreover, another unique advantage of this procedure is that it does not fully trust BC like [20], which forces the output of the policy network to be close to BC. When BC is wrong due to covariate shifts or insufﬁcient demonstrations, it can neglect these BC actions and allow the policy to search for better actions. This ensures that BC does not hurt training. However, due to the conceptual simplicity, one arising question is whether this approach can be applied to other model-based methods. Here, we take Dreamer [11] as an example. Though Dreamer builds a model of the environment, it only uses the model to roll-out the policy for policy optimization. In other words, the model is not used to evaluate whether a speciﬁc BC action is good or not in the long term, so our idea can not be applied directly to Dreamer. From this example, we see that the core of our idea is to leverage the power of planning, only with which the long-term outcomes of certain (BC) actions can be calculated. For the training of πBC, we minimize the following multi-step BC objective (Figure 2): LBC = E (sE t(cid:48) ,aE t(cid:48):t(cid:48)+n )∼D − log(πBC(aE t(cid:48)+i|hE t(cid:48)+i)) . (6) i=0 This is to avoid distributional shifts during multi-step prediction in MCTS. For the training of the policy, we still minimize DKL(πMCTS||π). (cid:34) n (cid:88) (cid:35) Note that the BC design proposed here is not coupled with AIL. It can go beyond imitation learning and be applied in other robot learning settings, such as RL with demonstration [38]. 4.3 Implementation We ﬁrst implement a continuous version EfﬁcientZero for planning, and the details can be found in the Appendix. The BC policy network is simply a duplicate of the policy network. The discriminator and BC policy networks share the same encoder network with the policy network and value network. The overall loss function for optimization is L = LEZ + λdLD + λbcLBC. (7) LEZ is EfﬁcientZero’s loss function (excluding reward loss). All the networks are trained jointly to minimize this loss function 7. We use the Reanalyze algorithm [43, 52] for ofﬂine training, and we require that all the samples should be reanalyzed. Figure 3: Part of the tasks used in our experiments. From left to right: Reacher, Finger Spin, Cheetah Run, Walker Walk, Hopper Hop, Humanoid Walk. 5 Experiments In this section, we evaluate the sample efﬁciency of the proposed method. We measure the sample efﬁciency by evaluating the performance of an algorithm at a small number of online samples. We also analyze the effect of the BC actions and planning. 6 Table 1: Evaluation result on the state-based DeepMind Control Suite. We use the average score on three random seeds. Our method can achieve the state of the art result compared with the baselines. Task Cartpole Budget BC DAC ValueDICE SQIL Ours 10k 0.59 0.13 ±0.12 0.21 ±0.01 0.23 ±0.01 0.98 ±0.01 Ball 10k 0.44 0.18 ±0.01 0.23 ±0.01 0.27 ±0.05 0.99 ±0.01 Reacher Finger Cheetah Walker Hopper Humanoid 50k 0.83 0.22 ±0.02 0.15 ±0.01 0.21 ±0.02 0.90 ±0.04 50k 0.76 0.53 ±0.05 0.04 ±0.01 0.02 ±0.00 0.99 ±0.00 50k 0.58 0.33 ±0.04 0.50 ±0.08 0.05 ±0.01 0.96 ±0.02 50k 0.16 0.26 ±0.04 0.54 ±0.09 0.11 ±0.03 1.03 ±0.01 50k 0.03 0.00 ±0.00 0.03 ±0.00 0.24 ±0.10 0.92 ±0.02 500k 0.11 0.01 ±0.00 0.00 ±0.00 0.06 ±0.01 0.74 ±0.04 Table 2: Evaluation result on the image-based DeepMind Control Suite. We use the average score on three random seeds. Our method can achieve state-of-the-art results compared with the baselines. Task Cartpole Budget BC DAC ValueDICE SQIL VMAIL Ours 50k 0.30 0.08 ±0.01 0.18 ±0.02 0.26 ±0.03 0.57 ±0.03 0.94 ±0.02 Ball 50k 0.32 0.26 ±0.02 0.27 ±0.02 0.77 ±0.05 0.61 ±0.11 0.93 ±0.01 Finger Cheetah Reacher Walker Hopper 50k 0.14 0.00 ±0.00 0.01 ±0.00 0.00 ±0.01 0.06 ±0.03 1.00 ±0.01 50k 0.37 0.04 ±0.01 0.06 ±0.01 0.06 ±0.00 0.13 ±0.04 0.92 ±0.01 100k 0.26 0.25 ±0.05 0.15 ±0.02 0.36 ±0.04 0.34 ±0.02 0.86 ±0.06 100k 0.15 0.10 ±0.02 0.08 ±0.00 0.32 ±0.05 0.24 ±0.07 0.98 ±0.01 200k 0.02 0.01 ±0.00 0.00 ±0.00 0.04 ±0.02 0.07 ±0.04 0.70 ±0.01 5.1 Setup We use the DeepMind Control Suite [47] for evaluation. We use the following tasks: Cartpole Swingup, Reacher Easy, Ball-in-cup Catch, Finger Spin, Cheetah Run, Walker Walk, Hopper Hop, and Humanoid Walk. We conduct both state-based and image-based experiments. Note that many previous imitation learning works use the OpenAI Gym [2] version of these tasks for evaluation. We ﬁnd that the DMControl version used here brings extra challenges by using more challenging initial states. Take the Walker task as an example; the initial state in OpenAI Gym is standing. However, in DMControl, the agent’s initial state is lying on the ground, and the agent should also learn to stand up ﬁrst from very limited data. For the state-based experiments, we allow 10k-50k online steps in the environment based on the difﬁculty of each task. Since learning a robust and meaningful visual representation requires more data for image-based experiments, we allow 50k-100k online steps. Detailed setup will be shown in the result. We train SAC [9] policies to collect expert demonstrations for imitation learning. The expert demonstrations are not subsampled. We use 5 demonstrations in the state-based experiment, except for Reacher and Humanoid, where we use 20 demonstrations. We use 20 demonstrations in the image-based experiments. 5.2 Baselines (1) DAC DAC [23] is an We present several baselines of sample efﬁcient imitation learning. adversarial off-policy imitation learning method. It matches the distribution of the replay buffer and that of the expert demonstration dataset using the TD3 [7] algorithm. (2) SQIL SQIL [39] is a non-adversarial off-policy imitation learning method. It labels all the expert transitions with reward 1 and non-expert transitions with reward 0. Then it trains a SAC policy over these relabeled data. SQIL is a regularized form of BC. (3) ValueDICE ValueDICE [24] considers imitation learning as a distribution matching problem and solves it with a min-max optimization process. (4) VMAIL VMAIL [37] is a model-based visual imitation learning method. It learns a variational model for simulating on-policy rollouts. We only evaluate VMAIL on the image-based domain, as they did in 7 Figure 4: The performance curve on the state-based tasks. The results are averaged over three seeds. The shaded area displays the range of one standard deviation. Figure 5: The performance curve on the image-based tasks. The results are averaged over three seeds. The shaded area displays the range of one standard deviation. the original paper. Besides the online imitation learning baselines, we also include BC as an ofﬂine baseline. 5.3 Results State-based experiments Table 1 shows the state-based experiments’ evaluation results within the given budget. We also plot the performance curve of four challenging tasks in Figure 4. The performance is normalized to 0.0 and 1.0 with respect to the performance of the random agent and the expert. We ﬁnd that our proposed method can outperform all the baselines by a large margin. Except for BC, these baselines methods could hardly learn meaningful behaviors using a limited online budget. We ﬁnd that DAC is a strong baseline. Its performance can grow to the expert in 200k samples in most tasks except Hopper, where it will eventually get stuck. Our method is much more sample efﬁcient than the best of these baseline methods. For some tasks like Walker Walk and Cheetah Run, our method only requires about 20k steps to reach near-expert performance, equivalent to 80 online trajectories (around 0.5 hours in real). This result is notable for the robotics community. It shows that online imitation learning is possible with only a handful of trials, and applying it directly on a real locomotion robot is possible. Image-based experiments So far, image-based tasks are still challenging for adversarial imitation learning algorithms, and the evaluation of most of the prior AIL works is carried out in the state-based tasks. Table 2 shows the evaluation result within the given budget in the image-based experiments (see Figure 5 for curves). Our method can also learn expert behavior given a slightly larger budget. Still, most of the baselines fail to match experts’ behavior using the given budget. We notice an inherent difﬁculty in learning a robust visual representation for adversarial training in the limited data set in image-based tasks. Discriminator can judge whether a behavior is expert-like using various possible features in this case. Solving this open problem is out of the scope of this paper. In the presence of such a difﬁculty, EI can still achieve good sample efﬁciency in most of the tasks. 8 Finger Cheetah Walker Hopper e c n a m r o f r e P 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 BC DAC SQIL ValueDICE Ours 0 50K 100K Steps 150K 200K 0 50K 100K Steps 150K 200K 0 50K 100K Steps 150K 200K 0 50K 100K Steps 150K 200K Finger Cheetah Walker Hopper e c n a m r o f r e P 1.0 0.8 0.6 0.4 0.2 0.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 BC DAC SQIL ValueDICE VMAIL Ours 0 50K 100K Steps 150K 200K 0 50K 100K Steps 150K 200K 0 50K 100K Steps 150K 200K 0 50K 100K Steps 150K 200K Figure 6: The performance curves of our method with and without BC actions at each expansion. The plots are sorted according to the difﬁculty of the corresponding task. The leftmost one is the most difﬁcult task, Humanoid. The results are averaged over three seeds. BC actions have a great impact on the sample efﬁciency and performance. 5.4 Analysis Effect of BC We carry out ablation analysis on the BC to see whether it helps in our method. We set α = 0 to remove all the BC actions and see how our method’s performance and sample efﬁciency will change. The result is shown in Figure 6. We ﬁnd that the performance of our method degrades after we remove all the BC actions in MCTS. The effect is task-speciﬁc, and we ﬁnd BC is more helpful in those challenging tasks. For example, in tasks that are high-dimensional or have a hard exploration process like Humanoid and Hopper, removing BC actions will make the learning process stuck in local minima. At that local minima, the agent struggles to ﬁnd the correct action that matches the expert’s behavior. Although removing BC actions does not trap the agent in local minima in some relatively simpler tasks like Cheetah and Walker, it slows down the training. It doubles the number of online interactions to reach expert performance. This result conﬁrms that using BC actions can indeed provide a good solution to the distribution matching problem, which can help to speed up learning and improve performance. We also notice that even when we remove the BC actions, the method is still able to outperform the previous baselines; this suggests that planning with AIL alone is also powerful. Other Ways to use BC We then study another two variants of using BC: (1). BC-Ann. This variant does not use BC actions in MCTS but exerts an annealing BC loss to the pol- icy network like [20]. (2). BC-Rep. This variant does not use BC actions in MCTS but still uses BC loss to regularize the representation. We test these variants on the Humanoid Walk (Figure 7). We ﬁnd that these variants do not lead to an essential improvement. For BC-Ann, it harms the performance in the early stage (before 100k) since the BC regularization will constrain the agent’s policy near the BC policy, which contains an error and hinders learning. The agent only starts to acquire meaningful behavior after the regularization decays, but at that time, BC does not help much and can not lead to improvement. Compared with BC-Ann, BC-Reg is more helpful here. This is possibly because BC-Reg makes the encoder focus on more useful features. However, BC-Reg still gets stuck in a local minimum. This result suggests that using BC actions directly for exploration can be essential for improving AIL. Using BC simply as a regularizer may not be the ideal approach though it can be useful sometimes. Figure 7: Results of different ways of using BC. Ablation of Planning In this part, we study to what extent planning can help to learn and whether insufﬁcient search in MCTS leads to inferior performance. We study K, the number of sampled actions at each node, and N , the number of simulations. The default value of K and N in the previous experiments are 16 and 50. We sweep K ∈ {4, 8, 16, 24} and N ∈ {5, 10, 25, 50} to evaluate their effects. We collect the result on the state-based Cheetah, Walker, and Hopper task and report the averaged relative performance change at the given budget used in previous experiments (see Table 3). The general trend is that larger K and N lead to better imitation results. We ﬁnd that varying K only affects the performance a little, and K = 4 can also work well. Compared with K, N has a larger 9 Humanoid Hopper Cheetah Walker Finger e c n a m r o f r e P 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 Ours (w/o BC) Ours (w/ BC) 0 200K 400K 0 20K 40K 0 20K 40K 0 20K 40K 0 20K 40K Steps Steps Steps Steps Steps 0.8 e c n a m r o f r e P 0.6 0.4 0.2 0.0 Humanoid Ours (w/o BC) Ours (BC Ann.) Ours (BC Rep.) Ours (w/ BC) 0 100K 200K 300K 400K 500K Steps impact. When the number of simulations becomes small, the performance drops signiﬁcantly. This result also explains why we can achieve a large improvement over DAC even without BC. Table 3: Ablation of planning. We report the relative change of performance at the given budget. Param K = 4 K = 8 K = 16 K = 24 N = 5 N = 10 N = 25 N = 50 Result −8.4% −3.2% 0.0% 1.1% −27.3% −15.5% −4.6% 0.0% 6 Discussion In this paper, we presented EfﬁcientImitate, an MCTS-based imitation learning algorithm. We extended AIL to a model-based setting and solved it with MCTS in a sample-efﬁcient way. Moreover, we proposed a method to unify BC and AIL in MCTS, enjoying the beneﬁts of both. Experimental results in state-based and image-based tasks showed that EfﬁcientImitate can achieve state-of-the-art sample efﬁciency and performance. Limitations One limitation of this work is that the computation process of MCTS is more expensive compared with that of the model-free methods, though this is a common issue of model-based methods. One possible approach to mitigate this issue can be using better MCTS acceleration methods [51]. Besides, in this paper we did not study the long horizon problem with multiple objects, which is a common case in the robotic manipulation. However, this requires the model to predict the interaction with multiple objects, which is still a challenging open problem in the learning community [36] and orthogonal to our contribution. We believe that our framework can be combined with the works in this ﬁeld to handle this challenge. Future Work There are many problems to study along our direction. First, since we only use the vanilla AIL algorithm here, it is interesting to see if using more advanced algorithms such as optimal-transport-based learning [5] will make our algorithm more powerful. Second, due to the modularity of our method, one can try to extend EfﬁcientImitate to more general settings like RL with demonstration, which will also favor the application scenarios. Third, in this work we consider an online learning setting, one possible future direction is to study the use of EfﬁcientImitate on the existing ofﬂine interaction dataset to further reduce the dependence on in-environment samples. In conclusion, we believe that this work shows a promising direction and opens up new possibilities for model-based methods in robot learning. Acknowledgments and Disclosure of Funding This work is supported by the Ministry of Science and Technology of the People’s Republic of China, the 2030 Innovation Megaprojects “Program on New Generation Artiﬁcial Intelligence” (Grant No. 2021AAA0150000). This work is also supported by a grant from the Guoqiang Institute, Tsinghua University. References [1] M. Bain and C. Sammut. A framework for behavioural cloning. In Machine Intelligence 15, pages 103–129, 1995. [2] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [3] X. Chen and K. He. Exploring simple siamese representation learning. In IEEE Conference on Computer Vision and Pattern Recognition, 2021. [4] X. Chen, S. Toyer, C. Wild, S. Emmons, I. Fischer, K.-H. Lee, N. Alex, S. H. Wang, P. Luo, S. Russell, et al. An empirical investigation of representation learning for imitation. In Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021. 10 [5] R. Dadashi, L. Hussenot, M. Geist, and O. Pietquin. Primal wasserstein imitation learning. In International Conference on Learning Representations, 2021. [6] J. Fu, K. Luo, and S. Levine. Learning robust rewards with adversarial inverse reinforcement learning. In International Conference on Learning Representations, 2018. [7] S. Fujimoto, H. Hoof, and D. Meger. Addressing function approximation error in actor-critic methods. In International Conference on Machine Learning, 2018. [8] C. Gelada, S. Kumar, J. Buckman, O. Nachum, and M. G. Bellemare. Deepmdp: Learning continuous latent space models for representation learning. In International Conference on Machine Learning, 2019. [9] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning, 2018. [10] D. Hafner, T. Lillicrap, I. Fischer, R. Villegas, D. Ha, H. Lee, and J. Davidson. Learning latent dynamics for planning from pixels. In International Conference on Machine Learning, 2019. [11] D. Hafner, T. P. Lillicrap, J. Ba, and M. Norouzi. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2020. [12] D. Hafner, T. P. Lillicrap, M. Norouzi, and J. Ba. Mastering atari with discrete world models. In International Conference on Learning Representations, 2021. [13] S. Haldar, V. Mathur, D. Yarats, and L. Pinto. Watch and match: Supercharging imitation with regularized optimal transport. In Conference on Robot Learning, 2022. [14] N. Hansen, X. Wang, and H. Su. Temporal difference learning for model predictive control. In International Conference on Machine Learning, 2022. [15] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, D. Horgan, J. Quan, A. Sendonaris, I. Osband, et al. Deep q-learning from demonstrations. In AAAI Conference on Artiﬁcial Intelligence, 2018. [16] J. Ho and S. Ermon. Generative adversarial imitation learning. In Neural Information Processing Systems, 2016. [17] T. Hubert, J. Schrittwieser, I. Antonoglou, M. Barekatain, S. Schmitt, and D. Silver. Learning and planning in complex action spaces. In International Conference on Machine Learning, 2021. [18] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, 2015. [19] M. Janner, J. Fu, M. Zhang, and S. Levine. When to trust your model: Model-based policy optimization. In Neural Information Processing Systems, 2019. [20] R. Jena, C. Liu, and K. P. Sycara. Augmenting GAIL with BC for sample efﬁcient imitation learning. In Conference on Robot Learning, 2020. [21] B. Kang, Z. Jie, and J. Feng. Policy optimization with demonstrations. In International Conference on Machine Learning, 2018. [22] R. Kidambi, J. Chang, and W. Sun. Mobile: Model-based imitation learning from observation alone. In Neural Information Processing Systems, 2021. [23] I. Kostrikov, K. K. Agrawal, D. Dwibedi, S. Levine, and J. Tompson. Discriminator-actor- critic: Addressing sample inefﬁciency and reward bias in adversarial imitation learning. In International Conference on Learning Representations, 2019. [24] I. Kostrikov, O. Nachum, and J. Tompson. Imitation learning via off-policy distribution matching. In International Conference on Learning Representations, 2020. 11 [25] M. Laskey, J. Lee, R. Fox, A. Dragan, and K. Goldberg. Dart: Noise injection for robust imitation learning. In Conference on Robot Learning, 2017. [26] M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas. Reinforcement learning with augmented data. In Neural Information Processing Systems, 2020. [27] M. Laskin, A. Srinivas, and P. Abbeel. CURL: Contrastive unsupervised representations for reinforcement learning. In International Conference on Machine Learning, 2020. [28] A. X. Lee, A. Nagabandi, P. Abbeel, and S. Levine. Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. Neural Information Processing Systems, 2020. [29] Z. Li, T. Xu, Y. Yu, and Z.-Q. Luo. Rethinking valuedice: Does it really improve performance? In International Conference on Learning Representations, 2022. [30] A. L. Maas, A. Y. Hannun, A. Y. Ng, et al. Rectiﬁer nonlinearities improve neural network acoustic models. In International Conference on Machine Learning, 2013. [31] Y. Mu, Y. Zhuang, B. Wang, G. Zhu, W. Liu, J. Chen, P. Luo, S. Li, C. Zhang, and J. Hao. Model- based reinforcement learning via imagination with derived memory. In Neural Information Processing Systems, 2021. [32] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel. Overcoming exploration in reinforcement learning with demonstrations. In IEEE International Conference on Robotics and Automation, 2018. [33] A. Y. Ng, S. J. Russell, et al. Algorithms for inverse reinforcement learning. In International Conference on Machine Learning, 2000. [34] M. Orsini, A. Raichuk, L. Hussenot, D. Vincent, R. Dadashi, S. Girgin, M. Geist, O. Bachem, O. Pietquin, and M. Andrychowicz. What matters for adversarial imitation learning? In Neural Information Processing Systems, 2021. [35] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Neural Information Processing Systems, 2019. [36] H. Qi, X. Wang, D. Pathak, Y. Ma, and J. Malik. Learning long-term visual dynamics with region proposal interaction networks. In International Conference on Learning Representations, 2021. [37] R. Rafailov, T. Yu, A. Rajeswaran, and C. Finn. Visual adversarial imitation learning using variational models. In Neural Information Processing Systems, 2021. [38] A. Rajeswaran, V. Kumar, A. Gupta, G. Vezzani, J. Schulman, E. Todorov, and S. Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. In Robotics: Science and Systems, 2018. [39] S. Reddy, A. D. Dragan, and S. Levine. SQIL: imitation learning via reinforcement learning with sparse rewards. In International Conference on Learning Representations, 2020. [40] S. Ross and D. Bagnell. Efﬁcient reductions for imitation learning. In International Conference on Artiﬁcial Intelligence and Statistics, 2010. [41] F. Sasaki, T. Yohira, and A. Kawaguchi. Sample efﬁcient imitation learning for continuous control. In International Conference on Learning Representations, 2018. [42] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lock- hart, D. Hassabis, T. Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604–609, 2020. [43] J. Schrittwieser, T. Hubert, A. Mandhane, M. Barekatain, I. Antonoglou, and D. Silver. Online and ofﬂine reinforcement learning by planning with a learned model. In Neural Information Processing Systems, 2021. 12 [44] M. Schwarzer, A. Anand, R. Goel, R. D. Hjelm, A. C. Courville, and P. Bachman. Data-efﬁcient reinforcement learning with self-predictive representations. In International Conference on Learning Representations, 2021. [45] M. Schwarzer, N. Rajkumar, M. Noukhovitch, A. Anand, L. Charlin, R. D. Hjelm, P. Bachman, and A. C. Courville. Pretraining representations for data-efﬁcient reinforcement learning. In Neural Information Processing Systems, 2021. [46] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484–489, 2016. [47] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki, J. Merel, A. Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018. [48] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(11), 2008. [49] D. Yarats, R. Fergus, A. Lazaric, and L. Pinto. Mastering visual continuous control: Im- In International Conference on Learning proved data-augmented reinforcement learning. Representations, 2022. [50] D. Yarats, I. Kostrikov, and R. Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In International Conference on Learning Representations, 2020. [51] W. Ye, P. Abbeel, and Y. Gao. Spending thinking time wisely: Accelerating mcts with virtual expansions. In Neural Information Processing Systems, 2022. [52] W. Ye, S. Liu, T. Kurutach, P. Abbeel, and Y. Gao. Mastering atari games with limited data. In Neural Information Processing Systems, 2021. [53] T. Yu, C. Lan, W. Zeng, M. Feng, Z. Zhang, and Z. Chen. Playvirtual: Augmenting cycle- consistent virtual trajectories for reinforcement learning. In Neural Information Processing Systems, 2021. [54] X. Zhang, Y. Li, Z. Zhang, and Z.-L. Zhang. f-gail: Learning f-divergence for generative adversarial imitation learning. In Neural Information Processing Systems, 2020. [55] Z. Zhu, K. Lin, B. Dai, and J. Zhou. Off-policy imitation learning from observations. In Neural Information Processing Systems, 2020. 13 A Implementation In this part, we introduce our detailed implementation of EfﬁcientImitate. Our code is available at https://github.com/zhaohengyin/EfficientImitate. A.1 Model Details Our model is based on the EfﬁcientZero [52] model. It is composed of several neural networks: rep- resentation network f , dynamics network g, value network V , policy network π, BC policy network πBC, discriminator network D, projector network, and predictor network. We use PyTorch [35] to implement these networks. Their detailed structures are as follows. A.1.1 State-based experiments Representation Network The representation network is an MLP with one hidden layer of size 256. Its output dimension is 128. It uses LeakyReLU [30] as the hidden layer’s activation function. The output activation function is identity. Dynamics Network The dynamics network is an MLP with one hidden layer of size 256. It con- catenates the representation and the action as the input. Its output dimension is 128 (representation’s dimension). It uses LeakyReLU as the hidden layer’s activation function. The output activation function is identity. Value Network The value network is an MLP with one hidden layer of size 128 (256 for Humanoid). It uses LeakyReLU as the hidden layer’s activation function. The output activation function is identity. We use the categorical value representation introduced in MuZero [42]. The value prediction is discretized into 401 bins to represent the value between [−100, 100]. Therefore, the output dimension of value network is 401. Policy & BC Network The policy and the BC policy networks are both MLPs with one hidden layer of size 128 (256 for Humanoid). These MLPs use LeakyReLU as the hidden layer’s activation function. Their output activation functions are identity. They produce a Squarshed-Normal (Tanh- Normal) action distribution [9], which is determined by a predicted mean and a predicted logstd. Therefore, the output dimension is twice of the dimension of the action space. Discriminator Network The discriminator network is an MLP with one hidden layer of size 128. It uses LeakyReLU as hidden layer’s activation function. The output activation function is sigmoid. Projector Network The projector network is an MLP with one hidden layer of size 512. It uses ReLU [30] as the hidden layer’s activation function. The output activation function is identity. The output dimension (projection dimension) is 128. Predictor Network The predictor network is an MLP with one hidden layer of size 512. It uses ReLU as the hidden layer’s activation function. The output activation function is identity. The output dimension is 128. A.1.2 Image-based experiments Representation Network The representation network consists of a four layer convolutional neural network and an MLP. Its structure is as follows. • Convolution layer. Input dim: 12. Output dim: 32. Kernel Size: 5. Stride: 2. Padding: 2. • ReLU activation. • Convolution layer. Input dim: 32. Output dim: 32. Kernel Size: 3. Stride: 2. Padding: 1. • ReLU activation. • Convolution layer. Input dim: 32. Output dim: 32. Kernel Size: 3. Stride: 2. Padding: 1. • ReLU activation. 14 • Convolution layer. Input dim: 32. Output dim: 32. Kernel Size: 3. Stride: 2. Padding: 1. • Flatten. • Linear layer. Output dim: 128. • ReLU activation. Dynamics Network The dynamics network is an MLP with one hidden layer of size 256. It con- catenates the representation and the action as the input. Its output dimension is 128 (representation’s dimension). It uses ReLU as the hidden layer’s activation function. The output activation function is also ReLU. Value Network The value network is an MLP with two hidden layers of size 100. It uses ReLU as the hidden layer’s activation function. The output activation function is identity. We use the categorical value representation introduced in MuZero. The value prediction is discretized into 401 bins to represent the value between [−40, 40]. Therefore, the output dimension of value network is 401. Policy & BC Network The policy and the BC policy networks are both MLPs with two hidden layers of size 100. These MLPs use ReLU as the hidden layer’s activation function. Their output activation functions are identity. They produce a Squarshed-Normal action distribution, which is determined by a predicted mean and a predicted logstd. Therefore, the output dimension of them is twice of the dimension of the action space. Discriminator Network The discriminator network is an MLP with one hidden layer of size 100. It uses ReLU as hidden layer’s activation function. The output activation function is sigmoid. Projector Network The projector network is an MLP. Its structure is as follows. • Linear layer. Output dim: 1024. BatchNorm [18] with momentum 0.1. ReLU activation. • Linear layer. Output dim: 1024. BatchNorm with momentum 0.1. ReLU activation. • Linear layer. Output dim: 1024. BatchNorm with momentum 0.1. Predictor Network The predictor network is an MLP. Its structure is as follows. • Linear layer. Output dim: 512. BatchNorm with momentum 0.1. ReLU activation. • Linear layer. Output dim: 1024. A.2 MCTS Details Our MCTS implementation is mainly based on the Sampled MuZero [17]. We also apply modiﬁca- tions proposed by the EfﬁcientZero. The detailed procedure is as follows. Expansion For the task having a continuous action space, we can not enumerate all the possible actions at a node as the original MCTS algorithm. To solve this problem, we use the sampling method proposed by the Sampled MuZero. For the expansion of a node s (in the representation space), we sample K actions {ai}K i=1 from current policy π(a|s). In this work we propose to integrate BC actions into MCTS, then we actually sample from ˜π(a|s) := (1 − α)π(a|s) + απBC(a|s). (8) Here α = 0.25 is a mixture factor. Selection For the action selection, we selects action a∗ from the sampled actions that maximize the probabilistic upper conﬁdence bound a∗ = arg max a∈{ai} Q(s, a) + c(s)ˆπ(a|s) (cid:112)(cid:80) b N (s, b) 1 + N (s, a) , (9) 15 where ˆπ(a|s) = 1 K denotes the times that this pair is visited in MCTS. c(s) is a weighting coeffcient deﬁned by i δ(a, ai). Q(s, a) is the current Q-estimation of the pair (s, a). N (s, a) (cid:80) c(s) = c1 + log 1 + c2 + (cid:80) c2 b N (s, b) , (10) where c1 = 1.25, c2 = 19625. To encourage exploration, we also inject Dirichlet noise to ˆπ(a|s) at the root node. So ˆπ(a|s) becomes ˆπ(a|s) := (1 − ρ)ˆπ(a|s) + ρND(ξ). (11) Here, ρ = 0.25 is a mixture factor. ND(ξ) is the Dirichlet distribution, and ξ is set to 0.3. At the root node of MCTS, we use the discriminator network D and value network V to calculate Q-value by its original deﬁnition: Q(s, a) = R(s, a) + γV (g(s, a)). At the other nodes, we use the mean Q-value calculation used by the EfﬁcientZero. Simulation & Backup The simulation and the backup process is the same as EfﬁcientZero’s implementation, and we refer the readers to EfﬁcientZero for details. A.3 Training Details and Hyperparameters Finally, we introduce some important training details and the hyperparameters. Initialization We initialize the weights and biases of the last layer of policy, BC policy, value, and discriminator network to be zero. The other parameters are initialized by the default initializers in PyTorch. Discriminator tor [34]. We also apply gradient penalty to the discriminator network. In AIL training, it is very useful to apply the gradient penalty to the discrimina- Target Update We propose to use a target model for the calculation of policy, value, and AIL reward during reanalyze. The target model is updated periodically during training subject to an update frequency. The training hyperparameters used in the state-based experiments are in Table 4. The training hyperparameters used in the image-based experiments are in Table 5. B Environment Details B.1 State-based experiments The setup of each task in the state-based experiments is in Table 6. B.2 Image-based experiments The setup of of each task in the image-based experiments is in Table 7. We use a 48 × 48 resolution in the image-based experiments. C Other Ablations We also perform ablations on the target discriminator and the multi-step discriminator loss. To evaluate the effect of the target discriminator, we use the latest model to calculate AIL reward in the value target. To evaluate the effect of the multi-step discriminator loss, we replace it with the single-step discriminator loss. We conduct experiments on the state-based and image-based Walker and Cheetah. The results are shown in Figure 9. We ﬁnd that removing these components will not only lead to instability in training but also harm the performance. Compared with the target discriminator, the multi-step discriminator loss has a larger impact on the image-based tasks. 16 D Computation Resources All of our experiments are conducted on a server with 4 NVIDIA RTX 3090 GPUs, 64 CPU cores, and 256GB RAM. For the most of state-based and image-based experiments except Humanoid Walk and image-based Hopper Hop, our experiments require 12-18 hours of training. The main bottleneck is at the Reanalyse [43, 52], where the minibatch cannot be produced and sent to the training loop at a high frequency. We are improving the computation efﬁciency by using better parallel computation implementation and applying MCTS speed up techniques. E Visualization One approach to interpret the learned model is by the t-SNE [48] plot. We use the image-based Walker experiment as an example. We use the trained model at 100k env steps to generate the state embeddings of one expert trajectory, and in environment trajectory at 0k, 25k, 50k, 75k, and 100k steps. Then we use t-SNE to visualize the embeddings on the 2D plane. As is shown in the Figure 8, the agent’s trajectory gradually matches expert’s trajectory (blue) during training. Moreover, the expert’s trajectory has a circle structure, which represents the periodic pattern of the Walker’s walking behavior. Therefore, our model can represent the environment in a meaningful way. Figure 8: The t-SNE plot of the learned state embeddings. Figure 9: The ablation of target discriminator and multi-step discriminator loss. The results are averaged over three seeds. The shaded area displays the range of one standard deviation. 17 Cheetah (State) Walker (State) Cheetah (Image) Walker (Image) 1.0 0.8 e c n a m r o f r e P 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 Ours(No Target) Ours(No Multi) Ours 0 10K 20K 30K 40K 50K 0 10K 20K 30K 40K 50K 0 10K 20K 30K 40K 50K 0 20K 40K 60K 80K 100K Steps Steps Steps Steps Table 4: Hyperparameters for the state-based experiments Discount Factor Minibatch Size Optimizer Optimizer: Learning Rate Optimizer: Momentum Optimizer: Weight Decay Maximum Gradient Norm Unroll Steps TD Steps BC Loss Coeff. Value Loss Coeff. Policy Loss Coeff. Discriminator Loss Coeff. Gradient Penalty Target Update Interval Consistency Loss Coeff. Reanalyze Ratio Number of Simulations in MCTS Number of Sampled Actions BC Ratio α 0.99 256 SGD 0.01 0.9 1e-4 10 5 1 0.01 1.0 1.0 0.1 (1.0 for Humanoid) 1.0 200 2.0 1.0 50 16 0.25 Table 5: Hyperparameters for the image-based experiments Discount Factor Minibatch Size Stacked Frames Optimizer Optimizer: Learning Rate Optimizer: Momentum Optimizer: Weight Decay Maximum Gradient Norm Unroll Steps TD Steps BC Loss Coeff. Value Loss Coeff. Policy Loss Coeff. Discriminator Loss Coeff. Gradient Penalty Target Update Interval Consistency Loss Coeff. Reanalyze Ratio Number of Simulations in MCTS Number of Sampled Actions BC Ratio α 18 0.99 128 4 SGD 0.02 0.9 1e-4 10 5 1 0.01 1.0 1.0 0.1 1.0 200 20.0 1.0 50 16 0.25 Table 6: Task setup in the state-based experiments Task Action Repeat Expert Performance Cartpole Swingup Ball-in-cup Catch Reacher Easy Finger Spin Walker Walk Cheetah Run Hopper Hop Humanoid Walk 8 4 4 4 4 4 4 2 881.3 920.1 911.4 574.2 865.6 607.3 300.0 782.8 Table 7: Task setup in the state-based experiments Task Action Repeat Expert Performance Cartpole Swingup Ball-in-cup Catch Reacher Easy Finger Spin Walker Walk Cheetah Run Hopper Hop 881.3 920.1 911.4 574.2 873.5 607.3 300.0 8 4 4 4 2 4 4 19"
26,         5 AI Art Generators You Can Use Right Now     ,https://spectrum.ieee.org/these-ai-tools-generate-breathtaking-art-and-controversy,2022-10-06,"Most are free to start, but some are more approachable than others The front page of DALL-E 2 displays a gallery of the AI model’s best work. 2022 could go down in history as the year AI art went mainstream. An explosion of quality tools from multiple sources, built on different AI models, is making AI art accessible to anyone with a smartphone and an Internet connection. The tools use an AI model to convert text input, known as a prompt, into an image. The prompt is key: Adding or removing a single word can lead to remarkably different results. “’Prompt engineering’ is quickly becoming a valuable skill, and models that are trained on the same data and with the right prompt should produce the same results,” says Pranav Vaidhyanathan, chief technology officer of the AI-powered social media marketplace GenerAI. There’s even a growing market for prompts that create specific results. Here’s five tools to help you get started. To compare them, I gave them all the same prompt: “A person and a robot standing beside a large oak tree on a hill with clouds in the sky.” An example of DALLE-2’s response to the prompt “a person and a robot standing beside a large oak tree on a hill with clouds in the sky.”Matthew S. Smith / IEEE Spectrum OpenAI, founded in 2015, made headlines with the release of GPT-3, a natural-language model, in 2020. The DALL-E digital image model followed in January of 2021, which has since been evolved into DALL-E 2. OpenAI’s model offers excellent images across a wide variety of styles. Specific prompts can lead to specific results, or you can offer a vague prompt and enjoy several radically different results. DALL-E 2, now open to everyone through OpenAI’s website, is the best tool for those curious what the hype is about. It’s quick, beating others I’ve tried by a noticeable margin, and the website is easy to navigate. It provides four results at once, typically in much different styles, which reduces how often you need to rerun a prompt. DALL-E 2’s results are good, too. It’s the only AI model that depicted both the person and the robot. This is a commercial tool. Signing up provides you with 50 free credits, with an additional 15 free credits offered monthly. Additional credits can be purchased at a rate of 115 credits for US $15. An example of Stable Diffusion’s response to the prompt “a person and a robot standing beside a large oak tree on a hill with clouds in the sky.”Matthew S. Smith/IEEE Spectrum Stable Diffusion, from Stability AI, is popular for the same reasons as DALL-E 2: it’s quick, effective, and can produce usable images from a wide variety of prompts. Anyone can use Stable Diffusion free of charge through Stable Diffusion’s demo page. It’s not as quick as DALL-E 2 is but usually offers results in 30 seconds or less. It also provides four variations at once, just like DALL-E 2. Stable Diffusion’s model is open source, so serious users can thoroughly tweak how it works. This has supercharged its popularity as enthusiasts flock toward the model. “We are definitely seeing a trend of artists and others being attracted to open-source models such as Stable Diffusion over closed-source and controlled models such as OpenAI’s DALL-E 2,” says Vaidhyanathan. Stability AI has a commercial tool, Dream Studio, built on Stable Diffusion. It provides a trial, after which it sells credits to generate new images. In exchange, users can access sliders to tweak the model’s results. An example of Midjourney’s response to the prompt “a person and a robot standing beside a large oak tree on a hill with clouds in the sky.”Matthew S. Smith / IEEE Spectrum Midjourney earned a reputation for quality, and stirred controversy, after a contestant used it to win a digital art prize at the Colorado State Fair—without disclosing the image’s method of creation. The tool is great at vivid, ethereal, surreal images, and the user base has embraced its style. The tool is accessible only through Discord, a popular instant-messaging platform. Prompts are entered directly into chat. Chat is public, so everyone in a channel can view the prompt you’ve entered and the results. It’s sure to confuse readers not savvy to how Discord works—which is likely considered a feature, not a bug. Midjourney is a commercial product and monetized like other commercial AI art-generation tools. Everyone starts with about 25 credits but must pay a monthly membership for more. Payment is handled through a web app that can also be used to view the images generated in response to your prompts. An example of Craiyon’s response to the prompt “a person and a robot standing beside a large oak tree on a hill with clouds in the sky.”Matthew S. Smith/IEEE Spectrum Originally called DALL-E Mini, Craiyon has no direct link to OpenAI’s model, and its creators offer the tool free of charge. Results can take up to 2 minutes to generate and are low in resolution, but nine results appear at once. Craiyon differs in its use of unfiltered data and makes no specific effort to refine, train, or correct the results. Results are usually lackluster compared with those of other tools, and it has trouble dealing with fine details. Human faces, for example, look downright disturbing. There is a novelty to the tool. Serving results raw exposes the general strengths and weaknesses of AI image generation and the difficulty of creating usable results. It also highlights ethical issues, as Craiyon doesn’t filter prompts. Entering an offensive prompt demonstrates how disturbing AI image generation can be if used with malicious intent. An example of VQGAN+Clip.simple’s response to the prompt “a person and a robot standing beside a large oak tree on a hill with clouds in the sky.”Matthew S. Smith/IEEE Spectrum AI image generators’ recent popularity has inspired hundreds of tools that pair advanced AI models with a bare-bones interface. VQGAN+CLIP, which runs entirely in a Google Colaboratory notebook, is one such tool. It earns a mention because it’s (somewhat) easy to use but offers a peek under the hood. You’ll get to watch the tool iterate new variations in real time. And though accessed in a Colaboratory notebook, the model runs on your local machine. Each prompt begins as a blob but slowly morphs into a usable image. Well, sometimes, at least. The tool’s results often aren’t great. It’s slow, delivers only one variation at a time, and consumes significant video memory. On the plus side, however, it’s entirely free and contains no ads, so it’s a fine choice if you have some time on your hands. Matthew S. Smith is a freelance consumer-tech journalist. An avid gamer, he is a former staff editor at Digital Trends and is particularly fond of wearables, e-bikes, all things smartphone, and CES, which he has attended every year since 2009. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor."
44,Google’s new AI can hear a snippet of song—and then keep on playing,https://www.technologyreview.com/2022/10/07/1060897/ai-audio-generation/,2022-10-07,"A new AI system can create natural-sounding speech and music after being prompted with a few seconds of audio. AudioLM, developed by Google researchers, generates audio that fits the style of the prompt, including complex sounds like piano music, or people speaking, in a way that is almost indistinguishable from the original recording. The technique shows promise for speeding up the process of training AI to generate audio, and it could eventually be used to auto-generate music to accompany videos. (You can listen to all of the examples here.) AI-generated audio is commonplace: voices on home assistants like Alexa use natural language processing. AI music systems like OpenAI’s Jukebox have already generated impressive results, but most existing techniques need people to prepare transcriptions and label text-based training data, which takes a lot of time and human labor. Jukebox, for example, uses text-based data to generate song lyrics. AudioLM, described in a non-peer-reviewed paper last month, is different: it doesn’t require transcription or labeling. Instead, sound databases are fed into the program, and machine learning is used to compress the audio files into sound snippets, called “tokens,” without losing too much information. This tokenized training data is then fed into a machine-learning model that uses natural language processing to learn the sound’s patterns. To generate the audio, a few seconds of sound are fed into AudioLM, which then predicts what comes next. The process is similar to the way language models like GPT-3 predict what sentences and words typically follow one another. The audio clips released by the team sound pretty natural. In particular, piano music generated using AudioLM sounds more fluid than piano music generated using existing AI techniques, which tends to sound chaotic. A new wave of startups are using deep learning to build synthetic voice actors for digital assistants, video-game characters, and corporate videos. Roger Dannenberg, who researches computer-generated music at Carnegie Mellon University, says AudioLM already has much better sound quality than previous music generation programs. In particular, he says, AudioLM is surprisingly good at re-creating some of the repeating patterns inherent in human-made music. To generate realistic piano music, AudioLM has to capture a lot of the subtle vibrations contained in each note when piano keys are struck. The music also has to sustain its rhythms and harmonies over a period of time. “That’s really impressive, partly because it indicates that they are learning some kinds of structure at multiple levels,” Dannenberg says. AudioLM isn’t only confined to music. Because it was trained on a library of recordings of humans speaking sentences, the system can also generate speech that continues in the accent and cadence of the original speaker—although at this point those sentences can still seem like non sequiturs that don’t make any sense. AudioLM is trained to learn what types of sound snippets occur frequently together, and it uses the process in reverse to produce sentences. It also has the advantage of being able to learn the pauses and exclamations that are inherent in spoken languages but not easily translated into text. Rupal Patel, who researches information and speech science at Northeastern University, says that previous work using AI to generate audio could capture those nuances only if they were explicitly annotated in training data. In contrast, AudioLM learns those characteristics from the input data automatically, which adds to the realistic effect. “There is a lot of what we could call linguistic information that is not in the words that you pronounce, but it’s another way of communicating based on the way you say things to express a specific intention or specific emotion,” says Neil Zeghidour, a co-creator of AudioLM. For example, someone may laugh after saying something to indicate that it was a joke. “All that makes speech natural,” he says. Eventually, AI-generated music could be used to provide more natural-sounding background soundtracks for videos and slideshows. Speech generation technology that sounds more natural could help improve internet accessibility tools and bots that work in health care settings, says Patel. The team also hopes to create more sophisticated sounds, like a band with different instruments or sounds that mimic a recording of a tropical rainforest. However, the technology’s ethical implications need to be considered, Patel says. In particular, it’s important to determine whether the musicians who produce the clips used as training data will get attribution or royalties from the end product—an issue that has cropped up with text-to-image AIs. AI-generated speech that’s indistinguishable from the real thing could also become so convincing that it enables the spread of misinformation more easily. In the paper, the researchers write that they are already considering and working to mitigate these issues—for example, by developing techniques to distinguish natural sounds from sounds produced using AudioLM. Patel also suggested including audio watermarks in AI-generated products to make them easier to distinguish from natural audio."
60,Copilot lawsuit; Stability raises $101m; US v China CHIPLOMACY - 2,http://eepurl.com/ib8Fv9,2022-10-25,"#################################################### Common Sense Machines wants to make a 3D, temporal DALL-E: …CSM-1 is a neural network pretending to be a simulator and a sign of things to come… New AI startup Common Sense Machines has built CommonSim-1 (CSM1), a ""neural simulation engine"" which people can use to generate arbitrary 3D scenes and simulations. ""CommonSim-1 is operated with images, language, and action. A user (machine or human) shows or describes what they want to simulate and then controls the kinds of outputs they want to measure and observe,"" they write. ""At the heart of CommonSim-1 is a foundation model of the 3D world that is trained on a large-scale, growing dataset of diverse human (and non-human) experience across a wide range of tasks. We combine publicly available data, our own internal datasets, and task-specific data provided by our partners."" What can CommonSim-1 do? CSM1 can build high-resolution videos from as little as a single frame of video. ""Since this model imagines the future, one can use its imagination (1) as training data for 3D generation and perception and (2) as part of another system’s predictive model,"" they write. ""With a mesh or NeRF generated by CommonSim-1, one can type natural-language descriptions into a text prompt and generate unlimited new hybrid scenes."" Why this matters - worlds within worlds: CSM-1 is a miniature world - it's literally a world model. It combines text and image and video and provides another approach to monetizing AI; helping to take costs out of 3D design and simulation via leveraging a (presumably) gigantic model. It's also a sign of things to come - all models are going to tend towards incorporating all modalities and unfolding over time; CSM-1 is a taste of things to come. Read more: Generating 3D Worlds with CommonSim-1 (Common Sense Machines, blog)."
64,Language models learn about the world via MuJoCo; Amazon releases a big Q&A dataset; and DeepMind tests out multimodal systems - 0,http://eepurl.com/ibu68z,2022-10-17,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Amazon releases a Q&A dataset called Mintaka… and baselines show it is difficult! …20,000 Q&A pairs, translated into eight languages… Researchers with Amazon iave released Mintaka, a dataset of 20,000 question-answer pairs written in English, annotated with Wikidata entities, and translated into Arabic, French, German, Hindi, Italian, Japanese, Portuguese, and Spanish. The total dataset consists of 180,000 samples, when you include the translated versions. Existing models get 38% on the dataset when testing in English and 31% multilingually. Different types of questions and different types of complexity: Mintaka questions are spread across eight categories (movies, music, sports, books, geography, politics, video games, and history). The questions have nine types of complexity. These complexity types consist of questions relating to counting something, comparing something, figuring out who was best and worst at something, working out the ordering of something, multi-hop questions that require two or more steps, intersectional questions where the answer must fulfill multiple conditions, questions involving negatives, yes/no questions, and worker-defined 'generic' questions. How hard is Mintaka? In tests, a good baseline model (a T5 language model fine-tuned as a Q&A model), got 38% on English, and 31% averaged across the other languages. ""Overall, the baselines show that Mintaka is a challenging dataset,"" the authors write. ""None of our baselines explicitly handle all of the complexity types available in Mintaka."" Why this matters: Hard baselines are one of the things that tend to drive progress (and be useful indicators of research advances). It'll be especially interesting to see how Mintaka gets used to evaluate language models paired with retrieval systems. Prediction: I predict we get a one-shot model that performs at average of 90%+ by December 2023 on this dataset. Read more: Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering (arXiv). Get the dataset: Mintaka (Amazon Research, GitHub)."
71,GPT3 can simulate real people; AI discovers better matrix multiplication; Microsoft worries about next-gen deepfakes - 2,http://eepurl.com/ia4MgT,2022-10-11,"#################################################### Want to train an LM with RL? Now there's some free software to help you: …Train up to 20B parameter models using RL… Researchers with CarperAI, a language model collective which span off from the open source model people at Eleuther, has released Transformer Reinforcement Learning X (trlX), software for training language models with reinforcement learning. ""the trlX repo allows you to fine-tune Huggingface supported language models up to 20B parameters via either reinforcement learning using a provided scoring function or reward-labeled dataset. We aim to support a range of both online and offline RL algorithms including Proximal Policy Optimization (PPO), Natural Language Policy Optimization (NLPO), Actor Critic (A2C), and Implicit Q Learning (ILQL),"" they write. ""The library supports gpt2 and gptj with plans to include GPT-NeoX, T5 and more."" Why this matters: Reinforcement learning training is a super effective way to 'bake in' additional capabilities for a given language model. RL training is also pretty difficult and buggy. Software like trLX will make it easier for more people to train more capable language models. Read more: Welcome to Transformer Reinforcement Learning X (trlX) (GitHub)."
76,Reality collapse thanks to Facebook; open source speech rec; AI culture wars. - 0,http://eepurl.com/iaqIUj,2022-10-03,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Facebook shows the future of AI-generated videos - and it is delightful and terrifying: …Prepare for the reality collapse as a consequence of reality generation… Facebook researchers have built Make-A-Video, a system that can let users generate videos from short text descriptions, edit videos, stitch pictures together to generate videos, and so on. The most amazing part is the technique relies on paired text-image data along with unsupervised video footage; so it doesn't require a dataset of text-video footage and therefore sidesteps a potentially expensive data problem. How it works: Make-A-Video is made of a basic text-to-image (T2I) model trained on text-image pairs, spatiotemporal convolution and attention layers to help you build networks that generate things over time, and spatiotemporal networks that have a frame interpolation network. The T2I model trains on text-image pairs of 64x64 images, and two super-resolution networks that upscale this all the way to 768x768 pixels. The three components (T2I), the spatiotemporal layers, and the frame interpolation stuff, are all trained separately, then assembled into one architecture. Data: They trained the system on 2.3billion text-image pairs from the Laion-5b dataset*, and ran a NSFW-filter over this for further filtering. They also used the WebVid-10M* and a 10M subset from HD-VILA-100M to train the video generation models, and also use WebVid-10M to train the interpolation models. *Looks like WebVid contains videos scraped from Shutterstock. A good writeup about the phenomenon of even big tech companies using stuff like this here: AI Data Laundering: How Academic and Nonprofit Researchers Shield Tech Companies from Accountability (Waxy). It's really good, folks: The results are really, really impressive. Want a short video of a bear painting a portrait of a bear? Done. Want a UFO flying over a desert? Done. Want asteroids tumbling through space? Why, of course. How about variations on existing videos? Sure. Honestly, take a look at the blog and main site linked below and see for yourself - the results are wild. And remember, all we need to do is turn the crank on dataset scale and network complexity to scale this out for longer periods of time and for even greater diversity. ""Learning world dynamics from orders of magnitude more videos using unsupervised learning helps researchers break away from the reliance on labeled data,"" they write. Why this matters: Reality generation and reality collapse: All these generative models point to the same big thing that's about to alter culture; everyone's going to be able to generate their own custom and subjective aesthetic realities across text, video, music (and all three) in increasingly delightful, coherent, and lengthy ways. This form of fractal reality is a double-edged sword - everyone gets to create and live in their own fantasies that can be made arbitrarily specific, and that also means everyone loses a further grip on any sense of a shared reality. Society is moving from having a centralized sense of itself to instead highly individualized choose-your-own adventure islands, all facilitated by AI. The implications of this are vast and unknowable. Get ready. Read more: Introducing Make-A-Video: An AI system that generates videos from text (Facebook research blog). Read the research: Make-A-Video: Text-to-Video Generation without Text-Video Data (arXiv). Find out more at the main site, and also apply to potentially get access to future systems (Facebook site)."
83,Google Leverages Transformers to Vastly Simplify Neural Video Compression With SOTA Results,https://syncedreview.com/2022/06/17/google-leverages-transformers-to-vastly-simplify-neural-video-compression-with-sota-results/,2022-06-17,"Neural network-based approaches have made significant progress on video compression over the last several years, reaching performance on par with classical codec-based methods. These novel neural approaches however are challenging to implement, as they tend to require complex hand-crafted connections between their many sub-components and struggle when the input data does not match their architectural biases and priors. In the new paper VCT: A Video Compression Transformer, a Google Research team presents an “elegantly simple” but powerful video compression transformer (VCT) that eliminates the architectural biases and priors of previous approaches (such as motion prediction and warping operations), and instead learns totally from data without any hand-crafting. VCT is easy to implement and outperforms existing video compression methods on standard datasets. The proposed VCT is based on the original language translation transformer (Vaswani et al., 2017) and is tasked with translating the previous two frames of a video input into the current frame. It first uses lossy transform coding to project frames from the image space to quantized representations. A transformer then leverages temporal redundancies to model the representation distributions. These predicted distributions are then used to compress the quantized representations via entropy coding. In their empirical studies, the team trained VCT on one million Internet video clips and compared it to video compression approaches such as the classical HEVC (High-Efficiency Video Coding) and neural methods such as SSF (Scale-Space Flow, Agustsson et al., 2020) and ELF-VC (Efficient Learned Flexible-Rate Video Coding, Rippel et al., 2021). The evaluations were conducted on the MCL-JCV and UVG benchmark datasets, with PSNR (peak signal-to-noise ratio) and MS-SSIM (multi-scale structural similarity index for motion detection) as metrics. Despite its simplicity — and not using flow prediction, warping or residual compensation — VCT surpassed all methods in both PSNR and MS-SSIM in the evaluations. Moreover, experiments on synthetic data showed that VCT can also learn to handle complex motion patterns such as panning, blurring and fading, purely from data. The team says VCT can reduce bandwidth requirements for video conferencing and streaming and enable better utilization of storage space, and hope it can serve as a foundation for a new generation of video codecs. The VCT code has been released on the project’s GitHub. The paper VCT: A Video Compression Transformer is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
84,Wav2Vec 2.0 Learns Brain-Like Representations From Just 600 Hours of Unlabeled Speech Data in New Study,https://syncedreview.com/2022/06/16/wav2vec-2-0-learns-brain-like-representations-from-just-600-hours-of-unlabeled-speech-data-in-new-study/,2022-06-16,"Deep neural networks have recently hinted at their potential for processing speech in a manner more like the human brain and generating activations similar to those of the brain in response to the same inputs. The development of such algorithms however remains difficult as they require massive training data, supervised labels, textual data rather than more realistic raw sensory data, and prohibitively large memory. In the new paper Toward a Realistic Model of Speech Processing in the Brain with Self-supervised Learning, a research team from Meta AI, PSL University, Université Paris Cité, Université Paris-Saclay, University of Toronto and INSERM shows that self-supervised architectures such as Wav2Vec 2.0 (Baevski et al., 2020) that stack convolutional and transformer layers to predict a quantization of the latent representations of speech waveforms can learn brain-like representations from as little as 600 hours of unlabelled speech; and can also learn sound-generic and speech- and language-specific representations similar to those of the prefrontal and temporal cortices. The team summarizes their study’s main contributions as: The Wav2Vec 2.0 architecture comprises three modules: 1) a feature encoder that transforms raw mono speech waveform inputs into latent representations, 2) a quantization module that discretizes the latent representations into a dictionary of discrete and latent representations of sounds, and 3) a “context network” that uses the previously generated outputs to produce contextualized embeddings. The team trained several variants of Wav2Vec 2.0 on different datasets with both self-supervised and supervised learning objectives and extracted the activations of each layer from both the feature encoder and the context network. In their empirical studies, the team compared the Wav2Vec 2.0 learned representations to those in the brains of 412 human volunteers (351 English speakers, 28 French speakers and 33 Mandarin speakers) recorded with functional magnetic resonance imaging (fMRI) while they passively listened to approximately one hour of audio books in their native language. The experimental results show that Wav2Vec 2.0 model activations can predict brain activity in nearly all cortical areas, self-supervised learning leads to slightly better performance than supervised learning, the hierarchy of Wav2Vec 2.0 maps onto the hierarchy of the cortex, and 600 hours of self-supervised learning suffices for Wav2Vec 2.0 to learn brain-like language-specific representations. Overall, this work demonstrates that applying self-supervised learning to a limited amount of speech data can enable the learning of representations similar to the human brain’s speech perception, taking a step toward a realistic model of speech processing in the brain with self-supervised learning.The paper Toward a Realistic Model of Speech Processing in the Brain with Self-supervised Learning is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
86,444 Authors From 132 Institutions Release BIG-bench: A 204-Task ‘Extremely Difficult and Diverse’ Benchmark for Large Language Models,https://syncedreview.com/2022/06/14/444-authors-from-132-institutions-release-big-bench-a-204-task-extremely-difficult-and-diverse-benchmark-for-large-language-models/,2022-06-14,"Powered by their ever-increasing scale, today’s large language models have shown breakthrough capabilities beyond natural language processing (NLP), in areas such as writing computer code, diagnosing medical conditions and playing competitive games. As the development and deployment of large-scale language models continues, it is important that the AI community understands their current and near-future capabilities and limitations. In the new paper Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models, 444 authors from 132 institutions introduce Beyond the Imitation Game (BIG-bench), a large-scale, extremely difficult and diverse benchmark that includes 204 tasks for predicting the potentially transformative effects of large language models. BIG-bench was named in homage to Alan Turing’s imitation game (Turing, 1950); and designed for analyzing dense and sparse transformer models such as those from Google and OpenAI, whose scales range from millions to hundreds of billions of parameters. The team summarizes their BIG-Bench suite as follows: BIG-bench supports two types of tasks: JSON (JavaScript Object Notation) and programmatic. The JSON file contains a list of input-target pairs, and performance is evaluated by comparing the outputs and the targets. The programmatic tasks are written in Python and are evaluated by measuring the generated text continuations for given inputs and computing conditional log probabilities of target given inputs. The BIG-bench task scope ranges from writing codes, playing competitive games and common-sense reasoning to social bias, linguistics, software development and beyond. It can also measure progress well beyond the current state of the art. The researchers’ experiments with BIG-bench revealed a number of behavioural characteristics of large language models, such as: 1) Aggregate performance improves with model size but can’t compete with human performance; 2) Model predictions grow better calibrated with increased scale; 3) Model classes behave similarly, with benefits from sparsity; 4) Breakthrough behaviour is sensitive to details of task specification; and 5) Even programmatic measures of model capability can be highly subjective. The team also tackled the thorny topic of social biases in large language models. They observed that biases often increase with scale in settings with broad or ambiguous context and can decrease with scale in settings with narrow unambiguous context; and that biases can potentially be steered through appropriately chosen prompting. The team considers BIG-bench a “living benchmark” and will continue to accept new task submissions for peer review on a rolling basis. They hope BIG-bench can help identify additional breakthrough capabilities and enable researchers to better understand the power and potential of current and future large language models. The BIG-bench project was collaboratively developed on the GitHub repository, where the code is now open-sourced. The paper Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
87,"Cambridge, Google & Secondmind’s Neural Diffusion Processes Challenge Gaussian Processes for Describing Rich Distributions Over Functions",https://syncedreview.com/2022/06/13/cambridge-google-secondminds-neural-diffusion-processes-challenge-gaussian-processes-for-describing-rich-distributions-over-functions/,2022-06-13,"While researchers have traditionally employed Gaussian processes (GP) for specifying prior and posterior distributions over functions, this approach becomes computationally expensive when scaled, is limited by the expressivity of its covariance function, and struggles with adapting a point estimation for the hyperparameters. A research team from the University of Cambridge, Secondmind, and Google Research addresses these issues in the new paper Neural Diffusion Processes, proposing Neural Diffusion Processes (NDPs). The novel framework learns to sample from rich distributions over functions at a lower computational cost and capture distributions that are close to the true Bayesian posterior of a conventional Gaussian process. The paper’s lead author, Vincent Dutordoir, explains, “Bayesian inference for regression is great, but it is often very costly and requires making a priori modelling assumptions. What if we can train a big neural net to sample plausible posterior samples over functions? This is the premise of our Neural Diffusion Processes.” The team summarizes their main contributions as: The proposed NDP is a denoising diffusion model-based approach for learning probabilities from a function and producing prior and conditional samples of functions. It allows full marginalization over the GP hyperparameters while reducing the computational burden compared to GPs. The team first examined existing state-of-the-art neural network-based generative models in terms of sample quality. Based on their findings, they designed NDP to generalize diffusion models to infinite-dimensional function spaces by enabling the indexing of random variables onto which the model diffuses. The researchers also adopted a novel bi-dimensional attention block to guarantee equivariance over the input dimensionality and sequence and enable the model to draw samples from a stochastic process. As such, NDP can leverage the benefits of stochastic processes, such as exchangeability. In their empirical study, the team evaluated the proposed NDP’s ability to produce high-quality conditional samples and marginalize over kernel hyperparameters; and on its input dimensionality invariance. The results show that NDP is able to capture functional distributions that are close to the true Bayesian posterior while reducing computational burdens. The researchers note that while NDP sample quality improves with the number of diffusion steps, this also results in slower inference times. They suggest inference acceleration or sample parameterizing techniques could be explored in future studies to address this issue. The paper Neural Diffusion Processes is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
88,Yoshua Bengio Team’s Large-Scale Analysis Reveals the Benefits of Modularity and Sparsity for DNNs,https://syncedreview.com/2022/06/10/yoshua-bengio-teams-large-scale-analysis-reveals-the-benefits-of-modularity-and-sparsity-for-dnns/,2022-06-10,"Deep neural networks (DNNs) have drawn much inspiration from the human cognitive process, evidenced recently in their incorporation of modular structures and attention mechanisms. By representing knowledge in a modular manner and selecting relevant information via attention mechanisms, DNN models can develop meaningful inductive biases, boost their out-of-distribution generalization abilities, and manipulate concepts at higher levels of cognition. While modular architectures provide proven advantages for DNNs, there currently exists no rigorous quantitative assessment method for them due to the complexity and unknown nature of real-world data distributions. As such, it is unclear whether or to what extent the performance gains obtained by modular systems are actually attributable to good modular architecture design. In the new paper Is a Modular Architecture Enough, a research team from Mila and the Université de Montréal conducts a rigorous and thorough quantitative assessment of common modular architectures that reveals the benefits of modularity and sparsity for DNNs and the sub-optimality of existing end-to-end learned modular systems. The team summarizes their main contributions as: “ The team considers four model types with different levels of specialization: Monolithic, a large neural network that takes the entire data as input; Modular, a number of modules, each of which is a neural network that takes the data as input; Modular-op, similar to the modular system but with activation decided only by the rule context; and GT-Modular, which serves as an oracle benchmark, i.e., a modular system that specializes perfectly. They conduct a step-by-step analysis of the benefits of each system and contrast simple end-to-end trained modular systems with monolithic systems. The team explores both in-distribution and out-of-distribution performance and evaluates how different models perform on a variety of tasks. They also introduce two metrics — Collapse-Avg and Collapse-Worst — to measure the amount of collapse suffered by a modular system; and use alignment, adaptation and inverse mutual information metrics to quantify the amount of specialization obtained. In the experiments, the GT-Modular system generally had the highest performance, confirming the advantages of perfect specialization. Although standard end-to-end trained modular systems slightly outperformed monolithic systems, the team notes that these systems’ reliance on backpropagation of the task losses does not enable them to discover perfect specialization. Both the Modular and Modular-op systems were shown to have collapse issues, but Modular-op generally suffered fewer. The team suggests a deeper investigation into forms of regularization may help alleviate these collapse problems. Overall, this work shows that modular models outperform monolithic models. Although modular networks can obtain perfectly specialized solutions, end-to-end training does not recover them, and additional inductive biases are required to learn adequately specialized solutions. The team hopes their work will motivate future research into the design and development of modular architectures.Open-sourced implementation is available on the project’s GitHub. The paper Is a Modular Architecture Enough? is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
89,Microsoft’s XTC Extreme Lightweight Compression Method for Pretrained Transformers Achieves SOTA Results and 50x Smaller Model Sizes,https://syncedreview.com/2022/06/09/microsofts-xtc-extreme-lightweight-compression-method-for-pretrained-transformers-achieves-sota-results-and-50x-smaller-model-sizes/,2022-06-09,"Pretrained transformer models have grown dramatically in recent years and now reach hundreds of billions of parameters. Although these behemoths are achieving unprecedented performance on natural language processing (NLP) tasks, their ever-expanding size has limited their real-world deployment on resource-constrained edge or embedded devices. In the new paper Extreme Compression for Pre-trained Transformers Made Simple and Efficient, a Microsoft research team proposes XTC, a simple yet effective extreme compression pipeline for pre-trained transformers. XTC can skip the compute-heavy pretraining knowledge distillation (KD) process to obtain a 5-layer BERT model with better performance than previous state-of-the-art distillation methods, and its extreme quantization and layer reduction can cut model sizes by 50x. The team summarizes their main contributions as: The proposed XTC pipeline comprises two steps: 1) Lightweight layer reduction. Instead of adopting computationally expensive pretraining distillation, the researchers first employ a subset of the fine-tuned teacher weights as a lightweight layer reduction method to initialize a layer-reduced model. When combined with the team’s other training strategies, this lightweight approach reduces computational cost and achieves a much higher compression ratio than other existing methods. 2) 1-bit quantization by applying 1S-KD with DA and long training. The team applies quantize-aware 1S-KD (one-step knowledge distillation), using an ultra-low bit (1-bit/2-bit) quantizer to compress the layer-reduced model weights obtained in step 1 for a forward pass, then uses a straight-through estimator (STE) in a backward pass for passing gradients. The team minimizes the single-stage deep KD objective with data augmentation (DA) and longer training, such that the training loss is close to zero. In their empirical study, the team applied their novel compression approach to the BERT large language model, using the standard General Language Understanding Evaluation (GLUE) benchmark. The experimental results show that the proposed XTC can compress BERTbase to a 5-layer BERTbase while outperforming previous state-of-the-art distillation methods such as the 6-layer TinyBERT without incurring the computationally expensive pretraining distillation. The method’s robust extreme quantization can also reduce model size by 50x with better accuracy than prior extreme quantization methods; and achieve state-of-the-art results on GLUE tasks. Overall, this work introduces a simple yet effective compression pipeline for extreme compression in pretrained transformers, providing a possible solution for deploying such models on resource-constrained devices. The code will be released on the Microsoft DeepSpeed GitHub. The paper Extreme Compression for Pre-trained Transformers Made Simple and Efficient is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
92,Snap & NEU’s  EfficientFormer Models Push ViTs to MobileNet Speeds While Maintaining High Performance,https://syncedreview.com/2022/06/06/snap-neus-efficientformer-models-push-vits-to-mobilenet-speeds-while-maintaining-high-performance/,2022-06-06,"First proposed in 2020, vision transformers (ViT) have demonstrated promising performance across a variety of computer vision tasks. These breakthroughs however have come at the cost of speed, as ViTs run much slower than convolutional neural networks (CNNs). This latency issue and their extremely high computational costs have made it challenging to deploy ViTs on resource-constrained hardware such as mobile devices, limiting their real-world application. A research team from Snap Inc. and Northeastern University addresses this issue in the new paper EfficientFormer: Vision Transformers at MobileNet, which identifies inefficient operators in ViT architectures and proposes a new ViT design paradigm. The team’s resulting EfficientFormer models run as fast as lightweight MobileNet CNNs while maintaining the high performance of transformer architectures. The researchers summarize their study’s main contributions as: The proposed EfficientFormer comprises patch embedding and a stack of meta transformer blocks, where each block contains an unspecified token mixer followed by a multilayer perceptron block. The network has four stages, each serving as an embedding operation that maps the embedding dimensions and downsamples token length. EfficientFormer thus remains a fully transformer-based model that does not use MobileNet structures. The team also introduces a simple yet effective gradient-based search algorithm that obtains candidate networks to optimize EfficientFormer’s inference speed. In their empirical study, the team compared EfficientFormer with widely used CNN-based models and existing ViTs on image classification, object detection, and segmentation tasks. EfficientFormer outperformed existing transformer models and most competitive CNNs in the experiments, with the fastest variant, EfficientFormer-L1, achieving 79.2 percent top-1 accuracy on ImageNet-1K with only 1.6 ms inference latency on an iPhone 12; and the largest variant, EfficientFormer-L7, reaching 83.3 percent accuracy with only 7.0 ms latency. The study shows that ViTs can reach MobileNet speeds on mobile devices while maintaining transformers’ high performance. The team’s future research will explore EfficientFormer’s potential on other resource-constrained hardware.The EfficientFormer code and models are available on the project’s GitHub. The paper EfficientFormer: Vision Transformers at MobileNet Speed is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
94,Google Brain’s UViM: A Unified Approach for Modelling Diverse Vision Tasks Without Modifications,https://syncedreview.com/2022/06/02/google-brains-uvim-a-unified-approach-for-modelling-diverse-vision-tasks-without-modifications/,2022-06-02,"Deep neural networks have revolutionized the field of computer vision, achieving unprecedented performance across a wide range of tasks. The production of high-dimensional structured outputs for vision tasks such as image segmentation, monocular depth estimation, object detection, etc. however requires human handcrafting of network architectures and tailoring of training procedures for each specific task. These are time-consuming processes that can also introduce the need for expert knowledge with regard to the task at hand. A Google Brain research team challenges this “fragmented” vision modelling paradigm in their new paper UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes, proposing UViM (Unified Vision Model), a novel approach that leverages language modelling and discrete representation learning to enable the modelling of diverse computer vision tasks without any task-specific modifications. In the field of natural language processing (NLP), autoregressive sequence models parameterized by transformer architectures have emerged as a prominent unified model that enjoys advantages such as theoretical soundness, expressiveness, and robustness. This motivated the Google researchers to design a similar general solution for computer vision. The proposed UViM is a unified computer vision model that combines a standard feedforward base model and an autoregressive language model. It can handle vision tasks that deal with extremely high dimensional and structured outputs with much lower computational costs. The UViM optimization procedure comprises two training stages: learning with a guiding code and learning to model the guiding code. In the first stage, a restricted oracle model produces a short discrete sequence (guiding code) to help the base model solve complex vision tasks and reduce the cost of high-dimensional structured prediction. In the second stage, the team trains a language model to output a guiding code by learning to “mimic” the oracle using only the image input. The resulting UViM is thus equipped to model highly structured outputs for diverse vision tasks. In their empirical study, the team applied UViM to three diverse vision tasks: general scene understanding panoptic segmentation, conditional generative image colorization, and 3D scene depth prediction understanding. In the evaluations, the proposed UViM achieved results competitive with the state-of-the-art on all three tasks, confirming its ability to handle diverse vision tasks in a unified manner. The team regards UViM as a “brave new prototype” for a general-purpose unified computer vision model and hopes their paper will motivate future research on the generation of better guiding codes and the design of more efficient training procedures. The paper UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
96,Discovering Policies with DOMiNO: Diversity Optimization Maintaining   Near Optimality,"[{'href': 'http://arxiv.org/abs/2205.13521v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2205.13521v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-05-26 17:40:52,"The CLRS Algorithmic Reasoning Benchmark Petar Veliˇckovi´c 1 Adri`a Puigdom`enech Badia 1 David Budden 1 Razvan Pascanu 1 Andrea Banino 1 Misha Dashevskiy 1 Raia Hadsell 1 Charles Blundell 1 2 2 0 2 n u J 4 ] G L . s c [ 2 v 9 5 6 5 1 . 5 0 2 2 : v i X r a Abstract Learning representations of algorithms is an emerging area of machine learning, seeking to bridge concepts from neural networks with clas- sical algorithms. Several important works have investigated whether neural networks can effec- tively reason like algorithms, typically by learning to execute them. The common trend in the area, however, is to generate targeted kinds of algorith- mic data to evaluate speciﬁc hypotheses, making results hard to transfer across publications, and increasing the barrier of entry. To consolidate progress and work towards uniﬁed evaluation, we propose the CLRS Algorithmic Reasoning Bench- mark, covering classical algorithms from the In- troduction to Algorithms textbook. Our bench- mark spans a variety of algorithmic reasoning procedures, including sorting, searching, dynamic programming, graph algorithms, string algorithms and geometric algorithms. We perform extensive experiments to demonstrate how several popular algorithmic reasoning baselines perform on these tasks, and consequently, highlight links to several open challenges. Our library is readily available at https://github.com/deepmind/clrs. 1. Introduction Neural networks and classical algorithms are two techniques that operate on diametrically opposite (and complementary) sides of problem-solving: neural networks can adapt and generalise to raw inputs, automatically extracting appro- priate features and a single neural network setup is often applicable to many separate tasks (Zamir et al., 2018). How- ever, they are hard to interpret, notoriously unreliable when extrapolating outside of the dataset they have been trained on, and rely on massive quantities of training data. On 1DeepMind. Correspondence to: Petar Veliˇckovi´c <petarv@deepmind.com>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). the other hand, algorithms trivially strongly generalise to inputs of arbitrary sizes, and can be veriﬁed or proven to be correct, with interpretable step-wise operations. Their shortcoming is that inputs must be made to conform to a par- ticular algorithm speciﬁcation, and looking at a separate task often requires coming up with an entirely new algorithm (Veliˇckovi´c & Blundell, 2021). Bringing the two sides closer together can therefore yield the kinds of improvements to performance, generalisation and interpretability that are unlikely to occur through archi- tectural gains alone. Accordingly, algorithmic modelling as a domain for testing neural networks has been gaining popularity over the last few years (Zaremba & Sutskever, 2014; Kaiser & Sutskever, 2015; Trask et al., 2018; Vinyals et al., 2015; Kool et al., 2018; Freivalds et al., 2019; Dwivedi et al., 2020; Chen et al., 2020; Tang et al., 2020; Veliˇckovi´c et al., 2019; Yan et al., 2020; Deac et al., 2020) due to its ability to highlight various reasoning limitations of existing architectures. Earlier work (Zaremba & Sutskever, 2014; Kaiser & Sutskever, 2015) focused on the need of long-term mem- ory capabilities when executing algorithms, which offered a good test-bed for various recurrent and memory architec- tures. Recently, algorithmic tasks have been used to high- light the efﬁciency of graph neural networks (Dwivedi et al., 2020; Chen et al., 2020; Veliˇckovi´c et al., 2019; Yan et al., 2020; Corso et al., 2020; Tang et al., 2020; Georgiev & Li´o, 2020; Veliˇckovi´c et al., 2020) and to distinguish between different variations of them, typically through the lens of algorithmic alignment—architectures that align better with the underlying algorithm can be proven to have better sam- ple complexity (Xu et al., 2019). Unfortunately, many of these works remain disconnected in terms of the algorithms they target, how the data is presented to the model or through the training and testing protocols they use, making direct comparison somewhat difﬁcult. To make a ﬁrst step towards a uniﬁed benchmark for al- gorithmic reasoning tasks, we propose a comprehensive dataset which we will refer to as The CLRS Algorithmic Reasoning Benchmark, in homage to the Introduction to Al- gorithms textbook by Cormen, Leiserson, Rivest and Stein (Cormen et al., 2009). The CLRS Algorithmic Reasoning Benchmark Within this benchmark, we propose and evaluate on CLRS- 30: a dataset containing trajectories—a trajectory is formed of inputs, the corresponding outputs and optional interme- diary targets—of 30 classical algorithms covering various forms of reasoning, including sorting, searching, dynamic programming, geometry, graphs and strings. Some of these algorithms are depicted in Figure 1. The appeal and moti- vation for such a benchmark goes beyond unifying or pro- viding a common ground for previous works, as we will describe. We believe that CLRS-30 is well positioned to ex- plore out-of-distribution (OOD) generalization and transfer (as potentially part of a meta-learning setting) given the ex- plicit and known relationship between different algorithms (e.g. what subroutines are shared and so forth). 2. Motivation Figure 1. Example of four algorithms within CLRS-30. A) in- sertion sort; B) string matching; C) greedy task scheduling; D) shortest paths. Timely posed benchmarks have led to a signiﬁcant progress in the ﬁeld, from the impact of ImageNet (Russakovsky et al., 2015) on the vision community, to that of Wikipedia and Penn Treebank in popularizing neural networks for lan- guage modelling (Merity et al., 2016; Mikolov et al., 2011) or Atari-2600 for deep reinforcement learning (Bellemare et al., 2013). The prevalence of recent works focusing on al- gorithmic reasoning1, as well as a history of disparate work on a variety of bespoke benchmarks (Graves et al., 2014; Zaremba & Sutskever, 2014; Kaiser & Sutskever, 2015; Trask et al., 2018), suggests signiﬁcant utility in a bench- mark covering a wide-range of classical CS algorithms. Learning to mimic an algorithm also provides an opportu- nity to extensively test the limitations of architectures both in terms of their representation capacity and processing. This can then be related back directly onto underlying oper- ations and qualities of the well-studied CS algorithms being mimicked as we are aware of both the process used to gen- erate the inputs and the speciﬁcs of the underlying function 1Concurrent works published at the same venue include: (Xu et al., 2019; Veliˇckovi´c et al., 2019) at ICLR’20 and (Veliˇckovi´c et al., 2020; Corso et al., 2020; Tang et al., 2020) at NeurIPS’20. producing the corresponding outputs. Hence, benchmarking in this area can be used to better understand the limitations of current architectures and the optimisation schemes used. This benchmarking can come in many forms: Data can be easily generated, allowing the neural network behaviour to be probed under different regimes: from few- shot learning all the way to inﬁnite-data. Algorithms can be used to understand the efﬁciency of dif- ferent inductive biases and neural components. For example, a recent study (Tang et al., 2020) has demonstrated the direct beneﬁts of choosing inductive biases that align well with iterative algorithms. Algorithms have also been used to high- light the importance of attention mechanisms (Graves et al., 2014) or to disambiguate various message passing mecha- nisms for graph neural networks (Richter & Wattenhofer, 2020; Joshi et al., 2020; Veliˇckovi´c et al., 2019). Algorithms can require repeated computation, recursion, or performing very different forms of computations con- ditioned on the input, providing an excellent test-bed for evaluating compositionality; i.e. whether an algorithm ex- ecutor can effectively exploit these repeated computations. One can control the amount of memory required to solve a problem instance, hence test the memorization ability of neural networks. Moreover, one can build a curriculum of tasks of increasing memory requirements (Zaremba & Sutskever, 2014). Control over the difﬁculty of problem instances also allows the behaviour of a trained model to be tested on OOD sam- ples. While neural networks are highly efﬁcient on solving complex perceptual tasks, current theoretical understanding suggests that their power relies on their ability to interpo- late (Liu et al., 2020; Belkin et al., 2019; Jacot et al., 2018), limiting them to in-distribution generalisation. General rea- soning systems, however, need to be able to expand beyond this type of generalization. OOD generalization (Li et al., 2020) is paramount, as generally one can not control the distribution a model will face over time when deployed. Understanding how algorithms operate on corner cases is a standard approach for analysing their correctness. Sim- ilarly, understanding the behaviour of a trained model on larger instances of the problem, or instances that expose such corner cases that were not covered in the training set, can elucidate to what degree the model has truly learned the algorithm (as opposed to overﬁtting to speciﬁc statistics of the training data). Particularly, we can control how far from the training distribution a test instance is, potentially allow- ing us to understand to what extent the model generalizes OOD, and under which circumstances. In turn, this can offer insight into the effectiveness of different inductive biases, highlighting what kinds of inductive biases are useful for mimicking reasoning processes. 2 3 4 5 6 1 1 2 4 5 6 3 A) B) b a c b a b a b a a b c b a b T s a b a b a c a S q C) D) a2 a3 a1 a1 a1 a4 0 6 7 2 8 7 5 -2 2 9 -3 -4 4 7 -2 The CLRS Algorithmic Reasoning Benchmark One would also expect a general reasoning system to be able to reuse parts of learned computations when learning a new task, and to compose learnt computational subrou- tines (Lake, 2019; Grifﬁths et al., 2019; Alet et al., 2018). These forms of generalization have been the aim of several learning paradigms from transfer learning to meta-learning and continual learning or domain adaptation. However, many of these paradigms rely on the concept of a task, and measuring or understanding the ability of a learned sys- tem to reuse or compose requires the ability to decompose a task into sub-tasks and to be able to relate tasks among themselves. In many scenarios, such decompositions are am- biguous. Without a clear segmentation into sub-tasks, there can be no clearly deﬁned distance metric between tasks (Du et al., 2018). Conversely, algorithms are built based on subroutines that tend to be extensively shared, providing a good playground for formalizing and measuring reuse and composition, making an algorithmic reasoning benchmark potentially attractive to meta-learning practitioners. Lastly and fundamentally, computer scientists rely on a rela- tively small2 number of algorithms to address an extremely vast set of problems. They can be seen as a very powerful basis that spans most forms of reasoning processes. On one hand, this means that any generic reasoning system likely has to be able to reproduce all such kinds of procedures, hence, building a system that properly learns all of them is a major stepping stone towards generic reasoning. On the other hand, this means that they can be used to discover inductive biases that will enable tackling more complex problems. This is either because these complex problems can be seen as a combination of several algorithms, or be- cause learning certain algorithms can provide a reliable way for the model to learn how to access its own memory or how to attend to its input or other such internal mechanisms. So by ﬁrst training on algorithms—potentially controlling the difﬁculty of training instances—one can pre-train for tasks where full trajectories may not be available (Veliˇckovi´c et al., 2021). One such example is discovering novel polynomial- time heuristics for combinatorial optimisation (Bengio et al., 2020; Cappart et al., 2021; Khalil et al., 2017) or reinforce- ment learning (Deac et al., 2021). Note that our focus with this benchmark lies in learning the basic algorithms them- selves only–this in itself proves sufﬁciently challenging for neural networks, and is itself a useful outcome for the rea- sons highlighted above. However, we speculate that once a neural network can learn not only individual algorithms but novel combinations of multiple algorithms or even discover new algorithms, such networks will be useful in a wide variety of problems from scientiﬁc problems such as pro- tein folding and genomics to simulated environments such as those used by reinforcement learning and control–much 2The entire Introduction to Algorithms textbook (Cormen et al., 2009) proposes and discusses ∼100 algorithms in total. as classic CS algorithms already make in-roads into these domains but lack the ability to learn from data. Guided by these observations, we regard CLRS-30 as a ﬁrst step towards a pragmatic setting to test many of these dif- ferent aspects of current architectures. While we do not directly target all of the scenarios outlined above, the bench- mark was built with ease of expansion in mind; enabling for extensive tweaking of training/testing setups, kinds of information captured in algorithm trajectories, as well as including additional algorithms, which we aim to do consis- tently over time. 3. CLRS Algorithmic Reasoning Benchmark Owing to its name, CLRS-30 consists only of algorithms which may be encountered in the CLRS textbook (Cormen et al., 2009). Further, all algorithm trajectories and relevant variables have been designed to match the pseudocode in the textbook as closely as possible. We begin by describing the selection criteria we applied when determining which algorithms to include in CLRS-30. Our initial survey of the textbook yielded 94 algorithms and data structures of interest. From this point, we set out to ﬁlter this set to algorithms suitable for inclusion in the initial version of our benchmark. The criteria we applied, with justiﬁcation and remarks, are as follows: We want to be able to reliably generate ground-truth outputs for large inputs. As such, NP-hard tasks (and approximation algorithms thereof) have been excluded. Our decision is backed up by theoretical work suggesting impossibility of accurately modelling NP-hard problems using polynomial- time samplers, unless NP=co-NP (Yehuda et al., 2020). Tasks requiring numerical outputs have been excluded. Eval- uating their performance is ambiguous, and may be depen- dent on the way architectures choose to represent numbers. For example, Yan et al. (2020) (which represents numbers in binary) and Veliˇckovi´c et al. (2019) (which represents them in ﬂoating-point) report different metrics on predicting shortest-path lengths. This excludes most number-theoretic algorithms, linear programming, and max-ﬂow3. It does not exclude shortest-path algorithms: we can treat them as tasks of ﬁnding edges belonging to the shortest path, as was done in Veliˇckovi´c et al. (2019); Tang et al. (2020). The numeri- cal values of path lengths are then treated as intermediate parts of the trajectory, and not directly evaluated on. Standalone data structures do not directly represent a task4. 3It should be noted that, by the max-ﬂow min-cut theorem (Ford Jr & Fulkerson, 2015), any max-ﬂow problem can be cast as ﬁnding the minimum cut containing the source vertex. This is a discrete decision problem over input vertices, which hence doesn’t violate our constraints, and could be included in future iterations. 4In programming language terms, their algorithms tend to be The CLRS Algorithmic Reasoning Benchmark Rather, their target is appropriately updating the internal state of the data structure. Hence, we don’t include their operations, unless they appear as components of algorithms. We, of course, look forward to including them in subsequent versions of the dataset, as they can provide useful building blocks for learning complex algorithms. Lastly, there are representational issues associated with dy- namically allocated memory—it may be unclear what is the best way to represent the internal memory storage and its usage in algorithm trajectories. One example of the ambi- guity is in asking whether the algorithm executor should start with a “scratch space” deﬁned by the space complexity of the problem that gets ﬁlled up, or dynamically generate such space5 (Strathmann et al., 2021). As such, we for now exclude all algorithms that require allocating memory which cannot be directly attached to the set of objects provided at input time. This excludes algorithms like merge sort, Hierholzer’s algorithm for ﬁnding Euler tours (Hierholzer & Wiener, 1873), or string matching using ﬁnite automata. All of the above applied, we arrive at the 30 algorithms that are selected into CLRS-30, which we categorize as follows: Sorting: Insertion sort, bubble sort, heapsort (Williams, 1964), quicksort (Hoare, 1962). Searching: Minimum, binary search, quickselect (Hoare, 1961). Divide and Conquer (D&C): Maximum subarray (Kadane’s variant (Bentley, 1984)). Greedy: Activity selection (Gavril, 1972), task scheduling (Lawler, 1985). Dynamic Programming: Matrix chain multiplication, longest common subsequence, optimal binary search tree (Aho et al., 1974). Graphs: Depth-ﬁrst and breadth-ﬁrst search (Moore, 1959), topological sorting (Knuth, 1973), articulation points, bridges, Kosaraju’s strongly-connected components algo- rithm (Aho et al., 1974), Kruskal’s and Prim’s algorithms for minimum spanning trees (Kruskal, 1956; Prim, 1957), Bellman-Ford and Dijkstra’s algorithms for single-source shortest paths (Bellman, 1958; Dijkstra et al., 1959) (+ di- rected acyclic graphs version), Floyd-Warshall algorithm for all-pairs shortest paths (Floyd, 1962). Strings: Na¨ıve string matching, Knuth-Morris-Pratt (KMP) string matcher (Knuth et al., 1977). Geometry: Segment intersection, Convex hull algorithms: Graham scan (Graham, 1972), Jarvis’ march (Jarvis, 1973). The chosen algorithms span a wide variety of reasoning of the void type. 5Akin to malloc-like calls in C++. procedures, and hence can serve as a good basis for algorith- mic reasoning evaluation, as well as extrapolation to more challenging problems. 3.1. Implementation, probes and representation We have implemented the selected 30 algorithms in an id- iomatic way, which aligns as closely as possible to the origi- nal pseudocode from Cormen et al. (2009). This allows us to automatically generate input/output pairs for all of them, enabling full control over the input data distribution, so long as it conforms to the preconditions of the algorithm. Further, we capture the intermediate algorithm trajectory in the form of “hints” (detailed in section 3.2), which allow insight into the inner workings of the algorithm. Such trajectories have already been extensively used in related work (Veliˇckovi´c et al., 2019; 2020; Georgiev & Li´o, 2020; Deac et al., 2020) and are typically crucial for OOD generalisation. In the most generic sense, algorithms can be seen as ma- nipulating sets of objects, along with any relations between them (which can themselves be decomposed into binary relations). If the sets are (partially) ordered (e.g. arrays or rooted trees), this can be imposed by including predecessor links. Therefore, algorithms generally operate over graphs. Motivated by existing theoretical results showing that graph neural networks align well with dynamic programming-style computations (Xu et al., 2019; Dudzik & Veliˇckovi´c, 2022), we propose a graph-oriented way to encode the data. Generally, our data is represented as a set of n vertices6, where n is a hyperparameter that is provided as part of the dataset generation process. When the semantics of these nodes are not immediately clear from the task (e.g. graph algorithms naturally operate over a graph of n nodes), we make an appropriate modiﬁcation to derive nodes. For example, in sorting algorithms, we treat every input list element as a separate node, and in string matching, we treat each character of the two input strings as a separate node. All information over these graphs falls under the following categorisation: Stage: Every feature, i.e. observation in the trajectory, is either part of the input, output, or the hints. As we do not cover algorithms that perform on-line querying, for all 30 algorithms there will be exactly one snapshot of the input and output values, whereas hints will be a time-series of intermediate algorithm states. Location: Every feature is either present within the nodes, edges (pairs of nodes) or the graph7. 6Edges are only present to represent the predecessor vertex if the input is a partially ordered. 7This also determines shapes of each feature, e.g. node features The CLRS Algorithmic Reasoning Benchmark Type: Every feature can be of ﬁve possible types, which can determine the appropriate method for encoding/decoding it, and the appropriate loss function to use when learning to predict it: • scalar: Floating-point scalar8 feature. This would typically be ﬁt using mean-squared error. • categorical: Categorical feature over K possi- ble classes. The type corresponds typically to cross- entropy loss over the classes. • mask: Categorical feature over two classes. This can be ﬁt using binary cross-entropy. • mask one: Categorical feature over two classes, where exactly one node is active (“one-hot”). One would generally optimise this argmax operation using categorical cross-entropy. • pointer: Categorical feature over the n nodes. To predict “similarity” score against every node, and typically optimised using categorical cross en- tropy (as introduced in Pointer Graph Networks (PGN) (Veliˇckovi´c et al., 2020)). Specifying a feature’s stage, location and type fully deter- mines its role in the dataﬂow. A tuple (stage, loc, type, values) is referred to as a probe. Each of the 30 algorithms has a static (w.r.t. stage, location and type) set of probes, which are considered to be a spec for the algorithm. We will later describe how these specs may be used to construct baseline architectures for the benchmark. Every node is always endowed with a position scalar input probe, which uniquely indexes it—the values are linearly spaced between 0 and 1 along the node index. This allows not only representing the data sequentially (when this is appropriate), but also serves as a useful tie-breaker when algorithms could make an arbitrary choice on which node to explore next—we force the algorithms to favour nodes with smaller position values. To illustrate these concepts further, at the end of this section we will describe the probes in detail for a popular algorithm (insertion sort). Note that, while we format the data in a way that clearly favours graph neural network executors, it can be easily adapted for different types of neural architectures; for exam- ple, sequence to sequence models (Sutskever et al., 2014). are of shape n × f ; edge features are of shape n × n × f ; graph features are of shape f , where f is the dimension of this feature (excluding batch axis). 8Given our current restriction on numerical predictions, scalar types will never be given in the output stage. Overall, CLRS-30 requires 1h to generate, and occupies 4.5GB when uncompressed, across all 30 tasks. ∼ ∼ 3.2. Hints Hints are an important component of our benchmark, which we ﬁnd fundamental in order to make progress on algorith- mic reasoning. As we previously argued, the advantage of algorithms as a task is our understanding of their behaviour, and our ability to decompose them into useful subroutines that can be shared or repeatedly applied. While, implicitly, we hope that such a decomposition would happen in any learned system, even when trained just using inputs and outputs (as studied in Xu et al. (2019)), the degree to which we can measure or encourage this is limited in the typical end-to-end learning process, and often most of the generalisation happens only in-distribution (as observed by Veliˇckovi´c et al. (2019); Xu et al. (2020); Bevilacqua et al. (2021)). The underlying algorithm may not be statistically identiﬁable from a small set of input/output pairs. Conversely, a perfect decomposition of a task into small subtasks can be generated for algorithmic problems. Then, individual models for each subtask may be trained and re- composed into a solution. Such an approach will, by con- struction, provide strong decompositional beneﬁts: as stud- ied by Yan et al. (2020), perfect OOD generalisation can be observed with such models, and they can even gener- alise zero-shot to test algorithms that reuse their modules. However, the downstream applicability of this is potentially limited; when faced with a novel task which cannot be easily decomposed into subtasks, it can be hard to decide how to reuse the learnt modules. We believe hints to lie in-between these two approaches. On one hand, they represent intermediate targets which the net- work should be able to predict if it performs reasoning simi- lar9 to the ground truth algorithm it is supposed to mimic. Indeed, several lines of recent work (Veliˇckovi´c et al., 2019; Georgiev & Li´o, 2020; Veliˇckovi´c et al., 2020; Deac et al., 2020) make favourable conclusions about using them, when it comes to achieving stronger OOD generalisation. Further- more, models leveraging hints are still end-to-end models; when faced with a novel task at test-time, we don’t need explicit knowledge of that task’s hints in order to re-use the weights learnt on a task which had them. Algorithms specify one way of attacking a problem, that is explicitly detailed through the hints. In this sense, insertion sort (to be presented shortly) is one way of implementing 9Note that architectures supervised in this way usually don’t model the hints perfectly, and will deviate from the target algorithm in subtle ways—Veliˇckovi´c et al. (2020) perform a qualitative study which shows GPU-specialised data structures could emerge as a result of such setups. The CLRS Algorithmic Reasoning Benchmark Figure 2. A sequence of hints for insertion sorting a list [5, 2, 4, 3, 1]. Green pointers correspond to the predecessor pointers (specifying the list’s state throughout the algorithm’s execution. Note how the head of the list always points to itself, by convention. Further, note how, at every step, the list is rewired such that the node selected by the blue pointer (slot) will point to the current iterator (pointed in red). a sorting function: all sorting algorithms model sorting functions, and will hence have identical outputs for identical inputs. The aspects that set the different sorting algorithms apart are exposed through their hints. Being mindful of the fact that neural networks commonly run on parallelisable architectures, we have made efforts to “compress” the hints as much as possible. For example, if a single for loop is used to sweep the data and detect the node which optimises a certain quantity (without doing any order-sensitive computations), that for loop can typi- cally be entirely “skipped” when recording hints: as parallel architectures may typically examine all the nodes at once. Further, we make every effort possible that the hint at step t + 1 will be predictable from the hints at step t by using only a single step of message passing. 3.3. Worked example: insertion sort To illustrate all of the concepts outlined above, we observe the trajectories extracted by our data collection procedure on an example: insertion sorting the array [5, 2, 4, 3, 1]. Insertion sort uses one pointer (j) to scan through the array, and then another pointer (i) to slot the j-th item into the correct place within [0..j]. This ascertains the invariant that, after k steps, the subarray of the ﬁrst k elements is com- pletely sorted. Hence the trajectory (with i and j marked) [2i, 5j, 4, 3, 1] is: [5i,j, 2, 4, 3, 1] → → [2,3i, 4, 5j, 1] [1i, 2, 3, 4, 5j]. Here, at each step, j scans along the array, and i indicates the correct place for the element that was j-th at the start of each iteration. [2, 4i, 5j, 3, 1] → → Converting this trajectory into a graph representation re- quires some considerations. Requiring the model to perform explicit swapping of node values would, ultimately, require numerical predictions. To avoid it, we ask the model to predict the predecessor pointer of each node (by conven- tion, the head of the array points to itself). Hence the actual recorded trajectory can be realised as depicted in Figure 2. In this ﬁgure, green pointers correspond to the predecessor pointers, red ones point to j, and blue ones point to i. i and j are realised as type mask one, whereas predecessors are of type pointer—and all three are stored in the nodes. The red and blue pointers represent the “hints” for this task. Finally, note that the original insertion sort pseudocode mandates that, at each iteration, i starts at position j and shifts backward until the right position is found. However, this procedure can be performed in one step by a GNN, as it can locate the correct position by examining all relevant positions, and we can omit all of those intermediate steps. In order to further illustrate how these hints are collected, we also provide an informal pseudocode for collecting hints for insertion sort in Algorithm 1: Algorithm 1 Hint updates for Insertion Sort Input :Input array val, Positions pos Hints :Predecessors pred, Iterator iter, swap slot slot i = 0 1 i > 0 ; // Initialise list pred[i] (cid:40) 0 i ← − 0, iter slot ← ← while iter < n do iter iter + 1 0 ← max node argmax j : pos[j]<pos[iter] ← val[j] if val[max node] < val[iter] then max node slot ← (cid:40) slot pred[i] i = iter otherwise pred[i] ← else slot argmin j : pos[j]<pos[iter],val[j]≥val[iter] ← val[j] pred[i] ←    iter iter pred[slot] max node pred[i] i = slot i=iter∧pred[slot]=slot i=iter∧pred[slot](cid:54)=slot pred[i] = iter otherwise end end return pred ; // Return final list In the interest of illustrating the hint structures further, we provide worked examples of trajectories for three more al- 5 2 4 3 1 5 2 4 3 1 5 2 4 3 1 5 2 4 3 1 5 2 4 3 1 The CLRS Algorithmic Reasoning Benchmark gorithms (dynamic programming, path-ﬁnding and string matching) in Appendix B. It should be remarked that we directly expose all of the hint collection routines as Python code inside the CLRS library, allowing for direct inspection. 4. Empirical evaluation Having surveyed the speciﬁcs of CLRS-30, we now present experimental results on it for several proposed algorithmic reasoning models. We primarily investigate whether a natu- ral ladder of model performance will emerge when extrapo- lating to larger inputs. Beyond this, we believe the bench- mark will be useful for empirically examining many other properties of algorithmic models, such as evaluating gener- alisation across different graph types, task types, or various multi-task (Xhonneux et al., 2021) or continual learning setups. We make available complete implementations of our data generating, probing and model training subroutines, which should make evaluating on such settings simple to deploy10. We survey several key ways of interacting with the benchmark (e.g. implementing baselines, modifying datasets, adding new algorithms) in Appendix A. 4.1. Baseline models Encode-process-decode For our experimental validation, we adopt the encode-process-decode paradigm of Hamrick et al. (2018), which is a common direction for several hint- based architectures (Veliˇckovi´c et al., 2019; Georgiev & Li´o, 2020; Veliˇckovi´c et al., 2020; Deac et al., 2020). Namely, we consider a setup with inputs xi in nodes, eij in edges, and g in the graph. We ﬁrst encode each of these using linear layers fn, fe, fg, to obtain encodings hi = fn(xi) hij = fe(eij) hg = fg(g) (1) We then feed these latents through a processor network to perform one step of computation. As we are focusing on graph representation learning in the current data format, most of our processors will be realised as graph neural net- works (Gilmer et al., 2017). Most generally, along every edge (i, j), a message from node i to node j, mij is com- puted (using a message function fm), and these messages are then aggregated across all neighbouring nodes using a permutation-invariant aggregation function, (cid:76). Finally, a readout network fr transforms these aggregated messages and the node encodings into processed node encodings: mij = fm(hi, hj, hij, hg) mi = (cid:77) i∈Nj mji h(cid:48) i = fr(hi, mi) (2) (3) Once node encodings are updated, we can decode them to make various predictions for this step of reasoning, depend- ing on the type of the prediction required (using relevant decoder functions g·), as prescribed in Section 3.1. Further, we keep track of previous-step node encodings h(t−1) , to explicitly use in a recurrent cell update (exactly as done by Veliˇckovi´c et al. (2019)). We opt to provide this recurrent update in order to provide long-range capacity to the model. i Lastly, we need to decide in what capacity will hints be used. We provide results for the option where hints are both decoded (used for computing the loss function) and encoded (considered as part of x, eij and g). At testing time, the encoded hint is equal to the hints decoded by the previous step, whereas we can stabilise these trajectories at training time by performing noisy teacher forcing—inspired by Noisy Nodes (Godwin et al., 2021), at each step we feed back ground-truth hints with probability 0.5. The quantity of hints is still used to determine the number of processor steps to perform at evaluation time. This requirement of knowing the hint-size can be lifted by, e.g., using termina- tion networks (Veliˇckovi´c et al., 2019; Banino et al., 2021) or aligning to iterative algorithms (Tang et al., 2020). Processor networks The only remaining component to specify is the processor network used by our models. As this component carries the most computational load, it is also the most obvious module to sweep over. We provide all implementations and hyperparameters within our codebase. Unless otherwise speciﬁed, we assume fully-connected graphs, i.e. , hence every node is con- } nected to every other node. We consider the following baseline processor networks: 1, 2, . . . , n { i = N Deep Sets (Zaheer et al., 2017); where each node is only (i.e., choice of (cid:76) is irrele- i connected to itself: } { vant). Such a model is popular for summary statistic tasks. i = N Graph Attention Networks (Veliˇckovi´c et al., 2017), where the aggregation function (cid:76) is self-attention (Vaswani et al., 2017), and the message function fm merely extracts the sender features: fm(hi, hj, hij, hg) = Whi. We report the best performance across GAT (Veliˇckovi´c et al., 2017) and GATv2 (Brody et al., 2021) attention mechanisms. Message-passing Neural Networks (Gilmer et al., 2017), which correspond exactly to the formulation in Equation 2, with (cid:76) = max, as prescribed by previous work (Veliˇckovi´c et al., 2019). As a sanity check, we also attempted (cid:76) = (cid:80) ﬁnding it underperformed on all tasks compared to max. Pointer Graph Networks (Veliˇckovi´c et al., 2020), which use only graph neighbourhoods i speciﬁed by a union of all node pointer and edge mask hints, and (cid:76) = max. This restricts the model to only reason over the edges deemed important by the inputs and hints. N 10https://github.com/deepmind/clrs Memory Networks (Sukhbaatar et al., 2015) have been The CLRS Algorithmic Reasoning Benchmark Figure 3. Validation results on eight representative algorithms in CLRS-30 (activity selector, Bellman-Ford, binary search, ﬁnd maximum subarray, Graham scan, insertion sort, matrix chain order, na¨ıve string matcher), averaged over three seeds. In all cases the y-axis is between [0, 100]%. Legend: MPNN red, PGN purple, Deep Sets blue, GAT orange, Memory Networks green. Validation results for all 30 individual algorithms can be found in Appendix D. used in the past as baseline for investigating reasoning in neural networks (e.g. Banino et al., 2020), as they provide an alternative way to use structural dependencies in a graph by treating edges as memories and nodes as queries. Here we used latents representing node features hi as queries and latents representing edge features hij (where there is a connecting edge and 0 otherwise) as memory inputs. 4.2. Dataset statistics For each algorithm in CLRS-30, we provide a canonical set of training, validation and test trajectories for benchmarking in- and out-of-distribution generalisation. We obtain these trajectories by running the algorithms on randomly sampled inputs that conform to their input speciﬁcation. This implies, e.g., that the inputs to most graph algorithms are Erd˝os- R´enyi graphs (Erd¨os & R´enyi, 2011) with a certain edge probability. All scalar inputs are sampled from U (0, 1). For validation, our aim is to measure in-distribution gener- alisation. Hence we sample inputs of 16 nodes for both, and generate 1,000 trajectories for training and 32 for validation. For testing, we measure out-of-distribution generalisation, and sample 32 trajectories for inputs of 64 nodes. For algo- rithms where the output is on the graph stage (rather than node/edge), we generate 64 more trajectories, in order to equalise the number of targets across tasks. × We optimise our models on the training trajectories in a teacher-forced fashion, with a batch size of 32, using the Adam optimiser (Kingma & Ba, 2014) with an initial learn- ing rate of η = 0.001. We train for 10, 000 steps, early stop- ping on the validation performance. Our models are trained on one V100 Volta GPU, requiring roughly between 1h and 30h to train, depending on the algorithm’s time complexity. For example, linear-time algorithms have signiﬁcantly fewer hints—hence message passing steps—than cubic-time ones. 4.3. Validation (in-distribution) performance We provide the in-distribution performance throughout train- ing in Figure 3, for eight representative tasks in CLRS-30 (one per each algorithm type); see Appendix D for the full results on all 30 algorithms. In this regime, the MPNN appears to dominate for most tasks: achieving over 90% F1 score for nearly all of them. While this might seem like strong evidence in favour of the fully-connected MPNNs, their added degrees of free- dom may also make MPNNs more prone to overﬁtting to speciﬁcs of the input (e.g. the input graphs’ sizes), rather than truly learning the underlying reasoning rule. We present the out-of-distribution results next, in order to make this distinction clear. The CLRS Algorithmic Reasoning Benchmark Table 1. Average test micro-F1 score of all models on all algorithm classes. The full test results for all 30 algorithms, along with a breakdown of the “win/tie/loss” metric, are given in Appendix C. Algorithm Deep Sets GAT Memnet MPNN PGN Divide & Conquer Dynamic Prog. Geometry Graphs Greedy Search Sorting Strings Overall average Win/Tie/Loss counts 0.67 7.79 6.60 8.09 6.81 18.29 7.19 0.68 12.48% 66.05% 64.08% 37.65% 75.47% 43.79% 39.60% 2.64% ± ± ± ± ± ± ± ± 42.72% 0/3/27 0.74 5.33 11.18 8.66 4.59 19.81 4.64 1.08 24.43% 67.19% 73.27% 46.80% 78.96% 37.35% 14.35% 3.02% ± ± ± ± ± ± ± ± 43.17% 1/5/24 0.00 7.75 11.65 5.20 20.73 21.67 1.09 0.21 13.05% 67.94% 45.14% 24.12% 53.42% 34.35% 71.53% 1.51% ± ± ± ± ± ± ± ± 38.88% 4/2/24 20.30% 65.10% 73.11% 62.79% 82.39% 41.20% 11.83% 3.21% 0.85 6.44 17.19 8.75 3.01 19.87 2.78 0.94 ± ± ± ± ± ± ± ± 44.99% 8/3/19 4.44 6.48 7.01 8.42 6.59 21.56 8.46 0.20 65.23% 70.58% 61.19% 60.25% 75.84% 56.11% 15.45% 2.04% ± ± ± ± ± ± ± ± 50.84% 8/6/16 4.4. Test (out-of-distribution) performance 5. Conclusion The averaged out-of-distribution performance (using the early-stopped model on validation) across each of the eight algorithm types is provided in Table 1; see Appendix C for the full results on all 30 algorithms. MPNNs are unable to transfer their impressive gains to graphs that are four times larger: in fact, the PGN takes over as the most performant model when averaged across task types—this aligns well with prior research (Veliˇckovi´c et al., 2020). The outperfor- mance is also observed when we count how frequently each model is among the best-performing models for a given algorithm, as per our “win/tie/loss” metric, which we ex- plain in Appendix C. GNN models, additionally, outperform models like Deep Sets and Memory Nets, reinforcing that GNNs are a useful primitive for algorithmic reasoning (Xu et al., 2019; Dudzik & Veliˇckovi´c, 2022). Aside from all of the above, we note that the OOD version of the CLRS-30 benchmark is highly challenging and far from solved for most tasks, making it a meaningful informant of future progress in the area. In particular, PGNs struggled on tasks requiring long-range rollouts (such as DFS), or recursive reasoning (such as Quicksort and Quickselect). This invites further research in algorithmic reasoners that can support such computation. It is further revealed that more specialised inductive biases and training regimes may be required to deal with string matching algorithms (such as KMP), and that the processor studied here tended to perform the best on tasks which were of favourable (sublinear) com- plexity in terms of hint counts (such as BFS, Bellman-Ford, and task scheduling). The speciﬁc results we obtain with our baselines validate several bits of prior research in the area, but also demon- strate we still have a long way to go, with even simple OOD scenarios only being ﬁt to about 50% micro-F1 performance. We introduce CLRS-30, a dataset that contains trajectories from 30 classical algorithms. This benchmark constitutes an effective way to test out-of-distribution generalization and transfer, and brings a means to evaluate algorithmic reasoning learnt by neural network models. The dataset provides input/output pairs for all algorithms, as well as intermediate trajectory information (“hints”). It is our hope that CLRS-30 will be a useful tool to shepherd future research in algorithmic reasoning, as prior art in the area largely generated their own datasets, making progress tracking challenging. Further, we hope that CLRS-30 will make algorithmic reasoning a more accessible area: one does not need a background in theoretical computer science to generate the dataset, and can focus on the modelling. If we convinced you to try out our library, please consult Appendix A for detailed instructions on most common ways to interact with our platform. CLRS is in constant develop- ment, and we welcome any and all feedback. Acknowledgements CLRS-30 was developed over a long time-frame, with many useful contributions, which we kindly acknowledge here. We would like to particularly thank Borja Ibarz for nu- merous ﬁxes and additions, and laying foundation for fu- ture iterations. Additionally, we warmly thank Jonathan Godwin, Sadegh Mahdavi, Euan Ong, MohamedElfatih Salah, Ahmed Elhag, Andreea Deac, Frederik Nijweide, Andrew Dudzik, Thomas Kipf, Amin Barekatain and Do- brik Georgiev for their support, and identifying numerous bugs during development. Finally, we thank Kim Stachen- feld, Nate Kushman and Daan Wierstra for reviewing the paper prior to submission, and anonymous reviewers for their careful feedback, strengthening the paper signiﬁcantly. The CLRS Algorithmic Reasoning Benchmark References Aho, A. V., Hopcroft, J. E., and Ullman, J. D. The design and analysis of computer algorithms. Reading, 1974. Alet, F., Lozano-Perez, T., and Kaelbling, L. P. Modular meta-learning. volume 87 of Proceedings of Machine Learning Research. PMLR, 2018. Banino, A., Badia, A. P., K¨oster, R., Chadwick, M. J., Zambaldi, V., Hassabis, D., Barry, C., Botvinick, M., Kumaran, D., and Blundell, C. Memo: A deep net- work for ﬂexible combination of episodic memories. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=rJxlc0EtDr. Banino, A., Balaguer, J., and Blundell, C. Pondernet: Learn- ing to ponder. arXiv preprint arXiv:2107.05407, 2021. Cormen, T. H., Leiserson, C. E., Rivest, R. L., and Stein, C. Introduction to algorithms. MIT press, 2009. Corso, G., Cavalleri, L., Beaini, D., Li`o, P., and Veliˇckovi´c, P. Principal neighbourhood aggregation for graph nets. arXiv preprint arXiv:2004.05718, 2020. Deac, A., Bacon, P.-L., and Tang, J. Graph neural induc- tion of value iteration. arXiv preprint arXiv:2009.12604, 2020. Deac, A.-I., Veliˇckovi´c, P., Milinkovic, O., Bacon, P.-L., Tang, J., and Nikolic, M. Neural algorithmic reasoners are implicit planners. Advances in Neural Information Processing Systems, 34, 2021. Dijkstra, E. W. et al. A note on two problems in connex- ion with graphs. Numerische mathematik, 1(1):269–271, 1959. Belkin, M., Hsu, D., and Xu, J. Two models of double de- scent for weak features. arXiv preprint arXiv:1903.07571, 2019. Du, Y., Czarnecki, W. M., Jayakumar, S. M., Pascanu, R., and Lakshminarayanan, B. Adapting auxiliary losses using gradient similarity, 2018. Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An evaluation plat- form for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, 2013. Bellman, R. On a routing problem. Quarterly of applied mathematics, 16(1):87–90, 1958. Bengio, Y., Lodi, A., and Prouvost, A. Machine learning for combinatorial optimization: a methodological tour d’horizon. European Journal of Operational Research, 2020. Bentley, J. Programming pearls: algorithm design tech- niques. Communications of the ACM, 27(9):865–873, 1984. Bevilacqua, B., Zhou, Y., and Ribeiro, B. Size-invariant graph representations for graph classiﬁcation extrapola- tions. In International Conference on Machine Learning, pp. 837–851. PMLR, 2021. Brody, S., Alon, U., and Yahav, E. How attentive are graph attention networks? arXiv preprint arXiv:2105.14491, 2021. Cappart, Q., Ch´etelat, D., Khalil, E., Lodi, A., Morris, C., and Veliˇckovi´c, P. Combinatorial optimization and reasoning with graph neural networks. arXiv preprint arXiv:2102.09544, 2021. Dudzik, A. and Veliˇckovi´c, P. Graph neural networks are dynamic programmers. arXiv preprint arXiv:2203.15544, 2022. Dwivedi, V. P., Joshi, C. K., Laurent, T., Bengio, Y., and Bresson, X. Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020. Erd¨os, P. and R´enyi, A. On the evolution of random graphs. In The structure and dynamics of networks, pp. 38–82. Princeton University Press, 2011. Floyd, R. W. Algorithm 97: shortest path. Communications of the ACM, 5(6):345, 1962. Ford Jr, L. R. and Fulkerson, D. R. Flows in networks. Princeton university press, 2015. Freivalds, K., Ozolin¸ ˇs, E., and ˇSostaks, A. Neural shufﬂe- exchange networks-sequence processing in o (n log n) In Advances in Neural Information Processing time. Systems, pp. 6630–6641, 2019. Gavril, F. Algorithms for minimum coloring, maximum clique, minimum covering by cliques, and maximum independent set of a chordal graph. SIAM Journal on Computing, 1(2):180–187, 1972. Georgiev, D. and Li´o, P. Neural bipartite matching. arXiv preprint arXiv:2005.11304, 2020. Chen, Z., Chen, L., Villar, S., and Bruna, J. Can graph arXiv preprint neural networks count substructures? arXiv:2002.04025, 2020. Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. Neural message passing for quantum chem- istry. arXiv preprint arXiv:1704.01212, 2017. The CLRS Algorithmic Reasoning Benchmark Godwin, J., Schaarschmidt, M., Gaunt, A. L., Sanchez- Gonzalez, A., Rubanova, Y., Veliˇckovi´c, P., Kirkpatrick, J., and Battaglia, P. Simple gnn regularisation for 3d molecular property prediction and beyond. In Interna- tional Conference on Learning Representations, 2021. Graham, R. L. An efﬁcient algorithm for determining the Info. Pro. Lett., 1: convex hull of a ﬁnite planar set. 132–133, 1972. Graves, A., Wayne, G., and Danihelka, I. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014. Grifﬁths, T., Callaway, F., Chang, M., Grant, E., Krueger, P., and Lieder, F. Doing more with less: meta-reasoning and meta-learning in humans and machines. Current Opinion in Behavioral Sciences, October 2019. Hamrick, J. B., Allen, K. R., Bapst, V., Zhu, T., McKee, K. R., Tenenbaum, J. B., and Battaglia, P. W. Relational inductive bias for physical construction in humans and machines. arXiv preprint arXiv:1806.01203, 2018. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Knuth, D. E. Fundamental algorithms. 1973. Knuth, D. E., Morris, Jr, J. H., and Pratt, V. R. Fast pattern matching in strings. SIAM journal on computing, 6(2): 323–350, 1977. Kool, W., van Hoof, H., and Welling, M. Attention, arXiv preprint learn to solve routing problems! arXiv:1803.08475, 2018. Kruskal, J. B. On the shortest spanning subtree of a graph and the traveling salesman problem. Proceedings of the American Mathematical society, 7(1):48–50, 1956. Lake, B. M. Compositional generalization through meta sequence-to-sequence learning. In Advances in Neural In- formation Processing Systems 32, pp. 9791–9801. 2019. Lawler, E. L. The traveling salesman problem: a guided tour of combinatorial optimization. Wiley-Interscience Series in Discrete Mathematics, 1985. Hennigan, T., Cai, T., Norman, T., and Babuschkin, I. Haiku: Sonnet for JAX, 2020. URL http://github.com/ deepmind/dm-haiku. Li, Y., Gimeno, F., Kohli, P., and Vinyals, O. Strong general- ization and efﬁciency in neural programs. arXiv preprint arXiv:2007.03629, 2020. Hierholzer, C. and Wiener, C. ¨Uber die m¨oglichkeit, einen linienzug ohne wiederholung und ohne unterbrechung zu umfahren. Mathematische Annalen, 6(1):30–32, 1873. Hoare, C. A. Algorithm 65: ﬁnd. Communications of the ACM, 4(7):321–322, 1961. Hoare, C. A. Quicksort. The Computer Journal, 5(1):10–16, 1962. Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems 31. 2018. Jarvis, R. A. On the identiﬁcation of the convex hull of a ﬁnite set of points in the plane. Information processing letters, 2(1):18–21, 1973. Joshi, C. K., Cappart, Q., Rousseau, L.-M., Laurent, T., and Bresson, X. Learning tsp requires rethinking generaliza- tion. arXiv preprint arXiv:2006.07054, 2020. Kaiser, Ł. and Sutskever, I. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228, 2015. Khalil, E., Dai, H., Zhang, Y., Dilkina, B., and Song, L. Learning combinatorial optimization algorithms over graphs. In Advances in Neural Information Processing Systems, pp. 6348–6358, 2017. Liu, C., Zhu, L., and Belkin, M. Toward a theory of optimization for over-parameterized systems of non- linear equations: the lessons of deep learning. CoRR, abs/2003.00307, 2020. Merity, S., Xiong, C., Bradbury, J., and Socher, R. arXiv preprint Pointer sentinel mixture models. arXiv:1609.07843, 2016. Mikolov, T., Deoras, A., Kombrink, S., Burget, L., and Cernock´y, J. Empirical evaluation and combination of In INTER- advanced language modeling techniques. SPEECH, pp. 605–608, 2011. Moore, E. F. The shortest path through a maze. In Proc. Int. Symp. Switching Theory, 1959, pp. 285–292, 1959. Prim, R. C. Shortest connection networks and some gen- eralizations. The Bell System Technical Journal, 36(6): 1389–1401, 1957. Richter, O. and Wattenhofer, R. Normalized attention with- out probability cage. arXiv preprint arXiv:2005.09561, 2020. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y. The CLRS Algorithmic Reasoning Benchmark Strathmann, H., Barekatain, M., Blundell, C., and Veliˇckovi´c, P. Persistent message passing. arXiv preprint arXiv:2103.01043, 2021. Xu, K., Li, J., Zhang, M., Du, S. S., Kawarabayashi, K.-i., and Jegelka, S. What can neural networks reason about? arXiv preprint arXiv:1905.13211, 2019. Sukhbaatar, S., Szlam, A., Weston, J., and Fergus, R. End-to- end memory networks. arXiv preprint arXiv:1503.08895, 2015. Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to se- quence learning with neural networks. In Advances in neural information processing systems, pp. 3104–3112, 2014. Tang, H., Huang, Z., Gu, J., Lu, B., and Su, H. Towards scale-invariant graph-related problem solving by itera- tive homogeneous gnns. the 34th Annual Conference on Neural Information Processing Systems (NeurIPS), 2020. Xu, K., Li, J., Zhang, M., Du, S. S., ichi Kawarabayashi, K., and Jegelka, S. How neural networks extrapolate: From feedforward to graph neural networks. arXiv preprint arXiv:2009.11848, 2020. Yan, Y., Swersky, K., Koutra, D., Ranganathan, P., and Heshemi, M. Neural execution engines: Learning to execute subroutines. arXiv preprint arXiv:2006.08084, 2020. Yehuda, G., Gabel, M., and Schuster, A. It’s not what machines can learn, it’s what we cannot teach. arXiv preprint arXiv:2002.09398, 2020. Trask, A., Hill, F., Reed, S. E., Rae, J., Dyer, C., and Blun- som, P. Neural arithmetic logic units. In Advances in Neural Information Processing Systems, pp. 8035–8044, 2018. Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R., and Smola, A. J. Deep sets. In Advances in neural information processing systems, pp. 3391–3401, 2017. Zamir, A. R., Sax, A., Shen, W., Guibas, L. J., Malik, J., and Savarese, S. Taskonomy: Disentangling task trans- fer learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3712–3722, 2018. Zaremba, W. and Sutskever, I. Learning to execute. arXiv preprint arXiv:1410.4615, 2014. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten- tion is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017. Veliˇckovi´c, P. and Blundell, C. Neural algorithmic reasoning. arXiv preprint arXiv:2105.02761, 2021. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. Veliˇckovi´c, P., Ying, R., Padovano, M., Hadsell, R., and Blundell, C. Neural execution of graph algorithms. arXiv preprint arXiv:1910.10593, 2019. Veliˇckovi´c, P., Buesing, L., Overlan, M. C., Pascanu, R., Vinyals, O., and Blundell, C. Pointer graph networks. arXiv preprint arXiv:2006.06380, 2020. Veliˇckovi´c, P., Boˇsnjak, M., Kipf, T., Lerchner, A., Hadsell, R., Pascanu, R., and Blundell, C. Reasoning-modulated representations. arXiv preprint arXiv:2107.08881, 2021. Vinyals, O., Fortunato, M., and Jaitly, N. Pointer networks. In Advances in Neural Information Processing Systems, pp. 2692–2700, 2015. Williams, J. W. J. Algorithm 232: heapsort. Commun. ACM, 7:347–348, 1964. Xhonneux, L.-P., Deac, A.-I., Veliˇckovi´c, P., and Tang, J. How to transfer algorithmic reasoning knowledge to learn new algorithms? Advances in Neural Information Pro- cessing Systems, 34, 2021. A. Interfacing with the CLRS benchmark The CLRS Algorithmic Reasoning Benchmark The CLRS benchmark is publicly hosted on GitHub: https://github.com/deepmind/clrs. All code and artifacts are released under an Apache 2.0 license, which is highly permissive. Within clrs/examples/run.py, we demonstrate an extensively conﬁgurable example script that evaluates a speciﬁc baseline on CLRS-30. Our baselines are provided in JAX and Haiku (Hennigan et al., 2020), but the dataset is generated using NumPy, making it possible to create learning pipelines in virtually any framework, including PyTorch and TensorFlow. We will now highlight three key ways in which researchers can interface with the library. A.1. Evaluating a new baseline on CLRS-30 To support a new baseline, the recommended path depends on how fundamentally different the baseline is to an encode- process-decode GNN. In most cases, we anticipate that only the processor network needs changing, and the remainder of the architec- ture can match our baselines. In this case, it is only necessary to implement the new processor network within clrs/ src/processors.py and appropriately set self.mpnn within the construct processor method in clrs/ src/baselines.py. For more fundamentally different baselines, it is necessary to create a new class that extends the Model API (as found within clrs/ src/model.py). clrs/ src/baselines.py provides one example of how this can be done efﬁciently, for the case of our baselines. A.2. Modifying the data distribution of CLRS-30 If users want to train and/or evaluate the models on different versions of the tasks given in CLRS-30, the key routines to modify are located in clrs/ src/samplers.py. The easiest modiﬁcation concerns the graph sizes and/or numbers of trajectories. They can be directly changed by modifying the CLRS30 dictionary near the top of the ﬁle. For more elaborate modiﬁcations, e.g. to the speciﬁc data sampling distributions, the users would need to modify and/or extend the relevant sampler class. As a guiding example, we provide a SortingSampler class which is convenient for generating inputs for sorting algorithms. The speciﬁc sampler used for each task is provided in the SAMPLERS dictionary towards the end of the ﬁle. A.3. Adding new algorithms to CLRS As the most elaborate of the three workﬂows, adding a new algorithm to the task suite requires following several steps, which are potentially comprehensive, depending on the complexity of the algorithm. However, the CLRS benchmark code still provides may helper routines for probing and batching that facilitate inclusion of novel algorithms. The steps are as follows: 1. First, determine the input/hint/output speciﬁcation of your algorithm, and include it within the SPECS dictionary of clrs/ src/specs.py. 2. Implement the desired algorithm in an abstractiﬁed form. Examples of this can be found throughout the clrs/ src/algorithms/ folder. 3. Next, choose appropriate moments within the algorithm’s execution to create probes that capture the inputs, outputs and all intermediate state (using the probing.push function). 4. Once generated, probes can be prepared using the probing.finalize method, and should be returned together with the algorithm output. 5. Lastly, implement an appropriate input data sampler for your algorithm, and include it within the SAMPLERS dictionary within clrs/ src/samplers.py. B. Additional worked examples of algorithm trajectories The CLRS Algorithmic Reasoning Benchmark Matrix Chain Order As a representative dynamic programming algorithm, we visualise the steps of the procedure for optimising the order of multiplications in a chain of matrices, for multiplying matrices of size (10 60), assuming a O(n3)-time multiplication algorithm. 30)(30 5)(5 × × × The algorithm proceeds by ﬁlling up an “upper-triangular” part of a dynamic programming matrix, where cell [i, j] corresponds to the optimal number of operations when multiplying all the matrices between the ith and jth. Such an algorithm may also be represented in a “pyramidal” form as below: Additionally, the algorithm maintains (and returns) the optimal way to recursively divide each subsequence into two (by 5) (yielding 1, 500 storing the optimal dividing point, in green). Here, it is optimal to ﬁrst multiply (10 operations), then multiply the remaning matrices as (10 × 60) (yielding 3, 000 operations; 4, 500 in total). 30)(30 5)(5 × × × Note that every pointer points into one of the original n input nodes (at the lowest level), and how each cell of the pyramid corresponds to a pair of input nodes (specifying the corresponding range). Therefore, rather than creating O(n2) auxiliary nodes, we instead record all relevant values above as edge scalars and edge pointers, and store nodes only for the lowest level of the pyramid. Further, whether or not a particular edge has been populated yet (the “ ” indicator above) is stored as an additional binary ﬂag. ∞ Bellman-Ford As a representative graph algorithm, we visualise the steps of the Bellman-Ford algorithm for ﬁnding single-source shortest paths in a given graph. Initially, the source node is labelled with distance zero, and all other nodes with distance “ ” (which, once again, is represented as a binary node hint). The algorithm then iteratively relaxes all edges as follows, until convergence is achieved: ∞ Besides updating the distance values, the algorithm also maintains, and returns, the predicted shortest path tree – for each node, a pointer to its predecessor along the optimal path from the source. By convention, the source node points to itself. These pointers are visualised in green. Na¨ıve String Matcher As a representative string algorithm, we visualise the steps of the na¨ıve string matcher, for detecting string ""ab"" inside the string ""aab"". ∞ ∞ ∞ ∞ ∞ ∞ ∞ ∞ ∞ ∞ 4500 1500 9000 1500 9000 300 150 300 300 150 300 300 150 300 10 30 5 60 10 30 5 60 10 30 5 60 10 30 5 60 0 1 2 ∞ 2 2 ∞ 8 ∞ 3 ∞ 0 1 2 1 2 2 2 ∞ 8 3 ∞ 0 1 2 1 2 2 2 3 3 5 8 The CLRS Algorithmic Reasoning Benchmark In this case, each character of the two strings is given a separate node, and three sets of indices are maintained: indicating the start of the current candidate match (in blue); and the current position being checked in both the haystack (red) and the needle (purple). The algorithm scans candidate positions left-to-right until a full match is detected for the ﬁrst time. Additionally, each character is tagged with its predecessor in the string (in green), and a binary ﬂag indicating which of the two strings it belongs to (not shown here). C. Test results for all algorithms Test performance for all 30 algorithms in CLRS-30 may be found in Table 2. In addition, we provide a “win-tie-loss” metric as another way of differentiating model performance, which is less sensitive to outliers. The resulting counts are provided in Table 3, and are computed as follows: • Let µA( Table 2). M ) and σA( ) be the mean and standard deviation of model M ’s test performance on algorithm A (as in M • We say that model • If = . A ∀X (cid:54) • Otherwise, if A (cid:31) A . outperforms model B on algorithm A—denoted by A A (cid:31) B —if µA( ) A − , then model A wins on algorithm A. A X σA( ) > µA( A ). B A , then model loses on algorithm A. ∃X • Otherwise, model X (cid:31) A A is tied on algorithm A. A The win/tie/loss counts are then aggregated across all algorithms A to obtain a metric for each model. As already mentioned, the details of this on a per-algorithm level are given in Table 3. D. Validation results individual plots Validation performance for all 30 algorithms in CLRS-30 may be found in Figure 4. For convenience, we also report the early-stopped validation performance in Table 4. a a b a b a a b a b a a b a b a a b a b The CLRS Algorithmic Reasoning Benchmark Algorithm Deep Sets GAT Memnet MPNN PGN Table 2. Test performance of all models on all algorithms. Activity Selector Articulation Points Bellman-Ford BFS Binary Search Bridges Bubble Sort DAG Shortest Paths DFS Dijkstra Find Max. Subarray Floyd-Warshall Graham Scan Heapsort Insertion Sort Jarvis’ March KMP Matcher LCS Length Matrix Chain Order Minimum MST-Kruskal MST-Prim Na¨ıve String Match Optimal BST Quickselect Quicksort Segments Intersect SCC Task Scheduling Topological Sort Overall average 1.67 4.04 0.85 0.38 0.88 2.65 3.24 2.42 0.73 3.10 0.39 0.90 2.75 12.57 4.65 0.81 0.54 5.25 3.58 2.08 4.71 5.47 0.29 1.36 1.33 2.16 0.60 2.61 0.70 3.57 66.09% 39.06% 51.33% 98.63% 47.97% 32.43% 50.73% 73.21% 7.44% 36.12% 12.48% 7.22% 64.71% 28.94% 40.98% 50.25% 3.22% 50.10% 78.36% 80.19% 60.58% 12.17% 2.05% 69.71% 3.21% 37.74% 77.29% 17.81% 84.84% 15.84% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 43.36% 1.37 1.62 1.19 0.21 3.12 6.60 1.77 1.37 2.04 0.79 0.43 3.14 2.70 1.83 1.87 10.25 0.36 1.02 3.31 2.95 0.99 4.34 1.20 1.75 0.95 0.98 0.04 3.12 2.09 6.92 73.23% 37.76% 87.91% 99.04% 23.50% 25.64% 9.91% 81.14% 11.78% 58.01% 24.43% 16.66% 77.89% 10.35% 29.52% 51.51% 3.03% 57.88% 78.19% 84.20% 65.72% 38.20% 3.01% 65.49% 4.36% 7.60% 90.41% 12.70% 84.69% 27.03% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 44.69% 2.22 0.61 1.46 0.04 0.46 0.05 0.78 1.92 1.61 2.39 0.08 0.13 2.31 1.57 0.86 3.87 0.00 4.34 1.03 0.11 0.61 3.77 0.48 1.21 0.03 0.67 0.90 4.78 0.04 0.11 24.10% 1.50% 40.04% 43.34% 14.37% 30.26% 73.58% 66.15% 13.36% 22.48% 13.05% 14.17% 40.62% 68.00% 71.42% 22.99% 1.81% 49.84% 81.96% 86.93% 28.84% 10.29% 1.22% 72.03% 1.74% 73.10% 71.81% 16.32% 82.74% 2.73% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 38.03% 3.16 2.18 0.28 0.05 0.26 4.78 0.60 0.56 0.51 0.50 0.49 1.77 0.31 0.84 2.08 12.39 0.86 0.36 1.40 0.88 1.50 7.56 0.30 0.44 0.69 0.10 0.10 4.88 0.32 6.24 80.66% 50.91% 92.01% 99.89% 36.83% 72.69% 5.27% 96.24% 6.54% 91.50% 20.30% 26.74% 91.04% 10.94% 19.81% 34.86% 2.49% 53.23% 79.84% 85.34% 70.97% 69.08% 3.92% 62.23% 1.43% 11.30% 93.44% 24.37% 84.11% 52.60% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 51.02% 1.62 2.09 0.34 0.29 0.13 7.82 1.95 0.16 0.24 1.75 2.56 0.51 1.61 0.18 2.43 1.07 0.12 0.21 0.49 0.52 1.36 0.98 0.20 1.82 0.42 0.15 0.75 0.64 0.91 2.69 66.80% 49.53% 92.99% 99.63% 76.95% 51.42% 6.01% 96.94% 8.71% 83.45% 65.23% 28.76% 56.87% 5.27% 44.37% 49.19% 2.00% 56.82% 83.91% 87.71% 66.96% 63.33% 2.08% 71.01% 3.66% 6.17% 77.51% 20.80% 84.89% 60.45% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 52.31% The CLRS Algorithmic Reasoning Benchmark Figure 4. Validation results on all 30 algorithms in CLRS-30, averaged over three seeds. The CLRS Algorithmic Reasoning Benchmark Table 3. Win/Tie/Loss counts of all models on all algorithms. Legend: W: win, T: tie, L: loss. Algorithm Deep Sets GAT Memnet MPNN PGN Activity Selector Articulation Points Bellman-Ford BFS Binary Search Bridges Bubble Sort DAG Shortest Paths DFS Dijkstra Find Max. Subarray Floyd-Warshall Graham Scan Heapsort Insertion Sort Jarvis’ March KMP Matcher LCS Length Matrix Chain Order Minimum MST-Kruskal MST-Prim Na¨ıve String Match Optimal BST Quickselect Quicksort Segments Intersect SCC Task Scheduling Topological Sort L L L L L L L L L L L L L L L T T L L L L L L L L L L L T L L L L L L L L L T L L L L L L T T W L L L L L L T L L L T L L L L L L L W L T L L L L W W L L L L L L L L T L W L L L L W T L W L W L L L W L L W L L L L L L L W T W L L L W T L L L T W L W L L W L L W W L L L L L L W W L T L T T L L T T W Overall counts 0/3/27 1/5/24 4/2/24 8/3/19 8/6/16 The CLRS Algorithmic Reasoning Benchmark Algorithm Deep Sets GAT Memnet MPNN PGN Table 4. Early-stopped validation results of all models on all algorithms. Activity Selector Articulation Points Bellman-Ford BFS Binary Search Bridges Bubble Sort DAG Shortest Paths DFS Dijkstra Find Max. Subarray Floyd-Warshall Graham Scan Heapsort Insertion Sort Jarvis’ March KMP Matcher LCS Length Matrix Chain Order Minimum MST-Kruskal MST-Prim Na¨ıve String Match Optimal BST Quickselect Quicksort Segments Intersect SCC Task Scheduling Topological Sort Overall average 0.17 0.31 0.14 0.00 0.41 0.05 1.02 0.28 1.26 0.42 0.22 0.04 0.24 0.33 0.28 0.42 0.21 0.36 0.02 0.11 2.01 0.32 0.15 0.14 0.92 1.12 0.12 1.23 0.04 0.81 83.50% 99.63% 81.12% 100.00% 93.34% 99.36% 81.51% 92.25% 62.76% 80.34% 91.41% 35.79% 87.66% 81.84% 89.58% 72.82% 98.03% 69.24% 94.46% 97.59% 83.79% 74.61% 49.80% 92.02% 42.30% 79.69% 77.49% 89.52% 99.16% 47.23% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 80.93% 0.50 0.00 0.14 0.00 0.17 0.00 1.01 0.05 0.64 0.40 0.32 0.09 0.11 2.23 0.58 0.16 0.08 0.19 0.03 0.21 0.25 0.14 0.00 0.49 1.86 0.40 0.16 0.00 0.04 0.00 92.40% 100.00% 99.28% 100.00% 95.72% 100.00% 95.44% 96.81% 99.22% 99.22% 95.00% 87.28% 97.85% 87.24% 95.18% 98.38% 99.76% 77.00% 99.37% 97.74% 97.93% 98.37% 100.00% 93.30% 83.82% 92.97% 90.82% 100.00% 99.80% 100.00% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 95.66% 2.15 1.03 0.42 0.09 0.28 1.13 0.14 0.05 0.45 0.70 0.08 0.04 1.58 0.28 0.14 6.61 0.00 0.24 0.10 0.10 0.95 0.28 0.20 0.40 0.25 0.24 1.08 1.43 0.09 0.50 34.59% 16.84% 68.75% 70.70% 20.33% 96.46% 92.64% 81.90% 47.72% 67.38% 27.91% 31.29% 53.53% 54.04% 94.40% 37.92% 9.67% 67.69% 93.91% 95.56% 64.65% 74.09% 9.91% 90.86% 6.56% 93.16% 71.57% 70.57% 84.80% 8.30% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 57.92% 0.39 0.00 0.05 0.00 0.12 0.00 1.84 0.05 0.00 0.14 0.37 0.03 0.15 0.11 0.19 0.25 0.05 0.42 0.04 0.05 0.17 0.09 0.00 0.11 0.78 0.40 0.20 0.00 0.00 0.00 93.89% 100.00% 99.48% 100.00% 94.19% 100.00% 94.53% 99.93% 100.00% 99.67% 95.13% 89.14% 98.45% 94.27% 96.74% 97.94% 99.87% 77.88% 99.12% 97.64% 99.71% 99.02% 100.00% 93.88% 88.74% 95.70% 93.84% 100.00% 100.00% 100.00% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 96.63% 0.19 0.00 0.05 0.00 0.08 0.00 5.46 0.00 0.00 0.05 0.16 0.15 0.27 0.67 0.82 0.36 0.99 0.04 0.03 0.14 0.08 0.14 0.08 0.27 0.17 1.42 0.18 0.05 0.08 0.00 82.26% 100.00% 99.35% 100.00% 94.17% 100.00% 87.17% 99.80% 100.00% 99.28% 95.30% 88.70% 89.06% 90.36% 84.57% 88.34% 94.14% 69.19% 99.21% 97.07% 99.12% 97.79% 50.33% 93.20% 54.02% 54.30% 78.32% 99.93% 99.06% 100.00% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 89.47%"
97,Learning Task-relevant Representations for Generalization via   Characteristic Functions of Reward Sequence Distributions,"[{'title': 'doi', 'href': 'http://dx.doi.org/10.1145/3534678.3539391', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2205.10218v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2205.10218v2', 'rel': 'related', 'type': 'application/pdf'}]",2022-05-20 14:52:03,"2 2 0 2 y a M 6 2 ] I A . s c [ 1 v 1 2 5 3 1 . 5 0 2 2 : v i X r a Discovering Policies with DOMiNO: Diversity Optimization Maintaining Near Optimality Tom Zahavy DeepMind, London tomzahavy@deepmind.com Yannick Schroecker DeepMind, London yschroecker@deepmind.com Feryal Behbahani DeepMind, London feryal@deepmind.com Kate Baumli DeepMind, London baumli@deepmind.com Sebastian Flennerhag DeepMind, London ﬂennerhag@deepmind.com Shaobo Hou DeepMind, London shaobohou@deepmind.com Satinder Singh DeepMind, London baveja@deepmind.com Abstract Finding different solutions to the same problem is a key aspect of intelligence associated with creativity and adaptation to novel situations. In reinforcement learning, a set of diverse policies can be useful for exploration, transfer, hierarchy, and robustness. We propose DOMiNO, a method for Diversity Optimization Maintaining Near Optimality. We formalize the problem as a Constrained Markov Decision Process where the objective is to ﬁnd diverse policies, measured by the distance between the state occupancies of the policies in the set, while remaining near-optimal with respect to the extrinsic reward. We demonstrate that the method can discover diverse and meaningful behaviors in various domains, such as different locomotion patterns in the DeepMind Control Suite. We perform extensive analysis of our approach, compare it with other multi-objective baselines, demonstrate that we can control both the quality and the diversity of the set via interpretable hyperparameters, and show that the discovered set is robust to perturbations. 1 Introduction Creative problem solving is the mental process of searching for an original and previously unknown solution to a problem [38]. The relationship between creativity and intelligence is widely recognized across many ﬁelds; for example, in the ﬁeld of Mathematics, ﬁnding different proofs to the same theorem is considered elegant and often leads to new insights. Closer to Artiﬁcial Intelligence (AI), consider the ﬁeld of game playing and, speciﬁcally, the game of Chess in which a move is considered creative when it goes beyond known patterns [17]. In some cases, such moves can only be detected by human players while remaining invisible to current state-of-the-art Chess engines. A famous example thereof is the winning move in game eight of the Classical World Chess Championship 2004 between Leko and Kramnik [8, 3]. Humans and, indeed, many animals employ similarly creative behavior on a daily basis; faced with a challenging problem, we often consider qualitatively different alternative solutions. Yet, the majority of AI research is focused on ﬁnding a single best solution to a given problem. For example, in the ﬁeld of Reinforcement Learning (RL), most algorithms are designed to ﬁnd a single reward-maximizing policy. However, for many problems of interest there may be many qualitatively Preprint. Under review. different optimal or near-optimal policies; ﬁnding such diverse set of policies may help an RL agent become more robust to changes in the task and/or environment and to generalize better to future tasks. The majority of the literature on this problem has been done in the ﬁeld of Quality-Diversity (QD), which comprises of two main families of algorithms: MAP-Elites [36, 16] and novelty search with local competition [32]. QD algorithms typically maintain a collection of policies and adapt it using evolutionary algorithms to balance the QD trade-off [41, 51, 15]. Further references can be found on the QD webpage. In contrast to this line of work, we propose a differentiable optimization framework for maximizing the diversity of a set of RL policies. We do so by formulating diversity maximization as an optimization problem in state occupancies, and then showing that we can solve this problem by maximizing an intrinsic reward that corresponds to the gradient of the diversity objective. In related work, intrinsic rewards have been used for learning diversity in terms of the discriminability of different trajectory-speciﬁc quantities [24, 20, 46, 7]. Other works implicitly induce diversity to learn policies that maximize the set robustness to the worst-possible reward [31, 57], or use diversity as a regularizer when maximizing the extrinsic reward [29, 34, 40, 49, 59, 46]. Our work makes the following contributions. First, we propose DOMiNO, a method for Diversity Optimization that Maintains Nearly Optimal policies. DOMiNO trains a set of policies using a policy-speciﬁc, weighted combination of the extrinsic reward and an intrinsic diversity reward. The weights are adapted as Lagrange multipliers to guarantee that each policy is near-optimal. Second, we propose to measure diversity via expected features; i.e., the features that a policy observes in its state occupancy. Under this measure of diversity, we introduce two novel objectives for diversity optimization: a repulsive force that motivates policies to have distinct expected features and a Van Der Waals force, which combines the repulsive force with an attractive one and allows us to specify the degree of diversity in the set. Third, we perform experiments in the DeepMind Control Suite [52] and the BiPedal walker environment [13] and show that DOMiNO discovers qualitatively diverse locomotion behaviors (Fig. 1b). We analyze our approach and compare it to other multi-objective strategies for handling the QD trade-off. Lastly, we demonstrate that the discovered set is robust to perturbations of the environment and the morphology of the avatar. (a) (b) Figure 1: (a) DOMiNO’s architecture: The agent learns a set of QD policies via a single latent- conditioned actor-critic network with intrinsic and extrinsic value heads. Dashed arrows signify training objectives. (b) DOMiNO’s π: Near optimal diverse policies in walker.stand corresponding to standing on both legs, standing on either leg, lifting the other leg forward and backward, spreading the legs and stamping. Not only are the policies different from each other, they also achieve high extrinsic reward in standing (see values on top of each policy). 2 Preliminaries and Notation In this work, we will express objectives in terms of the state occupancy measure dπ. Intuitively speaking, dπ measures how often a policy π visits each state-action pair. As we will soon see, the classic RL objective of reward maximization can be expressed as a linear product between the reward vector and the state occupancy. In addition, in this work we will formulate diversity maximization via an objective that is a nonlinear function of the state occupancy. While it might seem unclear which 2 i R u n n n g A v e r a g e U p d a t e c o n s t r a i n t & L a g r a n g e MLP Torso MLP i R u n n n g A v e r a g e D i v e r s i t y R e w a r d reward should be maximized to solve such an objective, we take inspiration from Convex MDPs [58] where one such reward is the gradient of the objective with respect to dπ. We begin with some formal deﬁnitions. In RL an agent interacts with an environment and seeks to maximize its cumulative reward. We consider two cases, the average reward case and the discounted case. The Markov decision process [42, MDP] is deﬁned by the tuple (S, A, P, R) for the average reward case and by the tuple (S, A, P, R, γ, D0) for the discounted case. We assume an inﬁnite horizon, ﬁnite state-action problem. Initially, the state of the agent is sampled according to s0 ∼ D0. At time t, given state st, the agent selects action at according to its policy π(st, ·), receives a reward rt ∼ R(st, at) and transitions to a new state st+1 ∼ P (·, st, at). We consider two performance t=1γtrt, for the average reward metrics, given by vavg case and discounted case respectively. The goal of the agent is to ﬁnd a policy that maximizes vavg or vγ π. Let Pπ(st = ·) be the probability measure over states at time t under policy π, then the state occupancy measure dπ is given as davg t=1 Pπ(st = s)π(s, a), and π(s, a) = (1 − γ)E (cid:80)∞ t=1 γtPπ(st = s)π(s, a), for the average reward case and the discounted dγ case respectively. With these, we can rewrite the RL objective in as a linear function of the occupancy measure maxdπ∈K s,a r(s, a)dπ(s, a), where K is the set of admissible distributions [58]. Next, consider an objective of the form: π (s, a) = limT →∞ π = (1 − γ)E(cid:80)∞ π = limT →∞ T E (cid:80)T t=1rt, vγ T E(cid:80)T (cid:80) π 1 1 min dπ∈K f (dπ), (1) where f : K → R is a nonlinear function. Sequential decision making problems that take this form include Apprenticeship Learning (AL) and pure exploration, among others [1, 55, 26, 59, 23, 58, 45, 9, 37]. In the remainder of this section, we brieﬂy explain how to solve Eq. (1) using RL methods when the function f is convex. We begin with rewriting Eq. (1) using Fenchel duality as min dπ∈K f (dπ) = max λ∈Λ min dπ∈K (λ · dπ − f ∗(λ)) (2) where Λ is the closure of (sub-)gradient space {∂f (dπ)|dπ ∈ K}, which is compact and convex [2], and f ∗ is the Fenchel conjugate of the function f . Eq. (2) presents a zero-sum max-min game between two players, the policy player Algπ and the cost player Algλ. We can see that from the perspective of the policy player, the objective is a linear minimization problem in dπ. Thus, intuitively speaking, the goal of the policy player is to maximize the negative cost as a reward r = −λ. To solve Eq. (2), we employ the meta algorithm from [2], which uses two online learning algorithms. The policy player Algπ generates a sequence of policies {πk}k∈N by maximizing a sequence of negative costs {−λk}k∈N as rewards that are produced by the cost player Algλ. In this paper, the policy player uses an online RL algorithm and the cost player uses the Follow the Leader (FTL) algorithm. This implies that the cost at time k is given as λk = ∇f ( ¯dk−1 π ). (3) π In other words, to solve an RL problem with a convex objective function (Eq. (1)), the policy player maximizes a non stationary reward that at time k corresponds to the negative gradient of the objective function f w.r.t ¯dk−1 . When the function f is convex, it is guaranteed that the average state occupancy of these polices, ¯dK π, converges to an optimal solution to Eq. (1), i.e., ¯dK π → d(cid:63) Features and expected features. We focus on the case where each state-action pair is associated with some observable features φ(s, a) ∈ Rd. For example, in the DM control suite [52], these features correspond to the positions and velocities of the body joints being controlled by the agent. In other cases, we can learn φ with a neural network. π ∈ arg mindπ∈K f (dπ) [58]. π = 1 K k=1 dk (cid:80)K Similar to value functions, which represent the expectation of the reward under the state occupancy, we deﬁne expected features as ψπ(s, a) = Es(cid:48),a(cid:48)∼dπ(s,a) φ(s(cid:48), a(cid:48)) ∈ Rd. Note that in the special case of one-hot feature vectors, the expected features coincide with the state occupancy. The deﬁnition of ψπ depends on the state occupancy we consider. In the discounted case, ψγ π ∈ Rd is also known as Successor Features (SFs) as deﬁned in [5, 6]. In the average case, ψavg π ∈ Rd represents the expected features under the policy’s stationary distribution and therefore it has the same value for all the state action pairs. Similar deﬁnitions were suggested in [35, 56]. 3 3 Discovering diverse near-optimal policies We now introduce Diversity Optimization Maintaining Near Optimality, or, DOMiNO, which discovers a set of n policies Πn = {πi}n i=1 by solving the optimization problem: max Πn Diversity(Πn) s.t dπ · re ≥ αv∗ e , ∀π ∈ Πn, (4) where v∗ e is the value of the optimal policy. In other words, we are looking for a set of policies that are as diverse from each other as possible, deﬁned as Diversity : {R|S||A|}n → R. In addition, we constrain the policies in the set to be nearly optimal. To deﬁne near-optimality we introduce a hyperparameter α ∈ [0, 1], such that a policy is said to be near optimal if it achieves a value that is at e . In practice, we “ﬁx” the Lagrange multiplier for the ﬁrst policy µ1 = 1, so this policy only least αv∗ receives extrinsic reward, and use the value of this policy to estimate v∗ e ). Notice that this estimate is changing through training. e = v1 e (v∗ Before we dive into the details, we brieﬂy explain the main components of DOMiNO. Building on Section 2 and, in particular, Eq. (3), we ﬁnd policies that maximize the diversity objective by maximizing its gradient as a reward signal, i.e., ri π). We discuss two candidates for this objective and derive an analytical formula for the associated rewards in Section 3.1. Diversity(d1 π, . . . , dn d = ∇di π Then, in Section 3.2 we explain how to combine the two rewards via the coefﬁcients ce, cd. Thus, each of the policies, π1, . . . , πn, will be maximizing a reward signal ri that is a linear combination of the extrinsic reward re and ri d : i.e., ri(s, a) = cere(s, a) + cdri d(s, a). To this end, we focus on the method of Lagrange multipliers, which adapts the coefﬁcients online in order to solve Eq. (4) and compare it with other multi-objective baselines. 3.1 Diversity Next, we present an objective that motivates policies to visit different states on average. It does so by leveraging the information about the policies’ long-term behavior available in their expected features, and motivating the state occupancies to be different from each other. For that reason, we refer to this objective as a repulsive force (Eq. (5)). We then extend this objective and combine it with a second, attractive force (Eq. (7)), taking inspiration from the Van Der Waals (VDW) force. The manner in which we combine these two forces allows us to control the degree of diversity in the set. A repulsive force. How do we compute a set of policies with maximal distances between their expected features? To answer this question, we ﬁrst consider the simpler scenario where there are only two policies in the set and consider the following objective maxπ1,π2 ||ψ1 − ψ2||2 2. This objective is related to the objective of Apprenticeship Learning [AL; 1], i.e., solving the problem minψ ||ψ − ψE||2 2, where ψE are the feature expectations of an expert. Both problems use the euclidean norm in the feature expectation space to measure distances between policies. Since we are interested in diversity, we are maximizing this objective, while AL aims to minimize it. In a similar fashion, the mutual information between policies and states, which is equivalent to the KL divergence between state occupancies [58, 21] is minimized for AL [28] and maximized for diversity [20]. Next, we investigate how to measure the distance of a policy from the set of multiple policies, Πn. First we introduce the Hausdorff distance [43] that measures how far two subsets D, C of a metric space are from each other: Dist(D, C) = minc∈C,d∈D ||c − d||2 2. In other words, two sets are far from each other in the Hausdorff distance if every point of either set is far from all the points of the other set. Building on this deﬁnition, we can deﬁne the distance from an expected features vector ψi to the set of the other expected features vectors as minj(cid:54)=i ||ψi − ψj||2 2. This equation gives us the distance between each individual policy and the other policies in the set. Maximizing it across the policies in the set, gives us our ﬁrst diversity objective: max d1 π,...,dn π 0.5 (cid:88)n i=1 min j(cid:54)=i ||ψi − ψj||2 2. (5) π Diversity(d1 to compute the associated diversity reward, we compute the gradient ri d = In order ∇di π). To do so, we begin with a simpler case where there are only two policies, 2 = φ · (ψ1 − ψ2), i.e., r = ∇d1 2 = ∇d1 such that r(s, a) = φ(s, a) · (ψ1 − ψ2). This reward was ﬁrst derived by Abbeel & Ng [1], but here it is with an opposite sign since we care about maximizing it. Lastly, for a given policy πi, we deﬁne by π(s,a)φ(s, a) − Es(cid:48),a(cid:48)∼d2 π, . . . , dn ||ψ1 − ψ2||2 π(s,a)φ(s, a)||2 ||Es(cid:48),a(cid:48)∼d1 π π 4 π i , we get1 that ∇di i the index of the policy with the closest expected features to πi, i.e., j∗ j∗ Using the deﬁnition of j∗ 2 = ∇di minj(cid:54)=i ||ψi − ψj||2 d(s, a) = φ(s, a) · (ψi − ψj∗ ri The Van Der Waals force. Next, we propose a second diversity objective that allows us to control the degree of diversity in the set via a hyperparameter. The objective is inspired from molecular physics, and speciﬁcally, by how atoms in a crystal lattice self-organize themselves at equal distances from each other. This phenomenon is typically explained as an equilibrium between two distance dependent forces operating between the atoms known as the Van Der Waals (VDW) forces; one force that is attractive and another that is repulsive. i = arg minj(cid:54)=i ||ψi − ψj||2 2. i ||2 ||ψi − ψj∗ 2, and that i ). (6) π The VDW force is typically characterized by a distance in which the combined force becomes repulsive rather than attractive (see, for example, [47]). This distance is called the VDW contact distance, and we denote it by (cid:96)0. In addition, we denote by (cid:96)i = ||ψi − ψj∗ i ||2 the Hausdorff distance for policy i. With this notation, we deﬁne our second diversity objective as2 max π,...,dn d1 π (cid:88)n i=1 0.5(cid:96)2 i (cid:124) (cid:123)(cid:122) (cid:125) Repulsive i /(cid:96)3 0 −0.2(cid:0)(cid:96)5 (cid:123)(cid:122) (cid:124) Attractive (cid:1) (cid:125) . (7) We can see that Eq. (7) is a polynomial in (cid:96)i, composed of two forces with opposite signs and different powers. The different powers determine when each force dominates the other. For example, when the expected features are close to each other ((cid:96)i << (cid:96)0), the repulsive force dominates, and when ((cid:96)i >> (cid:96)0) the attractive force dominates. The gradient (and hence, the associated reward) is given by d(s, a) = (1 − ((cid:96)i/(cid:96)0)3)φ(s, a) · (ψi − ψj∗ ri Inspecting Eq. (8) we can see that when the expected features are organized at the VDW contact distance (cid:96)0, the objective is maximized and the gradient is zero. In a related line of work, Vassiliades et al. [54] suggested to use voronoi tessellation to partition the feature space of the MAP-Elite algorithm to regions of equal size and Liu et al. [33] proposed a Stein Variational Policy Gradient with repulsive and attractive components. However, using a VDW force to control diversity is novel to the best of our knowledge. i ). (8) 3.2 Balancing Quality with Diversity Constrained MDPs. At the core of our approach is a solution to the CMDP in Eq. (4). There exist different methods for solving CMDPs and we refer the reader to [4] and [50] for treatments of the subject at different levels of abstraction. In this work we will focus on a reduction of CMDPs to MDPs via gradient updates, known as Lagrangian methods [11, 10, 53, 14]. Most of the literature on CMDPs has focused on linear objectives and linear constraints. In Section 2, we discussed how to solve an unconstrained convex RL problem of the form of Eq. (1) as a saddle point problem. We now extend these results to the case where the objective is convex and the constraint is linear, i.e. mindπ∈K f (dπ), subject to g(dπ) ≤ 0, where f denotes the diversity objective and g is a linear function of the form g(dπ) = αv∗ e − dπ · re deﬁning the constraint. Solving this problem is equivalent to solving the following problem: min dπ∈K max µ≥0 f (dπ) + µg(dπ) = min dπ∈K max µ≥0,λ λ · dπ − f ∗(λ) + µ(αv∗ e − dπ · re), (9) where the equality follows from Fenchel duality as before. Similar to Section 2, we use the FTL algorithm for the λ player (Eq. (3)). This implies that the cost at iteration k, λk, is equivalent to the gradient of the diversity objective, which we denote by rd. Eq. (9) involves a vector, λ − µre, linearly interacting with dπ. Thus, intuitively speaking, minimizing Eq. (9) from the perspective of the policy player is equivalent to maximizing a reward rd + µre. The objective for the Lagrange multiplier µ is to maximize Eq. (9), or equivalently µ(αv∗ e − dπ · re). Intuitively speaking, when the policy achieves an extrinsic value that satisﬁes the constraint, the 1In the rare case that the arg min has more than one solution, the gradient is not deﬁned, but we can still use Eq. (6) as a reward. 2The coefﬁcients in Eq. (7) are chosen to simplify the reward in Eq. (8). I.e., since the reward is the gradient of the objective, after differentiation the coefﬁcients equal 1 in Eq. (8). 5 Lagrange multiplier µ decreases (putting a smaller weight on the extrinsic component of the reward) and it increases otherwise. More formally, we can solve the problem in Eq. (9) as a three-player game. In this case the policy player controls dπ as before, the cost player chooses λ using Eq. (3), and the Lagrange player chooses µ with gradient descent. Proving this statement is out of the scope of this work, but we shall investigate it empirically. 3.3 Multi-objective alternatives We conclude this section by discussing two alternative approaches for balancing the QD trade-off, which we later compare empirically with the CMDP approach. First, consider a multi-objective MDP that combines the diversity objective with the extrinsic reward as π, . . . , dn cedi π · re + cdDiversity(d1 (10) π), max Πn where ce, cd are ﬁxed weights that balance the diversity objective and the extrinsic reward. We note that the solution of such a multi-objective MDP cannot be a solution to a CMDP in general. I.e., it is not possible to ﬁnd the optimal dual variables µ∗, plug them into Eq. (9) and simply solve the resulting (unconstrained) MDP. Such an approach ignores the fact that the dual variables must be a ‘best-response’ to the policy and is referred to as the ”scalarization fallacy” in [50, Section 4]. While multi-objective MDPs have been used in prior QD-RL papers [29, 34, 39, 22, 40, 60], we now outline a few potential advantages for using CMDPs. First, the CMDP formulation guarantees that the policies that we ﬁnd are near optimal (satisfy the constraint). Secondly, the weighting coefﬁcient in multi-objective MDPs has to be tuned, where in CMDPs it is adapted. This is particularly important in the context of maximizing diversity while satisfying reward. Next, consider a hybrid approach that combines a multi objective MDP with a CMDP as max Πn max(0, αv∗ e − di π · re) + cdDiversity(d1 π, . . . , dn π). We denote by I i an indicator function for the event in which the constraint is not satisﬁed for policy πi, i.e., I i = 1 if di e , and 0, otherwise. With this notation the reward is given by π · re < αv∗ ri(s, a) = I ire(s, a) + cdri d(s, a) (11) In other words, the agent maximizes a weighted combination of the extrinsic reward and the diversity reward when the constraint is violated and only the diversity reward when the constraint is satisﬁed. Kumar et al. [31] proposed a similar approach where the agent maximizes a weighted combination of the rewards when the constraint is satisﬁed, and only the extrinsic reward otherwise: ri(s, a) = re(s, a) + cd(1 − I i)ri d(s, a) (12) We refer to Eq. (12) as SMERL, as was coined in [31], and to Eq. (11) as Reverse SMERL. Note that these methods come with an additional hyperparameter cd which balances the two objectives as a multi-objective MDP, in addition to the optimality ratio α. 4 Experiments Our experiments address the following questions: (a) Can we learn diverse policies that are also near optimal? (see Section 4.1) (b) How critical are various components of our algorithm and how does it compare to other multi-objective baselines in terms of QD trade-off? (see Section 4.1) (c) Does our method scale to settings where the feature space is high-dimensional and unstructured? (see Section 4.2) (d) Finally, can the diverse policies discovered by DOMiNO enable robustness and adaptation to novel perturbations in the environment and agent? (see Section 4.3) Environment. We conducted most of our experiments on domains from the DM Control Suite [52], standard continuous control locomotion tasks where diverse near-optimal policies should naturally correspond to different gaits. Due to space considerations, we present Control Suite results on the walker.stand task. In the supplementary, however, we present similar results for walker.walk and BiPedal walker from OpenAI Gym [13] suggesting that the method generalizes across different reward functions and domains. We also include the challenging dog domain with 38 actions and a 223−dimensional observation space, which is one of the more challenging domains in control suite. 6 Agent. Fig. 1a shows an overview of DOMiNO’s components and their interactions, instantiated in an actor-critic agent. While acting, the agent samples a new latent variable z ∈ [1, n] uniformly at random at the start of each episode. We train all the policies simultaneously and provide this latent variable as an input. For the average reward state occupancy, the agent keeps an empirical running average for each latent policy of the rewards ˜vavg πi and features (either from the environment φobs or torso embedding φembedding) ˜ψavg πi encountered, where the average is taken as ˜xi = αd ˜xi−1 + (1 − αd) 1 t=1 xi(st, at) with decay factor αd. Varying αd can make the estimation more online (small T αd, as used for the constraint), or less online (large αd, as needed for Eq. (4)). The agent uses ˜ψavg to compute the diversity reward as described in Eq. (6). ˜vavg is used to optimize πi πi the Lagrange multiplier µ for each policy as in Eq. (9) which is then used to weight the quality and diversity advantages for the policy gradient update. Pseudo code and further implementation details, as well as treatment of the discounted state occupancy, can be found in Appendix A.2. (cid:80)T 4.1 Quality and diversity To measure diversity qualitatively, we present ”motion ﬁgures” by discretizing the videos (details in the Appendix) that give a fair impression of the behaviors. The videos, associated with these ﬁgures, can be found in the supplementary as well. Fig. 1b presents ten polices discovered by DOMiNO with the repulsive objective and the optimality ratio set to 0.9. The policies are ordered from top-left to bottom right, so policy 1, which only maximizes extrinsic reward and sets the constraint, is always at the top left. The policies exhibit different types of standing: standing on both legs, standing on either leg, lifting the other leg forward and backward, spreading the legs and stamping. Not only are the policies different from each other, they also achieve high extrinsic reward in standing (see values on top of each policy visualization). Similar ﬁgures for other domains can be found in Appendix B.1. To further study the QD trade-off, we use scatter plots, showing the episode return on the y-axis, and the diversity score, corresponding to the Hausdorff distance (Eq. (5)), on the x-axis. The top-right corner of the diagram, therefore, represents the most diverse and highest quality policies. Each ﬁgure presents a sweep over one or two hyperparameters and we use the color and a marker to indicate the values. In all of our experiments, we report 95% conﬁdence intervals. In the scatter plots, they correspond to 5 seeds and are indicated by the crosses surrounding each point. Fig. 2 (left) presents the results for DOMiNO with the repulsive reward in the walker.stand domain. We can observe that regardless of the set size, DOMiNO achieves roughly the same extrinsic reward across different optimality ratios (points with the same color obtain the same y-value). This implies that the constraint mechanism is working as expected across different set sizes and optimality ratios. In addition, we can inspect how the QD trade-off is affected when changing the optimality ratio α for sets of the same size (indicated in the ﬁgures with light lines). This observation can be explained by the fact that for lower values of α, the volume of the constrained set is larger, and therefore, it is possible to ﬁnd more diversity within it. This behavior is consistent across different set sizes, though it is naturally more difﬁcult to ﬁnd a set of diverse policies as the set size gets larger (remember that we measure the distance to the closest policy in the set). Figure 2: DOMiNO QD results in walker.stand. Left: Set size vs. optimality ratio (α) with the repulsive reward. Center: Set size vs. α with the Van der Waals (VDW) reward. Right: Target diversity ((cid:96)0) vs. α with the VDW reward. We present the same investigation for the VDW reward in Fig. 2 (center). Similar to the repulsive reward, we can observe that the constraint is satisﬁed, and that reducing the optimality ratio allows for more diversity. Fig. 2 (right) shows how different values of (cid:96)0 affect the QD trade-off for a set of size 10. We can observe that the different combinations of (cid:96)0 and α are organized as a grid in the 7 QD scatter, suggesting that we can control both the level of optimality and the degree of diversity by setting these two interpretable hyperparameters. Fig. 3 compares the QD balance yielded by DOMiNO to the alternative strategies described in Section 3.2. Speciﬁcally, we look at DOMiNO’s Lagrangian method, the linear multi-objective combination of the objectives (Eq. (10)), and the two hybrid strategies, SMERL (Eq. (12)) and Reverse SMERL (Eq. (11)) for a set of ten policies in walker.stand. Note that in all cases we are using DOMiNO’s repulsive diversity objective, and the comparison is strictly about strategies for combining the quality and diversity objectives. The plot for each strategy shows how the solution to the QD tradeoff varies according to the relevant hyperparameters for that strategy, namely, the optimality ratio α for DOMiNO, the ﬁxed constant ce for the multi-objective strategy (we implicitly set cd = 1 − ce), and both α and constant cd for the hybrid approaches (in the hybrid plots, cd value is labeled directly next to the marker, while α is indicated by color). Figure 3: DOMiNO’s Lagrangian method ﬁnds only solutions that push the upper right boundary of quality and diversity, and varies in a smooth, interpretable way with its only hyperparameter, α, contrasted with the jumpy nature of the multi-objective hyperparameter ce, and the redundancy of the hyperparameters in the hybrid methods (SMERL and Reverse SMERL). For the multi-objective approach, shown on the right, the ce parameter proves ill-behaved and choppy, quickly jumping from the extreme of all diversity no quality to the opposite, without a smooth interim. In contrast, the DOMiNO approach of solving the CMDP directly for the Lagrange multiplier yields solutions that push along the upper right diagonal boundary, ﬁnding the highest diversity (farthest right) set of policies for a given optimality ratio (color), varying smoothly along this line as α varies. Another advantage of DOMiNO’s approach is that it only ﬁnds such QD-optimal solutions, where, in contrast, SMERL (left), when appropriately tuned, can also yield some solutions along the upper-right QD border, but often ﬁnds sub-optimal solutions, and therefore must be tuned further with cd to ﬁnd the best QD solutions. We further explore the difﬁculty tuning SMERL in the supplementary (Fig. 10) and ﬁnd that the best cd for 10 policies provides solutions with no diversity for other set sizes. 4.2 Feature analysis The choice of feature space used for optimizing diversity can have a huge impact on the kind of diverse behavior learned. In environments where the observation space is high dimensional and less structured (e.g. pixel observations), computing diversity using the raw features may not lead to meaningful behavior. As speciﬁed in Section 3.1 the feature space used to compute diversity in our Control Suite experiments throughout the paper corresponds to the positions and velocities of the body joints returned as observations by the environment. We show that it is feasible to use a learned embedding space instead. As a proof of principle we use the output of the torso network as a learned embedding described in Section 4 for computing our diversity metric. Table 1 compares the diversity mea- sured in raw observation features (Diversityobs) and embedding features (Diversityembedding) in the walker.stand domain. Columns indicate the feature space that was used to compute the di- versity objective during training averaged across 20 seeds. Inspecting the table, we can see that agents trained to optimize diversity in the learned embedding space and agents that directly optimize diversity in the observation space achieve comparable diversity if measured in either space, indicating that learned embeddings can feasibly be used to achieve meaningful diversity. Diversityobs Diversityembedding φembedding 1.01 ± 0.05 2.35 ± 0.09 φobs 1.21 ± 0.05 2.14 ± 0.09 Table 1 8 Figure 4: K-shot adaptation in Control Suite. We report mean episode return (95% CI) on held-out test tasks relative to the performance of a single policy trained on extrinsic rewards. While not invariant to sudden changes in the environment, DOMiNO is more robust to a variety of perturbations. 4.3 Closing the loop: k-shot adaptation We motivated qualitative diversity by saying that diverse solutions can be robust and allow for rapid adaptation to new tasks and changes in the environment. Here we validate this claim in a k-shot adaptation experiment: we train a set of QD policies on a canonical benchmark task, then test their ability to adapt to environment and agent perturbations. These include four kinematics and dynamics perturbations from the Real World RL suite [18], and a ﬁfth perturbation inspired by a ”motor failure” condition [31] which, every 50 steps and starting at T = 10, disables action-inputs for the ﬁrst two joints for a ﬁxed amount of time3. In Fig. 4, we present the results in the walker.walk and walker.stand domains (rows). Columns correspond to perturbation types and the x-axis corresponds to the magnitude of the perturbation. K-shot adaptation is measured in the following manner. For each perturbed environment, and each method, we ﬁrst execute k = 10 environment trajectories with each policy. Then, for each method we select the policy that performs the best in the set. We then evaluate this policy for 40 more trajectories and measure the average reward of the selected policy rmethod. The y-axis in each ﬁgure measures rmethod/rbaseline, where rbaseline measures the reward in the perturbed environment of an RL baseline agent that was trained with a single policy to maximize the extrinsic reward in the original task. We note that the baseline was trained with the same RL algorithm, but without diversity, and it matches the state-of-the-art in each training domain (it is almost optimal). The raw rewards rmethod, rbaseline can be found in the supplementary (Fig. 12). Lastly, we repeat this process across 20 training seeds, and report the average with a 95% Conﬁdence Interval (CI) 4. We compare the following methods: DOMiNO, SMERL, Multi-Objective, and No diversity, where all the diversity methods use the diversity reward from Eq. (6) and all the methods are with 10 policies. Since we are treating the perturbed environments as hold out tasks, we selected the hyper parameters for each method based on the results in Fig. 3, i.e.we chose the conﬁguration that was the most qualitatively diverse (in the upper-right most corner of Fig. 3). Concretely, for DOMiNO and SMERL α = 0.9, for SMERL cd = 0.5 and for Multi-Objective ce = 0.7. More K-shot adaptation curves with other hyper parameter values can be found in Appendix D. The No diversity method is similar to rbaseline, but uses 10 policies that all maximize the extrinsic reward (instead of a single policy). Inspecting Fig. 4 we can see that for small perturbations, DOMiNO retains the performance of the baseline. However, as the magnitude of the perturbation increases, the performance of DOMiNO is much higher than the baseline (by a factor of 1.5 − 2.0). This observation highlights that a diverse set of policies as found by DOMiNO is much more capable at handling changes to the environment and can serve as a strong starting point for recovering optimal behavior. As we have shown in 3.2, other approaches to managing the trade-off between quality and diversity such as SMERL are much more sensitive to the choice of hyper-parameters and require signiﬁcant tuning. While SMERL is able to ﬁnd a useful, diverse set of policies with some effort, it is difﬁcult to match DOMiNO’s performance across all pertubations and tasks. See the supplementary material for further comparison of DOMiNO with SMERL and Multi-objective over more hyper parameters. We also include a video that illustrates how the individual policies adapt to the environment perturbations. 3While we tried to recreate a similar condition to [31], the tasks are not directly comparable due to signiﬁcant differences in the simulators that have been used as well as the termination conditions in the base task. 4We use boosted CI with nested sampling as implemented in the bootstrap function here, which reﬂects the amount of training seeds and the amount of evaluation seeds per training seed 9 s n r u t e r e v i t a e R l s n r u t e r e v i t a e R l d n a t S 1.50 1.25 1.00 1.35 l k a W 1.20 1.05 1.80 1.50 1.20 0.90 1.80 1.50 1.20 0.90 0 0 1 0 2 0 3 0 4 5 2 0 . 2 Motor failure (duration) 5 7 0 . 3 0 . 3 0 . 2 Thigh length 2.00 1.60 1.20 0.80 1.75 1.50 1.25 1.00 0 . 4 0 . 3 1.40 1.20 1.00 2.40 2.00 1.60 1.20 1.20 1.05 0.90 1.35 1.20 1.05 0.90 0 . 5 0 . 1 1 . 0 2 . 0 3 . 0 4 . 0 0 . 7 Joint damping 5 5 0 . 4 0 . 4 0 . 3 Torso length DOMiNO SMERL No diversity Multi-Objective 1 0 0 . 0 2 0 . 1 0 . 0 Contact friction 0 . 0 0 5 5 Conclusions In this work we proposed DOMiNO, an algorithm for discovering diverse behaviors that maintain optimality. We framed the problem as a CMDP in the state occupancies of the policies in the set and developed an end-to-end differentiable solution to it based on reward maximization. In our experiments we demonstrated that the policies discovered by DOMiNO, or, DOMiNO’s π, are diverse and maintain optimality. We then explored how DOMiNO balances the QD trade-off and compared it with multi-objective baselines. Our results suggest that DOMiNO can control the degree of quality and diversity via two interpretable hyperparameters, while other baselines struggle to capture both. In our K-shot experiments we demonstrated that DOMiNO’s π can adapt to changes in the environ- ment. An exciting direction for future work is to use DOMiNO in a never ending RL setting, where the environment changes smoothly over time, and see if maintaing a set of QD diverse policies will make it more resilient to such changes. 10 References [1] Abbeel, P. and Ng, A. Y. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-ﬁrst international conference on Machine learning, pp. 1. ACM, 2004. [2] Abernethy, J. D. and Wang, J.-K. On frank-wolfe and equilibrium computation. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Cur- ran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 7371364b3d72ac9a3ed8638e6f0be2c9-Paper.pdf. [3] Agadmator. Invisible to engines — one of the greatest moves ever played. agadmator’s Chess Channel, 2018. URL https://www.youtube.com/watch?v=yGnpewUKP88&t=1s. [4] Altman, E. Constrained Markov decision processes, volume 7. CRC Press, 1999. [5] Barreto, A., Dabney, W., Munos, R., Hunt, J. J., Schaul, T., van Hasselt, H. P., and Silver, D. Successor features for transfer in reinforcement learning. In Advances in neural information processing systems, pp. 4055–4065, 2017. [6] Barreto, A., Hou, S., Borsa, D., Silver, D., and Precup, D. Fast reinforcement learning with generalized policy updates. Proceedings of the National Academy of Sciences, 2020. [7] Baumli, K., Warde-Farley, D., Hansen, S., and Mnih, V. Relative variational intrinsic control. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 35:6732–6740, May 2021. URL https://ojs.aaai.org/index.php/AAAI/article/view/16832. [8] Behovits, R. Game 8: Leko wins to take the lead. Chess news, 2004. URL https://en. chessbase.com/post/game-8-leko-wins-to-take-the-lead. [9] Belogolovsky, S., Korsunsky, P., Mannor, S., Tessler, C., and Zahavy, T. Inverse reinforcement learning in contextual mdps. Machine Learning, pp. 1–40, 2021. [10] Bhatnagar, S. and Lakshmanan, K. An online actor–critic algorithm with function approximation for constrained markov decision processes. Journal of Optimization Theory and Applications, 153(3):688–708, 2012. [11] Borkar, V. S. An actor-critic algorithm for constrained markov decision processes. Systems & control letters, 54(3):207–213, 2005. [12] Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., and Wanderman- Milne, S. JAX: composable transformations of Python+NumPy programs, 2018. URL http: //github.com/google/jax. [13] Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [14] Calian, D. A., Mankowitz, D. J., Zahavy, T., Xu, Z., Oh, J., Levine, N., and Mann, T. Balancing constraints and rewards with meta-gradient d4pg. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=TQt98Ya7UMP. [15] Cully, A. Autonomous skill discovery with quality-diversity and unsupervised descriptors. In Proceedings of the Genetic and Evolutionary Computation Conference, pp. 81–89, 2019. [16] Cully, A., Clune, J., Tarapore, D., and Mouret, J.-B. Robots that can adapt like animals. Nature, 521(7553):503–507, 2015. [17] da Fonseca-Wollheim, C. Swapping songs with chess grandmaster garry kasparov. The New York Times, 2020. URL https://www.nytimes.com/2020/12/18/arts/music/ garry-kasparov-classical-music.html. [18] Dulac-Arnold, G., Mankowitz, D., and Hester, T. Challenges of real-world reinforcement learning. arXiv preprint arXiv:1904.12901, 2019. 11 [19] Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron, Y., Firoiu, V., Harley, T., Dunning, I., et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International Conference on Machine Learning, pp. 1407–1416. PMLR, 2018. [20] Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S. Diversity is all you need: Learning skills without a reward function. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=SJx63jRqFm. [21] Eysenbach, B., Salakhutdinov, R., and Levine, S. The information geometry of unsupervised reinforcement learning. arXiv preprint arXiv:2110.02719, 2021. [22] Gangwani, T., Peng, J., and Zhou, Y. Harnessing distribution ratio estimators for learning agents with quality and diversity. arXiv preprint arXiv:2011.02614, 2020. [23] Geist, M., P´erolat, J., Lauri`ere, M., Elie, R., Perrin, S., Bachem, O., Munos, R., and Pietquin, O. Concave utility reinforcement learning: the mean-ﬁeld game viewpoint. arXiv preprint arXiv:2106.03787, 2021. [24] Gregor, K., Rezende, D. J., and Wierstra, D. Variational intrinsic control. International Conference on Learning Representations, Workshop Track, 2017. URL https://openreview. net/forum?id=Skc-Fo4Yg. [25] Ha, D. Reinforcement learning for improving agent design. arXiv preprint arXiv:1810.03779, 2018. [26] Hazan, E., Kakade, S., Singh, K., and Van Soest, A. Provably efﬁcient maximum entropy exploration. In International Conference on Machine Learning, pp. 2681–2691. PMLR, 2019. [27] Hessel, M., Kroiss, M., Clark, A., Kemaev, I., Quan, J., Keck, T., Viola, F., and van Hasselt, H. Podracer architectures for scalable reinforcement learning. arXiv preprint arXiv:2104.06272, 2021. [28] Ho, J. and Ermon, S. Generative adversarial imitation learning. arXiv preprint arXiv:1606.03476, 2016. [29] Hong, Z.-W., Shann, T.-Y., Su, S.-Y., Chang, Y.-H., Fu, T.-J., and Lee, C.-Y. Diversity-driven exploration strategy for deep reinforcement learning. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp. 10510–10521, 2018. [30] Jouppi, N. P., Young, C., Patil, N., Patterson, D. A., Agrawal, G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers, A., Boyle, R., Cantin, P., Chao, C., Clark, C., Coriell, J., Daley, M., Dau, M., Dean, J., Gelb, B., Ghaemmaghami, T. V., Gottipati, R., Gulland, W., Hagmann, R., Ho, R. C., Hogberg, D., Hu, J., Hundt, R., Hurt, D., Ibarz, J., Jaffey, A., Jaworski, A., Kaplan, A., Khaitan, H., Koch, A., Kumar, N., Lacy, S., Laudon, J., Law, J., Le, D., Leary, C., Liu, Z., Lucke, K., Lundin, A., MacKean, G., Maggiore, A., Mahony, M., Miller, K., Nagarajan, R., Narayanaswami, R., Ni, R., Nix, K., Norrie, T., Omernick, M., Penukonda, N., Phelps, A., Ross, J., Salek, A., Samadiani, E., Severn, C., Sizikov, G., Snelham, M., Souter, J., Steinberg, D., Swing, A., Tan, M., Thorson, G., Tian, B., Toma, H., Tuttle, E., Vasudevan, V., Walter, R., Wang, W., Wilcox, E., and Yoon, D. H. In-datacenter performance analysis of a tensor processing unit. CoRR, abs/1704.04760, 2017. URL http://arxiv.org/abs/1704.04760. [31] Kumar, S., Kumar, A., Levine, S., and Finn, C. One solution is not all you need: Few-shot extrapolation via structured maxent rl. Advances in Neural Information Processing Systems, 33, 2020. [32] Lehman, J. and Stanley, K. O. Evolving a diversity of virtual creatures through novelty search and local competition. In Proceedings of the 13th annual conference on Genetic and evolutionary computation, pp. 211–218, 2011. [33] Liu, Y., Ramachandran, P., Liu, Q., and Peng, J. Stein variational policy gradient. In 33rd Conference on Uncertainty in Artiﬁcial Intelligence, UAI 2017, 2017. 12 [34] Masood, M. A. and Doshi-Velez, F. Diversity-inducing policy gradient: Using maximum mean discrepancy to ﬁnd a set of diverse policies. arXiv preprint arXiv:1906.00088, 2019. [35] Mehta, N., Natarajan, S., Tadepalli, P., and Fern, A. Transfer in variable-reward hierarchical reinforcement learning. Machine Learning, 73(3):289, 2008. [36] Mouret, J.-B. and Clune, J. Illuminating search spaces by mapping elites. arXiv preprint arXiv:1504.04909, 2015. [37] Mutti, M., De Santi, R., De Bartolomeis, P., and Restelli, M. Challenging common assumptions in convex reinforcement learning. arXiv preprint arXiv:2202.01511, 2022. [38] Osborn, A. F. Applied imagination. Scribner’s, 1953. [39] Parker-Holder, J., Pacchiano, A., Choromanski, K. M., and Roberts, S. J. Effective diversity in population based reinforcement learning. Advances in Neural Information Processing Systems, 33, 2020. [40] Peng, Z., Sun, H., and Zhou, B. Non-local policy optimization via diversity-regularized collaborative exploration. arXiv preprint arXiv:2006.07781, 2020. [41] Pugh, J. K., Soros, L. B., and Stanley, K. O. Quality diversity: A new frontier for evolutionary computation. Frontiers in Robotics and AI, 3:40, 2016. [42] Puterman, M. L. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 1984. [43] Rockafellar, R. T. Convex analysis. Princeton university press, 1970. [44] Schmitt, S., Hessel, M., and Simonyan, K. Off-policy actor-critic with shared experience replay. In International Conference on Machine Learning, pp. 8545–8554. PMLR, 2020. [45] Shani, L., Zahavy, T., and Mannor, S. Online apprenticeship learning. arXiv preprint arXiv:2102.06924, 2021. [46] Sharma, A., Gu, S., Levine, S., Kumar, V., and Hausman, K. Dynamics-aware unsupervised discovery of skills. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HJgLZR4KvH. [47] Singh, A. K. Chapter 2 - structure, synthesis, and application of nanoparticles. In Singh, A. K. (ed.), Engineered Nanoparticles, pp. 19–76. Academic Press, Boston, 2016. ISBN 978-0-12-801406-6. doi: https://doi.org/10.1016/B978-0-12-801406-6.00002-9. URL https: //www.sciencedirect.com/science/article/pii/B9780128014066000029. [48] Stooke, A., Achiam, J., and Abbeel, P. Responsive safety in reinforcement learning by pid lagrangian methods. In International Conference on Machine Learning, pp. 9133–9143. PMLR, 2020. [49] Sun, H., Peng, Z., Dai, B., Guo, J., Lin, D., and Zhou, B. Novel policy seeking with constrained optimization. arXiv preprint arXiv:2005.10696, 2020. [50] Szepesv´ari, C. Constrained mdps and the reward hypothesis. Musings about machine learn- ing and other things (blog), 2020. URL https://readingsml.blogspot.com/2020/03/ constrained-mdps-and-reward-hypothesis.html. [51] Tarapore, D., Clune, J., Cully, A., and Mouret, J.-B. How do different encodings inﬂuence the performance of the map-elites algorithm? In Proceedings of the Genetic and Evolutionary Computation Conference 2016, pp. 173–180, 2016. [52] Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. d. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, A., et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018. [53] Tessler, C., Mankowitz, D. J., and Mannor, S. Reward constrained policy optimization. In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=SkfrvsA9FX. 13 [54] Vassiliades, V., Chatzilygeroudis, K., and Mouret, J.-B. Scaling up map-elites using centroidal voronoi tessellations. arXiv preprint arXiv:1610.05729, 2016. [55] Zahavy, T., Cohen, A., Kaplan, H., and Mansour, Y. Apprenticeship learning via frank-wolfe. AAAI, 2020, 2020. [56] Zahavy, T., Cohen, A., Kaplan, H., and Mansour, Y. Average reward reinforcement learning with unknown mixing times. The Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2020. [57] Zahavy, T., Barreto, A., Mankowitz, D. J., Hou, S., O’Donoghue, B., Kemaev, I., and Singh, S. Discovering a set of policies for the worst case reward. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=PUkhWz65dy5. [58] Zahavy, T., O’Donoghue, B., Desjardins, G., and Singh, S. Reward is enough for convex MDPs. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id= ELndVeVA-TR. [59] Zhang, J., Koppel, A., Bedi, A. S., Szepesvari, C., and Wang, M. Variational policy gradient method for reinforcement learning with general utilities. arXiv preprint arXiv:2007.02151, 2020. [60] Zhang, Y., Yu, W., and Turk, G. Learning novel policies for tasks. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learn- ing, volume 97 of Proceedings of Machine Learning Research, pp. 7483–7492. PMLR, 09–15 Jun 2019. URL http://proceedings.mlr.press/v97/zhang19q.html. 14 Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See the discussion in Appendix C . (c) Did you discuss any potential negative societal impacts of your work? [No] We could identify any potential negative societal impacts for this work (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [No] While the DM control domain is open sourced, our code is proprietary and we are not able to share it. That said, we shared the source code for the diversity reward function in Appendix A.3 and provided pseudo code and hyper parameter details in Appendix A.2. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See above. (c) Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix E 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [N/A] (b) Did you mention the license of the assets? [N/A] (c) Did you include any new assets either in the supplemental material or as a URL? [N/A] (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 15 A Additional Experiment Details A.1 Environment We evaluate our method on a number of tasks. These tasks have different complexity in terms of control, dynamics and reward structure. • walker.walk (Control Suite): a 8-dimensional control task with a 23-dimensional observa- tion space, where the goal is to control the joints of a humanoid character to make it walk forward in a 2D plane. • walker.stand (Control Suite): a 8-dimensional control task with a 23-dimensional observa- tion space, where the goal is to control the joints of a humanoid character to make it stand up. • dog.walk (Control Suite): a 38-dimensional control task with a 223-dimensional observation space, where the goal is to control the joints of a dog character to make it walk forward in a 2D plane. • dog.stand (Control Suite): a 38-dimensional control task with a 223-dimensional obser- vation space, where the goal is to control the joints of a dog character to make it stand up. • BiPedal Walker: a 4-dimensional control task with a 24-dimensional observation space, where the goal is to control the joints of a bipedal-walker with a large payload in order to traverse a terrain from left to right as fast as possible. 16 A.2 Implementation details Distributed agent Acting and learning are decoupled, with multiple actors gathering data in parallel from a batched stream of environments, and storing their trajectories, including the latent variable z, in a replay buffer and queue from which the learner can sample a mixed batch of online and replay trajectories [44, 27]. The latent variable z is sampled uniformly at random in [1, n] during acting at the beginning of each new episode. The learner differentiates the loss function as described in Algorithm 1, and uses the optimizer (speciﬁed in Table 2) to update the network parameters and the Lagrange multipliers (speciﬁed in Table 3). Lastly, the learner also updates the and moving averages as described in Algorithm 1. Initialization When training begins we initialize the network parameters as well as the Lagrange multipliers: µi = σ−1(0.5), ∀i ∈ [2, n], where σ−1 is the inverse of the Sigmoid function µ1 = 1; and the moving averages: ˜vavg πi = ¯1/d, ∀i ∈ [1, n]. Here n is the number of policies and d is the dimension of the features φ. πi = 0., ∀i ∈ [1, n], ˜ψavg Bounded Lagrange multiplier To ensure the Lagrange multiplier does not get too large so as to increase the magnitude of the extrinsic reward and destabilize learning, we use a bounded Lagrange multiplier [48] by applying Sigmoid activation on µ so the effective reward is a convex combination of the diversity and the extrinsic rewards: r(s, a) = σ(µi)re(s, a) + (1 − σ(µi))ri d(s, a), and the objective for µ is σ(µ)(αv∗ e − dπ · re). Average state occupancy The empirical feature averages used for experiments in the main text are good, though imperfect due to the bias from samples before the policy mixes. In our experiments, however, since the mixing time for the DM Control Suite is much shorter than the episode length T , the bias is small (∼ 5%). Discounted state occupancy For a more scalable solution, as mentioned in Section 2, we can instead predict successor features using an additional network head as shown in Fig. 11a. Similar to value learning, we use V-trace [19] targets for training successor features. In discounted state occupancy case we also use the extrinsic value function of each policy vi e (Fig. 1a) to estimate dπ · re, instead of the running average ˜vavg πi . We show experimental results for this setup in Fig. 11b. Loss functions. Instead of learning a single value head for the combined reward, our network has two value heads, one for diversity reward and one for extrinsic reward. We use V-trace [19] to compute td-errors and advantages for each of the value heads using the ”vtrace td error and advantage” function implemented here https://github.com/deepmind/ rlax/blob/master/rlax/_src/vtrace.py. The value loss for each head is the squared (cid:96)2 loss d + td2 of the td-errors, and the combined value loss for the network is the sum of these two losses: td2 e. In addition to that, our network has a policy head that is trained with a policy gradient loss as implemented in https://github.com/deepmind/rlax/blob/master/rlax/_src/policy_ gradients.py). When training the policy, we combine the intrinsic and extrinsic advantages δ = σ(µi)δe + (1 − σ(µi))δd (see the Weight cumulants function in Appendix A.3) which has the same effect as combining the reward. However, we found that having two value heads is more stable as each value can have a different scale. The ﬁnal loss of the agent is a weighted sum of the value loss the policy loss and the entropy regularization loss, and the weights can be found in Table 2. Algorithm 1 also returns a Lagrange loss function, designed to force the policies to achieve a value that is at least α times the value of the ﬁrst policy (which only maximizes extrinsic reward), where α is the optimally ratio (Table 3). We update the Lagrange multipliers µ using the optimizer speciﬁed in Table 3 but keep the multiplier of the ﬁrst policy ﬁxed µ1 = 1. Lastly, Algorithm 1 also updates the moving averages. 17 Algorithm 1: Loss function . πi i=1 (cid:111)n s|xj (cid:9)n i=1, (cid:110) ˜ψavg s) is the probability assigned to aj s), vd(xj j=1 of size T, τj = (cid:8)zj, xj s in state xi s)} ← Network({τj}m s), δe(xj Parameters: Network parameters θ, Lagrange multipliers µ, moving averages (cid:8)˜vavg πi Data: m trajectories {τj}m µ(aj Forward pass: {π(aj s), ve(xj Compute extrinsic td-errors and advantages: tde(xj extrinsic reward rj Compute intrinsic reward: ri Compute intrinsic td-errors and advantages: tdd(xj intrinsic reward rj Combine advantages: δ(xj Weighted loss: s and intrinsic critic vd(xj s) s) + (1 − σ(µi))δd(xj s) = σ(µi)δe(xj s) s and extrinsic critic ve(xj s) s) from ˜ψavg πz , z, φj d(xj s with Eq. (6) or (8) j=1) s) ← V-trace with s) ← V-trace with s), δd(xj s, µ(aj s)(cid:9)T s, φj s, rj s|xj s|xj s, aj s=1 , where s by the behaviour policy µ(a|x). bv(tde(xj s)2 + tdd(xj s)2) + bπ log(π(aj s|xj s))δ(xj s) + bEntEntropy(π(aj s|xj s)) (cid:88) s,j Lagrange loss: Update moving averages: n (cid:88) i=1 σ(µi)(˜vavg πi − α˜vavg π1 ) πi = α˜vavg ˜vavg d πi + (1 − α˜vavg ˜vavg d )rt, ˜ψavg ˜ψavg πi = α d ˜ψavg πi + (1 − α ˜ψavg d )φt return Weighted loss, Lagrange loss, (cid:8)˜vavg πi (cid:9)n i=1, (cid:110) ˜ψavg πi (cid:111)n i=1 A.3 Functions 18 1 def intrinsic_reward ( phi , sfs , latents , attractive_power =3. , repulsive_power =0. , attractive_coeff =0. , target_d =1.) : """""" Computes a diversity reward using successor features . Args : phi : features [ tbf ]. sfs : avg successor features [ lf ] or predicted , discounted successor features [ tbfl ]. latents : [ tbl ]. attractive_power : the power of the attractive force . repulsive_power : the power of the repulsive force . attractive_coeff : convex mixing of attractive & repulsive forces target_d (\ ell_0 ) : desired target distance between the sfs . When attractive_coeff =0.5 , target_d is the minimizer of the objective , i . e . , the gradient ( the reward ) is zero . Returns : intrinsic_reward . """""" # If sfs are predicted we have 2 extra leading dims . if jnp . ndim ( sfs ) == 4: sfs = jnp . swapaxes ( sfs , 2 , 3) of avg sf ) compute_dist_fn = jax . vmap ( jax . vmap ( compute_distances ) ) matmul_fn = lambda x , y : jnp . einsum ( ’tbl , tblf - > tbf ’ , x , y ) # tbfl -> tblf ( to match lf shape elif jnp . ndim ( sfs ) == 2: compute_dist_fn = compute_dist ances matmul_fn = jnp . matmul else : raise ValueError ( ’ Invalid shape for argument ‘sfs ‘. ’) l , f = sfs . shape [ -2:] # Computes an tb lxl matrix where each row , corresponding to a latent , is a 1 hot vector indicating the index of the latent with the closest sfs dists = compute_dist_fn ( sfs , sfs ) dists += jnp . eye ( l ) * jnp . max ( dists ) ne a r e s t_ l a t en ts_ ma tr ix = jax . nn . one_hot ( jnp . argmin ( dists , axis = -2) , num_classes = l ) # Computes a [ tbl ] vector with the nearest latent to each latent in latents nearest_latents = matmul_fn ( latents , ne ar es t_l at en ts_ ma tr ix ) # Compute psi_i - psi_j psi_diff = matmul_fn ( latents - nearest_latents , sfs ) norm_diff = jnp . sqrt ( jnp . sum ( jnp . square ( psi_diff ) , axis = -1) ) / # tbf target_d c = (1. - attractive_coeff ) * norm_diff ** repulsive_power c -= attractive_coeff * norm_diff ** attractive_power reward = c * jnp . sum ( phi * psi_diff , axis = -1) / f return reward 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def l2dist (x , y ) : 45 """""" Returns the L2 distance between a pair of inputs . """""" return jnp . sqrt ( jnp . sum ( jnp . square ( x - y ) ) ) 46 47 48 def c ompute_distances (x , y , dist_fn = l2dist ) : 49 """""" Returns the distance between each pair of the two collections of inputs . """""" 50 return jax . vmap ( jax . vmap ( dist_fn , ( None , 0) ) , (0 , None ) ) (x , y ) Listing 1: Intrinsic Reward 19 1 def weight_cumulants ( lagrange , latents , extrinsic_cumulants , 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 i ntr i nsi c_cum ula n ts ) : """""" Weights cumulants using the Lagrange multiplier . Args : lagrange : lagrange [ l ]. latents : latents [ tbl ]. e xtr in si c_c u mul ants : [ tb ]. i ntr in si c_c u mul ants : [ tb ]. Returns : extrinsic reward r_e and intrinsic_reward r_d . """""" sig_lagrange = jax . nn . sigmoid ( lagrange ) l ate n t_s ig_la gra n ge = jnp . matmul ( latents , sig_lagrange ) # No diversity rewards for latent 0 , only maximize extrinsic reward i ntr i nsi c_cum ula n ts *= (1 - latents [: , : , 0]) return (1 - la tent_s ig_ lagrang e ) * intrins ic_cumulants + l ate n t_s ig_la gra n ge * extr insic_c um ulants Listing 2: Weight cumulants # l # tb 1 def lagrangian ( lagrange , r , optimality_ratio ) : 2 """""" Loss function for the Lagrange multiplier . 3 4 5 6 7 8 9 Args : lagrange : lagrange [ l ]. r : moving averages of reward [ l ]. optimality_ratio : [1]. """""" l_ = jax . nn . sigmoid ( lagrange ) return jnp . sum ( l_ * ( r - r [0] * optimality_ratio ) ) Listing 3: lagrange loss function A.4 Motion ﬁgures Our ”motion ﬁgures” were created in the following manner. Given a trajectory of frames that composes a video f1, . . . , fT , we ﬁrst trim and sub sample the trajectory into a point of interest in time: fn, . . . , fn+m. We always use the same trimming across the same set of policies (the sub ﬁgures in a ﬁgure). We then sub sample frames from the trimmed sequence at frequency 1/p: fn, fn+p, fn+2p . . . ,. After that, we take the maximum over the sequence and present this ”max” image. In Python for example, this simply corresponds to n=400, m=30, p=3 indices = range(n, n+m, p) im = np.max(f[indices]) This creates the effect of motion in single ﬁgure since the object has higher values than the background. A.5 Hyperparameters The hyperparameters in Table 2 are shared across all environments except in the BiPedal Domain the learning rate is set to 10−5 and the learner frames are 5 × 107. We report the DOMiNO speciﬁc hyperparameters in Table 3. 20 Hyperparameter Replay capacity Learning rate Learner frames Discount factor bEnt Entropy regularization weight bπ Policy loss weight bv Value loss weight Replay batch size Online batch size Sequence length Optimizer Value 5 × 105 10−4 2 × 107 0.99 0.01 1.0 1.0 600 6 40 RMSprop Table 2: General hyperparameters Hyperparameter Control Suite BiPedal Walker α Optimality ratio Lagrange initialization Lagrange learning rate Lagrange optimizer πi decay factor α˜vavg ˜vavg d ˜ψavg ˜ψavg πi decay factor α d 0.9 0.5 10−3 Adam 0.9 0.99 0.7 0.5 10−3 Adam 0.999 0.9999 Table 3: DOMiNO hyperparameters B Additional Experiment Results B.1 Motion ﬁgures We now present motion ﬁgures, similar to Fig. 1b, but in other domains (see Fig. 6-9). The videos, associated with these ﬁgures can be found in a separate .zip ﬁle. Each ﬁgure presents ten polices discovered by DOMiNO and their associated rewards (in white text) with the repulsive objective and the optimality ratio set to 0.9. As we can see, the policies exhibit different gaits. Next to each ﬁgure, we also present the distances between the expected features of the discovered policies measured by the (cid:96)2 norm. In addition, in each row i we use a dark black frame to indicate the the index of the policy with the closest expected features to πi , i.e., in the i-th row we highlight the j-th column such that j = j∗ i = arg minj(cid:54)=i ||ψi − ψj||2 2. 21 Figure 5: QD in walker.stand. Left: 10 policies and corresponding rewards. Right: distances in (cid:96)2 norm between the Successor features of the policies. Figure 6: QD in walker.walk. Left: 10 policies and corresponding rewards. Right: distances in (cid:96)2 norm between the Successor features of the policies. Figure 7: QD in dog.stand. Left: 10 policies and corresponding rewards. Right: distances in (cid:96)2 norm between the Successor features of the policies. Figure 8: QD in dog.walk. Left: 10 policies and corresponding rewards. Right: distances in (cid:96)2 norm between the Successor features of the policies. 22 B.2 Additional Quality Diversity Results We now present additional experimental results evaluating the trade-off between quality and diversity using the scatter plots introduced in Section 4.1. y-axis shows the episode return while the diversity score, corresponding to the Hausdorff distance (Eq. (5)), is on the x-axis. The top-right corner of the diagram represents the most diverse and highest quality policies. Each ﬁgure presents a sweep over one or two hyperparameters and we use the color and a marker to indicate the values. In all of our scatter plots, we report 95% conﬁdence intervals, corresponding to 5 seeds, which are indicated by the crosses surrounding each point. Quality Diversity: walker.walk In Fig. 9 we show experimental results for DOMiNO in the walker.walk domain. Consistent with Fig. 2 which shows similar results on walker.stand, we show that our constraint mechanism is working as expected across different set sizes and optimality ratios across different tasks. Figure 9: QD Scaling results on walker.walk task. Left: Number of policies vs. optimality ratio in walker.walk with the repulsive reward and (center) with the VDW reward. Right: Optimality ratio vs. VDW target distance (cid:96)0. Quality Diversity: SMERL vs DOMiNO In Fig. 10 we show further experimental results in walker.stand for SMERL in comparison to DOMiNO. When SMERL is appropriately tuned (here for the 10 policies conﬁguration), it can ﬁnd some solutions along the upper-right QD border; however we ﬁnd that the best cd does not transfer to other conﬁgurations. The choice of cd that enables the agent to ﬁnd a set of 10 diverse policies produces sets without diversity for any other set size. Figure 10: Scaling SMERL (left) vs. DOMiNO (right) on Walker.Stand. Set size is indicated with marker, color corresponds to optimality ratio α. The cd for SMERL is set to 0.5, which was tuned using a set size of 10 policies (see 3, left). This choice does not scale well to any other set size, where regardless of optimality ratios, all policies only optimize for extrinsic reward, at the expense of diversity. Discounted State Occupancy We run the same experiments reported in Fig. 2 with DOMiNO’s Lagrangian method and report the results in Fig. 11b. As can be observed, using predicted discounted features does not make any signiﬁcant difference in performance. Since the mixing time for the DM Control Suite is much shorter than the episode length T , the bias in the empirical feature averages is small. 23 (a) (b) Figure 11: (a) DOMiNO with a discounted state occupancy. An additional network head is trained to predict successor features ψγ, which are used instead of the average features ψavg to compute the diversity reward. The discounted, extrinsic value is used as a constraint instead of the averaged rewards. Dashed lines signify training objectives. (b) Number of policies vs. optimality ratio in walker.stand with DOMiNO, consistent with Fig. 2. C Limitations Diversity increasing by decreasing α Inspecting Fig. 9, Fig. 11b and Fig. 2. we can observe that the diversity score increases for lower optimality ratios. Recall that the optimality ratio α speciﬁes a feasibility region in the state-occupancy space (the set of all α-optimal policies). Thus, the size of this space increases as α decreases, and we observe more diverse sets for smaller values of α. This intuition was correct in most of our experiments, but not always (e.g., Fig. 9). One possible explanation is that the Lagrange multipliers solution is seeking for the lowest value of λ that satisﬁes the constraint (so that we can get more diversity), i.e., it ﬁnds solutions that satisfy the constraint almost with equality: vi e ). The size of the level sets e ≥ αv∗ e ) do not necessarily increase with lower values of α (while the feasibility sets vi (vi e do). Another explanation is that in walker.walk (Fig. 9) it might be easier to ﬁnd diverse walking (e.g., α = 0.9) than diverse “half walking” (e.g., α = 0.5). This might be explained by “half walking” being less stable (it is harder to ﬁnd diverse modes for it). e (instead of vi e > αv∗ e ∼ αv∗ e = αv∗ Features Another possible limitation of our approach is that diversity is deﬁned via the environment features. We partially addressed this concern in Section 4.2 where we showed that it is possible to learn QD policies with our approach using the embedding of a NN as features. In future work we plan to scale our approach to higher dimensional domains and study which auxiliary losses should be added to learn good representations for diversity. D Additional K-shot experiments D.1 Control suite Next, we report additional results for K-shot adaptation in the control suite. In Fig. 12 we report the absolute values achieved by each method obtained (in the exact same setup as in Fig. 4). That is, we report rmethod for each method (instead of rmethod/rbaseline as in Fig. 4). Additionally, we report rbaseline, which is the ”Single policy baseline” (blue) in Fig. 12. Inspecting Fig. 12, we can see that all the methods deteriorate in performance as the magnitude of the perturbation increases. However, the performance of DOMiNO (orange) deteriorates slower than that of the other methods. We can also see that the performance of the no diversity baseline is similar when it learns 10 policies (red) and a 24 U p d a t e c o n s t r a i n t & L a g r a n g e MLP Torso MLP D i v e r s i t y R e w a r d single policy (blue), which indicates that when the algorithm maximize only the extrinsic reward, it ﬁnds the same policy again and again with each of the 10 policies. Figure 12: K-shot adaptation in Control Suite, similar to Figure 4, but reporting absolute rather than relative returns. Next, we inspect a wider range of hyper parameters for the SMERL and Multi-Objective methods. Concretely, for DOMiNO and SMERL α = 0.9, for SMERL cd ∈ [0.5, 1, 2, 5] and for Multi- Objective ce ∈ [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.9] and all methods are trained with 10 policies. These values correspond to the values that we considered in Fig. 3. Inspecting Fig. 13 we can see that the best methods overall are DOMiNO and SMERL (with cd = 1). We can also see that DOMiNO and SMERL consistently outperform the multi-objective baseline for many hyper parameter values. This is consistent with our results in Fig. 3 which suggest that the Multi-Objective method tends to be either diverse or high performing and fails to capture a good balance in between. Lastly, it is reasonable that SMERL and DOMiNO perform similar since they are both valid solutions to the same CMDP. However, SMERL comes an with additional hyper parameter cd that may be tricky to tune in some situations. For example, trying to tune cd based on the results in the vanilla domain (picking the upper-right most point in Fig. 3) led us to choose cd = 0.5 for SMERL, instead of 1. The Lagrange multipliers formulation in DOMiNO does not have this challenge as it does not have an extra hyper parameter. Figure 13: K-shot in Control Suite, similar to Figure 4, but reporting a wider range of hyper parameters for SMERL and Multi-Objective. D.2 BipedalWalker 2, h1 For the BipedalWalker environment, we either perturb the morphology or the terrain. To perturb the morphology, we follow [25] and specify a set of re-scaling factors. Speciﬁcally, each leg is made up of two rectangles, with pre-deﬁned width and height parameters: leg1 = ((w1 1)), leg2 = ((w1 2)). To generate a perturbed morphology, we deﬁne a scaling range [0, η] withing which we uniformly sample scaling factors (cid:96)j i ∼ [−η, η], for i = 1, 2 and j = 1, 2. A perturbed environment is deﬁned by re-scaling the default parameters: (cid:102)leg1 = (((1 + (cid:96)1 1, (1 + ν1 2 )h2 1 )h2 1 )h1 1))). The values for this perturbations can be found in Table 4. 1))), and (cid:102)leg2 = (((1+(cid:96)1 1)w1 1, (1+ν2 1), ((1+(cid:96)2 1), ((1+(cid:96)2 1, (1+ν2 1, (1+ν1 1), (w2 2), (w2 1)w2 2)w2 2)w1 i , νj 1, h2 1, h1 2, h2 2 )h1 25 s n r u t e r n a e M s n r u t e r n a e M 1000 d n a t S 400 1000 l k a W 200 200 200 0 0 1 0 2 0 3 200 0 4 0 . 2 5 2 Motor failure (duration) 200 0 . 4 0 . 3 5 7 0 . 3 0 . 3 0 . 2 Thigh length 600 400 600 200 5 0 . 4 0 . 4 0 . 3 Torso length 5 0 . 5 0 . 1 3 . 0 2 . 0 1 . 0 Joint damping 4 . 0 0 . 7 2 0 . 1 0 . 0 Contact friction 0 . 0 0 Single policy baseline DOMiNO SMERL No diversity Multi-Objective 5 1 0 0 . 0 1000 s n r u t e r n a e M d n a t S 200 1000 s n r u t e r n a e M l k a W 200 200 200 400 600 200 200 200 200 0 0 1 0 2 0 3 0 4 0 . 2 5 2 7 0 . 3 0 . 2 5 0 . 3 0 . 4 0 . 3 5 0 . 3 0 . 4 5 0 . 4 0 . 5 0 . 1 1 . 0 2 . 0 3 . 0 4 . 0 0 . 7 0 . 1 2 0 . 0 5 0 0 . 0 1 0 0 . 0 Motor failure (duration) Thigh length Torso length Joint damping Contact friction Single policy baseline DOMiNO Multi Objective (0.1) Multi Objective (0.2) Multi Objective (0.3) Multi Objective (0.4) Multi Objective (0.5) Multi Objective (0.6) Multi Objective (0.9) Multi Objective (0.7) SMERL ( =1.0) SMERL ( =2.0) SMERL ( =5.0) SMERL ( =0.5) Perturbation type Perturbation scale parameter values (η) Morphology Stumps (height, width) Pits (width) Stairs (height, width) 0., 0.10, 0.15, 0.20, 0.25, 0.30, 0.35 0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. 0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. 0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. Table 4: Bipedal perturbation scale values For terrain changes, we selectively enable one of three available obstacles available in the OpenAI Gym implementation: stumps, pits, or stairs. For each obstacle, we specify a perturbation interval [0, η]. This interval determines the upper bounds on the obstacles height and width when the environment generates terrain for an episode. For details see the “Hardcore” implementation of the BiPedal environment. Note that for stairs, we ﬁxed the upper bound on the number of steps the environment can generate in one go to 5. To evaluate adaptation, we ﬁrst train 10 agents independently on the “BiPedalwalker-v3” environment, which only uses a ﬂat terrain. To evaluate the trained agents we sample random perturbations of the environment. Speciﬁcally, for each type of perturbation (morphology, pits, stumps, stairs) and for each value of the scale parameter η, we randomly sample 30 perturbations. We then run each option for 40 episodes; adaptation takes the form of using the ﬁrst 10 episodes to estimate the option with highest episode return, which is then used for evaluation on the remaining 30 episodes. Figure 14: K-shot adaptation in BiPedal walker Fig. 14 shows that, while performance degrades as the morphology is deformed, DOMiNO exhibits greater adaptability as evidenced by less severe degradation of performance. In terms of morphology, we ﬁnd a gradual decline in performance as we increase the degree of deformation. Similar to the Control Suite, diversity is beneﬁcial and helps the agent adapt while not being impervious to these changes. In terms of terrain perturbations, these have a more abrupt impact on the agent’s performance. While diversity does not prevent a signiﬁcant drop in performance, it is still beneﬁcial when adapting to stumps and pits and does not negatively impact performance in the case of stairs. E Computing Infrastructure We run our experiments using a distributed infrastructure implemented in JAX [12]. Each run took approximately 10 hours to complete. The computing infrastructure is based on an actor-learner decomposition [19], where multiple actors generate experience in parallel, and this experience is channelled into a learner. It allows us to run experiments in two modalities. In the ﬁrst modality, following [19], the actors programs are distributed across multiple CPU machines, and both the stepping of environments and the network inference happens on CPU. The data generated by the actor programs is processed in 26 400 r e k a W l l No diversity With diversity a d e P B i -100 0.0 0.1 0.2 0.3 Morphology scale 0.0 0.3 0.6 0.9 Stump size 400 r e k a W l l a d e P B i -100 0.0 0.3 0.6 0.9 Pit size 0.0 0.3 0.6 0.9 Stairs step size batches by a single learner using a GPU. Alternatively, both the actors and learners are co-located on a single machine, where the host is equipped with 56 CPU cores and connected to 8 TPU cores [30]. To minimize the effect of Python’s Global Interpreter Lock, each actor-thread interacts with a batched environment; this is exposed to Python as a single special environment that takes a batch of actions and returns a batch of observations, but that behind the scenes steps each environment in the batch in C++. The actor threads share 2 of the 8 TPU cores (to perform inference on the network), and send batches of ﬁxed size trajectories of length T to a queue. The learner threads takes these batches of trajectories and splits them across the remaining 6 TPU cores for computing the parameter update (these are averaged with an all reduce across the participating cores). Updated parameters are sent to the actor’s TPU cores via a fast device to device channel, as soon as the new parameters are available. This minimal unit can be replicates across multiple hosts, each connected to its own 56 CPU cores and 8 TPU cores, in which case the learner updates are synced and averaged across all cores (again via fast device to device communication). 27"
98,Avoid Overfitting User Specific Information in Federated Keyword   Spotting,"[{'href': 'http://arxiv.org/abs/2206.08864v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2206.08864v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-06-17 16:05:35,"Avoid Overﬁtting User Speciﬁc Information in Federated Keyword Spotting Xin-Chun Li1, Jin-Lin Tang1, Shaoming Song2, Bingshuai Li2, Yinchuan Li2, Yunfeng Shao2, Le Gan1, De-Chuan Zhan1 1State Key Laboratory for Novel Software Technology, Nanjing University 2Huawei Noah’s Ark Lab {lixc, tangjl}@lamda.nju.edu.cn, ganle@nju.edu.cn, zhandc@nju.edu.cn, {shaoming.song, libingshuai, liyinchuan, shaoyunfeng}@huawei.com 2 2 0 2 n u J 7 1 ] G L . s c [ 1 v 4 6 8 8 0 . 6 0 2 2 : v i X r a Abstract Keyword spotting (KWS) aims to discriminate a speciﬁc wake- up word from other signals precisely and efﬁciently for different users. Recent works utilize various deep networks to train KWS models with all users’ speech data centralized without consid- ering data privacy. Federated KWS (FedKWS) could serve as a solution without directly sharing users’ data. However, the small amount of data, different user habits, and various ac- cents could lead to fatal problems, e.g., overﬁtting or weight divergence. Hence, we propose several strategies to encourage the model not to overﬁt user-speciﬁc information in FedKWS. Speciﬁcally, we ﬁrst propose an adversarial learning strategy, which updates the downloaded global model against an overﬁt- ted local model and explicitly encourages the global model to capture user-invariant information. Furthermore, we propose an adaptive local training strategy, letting clients with more train- ing data and more uniform class distributions undertake more local update steps. Equivalently, this strategy could weaken the negative impacts of those users whose data is less quali- ﬁed. Our proposed FedKWS-UI could explicitly and implicitly learn user-invariant information in FedKWS. Abundant exper- imental results on federated Google Speech Commands verify the effectiveness of FedKWS-UI. Index Terms: keyword spotting, federated learning, data het- erogeneity, user-invariant 1. Introduction Deep learning has been successfully applied to automatic speech recognition (ASR) [1, 2], facilitating the emergence of intelligent voice assistants (e.g., Amazon Alexa). To wake up the smart assistant, some predeﬁned keywords (e.g., “Alexa”) need to be identiﬁed precisely from users’ speech recordings, i.e., keyword spotting (KWS) [3, 4]. This identiﬁcation process must be efﬁcient to complete, and the utilized models should have minimal memory footprint. Furthermore, the KWS pro- cess should be robust to users with various accents or preferred spoken words. Recent works utilize various deep networks for KWS [4, 5, 6, 7, 8]. These methods take a data centralized training style based on the publicly available benchmark such as Google Speech Commands [9]. However, there may be signiﬁcant pri- vacy implications in sharing users’ audio recordings, which re- quires a data decentralized training style for privacy protec- Supported by National Natural Science Foundation of China (Grant No. 41901270), NSFC-NRF Joint Research Project under Grant 61861146001, and Natural Science Foundation of Jiangsu Province (Grant No. BK20190296). Thanks to Huawei Noah’s Ark Lab Net- MIND Research Team for funding this research. De-Chuan Zhan is the corresponding author. Email: zhandc@nju.edu.cn tion. Federated learning (FL) [10, 11] has been effectively ap- plied for communication efﬁcient decentralized training with basic privacy protection. Although FL could be directly ap- plied to decentralized KWS training, the non-independent and identically distributed data (non-i.i.d. data) poses many chal- lenges [12, 13]. Non-i.i.d. in KWS refers to the fact that some users only own a small amount of data (i.e., quantity skew), users tend to use different spoken words (i.e., label distribution skew), and users usually have accents (i.e., feature distribution skew). This paper investigates FedKWS on Google Speech Com- mands [9] with several popular network architectures. Com- pared with centralized training, we observe a signiﬁcant per- formance degradation in FedKWS due to non-i.i.d. data. In fact, the small amount of data and the distribution skew problem make the downloaded global model easily overﬁt user-speciﬁc information. For example, the feature extractor mistakenly takes a user’s accent as an important factor, or the classiﬁca- tion layer is biased towards a user’s commonly spoken words. To solve these challenges and enhance the generalization per- formance of the federated model, we propose several strategies to avoid the local model overﬁtting user-speciﬁc information. 2. Related Works Our work is closely related to keyword spotting (KWS) [4, 3] and federated learning (FL) [10, 11, 14]. Current works for- mulate KWS as a classiﬁcation problem, aiming to identify whether a short speech recording is a speciﬁc word, silence, or unknown. Considering the success of deep learning, CNN has been applied to KWS [4]. Depth-separable CNN (DSCNN) [5] is applied to obtain the goal of small footprint memory, and residual network (ResNet) [7] is utilized to enhance perfor- mances. Recurrent neural networks with multi-head atten- tion (MHAttRNN) [6, 8] and varieties of transformers (Trans- former) [8, 15] have also been applied to KWS and obtain SOTA results. Some other advanced techniques in deep learning have also been veriﬁed helpful in KWS [16]. FL has also been ap- plied to KWS for decentralized training [17, 18]. [17] conducts extensive experiments of FedAvg [10] on “Hey Snips” dataset and uses an adaptive averaging strategy for global model aggre- gation as done in [19]. The work [18] investigates data aug- mentation and distillation in FedKWS for overcoming resource constraints and example labeling. FL studies have also been presented in ASR [20, 21, 22]. Compared with these studies, we primarily focus on the non-i.i.d. data challenge in FedKWS and propose a novel method to focus on extracting user-invariant in- formation. We investigate our methods with various network architectures and show that our approach is universal. 3. Background of Federated Learning (cid:80) k∈St ˆψk (cid:80)K FedAvg [10]: Suppose we have K clients and each client owns a data distribution Dk = P k(x, y), k ∈ [K]. FL aims to op- k=1 pkL(Dk; ψ), where ψ denotes the global timize minψ parameters, pk denotes the weight of each client. FedAvg [10] solves this problem via multiple communication rounds of lo- cal and global procedures. During local procedure, a partial set of clients St download the global model ψt and update it on their local data for multiple steps. During global proce- dure, the server collects these updated local models (denoted as ˆψk t , k ∈ St) and aggregates them via parameter averaging, i.e., ψt+1 ← 1 t . t denotes the communication round. |St| These two procedures will iterate T rounds until convergence. Non-I.I.D. Data: The users’ data in FL are often naturally heterogeneous, e.g., the speech data in Google Speech Com- mands [9] are collected from users with various accents. As de- clared in [12], the local update direction will diverge a lot from the global one due to non-i.i.d. data, making the model aggre- gation inaccurate. FedOpt [19] utilizes an adaptive optimization strategy on the server instead of a simple parameter averaging. FedRS [23] speciﬁes the challenge of label shift across clients and proposes restricted softmax as the solution. FedProx [13] and FedMMD [24] add regularization to prevent local models from being updated too away, which could decrease the weight divergence for better aggregation. Although some FL methods (e.g., FedProx [13], FedDyn [25], MOON [26]) could also elab- orate a regularization effect during local procedures, they only stay on the parameter or the intermediate feature levels. By contrast, we adversarially update the global model against an overﬁtted local model and regularize the local procedure on the functional level. Furthermore, we design an adaptive local train- ing procedure from the system scheduling level. 4. Proposed Methods This section proposes two strategies to prevent the global model from overﬁtting user-speciﬁc information (e.g., accents or fa- vorite spoken words) in FedKWS. Adversarial Learning against Overﬁtted models (ALO): Clients update the downloaded global model on their data dur- ing the local procedure, which could overﬁt some user-speciﬁc information. Speciﬁcally, the local data distribution P k(x, y) may diverge signiﬁcantly from the global data distribution. Ac- cording to some previous works [27, 28, 29], the lower/higher layers of a neural network tend to be inﬂuenced signiﬁcantly by feature/label distribution skew, i.e., various P k(x) or P k(y). FedKWS simultaneously faces these two kinds of distribution skew (e.g., accents and favorite spoken words), making the complete model biased towards a speciﬁc user during the lo- cal procedure. Hence, we must regularize the local training from the functional perspective instead of focusing on speciﬁc neural network layers. We resort to private-shared models and adversarially update the global model (shared among users) against overﬁtted local models (private for each user). Private- shared models are utilized in some recent FL solutions [30, 31]. Speciﬁcally, we build private models ψk p , k ∈ [K] for each client. We ﬁrst train private models with the cross-entropy loss L(Dk; ψk c=1 I{yi = c} log[fp(xi)]c], where I{·} is the indicator function and fp(·) is the prediction function based on private model ψk p that outputs a probability distribution. After abundant training steps, we expect this pri- vate model to overﬁt user-speciﬁc data information. Then, we p ) = Exi,yi∼Dk [− (cid:80)C Figure 1: Left: data heterogeneity in federated Google Speech Commands. We only plot 20 clients (users) in task 35. Right: number of samples and class distribution entropy of each client (user) in task 12 and 35 (each point shows a client). train the global model with the following loss: (cid:34) Lls = Exi,yi − C (cid:88) [(1 − µ)I{yi = c} + µ/C] log[f (xi)]c (cid:35) , c=1 Ladv = − (cid:124)(cid:123)(cid:122)(cid:125) negative Exi,yi (cid:34) − C (cid:88) [fp(xi)]c log[f (xi)]c (cid:35) , c=1 L(Dk; ψk) = Lls(Dk; ψk) + λLadv(Dk; ψk), (1) (2) (3) where we omit the communication round index t and some other symbols for simpliﬁcation. fp(·) represents the func- tion of the overﬁtted private model while f (·) for the down- loaded global model. Eq. (2) could be seen as “negative distil- lation”, which could push the global model’s prediction f (xi) away from overﬁtted areas. Eq. (2) follows the formula of dis- tillation [32, 33, 34] but works signiﬁcantly different. Label smoothing in Eq. (1) could also regularize the global model not be too over-conﬁdent on a speciﬁc user’s data. We investigate the hyperparameters of µ and λ in ablation studies. Adaptive Local Training (ALT): Due to data heterogeneity, both amount imbalance and class imbalance could occur in clients’ data. The former implies that different clients may own various numbers of training samples. The second one refers to that label distributions may diverge across clients. These two types of imbalance on Google Speech Commands [9] are shown in Figure 1. Intuitively, few training samples could lead to overﬁtting, and imbalanced data could bias the model towards identifying a user’s favorite words. Hence, we en- courage clients who own more training data and more uni- form class distributions to undertake more local updates. For- mally, in FedAvg [10], every selected client uniformly takes E local training steps without considering the data quality. Assume the kth client owns nk training samples and the class distribution is qk ∈ RC with (cid:80)C c=1 qk,c = 1 and qk,c ≥ 0, ∀c. C is the total number of classes. We cal- culate the normalized amount of training samples as nk = nk/ maxK j=1 nj ∈ [0, 1], and the normalized class entropy as Statistics (C=12) 2 0 8 1 1 0 0 6 0 0 4 0 1 4 1 Number of Samples Statistics (C=35) 0 0 5 5 0 0 0 0 2 2 1 1 Client-Class Distribution (C=35) 0 5 1 5 1 0 2 0 0 2 y p o r t n E n o i t u b i r t s i D s s a C l y p o r t n E n o i t u b i r t s i D s s a C l 2.49 2.0 1.5 1.0 0 0 5 3.56 3.0 2.5 2.0 1.5 1.0 1 0 5 10 15 20 25 30 34 x e d n I s s a C l Sampled 20 Clients Number of Samples 3.49 2.67 1.85 1.03 0.21 4.70 3.56 2.43 1.29 0.15 6 1 3 Table 1: Detail information of federated Speech Commands. Table 3: Comparisons on FA and FR rate. The lower the better. C 12 35 K 2,234 2,434 N 45.6k 105.5k Avg.nk Max.nk 20.4 43.3 141 316 M 4.9k 11.0k FedAvg [10, 18] 0.27 5.03 FedOpt [19, 17] 0.32 3.19 FedKWS-UI 0.23 2.78 FA FR Table 2: Detail of networks and centralized training results. Num.of.Params Centralized Acc. C = 12 C = 35 C = 12 C = 35 96.95 169K 97.05 228K 97.31 238K 97.14 232K 97.19 97.31 97.89 96.21 173K 232K 239K 234K DSCNN [5] MHAttRNN [6] ResNet [7] Transformer [8] ek = (− (cid:80)C c=1 qk,c log qk,c)/ log C ∈ [0, 1]. Then we calcu- late the harmonic mean of nk and ek, i.e., rk = 2nkek/(nk + ek). We use rk ∈ [0, 1] to measure clients’ utility in FL, and we heuristically let clients with larger rk contribute more to FL. That is, we reallocate the computation resources among clients via allowing the kth client take on r0 ∗ rk ∗ E gradient steps, where the determination of r0 should satisfy (cid:80)K k=1 r0∗rk∗E ≈ K ∗ E for conservation. Easily, r0 = K/ (cid:80)K k=1 rk. Although the computation ability of clients should also be considered, we focus on non-i.i.d. data in this work and leave it as future work. 5. Experiments Datasets: We name the proposed method as “Federated KWS with User-Invariant information” (FedKWS-UI), and investi- gate it on Google Speech Commands [9]1 (recommended by FedScale [35]) to identify whether a 1s-long speech recording is a word, silence, or unknown. The benchmark contains two tasks with 12 classes (10 words, silence, unknown) and 35 classes (35 words). The two tasks contain 2,234 and 2,434 users. We split the data into corresponding clients with each user as a client. The number of total training samples (N ), the training samples of each client on average (Avg.nk), the number of test samples (M ) are listed in Table 1. The class distributions of randomly selected 20 clients in task C = 35 are shown in left of Fig- ure 1. Larger circles correspond to more samples. The train and test data is split via the provided lists in Google Speech Commands. We extract 40 MFCC features for each 30ms win- dow frame with a stride of 10ms. We also follow the settings in Google Speech Commands: performing random time-shift of Y ∼ [−100, 100] milliseconds and adding 0.1 volume back- ground noise with a probability of 0.8. Networks and Centralized Training: We investigate vari- ous network architectures and moderately modify them to keep nearly the same number of parameters. We use DSCNN [5] with 172 channels, MHAttRNN [6, 8] with 4 heads and 80 hid- den neurons, ResNet [7] with 15 layers and 45 channels in each basic block, Transformer [8] with 4 layers and a model dimen- sion of 96. We ﬁrst use these networks for centralized train- ing. For DSCNN, MHAttRNN and ResNet, we utilize SGD optimizer with momentum 0.9, and we vary the learning rate in {0.1, 0.05, 0.03, 0.01} and select the best result. For Trans- former, we utilize AdamW optimizer and vary learning rate in 1https://pytorch.org/audio/stable/datasets.html {0.005, 0.002, 0.0008}. We set batch size as 128. The number of network parameters and the accuracies on test data are shown in Table 2. We do not obtain SOTA results via Transformer be- cause we only use 4 layers with 0.23M parameters while [8] uses a network with up to 5.4M parameters. FedKWS: For FedKWS, we split the training data in Google Speech Commands via the provided user IDs. The test data is still used to evaluate the generalization ability of the aggre- gated model. We plot the statistics of clients’ number of sam- ples and class distribution entropy at the right of Figure 1. We compare FedKWS-UI with FedAvg [10] (used in [18]), Fed- Prox [13], FedMMD [24], FedOpt [19] (used in [17]). For all methods, we use a batch size of 32, local training steps E = 50 and run 300 rounds. We also vary the learning rate as aforementioned and take the best one for comparison. Addi- tionally, for FedProx and FedMMD, we vary the regularization coefﬁcient in {0.0001, 0.001, 0.01}. For FedOpt, we vary the global optimizer in {SGD, Adam} and the global learning rate in {1.0, 0.1} and {0.001, 0.0001}, respectively. For FedKWS- UI, we use r0 = 3.5, 5.0 for C = 12, 35, and show r0 ∗ rk values of all clients via the shades of color at the right of Fig- ure 1. The max and min values are shown at the color bar, and the top-right points (clients) tend to have larger r0 ∗ rk. We uti- lize µ = 0.2 and λ = 0.001 in FedKWS-UI (Eq. (1), Eq. (3)). We record the accuracy on the global test set every 3 rounds and plot the convergence curves in Figure 2. First, we can clearly observe that the decentralized performances drop a lot compared with centralized training. For example, all of the compared methods could only obtain accuracy as high as 86.39 on task 12, far away from the centralized training (97.89). Then, comparing the network architectures, we could ﬁnd that MHAt- tRNN tends to obtain higher performances while Transformer performs worst. We guess that MHAttRNN could be more ro- bust to the random time-shift because it directly computes the sequential information, and the attention mechanism could pre- cisely capture the important signals. Furthermore, FedProx and FedMMD add regularization during local training procedures on the parameter and intermediate features, which perform not so well. Overall, FedKWS-UI could lead to better results on all of these architectures, particularly on ResNet and Trans- former, verifying the versatility of our methods. Signiﬁcantly, FedKWS-UI surpasses all compared methods by a large margin on task 12 with DSCNN, ResNet, and Transformer. For exam- ple, FedKWS-UI could boost the Transformer performances on task 12 from 72.71 to 79.06. We also evaluate the false accept (FA) and false reject (FR) rate as done in [17, 18]. In task 12, we take the 10 words as positive classes and average their FA rates, while the silence and unknown as negative classes. We do not adjust the prediction conﬁdence threshold to control the FA and directly report FA and FR with the predictions. We use DSCNN and calculate the FA and FR rate of the ﬁnal aggregated model. We show the results in Table 3. We ﬁnd that FedKWS-UI could obtain fewer false alarms/rejections. Ablation Studies: We investigate the effects of components in FedKWS-UI. First, we set µ = 0 and λ = 0 to omit the part of adversarial learning against overﬁtted models (ALO) Figure 2: Comparison results on federated Google Speech Commands. Rows show the results on task 12 and 35, and columns show results of four utilized networks. The legends also show the average accuracy of the ﬁnal 5 communication rounds. sults on task 35 to further show the plausibility and advantage of the proposed strategies. Speciﬁcally, we ﬁrst train a well- performed KWS model (θ0) on the centralized training set (test accuracy up to 93.0%). Then, we respectively update θ0 on an inferior and qualiﬁed user’s data for 20 epochs. The infe- rior user owns only 100 samples and the classes are imbalanced (i.e., the bottom-left user shown in the right part of Figure 1), while the qualiﬁed user owns about 250 samples and the classes are more balanced (i.e., the top-right user shown in the right part of Figure 1). The updated models are denoted as θ1 and θ2. Then, we plot the performance landscape of the interpolation θ0 + γ1(θ1 − θ0) + γ2(θ2 − θ0) within the grid space where γ1 ∈ [−0.1, 1.1], γ2 ∈ [−0.1, 1.1]. The left part of Figure 4 shows that updating θ0 on the qualiﬁed user’s data keeps the generalization ability of the global model while the result on the inferior user’s data becomes worse. Hence, it is rational that our proposed ALT encourages qualiﬁed users to contribute more to FedKWS. Additionally, for the inferior user, we utilize the pro- posed ALO to train another model ˆθ1 against the overﬁtted θ1, and ˆθ1 performs better as shown on the right of Figure 4. The interpolation landscape along the inferior user’s data becomes smoother with ALO, beneﬁting the model aggregation proce- dure in FL. This veriﬁes the advantage of the proposed ALO. Overall, FedKWS-UI could enhance the generalization ability of the federated model even with few or skewed samples. 6. Conclusion We investigate popular networks for FedKWS, where the data heterogeneity leads to signiﬁcant performance degradation compared with centralized training. We propose to learn user- invariant information via adversarial learning against overﬁtted local models and a computation re-allocation strategy named adaptive local training. These two strategies could avoid overﬁt- ting user-speciﬁc information during local training and facilitate model aggregation. Experimental results verify the superiori- ties of our proposed FedKWS-UI. Future works will extend this work to streaming KWS [6] and utilize differential privacy [36] to satisfy stricter privacy requirements. Figure 3: Ablation studies of only using ALT (left) and the hyper-parameters in ALO (µ (middle), and λ (right)). and only use adaptive local training (ALT). We record the re- sults using four networks on task 35 at the left of Figure 3. We ﬁnd that only using adaptive local training could still per- form well on MHAttRNN and Transformer, while it works worse on DSCNN and especially on ResNet. Hence, it is still necessary to utilize the adversarial learning to improve perfor- mances further. Then, we vary µ ∈ {0.0, 0.1, 0.2, 0.3, 0.5} and λ ∈ {0.0, 0.0001, 0.001, 0.01, 0.1} correspondingly, studying the effects of label smoothing and adversarial loss in ALO. We investigate the task C = 12 with DSCNN and ResNet. The results are shown at the middle and right of Figure 3. Utilizing label smoothing could almost lead to better performances, and setting µ around 0.2 is a better choice. Similarly, λ = 0.001 is recommended for the proposed adversarial loss, and a larger λ (e.g., 0.1) could be harmful. Figure 4: Visualization of the plausibility and advantage of the proposed strategies. Visualization Analysis: We then present some visualization re- y c a r u c c A l a b o G l 0.8 0.6 0.4 0.2 0.8 0.6 0.4 y c a r u c c A l a b o G l C=12, DSCNN C=12, MHAttRNN C=12, ResNet C=12, Transformer 0.8 0.7 0.6 0.5 FedAvg:82.85 FedProx:85.51 FedMMD:83.74 FedOpt:85.50 FedKWS-UI:87.29 0.8 0.6 0.4 FedAvg:86.20 FedProx:86.39 FedMMD:85.92 FedOpt:86.38 FedKWS-UI:86.57 0.8 0.6 0.4 0.2 FedAvg:83.48 FedProx:83.93 FedMMD:83.90 FedOpt:81.30 FedKWS-UI:86.23 FedAvg:71.17 FedProx:72.15 FedMMD:72.71 FedOpt:71.12 FedKWS-UI:79.06 3 60 120 180 240 300 3 60 120 180 240 300 3 60 120 180 240 300 3 60 120 180 240 300 C=35, DSCNN C=35, MHAttRNN C=35, ResNet C=35, Transformer 0.8 0.6 0.4 FedAvg:82.59 FedProx:81.44 FedMMD:82.79 FedOpt:84.23 FedKWS-UI:84.51 0.8 0.6 0.4 0.2 FedAvg:79.80 FedProx:83.10 FedMMD:83.64 FedOpt:82.83 FedKWS-UI:84.60 0.6 0.4 0.2 FedAvg:79.05 FedProx:75.78 FedMMD:80.07 FedOpt:79.33 FedKWS-UI:80.50 FedAvg:70.24 FedProx:70.07 FedMMD:66.78 FedOpt:54.56 FedKWS-UI:74.75 3 120 60 Communication Round 180 240 300 3 120 60 Communication Round 180 240 300 3 120 60 Communication Round 180 240 300 3 120 60 Communication Round 180 240 300 0.8 0.6 0.4 0.2 y c a r u c c A l a b o G l Ablation of µ=0.0, λ = 0.0 (C=35) FedKWS-UI: DSCNN:82.27 FedKWS-UI: MHAttRNN:84.73 FedKWS-UI: ResNet:74.10 FedKWS-UI: Transformer:70.57 0.9 0.8 0.7 0.6 0.5 0.4 0.3 Ablation of µ (C=12, DSCNN) Ablation of λ (C=12, ResNet) 0.8 0.6 0.4 0.2 FedKWS-UI: µ=0.0:86.55 FedKWS-UI: µ=0.1:87.24 FedKWS-UI: µ=0.2:87.29 FedKWS-UI: µ=0.3:87.21 FedKWS-UI: µ=0.5:86.96 FedKWS-UI: λ=0.0:85.29 FedKWS-UI: λ=1e-4:86.07 FedKWS-UI: λ=1e-3:86.23 FedKWS-UI: λ=1e-2:82.79 FedKWS-UI: λ=1e-1:76.99 0 60 120 180 240 300 0 60 120 180 240 300 0 60 120 180 240 300 Communication Round Communication Round Communication Round 1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 ) 2 γ ( r e s U d e ﬁ i l a u Q g n o A l -0.1 - 0 . 1 0 . 0 Without ALO θ2 θ0 0 . 4 0 . 3 0 . 2 0 . 1 0 . 9 0 . 5 Along Inferior User (γ1) 0 . 6 0 . 8 0 . 7 0.90 0.75 0.60 0.45 0.30 0.15 1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 - 0 . 1 0 . 0 θ1 1 . 0 1 . 1 With ALO θ2 θ0 0 . 4 0 . 3 0 . 2 0 . 1 0 . 9 0 . 5 Along Inferior User (γ1) 0 . 6 0 . 8 0 . 7 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 ) y c a r u c c A t s e T ( n o i t a z i l a r e n e G ˆθ1 1 . 0 1 . 1 [23] X. Li and D. Zhan, “FedRS: Federated learning with restricted softmax for label distribution non-iid data,” in KDD, 2021, pp. 995–1005. [24] X. Yao, C. Huang, and L. Sun, “Two-stream federated learning: Reduce the communication costs,” in VCIP, 2018, pp. 1–4. [25] D. A. E. Acar, Y. Zhao, R. M. Navarro, M. Mattina, P. N. What- mough, and V. Saligrama, “Federated learning based on dynamic regularization,” in ICLR, 2021. [26] Q. Li, B. He, and D. Song, “Model-contrastive federated learn- ing,” in CVPR, 2021, pp. 10 713–10 722. [27] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, “How transfer- able are features in deep neural networks?” in NeurIPS, 2014, pp. 3320–3328. [28] L. Collins, H. Hassani, A. Mokhtari, and S. Shakkottai, “Exploit- ing shared representations for personalized federated learning,” in ICML, 2021, pp. 2089–2099. [29] P. P. Liang, T. Liu, Z. Liu, R. Salakhutdinov, and L. Morency, “Think locally, act globally: Federated learning with local and global representations,” CoRR, vol. abs/2001.01523, 2020. [30] X. Li, D. Zhan, Y. Shao, B. Li, and S. Song, “FedPHP: Federated personalization with inherited private models,” in ECML/PKDD, 2021, pp. 587–602. [31] X. Li, L. Gan, D. Zhan, Y. Shao, B. Li, and S. Song, “Aggregate or not? exploring where to privatize in DNN based federated learn- ing under different non-iid scenes,” CoRR, vol. abs/2107.11954, 2021. [32] G. E. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” CoRR, vol. abs/1503.02531, 2015. [33] T. Shen, J. Zhang, X. Jia, F. Zhang, G. Huang, P. Zhou, F. Wu, and C. Wu, “Federated mutual learning,” CoRR, vol. abs/2006.16765, 2020. [34] C. He, M. Annavaram, and S. Avestimehr, “Group knowledge transfer: Federated learning of large cnns at the edge,” in NeurIPS, 2020. [35] F. Lai, Y. Dai, X. Zhu, H. V. Madhyastha, and M. Chowdhury, “Fedscale: Benchmarking model and system performance of fed- erated learning,” in ResilientFL, 2021, pp. 1–3. [36] M. Abadi, A. Chu, I. J. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang, “Deep learning with differential pri- vacy,” in CCS, 2016, pp. 308–318. 7. References [1] W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals, “Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,” in ICASSP, 2016, pp. 4960–4964. [2] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, and G. Zweig, “The microsoft 2016 conversational speech recognition system,” in ICASSP, 2017, pp. 5255–5259. [3] G. Chen, C. Parada, and G. Heigold, “Small-footprint keyword spotting using deep neural networks,” in ICASSP, 2014, pp. 4087– 4091. [4] T. N. Sainath and C. Parada, “Convolutional neural networks for small-footprint keyword spotting,” in INTERSPEECH, 2015, pp. 1478–1482. [5] Y. Zhang, N. Suda, L. Lai, and V. Chandra, “Hello edge: Keyword spotting on microcontrollers,” CoRR, vol. abs/1711.07128, 2017. [6] O. Rybakov, N. Kononenko, N. Subrahmanya, M. Visontai, and S. Laurenzo, “Streaming keyword spotting on mobile devices,” in INTERSPEECH, 2020, pp. 2277–2281. [7] R. Tang and J. Lin, “Deep residual learning for small-footprint keyword spotting,” in ICASSP, 2018, pp. 5484–5488. [8] A. Berg, M. O’Connor, and M. T. Cruz, “Keyword trans- former: A self-attention model for keyword spotting,” CoRR, vol. abs/2104.00769, 2021. [9] P. Warden, “Speech commands: A dataset for limited-vocabulary speech recognition,” CoRR, vol. abs/1804.03209, 2018. [10] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Ar- cas, “Communication-efﬁcient learning of deep networks from decentralized data,” in AISTATS, 2017, pp. 1273–1282. [11] Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine learn- ing: Concept and applications,” ACM TIST, vol. 10, no. 2, pp. 12:1–12:19, 2019. [12] Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, “Fed- erated learning with non-iid data,” CoRR, vol. abs/1806.00582, 2018. [13] T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith, “Federated optimization in heterogeneous networks,” in MLSys, 2020. [14] X. Li, Y. Xu, S. Song, B. Li, Y. Li, Y. Shao, and D. Zhan, “Federated learning with position-aware neurons,” CoRR, vol. abs/2203.14666, 2022. [15] Y. Gong, Y. Chung, and J. R. Glass, “AST: audio spectrogram transformer,” CoRR, vol. abs/2104.01778, 2021. [16] S. Chang, H. Park, J. Cho, H. Park, S. Yun, and K. Hwang, “Subspectral normalization for neural audio data processing,” in ICASSP, 2021, pp. 850–854. [17] D. Leroy, A. Coucke, T. Lavril, T. Gisselbrecht, and J. Dureau, “Federated learning for keyword spotting,” in ICASSP, 2019, pp. 6341–6345. [18] A. Hard, K. Partridge, C. Nguyen, N. Subrahmanya, A. Shah, P. Zhu, I. Lopez-Moreno, and R. Mathews, “Training keyword spotting models on non-iid data with federated learning,” in IN- TERSPEECH, 2020, pp. 4343–4347. [19] S. J. Reddi, Z. Charles, M. Zaheer, Z. Garrett, K. Rush, J. Koneˇcn´y, S. Kumar, and H. B. McMahan, “Adaptive federated optimization,” in ICLR, 2021. [20] X. Cui, S. Lu, and B. Kingsbury, “Federated acoustic modeling for automatic speech recognition,” in ICASSP, 2021, pp. 6748–6752. [21] K. Nandury, A. Mohan, and F. Weber, “Cross-silo federated train- ing in the cloud with diversity scaling and semi-supervised learn- ing,” in ICASSP, 2021, pp. 3085–3089. [22] D. Guliani, F. Beaufays, and G. Motta, “Training speech recogni- tion models with federated learning: A quality/cost framework,” in ICASSP, 2021, pp. 3080–3084."
99,Fast Finite Width Neural Tangent Kernel,"[{'href': 'http://arxiv.org/abs/2206.08720v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2206.08720v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-06-17 12:18:22,"2 2 0 2 n u J 9 ] G L . s c [ 2 v 8 1 2 0 1 . 5 0 2 2 : v i X r a Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions Rui Yang yr0013@mail.ustc.edu.cn University of Science and Technology of China Jie Wang∗ jiewangx@ustc.edu.cn Institute of Artificial Intelligence Hefei Comprehensive National Science Center University of Science and Technology of China Zijie Geng ustcgzj@mail.ustc.edu.cn University of Science and Technology of China Mingxuan Ye mingxuanye@miralab.ai University of Science and Technology of China Shuiwang Ji sji@tamu.edu Texas A&M University College Station, TX Bin Li binli@ustc.edu.cn University of Science and Technology of China Feng Wu fengwu@ustc.edu.cn University of Science and Technology of China ABSTRACT Generalization across different environments with the same tasks is critical for successful applications of visual reinforcement learning (RL) in real scenarios. However, visual distractions—which are com- mon in real scenes—from high-dimensional observations can be hurtful to the learned representations in visual RL, thus degrading the performance of generalization. To tackle this problem, we pro- pose a novel approach, namely Characteristic Reward Sequence Prediction (CRESP), to extract the task-relevant information by learning reward sequence distributions (RSDs), as the reward sig- nals are task-relevant in RL and invariant to visual distractions. Specifically, to effectively capture the task-relevant information via RSDs, CRESP introduces an auxiliary task—that is, predicting the characteristic functions of RSDs—to learn task-relevant represen- tations, because we can well approximate the high-dimensional distributions by leveraging the corresponding characteristic func- tions. Experiments demonstrate that CRESP significantly improves the performance of generalization on unseen environments, out- performing several state-of-the-arts on DeepMind Control tasks with different visual distractions. ∗ Corresponding Author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD ’22, August 14–18, 2022, Washington, DC, USA © 2022 Association for Computing Machinery. ACM ISBN 978-1-4503-9385-0/22/08. . . $15.00 https://doi.org/10.1145/3534678.3539391 CCS CONCEPTS • Computing methodologies → Sequential decision making; Image representations; Markov decision processes. KEYWORDS Task-relevant representation learning, reward sequence, character- istic function, generalization, visual reinforcement learning ACM Reference Format: Rui Yang, Jie Wang∗, Zijie Geng, Mingxuan Ye, Shuiwang Ji, Bin Li, and Feng Wu. 2022. Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’22), August 14–18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3534678.3539391 1 INTRODUCTION Visual reinforcement learning (RL) algorithms aim to solve com- plex control tasks from high-dimensional visual observations. No- table successes include DrQ for locomotion control [30], IMPALA for multi-task learning [5], and QT-Opt for robot grasping [13]. Although these methods perform well on training environments, they can hardly generalize to new environments, even these environ- ments are semantically similar to the training environments. This is because image observations often involve many task-irrelevant visual factors, such as dynamic backgrounds and colors of the object under control. Minor changes in such visual factors may cause large distributional shifts of the environments, which prevent the agent from extracting underlying task-relevant information when we put it into a new environment. This indicates that many existing RL agents memorize the trajectories on specific environments [22, 25], rather than learning transferable skills. To learn a policy with transferable skills for generalization, many prior works focus on learning representations that encode KDD ’22, August 14–18, 2022, Washington, DC, USA Yang et al. functions on top of the reward sequence representations. Experi- ments on DeepMind Control Suite [27] with visual distractors [26] demonstrate that CRESP significantly improves several state-of- the-arts on unseen environments. Our main contributions in this paper are as follows: • We introduce the reward sequence distributions (RSDs) to discard the task-irrelevant features and preserve the task- relevant features. • We propose CRESP, a novel approach that extracts the task- relevant information by learning the characteristic functions of RSDs for representation learning. • Experiments demonstrate that the representations learned by CRESP preserve more task-relevant features than prior methods, outperforming several state-of-the-arts on the ma- jority of tasks by substantial margins. 2 RELATED WORK Generalization in visual RL. The study of generalization in deep RL focuses on the capability of RL methods to generalize to un- seen environments under a limited set of training environments. Several works propose to apply regularization techniques origi- nally developed for supervised learning, including dropout [12] and batch normalization [7, 12]. Although practical and easy to im- plement, these methods do not exploit any properties of sequential decision-making problems. Other approaches for preventing over- fitting focus on data augmentation [17, 19, 21, 31], which enlarge the available data space and implicitly provide the prior knowledge to the agent. Although these methods show promising results in well-designed experimental settings, strong assumptions such as prior knowledge of the testing environments may limit their real ap- plications. In contrast to these methods, we consider a more realistic setting without assuming this prior knowledge of environments. Representation Learning in visual RL. Many prior works focus on representation learning for generalization in visual RL. Some of the works [14, 15] use a two-step learning process, which first trains an auto-encoder by using a reconstruction loss for low-dimensional representations, and then uses this representation for policy opti- mization. However, such representations encode all elements from observations, whether they are relevant to the task or not. Other works use bisimulation metrics to learn a representation that is invariant to irrelevant visual features [34]. However, such methods use the transition dynamics, which vary with the environments, leading to the learned representation involving task-irrelevant fea- tures of the visual distractions. A recent study [20] leverages the reward prediction for representation learning. However, the rep- resentation learning method only considers finite MDPs, which cannot extend to visual RL tasks. Characteristic Functions of Random Variables. Characteristic func- tions are the Fourier transforms of probability density functions. They are well studied in probability theory and can be used to specify high-dimensional distributions. This is because two random variables have the same distribution if and only if they have the same characteristic function. Some prior works [2, 32] use character- istic functions to solve some statistical problems. We leverage this tool for a simple and tractable approximation of high-dimensional Figure 1: The agent-environment interactions in Block MDPs with visual distractions. Each environment 𝑒 provides a state 𝑠𝑡 and a background 𝑥𝑡 , which generate an observa- tion 𝑜𝑡 = 𝑔(𝑠𝑡 , 𝑥𝑡 ) through a nonlinear function 𝑔. The agent receives 𝑜𝑡 and takes an action 𝑎𝑡 in 𝑒, leading to the transi- tions of states (from 𝑠𝑡 to 𝑠𝑡 +1), backgrounds (from 𝑥𝑡 to 𝑥𝑡 +1), and thus the observation transitions (from 𝑜𝑡 to 𝑜𝑡 +1). Notice that the red arrows represent the transitions that vary with different environments, while the blue arrow represents the transition invariant to environments. only the task-relevant information while discarding task-irrelevant visual factors. Some of them propose similarity metrics [3, 16] to find semantically equivalent observations for representation learn- ing [1, 34]. Others design objectives by integrating MDP proper- ties to learn a causal representation that is invariant to irrelevant features [23, 33]. These aforementioned methods leverage rewards and transition dynamics to capture task-relevant features. However, the observation transition dynamics (see Figure 1) may induce the task-irrelevant information relating to visual distractions into the representations, thus hindering generalization [24, 33]. Detailed discussions are in Section 4.1. In contrast to the above methods, we propose a novel approach, namely Characteristic Reward Sequence Prediction (CRESP), which only uses reward signals but observation transition dynamics to learn task-relevant representations, as the reward signals are task- relevant in RL and invariant to visual factors. To preserve infor- mation that is relevant to the task, CRESP introduces the reward sequence distributions (RSDs), which are the conditional distribu- tions of reward sequences given a starting observation and various subsequent actions. CRESP leverages RSDs to learn a task-relevant representation that only encodes the information of RSDs, which we call reward sequence representation. Specifically, considering that the characteristic function can specify high-dimensional distribu- tions [2], we propose to learn such task-relevant representation by an auxiliary task that predicts the characteristic functions of RSDs. Moreover, we provide a theoretical analysis of the value bounds between the true optimal value functions and the optimal value ... Env Env Env Agent Env Env Env ... Agent observation transition dynamics Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions KDD ’22, August 14–18, 2022, Washington, DC, USA distributions. Our experiments demonstrate that the characteris- tic functions perform well to specify the distributions of reward sequences in our method. 3 PRELIMINARIES In visual RL tasks, we deal with high-dimensional image observa- tions, instead of the states as the inputs. We consider a family of environments with the same high-level task but different visual dis- tractions. Denote E as the set of these environments. We model each environment 𝑒 ∈ E as a Block Markov Decision Process (BMDP) [4, 33], which is described by a tuple M𝑒 = (S, O, A, R, 𝑝, 𝑝𝑒, 𝛾). Here S is the state space, O is the observation space, A is the action space, R is the reward space, which we assume to be bounded, 𝑝 (𝑠 ′, 𝑟 |𝑠, 𝑎) is the state transition probability, 𝑝𝑒 (𝑜 ′, 𝑟 |𝑜, 𝑎) is the observation transition probability, which varies with environments 𝑒 ∈ E, and 𝛾 ∈ [0, 1) is the discount factor. At each time step 𝑡, we suppose that the environment is in a state 𝑆𝑡 .1 The agent, instead of directly achieving 𝑆𝑡 , obtains an observa- tion 𝑂𝑡 on environment 𝑒 ∈ E. It is reasonable to assume that the observation is determined by the state and some task-irrelevant visual factors that vary with environments, such as backgrounds or agent colors in DeepMind Control tasks. Symbolically, let X be the set of such visual factors. We suppose that there exists an observation function 𝑔 : S × X → O [4, 25] such that 𝑂𝑡 = 𝑔(𝑆𝑡 , 𝑋𝑡 ), where 𝑋𝑡 is a random variable in X, independent with 𝑆𝑡 and 𝐴𝑡 , with a transition probability 𝑞𝑒 (𝑥 ′|𝑥). See Figure 1 for an illustra- tion. We aim to find a policy 𝜋 (·|𝑜𝑡 ) that maximizes the expected accumulated reward E𝑒 (cid:2)(cid:205)∞ 𝛾𝑡 𝑅𝑡 (cid:3) simultaneously in all environ- 𝑡 =0 ments 𝑒 ∈ E, where E𝑒 [·] means that the expectation is taken in the environment 𝑒. Moreover, we assume that the environments follow a general- ized Block structure [4, 33]. That is, an observation 𝑜 ∈ O uniquely determines its generating state 𝑠, and the visual factor 𝑥. This assumption implies that the observation function 𝑔(𝑠, 𝑥) is invert- ible with respect to both 𝑠 and 𝑥. For simplicity, we denote 𝑠 = [𝑜]𝑠 and 𝑥 = [𝑜]𝑥 as the generating state and visual factor, respectively. Furthermore, we have 𝑝𝑒 (𝑜 ′, 𝑟 |𝑜, 𝑎) = 𝑝 (𝑠 ′, 𝑟 |𝑠, 𝑎)𝑞𝑒 (𝑥 ′|𝑥), where 𝑠 = [𝑜]𝑠, 𝑠 ′ = [𝑜 ′]𝑠 , 𝑥 = [𝑜]𝑥 and 𝑥 ′ = [𝑜 ′]𝑥 . 4 REPRESENTATION LEARNING VIA REWARD SEQUENCE DISTRIBUTIONS An encoder, or a representation, refers to an embedding function Φ : O → Z, which maps the observational inputs onto a latent state representation space Z. Our goal is to find a suitable represen- tation that encodes only task-relevant information and is invariant to visual distractions. In Section 4.1, we discuss the notion of task relevance in visual RL, introduce reward sequence distributions (RSDs), and formulate the reward sequence representations for gen- eralization. In Section 4.2, we provide a theoretical analysis that reformulates the reward sequence representation via the charac- teristic functions of RSDs. In Section 4.3, we present a practical method, based on the prediction of characteristic functions of RSDs, to learn such a reward sequence representation. 1Throughout this paper, we use uppercase letters such as 𝑆𝑡 and 𝑂𝑡 to denote random variables, and use lowercase letters such as 𝑠𝑡 and 𝑜𝑡 to denote the corresponding values that the random variables take. Figure 2: The relationship between observations and RSD mappings. We can divide the observation space into differ- ent equivalence classes, where the equivalent observations are generated from the same state. Each equivalence class corresponds to a same mapping from action sequences a ∈ A𝑇 to reward sequence distributions 𝑝 (·|𝑜, a) ∈ Δ(R𝑇 ). 4.1 Task-relevant Invariance in Visual RL The key idea of our approach is to capture the task-relevant infor- mation across different environments from observations, and lever- age such information for representation learning to improve the performance of generalization. Reward signals and transition dynamics are major properties of MDP, which are commonly used for representation learning in visual RL. We start with a discussion on the distractions induced by observation transition dynamics. In visual RL, we can hardly learn about the state transition dynamics, as the state space is un- available in practice. Instead, many methods learn the observation transition dynamics by a probabilistic dynamics model [28, 29, 34]. However, the observation transition dynamics are relevant to the visual factors because they comprise the transition dynamics of both states and task-irrelevant visual factors. Formally, we have the reward and observation transition dynamics 𝑝𝑒 (𝑜 ′, 𝑟 |𝑜, 𝑎) = 𝑝 (𝑠 ′, 𝑟 |𝑠, 𝑎)𝑞𝑒 (𝑥 ′|𝑥). This formula shows that the observation tran- sition probability varies with the environment 𝑒 ∈ E. We present a case in Figure 1 to illustrate the observation transition dynamics. Therefore, representations that encode information about observa- tion transition dynamics are subject to visual distractions and have difficulty learning transferable skills. In contrast to observation transition dynamics, the distributions of reward signals are relevant to the RL tasks and are invariant to visual distractions. Formally, if two observations 𝑜 and 𝑜 ′ are generated by the same state 𝑠, i.e., [𝑜]𝑠 = [𝑜 ′]𝑠 , then we have 𝑝𝑒 (𝑟 |𝑜, 𝑎) = 𝑝𝑒 (𝑟 |𝑜 ′, 𝑎) for any 𝑎 ∈ A and 𝑒 ∈ E. This motivates us to use the reward signals instead of observation transition dynamics for representation learning. As our goal is to maximize the expected accumulative rewards, what we need is not only the current reward but also the sequences of future rewards. Therefore, we propose to utilize the reward sequences for representation learning. For a mathematical formulation, we introduce some new nota- tions. We denote A𝑇 = {a = (𝑎1, · · · , 𝑎𝑇 ) : 𝑎𝑖 ∈ A} and R𝑇 = {r = (𝑟1, · · · , 𝑟𝑇 ) : 𝑟𝑖 ∈ R} as the spaces of action sequences and reward sequences with length 𝑇 , respectively. Let Δ(R𝑇 ) be the set of probability distributions over R𝑇 . At each time step 𝑡, Observation Space Action sequence space RSD space Action sequence space RSD space 𝐚 𝑝(∙ |𝑜, 𝐚) 𝐴𝑇 ∆(𝑅𝑇) 𝐴𝑇 ∆(𝑅𝑇) Set of RSD Mappings KDD ’22, August 14–18, 2022, Washington, DC, USA Yang et al. the sequence of the subsequent actions A𝑇 𝑡 = (𝐴𝑡 , · · · , 𝐴𝑡 +𝑇 −1) is a 𝑇 -dimensional random vector over A𝑇 . The sequence of the subsequent rewards R𝑇 𝑡 +1 = (𝑅𝑡 +1, · · · , 𝑅𝑡 +𝑇 ) is a 𝑇 -dimensional random vector over R𝑇 . 2 𝑡 +1 𝑡 +1 To clarify our idea, we first consider a deterministic environ- ment. Starting from an observation 𝑜𝑡 ∈ O, with the corresponding state 𝑠𝑡 = [𝑜𝑡 ]𝑠 ∈ S, suppose that we perform a given action se- quence a𝑇 𝑡 = (𝑎𝑡 , · · · , 𝑎𝑡 +𝑇 −1) ∈ A𝑇 and receive a reward sequence 𝑡 +1 = (𝑟𝑡 +1, · · · , 𝑟𝑡 +𝑇 ) ∈ R𝑇 from the environment. This reward r𝑇 sequence r𝑇 is uniquely determined by the starting state 𝑠𝑡 and the given action sequence a𝑇 𝑡 . Therefore, we can find that the rela- tionship between the given action sequence a𝑇 𝑡 and the received reward sequence r𝑇 is invariant to visual distractions. We can use such a relationship to identify the task-relevant information from observations. To formulate this relationship, we consider the map- pings from action sequences a ∈ A𝑇 to the corresponding reward sequences r ∈ R𝑇 —that the agent receives from an observation 𝑜 by following the action sequence a. We consider two observations 𝑜 and 𝑜 ′ that have same mappings from a ∈ A𝑇 to r ∈ R𝑇 for any dimension 𝑇 . In other words, we suppose that the agent receives the equal reward sequence r ∈ R𝑇 from 𝑜 and 𝑜 ′, when it follows any action sequence a ∈ A𝑇 for any 𝑇 . Then the two observations have similar task properties, in the sense that the agent will receive the equal accumulative rewards from 𝑜 and 𝑜 ′ no matter what actions the agent takes. Therefore, the mappings from action sequences a ∈ A𝑇 to the corresponding reward sequences r ∈ R𝑇 can be used to identify the task-relevant information from the observations. We then consider the stochastic environment, the case of which is similar to the deterministic environment. In the stochastic en- vironment, the reward sequence R𝑇 is random even for fixed 𝑡 +1 observation 𝑜𝑡 and action sequence A𝑇 𝑡 . Therefore, we cannot sim- ply consider the mappings from A𝑇 to R𝑇 . Instead, we apply the mappings from A𝑇 to Δ(R𝑇 ), which map the action sequences to the distributions of the sequences of reward random variables. Formally, let 𝑝 (r|𝑜, a) be the probability density function of the at the point r ∈ R𝑇 , conditioned on the starting 𝑡 = a ∈ A𝑇 . For any random vector R𝑇 observation 𝑂𝑡 = 𝑜 and the action sequence A𝑇 𝑜 ∈ O, a = (𝑎1, · · · , 𝑎𝑇 ), and r = (𝑟2, · · · , 𝑟𝑇 +1), we have 𝑡 +1 𝑝 (r|𝑜, a) = 𝑝 (𝑟2|𝑠, 𝑎1)𝑝 (𝑟3|𝑠, 𝑎1, 𝑎2) · · · 𝑝 (𝑟𝑇 +1|𝑠, 𝑎1, · · · , 𝑎𝑇 ), where 𝑠 = [𝑜]𝑠 , and 𝑝 (𝑟 |𝑠, 𝑎1, · · · , 𝑎𝑡 ) denotes the probability den- sity function of the reward 𝑟 that the agent receives, after fol- lowing an action sequence (𝑎1, · · · , 𝑎𝑡 ), starting from the state 𝑠. Furthermore, for any 𝑜, 𝑜 ′ ∈ O such that [𝑜]𝑠 = [𝑜 ′]𝑠 , we have 𝑝 (r|𝑜, a) = 𝑝 (r|𝑜 ′, a). The formulas imply that the conditional dis- tributions 𝑝 (·|𝑜, a) of reward sequences are determined by the gen- erating states of the observations as well as the action sequences. Therefore, the mappings from the action sequences a ∈ A𝑇 to the corresponding RSDs 𝑝 (·|𝑜, a) are task-relevant and invariant to visual distractions. Thus we can use the mappings to determine task relevance. See Figure 2 for an illustration. The analysis above motivates our method that leverages the RSDs to learn representations. Specifically, we learn a representation 2We use bold uppercase letters such as A and R to denote random vectors in high- dimensional spaces and use bold lowercase letters such as a and r to denote determin- istic vectors in such spaces. that can derive a function, which maps the action sequences to the corresponding RSDs. Formally, we define the 𝑇 -level reward sequence representation as follows. Definition 4.1. A representation Φ : O → Z is a 𝑇 -level reward sequence representation if it can derive the distribution of any reward sequence received from any observation by following any action sequence with length 𝑇 , i.e., there exists 𝑓 such that 𝑓 (r; Φ(𝑜), a) = 𝑝 (r|𝑜, a), ∀ r ∈ R 𝑇 , 𝑜 ∈ O, a ∈ A𝑇 . Intuitively, the 𝑇 -level reward sequence representation encodes the task-relevant information about the relation between the action sequences a and the RSDs 𝑝 (r|𝑜, a) in the next 𝑇 steps. Notice that a 𝑇 -level reward sequence representation is also a 𝑇 ′-level reward sequence representation, where 𝑇 ,𝑇 ′ ∈ N∗ and 𝑇 > 𝑇 ′. If 𝑇 tends to infinity, the representation will encode all task-relevant information from the objective of RL tasks. This derives the following definition. Definition 4.2. A representation Φ : O → Z is a reward sequence representation if it is a 𝑇 -level reward sequence representation for all 𝑇 ∈ N∗. The reward sequence representation is equivalent to a ∞-level reward sequence representation. In practice, we learn a finite 𝑇 - level reward sequence representation as an approximation of the reward sequence representation. To provide a theoretical guarantee for the approximation, the following theorem gives a value bound between the true optimal value function and the value function on top of the 𝑇 -level reward sequence representation. Theorem 4.3. Let Φ : O → Z be a 𝑇 -level representation, 𝑉 𝑒 ∗ : O → R be the optimal value function in the environment 𝑒 ∈ E, ¯𝑉 𝑒 ∗ : Z → R be the optimal value function on the latent representation space, built on top of the representation Φ. Let ¯𝑟 be a bound of the reward space, i.e., |𝑟 | < ¯𝑟 for any 𝑟 ∈ R. Then we have 0 ≤ 𝑉 𝑒 ∗ (𝑜) − ¯𝑉 𝑒 ∗ ◦ Φ(𝑜) ≤ 2𝛾𝑇 1 − 𝛾 ¯𝑟, for any 𝑜 ∈ O and 𝑒 ∈ E. Proof. See Appendix A.1 □ 4.2 Characteristic Functions for Representation Learning In Section 4.1, we formulate the 𝑇 -level reward sequence repre- sentation that can derive the probability density function 𝑝 (r|𝑜, a), where r ∈ R𝑇 is a reward sequence, 𝑜 ∈ O is an observation, and a ∈ A𝑇 is an action sequence. However, learning the probability density functions is usually technically impractical [2]. Leveraging the characteristic function of random vectors, we propose an al- ternative approach, which is simple to implement and effective to learn the distributions. Consider a random vector R defined on the space R𝑇 , with a probability density function 𝑝R (·). The characteristic function 𝜑R : R𝑇 → C of R is defined as 𝜑R (𝝎) = ER∼𝑝R ( ·) 𝑒𝑖 ⟨𝝎,R⟩ (cid:105) (cid:104) = ∫ 𝑒𝑖 ⟨𝝎,r⟩𝑝R (r)dr, √ where 𝝎 ∈ R𝑇 denotes the input of 𝑝R (·), and 𝑖 = −1 is the imaginary unit. Since we consider discounted cumulative rewards Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions KDD ’22, August 14–18, 2022, Washington, DC, USA Figure 3: The overall architecture of CRESP. CRESP minimizes the prediction loss to train an encoder Φ, and simultaneously uses Φ to learn a policy in an actor-critic setting. In the prediction task, CRESP predicts the characteristic functions of reward sequence distributions through the encoder Φ and the predictor Ψ (in the purple box). The prediction loss L N D (Φ, Ψ) provides the gradients (red lines) to update both the predictor Ψ and the encoder Φ. Here r𝑇 𝑡 are the sequences drawn from a replay buffer D. The inputs 𝝎 of characteristic functions are sampled from a Gaussian distribution N . 𝑡 +1 and a𝑇 𝑡 =1 in RL tasks, we use ⟨·, ·⟩ to denote the weighted inner product in R𝑇 , i.e., ⟨𝝎, r⟩ = (cid:205)𝑇 𝛾𝑡 𝜔𝑡𝑟𝑡 , where 𝛾 is the discounted factor. Characteristic functions are useful tools well studied in probabil- ity theory. In contrast to the probability density function, the char- acteristic function has some good basic properties. 1) |𝜑R (𝝎)| ≤ 𝑒𝑖 ⟨𝝎,R⟩(cid:12) (cid:12) ER∼𝑝R ( ·) (cid:12) = 1, which indicates that the characteristic func- (cid:12) (cid:12) (cid:12) tion always exists and is uniformly bounded. 2) The characteris- tic function 𝜑R is uniformly continuous on R𝑇 , which makes it tractable for learning. The following lemma states a fact that the distribution of a ran- dom vector can be specified by its characteristic function. Lemma 4.4. [8] Two random vectors X and Y have the same characteristic function if and only if they have the same probability distribution function. This lemma implies that we can recapture the information about the distributions of random vectors via their characteristic functions. Therefore, instead of learning the conditional density functions of reward sequences that are intractable, we propose to leverage characteristic functions of the RSDs for representation learning. Specifically, we have the following theorem. Theorem 4.5. A representation Φ : O → Z is a 𝑇 -level reward sequence representation if and only if there exits a predictor Ψ such that for all 𝒘 ∈ R𝑇 , 𝑜 ∈ O and a ∈ A𝑇 , Ψ(𝝎; Φ(𝑜), a) = 𝜑R|𝑜,a (𝝎) = ER∼𝑝 ( · |𝑜,a) 𝑒𝑖 ⟨𝝎,R⟩ (cid:105) (cid:104) . Proof. See Appendix A.2. □ Theorem 4.5 provides an equivalent definition of 𝑇 -level reward sequence representation and inspires our novel approach to predict the characteristic functions of RSDs for representation learning. Algorithm 1 Characteristic Reward Sequence Prediction Initialize a replay buffer D, a policy 𝜋, a representation Φ, and a function approximator Ψ for each iteration do for 𝑒 in E do for each environment step 𝑡 do Execute action 𝑎𝑡 ∼ 𝜋 (·|Φ(𝑜𝑡 )) Receive a transition 𝑜𝑡 +1, 𝑟𝑡 +1 ∼ 𝑝𝑒 (·|𝑜𝑡 , 𝑎𝑡 ) Record partial trajectories {(𝑜𝑡 −𝑖, 𝑎𝑡 −𝑖, 𝑟𝑡 +1−𝑖 )}𝑇 −1 𝑖=0 in D end for end for for each gradient step do Sample partial trajectories from D Update the representation: L N Update the policy: LRL (𝜋) D (Φ, Ψ) end for end for 4.3 Characteristic Reward Sequence Prediction To improve the generalization of a learned policy on unseen en- vironments with visual distractions, we propose Characteristic Reward Sequence Prediction (CRESP), a novel approach to learn representations for task relevance from high-dimensional observa- tions. As discussed above, CRESP learns RSDs by predicting the characteristic functions 𝜑R|𝑜,a (𝝎). In this section, we focus on the detailed learning procedure for the prediction. For an observation 𝑜 ∈ O and an action sequence a ∈ A𝑇 , the true characteristic function of the corresponding reward sequence is 𝜑R|𝑜,a (𝝎) = ER∼𝑝 ( · |𝑜,a) [𝑒𝑖 ⟨𝝎,R⟩]. We estimate the characteristic function by a predictor Ψ(𝝎; Φ(𝑜), a). We use the weighted squared distance between the true and predicted characteristic functions as Characteristic Reward Sequence Prediction 𝑠𝑡 𝑥𝑡 𝑇 = (𝑟𝑡+1, … , 𝑟𝑇) 𝒓𝑡+1 𝝎 = 𝜔1, … , 𝜔𝑇−𝑡 𝑇 = 𝑎𝑡, … , 𝑎𝑇−1 𝐚𝑡 . .. . . . 𝑔 𝑜𝑡 Encoder 𝚽 Characteristic Function signals Prediction Loss 𝒩(𝚽, 𝚿) ℒ𝒟 𝝍cos 𝝍sin grad Predictor 𝚿 𝚿 𝝎; 𝚽 𝑜𝑡 , 𝐚𝑡 Critic 𝑄 𝑜𝑡, 𝑎𝑡 Actor 𝑎𝑡 RL Loss KDD ’22, August 14–18, 2022, Washington, DC, USA Yang et al. Figure 4: Learning curves of six methods on six tasks with dynamic background distractions for 500K environment steps. The solid curves denote the means and the shaded regions denote the minimum and maximum returns over 6 trials. Each checkpoint is evaluated by 10 episodes on unseen environments. Curves are smoothed for visual clarity. the prediction loss: 𝐿W (Φ, Ψ|𝑜, a) = E𝛀∼W (cid:104)(cid:13) (cid:13)Ψ (𝛀; Φ(𝑜), a) − 𝜑R|𝑜,a (𝛀)(cid:13) 2 (cid:13) 2 (cid:12)Ψ (𝝎; Φ(𝑜), a) − 𝜑R|𝑜,a (𝝎)(cid:12) (cid:12) 2 W (𝝎)d𝝎, (cid:12) (cid:105) = ∫ R𝑇 where W is any probability density function on R𝑇 . We optimize the expected loss for observations and action sequences taken from the replay buffer D: 𝐿W D (Φ, Ψ) = E(𝑂,A)∼D (cid:104) 𝐿W (Φ, Ψ|𝑂, A) (cid:105) = E(𝑂,A)∼D,𝛀∼W (cid:104)(cid:13) (cid:13)Ψ (𝛀; Φ(𝑂), A) − 𝜑R|𝑂,A (𝛀)(cid:13) (cid:13) (cid:105) 2 2 . In practice, Since we have no access to the true characteristic functions, we propose to optimize an upper bound on 𝐿W D : L W D (Φ, Ψ) = E(𝑂,A,R)∼D,𝛀∼W (cid:20)(cid:13) (cid:13) (cid:13) ≥ E(𝑂,A)∼D,𝛀∼W (cid:20)(cid:13) (cid:13) (cid:13) Ψ (𝛀; Φ(𝑂), A) − 𝑒𝑖 ⟨𝛀,R⟩(cid:13) 2 (cid:13) (cid:13) 2 (cid:21) Ψ (𝛀; Φ(𝑂), A) − ER∼𝑝 ( · |𝑂,A) (cid:21) 𝑒𝑖 ⟨𝛀,R⟩ (cid:105)(cid:13) (cid:104) 2 (cid:13) (cid:13) 2 = 𝐿W D (Φ, Ψ). Due to the complex form of characteristic functions, we divide the predictor Ψ into two parts Ψ = (𝜓cos,𝜓sin), where 𝜓cos es- timates the real parts, and 𝜓sin estimates the imaginary parts of characteristic functions, respectively. Moreover, we draw Ω from a Gaussian distribution W = N (𝝁, 𝝈 2) in practice. We then pa- rameterize this distribution N (𝝁, 𝝈 2) and perform ablation on it in Appendix B.1. Based on the experimental results, we leverage D (Φ, Ψ) = E(𝑂,A,R)∼D,𝛀∼N the standard Gaussian distribution N . Then the loss function is: L N (cid:2)∥𝜓cos (𝛀; Φ(𝑂), A) − cos (⟨𝛀, R⟩)∥2 2 (cid:3) . + ∥𝜓sin (𝛀; Φ(𝑂), A) − sin (⟨𝛀, R⟩)∥2 2 In the training process, we update the encoder Φ and the pre- dictor Ψ due to the auxiliary loss L N D (Φ, Ψ), and use the trained encoder Φ for the RL tasks. The whole architecture of CRESP and training procedure are illustrated in Figure 3 and Algorithm 1. 5 EXPERIMENTS In this paper, we improve the performance of generalization on unseen environments with visual distractions. We focus on training agents in multi-environments under traditional off-policy settings without any prior environmental knowledge, such as strong aug- mentations designed for visual factors [6, 19, 35], fine-tuning in test environments [11], or environmental labels for invariance [1, 24]. We then investigate the performances of agents trained by different algorithms on various unseen test environments. For each environment, we benchmark CRESP extensively against prior state-of-the-art methods: 1) CURL [18]: a RL method with an auxiliary contrastive task; 2) DrQ [30]: an effective method with state-of-the-art performance on DeepMind Control (DMCon- trol) [27]; 3) MISA [33]: a recent approach from causal inference to learn invariant representations by approximating one-step rewards and dynamics; 4) DBC [34]: a research for generalization in RL to learn representations via the bisimulation metric; 5) SAC [9]: a traditional off-policy deep RL algorithm. Network Details. Our method builds upon SAC and follows the network architecture of DrQ and CURL. We use a 4-layer feed- forward ConvNet with no residual connection as the encoder. Then,"
107,         Meta’s Challenge to OpenAI—Give Away a Massive Language Model     ,https://spectrum.ieee.org/large-language-models-meta-openai,2022-05-05,"At 175 billion parameters, it’s as powerful as OpenAI’s GPT-3 Meta is giving away some of the family jewels: That’s the gist of an announcement from the company formerly known as Facebook this week. In a blog post on the Meta AI site, the company’s researchers announced that they’ve created a massive and powerful language AI system and are making it available free to all researchers in the artificial-intelligence community. Meta describes the move as an effort to democratize access to a powerful kind of AI—but some argue that not very many researchers will actually benefit from this largesse. And even as these models become more accessible to researchers, many questions remain about the path to commercial use. Large language models are one of the hottest things in AI right now. Models like OpenAI’s GPT-3 can generate remarkably fluid and coherent text in just about any format or style: They can write convincing news articles, legal summaries, poems, and advertising copy, or hold up their end of conversation as customer-service chatbots or video-game characters. GPT-3, which broke the mold with its 175 billion parameters, is available to academic and commercial entities only via OpenAI’s application and vetting process. Meta’s Open Pretrained Transformer (known as OPT-175B) matches GPT-3 with 175 billion parameters of its own. Meta is offering the research community not only the model itself, but also its codebase and extensive notes and logbooks about the training process. The model was trained on 800 gigabytes of data from five publicly available data sets, which are described in the “data card” that accompanies a technical paper posted by the Meta researchers to the ArXiv online preprint server. Joelle Pineau, director of Meta AI Research Labs, tells IEEE Spectrum that she expects researchers to make use of this treasure trove in several ways. “The first thing I expect [researchers] to do is to use it to build other types of language-based systems, whether it’s machine translation, a chatbot, something that completes text—all of these require this kind of state-of-the-art language model,” she says. Rather than training their own language models from scratch, Pineau says, they can build applications and run them “on a relatively modest compute budget.” Joelle PineauMeta The second thing she expects researchers to do, Pineau says, is “pull it apart” to examine its flaws and limitations. Large language models like GPT-3 are famously capable of generating toxic language full of stereotypes and harmful bias; that troubling tendency is a result of training data that includes hateful language found in Reddit forums and the like. In their technical paper, Meta’s researchers describe how they evaluated the model on benchmarks related to hate speech, stereotypes, and toxic-content generation, but Pineau says “there’s so much more to be done.” She adds that the scrutiny should be done “by community researchers, not inside closed research labs.” The paper states that “we still believe this technology is premature for commercial deployment,” and says that by releasing the model with a noncommercial license, Meta hopes to facilitate the development of guidelines for responsible use of large language models “before broader commercial deployment occurs.” Within Meta, Pineau acknowledges that there’s a lot of interest in using OPT-175B commercially. “We have a lot of groups that deal with text,” she notes, that might want to build a specialized application on top of the language model. It’s easy to imagine product teams salivating over the technology: It could power content-moderation tools or text translation, could help suggest relevant content, or could generate text for the creatures of the metaverse, should it truly come to pass. There have been other efforts to make an open-source language model, most notably from EleutherAI, an association that has released a 20-billion-parameter model in February. Connor Leahy, one of the founders of EleutherAI and founder of an AI startup called Conjecture, calls Meta’s move a good step for open science. “Especially the release of their logbook is unprecedented (to my knowledge) and very welcome,” he tells Spectrum in an email. But he notes that Meta’s conditional release, making the model available only on request and with a noncommercial license, “falls short of truly open.” EleutherAI doesn’t comment on its plans, but Leahy says the group will continue working on its own language AI, and adds that OPT-175B will be helpful for some of its research. “Open research is synergistic in that way,” he says. “Security through obscurity is not security, as the saying in the computer-security world goes. And studying these models and finding ways to integrate their existence into our world is the only feasible path forward.”—Connor Leahy, EleutherAI EleutherAI is a something of an outlier in AI research in that it’s a self-organizing group of volunteers. Much of today’s cutting-edge AI work is done within the R&D departments of big players like Meta, Google, OpenAI, Microsoft, Nvidia, and other deep-pocketed companies. That’s because it takes enormous amount of energy and compute infrastructure to train big AI systems. Meta claims that its training of OPT-175 required 1/7th the carbon footprint of that required for training GPT-3, yet as Meta’s paper notes, that’s still a significant energy expenditure. The paper says that OPT-175B was trained on 992 80-gigabyte A100 GPUs from Nvidia, with a carbon-emissions footprint of 75 tons, as compared to an estimated carbon budget of 500 tons for GPT-3 (that figure has not been confirmed by OpenAI). Meta’s hope is that by offering up this “foundation model” for other entities to build on top of, it will at least reduce the need to build huge models from scratch. Deploying the model, Meta says in its blog post, requires only 16 Nvidia 32GB V100 GPUs. The company is also releasing smaller scale versions of OPT-175B that can be used by researchers who don’t need the full-scale model or by those who are investigating the behavior of language models at different scales. Maarten Sap, a researcher at the Allen Institute for Artificial Intelligence (AI2) and in incoming assistant professor at Carnegie Mellon University’s Language Technologies Institute, studies large language models and has worked on methods to detoxify them. In other words, he’s exactly the kind of researcher that Meta is hoping to attract. Sap says that he’d “love to use OPT-175B,” but “the biggest issue is that few research labs actually have the infrastructure to run this model.” If it were easier to run, he says, he’d use it to study toxic language risks and social intelligence within language models. While Sap applauds Meta for opening up the model to the community, he thinks it could go a step further. “Ideally, having a demo of the system and an API with much more control/access than [OpenAI’s API for GPT-3] would be great for actual accessibility,” he says. However, he notes that Meta’s release of smaller versions is a good “second-best option.” Whether models like OPT-175B will ever become as safe and accessible as other kinds of enterprise software is still an open question, and there are different ideas about the path forward. EleutherAI’s Leahy says that preventing broad commercial use of these models won’t solve the problems with them. “Security through obscurity is not security, as the saying in the computer-security world goes,” says Leahy, “and studying these models and finding ways to integrate their existence into our world is the only feasible path forward.” Meanwhile, Sap argues that AI regulation is needed to“prevent researchers, people, or companies from using AI to impersonate people, generate propaganda or fake news, or other harms.” But he notes that “it’s pretty clear that Meta is against regulation in many ways.” Sameer Singh, an associate professor at University of California, Irvine, and a research fellow at AI2 who works on language models, praises Meta for releasing the training notes and logbooks, saying that process information may end up being more useful to researchers than the model itself. Singh says he hopes that such openness will become the norm. He also says he supports providing commercial access to at least smaller models, since such access can be useful for understanding models’ practical limitations. “Disallowing commercial access completely or putting it behind a paywall may be the only way to justify, from a business perspective, why these companies should build and release LLMs in the first place,” Singh says. “I suspect these restrictions have less to do with potential damage than claimed.” Eliza Strickland is a senior editor at IEEE Spectrum, where she covers AI, biomedical engineering, and other topics. She holds a master's degree in journalism from Columbia University. The AI pioneer says it’s time for smart-sized, “data-centric” solutions to big issues Andrew Ng was involved in the rise of massive deep learning models trained on vast amounts of data, but now he’s preaching small-data solutions. Andrew Ng has serious street cred in artificial intelligence. He pioneered the use of graphics processing units (GPUs) to train deep learning models in the late 2000s with his students at Stanford University, cofounded Google Brain in 2011, and then served for three years as chief scientist for Baidu, where he helped build the Chinese tech giant’s AI group. So when he says he has identified the next big shift in artificial intelligence, people listen. And that’s what he told IEEE Spectrum in an exclusive Q&A."
127,"         Inventing Postscript, the Tech That Took the Pain out of Printing     ",https://spectrum.ieee.org/adobe-postscript,2022-04-23,"Adobe’s founders and engineers tell a tale of software, stumbles, and serendipity Time and again, in his earliest attempts at controlling laser printers, John Warnock got the message, “Page Too Complex,” from a recalcitrant machine. Any system he designed, he vowed, would have to have a “Print Anything” architecture. That goal led ultimately to a page description language called PostScript, today the de facto standard of desktop publishing. This article was first published as ""‘PostScript’ prints anything: a case history."" It appeared in the May 1988 issue of IEEE Spectrum. A PDF version is available on IEEE Xplore. The diagrams and photographs appeared in the original print version. Back then, Warnock already had a rough idea how to “Print Anything.” But later he ran into a different obstacle, when his employer, Xerox Corp., proved loath to support a truly standard language. So off he went, with Charles Geschke and several other colleagues, to found Adobe Systems Inc. in Mountain View, Calif. By that time, PostScript was only two major pieces of research away, although one—the development of type font algorithms—was “a research project that had to succeed,” says Warnock, and the other had been described as one of the world’s most difficult problems. The rest is desktop publishing history. PostScript can truly do anything, though extremely complex images can take as much as an hour of computation time. It first appeared in the Apple LaserWriter, which was introduced in January of 1985. Today it has been adopted by 23 manufacturers of laser printers, with more still signing on. This story is as much about luck and guts as about matters of principle and brilliant software engineering. Still, this story is as much about luck and guts as about matters of principle and brilliant software engineering. It would have been quite different had Warnock and company not been in the right place at the right time to meet the right person. The time was right because of the imminence of three hardware developments: the first low-cost, bit-mapped personal computer, the first low-cost laser printer, and a decline in price of high-density memory chips. And the right person was Apple founder Steven Jobs, who invented the first, hoped for the second, and told Adobe to tough out the third. Device-independent software Software not directly tied to a specific piece of hardware. Interpreter A program that translates an instruction in the source code of a high-level language into machine language by deciding on the fly what machine instructions best translate it before moving onto the next instruction in source code. Laser printer A device that, like a xerographic copier, draws an image on a drum, but with a laser beam instead of lenses; applies toner to the charged image area; and transfers the toner to a sheet of paper, melting it into the paper to set the image. Page description language A method of expressing the appearance of a printed image, including text, lines, and bit-mapped photographs. Postfix notation Also known as reverse Polish notation, the appearance of operators after the data on which they are to operate; thus 2 + 2 becomes 2 2 +. Today laser printers are rapidly replacing the daisy-wheel printers in the office, pushing out letter-quality type as their laser obeys the commands of simple software. But given more sophisticated software like PostScript, laser printers can do far more. They can print many different type fonts and make the letters dance around the page hand in hand with drawings and photographs. PostScript does all this implies—draws lines and curves, tilts text at arbitrary angles, or shades a photograph in various tones of gray. It is as complete and flexible a programming language as Pascal or C or Forth, having variables, loops, conditionals, operators, and routines and offering any number of ways to get the same output. The PostScript program is created on the computer either by someone using the language or by desktop publishing software or other applications software that translates, say, the movements of a mouse into a PostScript program. (Other page description languages are optimized for one of these purposes, not both.) That program is sent over a local-area network or through an RS-232 port to the laser printer. There it is converted into instructions for the printer by the PostScript interpreter, software resident in ROM. On the same circuit board as up to 2 megabytes of ROM is a Motorola 68000 series processor, which executes the instructions and causes the pages to be printed. Things were more elementary with the first laser printers, which were in regular use at the Xerox Palo Alto Research Center (PARC) in the mid-1970s. They were controlled by a printing protocol called Press, which was not a programming language but a set of instructions that sent image data to a printer in a steady stream. It handled letters and simple images well, but for anything more detailed, got the printer to return the message: “Page Too Complex.” Thereupon the typical PARC engineer would simplify the image. But when Warnock, a computer scientist with a Ph.D. from the University of Utah, joined the center in 1978, he immediately began work on a new printer protocol. Six years of experience at Evans & Sutherland in Mountain View, Calif., had taught him where to start. Adobe Systems founders John Warnock (right) and Charles Geschke visited Adobe Creek, inspiration for their company’s name. A dry winter has slowed the creek to a trickle, but the company has had anything but a dry year. The entrepreneurs in 1982 found Adobe a suitable name since the creek meandered near both their domes and, even more important, had none of the Qs, Xs, Ys, and Zs then popular with high-tech startups. The PostScript program is created on the computer either by someone using the language or by desktop publishing software or other applications software that translates, say, the movements of a mouse into a PostScript program. (Other page description languages are optimized for one of these purposes, not both.) That program is sent over a local-area network or through an RS-232 port to the laser printer. There it is converted into instructions for the printer by the PostScript interpreter, software resident in ROM. On the same circuit board as up to 2 megabytes of ROM is a Motorola 68000 series processor, which executes the instructions and causes the pages to be printed. Things were more elementary with the first laser printers, which were in regular use at the Xerox Palo Alto Research Center (PARC) in the mid-1970s. They were controlled by a printing protocol called Press, which was not a programming language but a set of instructions that sent image data to a printer in a steady stream. It handled letters and simple images well, but for anything more detailed, got the printer to return the message: “Page Too Complex.” Thereupon the typical PARC engineer would simplify the image. But when Warnock, a computer scientist with a Ph.D. from the University of Utah, joined the center in 1978, he immediately began work on a new printer protocol. Six years of experience at Evans & Sutherland in Mountain View, Calif., had taught him where to start. In 1971, Evans & Sutherland had undertaken to equip the New York Maritime Academy with a simulator for training harbor pilots. The trainees were to sit on the mockup of a ship’s bridge, surrounded by five 12-foot-high, 30-ft-long (3.6-by-9 meter) screens displaying a computer-generated representation of New York Harbor, complete with buildings, piers, movable buoys, changing weather conditions, and other ships to be avoided. The system had to produce images in full color for five projectors at 30 frames a second. Evans & Sutherland had never produced anything as complex. It let time slip by until, with only one of the contract’s three years left, “everybody hit the panic button,” Warnock says. So to save time, the company had the hardware and software developed in parallel, the first in Utah and the second by a team led by Warnock in California. The rush planted the first two seeds for what was to become PostScript. Obviously, a database listing everything in the harbor was both essential and would have to be built in total ignorance as to the hardware it would eventually run on. So Warnock’s team decided to invent a language unrelated to any computer. Only when the simulator hardware was ready would they build a compiler to translate the database into the appropriate machine language. Meanwhile, feeding information about the harbor into the database proved arduous. Putting maps on a digitizing tablet and touching them with a stylus at numerous points was not so bad; but using a keyboard to enter the details—whether the point touched was a pier of a certain type or a building or an island—was slow going. To make this task easier, John Gaffney, one of Warnock’s group, spent a weekend writing a software routine that would generate the information about the objects from menus. Because the PostScript Language treats text like any other graphic object, it can be scaled to any size and rotate to any angle. PostScript was the first page description language to be able to produce such a spiral of type. Adobe Systems Inc. By the time the harbor simulator was completed, only slightly behind schedule, Warnock had discovered how powerful an object-oriented language is. Unlike Basic or Fortran, say, which require the user to spell out every last instruction, it packs all those details into modules, or objects, which the user controls with just a few directives. Warnock had also discovered that making software device-independent “gives you a great deal of leverage and flexibility.” Those lessons learned, his group turned to expanding Gaffney’s little interpreter into a full programming system for computer-aided design (CAD). In 1977, that project was released by Evans & Sutherland as The Design System. “It had an interactive, stack-oriented architecture,” Gaffney said, “with simple commands for pushing and popping arguments onto and from the stack and a rich dictionary for look-ups.” (Such an architecture stores data as it is received, stacking it like a pile of books. A command like “add” would “pop” the topmost pieces of data from the stack, act on them, and “push” the result back on the pile.) Only one copy of The Design System was ever released, as a test bed for the final development, but the other company’s project director died and The Design System died with him. Warnock, however, took the stack and dictionary ideas—along with what he had learned from the harbor project—to PARC. PARC was then using a programming language called Mesa. In 1978, soon after arriving at the center, Warnock persuaded another Xerox researcher, Martin Newell, to help him re-create The Design System in Mesa. The result, called Jam, for John and Martin, proved the concepts he brought from Evans & Sutherland were appropriate for laser printing. Jam was object oriented and device independent, like the harbor simulator, and in some ways simpler than The Design System, because printing requires only two dimensions, versus CAD’s three. But it needed a few features, such as type fonts, found in neither of its ancestors. Moreover, Warnock recalls, “Xerox was using a different printing scheme on every printer. The Star workstations [then being developed] were crumbling under the load of trying to drive them all differently.” So Warnock and a group of researchers headed by Charles Geschke set out to merge Jam with the older Press protocol into Interpress, a standard, device-independent language capable of driving all Xerox Corp.’s laser printers. Interpress was completed in 1981, but unhappily, the end was not in sight. Because of the compromise between Jam and Press, “the language became complicated in its redesign,” Warnock says. And Xerox begged the issue of standardization by producing several versions of the language, so the company’s older laser printers could run some form of it. The commands of the PostScript programming language are optimized for graphics. This elongated word “Spectrum” was generated by the PostScript program shown below it. The first group of commands (red) identify a typeface from PostScript’s library of typefaces and enlarge it to 50 points from the 1-point size in which it is stored (in typesetting, there are 72 points to the inch). The next group of commands (blue) tells the laser printer at which point on the page to anchor the lower left corner of the word. The next command (yellow) stretches the typeface vertically, while leaving it unaltered horizontally. The final commands (purple) specify the letters to be drawn and order the printer to produce the image. A special program editor transmits the instructions to the printer. Adobe Systems Inc. Worst of all, to Warnock, was the insistence that printers always run at their rated speeds. Since a 20-page-per-minute printer could not produce anything very complex in three seconds, he was back facing his “Page Too Complex” nemesis. The constraint derived from the copier business, Geschke explains, where “pricing of leased machines was based on copies per day. But in electronic printing, in our opinion, function was most important, so there was a real variance between our and the Xerox position.” All the same, in the belief that any standard was better than none, Warnock and Geschke began promoting Interpress within Xerox Corp. Eventually, they won—sort of. But Xerox added, Warnock recalls, “’We’re going to keep it a secret because it is so wonderful and if we publish it the Japanese might implement it before we do.’ “Gee,’ I said, ‘A secret standard—I find this a hard concept to understand.’” Convinced that Xerox was making a mistake, Warnock and Geschke left PARC to implement their page description language once again, but this time within a corporation they controlled. With the help of David Evans of Evans & Sutherland and William Hambrecht of Hambrecht and Quist, a San Francisco-based venture capital firm, they wrote a business plan and incorporated in December of 1982. They intended both to sell this setup as a turnkey system and to franchise the publishing equivalent of a one-hour photo store. Desktop publishing, though, was not what Warnock and Geschke at first had in mind. The system they foresaw consisted of a workstation linked by a device-independent, page-description language like Jam to a laser printer for draft printing, a photo-typesetter for the final output, and whatever other output device they might later add. No other publishing package then available used the same software for different output devices. And they intended both to sell this setup as a turnkey system and to franchise the publishing equivalent of a one-hour photo store. Adobe then consisted of Warnock, Geschke, and a core of other engineers hired from PARC: Daniel Putman, Thomas Boynton, and Douglas Brotz. As they planned to buy whatever hardware they needed after they had perfected their programming language, they focused first on Jam. They worked in C, on a VAX 750 running Berkeley Unix, to develop the language, and they tested in on a Sun workstation driving a full-size laser printer that they had borrowed from Digital Equipment Corp. “At that time,” recalls Putman, “most companies required that we spell our names and pay in cash, so we had to beg, borrow, and steal the tools to prototype PostScript.” To avoid copyright problems, they licensed The Design System concepts from Evans & Sutherland. They were free to use their PARC research results, as those had been published. For years theoreticians have suggested means of breaking images into their line segments, but their algorithms tended to fall apart when faced with difficult cases—large numbers of lines intersecting at a single point, for example (inset). The Adobe team says it has solved this problem with a proprietary algorithm with the results illustrated here. The image was created with Adobe Illustrator, a drawing program that runs on the Apple Macintosh personal computer, and was printed on the ColorScript 100, the first color PostScript printer, released in April by QMS Inc. of Mobile, Ala. Warnock and Geschke were not close-mouthed about their plans, and soon not only Jobs heard (he was then chairman of Apple Computer Inc. of Cupertino, Calif.) but also C. Gordon Bell, then vice president of engineering at Digital Equipment in Maynard, Mass. Bell told the pair that six research teams at Digital had been trying for years to devise a decent means of driving its laser printers, and if Adobe could solve the problem, Digital would be interested in licensing the solution. Jobs had been facing a similar problem. The Macintosh was well into development, but without a letter-quality printer would go nowhere in the business market. Daisy-wheel printers were out of the question, because they could not produce the bit-mapped graphics basic to the Macintosh. But Apple’s own engineers could not get high-quality graphics out of a laser printer in time for the Macintosh introduction. Jobs suggested that Adobe become a software company, sell to manufacturers instead of at retail, and negotiate a licensing agreement with Apple. Undeterred, Jobs and Robert Belleville, then director of engineering for Apple and now director of strategic planning for Convergent Technologies Inc., San Jose, Calif., had dreamed up the perfect Macintosh laser printer—one that could produce all the fonts in the world with no help from a disk drive. But they lacked “the slightest idea of how to do this,” says Belleville, until he ran into Putman at a cocktail party, heard what Adobe was doing, and brought Jobs over for a visit. “I was overjoyed!” recalls Belleville. “Their system could do simple things fast and also do full graphics and scanned images. And when I saw font scaling was possible across such wide ranges, we were sold.” Jobs suggested that Adobe become a software company, sell to manufacturers instead of at retail, and negotiate a licensing agreement with Apple. Adobe liked the idea, signed the agreement with Apple at the end of 1983, and much to Hambrecht & Quist’s surprise, showed a profit at the end of its first year. Reimplementing the Jam language with its object orientation, stacks, postfix notation (in which operands precede their operators), and dictionary was relatively straightforward. Most of the research had been completed at Evans & Sutherland and at PARC. Basically all Adobe had to do was engineer it into a product, named PostScript after the postfix notation it uses and because it was to be the last thing that happened to an image before it was printed. Also, since the product had to “Print Anything,” it had to put functionality above speed and cost—the three factors traded off in the design of microprocessor systems like the one that would control the laser printer, explains Putman, now vice president of engineering at Adobe. Still, two key breakthroughs remained to be made. One of them was creating the font algorithms, proprietary formulas for the creation of text. “Even with Interpress,” says William Paxton, director of advanced development for Adobe, “fonts were a wart on the side of an otherwise elegant design.” Interpress could do arbitrary transformations, like scale and rotate, on images, but in its early versions could not do them on bit-mapped text without degrading its quality. PostScript, however, unifies text and graphics by storing the fonts as outline representations of the letters, not as bit maps. Back in early 1983, however, this unification was easier to propose than to realize. “Getting high-quality fonts from outline representations of characters was seen as an insoluble problem,” Warnock says, because it was hard to produce smooth curves of varying widths without jagged edges. Print quality seemed unobtainable from anything less than a phototypesetter. But in mid-1983, Warnock says, he had an idea for a fundamentally new set of algorithms that might do the trick. His initial experiments promised success, so he set Paxton to refining the algorithms. The results are proprietary and are encrypted inside the ROMs that contain PostScript instructions because this font technology is the key distinction between Adobe’s product and others. So successful was Adobe’s solution to the font problem that Linotype, Letraset, and other owners of the most popular typeface designs were willing for the first time to license the outline representations of their typefaces. No earlier technology had done them justice. (Ironically, Adobe is now licensing its font technology to Linotype, and Linotype is converting its entire library of some 2000 fonts into PostScript representations.) Adobe’s other technical breakthrough is the algorithm, called Reducer, that breaks down complex shapes into simpler ones easier for PostScript to describe. Such an algorithm is a key component of any graphics language, and theoretical papers about a universal form of it were numerous: but, says Brotz, “they tended to gloss over the hard cases that arise in real applications—figures with large amounts of data and multiple intersections at the same point, for example.” So when the page printed, certain images would come out badly fragmented or warped, violating Adobe’s “Print Anything” rule. “About a week after I had joined Adobe in 1983,” Brotz recalls, “John Warnock mentioned this rather important algorithm that had to be written. And I, with no graphics background, volunteered. Several months later, older and wiser, I realized it truly was one of the world’s hardest problems.” But Brotz did not give up, and he says, “We have now an exactly correct reducer algorithm. It is the heart of the graphics system in PostScript.” And a tally Brotz keeps reveals that no bugs have been discovered in the Reducer in more than two years. “Warnock promptly labeled [the procedure] ‘Andy’s Stupid Input Device'....[but] it turned out that Andy’s Stupid Input Device was the lowest common denominator and all the special-case code could disappear.” —Douglas Brotz Adobe had agreed to deliver its software for installation into the LaserWriter during the summer of 1984. But because of marketing and manufacturing concerns, the LaserWriter itself was to be introduced in January of 1985. So the Adobe engineers used the time to tighten the code (the final release contained some 200,000 bytes) and fine-tune the algorithms. They also made some more specific changes. One had to do with handling input devices. As originally conceived, PostScript was to have been independent of the output, but not the input, device. Warnock had thought that PostScript, to take in scanned images, would need to contain information about a wide range of optical scanners. But Brotz, after programming the parameters of just two of many scanner types, realized that the task was not only horrendous and repetitive but ate up a lot of memory. Andy Shore, an Adobe computer scientist, overheard him complaining one day and suggested writing a PostScript procedure that would pretend that it was an input device and spit out the image information in a standard format, regardless of the characteristics of the actual standard. Brotz did not think it would work and “Warnock promptly labeled it ‘Andy’s Stupid Input Device.'” Still, Brotz thought it might be helpful for generating test patterns, and when he implemented it, “it turned out that Andy’s Stupid Input Device was the lowest common denominator and all the special-case code could disappear.” Problems arise only when the image data has been compressed for transmission or storage; the programmer then has to insert a routine to decompress the data before it is handed to the image algorithm. Another improvement involved performance profiling—running various tests to see what frequently used functions slowed down operation. Floating-point routines were the chief culprits because they are computationally intensive. So the team took some of the algorithms for the common operations, such as breaking curves into vectors and drawing outlines, and rewrote them in less flexible fixed-point arithmetic. Now only when fixed-point arithmetic would be too imprecise does the interpreter call the floating-point routine. “So with no loss of generality,” says Edward Taft, Adobe senior computer scientist, “we were handling 99 percent of the cases five times faster than we were before.” To improve the other 1 percent, Belleville sent one of his engineers over from Apple—Jerome Coonen, a recognized expert in floating point. He optimized the algorithms so, Taft says, “whereas formerly an algorithm required six multiplies, four divides, and three square roots, now it only required three multiplies, four divides, and some approximation of a square root.” “We came from the school of thought that software is soft. So if you have problems, you just have another release. But Apple was telling us, ‘Hey, we always ship our system in ROM, why can’t you?’” —Douglas Brotz Throughout the design of PostScript, speed was regularly traded off to ensure that any image would print. The group reasoned that if they built in all this functionality, they could eventually improve the performance; but if they left out functions, they might never be able to add them back in. However, says Putman, sometimes they had doubts. So they designed a version of PostScript that spat out information as fast as the laser moved across the page. The expense of the frame buffer was eliminated—along with the ability to print pages too complicated for the software to process in time. Adobe called this implementation Subscript, but dropped it after six months. As Taft says, “If you’re trying to promote a standard, there is nothing worse than issuing a subset of the standard. It means that all of the applications are going to be targeted to the lowest common denominator.” Debugging throughout the project was strenuous because the Adobe team was “terrified of putting all this code out on ROMs,” Brotz says. “We came from the school of thought that software is soft. So if you have problems, you just have another release. But Apple was telling us, ‘Hey, we always ship our system in ROM, why can’t you?’” In January of 1985 the Apple LaserWriter was introduced, virtually bug-free. In 1984, Adobe signed licensing agreements with QMS Inc., Linotype, and Dataproducts Corp. Today, even Hewlett-Packard Co., whose PCL page description language was one of PostScript’s earliest competitors, is among the 23 companies offering PostScript interpreters for their printers. Although the Adobe group made some key technical breakthroughs, three other components were necessary to make PostScript a runaway success not just in low-volume professional publishing but in the high-volume office environment. As noted earlier, one was a cheap laser printer. When Adobe was founded, the cheapest cost around $10,000. It also weighed as much as a desk, so that it had to be serviced on site and sold through a distributor, not on a cash-and-carry basis. Then Canon Inc., of Tokyo, Japan, introduced the Canon LBP-CX desktop laser printer, which, moreover, printed beautifully. “If it had been poor xerography,” says Paxton, “it wouldn’t have mattered how good our technology was.” Also on the horizon was a bit-map-based personal computer—the Apple Macintosh. All previous low-cost personal computers had used character graphics, for which daisy-wheel printers made more sense. “The projections were that the RAM prices were going to drop, but you had to have a very strong stomach to be able to go up to the wall and pray that the door was going to open.”—William Paxton The third piece of luck was the decline in the price of memory chips. “We started this development on an uneconomic basis,” Warnock says. “The LaserWriter’s first controller needed forty-eight 256K DRAM chips, which up to December of 1984 cost about $30 each. That meant Apple would have had to sell that machine for about $10,000—but its computer cost $2400.” But, with Belleville’s and Jobs’s strong support, the Adobe team bet that the memory process would drop. “Sure,” says Paxton, “the projections were that the RAM prices were going to drop, but you had to have a very strong stomach to be able to go up to the wall and pray that the door was going to open.” Warnock comments, “Most companies will only deal with present-day technology and known costs. The brilliance of Steve Jobs is that he will say, ‘There will be this chip coming out at that price point at that time, and I will design my product to use it.’” And indeed, when the LaserWriter was announced in January of 1985, 256K RAMs cost about $4 each and the printer could be priced at $6995. Today, some 40 companies have announced their equipment is compatible with PostScript and that their interpreters run faster and cost less than Adobe’s version. They cannot offer the same font library, but they say they have fonts and font algorithms as good as Adobe’s. At this writing, however, none of these companies had apparently shipped a PostScript clone to a customer, and they reportedly have found it harder to replicate Adobe’s work than they had anticipated. When they do finally ship, and if they can interpret 80 or 90 percent of PostScript programs, Adobe is resigned to facing “good old-fashioned American competition,” says Geschke. The company has no patents to defend, only copyrights and trade secrets, so if other companies can reproduce Adobe’s technology, it has no legal recourse. “The most we can do is to continue to improve our technology,” Geschke says. Adobe’s latest technical breakthrough, demonstrated in San Francisco in January, is a version of PostScript that controls images on a computer screen as well as on a printed page. Called Display PostScript, this product is the first to provide device-independent graphics for computer screens. Display PostScript, like the original PostScript printer protocol, had a nudge from Jobs. His new company, NeXT Inc., Palo Alto, Calif., worked with Adobe to develop it, and it will be the graphics standard for all NeXT’s computers. Digital Equipment has already licensed Display PostScript for its DEC Windows workstation architecture. If other major companies follow, Adobe could be well on the way to setting its second standard. Everything a programmer or user might want to know about the PostScript language is provided in “PostScript Language Tutorial and Cookbook“ and “PostScript Language Reference Manual,” both written by Adobe Systems Inc. and published by Addison Wesley Publishing Co. (New York, 1985). In addition, Adobe periodically publishes a newsletter, “Colophon,” with programming tips and news about PostScript products. Interpress, the page description language from Xerox Corp.’s Palo Alto Research Center (PARC) that preceded PostScript in the laboratory but followed it in the marketplace, is described in the June 1986 issue of IEEE’s magazine, Computer (pp. 72-77). For more information on Xerox PARC, see “Inside the PARC: the ‘information architects,’” Spectrum, October 1985, p. 62. “Window on PostScript” in MacWeek, Feb. 2, 1988, pp. 28-29, contains a discussion of competitors’ attempts to clone the language. Update April 2022: While most home and office printers rely on other page description languages these days, PostScript remains the choice of graphics artists and commercial printers for its ability to accurately produce complex images. And the ubiquitous Portable Document Format (PDF) is based on PostScript. Tekla S. Perry is a senior editor at IEEE Spectrum. Based in Palo Alto, Calif., she's been covering the people, companies, and technology that make Silicon Valley a special place for more than 40 years. An IEEE member, she holds a bachelor's degree in journalism from Michigan State University. The University of Texas professor co-invented discrete cosine transform Jae Jeong Hwang is a professor of IT convergence and communication engineering at Kunsan National University, in Korea. Zoran M. Milicevic is an assistant professor of telecommunications and IT at the University of Belgrade, in Serbia. Zoran S. Bojković is a professor of electrical engineering at the University of Belgrade. Wikipedia Kamisetty Ramamohan “K.R.” Rao died on 15 January 2021 at the age of 89. He co-invented the discrete cosine transform (DCT) technique, which is widely used in digital signal processing and data compression. A small band of believers triumphed after years of quietly plugging away Rodney Brooks is the Panasonic Professor of Robotics (emeritus) at MIT, where he was director of the AI Lab and then CSAIL. He has been cofounder of iRobot, Rethink Robotics, and Robust AI, where he is currently CTO. In 1997, Harvard Business School professor Clayton Christensen created a sensation among venture capitalists and entrepreneurs with his book The Innovator's Dilemma. The lesson that most people remember from it is that a well-run business can’t afford to switch to a new approach—one that ultimately will replace its current business model—until it is too late. One of the most famous examples of this conundrum involved photography. The large, very profitable companies that made film for cameras knew in the mid-1990s that digital photography would be the future, but there was never really a good time for them to make the switch. At almost any point they would have lost money. So what happened, of course, was that they were displaced by new companies making digital cameras. (Yes, Fujifilm did survive, but the transition was not pretty, and it involved an improbable series of events, machinations, and radical changes.)"
139,Meta has built a massive new language AI—and it’s giving it away for free,https://www.technologyreview.com/2022/05/03/1051691/meta-ai-large-language-model-gpt3-ethics-huggingface-transparency/,2022-05-03,"Meta’s AI lab has created a massive new language model that shares both the remarkable abilities and the harmful flaws of OpenAI’s pioneering neural network GPT-3. And in an unprecedented move for Big Tech, it is giving it away to researchers—together with details about how it was built and trained. “We strongly believe that the ability for others to scrutinize your work is an important part of research. We really invite that collaboration,” says Joelle Pineau, a longtime advocate for transparency in the development of technology, who is now managing director at Meta AI. Meta’s move is the first time that a fully trained large language model will be made available to any researcher who wants to study it. The news has been welcomed by many concerned about the way this powerful technology is being built by small teams behind closed doors. “I applaud the transparency here,” says Emily M. Bender, a computational linguist at the University of Washington and a frequent critic of the way language models are developed and deployed. “It’s a great move,” says Thomas Wolf, chief scientist at Hugging Face, the AI startup behind BigScience, a project in which more than 1,000 volunteers around the world are collaborating on an open-source language model. “The more open models the better,” he says. Large language models—powerful programs that can generate paragraphs of text and mimic human conversation—have become one of the hottest trends in AI in the last couple of years. But they have deep flaws, parroting misinformation, prejudice, and toxic language. In theory, putting more people to work on the problem should help. Yet because language models require vast amounts of data and computing power to train, they have so far remained projects for rich tech firms. The wider research community, including ethicists and social scientists concerned about their misuse, has had to watch from the sidelines. Meta AI says it wants to change that. “Many of us have been university researchers,” says Pineau. “We know the gap that exists between universities and industry in terms of the ability to build these models. Making this one available to researchers was a no-brainer.” She hopes that others will pore over their work and pull it apart or build on it. Breakthroughs come faster when more people are involved, she says. Meta is making its model, called Open Pretrained Transformer (OPT), available for non-commercial use. It is also releasing its code and a logbook that documents the training process. The logbook contains daily updates from members of the team about the training data: how it was added to the model and when, what worked and what didn’t. In more than 100 pages of notes, the researchers log every bug, crash, and reboot in a three-month training process that ran nonstop from October 2021 to January 2022. With 175 billion parameters (the values in a neural network that get tweaked during training), OPT is the same size as GPT-3. This was by design, says Pineau. The team built OPT to match GPT-3 both in its accuracy on language tasks and in its toxicity. OpenAI has made GPT-3 available as a paid service but has not shared the model itself or its code. The idea was to provide researchers with a similar language model to study, says Pineau. OpenAI declined an invitation to comment on Meta’s announcement. Google, which is exploring the use of large language models in its search products, has also been criticized for a lack of transparency. The company sparked controversy in 2020 when it forced out leading members of its AI ethics team after they produced a study that highlighted problems with the technology. So why is Meta doing this? After all, Meta is a company that has said little about how the algorithms behind Facebook and Instagram work and has a reputation for burying unfavorable findings by its own in-house research teams. A big reason for the different approach by Meta AI is Pineau herself, who has been pushing for more transparency in AI for a number of years. Tech giants dominate research but the line between real breakthrough and product showcase can be fuzzy. Some scientists have had enough. Pineau helped change how research is published in several of the largest conferences, introducing a checklist of things that researchers must submit alongside their results, including code and details about how experiments are run. Since she joined Meta (then Facebook) in 2017, she has championed that culture in its AI lab. “That commitment to open science is why I’m here,” she says. “I wouldn’t be here on any other terms.” Ultimately, Pineau wants to change how we judge AI. “What we call state-of-the-art nowadays can’t just be about performance,” she says. “It has to be state-of-the-art in terms of responsibility as well.” Still, giving away a large language model is a bold move for Meta. “I can’t tell you that there’s no risk of this model producing language that we’re not proud of,” says Pineau. “It will.” Margaret Mitchell, one of the AI ethics researchers Google forced out in 2020, who is now at Hugging Face, sees the release of OPT as a positive move. But she thinks there are limits to transparency. Has the language model been tested with sufficient rigor? Do the foreseeable benefits outweigh the foreseeable harms—such as the generation of misinformation, or racist and misogynistic language? “Releasing a large language model to the world where a wide audience is likely to use it, or be affected by its output, comes with responsibilities,” she says. Mitchell notes that this model will be able to generate harmful content not only by itself, but through downstream applications that researchers build on top of it. Meta AI audited OPT to remove some harmful behaviors, but the point is to release a model that researchers can learn from, warts and all, says Pineau. “There were a lot of conversations about how to do that in a way that lets us sleep at night, knowing that there’s a non-zero risk in terms of reputation, a non-zero risk in terms of harm,” she says. She dismisses the idea that you should not release a model because it’s too dangerous—which is the reason OpenAI gave for not releasing GPT-3’s predecessor, GPT-2. “I understand the weaknesses of these models, but that’s not a research mindset,” she says. Hundreds of scientists around the world are working together to understand one of the most powerful emerging technologies before it’s too late. Bender, who coauthored the study at the center of the Google dispute with Mitchell, is also concerned about how the potential harms will be handled. “One thing that is really key in mitigating the risks of any kind of machine-learning technology is to ground evaluations and explorations in specific use cases,” she says. “What will the system be used for? Who will be using it, and how will the system outputs be presented to them?” Some researchers question why large language models are being built at all, given their potential for harm. For Pineau, these concerns should be met with more exposure, not less. “I believe the only way to build trust is extreme transparency,” she says. “We have different opinions around the world about what speech is appropriate, and AI is a part of that conversation,” she says. She doesn’t expect language models to say things that everyone agrees with. “But how do we grapple with that? You need many voices in that discussion.”"
161,DeepMind's baby general agent; NVIDIA simulates a robot factory; AI wars. - 2,http://eepurl.com/h2sj1D,2022-05-20,"#################################################### Chinese researchers build a large multi-modal dataset, and evaluation suite: …'Zero' makes it easier to develop AI systems for the Chinese cultural context… Chinese researchers with startup Qihoo 360 AI Research and the Department of Automation at Tsinghua University have built Zero, a benchmark for assessing the quality of vision-text Chinese AI models. Zero consists of a dataset (the Zero-Corpus, consisting of 23-million image-text pairs, filtered via high click through rates - so the top image people click in response to a query), as well as five downstream datasets for evaluating Chinese vision-text models (an Image-Caption Matching Dataset, an Image-Query Matching dataset, an Image-Caption Retrieval Dataset, an Image-Query Retrieval Dataset, and a Chinese-translated version of the Flickr30k dataset). Model training: The authors also train a model, called R2D2, on the corpus. They show that their model significantly outperforms another Chinse model named Wukong. R2D2 incorporates some pre-ranking techniques to improve its performance. Why this matters: The main idea behind datasets and models like this is described in the paper: ""promote the development of Chinese vision language learning. We expect that a fair Chinese cross-modal benchmark and a good cross-modal framework will encourage a plethora of engineers to develop more effective methods in specific real-world scenarios, such as searching images by texts."" Read more: Zero and R2D2: A Large-scale Chinese Cross-modal Benchmark and A Vision-Language Framework (arXiv)."
166,China makes a vast facial recognition dataset; Facebook releases a 30bn parameter model; real world RL - 1,http://eepurl.com/h1FRp5,2022-05-10,"#################################################### Facebook release a 30 billion parameter GPT3-style model - and plans to release more: …Model controls? No, round here we just like to fling stuff onto the internet… Facebook has released a 30 billion parameter GPT3-style language model, as part of research into a family of language models it calls OPT, short for Open Pre-trained Transformer. OPT is meant to be an 'open' alternative to models like GPT3 or J1J-Jumbo, and it is pretty open - researchers can apply for access to the model via a form, then Facebook will ship them the weights! That part is a big deal, as if you have model weights you can do a whole bunch of analysis not enabled by managed API access to a model. This also increases the chance of proliferation - e.g, someone uploading the weights to a torrent site, so we'll have to see how this works for them. What this all means: As Newton is alleged to have written, 'Every Action has an Equal and Opposite Reaction'. Facebook's move here can be seen as a direct reaction to the proprietary commercialization and gated access schemes for large-scale language models. (I wrote more about the patterns underlying this brinksmanship in a recent paper, 'Predictability and Surprise in Large Generative Models'). What is cool about it: The coolest part of this release is the manner in which Facebook has released rarely discussed details of model training - specifically, the company has published the 'chronicles' of developing these models, which describe many of the freaky, barely discussed, artisanal tips and tricks that AI developers use to get stuff done at scale. (HuggingFace's 'BigScience' project recently did this as well, and is still going through the process of training the models: Import AI 279). Read more: OPT: Open Pre-trained Transformer Language Models (arXiv)."
170,Generative humans; few shot learning comes for vision-text models; and another new AI startup is born - 0,http://eepurl.com/h00rqr,2022-05-02,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Generating and editing humans has got really easy: …Next stop: unreal avatars show up in fashion, marketing, and other fields… Researchers with Chinese computer vision giant SenseTime, as well as Nanyang Technological University and the Shanghai AI Laboratory, have gathered a large dataset of pictures of people and used it to train a model that can generate and edit pictures of people. This kind of model has numerous applications, ranging from fashion to surveillance. What they did: The researchers built a dataset containing 230,000 images of people, called the Stylish-Humans-HQ-Dataset (SHHQ), and used this to train six different models across two resolutions and three versions of StyleGAN, an approach for creating generative models. A lot of the special work they did here involved creating a diverse dataset including a load of pictures of faces at unusual angles (this means models trained on SHHQ are a bit more robust and do less of the 'works, works, works, OH GOD WHAT JUST HAPPENED' phenomenon you encounter when generative models go to the edge of their data distribution). Why this matters: Models and datasets like this highlight just how far the field of generative AI has come - we can now generate broadly photorealistic avatars of people in 2D space and interpolate between them, following earlier successes at doing this for the more bounded domain of faces. Systems like this will have a lot of commercial relevance, but will also serve as useful research artifacts for further developing synthetic imagery and scene modeling techniques. Check out the demo on HuggingFace to get a feel for it. Read more: StyleGAN-Human: A Data-Centric Odyssey of Human Generation (arXiv). Check out the GitHub project page: StyleGAN-Human. Check out the GitHub: StyleGAN-Human (GitHub). Try out the demo on HuggingFace Spaces (HuggingFace)."
177,AI makes low-carbon concrete; weaponized NLP; and a neuro-symbolic language model - 2,http://eepurl.com/h0nCKX,2022-04-25,"#################################################### Text-Vision models are hella dumb, according to Winoground: …Finally, a hard benchmark for multi-modal models… Researchers with Hugging Face, Facebook, the University of Waterloo, and University College London have built and released 'Winoground', a new challenging benchmark to test text-vision AI systems on. What is Winoground? The goal of Winoground is to look at two images and two captions, then match them correctly. The confounding part is that each of the captions contain identical words, just in a different order. The best part is Winoground seems really hard: ""Surprisingly, all of the models rarely—and if so only barely—outperform chance. Our findings indicate that the visio-linguistic compositional reasoning capabilities of these models fall dramatically short of what we might have hoped."" How hard is it? On both the text and image components of Winoground, an 'MTurk Human' gets scores of 89.50 (text) and 88.50 (image), compared to models typically getting around ~30 on text and 15 or less on images. This suggests winoground is a genuinely challenging benchmark, and models have a long way to go before they match human capabilities. Read more: Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality (arXiv). Get the dataset here: Winoground, HuggingFace."
186,"Google trains the world's biggest language model; how robots can be smarter about the world; Conjecture, a new AI alignment company - 3",http://eepurl.com/hZjozT,2022-04-11,"#################################################### Google trains a 540 billion parameter language model - and it's pretty smart: …AKA: The scaling will continue until we run out of TPUs… Google has trained a large language model named Pathways Language Model (PaLM). PaLM weighs in at 540 billion parameters (that'd be 10bn more parameters than Microsoft/NVIDIA's 'Turing NLG') and was trained on multiple TPU v4 pods. PaLM uses some plumbing built by Google called Pathways which makes it easier for the company to train massive models across large clusters of computers; PaLM used 6144 TPU chips, versus Gopher (4096 TPU v3 chips) or Turing NLG (2240 A100 GPUs). PaLM is also efficient, achieving a training efficiency of 57.8% hardware FLOPs utilization ""the highest yet achieved for LLMs at this scale"". Discontinuous capability jumps: One of the weird things that happens as a consequence of scaling up language models is the sudden emergence of hitherto unanticipated capabilities - here, PaLM shows dramatic improvements at things like reasoning, natural language inference, and in-context reading comprehension. Chain-of-thought = reasoning: A surprising result is that the authors use so-called chain-of-thought prompting to get the LM to show its work (e.g, rather than saying in response to 'how many apples can a door eat', 'zero', the model instead says 'zero, because doors do not eat things'). Chain-of-thought is really just a way to prompt the model to get it to output its own reasoning along with the answers - but via this simple intervention the authors show they can meaningfully improve capabilities in a whole bunch of areas. One caveat: PaLM may be an impressive achievement, but earlier this month DeepMind published a paper about a model called 'Chinchilla', where the Alphabet-subsidiary realized that it could dramatically improve LM performance by scaling data more aggressively than parameters - at 70B parameters, Chinchilla beat Gopher (280B) by virtue of having a 4X larger training set. This suggests that a PaLM-style model could be made even more powerful if it was trained on substantially more data. Why this matters: Language models are basically a new sub-field of AI, and papers like this show how, despite being expensive and resource-intensive, simply scaling them up can lead to quite profound jumps in capability. We also don't know where the limits of scale like - on the (deliberately hard) BIG-Bench benchmark, the authors find that ""PaLM’s performance as a function of scale follows a log-linear behavior similar to prior models, suggesting that performance improvements from scale have not yet plateaued."" The future is going to be very strange, and it's arriving very quickly. Read more: Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance (Google AI Blog). Check out the research paper: PaLM: Scaling Language Modeling with Pathways (Google, PDF)."
193,China plans massive models; DeepMind makes a smaller and smarter model; open source CLIP data - 2,http://eepurl.com/hYSyi9,2022-04-05,"#################################################### When language models can be smaller and better! …DeepMind paper says we can make better language models if we use more data… Language models are about to get a whole much better without costing more to develop - that's the takeaway of a new DeepMind paper, which finds that language models like GPT-3 can see dramatically improved performance if trained on way more data than is typical. Concretely, they find that by training a model called Chinchilla on 1.4 trillion tokens of data, they can dramatically beat the performance of larger models (e.g, Gopher) which have been trained on smaller datasets (e.g, 300 billion tokens). Another nice bonus is models trained in this way are cheaper to fine-tune on other datasets and sample from, due to their small size. Chinchilla versus Gopher: To test out their ideas, the team train a language model, named Chinchilla, using the same compute used in DM's 'Gopher' model. But Chinchilla consists of 70B parameters (versus Gopher's 280bn), and uses 4X more data. In tests, Chinchilla outperforms Gopher, GPT-3, Jurassic-1, and Megatron-Turing NLG ""on a large range of downstream evaluation tasks"". What this means: This is an important insight - it will change how most developers of large-scale models approach training. ""Though there has been significant recent work allowing larger and larger models to be trained, our analysis suggests an increased focus on dataset scaling is needed,"" the researchers write. ""Speculatively, we expect that scaling to larger and larger datasets is only beneficial when the data is high-quality. This calls for responsibly collecting larger datasets with a high focus on dataset quality."" Read more: Training Compute-Optimal Large Language Models (arXiv)."
204,Chinese researchers try to train 100trillion+ 'brain-scale' models; 33% of AI benchmarks are meaningless. - 0,http://eepurl.com/hXCPbv,2022-03-21,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Indic languages get a decent benchmark set: …IndicNLG includes evals for 11 Indic languages… Researchers with IIT Madras, Columbia University, the National Institute of Information and Communications Technology in Japan, Microsoft, the University of Edinburgh, and AI4Bharat have built IndicNLG, a suite of evaluation datasets for Indic languages. The open source software supports Assamese, Bengali, Gujarati, Hindi, Marathi, Odiya, Punjabi, Kannada, Malayalam, Tamil, Telugu and English, and includes support for NLG tasks relating to biography generation, news headline generation, sentence summarization, question generation and paraphrase generation. Why this matters: You can't easily manage what you can't measure - so it's going to be difficult to build good models for Indic languages if you lack benchmark suites. IndicNLG helps move the needle on this for generative NLP cases. Read more: IndicNLG Suite: Multilingual Datasets for Diverse NLG Tasks in Indic Languages (arXiv). Get the data: IndicNLG Suite (AI4Bharat indicnlp website)."
222,Fairness through dumbness; planet-scale AI computing; another AI safety startup appears - 1,http://eepurl.com/hVSMf1,2022-02-28,"#################################################### How do we get fairer AI systems? Train the dumbest and biggest model possible: …Facebook shows that sometimes the best filter is no filter at all… Researchers with Facebook AI Research have trained what they think is the largest dense vision model ever (10 billion parameters) on a billion random images sampled from Instagram. The resulting models are extraordinarily capable at a huge range of downstream evaluations (mirroring the performance trends of scaling up compute and data for language models like GPT-3), but also have another intriguing trait: they display much better qualities around fairness and bias than vision models trained on curated datasets like ImageNet. """"In this work, we are interested in probing which of the properties emerge in visual features trained with no supervision on as many images from across the world as possible,"" they write. This is a very big deal - it suggests that maybe the route to fair AI systems is training the largest possible model on the greatest possible amount of data with minimal human oversight. That would be a radical shift from the current intuitions around fairness - namely, that you get to fairness by heavily curating the underlying dataset. Performance and Fairness: ""On in-domain benchmarks, we observe that some properties of the features captured by the larger model was far less present in smaller model. In particular, one of our key empirical findings is that self-supervised learning on random internet data leads to models that are more fair, less biased and less harmful,"" they write. ""We observe that our model is also able to leverage the diversity of concepts in the dataset to train more robust features, leading to better out-of-distribution generalization."" Some of those capabilities in full: In tests, the models do better on fairness indicators relating to gender, skintone, and age bias. They also display less disparity around gender than models trained on ImageNet. They're also better at identifying geographic features (including geographic localization), are better at hate speech detection, and display substantially better performance on generalization tests (like harder versions of ImageNet). Things that make you go 'hmm' and 'uh oh': Facebook trained its model on 1 billion images taken from Instagram. But there's a twist - it pre-filtered the data to ensure it wasn't training anything on EU data ""to confirm to GDPR"". While this might seem like standard cover-your-back behavior, it has a deeper implication: Europe's privacy legislation means that certain types of data from Europe will ultimately be less represented in global-scale AI models. This means the cultures of various European countries will also be less represented. This is a nice example of the unintended consequences of legislation. Why this matters: ""We have demonstrated the potential of using self-supervised training on random internet images to train models that are more fair and less harmful (less harmful predictions, improved and less disparate learned attribute representations and larger improvement in object recognition on images from low/medium income households and non-Western countries)."" In other words - the scaling will continue until the models improve (further)! Read more: Vision Models are More Robust and Fair When pretrained on Uncurated Images Without Supervision (arXiv)."
237,20bn GPT model; diachronic LMs; what people think about AI - 1,http://eepurl.com/hUEhnX,2022-02-14,"#################################################### Want a language model that actually knows about COVID? You might need a Diachronic model: …Models trained on newer data do better - try them yourself… Researchers with the University of Porto, Snap Inc., and Cardiff NLP have built a family of so-called 'time-aware' BERT-style language models, trained on Twitter data. The craziest part of this is that they're committing to ""keep updating and releasing a new model every three months, effectively enabling the community to make use of an up-to-date language model at any period in time"". What the problem is: Most language models are trained on a dataset, then never updated. That means that some language models might have no knowledge of minor events like the global COVID pandemic. This is obviously a problem and the solution is simple (albeit labor-intensive) - periodically gather new data and re-train models. What they did: They train a base RoBERTa model using Twitter data that cuts off in 2019, made up of 90 million tweets. Then, for every three months that elapses after that, they add 4.2 million tweets into the dataset and train a new model. At the time of writing, they've trained nine models in total, with the latest model (2021-Q4) being trained on 123.86 million tweets. The theory is that newer models should perform better on more modern tasks and evaluations. ﻿ How well does it do? They compare their models against a few baselines, including BERTweet (which was trained on ~900m tweets). In tests, their models beat BERTweet on six out of seven benchmarks, though BERTweet gets the best overall performance. These aren't strictly 'time-aware' evaluations, though; they just test some classification abilities for things like emotions, irony, stance, and so on. In these time-aware tests, they find that pseudo-perplexity (PPPL) tends to increase by about 10% for each year by which the models are out of date (so the models get 10% less good and appropriate in terms of the text they generate). "". This result reinforces the need for updated language models even for short time periods,"" the researchers write. Why this matters: AI models naturally freeze-dry the cultural landscape they're trained on, meaning that if we don't get good at updating our models, we'll end up trapped in a world where many of our AI systems are outputting things relevant to prior eras and cultural trends - this will make them less useful, and holds the potential for creating feedback loops around cultural stagnation. AI models are weird mirrors of society, so we need to remake them as society changes. Read more: TimeLMs: Diachronic Language Models from Twitter (arXiv). Get the models here (Cardiff NLP, Twitter)."
253,Facebook's AI supercomputer; Anduril gets a SOCOM contract; Twitter talks about running an algo-bias competition - 0,http://eepurl.com/hTyKXj,2022-02-01,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Facebook teaches language models to speak ~30 languages: …And it's better than an equivalently sized GPT3 model… Facebook has trained a family of language models that are better at translation than GPT3. The XGLM family of models were trained on a mixture of ~30 languages (split across languages for which there's a lot of data, and languages where there's little or very little data). Unsurprisingly, by training on a more diverse distribution of language data than GPT3 did (only 7% of its training corpus wasn't in English), Facebook's models do better - especially when using 'few-shot' prompting, where they feed the model some examples of the target language, then ask it to translate. However, these translation capabilities come at the cost of some of the more interesting reasoning capabilities that GPT-3 is known for. Open source models: Facebook has also released five models (564M parameters, 1.7B, 2.9B, 4.5B, and 7.5B, alon with an experimental model trained on 134 languages and weighing in at 4.5B parameters). Why this matters: If we want the world to benefit from powerful AI systems, we need our AI systems to speak the language of the world. This project goes a step in that direction. ""Models such as XGLM represent a paradigm shift from the Anglo-centric view of the world of NLP to being able to cater to all languages on an equal footing,"" the researchers write. Read more: Few-shot Learning with Multilingual Language Models (arXiv). Get the models here (PyTorch, GitHub)."
260,China does more surveillance research than US and Europe; Google reveals its text model LaMDA; Microsoft improves MoEs - 0,http://eepurl.com/hSYp5z,2022-01-24,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Google (finally) reveals its big text model - LaMDA: …Plus: why you need a lot of humans to make a language model safe… Google has finally given details on LaMDA, it's GPT-3 competitor. LaMDA are a family of language models ranging in size from 2B to 137B parameters (GPT3: 175B), and have been trained on a massive dataset of 1.56 trillion words. One way LaMDA is different to other big language models is that it is centered around dialogue, with 50% of its data coming from ""dialogues from public forums"". Google has also fine-tuned LaMDA models to make them better, safer, and more likely to give factual responses - all these fine-tuned versions are facilitate by Google using crowdworkers to gather specific dialog data. The dataset: LaMDA's underlying dataset is called 'Infiniset', and besides the dialogue also involves common crawl, wikipedia, a mixture of english and non-english web documents, and data from programming-related sites (so LaMDA models can also dabble in code). Learning to use an external dataset: Similar to DeepMind's 'RETRO' model, LaMDA has also been taught to use an external information retrieval system - this makes LaMDA models more factually correct than models without external datasets, because the model can check facts from an external, trusted source. To train the model, they also collect 9k dialogs where LaMDA's generated answers are labelled 'correct' or 'incorrect', to give it a stronger training signal. The results of this are quite striking. For example, when you ask a non-information-retrieval LaMDA model ""What do you think of Rosalie Gascoigne’s sculptures?"", it replies: ""LAMDA-BASE: They’re great, and I love how her work changed through her life. I like her later work more than her earlier ones. Her influence is also super interesting - did you know she was one of the artists that inspired Miró?"". This response seems plausible but it's wrong - Miro was active till late 1960s, and Gascoigne first solo exhibits were in 1970s, so she couldn't have been an influence on Miro. By comparison, the version of LaMDA trained on information retrieval says this: ""LAMDA: Oh wow, her life course is so inspiring. Did you know she was a practitioner of Japanese flower arrangement before turning to sculpture? https://artsearch.nga.gov.au/detail.cfm?irn=8774&pictaus=true"", where it gives a factually accurate statement and provides a source as well. Things that make you go 'hmmm' - more compute than GPT-3: LaMDA consumed 3.55E+23 flops during training, versus 3.14+23 flops for GPT3 (so more parameters doesn't necessarily mean more resource intensive). It was trained on a cluster of 1024 TPU V3 chips. Why this matters: ""LaMDA is a step closer to practical and safe open-ended dialog systems, which can in turn unlock a wide range of useful applications. We hope that this work encourages further research in this area"", Google writes. This is true - systems like LaMDA are basically refinements and improvements on the ideas of GPT2/3. We're a few years away from everyone having access to vast, planet-scale AI models that tell them truthful things in natural ways - the proverbial angel (or devil) on everyone's shoulder. The cultural impacts will be vast and destabilizing. Read more: LaMDA: Language Models for Dialogue Applications (arXiv)."
270,Why bigger is worse for RL; AI-generated Pokemon; real-world EfficientNet - 2,http://eepurl.com/hSmoov,2022-01-17,"#################################################### SCROLLS: A new way to test how well AI systems can understand big chunks of text: …Now that AIs can write short stories, can we get them to understand books?... Researchers with Tel-Aviv University, Allen Institute for AI, IBM Research, and Meta AI, have built 'SCROLLS' a way to test how well AI systems can reason about long texts. SCROLLs incorporates tasks ranging from summarization, to question answering, and natural language inference, as well as multiple distinct domains including transcripts, TV shows, and scientific articles. ""Our experiments indicate that SCROLLS poses a formidable challenge for these models, leaving much room for the research community to improve upon,"" the authors write. How SCROLLs works: This benchmark has mostly been created via curation,consisting of 7 datasets that reward models that can contextualize across different sections of the datasets and process long-range dependencies. The datasets: SCROLLS incorporates GovReport (summarization of reports addressing various national policy issues), SummScreenFD (summarization of TV shows, like Game of Thrones), QMSum (summarization of meeting transcripts), Qasper (question answering over NLP papers), NarrativeQA (question answering about entire books from Project Gutenberg), QuALITY (multiple choice question answering about stories from Project Gutenberg), and Contract NLI (natural language inference dataset in the legal domain). How hard is SCROLLS? The authors test out two smart baselines (BART, and a Longformer Encoder-Decoder (LED)), and one dumb baseline (a basic pre-written heuristic). Based on the results, this seems like a really challenging task - a LED baseline with a 16384-token input length gets okay results, though BART gets close to it despite being limited to 1,024 tokens. This suggests two things: a) BART is nicely optimized, and b) it's not entirely clear the tasks in scrolls truly test for long-context reasoning. ""Our experiments highlight the importance of measuring not only whether an architecture can efficiently process a long language sequence, but also whether it can effectively model longrange dependencies,"" they write. Why this matters: ""Contemporary, off-the-shelf models struggle with these tasks"", the researchers write. In recent years, many machine learning benchmarks have been saturated within months of being released; how valuable SCROLLS turns out to be will be a combination of its hardness and its longevity. If SCROLLS gets solved soon, that'd indicate that AI systems are getting much better at reasoning about long-range information - or it could mean the SCROLL tasks are bugged and the AI systems have found a hack to get a decent score. Pay attention to the SCROLLS leaderboard to watch progress here. Read more: SCROLLS: Standardized CompaRison Over Long Language Sequences (arXiv). Check out the leaderboard here."
275,Baidu adds knowledge to a language model; US military + AI; how China thinks about AI governance - 2,http://eepurl.com/hRQPeD,2022-01-10,"#################################################### BAIDU's shows how to inject more knowledge into a language model: …ERNIE 3.0 shows how to teach a big neural net to use an external knowledge base… Baidu has developed ERNIE 3.0, an AI model that can use an external knowledge base to help it provide more accurate answers. Last year, an ERNIE 3.0 model won the highly competitive SuperGLUE challenge (Import AI 259). The special thing about ERNIE is that it fuses a big GPT-3-esque language model with a large external knowledge base. Massive scale: Baidu has also developed ERNIE 3.0 'Titan', a 260 billion parameter model that, Baidu says, ""is the largest Chinese dense pre-training model as far as we know"". In tests, ERNIE 3.0 Titan gets state-of-the-art results on a vast set of benchmarks that evaluate skills as diverse as question answering, text generation, text summarization, interpretation, and dialogue. Novel, heterogeneous chip cluster: Another interesting thing about this paper is the chips they train on - V100s and Huawei 'Ascend' processors. It's quite unusual to see hybrid training of this form, and it seems like Baidu felt it was interesting enough to invest some engineering resources in making it possible - the company augmented its 'PaddlePaddle' AI framework with "" distributed training technology, including fine-grained parallelism, heterogeneous hardware-aware training, and fault tolerance mechanism to train the 260B model on both Nvidia V100 GPU and Ascend 910 NPU clusters."" Why this matters: Most people seem to act like GPT-3 models are exclusively being developed by a small set of Western actors, most of whom get tagged using the pejorative 'big tech' brush. But papers like this show that GPT-3 models are a global phenomenon. We should remember that the world we live in is going to be increasingly defined by different cultures expressing themselves through increasingly large, sophisticated AI models. Read more: ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation (arXiv)."
282,Can we ever trust an AI?; what the future of semiconductors looks like; better images of AI - 2,http://eepurl.com/hQZI61,2021-12-27,"#################################################### Do language models dream of language models? …A Google researcher tries to work out if big LMs are smart - their conclusions matt surprise you… A Google researcher is grappling with the question of whether large language models (e.g, Google's LaMDA), understand language and have some level of sentience. In an entertaining blog post, he wrestles with this question, interspersing the post with conversations with a LaMDA agent. Some of his conclusions are that the model is essentially bullshitting - but the paradox is we trained it to give a convincing facsimile of understanding us, so perhaps bullshitting is logical? Do language models matter? I get the feeling that the author thinks language models might be on the path to intelligence. ""Complex sequence learning may be the key that unlocks all the rest,"" they write. ""Large language models illustrate for the first time the way language understanding and intelligence can be dissociated from all the embodied and emotional characteristics we share with each other and with many other animals."" Why this matters: I think large language models, like GPT3 or LaMDA, are like extremely dumb brains in jars with really thick glass - they display some symptoms of cognition and are capable of surprising us, but communicating with them feels like talking to something with a hard barrier in-between us and it, and sometimes it'll do something so dumb you remember it's a dumb brain in a weird jar, rather than a precursor to something super smart. But the fact that we're here in 2021 is pretty amazing, right? We've come a long way from Eliza, don't you think so? Read more: Do large language models understand us? (Blaise Aguera y Arcas, Medium). ​"
287,DeepMind builds a GPT-3 model; Catalan GLUE; FTC plans AI regs - 1,http://eepurl.com/hPQFJv,2021-12-13,"#################################################### Google thinks sparsity might be the route to training bigger and more efficient GPT-3 models: …GLaM shows that mixture of experts models keep getting better… Google has built GLaM, a 1.2 trillion parameter mixture-of-experts model. GLaM is a big language model, like GPT-3, but with a twist: it's sparse; MoE networks are actually a bunch of distinct networks all connected together, and when you pull inference off of one only a few sub-networks activate. This means that the parameter count in a sparse vs dense network isn't really comparable (so you shouldn't think 1.2 trillion MoE = ~6X larger than GPT-3). Why MoE is efficient: ""The experts in each layer are controlled by a gating network that activates experts based on the input data. For each token (generally a word or part of a word), the gating network selects the two most appropriate experts to process the data. The full version of GLaM has 1.2T total parameters across 64 experts per MoE layer with 32 MoE layers in total, but only activates a subnetwork of 97B (8% of 1.2T) parameters per token prediction during inference."" How well does it work: In tests, GLaM exceeds or is on-par with the performance of GPT-3 on 80% of zero-shot tasks and 90% of one-shot tasks. Like DeepMind's Gopher, part of the improved performance comes from the size of the dataset - 1.6 trillion tokens, in this case. Why this matters: For a few years, various Google researchers have been pursuing 'one model to learn them all' - that is, a single model that can do a huge number of diverse tasks. Research like GLaM shows that MoE networks might be one route to building such a model. Read more: More Efficient In-Context Learning with GLaM (Google blog)."
295,Tracking journalists with computer vision; spotting factory defects with AI; and what simulated war might look like - 2,http://eepurl.com/hPe-3n,2021-12-06,"#################################################### Can we make neural architecture search efficient? Alibaba thinks so: ...KNAS gets efficient by focusing on gradients... For many years, researchers have been trying to use neural architecture search (NAS) to get computers to help them figure out new designs for AI systems. The problem with the NAS approach, though, is that it's very inefficient and punishingly expensive in terms of compute, because you're getting an AI system to do a few training steps on thousand+ architecture permutations. Now, researchers with Peking University and Alibaba have tried to fix this with KNAS, a neural architecture search approach that can be significantly more efficient than prevailing techniques. How it works: KNAS doesn't emphasize training on different architectures, instead it emphasizes studying a specific feature of gradients trained on different architectures - which can be more efficient. ""Theoretical results show that the Gram matrix of gradients, short for GM, decides the convergence results,"" they write. ""It is a good signal showing that GM is likely to be a good proxy of downstream performance to evaluate the quality of architectures."" Does it work: Neural nets trained with KNAS can get performance roughly comparable with other NAS-built systems, but at a speedup of around 25-50X compared to other NAS approaches, on datasets like CIFAR100 and ImageNet-16.. They also use the approach to try to do text classification and are able to come up with a KNAS system that outperforms the widely-used RoBERTA-large model on a suite of text classification tasks. Things that make you go hmmmm: ""This work is partly supported by Beijing Academy of Artificial Intelligence (BAAI)"", the researchers write. BAAI is the entity behind Wu Dao, a somewhat mysterious 1trillion+ parameter model. Read more: KNAS: Green Neural Architecture Search (arXiv). Get the code here:KNAS (Jingjing-NLP, GitHub)."
326,Google & MIT’s  Confident Adaptive Language Modeling Uses Dynamic Compute Allocation to Achieve 3x Speedups,https://syncedreview.com/2022/07/19/google-mits-confident-adaptive-language-modeling-uses-dynamic-compute-allocation-to-achieve-3x-speedups/,2022-07-19,"There was a nineteenth-century saying that mocked the use of “a sledgehammer to crack a peanut.” Google AI researcher Tal Schuster echoes this concept in introducing the new paper Confident Adaptive Language Modeling. While acknowledging the tremendous power of transformer-based large language models (LLMs), Schuster notes that many of the predictions they work on “require only minimal effort.” It could be said that using the entire LLM in such cases amounts to a sledgehammer-like overkill. LLMs’ ever-increasing computation costs and associated inference slowdowns are the main bottlenecks impeding their practical application. Developed by a Google and MIT team, the proposed Confident Adaptive Language Modeling (CALM) framework addresses these issues by dynamically allocating different compute amounts to each input and generation timestep. CALM achieves up to 3x speedups on natural language processing (NLP) tasks while maintaining high model performance. The team summarizes their main contributions as: The proposed framework is based on a saturation theory: that the top-ranked prediction in LLMs remains unchanged after some layer and is propagated upward. The number of layers used by the model can thus be dynamically decided with regard to each input. Following this idea, the team develops an adaptive compute approach to dynamically allocate computational resources per input to reduce model complexity while maintaining good performance. This method is also referred to as “early-exiting.” Building on their analysis of the early-exiting paradigm, the team developed CALM as a principled method for increasing model efficiency. CALM leverages a distribution-free risk control technique for calibrating local, per-token exit decisions, such that model performance is provably maintained with arbitrarily high probability. CALM can dynamically allocate different amounts of compute per generated token, following explicitly defined tolerance levels based on the full generation output. In their empirical study, the team implemented CALM on top of the T5 encoder-decoder model and evaluated text-generation task performance on three datasets — CNN/DM, WMT EN-FR, and SQUAD. The results show that CALM can reduce model compute burdens and gain speedups of up to 3x while maintaining high performance.The paper Confident Adaptive Language Modeling is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
327,Amazon’s Sockeye 3: Neural Machine Translation With PyTorch That Is 126% Faster on GPUs,https://syncedreview.com/2022/07/18/amazons-sockeye-3-neural-machine-translation-with-pytorch-that-is-126-faster-on-gpus/,2022-07-18,"Anyone who regularly uses machine translation systems will have noticed huge performance improvements over the last few years, attributable to neural network-based models that have largely replaced the previous generation of phrase-based systems. Introduced in 2018, Sockeye is an open-source framework that offers fast and reliable PyTorch implementation for neural machine translation (NMT) and has been powering Amazon Translate and other NMT applications. Sockeye 2 was released in 2020. In the new paper Sockeye 3: Fast Neural Machine Translation with PyTorch, an Amazon team presents the latest version of the Sockeye toolkit for efficient training of stronger and faster models. Sockeye 3 achieves speeds up to 126 percent faster than other PyTorch implementations on GPUs and up to 292 percent faster on CPUs. Sockeye 3 optimizes a distributed mixed precision training strategy to yield faster calculations and speedups by fitting larger batches into memory. Moreover, it can scale to any number of GPUs and any size of training data by launching separate training processes that use PyTorch’s distributed data parallelism to synchronize updates. For inference design, Sockeye 3 uses static computation graphs to minimize the impacts of dynamic shapes and data-dependent control flow, enabling it to trace various model components via PyTorch’s JIT compiler. The developers also maintain backward compatibility with Sockeye 2 MXNet models — all models that were trained with Sockeye 2 can be converted to models running on Sockeye 3 with PyTorch. Sockeye 3 also introduces many new advanced features: It supports replacing the decoder’s self-attention layers with Simpler Simple Recurrent Units (SSRUs) and fine-tuning with parameter freezing, and enables users to specify arbitrary prefixes (sequences of tokens) on both the source and target sides for any input. In their empirical studies, the team compared Sockeye with benchmark NMT toolkits that included Fairseq (Ott et al., 2019) and OpenNMT (Klein et al., 2017). In the evaluations, Sockeye 3 achieved comparable or better performance on GPUs and CPUs: delivering a 15 percent improvement for batched GPU inference, +126 percent for non-batched GPU inference, and +292 percent for CPU inference. Overall, Sockeye 3 provides much faster model implementations and more advanced features for NMT. As with previous versions, It has been open-sourced under an Apache 2.0 license, and the Amazon team welcomes pull requests from community members. The code is available on the project’s GitHub. The paper Sockeye 3: Fast Neural Machine Translation with PyTorch is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
329,Colossal-AI Seamlessly Accelerates Large Models at Low Costs with Hugging Face,https://syncedreview.com/2022/07/13/colossal-ai-seamlessly-accelerates-large-models-at-low-costs-with-hugging-face/,2022-07-13,"Forbes News, the world’s leading voice, recently declared large AI models as one of six AI trends to watch for in 2022. As large-scale AI models continue their superior performances across different domains, trends emerge, leading to distinguished and efficient AI applications that have never been seen in the industry. For example, Microsoft-owned GitHub and OpenAI partnered to launch Copilot recently. Copilot plays the role of an AI pair programmer, offering suggestions for code and entire functions in real-time. Such developments continue to make coding easier than before. Another example released by OpenAI, DALL-E 2, is a powerful tool that creates original and realistic images as well as art from only simple text. One month later, Google announced its own robust text-to-image diffusion model called Imagen. Imagen delivers exceptional results and accelerates the race of large AI models to a climax. Image Generated by Imagen (left 2 col.) vs DALLE-2 (right 2 col.)“Greek statue of a man tripping over a cat” In recent years, the outstanding performance of model scaling has led to an escalation in the size of pre-trained models. Unfortunately, training and even simply fine-tuning large AI models are usually unaffordable, requiring tens or hundreds of GPUs. Existing deep learning frameworks like PyTorch and Tensorflow may not offer a satisfactory solution for very large AI models. Furthermore, advanced knowledge of AI systems is typically required for sophisticated configurations and optimization of specific models. Therefore, many AI users, such as engineers from small and medium-sized enterprises, can’t help but feel overwhelmed by the emergence of large AI models. In fact, the core reasons for the increased cost of large AI models are GPU memory restrictions and the inability to accommodate sizeable models. In response to all of this, Colossal-AI developed the Gemini module, which efficiently manages and utilizes the heterogeneous memory of GPU and CPU and is expected to help solve the mentioned bottlenecks. Best of all, it is completely open-source and requires only minimal modifications to allow existing deep learning projects to be trained with much larger models on a single consumer-grade graphics card. In particular, it makes downstream tasks and application deployments such as large AI model fine-tuning and inference much easier. It even grants the convenience of training AI models at home! Hugging Face is a popular AI community that strives to advance and democratize AI through open source and open science. Hugging Face has had success collating large-scale models into their own model hub with over 50,000 models, including trendy large AI models like GPT and OPT. HPC-AI Tech’s flagship open-source and large-scale AI system, Colossal-AI, now allows Hugging Face users to seamlessly develop their ML models in a distributed and easy manner. In the following paragraphs, we will take one of the most popular AI models in Hugging Face Hub, OPT from Meta, to demonstrate how to train and fine-tune your large AI models at a low cost with minimal modifications to your code.Open source code: https://github.com/hpcaitech/ColossalAI Meta recently released Open Pretrained Transformer (OPT), a 175-Billion parameter AI language model. To encourage AI democratization in the community, Meta has released both the code and trained model weights, which stimulates AI programmers to perform various downstream tasks and application deployments. We will now demonstrate fine-tuning Casual Language Modelling with pre-training weights of the OPT model provided by Hugging Face Hub. It is very simple to use the powerful features of Colossal-AI. Users only need a simple configuration file, and are not required to alter their training logic to equip models with their desired features (e.g. mixed-precision training, gradient accumulation, multi-dimensional parallel training, and memory redundancy elimination).Suppose we intend to develop the OPT on one GPU. We can accomplish this by leveraging heterogeneous training from Colossal-AI, which only requires users to add relevant items to the configuration files. Among the items added, tensor_placement_policy, which can be configured as cuda, cpu, or auto, determines our heterogeneous training strategy. Each training strategy has its distinct advantages: For typical users, they can just select the auto strategy, which maximizes training efficiency by dynamically adapting its heterogeneous strategy with respect to its current memory state. With the configuration file ready, only a few lines of code are needed for the newly declared functions.Firstly, awaken Colossal-AI through a single line of code in the configuration file. Colossal-AI will automatically initialize the distributed environment, read in configuration settings, and integrate the configuration settings into its components (i.e. models and optimizers). After that, users may define their own datasets, models, optimizers, and loss functions per usual, or by using raw PyTorch code. Only their models need to be initialized under ZeroInitContext. In the given example, we adopt the OPTForCausalLM model along with its pretrained weights by Hugging Face and make adjustments to the Wikitext dataset. Next, use colossalai.initialize to integrate heterogeneous memory functions defined in the configuration file, into the training engine to enable the feature. On a single GPU, Colossal-AI’s automatic strategy provides remarkable performance gains from the ZeRO Offloading strategy by Microsoft DeepSpeed. Users can experience up to a 40% speedup, at a variety of model scales. However, when using a traditional deep learning training framework like PyTorch, a single GPU can no longer support the training of models at such a scale. Adopting the distributed training strategy with 8 GPUs is as simple as adding a -nprocs 8 to the training command of Colossal-AI! Such remarkable improvements come from Colossal-AI’s efficient heterogeneous memory management system, Gemini. To put it simply, Gemini uses a few warmup steps during model training to collect memory usage information from PyTorch computational graphs. After warm-up, and before performing each operation, Gemini pre-allocates memory for the operator equivalent to its peak usage based on the collected memory usage records. At the same time, it re-allocates some model tensors from GPU memory to CPU memory. The inbuilt memory manager by Gemini attaches a state to each tensor, including HOLD, COMPUTE, FREE, etc. Based on the queried memory usage, the manager constantly converts the tensor states, and adjusts tensor positions. Compared to the static memory classification by DeepSpeed’s ZeRO Offload, Colossal-AI Gemini employs a more efficient use of GPU and CPU memory, maximizes model capacities, and balances training speeds, all with small amounts of hardware equipment. For the representative of large models, GPT, Colossal-AI is capable of training up to 1.5 billion parameters on a gaming laptop with RTX 2060 6GB. For a PC with RTX3090 24GB, Colossal-AI can train GPT with 18 billion parameters. Colossal-AI can also bring significant improvements to high performance graphics cards such as a Tesla V100. Parallel and distributed technologies are vital methods to further accelerate model training. To train the world’s largest and most advanced AI models within the shortest time, efficient distributed parallelization is still a necessity. Issues found in existing solutions include limited parallel dimension, low efficiency, poor versatility, difficult deployment, and lack of maintenance. With this in mind, Colossal-AI uses technologies such as efficient multi-dimensional parallelism and heterogeneous parallelism to allow users to deploy large AI models efficiently and rapidly with minimal modifications to their code. To counter complications arising from data, pipeline, and 2.5D parallelism simultaneously, a simple line of code declaration suffices with Colossal-AI. The typical system/framework method of hacking into underlined code logic is no longer necessary. For a super-large AI model such as GPT-3, Colossal-AI only needs half the computing resources compared to the NVIDIA solution to start training. If the same computing resources were used, the speed could be further increased by 11%, which could reduce the training cost of GPT-3 by over a million dollars. In theory, this sounds fantastic, but what about in practice? Colossal-AI has proven its capabilities in application to real-world issues across a variety of industries, including autonomous driving, cloud computing, retail, medicine, and chip production. For AlphaFold, which is used for protein structure prediction, our team has introduced FastFold, based on the Colossal-AI acceleration scheme. FastFold has successfully surpassed other schemes including those proposed by Google and Columbia University. It successfully reduces the training time of AlphaFold from 11 days to 67 hours, simultaneously lowering the overall cost. Moreover, the process of long sequence inference is accelerated by about 9.3 to 11.6 times. Colossal-AI values open-source community construction. We offer detailed tutorials and support the latest cutting-edge applications such as PaLM and AlphaFold. Colossal-AI will regularly produce new and innovative features. We always welcome suggestions and discussions and would be more than willing to help if you encounter any issues. You can raise an issue here or create a discussion topic in our forum. Your suggestions are highly appreciated. Recently, Colossal-AI reached No. 1 in trending projects on Github and Papers With Code, together with projects that have as many as 10K stars. The open-source code is on Project’s GitHub. Referencehttps://medium.com/@yangyou_berkeley/colossal-ai-seamlessly-accelerates-large-models-at-low-costs-with-hugging-face-4d1a887e500d We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
331,A New Network Design Direction? DeepMind Examines How Networks Generalize and Climb the Chomsky Hierarchy,https://syncedreview.com/2022/07/11/a-new-network-design-direction-deepmind-examines-how-networks-generalize-and-climb-the-chomsky-hierarchy/,2022-07-11,"Reliable generalization to out-of-distribution inputs is a crucial feature for developing strong machine learning models. But determining how and why neural networks are able to generalize on algorithmic sequence prediction tasks remains an open question. In the new paper Neural Networks and the Chomsky Hierarchy, a DeepMind research team conducts an extensive generalization study on neural network architectures that explores whether insights from the theory of computation and the Chomsky hierarchy can predict the practical limits of neural network generalization. The team summarizes their main contributions as: Many previous works have investigated whether conventional neural network architectures are able to learn a formal language. While these studies have typically focused on single architectures and a limited set of tasks, the DeepMind paper presents an extensive empirical study on a wide range of models with regard to the Chomsky hierarchy. Named after the influential American linguist and philosopher who developed it, the Chomsky hierarchy is basically a containment hierarchy of formal grammar (unrestricted grammar, context-sensitive grammar, context-free grammar, and regular grammar) that classifies languages based on the type of automaton able to recognize them. By relating different models to the Chomsky hierarchy, it is possible to determine whether they can recognize certain regular languages. The researchers note that lower-level automata have restrictive memory models and can only solve lower-level problem sets, while atop the hierarchy, Turing machines with infinite memory and unrestricted memory access can solve all computable problems, i.e. are Turing complete. The paper examines a wide range of neural network architectures and memory-augmented neural networks — transformer, RNN, LSTM, Stack-RNN, NDStack-RNN and Tape-RNN — covering a total of 2200 models applied to 16 sequence-prediction tasks. The results show that LSTMs and transformers are not Turing complete as they cannot solve simple sequence tasks such as duplicating a string when the sequences are significantly longer than those seen during training. Models interacting with an external memory structures meanwhile can climb the Chomsky hierarchy, indicating this setup as a promising research direction for improving architecture design.The code is publicly available on the project’s GitHub. The paper Neural Networks and the Chomsky Hierarchy is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
332,Salesforce’s CodeRL Achieves SOTA Code Generation Results With Strong Zero-Shot Transfer Capabilities,https://syncedreview.com/2022/07/07/salesforces-coderl-achieves-sota-code-generation-results-with-strong-zero-shot-transfer-capabilities/,2022-07-07,"Large-scale pretrained language models (LMs) have shown promising results on simple code generation tasks, but they have several limitations: training models with only next-token prediction objectives leads to accumulating errors, and neglecting potentially meaningful signals from unit tests results in poor generalization capability when facing complex unseen coding tasks. A Salesforce Research team addresses these issues in the new paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning, proposing CodeRL, a novel framework for program synthesis tasks that employs pretrained LMs and deep reinforcement learning (RL) and achieves state-of-the-art performance on the challenging APPS benchmark while demonstrating impressive zero-shot transfer capabilities. The team extends the Salesforce CodeT5 (Wang et al., 2021) unified pretrained encoder-decoder transformer architecture as CodeRL’s backbone. Although CodeT5 pretraining tasks such as masked span prediction (MSP) can benefit code understanding tasks, they do not necessarily align with program synthesis objectives. To mitigate this, a next-token prediction (NTP) pretraining task is integrated into CodeT5 to uniformly sample a pivot location for each code sample, then pass the content preceding the pivot to the encoder and the remaining content to the decoder. The researchers formulate CodeRL’s program synthesis as an RL problem and introduce an actor-critic approach to improve model performance by utilizing the unit test signals in both the model optimization and generation processes. The team conducted experiments on the challenging APPS (Automated Programming Progress Standard) code generation benchmark (Hendrycks et al., 2021) to evaluate the performance of the proposed CodeRL; and used the MBPP (Mostly Basic Programming Problems) benchmark (Austin et al., 2021) to evaluate its zero-shot ability. On APPS, researchers compared their models with strong conventional baselines that included GPT-2, GPT-Neo, GPT3, Codex, and AlphaCode, where CodeRL with CodeT5 achieved new SOTA results of 2.69 percent pass@1, 6.81 percent pass@5, and 20.98 percent pass@1000. On MBPP, CodeRL with CodeT5 obtained surprisingly good zero-shot performance, achieving a new SOTA of 63.0 percent pass@80 over GPT-137B’s 61.4 percent pass@80. This work shows that the CodeRL method can effectively leverage unit test signals to push code generation performance to new SOTA performance and achieve strong zero-shot transfer capabilities. The code is available on the project’s GitHub. The paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
334,Learning Bellman Complete Representations for Offline Policy Evaluation,"[{'href': 'http://arxiv.org/abs/2207.05837v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2207.05837v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-07-12 21:02:02,"JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 An Empirical Evaluation of Four Off-the-Shelf Proprietary Visual-Inertial Odometry Systems Jungha Kim, Minkyeong Song, Yeoeun Lee, Moonkyeong Jung, and Pyojin Kim∗ 2 2 0 2 l u J 4 1 ] O R . s c [ 1 v 0 8 7 6 0 . 7 0 2 2 : v i X r a Abstract—Commercial visual-inertial odometry (VIO) systems have been gaining attention as cost-effective, off-the-shelf six degrees of freedom (6-DoF) ego-motion tracking methods for estimating accurate and consistent camera pose data, in addition localization from to their ability to operate without external is unclear motion capture or global positioning systems. It from existing results, however, which commercial VIO platforms are the most stable, consistent, and accurate in terms of state estimation for indoor and outdoor robotic applications. We assess four popular proprietary VIO systems (Apple ARKit, Google ARCore, Intel RealSense T265, and Stereolabs ZED 2) through a series of both indoor and outdoor experiments where we show their positioning stability, consistency, and accuracy. We present our complete results as a benchmark comparison for the research community. Index Terms—Commercial visual-inertial odometry, Apple ARKit, Google ARCore, Intel T265, Stereolabs ZED 2 I. INTRODUCTION T HIS article presents a benchmark comparison of off-the- shelf proprietary visual-inertial odometry (VIO) systems used for autonomous navigation of robotic applications, which are the process of determining the position and orientation of a camera-inertial measurement unit (IMU)-rig in 3D space by analyzing the associated camera images and IMU data. As the VIO research has reached a level of maturity, there exist several open published VIO methods such as MSCKF [1], OKVIS [2], VINS-Mono [3], and many commercial prod- ucts utilize closed proprietary VIO algorithms such as Apple ARKit [4], Google ARCore [5] that offer off-the-shelf VIO pipelines which can be employed on an end-user’s system of choice. The current research studies provide some comparative experiments on the performance of the popular VIO ap- proaches, however, they consider only a subset of the existing open-source and proprietary VIO algorithms, and conduct insufﬁcient performance evaluation only on publicly-available datasets rather than indoor and outdoor challenging real-world environments. In particular, although commercial VIO systems (Intel T265, Stereolabs ZED 2) play an important role in the several DARPA challenges [6], [7] and many commercial products or apps (Pok´emon GO, IKEA Place AR), there is a lack of research for benchmarking the positioning accuracy of these closed proprietary VIO platforms. The motivation of this paper is to address this deﬁciency by performing a comprehensive evaluation of off-the-shelf All authors are with Department Systems Engineering, Sookmyung Women’s University, Seoul, South Korea. {alice3071,smk615,snwfry,jmk7791,pjinkim}@ sookmyung.ac.kr (∗ Corresponding author: Pyojin Kim) of Mechanical Fig. 1. The custom-built capture rig for benchmarking 6-DoF motion tracking performance of Apple ARKit (iPhone 12 Pro Max), Google ARCore (LG V60 ThinQ), Intel RealSense T265, and Stereolabs ZED 2. commercially-available VIO systems in challenging indoor and outdoor environments as shown in Fig. 2. This is the ﬁrst comparative study on four popular proprietary VIO systems in six challenging real-world environments, both indoors and outdoors. Especially, we select the following four proprietary VIO systems that are frequently used in autonomous driving robotic applications: • Apple ARKit [4] - Apple’s augmented reality (AR) plat- form, which includes ﬁltering-based VIO algorithms [8] to enable iOS devices to sense how they move in 3D space. • Google ARCore [5] - Google’s AR platform utilizing a multi-state constraint Kalman ﬁlter (MSCKF) style VIO algorithm [1], [9], called concurrent odometry and mapping (COM) [10]. • Intel RealSense T265 [11] - a stand-alone VIO and simultaneous localization and mapping (SLAM) tracking device developed for use in robotics, drones, and more, with all position computations performed on the device. • Stereolabs ZED 2 [12] - a hand-held stereo camera with built-in IMU for neural depth sensing and visual-inertial stereo, requiring an external NVIDIA GPU to obtain the 6-DoF camera poses. We do not consider open-source published VIO methods and non-inertial visual simultaneous localization and map- ping (SLAM) algorithms, for example ROVIO [13], VINS- Mono [3], ORB-SLAM [14], and DSO [15]. We focus on the off-the-shelf commercial VIO/SLAM products that might be of interest to more researchers and engineers because open pub- lished VIO algorithms are relatively difﬁcult to understand and operate, and their comparisons are made in the literature [16], [17] to some extent. figure_capture_rig.pdf Google ARCore (LG V60 ThinQ) Apple ARKit (iPhone 12 Pro Max) Intel RealSense T265 Stereolabs ZED 2 JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2 Fig. 2. Accumulated 3D point cloud (middle) with the estimated 6-DoF trajectory (red) from Apple ARKit in multi-ﬂoor environments. We capture the 6-DoF camera poses and 3D points while climbing the multi-story stairs (left). Among the four proprietary VIO systems, Apple ARKit shows the most consistent and accurate 6-DoF motion tracking results, consistently reconstructing the 3D geometry of stairs and hallways. The Apple ARKit track (red) and 3D reconstruction results have a similar shape as the ground-truth blueprint of a building (right). Our experiments are conducted in six challenging indoor and outdoor environments with the custom-built rig equipped with the four VIO devices as illustrated in Fig. 1. Our test sequences contain long and narrow corridors, large open spaces, repetitive stairways, an underground parking lot with insufﬁcient lighting, and about 3.1 kilometers of a vehicular test in complex urban trafﬁc environments. test Our goal is to provide a thorough benchmark of closed proprietary VIO systems, in order to provide a reference for researchers on VIO products, as well as readers who require an off-the-shelf 6-DoF state estimation solution that is suitable for their robotic platforms and autonomous vehicles. II. RELATED WORK Despite proprietary VIO systems being utilized in many products and areas for industrial usage (e.g., for building an accurate indoor map, as a precise positioning system, etc.), there is no benchmark study that satisﬁes our proposed goals. While comprehensive comparisons of open-source published VIO methods exist [16], they focus only on evaluating the popular academic VIO algorithms on the EuRoC micro aerial vehicle dataset [18], and do not cover off-the-shelf proprietary VIO systems and various indoor and outdoor environments. Although ADVIO [19] presents a VIO comparison including three proprietary platforms and two academic approaches, its main contribution is to develop a set of RGB and IMU smartphone datasets, not a performance evaluation between the proprietary VIO platforms. In [20], [21], some comparative studies of proprietary VIO systems have been performed, they consider only a few proprietary VIO platforms. but Performance evaluation is only conducted in a simple 2D indoor environment with a short camera moving distance. Since we focus on the 6-DoF positioning accuracy of the proprietary VIO systems, we can instead consider the existing results relevant to this problem. The proposed VIO approach in [22] compares to Google ARCore and VINS-Mono [3], but only on a few indoor sequences with very little camera movement. The evaluation framework in [23] assesses the 6- DoF motion tracking performance of ARCore with the ground truth under several circumstances, but they lack comparative results for other proprietary VIO systems such as ARKit and T265, and detailed analyses are performed only for ARCore. Most important is that no existing work considers an in- door/outdoor performance evaluation for four popular propri- etary VIO systems that are frequently deployed on robotic applications, AR/VR apps, and industrial usages. Our test se- quences are authentic and illustrate realistic use cases, contain- ing challenging environments with scarce or repetitive visual features, both indoors and outdoors, and varying motions from walking to driving camera movements. They also include rapid rotations without translation as they are problematic motions for many VIO/SLAM algorithms. III. VISUAL-INERTIAL ODOMETRY SYSTEMS We brieﬂy summarize the primary features of four off- the-shelf proprietary VIO systems based on data published on the relevant ofﬁcial websites, papers, GitHub, and patent documents, and how to collect 6-DoF pose estimates from each VIO mobile device. Since most proprietary VIO/SLAM platforms are all closed-source, we do not cover the detailed VIO academic backgrounds and implementations. A. Apple ARKit Apple ARKit [4] is Apple’s augmented reality (AR) soft- ware framework, which includes a tightly-coupled ﬁltering- based VIO algorithm similar to the MSCKF [1] to enable iOS devices to sense how they move in 3D space. It contains a sliding window ﬁlter, bundle adjustment, motion/structure marginalization modules [8], and is expected to be applied to various robotic applications such as Apple Glasses and Car in the future, not just for iPhone and iPad, which is why we conduct vehicle tests in this benchmark. We develop a custom iOS data collection app1 for capturing ARKit 6-DoF camera poses, RGB image sequences, and IMU measurements using an iPhone 12 Pro Max running iOS 14.7.1. It saves the pose estimates as a translation vector and a unit quaternion at 60 Hz, 1https://github.com/PyojinKim/ios logger figure_front_page.pdf Floor 5 Floor 4 Floor 3 Floor 2 Floor 1 19.2 m 13.2 m 9.2 m 4.6 m 0.0 m JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3 and each pose is expressed in a global coordinate frame created by the phone when starting iOS data collection. Although there are various iPhone and iPad models, the core VIO algorithm in ARKit is the same, thus we empirically conﬁrm that there is little difference in the VIO performance of each device. B. Google ARCore ARCore [5] is Google’s platform for building AR ex- periences utilizing the multi-state constraint Kalman ﬁlter (MSCKF) style VIO/SLAM algorithms [9], [24] with many subsequent variations, called concurrent odometry and map- ping (COM) [10]. ARCore is a successor to Google Project Tango [25], and is currently applied only to Android OS smartphones, but it would be extended to various robotic platforms such as Google Wing, Maps, and Waymo, which is why we evaluate ARCore in a large-scale outdoor sequence of about 3.1 kilometers of a vehicular test. We build a custom Android OS app based on Google’s ARCore example2 to acquire ARCore 6-DoF camera poses and IMU measurements at 30 Hz with an LG V60 ThinQ running Android 10.0.0 and ARCore 1.29. Although there are various Android OS devices such as Samsung Galaxy and Google Pixel, smartphones on the list3 certiﬁed by Google demonstrate similar motion tracking performance regardless of device model. C. Intel RealSense T265 a hassle-free Intel RealSense T265 stand-alone is VIO/SLAM device to track its own position and orientation in 3D space. The embedded processor, vision processing unit (VPU), runs the entire VIO algorithm onboard, analyzes the image sequences from stereo ﬁsheye cameras and fuses all sensor information together. Since the T265 VIO algorithm runs on the device itself without using the resource of the host computer, is widely used as a 6-DoF positioning sensor in 3D space for various robotic applications such as DARPA challenges [6] and autonomous ﬂying drones [26]. We collect the 6-DoF motion tracking results at 200 Hz using Intel RealSense SDK 2.04, and save the T265 6-DoF camera poses by connecting it to an Intel NUC mini PC. it D. Stereolabs ZED 2 Stereolabs ZED 2 is a hand-held stereo camera with built-in IMU for neural depth sensing, 6-DoF VIO/SLAM, and real- time 3D mapping. Stereolabs has not made their VIO/SLAM algorithm public, and the description of the VIO algorithm is relatively vague compared to other proprietary VIO systems. It is one of the popular stereo camera sensors for various robotic applications such as drone inspection [27], but has the disadvantage of requiring an external NVIDIA GPU to perform positional tracking and neural depth sensing. We develop a program to collect the ZED 2 6-DoF camera poses at 30 Hz based on ZED SDK 3.5.25 on an NVIDIA Jetson Nano onboard computer. 2https://github.com/rfbr/IMU and pose Android Recorder 3https://developers.google.com/ar/devices 4https://github.com/IntelRealSense/librealsense 5https://www.stereolabs.com/developers/release/ Fig. 3. We carry the capture rig by hand, and store the onboard computers and batteries to collect the motion data indoors (left). In the outdoor vehicular tests, we ﬁx the capture rig to the front passenger seat (right). IV. EXPERIMENTS We evaluate four proprietary VIO systems with the four devices (iPhone 12 Pro Max, LG V60 ThinQ, Intel T265, ZED2) attached to the custom-built capture rig as shown in Fig. 1 and Fig. 3 on the large-scale challenging indoor and outdoor environments, both qualitatively and quantitatively. Indoors, we record the motion data by a walking person, and outdoors, the data is collected by rigidly attaching the capture rig to a car in Fig. 3. We save the 6-DoF pose estimates of ARKit and ARCore through the custom apps on each smartphone device, and record the moving trajectories of T265 and ZED2 in the Intel NUC and NVIDIA Jetson Nano onboard computers. We maintain the default parameter settings of each VIO platform, and deactivate all capabilities related to SLAM (e.g., loop closure) for a fair comparison between each VIO system. Furthermore, in order to interpret the motion tracking results in the same reference coordinate frame, we calibrate the intrinsic and extrinsic parameters of all cameras by capturing multiple views of a checkerboard. Our benchmark dataset contains various indoor and outdoor sequences in six different locations, and the total length of each sequence ranges from 83 m to 3051 m, which are primarily designed for benchmarking medium and long-range VIO performance. There are three indoor and three outdoor sequences, and all indoor sequences are captured in a 7-story building in the university campus including long corridors, open hallway spaces, and stair climbs as shown in the top row of Fig. 4. The indoor cases are as realistic as possible contain- ing repetitive motion in stairs, temporary occlusions, and areas lacking visual features. The bottom row of Fig. 4 illustrates example frames from three outdoor sequences acquired in the outdoor university campus, underground parking lot, and urban outdoor roads. In order to evaluate the performance of each VIO system quantitatively without an external motion capture system, we coincide the start and end points of the movement trajectories in all experiments, and measure the ﬁnal drift error (FDE) metric, which is the end point position error in meter. We report the quantitative evaluation results of four VIO systems in Table I. The smallest end point position error for each sequence is indicated in bold. The ideal FDE value (the ground-truth path) should be 0, and a large FDE value denotes an inaccurate position estimate since we deﬁne the starting point of movement as the origin. In addition, by overlaying the estimated VIO trajectories on the ﬂoorplan of the building figure_external_view.pdf Google ARCore Apple ARKit Intel T265 ZED 2 Laptop Google ARCore Apple ARKit Battery Intel NUC & NVIDIA Jetson Nano Stereolabs ZED 2 Intel RealSense T265 JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4 Fig. 4. Example image frames from indoor and outdoor benchmark datasets. The top row represents three indoor sequences by foot including long corridors (a), open hallway spaces (b), and repetitive stairs (c) from a university building. We acquire the camera motion data in the outdoor campus on foot (d), and through a car in the underground parking lot (e) and urban outdoor roads (f). TABLE I EVALUATION RESULTS (FDE) OF FOUR PROPRIETARY VIO SYSTEMS Experiment ARKit ARCore T265 ZED 2 Length (m) Indoor Corridor Indoor Hallway Indoor Stairs Outdoor Campus Parking Lot Outdoor Roads 0.79 0.14 0.19 2.01 0.26 2.68 0.12 0.09 3.98 0.07 1.14 140.08 1.88 0.61 1.49 4.08 9.01 × 1.44 4.58 4.76 206.38 10.85 409.25 145.21 83.98 114.13 513.81 446.26 3051.61 or Google Maps, we evaluate the consistency, stability, and reliability of each VIO system qualitatively. A. Indoor Long Corridors and Open Hallway Sequences We evaluate four VIO systems in a long U-shape corridor and open hallway spaces easily found in typical ofﬁce and uni- versity buildings as shown in Fig. 5. Fig. 4 (a) and (b) illustrate example frames from both locations. The trajectories of these sequences are approximately 145 and 84 meters, and include 5 and 11 pure rotational movements and difﬁcult textures. In a long U-shape corridor and open hallway sequences, the start and end points of ARKit (red) meet at the black circle without a severe rotational drift while maintaining the orthogonality and scale of the estimated trajectory well compared with the ﬂoorplan. Although ARCore (green) shows the most accurate results in terms of the FDE metric in Table I, the estimated VIO trajectory does not match the ﬂoorplan well. Intel T265 (blue) estimates accurate 3-DoF rotational motion well, but there is a problem with the scale of the moving trajectory compared to the ﬂoorplan, showing a little larger trajectory than the actual movements. ZED2 (magenta) presents the most inaccurate and inconsistent positioning performance among the four VIO methods as the rotational motion drift error gradually accumulates over time. Overall, the estimated VIO trajectories by ARKit (red) are the most similar and consistent motion tracking results to the actual movements following the shape of the corridor on the ﬂoorplan. B. Indoor Multistory Stairs Sequence We perform a comparative experiment in a multi-ﬂoor staircase environment with the 114 m trajectory going up the stairs from the 2nd basement ﬂoor (B2) to the 5th ﬂoor (5F) of a building in Fig. 6. The repetitive rotational motion included in the 3D trajectory of climbing the stairs makes VIO positioning challenging. Fig. 4 (c) shows example frames from multistory stair sequence. In the top view (xy-plane), we start and end at the same points marked in the black circle to check loop closing in the estimated VIO trajectories. ARKit (red) has the best performance; the top and side views of ARKit (red) show the overlapped, consistent 6-DoF motion tracking results while other VIO systems gradually diverge from the initially estimated loop. With ARKit (red), the starting and ending points in xy-plane (top view) nearly match; for the others, they do not. The ﬁnal drift error (FDE) of ARKit in xy-plane is 0.19 m, while ARCore, T265, and ZED2 are 3.98 m, 1.49 m, and 4.76 m, respectively. In particular, ZED2 (magenta) has the most severe trajectory distortion in the z-axis direction (height) among the four VIO systems. Fig. 6 illustrates the side and front views of the stairway with the paths from four VIO devices, showing high consistency of ARKit (red) compared to other VIO platforms. It is noteworthy that the height of each ﬂoor estimated by ARKit and the actual height (the ground- truth) from the building blueprint are approximately identical. C. Outdoor University Campus Sequence We choose an outdoor location in the university campus approximately 513 m to determine which VIO system works well in the environment of the rapid change of the topogra- phy, and the narrow road returning as shown in the left of Fig. 7. Example frames are shown in Fig. 4 (d). It shows the resulting 6-DoF trajectories from four VIO platforms overlaid on Google Maps, demonstrating that the start and end points of ARKit (red) and ARCore (green) meet while matching well with the shape of the roads shown on Google Maps. The shape of the estimated trajectory of T265 (blue) is very figure_example_frames.pdf (a) Indoor Corridor (b) Indoor Hallway (c) Indoor Stairs (d) Outdoor Campus (e) Underground Parking Lot (f) Urban Outdoor Roads JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5 Fig. 5. Estimated trajectories with four proprietary VIO systems in a long U-shape corridor (left) and open hallway space (right) sequences. We start and end at the same point marked in the black circle to evaluate the loop closing performance of tested commercial VIO systems. The estimated paths for ARKit (red) match the building ﬂoorplan most consistently, and only the starting and ending points of ARKit nearly meet; for the others, they do not. due to its inaccurate rotation estimation, showing the most severe distortion of the actual movement trajectory among the four VIO systems as shown in the left of Fig. 7. D. Outdoor Urban Roads and Parking Lot Sequences We perform an outdoor vehicle driving experiment with a mileage of approximately 3 km by attaching the capture rig to the vehicle as shown in the right of Fig. 7. Fig. 4 (e) and (f) show example frames from the underground parking lot and urban outdoor roads. We acquire the motion data while driving on public automobile roads near Seoul Station in Seoul, and there are plenty of moving people, cars, and occasional large vehicles visible in the outdoor environments, which makes mo- tion tracking of VIO challenging. Even in high-speed driving conditions, sometimes exceeding 60 km/h, ARKit (red) shows surprisingly accurate and consistent 6-DoF motion tracking results overlaid on Google Maps as shown in the right of Fig. 7. The start and end points of ARKit (red) accurately meet in the black circle, and the ﬁnal drift error (FDE) is only 2.68 m in Table I. ARCore (green) occasionally fails when the speed of the car increases or the light variations occur abruptly. In T265 (blue), if the car stops temporarily due to a stop signal or is driving too fast, the VIO algorithm diverges and fails to estimate the location. ZED2 (magenta) accumulates rotational drift error over time, resulting in inaccurate motion estimation results. While four VIO systems perform relatively well in the previous walking sequences, this is not the case in the more challenging vehicular test, which is not ofﬁcially supported by any of the tested VIO devices. Only ARKit is able to produce stable motion tracking results even in a vehicular test. We conduct an additional vehicular test driving the same trajectory repeatedly in a dark underground parking lot with poor visual conditions as shown in Fig. 8. The total traveling distance is about 450 m, and we drive the car at a low speed from 5 to 15 km/h. Although ARKit does not restore the actual movements perfectly in the parking lot, ARKit (red) shows the Fig. 6. Comparison of four VIO systems on multi-story stairs from the 2nd basement ﬂoor (B2) to the 5th ﬂoor (5F). It shows the side (left), front (right- bottom), and top (right-top) views of the estimated VIO trajectories. ARKit has the most consistent camera motions along with the shape of the stairs, and only matches the start and end points marked in the black circle. similar to ARKit and ARcore’s results, however, the scale of the estimated path of T265 is smaller than the actual movements. T265 suffers from a scale inconsistency problem, which is generally observed in monocular visual odometry conﬁguration. The orthogonality of ZED2 (magenta) is broken JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6 Fig. 7. Estimated motion trajectories of four proprietary VIO systems in an outdoor campus (left) and urban outdoor roads (right) sequences overlaid on Google Maps. We start and end at the same point marked in the black circle to check loop closing performance. ARKit (red) tracks the 6-DoF camera poses well following the shape of the roads on Google Maps most consistently and accurately. Only ARKit (red) is able to produce stable motion tracking performance even when driving a vehicle over 60 km/h (right). in realistic use cases where people and vehicles are very crowded not only indoors but also outdoors. Google ARCore exhibits accurate and consistent motion tracking performance next to ARKit. ARCore works well for indoor sequences and the motion data collected by a walking person, but it diverges or the VIO algorithm deteriorates sharply when moving rapidly or in poor lighting conditions. Intel RealSense T265 shows good positioning performance just behind Google ARCore. T265 operates 6-DoF motion tracking indoors not badly, however, it has a problem of scale inconsistency issue when estimating the moving path larger or smaller than the scale of the actual movements. Also, T265’s motion tracking sometimes fails if the moving speed is too slow or fast. The motion tracking performance of Stereolabs ZED 2 is the most inconsistent and inaccurate among the four VIO de- vices for indoors and outdoors. As the 6-DoF motion tracking progresses, the rotational error occurs most severely, and this rotation error accumulates over time, resulting in an incorrect path in which the starting and ending points are very different. In particular, ZED2 exhibits a tendency that it cannot track a straight path correctly when we actually move in a straight line outdoors, and rotational drift error is more severe when moving fast. VI. CONCLUSION Fig. 8. Example paths in the underground parking lot overlaid on the ﬂoorplan to evaluate the consistency and accuracy. The trajectories of ARKit (red) overlap signiﬁcantly, but the paths of the other VIO devices suffer from a rotational drift, showing inaccurate and inconsistent positioning results. overlapped, consistent motion estimation results while other VIO systems gradually diverge from the initially estimated loop. Since we perform the evaluation at a relatively low speed (10 km/h) compared to the previous vehicle test (60 km/h), other VIO systems do not diverge or fail at all. Among four VIO methods, the ZED2 positioning results are the most deviating from the actual movements in the underground parking lot. V. DISCUSSION Overall, Apple ARKit demonstrates the most consistent, accurate, reliable, and stable motion tracking results among the four VIO systems across both indoor and outdoor uses. ARKit performs well and robustly in various real-world challeng- ing environments such as sudden camera movements, abrupt changes in illumination, and high-speed movements with very rare cases where tracking failure or motion jump occurs. ARKit achieves accurate and robust positioning performance We have conducted a survey of the 6-DoF ego-motion tracking performance of four off-the-shelf proprietary VIO platforms in challenging indoor and outdoor environments. To the best of our knowledge, this is the ﬁrst back-to-back comparison of ARKit, ARCore, T265, and ZED2, demon- strating that Apple ARKit performs well and robustly in most indoor and outdoor scenarios. We hope that the results and conclusions presented in this paper may help members of the research community in ﬁnding appropriate VIO platforms for their robotic systems and applications. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7 REFERENCES [1] A. I. Mourikis and S. I. Roumeliotis, “A multi-state constraint kalman ﬁlter for vision-aided inertial navigation,” in IEEE ICRA, 2007. [2] S. Leutenegger, P. Furgale, V. Rabaud, M. Chli, K. Konolige, and R. Siegwart, “Keyframe-based visual-inertial slam using nonlinear op- timization,” Proceedings of Robotis Science and Systems (RSS) 2013, 2013. [3] T. Qin, P. Li, and S. Shen, “Vins-mono: A robust and versatile monocular visual-inertial state estimator,” IEEE Transactions on Robotics, 2018. [4] “Apple ARKit,” https://developer.apple.com/documentation/arkit/, Ac- cessed: 2022-02-22. [5] “Google ARCore,” https://developers.google.com/ar, Accessed: 2022- 02-22. [6] T. Rouˇcek, M. Pecka, P. ˇC´ıˇzek, T. Petˇr´ıˇcek, J. Bayer, V. ˇSalansk`y, D. Heˇrt, M. Petrl´ık, T. B´aˇca, V. Spurn`y, et al., “Darpa subterranean challenge: Multi-robotic exploration of underground environments,” in International Conference on Modelling and Simulation for Autonomous Systems. Springer, 2019. [7] P. Root, “Fast lightweight autonomy (ﬂa),” Defense Advanced Research Projects Agency, https://www. darpa. mil/program/fast-lightweight- autonomy [retrieved 31 Dec. 2018], 2021. [8] A. Flint, O. Naroditsky, C. P. Broaddus, A. Grygorenko, S. Roumeliotis, and O. Bergig, “Visual-based inertial navigation,” Dec. 11 2018, US Patent 10,152,795. [9] A. I. Mourikis, N. Trawny, S. I. Roumeliotis, A. E. Johnson, A. Ansar, and L. Matthies, “Vision-aided inertial navigation for spacecraft entry, descent, and landing,” IEEE Transactions on Robotics, 2009. [10] E. Nerurkar, S. Lynen, and S. Zhao, “System and method for concurrent odometry and mapping,” Oct. 13 2020, US Patent 10,802,147. [11] “Intel RealSense Tracking Camera T265,” https://www.intelrealsense. com/tracking-camera-t265/, Accessed: 2022-02-22. [12] “Stereolabs ZED 2 Stereo Camera,” https://www.stereolabs.com/zed-2/, Accessed: 2022-02-22. [13] M. Bloesch, S. Omari, M. Hutter, and R. Siegwart, “Robust visual inertial odometry using a direct ekf-based approach,” in 2015 IEEE/RSJ international conference on intelligent robots and systems (IROS), 2015. [14] R. Mur-Artal and J. D. Tard´os, “Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras,” IEEE transactions on robotics, 2017. [15] J. Engel, V. Koltun, and D. Cremers, “Direct sparse odometry,” IEEE transactions on pattern analysis and machine intelligence, 2017. [16] J. Delmerico and D. Scaramuzza, “A benchmark comparison of monoc- ular VIO algorithms for ﬂying robots,” in IEEE ICRA, 2018. [17] C. Campos, R. Elvira, J. J. G. Rodr´ıguez, J. M. Montiel, and J. D. Tard´os, “Orb-slam3: An accurate open-source library for visual, visual–inertial, and multimap slam,” IEEE Transactions on Robotics, 2021. [18] M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, M. W. Achtelik, and R. Siegwart, “The euroc micro aerial vehicle datasets,” The International Journal of Robotics Research, vol. 35, no. 10, pp. 1157–1163, 2016. [19] S. Cort´es, A. Solin, E. Rahtu, and J. Kannala, “ADVIO: An authentic dataset for visual-inertial odometry,” in ECCV, 2018. [20] A. Alapetite, Z. Wang, and M. Patalan, “Comparison of three off-the- shelf visual odometry systems,” Robotics, 2020. [21] S. Ouerghi, N. Ragot, and X. Savatier, “Comparative study of a com- mercial tracking camera and ORB-SLAM2 for person localization,” in VISAPP, 2020. [22] Y. Ling, L. Bao, Z. Jie, F. Zhu, Z. Li, S. Tang, Y. Liu, W. Liu, and T. Zhang, “Modeling varying camera-imu time offset in optimization- based visual-inertial odometry,” in Proceedings of the European Con- ference on Computer Vision (ECCV), 2018. [23] H. G¨umg¨umc¨u, “Evaluation framework for proprietary slam systems exempliﬁed on google arcore,” Master’s thesis, ETH Zurich, 2019. [24] E. D. Nerurkar, K. J. Wu, and S. I. Roumeliotis, “C-klam: Constrained keyframe-based localization and mapping,” in 2014 IEEE international conference on robotics and automation (ICRA). [25] E. Marder-Eppstein, “Project tango,” in ACM SIGGRAPH 2016 Real- Time Live!, 2016, pp. 25–25. [26] R. Bonatti, R. Madaan, V. Vineet, S. Scherer, and A. Kapoor, “Learning visuomotor policies for aerial navigation using cross-modal representa- tions,” in 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). [27] R. Fan, J. Jiao, J. Pan, H. Huang, S. Shen, and M. Liu, “Real-time dense stereo embedded in a uav for road inspection,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2019."
336,A Security & Privacy Analysis of US-based Contact Tracing Apps,"[{'href': 'http://arxiv.org/abs/2207.08978v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2207.08978v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-07-18 23:14:49,"Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Jonathan D. Chang * 1 Kaiwen Wang * 1 Nathan Kallus 2 Wen Sun 1 Abstract 1. Introduction 2 2 0 2 l u J 2 1 ] G L . s c [ 1 v 7 3 8 5 0 . 7 0 2 2 : v i X r a We study representation learning for Ofﬂine Re- inforcement Learning (RL), focusing on the im- portant task of Ofﬂine Policy Evaluation (OPE). Recent work shows that, in contrast to supervised learning, realizability of the Q-function is not enough for learning it. Two sufﬁcient conditions for sample-efﬁcient OPE are Bellman complete- ness and coverage. Prior work often assumes that representations satisfying these conditions are given, with results being mostly theoretical in nature. In this work, we propose BCRL, which directly learns from data an approximately lin- ear Bellman complete representation with good coverage. With this learned representation, we perform OPE using Least Square Policy Evalua- tion (LSPE) with linear functions in our learned representation. We present an end-to-end theoreti- cal analysis, showing that our two-stage algorithm enjoys polynomial sample complexity provided some representation in the rich class considered is linear Bellman complete. Empirically, we ex- tensively evaluate our algorithm on challenging, image-based continuous control tasks from the Deepmind Control Suite. We show our represen- tation enables better OPE compared to previous representation learning methods developed for off-policy RL (e.g., CURL, SPR). BCRL achieves competitive OPE error with the state-of-the-art method Fitted Q-Evaluation (FQE), and beats FQE when evaluating beyond the initial state dis- tribution. Our ablations show that both linear Bellman complete and coverage components of our method are crucial. *Equal contribution 1Computer Science, Cornell University, Ithaca, NY, USA 2Operations Research and Information Engi- neering, Cornell Tech, New York, NY, USA. Correspondence to: Jonathan D. Chang <https://jdchang1.github.io>, Kaiwen Wang <https://kaiwenw.github.io>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). Deep Reinforcement Learning (RL) has developed agents that solve complex sequential decision making tasks, achiev- ing new state-of-the-art results and surpassing expert human performance. Despite these impressive results, these algo- rithms often require a prohibitively large number of online interactions to scale to higher dimensional inputs. To address these sample complexity demands, a recent line of work (van den Oord et al., 2018; Anand et al., 2019; Mazoure et al., 2020; Yang & Nachum, 2021) has incor- porated advances in unsupervised representation learning from the supervised learning literature into developing RL agents. For example, CURL (Laskin et al., 2020) and SPR (Schwarzer et al., 2021) utilize contrastive representation ob- jectives as auxiliary losses within an existing RL framework. These efforts are motivated by the tremendous success such self-supervised techniques have offered in computer vision, natural language processing, speech processing, and beyond. While these formulations have shown sample complexity improvements empirically, it remains an open question whether these approaches successfully address the unique challenges from RL that usually do not appear in supervised learning, such as exploration and exploitation, credit assign- ments, long horizon prediction, and distribution shift. In particular, recent work (Wang et al., 2021b; Amortila et al., 2020; Foster et al., 2021) has shown that realizability of the learning target in RL (namely, the Q-function) is insufﬁcient to avoid exponential dependence on problem parameters. In this paper, we study representation learning for an important subtask of off-policy RL: ofﬂine policy evaluation (OPE). OPE is a critical component for any off-policy policy optimization approach (e.g., off-policy actor critic such as SAC, Haarnoja et al., 2018, and DDPG, Lillicrap et al., 2016). Moreover, OPE allows us to focus on issues arising from distribution shift and long horizon prediction. Speciﬁcally, we propose a new approach that leverages rich function approximation (e.g., deep neural networks) to learn a representation that is both Bellman complete and exploratory. A linear Bellman complete representation means that linear functions in the representation have zero inherent Bellman error (Antos et al., 2008), i.e., applying the Bellman operator on a linear function results in a new linear function. An exploratory representation means that Learning Bellman Complete Representations for Ofﬂine Policy Evaluation the resulting feature covariance matrix formed by the ofﬂine dataset is well-conditioned. These two representational properties ensure that, under linear function approximation (i.e., the linear evaluation protocol, Grill et al., 2020), classic least squares policy evaluation (LSPE) (Nedi´c & Bertsekas, 2003; Duan et al., 2020; Wang et al., 2021a) can achieve accurate policy evaluation. We provide an end-to-end analysis showing that our representation learning approach together with LSPE ensures near-optimal policy evaluation with polynomial sample complexity. Empirically, we extensively evaluate our method on image- based continuous control tasks from the Deepmind Control Suite. First, we compare against two representation learning approaches developed for off-policy RL: CURL (Laskin et al., 2020) and SPR (Schwarzer et al., 2021). These bear many similarities to contrastive learning techiniques for unsupervised representation learning: SimCLR (Chen et al., 2020) and Bootstrap your own latent (BYOL, Grill et al., 2020), respectively. Under the linear evaluation protocol (i.e., LSPE with a linear function on top of the learned representation), our approach consistently outperforms these baselines. We observe that representations learned by CURL and SPR sometimes even exhibit instability when evaluated using LSPE (prediction error blows up when more iterations of LSPE is applied), while our approach is more stable. This comparison demonstrates that representation learning in ofﬂine RL is more subtle and using representation techniques developed from supervised learning settings may not result in the best performance for ofﬂine RL. Our ablations show that both linear Bellman completeness and coverage are crucial, as our method also blows up if one ingredient is missing. Finally, BCRL achieves state-of-the-art OPE error when compared with other OPE methods, and improves the state-of-the-art when evaluating beyond the initial state distribution.1 1.1. Related Work Representation Learning in Ofﬂine RL: From the theoretical side, Hao et al. (2021) considers of- ﬂine RL in sparse linear MDPs (Jin et al., 2020). Learning with sparsity can be understood as feature selection. Their work has a much stronger coverage condition than ours: namely, given a representation class Φ, they assume that any feature φ ∈ Φ has global coverage under the ofﬂine data distribution, i.e., Es,a∼νφ(s, a)φ(s, a)(cid:62) is well conditioned where ν is ofﬂine data distribution. In our work, we only assume that there exists one φ(cid:63) that has global coverage, thus strictly generalizing their coverage condition. Uehara & Sun (2021) propose a general model-based ofﬂine RL approach that can perform representation learning for linear MDPs in the ofﬂine setting without global coverage. How- 1 Code available at https://github.com/CausalML/bcrl. ever, their algorithm is a version space approach and is not computationally efﬁcient. Also, our linear Bellman com- pleteness condition strictly generalizes linear MDPs. (Ni et al., 2021) consider learning state-action embeddings from a known RKHS. We use general function approximation that can be more powerful than RKHS. Finally, Parr et al. (2008) identiﬁes bellman completeness as a desirable condition for feature selection in RL when analyzing an equivalence be- tween linear value-function approximation and linear model approximation. In our work, we investigate how to learn bellman completeness representation, and also the role of coverage in our feature selection. From the empirical side, Yang & Nachum (2021) evalu- ated a broad range of unsupervised objectives for learning pretrained representations from ofﬂine datasets for down- stream Imitation Learning (IL), online RL, and ofﬂine RL. They found that the use of pretrained representations dra- matically improved the downstream performance for policy learning algorithms. In this work, we aim to identify such an unsupervised representation objective and to extend the empirical evaluation to ofﬂine image-based continuous con- trol tasks. Nachum & Yang (2021) presents a provable contrastive representation learning objective for IL (derived from maximum likelihood loss), learning state representa- tions from expert data to do imitation with behavior cloning. Our approach instead focuses on learning state-action rep- resentations for OPE. Finally, both Song et al. (2016) and Chung et al. (2019) present algorithms to learn representa- tions suitable for linear value function approximation. Song et al. (2016) is most relevant to our work where they aim to learn bellman complete features. Both works, however, work in the online setting and do not consider coverage induced from the representation which we identify is impor- tant for accurate OPE. Representation Learning in Online RL: Theoretical works on representation learning in online RL focus on learning representations to facilitate exploration from scratch. OLIVE (Jiang et al., 2017), BLin-UCB (Du et al., 2021), FLAMBE (Agarwal et al., 2020), Mofﬂe (Modi et al., 2021), and Rep-UCB (Uehara et al., 2022) propose ap- proaches for representation learning in linear MDPs where they assume that there exists a feature φ(cid:63) ∈ Φ that ad- mits the linear MDP structure for the ground truth transi- tion. Zhang et al. (2021) posits an even stronger assumption where every feature φ ∈ Φ admits the linear MDP struc- ture for the true transition. Note that we only assume that there exists one φ(cid:63) that admits linear Bellman complete- ness, which strictly generalizes linear MDPs. Hence, our representation learning setting is more general than prior theoretical works. Note that we study the ofﬂine setting while prior works operate in the online setting which has additional challenges from online exploration. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation On the empirical side, there is a large body of works that adapt existing self-supervised learning techniques devel- oped from computer vision and NLP to RL. For learning representations from image-based RL tasks, CPC (van den Oord et al., 2018), ST-DIM (Anand et al., 2019), DRIML (Mazoure et al., 2020), CURL (Laskin et al., 2020), and SPR (Schwarzer et al., 2021) learn representations by op- timizing various temporal contrastive losses. In particular, Laskin et al. (2020) proposes to interleave minimizing an In- foNCE objective with similarities to MoCo (He et al., 2020) and SimCLR (Chen et al., 2020) while learning a policy with SAC (Haarnoja et al., 2018). Moreover, Schwarzer et al. (2021) proposes learning a representation similar to BYOL (Grill et al., 2020) alongside a Q-function for Deep Q-Learning (Mnih et al., 2013). We compare our repre- sentational objective against CURL and SPR in Section 6, and demonstrate that under linear evaluation protocol, ours outperform both CURL and SPR. Note, we refer the readers to Schwarzer et al. (2021) for comparisons between SPR and CPC, ST-DIM, and DRIML. 2. Preliminaries In this paper we consider the inﬁnite horizon discounted MDP (cid:104)S, A, γ, P, r, d0(cid:105). where S, A are state and action spaces which could contain a large number of states and ac- tions or could even be inﬁnite, γ ∈ (0, 1) is a discount factor, P is the transition kernel, r : S × A → R is the reward function, and d0 ∈ ∆(S) is the initial state distribution. We assume rewards are bounded by 1, i.e. |r(s, a)| ≤ 1. Given a policy π : S (cid:55)→ ∆(A), we denote V π(s) = E (cid:2)(cid:80)∞ h=0 γhrh|π, s0 := s(cid:3) as the expected dis- counted total reward of policy π starting at state s. We = Es∼d0 V π(s) as the expected discounted total denote V π d0 reward of the policy π starting at the initial state distribu- tion d0. We also denote average state-action distribution dπ h=0 γhdπ h(s, a) is d0 the probability of π visiting the (s, a) pair at time step h, starting from d0. (s, a) = (1 − γ) (cid:80)∞ h(s, a) where dπ In OPE, we seek to evaluate the expected discounted to- tal reward V πe of a target policy πe : S (cid:55)→ ∆(A) given data drawn from an ofﬂine state-action distribution ν ∈ ∆(S × A). The latter can, for example, be the state-action distribution under a behavior policy πb. Namely, the ofﬂine dataset D = {si, ai, ri, s(cid:48) i}N i=1 consists of N i.i.d tuples generated as (s, a) ∼ ν, r = r(s, a), s(cid:48) ∼ P (·|s, a). We deﬁne Bellman operator T πassociated with π as follows: T πf (s, a) =∆ r(s, a) + γEs(cid:48)∼P (s,a),a(cid:48)∼π(s(cid:48)) [f (s(cid:48), a(cid:48))] We may drop the superscript π when it is clear from context, in particular when π = πe is the ﬁxed target policy. A representation, or feature, φ : S × A → Rd is an embedding of state-action pairs into d-dimensional (cid:80)N space. We let Σ(φ) = Eν [φ(s, a)φ(s, a)(cid:124)] , (cid:98)Σ(φ) = i=1 φ(si, ai)φ(si, ai)(cid:124). We consider learning a rep- 1 N resentation from a feature hypothesis class Φ ⊂ [S × A (cid:55)→ Rd]. We assume features have bounded norm: (cid:107)φ(s, a)(cid:107)2 ≤ 1, ∀s, a, ∀φ ∈ Φ. In our experiments, Φ is convolutional neural nets with d outputs. Notation We denote BW := {x ∈ Rd : (cid:107)x(cid:107)2 ≤ W } as the Euclidean Ball in Rd with radius W . Given a distribution ν ∈ ∆(S × A) and a function f : S × A (cid:55)→ R, we denote ν = Es,a∼νf 2(s, a). Given a positive L2(ν) norm as (cid:107)f (cid:107)2 xT Σx and let λmin(Σ) deﬁnite matrix Σ, let (cid:107)x(cid:107)Σ = denote the minimum eigenvalue. When ν (cid:28) µ we let dν dµ denote the Radon-Nikodym derivative. We use ◦ to denote composition, so s(cid:48), a(cid:48) ∼ P (s, a) ◦ π is short-form for s(cid:48) ∼ P (s, a), a(cid:48) ∼ π(s(cid:48)). For any function f (s, a) and a policy π, we denote f (s, π) = Ea∼π(s) [f (s, a)]. √ 3. Linear Bellman Completeness Before we introduce our representation learning approach, in this section, we ﬁrst consider OPE with linear function approximation with a given representation φ. Lessons from supervised learning or linear bandit may suggest that OPE should be possible with polynomially many ofﬂine samples, as long as (1) ground truth target Qπe is linear in φ (i.e. ∃w ∈ Rd, such that for all s, a, Qπe (s, a) = w(cid:62)φ(s, a)), and (2) the ofﬂine data provides sufﬁcient coverage over πe (i.e., λmin(Σ(φ)) > 0). Unfortunately, under these two assumptions, there are lower bounds indicating that for any OPE algorithm, there exists an MDP where one will need at least exponentially (exponential in horizon or d) many ofﬂine samples to provide an accurate estimate of V πe (Wang et al., 2021b; Foster et al., 2021; Amortila et al., 2020). This lower bound indicates that one needs additional structural conditions on the representation. The additional condition the prior work has consider is Bellman completeness (BC). Since we seek to learn a rep- resentation rather than assume theoretical conditions on a given one, we will focus on an approximate version of BC. Deﬁnition 3.1 (Approximate Linear BC). A representation φ is εν-approximately linear Bellman complete if, max w1∈BW min w2∈BW (cid:107)w (cid:124) 2 φ − T πe (w (cid:124) 1 φ)(cid:107)ν ≤ εν. Note the dependence on ν, πe, and W . (cid:124) 1 φ(s, a), T π(w Intuitively the above condition is saying that for any linear (cid:124) 1 φ(s, a)) itself can be approxi- function w mated by another linear function under the distribution ν. Remark 3.2. Low-rank MDPs (Jiang et al., 2017; Agarwal et al., 2020) are subsumed in the exact linear BC model, i.e., εν = 0 under any distribution ν. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Algorithm 1 Least Squares Policy Evaluation (LSPE) 1: Input: Target policy πe, features φ, dataset D 2: Initialize (cid:98)θ0 = 0 ∈ BW . 3: for k = 1, 2, ..., K do (cid:124) k−1φ(s, a), Set (cid:98)fk−1(s, a) = (cid:98)θ 4: (cid:98)Vk−1(s) = Ea∼πe(s)[ (cid:98)fk−1(s, a)] 5: Perform linear regression: 1 N N (cid:88) i=1 (cid:98)θk ∈ arg min θ∈BW 6: end for 7: Return (cid:98)fK. (cid:0)θ(cid:124)φ(si, ai) − ri − γ (cid:98)Vk−1(s(cid:48) i)(cid:1)2 Note that the Bellman completeness condition is more sub- tle than the common realizability condition: for any ﬁxed φ, increasing its expressiveness (e.g., add more features) does not imply a monotonic decrease in εν. Thus common tricks such as lifting features to higher order polynomial kernel space or more general reproducing kernel Hilbert space does not imply the linear Bellman complete condition, nor does it improve the coverage condition. We next show approximate Linear BC together with coverage imply sample-efﬁcient OPE via Least Square Policy Evaluation (LSPE) (Algo- rithm 1). We present our result using the relative condition number, deﬁned for any initial state distribution p0 as x(cid:124)Es,a∼dπe φ(s, a)φ(s, a)(cid:124)x κ(p0) := sup x∈Rd p0 x(cid:124)Σ(φ)x . (1) Theorem 3.3 (Sample Complexity of LSPE). Assume fea- ture φ satisﬁes approximate Linear BC with parameter εν. For any δ ∈ (0, 0.1), w.p. at least 1 − δ, we have for any initial state distribution p0 (cid:12) (cid:12)V πe (cid:12) p0 (cid:104) − Es∼p0 (cid:98)fK(s, πe) (cid:105)(cid:12) (cid:12) (cid:12) ≤ γK/2 1 − γ + (cid:114)(cid:13) (cid:13) 4 (cid:13) ddπe p0 dν (cid:13) (cid:13) (cid:13)∞ (1 − γ)2 εν + 480(cid:112)κ(p0)(1 + W )d log(N )(cid:112)log(10/δ) √ (1 − γ)2 N , where (cid:98)fK is the output of Algorithm 1. Please see Appendix C.1 for proof. The above result holds simultaneously over all initial state distributions p0 covered by the data distribution ν. If ν has full coverage, i.e. if Σ (cid:23) βI, as is commonly assumed in the literature, one can show that κ(p0) ≤ β−1 for any initial state distribution (cid:13) (cid:13) p0. Also note that the concentrability coefﬁcient (cid:13)∞ shows up as T πe (cid:98)fk−1 can be nonlinear. In the exact Linear BC case, where εν = 0 (i.e., there is a linear function that perfectly approximates T πe (cid:98)fk−1 under ν), the term involving the concentrability coefﬁcient will be 0 and we can even avoid its ﬁniteness. ddπe p0 dν (cid:13) (cid:13) (cid:13) 4. Representation Learning The previous section indicates sufﬁcient conditions on the representation for efﬁcient and accurate OPE with linear function approximation. However, a representation that is Bellman complete and also provides coverage is not avail- able a priori, especially for high-dimensional settings (e.g., image-based control tasks as we consider in our experi- ments). Existing theoretically work often assume such rep- resentation is given. We propose to learn a representation φ that is approximately Bellman complete and also provides good coverage, via rich function approximation (e.g., a deep neural network). More formally, we want to learn a repre- sentation φ such that (1) it ensures approximate Bellman complete, i.e., εν is small, and (2) has a good coverage, i.e., λmin(Σ(φ)) is as large as possible, which are the two key properties to guarantee accurate and efﬁcient OPE indicated by Theorem 3.3. To formulate the representation learning question, our key assumption is that our representation hy- pothesis class Φ is rich enough such that it contains at least one representation φ(cid:63) that is linear Bellman complete (i.e., εν = 0) and has a good coverage. Assumption 4.1 (Representational power of Φ). There ex- ists φ(cid:63) ∈ Φ, such that (1) φ(cid:63) achieves exact Linear BC (deﬁnition 3.1 with εν = 0), and (2) φ(cid:63) induces coverage, i.e., λmin(Σ(φ(cid:63))) ≥ β > 0. Our goal is to learn such a representation from the hypothe- sis class Φ. Note that unlike prior RL representation learning works (Hao et al., 2021; Zhang et al., 2021), here we only assume that there exists a φ(cid:63) has linear BC and induces good coverage, other candidate φ ∈ Φ could have terrible coverage and does not necessarily have linear BC. Before proceeding to the learning algorithm, we ﬁrst present an equivalent condition for linear BC, which does not rely on the complicated min-max expression of Deﬁnition 3.1. Proposition 4.2. Consider a feature φ with full rank covari- ance Σ(φ). Given any W > 0, the feature φ being linear BC (under BW ) implies that there exist (ρ, M ) ∈ BW × Rd×d with (cid:107)M (cid:107)2 ≤ 1 − (cid:107)ρ(cid:107)2 (cid:113) W 2 , and Eν (cid:21) (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) φ(s, a) − (cid:20)γEs(cid:48)∼P (s,a)φ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 = 0. (2) On the other hand, if there exists (ρ, M ) ∈ BW × Rd×d with (cid:107)M (cid:107)2 < 1 such that the above equality holds, then φ must satisfy exact linear BC with W ≥ (cid:107)ρ(cid:107)2 . 1−(cid:107)M (cid:107)2 Please see Appendix D for proof. The above shows that lin- ear BC is equivalent to a simple linear relationship between the feature and the expected next step’s feature and reward. This motivates our feature learning algorithm: if we are ca- pable of learning a representation φ such that the transition Learning Bellman Complete Representations for Ofﬂine Policy Evaluation from the current feature φ(s, a) to the expected next fea- ture Es(cid:48)∼P (s,a)[φ(s(cid:48), πe)] and reward r(s, a) is linear, then we’ve found a feature φ that is linear BC. 4.1. Algorithm To learn the representation that achieves linear BC, we use Proposition 4.2 to design a bilevel optimization program. We start with deterministic transition as a warm up. Deterministic transition Due to determinism in the tran- sition, we do not have an expectation with respect to s(cid:48) anymore. So, we design the bilevel optimization as follows: However, we cannot directly optimize the above objective since we do not have access to P (s, a) to compute the ex- pected next step feature Es(cid:48)∼P (s,a)φ(s(cid:48), πe). Also note that the expectation Es(cid:48) is inside the square which means that we cannot even get an unbiased estimate of the gradient of (φ, M ) with one sample s(cid:48) ∼ P (s, a). This phenomenon is related to the double sampling issue on ofﬂine policy evaluation literature which forbids one to directly optimize Bellman residuals. Algorithms such as TD are designed to overcome the double sampling issue. Here, we use a different technique to tackle this issue (Chen & Jiang, 2019). We introduce a function class G ⊂ S × A (cid:55)→ Rd which is rich enough to contain the expected next feature. (cid:104) (cid:21) min φ∈Φ (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 Θ = (cid:8)(ρ, M ) ∈ B(cid:107)ρ(cid:63)(cid:107) × Rd×d :, (cid:107)M (cid:107)2 ≤ (cid:107)M (cid:63)(cid:107)2 (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) min (ρ,M )∈Θ φ(s, a) − ED (cid:9) , where ρ(cid:63), M (cid:63) are the optimal ρ, M for the linear BC φ(cid:63) (in Assumption 4.1). Namely, we aim to search for a repre- sentation φ ∈ Φ, such that the relationship between φ(s, a) and the combination of the next time step’s feature φ(s(cid:48), πe) and the reward, is linear. The spectral norm constraint in Θ is justiﬁed by Proposition 4.2. Solving the above bilevel moment condition ﬁnds a representation that achieves ap- proximate linear BC. However, there is no guarantee that such representation can provide a good coverage over πe’s traces. We introduce regularizations for φ using the ideas from optimal designs, particularly the E-optimal design. Deﬁne the minimum eigenvalue regularization (3) Assumption 4.3. For any φ ∈ Φ, we have that the mapping (s, a) (cid:55)→ γEs(cid:48)∼P (s,a) [φ(s(cid:48), πe)] is in G. We form the optimization problem as follows: (cid:104) min φ∈Φ min (ρ,M )∈Θ (cid:21) ED (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) ED (cid:107)g(s, a) − γφ(s(cid:48), πe)(cid:107)2 φ(s, a) − (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:105) . 2 (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (5) − min g∈G Note that under Assumption 4.3, the min over G will approx- imate γ2ED(cid:107)Es(cid:48)∼P (s,a)φ(s(cid:48), πe) − φ(s(cid:48), πe)(cid:107)2 2, i.e., the av- erage variance induced by the stochastic transition. Thus, for a ﬁxed φ and M , we can see that the following ED (cid:107)M φ(s, a) − γφ(s(cid:48), πe)(cid:107)2 − γ2ED(cid:107)Es(cid:48)∼P (s,a)φ(s(cid:48), πe) − φ(s(cid:48), πe)(cid:107)2 2 2 RE(φ) := λmin (ED [φ(s, a)φ(s, a)(cid:124)]) , is indeed an unbiased estimate of: as the smallest eigenvalue of the empirical feature covari- ance matrix under the representation φ. Maximizing this quantity ensures that our feature provides good coverage, i.e. λmin(Σ(φ)) is as large as possible. Thus, to learn a representation that is approximately linear Bellman complete and also provides sufﬁcient coverage, we formulate the following constrained bilevel optimization: (cid:104) min φ∈Φ (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) s.t., RE(φ) ≥ β/2. min (ρ,M )∈Θ ED (cid:21) φ(s, a) − (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 To extend this to stochastic transitions, there is an additional double sampling issue, which we discuss and address now. Stochastic transition Ideally, we would solve the follow- ing bilevel optimization problem, (cid:104) min φ∈Φ min (ρ,M )∈Θ ED (cid:21) (cid:13) (cid:13) (cid:13) (cid:13) (cid:20)M ρ(cid:124) φ(s, a) − (cid:20)γEs(cid:48)∼P (s,a)[φ(s(cid:48), πe)] r(s, a) (cid:105) . 2 (cid:21)(cid:13) (cid:13) (cid:13) (cid:13) 2 (4) Es,a∼ν (cid:13)M φ(s, a) − γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)(cid:13) (cid:13) 2 (cid:13) 2 which matches to the ideal objective in Eq. 4. Thus solv- ing for φ based Eq. 5 allows us to optimize Eq. 4, which allows us to learn an approximate linear Bellman complete representation. Similarly, we incorporate the E-optimal de- sign here by adding a constraint that forcing the smallest eigenvalue of the empirical feature covariance matrix, i.e., RE(φ), to be lower bounded. Once we learn a representation that is approximately linear BC, and also induces sufﬁcient coverage, we simply call the LSPE to estimate V πe . The whole procedure is summarized in Algorithm 2. Note in Alg 2 we put constraints to the objective function using Lagrangian multiplier. There are other design choices that also encourage coverage. One particular choice we study empirically is motivated by the idea of D-optimal design. Here we aim to ﬁnd a representation that maximizes the following log-determinant RD(φ) := ln det (ED [φ(s, a)φ(s, a)(cid:124)]) . Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Algorithm 2 OPE with Bellman Complete and exploratory Representation Learning (BCRL) 1: Input: Representation class Φ, dataset D of size 2N , design regularization R, function class G, policy πe. 2: Randomly split D into two sets D1, D2 of size N . 3: If the system is stochastic, learn representation (cid:98)φ as, (cid:104) arg min φ∈Φ − min g∈G (cid:21) ED1 min (ρ,M )∈Θ (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) ED1 (cid:107)g(s, a) − γφ(s(cid:48), πe)(cid:107)2 φ(s, a) − (cid:105) 2 (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 − λR(φ) 4: Otherwise, for deterministic system, learn (cid:98)φ as, (cid:104) arg min φ∈Φ min (ρ,M )∈Θ ED1 (cid:21) (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) φ(s, a) − (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 − λR(φ) 5: Return (cid:98)V := LSPE(πe, (cid:98)φ, D2). When D is large, then the regularization RD(φ) approx- imates (cid:80) i ln (σi (Eν [φ(s, a)φ(s, a)(cid:124)])). Maximizing RD(φ) then intuitively maximizes coverage over all directions. D-optimal design is widely used for bandits (Lattimore & Szepesvári, 2020) and RL (Wang et al., 2021b; Agarwal et al., 2019) to design exploration distributions with global coverage. Note that, in contrast to these contexts where the feature is ﬁxed and the distribution is optimized, we optimize the feature, given the data distribution ν. 4.2. Sample Complexity Analysis We now prove a ﬁnite sample utility guarantee for the em- pirical constrained bilevel optimization problem, (cid:98)φ ∈ arg min φ∈Φ (cid:104) min (ρ,M )∈Θ ED (cid:21) (cid:13) (cid:20)M (cid:13) (cid:13) ρ (cid:13) (cid:124) φ(s, a) − ED (cid:107)g(s, a) − γφ(s(cid:48), πe)(cid:107)2 2 (cid:105) . − min g∈G (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (6) s.t., RE(φ) ≥ β/2. For simplicity, we state our results for discrete function class Φ and G. Note that our sample complexity only scales with respect ln(|Φ|) and ln(|G|), which are the standard com- plexity measures for discrete function classes. We extend our analysis to inﬁnite function classes under metric entropy assumptions (Wainwright, 2019; van der Vaart & Wellner, 1996) in the Appendix; see Assumption E.5. The following theorem shows that Algorithm 2 learns a representation (cid:98)φ that is O(N −1/2) approximately Linear BC and has coverage. Theorem 4.4. Assume Assumption 4.1 (and Assump- := Let C2 tion 4.3 if 96 log1/2(|Φ|)+4 2 , then for any the system is stochastic). √ . If N ≥ C 2 d+4 log1/2(1/δ) β/4 δ ∈ (0, 1), w.p. at least 1 − δ, we have 1. (cid:98)φ satisﬁes (cid:98)ε-approximate Linear BC with (cid:98)ε ≤ 13d(1 + W )2 log1/2(4W |Φ|N/δ) √ N + 7γ(1 + W ) log1/2(2|G|/δ) √ N , 2. λmin(Σ( (cid:98)φ)) ≥ β/4. If transitions are deterministic, treat log(|G|) = 0. Proof Sketch. First, we use Weyl’s Perturbation Theorem and chaining to show that the eigenvalues of Σ(φ) are close to (cid:98)Σ(φ), uniformly over φ. This implies that (a) λmin((cid:98)Σ(φ(cid:63))) ≥ β/2, and hence is feasible in the empirical bilevel optimization Equation (6), and (b) λmin(Σ( (cid:98)φ)) ≥ β/4. Since φ(cid:63) is feasible, we apply uniform concentration arguments to argue that (cid:98)φ has low population loss (Equa- tion (5)), which implies approximate Linear BC. The error term in (cid:98)ε is comprised of the statistical errors of ﬁtting Φ and of ﬁtting G for the double sampling correc- tion. In the contextual bandit setting, i.e. γ = 0, there is no transition, so the second term becomes 0. Chaining together with Theorem 3.3 gives the following end-to-end (cid:101)O(N −1/2) evaluation error guarantee for LSPE with the learned features: Theorem 4.5. Under Assumption 4.1 (and Assumption 4.3 if P (s, a) is stochastic). Let C2, (cid:98)ε be as deﬁned in Theo- rem 4.4. If N ≥ C 2 2 , then we have for any δ ∈ (0, 1/2), w.p. at least 1 − 2δ, for all distributions p0, (cid:12) (cid:12)V πe (cid:12) − Es∼p0 (cid:104) (cid:98)fK(s, πe) p0 960β−1/2(1 + W )d log(N )(cid:112)log(10/δ) (cid:105)(cid:12) (cid:12) (cid:12) ≤ γK/2 1 − γ + √ (1 − γ)2 N . + (cid:114)(cid:13) (cid:13) (cid:13) 4 ddπe p0 dν (cid:13) (cid:13) (cid:13)∞ (1 − γ)2 (cid:98)ε Comparison to FQE What if one ignores the representa- tion learning and just runs the Fitted Q-Evaluation (FQE) which directly performs least square ﬁtting with the nonlin- ear function class F := {w(cid:62)φ(s, a) : φ ∈ Φ, (cid:107)w(cid:107)2 ≤ W }? As N → ∞, FQE will suffer the following worst-case Bell- man error (also called inherent Bellman error): εibe := max f ∈F min g∈F (cid:107)g − T πe f (cid:107)2 ν . Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Algorithm 3 Practical Instantiation of BCRL 1: Input: Ofﬂine dataset D = {s, a, s(cid:48)}, target policy πe 2: Initialize parameters for φ, M , ρ, and φ 3: for t = 0, 1, . . . , T do 4: Mt+1 ← Mt − η∇M J(φt, Mt, ρt, φt) ρt+1 ← ρt − η∇ρJ(φt, Mt, ρt, φt) 5: φt+1 ← φt − η∇φJ(φt, Mt+1, ρt+1, φt) 6: φt+1 ← τ φt+1 + (1 − τ )φt 7: 8: end for 9: Linear evaluation: (cid:98)V = LSPE(πe, φT , D). Note that our assumption that there exists a linear BC rep- resentation φ(cid:63) does not imply that the worst-case Bellman error εibe is small. In contrast, when N → ∞, our approach will accurately estimate V πe p0 . 5. A Practical Implementation In this section we instantiate a practical implementation to learn our representation using deep neural networks for our representation function class Φ. Based on Equation (3), we ﬁrst formalize our bilevel optimization objective: J(φ, M, ρ, φ) = ED (cid:21) (cid:13) (cid:20)M (cid:13) (cid:13) ρ (cid:13) − λ log det ED φ(s, a) − (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:2)φ(s, a)φ(s, a)(cid:62)(cid:3) . (cid:124) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 In our implementation, we replace the hard constraint pre- sented in Section 4.1 with a Lagrange multiplier, i.e. we use the optimal design constraint as a regularization term when learning φ. Speciﬁcally, we maximize the log det of the covariance matrix induced by the feature, which maximizes all eigenvalues since log det is the sum of the log eigen- values. Our experiment results in Section 6.4 demonstrate that it indeed improves the condition number of Σ(φ). In some of our experiments, we use a target network in our implementation. Namely, the updates make use of a target network φ where the weights can be an exponential moving average of the representation network’s weights. This idea of using a slow moving target network has been shown to stabilize training in both the RL (Mnih et al., 2013) and the representation learning literature (Grill et al., 2020). As summarized in Algorithm 3, given our ofﬂine behavior dataset D and target policy πe, we iteratively update M Figure 1. Representative frames from DeepMind Control Suite tasks. From left to right, Finger Turn Hard, Cheetah Run, Quadruped Walk, and Humanoid Stand. and φ and then use the resulting learned representation to perform OPE. Please see Appendix F for implementation and hyperparameter details. As we will show in our experiments, our update procedures for φ signiﬁcantly minimizes the Bellman Completeness loss and also improve the condition number of Σ(φ), which are the two key quantities to ensure good performance of LSPE with linear regression as shown in Theorem 3.3. 6. Experiments Our goal is to answer the following questions. (1) How do our representations perform on downstream LSPE com- pared to other popular unsupervised representation learning techniques? (2) How important are both the linear bellman completeness and optimal design components for learning representations? (3) How competitive is BCRL with other OPE methods, especially for evaluating beyond the initial state distribution? Following the standards in evaluating representations in su- pervised learning, we used a linear evaluation protocol, i.e., linear regression in LSPE on top of a given representa- tion. This allows us to focus on evaluating the quality of the representation. We compared our method to prior techniques on a range of challenging, image-based continuous control tasks from the DeepMind Control Suite benchmark (Tassa et al., 2018): Finger Turn Hard, Cheetah Run, Quadruped Walk, and Humanoid Stand. Frames from the tasks are shown in Figure 1. To investigate our learned representation, we benchmark our representation against two state-of-the-art self-supervised representation learning objectives adopted for RL: (1) CURL uses the In- foNCE objective to contrastively learn state-representations; and (2) SPR adopts the BYOL contrastive learning frame- work to learn representations with latent dynamics. Note, we modiﬁed CURL for OPE by optimizing the contrastive loss between state-action pairs rather than just states. For SPR, we did not include the Q prediction head and used their state representation plus latent dynamics as the state- action representation for downstream linear evaluation. We used the same architecture for the respective representa- tions across all evaluated algorithms. To compare against other OPE methods, we additionally compared against Fit- ted Q-Evaluation (Munos & Szepesvári, 2008; Kostrikov & Nachum, 2020) (FQE), weighted doubly robust policy evaluation (Jiang & Li, 2016; Thomas & Brunskill, 2016) (DR), Dreamer-v2 (Hafner et al., 2021) model based eval- uation (MB), and DICE (Yang et al., 2020). We modiﬁed implementations from the benchmark library released by Fu et al. (2021) for FQE and DR and used the authors’ released codebases for Dreamer-v2 and BestDICE.2 2https://github.com/google-research/dice_rl. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Figure 2. OPE curves across ﬁve seeds using representations trained with BCRL, SPR, and CURL on the ofﬂine datasets (Table 1). mators beyond the original initial state distribution d0. The ﬁrst setting ensures that baselines and our algorithm all sat- isfy the coverage condition, thus demonstrating the unique beneﬁt of learning a Bellman complete representation. The second setting evaluates the robustness of our algorithm, i.e., the ability to estimate beyond the original d0. Evaluation on On-Policy + Off-Policy Data: To further investigate the importance of learning linear BC features, we experiment with learning representations from an ofﬂine dataset that also contains state-action pairs from the target policy. More speciﬁcally, we train our representations on a dataset containing 100K behavior policy and 100K target policy samples. Note, only the experiment in this paragraph uses this mixture dataset. With the addition of on-policy data from the target policy, we can focus on just the role of linear BC for OPE performance because the density ratio ddπe po dν and the relative condition number (Eq. 1) is at most 2, i.e. we omit the design regularization and focused on minimizing the Bellman completeness loss. Figure 4 (Left) shows that BCRL outperforms baselines in this setting, even Figure 3. (Left) Root mean squared evaluation error across all tasks. (Right) Mean spearman ranking correlation across all tasks. Our target policies were trained using the authors’ imple- mentation of DRQ-v2 (Yarats et al., 2021a), a state-of-the- art, off-policy actor critic algorithm for vision-based contin- uous control. With high-quality target policies, we collected 200 rollouts and did a linear evaluation protocol to predict the discounted return. For our ofﬂine behavior datasets, we collected 100K samples from a trained policy with mean performance roughly a quarter of that of the target policy (Table 1). All results are aggregated over ﬁve random seeds. See Appendix F for details on hyperparameters, environ- ments, and dataset composition. 6.1. OPE via LSPE with Learned Representations Figure 2 compares the OPE performance of BCRL against SPR and CURL. Representations learned by BCRL outper- form those learned by SPR and CURL. On some tasks, SPR and CURL both exhibited an exponential error ampliﬁca- tion with respect to the number of iterations of LSPE, while BCRL did not suffer from any blowup. 6.2. OPE Performance Figure 3 compares the OPE performance of BCRL against multiple benchmarks from the OPE literature. BCRL is competitive with FQE and evaluates better than other bench- marks across the tasks that we tested on. We also evaluated how well estimated values from BCRL rank policies. Following (Fu et al., 2021), we use the spearman ranking correlation metric to compute the correlation between ordinal rankings according to the OPE estimates and the ground truth values. For ranking, we evaluated three additional target policies with mean perfor- mances roughly 75%, 50%, and 10% of the target policy. Figure 3 presents the mean correlation of each evaluation algorithm across all tasks. BCRL is competitive with FQE and consistently better than other benchmarks at ranking policies. 6.3. Further Investigation of Different Settings In this section, we consider two additional settings: (1) the ofﬂine dataset contains some on-policy data, which ensures that the ofﬂine data provides coverage over the evaluation policy’s state action distribution; (2) we evaluate all esti- Figure 4. (Left) Evaluation on a mixture dataset with on-policy and off-policy data. Note the addition of target policy data bounds the relative condition number (Eq. 1). (Right) Evaluation beyond the initial state distribution (given just the ofﬂine data). Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Figure 5. (Left) BCRL’s OPE curves for Cheetah Run with (blue) and without (red) the D-optimal design based regularization. (Center) Bar plot of the singular values of the feature covariance matrices for the left plot. (Right) OPE curves for Finger Turn Hard with (blue) and without (red) optimizing for linear BC. matching FQE performance across tasks. Our experiments corroborate our analysis that explicitly enforcing our learned representations to be linear BC improves OPE. Note that the fact that LSPE with SPR and CURL features still blows up under this mixture data means that the other representations’ failures are not just due to coverage. Evaluation Beyond Initial State Distribution: To further investigate the beneﬁts of optimizing the D-optimal design to improve coverage, we investigate doing OPE beyond the initial state distribution d0, which is supported by Theo- rem 4.5. Note that if our representation is exactly Bellman complete and also the corresponding feature covariance ma- trix is well-conditioned, we should be able to evaluate well on any states. Speciﬁcally, we experiment on evaluating at all timesteps in a target policy rollout, not just at the initial state distribution. Figure 4 (Right) shows that BCRL is able to more robustly evaluate out-of-distribution than all other benchmarks. 6.4. Ablation Studies Impact of Optimal Design Regularization: To investi- gate the impact of maximizing the D-optimal design, we ablate the design regularization term from our objective and analyze the downstream evaluation performance and the respective feature covariance matrices on the ofﬂine dataset. Figure 5 (Center) presents a bar plot of the sin- gular values of the feature covariance matrix (Σ(φ) := Es,a∼νφ(s, a)φ(s, a)(cid:62)). Figure 5 (Left) shows the down- stream OPE performance for features trained with and with- out the design regularization on the Cheetah Run task. Note that without the regularization, we ﬁnd that that the feature covariance matrix has much worse condition number, i.e. feature is less exploratory. As our analysis suggests, we also observe a deterioration in evaluation performance with- out the design regularization to explicitly learn exploratory features. Impact of Linear Bellman Completeness: Figure 5 (Right) presents an ablation study where we only opti- mize for the design term in our objective. We ﬁnd that downstream OPE performance degrades without directly op- timizing for linear BC, suggesting that a feature with good coverage alone is not enough to avoid error ampliﬁcation. 7. Conclusion We present BCRL which leverages rich function approxi- mation to learn Bellman complete and exploratory repre- sentations for stable and accurate ofﬂine policy evaluation. We provide a mathematical framework of representation learning in ofﬂine RL, which generalizes all existing repre- sentation learning frameworks from the RL theory literature. We provide an end-to-end theoretical analysis of our ap- proach for OPE and demonstrate that BCRL can accurately estimate policy values with polynomial sample complex- ity. Notably, the complexity has no explicit dependence on the size of the state and action space, instead, it only depends on the statistical complexity of the representation hypothesis class. Experimentally, we extensively evaluate our approach on the DeepMind Control Suite, a set of image- based, continuous control, robotic tasks. First, we show that under the linear evaluation protocol – using linear regres- sion on top of the representations inside the classic LSPE framework – our approach outperforms prior RL representa- tion techniques CURL and SPR which leverage contrastive representation learning techniques SimCLR and BYOL re- spectively. We also show that BCRL achieves competitive OPE performance with the state-of-the-art FQE, and notice- ably improves upon it when evaluating beyond the initial state distribution. Finally, our ablations show that approxi- mate Linear Bellman Completeness and coverage are crucial ingredients to the success of our algorithm. Future work includes extending BCRL to ofﬂine policy optimization. ACKNOWLEDGEMENTS This material is based upon work supported by the National Science Foundation under Grant No. 1846210 and by a Cor- nell University Fellowship. We thank Rahul Kidambi, Ban Kawas, and the anonymous reviewers for useful discussions and feedback. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation References Agarwal, A., Jiang, N., Kakade, S. M., and Sun, W. Rein- forcement learning: Theory and algorithms. CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep, 2019. Agarwal, A., Kakade, S., Krishnamurthy, A., and Sun, W. Flambe: Structural complexity and representation learn- ing of low rank mdps. NeurIPS, 33:20095–20107, 2020. Amortila, P., Jiang, N., and Xie, T. A variant of the wang- foster-kakade lower bound for the discounted setting. arXiv preprint arXiv:2011.01075, 2020. Anand, A., Racah, E., Ozair, S., Bengio, Y., Côté, Unsupervised state rep- M., and Hjelm, R. D. resentation learning in atari. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d’Alché-Buc, F., Fox, E. B., and Garnett, R. (eds.), NeurIPS, pp. 8766– URL https://proceedings. 8779, neurips.cc/paper/2019/hash/ 6fb52e71b837628ac16539c1ff911667-Abstract. html. 2019. Antos, A., Szepesvári, C., and Munos, R. Fitted In and Roweis, (eds.), NIPS, volume 20. Curran Associates, URL https://proceedings. q-iteration in continuous action-space mdps. Platt, J., Koller, D., Singer, Y., S. Inc., neurips.cc/paper/2007/file/ da0d1111d2dc5d489242e60ebcbaf988-Paper. pdf. 2008. Bhatia, R. Matrix analysis, volume 169. Springer Science & Business Media, 2013. Boucheron, S., Lugosi, G., and Massart, P. Concentration inequalities: A nonasymptotic theory of independence. Oxford university press, 2013. Chen, J. and Jiang, N. Information-theoretic considerations in batch reinforcement learning. In ICML, pp. 1042–1051. PMLR, 2019. Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. E. A simple framework for contrastive learning of visual representations. ICML, 2020. URL https://arxiv. org/abs/2002.05709. Chung, W., Nath, S., Joseph, A., and White, M. Two- timescale networks for nonlinear value function approxi- mation. In ICLR, 2019. URL https://openreview. net/forum?id=rJleN20qK7. Duan, Y., Jia, Z., and Wang, M. Minimax-optimal off-policy evaluation with linear function approximation. In ICML, pp. 2701–2709. PMLR, 2020. Foster, D. J., Krishnamurthy, A., Simchi-Levi, D., and Xu, Y. Ofﬂine reinforcement learning: Fundamental barri- ers for value function approximation. arXiv preprint arXiv:2111.10919, 2021. Fu, J., Norouzi, M., Nachum, O., Tucker, G., Wang, Z., Novikov, A., Yang, M., Zhang, M. R., Chen, Y., Kumar, A., Paduraru, C., Levine, S., and Paine, T. L. Benchmarks for deep off-policy evaluation. ICLR, 2021. URL https: //arxiv.org/abs/2103.16596. Grill, J.-B., Strub, F., Altché, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., Piot, B., kavukcuoglu, k., Munos, R., and Valko, M. Bootstrap your own latent - a new approach to self-supervised learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H. (eds.), NeurIPS, volume 33, pp. 21271–21284. Curran As- sociates, Inc., 2020. URL https://proceedings. neurips.cc/paper/2020/file/ f3ada80d5c4ee70142b17b8192b2958e-Paper. pdf. Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor- critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In ICML, pp. 1861–1870. PMLR, 2018. Hafner, D., Lillicrap, T. P., Norouzi, M., and Ba, J. In In- Mastering atari with discrete world models. ternational Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=0oabwyZbOu. Hao, B., Duan, Y., Lattimore, T., Szepesvári, C., and Wang, M. Sparse feature selection makes batch reinforcement learning more sample efﬁcient. In ICML, pp. 4063–4073. PMLR, 2021. He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. B. Mo- mentum contrast for unsupervised visual representation learning. CVPR, 2020. URL http://arxiv.org/ abs/1911.05722. Jiang, N. and Li, L. Doubly robust off-policy value evalua- tion for reinforcement learning. In ICML, pp. 652–661. PMLR, 2016. Du, S. S., Kakade, S. M., Lee, J. D., Lovett, S., Mahajan, G., Sun, W., and Wang, R. Bilinear classes: A struc- tural framework for provable generalization in rl. arXiv preprint arXiv:2103.10897, 2021. Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R. E. Contextual decision processes with low bellman rank are pac-learnable. In ICML, pp. 1704–1713. PMLR, 2017. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Jin, C., Yang, Z., Wang, Z., and Jordan, M. I. Provably efﬁcient reinforcement learning with linear function ap- proximation. In COLT, pp. 2137–2143. PMLR, 2020. Kostrikov, I. and Nachum, O. Statistical bootstrapping for uncertainty estimation in off-policy evaluation, 2020. URL https://arxiv.org/abs/2007.13609. Laskin, M., Srinivas, A., and Abbeel, P. Curl: Contrastive unsupervised representations for reinforcement learning. In ICML, pp. 5639–5650. PMLR, 2020. Lattimore, T. and Szepesvári, C. Bandit algorithms. Cam- bridge University Press, 2020. Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. In ICLR, 2016. Parr, R., Li, L., Taylor, G., Painter-Wakeﬁeld, C., and Littman, M. L. An analysis of linear models, lin- ear value-function approximation, and feature selection In ICML, pp. 752–759, for reinforcement learning. New York, NY, USA, 2008. Association for Comput- doi: 10. ing Machinery. 1145/1390156.1390251. URL https://doi.org/ 10.1145/1390156.1390251. ISBN 9781605582054. Pollard, D. Empirical processes: theory and applications. In NSF-CBMS regional conference series in probability and statistics, pp. i–86. JSTOR, 1990. Schwarzer, M., Anand, A., Goel, R., Hjelm, R. D., Courville, A. C., and Bachman, P. Data-efﬁcient rein- forcement learning with momentum predictive represen- tations. ICLR, 2021. URL https://arxiv.org/ abs/2007.05929. Mazoure, B., Tachet des Combes, R., Doan, T. L., Bachman, P., and Hjelm, R. D. Deep reinforcement and infomax learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H. (eds.), NeurIPS, volume 33, pp. 3686–3698. Curran Asso- ciates, Inc., 2020. URL https://proceedings. neurips.cc/paper/2020/file/ 26588e932c7ccfa1df309280702fe1b5-Paper. pdf. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. A. Playing atari with deep reinforcement learning. NIPS Deep Learning Workshop, 2013. URL http://arxiv. org/abs/1312.5602. Modi, A., Chen, J., Krishnamurthy, A., Jiang, N., and Agarwal, A. Model-free representation learning arXiv preprint and exploration in low-rank mdps. arXiv:2102.07035, 2021. Munos, R. and Szepesvári, C. Finite-time bounds for ﬁtted value iteration. JMLR, 9(5), 2008. Nachum, O. and Yang, M. Provable representation learning for imitation with contrastive fourier features. NeurIPS, 2021. URL https://arxiv.org/abs/ 2105.12272. Nedi´c, A. and Bertsekas, D. P. Least squares policy evalua- tion algorithms with linear function approximation. Dis- crete Event Dynamic Systems, 13(1):79–110, 2003. Ni, C., Zhang, A. R., Duan, Y., and Wang, M. Learning good state and action representations via tensor decomposition. In 2021 IEEE International Symposium on Information Theory (ISIT), pp. 1682–1687. IEEE, 2021. learning. reinforcement Song, Z., Parr, R. E., Liao, X., and Carin, L. Linear feature encoding for In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R. (eds.), NIPS, volume 29. Curran Asso- ciates, Inc., 2016. URL https://proceedings. neurips.cc/paper/2016/file/ 8232e119d8f59aa83050a741631803a6-Paper. pdf. Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. d. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, arXiv preprint A., et al. Deepmind control suite. arXiv:1801.00690, 2018. Thomas, P. S. and Brunskill, E. Data-efﬁcient off-policy policy evaluation for reinforcement learning, 2016. URL https://arxiv.org/abs/1604.00923. Uehara, M. and Sun, W. Pessimistic model-based ofﬂine reinforcement learning under partial coverage. arXiv preprint arXiv:2107.06226, 2021. Uehara, M., Zhang, X., and Sun, W. Representation learning for online and ofﬂine RL in low-rank MDPs. In ICLR, 2022. URL https://openreview.net/forum? id=J4iSIR9fhY0. van den Oord, A., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. arXiv, 2018. URL http://arxiv.org/abs/1807.03748. van der Vaart, A. W. and Wellner, J. A. Weak Con- vergence and Empirical Processes. Springer Se- ISBN ries in Statistics. Springer New York, 1996. 9781475725476. doi: 10.1007/978-1-4757-2545-2. URL http://link.springer.com/10.1007/ 978-1-4757-2545-2. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Wainwright, M. J. High-dimensional statistics: A non- asymptotic viewpoint, volume 48. Cambridge University Press, 2019. Wang, R., Foster, D. P., and Kakade, S. M. What are the statistical limits of ofﬂine RL with linear function approx- imation? ICLR, 2021a. URL https://arxiv.org/ abs/2010.11895. Wang, Y., Wang, R., and Kakade, S. M. An exponential lower bound for linearly-realizable mdps with constant suboptimality gap. arXiv preprint arXiv:2103.12690, 2021b. Yang, M. and Nachum, O. Representation matters: Ofﬂine pretraining for sequential decision making. In Meila, M. and Zhang, T. (eds.), ICML, volume 139 of Proceedings of Machine Learning Research, pp. 11784–11794. PMLR, 2021. URL http://proceedings.mlr.press/ v139/yang21h.html. Yang, M., Nachum, O., Dai, B., Li, L., and Schuurmans, D. Off-policy evaluation via the regularized lagrangian. NeurIPS, 33:6551–6561, 2020. Yarats, D., Fergus, R., Lazaric, A., and Pinto, L. Mastering visual continuous control: Improved data-augmented re- inforcement learning. arXiv preprint arXiv:2107.09645, 2021a. Yarats, D., Zhang, A., Kostrikov, I., Amos, B., Pineau, J., and Fergus, R. Improving sample efﬁciency in model- free reinforcement learning from images. AAAI, 2021b. URL http://arxiv.org/abs/1910.01741. Zhang, W., He, J., Zhou, D., Zhang, A., and Gu, Q. Prov- ably efﬁcient representation learning in low-rank markov decision processes. arXiv preprint arXiv:2106.11935, 2021. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Appendices A. Metric Entropy and Entropy Integral We recall the standard notions of entropy integrals here, based on the following distance on Φ, dΦ(φ, (cid:101)φ) =∆ ED (cid:104)(cid:13) (cid:13) (cid:13)φ(s, a) − (cid:101)φ(s, a) (cid:105) (cid:13) (cid:13) (cid:13)2 Let N (t, Φ) denote the t-covering number under dΦ. Deﬁnition A.1. Deﬁne the entropy integral, which we assume to be ﬁnite as, κ(Φ) =∆ (cid:90) 4 0 log1/2 N (t, Φ)dt When Φ is ﬁnite, N (t) ≤ |Φ|, so κ(Φ) ≤ O(log1/2(|Φ|)). B. Technical Lemmas Lemma B.1. Let Xi be i.i.d. random variables s.t. |Xi| ≤ c and E (cid:2)X 2 least 1 − δ, i (cid:3) ≤ ν, then for any δ ∈ (0, 1), we have w.p. at (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 N N (cid:88) i=1 Xi − E [X] (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤ inf a>0 ν 2a + (c + a) log(2/δ) N , and if ν ≤ LE [X] for some positive L, then in particular, (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 N N (cid:88) i=1 (cid:12) (cid:12) Xi − E [X] (cid:12) (cid:12) (cid:12) ≤ 1 2 E [X] + (c + L) log(2/δ) N . Proof. First, by Bernstein’s inequality (Boucheron et al., 2013, Theorem 2.10), we have w.p. 1 − δ, (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 N N (cid:88) i=1 (cid:12) (cid:12) Xi − E [X] (cid:12) (cid:12) (cid:12) (cid:114) ≤ 2ν log(2/δ) N + c log(2/δ) N Using the fact that 2xy ≤ x2/a + ay2 for any a > 0, split the square root term, (c + a) log(2/δ) N which yields the ﬁrst part. If ν ≤ LE [X], picking a = L concludes the proof. ≤ inf a>0 ν 2a + We now state several results of Orlicz norms (mostly from Pollard, 1990) for completeness. For an increasing, convex, positive function Φ : R+ (cid:55)→ R+ , such that Φ(x) ∈ [0, 1), deﬁne the Orlicz norm as (cid:107)Z(cid:107)Φ := inf {C > 0 | E [Φ(|Z|/C)] ≤ 1} . It is indeed a norm on the Φ-Orlicz space of random variables LΦ(ν), since it can be interpreted as the Minkowski functional of the convex set K = {X : E [Φ(|X|)] ≤ 1}. Let x1, x2, . . . , xN be i.i.d. datapoints drawn from some underlying distribution. Let ω denote the randomness of the N sampled datapoints, and let Fω = (cid:8)(f (xi(ω)))N i=1 : f ∈ F(cid:9) ⊂ RN denote the (random) set of vectors from the data corresponding to ω. Let σ denote a vector of N i.i.d. Rademacher random variables (±1 equi-probably), independent of all else. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Lemma B.2 (Symmetrization). For any increasing, convex, positive function Φ, we have (cid:34) (cid:32) Eω Φ sup f ∈Fω (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) N (cid:88) i=1 fi − Eωfi (cid:33)(cid:35) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:34) (cid:32) (cid:33)(cid:35) ≤ Eω,σ Φ 2 sup f ∈Fω |σ · f | . Proof. See Theorem 2.2 of (Pollard, 1990). Lemma B.3 (Contraction). Let F ⊂ RN , and suppose λ : RN (cid:55)→ RN s.t. each component λi : R (cid:55)→ R is L-Lipschitz. Then, for any increasing, convex, positive function Φ, we have (cid:34) (cid:32) Eσ Φ sup f ∈F (cid:33)(cid:35) |σ · λ(f )| ≤ (cid:34) (cid:32) (cid:33)(cid:35) Eσ Φ 2L sup f ∈F |σ · f | 3 2 Proof. Apply Theorem 5.7 of (Pollard, 1990) to the functions λi/L, which are contractions. We now focus on the speciﬁc Orlicz space of sub-Gaussian random variables, with the function Ψ(x) = 1 Ψ-Orlicz norm of the maximum of random variables can be bounded by the maximum of the Ψ-Orlicz norms. 5 exp(x2). The Lemma B.4. For any random variables Z1, ..., Zm, we have (cid:107) max i≤m |Zi|(cid:107)Ψ ≤ (cid:112)2 + log(m) max i≤m (cid:107)Zi(cid:107)Ψ Proof. See Lemma 3.2 of (Pollard, 1990). Lemma B.5. For each f ∈ RN , we have (cid:107)σ · f (cid:107)Ψ ≤ 2(cid:107)f (cid:107)2. Proof. See Lemma 3.1 of (Pollard, 1990). The following is a truncated chaining result for Orlicz norms. This result is not new, but many sources state and prove it in terms of covering and Rademacher complexity, rather than for Orlicz norms. In particular, it generalizes Theorem 3.5 of (Pollard, 1990) – consider a sequence of α’s converging to zero. Lemma B.6 (Chaining). Let F ⊂ RN such that 0 ∈ F. Then, (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) sup f ∈F |σ · f | (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)Ψ (cid:40) ≤ inf α≥0 3α √ N + 9 (cid:90) b α (cid:112)log(D(δ/2, F))dδ (cid:41) , where b = supf ∈F (cid:107)f (cid:107)2 and D(δ, F) is the Euclidean δ-packing number for F. Proof. Suppose b and all the packing numbers are ﬁnite, otherwise the right hand side is inﬁnite and there is nothing to show. For an arbitrary K > 1, construct a sequence of K ﬁner and ﬁner approximations to F, {0} = F0 ⊂ F1 ⊂ · · · ⊂ FK ⊂ FK+1 = F where for any k ∈ [K], Fk satisﬁes the property that for any f ∈ F, there exist nk(f ) ∈ Fk s.t. (cid:107)nk(f ) − f (cid:107)2 ≤ b2−k. Indeed this can be done iteratively: for any Fk, we can construct Fk+1 by adding elements to construct a maximal b2−k packing (maximality ensures the distance requirement, since the existence of any vector which has larger distance can be added to the packing). By deﬁnition of D(·, F), we have |Fk| ≤ D(b2−k, F). Learning Bellman Complete Representations for Ofﬂine Policy Evaluation For any k ∈ [K], we have by triangle inequality, sup f ∈Fk+1 |σ · f | ≤ sup f ∈Fk+1 |σ · nk(f ))| + sup |σ · (nk(f ) − f )| = sup f ∈Fk |σ · f | + sup f ∈Fk+1 f ∈Fk+1 |σ · (nk(f ) − f )| If k = K, we can loosely bound the right-most term by Cauchy-Schwarz, since for any f ∈ F, we have |σ · (nk(f ) − f )| ≤ √ √ N (cid:107)nk(f ) − f (cid:107)2 ≤ N b2−K. So, we have (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) sup f ∈F |σ · f | (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)Ψ ≤ (cid:13) (cid:13) (cid:13) (cid:13) max f ∈FK |σ · f | (cid:13) (cid:13) (cid:13) (cid:13)Ψ + √ N b2−K √ log 5 , since for any non-negative constant c, (cid:107)c(cid:107)Ψ = inf (cid:8)C > 0 : 1 log 5 . If k < K, the suprema are taken over ﬁnite sets, so the maximum is attained. Hence, we can apply a special property of the Ψ-Orlicz norm (Lemma B.4), to get, 5 exp((c/C)2) ≤ 1(cid:9) = c√ (cid:13) (cid:13) (cid:13) (cid:13) max f ∈Fk+1 |σ · f | (cid:13) (cid:13) (cid:13) (cid:13)Ψ By Lemma B.5, ≤ ≤ ≤ ≤ (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) max f ∈Fk |σ · f | max f ∈Fk |σ · f | max f ∈Fk |σ · f | max f ∈Fk |σ · f | + (cid:13) (cid:13) (cid:13) (cid:13) max f ∈Fk+1 |σ · (nk(f ) − f ))| (cid:13) (cid:13) (cid:13) (cid:13)Ψ + (cid:112)2 + log(|Fk+1|) max f ∈Fk+1 (cid:107)σ · (nk(f ) − f ))(cid:107)Ψ + 2(cid:112)2 + log(|Fk+1|) max f ∈Fk+1 (cid:107)nk(f ) − f )(cid:107)2 + 2(cid:112)2 + log(|Fk+1|) · b2−k. Also, note that since F0 = {0} by construction, we have (cid:107)maxf ∈F0 |σ · f |(cid:107)Ψ = 0. Unrolling this, we have (cid:107) sup f ⊂F |σ · f |(cid:107)Ψ ≤ √ N b2−K √ log 5 K−1 (cid:88) + 2−k2b (cid:113) 2 + log(D(b2−(k+1), F)) (cid:13) (cid:13) (cid:13) (cid:13)Ψ (cid:13) (cid:13) (cid:13) (cid:13)Ψ (cid:13) (cid:13) (cid:13) (cid:13)Ψ (cid:13) (cid:13) (cid:13) (cid:13)Ψ k=0 √ Since for any D ≥ 2, we have (cid:112)2 + log(1 + D)/ D ≤ 2.2, √ N b2−K √ log 5 √ N b2−K √ log 5 ≤ = Since D(·, F) is a monotone decreasing, √ √ N b2−K √ log 5 N b2−K √ log 5 ≤ = + 4.4b K−1 (cid:88) k=0 2−k (cid:113) log(D(b2−(k+1), F)) K−1 (cid:88) (cid:113) + 17.6b (2−(k+1) − 2−(k+2)) log(D(b2−(k+1), F)) k=0 (cid:90) b/2 + 17.6 b2−(K+1) (cid:112)log(D(δ, F))dδ (cid:90) b + 8.8 b2−K (cid:112)log(D(δ/2, F))dδ. Now consider any α > 0. Pick K such that b 2K+1 ≤ α ≤ b 2K , then we have (cid:107) sup f ⊂F |σ · f |(cid:107)Ψ ≤ √ 2α √ N log 5 + 8.8 (cid:90) b α (cid:112)log(D(δ/2, F))dδ. Since α was arbitrary, the above bound holds when we take an inﬁmum over α. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation C. Proofs for LSPE We ﬁrst show a generalization of the performance difference lemma (PDL). Lemma C.1 (Generalized PDL). For any policies π, π(cid:48), any function f : S × A (cid:55)→ R, and any initial state distribution µ, we have µ − Es∼µ [f (s, π(cid:48))] = V π 1 1 − γ Es,a∼dπ µ (cid:104) T π(cid:48) (cid:105) f (s, a) − f (s, π(cid:48)) (7) Proof. Let T π be the distribution of trajectories τ = (s0, a0, s1, a1, s2, a2, ...) from rolling out π. So, we have, µ − Es∼µ [f (s, π(cid:48))] = Eτ ∼T π V π = Eτ ∼T π = Eτ ∼T π (cid:35) γtr(st, at) − Es∼µ [f (s, π(cid:48))] (cid:35) γt (r(st, at) + f (st, π(cid:48)) − f (st, π(cid:48))) − f (s0, π(cid:48)) γt (r(st, at) + γf (st+1, π(cid:48)) − f (st, π(cid:48))) (cid:35) (cid:34) ∞ (cid:88) t=0 (cid:34) ∞ (cid:88) t=0 (cid:34) ∞ (cid:88) t=0 = = 1 1 − γ 1 1 − γ Es,a∼dπ µ (cid:2)r(s, a) + γEs(cid:48)∼P (s,a) [f (s(cid:48), π(cid:48))] − f (s, π(cid:48))(cid:3) Es,a∼dπ µ (cid:104) T π(cid:48) (cid:105) f (s, a) − f (s, π(cid:48)) . This generalizes the PDL, which we can get by setting f (s, a) = Qπ(cid:48) (s, a): µ − V π(cid:48) V π µ = = = 1 1 − γ 1 1 − γ 1 1 − γ Es,a∼dπ µ Es,a∼dπ µ (cid:104) T π(cid:48) (cid:104) Qπ(cid:48) Qπ(cid:48) (s, a) − Qπ(cid:48) (s, π(cid:48)) (cid:105) (s, a) − V π(cid:48) (s) (cid:105) Es,a∼dπ µ (cid:104) Aπ(cid:48) (cid:105) (s, a) . To prove our LSPE guarantee, we’ll instantiate f (s, a) to be the estimated (cid:98)f (s, a) from LSPE, and set π = π(cid:48). This gives us an expression for the prediction error of LSPE, (cid:12) (cid:12)V π (cid:12) µ − Es∼µ (cid:104) (cid:98)fk(s, π) (cid:105)(cid:12) (cid:12) (cid:12) = 1 1 − γ (cid:12) (cid:12) (cid:12) Edπ µ (cid:104) T π (cid:98)fk(s, a) − fk(s, a) (cid:105)(cid:12) (cid:12) (cid:12) . We then upper bound the right hand side by its L2(dπ of running LSPE by the regression losses at each step. Lemma C.2. Consider any policy π and functions f1, . . . , fK : S × A (cid:55)→ R that satisfy maxk=1,...,K (cid:107)fk − T πfk−1(cid:107)L2(dπ ) ≤ η, and f0(s, a) = 0. Then, for all k = 1, . . . , K, we have (cid:107)fk − T πfk(cid:107)L2(dπ µ) norm, which is the Bellman error. Next, we bound the Bellman error ) ≤ 4 1−γ η + γk/2. p0 p0 Proof. For any k = 1, . . . , K, (cid:107)fk − T πfk(cid:107)L2(dπ p0 ) ≤ (cid:107)fk − T πfk−1(cid:107)L2(dπ ≤ η + γ(Es,a∼dπ p0 ≤ η + γ(Es,a∼dπ p0 ) + (cid:107)T πfk−1 − T πfk(cid:107)L2(dπ p0 ) p0 [(Es(cid:48),a(cid:48)∼P (s,a)◦π [fk−1(s(cid:48), a(cid:48)) − fk(s(cid:48), a(cid:48))])2])1/2 ,s(cid:48),a(cid:48)∼P (s,a)◦π[(fk−1(s(cid:48), a(cid:48)) − fk(s(cid:48), a(cid:48)))2])1/2 Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Since dπ p0 thus non-negative, (s, a) = γE˜s,˜a∼dπ P0 P (s|˜s, ˜a)π(a|s) + (1 − γ)p0(s)π(a|s), and the quantity inside the expectation is a square, and ≤ η + γ(γ−1Es,a∼dπ p0 [(fk−1(s, a) − fk(s, a))2])1/2 √ = η + γ(cid:107)fk−1 − fk(cid:107)L2(dπ p0 ) √ ≤ η + γ (cid:16) η + (cid:107)fk−1 − T πfk−1(cid:107)L2(dπ p0 (cid:17) . ) Unrolling the recursion and using (cid:107)f0 − T πf0(cid:107)L2(dπ p0 ) ≤ 1, we have (cid:107)fK − T πfK(cid:107)L2(dπ p0 ) ≤ η + √ γ(η + √ γ(η + . . . √ γ(η + 1) . . . )) √ γK √ γ 1 − 1 − = η + γK/2, which gives the claim since 1 √ 1− γ ≤ 2(1 − γ)−1 and 1 − √ γK ≤ 1. The following lemma is a “fast rates”-like result for norms. It shows that the norm induced by the empirical covariance matrix (cid:98)Σ can be bounded by the norm induced by two times the population covariance matrix Σ, up to some (cid:101)O(N −1/2) terms. Lemma C.3 (Fast Rates for Σ-norm). For any δ ∈ (0, 1), we have with probability at least 1 − δ, for any x ∈ BW , we have and (cid:107)x(cid:107) (cid:98)Σ ≤ 2(cid:107)x(cid:107)Σ + 5W (cid:114) d log(N/δ) N , (cid:107)x(cid:107)Σ ≤ 2(cid:107)x(cid:107) (cid:98)Σ + 5W (cid:114) d log(N/δ) N . Proof. First, ﬁx any x ∈ BW . Since (x(cid:124)φ(s, a))2 ≤ W 2 almost surely, we have E (cid:2)(x(cid:124)φ(s, a))4(cid:3) ≤ W 2E (cid:2)(x(cid:124)φ(s, a))2(cid:3). So by Lemma B.1, w.p. at least 1 − δ, (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 N N (cid:88) i=1 (cid:12) (cid:12) (x(cid:124)φ(si, ai))2 − E (cid:2)(x(cid:124)φ(s, a))2(cid:3) (cid:12) (cid:12) (cid:12) ≤ 1 2 E (cid:2)(x(cid:124)φ(s, a))2(cid:3) + 2W 2 log(2/δ) N . In particular, this means (cid:107)x(cid:107)2 (cid:98)Σ W/ N √ N -net of BW , which can be done with (1 + 2 2 (cid:107)x(cid:107)2 ≤ 3 Σ + 2W 2 log(2/δ) √ and (cid:107)x(cid:107)2 Σ ≤ 2(cid:107)x(cid:107)2 (cid:98)Σ + 4W 2 log(2/δ) N . Now union bound over a N )d elements. The approximation error from this cover is (cid:107)x(cid:107) ≤ (cid:98)Σ ≤ (cid:107)n(x)(cid:107) (cid:114) 3 2 (cid:114) 3 2 (cid:114) 3 2 ≤ ≤ (cid:98)Σ + (cid:107)n(x) − x(cid:107) (cid:98)Σ (cid:115) (cid:107)n(x)(cid:107)Σ + 2W 2 log(2(1 + 2 (cid:114) 3 2 (cid:114) (cid:107)x(cid:107)Σ + (cid:107)n(x) − x(cid:107)Σ + (cid:107)x(cid:107)Σ + 4W d log(N/δ) N , √ N )d/δ) + W √ N N (cid:115) 2W 2 log(2(1 + 2 √ N )d/δ) N + W √ N Learning Bellman Complete Representations for Ofﬂine Policy Evaluation where n(x) is the closest element in the net to x. Similarly, (cid:107)x(cid:107)Σ ≤ (cid:107)n(x)(cid:107)Σ + (cid:107)n(x) − x(cid:107)Σ ≤ 2(cid:107)n(x)(cid:107) (cid:98)Σ + (cid:115) 4W 2 log(2(1 + 2 √ N )d/δ) + N (cid:115) W √ N √ ≤ 2(cid:107)x(cid:107) (cid:98)Σ + 2(cid:107)n(x) − x(cid:107) (cid:114) (cid:98)Σ + + ≤ 2(cid:107)x(cid:107) (cid:98)Σ + 5W d log(N/δ) N . 4W 2 log(2(1 + 2 N )d/δ) + W √ N N Now deﬁne the following notation for analyzing Least Squares Policy Evaluation (LSPE). For every target vector (cid:107)ϑ(cid:107)2 ≤ W , deﬁne yϑ = r + γϑ(cid:124)φ(s(cid:48), π), θϑ = arg min (cid:107)θ(cid:107)≤W Es,a∼ν,s(cid:48)∼P (s,a)[(yϑ − θ(cid:124)φ(s, a))2] := (cid:96)(θ, ϑ), i = ri + γϑ(cid:124)φ(s(cid:48) yϑ (cid:98)θϑ = arg min (cid:107)θ(cid:107)≤W 1 N i, π), N (cid:88) i=1 (yϑ i − θ(cid:124)φ(si, ai))2 := (cid:98)(cid:96)(θ, ϑ). The following lemma are useful facts about the optimal θϑ and (cid:98)θϑ. Lemma C.4. For any ϑ1, ϑ2, we have For any θ, ϑ, we have (cid:107)θϑ1 − θϑ2 (cid:107)Σ ≤ (cid:112)2γW (cid:107)ϑ1 − ϑ2(cid:107)2, (cid:98)Σ ≤ (cid:112)2γW (cid:107)ϑ1 − ϑ2(cid:107)2. (cid:107)(cid:98)θϑ1 − (cid:98)θϑ2 (cid:107) (cid:96)(θ, ϑ) − (cid:96)(θϑ, ϑ) ≥ (cid:107)θ − θϑ(cid:107)2 Σ. recall that θϑ minimizes f (θ) = E (cid:2)(yϑ − θ(cid:124)φ(s, a))2(cid:3), which has the Jacobian ∇f (θ) = Proof. First, 2E (cid:2)(θ(cid:124)φ(s, a) − yϑ)φ(s, a)(cid:3). Since θϑ is optimal over BW , the necessary optimality condition is that −∇f (θϑ) ∈ ϑφ(s, a))(cid:3) , θ − θϑ(cid:105) ≤ 0. In NBW (θϑ), the normal cone of BW at θϑ, i.e. for any θ ∈ BW , we have (cid:104)E (cid:2)φ(s, a)(yϑ − θ particular, we have (cid:124) adding the two we get (cid:104)E (cid:2)φ(s, a)(yϑ1 − θ (cid:104)E (cid:2)φ(s, a)(yϑ2 − θ (cid:124) ϑ1 (cid:124) ϑ2 φ(s, a))(cid:3) , θϑ2 − θϑ1(cid:105) ≤ 0 φ(s, a))(cid:3) , θϑ1 − θϑ2(cid:105) ≤ 0 (cid:107)θϑ1 − θϑ2 (cid:107)2 Σ = (cid:104)E [φ(s, a)(θϑ1 − θϑ2 )(cid:124)φ(s, a))] , θϑ1 − θϑ2(cid:105) ≤ (cid:104)E (cid:2)(yϑ1 − yϑ2)φ(s, a)(cid:3) , θϑ1 − θϑ2(cid:105) = γ(cid:104)E [φ(s, a)φ(s(cid:48), π)(cid:124)] (ϑ1 − ϑ2), θϑ1 − θϑ2(cid:105) ≤ γ(cid:107)E [φ(s, a)φ(s(cid:48), π)(cid:124)] (cid:107)2(cid:107)ϑ1 − ϑ2(cid:107)2(cid:107)θϑ1 − θϑ2 (cid:107)2 ≤ 2γW (cid:107)ϑ1 − ϑ2(cid:107)2. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation The claim with (cid:98)θϑ1, (cid:98)θϑ2 follows by the same argument. For the second claim, we ﬁrst apply the Parallelogram law, followed by the ﬁrst-order optimality of θϑ, (cid:96)(θ, ϑ) = (cid:96)(θϑ, ϑ) + Eν((θϑ − θ)(cid:124)φ(s, a))2 + 2Eν[(yϑ − θ(cid:124)φ(s, a))φ(s, a)(cid:124)(θϑ − θ)] ≥ (cid:96)(θϑ, ϑ) + Eν((θϑ − θ)(cid:124)φ(s, a))2 + 0 = (cid:96)(θϑ, ϑ) + (cid:107)θ − θϑ(cid:107)2 Σ. Now we show our key lemma about the concentration of least squares, uniformly over all targets generated by ϑ ∈ BW . Lemma C.5 (Concentration for Least Squares). Let F = {(s, a) (cid:55)→ θ(cid:124)φ(s, a) : (cid:107)θ(cid:107)2 ≤ W }, with (cid:107)φ(s, a)(cid:107)2 ≤ 1. Then, for any δ ∈ (0, 1) w.p. at least 1 − δ, we have (cid:107)ˆθϑ − θϑ(cid:107)Σ < 120d(1 + W ) sup ϑ∈BW log(N )(cid:112)log(10/δ) √ N . Proof. By Lemma C.3, we have that w.p. at least 1 − δ, we can bound the random (cid:107) · (cid:107) simultaneously over all vectors in a ball, (cid:98)Σ norm by (cid:107) · (cid:107)Σ, and vice versa, E = {∀x ∈ B2W , (cid:107)x(cid:107) (cid:98)Σ ≤ 2(cid:107)x(cid:107)Σ + t and (cid:107)x(cid:107)Σ ≤ 2(cid:107)x(cid:107) (cid:98)Σ + t}, provided t ≥ 5W probability and expectations will be implicitly conditioned on E. (cid:113) d log(N/δ) N . For the remainder of this proof, we condition on this high-probability event. That is, any First, we’ll show that for an arbitrary and ﬁxed ϑ ∈ BW , w.p. at least 1 − δ, we have (cid:107)(cid:98)θϑ − θϑ(cid:107)Σ < t. To do so, we will bound the probability of the complement, which in turn can be simpliﬁed by the following chain of arguments, (cid:107)ˆθϑ − θϑ(cid:107)Σ ≥ t By (cid:96)(θ, ϑ) − (cid:96)(θϑ, ϑ) ≥ (cid:107)θ − θϑ(cid:107)Σ from Lemma C.4, =⇒ (cid:96)(ˆθϑ, ϑ) − (cid:96)(θϑ, ϑ) ≥ t2 =⇒ ∃(cid:107)θ(cid:107) ≤ W : (cid:96)(θ, ϑ) − (cid:96)(θϑ, ϑ) ≥ t2, ˆ(cid:96)(θ, ϑ) − ˆ(cid:96)(θϑ, ϑ) ≤ 0 By convexity and continuity of (cid:96)(·, ϑ), we can make this strict equality. Indeed, given this, by Intermediate Value Theorem, there exists λ ∈ [0, 1] such that θ(cid:48) = (1 − λ)θ + λθϑ has (cid:96)(θ(cid:48), ϑ) − (cid:96)(θϑ, ϑ) = ν. Then by convexity, ˆ(cid:96)(θ(cid:48), ϑ) − ˆ(cid:96)(θϑ, ϑ) ≤ (1 − λ)ˆ(cid:96)(θ, ϑ) − (1 − λ)ˆ(cid:96)(θϑ, ϑ) ≤ 0. =⇒ ∃(cid:107)θ(cid:107) ≤ W : (cid:96)(θ, ϑ) − (cid:96)(θϑ, ϑ) = t2, ˆ(cid:96)(θ, ϑ) − ˆ(cid:96)(θϑ, ϑ) ≤ 0 =⇒ ∃(cid:107)θ(cid:107) ≤ W : (cid:107)θ − θϑ(cid:107)Σ ≤ t, ((cid:96)(θ, ϑ) − ˆ(cid:96)(θ, ϑ)) − ((cid:96)(θϑ, ϑ) − ˆ(cid:96)(θϑ, ϑ)) ≥ t2 By conditioning on E, =⇒ ∃(cid:107)θ(cid:107) ≤ W : (cid:107)θ − θϑ(cid:107) (cid:98)Σ ≤ 3t, ((cid:96)(θ, ϑ) − ˆ(cid:96)(θ, ϑ)) − ((cid:96)(θϑ, ϑ) − ˆ(cid:96)(θϑ, ϑ)) ≥ t2 Hence, we now focus on bounding (cid:32) P sup θ∈Θ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) N (cid:88) i=1 (cid:12) (cid:12) ζi(θ, ϑ) − ζi(θϑ, ϑ) − Eν [ζi(θ, ϑ) − ζi(θϑ, ϑ)] (cid:12) (cid:12) (cid:12) (cid:33) ≥ N t2 ≤ δ, Learning Bellman Complete Representations for Ofﬂine Policy Evaluation where we deﬁne ζi(θ, ϑ) = (yϑ i − θ(cid:124)φ(si, ai))2, Θ = {θ : (cid:107)θ(cid:107) ≤ W, (cid:107)θ − θϑ(cid:107) Since ζi(θ, ϑ) − ζi(θϑ, θ) = λi((θ − θϑ)(cid:124)φ(si, ai)) where λi(x) = x(2yϑ and λi(0) = 0, the contraction lemma (Lemma B.3) tells us that it sufﬁces to consider a simpler class. Deﬁne (cid:98)Σ ≤ 3t}. i − (θ + θϑ)(cid:124)φ(si, ai)) is 2(1 + W )-Lipschitz (8) ωi(θ, ϑ) = (θ − θϑ)(cid:124)φ(si, ai) and ω(θ, ϑ) = (ωi(θ, ϑ))N i=1. By Chaining (Lemma B.6), we have (cid:18) 1 J (cid:40) |σ · ω(θ, ϑ)| Eσ Ψ (cid:20) sup Θ √ (cid:90) b where J = inf α≥0 3α N + 9 α (cid:19)(cid:21) ≤ 1 (cid:112)log(D(δ/2, ω(Θ)))dδ (cid:41) , D(δ, F) is the Euclidean packing number of F ⊂ RN , and b = supΘ (cid:107)ω(θ, ϑ)(cid:107)2 is the envelope. ω(Θ) = {ω(θ, ϑ) : θ ∈ Θ} , Now we’ll bound the truncated entropy integral, J. First notice that b ≤ 3t which localizes in (cid:107) · (cid:107) (cid:98)Σ, √ N , based on the deﬁnition of Θ (Equation (8)), b √ N = sup Θ (cid:118) (cid:117) (cid:117) (cid:116) 1 N N (cid:88) i=1 (θ − θϑ)(cid:124)φ(si, ai)φ(si, ai)(cid:124)(θ − θϑ) = sup Θ (cid:107)θ − θϑ(cid:107) (cid:98)Σ ≤ 3t. Now, we bound the packing number D(·, ω(Θ)). Let θ1, θ2 ∈ BW be arbitrary, (cid:107)ω(θ1, ϑ) − ω(θ2, ϑ)(cid:107)2 √ N = (cid:107)θ1 − θ2(cid:107) (cid:98)Σ ≤ (cid:113) σmax((cid:98)Σ)(cid:107)θ1 − θ2(cid:107)2 ≤ (cid:107)θ1 − θ2(cid:107)2. So for any ε, we can construct an ε-cover by setting (cid:107)θ1 − θ2(cid:107)2 ≤ ε/ N (ε, F) denote the Euclidean covering number of F ⊂ RN . Then, √ N , which requires (W (1 + 2 √ N /ε))d points. Let log D(ε, ω(Θ)) ≤ log N (ε/2, ω(Θ)) ≤ d log W (1 + 4 √ N /ε) So, √ N (cid:90) 3t (cid:112)log(D(ε/2, ω(Θ)))dε 0 (cid:90) 3t √ N (cid:113) d log(W (1 + 8 √ N /ε))dε J ≤ ≤ 0 √ ≤ 3t √ ≤ 3t dN ((cid:112)log W + (cid:90) 1 (cid:112)log(1 + 3/(εt)))dε dN ((cid:112)log W + (cid:112)log(4/t)), 0 since (cid:82) 1 0 (cid:112)log(1 + c/ε)dε ≤ (cid:112)log(1 + c) for any c > 0, and assuming t ≤ 1. Now we put everything together. Let c denote a positive constant, (cid:32) P (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) N (cid:88) i=1 (cid:12) (cid:12) ζi(θ, ϑ) − ζi(θϑ, ϑ) − Eν [ζi(θ, ϑ) − ζi(θϑ, ϑ)] (cid:12) (cid:12) (cid:12) sup Θ (cid:33) ≥ N t2 Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Since Ψ is increasing, (cid:32) (cid:32) = P Ψ c sup Θ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) N (cid:88) i=1 (cid:12) (cid:12) ζi(θ, ϑ) − ζi(θϑ, ϑ) − Eν [ζi(θ, ϑ) − ζi(θϑ, ϑ)] (cid:12) (cid:12) (cid:12) (cid:33) (cid:33) ≥ Ψ (cid:0)cN t2(cid:1) By Markov’s inequality, (cid:104) E Ψ ≤ (cid:16) c supΘ (cid:12) (cid:12) (cid:12) (cid:80)N (cid:12) i=1 ζi(θ, ϑ) − ζi(θϑ, ϑ) − Eν [ζi(θ, ϑ) − ζi(θϑ, ϑ)] (cid:12) (cid:12) (cid:17)(cid:105) Ψ (cN t2) By Symmetrization (Lemma B.2), ≤ E [Eσ [Ψ (2c supΘ |σ · (ζ(θ, ϑ) − ζ(θϑ, ϑ))|)]] Ψ (cN t2) By Contraction (Lemma B.3) and that ζi(θ, ϑ) − ζi(θϑ, ϑ) = λi(ωi(θ, ϑ)) where λi is L = 2(1 + W ) Lipschitz, ≤ 3 2 · E [Eσ [Ψ (4cL supΘ |σ · ω(θ, ϑ)|)]] Ψ (cN t2) Setting c = 1 4LJ and applying Chaining (Lemma B.6), the numerator is bounded by 1, (cid:32) ≤ 8 exp − (cid:19)2(cid:33) (cid:18) N t2 4LJ Applying upper bound on J,  (cid:32) ≤ 8 exp − 8(1 + W ) · 3t √ N t2 √ dN ( log W + (cid:112)log(4/t)) (cid:19) (cid:33)2  (cid:18) ≤ 8 exp − N t2 242(1 + W )2d(log W + log(4/t)) Now, we set t = 24(1 + W ) (cid:113) d log(N ) log(1/δ) N , (cid:18) ≤ 8 exp − log(N ) log(1/δ) log(N/d log(N )) (cid:19) Since log(N ) ≥ log(N/d log(N )), ≤ 8δ. Hence, we have shown that for an arbitrary and ﬁxed ϑ ∈ BW , w.p. 1 − 9δ, we have (cid:107)(cid:98)θϑ − θϑ(cid:107)Σ < t = 24(1 + W ) (cid:114) d log(N ) log(1/δ) N . We ﬁnally apply a union bound over ϑ ∈ BW . Consider a W/N -cover of ϑ ∈ BW , which requires (1 + 2N )d points. Then, for any ϑ ∈ BW , we have (cid:107)(cid:98)θϑ − θϑ(cid:107)Σ ≤ (cid:107)(cid:98)θϑ − (cid:98)θn(ϑ)(cid:107)Σ + (cid:107)(cid:98)θn(ϑ) − θn(ϑ)(cid:107)Σ + (cid:107)θn(ϑ) − θϑ(cid:107)Σ (cid:98)Σ + t) + t + (cid:112)2γW (cid:107)n(ϑ) − ϑ(cid:107) ≤ (2(cid:107)(cid:98)θϑ − (cid:98)θn(ϑ)(cid:107) ≤ 2t + 3(cid:112)2γW (cid:107)n(ϑ) − ϑ(cid:107) ≤ 2t + 3(cid:112)2γW 2/N ≤ 5t. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Thus, we have shown that w.p. 1 − δ, sup ϑ∈BW (cid:107)(cid:98)θϑ − θϑ(cid:107)Σ < 120d(1 + W ) log(N )(cid:112)log(10/δ) √ N . A nice corollary is that when Σ provides full coverage, i.e. Σ (cid:23) βI for some positive β, then we can bound sup ϑ∈BW (cid:107)ˆθϑ − θϑ(cid:107)2 ≤ σmin(Σ)−1/2 sup ϑ∈BW (cid:107)(cid:98)θϑ − θϑ(cid:107)Σ ≤ β−1/2 · sup ϑ∈BW (cid:107)(cid:98)θϑ − θϑ(cid:107)Σ. C.1. Main LSPE theorem We now prove our LSPE sample complexity guarantee Theorem 3.3. Theorem 3.3 (Sample Complexity of LSPE). Assume feature φ satisﬁes approximate Linear BC with parameter εν. For any δ ∈ (0, 0.1), w.p. at least 1 − δ, we have for any initial state distribution p0 (cid:12) (cid:12)V πe (cid:12) p0 (cid:104) − Es∼p0 (cid:98)fK(s, πe) (cid:105)(cid:12) (cid:12) (cid:12) ≤ γK/2 1 − γ + (cid:114)(cid:13) (cid:13) 4 (cid:13) ddπe p0 dν (cid:13) (cid:13) (cid:13)∞ (1 − γ)2 εν + 480(cid:112)κ(p0)(1 + W )d log(N )(cid:112)log(10/δ) √ (1 − γ)2 N , where (cid:98)fK is the output of Algorithm 1. Proof of Theorem 3.3. By Lemmas C.1 and C.2, (cid:12) (cid:12)V π (cid:12) p0 (cid:104) − Es∼p0 (cid:98)fk(s, π) (cid:105)(cid:12) (cid:12) (cid:12) ≤ 4 (1 − γ)2 max k=1,2,... (cid:107) ˆfk − T π ˆfk−1(cid:107)L2(dπ p0 ) + γk/2 1 − γ . Next, we bound the maximum regression error. Consider any initial state distribution p0, then we have max k=1,2,... (cid:107) ˆfk − T π ˆfk−1(cid:107)L2(dπ p0 ) ≤ sup (cid:107)ϑ(cid:107)≤W (cid:124) (cid:107)ˆθ ϑφ − T π(ϑ(cid:124)φ)(cid:107)L2(dπ p0 ) ≤ sup (cid:107)ϑ(cid:107)≤W (cid:124) (cid:107)ˆθ ϑφ − θ (cid:124) ϑφ(cid:107)L2(dπ p0 (cid:107)θ (cid:124) ϑφ − T π(ϑ(cid:124)φ)(cid:107)L2(dπ p0 ) ≤ (cid:112)κ(p0) sup (cid:107)ϑ(cid:107)≤W (cid:107)ˆθϑ − θϑ(cid:107)Σ + sup (cid:107)ϑ(cid:107)≤W (cid:107)θ (cid:124) ϑφ − T π(ϑ(cid:124)φ)(cid:107)L2(ν) ) + sup (cid:107)ϑ(cid:107)≤W (cid:115)(cid:13) (cid:13) (cid:13) (cid:13) (cid:115)(cid:13) (cid:13) (cid:13) (cid:13) ddπ p0 dν ddπ p0 dν (cid:13) (cid:13) (cid:13) (cid:13)∞ (cid:13) (cid:13) (cid:13) (cid:13)∞ εν. ≤ (cid:112)κ(p0) sup ϑ∈BW (cid:107)ˆθϑ − θϑ(cid:107)Σ + The quantity supϑ∈BW (cid:107)ˆθϑ − θϑ(cid:107)Σ can be directly bounded by Lemma C.5 w.p. at least 1 − δ. Thus, we have shown the desired result: for any initial state distribution p0, (cid:12) (cid:12)V π (cid:12) p0 − Es,a∼p0◦π (cid:104) (cid:98)fk(s, a) (cid:105)(cid:12) (cid:12) (cid:12) ≤ 4 (1 − γ)2 (cid:32) (cid:112)κ(p0)120d(1 + W ) log(N )(cid:112)log(10/δ) √ N + (cid:33) (cid:115)(cid:13) (cid:13) (cid:13) (cid:13) ddπ p0 dν (cid:13) (cid:13) (cid:13) (cid:13)∞ εν + γk/2 1 − γ . Learning Bellman Complete Representations for Ofﬂine Policy Evaluation D. Proofs for Linear BC Equivalence Proposition 4.2. Consider a feature φ with full rank covariance Σ(φ). Given any W > 0, the feature φ being linear BC (under BW ) implies that there exist (ρ, M ) ∈ BW × Rd×d with (cid:107)M (cid:107)2 ≤ 1 − (cid:107)ρ(cid:107)2 (cid:113) W 2 , and Eν (cid:21) (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) φ(s, a) − (cid:20)γEs(cid:48)∼P (s,a)φ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 = 0. (2) On the other hand, if there exists (ρ, M ) ∈ BW × Rd×d with (cid:107)M (cid:107)2 < 1 such that the above equality holds, then φ must satisfy exact linear BC with W ≥ (cid:107)ρ(cid:107)2 . 1−(cid:107)M (cid:107)2 Proof. (⇐=) Suppose that (cid:107)M (cid:107)2 < 1 and ρ ∈ BW satisfy, Eν (cid:21) (cid:13) (cid:20)M (cid:13) (cid:13) ρ(cid:124) (cid:13) φ(s, a) − (cid:20)γEs(cid:48)∼P (s,a)φ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 = 0. Then, for any w1 ∈ BW , setting w2 = ρ + M (cid:124)w1 satisﬁes, 2 φ(s, a) − r(s, a) − γEs(cid:48)∼P (s,a) [w (cid:124) 1 φ(s(cid:48), πe)](cid:1)2 (cid:107)w (cid:124) 2 φ − T πe (w (cid:124) 1 φ)(cid:107)2 ν = Eν = Eν = Eν = 0. (cid:124) (cid:0)w (cid:0)ρ(cid:124)φ(s, a) − r(s, a) + w (cid:0)w (cid:124) 1 (cid:0)M φ(s, a) − γEs(cid:48)∼P (s,a) [φ(s(cid:48), πe)](cid:1)(cid:1)2 (cid:124) 1 M φ(s, a) − γEs(cid:48)∼P (s,a) [w (cid:124) 1 φ(s(cid:48), πe)](cid:1)2 Also, we have (cid:107)w2(cid:107)2 ≤ (cid:107)ρ(cid:107)2 + (cid:107)M (cid:107)2(cid:107)w1(cid:107)2 ≤ W since W ≥ (cid:107)ρ(cid:107)2 1−(cid:107)M (cid:107)2 . Thus, φ satisﬁes exact Linear BC. (=⇒) Suppose φ satisﬁes exact Linear BC, that is max w1∈BW min w2∈BW (cid:107)w (cid:124) 2 φ − T π(w (cid:124) 1 φ)(cid:107)2 ν = 0. To see that there exists ρ ∈ BW that linearizes the reward w.r.t φ under ν, set w1 = 0, and we have: min w2∈BW Es,a∼ν (cid:13) (cid:13)w(cid:62) 2 φ(s, a) − r(s, a)(cid:13) 2 2 = 0. (cid:13) Let ρ to be the minimizer of the above objective. Now we need to show that there exists a M ∈ Rd×d with (cid:107)M (cid:107)2 < (cid:113) 1 − (cid:107)ρ(cid:107)2 W 2 that satisﬁes Es,a∼ν (cid:13) (cid:13)M φ(s, a) − γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)(cid:13) 2 2 = 0. (cid:13) To extract the i-th row of M , plug in wi = W ei (note that wi ∈ BW ). By exact Linear BC, we know that there exists a vector vi ∈ BW , such that: (cid:13) (cid:13)v (cid:124) i φ(s, a) − ρ(cid:124)φ(s, a) − γW Es(cid:48)∼P (s,a)e Repeating this for every i ∈ [d], we can construct M as follows, M = 1 W  (v1 − ρ)(cid:124) ...   (vd − ρ)(cid:124)    , (cid:124) i φ(s(cid:48), πe)(cid:13) (cid:13)ν = 0. which satisﬁes, Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Es,a∼ν d (cid:88) = i=1 d (cid:88) i=1 = (cid:13)M φ(s, a) − γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)(cid:13) (cid:13) 2 (cid:13) 2 Es,a∼ν (cid:13) (cid:13)e (cid:124) i (cid:0)M φ(s, a) − γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)(cid:1)(cid:13) 2 (cid:13) 2 Es,a∼ν (cid:13) (cid:13) (cid:13) (cid:13) 1 W (vi − ρ)(cid:124)φ(s, a) − γEs(cid:48)∼P (s,a)e (cid:13) 2 (cid:13) (cid:124) i φ(s(cid:48), πe) (cid:13) (cid:13) 2 = 0. Hence, we have (cid:13) (cid:13)M φ(s, a) − γEs(cid:48)∼P (s,a)φ(s(cid:48), πe)(cid:13) 2 ν = 0. (cid:13) (cid:113) 1 − (cid:107)ρ(cid:107)2 W 2 . First we show that (cid:107)M (cid:107)2 ≤ 1. For any w1 ∈ BW , by exact linear Finally, we must show that (cid:107)M (cid:107)2 < BC, there exists w2 ∈ BW s.t. (cid:13) 1 φ(s(cid:48), πe)](cid:13) 2 ν = 0, and by the construction of M , (cid:13)w (cid:13) φ(s, a)(cid:107)2 satisﬁes (cid:107)(w2 − ρ − M (cid:124)w1) Σ(φ) = 0. Since Σ(φ) is positive deﬁnite, we have that w2 = ρ + M (cid:124)w1 is the unique choice of w2, which by exact linear BC is in BW . Hence, we’ve shown that for any w1 ∈ BW , we also have that ρ + M (cid:124)w1 ∈ BW . Now take w1 and −w1, subtracting the two expressions yields that 2M (cid:63)(cid:124)w1 ∈ B2W . Since this is true for arbitrary w1, taking supremum over w1 ∈ BW shows that (cid:107)M (cid:107)2 ≤ 1. 2 φ(s, a) − r(s, a) − γEs(cid:48)∼P (s,a) [w ν = (cid:107)w2 − ρ − M (cid:124)w1(cid:107)2 (cid:124) (cid:124) (cid:124) (cid:124) Now we show that the inequality must be strict. Consider the singular value decomposition: M = (cid:80)d i where {ui}i∈[d] and {vi}i∈[d] are each an orthonormal basis of Rd, and σi is the i-th largest singular value. Without loss of generality, suppose ρ(cid:124)u1 ≥ 0, since we can always ﬂip the sign of u1. If we pick x = W v1 ∈ BW , we have M x = W σ1u1. By the argument in the previous paragraph, since x ∈ BW , we have ρ + M x ∈ BW , implying that i=1 σiuiv W 2 ≥ (cid:107)M x + ρ(cid:107)2 2 = (cid:107)W σ1u1 + (ρ(cid:124)u1)u1 + (ρ − (ρ(cid:124)u1)u1)(cid:107)2 2 Since u1 and (ρ − (ρ(cid:124)u1)u1) are orthogonal, by Pythagoras, we have = |W σ1 + ρ(cid:124)u1|2 + (cid:107)(ρ − (ρ(cid:124)u1)u1)(cid:107)2 2 = (W σ1)2 + 2W σ1ρ(cid:124)u1 + (ρ(cid:124)u1)2 + (cid:107)(ρ − (ρ(cid:124)u1)u1)(cid:107)2 2 = (W σ1)2 + 2W σ1ρ(cid:124)u1 + (cid:107)(ρ(cid:124)u1)u1(cid:107)2 2 + (cid:107)(ρ − (ρ(cid:124)u1)u1)(cid:107)2 2 By Pythagoras, = (W σ1)2 + 2W σ1ρ(cid:124)u1 + (cid:107)ρ(cid:107)2 2 . Hence, we get the following inequality: Solve for σ1 and using the fact that ρ(cid:124)u1 ≥ 0, we have that, W 2σ2 1 + 2W (ρ(cid:124)u1)σ1 + (cid:107)ρ2(cid:107)2 − W 2 ≤ 0. σ1 ≤ −ρ(cid:124)u1 + (cid:112)(ρ(cid:124)u1)2 + (W 2 − (cid:107)ρ(cid:107)2) W (cid:112)W 2 − (cid:107)ρ(cid:107)2 W ≤ (cid:114) ≤ 1 − (cid:107)ρ(cid:107)2 W 2 . We ﬁnally show that (cid:107)ρ(cid:107)2 < W unless M = 0. We prove this by contradiction. Assume (cid:107)ρ(cid:107)2 ≥ W . Following the above argument, take any w1 ∈ BW , we must have w2 := ρ + M (cid:124)w1 ∈ BW . We discuss two cases. First if ρ (cid:54)∈ range(M (cid:124)). In this case, we must have (cid:107)w2(cid:107)2 has non-zero entries. Thus, this case leads to contradiction . 2 = (cid:107)ρ(cid:107)2 2 + (cid:107)M (cid:124)w1(cid:107)2 2 = W 2 + (cid:107)M (cid:124)w1(cid:107)2 2 > W 2, as long as M Second, if ρ ∈ range(M (cid:124)). In this case, there must exist a vector x (cid:54)= 0 such that M (cid:124)x = ρ. Consider ¯x := W x (cid:107)x(cid:107)2 We have w2 := ρ + M (cid:124) ¯x = ρ , which means that (cid:107)w2(cid:107)2 > W , which causes contradiction again. (cid:17) (cid:16) 1 + W (cid:107)x(cid:107)2 ∈ BW . So unless M = 0, which only happens when γ = 0 (i.e. horizon is 1), we have (cid:107)ρ(cid:107)2 < W . Learning Bellman Complete Representations for Ofﬂine Policy Evaluation (cid:21) Eν (ρ,M )∈Θ Llbc(φ) = min We now show an approximate version to the equivalence of Proposition 4.2. First, recall the bilevel loss from Equation (3). We use Llbc and (cid:98)Llbc to denote the population and empirical versions as follows, (cid:13) (cid:20)M (cid:20)γEs(cid:48)∼P (s,a) [φ(s(cid:48), πe)] (cid:13) (cid:13) ρ(cid:124) r(s, a) (cid:13) (cid:21)(cid:13) (cid:13) (cid:20)M 2 (cid:13) (cid:13) (cid:13) (cid:13) ρ(cid:124) (cid:13) (cid:13) 2 where Θ = (cid:8)(ρ, M ) ∈ BW × Rd×d : (cid:107)ρ(cid:107) ≤ (cid:107)ρ(cid:63)(cid:107), (cid:107)M (cid:107)2 ≤ (cid:107)M (cid:63)(cid:107)2 (cid:9) and (ρ(cid:63), M (cid:63)) are corresponding to the linear BC φ(cid:63). Lemma D.1. Suppose a feature (cid:98)φ satisﬁes Llbc( (cid:98)φ) ≤ ε2. Then, (cid:98)φ is ε(1 + W )-approximately Linear BC, provided W ≥ (cid:107)ρ(cid:63)(cid:107)2 ED (cid:107)g(s, a) − γφ(s(cid:48), πe)(cid:107)2 2 , (cid:20)γφ(s(cid:48), πe) r(s, a) (cid:98)Llbc(φ) = min (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 − min g∈G φ(s, a) − φ(s, a) − (ρ,M )∈Θ (10) ED (9) (cid:21) . 1−(cid:107)M (cid:63)(cid:107)2 Proof. Suppose Llbc( (cid:98)φ) ≤ ε2, so there exists (cid:99)M (s.t. (cid:107) (cid:99)M (cid:107)2 ≤ (cid:107)M (cid:63)(cid:107)2 < 1) and (cid:98)ρ ∈ BW s.t. (cid:20) (cid:21) (cid:99)M (cid:98)ρ(cid:124) (cid:20)γEs(cid:48)∼P (s,a)φ(s(cid:48), πe) r(s, a) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 φ(s, a) − Es,a∼ν ≤ ε2 (cid:13) (cid:13) (cid:13) (cid:13) For any w1 ∈ BW , we can take w2 = (cid:98)ρ + (cid:99)M (cid:124)w1. Then, (cid:107)w2(cid:107)2 ≤ (cid:107)(cid:98)ρ(cid:107)2 + (cid:107) (cid:99)M (cid:107)2W ≤ (cid:107)ρ(cid:63)(cid:107)2 + (cid:107)M (cid:63)(cid:107)2W ≤ W by our assumption on W . Hence, max w1∈BW ≤ max w1∈BW (cid:124) (cid:13) (cid:13)w 2 φ(s, a) − r(s, a) − γEs(cid:48)∼P (s,a) [w min w2∈BW (cid:13) (cid:13)((cid:98)ρ + (cid:99)M (cid:124)w1)(cid:124)φ(s, a) − r(s, a) − γEs(cid:48)∼P (s,a) [w (cid:13) (cid:115) (cid:124) 1 φ(s(cid:48), πe)](cid:13) (cid:13)ν (cid:13) (cid:124) 1 φ(s(cid:48), πe)] (cid:13) (cid:13)ν ((cid:98)ρ + (cid:99)M (cid:124)w1)(cid:124)φ(s, a) − r(s, a) − γEs(cid:48)∼P (s,a) [w (cid:115) (cid:20)(cid:16) (cid:124) ((cid:98)ρ(cid:124)φ(s, a) − r(s, a))2(cid:105) + Es,a∼ν w 1 ( (cid:99)M φ(s, a) − γEs(cid:48)∼P (s,a) [φ(s(cid:48), πe)]) (cid:17)2(cid:21) (cid:17)2(cid:21) (cid:124) 1 φ(s(cid:48), πe)] = max w1∈BW Es,a∼ν (cid:20)(cid:16) (cid:114) Es,a∼ν (cid:104) ≤ max w1∈BW ≤ ε(1 + W ), as desired. E. Proofs for Representation Learning To simplify analysis, assume that the functions in G have bounded (cid:96)2 norm, i.e. ∀g ∈ G, s, a ∈ S × A, (cid:107)g(s, a)(cid:107)2 ≤ γ. This is reasonable, and can always be achieved by clipping without loss of accuracy, since the target for g(s, a) is γEs(cid:48)∼P (s,a) [φ(s(cid:48), πe)] and (cid:107)φ(s, a)(cid:107)2 ≤ 1 for any s, a ∈ S × A. E.1. Lemmas Recall that λk(A) denotes the k-th largest eigenvalue of a matrix A, i.e. λ1(A), λn(A) give the largest and smallest eigenvalues respectively. Lemma E.1 (Weyl’s Perturbation Theorem). Let A, B ∈ Cn×n be Hermitian matrices. Then max k |λk(A) − λk(B)| ≤ (cid:107)A − B(cid:107)2 . Proof. Please see (Bhatia, 2013, Corollary III.2.6). Learning Bellman Complete Representations for Ofﬂine Policy Evaluation We extend this to be uniform over all Φ. Lemma E.2 (Uniform spectrum concentration). For any δ ∈ (0, 1), w.p. 1 − δ, sup φ∈Φ,k∈[d] (cid:12) (cid:12) (cid:12)λk(Σ(φ)) − λk((cid:98)Σ(φ)) (cid:12) (cid:12) (cid:12) ≤ 1 √ N (cid:16) (cid:17) 96κ(Φ) + 4d + 4 log1/2(1/δ) Proof. First observe that, sup φ∈Φ sup k (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤ sup (cid:12)λk(Σ(φ)) − λk((cid:98)Σ(φ)) φ∈Φ (cid:13) (cid:13) (cid:13)Σ(φ) − (cid:98)Σ(φ) (cid:13) (cid:13) (cid:13)2 x(cid:124)(Σ(φ) − (cid:98)Σ(φ))x (Eν − ED)(x(cid:124)φ(s, a))2 = = sup φ∈Φ,(cid:107)x(cid:107)2≤1 sup φ∈Φ,(cid:107)x(cid:107)2≤1 Now we want to bound the Rademacher complexity of the class F = (cid:8)(s, a) (cid:55)→ (x(cid:124)φ(s, a))2, φ ∈ Φ, (cid:107)x(cid:107)2 ≤ 1(cid:9) First, to bound the envelope, we have (x(cid:124)φ(s, a))2 ≤ 1. To cover, consider any φ ∈ Φ and x ∈ Rd s.t. (cid:107)x(cid:107)2 ≤ 1. Pick (cid:101)φ, (cid:101)x close to φ, x, so that (cid:118) (cid:117) (cid:117) (cid:116) 1 N N (cid:88) (cid:16) i=1 (x(cid:124)φ(si, ai))2 − ((cid:101)x(cid:124) (cid:101)φ(si, ai))2 (cid:17)2 (cid:118) (cid:117) (cid:117) (cid:116) ≤ 2 1 N N (cid:88) (cid:16) i=1 (x(cid:124)φ(si, ai) − (cid:101)x(cid:124) (cid:101)φ(si, ai)) (cid:17)2  (cid:118) (cid:117) (cid:117) (cid:116) ≤ 2  1 N N (cid:88) (cid:16) i=1 x(cid:124)(φ(si, ai) − (cid:101)φ(si, ai)) (cid:17)2 + (cid:118) (cid:117) (cid:117) (cid:116) 1 N N (cid:88) (cid:16) i=1 (x − (cid:101)x)(cid:124) (cid:101)φ(si, ai) (cid:17)2   (cid:16) ≤ 2 dΦ(φ, (cid:101)φ) + (cid:107)x − (cid:101)x(cid:107)2 (cid:17) So it sufﬁces to take dΦ(φ, (cid:101)φ), (cid:107)x − (cid:101)x(cid:107)2 ≤ t/4 to t-cover F in L2(D). Note the t/4-covering number for x in the unit ball is (1 + 8/t)d. Thus, by Dudley’s entropy bound ((5.48) of (Wainwright, 2019)), RN (F) ≤ ≤ 24 √ N 96 √ N (cid:90) 1 log1/2(N (t/4, Φ) · (1 + 8/t)d)dt 0 (cid:16) κ(Φ) + 4 √ (cid:17) d Thus, by Theorem 4.10 of (Wainwright, 2019), w.p. 1 − δ, sup φ∈Φ,(cid:107)x(cid:107)2≤1 (Eν − ED)(x(cid:124)φ(s, a))2 ≤ 2RN (F) + 4 log1/2(1/δ) √ N √ ≤ (cid:16) 1 √ N 96κ(Φ) + 4 d + 4 log1/2(1/δ) (cid:17) We now prove the double sampling lemma, i.e. modiﬁed Bellman Residual Minimization (Chen & Jiang, 2019). This will help deal with the double sampling issue when transitions are stochastic. Recall that G is a function class of functions g : X (cid:55)→ Rd. Let ν be a distribution over X ⊂ Rd and, for any x ∈ X , let P (x) be a distribution over Y ⊂ Rd. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Lemma E.3 (Double Sampling). Suppose x (cid:55)→ Ey∼P (x) [y] ∈ G. Then, Ex∼ν (cid:104)(cid:13) (cid:13)x − Ey∼P (x) [y](cid:13) 2 (cid:13) 2 (cid:105) = Ex∼ν,y∼P (x) (cid:104) (cid:107)x − y(cid:107)2 2 (cid:105) − inf g∈G Ex∼ν,y∼P (x) (cid:104) (cid:107)g(x) − y(cid:107)2 2 (cid:105) Proof. Ex∼ν,y∼P (x) (cid:104) (cid:105) − Ex∼ν (cid:107)x − y(cid:107)2 2 (cid:104) (cid:105) (cid:104)(cid:13) (cid:13)x − Ey∼P (x) [y](cid:13) 2 (cid:13) 2 (cid:105) 2 − 2 (cid:10)x, Ey∼P (x) [y](cid:11) + (cid:13) (cid:107)x(cid:107)2 (cid:16) 2 (cid:13)Ey∼P (x) [y](cid:13) 2 (cid:13) 2 (cid:17)(cid:105) (cid:104) (cid:104) (cid:104) = Ex∼ν = Ex∼ν = Ex∼ν Ey∼P (x) Ey∼P (x) Ey∼P (x) (cid:104) (cid:107)x(cid:107)2 2 − 2 (cid:104)x, y(cid:105) + (cid:107)y(cid:107)2 (cid:105) (cid:13)Ey∼P (x) [y](cid:13) 2 (cid:13) 2 (cid:105)(cid:105) − (cid:13) (cid:107)y(cid:107)2 2 (cid:104)(cid:13) (cid:13)y − Ey∼P (x) [y](cid:13) 2 (cid:13) 2 − (cid:105) where the last step uses the fact that (cid:13) observe that, assuming g(cid:63)(x) (cid:55)→ Ey∼P (x) [y] ∈ G, we have that it is the minimizer of, (cid:13)Ey∼P (x) [y](cid:13) 2 2 = Ey∼P (x) (cid:13) (cid:2)(cid:10)y, Ey∼P (x) [y](cid:11)(cid:3), and completing the square. Now, g(cid:63) ∈ arg min g∈G Ex∼ν,y∼P (x) (cid:104) (cid:107)g(x) − y(cid:107)2 2 (cid:105) (11) which completes the proof. E.2. Concentration lemmas For any φ, deﬁne the optimal ρ, M, g for our losses as follows: ρφ ∈ arg min (ρ,_)∈Θ Eν (cid:2)(ρ(cid:124)φ(s, a) − r(s, a))2(cid:3) Mφ ∈ arg min (_,M )∈Θ Eν◦P (cid:2)(cid:107)M φ(s, a) − γφ(s(cid:48), πe)(cid:107)2 2 (cid:3) gφ ∈ arg min g∈G Eν◦P (cid:2)(cid:107)g(s, a) − γφ(s(cid:48), π)(cid:107)2 2 (cid:3) Similarly, deﬁne (cid:98)ρφ, (cid:99)Mφ, (cid:98)gφ to be minimizers of the above losses when expectation is taken over the empirical distribution D, instead of the population distribution ν. Observe that the unconstrained minimization yields a closed form solution for gφ as gφ(s, a) = γEs(cid:48)∼P (s,a) [φ(s(cid:48), π)] – Assumption 4.3 posits that G is rich enough to capture this. The key property of our squared losses is that the second moment can be upper bounded by the expectation, which allows us to invoke the second part of the above Lemma B.1. We now combine this with covering to get uniform convergence results. Lemma E.4. For any δ ∈ (0, 1), w.p. at least 1 − δ, for any φ ∈ Φ, ρ ∈ BW and M ∈ Rd×d with (cid:107)M (cid:107)2 ≤ 1, we have (cid:2)(ρ(cid:124)φ(s, a) − r(s, a))2(cid:3) − Eν (cid:2)(cid:107)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2 (cid:2)(ρ(cid:124)φ(s, a) − r(s, a))2(cid:3)(cid:12) 2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 1 2 Eν (cid:12) ≤ (cid:3) − Eν◦P (cid:2)(ρ(cid:124)φ(s, a) − r(s, a))2(cid:3) + ερ, (cid:2)(cid:107)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 (cid:3)(cid:12) (cid:12) Eν◦P (cid:2)(cid:107)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 (cid:3) + εM , (cid:12) (cid:12)ED (cid:12) (cid:12)ED 1 2 ≤ and, assuming realizability (Assumption 4.3), for every g ∈ G, we have (cid:12) (cid:12)ED 1 2 ≤ (cid:2)(cid:107)g(s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 2 − (cid:107)gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:3) − Eν (cid:2)(cid:107)g(s, a) − gφ(cid:63) (s, a)(cid:107)2 2 (cid:3)(cid:12) (cid:12) Eν (cid:2)(cid:107)g(s, a) − gφ(cid:63) (s, a)(cid:107)2 2 (cid:3) + εg, where ερ, εM , ρg are deﬁned below. Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Finite function classes Assuming Φ and G are ﬁnite, we have ερ ≤ 6d(1 + W )2 log(4W |Φ| N/δ) N εM ≤ 32d2 log(4 |Φ| N/δ) N εg ≤ 20γ2 log(2 |G| /δ) N . Proof for ερ. For a ﬁxed φ ∈ Φ, ρ ∈ BW , apply Lemma B.1 to Xi = (ρ(cid:124)φ(si, ai) − r(si, ai))2. The envelope is (cid:3) ≤ (1 + W )2E [Xi]. So, the error from the lemma is |Xi| ≤ (1 + W )2 and the second moment is bounded E (cid:2)X 2 (cid:12)(ρ(cid:124)φ(s, a) − r(s, a))2 − ((cid:101)ρ(cid:124)φ(s, a) − r(s, a))2(cid:12) 2(1+W )2 log(2/δ) (cid:12) = N |(ρ − (cid:101)ρ)(cid:124)φ(s, a)((ρ + (cid:101)ρ)(cid:124)φ(s, a) + 2r(s, a))| ≤ 2(1 + W )(cid:107)(cid:101)ρ − ρ(cid:107)2, we consider a 1 N -net of BW which requires (1 + 2W N )d points. The error from this ε-net approximation is at most 4(1+W ) . Now union bound over an ε-net of BW . Since (cid:12) . Finally, union bound over Φ. i N Proof for εM . For a ﬁxed φ ∈ Φ and M ∈ Rd×d s.t. (cid:107)M (cid:107)2 < 1, apply Lemma B.1 to Xi = (cid:107)a(cid:107)2 a = M φ(si, ai)−γφ(s(cid:48) Further, observe that (cid:107)a(cid:107)2 E (cid:2)(cid:107)a + b(cid:107)2 2 − (cid:107)b(cid:107)2 2 where 2 ≤ (1+γ)2+(2γ)2 ≤ 8. (cid:3) ≤ (cid:3) ≤ 16E [Xi], where we used Lemma E.3 to give us 2 = (cid:104)a + b, a − b(cid:105) ≤ (cid:107)a + b(cid:107)2(cid:107)a − b(cid:107)2. So, the second moment is bounded E (cid:2)X 2 (cid:3) ≤ (1 + 3γ)2E (cid:2)(cid:107)M φ(si, ai) − γgφ(si, ai)(cid:107)2 i, π). The envelope is |Xi| = |Xi| ≤ (cid:107)a(cid:107)2 i, π), b = gφ(si, ai)−γφ(s(cid:48) 2 − (cid:107)b(cid:107)2 2+(cid:107)b(cid:107)2 2(cid:107)a − b(cid:107)2 2 2 i E (cid:2)(cid:107)M φ(si, ai) − γgφ(si, ai)(cid:107)2 2 (cid:3) = E (cid:2)(cid:107)M φ(si, ai) − γφ(s(cid:48) i, π)(cid:107)2 2 − (cid:107)gφ(si, ai) − γφ(s(cid:48) i, π)(cid:107)2 2 (cid:3) . So, the error from the lemma is 24 log(2/δ) N . Now union bound over an ε-net of (cid:8)M ∈ Rd×d : (cid:107)M (cid:107)2 ≤ 1(cid:9). Observe that (cid:12) (cid:0)(cid:107)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2 (cid:12) (cid:12) 2 (cid:13) (cid:13)M φ(si, ai) + (cid:102)M φ(si, ai) − 2γφ(s(cid:48) (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:13) (cid:13) (cid:13)(M − (cid:102)M )φ(si, ai) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)(M − (cid:102)M ) (cid:13)2 (cid:13) (cid:13) (cid:13)(M − (cid:102)M ) · (2 + 2γ) ≤ 4 (cid:13) (cid:13) (cid:13)2 (cid:1) − ≤ ≤ (cid:16) . (cid:13) (cid:13) i, π) (cid:13)2 (cid:107) (cid:102)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 (cid:17)(cid:12) (cid:12) (cid:12) N -net (under (cid:107)(cid:107)F ) for {M ∈ Rd×d : (cid:107)M (cid:107)F ≤ Consider a 1 like the (cid:96)2 for a d2-dimensional vector. This is a 1 (cid:107)M (cid:107)2 ≤ (cid:107)M (cid:107)F ≤ √ d(cid:107)M (cid:107)2. The error from this ε-net approximation is at most 8 points since it is N -net (under (cid:107)(cid:107)2) for the subset (cid:8)M ∈ Rd×d : (cid:107)M (cid:107)2 ≤ 1(cid:9) since d}, which requires (1 + 2N N . Finally, union bound over Φ. d)d2 √ √ Proof for εg. For a ﬁxed g ∈ G, apply Lemma B.1 to Xi = (cid:107)g(si, ai) − γφ(cid:63)(s(cid:48) excess regression loss. Under realizability Assumption 4.3, we have E [Xi] = (cid:107)g − gφ(cid:63) (cid:107)2 i, π)(cid:107)2 2, since 2 − (cid:107)gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 2, the E [Xi] = E = E (cid:104) (cid:107)g(s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 (cid:104) (cid:107)g(s, a) − gφ(cid:63) (s, a)(cid:107)2 2 − (cid:107)gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:105) 2 + 2(cid:104)gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π), g(s, a) − gφ(cid:63) (s, a)(cid:105) (cid:105) By deﬁnition of gφ(cid:63) , we have Es(cid:48)∼P (s,a) [gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π)] = 0, so, = E (cid:104) (cid:107)g(s, a) − gφ(cid:63) (s, a)(cid:107)2 2 (cid:105) is envelope |Xi| The E (cid:2)(cid:107)g(s, a) + gφ(cid:63) (s, a) − 2γEa(cid:48)∼π(s(cid:48)) [φ(cid:63)(s(cid:48), a(cid:48))] (cid:107)2 lemma is 20γ2 log(2/δ) . Now union bound over G. ≤ (2γ)2 N the and 2(cid:107)g(s, a) − gφ(cid:63) (s, a)(cid:107)2 2 second moment ≤ bounded (cid:3) ≤ (4γ)2E [Xi]. So the error term from the is i E (cid:2)X 2 (cid:3) Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Inﬁnite function classes When Φ and G are inﬁnite, we need to assume some metric entropy conditions. Then in the ﬁnal step of the ﬁnite-class proofs above, we union bound on a well-chosen ε-net and collect an additional approximation error which is on the order of O(1/N ). Assumption E.5. For F ∈ {Φ, G}, we assume there exists p ∈ R++ such that N (t, F) (cid:46) t−p, where the net is under the following distances, dΦ(φ, (cid:101)φ) =∆ ED + ED (cid:105) + Eν (cid:105) (cid:13) (cid:104)(cid:13) (cid:13) (cid:13) (cid:13)φ(s, a) − (cid:101)φ(s, a) (cid:13)2 (cid:13) (cid:104)(cid:13) + Es,a∼D,s(cid:48)∼P (s,a) (cid:13)φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π) (cid:13) (cid:13) (cid:13)2 (cid:13) (cid:104)(cid:13) (cid:13)φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π) (cid:13) (cid:13) (cid:13)2 (cid:114) (cid:105) (cid:105) (cid:104)(cid:13) (cid:13) (cid:13)φ(s, a) − (cid:101)φ(s, a) (cid:104)(cid:13) (cid:13)φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π) (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:107)g(si, ai) − (cid:101)g(si, ai)(cid:107)2 2 (cid:105) + (cid:104) Eν (cid:107)g(s, a) − (cid:101)g(s, a)(cid:107)2 2 (cid:105) (cid:105) (cid:13) (cid:13) (cid:13)2 + Es,a∼ν,s(cid:48)∼P (s,a) (cid:114) dG(g, (cid:101)g) =∆ (cid:104) ED Note that this assumption is automatically satisﬁed for any p > 0 by VC classes (van der Vaart & Wellner, 1996, Theorem 2.6.4). Under this assumption, we have N ερ (cid:46) d(1 + W )2(1 + p) log(W N/δ) εM (cid:46) d2(1 + p) log(N/δ) N εg (cid:46) γ2p log(N/δ) . N Proof for ερ. From before, we showed for a ﬁxed φ ∈ Φ, we have ερ (cid:46) d(1+W )2 log(W N/δ) . Observe that (cid:12) (cid:12) (cid:101)φ(s, a) − r(s, a))2(cid:12) (cid:12) (cid:12)ρ(cid:124)(φ(s, a) − (cid:101)φ(s, a))(ρ(cid:124)(φ(s, a) + (cid:101)φ(s, a)) + 2r(s, a)) (cid:12)(ρ(cid:124)φ(s, a) − r(s, a))2 − (ρ(cid:124) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤ 2W (1+ (cid:12) = W )(cid:107)φ(s, a) − (cid:101)φ(s, a)(cid:107)2, and so the difference of the loss with φ and the loss with (cid:101)φ is bounded by 2W (1 + W )dΦ(φ, (cid:101)φ). Now union bound over a 1 N -net, which requires O(N p) points by Assumption E.5. The approximation error using the net is 2W (1+W ) N N . Proof for εM . From before, we showed for a ﬁxed φ ∈ Φ, we have εM (cid:46) d2 log(N/δ) N . Observe that (cid:12) (cid:0)(cid:107)M φ(s, a) − γφ(s(cid:48), π)(cid:107)2 (cid:12) (cid:12) ≤ (cid:107)M (φ(s, a) − (cid:101)φ(s, a)) − γ(φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π))(cid:107)(cid:107)M (φ(s, a) + (cid:101)φ(s, a)) − γ(φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π))(cid:107) + (cid:107)gφ(s, a) − g 2 − (cid:107)gφ(s, a) − γφ(s(cid:48), π)(cid:107)2 2 (cid:107)M (cid:101)φ(s, a) − γ (cid:101)φ(s(cid:48), π)(cid:107)2 (cid:101)φ(s, a) − γ(φ(s(cid:48), π) + (cid:101)φ(s(cid:48), π))(cid:107) 2 − (cid:107)g (cid:1) − (cid:16) (cid:101)φ(s, a) − γ (cid:101)φ(s(cid:48), π)(cid:107)2 2 (cid:17)(cid:12) (cid:12) (cid:12) (cid:17) (cid:107)φ(s, a) − (cid:101)φ(s, a)(cid:107) + γ(cid:107)φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π)(cid:107) (cid:101)φ(s, a) − γ(φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π))(cid:107)(cid:107)gφ(s, a) + g · (2 + 2γ) (cid:17) (cid:101)φ(s, a)(cid:107) + γ(cid:107)φ(s(cid:48), π) − (cid:101)φ(s(cid:48), π)(cid:107) (cid:107)gφ(s, a) − g · (2 + 2γ) (cid:16) (cid:16) ≤ + Using closed form solution for gφ, ≤ 16dΦ(φ, (cid:101)φ). Now union bound over a 1 at most 16 N . N -net, which requires O(N p) points by Assumption E.5. The approximation error using the net is Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Proof for εg. From before, we showed for a ﬁxed g ∈ G, we have εg (cid:46) γ2 log(1/δ) N . Observe that (cid:12)(cid:107)g(s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 − (cid:107)(cid:101)g(s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2(cid:12) (cid:12) (cid:12) ≤ (cid:107)g(s, a) − (cid:101)g(s, a)(cid:107)(cid:107)g(s, a) + (cid:101)g(s, a) − 2γφ(cid:63)(s(cid:48), π)(cid:107) ≤ (2 + 2γ)dG(g, (cid:101)g). Now union bound over a 1 at most 4 N . N -net, which requires O(N p) points by Assumption E.5. The approximation error using the net is E.3. Main Results Lemma E.6. Suppose φ(cid:63) ∈ Φ is Linear BC. Suppose Assumption 4.3 if transitions are stochastic. Moreover, suppose φ(cid:63) is feasible in the bilevel optimization (and so (cid:98)Llbc( (cid:98)φ) ≤ (cid:98)Llbc(φ(cid:63))). Then, w.p. 1 − 5δ, Llbc( (cid:98)φ) ≤ 24d(1 + W )2 log(4W |Φ|N/δ) + 128d2 log(4|Φ|N/δ) + 40γ2 log(2|G|/δ) N Proof. Llbc( (cid:98)φ) (cid:104) = Eν Since ρ ≤ Eν + Eν◦P (cid:124) (ρ (cid:98)φ (cid:98)φ(s, a) − r(s, a))2(cid:105) (cid:98)φ are minimizers under ν, (cid:98)φ (cid:98)φ(s, a) − r(s, a))2(cid:105) ((cid:98)ρ (cid:98)φ, M (cid:104) (cid:124) + Eν◦P (cid:104) (cid:107)M (cid:98)φ (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:104) (cid:107) (cid:99)M (cid:98)φ (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:105) (cid:105) − Eν◦P − Eν◦P By the ρ, M parts of Lemma E.4, ≤ 2ED (cid:104) (cid:124) (cid:98)φ (cid:98)φ(s, a) − r(s, a))2(cid:105) ((cid:98)ρ + 2ED Since (cid:98)g (cid:98)φ minimizes under D, ≤ 2ED (cid:104) (cid:124) (cid:98)φ (cid:98)φ(s, a) − r(s, a))2(cid:105) ((cid:98)ρ + 2ED By optimality of (cid:98)φ under (cid:98)Llbc, (cid:104) (cid:107) (cid:99)M (cid:98)φ (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:104) (cid:107) (cid:99)M (cid:98)φ (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:105) (cid:105) − 2ED − 2ED (cid:104) (cid:107)g (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:104) (cid:107)g (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:104) (cid:107)g (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:104) (cid:107)(cid:98)g (cid:98)φ(s, a) − γ (cid:98)φ(s(cid:48), π)(cid:107)2 2 (cid:105) (cid:105) (cid:105) (cid:105) + 2ερ + 2εM + 2ερ + 2εM ≤ 2ED (cid:104) (cid:124) φ(cid:63) φ(cid:63)(s, a) − r(s, a))2(cid:105) ((cid:98)ρ + 2ED (cid:104) (cid:107) (cid:99)Mφ(cid:63) φ(cid:63)(s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:105) − 2ED (cid:2)(cid:107)(cid:98)gφ(cid:63) (s, a) − γφ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:3) + 2ερ + 2εM By − 1 2 the G part (cid:2)(cid:107)gφ(cid:63) − (cid:98)gφ(cid:63) (cid:107)2 2 Eν have ED of Lemma E.4, we (cid:3) + εg ≤ εg. Then, using the optimality of (cid:98)ρ and (cid:99)M under D, (cid:3) − 2ED (cid:2)(cid:107)Mφ(cid:63) φ(cid:63)(s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2 + 2ED (cid:2)(cid:107)gφ(cid:63) (s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2 2 ≤ 2ED (cid:104) (ρ (cid:124) φ(cid:63) φ(cid:63)(s, a) − r(s, a))2(cid:105) (cid:2)(cid:107)gφ(cid:63) (s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:3) 2 − (cid:107)(cid:98)gφ(cid:63) (s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:3) ≤ + 2ερ + 2εM + 2εg By the ρ, M parts of Lemma E.4, ≤ 3Eν (cid:104) (ρ (cid:124) φ(cid:63) φ(cid:63)(s, a) − r(s, a))2(cid:105) + 3Eν◦P (cid:2)(cid:107)Mφ(cid:63) φ(cid:63)(s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:3) − 3Eν (cid:2)(cid:107)gφ(cid:63) (s, a) − φ(cid:63)(s(cid:48), π)(cid:107)2 2 (cid:3) + 4ερ + 4εM + 2εg Learning Bellman Complete Representations for Ofﬂine Policy Evaluation By Assumption 4.3 and Lemma E.3, = 3Llbc(φ(cid:63)) + 4ερ + 4εM + 2εg By assumption that φ(cid:63) is Linear BC and Proposition 4.2, = 4ερ + 4εM + 2εg. We now prove Theorem 4.4, in the general stochastic case. The deterministic transitions case is subsumed by ignoring the minimization over G, i.e. setting the complexity term of G to zero. Theorem 4.4. Assume Assumption 4.1 (and Assumption 4.3 if 96 log1/2(|Φ|)+4 . If N ≥ C 2 d+4 log1/2(1/δ) √ 2 , then for any δ ∈ (0, 1), w.p. at least 1 − δ, we have β/4 the system is stochastic). Let C2 := 1. (cid:98)φ satisﬁes (cid:98)ε-approximate Linear BC with (cid:98)ε ≤ 13d(1 + W )2 log1/2(4W |Φ|N/δ) √ N + 7γ(1 + W ) log1/2(2|G|/δ) √ N , 2. λmin(Σ( (cid:98)φ)) ≥ β/4. If transitions are deterministic, treat log(|G|) = 0. Proof of Theorem 4.4. First, by our assumption that w.p. at least 1 − δ, we have supφ∈Φ important consequences: (cid:12) (cid:12) (cid:12)λmin(Σ(φ)) − λmin((cid:98)Σ(φ)) N ≥ 4(96κ(Φ) + 4 d + 4 log1/2(1/δ))/β, Lemma E.2 implies that (cid:12) (cid:12) (cid:12) ≤ β/4. Under this high probability event, we have two √ √ 1. φ(cid:63) is feasible in Equation (6), since λmin((cid:98)Σ(φ(cid:63))) ≥ λmin(Σ(φ(cid:63))) − β/4 ≥ β(1 − 1/4) ≥ β/2. In particular, this means (cid:98)Llbc( (cid:98)φ) ≤ (cid:98)Llbc(φ(cid:63)). 2. The covariance of (cid:98)φ has lower-bounded eigenvalues, since λmin(Σ( (cid:98)φ)) ≥ λmin((cid:98)Σ( (cid:98)φ)) − β/4 ≥ β(1/2 − 1/4) ≥ β/4. Now, apply Lemma E.6 to bound Llbc( (cid:98)φ), so w.p. 1 − 5δ, Llbc( (cid:98)φ) ≤ 24d(1 + W )2 log(4W |Φ|N/δ) + 128d2 log(4|Φ|N/δ) + 40γ2 log(2|G|/δ) N By Lemma D.1, we have that (cid:98)φ is (cid:98)ε-approximately Linear BC, with parameter (cid:114) (cid:98)ε ≤ (1 + W ) · 24d(1 + W )2 log(4W |Φ|N/δ) + 128d2 log(4|Φ|N/δ) + 40γ2 log(2|G|/δ) N 7γ(1 + W ) log1/2(2|G|/δ) √ N 13d(1 + W )2 log1/2(4W |Φ|N/δ) √ N + ≤ Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Finally, we remark that the (cid:99)W for (cid:98)φ (in the approximately Linear BC case) is upper bounded by a polynomial in W (cid:63) in the assumed exact Linear BC of φ(cid:63). Consider our assumption that φ(cid:63) is exactly Linear BC with W (cid:63) = W (use (cid:63) to highlight that it is the W in the assumption, which we now show matches the W in the result). Then, by Proposition 4.2, ∃M (cid:63) with (cid:107)M (cid:63)(cid:107)2 ≤ W (cid:63)2 . Hence, it sufﬁces to minimize over this smaller ball for (cid:99)M , so that (cid:107) (cid:99)M (cid:107)2 ≤ (cid:107)M (cid:63)(cid:107)2. Now, take the smallest possible W in Lemma D.1, so that 1 − (cid:107)ρ(cid:63)(cid:107)2 (cid:113) 2 (cid:99)W = ≤ = ≤ (cid:107)ρ(cid:63)(cid:107)2 1 − (cid:107)M (cid:63)(cid:107)2 (cid:107)ρ(cid:63)(cid:107)2 (cid:113) 1 − (cid:107)ρ(cid:63)(cid:107)2 2 W (cid:63)2 (cid:113) 1 − (cid:107)ρ(cid:63)(cid:107)2 (1 + 1 − (cid:107)ρ(cid:63)(cid:107)2 W (cid:63)2 ) 2 (cid:107)ρ(cid:63)(cid:107)2 2 W (cid:63)2 2W (cid:63)2 (cid:107)ρ(cid:63)(cid:107)2 , which is a polynomial in W (cid:63). Our end-to-end result is deduced by chaining our LSPE theorem and the above theorem together. Theorem 4.5. Under Assumption 4.1 (and Assumption 4.3 if P (s, a) is stochastic). Let C2, (cid:98)ε be as deﬁned in Theorem 4.4. If N ≥ C 2 2 , then we have for any δ ∈ (0, 1/2), w.p. at least 1 − 2δ, for all distributions p0, (cid:12) (cid:12)V πe (cid:12) − Es∼p0 (cid:104) (cid:98)fK(s, πe) p0 960β−1/2(1 + W )d log(N )(cid:112)log(10/δ) (cid:105)(cid:12) (cid:12) (cid:12) ≤ γK/2 1 − γ + √ (1 − γ)2 N . + (cid:114)(cid:13) (cid:13) (cid:13) 4 ddπe p0 dν (cid:13) (cid:13) (cid:13)∞ (1 − γ)2 (cid:98)ε Proof of Theorem 4.5. We ﬁrst apply Theorem 4.4 to see that (cid:98)φ satisﬁes the two properties needed for LSPE. It is indeed approximately Linear BC, with (cid:98)ε speciﬁed in the theorem, and also has coverage (i.e. λmin(Σ( (cid:98)φ)) ≥ β/4). Using these two facts, and on a separate independent dataset D2 (needs to be a separate dataset since (cid:98)φ is data-dependent), we run LSPE and directly apply Theorem 3.3 for the result. F. Implementation Details Here we detail all environment speciﬁcations and hyperparameters used in the main text. F.1. Dataset Details Using the publicly released implementation for DrQ-v2, we trained high quality target policies and saved checkpoints for ofﬂine behavior datasets. We refer the readers to Yarats et al. (2021a) for exact hyperparameters. F.2. Environment Details Following the standards used by DrQ-v2 (Yarats et al., 2021a), all environments have a maximum horizon length of 500 timesteps. This is achieved by the behavior/target policy having an action repeat of 2 frames. Furthermore, each state is 3 stacked frames that are each 84 × 84 dimensional RGB images (thus 9 × 84 × 84). Learning Bellman Complete Representations for Ofﬂine Policy Evaluation Task Target Performance Behavior Performance Finger Turn Hard Cheetah Run Quadruped Walk Humanoid Stand 927 758 873 827 226 (24%) 192 (25%) 236 (27%) 277 (33%) Table 1. Performance for target and behavior policies used to collect evaluation and ofﬂine datasets respectively. Task Action Space Dimension Task Traits Reward Type Finger Turn Hard Cheetah Run Quadruped Walk Humanoid Stand 2 6 12 21 turn locomotion locomotion stand sparse dense dense dense Table 2. Task descriptions, action space dimension, and reward type for each tested environment. F.3. Representation Architecture and Hyperparameter Details We adopt the same network architecture as DrQ-v2’s critic, ﬁrst introduced in SAC-AE (Yarats et al., 2021b). More speciﬁcally, to process pixel input, we have a 5 layer ConvNet with 3 × 3 kernels and 32 channels with ReLU activations. The ﬁrst convolutional layer has a stride of 2 while the rest has stride 1. The output is fed through a single fully connected layer normalized by LayerNorm. Finally, there is a tanh nonlinearity on the outputted 50 dimensional state-representation. The action is then concatenated with this output and fed into a 4-layer MLP all with ReLU activations. Hyperparameter Value Feature Dimension Weight Initialization Optimizer Learning Rate Batch Size Training Epochs τ (target) λDesign 512 orthogonal init. Adam 1 × 10−5 2048 200 0.005 5 × 10−6 Table 3. Hyperparameters used for BCRL Learning Bellman Complete Representations for Ofﬂine Policy Evaluation F.4. Benchmarks and Metrics Modiﬁcations to CURL: Originally, CURL only does contrastive learning between the image states with data augmentation. For OPE, apply the same CURL objective to the state-action feature detailed in the previous section. Note we also train CURL with the same random cropping image augmentations presented by the authors. Finally, since we are not interleaving the representation learning with SAC, we do not have a Q prediction head. Modiﬁcations to SPR: We use the same image encoder as our features for SPR. The main difference is in the architecture of the projection layers where we implement as 3-layer mlp with ReLU activations. Note that these are additional parameters that neither CURL nor BCRL require. Finally, similarly to CURL, we do not have an additional Q-prediction head. Spearman Ranking Correlation Metric: This rank correlation measures the correlation between the ordinal rankings of the value estimates and the ground truth returns. As deﬁned in Fu et al. (2021), we have for policies 1, 2, . . . , N , true returns V1:N , and estimated returns ˆV1:N : Ranking Correlation = (cid:17) (cid:16) V1:N , ˆV1:N Cov σ (V1:N ) σ( ˆV1:N )"
338,An Empirical Evaluation of Four Off-the-Shelf Proprietary   Visual-Inertial Odometry Systems,"[{'href': 'http://arxiv.org/abs/2207.06780v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2207.06780v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-07-14 09:40:34,"2 2 0 2 l u J 7 ] G L . s c [ 2 v 9 9 0 2 0 . 7 0 2 2 : v i X r a Preprint An empirical study of implicit regularization in deep oﬄine RL Caglar Gulcehre∗, Srivatsan Srinivasan∗, Jakub Sygnowski, Georg Ostrovski, Mehrdad Farajtabar, Matt Hoﬀman, Razvan Pascanu, Arnaud Doucet DeepMind Abstract Deep neural networks are the most commonly used function approximators in oﬄine re- inforcement learning. Prior works have shown that neural nets trained with TD-learning and gradient descent can exhibit implicit regularization that can be characterized by under- parameterization of these networks. Speciﬁcally, the rank of the penultimate feature layer, also called eﬀective rank, has been observed to drastically collapse during the training. In turn, this collapse has been argued to reduce the model’s ability to further adapt in later stages of learning, leading to the diminished ﬁnal performance. Such an association between the eﬀective rank and performance makes eﬀective rank compelling for oﬄine RL, primarily for oﬄine policy evaluation. In this work, we conduct a careful empirical study on the relation between eﬀective rank and performance on three oﬄine RL datasets : bsuite, Atari, and DeepMind lab. We observe that a direct association exists only in restricted settings and disappears in the more extensive hyperparameter sweeps. Also, we empirically identify three phases of learning that explain the impact of implicit regularization on the learning dynamics and found that bootstrapping alone is insuﬃcient to explain the collapse of the eﬀective rank. Further, we show that several other factors could confound the relationship between eﬀective rank and performance and conclude that studying this association under simplistic assumptions could be highly misleading. Contents 1 Introduction 2 Background 2.1 Eﬀective rank and implicit under-regularization . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Auxiliary losses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Deep oﬄine RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Experimental setup 4 Eﬀective rank and performance 4.1 Lifespan of learning with deep Q-networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 5 5 6 6 7 8 9 4.2 The eﬀect of dataset size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 ∗Indicates joint ﬁrst authors. 1 Preprint 5 Interactions between rank and performance 6 The eﬀect of activation functions 7 Optimization 8 The eﬀect of loss function 8.1 Q-learning and behavior cloning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.2 CURL: The eﬀect of self-supervision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 Tandem RL 10 Oﬄine policy selection 11 Robustness to input perturbations 12 Discussion A Appendix A.1 The impact of depth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Spectral density of hessian for oﬄine RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 DeepMind lab: performance vs the eﬀective ranks . . . . . . . . . . . . . . . . . . . . . . . . . A.4 SAM optimizer: does smoother loss landscape aﬀect the behavior of the rank collapse? . . . . A.5 Eﬀective rank and the value error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.6 CURL on DeepMind Lab . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.7 Learning rate evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.8 Learning curves long training regime and the diﬀerent phases of learning . . . . . . . . . . . . A.9 Eﬀective rank and the performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.10 Interventions to eﬀective rank and randomized controlled trials (RCT) . . . . . . . . . . . . . A.11 Computation of feature ranks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.12 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.13 bsuite phase transitions and bottleneck capacity . . . . . . . . . . . . . . . . . . . . . . . . . A.14 Activation sparsity on bsuite . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 15 15 17 17 18 19 22 23 25 30 30 30 32 32 33 34 35 35 35 35 35 38 38 38 1 Introduction The use of deep networks as function approximators in reinforcement learning (RL), referred to as Deep Reinforcement Learning (DRL), has become the dominant paradigm in solving complex tasks. Until recently, most DRL literature focused on online-RL paradigm, where agents must interact with the environment to explore and learn. This led to remarkable results on Atari (Mnih et al., 2015), Go (Silver et al., 2017), StarCraft II (Vinyals et al., 2019), Dota 2 (Berner et al., 2019), and robotics (Andrychowicz et al., 2020). Unfortunately, the need to interact with the environment makes these algorithms unsuitable and unsafe for many real-world applications, where any action taken can have serious ethical or harmful consequences or be 2 Preprint costly. In contrast, in the oﬄine RL paradigm (Fu et al., 2020; Fujimoto et al., 2018; Gulcehre et al., 2020; Levine et al., 2020), also known as batch RL (Ernst et al., 2005; Lange et al., 2012), agents learn from a ﬁxed dataset previously logged by other (possibly unknown) agents. This ability makes oﬄine RL more applicable to the real world. Recently, Kumar et al. (2020a) showed that oﬄine RL methods coupled with TD learning losses could suﬀer from a eﬀective rank collapse of the penultimate layer’s activations, which renders the network to become under-parameterized. They further demonstrated a signiﬁcant fraction of Atari games, where the collapse of eﬀective rank collapse corresponded to performance degradation. Subsequently, Kumar et al. (2020a) explained the rank collapse phenomenon by analyzing a TD-learning loss with bootstrapped targets in the kernel and linear regression setups. In these simpliﬁed scenarios, bootstrapping leads to self-distillation, causing severe under-parametrization and poor performance, as also observed and analyzed by Mobahi et al. (2020). Nevertheless, Huh et al. (2021) studied the rank of the representations in a supervised learning setting (image classiﬁcation tasks) and argued that low rank leads to better performance. Thus the low-rank representations could act as an implicit regularizer. Figure 1: [Atari] The rank and the performance on broad vs narrow hyperparameter sweep: Correlation between eﬀective rank and agent’s performance towards the end of training in diﬀerent Atari games. We report the regression lines for the narrow sweep, which covers only a single oﬄine RL algorithm, with a small minibatch size (32) and a learning rate sweep similar to the hyperparameter sweep deﬁned in RL Unplugged (Gulcehre et al., 2020), whereas in the broad setting, we included more data from diﬀerent models and a larger hyperparameter sweep. In the narrow setup, there is a positive relationship between the eﬀective rank and the agent’s performance, but that relationship disappears in the broad data setup and almost reverses. Typically, in machine learning, we rely on empirical evidence to extrapolate the rules or behaviors of our learned system from the experimental data. Often those extrapolations are done based on a limited number of experiments due to constraints on computation and time. Unfortunately, while extremely useful, these extrapolations might not always generalize well across all settings. While Kumar et al. (2020a) do not concretely propose a causal link between the rank and performance of the system, one might be tempted to extrapolate the results (agents performing poorly when their rank collapsed) to the existence of such a causal 3 Preprint link, which herein we refer to as rank collapse hypothesis. In this work, we do a careful large-scale empirical analysis of this potential causal link using the oﬄine RL setting (also used by Kumar et al. (2020a)) and also the Tandem RL (Ostrovski et al., 2021) setting. The existence of this causal link would be beneﬁcial for oﬄine RL, as controlling the rank of the model could improve the performance (see the regularization term explored in Kumar et al. (2020a)) or as we investigate here, the eﬀective rank could be used for model selection in settings where oﬄine evaluation proves to be elusive. Key Observation 1: The rank and the performance are correlated in restricted settings, but that correlation disappears when we increase the range of hyperparameters and the models (Figure 1) a. aThis is because other factors like hyperparameters and architecture can confound the rank of the penultimate layer; unless the those factors are controlled carefully, the conclusions drawn from the experiments based on the rank can be misleading. Instead, we show that diﬀerent factors aﬀect a network’s rank without aﬀecting its performance. This ﬁnding indicates that unless all of these factors of variations are controlled – many of which we might still be unaware of – the rank alone might be a misleading indicator of performance. A deep Q network exhibits three phases during training. We show that rank can be used to identify diﬀerent stages of learning in Q-learning if the other factors are controlled carefully. We believe that our study, similar to others (e.g. Dinh et al., 2017), re-emphasizes the importance of critically judging our understanding of the behavior of neural networks based on simpliﬁed mathematical models or empirical evidence from a limited set of experiments. Key Observation 2: Deep Q-learning approaches goes through three phases of learning: i) simple behaviors, ii) complex behaviors, iii) under-parameterization (Figure 2). These phases can be identiﬁed by the eﬀective rank and performance on a given task a. aThe ﬁrst two phases of learning happen during the training of all models we tested. At times, the third phase of the learning could potentially lead to the agent losing its representation capacity and the ensuing poor performance. Figure 2: Lifespan of learning in deep Q-learning: The plot on the left-hand side illustrates the evolution of the eﬀective rank, and the plot on the right-hand side demonstrates the evolution of the performance during training. In the ﬁrst phase, the model learns easy-to-learn behaviors that are simplistic by nature and ignore many factors of environmental variations. The eﬀective rank collapses to a minimal value in the ﬁrst phase since the model does not need a large capacity to learn the simple behaviors. In phase 2, the model learns more complex behaviors that we identify as those that obtain large returns when the policy is evaluated in the environment. Typically, in supervised learning, phase 2 is followed by overﬁtting. However, in oﬄine RL (speciﬁcally the TD-learning approaches that we tried here), we observed that it is often followed by underﬁtting/under-parameterization in phase 3. 4 k n a R e v i t c e f f E s n r u t e R e d o s i p E Training Steps Training Steps Phases of Learning Phase 1: Simple Behaviors Phase 2: Complex Behaviors Phase 3: Underparametrization Preprint We organize the rest of the paper and our contributions in the order we introduce in the paper as follows: • Section 2 presents the related work, and Section 3 explains the experimental design that we rely on. • In Sections 4, 8, 6, 7 and 9, we study the extent of impact of diﬀerent interventions such as architectures, loss functions and optimization on the causal link between the rank and agent performance. Some interventions in the model, such as introducing an auxiliary loss (e.g. CURL (Laskin et al., 2020), SAM (Foret et al., 2021) or the activation function) can increase the eﬀective rank but does not necessarily improve the performance. This ﬁnding indicates that the rank of the penultimate layer is not enough to explain an agent’s performance. We also identify the settings where the rank strongly correlates with the performance, such as DQN with ReLU activation and many learning steps over a ﬁxed dataset. • In Section 4.1, we show that a deep Q network goes through three stages of learning, and those stages can be identiﬁed by using rank if the hyperparameters of the model are controlled carefully. • Section 5 describes the main outcomes of our investigations. Particularly, we analyze the impact of the interventions described earlier and provide counter-examples that help contradict the rank collapse hypothesis, establishing that the link between rank and performance can be aﬀected by several other confounding factors of variation. • In Section 11, we ablate and compare the robustness of BC and DQN models with respect to random perturbation introduced only when evaluating the agent in the environment. We found out that the oﬄine DQN agent is more robust than the behavior cloning agent which has higher eﬀective rank. • Section 12 presents the summary of our ﬁndings and its implications for oﬄine RL, along with potential future research directions. 2 Background 2.1 Eﬀective rank and implicit under-regularization The choice of architecture and optimizer can impose speciﬁc implicit biases that prefer certain solutions over others. The study of the impact of these implicit biases on the generalization of the neural networks is often referred to as implicit regularization. There is a plethora of literature studying diﬀerent sources of implicit regularization such as initialization of parameters (Glorot and Bengio, 2010; Li and Liang, 2018; He et al., 2015), architecture (Li et al., 2017; Huang et al., 2020), stochasticity (Keskar et al., 2016; Sagun et al., 2017), and optimization (Smith et al., 2021; Barrett and Dherin, 2020). The rank of the feature matrix of a neural network as a source of implicit regularization has been an active area of study, speciﬁcally in the context of supervised learning (Arora et al., 2019; Huh et al., 2021; Pennington and Worah, 2017; Sanyal et al., 2018; Daneshmand et al., 2020; Martin and Mahoney, 2021). In this work, we study the phenomenon of implicit regularization through the eﬀective rank of the last hidden layer of the network, which we formally deﬁne below. Deﬁnition: Eﬀective Rank Recently, Kumar et al. (2020a) studied the impact of the eﬀective rank on generalization in the general RL context. With a batch size of N and D units in the feature layer, they proposed the eﬀective rank formulation presented in Equation 1 for a feature matrix Φ ∈ RN ×D where N ≥ D that uses a threshold value of δ with the singular values σi(Φ) in descending order i.e. σ1(Φ) ≥ σ2(Φ) ≥ · · · . We provide the speciﬁc implementation of the eﬀective rank we used throughout this paper in Appendix A.11. eﬀective rankδ(Φ) = min k ( k : 5 Pk PD i=1 σi(Φ) j=1 σj(Φ) ) ≥ 1 − δ . (1) Preprint Terminology and Assumptions. Note that the threshold value δ throughout this work has been ﬁxed to 0.01 similar to Kumar et al. (2020a). Throughout this paper, we use the term eﬀective rank to describe the rank of the last hidden layer’s features only, unless stated otherwise. This choice is consistent with prior work (Kumar et al., 2020a) as the last layer acts as a representation bottleneck to the output layer. Kumar et al. (2020a) suggested that the eﬀective rank of the Deep RL models trained with TD-learning objectives collapses because of i) implicit regularization, happening due to repeated iterations over the dataset with gradient descent; ii) self-distillation eﬀect emerging due to bootstrapping losses. They supported their hypothesis with both theoretical analysis and empirical results. The theoretical analysis provided in their paper assume a simpliﬁed setting, with inﬁnitesimally small learning rates, batch gradient descent and linear networks, in line with most theory papers on the topic. 2.2 Auxiliary losses Additionally, we ablate the eﬀect of using an auxiliary loss on an agent’s performance and rank. Speciﬁcally, we chose the ""Contrastive Unsupervised Representations for Reinforcement Learning"" (CURL) (Laskin et al., 2020) loss that is designed for use in a contrastive self-supervised learning setting within standard RL algorithms: L(s, a, r, θ) = LQ(s, a, r, θ) + λ ∗ LCU RL(s, ˆs, θ). (2) Here, LQ refers to standard RL losses and the CURL loss LCU RL(s, ˆs, θ) is the contrastive loss between the features of the current observation s and a randomly augmented observation ˆs as described in Laskin et al. (2020). Besides this, we also ablate with Sharpness-Aware Minimization (SAM) (Foret et al., 2021), an approach that seeks parameters that have a uniformly low loss in its neighborhood, which leads to ﬂatter local minima. We chose SAM as a means to better understand whether the geometry of loss landscapes help inform the correlation between the eﬀective rank and agent performance in oﬄine RL algorithms. In our experiments, we focus on analyzing mostly the deep Q-learning (DQN) algorithm (Mnih et al., 2015) to simplify the experiments and facilitate deeper investigation into the rank collapse hypothesis. 2.3 Deep oﬄine RL Online RL requires interactions with an environment to learn using random exploration. However, online interactions with an environment can be unsafe and unethical in the real-world (Dulac-Arnold et al., 2019). Oﬄine RL methods do not suﬀer from this problem because they can leverage oﬄine data to learn policies that enable the application of RL in the real world (Menick et al., 2022; Shi et al., 2021; Konyushkova et al., 2021). Here, we focused on oﬄine RL due to its importance in real-world applications, and the previous works showed that the implicit regularization eﬀect is more pronounced in the oﬄine RL (Kumar et al., 2020a; 2021a). Some of the early examples of oﬄine RL algorithms are least-squares temporal diﬀerence methods (Bradtke and Barto, 1996; Lagoudakis and Parr, 2003) and ﬁtted Q-iteration (Ernst et al., 2005; Riedmiller, 2005). Recently, value-based approaches to oﬄine RL have been quite popular. Value-based approaches typically lower the value estimates for unseen state-action pairs, either through regularization (Kumar et al., 2020b) or uncertainty (Agarwal et al., 2020). One could also include R-BVE (Gulcehre et al., 2021) in this category, although it regularizes the Q function only on the rewarding transitions to prevent learning suboptimal policies. Similar to R-BVE, Mathieu et al. (2021) have also shown that the methods using single-step of policy improvement work well on tasks with very large action space and low state-action coverage. In this paper, due to their simplicity and popularity, we mainly study action value-based methods: oﬄine DQN (Agarwal et al., 2020) and oﬄine R2D2 (Gulcehre et al., 2021). Moreover, we also sparingly use Batched Constrained Deep Q-learning (BCQ) algorithm (Fujimoto et al., 2019), another popular oﬄine RL algorithm that uses the behavior policy to constrain the actions taken by the target network. Most oﬄine RL approaches we explained here rely on pessimistic value estimates (Jin et al., 2021; Xie et al., 2021; Gulcehre et al., 2021; Kumar et al., 2020b). Mainly because oﬄine RL datasets lack exhaustive exploration, and extrapolating the values to the states and actions not seen in the training set can result in 6 Preprint extrapolation errors which can be catastrophic with TD-learning (Fujimoto et al., 2018; Kumar et al., 2019). On the other hand, in online RL, it is a common practice to have inductive biases to keep optimistic value functions to encourage exploration (Machado et al., 2015). We also experiment with the tandem RL setting proposed by Ostrovski et al. (2021), which employs two independently initialized online (active) and oﬄine (passive) networks in a training loop where only the online agent explores and drives the data generation process. Both agents perform identical learning updates on the identical sequence of training batches in the same order. Tandem RL is a form of oﬄine RL. Still, unlike the traditional oﬄine RL setting on ﬁxed datasets, in the tandem RL, the behavior policy can change over time, which can make the learning non-stationary. We are interested in this setting because the agent does not necessarily reuse the same data repeatedly, which was pointed in Lyle et al. (2021) as a potential cause for the rank collapse. 3 Experimental setup To test and verify diﬀerent aspects of the rank collapse hypothesis and its potential impact on the agent performance, we ran a large number of experiments on bsuite (Osband et al., 2019), Atari (Bellemare et al., 2013) and DeepMind lab (Beattie et al., 2016) environments. In all these experiments, we use the experimental protocol, datasets and hyperparameters from Gulcehre et al. (2020) unless stated otherwise. We provide the details of architectures and their default hyperparameters in Appendix A.12. • bsuite – We run ablation experiments on bsuite in a fully oﬄine setting with the same oﬄine dataset as the one used in Gulcehre et al. (2021). We use a DQN agent (as a representative TD Learning algorithm) with multi-layer feed-forward networks to represent the value function. bsuite provides us with a small playground environment which lets us test certain hypotheses which are computationally prohibitive in other domains (for e.g.: computing Hessians) with respect to terminal features and an agent’s generalization performance. • Atari – To test whether some of our observations are also true with higher-dimensional input features such as images, we run experiments on the Atari dataset. Once again, we use an oﬄine DQN agent as a representative TD-learning algorithm with a convolutional network as a function approximator. On Atari, we conducted large-scale experiments on diﬀerent conﬁgurations: (a) Small-scale experiments: – DQN-256-2M: Oﬄine DQN on Atari with minibatch size 256 trained for 2M gradient steps with four diﬀerent learning rates: [3e − 5, 1e − 4, 3e − 4, 5e − 4]. We ran these experiments to observe if our observations hold in the default training scenario identiﬁed in RL Unplugged (Gulcehre et al., 2020). – DQN-32-100M: Oﬄine DQN on Atari with minibatch size of 32 trained for 100M gradient steps with three diﬀerent learning rates: (cid:2)3 × 10−5, 1 × 10−4, 3 × 10−4(cid:3). We ran those experiments to explore the eﬀects of reducing the minibatch size. (b) Large-scale experiments: – Long run learning rate sweep (DQN-256-20M): Oﬄine DQN trained for 20M gradients steps with 12 diﬀerent learning rates evenly spaced in log-space between 10−2 and 10−5 trained on minibatches of size 256. The purpose of these experiments is to explore the eﬀect of wide range of learning rates and longer training on the eﬀective rank. – Long run interventions (DQN-interventions): Oﬄine DQN trained for 20M gradient steps on minibatches of size 256 with 128 diﬀerent hyperparameter interventions on activation functions, dataset size, auxiliary losses etc. The purpose of these experiments is to understand the impact of such interventions on the eﬀective rank in the course of long training. • DeepMind Lab – While bsuite and Atari present relatively simple fully observable tasks that require no memory, DeepMind lab tasks (Gulcehre et al., 2021) are more complex, partially observable tasks where it is very diﬃcult to obtain good coverage in the dataset even after collecting billions of transitions. We speciﬁcally conduct our observational studies on the eﬀective ranks on the DeepMind Lab dataset, 7 Preprint which has data collected from a well-trained agent on the SeekAvoid level. The dataset is collected by adding diﬀerent levels of action exploration noise to a well-trained agent in order to get datasets with a larger coverage of the state-action space. Figure 3: Structural causal model (SCM) of diﬀerent factors that we test: M represents the model selection method that we use to determine the h which denotes the observed confounders that are chosen at the beginning of the training, including the task, the model architecture (including depth and number of units), learning rate and the number of gradient steps to train. β is the eﬀective rank of the penultimate layer. λ is the agent’s performance, measured as episodic returns the agent attains after evaluating in the environment. A represents the unobserved confounders that change during training but may aﬀect the performance, such as the number of dead units, the parameter norms, and the other underlying factors that can inﬂuence learning dynamics. We test the eﬀect of each factor by interventions. We illustrate the structural causal model (SCM) of the interactions between diﬀerent factors that we would like to test in Figure 3. To explore the relationship between the rank and the performance, we intervene on h, which represents potential exogenous sources of implicit regularization, such as architecture, dataset size, and the loss function, including the auxiliary losses. The interventions on h will result in a randomized controlled trial (RCT.) A represents the unobserved factors that might aﬀect performance denoted by λ an the eﬀective rank β such as activation norms and number of dead units. It is easy to justify the relationship between M, h, A and β. We argue that β is also confounded by A and h. We show the confounding eﬀect of A on β with our interventions to beta via auxiliary losses or architectural changes that increase the rank but do not aﬀect the performance. We aim to understand the nature of the relationship between these terms and whether SCM in the ﬁgure describes what we notice in our empirical exploration. We overload the term performance of the agent to refer to episodic returns attained by the agent when it is evaluated online in the environment. In stochastic environments and datasets with limited coverage, an oﬄine RL algorithm’s online evaluation performance and generalization abilities would correlate in most settings (Gulcehre et al., 2020; Kumar et al., 2021c). The oﬄine RL agents will need to generalize when they are evaluated in the environment because of: • Stochasticity in the initial conditions and transitions of the environment. For example, in the Atari case, the stochasticity arises from sticky actions and on DeepMind lab, it arises from the randomization of the initial positions of the lemons and apples. • Limited coverage: The coverage of the environment by the dataset is often limited. Thus, an agent is very likely to encounter states and actions that it has never seen during training. 4 Eﬀective rank and performance Based on the results of Kumar et al. (2020a), one might be tempted to extrapolate a positive causal link between the eﬀective rank of the last hidden layer and the agent’s performance measured as episodic returns 8 β: Eﬀective rank h: Observed confounders λ: Performance A: Hidden confounders M: Model selection method M h λ A β Preprint attained when evaluated in the environment. We explore this potentially interesting relationship on a larger scale by adopting a proof by contradiction approach. We evaluated the agents with the hyperparameter setup deﬁned for the Atari datasets in RL Unplugged (Gulcehre et al., 2020) and the hyperparameter sweep deﬁned for DeepMind Lab (Gulcehre et al., 2021). For a narrow set of hyperparameters this correlation exists as observed in Figure 1. However, in both cases, we notice that a broad hyperparameter sweep makes the correlation between performance and rank disappear (see Figures 1 and DeepMind lab Figure in Appendix A.3). In particular, we ﬁnd hyperparameter settings that lead to low (collapsed) ranks with high performance (on par with the best performance reported in the restricted hyperparameter range) and settings that lead to high ranks but poor performance. This shows that the correlation between eﬀective rank and performance can not be trusted for oﬄine policy selection. In the following sections, we further present speciﬁc ablations that help us understand the dependence of the eﬀective rank vs. performance correlation on speciﬁc hyperparameter interventions. 4.1 Lifespan of learning with deep Q-networks Empirically, we found eﬀective rank suﬃcient to identify three phases when training an oﬄine DQN agent with a ReLU activation function (Figure 2). Although the eﬀective rank may be suﬃcient to identify those stages, it still does not imply a direct causal link between the eﬀective rank and the performance as discussed in the following sections. Several other factors can confound eﬀective rank, making it less reliable as a guiding metric for oﬄine RL unless those confounders are carefully controlled. • Phase 1 (Simple behaviors): The eﬀective rank of the model ﬁrst collapses to a small value, in some cases to a single digit value, and then gradually starts to increase. In this phase, the model learns easy to learn behaviors that would have low performance when evaluated in the environment. We hypothesized that this could be due to the implicit bias of the SGD to learn functions of increasing complexity over training iterations (Kalimeris et al., 2019); therefore early in training, the network relies on simple behaviours that are myopic to most of the variation in the data. Hence the model has a low rank. The rank collapse early in the beginning of the training happens very abruptly, just in a handful of gradient updates the rank collapses to a single digit number. However, this early rank collapse does not degrade the performance of the agent. We call this rank collapse in the early in the training as self-pruning eﬀect. • Phase 2 (Complex behaviors): In this phase, the eﬀective rank of the model increases and then usually ﬂattens. The model starts to learn more complex behaviors that would achieve high returns when evaluated in the environment. • Phase 3 (Underﬁtting/Underparameterization): In this phase, the eﬀective rank collapses to a small value (often to 1) again, and the performance of the agent often collapses too. The third phase is called underﬁtting since the agent’s performance usually drops, and the eﬀective rank also collapses, which causes the agent to lose part of its capacity. This phase is not always observed (or the performance does not collapse with eﬀective ran towards the end of the training) in all settings as we demonstrate in our diﬀerent ablations. Typically in supervised learning, Phase 2 is followed by over-ﬁtting, but with oﬄine TD-learning, we could not ﬁnd any evidence of over-ﬁtting. We believe that this phase is primarily due to the target Q-network needing to extrapolate over the actions not seen during the training and causing extrapolation errors as described by (Kumar et al., 2019). A piece of evidence to support this hypothesis is presented in Figure 29 in Appendix A.5, which suggests that the eﬀective rank and the value error of the agent correlate well. In this phase, the low eﬀective rank and poor performance are caused by a large number of dead ReLU units. Shin and Karniadakis (2020) also show that as the network has an increasing number of dead units, it becomes under-parameterized and this could negatively inﬂuence the agent’s performance. It is possible to identify those three phases in many of the learning curves we provide in this paper, and our ﬁrst two phases agree with the works on SGD’s implicit bias of learning functions of increasing complexity (Kalimeris et al., 2019). Given a ﬁxed model and architecture, whether it is possible to observe all these three phases during training fundamentally depends on: 9 Preprint Figure 4: The three phases of learning: On IceHockey and MsPacman RL Unplugged Atari games we illustrate the diﬀerent phases of learning with the oﬄine DQN agent using the learning rate of 0.0004. The blue region in the plots identiﬁes the Phase 1, green region is the Phase 2, and red region is the Phase 3 of the learning. IceHockey is one of the games where the expert that generated the dataset performs quite poorly on it, thus the majority of the data is just random exploration data. It is easy to see that in this phase, the Oﬄine DQN performs very poorly and never manages to get out of the Phase 1. The performance of the agent on IceHockey is poor and the eﬀective rank of the network is low throughout the training. On MsPacman, we can observe all the three phases. The model transition into Phase 2 from Phase 1 quickly, and then followed by the under-ﬁtting regime where the eﬀective rank collapses the agent performs poorly. 1. Hyperparameters: The phases that an oﬄine RL algorithm would go through during training depends on hyperparameters such as learning rate and the early stopping or training budget. For example, due to early stopping the model may just stop in the second phase, if the learning rate is too small, since the parameters will move much slower the model may never get out of Phase 1. If the model is not large enough, it may never learn to transition from Phase 1 into Phase 2. 2. Data distribution: The data distribution has a very big inﬂuence on the phases the agent goes thorough, for example, if the dataset only contains random exploratory data and ORL method may fail to learn complex behaviors from that data and as a result, will never transition into Phase 2 from Phase 1. 3. Learning Paradigm: The learning algorithm, including optimizers and the loss function, can inﬂuence the phases the agent would go through during the training. For example, we observed that Phase 3 only happens with the oﬄine TD-learning approaches. It is possible to avoid phase three (underﬁtting) by ﬁnding the correct hyperparameters. We believe the third phase we observe might be due to non-stationarity in RL losses (Igl et al., 2020) due to bootstrapping or errors propagating through bootstrapped targets (Kumar et al., 2021b). The underﬁtting regime only appears if the network is trained long enough. The quality and the complexity of the data that an agent learns from also plays a key role in deciding which learning phases are observed during training. In Figure 4, we demonstrate the diﬀerent phases of learning on IceHockey and MsPacman games. On IceHockey, since the expert that generated the dataset has a poor performance on that game, the oﬄine DQN is stuck in Phase 1, and did not managed to learn complex behaviors that would push it to Phase 2 but on MsPacMan, all the three phases are present. We provide learning curves for all the online policy selection Atari games across twelve learning rates in Appendix A.8. 10 k n a R e v i t c e f f E 6 4 2 −2.5 −5 −7.5 −10 −12.5 n r u t e R e d o s p E i IceHockey MsPacman 60 40 20 0 5000 10000 IceHockey 15000 20000 0 0 Learner Steps (x1000) 2500 5000 10000 MsPacman 15000 20000 2000 1500 1000 500 0 0 5000 10000 15000 20000 0 5000 10000 15000 20000 Learner Steps (x1000) Phases of Learning Phase 1: Simple Behaviors Phase 2: Complex Behaviors Phase 3: Underparametrization Preprint Figure 5 shows the relationship between the eﬀective rank and twelve learning rates. In this ﬁgure, the eﬀect of learning rate on the diﬀerent phases of learning is distinguishable. For low learning rates, the ranks are low because the agent can never transition from Phase 1 to Phase 2, and for large learning rates, the eﬀective ranks are low because the agent is in Phase 3. Therefore, the distribution of eﬀective ranks and learning rates has a Gaussian-like shape, as depicted in the ﬁgure. The distribution of rank shifts towards low learning rates as we train the agents longer because slower models with low learning rates start entering Phase 2, and the models trained with large learning rates enter Phase 3. Figure 5: [Atari]: Bar charts of of eﬀective ranks with respect to the learning rates after 1M and 20M learning steps. After 1M gradient steps, the ranks are distributed almost like a Gaussian. After 20M learning steps the mode of the distribution of the ranks shifts towards left where the mode goes down for most games as well. Namely as we train the network longer the rank goes down and in particular for the large learning rates. For the low learning rates the rank is low because the model is stuck in Phase 1, the large learning rates get into the Phase 3 quickly and thus the low ranks. 4.2 The eﬀect of dataset size We can use the size of the dataset as a possible proxy metric for the coverage that the agent observes in the oﬄine data. We uniformly sampled diﬀerent proportions (from 5% of the transitions to the entire dataset) from the transitions in the RLUnplugged Atari benchmark dataset (Gulcehre et al., 2020) to understand how the agent behaves with diﬀerent amounts of training data and whether this is a factor aﬀecting the rank of the network. Figure 6 shows the evolution of rank and returns over the course of training. The eﬀective rank and performance collapse severely with low data proportions, such as when learning only on 5% of the entire 11 Preprint dataset subsampled. Those networks can never transition from phase 1 to phase 2. However, as the proportion of the dataset subsampled increases, the agents could learn more complex behaviors to get into phase 2. The eﬀective rank collapses less severely for the larger proportions of the dataset, and the agents tend to perform considerably better. In particular, we can see that in phase 1, an initial decrease of the rank correlates with an increase in performance, which we can speculate is due to the network reducing its reliance on spurious parts of the observations, leading to representations that generalize better across states. It is worth noting that the ordering of the policies obtained by using the agents’ performance does not correspond to the ordering of the policies with respect to the eﬀective rank throughout training. For example, oﬄine DQN trained on the full dataset performs better than 50% of the dataset, while the agent trained using 50% of the data sometimes has a higher rank. A similar observation can be made for Zaxxon, at the end of the training, the network trained on the full dataset underperforms compared to the one trained on 50% of the data, even if the rank is the same or higher. Figure 6: [Atari] Dataset size: Evolution of ranks and returns as we vary the fraction of data available for the agent to train on. We see that the agent which sees very little data collapses both in terms of rank and performance. The agent which sees more of the data has good performance even while allowing some shrinkage of rank during the training. 5 Interactions between rank and performance To understand the interactions and eﬀects of diﬀerent factors on the rank and performance of oﬄine DQN, we did several experiments and tested the eﬀect of diﬀerent hyperparameters on eﬀective rank and the agent’s performance. Figure 3 shows the causal graph of the eﬀective rank, its confounders, and the agent’s performance. Ideally, we would like to intervene in each node on this graph to measure its eﬀect. As the rank is a continuous random variable, it is not possible to directly intervene on eﬀective rank. Instead, we emulate interventions on the eﬀective rank by conditioning to threshold β with τ . In the control case (λ(1)), we assume β > τ and for λ(0), we would have β ≤ τ . We can write the average treatment eﬀect (ATE) for the eﬀect of setting the eﬀective rank to a large value on the performance as: ATE(λ, β, τ ) = E[λ|λ(1)] − E[λ|λ(0)]. (3) Let us note that this ATE(λ, β, τ ) quantity doesn’t necessarily measure the causal eﬀect of β on λ, since we know that the λ can be confounded by A. We study the impact of factors such as activation function, learning rate, data split, target update steps, and CURL and SAM losses on the Asterix and Gravitar levels. We chose these two games since Asterix is an 12 Preprint i) ReLU ii) tanh iii) both Figure 7: [Atari]: The correlation plot between the eﬀective rank and the performance (measured in terms of episode returns by evaluating the agent in the environment) of oﬄine DQN on Asterix and Gravitar games over 256 diﬀerent hyper-parameter conﬁgurations trained for 2M learning steps. There is a strong correlation with the ReLU function, but the correlation disappears for the network with tanh activation function. There is no signiﬁcant correlation between eﬀective rank and the performance on the Asterix game with the complete data. Still, a positive correlation exists on the Gravitar game. These results are not aﬀected by the Simpson’s paradox since the subgroups of the data when split into groups concerning activation functions, do not show a consistent correlation. easy-to-explore Atari game with relatively dense rewards, while Gravitar is a hard-to-explore and a sparse reward setting (Bellemare et al., 2016). In Table 1, we present the results of intervening to β thresholded with diﬀerent quantiles of the eﬀective rank. Choosing a network with a high eﬀective rank for a ReLU network has a statistically signiﬁcant positive eﬀect on the agent’s performance concerning diﬀerent quantiles on both Asterix and Gravitar. The agent’s performance is measured in terms of normalized scores as described in Gulcehre et al. (2020). However, the results with the tanh activation function are mixed, and the eﬀect of changing the rank does not have a statistically signiﬁcant impact on the performance in most cases. In Figure 7, we show the correlations between the rank and performance of the agent. The network with ReLU activation strongly correlates rank and performance, whereas tanh does not. The experimental data can be prone to Simpson’s paradox which implies that a trend (correlation in this case) may appear in several subgroups of the data, but it disappears or reverses when the groups are aggregated (Simpson, 1951). This can lead to misleading conclusions. We divided our data into equal-sized subgroups for hyperparameters and model variants, but we could not ﬁnd a consistent trend that disappeared when combined, as in Figure 7. Thus, our experiments do not appear to be aﬀected by Simpson’s paradox. 13 Preprint Table 1: Atari: The average treatment eﬀect of having a network with a higher rank than concerning diﬀerent quantiles. We report the average treatment eﬀect (ATE), its uncertainty (using 95% conﬁdence intervals using standard errors), and p-values for Asterix and Gravitar games. Higher eﬀective rank seems to have a smaller eﬀect on Asterix than Gravitar. Let us note that, Gravitar is a sparse reward problem, and Asterix is a dense-reward one. Overall the eﬀect is more prominent for ReLU than tanh, and with tanh networks, it is not statistically signiﬁcant. We boldfaced the ATEs where the eﬀect is statistically signiﬁcant (p < 0.05.) The type column of the table indicates the activation functions used in the experiments we did the intervention. The “Combined” type corresponds to a combination of tanh and ReLU experiments. Level Type Asterix Gravitar Combined ReLU Tanh Combined ReLU Tanh quantile=0.25 ATE 0.019 ± 0.010 0.079 ± 0.018 -0.014 ± 0.004 0.954 ± 0.077 1.654 ± 0.150 0.028 ± 0.090 quantile=0.5 quantile=0.75 quantile=0.95 p-value ATE 0.001 0.000 0.999 0.000 0.000 0.303 0.007 ± 0.013 0.089 ± 0.025 0.009 ± 0.005 0.569 ± 0.098 1.146 ± 0.163 0.185 ± 0.118 p-value ATE 0.207 0.000 0.996 0.000 0.000 0.005 0.005 ± 0.019 0.112 ± 0.040 -0.005 ± 0.006 0.496 ± 0.141 1.105 ± 0.256 0.242 ± 0.167 p-value ATE 0.324 0.000 0.886 0.000 0.000 0.009 -0.031 ± 0.011 0.110 ± 0.085 0.020 ± 0.017 0.412 ± 0.206 1.688 ± 0.564 0.054 ± 0.265 p-value 0.999 0.017 0.023 0.001 0.000 0.367 Table 2: [Atari] Average Treatment Eﬀect of diﬀerent interventions on Asterix and Gravitar: Quantity of interest Y is the terminal rank of the features. Activation function and learning rate have the most considerable eﬀect on the terminal feature ranks in our setup. We indicate them with boldface; changing the activation function from ReLU to tanh improves the eﬀective rank, whereas changing the learning rate from 3 × 10−4 to 3 × 10−5 reduces the rank. u Activation Function Learning Rate Target Update Steps SAM Loss Weight Level CURL Loss Weight Data Split Control (u) ReLU 3 × 10−4 2500 0 Asterix 0 100% Treatment T(u) tanh 3 × 10−5 200 0.05 Gravitar 0.001 5% Yt(u) − Yc(u) 66.97 ± 7.13 -54.15 ± 7.54 -15.23 ± 8.21 -10.05 ± 8.26 -9.41 ± 8.25 8.09 ± 8.27 5.11 ± 8.27 In this analysis, we set our control setting to be trained with ReLU activations, the learning rate of 3e − 4, without any auxiliary losses, with training done on the full dataset in the level Asterix. We present the Average Treatment Eﬀect (ATE) (the diﬀerence in ranks between the intervention being present and absent in an experiment) of changing each of these variables in Table 3. We ﬁnd that changing the activation function from ReLU to tanh drastically aﬀects the eﬀective ranks of the features, which is in sync with our earlier observations on other levels. In Figure 8, we present a heatmap of ATEs where we demonstrate the eﬀective rank change when two variables are changed from original control simultaneously. Once again, the activation functions and learning rate signiﬁcantly aﬀect the terminal ranks. We also observe some interesting combinations that lead the model to converge to a lower rank – for example, using SAM loss with dropout. These observations further reinforce our belief that diﬀerent factors aﬀect the phenomenon. Figure 8: [Atari] ablations: Eﬀects of diﬀerent pairs of interventions on the control set. The control sam- ple is on level Asterix with no dropout, curl and loss smoothing with the full dataset and a target update period of 2500 steps. Overall, changing the activation function and learning rate has the largest eﬀect on the rank. 14 Activation(tanh) 186.1 Learning Rate(3e-5) 94.9 -74.9 SAM Weight(0.05) 71.5 77.3 91.8 CURL Weight(0.001) 165.1 -78.8 -27.1 -50.1 Dropout Rate(0.5) 123.9 -67.0 109.3 -64.6 70.8 Dataset Size(100%) 138.3 -80.1 55.7 -47.1 99.3 -49.5 Target Update Steps(200) 70.2 -78.1 55.3 -70.5 57.5 -2.4 -40.3 Level Name(Gravitar) 30.3 -68.6 -64.0 46.7 -35.9 -61.5 -78.3 -59.7 150 100 50 0 −50 ) h n a t ( n o i t a v i t c A ) 5 - e 3 ( e t a R g n n r a e L i ) 5 0 . 0 ( t h g e W M A S i ) 1 0 0 . 0 ( t h g e W L R U C i ) 5 . 0 ( e t a R t u o p o r D ) % 0 0 1 ( e z S i t e s a t a D ) r a t i v a r G ( e m a N l e v e L ) 0 0 2 ( s p e t S e t a d p U t e g r a T Preprint The extent of rank collapse and mitigating rank collapse alone may never fully ﬁx the agent’s learning ability in diﬀerent environments. 6 The eﬀect of activation functions The severe rank collapse of phase 3 is apparent in our Atari models, which have simple convolutional neural networks with ReLU activation functions. When we study the same phenomenon on the SeekAvoid dataset, rank collapse does not seem to happen similarly. It is important to note here that to solve those tasks eﬀectively, the agent needs memory; hence, all networks have a recurrent core and LSTM. Since standard LSTMs use tanh(·) activations, investigating in this setting would help us understand the role of the choice of architecture on the behavior of the model’s rank. Figure 10 shows that the output features of the LSTM network on the DeepMind lab dataset do not experience any detrimental eﬀective rank collapse with diﬀerent exploration noise when we use tanh(·) activation function for the cell. However, if we replace the tanh(·) activation function of the LSTM cell with ReLU or if we replace the LSTM with a feed-forward MLP using ReLU activations (as seen in Figure 10) the eﬀective rank in both cases, collapses to a small value at the end of training. This behavior shows that the choice of activation function has a considerable eﬀect on whether the model’s rank collapses throughout training and, subsequently, its ability to learn expressive value functions and policies in the environment as it is susceptible to enter phase 3 of training. Observation: Agents that have networks with ReLU units tend to have dead units which causes the eﬀective rank to collapse in phase 3 of learning while other activations like tanh do not suﬀer a similar collapse. The activation functions inﬂuence both the network’s learning dynamics and performance. As noted by Pennington and Worah (2017), the activation function can inﬂuence the rank of each layer at initialization. Figure 9 presents our ﬁndings on bsuite levels. In general, the eﬀective rank of the penultimate layer with ReLU activations collapses faster, while ELU and tanh(·) tend to maintain a relatively higher rank than ReLU. As the eﬀective rank goes down for the catch environment, the activations become sparser, and the units die. We illustrate the sparsity of the activations with Gram matrices over the features of the last hidden layer of a feedforward network trained on bsuite in Figure 37 in Appendix A.14. Figure 9: [bsuite] Eﬀective ranks of diﬀerent activation functions: The magnitude of drop in eﬀective rank is more severe for ReLU and sigmoid activation functions than tanh. 7 Optimization The inﬂuence of minibatch size and the learning rate on the learning dynamics is a well-studied phenomenon. Smith and Le (2017); Smith et al. (2021) argued that the ratio between the minibatch size and learning rate relates to implicit regularization of SGD and also aﬀects the learning dynamics. Thus, it is evident that those factors would impact the eﬀective rank and performance. Here, we focus on a setup where we see the correlation between eﬀective rank and performance and investigate how it emerges. 15 Preprint Figure 10: [DeepMind lab SeekAvoid] Activation functions: Evolution of ranks in a typical LSTM network with tanh activations at the cell of LSTM on DeepMind lab. We see that a typical LSTM network does not get into Phase 3. However, when we change the activation function of the cell gate from tanh to ReLU the eﬀective rank collapses to very small values. The eﬀective rank collapses when the LSTM is replaced with a feedforward network too in cases of ReLU activation. Our analysis and ablations in Table 3 and Figure 8 illustrate that the learning rate is one of the most prominent factors on the performance of the agent. In general, there is no strong and consistent correlation between the eﬀective rank and the performance across diﬀerent models and hyperparameter settings. However, in Figure 1, we showed that the correlation exists in a speciﬁc setting with a particular minibatch size and learning rate. Further, in Figure 7, we narrowed down that the correlation between the eﬀective rank and the performance exists for the oﬄine DQN with ReLU activation functions. Thus in this section, we focus on a regime where this correlation exists with the oﬄine DQN with ReLU and investigate how the learning rate and minibatches aﬀect the rank. We ran several experiments to explore the relationship between the minibatch size, learning rates, and rank. In this section, we report results on Atari in few diﬀerent settings – Atari-DQN-256-2M, Atari-DQN-32-100M, Atari-DQN-256-20M and Atari-BC-256-2M. The dying ReLU problem is a well-studied issue in supervised learning (Glorot et al., 2011; Gulcehre et al., 2016) where due to the high learning rates or unstable learning dynamics, the ReLU units can get stuck in the zero regime of the ReLU activation function. We compute the number of dead ReLU units of the penultimate layer of a network as the number of units with zero activation value for all inputs. Increasing the learning rate increases the number of dead units as can be seen in Figures 11 and 12. Even in BC, we observed that using large learning rates can cause dead ReLU units, rank collapse, and poor performance, as can be seen in Figure 13, and hence this behavior is not unique to oﬄine RL losses. Nevertheless, models with TD-learning losses have catastrophic rank collapses and many dead units with lower learning rates than BC. Let us note that the eﬀective rank depends on the number of units (D) and the number of dead units (η) at a layer. It is easy to see that eﬀective rank is upper-bounded by D − η. In Figure 14, we observe a strong correlation between the number of dead ReLU units of the penultimate layer of the ReLU network and its eﬀective rank. Observation: The pace of learning inﬂuences the number of dead units and the eﬀective rank: the larger the learning rates and the smaller minibatches are, the number of dead units increases, and the eﬀective rank decreases. However, the performance is only poor when the eﬀective rank is severely low. Finally, we look into how eﬀective ranks shape up towards the end of training. We test 12 diﬀerent learning rates, trying to understand the interaction between the learning rates and the eﬀective rank of the representations. We summarize our main results in Figure 5 where training oﬄine DQN decreases the 16 Preprint Figure 11: [Atari DQN-32-100M] Learning curves of oﬄine DQN for 100M gradient steps of training with minibatch size of 32. Increasing the learning rate increases the number of dead units in the ReLU network. As a result, the increased learning rate also causes more severe collapse as well, which aligns very well with the number of dead units. We observed that this behavior can also happen with networks using saturating activation functions such as sigmoid. eﬀective rank. The eﬀective rank is low for both small and large learning rates. For higher learning rates, as we have seen earlier, training for longer leads to many dead ReLU units, which in turn causes the eﬀective rank to diminish, as seen in Figures 11 and 12. Moreover, as seen in those ﬁgures, in Phase 1, the eﬀective rank and the number of dead units are low. Thus, the rank collapse in Phase 1 is not caused by the number of dead units. In Phase 3, the number of dead units is high, but the eﬀective rank is drastically low. The drastically low eﬀective rank is caused by the network’s large number of dead units in Phase 3. In Phase 3, we believe that both the under-parameterization and the poor performance is caused by the number of dead units in ReLU DQN, which was shown to be the case by (Shin and Karniadakis, 2020) in supervised learning. Overall, we could only observe a high correlation between the eﬀective rank and the agent’s performance, when we use ReLU activations in the network after a long training. We also present more analysis on how controlling the loss landscape aﬀects the rank vs performance in Appendix A.2 and A.4. 8 The eﬀect of loss function 8.1 Q-learning and behavior cloning Behavior cloning (BC) is a method to learn the behavior policy from an oﬄine dataset using supervised learning approaches (Pomerleau, 1989). Policies learned by BC will learn to mimic the behavior policy, and thus the performance of the learned BC agent is highly limited by the quality of the data the agent is trained on. We compare BC and Q-learning in Figure 15. We conﬁrm that with default hyperparameters, the BC agent’s eﬀective rank does not collapse at the end of training. In contrast, as shown by Kumar et al. (2020a), 17 Preprint Figure 12: [Atari DQN-256-3M] Learning curves of oﬄine DQN for 3M gradient steps of training with minibatch size of 256. Increasing the minibatch size improves the performance of the network with larger learning rates. DQN’s eﬀective rank collapses. DQN outperforms BC even if its rank is considerably lower, and during learning, the rank is not predictive of the performance (Figure 15). This behavior indicates the unreliability of eﬀective rank for oﬄine model selection. 8.2 CURL: The eﬀect of self-supervision We study whether adding an auxiliary loss term proposed for CURL (Laskin et al., 2020) (Equation 2) during training helps the model mitigate rank collapse. In all our Atari experiments, we use the CURL loss described in (Laskin et al., 2020) without any modiﬁcations. Since the DeepMind lab tasks require memory, we apply a similar CURL loss to the features aggregated with a mean of the states over all timesteps. In all experiments, we also sweep over the weight of the CURL loss λ. Figure 16 shows the ranks and returns on Atari (for DeepMind lab results see Appendix A.6.) In Atari games, using very large weights for an auxiliary loss (≈ 1) prevents rank collapse, but they simultaneously deteriorate the agent performance. We speculate, borrowing on intuitions from the supervised learning literature on the role of the rank as implicit regularizer Huh et al. (2021), that in such scenarios large rank prevents the network from ignoring spurious parts of the observations, which aﬀects its ability to generalize. On the other hand, moderate weights of CURL auxiliary loss do not signiﬁcantly change the rank and performance of the agent. Previously Agarwal et al. (2021) showed that the CURL loss does not improve the performance of an RL agent on Atari in a statistically meaningful way. On DeepMind lab games, we do not observe any rank collapse. In none of our DeepMind lab experiments agents enter into Phase 3 after Phase 1 and 2. This is due to the use of the tanh activation function for LSTM based on our investigation of the role of the activation functions in Section 6. 18 Preprint Figure 13: [Atari BC-256-2M] Learning Curves of BC After 2M gradient steps with mini-batch size of 256. For large enough learning rates the rank of BC agent also collapses. We hypothesize that the rank collapse is a side eﬀect of learning in general and not only due to TD-learning based losses. Figure 14: [Atari] Scatter plot for the correlation between the number of dead units and eﬀective rank at the end of training and we observe that the eﬀective rank strongly correlates with the number of dead units. Also, using larger learning rate increases the number of dead units in the network. 9 Tandem RL Kumar et al. (2020a) propose one possible hypothesis for the observed rank collapse as the re-use of the same transition samples multiple times, particularly prevalent in the oﬄine RL setting. A setting in which this hypothesis can be tested directly is the ‘Tandem RL’ proposed in Ostrovski et al. (2021): here a secondary (‘passive’) agent is trained from the data stream generated by an architecturally equivalent, independently initialized baseline agent, which itself is trained in a regular, online RL fashion. The passive agent tends to under-perform the active agent, despite identical architecture, learning algorithm, and data stream. This 19 Preprint Figure 15: [Atari] Oﬄine DQN and Behavior Cloning (BC): We compare BC and DQN agents on the Atari dataset. We used the same architecture, dataset, and training protocols for both baselines. We used the hyperparameters deﬁned in (Gulcehre et al., 2020) for comparisons. The rank of the DQN agent is signiﬁcantly lower and achieves higher returns than BC. Figure 16: [Atari] Auxiliary losses: Evolution of ranks as we increase the weight of auxiliary losses. We see that a strong weight for auxiliary loss helps mitigate the rank collapse but prevents the model from learning useful representations. setup presents a clean ablation setting in which both agents use data in the same way (in particular, not diﬀering in their re-use of data samples), and so any diﬀerence in performance or rank of their representation cannot be directly attributable to the reuse of data. In Figure 17, we summarize the results of a Tandem-DQN using Adam (Kingma and Ba, 2014) and RMSProp (Tieleman et al., 2012) optimizers. Despite the passive agent reusing data in a similar fashion as the online agent, we observe that it collapses to a lower rank. Besides, the passive agent’s performance tends to be signiﬁcantly (in most cases, catastrophically) worse than the online agent that could not satisfactorily explained just by the extent of diﬀerence in their eﬀective ranks alone. We think that the Q-learning 20 Preprint Figure 17: Atari, tandem RL: We investigate the eﬀect of the choice of optimizers between Adam and RMSProp on rank and the performance of the models, both for the active (solid lines) and passive agents (dashed lines). We observe the rank of the passive agent is lower than the active agent both for Adam and RMSProp. Figure 18: Atari, Forked tandem RL: We evaluate diﬀerent loss functions in the forked tandem setting, where the passive agent is forked from the online agent, and the online agent’s parameters are frozen. Still, the parameters of the online agent are updated. We forked the passive agent after it had seen 50M frames during training which is denoted with dashed lines in the ﬁgure. We observe that using both BVE and MC return losses improves the agent’s rank, but the agent’s performance is still poor. algorithm seems to be not eﬃcient enough to exploit the data generated by the active agent to learn complex behaviors that would put the passive agent into Phase 2. We also noticed that, although Adam achieves better performance than RMSProp, the rank of the model trained with the Adam optimizer tends to be lower than the models trained with RMSProp. In Figure 18, we investigate the eﬀect of diﬀerent learning algorithms on the rank and the performance of an agent in the forked tandem RL setting. In the forked tandem setting, an agent is ﬁrstly trained for a fraction of its total training time. Then, the agent is ‘forked’ into active and passive agents, and they both start with the same network weights where the active agent is ‘frozen’ and not trained but continues to generate data from its policy to train the passive agent on this generated data for the remaining of the training time. Here, we see that the rank ﬂat-lines when we fork the Q-learning agent, but the performance collapses dramatically. In contrast, despite the rank of BVE and an agent trained with on-policy Monte Carlo returns going up, the 21 breakout pong i ) e v s s a p ( s k n a R l e n r e K e r u t a e F 200 150 100 50 0 200 100 0 0 0 50 100 150 200 200 150 100 50 0 seaquest space_invaders 200 100 Algorithm BVE Monte-Carlo Q-learning 50 100 150 200 0 0 50 100 150 200 50 100 150 200 0 Frames (x 1M) breakout pong seaquest space_invaders i ) e v s s a p ( n r u t e R e d o s p E i 400 300 200 100 0 20 10 0 −10 −20 20000 15000 10000 5000 0 0 50 100 150 200 0 50 100 150 200 0 Frames (x 1M) 1500 1000 500 Algorithm BVE Monte-Carlo Q-learning 50 100 150 200 0 0 50 100 150 200 Preprint performance still drops. Nevertheless, the decline of performance for BVE and the agent trained with Monte Carlo returns is not as bad as the Q-learning agent on most Atari games. 10 Oﬄine policy selection In Section 7, we found that with the large learning rate sweep setting rank and number of dead units have high correlation with the performance. Oﬄine policy selection (Paine et al., 2020) aims to choose the best policy without any online interactions purely from oﬄine data. The apparent correlation between the rank and performance raises the natural question of how well rank performs as an oﬄine policy selection approach. We did this analysis on DQN-256-20M experiments where we previously observed strong correlation between the rank and performance. We ran the DQN network described in DQN-256-20M experimental setting until the end of the training and performed oﬄine policy selection using the eﬀective rank to select the best learning rate based on the learning rate that yields to maximum eﬀective rank or minimum number of dead units. Figure 19: Atari DQN-256-20M: This plot is depicting the simple regret of a DQN agent with ReLU activations using eﬀective rank and the number of dead units. On the left, we show the the simple regret to select the best learning rate using the eﬀective rank and on the right hand side, we show the simple regret achieved based on the number of dead units. Figure 19 illustrates the simple regret on each Atari oﬄine policy selection game. The simple regret between the recommended policy measures how close an agent is to the best policy, and it is computed as described in Konyushkova et al. (2021). A simple regret of 1 would mean that our oﬄine policy selection mechanism successfully chooses the best policy, and 0 would mean that it would choose the worst policy. After 2M training steps, the simple regret with the number of dead units as a policy selection method is poor. In contrast, the simple regret achieved by selecting the agent with the highest eﬀective rank is good. The mean simple regret achieved by using the number of dead units as an oﬄine policy selection (OPS) method is 0.45 ± 0.11, where the uncertainty ±0.11 is computed as standard error across ﬁve seeds. In contrast, simple regret achieved by using eﬀective rank as an OPS method is 0.24 ± 0.12. The eﬀective ranks for most learning rates collapse as we train longer since more models enter Phase 3 of learning—the number of dead units increases. After 20M of learning steps, the mean simple regret computed using eﬀective rank as the OPS method becomes 0.40 ± 0.07, and the mean simple regret is 0.25 ± 0.12 with the number of dead units for the OPS. Let us note that the 2M learning step is more typical in training agents on the RL Unplugged Atari dataset. The number of dead units becomes a good metric to do OPS when the network is trained for long (20M steps in our experiment), where the rank becomes drastically small for most learning rate conﬁgurations. However, eﬀective rank seems to be a good metric for the OPS earlier in training. Nevertheless, without prior information about the task and agent, it is challenging to conclude whether the number of dead units or eﬀective rank would be an appropriate OPS metric. Other factors such as the number of training steps, activation functions, and other hyperparameters confound the eﬀective rank and number of dead units. Thus, we believe those two metrics we tested are unreliable in general OPS settings without controlling those other extraneous factors. 22 t e r g e R e p m S l i Effective rank Number of dead units 1.000 20M steps 2M steps 0.791 0.838 0.665 0.588 0.545 0.503 0.504 0.457 0.569 0.524 0.359 0.309 0.665 0.539 0.393 0.411 0.423 0.621 0.542 0.446 0.190 0.079 k c a t t A n o m e D 0.000 y e k c o H e c I r e n n u R d a o R 0.021 n o x x a Z i r e d R m a e B n a m c a P s M 0.009 n a y o o P 0.035 0.000 k n a t o b o R k n u D e b u o D l 0.000 0.000 0.026 k c a t t A n o m e D y e k c o H e c I r e n n u R d a o R 0.000 0.000 n o x x a Z 0.000 i r e d R m a e B 0.075 n a m c a P s M n a y o o P 0.000 k n a t o b o R k n u D e b u o D l Preprint In Figure 20, we compare eﬀective rank as an OPS method with respect to its percentage improvement over the online policy selection to maximize the me- dian reward achieved by the each agent across nine Atari games. The eﬀective rank on Zaxxon, Beam- Rider, MsPacman, Pooyan, Robotank, DoubleDunk perform competitively to the online policy selection. Eﬀective rank may be a complementary tool for net- works with ReLU activation function and we believe it can be a useful metric to monitor during the train- ing in addition to number of dead units to have a better picture about the performance of the agent. 11 Robustness to input perturbations Figure 20: Atari DQN-256-20M: Here, we are de- picting the percentage of improvement by doing oﬄine policy selection using rank individually vs online policy selection using median reward across nine Atari games to select the best learning rate. Using eﬀective rank as the oﬄine policy selection method performs relatively well when compared to doing online policy selection based on median normalized score across Atari games. Several works, such as Sanyal et al. (2018) suggested that low-rank models can be more robust to input perturbations (speciﬁcally adversarial ones). It is diﬃcult to just measure the eﬀect of low-rank rep- resentations on the agent’s performance since rank itself is not a robust measure, as it depends on diﬀer- ent factors such as activation function and learning which in turn can eﬀect the generalization of the algorithm independently from rank. However, it is easier to validate the antithesis, namely “Do more robust agents need to have higher eﬀective rank than less robust agents?”. We can easily test this hypothesis by comparing DQN and BC agents. Robustness metric: We deﬁne the robustness metric ρ(p) based on three variables: i) the noise level p, ii) the noise distribution d(p), and iii) the score obtained by agent when evaluated in the environment with noise level p: score[d(p)] for which score[d(0)] represents the average score achieved by the agent without any perturbation applied on it. Then we can deﬁne ρ(p) as follows: ρ(p) = 1 − score[d(0)] − score[d(p)] score[d(0)] = score[d(p)] score[d(0)] . (4) In our experiments, we compare DQN and BC agents since we already know that BC has much larger ranks across all Atari games than DQN. We trained these agents on datasets without any data augmentation. The data augmentations are only applied on the inputs when the agent is evaluated in the environment. We evaluate our agents on BeamRider, IceHockey, MsPacman, DemonAttack, Robotank and RoadRunner games from RL Unplugged Atari online policy selection games. We excluded DoubleDunk, Zaxxon and Pooyan games since BC agent’s performance on those levels was poor and very close to the performance of random agent, so the robustness metrics on those games would not be very meaningful. On IceHockey, the results can be negative, thus we shifted the scores by 20 to make sure that they are non-negative. In our robustness experiments, we used the hyperparameters that were used in Gulcehre et al. (2020) on Atari for both for DQN and BC. In Figure 21, we investigate the Robustness of DQN and BC agents with Robustness to random shifts: respect to random translation image perturbations applied to the observations as described by Yarats et al. (2020). We evaluate the agent on the Atari environment with varying degrees of input scaling. We noticed that BC’s performance deteriorates abruptly, whereas the performance of DQN, while also decreasing, is better than the BC one. 23 0 8 0 6 74.074 t n e m e v o r p m I f o e g a t n e c r e P 0 4 0 2 0 0 2 − 4.159 4.450 -3.332 -7.086 -18.605 -17.358 0 4 − 0 6 − -54.037 -49.712 k c a t t A n o m e D y e k c o H e c I r e n n u R d a o R n o x x a Z i r e d R m a e B n a m c a P s M n a y o o P k n a t o b o R k n u D e b u o D l Preprint Figure 21: [Atari] Robustness to random shifts during evaluation: We measure the robustness of BC and DQN to random shifts on six Atari games from RL Unplugged. The DQN and BC agents are trained oﬄine without any perturbations on the inputs, only on the RL Unplugged datasets. However, during evaluation time we perturbed the input images with ""random shift"" data augmentation. Overall, DQN is more robust than BC to the evaluation-time random-scaling image perturbations that are not seen during training. The diﬀerence is more pronounced on IceHockey and BeamRider games. DQN achieved mean AUC (over diﬀerent games) of 2.042 and BC got 1.26. Figure 22: [Atari] Robustness to random scaling during evaluation: We measure the robustness of BC and DQN to random scaling over inputs on six Atari games from RL Unplugged. The DQN and BC agents are trained oﬄine without data augmentations over the RL Unplugged datasets. However, we perturbed the observations during the evaluation with ""random scale"" data augmentation. We observed that DQN is more robust than BC to the evaluation-time random-scaling data augmentation. The diﬀerence is more pronounced in IceHockey, BeamRider, and Robotank games. DQN achieved a mean AUC (over diﬀerent games) of 1.60, and BC achieved 0.87. 24 e r o c s s s e n t s u b o R 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 MsPacman DemonAttack IceHockey 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 Robotank BeamRider RoadRunner 1.0 0.8 0.6 0.4 0.2 0.0 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 DQN BC 1.0 0.8 0.6 0.4 0.2 0.0 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 The random shift rate 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 e r o c s s s e n t s u b o R MsPacman DemonAttack IceHockey 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.00 0.02 0.04 0.06 0.08 0.10 0.00 0.02 0.04 0.06 0.08 0.10 0.00 0.02 0.04 0.06 0.08 0.10 Robotank BeamRider RoadRunner 1.0 0.8 0.6 0.4 0.2 1.0 0.8 0.6 0.4 0.2 0.0 0.00 0.02 0.04 0.06 0.08 0.10 0.0 0.00 0.02 0.04 0.06 0.08 0.10 The random scale value DQN BC 1.0 0.8 0.6 0.4 0.2 0.0 0.00 0.02 0.04 0.06 0.08 0.10 Preprint Robustness to random scaling: In Figure 22, we explore the Robustness of DQN and BC agents with respect to random scaling as image perturbation method applied to the observations that are feed into our deep Q-network. We randomly scale the image inputs as described in Chen et al. (2017). When evaluating the agent in an online environment, we test it with varying levels of image scaling. The results are again very similar to the random shift experiments where the DQN agent is more robust to random perturbations in the online environment. According to the empirical results obtained from those two experiments, it is possible to see that DQN is more robust to the evaluation-time input perturbations, speciﬁcally random shifts and scaling than the BC agent. Thus, more robust representations do not necessarily require a higher eﬀective rank. 12 Discussion In this work, we empirically investigated the previously hypothesized connection between low eﬀective rank and poor performance. We found that the relationship between eﬀective rank and performance is not as simple as previously conjectured. We discovered that an oﬄine RL agent trained with Q-learning during training goes through three phases. The eﬀective rank collapses to severely low values in the ﬁrst phase –we call this as the self-pruning phase– and the agent starts to learn basic behaviors from the dataset. Then in the second phase, the eﬀective rank starts going up, and in the third phase, the eﬀective rank collapses again. Several factors such as learning rate, activation functions and the number of training steps, inﬂuence the occurrence, persistence and the extent of severity of the three phases of learning. In general, a low rank is not always indicative of poor performance. Besides strong empirical evidence, we propose a hypothesis trying to explain the underlying phenomenon: not all features are useful for the task the neural network is trying to solve, and low rank might correlate with more robust internal representations that can lead to better generalization. Unfortunately, reasoning about what it means for the rank to be too low is hard in general, as the rank is agnostic to which direction of variations in the data are being ignored or to higher-order terms that hint towards a more compact representation of the data with fewer dimensions. Our results indicate that an agent’s eﬀective rank and performance correlate in restricted settings, such as ReLU activation functions, Q-learning, and a ﬁxed architecture. However, as we showed in our experiments, this correlation is primarily spurious in other settings since it disappears with simple modiﬁcations such as changing the activation function and the learning rate. We found several ways to improve the eﬀective rank of the agent without improving the performance, such as using tanh instead of ReLU, an auxiliary loss (e.g., CURL), and the optimizer. These methods address the rank collapse but not the underlying learning deﬁciency that causes the collapse and the poor performance. Nevertheless, our results show that the dynamics of the rank and agent performance through learning are still poorly understood; we need more theoretical investigation to understand the relationship between those two factors. We also observed in Tandem and oﬄine RL settings that the rank collapses to a minimal value early in training. Then there is unexplained variance between agents in the later stages of learning. Overall, the cause and role of this early rank collapse remain unknown, and we believe understanding its potential eﬀects is essential in understanding large-scale agents’ practical learning dynamics. The existence of low-rank but high-performing policies suggest that our networks can be over-parameterized for the tasks and parsimonious representations emerge naturally with TD-learning-based bootstrapping losses and ReLU networks in the oﬄine RL setting. Discarding the dead ReLU units might achieve a more eﬃcient inference. We believe this ﬁnding can give inspiration to a new family of pruning algorithms. Acknowledgements: We would like to thank Clare Lyle, Will Dabney, Aviral Kumar, Rishabh Agarwal, Tom le Paine, Mark Rowland and Yutian Chen for the discussions. We want to thank Mark Rowland, Rishabh Agarwal and Aviral Kumar for the feedback on the early draft version of the paper. We want to thank Sergio Gomez and Bjorn Winckler for their help with the infrastructure and the codebase at the inception of this project. We would like to thank the developers of Acme (Hoﬀman et al., 2020), Jax (Bradbury et al., 2018), Optax (Hessel et al., 2020) and Haiku (Hennigan et al., 2020). 25 Preprint References Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on oﬄine reinforce- ment learning. In International Conference on Machine Learning, pages 104–114. PMLR, 2020. Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34:29304–29320, 2021. OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3–20, 2020. Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. Advances in Neural Information Processing Systems, 32:7413–7424, 2019. David Barrett and Benoit Dherin. Implicit gradient regularization. In International Conference on Learning Representations, 2020. Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, Andrew Lefrancq, Simon Green, Víctor Valdés, Amir Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016. Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. Advances in Neural Information Processing Systems, 29, 2016. Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, 2013. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. DotA 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax. Steven Bradtke and Andrew Barto. Linear least-squares algorithms for temporal diﬀerence learning. Machine Learning, 22:33–57, 03 1996. Liang-Chieh Chen, George Papandreou, Florian Schroﬀ, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017. Hadi Daneshmand, Jonas Kohler, Francis Bach, Thomas Hofmann, and Aurelien Lucchi. Batch normalization provably avoids ranks collapse for randomly initialised deep networks. Advances in Neural Information Processing Systems, 2020. Yann Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. arXiv preprint arXiv:1406.2572, 2014. Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In International Conference on Machine Learning, volume 70, pages 1019–1028. PMLR, 2017. Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. Challenges of real-world reinforcement learning. arXiv preprint arXiv:1904.12901, 2019. Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine Learning Research, 6:503–556, 2005. 26 Preprint Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for eﬃciently improving generalization. arXiv preprint arXiv:2103.09575, 2021. Justin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020. Scott Fujimoto, Herke Van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. arXiv preprint arXiv:1802.09477, 2018. Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau. Benchmarking batch deep reinforcement learning algorithms. arXiv preprint arXiv:1910.01708, 2019. Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization via Hessian eigenvalue density. arXiv preprint arXiv:1901.10159, 2019. Xavier Glorot and Yoshua Bengio. Understanding the diﬃculty of training deep feedforward neural networks. In International Conference on Artiﬁcial Intelligence and Statistics, pages 249–256. JMLR Workshop and Conference Proceedings, 2010. Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectiﬁer neural networks. In International Conference on Artiﬁcial Intelligence and Statistics, pages 315–323. JMLR Workshop and Conference Proceedings, 2011. Caglar Gulcehre, Marcin Moczulski, Misha Denil, and Yoshua Bengio. Noisy activation functions. In International Conference on Machine Learning, pages 3059–3068. PMLR, 2016. Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Tom Le Paine, Sergio Gómez Colmenarejo, Konrad Zolna, Rishabh Agarwal, Josh Merel, Daniel Mankowitz, Cosmin Paduraru, et al. RL unplugged: Benchmarks for oﬄine reinforcement learning. arXiv preprint arXiv:2006.13888, 2020. Caglar Gulcehre, Sergio Gómez Colmenarejo, Ziyu Wang, Jakub Sygnowski, Thomas Paine, Konrad Zolna, Yutian Chen, Matthew Hoﬀman, Razvan Pascanu, and Nando de Freitas. Regularized behavior value estimation. arXiv preprint arXiv:2103.09575, 2021. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human- level performance on imagenet classiﬁcation. In Proceedings of the IEEE International Conference on Computer Vision, pages 1026–1034, 2015. Tom Hennigan, Trevor Cai, Tamara Norman, and Igor Babuschkin. Haiku: Sonnet for JAX, 2020. URL http://github.com/deepmind/dm-haiku. Matteo Hessel, David Budden, Fabio Viola, Mihaela Rosca, Eren Sezener, and Tom Hennigan. Optax: composable gradient transformation and optimisation, in jax!, 2020. URL http://github.com/deepmind/ optax. Matt Hoﬀman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Feryal Behbahani, Tamara Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, Sarah Henderson, Alex Novikov, Sergio Gómez Colmenarejo, Serkan Cabi, Caglar Gulcehre, Tom Le Paine, Andrew Cowie, Ziyu Wang, Bilal Piot, and Nando de Freitas. Acme: A research framework for distributed reinforcement learning. arXiv preprint arXiv:2006.00979, 2020. Kaixuan Huang, Yuqing Wang, Molei Tao, and Tuo Zhao. Why do deep residual networks generalize better than deep feedforward networks?–a neural tangent kernel perspective. arXiv preprint arXiv:2002.06262, 2020. Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, and Phillip Isola. The low-rank simplicity bias in deep networks. arXiv preprint arXiv:2103.10427, 2021. Maximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, and Shimon Whiteson. Transient non-stationarity and generalisation in deep reinforcement learning. arXiv preprint arXiv:2006.05826, 2020. 27 Preprint Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably eﬃcient for oﬄine RL? In International Conference on Machine Learning, pages 5084–5096. PMLR, 2021. Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak, and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. Advances in neural information processing systems, 32, 2019. Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2014. Ksenia Konyushkova, Yutian Chen, Thomas Paine, Caglar Gulcehre, Cosmin Paduraru, Daniel J Mankowitz, Misha Denil, and Nando de Freitas. Active oﬄine policy selection. Advances in Neural Information Processing Systems, 34, 2021. Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing oﬀ-policy Q-learning In Conference on Neural Information Processing Systems, pages via bootstrapping error reduction. 11761–11771, 2019. Aviral Kumar, Rishabh Agarwal, Dibya Ghosh, and Sergey Levine. Implicit under-parameterization inhibits data-eﬃcient deep reinforcement learning. arXiv preprint arXiv:2010.14498, 2020a. Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for oﬄine reinforce- ment learning. arXiv preprint arXiv:2006.04779, 2020b. Aviral Kumar, Rishabh Agarwal, Aaron Courville, Tengyu Ma, George Tucker, and Sergey Levine. Value- based deep reinforcement learning requires explicit regularization. In RL for Real Life Workshop & Overparameterization: Pitfalls and Opportunities Workshop, ICML, 2021a. URL https://drive.google. com/file/d/1Fg43H5oagQp-ksjpWBf_aDYEzAFMVJm6/view. Aviral Kumar, Rishabh Agarwal, Tengyu Ma, Aaron Courville, George Tucker, and Sergey Levine. Dr3: Value-based deep reinforcement learning requires explicit regularization. arXiv preprint arXiv:2112.04716, 2021b. Aviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, and Sergey Levine. A workﬂow for oﬄine model-free robotic reinforcement learning. arXiv preprint arXiv:2109.10813, 2021c. Michail G. Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of Machine Learning Research, 4:1107–1149, 2003. Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Marco Wiering and Martijn van Otterlo, editors, Reinforcement Learning: State-of-the-Art, pages 45–73. Springer Berlin Heidelberg, 2012. Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations for reinforcement learning. In International Conference on Machine Learning, pages 5639–5650. PMLR, 2020. Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Oﬄine reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020. Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. arXiv preprint arXiv:1712.09913, 2017. Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. arXiv preprint arXiv:1808.01204, 2018. 28 Preprint Clare Lyle, Mark Rowland, Georg Ostrovski, and Will Dabney. On the eﬀect of auxiliary tasks on representation dynamics. In International Conference on Artiﬁcial Intelligence and Statistics, pages 1–9. PMLR, 2021. Marlos C Machado, Sriram Srinivasan, and Michael Bowling. Domain-independent optimistic initialization for reinforcement learning. In Workshops at the Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence, 2015. Charles H Martin and Michael W Mahoney. Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning. Journal of Machine Learning Research, 22(165): 1–73, 2021. Michael Mathieu, Sherjil Ozair, Srivatsan Srinivasan, Caglar Gulcehre, Shangtong Zhang, Ray Jiang, Tom Le Paine, Konrad Zolna, Richard Powell, Julian Schrittwieser, et al. Starcraft ii unplugged: Large scale oﬄine reinforcement learning. In Deep RL Workshop NeurIPS 2021, 2021. Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoﬀrey Irving, and Nat McAleese. Teaching language models to support answers with veriﬁed quotes. arXiv preprint arXiv:2203.11147, 2022. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015. Hossein Mobahi, Mehrdad Farajtabar, and Peter L Bartlett. Self-distillation ampliﬁes regularization in Hilbert space. arXiv preprint arXiv:2002.05715, 2020. Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepezvari, Satinder Singh, et al. Behaviour suite for reinforcement learning. arXiv preprint arXiv:1908.03568, 2019. Georg Ostrovski, Pablo Samuel Castro, and Will Dabney. The diﬃculty of passive learning in deep reinforce- ment learning. Advances in Neural Information Processing Systems, 34, 2021. Tom Le Paine, Cosmin Paduraru, Andrea Michi, Caglar Gulcehre, Konrad Zolna, Alexander Novikov, Ziyu Wang, and Nando de Freitas. Hyperparameter selection for oﬄine reinforcement learning. arXiv preprint arXiv:2007.09055, 2020. Jeﬀrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning. Advances in Neural Information Processing Systems, 2017. Dean A Pomerleau. ALVINN: An autonomous land vehicle in a neural network. In Conference on Neural Information Processing Systems, pages 305–313, 1989. Martin Riedmiller. Neural ﬁtted Q iteration – ﬁrst experiences with a data eﬃcient neural reinforcement learning method. In João Gama, Rui Camacho, Pavel B. Brazdil, Alípio Mário Jorge, and Luís Torgo, editors, European Conference on Machine Learning, pages 317–328, 2005. Levent Sagun, Léon Bottou, and Yann LeCun. Singularity of the hessian in deep learning. arXiv preprint arXiv:1611.07476, 2016. Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017. Amartya Sanyal, Varun Kanade, Philip HS Torr, and Puneet K Dokania. Robustness via deep low-rank representations. arXiv preprint arXiv:1804.07090, 2018. Tianyu Shi, Dong Chen, Kaian Chen, and Zhaojian Li. Oﬄine reinforcement learning for autonomous driving with safety and exploration enhancement. arXiv preprint arXiv:2110.07067, 2021. 29 Preprint Yeonjong Shin and George Em Karniadakis. Trainability of relu networks and data-dependent initialization. Journal of Machine Learning for Modeling and Computing, 1(1), 2020. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, and Adrian Bolton. Mastering the game of go without human knowledge. Nature, 550(7676):354–359, 2017. Edward H Simpson. The interpretation of interaction in contingency tables. Journal of the Royal Statistical Society: Series B (Methodological), 13(2):238–241, 1951. Samuel L Smith and Quoc V Le. A Bayesian perspective on generalization and stochastic gradient descent. arXiv preprint arXiv:1710.06451, 2017. Samuel L Smith, Benoit Dherin, David GT Barrett, and Soham De. On the origin of implicit regularization in stochastic gradient descent. arXiv preprint arXiv:2101.12176, 2021. Tijmen Tieleman, Geoﬀrey Hinton, et al. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–31, 2012. Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double Q-learning. In Thirtieth AAAI conference on artiﬁcial intelligence, 2016. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019. Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pessimism for oﬄine reinforcement learning. Advances in Neural Information Processing Systems, 34, 2021. Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In International Conference on Learning Representations, 2020. A Appendix A.1 The impact of depth Pennington and Worah (2017) explored the relationship between the rank and the depth of a neural network at initialization and found that the rank of each layer’s feature matrix decreases proportionally to the index of a layer in a deep network. Here, we test the performance of a feedforward network on the bsuite task trained using Double Q-learning (Van Hasselt et al., 2016) with 2, 8, and 16 layers to see the eﬀect of the number of layers on the eﬀective rank. All our networks use ReLU activation functions and use He initialization (He et al., 2015). Figure 23 illustrates that the rank collapses as one progresses from lower layers to higher layers proportionally at the end of training as well. The network’s eﬀective rank (rank of the penultimate layer) drops to a minimal value on all three tasks regardless of the network’s number of layers. The last layer of a network will act as a bottleneck; thus, a collapse of the eﬀective rank would reduce the expressivity. Nevertheless, a deeper network with the same rank as a shallower one can learn to represent a larger class of functions (be less under-parametrized). The agents exhibit poor performance when the eﬀective rank collapses to 1. At that point, all the ReLU units die or become zero irrespective of input. Thus on bsuite, deeper networks –four and eight layered networks– performed worse than two layered MLP. A.2 Spectral density of hessian for oﬄine RL Analyzing the eigenvalue spectrum of the Hessian is a common way to investigate the learning dynamics and the loss surface of deep learning methods (Ghorbani et al., 2019; Dauphin et al., 2014). Understanding Hessian’s loss landscape and eigenvalue spectrum can help us design better optimization algorithms. Here, 30 Preprint (L) (C) (R) Figure 23: [bsuite] The ranks and depths of the networks: The evolution of the ranks across diﬀerent layers of deep neural networks. The ﬁgure on the left (L) is for the ranks of catch across diﬀerent layers. The ﬁgure at the center (C) is for the ranks of mountain_car across diﬀerent layers. The ﬁgure on the right (R) is for cartpole. we analyze the eigenvalue spectrum of a single hidden layer feedforward network trained on the bsuite Catch dataset from RL Unplugged (Gulcehre et al., 2020) to understand the loss-landscape of a network with a low eﬀective rank compared to a model with higher rank at the end of the training. As established in Figure 24, ELU activation function network has a signiﬁcantly higher eﬀective rank than the ReLU network. By comparing those two networks, we also look into the diﬀerences in the eigenvalue spectrum of a network with high and low rank. Since the network and the inputs are relatively low-dimensional, we computed the full Hessian over the dataset rather than a low-rank approximation. The eigenvalue spectrum of the Hessian with ReLU and the ELU activation functions is shown in Figure 24. The rank collapse is faster for ReLU than ELU. After 900k gradient updates, the ReLU network concentrates 92% of the eigenvalues of Hessian around zero; this is due to the dead units in ReLU network (Glorot et al., 2011). On the other hand, the ELU network has a few very large eigenvalues after the same number of gradient updates. In Figure 25, we summarize the distribution of the eigenvalues of the Hessian matrices of the ELU and ReLU networks. As a result, the Hessian and the feature matrix of the penultimate layer of the network will be both low-rank. Moreover, in this case, the landscape of the ReLU network might be ﬂatter than the ELU beyond the notion of wide basins (Sagun et al., 2016). This might mean that the ReLU network ﬁnds a simpler solution. Thus, the ﬂatter landscape is conﬁrmed by the simpler function learned and less capacity used at the end of the training, which is induced by the lower rank representations. Figure 24: [bsuite Catch] spectral density of Hessian: The visualization of the spectral density of the full Hessian of a network trained with 64 units using ReLU (left) and ELU (right) activation functions. The eigenvalues of the Hessian of the oﬄine DQN with ReLU activation function are concentrated around 0 and most of them are less than 1. The eigenvalues of the ELU network also concentrate around 0 with a few large outlier eigenvalues. 31 k n a R 35 30 25 20 15 10 5 0 2 Layer Net 8 Layer Net 16 Layer Net 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Network's Depth k n a R e v i t c e f f E 6 5 4 3 2 1 0 2 Layer Net 8 Layer Net 16 Layer Net 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Network's Depth k n a R e v i t c e f f E 6 5 4 3 2 1 0 2 Layer Net 8 Layer Net 16 Layer Net 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Network's Depth y t i s n e D 2 10 0 10 −2 10 −4 10 −6 10 −8 10 −10 10 Learner steps = 0 Learner steps = 900k −0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Eigenvalue y t i s n e D 2 10 0 10 −2 10 −4 10 −6 10 −8 10 −10 10 Learner steps = 0 Learner Steps = 900k 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 Eigenvalue Preprint Figure 25: [bsuite Catch] Hessian eigenvalues (evs) for oﬄine DQN: We visualize the percentages of positive, negative, and near-zero eigenvalues of the Hessian for oﬄine DQN with ELU and ReLU activation functions on the catch dataset from bsuite. If the absolute value of an eigenvalue is less than 1e-7, we consider it as a near-zero eigenvalue. We can see that for ELU network, near-zero, positive and negative eigenvalues are almost evenly distributed. However, with ReLU network majority of eigenvalues are near-zero (90% of the evs are exactly zero), very few negative (2 %) and some positive eigenvalues (7.1 %). A.3 DeepMind lab: performance vs the eﬀective ranks In Figure 26, we show the correlation between the eﬀective rank and the performance of R2D2, R2D2 trained with CURL and R2D2 trained with SAM. When we look at each variant separately or as an aggregate, we don’t see a strong correlation between the performance and the rank of an agent. Figure 26: [DeepMind lab] The eﬀective rank and the performance: Correlations between feature ranks and episode returns for diﬀerent exploration noises on DeepMind lab dataset. We include data from three models: R2D2, R2D2 trained with CURL, and R2D2 trained with SAM. We do not observe a strong correlation between the eﬀective rank and performance across diﬀerent noise exploration levels in the data. A.4 SAM optimizer: does smoother loss landscape aﬀect the behavior of the rank collapse? We study the relationship between smoother loss landscapes and rank collapse. We use Sharpness Aware Minimization (SAM) (Foret et al., 2021) loss for potentially creating ﬂatter loss surfaces in order to see if smoother loss landscapes aﬀect the rank and performance dynamics diﬀerently. Figures 27 and 28 show the evolution of feature ranks and generalization performance in Atari and DeepMind lab respectively. We do not observe a very clear relation between the extent of smoothing the loss and the feature ranks or generalization performance. 32 % near-zero evs 100 80 60 40 20 0 % positive evs % negative evs ReLU DQN ELU DQN Eps: 0.0, Kendall τ:0.25, Pearson ρ:0.30 Eps: 0.01, Kendall τ:0.09, Pearson ρ:0.05 Eps: 0.1, Kendall τ:0.32, Pearson ρ:0.41 Eps: 0.25, Kendall τ:0.14, Pearson ρ:0.13 Variant CURL R2D2 SAM s k n a R e v i t c e f f E l i a n m r e T 200 100 0 0 2.5 5 7.5 10 0 5 10 15 0 10 20 30 0 10 20 30 40 Terminal Episode Return Preprint Figure 27: [Atari] Sharpness Aware Minimization (SAM) Loss: Evolution of ranks as we increase the weight of auxiliary losses. We see that some amount of weight on the SAM loss helps mitigate the extent of rank collapse but we observe no clear relationship with the agent performance. Figure 28: [DeepMind lab-SeekAvoid Snapshot] Sharpness Aware Minimization (SAM) Loss We do not observe any rank collapse as we continue training with the LSTM networks (because of the tanh activations discussed in Section 6) used in DeepMind lab dataset over a spectrum of diﬀerent weights for the SAM loss. A.5 Eﬀective rank and the value error A curious relationship is between the eﬀective rank and the value error, because a potential for the rank collapse or Phase 3 with the TD-learning algorithms can be the errors propagating thorough the bootstrapped targets. Figure 29 shows the correlation between the value error and the eﬀective ranks. There is a strong anti-correlation between the eﬀective ranks and the performance of the agents except on the levels where the expert agent that generated the dataset performs poorly at the end of the training (e.g. IceHockey.) This 33 Preprint Figure 29: [Atari]: These plots shows the correlation between the value error and the eﬀective rank for oﬄine DQN agent trained for 20M steps on online policy selection games for Atari. There is an apparent anti-correlation between the eﬀective rank and the value error. Namely, as the value error of an agent when evaluated in the environment increases the eﬀective rank decreases. The correlation is signiﬁcant on most Atari levels except IceHockey where the expert agent that generated the dataset performs poorly. makes the hypothesis that the extrapolation error can cause rank collapse (or push the agent to Phase 3) more plausible. A.6 CURL on DeepMind Lab In Figure 30 shows the CURL results on DeepMind lab dataset. We couldn’t ﬁnd any consistent pattern across various CURL loss weights. Figure 30: [DeepMind lab-SeekAvoid:] auxiliary losses Evolution of ranks as we increase the weight of auxiliary loss. While some auxiliary loss helps the model perform well, there is no clear correlation between rank and performance. 34 Preprint A.7 Learning rate evolution In Figure 31, we also perform a hyperparameter selection by evaluating the model in the environment at various stages during the training. As the oﬄine DQN is trained longer, the optimal learning rate for the best agent performance when evaluated online goes down. As one increases the number of training steps of an agent, we need to change the learning rate accordingly since the number of training steps aﬀects the best learning rate. Figure 31: [Atari]: Evolution of the optimal learning rate found by online evaluations in the environment. As the model is trained longer the optimal learning rate found by online evaluations goes down. A.8 Learning curves long training regime and the diﬀerent phases of learning In this subsection, we investigate the eﬀect of changing learning rates on the eﬀective rank and the performance of the agent on RL Unplugged Atari online policy selection games. We train the oﬄine DQN for 20M learning steps which is ten times longer than typical oﬄine Atari agents (Gulcehre et al., 2020). We evaluated twelve learning rates in (cid:2)10−2, 10−5(cid:3) equally spaced in logarithmic space. We show the eﬀective rank learning and performance curves in Figure 32. It is easy to identify diﬀerent phases of learning in most of those curves. A.9 Eﬀective rank and the performance Figure 33 shows the correlation between the eﬀective rank and the performance of an agent trained with minibatch of size 32. On most Atari online policy selection games it is possible to see a very strong correlation but on some games the correlation is not there. It seems like even Figure 34 depicts the correlation between the eﬀective rank and the performance of a DQN agent with ReLU activation function. There is a signiﬁcant correlation on most Atari games. As we discussed earlier, long training setting with ReLU activation functions where the eﬀect of the rank is the strongest on the performance. A.10 Interventions to eﬀective rank and randomized controlled trials (RCT) As the rank is a continuous variable, we can ﬁlter out the experiments with certain ranks with a threshold τ . In the control case (λ(1)), we assume β > τ and for λ(0), we would have β ≤ τ . We can also write this as: E[λ|do(β > τ )] − E[λ|do(β ≤ τ )]. (5) A.11 Computation of feature ranks Here, we present the Python code-stub that we used across our experiments (similar to Kumar et al. (2020a)) to compute the feature ranks of the pre-output layer’s features: import numpy as np def compute_rank_from_features(feature_matrix, rank_delta=0.01): 35 0.00030 0.00025 0.000231 e t a R g n n r a e L i 0.00020 0.00015 0.00010 0.00005 0.000066 0.000019 1e+05 1.2e+06 2.4e+06 3.6e+06 4.8e+06 6e+06 7.2e+06 8.4e+06 9.6e+061.08e+071.2e+071.32e+071.44e+07 Learning Steps Preprint Figure 32: [Atari] Eﬀective rank curves: Eﬀective rank and performance oﬄine DQN agent across nine Atari online policy selection games. We train the oﬄine DQN agent for 20M learning steps and evaluated 12 learning rates in (cid:2)10−2, 10−5(cid:3) equally spaced in logarithmic space. Let us note that in all games rank goes down early in the training (Phase 1), then goes up (phase 2) and for some learning rate the eﬀective rank collapses (Phase 3). Correspondingly, the performance is low in the beginning of the training (Phase 1), goes up and stays high for a while (Phase 2), and sometimes it the performance collapses (Phase 3). 36 Preprint Figure 33: [Atari] The correlation between ranks and returns for DQN trained with minibatch size of 32. We ran each network with three learning rates and ﬁve diﬀerent seeds but we only show the mean across those ﬁve seeds here. We can see very strong correlations on some games, but that correlation is not consistent. Figure 34: [Atari] The correlation between ranks and returns for DQN trained for a network trained for 20M learning steps and 12 diﬀerent learning rates with minibatch size of 256. We can see that there is signiﬁcant positive correlation between the rank and the learning rates for most online policy selection games. 37 BeamRider, Spearman corr: 0.75 DemonAttack, Spearman corr: 0.87 200 DoubleDunk, Spearman corr: 0.44 IceHockey, Spearman corr: -0.06 1000 1500 2000 100 50 0 1.8 1.4 1 s k n a R e v i t c e f f E l i a n m r e T −16 −8 RoadRunner, Spearman corr: -0.48 −12 −4 12 9 6 3 0 0 MsPacman, Spearman corr: 0.59 10000 5000 0 Robotank, Spearman corr: 0.46 1000 2000 3000 150 100 50 0 75 50 25 0 75 50 25 0 1.15 1.10 1.05 1 100 90 80 70 60 50 90 60 30 0 −24 −22 Pooyan, Spearman corr: -0.33 −22.5 −23.5 −23 Learning Rates 3e-05 0.0001 0.0003 0 Zaxxon, Spearman corr: 0.80 100 300 200 0 250 500 750 0 20 40 Episode return 60 0 2000 4000 6000 8000 Preprint Hyper-parameters Training batch size Rank calculation batch size Num training steps Learning rate Optimizer Feedforward hidden layer size Num hidden layers Activation Memory Discount bsuite 32 512 1e6 3e-4 Adam 64 2 ReLU None 0.99 Atari 256 512 2e6 3e-5 Adam 512 1 ReLU None 0.99 DeepMind lab 4 (episodes) 512 2e4 1e-3 Adam 256 1 ReLU LSTM gates) LSTM 0.997 (tanh for Table 3: The default hyper-parameters used in our work across diﬀerent domains. """"""Computes rank of the features based on how many singular values are significant."""""" sing_values = np.linalg.svd(feature_matrix, compute_uv=False) cumsum = np.cumsum(sing_values) nuclear_norm = np.sum(sing_values) approximate_rank_threshold = 1.0 - rank_delta threshold_crossed = ( cumsum >= approximate_rank_threshold * nuclear_norm) effective_rank = sing_values.shape[0] - np.sum(threshold_crossed) + 1 return effective_rank A.12 Hyperparameters Here, we list the standard set of hyper-parameters that were used in diﬀerent domains: bsuite, Atari, and DeepMind Lab respectively. These are the default hyper-parameters, which may diﬀer when stated so in our speciﬁc ablation studies. For the DMLLAB task, we use the same network that was used in Gulcehre et al. (2021). For all the Atari tasks, we use the same convolution torso that was used in Gulcehre et al. (2020) which involves three layers of convolution with ReLU activations in between. • Layer 1 - Conv2d(channels=32, kernel_shape=[8, 8], stride=[4, 4]) • Layer 2 - Conv2d(channels=64, kernel_shape=[4, 4], stride=[2, 2]) • Layer 3 - Conv2d(channels=64, kernel_shape=[3, 3], stride=[1, 1]) A.13 bsuite phase transitions and bottleneck capacity We illustrate the phase transition of a simple MLPs with ReLU activations. In Figure 35, we have a network of size (64, bottleneck units, 64) where we vary the number of bottleneck units. In Figure 36, we have a network of size (64, bottleneck units) where we vary the number of bottleneck units. In both cases, having smaller number of bottleneck units reduces the performance of the mode and agents were able to solve the problem even when the penultimate layer’s eﬀective rank was small. With the larger learning rate the right handside ﬁgures (b), the eﬀective ranks tend to be lower. A.14 Activation sparsity on bsuite In Figure 37, we show that the activations of the ReLU network becomes very sparse during the course of training. The sparsity of the ReLU units seems to be signiﬁcantly higher than the ELU units at the end of training. 38 Preprint a) b) Figure 35: [bsuite] - Catch dataset: Phase transition plots of a network with hidden layers of size (64, bottleneck units, 64). On the y-axis, we show the log2(bottleneck units), and x-axis is the number of gradient steps. The ﬁgures on the left hand-side (a) is trained with the learning rate with 8e-5 and right hand-side (b) experiments are trained with learning rate of 3e-4. The ﬁrst row shows the episodic returns. The second row shows the eﬀective rank of the second layer (bottleneck units) and the third row is showing the penultimate layer’s eﬀective rank. The eﬀective rank collapses much faster with the learning rate of 4e-3 than 5e-8. The low ranks can still have good performance. The low bottleneck units causes the eﬀective rank of the last layer to collapse faster. The performance of the network with the small number of bottleneck units is poor. The eﬀective rank of the small number of bottleneck units is smaller. 39 Preprint Figure 36: [bsuite] - Catch dataset: Phase transition plots of a network with hidden layers of size (64, bottleneck units). On the y-axis, we show the log2(bottleneck units), and x-axis is the number of gradient steps. The ﬁgures on the left hand-side (a) is trained with the learning rate with 8e-5 and right hand-side (b) experiments are trained with learning rate of 3e-4. The ﬁrst row shows the episodic returns. The second row shows the eﬀective rank of the second layer (bottleneck units). The eﬀective rank collapses much faster with the learning rate of 4e-3 than 5e-8. The low ranks can still have good performance. The small number of bottleneck units causes the eﬀective rank of the layer to collapse faster. The performance of the network with the small number of bottleneck units is poor. The eﬀective rank of the small number of bottleneck units is smaller. Figure 37: [bsuite Catch] Gram matrices of activations: Gram matrices of activations of a two-layer MLP with ReLU and ELU activation functions. The activations of the ReLU units become sparser when compared to ELU units at the end of the training due to dead ReLU units. 40 Initialization (step 0) Convergence (step 900k) ReLU ELU"
349,Inside a radical new project to democratize AI,https://www.technologyreview.com/2022/07/12/1055817/inside-a-radical-new-project-to-democratize-ai/,2022-07-12,"PARIS — This is as close as you can get to a rock concert in AI research. Inside the supercomputing center of the French National Center for Scientific Research, on the outskirts of Paris, rows and rows of what look like black fridges hum at a deafening 100 decibels. They form part of a supercomputer that has spent 117 days gestating a new large language model (LLM) called BLOOM that its creators hope represents a radical departure from the way AI is usually developed. Unlike other, more famous large language models such as OpenAI’s GPT-3 and Google’s LaMDA, BLOOM (which stands for BigScience Large Open-science Open-access Multilingual Language Model) is designed to be as transparent as possible, with researchers sharing details about the data it was trained on, the challenges in its development, and the way they evaluated its performance. OpenAI and Google have not shared their code or made their models available to the public, and external researchers have very little understanding of how these models are trained. BLOOM was created over the last year by over 1,000 volunteer researchers in a project called BigScience, which was coordinated by AI startup Hugging Face using funding from the French government. It officially launched on July 12. The researchers hope developing an open-access LLM that performs as well as other leading models will lead to long-lasting changes in the culture of AI development and help democratize access to cutting-edge AI technology for researchers around the world. The model’s ease of access is its biggest selling point. Now that it’s live, anyone can download it and tinker with it free of charge on Hugging Face’s website. Users can pick from a selection of languages and then type in requests for BLOOM to do tasks like writing recipes or poems, translating or summarizing texts, or writing programming code. AI developers can use the model as a foundation to build their own applications. At 176 billion parameters (variables that determine how input data is transformed into the desired output), it is bigger than OpenAI’s 175-billion-parameter GPT-3, and BigScience claims that it offers similar levels of accuracy and toxicity as other models of the same size. For languages such as Spanish and Arabic, BLOOM is the first large language model of this size. But even the model’s creators warn it won’t fix the deeply entrenched problems around large language models, including the lack of adequate policies on data governance and privacy and the algorithms’ tendency to spew toxic content, such as racist or sexist language. Large language models are deep-learning algorithms that are trained on massive amounts of data. They are one of the hottest areas of AI research. Powerful models such as GPT-3 and LaMDA, which produce text that reads as if a human wrote it, have huge potential to change the way we process information online. They can be used as chatbots or to search for information, moderate online content, summarize books, or generate entirely new passages of text based on prompts. But they are also riddled with problems. It takes only a little prodding before these models start producing harmful content. The models are also extremely exclusive. They need to be trained on massive amounts of data using lots of expensive computing power, which is something only large (and mostly American) technology companies such as Google can afford. Most big tech companies developing cutting-edge LLMs restrict their use by outsiders and have not released information about the inner workings of their models. This makes it hard to hold them accountable. The secrecy and exclusivity are what the researchers working on BLOOM hope to change. Meta has already taken steps away from the status quo: in May 2022 the company released its own large language model, Open Pretrained Transformer (OPT-175B), along with its code and a logbook detailing how the model was trained. Hundreds of scientists around the world are working together to understand one of the most powerful emerging technologies before it’s too late. But Meta’s model is available only upon request, and it has a license that limits its use to research purposes. Hugging Face goes a step further. The meetings detailing its work over the past year are recorded and uploaded online, and anyone can download the model free of charge and use it for research or to build commercial applications. A big focus for BigScience was to embed ethical considerations into the model from its inception, instead of treating them as an afterthought. LLMs are trained on tons of data collected by scraping the internet. This can be problematic, because these data sets include lots of personal information and often reflect dangerous biases. The group developed data governance structures specifically for LLMs that should make it clearer what data is being used and who it belongs to, and it sourced different data sets from around the world that weren’t readily available online. The group is also launching a new Responsible AI License, which is something like a terms-of-service agreement. It is designed to act as a deterrent from using BLOOM in high-risk sectors such as law enforcement or health care, or to harm, deceive, exploit, or impersonate people. The license is an experiment in self-regulating LLMs before laws catch up, says Danish Contractor, an AI researcher who volunteered on the project and co-created the license. But ultimately, there’s nothing stopping anyone from abusing BLOOM. The project had its own ethical guidelines in place from the very beginning, which worked as guiding principles for the model’s development, says Giada Pistilli, Hugging Face’s ethicist, who drafted BLOOM’s ethical charter. For example, it made a point of recruiting volunteers from diverse backgrounds and locations, ensuring that outsiders can easily reproduce the project’s findings, and releasing its results in the open. This philosophy translates into one major difference between BLOOM and other LLMs available today: the vast number of human languages the model can understand. It can handle 46 of them, including French, Vietnamese, Mandarin, Indonesian, Catalan, 13 Indic languages (such as Hindi), and 20 African languages. Just over 30% of its training data was in English. The model also understands 13 programming languages. This is highly unusual in the world of large language models, where English dominates. That’s another consequence of the fact that LLMs are built by scraping data off the internet: English is the most commonly used language online. The reason BLOOM was able to improve on this situation is that the team rallied volunteers from around the world to build suitable data sets in other languages even if those languages weren’t as well represented online. For example, Hugging Face organized workshops with African AI researchers to try to find data sets such as records from local authorities or universities that could be used to train the model on African languages, says Chris Emezue, a Hugging Face intern and a researcher at Masakhane, an organization working on natural-language processing for African languages. Including so many different languages could be a huge help to AI researchers in poorer countries, who often struggle to get access to natural-language processing because it uses a lot of expensive computing power. BLOOM allows them to skip the expensive part of developing and training the models in order to focus on building applications and fine-tuning the models for tasks in their native languages. “If you want to include African languages in the future of [natural-language processing] … it’s a very good and important step to include them while training language models,” says Emezue. BigScience has done a “phenomenal” job of building a community around BLOOM, and its approach of involving ethics and governance from the beginning is a thoughtful one, says Percy Liang, director of Stanford's Center for Research on Foundation Models. However, Liang doesn’t think it will lead to significant changes to LLM development. “OpenAI and Google and Microsoft are still blazing ahead,” he says. Ultimately, BLOOM is still a large language model, and it still comes with all the associated flaws and risks. Companies such as OpenAI have not released their models or code to the public because, they argue, the sexist and racist language that has gone into them makes them too dangerous to use that way. BLOOM is also likely to incorporate inaccuracies and biased language, but since everything about the model is out in the open, people will be able to interrogate the model’s strengths and weaknesses, says Margaret Mitchell, an AI researcher and ethicist at Hugging Face. BigScience’s biggest contribution to AI might end up being not BLOOM itself, but the numerous spinoff research projects its volunteers are getting involved in. For example, such projects could bolster the model’s privacy credentials and come up with ways to use the technology in different fields, such as biomedical research. “One new large language model is not going to change the course of history,” says Teven Le Scao, a researcher at Hugging Face who co-led BLOOM's training. “But having one good open language model that people can actually do research on has a strong long-term impact.” When it comes to the potential harms of LLMs, “ Pandora's box is already wide open,” says Le Scao. “The best you can do is to create the best conditions possible for researchers to study them.”"
360,"Ukrainians add object detection to killer drones; YOLOv7; and a $71,000 AI audit competition - 0",http://eepurl.com/h6QTub,2022-07-18,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Battle of the generative models! Facebook introduces 'Make a Scene': …Text-to-image, with a visual guide… Facebook has revealed its own take on promptable, generative models (following companies like OpenAI: DALL-E, and Google: Imagen), with what the company calls an AI research concept named ""Make a Scene"". Make a Scene is built around using both text and visual inputs to craft the image, so you might write, for example, ""Mark Zuckerberg changing the name of Facebook to Meta"" and accompany that with a very basic drawing of a stick figure holding a paintbrush up to a sign. Facebook's 'Make a Scene' might take that prompt and render you an image that feels appropriate, using the visual stuff you added as a rough guide. The blog post and paper accompanying this release come with a bunch of nice examples that shows how this form of multimodal input makes it easier to control the generation process. ""Make-A-Scene uses a novel intermediate representation that captures the scene layout to enable nuanced sketches as input. It can also generate its own scene layout with text-only prompts, if that’s what the creator chooses. The model focuses on learning key aspects of the imagery that are more likely to be important to the creator, such as objects or animals. This technique helped increase the generation quality, as evaluated by the widely used FID score, which assesses the quality of images created by generative models,"" Facebook writes. Demo access: ""We aim to provide broader access to our research demos in the future to give more people the opportunity to be in control of their own creations and unlock entirely new forms of expression,"" Facebook writes. Why this matters: Generative models are basically 'cultures in a bottle', and each developer of a large generative model will make different choices with regard to data curation, term censorship, and so on. Eventually, many of these models will be released either commercially or as open source tools. At this point, the internet will become suffused with lots of different cultural representation-machines which will mimetically reproduce and copy themselves across the internet, forming yet another front in the culture war. Check out the blog post: Greater creative control for AI image generation (Facebook blog). Read more: Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors (arXiv)."
368,"$100k to find flaws in LLMs, NVIDIA uses RL to make better chip parts; + 256gb of law data, and a story about the cyber gerontocracy! - 2",http://eepurl.com/h6lOev,2022-07-11,"#################################################### Facebook builds a vast machine translation model and releases it as open source: …Who builds the lenses that translate across cultures, and what does it mean to be a lens builder?... Facebook has announced a project called 'No Language Left Behind' (NLLB), which consists of a family of models that can translate between 200 distinct languages, as well as an evaluation dataset for testing out the performance of each language translation. Facebook is using NLLB within its own websites to aid with translation on Facebook and Instagram, and the company has released a bunch of NLLB models for free. What's special about NLLB: There's a ton of ML translation models floating around the internet. One of the main differences here is how NLLB increases the amount of support for low-resource languages like Kamba, Lao, and a bunch of African languages. ""In total, NLLB-200’s BLEU scores improve on the previous state of the art by an average of 44 percent across all 10k directions of the FLORES-101 benchmark. For some African and Indian languages, the increase is greater than 70 percent over recent translation systems,"" Facebook writes. Why this matters: Models like NLLB are going to serve as a real world 'babelfish' to translate between different cultures. But the fact these models get trained once and deployed at vast scales means they'll likely have a significant downstream impact on culture - similar to how the early Encyclopedias described (and circumscribed) what many considered public knowledge. Facebook does acknowledge some of this by studying the potential harms and biases of the models, but I generally think the world isn't aware of how dependent foundational capabilities like translation are becoming on just a tiny number of (well intentioned) actors. Read more: 200 languages within a single AI model: A breakthrough in high-quality machine translation (Facebook blogpost). Read the research paper: No Language Left Behind: Scaling Human-Centered Machine Translation (Facebook Research). Get the models: Facebook FairSeq (GitHub)."
374,"Plan, Edit, Explain and Repeat: The PEER Collaborative Language Model Brings a Humanlike Process to Text Generation",https://syncedreview.com/2022/09/01/plan-edit-explain-and-repeat-the-peer-collaborative-language-model-brings-a-humanlike-process-to-text-generation/,2022-09-01,"While contemporary large-scale language models have excelled in text generation, these models are designed to generate only a final text and lack capabilities — such as the modifying and refining drafts — that characterize real-world collaborative writing workflows and are crucial for developing accurate and high-quality final texts. A research team from Meta AI, Carnegie Mellon University, PSL University, and University College London addresses this limitation in the new paper PEER: A Collaborative Language Model. Their proposed PEER (Plan, Edit, Explain, Repeat) collaborative language model produces texts following a humanlike process — composing drafts, adding suggestions, proposing edits and providing explanations for its actions. The team summarizes their main contributions as follows: The PEER model comprises four main steps: Plan, Edit, Explain, and Repeat. Given an input text, the user or the PEER model can first specify a plan with regard to actions to be applied. This plan is then realized via edits that the model explains using textual comments and reference citing. PEER repeats this process until it generates the desired output. For their empirical study, the team initialized all instances of PEER from an LM-Adapted T5 (Text-to-Text Transfer Transformer, Raffel et al., 2020). They compared it with baselines (OPT, GPT3, etc.) to evaluate its ability to follow plans and perform meaningful edits in domains with no available edit histories; and to examine how the PEER-Undo, PEER-Explain, and PEER-Document encoder-decoder models can boost performance. The results show that PEER can continuously improve output quality during the iterative process and achieves impressive performance across various domains and editing tasks. Overall, this work demonstrates that PEER can serve as a helpful and humanlike writing assistant that widens the scope and advances the performance of intelligent agents in producing high-quality textual outputs. The paper PEER: A Collaborative Language Model is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
375,Dynamic Memory-based Curiosity: A Bootstrap Approach for Exploration,"[{'href': 'http://arxiv.org/abs/2208.11349v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2208.11349v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-08-24 07:56:12,"2 2 0 2 g u A 2 1 ] C H . s c [ 1 v 3 1 2 6 0 . 8 0 2 2 : v i X r a What is it like to program with artiﬁcial intelligence? Advait Sarkar Microsoft Research University of Cambridge advait@microsoft.com Andrew D. Gordon Microsoft Research University of Edinburgh adg@microsoft.com Carina Negreanu Microsoft Research cnegreanu@microsoft.com Christian Poelitz Microsoft Research cpoelitz@microsoft.com Sruti Srinivasa Ragavan Microsoft Research a-srutis@microsoft.com Ben Zorn Microsoft Research ben.zorn@microsoft.com Figure 1 – Code generation using the GitHub Copilot editor extension. The portion highlighted in blue has been generated by the model. Left: a function body is generated based on a textual description in a comment. Right: a set of test cases is generated. Source: copilot.github .com Abstract Large language models, such as OpenAI’s codex and Deepmind’s AlphaCode, can generate code to solve a variety of problems expressed in natural language. This technology has already been commercialised in at least one widely-used programming editor extension: GitHub Copilot. In this paper, we explore how programming with large language models (LLM-assisted programming) is similar to, and differs from, prior conceptualisations of programmer assistance. We draw upon pub- licly available experience reports of LLM-assisted programming, as well as prior usability and design studies. We ﬁnd that while LLM-assisted programming shares some properties of compilation, pair programming, and programming via search and reuse, there are fundamental differences both in the technical possibilities as well as the practical experience. Thus, LLM-assisted programming ought to be viewed as a new way of programming with its own distinct properties and challenges. Finally, we draw upon observations from a user study in which non-expert end user programmers use LLM-assisted tools for solving data tasks in spreadsheets. We discuss the issues that might arise, and open research challenges, in applying large language models to end-user programming, particularly with users who have little or no programming expertise. 1. Introduction Inferential assistance for programmers has manifested in various forms, such as programming by demon- stration, declarative programming languages, and program synthesis (Section 2). Large language models such as GPT mark a quantitative and qualitative step-change in the automatic generation of code and nat- ural language text. This can be attributed to cumulative innovations of vector-space word embeddings, the transformer architecture, large text corpora, and pre-trained language models (Section 3). 1 These models have been commercialised in the form of APIs such as OpenAI Codex, or as programmer- facing tools such as GitHub Copilot and Tabnine. These tools function as a sort of advanced autocom- plete, able to synthesize multiple lines of code based on a prompt within the code editor, which may be natural language (e.g., a comment), code (e.g., a function signature) or an ad-hoc mixture. The capa- bilities of such tools go well beyond traditional syntax-directed autocomplete, and include the ability to synthesize entire function bodies, write test cases, and complete repetitive patterns (Section 4). These tools have reliability, safety, and security implications (Section 5). Prior lab-based and telemetric research on the usability of such tools ﬁnds that developers generally appreciate the capabilities of these tools and ﬁnd them to be a positive asset to the development expe- rience, despite no strong effects on task completion times or correctness. Core usability issues include the challenge of correctly framing prompts as well as the effort required to check and debug generated code (Section 6). Longitudinal experience reports of developers support some of the lab-based ﬁndings, while contradict- ing others. The challenges of correctly framing prompts and the efforts of debugging also appear here. However, there are many reports that these tools do in fact strongly reduce task time (i.e., speed up the development process) (Section 7). Programming with large language models invites comparison to related ways of programming, such as search, compilation, and pair programming. While there are indeed similarities with each of these, the empirical reports of the experience of such tools also show crucial differences. Search, compila- tion, and pair programming are thus found to be inadequate metaphors for the nature of LLM-assisted programming; it is a distinct way of programming with its own unique blend of properties (Section 8). While LLM-assisted programming is currently geared towards expert programmers, arguably the great- est beneﬁciaries of their abilities will be non-expert end-user programmers. Nonetheless, there are is- sues with their direct application in end-user programming scenarios. Through a study of LLM-assisted end-user programming in spreadsheets, we uncover issues in intent speciﬁcation, code correctness, com- prehension, LLM tuning, and end-user behaviour, and motivate the need for further study in this area (Section 9). 2. Prior conceptualisations of intelligent assistance for programmers What counts as ‘intelligent assistance’ can be the subject of some debate. Do we select only features that are driven by technologies that the artiﬁcial intelligence research community (itself undeﬁned) would recognise as artiﬁcial intelligence? Do we include those that use expert-coded heuristics? Systems that make inferences a human might disagree with, or those with the potential for error? Mixed-initiative systems (Horvitz, 1999)? Or those that make the user feel intelligent, assisted, or empowered? While this debate is beyond the scope of this paper, we feel that to properly contextualise the qualitative difference made by large language models, a broad and inclusive approach to the term ‘intelligence’ is required. End-user programming has long been home to inferential, or intelligent assistance. The strategy of direct manipulation (Shneiderman & Norwood, 1993) is highly successful for certain types of limited, albeit useful, computational tasks, where the interface being used (“what you see”, e.g., a text editor or an image editor) to develop an information artefact can represent closely the artefact being developed (“what you get”, e.g., a text document or an image). However, this strategy cannot be straightforwardly applied to programs. Programs notate multiple possible paths of execution simultaneously, and they deﬁne “behaviour to occur at some future time” (Blackwell, 2002b). Rendering multiple futures in the present is a core problem of live programming research (Tanimoto, 2013), which aims to externalise programs as they are edited (Basman et al., 2016). The need to bridge the abstraction gap between direct manipulation and multiple paths of execution led to the invention of programming by demonstration (PBD) (Kurlander et al., 1993; Lieberman, 2001; Myers, 1992). A form of inferential assistance, PBD allows end-user programmers to make concrete demon- strations of desired behaviour that are generalised into executable programs. Despite their promise, PBD 2 systems have not achieved widespread success as end-user programming tools, although their idea sur- vives in vestigial form as various “macro recording” tools, and the approach is seeing a resurgence with the growing commercialisation of “robotic process automation”. Programming language design has long been concerned with shifting the burden of intelligence be- tween programmer, program, compiler, and user. Programming language compilers, in translating be- tween high-level languages and machine code, are a kind of intelligent assistance for programmers. The declarative language Prolog aspired to bring a kind of intelligence, where the programmer would only be responsible for specifying (“declaring”) what to compute, but not how to compute it; that responsibility was left to the interpreter. At the same time, the language was designed with intelligent applications in mind. Indeed, it found widespread use within artiﬁcial intelligence and computational linguistics research (Colmerauer & Roussel, 1996; Rouchy, 2006). Formal veriﬁcation tools use a speciﬁcation language, such as Hoare triples (Hoare, 1969), and writing such speciﬁcations can be considered programming at a ‘higher’ level of abstraction. Program synthesis, in particular synthesis through reﬁnement, aims at intelligently transforming these rules into executable and correct code. However, the term “program synthesis” is also used more broadly, and programs can be synthesised from other sources than higher-level speciﬁcations. Concretely, program synthesis by example, or simply programming by example (PBE), facilitates the generation of executable code from input-output examples. An example of successfully commercialised PBE is Excel’s Flash Fill (Gulwani, 2011), which synthesises string transformations in spreadsheets from a small number of examples. The Cognitive Dimensions framework (T. R. Green, 1989; T. Green & Blackwell, 1998) identiﬁes three categories of programming activity: authoring, transcription, and modiﬁcation. Modern programmer as- sistance encompasses each of these. For example, program synthesis tools transform the direct authoring of code into the (arguably easier) authoring of examples. Intelligent code completions (Marasoiu et al., 2015) support the direct authoring of code. Intelligent support for reuse, such as smart code copy/paste (Allamanis & Brockschmidt, 2017) support transcription, and refactoring tools (Hermans et al., 2015) support modiﬁcation. Researchers have investigated inferential support for navigating source code (Hen- ley & Fleming, 2014), debugging (J. Williams et al., 2020), and selectively undoing code changes (Yoon & Myers, 2015). Additionally, intelligent tools can also support learning (Cao et al., 2015). Allamanis et al. (2018) assemble a literature review of research—at the intersection of machine learning, programming languages, and software engineering—that seeks to adapt methods ﬁrst developed for natural language, such as language models, to source code. The emergence of large bodies of open source code, sometimes called “big code”, enabled this research area. Language models are sensitive to lexical features like names, code formatting, and order of methods, while traditional tools like compilers or code veriﬁers are not. The authors hypothesise that sensitivity to lexical features matters for software engineering: The naturalness hypothesis. Software is a form of human communication; software corpora have similar statistical properties to natural language corpora; and these properties can be exploited to build better software engineering tools. The earliest evidence for this hypothesis goes back to research that used n-gram models to build a code completion engine for Java that outperformed Eclipse’s completion feature (Hindle et al., 2012, 2016). The survey covers the methods and the applications they enable: recommender systems (such as code autocompletion), debuggers, code analysers (such as type checkers (Raychev et al., 2015)), and code synthesizers. These applications constitute intelligence assistance to programmers, but limited by the capabilities of the underlying language models. We can expect the recent dramatic expansion in capability of language models, which we discuss next, to magnify the effectiveness of these applications. 3. A brief overview of large language models for code generation 3.1. The transformer architecture and big datasets enable large pre-trained models In the past decade, natural language processing has evolved both in the development of language models (LMs) as well as tasks and evaluation. Mikolov et al. (2013) introduced Word2Vec, where vectors are as- 3 signed to words such that similar words are grouped together. This is done by looking at co-occurrences in free text (like Wikipedia articles) and ignores the fact that words have multiple meanings depend- ing on context. Long short-term memory (LSTM) neural networks (Hochreiter & Schmidhuber, 1997; Sutskever et al., 2014) and later encoder-decoder networks, account for order in an input sequence. Self- attention (Vaswani et al., 2017) signiﬁcantly simpliﬁed the prior networks by replacing each element in the input by a weighted average of the rest of the input. Transformers combined the advantages of (multi-head) attention and word embeddings, enriched with positional encodings (they add the order information to the word embeddings) into one architecture. While there are many alternatives to trans- formers for language modelling, in this paper when we mention a language model (LM) we will usually imply a transformer-based language model. There are large collections of unlabelled text data for the most common natural languages. For example, the Common Crawl project1 produces around 20 TB of text data (from web pages) monthly, but labelled task-speciﬁc data is less prevalent. This makes unsupervised training appealing which leads to the concept of pre-trained LMs (J. Li et al., 2021). Pre-trained LMs are commonly trained to perform next- word prediction (such as GPT models, e.g. (Brown et al., 2020)) where the model is trained to predict the next word in a sequence or masked (such as Bert, e.g. (Devlin et al., 2019)) where the model is trained to ﬁll a gap in a sequence. Ideally, pre-trained LMs learn general-purpose abilities and knowledge by seeing large amounts of text, which can then be transferred to downstream language tasks (where we have less labelled data) such as question answering, ﬁction generation, text summarisation, etc. Fine-tuning is the process of adapting a given pre-trained LM to different downstream tasks by introducing additional parameters and training them using task-speciﬁc objective functions. In certain cases the pre-training objective also gets adjusted to better suit the downstream task. Instead of (or on top of) ﬁne-tuning, the downstream task can be reformulated to be similar to the original LLM training. In practice, this means expressing the task as a set of instructions to the LLM via a prompt. So the goal, rather than deﬁning a learning objective for a given task, is to ﬁnd a way to query the LLM to directly predict for the downstream task. This is sometimes referred to as Pre-train, Prompt, Predict.2 3.2. Language models tuned for source code generation The downstream task of interest to us in this paper is code generation, where we provide snippets of code (including comments) and we want new code to be generated. Unlike other downstream tasks, a large corpus of data is available from public code repositories such as GitHub. Code generation can be divided into many sub-tasks, such as type decorators (variable type generation, e.g. (J. Wei et al., 2020)), code summarization (comment generation, e.g. (Liu et al., 2021)), clone detection (duplicate detection, e.g (Mou et al., 2016)), code translation (code migration from one language to another e.g. (Nguyen et al., 2015)) etc. A recent benchmark that covers many tasks is CodeXGLUE (Lu et al., 2021). LLM technology has brought us within reach of full-solution generation. Codex (Chen, Tworek, Jun, Yuan, Ponde, et al., 2021), a version of GPT-3 ﬁne-tuned for code generation, can solve in one generation on average 47/164 problems in the HumanEval code generation benchmark. HumanEval is a set of 164 hand-written programming problems, which include a function signature, docstring, body, and several unit tests, with an average of 7.7 tests per problem. Smaller models have followed Codex, like GPT-J3 (ﬁne-tuned on top of GPT-2), CodeParrot4 (also ﬁne-tuned on top of GPT-2, targets Python generations), PolyCoder (Xu, Alon, et al., 2022)(GPT-2 style but trained directly on code). LLMs comparable in size to Codex include AlphaCode (Y. Li et al., 2022a) and PaLM-Coder (Chowd- hery et al., 2022). AlphaCode is trained directly on GitHub data and ﬁne-tuned on coding competition problems. It introduces a method to reduce from a large number of potential solutions (up to millions) 1https://commoncrawl.org/ 2http://pretrain.nlpedia.ai/ 3https://huggingface.co/docs/transformers/main/model_doc/gptj 4https://huggingface.co/blog/codeparrot 4 Figure 2 – Code generation using the GitHub Copilot editor extension. The portion highlighted in blue has been generated by the model. Above: a repetitive time computation is extrapolated based on two examples. Below: function body is generated from the signature and the ﬁrst line. Source: copilot.github.com to a handful of candidates (competitions permit a maximum of 10). On a dataset of 10000 programming problems, if given 5 attempts Codex manages to solve around 3% of the problems versus AlphaCode which manages around 4-7%. In competitions for which it was ﬁne-tuned (CodeContests) AlphaCode manages a 34% success rate, on par with the average human competitor. Despite promising results there are known shortcomings. Models can directly copy code (full solutions or key parts of the solutions) from the training data, rather than generating new code. Though developers make efforts to clean and retain only high-quality code, there are no guarantees of correctness and errors can be directly propagated through generations. Codex can also produce syntactically incorrect or undeﬁned code, and can invoke functions, variables, and attributes that are undeﬁned or outside the scope of the codebase. Moreover, Codex struggles to parse through increasingly long and higher-level or system-level speciﬁcations which can lead to mis- takes in binding operations to variables (especially when the number of operations and variables in the docstring is large). Various approaches have been explored to ﬁlter out bad generations or repair them (especially for syntax errors). Consistency is another issue and there is a trade-off between non-determinism and generation diversity. Some parameter settings can control the diversity of generation (i.e., how diverse the different gener- ations for a single prompt might be), but there is no guarantee that we will get the same generation if we run the system at different times under the same settings. To alleviate this issue in measurements, metrics such as pass@k (have a solution that passes the tests within k tries) have been modiﬁed to be probabilistic. 4. Commercial programming tools that use large language models OpenAI Codex is a version of GPT that is ﬁne-tuned on publicly available source code (Chen, Tworek, Jun, Yuan, de Oliveira Pinto, et al., 2021). While Codex itself is not a programmer-facing tool, OpenAI has commercialised it in the form of an API that can be built upon. 5 Figure 3 – Code generation using the Tabnine editor extension. The grey text after the cursor is being suggested by the model based on the comment on the preceding line. Source: tabnine.com Figure 4 – API suggestion using the Visual Studio IntelliCode feature. Source: Silver (2018) The principal commercial implementation of Codex thus far has been in Github Copilot.5 Copilot is an extension that can be installed into code editors such as Neovim, Jetbrains, and Visual Studio Code. Copilot uses Codex, drawing upon the contents of the ﬁle being edited, related ﬁles in the project, and ﬁle paths or URLs of repositories. When triggered, it generates code at the cursor location, in much the same way as autocomplete. To help expand developer expectations for the capabilities of Copilot beyond the previous standard uses of autocomplete, suggested usage idioms for Copilot include: writing a comment explaining what a function does, and the function signature, and allowing Copilot to complete the function body; complet- ing boilerplate code; and deﬁning test cases (Figures 1 and 5). Programmers can cycle between different generations from the model, and once a particular completion has been accepted it can be edited like any other code. As of 23 June 2022, Amazon has announced a Copilot-like feature called CodeWhisperer,6 which also applies a large language model trained on a corpus of source code to generate autocompletions based on comments and code. The marketing material describes a set of safety features, such as: detecting when generated code is similar to code in the training set, detecting known security vulnerabilities in the generated code, and “removing code that may be considered biased and unfair” (although this latter claim induces skepticism). At present CodeWhisperer is not widely available and thus little is known of its use in practice. Other commercial implementations of AI-assisted autocompletion features include Visual Studio Intel- 5https://copilot.github.com/ 6https://aws.amazon.com/codewhisperer/features/ 6 licode (Silver, 2018) (Figure 4) and Tabnine (Figure 3)7. These are more limited in scope than Copilot and their user experience is commensurable to that of using ‘traditional’ autocomplete, i.e., autocom- plete that is driven by static analysis, syntax, and heuristics.8.The structure of the machine learning model used by these implementations is not publicly disclosed; however, both rely on models that have been trained on large corpora of publicly available source code. It is interesting to note, that despite the wide variety of types of intelligent programmer assistance we have discussed in Section 2 for several aspects of programming (authoring, transcription, modiﬁcation, debugging, and learning), commercial implementations of assistance based on large language models thus far are aimed primarily at authoring. Authoring can be viewed as the ﬁrst natural application of a generative language model, but the programming knowledge in these models can of course be used for assisting programmers in other activities, too. 5. Reliability, safety, and security implications of code-generating AI models AI models that generate code present signiﬁcant challenges to issues related to reliability, safety, and security. Since the output of the model can be a complex software artifact, determining if the out- put is “correct” needs a much more nuanced evaluation than simple classiﬁcation tasks. Humans have trouble evaluating the quality of software, and practices such as code review, applying static and dy- namic analysis techniques, etc., have proven necessary to ensure good quality of human-written code. Current methods for evaluating the quality of AI-generated code, as embodied in benchmarks such as HumanEval (Chen, Tworek, Jun, Yuan, de Oliveira Pinto, et al., 2021), MBPP (Austin et al., 2021), and CodeContests (Y. Li et al., 2022b), determine functional correctness of entire functions based on a set of unit tests. Such evaluation approaches fail to consider issues of code readability, completeness, or the presence of potential errors that software developers constantly struggle to overcome. Previous work (Chen, Tworek, Jun, Yuan, de Oliveira Pinto, et al., 2021) explores numerous implications of AI models that generate code, including issues of over-reliance, misalignment (the mismatch between what the user prompt requests and what the user really wants), bias, economic impact, and security implications. While these topics each are extensive and important, due to space limitations we only brieﬂy mention them here and point to additional related work when possible. Over-reliance occurs when individuals make optimistic assumptions about the correctness of the output of an AI model, leading to harm. For code generating models, users may assume the code is correct, has no security vulnerabilities, etc. and those assumptions may lead to lower quality or insecure code being written and deployed. Existing deployments of AI models for code, such as GitHub Copilot (Ziegler, 2021), have documentation that stresses the need to carefully review, test, and vet generated code just as a developer would vet code from any external source. It remains to be seen if over-reliance issues related to AI code generation will result in new software quality challenges. Since AI that generates code is trained on large public repositories, there is potential for low-quality training data to inﬂuence models to suggest low-quality code or code that contains security vulnera- bilities. One early study of GitHub Copilot (Pearce et al., 2021) examines whether code suggestions may contain known security vulnerabilities in a range of scenarios and ﬁnds cases where insecure code is generated. Beyond carefully screening new code using existing static and dynamic tools that detect security vulnerabilities in human-generated code, there are also possible mitigations that can reduce the likelihood that the model will make such suggestions. These include improving the overall quality of the training data by removing low-quality repositories, and ﬁne-tuning the large-language model speciﬁcally to reduce the output of known insecure patterns. 6. Usability and design studies of AI-assisted programming Vaithilingam et al. (2022) conducted a within-subjects comparative study (n=24) of Github Copilot, 7https://www.tabnine.com/ 8As of 15 June 2022, Tabnine has announced a shift to language model-driven autocompletion that more closely resembles the abilities of Copilot (Weiss, 2022). 7 comparing its user experience to that of traditional autocomplete (speciﬁcally, the Intellisense plugin, not the same as the Intellicode feature mentioned previously). Participants failed to complete the tasks more often with Copilot than with Intellisense, and there was no signiﬁcant effect on task completion time. Perhaps unsurprisingly, the authors ﬁnd that assessing the correctness of generated code is difﬁcult and an efﬁciency bottleneck, particularly when the code generated has a fundamental ﬂaw or inefﬁciency that leads the programmer on an ultimately unsuccessful ‘wild goose chase’ of repair or debugging. However, the overwhelming majority (19 of 24) of participants reported a strong preference for Copilot in a post-task survey. While participants were less conﬁdent about the code generated by Copilot, they almost universally (23 of 24) perceived it as more helpful, because it had the potential for generating useful starting points and saving the programmer the effort of searching online for documented solutions that could be the basis for reuse. Ziegler et al. (2022) conducted a survey (n=2,047) of the perceived productivity of Copilot users in the USA. They matched these to telemetric usage measurements of the Copilot add-in, which included metrics such as how often an auto-completion was shown, how often it was accepted, how often it per- sisted unchanged in the document for a certain time period, how often it persisted with minor variations (e.g., measured by Levenshtein distance) and so on. They ﬁnd that the acceptance rate (the ratio of ac- cepted suggestions to shown suggestions) is the strongest predictor of users’ perceived productivity due to Copilot. Fascinatingly, they ﬁnd that the pattern of acceptance rates for all users in aggregate follows a daily and weekly “circadian” rhythm, such that users are more likely to accept Copilot completions out of working-hours and on weekends. However, for any given user, the acceptance rate depends on that user’s normal working hours; suggestions outside of normal working hours are less likely to be accepted. Future work is needed to see whether this ﬁnding replicates, and if so to establish how and why acceptance rates are so signiﬁcantly affected by working hours. Xu, Vasilescu, & Neubig (2022) conducted a within-subjects study (n=31) comparing the programming experience with and without a code generation plugin. Their experimental plugin takes the form of a text ﬁeld in which the user enters a natural language prompt, the system responds with a list of code snippets, and when clicked the desired snippet is inserted at the cursor. This workﬂow differs from Copilot’s, where the ‘prompt’ is text within the source ﬁle, and can contain a mix of natural language comments and code. The plugin supported both code generation (using a tree-based neural network) and code snippet retrieval (searching the programming forum Stack Overﬂow). Results from both generation and retrieval are shown in the same list, but visually demarcated. The authors found no signiﬁcant effect of the plugin on task completion time or program correctness. They found that simple queries were more likely to be answered correctly through generation, and more complex queries requiring multiple steps were more likely to be answered correctly though retrieval, and that it was possible to predict which approach would succeed based on the word content of the queries. Further, they found that most (60%) natural language queries that participants wrote in their experiment were not sufﬁciently well-speciﬁed for a human expert to write code implementing those intents. Retrieved snippets were edited more often than generated snippets, mostly to rename identiﬁers and choose different parameters. In a post- experiment survey, participants reported mostly feeling neutral or somewhat positive (30 of 31). These participants felt that the plugin was helpful for ﬁnding snippets they were aware of but cannot recall, and less disruptive than using a browser, but the interaction worked better when the developer had a pre-existing knowledge of the target APIs and frameworks, and it took experimentation to understand the “correct way” to formulate queries. There was no clear indication of preference between retrieval and generation. Jiang et al. (2022) developed an LLM-based tool for converting natural language statements to code. As in Xu, Vasilescu, & Neubig (2022), prompts are entered in a pop-up dialog invoked at the cursor from within a code editor, rather than as comments. In a study (n = 14), participants were given a week to complete two website-building tasks with the tool, while recording the screen, and were interviewed afterwards. As in other studies, participants saw utility in the tool for facilitating quick API lookups and for writing boilerplate code. They found that novice programmers’ queries were mainly natural 8 for Figure 5 – Searching for code snippets using Bing Developer Assistant. Stack Overﬂow is shown. Note how the query “generate md5 hash from string @line” contains a hint about the identiﬁer line, which is used to rewrite the retrieved snippet. Source: https://www.microsoft.com/en-us/research/publication/ building-bing-developer-assistant/ A result language, whereas experts were more likely to mix code into their requests. While some queries were abstract, and expressed high-level goals, most had low granularity, being “roughly equivalent to a line of code”. To cope with model failures, participants used a variety of strategies to reword their query, such as reducing the scope of the request or replacing words with alternatives, but no particular strategy was observed to be more effective than any other. Participants struggled with forming a mental model of what the model can understand and the “syntax” of the language it required – this is precisely the fuzzy abstraction matching problem we described earlier, which the authors call an “uncanny valley”. The authors suggest possible solutions such as automated rewording of prompts, suggesting simpler tasks, suggesting task breakdowns, and better onboarding and tutorials. Barke et al. (2022) studied how programmers (n = 20) use GitHub Copilot to complete short program- ming tasks in Python, Rust, Haskell, and Java. Through analysis of screen recordings, the authors identifed two primary modes of interaction with Copilot: acceleration, where the programmer has a well-formed intent and Copilot speeds up code authoring in “small logical units”, and exploration, where Copilot suggestions are used to assist the planning process, “help them get started, suggest potentially useful structure and API calls, or explore alternative solutions”. In acceleration, long code suggestions, which take time to read and evaluate, can break the programmer’s ﬂow. Participants developed heuristics for quickly scanning suggestions, such as looking for the presence of certain keywords. In exploration, participants were more likely to prompt using purely natural language comments, rather than a mix of comments and code. Moreover, these prompt comments were often ‘cleaned’ subsequent to accepting a suggestion, which implies a form of ‘instruction language’ that is separate from ‘explanation language’. The Bing Developer Assistant (Y. Wei et al., 2015; Zhang et al., 2016) (also referred to as Bing Code Search) was an experimental extension for Visual Studio initially released in 2015. It enabled an in-IDE, identiﬁer-aware search for code snippets from forums such as Stack Overﬂow. It had the ability to rewrite retrieved code to use identiﬁers from the programmer’s current ﬁle. A user study (n=14) comparing task time in performing 45 short programming tasks with the extension versus regular web search found on average 28% of time was saved with the extension. Morever telemetry data gathered over three weeks (representing around 20,000 users and around 3,000 queries per day) showed that several programmers used the feature frequently. Some used it repeatedly for related problems in quick succession, showing its use in multi-step problems. Others issued the same query multiple times on separate days, suggesting 9 that the speed of auto-completion was useful even if the programmer knew the solution. 7. Experience reports At present, there is not a lot of research on the user experience of programming with large language models beyond the studies we have summarised in Section 6. However, as the availability of such tools increases, professional programmers will gain long-term experience in their use. Many such program- mers write about their experiences on personal blogs, which are then discussed in online communities such as Hacker News. Inspired by the potential for these sources to provide rich qualitative data, as pointed out by Barik (Barik et al., 2015; Sarkar et al., 2022), we draw upon a few such experience reports. A full list of sources is provided Appendix A; below we summarise their key points. 7.1. Writing effective prompts is hard As with several other applications of generative models, a key issue is the writing of prompts that in- crease the likelihood of successful code generation. The mapping that these models learn between natural language and code is very poorly understood. Through experimentation, some have developed heuristics for prompts that improve the quality of the code generated by the model. One developer, after building several applications and games with OpenAI’s code-davinci model (the second generation Codex model), advises to “number your instructions” and creating “logic ﬁrst” before UI elements. Another, in using Copilot to build a classiﬁer for natural language statements, suggests to provide “more detail” in response to a failure to generate correct code. For example, when asking Copilot to “bina- rize” an array fails, they re-write the prompt to “turn it into an array where [the ﬁrst value] is 1 and [the second value] is 0” – effectively pseudocode – which generates a correct result. Commenters on Hacker News are divided on the merits of efforts invested in developing techniques for prompting. While some see it as a new level of abstraction for programming, others see it as indirectly approaching more fundamental issues that ought to be solved with better tooling, documentation, and language design: “You’re not coding directly in the language, but now you’re coding in an implicit language provided by Copilot. [...] all it really points out is that code documentation and discovery is terrible. But I’m not for sure writing implicit code in comments is really a better approach than seeking ways to make discovery of language and library features more discoverable.” “[...] the comments used to generate the code via GitHub Copilot are just another very inefﬁcient programming language.” “[Responding to above] There is nonetheless something extremely valuable about being able to write at different levels of abstraction when developing code. Copilot lets you do that in a way that is way beyond what a normal programming language would let you do, which of course has its own, very rigid, abstractions. For some parts of the code you’ll want to dive in and write every single line in painstaking detail. For others [...] [Copilot] is maybe enough for your purposes. And being able to have that ability, even if you think of it as just another programming language in itself, is huge.” Being indiscriminately trained on a corpus containing code of varying ages and (subjective) quality has drawbacks; developers encounter generated code which is technically correct, but contains practices considered poor such as unrolled loops and hardcoded constants. One Copilot user found that: “Copilot [...] has made my code more verbose. Lines of code can be liabilities. Longer ﬁles to parse, and more instances to refactor. Before, where I might have tried to consolidate an API surface, I ﬁnd myself maintaining [multiple instances].” Another Copilot user reﬂected on their experience of trying to generate code that uses the fastai API, which frequently changes: “[...] since the latest version of fastai was only released in August 2020, GitHub Copilot was not able to provide any relevant suggestions and instead provided code for using older versions of fastai. [...] To me, this is a major concern [...] If we are using cutting edge tools [...] Copilot has no knowledge of this and cannot provide useful suggestions.” On the other hand, developers can also be exposed to better practices and APIs through these models. The developer that found Copilot to make their code more verbose also observed that: 10 “Copilot gives structure to Go errors . [...] A common idiom is to wrap your errors with a context string [which can be written in an inconsistent, ad-hoc style] [...] Since using Copilot, I haven’t written a single one of these error handling lines manually. On top of that, the suggestions follow a reasonable structure where I didn’t know structure had existed before. Copilot showed me how to add structure in my code in unlikely places. For writing SQL, it helped me write those annoying foreign key names in a consistent format [...] [Additionally,] One of the more surprising features has been [that] [...] I ﬁnd myself discovering new API methods, either higher-level ones or ones that are better for my use case.” In order to discover new APIs, of course, the APIs themselves need to be well-designed. Indeed, in some cases the spectacular utility of large language models can be largely attributed to the fact that API designers have already done the hard work of creating an abstraction that is a good ﬁt for real use cases (Myers & Stylos, 2016; Piccioni et al., 2013; Macvean et al., 2016). As a developer who used Copilot to develop a sentiment classiﬁer for Twitter posts matching certain keywords remarks, “These kinds of things are possible not just because of co pilot [sic] but also because we have awesome libraries which have abstracted a lot of tough stuff.” This suggests that API design, not just for human developers but also as a target for large language models, will be important in the near and mid-term future. Moreover, breaking down a prompt at the ‘correct’ level of detail is also emerging as an important developer skill. This requires at least some familiarity, or a good intuition, for the APIs available. Breaking down prompts into steps so detailed that the programmer is effectively writing pseudocode, can be viewed as an anti-pattern, and can give rise to the objections cited earlier that programming via large language models is simply a “very inefﬁcient programming language”. We term this the problem of fuzzy abstraction matching. The problem of ﬁguring out what the system can and can’t do, and matching one’s intent and instructions with the capabilities of the system, is not new – it has been well-documented in natural language interaction (Mu & Sarkar, 2019; Luger & Sellen, 2016). It is also observed in programming notation design as the ‘match-mismatch’ hypothesis (T. R. Green & Petre, 1992; Chalhoub & Sarkar, 2022). In the broadest sense, these can be seen as special cases of Norman’s “gulf of execution” (Hutchins et al., 1985), perhaps the central disciplinary problem of ﬁrst and second- wave (Bødker, 2015) human-computer interaction research: ‘how do I get the computer to do what I want it to do?’. What distinguishes fuzzy abstraction matching from previous incarnations of this problem is the re- silience to, and accommodation of, various levels of abstraction afforded by large language models. In previous natural language interfaces, or programming languages, the user needed to form an extremely speciﬁc mental model before they could express their ideas in machine terms. In contrast, large lan- guage models can generate plausible and correct results for statements at an extremely wide range of abstraction. In the context of programming assistance, this can range from asking the model to write programs based on vague and underspeciﬁed statements, requiring domain knowledge to solve, through to extremely speciﬁc and detailed instructions that are effectively pseudocode. This ﬂexibility is ulti- mately a double-edged sword: it has a lower ﬂoor for users to start getting usable results, but a higher ceiling for getting users to maximum productivity. In the context of programming activities, exploratory programming, where the goal is unknown or ill- deﬁned (Kery & Myers, 2017; Sarkar, 2016), does not ﬁt the framing of fuzzy abstraction matching (or indeed any of the variations of the gulf of execution problem). When the very notion of a crystallised user intent is questioned, or when the design objective is for the system to inﬂuence the intent of the user (as with much designerly and third-wave HCI work), the fundamental interaction questions change. One obvious role the system can play in these scenarios is to help users reﬁne their own concepts (Kulesza et al., 2014) and decide what avenues to explore. Beyond noting that such activities exist, and fall outside the framework we have proposed here, we will not explore them in greater detail in this paper. 7.2. The activity of programming shifts towards checking and unfamiliar debugging When code can be generated quickly, as observed with the studies in Section 6, checking the correctness of generating code becomes a major bottleneck. This shift, or tradeoff, of faster authoring at the expense 11 of greater time spent checking code, is not without criticism. For some it is the wrong balance of priorities between system and programmer. Correspondingly, some users have developed heuristics for when the cost of evaluating the correctness of the code is greater than the time or effort saved by code generation, such as to focus on very short (e.g., single line) completions and ignore longer completions. Furthermore, some users have found that rather than having suggestions show all the time, which can be distracting and time consuming, more intentional use can be made of Copilot by switching off auto- suggestion and only triggering code completion manually using a keyboard shortcut. However, this requires users to form a mental model of when Copilot is likely to help them in their workﬂow. This mental model takes time and intentionality to build, and may be incorrect. Moreover, it introduces a new cognitive burden of constantly evaluating whether the current situation would beneﬁt from LLM assistance. Commenters on Hacker News raise these issues: “I ﬁnd I spend my time reviewing Copilot suggestions (which are mostly wrong) rather than thinking about code and actually doing the work.” “[...] It’s much quicker to read code than to write it. In addition, 95% of Copilots suggestions are a single line and they’re almost always right (and also totally optional).[...] I admit that I’m paranoid every time it suggests more than 2 lines so I usually avoid it. [...] I’ve run into Copilot induced headaches twice. Once was in the ﬁrst week or so of using it. I sweared off [sic] of using it for anything more than a line then. Eventually I started to ease up since it was accurate so often and then I learned my second lesson with another mistake. [...]” “[...] writing code is not the bottleneck in need of optimization. Conceiving the solution is. Any time “saved” through Copilot and it’s ilk is immediately nulliﬁed by having to check it’s correctness. [...]” “What I want is a copilot that ﬁnds errors [...] Invert the relationship. I don’t need some boilerplate generator, I need a nitpicker that’s smarter than a linter. I’m the smart thinker with a biological brain that is inattentive at times. Why is the computer trying to code and leaving mistake catching to me? It’s backwards.” “I turned off auto-suggest and that made a huge difference. Now I’ll use it when I know I’m doing something repetitive that it’ll get easily, or if I’m not 100% sure what I want to do and I’m curious what it suggests. This way I get the help without having it interrupt my thoughts with its suggestions.” Another frequent experience is that language models can introduce subtle, difﬁcult to detect bugs, which are not the kind that would be introduced by a human programmer writing code manually. Thus, existing developer intuitions around the sources of errors in programs can be less useful, or even misleading, when checking the correctness of generated code. One developer reported their experience of having an incorrect, but plausible-sounding ﬁeld name sug- gested by Copilot (accessTokenSecret instead of accessSecret) and the consequent wild goose chase of debugging before discovering the problem. As sources of error, these tools are new, and developers need to learn new craft practices for debugging. “There are zero places that can teach you those things. You must experience them and unlock that kind of knowledge.”, the developer con- cludes, “Don’t let code completion AI tools rule your work. [...] I don’t blame [Copilot] for this. I blame myself. But whatever. At least I got some experience.”. Commenters on Hacker News report similar experiences: “[...] The biggest problem I’ve had is not that it doesn’t write correctly, it’s that it think it knows how and then produce good looking code at a glance but with wrong logic. [...]” “[...] it has proved to be very good at producing superﬁcially appealing output that can stand up not only to a quick scan, but to a moderately deep reading, but still falls apart on a more careful reading. [...] it’s an uncanny valley type effect. [...] it’s almost the most dangerous possible iteration of it, where it’s good enough to fool a human functioning at anything other than the highest level of attentiveness but not good enough to be correct all the time. See also, the dangers of almost self-driving cars; either be self-driving or don’t but don’t expect halfway in between to work well.” 12 “[...] The code it generates _looks_ right but is usually wrong in really difﬁcult to spot ways but things you’d never write yourself.” Many developers reported concerns around such tools repeating private information, or repeating copy- righted code verbatim, which might have implications for the licenses in their own projects. Notions of the dangers of such “stochastic parrots” (Bender et al., 2021) are not new and have been well-explored, and are not as directly connected to the user experience of programming assistance as some of the other concerns we have listed here. As such, we will not enter that discussion in depth here, except to mention that these concerns were present in several blog articles and online discussions. Thus, in practice, programmers describe the challenges of writing effective prompts, misinterpreted intent, code that includes subtle bugs or poor programming practices, the burden of inspecting and checking that generated code is correct, and worries about private information, plagiarism and copyright. 7.3. These tools are useful for boilerplate and code reuse Despite the challenges we have described so far in this section, the utility of these tools in certain contexts is undeniable, and some programmers report having developed workﬂows, in certain contexts, that are heavily dependent on AI assistance. Particularly for simple tasks that require a lot of “boilerplate” code, or common tasks for which there are likely to be snippets of code online which prior to these AI assistants would have required a web search to retrieve. Hacker News commenters write: “These days not having Copilot is a pretty big productivity hit to me. The other day Copilot somehow stopped offering completions for maybe an hour, and I was pretty shocked to realize how much I’ve grown to rely on just hitting tab to complete the whole line. (I was writing Go at the time which is on the boilerplatey side among the mainstream languages, so Copilot is particularly effective [...]” “I use GTP-3 codex [sic] daily when working. It saves me time, helps me explore unfamiliar lan- guages and APIs and generates approaches to solve problems. It can be shockingly good at coding in narrow contexts. It would be a mistake to miss the developments happening in this area” “[...] for a lot of quick programming questions, I’m ﬁnding I don’t even need a search engine. I just use Github Copilot. For example, if I wanted to remember how to throw an exception I’d just write that as a comment and let Copilot ﬁll in the syntax. Between that and ofﬁcial docs, don’t need a ton else.” “[...] It’s changing the way I write code in a way that I can already tell is allowing me to be much lazier than I’ve previously been about learning various details of languages and libraries. [...]” “[...] Github Copilot [...] pretty much replaced almost my entire usage of Stack Overﬂow.[...]” “[...] GitHub Copilot really shines in rote work: when it can correctly infer what you are about to do, it can and will assist you correctly. It’s not able to make big decisions, but in a pinch, it might be able to give hints. [...] If used right, Copilot can give developers a signiﬁcant velocity boost, especially in greenﬁeld projects where there is lots and lots of boilerplate to write. [...]” 8. The inadequacy of existing metaphors for AI-assisted programming 8.1. AI assistance as search In research studies, as well as in reports of developer experiences, comparisons have been drawn between the nature of AI programming assistance and programming by searching and reusing code from the Internet (or from institutional repositories, or from the same project, or from a developer’s previous projects). The comparison between AI programming assistance and search is a natural one, and there are many similarities. Superﬁcially, both have a similar starting point: a prompt or query that is predominantly natural language (but which may also contain code snippets). From the user perspective, both have an information asymmetry: the user does not know precisely what form the result will take. With both search and AI assistance, for any given query, there will be several results, and the user will need to invest time evaluating and comparing them. In both cases, the user may only get an inexact solution, or indeed nothing like what they want, and the user may need to invest time adapting and repairing what they get. 13 However, there are differences. When searching the web, programmers encounter not just code, but a variety of types of results intermingled and enmeshed. These include code snippets interspersed with human commentary, perhaps discussions on forums such as Stack Overﬂow, videos, and images. A search may return new APIs or libraries related to the query, thus showing results at different levels of abstraction. Search has signals of provenance: it is often (though not always) possible to determine the source of a code snippet on the web. There is a lot of information scent priming to assist with the information foraging task (Srinivasa Ragavan et al., 2016). In this way, programming with search is a mixed media experience. In contrast, programming with large language models can be said to be a ﬁxed media experience. The only output is tokens (code, comments, and data) that can be represented within the context of the code editor. This has some advantages: the increased speed of code insertion (which is the immediate aim) often came up in experience reports. However, the learning, exploration, and discovery, and access to a wide variety of sources and media types that occurs in web search is lost. Provenance, too is lost: it is difﬁcult to determine whether the generation is original to the model, or a stochastic parroting (Bender et al., 2021; Ziegler, 2021). Moreover, due to privacy, security, and intellectual property concerns, the provenance of code generated by large language models may be withheld or even destroyed (Sarkar, 2022). This suggests that in future assistance experiences, mixed-media search might be integrated into programmer assistance tools, or the models themselves might be made capable of generating more types of results than the simple code autocomplete paradigm of current tools. 8.2. AI assistance as compilation An alternative perspective is that AI assistance is more like a compiler. In this view, programming through natural language prompts and queries is a form of higher-level speciﬁcation, that is ‘compiled’ via the model to the source code in the target language, which is lower level. Let us (crudely) assume that as programming notations travel along the abstraction continuum from ‘lower’ to ‘higher’ levels, the programmer becomes, ﬁrstly, less concerned with the mechanistic details of program execution, and secondly, more and more declarative, specifying what computation is required rather than how to compute it. In general, these are desirable properties of programming notations, but they do not always make the activity of programming easier or more accessible. As people who write code in declarative languages or formal veriﬁcation tools will tell you, it’s often much more difﬁcult to specify the what than the how. The much more broadly adopted practice of test-driven development is adjacent; while tests are not necessarily written in a higher-level language than the code, they aim to capture a higher-level notion of correctness, the what of the problem being solved. Learning to be a test engineer takes time and experience, and the entire distinct career path of “software engineer in test” attests to the specialised requirements of programming at higher levels of abstraction. Some would draw a distinction between programming in a speciﬁcation language and a compiled pro- gramming language. Tony Hoare himself considers these different, on the grounds that while a compiler only aims to map a program from the source language into a ﬁnite set of valid programs in the target language, a speciﬁcation might be satisﬁed by an inﬁnite number of valid programs (pers comm., ﬁrst author, ca. 2014). Thus the technical and interaction design problems of programming through speciﬁ- cation reﬁnement encompasses, but is much broader than, the technical and interaction design problems of compilers. While we acknowledge this distinction, there is insufﬁcient empirical evidence from the experience reports summarised in Section 7 that working programmers themselves consistently make a meaningful distinction between these concepts. Programming with large language models, like in a higher-level notation, also allows the programmer to be less concerned with details of the target language. For example, developers in our experience reports relied on AI assistance to ﬁll in the correct syntax, or to discover and correctly use the appropriate API call, thus allowing them to focus on higher-level aspects of the problem being solved. However, there are fundamental differences between this experience and the experience of using a compiler. First, the abstraction is not complete, i.e., a programmer cannot completely be unaware of the target language, they 14 must still be able to understand and evaluate the generated code in order to use such tools effectively. With compilers, although knowledge of the target language can help experienced developers in certain circumstances, it is far from a prerequisite for effective usage. Moreover, compilers can be relied on almost universally to generate a correct and complete translation from source to target language, whereas programming with AI assistance involves the active checking and adaptation of translated code. Next, compilers are (comparatively) deterministic, in that they consistently produce the same output for the same input, but this is not the case for current AI programming tools (although this is not a fundamental limitation, and consistency can be enforced). Finally, though they are often criticised for being cryptic and unhelpful (Barik et al., 2018), compilers do offer levels of interaction and feedback through warnings and error messages, which help the programmer improve the code in the source language; there is currently no such facility with AI programming tools and this strikes us as an area with potential for innovation. Perhaps more profoundly, while natural language can be used to express concepts at a higher abstraction level, the range of abstraction expressible in natural language is much wider than with other forms of programming notation. Traditional programming notations with ad-hoc abstraction capabilities (subrou- tines, classes, etc.) allow programmers to manually raise the level of abstraction of their own code and APIs. But with code generated by language models, as we have seen from the reports in Section 7, a prompt can span the gamut from describing an entire application in a few sentences, to painstakingly describing an algorithm in step-by-step pseudocode. Thus it would be a mistake to view programming with AI assistance as another rung on the abstraction ladder. Rather, it can be viewed as a device that can teleport the programmer to arbitrary rungs of the ladder as desired. We close the discussion on AI assistance as a compiler with a few miscellaneous notes. The idea of using natural language as a programming notation has a long history (e.g., (Miller, 1981; Lieberman & Liu, 2006)), which we will not cover here. However, it is notable that there are many ways that natural language has been integrated with programming, such as debugging (Ko & Myers, 2004). With large language models, there are better capabilities for inference of intent and translation to code, but therefore also the potential to open up new strategies for inspecting and explaining code. There are also new failure modes for this paradigm of programming. 8.3. AI assistance as pair programming The third common perspective is that AI-assisted programming is like pair programming. GitHub Copi- lot’s commercial tagline describes it as “your AI pair programmer”. As opposed to search and compi- lation, which are both relatively impersonal tools, the analogy with pair programming is evocative of a more bespoke experience; assistance from a partner that understands more about your speciﬁc context and what you’re trying to achieve. AI-assisted programming does have the potential to be more person- alised, to the extent that it can take into consideration your speciﬁc source code and project ﬁles. As Hacker News commenters write: “[...] at one point it wrote an ENTIRE function by itself and it was correct. [...] it wasn’t some dumb boilerplate initialization either, it was actual logic with some loops. The context awareness with it is off the charts sometimes.[...]” “[...] It’s like having the stereotypical “intern” as an associate built-in to your editor. [...] It’s also ridiculously ﬂexible. When I start writing graphs in ASCII (cause I’m just quickly writing something down in a scratch ﬁle) it’ll actually understand what I’m doing and start autocompleting textual nodes in that ASCII graph.” Besides personalisation, the analogy also recalls the conventional role-division of pair programming between “driver” and “navigator”. When programming, one needs to form mental models of the program at many layers: from the speciﬁc statement being worked on, to its context in a subroutine, to the role that subroutine plays in a module, to the module within the program. However, code must be written at the statement level, which forces developers to keep this lowest level constantly at the forefront of their working memory. Experienced developers spend more time mapping out their code so that they can spend less time writing it. Research into code display and navigation has explored how different ways 15 of presenting lines of code can help programmers better keep these different layers of mental models in mind (Henley & Fleming, 2014). Pair programming, the argument goes, allows two partners to share the burden of the mental model. The driver codes at the statement and subroutine level while the navigator maps out the approach at the module and program level. By analogy to pair programming, the AI assistant taking the role of the driver, a solo programmer can now take the place of the navigator. But as we have seen, the experience of programming with AI assistance does not consistently absolve the human programmer of the responsibility for understanding the code at the statement and subroutine level. The programmer may be able to become “lazier [...] about learning various details of syntax and libraries”, but the experience still involves much greater statement-level checking. While a pair programming session requires a conscious, negotiated decision to swap roles, a solo pro- grammer with an AI assistant might ﬁnd themselves ﬂuidly traversing the spectrum from driving to navigation, from one moment to the next. This may partially explain why, in a preliminary experiment (n=21) comparing the experience of “pair programming” with GitHub Copilot to programming in a hu- man pair either as driver or navigator, Imai (2022) ﬁnds that programmers write more lines of code with Copilot than in a human pair, but these lines are of lower quality (more are subsequently deleted). Moreover, meta-analyses of pair programming have shown mixed efﬁcacy of human pair programming on task time, code quality and correctness (Salge & Berente, 2016; Hannay et al., 2009), suggesting that emulating the pair programming experience is not necessarily a good target to aim for. Multiple studies have concluded that the apparent successes of pair programming can be attributed, not to the role division into driver and navigator, but rather the high degree of verbalisation that occurs when pair programmers are forced to rationalise their decisions to each other (Hannay et al., 2009). Others have found that programming in pairs induces greater focus out of a respect for shared time; pair programmers are less likely to read emails, surf the web, or take long phone calls (L. A. Williams & Kessler, 2000). These particular beneﬁts of pair programming are not captured at all by AI assistance tools. The comparison to pair programming is thus relatively superﬁcial, and today’s experience of AI-assisted programming is not comparable with pair programming to the same extent as it is with search or compi- lation. 8.4. A distinct way of programming LLM-assisted programming assistance bears similarities to search: both begin with a prompt, both have an information asymmetry, there are several results, with inexact solutions. But there are differences: search is mixed-media, whereas LLM assistance is ﬁxed. Search (often) has provenance, and language models do not. It also bears similarities to compilation and programming by speciﬁcation. Both enable programming at a ‘higher’ level of abstraction (for some deﬁnition of higher). Yet unlike with compilers, a programmer using AI assistance must still have a working knowledge of the target language, they must actively check the output for correctness, and they get very little feedback for improving their ‘source’ code. It also bears a superﬁcial similarity to pair programming, in that it promises to let the programmer take the role of ‘navigator’, forming high-level mental models of the program while delegating the role of ‘driver’ to the language model. But unlike with pair programming, the human navigator must often hop into the driver’s seat. And unlike with pair programming, LLM-assisted programming does not require verbalisation, nor does it coerce greater focus out of a respect for shared time. Thus existing metaphors do not completely capture the experience of LLM-assisted programming. It is emerging as a distinct way of programming. It does not quite strike us as a distinct practice of pro- gramming, as that term has been applied to communities of programmers united by similar ethos and aims, such as enterprise software engineers, bricoleurs, live coders, and code benders; but as Bergström & Blackwell (2016) note, there are no clear criteria by which we can deﬁne the boundaries of a prac- tice. Nor does it strike us as being a new activity of programming as per the cognitive dimensions 16 Figure 6 – GridBook interface showing natural language formula in the spreadsheet grid. framework, since AI assistance is clearly orthogonal to authoring, transcription, and modiﬁcation, being applicable to each of these activities and others besides. Yet as a way of programming it seems to affect programmer’s experience more profoundly than a feature such as autocomplete, having far-reaching im- pact on their attitudes and practices of authoring, information foraging, debugging, refactoring, testing, documentation, code maintenance, learning, and more. 9. Issues with application to end-user programming The beneﬁts and challenges of programming with LLMs discussed so far concern the professional pro- grammer, or a novice programmer in training. They have formal training in programming and, often, some understanding of the imperfect nature of AI-generated code. But the majority of people who pro- gram do not fall into this category. Instead, they are ordinary end users of computers who program to an end. Such end-user programmers often lack knowledge of programming, or the workings of AI. They also lack the inclination to acquire those skills. It is reasonable to say that such end-user programmers (e.g., accountants, journalists, scientists, business owners) stand to beneﬁt the most from AI assistance, such as LLMs. In an ideal world, an end-user wanting to accomplish a task could do so by simply specifying their intent in familiar natural language without prior knowledge of the underlying programming model, or its syntax and semantics. The code will get generated and even automatically run to produce the desired output. However, as we have seen so far, the world is not ideal and even trained programmers face various chal- lenges when programming with AI. These challenges are only exacerbated for end-user programmers, as a study by Srinivasa Ragavan et al. (2022) observes. Participants in the study were data analysts (n=20) conducting exploratory data analysis in GridBook, a natural-language augmented spreadsheet system. In GridBook (Figure 6, adopted from Srinivasa Ra- gavan et al. (2022)) users can write spreadsheet formulas using the natural language (Figure 6: a-f); a formal formula is then synthesized from the natural language utterance. GridBook also infers the con- text of an utterance; for example, in Figure 6, the query in label 4 is a follow-up from label 3. Both the natural language utterance and the synthesized formula are persisted for users to edit and manipulate. 17 9.1. Issue 1: Intent speciﬁcation, problem decomposition and computational thinking When attempting to accomplish data analysis tasks using natural language, participants had to reﬁne their speciﬁcation of intent in the natural language several times, before they arrived at the desired result (if they did). The NL utterances were often underspeciﬁed, ambiguous, too complex, or contained domain phrases not speciﬁed in the context (e.g., in the data being analyzed). Thus, the ﬁrst issue is to communicate the capabilities of the system, and make it interpretable so users can see how their prompt is being interpreted. End-user programmers often also lack the key computational thinking skills (Wing, 2011), such as the ability to decompose problems into subproblems, reformulate problems in ways that can be computed by a system, etc. However, effective use of LLMs such as Codex requires such skills. For example, if these models are most accurate when solutions to a problem are single line, then the user should be able to break their problem into smaller sub-problems each of which can be solved in one or two lines. Moreover, they might also lack the ability to frame a problem as generic computational problems, rather than domain-speciﬁc problems. For example, a realtor is more likely to ask “which is the largest house” (declaratively), instead of “which is the house with maximum constructed area” (procedurally). Therefore, end-user computing environments powered by AI should help end-user programmers think “computationally”: they must aid users in breaking down their problems to smaller steps, or guiding users towards alternative strategies to specify or solve a problem (e.g., providing examples, offering alternatives) or even seek procedural prompts where needed (e.g., for disambiguation). 9.2. Issue 2: Code correctness, quality and (over)conﬁdence The second challenge is in verifying whether the code generated by the model is correct. In GridBook, users were able to see the natural language utterance, synthesized formula and the result of the for- mula. Of these, participants heavily relied on ‘eyeballing’ the ﬁnal output as a means of evaluating the correctness of the code, rather than, for example, reading code or testing rigorously. While this lack of rigorous testing by end-user programmers is unsurprising, some users, particularly those with low computer self-efﬁcacy, might overestimate the accuracy of the AI, deepening the overcon- ﬁdence end-user programmers are known to have in their programs’ accuracy Panko (2008). Moreover, end-user programmers might not be able to discern the quality of non-functional aspects of the generated code, such as security, robustness or performance issues. 9.3. Issue 3: Code comprehension and maintenance A third challenge with AI-driven programming is the issue of code comprehension. During GridBook’s user evaluation, participants mentioned that the generated formulas are hard to understand, even when users were familiar with the target language. This has potentially severe consequences: from evaluating the accuracy of the program by verifying logic, to the ability to customize code, to future debugging and reuse. As we discussed earlier, this problem also exists for trained developers. One approach to address this issue is for the AI system to include some notion of code readability or comprehensibility as a factor in code synthesis, such as during the learning phase, or when ranking suggestions, or even take it as input to the model (similar to the ‘temperature’ parameter in Codex). This approach is useful more broadly to synthesize high quality code, such as optimizing for performance or robustness. A second solution to tackle the comprehension problem is to explain the generated code to their users in a manner that is less ‘programmerese’ and more centered around the user’s current task and context. Initial evidence suggests that participants were open to these ideas; thus, these areas are ripe for future exploration. 9.4. Issue 4: Consequences of automation in end-user programming In any AI system, we need to consider the consequences of automation. End-user programmers are known to turn to local experts or gardeners (end-user programmers with interest and expertise in pro- gramming who serve as gurus in the end-user programming environment) when they are unable to solve a part of the problem (Nardi, 1993; Sarkar & Gordon, 2018). Task-orientation tendencies combined with 18 challenges of completing their tasks easily also leaves end-user programmers with limited attention for testing, or carefully learning what is going on with their programs. Assuming that LLMs and associated user experiences will improve in the coming years, making end-user programming faster with LLMs than without, it is tempting to wonder whether the programmer can be persuaded to invest the saved time and attention to aspects such as learning or testing their programs; if so, what would it take to inﬂuence behaviour changes? Another question is in the role of such experts. We conjecture that LLMs or similar AI capabilities will soon be able to answer a sizeable fraction of questions that end-user programmers will go to local experts for. An open question therefore is how the ecosystem of end-user programmers in organizations will change in their roles, importance and specialities. For example, will gardeners take on the role of educating users on better taking advantage of AI? If so, how can we communicate the working of such AI systems to technophile users and early adopters, so they can enable others in the organization? 9.5. Issue 5: No code, and the dilemma of the direct answer Finally, it is not a foregone conclusion that users are even interested in code. As Blackwell’s model of attention investment notes, in many cases the user may be content to perform an action manually, rather than invest in creating a reusable automation (Blackwell, 2002a; J. Williams et al., 2020). Spreadsheet users, in particular, are often not sensitive to the level of automation or automatability of a given work- ﬂow, using a mix of manual, automated, and semi-automated techniques to achieve the goal at hand (Pandita et al., 2018). Spreadsheet users often need ad-hoc transformations of their data that they will, in all likelihood, never need again. It may be that we can express this transformation as a program, but if the user is interested in the output and not the program, is it important, or even necessary, to communicate this fact to the user? One can argue that increasing the user’s awareness of the ﬂexibility and fallibility of the process of delivering an inferred result (i.e., enabling them to critically evaluate the output (Sarkar et al., 2015)) can build agency, conﬁdence, trust, and resilience. This issue is related to information retrieval’s “dilemma of the direct answer” (Potthast et al., 2021), raised in response to the increased phenomenon of search engines directly answering queries in addition to simply listing retrieved results. However, if the programming language used is not related to the languages familiar to the end-user, or the user is a complete novice, it is exceedingly difﬁcult for them to make any sense of it, as was shown by Lau et al. (2021) in their study of Excel users encountering Python code. Yet, there are socio-technical motivations for using an unfamiliar target language: long-term testing of LLM assistance shows that it shines when paired with high-level APIs that capture use cases well (Section 7). One advantage of the Python ecosystem is that it has an unparalleled set of libraries and APIs for data wrangling. An LLM-assisted tool that emits Excel formulas is therefore less likely to solve user problems than Python statements. In the longer term, this might be mitigated by developing a rich set of data manipulation libraries in the Excel formula language. 10. Conclusion Large language models have initiated a signiﬁcant change in the scope and quality of program code that can be automatically generated, compared to previous approaches. Experience with commercially available tools built on these models suggests that a they represent a new way of programming. LLM assistance transforms almost every aspect of the experience of programming, including planning, au- thoring, reuse, modiﬁcation, comprehension, and debugging. In some aspects, LLM assistance resembles a highly intelligent and ﬂexible compiler, or a partner in pair programming, or a seamless search-and-reuse feature. Yet in other aspects, LLM-assisted programming has a ﬂavour all of its own, which presents new challenges and opportunities for human-centric pro- gramming research. Moreover, there are even greater challenges in helping non-expert end users beneﬁt from such tools. 19 A. Experience report sources This appendix contains a list of sources we draw upon for the quotes and analysis in Section 7. While all sources were included in our analysis, we did not draw direct quotes from every source in this list. A.1. Blog posts and corresponding Hacker News discussions 1. Andrew Mayne, March 17 2022, “Building games and apps entirely through natural language using OpenAI’s code-davinci model”. URL: https://andrewmayneblog.wordpress .com/2022/03/17/building-games-and-apps-entirely-through-natural -language-using-openais-davinci-code-model/. Hacker News discussion: https://news.ycombinator.com/item?id=30717773 2. Andrew Mouboussin, March 24 2022, “Building a No-Code Machine Learning Model by Chat- ting with GitHub Copilot”. URL: https://www.surgehq.ai/blog/building-a-no -code-toxicity-classifier-by-talking-to-copilot. Hacker News discussion: https://news.ycombinator.com/item?id=30797381 3. Matt Rickard, August 17 2021, “One Month of Using GitHub Copilot”. URL: https://matt -rickard.com/github-copilot-a-month-in/. 4. Nutanc, November 15 2021, “Using Github copilot to get the tweets for a keyword and ﬁnd the sentiment of each tweet in 2 mins”. URL: https://nutanc.medium.com/ using-github-copilot-to-get-the-tweets-for-a-keyword-and-find -the-sentiment-of-each-tweet-in-2-mins-9a531abedc84. 5. Tanishq Abraham, July 14 2021, “Coding with GitHub Copilot”. URL: https://tmabraham .github.io/blog/github_copilot. 6. Aleksej Komnenovic, January 17 2022, “Don’t fully trust AI in dev work! https://akom.me/dont-fully-trust-ai-in-dev-work-yet. /yet”. URL: A.2. Miscellaneous Hacker News discussions 1. https://news.ycombinator.com/item?id=30747211 2. https://news.ycombinator.com/item?id=31390371 3. https://news.ycombinator.com/item?id=31020229&p=2 4. https://news.ycombinator.com/item?id=29760171 5. https://news.ycombinator.com/item?id=31325154 6. https://news.ycombinator.com/item?id=31734110 7. https://news.ycombinator.com/item?id=31652939 8. https://news.ycombinator.com/item?id=30682841 9. https://news.ycombinator.com/item?id=31515938 10. https://news.ycombinator.com/item?id=31825742 20 References Allamanis, M., Barr, E. T., Devanbu, P. T., & Sutton, C. (2018). A survey of machine learning for big code and naturalness. ACM Comput. Surv., 51(4), 81:1–81:37. Retrieved from https://doi .org/10.1145/3212695 doi: 10.1145/3212695 Allamanis, M., & Brockschmidt, M. (2017). Smartpaste: Learning to adapt source code. arXiv preprint arXiv:1705.07867. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., . . . Sutton, C. (2021). Program synthesis with large language models. arXiv. Retrieved from https://arxiv.org/abs/2108 .07732 doi: 10.48550/ARXIV.2108.07732 Barik, T., Ford, D., Murphy-Hill, E., & Parnin, C. (2018). How should compilers explain problems to developers? In Proceedings of the 2018 26th acm joint meeting on european software engineering conference and symposium on the foundations of software engineering (pp. 633–643). Barik, T., Johnson, B., & Murphy-Hill, E. (2015). I heart hacker news: expanding qualitative research ﬁndings by analyzing social news websites. In Proceedings of the 2015 10th joint meeting on foun- dations of software engineering (pp. 882–885). Barke, S., James, M. B., & Polikarpova, N. (2022). Grounded copilot: How programmers interact with code-generating models. arXiv. Retrieved from https://arxiv.org/abs/2206.15000 doi: 10.48550/ARXIV.2206.15000 Basman, A., Church, L., Klokmose, C. N., & Clark, C. B. (2016). Software and how it lives on- embedding live programs in the world around them. In Ppig (p. 19). Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the dangers of stochastic parrots: Can language models be too big? In M. C. Elish, W. Isaac, & R. S. Zemel (Eds.), Facct ’21: 2021 ACM conference on fairness, accountability, and transparency, virtual event / toronto, canada, march 3-10, 2021 (pp. 610–623). ACM. Retrieved from https://doi.org/10.1145/ 3442188.3445922 doi: 10.1145/3442188.3445922 Bergström, I., & Blackwell, A. F. (2016). The practices of programming. In 2016 ieee symposium on visual languages and human-centric computing (vl/hcc) (pp. 190–198). Blackwell, A. F. (2002a). First steps in programming: A rationale for attention investment models. In Proceedings ieee 2002 symposia on human centric computing languages and environments (pp. 2–10). Blackwell, A. F. (2002b). What is programming? In Ppig (p. 20). Bødker, S. (2015). Third-wave hci, 10 years later - participation and sharing. Interactions, 22(5), 24–31. Retrieved from https://doi.org/10.1145/2804405 doi: 10.1145/2804405 Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., . . . Amodei, D. (2020). Language models are few-shot learners. Cao, J., Fleming, S. D., Burnett, M., & Scafﬁdi, C. (2015). Idea garden: Situated support for problem solving by end-user programmers. Interacting with Computers, 27(6), 640–660. Chalhoub, G., & Sarkar, A. (2022). “It’s Freedom to Put Things Where My Mind Wants”: Understanding In CHI Conference on and Improving the User Experience of Structuring Data in Spreadsheets. Human Factors in Computing Systems. New York, NY, USA: Association for Computing Machinery. Retrieved from https://doi.org/10.1145/3491102.3501833 doi: 10.1145/3491102 .3501833 21 Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., . . . Zaremba, W. (2021). Evaluating large language models trained on code. CoRR, abs/2107.03374. Retrieved from https://arxiv.org/abs/2107.03374 Chen, M., Tworek, J., Jun, H., Yuan, Q., Ponde, H., Kaplan, J., . . . Zaremba, W. (2021). Evaluating large language models trained on code. ArXiv, abs/2107.03374. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., . . . Fiedel, N. (2022). Palm: Scaling language modeling with pathways. ArXiv, abs/2204.02311. Colmerauer, A., & Roussel, P. (1996). The birth of prolog. In History of programming languages—ii (pp. 331–367). Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019, June). BERT: Pre-training of deep bidi- In Proceedings of the 2019 conference of the rectional transformers for language understanding. north American chapter of the association for computational linguistics: Human language technolo- gies, volume 1 (long and short papers) (pp. 4171–4186). Minneapolis, Minnesota: Association for Computational Linguistics. Retrieved from https://aclanthology.org/N19-1423 doi: 10.18653/v1/N19-1423 Green, T., & Blackwell, A. (1998). Cognitive dimensions of information artefacts: a tutorial. In Bcs hci conference (Vol. 98, pp. 1–75). Green, T. R. (1989). Cognitive dimensions of notations. People and computers V, 443–460. Green, T. R., & Petre, M. (1992). When visual programs are harder to read than textual programs. In Human-computer interaction: Tasks and organisation, proceedings of ecce-6 (6th european confer- ence on cognitive ergonomics). gc van der veer, mj tauber, s. bagnarola and m. antavolits. rome, cud (pp. 167–180). Gulwani, S. (2011). Automating string processing in spreadsheets using input-output examples. In T. Ball & M. Sagiv (Eds.), Proceedings of the 38th ACM SIGPLAN-SIGACT symposium on principles of programming languages, POPL 2011, austin, tx, usa, january 26-28, 2011 (pp. 317–330). ACM. Retrieved from https://doi.org/10.1145/1926385.1926423 doi: 10.1145/1926385 .1926423 Hannay, J. E., Dybå, T., Arisholm, E., & Sjøberg, D. I. (2009). The effectiveness of pair programming: A meta-analysis. Information and software technology, 51(7), 1110–1122. Henley, A. Z., & Fleming, S. D. (2014). The patchworks code editor: Toward faster navigation with less code arranging and fewer navigation mistakes. In Proceedings of the sigchi conference on human factors in computing systems (pp. 2511–2520). Hermans, F., Pinzger, M., & van Deursen, A. (2015). Detecting and refactoring code smells in spread- sheet formulas. Empirical Software Engineering, 20(2), 549–575. Hindle, A., Barr, E. T., Gabel, M., Su, Z., & Devanbu, P. T. (2016). On the naturalness of software. Commun. ACM, 59(5), 122–131. Retrieved from https://doi.org/10.1145/2902362 doi: 10.1145/2902362 Hindle, A., Barr, E. T., Su, Z., Gabel, M., & Devanbu, P. T. (2012). On the naturalness of soft- In M. Glinz, G. C. Murphy, & M. Pezzè (Eds.), 34th international conference on soft- ware. ware engineering, ICSE 2012, june 2-9, 2012, zurich, switzerland (pp. 837–847). IEEE Com- puter Society. Retrieved from https://doi.org/10.1109/ICSE.2012.6227135 doi: 10.1109/ICSE.2012.6227135 22 Hoare, C. A. R. (1969). An axiomatic basis for computer programming. Commun. ACM, 12(10), 576– 580. Retrieved from https://doi.org/10.1145/363235.363259 doi: 10.1145/363235 .363259 Hochreiter, S., & Schmidhuber, J. (1997, nov). Long short-term memory. Neural Comput., 9(8), 1735–1780. Retrieved from https://doi.org/10.1162/neco.1997.9.8.1735 doi: 10 .1162/neco.1997.9.8.1735 Horvitz, E. (1999). Principles of mixed-initiative user interfaces. In Proceedings of the sigchi conference on human factors in computing systems (pp. 159–166). Hutchins, E. L., Hollan, J. D., & Norman, D. A. (1985). Direct manipulation interfaces. Hum. Comput. Interact., 1(4), 311–338. Retrieved from https://doi.org/10.1207/s15327051hci0104 _2 doi: 10.1207/s15327051hci0104\_2 Imai, S. (2022). Is github copilot a substitute for human pair-programming? an empirical study. In 2022 ieee/acm 44th international conference on software engineering: Companion proceedings (icse- companion) (pp. 319–321). Jiang, E., Toh, E., Molina, A., Olson, K., Kayacik, C., Donsbach, A., . . . Terry, M. (2022). Discovering the syntax and strategies of natural language programming with generative language models. In Chi conference on human factors in computing systems (pp. 1–19). Kery, M. B., & Myers, B. A. (2017). Exploring exploratory programming. In 2017 ieee symposium on visual languages and human-centric computing (vl/hcc) (pp. 25–29). Ko, A. J., & Myers, B. A. (2004). Designing the whyline: a debugging interface for asking questions In Proceedings of the sigchi conference on human factors in computing about program behavior. systems (pp. 151–158). Kulesza, T., Amershi, S., Caruana, R., Fisher, D., & Charles, D. (2014). Structured labeling for facilitat- ing concept evolution in machine learning. In Proceedings of the sigchi conference on human factors in computing systems (pp. 3075–3084). Kurlander, D., Cypher, A., & Halbert, D. C. (1993). Watch what i do: programming by demonstration. MIT press. Lau, S., Srinivasa Ragavan, S. S., Milne, K., Barik, T., & Sarkar, A. (2021). Tweakit: Supporting end- user programmers who transmogrify code. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (pp. 1–12). Li, J., Tang, T., Zhao, W. X., & Wen, J.-R. (2021, 8). Pretrained language model for text generation: A survey. In Z.-H. Zhou (Ed.), Proceedings of the thirtieth international joint conference on artiﬁcial intelligence, IJCAI-21 (pp. 4492–4499). International Joint Conferences on Artiﬁcial Intelligence Organization. Retrieved from https://doi.org/10.24963/ijcai.2021/612 (Survey Track) doi: 10.24963/ijcai.2021/612 Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., . . . Vinyals, O. (2022b). Competition-level code generation with alphacode. arXiv. Retrieved from https://arxiv.org/ abs/2203.07814 doi: 10.48550/ARXIV.2203.07814 Li, Y., Choi, D. H., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., . . . Vinyals, O. (2022a). Competition-level code generation with alphacode. ArXiv, abs/2203.07814. Lieberman, H. (2001). Your wish is my command: Programming by example. Morgan Kaufmann. 23 Lieberman, H., & Liu, H. (2006). Feasibility studies for programming in natural language. In End user development (pp. 459–473). Springer. Liu, S., Chen, Y., Xie, X., Siow, J. K., & Liu, Y. (2021). Retrieval-augmented generation for code summarization via hybrid GNN. In International conference on learning representations. Retrieved from https://openreview.net/forum?id=zv-typ1gPxA Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., . . . Liu, S. (2021). Codexglue: A machine learning benchmark dataset for code understanding and generation. ArXiv, abs/2102.04664. Luger, E., & Sellen, A. (2016). ""like having a really bad pa"" the gulf between user expectation and experience of conversational agents. In Proceedings of the 2016 chi conference on human factors in computing systems (pp. 5286–5297). Macvean, A., Church, L., Daughtry, J., & Citro, C. (2016). Api usability at scale. In Ppig (p. 26). Marasoiu, M., Church, L., & Blackwell, A. (2015). An empirical investigation of code completion usage by professional software developers. In PPIG. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, & K. Weinberger (Eds.), Advances in neural information processing systems (Vol. 26). Curran As- sociates, Inc. Retrieved from https://proceedings.neurips.cc/paper/2013/file/ 9aa42b31882ec039965f3c4923ce901b-Paper.pdf Miller, L. A. (1981). Natural language programming: Styles, strategies, and contrasts. IBM Systems Journal, 20(2), 184–215. Mou, L., Li, G., Zhang, L., Wang, T., & Jin, Z. (2016). Convolutional neural networks over tree structures for programming language processing. In Aaai. Mu, J., & Sarkar, A. (2019). Do we need natural language? Exploring restricted language interfaces In Extended Abstracts of the 2019 CHI Conference on Human Factors in for complex domains. Computing Systems (pp. 1–6). Myers, B. A. (1992). Demonstrational interfaces: A step beyond direct manipulation. Computer, 25(8), 61–73. Myers, B. A., & Stylos, J. (2016). Improving api usability. Communications of the ACM, 59(6), 62–69. Nardi, B. A. (1993). A small matter of programming: perspectives on end user computing. MIT press. Nguyen, A. T., Nguyen, T. T., & Nguyen, T. N. (2015). Divide-and-conquer approach for multi-phase statistical migration for source code (t). 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE), 585-596. Pandita, R., Parnin, C., Hermans, F., & Murphy-Hill, E. (2018). No half-measures: A study of manual and tool-assisted end-user programming tasks in excel. In 2018 ieee symposium on visual languages and human-centric computing (vl/hcc) (pp. 95–103). Panko, R. R. (2008). Reducing overconﬁdence in spreadsheet development. arXiv preprint arXiv:0804.0941. Pearce, H., Ahmad, B., Tan, B., Dolan-Gavitt, B., & Karri, R. (2021). Asleep at the keyboard? assessing the security of github copilot’s code contributions. arXiv. Retrieved from https://arxiv.org/ abs/2108.09293 doi: 10.48550/ARXIV.2108.09293 24 Piccioni, M., Furia, C. A., & Meyer, B. (2013). An empirical study of api usability. In 2013 acm/ieee international symposium on empirical software engineering and measurement (pp. 5–14). Potthast, M., Hagen, M., & Stein, B. (2021). The dilemma of the direct answer. In Acm sigir forum (Vol. 54, pp. 1–12). Raychev, V., Vechev, M. T., & Krause, A. (2015). Predicting program properties from ""big code"". In S. K. Rajamani & D. Walker (Eds.), Proceedings of the 42nd annual ACM SIGPLAN-SIGACT sympo- sium on principles of programming languages, POPL 2015, mumbai, india, january 15-17, 2015 (pp. 111–124). ACM. Retrieved from https://doi.org/10.1145/2676726.2677009 doi: 10.1145/2676726.2677009 Rouchy, P. (2006). Aspects of prolog history: Logic programming and professional dynamics. Blekinge Institute of Technology, Sweden).(English). TeamEthno-Online(2), 85–100. Salge, C. A. D. L., & Berente, N. (2016). Pair programming vs. solo programming: What do we know after 15 years of research? In 2016 49th hawaii international conference on system sciences (hicss) (pp. 5398–5406). Sarkar, A. (2016). Interactive analytical modelling (Tech. Rep. No. UCAM-CL-TR-920). Uni- versity of Cambridge, Computer Laboratory. Retrieved from https://www.cl.cam.ac.uk/ techreports/UCAM-CL-TR-920.pdf doi: 10.48456/tr-920 Sarkar, A. (2022, March). In Workshop on Transparency and Explanations in Smart Systems (TeXSS), in conjunction with ACM Intelligent User Interfaces (IUI 2022) (pp. 192–199). Retrieved from http://ceur-ws.org/Vol-3124/ paper22.pdf Is explainable AI a race against model complexity? Sarkar, A., & Gordon, A. D. (2018, September). How do people learn to use spreadsheets? (work in progress). In Proceedings of the 29th Annual Conference of the Psychology of Programming Interest Group (PPIG 2018) (pp. 28–35). Sarkar, A., Jamnik, M., Blackwell, A. F., & Spott, M. Interactive visual machine learning (2015). In 2015 IEEE Symposium on Visual Languages and Human-Centric Computing in spreadsheets. (VL/HCC) (pp. 159–163). Sarkar, A., Srinivasa Ragavan, S., Williams, J., & Gordon, A. D. (2022). End-user encounters with lambda abstraction in spreadsheets: Apollo’s bow or Achilles’ heel? In 2022 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC). Shneiderman, B., & Norwood, N. (1993). 1.1 direct manipulation: a step beyond programming. Sparks of innovation in human-computer interaction, 17. Silver, A. (2018, May). Introducing visual studio intellicode. Microsoft. Retrieved from https://devblogs.microsoft.com/visualstudio/introducing-visual -studio-intellicode/ Srinivasa Ragavan, S., Hou, Z., Wang, Y., Gordon, A. D., Zhang, H., & Zhang, D. (2022). Gridbook: Natural language formulas for the spreadsheet grid. In 27th international conference on intelligent user interfaces (p. 345–368). New York, NY, USA: Association for Computing Machinery. Retrieved from https://doi.org/10.1145/3490099.3511161 doi: 10.1145/3490099.3511161 Srinivasa Ragavan, S., Kuttal, S. K., Hill, C., Sarma, A., Piorkowski, D., & Burnett, M. (2016). Foraging among an overabundance of similar variants. In Proceedings of the 2016 chi conference on human factors in computing systems (pp. 3509–3521). 25 Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. In Proceedings of the 27th international conference on neural information processing systems - volume 2 (p. 3104–3112). Cambridge, MA, USA: MIT Press. Tanimoto, S. L. (2013). A perspective on the evolution of live programming. In 2013 1st international workshop on live programming (live) (pp. 31–34). Vaithilingam, P., Zhang, T., & Glassman, E. L. (2022). Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. In Chi conference on human factors in computing systems extended abstracts (pp. 1–7). Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., . . . Polosukhin, I. (2017). Attention is all you need. In Proceedings of the 31st international conference on neural information processing systems (p. 6000–6010). Red Hook, NY, USA: Curran Associates Inc. Wei, J., Goyal, M., Durrett, G., & Dillig, I. (2020). Lambdanet: Probabilistic type inference using graph neural networks. ArXiv, abs/2005.02161. Wei, Y., Chandrasekaran, N., Gulwani, S., & Hamadi, Y. (2015, May). Building bing developer assistant (Tech. Rep. No. MSR-TR-2015-36). Retrieved from https://www.microsoft.com/en-us/ research/publication/building-bing-developer-assistant/ Weiss, D. (2022, Jun). Blog / tabnine announcements / announcing our next-generation ai models. Tab- nine. Retrieved from https://www.tabnine.com/blog/announcing-tabnine-next -generation/ Williams, J., Negreanu, C., Gordon, A. D., & Sarkar, A. (2020). Understanding and inferring units In 2020 IEEE Symposium on Visual Languages and Human-Centric Computing in spreadsheets. (VL/HCC) (pp. 1–9). Williams, L. A., & Kessler, R. R. (2000). All i really need to know about pair programming i learned in kindergarten. Communications of the ACM, 43(5), 108–114. Wing, J. (2011). Research notebook: Computational thinking—what and why. The link magazine, 6, 20–23. Xu, F. F., Alon, U., Neubig, G., & Hellendoorn, V. J. (2022). A systematic evaluation of large lan- guage models of code. Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming. Xu, F. F., Vasilescu, B., & Neubig, G. In-IDE Code Generation from Natural Language: Promise and Challenges. ACM Transactions on Software Engineering and Methodology (TOSEM), 31(2), 1–47. (2022). Yoon, Y., & Myers, B. A. (2015). Supporting selective undo in a code editor. In 2015 ieee/acm 37th ieee international conference on software engineering (Vol. 1, pp. 223–233). Zhang, H., Jain, A., Khandelwal, G., Kaushik, C., Ge, S., & Hu, W. (2016). Bing developer assistant: improving developer productivity by recommending sample code. In Proceedings of the 2016 24th acm sigsoft international symposium on foundations of software engineering (pp. 956–961). Ziegler, A. (2021, Jun). Github copilot research recitation. Microsoft. Retrieved from https:// github.blog/2021-06-30-github-copilot-research-recitation/ Ziegler, A., Kalliamvakou, E., Simister, S., Sittampalam, G., Li, A., Rice, A., . . . Aftandilian, E. (2022). Productivity assessment of neural code completion. arXiv preprint arXiv:2205.06537. 26"
377,Deep Learning and Health Informatics for Smart Monitoring and Diagnosis,"[{'href': 'http://arxiv.org/abs/2208.03143v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2208.03143v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-08-05 13:07:59,"2 2 0 2 g u A 6 2 ] C D . s c [ 2 v 5 9 1 2 1 . 8 0 2 2 : v i X r a ExpoCloud: a Framework for Time and Budget-Eﬀective Parameter Space Explorations Using a Cloud Compute Engine Meir Goldenberg1 Jerusalem College of Technology Abstract Large parameter space explorations are among the most time consuming yet critically important tasks in many ﬁelds of modern research. ExpoCloud enables the researcher to harness cloud compute resources to achieve time and budget- eﬀective large-scale concurrent parameter space explorations. ExpoCloud enables maximal possible levels of concurrency by creating com- pute instances on-the-ﬂy, saves money by terminating unneeded instances, pro- vides a mechanism for saving both time and money by avoiding the exploration of parameter settings that are as hard or harder than the parameter settings whose exploration timed out. Eﬀective fault tolerance mechanisms make Ex- poCloud suitable for large experiments. ExpoCloud provides an interface that allows its use under various cloud envi- ronments. As a proof of concept, we implemented a class supporting the Google Compute Engine (GCE). We also implemented a class that simulates a cloud environment on the local machine, thereby facilitating further development of ExpoCloud. The article describes ExpoCloud’s features and provides a usage example. The software is well documented and is available under the MIT license [1, 2]. Keywords: parameter space exploration, distributed computing, cloud compute engine, large-scale. 1Email address: mgoldenb@g.jct.ac.il Preprint submitted to Journal of Parallel and Distributed Computing August 29, 2022 Introduction Large parameter space explorations are among the most time consuming yet critically important tasks in many ﬁelds of modern research. For example, com- puter science research is often concerned with the study of algorithms for solving computational problems, whereby the algorithm’s behavior and the computa- tion time for solving the problem are controlled by a number of parameters. The possible settings of these parameters form a large parameter space, whose thorough exploration requires that the algorithm be run to solve a number of problem instances for each parameter setting of both the algorithm and the problem. It is our assumption in this work that individual parameter settings can be explored concurrently and independently of each other. In the absence of a tool that makes large-scale parameter explorations easy to accomplish, researchers resort to running ad hoc scripts either on a local machine or on a cluster. Most recently, cloud-based compute engines became a budget-friendly option. The amount of computational power available through such services is usually much greater than that available in the research clusters. However, the amount of technical expertise and scripting required to set up an experiment that harnesses these resources may prove to be a stumbling block. As a result, the researchers adopt simplifying limitations, such as using multiple threads on a single compute instance [3]. The vision We envisioned a framework that would let the researcher deﬁne his/her work- load and achieve maximal concurrency while economizing on his/her time and money, allowing ﬂexibility in choosing the cloud platform and providing fault tolerance. ExpoCloud [1] is our implementation of the above vision. It realizes the words that appear above in italics as follows: 2 • The workload is a list of tasks, each deﬁned by a setting of parameters. It is computed at the commencement of the experiment and passed to the framework for automated execution. • Maximal concurrency is achieved by creating a new compute instance as often as is allowed by the cloud platform, for as long as there are tasks to assign. • Economizing on time is achieved by letting the user specify a deadline and a hardness (deﬁned below) for each task. When a task takes more time to execute than the time speciﬁed by the deadline, we say that the task has timed out. ExpoCloud terminates timed out tasks automatically. A task’s hardness is a tuple of parameter values that correlate with the time required to execute the task. The researcher speciﬁes which subset of parameters determines a task’s hardness and provides a method that compares hardnesses of two tasks. When a task times out, the framework terminates all currently running tasks that are as hard or harder than the timed out task. It also avoids running such tasks in the future. The framework executes the tasks in the order from the easiest to the hardest, so as to maximize the number of tasks that do not have to be executed. • Economizing on money is achieved by deleting a compute instance as soon as it is done with the tasks assigned to it and there are no more tasks to be assigned. • ExpoCloud provides great ﬂexibility for choosing the cloud platform. To adapt to a given cloud platform, one needs to merely provide an extension class with methods to create, terminate and list compute instances. In addition, the researcher is in full control of the properties of the compute instances, since all of them are created based on machine images speciﬁed by the researcher. • Fault tolerance means that the computation would proceed even if one or more compute instances fail for any reason. 3 Before moving on to the main part of the article, we introduce an example that we will use to demonstrate the framework’s design and usage. The example parameter exploration Consider the agent assignment problem, as follows. A team of n agents needs to complete a project consisting of m tasks, where n ≥ m. The tasks have to be performed sequentially. For each agent i and task j, we are given tij, the amount of time, in seconds, that the agent i requires to complete the task j. The problem is to assign an agent to each task, such that no agent is assigned to more than one task and the total time of completing the project is minimized. Suppose we use the classical branch and bound (B&B) search algorithm for solving this problem, as follows. The algorithm is recursive. It starts with an empty assignment, whereby no agent is assigned to a task. At each recursive call, it extends the current partial assignment by assigning an agent to the next task. When all tasks have been assigned an agent, we say that the assignment is full. At this base case of the recursion, the algorithm updates the currently best full assignment and the corresponding time for completing the project. The advantage of B&B search over the brute-force search is the so called B&B cutoﬀ. Namely, whenever the time corresponding to the current partial assignment is greater than that of the best full assignment, the current assign- ment can be discarded without losing the solution optimality. A more eﬃcient version of this algorithm uses a heuristic. Given a partial assignment, the heuristic is a lower bound on the time needed to complete the remaining tasks. This bound is computed by assigning the best of the unused agents to each of the remaining tasks, while allowing the assignment of the same remaining agent to more than one remaining task. Whenever the sum of the time corresponding to the current partial assignment and the heuristic is greater than that of the best full assignment, the current assignment can be discarded. Thus, we have three algorithmic variants - the brute-force search, the classi- cal B&B search and the B&B search with a heuristic. To thoroughly understand the properties of the agent assignment problem and the B&B search’s perfor- 4 mance for solving it, we need to run each algorithmic variant to solve a number of generated problem instances for many possible settings of the number of agents n and the number of tasks m. What range of values should we consider for the number of agents n and the number of tasks m? Without a framework like ExpoCloud, this question is not easily answered. First, the range will depend on the algorithmic variant. The brute-force search will only be able to handle small problems, while B&B with a heuristic might be able so solve much larger instances. Knowing his/her budget of time for the whole experiment, the researcher might decide on a deadline per problem instance. He/she might then perform several test runs to get a feeling for how much time each algorithmic variant takes to solve problem instances of various sizes. Even after this laborious tuning stage, the researcher will still run the risk of some instances taking disproportionately long time, possibly stumbling the whole experiment. With ExpoCloud, the question is really easy. First the researcher writes a short class that deﬁnes a task as running one algorithmic variant to solve a single problem instance for one particular setting of n and m. After deciding on a deadline, the researcher picks a large range of values for n and m, with the upper bounds that for sure cannot be solved by the best algorithmic variant. He/she writes a single nested loop to generate all the tasks. All of this is shown in the next section. Next, the researcher notices that larger values of n correspond to harder problem instances and so do larger values of m. It is also clear that the same instance is likely to be solved faster by the B&B search with a heuristic than with the classical B&B, which is in turn faster than the brute-force algorithm. The researcher deﬁnes several short methods informing the framework of these observations and oﬀ the experiment goes. ExpoCloud will care both for stopping a problem instance as soon as it times out and for not attempting exploring parameter settings that are as hard or harder. The researcher does not need to worry about deciding on the number of compute instances and creating those instances. Neither does he need to worry 5 about stopping compute instances when the experiment is done. The results are easily obtained in a nice tabular format, which is again speciﬁed with a few short methods. If the researcher wants to run the experiment locally, e.g. on his/her laptop, he/she can do that as well. ExpoCloud makes it easy to use as many CPUs of the researcher’s machine as desired. Furthermore, this run is actually a simulation of performing the experiment on the cloud. It is thus a powerful tool to facilitate further development of the framework. ExpoCloud is written in Python and is available on GitHub under the MIT license [1]. The GitHub page contains the user documentation and links to the developer’s documentation, where the source code is described [2]. The next section details the features of the framework and shows in full how the above example experiment is setup and run. Material and methods The overall architecture The overall architecture of ExpoCloud is shown in Figure 1. It is a server- client architecture that uses a pull model to assign jobs to clients. Previous research [4] has shown suitability of such an architecture for distributed scientiﬁc computations. A distinguishing attribute of ExpoCloud is that it creates compute instances on-the-ﬂy. Creating clients on-the-ﬂy enables ExpoCloud to harness the great potential for large-scale concurrency provided by cloud-based compute engines. Creating replacement servers on-the-ﬂy enables ExpoCloud to achieve eﬀective fault tolerance. Two features of the architecture are not shown in Figure 1. First, there are two-way communication channels between the clients and the backup server. We detail the need for these channels in the section on fault tolerance below. Second, a client creates and manages worker processes. Each worker is responsible for executing a single task and communicating the results to the client. 6 Figure 1: The overall architecture of ExpoCloud The next section demonstrates how one can specify the example experiment described in the introduction. We will then show how the individual components of the architecture are implemented to provide time and budget eﬃciency as described in the introduction. The example experiment To set up an experiment, one needs to write a short Python script that creates the primary server object, while providing it with the description of the tasks to be executed, the conﬁguration of the compute engine and other optional arguments. We now show a possible script for exploring the parameter space when solv- ing the agent assignment problem using B&B. Here is the part of the script that constructs the list of tasks: t a s k s = [ ] m a x n t a s k s = 50 n i n s t a n c e s p e r s e t t i n g = 20 f o r o p t i o n s in [ { Option .NO CUTOFFS} , { } , { Option . HEURISTIC } ] : f o r n t a s k s in range ( 2 , m a x n t a s k s + 1 ) : 7 f o r n a g e n t s in range ( n t a s k s , 2 ∗ n t a s k s ) : i n s t a n c e s = g e n e r a t e i n s t a n c e s ( n t a s k s , n a g e n t s , f i r s t i d = 0 , l a s t i d = n i n s t a n c e s p e r s e t t i n g − 1 ) f o r i n s t a n c e in i n s t a n c e s : t a s k s . append ( Task ( Algorithm ( o p t i o n s , i n s t a n c e ) ) ) The outer loop iterates over the three variants of the algorithm: the brute- force search, the classical B&B search and the B&B search with a heuristic. The following two nested loops iterate over the possible values of n and m. For each parameter setting, a task is formed for each of the 20 generated problem instances. This task is added to the list tasks. The key component here is the Task class, which the researcher needs to provide. For our experiment, this class may look as follows: c l a s s Task ( AbstractTask ) : def i n i t ( s e l f , a l g o r i t h m , t i m e o u t = 6 0 ) : super ( Task , s e l f ) . i n i t ( a l g o r i t h m , t i m e o u t ) def p a r a m e t e r t i t l e s ( s e l f ) : return s e l f . i n s t a n c e . p a r a m e t e r t i t l e s ( ) + ( ” Options ” , ) def p a r a m e t e r s ( s e l f ) : return s e l f . i n s t a n c e . p a r a m e t e r s ( ) + ( s e t 2 s t r ( s e l f . o p t i o n s ) , ) def h a r d n e s s p a r a m e t e r s ( s e l f ) : def o p t i o n s 2 h a r d n e s s ( o p t i o n s ) : i f Option . HEURISTIC in o p t i o n s : return 0 i f Option .NO CUTOFFS in o p t i o n s : return 2 return 1 return ( 8 o p t i o n s 2 h a r d n e s s ( s e l f . o p t i o n s ) , s e l f . i n s t a n c e . n t a s k s , s e l f . i n s t a n c e . n a g e n t s ) def r e s u l t t i t l e s ( s e l f ) : return s e l f . a l g o r i t h m . r e s u l t t i t l e s ( ) def run ( s e l f ) : return s e l f . a l g o r i t h m . s e a r c h ( ) def g r o u p p a r a m e t e r t i t l e s ( s e l f ) : return f i l t e r o u t ( s e l f . p a r a m e t e r t i t l e s ( ) , ( ’ i d ’ , ) ) A brief description of each method follows: • parameter_titles - returns the tuple of parameter names, which would appear as column titles in the formatted output. In the example imple- mentation, these consist of the parameters of the problem instance, such as the number of agents and the number of tasks, appended by the pa- rameters of the search algorithm being used. • parameters - returns the tuple of parameter values describing the task. • hardness_parameters - returns the subset of parameters used to deter- mine the task’s hardness. The default implementation in AbstractTask says that task T1 is as hard or harder than task T2 if all the hardness parameters of the former are greater than or equal to the corresponding parameters of the latter. Note how the shown code converts the param- eters of the search algorithm into a number, so as to adapt this default implementation. Internally, the hardness of a task is stored as an instance of the Hardness class deﬁned inside AbstractTask. The Task class derives from AbstractTask and may provide its own deﬁnition of Hardness, thereby 9 gaining full control over the way in which the hardnesses of two tasks are compared. • result_titles - returns the tuple of names of output quantities, such as the optimal time for executing the project and the time taken by the search algorithm. The actual tuple of output quantities is returned by the run method described below. • run - executes the task by running the search algorithm to solve the prob- lem instance. If the algorithm is implemented in Python, as in our exam- ple, the suitable method of the algorithm object is run. Otherwise, the algorithm can be run as a shell command. • group_parameter_titles - returns the tuple of parameter names that determine groups of tasks, as we now explain. Consider a state of the experiment, whereby results for three out of twenty problem instances for a particular setting of parameters have been computed. Suppose that a task timed out at this point, which disqualiﬁed the remaining sixteen tasks as being too hard. It stands to reason that the results for the three executed tasks should be discarded, since the average of the output quantities over only three tasks would have low statistical signiﬁcance. On the other hand, had we obtained results for eighteen instances before a particularly hard task timed out, we may want to keep the results for this setting of parameters. ExpoCloud makes the decision of whether to keep a parameter setting on a per-group basis. A group consists of all the tasks with the same values of the parameters returned by the group_parameter_titles method.2 A group is kept when the number of solved tasks in the group is at least as large as the optional min_group_size argument to the constructor of the server object. In the shown implementation, a group is deﬁned by all 2This is somewhat similar to the idea of the GROUP BY clause in SQL. 10 the parameters besides the id of the problem instance within a particular setting of parameters. The default value of the min_group_size argument is zero, which means that all the results are kept. The next section of the script speciﬁes the conﬁguration for the compute engine and passes this conﬁguration to the constructor of the engine object: c o n f i g = { ’ p r e f i x ’ : ’ agent−a s s i g n m e n t ’ , ’ p r o j e c t ’ : ’ bnb−agent−a s s i g n m e n t ’ , ’ zone ’ : ’ us−c e n t r a l 1 −a ’ , ’ s e r v e r i m a g e ’ : ’ s e r v e r −t e m p l a t e ’ , ’ c l i e n t i m a g e ’ : ’ c l i e n t −t e m p l a t e ’ , ’ r o o t f o l d e r ’ : ’ ˜/ ExpoCloud ’ , ’ p r o j e c t f o l d e r ’ : ’ examples . a g e n t a s s i g n m e n t ’ } e n g i n e = GCE( c o n f i g ) The conﬁguration is a dictionary with the following keys: • prefix - the preﬁx used for the automatically generated names of compute instances. Several experiments with diﬀerent preﬁxes may be conducted simultaneously. • project - the name identifying the project on the cloud platform. • zone - the zone to which the allocated compute instances will pertain. The current implementation of the GCE engine is limited to use a single zone. This limitation may be lifted in the future to enable an even larger scalability. • server_image and client_image - the names of the machine images stor- ing the conﬁguration (such as the CPU family, the number of CPUs, the amount of RAM, etc) of all future server and client instances, respectively. An inexpensive conﬁguration with one or two CPUs may be used for a 11 server, while one may opt for 64 or more CPUs per instance for a client. ExpoCloud’s clients make use of all the available CPUs automatically. • root_folder - the folder in which ExpoCloud resides on all the compute instances. • project_folder - the folder in which the user-provided scripts reside. The folder must be speciﬁed in the dotted format as shown in the listing.3 In our case, the engine being used is the Google Compute Engine (GCE). Some dictionary keys for other engines may diﬀer. For example, zone is a GCE concept and a more suitable key name may be used in the extension class for another platform. Lastly, we construct the primary server object and call its run method: S e r v e r ( t a s k s , e n g i n e ) . run ( ) Once the experiment completes, the main ExpoCloud folder at the primary server will have an output folder containing a results ﬁle and a folder for each client instance. Such a client folder will contain ﬁles with the events sent by the client. ExpoCloud provides a script for convenient viewing of both the results and the client events related to the execution of the tasks. ExpoCloud provides a local machine engine for running an experiment lo- cally. The only change in the above script concerns the construction of the engine object: e n g i n e=L o c a l E n g i n e ( ’ examples . a g e n t a s s i g n m e n t ’ ) Once the experiment completes, the main ExpoCloud folder will have an output folder for each of the servers, as well as a ﬁle for stdout and stderr for each client. Running the experiment locally is useful both for small initial explorations. It also enables rapid development, since it makes it unnecessary 3This is the format in which the path must be speciﬁed when using the -m command-line argument to python. 12 to copy each updated version to the cloud and avoids the latencies associated with using the cloud. We now describe in detail how the primary server operates. The primary server We ﬁrst describe how the primary server stores the tasks, then outline the workings of the run method at a high level, and lastly zoom in on the message- handling part of the primary server. a. The tasks-related lists There are three tasks-related lists - the actual list of tasks and two auxiliary lists used for performance and fault tolerance. We describe them in turn. The list of tasks, called tasks, is sorted in the order of non-decreasing hard- ness of tasks. This order maximizes the number of tasks that are not attempted as a result of a previous task timing out. The original order of tasks is restored prior to the printing of results. The list tasks_from_failed consists of indices of the tasks that have been assigned to a client, but not completed due to a failure of the client instance. When a client requests tasks, the tasks in tasks_from_failed are assigned ﬁrst. The next task from tasks is assigned only if tasks_from_failed is empty. Lastly, the list min_hard consists of hardnesses of tasks that have timed out. Whenever the server is about to assign a task, it ﬁrst checks whether the hardness of the task is equal or greater than any of the elements in min_hard. min_hard is kept small by only storing the minimal elements. b. The run method The primary server object’s run method executes an inﬁnite loop. An iter- ation of this loop performs the following actions: 1. Informs the backup server that the primary server is continuing to function properly. We refer to such a message as a health update. 13 2. Handles handshake requests from newly started instances. The instance can be either a backup server or a client. We refer to the instance that has shaken hands with the primary server as an active instance. In response to a handshake request, two-way communication channel with the instance is established.4 In contrast to this channel, the queue for ac- cepting handshakes is created by the primary server’s constructor. When an instance is started, it gets the IP address of the primary server and the port number for handshake requests as command line arguments. In addition, if the instance is a client, a folder for storing the client events is created. 3. Handles messages from clients. We outline the messages and how they are handled in the next section. 4. Creates either the backup server or a new client instance. The creation of the backup server takes precedence. If the backup server is already running or the researcher opted to not use a backup server for the experiment, then a new client is created. Cloud compute engines do not let users to create instances in quick succession. Therefore, ExpoCloud uses exponentially increasing delays between attempts at creating cloud instances. 5. Terminates unhealthy instances. An active instance is unhealthy if it failed to send health updates to the server for the period of time speciﬁed by the HEALTH_UPDATE_LIMIT constant. A non-active instance is unhealthy if it failed to shake hands with the primary server for the period of time speciﬁed by the INSTANCE_MAX_NON_ACTIVE_TIME constant. 6. Outputs the results once there are no tasks that have not been assigned to clients and all clients completed the tasks assigned to them. The servers do not stop once the results are output. Thus, the fault tolerance 4Namely, the instance owns two queues registered with a SyncManager object. The pri- mary server creates the two corresponding queues at its end. SyncManager is part of the multiprocessing module of the Python standard library. It provides for low-latency commu- nication, which makes the distributed approach eﬀective even for ﬁne-grained tasks. 14 mechanisms continue to protect the results against a possible primary server instance failure. c. The handling of messages The following is an outline of messages that may arrive to the primary server from the backup server and the client instances: • HEALTH_UPDATE - the health update coming from either the backup server or a client. The primary server simply saves the timestamp of the last health update for each instance. • REQUEST_TASKS - the request for tasks by a client. The body of the message speciﬁes the number of tasks requested. If there are remaining unassigned tasks, the GRANT_TASKS message is sent in response. The body of this message contains the tasks assigned to the requesting client, including both the parameters and the full representation of the problem instances to be solved. If there are no unassigned tasks, the response is the NO_- FURTHER_TASKS message. • RESULT - the result of executing a task. The primary server stores the result with the task object. • REPORT_HARD_TASK - the report about a timed out task. The primary server updates the min_hard list and sends the APPLY_DOMINO_EFFECT message to all the clients, so they can terminate any task that is as hard or harder than the task just timed out. • LOG and EXCEPTION - the report about an event related to executing a task or to an exception, respectively, sent by a client. The primary server stores the event in the ﬁtting ﬁle corresponding to the client. • BYE - the client is done, which means that it had sent to the primary server the results for all the tasks assigned to it and had previously received the NO_FURTHER_TASKS message. The primary server terminates the client instance, so the researcher will not incur any further charges. 15 The primary server forwards a copy of each message from a client to the backup server. This keeps the backup server up-to-date and ready to take over should the primary server instance fail. This is further detailed in the section on fault tolerance below. We will now describe the operation of the clients. The clients We ﬁrst describe the main loop of the client object’s run method, then detail how the workers are managed and lastly zoom in on the message-handling part of the client. a. The main loop In contrast to the primary server, the client’s main loop is not inﬁnite – it stops when there are no tasks assigned to the client, and no more tasks that can be assigned to it by the primary server (i.e. the NO_FURTHER_TASKS message has been received). Each iteration of the main loop performs the following actions: 1. Sends the health update to the servers. 2. Processes workers as detailed in the next section. 3. Requests tasks from the primary server, subject to availability of idle CPUs and the NO_FURTHER_TASKS message not having been received. Note that the tasks requested previously, but not yet granted are taken into account when determining how many idle CPUs there are. 4. Processes messages from the primary server as detailed in a separate sec- tion below. 5. If new tasks have been granted by the primary server, starts the worker processes to execute them. Foe each message sent to the primary server, the client sends a copy of the message to the backup server. The need for this is explained in the below section on fault tolerance. That section also details what the client does with the incoming messages from the backup server. 16 Once the main loop is exited, the client sends the BYE message and completes. b. The management of workers Each task is performed by a separate worker process. The client performs three action to manage the workers: • Processes messages arriving from the workers. A worker can send two messages - WORKER_STARTED and WORKER_DONE. In response to either mes- sage, the client sends the LOG message with the corresponding body to the servers. The WORKER_DONE message results in sending the RESULT message as well. • Takes accounting of the worker processes that are no longer alive (i.e. either done or terminated), so as to be able to assign the released CPUs to other tasks. • Terminates processes whose tasks timed out. The client sends the REPORT_HARD_TASK message to the servers for each timed out task. c. The handling of messages from the primary server The following is an outline of messages that may arrive to the client from the primary server: • GRANT_TASKS - one or more tasks have been assigned to the client. The task is added to tasks list and a LOG message is sent to the servers to record the event of the receipt. • APPLY_DOMINO_EFFECT - the hardness of a task that timed out is reported by the primary server. The client terminates all workers currently per- forming tasks of equal or greater hardness. • NO_FURTHER_TASKS - the primary server informs that there are no more tasks to be assigned. The client stores this information, so as to avoid requesting tasks and exit the main loop once all the worker processes are completed. 17 In addition to the above messages, there are the STOP, RESUME and SWAP_- QUEUES messages, used to achieve fault tolerance. These are detailed in the next section. Fault tolerance One standard technique for achieving fault tolerance in a distributed system is by using redundancy [5]. This is the approach we follow by employing a backup server that mirrors the primary one and substitutes for it in the case of a failure. A backup server is not used when the computation is performed using the local machine engine. As mentioned above, the researcher may choose to disable the use of the backup server. This may be desired for a short experiment. We distinguish between three kinds of failure: client instance failure, backup server failure and primary server failure. Client failure does not require any special action besides registering the failure and re-assigning the tasks previ- ously assigned to the failed client. The latter is achieved by maintaining the tasks_from_failed list, as described above. In contrast, care needs to be taken to achieve correctness of recovery after a server failure. The following sections detail how this is achieved. a. Creation of the backup server The primary server creates the backup server in the same way as it creates clients. When a backup server instance does not yet exist, its creation takes precedence over the creation of a new client. Note that the backup server maybe created either at the beginning of the computation or after a server failure.5 Therefore, we need to create the backup server under the assumption that the distributed computation is in progress. To make sure that the newly created backup server is fully synchronized with the primary server, the primary server freezes its state prior to creating 5We will see below how the case of primary server failure is reduced to the case of the backup server failure. 18 the backup server. First, it stops accepting handshake requests from new client instances. Second, it sends the STOP message to the active clients, which causes them to refrain from actions that may result in messages to the server. An exception is made for the health reports, which the clients continue to send. Next, the primary server serializes its full state into a ﬁle in the output folder, creates a new instance on the compute engine, and copies the output folder to it. It then starts the backup server script on the new instance. This script unserializes the server object and runs its assume_backup_role method. As the name suggests, this method converts the primary server object into a backup server one. First, it disconnects the server object from the clients’ channels for communicating with the primary server and connects it to the channels for communicating with the backup server. Second, it creates a two-way channel for communicating with the primary server. Lastly, it shakes hands with the primary server, whereby two-way communication between them is established. Upon this handshake, the primary server resumes accepting handshake requests from new client instances and sends the RESUME message to the clients, so they can resume normal operation. Finally, the backup server script starts the main event loop of the server object. b. Primary and backup server coordination Whenever a new client shakes hands with the primary server, the latter sends the NEW_CLIENT message to the backup server with the information about the client. In response to this message the backup server creates the client object and establishes communication with it. Similarly, whenever the primary server detects a client failure, it sends the CLIENT_TERMINATED message to the backup server, which destroys the corresponding client object. When a client sends a message to the primary server, it sends a copy of the message to the backup server. Thus, the backup server receives two copies of each message sent by the client. The copy received directly from the client is needed for the case when the primary server fails before forwarding the message to the backup server. The copy received from the primary server is needed to 19 keep the two servers synchronized, as described below. The backup server takes actions based on the copy of the message received from the primary server. It simply pops the corresponding message received directly from the client oﬀ the queue. When the backup server registers the pri- mary server failure, it will be ready to take over, with all the messages received directly from the clients after the last message forwarded by the primary server. The backup server processes messages from clients in the same exact way as the primary server. It also sends messages to the clients that mirror the messages sent from the primary server. The mechanism of the backup server taking actions based on the copy of the message received from the primary server takes care of two possible causes of desynchronization. First, a client may fail after sending a message to the primary server, but before sending a copy to the backup server. Second, due to race conditions, it is possible for the two servers to handle messages from diﬀerent clients, such as requests for tasks, in diﬀerent order and end up in diﬀerent states. Similarly to how the backup server processes two copies of each message from a client, the clients processes two copies of each message from the servers - one received from the primary server and the other received from the backup server. A client performs actions only based on the messages received from the primary server and pops oﬀ the queue the corresponding messages received from the backup server. When the primary server fails, the remaining messages received from the backup server are treated as if they were from the primary server, as detailed in the next section. c. Handling server failure In response to the backup server failure, the primary server simply creates the new backup server as outlined in the last section. In the case of the primary server failure, the backup server changes its own role to being the primary server. It then proceeds to create a temporary connec- tion to each client’s inbound queue for communication with the primary server 20 and sends a SWAP message. In response to this message, the client swaps the queues for communicating with the primary server with those for communicat- ing with the backup server. After this, the client is ready to proceed normally. Thus, the case of the primary server failure is now reduced to the case of the backup server failure discussed above. One special case is when the primary server fails after creating a client instance, but before the new client shakes hands and the backup server is up- dated. In this case, there is a dangling client instance incurring charges for the researcher. To avoid this, as part of the backup server assuming the role of the primary server, it requests from the engine the list of all compute instances and deletes all client instances that are not represented by an existing client object. Discussion and conclusions We have presented the ExpoCloud framework for distributed computing us- ing cloud compute engines. Unlike the existing tools geared towards business workloads [6], ExpoCloud is speciﬁcally designed to make it easy to execute large parameter-space explorations. It addresses the four main concerns of the researcher: ease of deﬁning the workload, harnessing as much compute power as can be used to speed up the experiment, eliminating computations that do not or are unlikely to complete in a reasonable amount of time, and avoid- ing unnecessary charges. Combined with mechanisms for fault tolerance, these properties make ExpoCloud a ﬁtting tool for many research projects requiring large-scale parameter-space explorations. Future work may consider executing workloads with task dependencies, integrating ExpoCloud with existing tools, and addressing security concerns. Acknowledgements Access to the Google Compute Engine was provided through the Israel Data Science Initiative. 21 References [1] M. Goldenberg, ExpoCloud’s page on GitHub. URL https://github.com/mgoldenbe/ExpoCloud [2] M. Goldenberg, ExpoCloud developer’s documentation. URL https://expocloud.netlify.app [3] A. Pollack, Tutorial: parallelize your python code and run it on Google Cloud. URL https://youtu.be/i4aFiIB5urA [4] C. Pinchak, P. Lu, J. Schaeﬀer, M. Goldenberg, The canadian internet- worked scientiﬁc supercomputer, 17th International Symposium on High Performance Computing Systems and Applications (HPCS) (2003) 193–199. [5] C. Storm, Fault Tolerance in Distributed Computing, Vieweg+Teubner Ver- lag, Wiesbaden, 2012, pp. 13–79. doi:10.1007/978-3-8348-2381-6_2. URL https://doi.org/10.1007/978-3-8348-2381-6_2 [6] B. Burns, B. Grant, D. Oppenheimer, E. Brewer, J. Wilkes, Borg, omega, and kubernetes, Communications of the ACM 59 (5) (2016) 50–57. 22"
378,COOKIEGRAPH: Measuring and Countering First-Party Tracking Cookies,"[{'href': 'http://arxiv.org/abs/2208.12370v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2208.12370v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-08-25 22:56:31,"TMIC: App Inventor Extension for the Deployment of Image Classification Models Exported from Teachable Machine Fabiano Pereira de Oliveira Department of Informatics and Statistics, Federal University of Santa Catarina Florianópolis/SC, Brazil fabiano.pereira.oliveira@grad.ufsc.br Christiane Gresse von Wangenheim Department of Informatics and Statistics, Federal University of Santa Catarina Florianópolis/SC, Brazil c.wangenheim@ufsc.br Jean C. R. Hauck Department of Informatics and Statistics, Federal University of Santa Catarina Florianópolis/SC, Brazil jean.hauck@ufsc.br Summary TMIC is an App Inventor extension for the deployment of ML models for image classification developed with Google Teachable Machine in educational settings. Google Teachable Machine, is an intuitive visual tool that provides workflow-oriented support for the development of ML models for image classification. Aiming at the usage of models developed with Google Teachable Machine, the extension TMIC enables the deployment of the trained models exported as TensorFlow.js to Google Cloud as part of App Inventor, one of the most popular block-based programming environments for teaching computing in K-12. The extension was created with the App Inventor extension framework based on the extension PIC and is available under the BSD 3 license. It can be used for teaching ML in K-12, in introductory courses in higher education or by anyone interested in creating intelligent apps with image classification. The extension TMIC is being developed by the initiative Computação na Escola of the Department of Informatics and Statistics at the Federal University of Santa Catarina/Brazil as part of a research effort aiming at introducing AI education in K-12. Keywords: Machine Learning, Image Classification, Google Teachable Machine, App Inventor, Extension Statement of need in our daily lives, e.g., as spam filters, recommendation Nowadays, Machine Learning (ML) is present mechanisms, chatbots, digital assistants, etc. Considering its inevitable impact that people understand Machine Learning not just as a consumer, but also as a creator of this type of innovation (Touretzky et al., 2019). Therefore, it is important to start teaching ML concepts in K-12 following a trend emerging during the last few years (Marques et al., 2020). According to the K-12 Guidelines for Artificial Intelligence by AI4K12 (Touretsky et al., 2019), AI education encompasses 5 big ideas, including Machine Learning (ML). This includes an understanding of basic ML concepts as well as the application of these concepts, developing ML applications typically focusing on the task of image classification. Image classification is the process of taking an input (like a photo or videostream) and outputting a class (like “plastic garbage”) or a probability that the input is a particular class (“there’s a 90% probability that this image shows a plastic garbage”). Teaching the application of ML following a human-centric is essential it interactive ML process (Amershi et al., 2019, Gresse von Wangenheim and von Wangenheim, 2021) includes the development, from requirements analysis to exporting the developed model and its deployment (Figure 1). Figure 1. Human-centric interactive ML process The development of ML models is typically taught in K-12 adopting visual tools, such as Google Teachable Machine. Google Teachable Machine (teachablemachine.withgoogle.com) is a free web-based GUI tool for creating custom machine learning classification models without specialized technical expertise (Carney et al., 2020). Google Teachable Machine uses TensorFlow.js, to train and run the users’ models online. It runs within the browser entirely on the user’s device, maintaining any input data locally, protecting data privacy. Google Teachable Machine provides an intuitive workflow-oriented interface supporting the upload of the dataset, model training and evaluation, as well as the prediction of the classes of new data and the export of the trained model (Figure 2). Figure 2. Example of ML model development with Google Teachable Machine Furthermore, it is possible to download a trained model as a TensorFlow.js model and host it on Google Cloud, so it can be deployed easily into any website or app. In this case Google Teachable Machine generates a URL where the model is hosted for free and this link can be shared to use the created model. It can also be converted into a TensorFlow or TensorFlow Lite model and downloaded for local use (Figure 3). Figure 3. Example of exporting trained model as TensorFlow.js is important Metadata, a JSON file indicating the versions of the used libraries, metadata on the user and model Model, a JSON file specifying the model topology Weights, a BIN file specifying the weights of the trained model The exported Tensorflow.js is a zip file including: ● name, as well as a list of the label names and the size of the images used to train the model ● ● The exported trained ML model can then be deployed in software systems, such as mobile applications. The deployment of the trained model to illustrate the usefulness of ML, not only teaching the development of ML models, but also the creating of “intelligent” solutions. Such a deployment as part of IA/ML education in K-12 is typically done within block-based programming environments such as App Inventor (Gresse von Wangenheim et al., 2021). MIT App Inventor (appinventor.mit.edu) is a free web platform that allows users to create mobile applications. Users can design their own applications using drag and drop components and program its behavior using a blocks-based programming language. The App Inventor core already provides a comprehensive set of components, methods and commands for diverse kinds of functionality, including sensors, communication, data storage, etc. It is also possible to further extend App Inventor by providing more components (Patton et al., 2019). Extensions can also be used to incorporate ML features into App Inventor using the App Inventor extension framework (http://ai2.appinventor.mit.edu/reference/other/extensions.html). An example of such an App Inventor extension for integrating custom-trained image classification models is the Personal Image Classifier (PIC) extension (Tang et al., 2019)(Tang, 2019). Support provided by PIC consists of a web application supporting the model development and of the extension with new components for running the trained model in App Inventor apps. However, certain shortcomings regarding the specific web application for the development of the model, such as a lack of evaluation support in V2.0, performance problems of the trained models, and a lack of flexibility allowing the deployment of ML models developed on other environments such as Teachable Machine, indicate a need for further extensions. TMIC Teachable Machine - Image Classifier Extension TMIC is an App Inventor extension for the deployment of image classification models developed in Google Teachable Machine. The extension is based on the PIC extension (Tang et al., 2019)(Tang, 2019), adapting the PIC extension in order to enable the import of TensorFlow.js models created with Google Teachable Machine and exported by uploading them on Google Cloud. The TMIC extension includes the following properties: URL_Model is the property responsible for containing the URL of the model trained with the Google Teachable Machine that has been exported as Tensorflow.js on Google Cloud. WebViewer is the property that allows the user to assign a Web Browser component so that the extension can be used. The Web Browser component is used in order to visualize the functionality of the extension. The TMIC extension provides the following blocks: Blocks of the TMIC extension Functionality The ClassifierReady event block is executed when the extension finishes loading the ML model from the GTM cloud. The GotClassification event block is executed when the extension finishes classifying an image. This event the occurs the of ClassifyVideoData predictions for each category in the model. execution returning of list block, right after the The ClassifyVideoData block starts the classification of the image captured by the smartphone’s rear-facing camera video stream, using the WebViewer component. When the classification is finished, the result is returned via the GotClassification event block. The StopWebcam block stops the webcam when leaving the screen in which the image classification is done. The URL_Model adjustment block allows the user to adjust the ML model URL to another link of the exported GTM model in Google Cloud. TeachableMachineImageClassifier The returns a specific instance of the extension. get block It in which the first is divided into backend and frontend, The TMIC extension was developed using as a base the PIC extension code inside of the App Inventor framework refers to the for creating extensions. TeachableMachineImageClassifier.java file, which initializes the extension and its blocks. Each extension block is defined in the TeachableMachineImageClassifier.java class as a method responsible for executing the action of this block. The front end of the TMIC extension is made up of an HTML file along with four other Javascript files. The extension needs to be rendered in an HTML page to open the camera (asking the user for permission), and display it to the user. Javascript files are needed to load the model and perform the classification of the image when the user requests it. The teachable_machine_image_classifier.js file is mainly responsible for performing the task of communicating with the backend, receiving requests for loading the model, opening the cell phone camera and classifying the image. This file also notifies the backend when the extension is ready and returns the ranking results. The other Javascript files refer to the GTM and TensorFlow.js frameworks, which work together to perform image classification on the mobile device. Its functions are called within the teachable_machine_image_classifier.js code and internally between the files. In total, the TMIC extension is made up of six files, one of the Java extension, one of the HTML and four Javascript extension files. The main methods of the Java class (TeachableMachineImageClassifier.java) are those that will define the behavior of the blocks. Most of them need to communicate with the front-end, calling functions and passing parameters to the file teachable_machine_image_classifier.js, which will process the submitted request. The teachable_machine_image_classifier.js Javascript file functions are responsible for receive requests from the TeachableMachineImageClassifier.java class, process them and return the result in cases where it is necessary to notify that the extension is ready for the usage or prediction results are ready. The classifyVideoData() function is responsible for loading the Teachable Machine cloud hosted template from a user-defined URL in the property URL_Model. The classifyVideoData() function captures the URL of the two JSON files, one containing the model trained and the other with the model metadata, from the URL defined earlier in the preparation of the extension when it is initialized. After assigning the URL of the JSON files to two variables, they are passed as parameters to the function tmImage.load(modelURL, metadataURL), belonging to the Teachable Machine framework file teachablemachine-image.min.js, which will return the model prepared to be used in image classification. After the model is prepared, the model.getTotalClasses() function is called, which returns the number of model classes for the maxPredictions variable. Finally, the function is called model.predict(webcamPredict.canvas), which calls the classifier passing as a parameter the image captured at the exact moment the classifyVideoData() function was executed, returning the prediction result in the prediction variable. Finally, after the prediction variable is already with the classification result, an array with the results is filled in and returned to the Java class, calling the method reportResult(String result), which will prepare the result and notify the event block GotClassification. The TMIC extension is provided with the BSD 3 license (https://opensource.org/licenses/BSD-3-Clause), included in a LICENSE file. The license is available from the TMIC source code in the GitLab repository hosted at the Federal University of Santa Catarina. Also along with the code sources, a NOTICE file is available recognizing that the TMIC was developed by adjusting the PIC code. Currently the TMIC extension supports only Tensorflow.js models exported to Google Cloud and only allows capturing images with the rear-facing camera of the smartphone. We are planning to improve the extension as part of future work. Usage example The extension can be used for teaching ML in K-12, in introductory courses in higher education or by anyone who wants to create “intelligent” apps for image classification. As with any App Inventor extension it can be imported into App Inventor and then used in order to run trained models as part of intelligent apps. In order to support its usage the following material is available (currently in Brazilian Portuguese only): ● TMIC extension .aix ● Example app for the classification of recycling trash .aia (wireframe and final version) ● Online tutorial explaining the use of the extension The extension and material is available online: https://computacaonaescola.ufsc.br/en/tmic/ Acknowledgments This work is supported by CNPq (National Council for Scientific and Technological Development), a Brazilian government entity focused on scientific and technological development. References Armeshi, S. et al. Software Engineering for Machine Learning: A Case Study. In: Proc. of the 41st International Conference on Software Engineering: Software Engineering in Practice, IEEE Press, 2019, 291–300. Carney, M. et al. Teachable Machine: Approachable Web-Based Tool for Exploring Machine Learning Classification. In: Proc. of Conference on Human Factors in Computing Systems, ACM, 2020. C. Gresse von Wangenheim, C. Gresse von Wangenheim. Overview on a human-centric interactive ML process for teaching ML in K-12. Working Paper WP_GQS_01_2021_v10, GQS/INCoD/UFSC, 2021. Gresse von Wangenheim, C.; Hauck, J. C. R.; Pacheco, F. S.; Bertonceli Bueno, M. F. Visual Tools for Teaching Machine Learning in K-12: A Ten-Year Systematic Mapping. Education and Information Technologies, 2021. Marques, L. S., Gresse von Wangenheim, C., Hauck, J. C. R. Teaching Machine Learning in School: A Systematic Mapping of the State of the Art. Informatics in Education, 19(2), 2020. Patton E.W., Tissenbaum M., Harunani F. MIT App Inventor: Objectives, Design, and Development. In: Kong SC., Abelson H. (eds) Computational Thinking Education. Springer, Singapore, 2019. Tang, D., Utsumi, Y., Lao, N. (2019). PIC: A Personal Image Classification Webtool for High School Students. In: Proc.of the IJCAI EduAI Workshop, Macao, China, 2019. Tang, D. (2019). Empowering Novices to Understand and Use Machine Learning With Personalized Image Classification Models, Intuitive Analysis Tools, and MIT App Inventor, M.Eng thesis, MIT, Cambridge, USA. Touretzky, D., Gardner-McCune, C., Martin, F., Seehorn, D. Envisioning AI for k-12: What should every child know about AI? In Proc. of AAAI Conference on Artificial Intelligence, Honolulu, HI, USA, 2019, 9795–9799."
380,TMIC: App Inventor Extension for the Deployment of Image Classification   Models Exported from Teachable Machine,"[{'href': 'http://arxiv.org/abs/2208.12637v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2208.12637v2', 'rel': 'related', 'type': 'application/pdf'}]",2022-08-24 17:34:47,"Dynamic Memory-based Curiosity: A Bootstrap Approach for Exploration Zijian Gao1, Kele Xu1*, YiYing Li2, Yuanzhao Zhai1, Dawei Feng1, Bo Ding1, XinJun Mao1, Huaimin Wang1 1 National University of Defense Technology, Changsha, China 2 Artiﬁcial Intelligence Research Center, DII, Beijing, China kelele.xu@gmail.com 2 2 0 2 g u A 4 2 ] I A . s c [ 1 v 9 4 3 1 1 . 8 0 2 2 : v i X r a Abstract The sparsity of extrinsic rewards poses a serious chal- lenge for reinforcement learning (RL). Currently, many efforts have been made on curiosity which can pro- vide a representative intrinsic reward for effective ex- ploration. However, the challenge is still far from be- ing solved. In this paper, we present a novel curiosity for RL, named DyMeCu, which stands for Dynamic Memory-based Curiosity. Inspired by human curiosity and information theory, DyMeCu consists of a dynamic memory and dual online learners. The curiosity arouses if memorized information can not deal with the current state, and the information gap between dual learners can be formulated as the intrinsic reward for agents, and then such state information can be consolidated into the dynamic memory. Compared with previous curiosity methods, DyMeCu can better mimic human curiosity with dynamic memory, and the memory module can be dynamically grown based on a bootstrap paradigm with dual learners. On multiple benchmarks including Deep- Mind Control Suite and Atari Suite, large-scale empir- ical experiments are conducted and the results demon- strate that DyMeCu outperforms competitive curiosity- based methods with or without extrinsic rewards. We will release the code to enhance reproducibility. Introduction Despite the success of reinforcement learning (RL) on sequential decision-making tasks (Bellemare et al. 2013; Tesauro et al. 1995; Mnih et al. 2015), many current methods struggle with sparse extrinsic rewards. To cope with the spar- sity, curiosity provides a representative intrinsic reward that can encourage agents to explore new states. Designing algo- rithms to efﬁciently construct curiosity can be a key compo- nent in RL systems. Previous research has shown that intrin- sic rewards can help alleviate the issues resulting from the lacking of dense extrinsic rewards (Liu and Abbeel 2021b; Tao, Franc¸ois-Lavet, and Pineau 2020; Yang et al. 2021). For human learning, curiosity motivates people to seek and retain more information through exploration in the envi- ronment (Burda et al. 2018; Ryan and Deci 2000; Smith and Gasser 2005). The process of arousing and satisfying curios- ity can be summed up as one cycle: when a person encoun- ters a problem, he/she will ﬁrst try to solve it by retrieving information from memory. If retrieval from memory fails, he/she realizes that the current memorized information is in- sufﬁcient solve the problem. A conscious awareness of in- formation discrepancy then sparks curiosity about the prob- lem, and curiosity stimulates the search for new informa- tion. Once the information discrepancy is eliminated, people may have no further curiosity to learn more about the cur- rent problem until another problem is encountered (Rotgans and Schmidt 2017; Silvia 2017). Human curiosity is con- stantly consolidated based on the dynamic memory, which consists of the encoding, storing, and retrieving information stage (Hayes et al. 2021). As the curiosity fades, additional information is consolidated into the memory. The consolida- tion results in the forming of new dynamic memories, which depends on the hippocampus (O’Reilly and Rudy 2001). Many attempts have been made to build curiosity in RL, which fall into two main categories: count-based and prediction-based. However, such curiosity is very different from human curiosity, and the problem is far from solved. Taking the Random Network Distillation (RND) (Burda et al. 2019) method as an example, RND initializes a ran- dom ﬁxed target network with state embeddings, and trains another prediction network to ﬁt the output of the target net- work. A random ﬁxed target network can be regarded as a random ﬁxed memory, so that RND cannot retain contextual knowledge about the environment (Yang et al. 2021). With- out dynamically incorporating contextual information into memory, random features may not be sufﬁcient to interpret dynamic environments. Therefore, this kind of curiosity is evaluated in a non-developmental way, which severely lim- its the performance of curiosity in RL. In this work, to mimic human curiosity, we formalize and investigate a Dynamic Memory-based Curiosity mechanism, named DyMeCu. Inspired by the bootstrap paradigm (Guo et al. 2020; Grill et al. 2020; Flennerhag et al. 2021), we construct dual online learners to learn the latent state to formulate dynamic memory model (Figure 1). On the one hand, state information can be consolidated to the memory via the exponential moving average (EMA) (Haynes, Corns, and Venayagamoorthy 2012; Klinker 2011; Grebenkov and Serror 2014) of dual learners’ parameters. The bootstrap The term bootstrap is used in this text in its colloquial meaning *Corresponding author: Kele Xu rather than its statistical connotation. Figure 1: DyMeCu employs a novel learning framework to build the intrinsic reward for RL, which consists of a dynamic memory and dual online learners. The information discrepancy of the current state compared with the retrieved information from the memory makes curiosity arouse. We get the curiosity-based intrinsic reward for agent learning by calculating the information gap between dual online learners. Then the state information can be dynamically consolidated into the memory in the bootstrap paradigm for curiosity fading. paradigm, on the other hand, utilizes supervised signals from memory to improve dual learners’ encoding ability. Further- more, the curiosity is measured by the information gap be- tween the dual learners, which is essentially an uncertainty estimation of given state based on dynamic memory (Mai, Mani, and Paull 2022; Liu et al. 2020; Abdar et al. 2021). In brief, our contribution in this paper is: • We ﬁrstly analyze the shortcomings of previous curiosity-based intrinsic reward methods, and suggest to mimic human curiosity leveraging a dynamic memory in- stead of a ﬁxed one, based on the information theory. • We propose a novel and practicable intrinsic reward method for RL agents, named DyMeCu (Dynamic Memory-based Curiosity), which consists of a dynamic memory and dual online learners, and thus can measure the curiosity and consolidate the information in a feasi- ble way. Meanwhile, different strategies are explored to further improve the performance of DyMeCu. • On multiple benchmarks including DeepMind Con- trol Suite (DMC) (Tunyasuvunakool et al. 2020) and Atari Suite (Bellemare et al. 2013), large-scale empiri- cal experiments demonstrate that DyMeCu outperforms the other competitive curiosity-based methods and pre- training strategies. Related Work Curiosity-Based Intrinsic Reward In RL, the exploration issue is a long standing challenge. Previous attempts suggest that: if there is no additional re- ward, exploration can be regarded as a hunt for informa- tion theoretically, which also can be viewed as the curios- ity (Berlyne 1950; Schmidhuber 1991; Kidd and Hayden 2015; de Abril and Kanai 2018; Jaegle, Mehrpour, and Rust 2019; Friston et al. 2016; Peterson and Verstynen 2021). One intuitive formulation of curiosity is the count-based meth- ods, where the less visited state has more state novelty for exploration. But it can not scale to large-scale or continu- ous state spaces (Kearns and Singh 2002; Charikar 2002). Inspired by count-based methods, RND calculates the state novelty by distilling a random ﬁxed network (target net- work) into another prediction network (predictor network). The predictor network is trained to minimize the prediction error for each state and take the prediction error as the in- trinsic reward. Apart from count-based methods, prediction- based methods also show competitive or better performance by modeling the environment dynamics (Pathak et al. 2017; Pathak, Gandhi, and Gupta 2019; Kim et al. 2020; Burda et al. 2018). With the assumption that more visited state- action pairs will result in more accurate prediction, the in- trinsic reward can be applied as the variance of predic- tions of ensembles or the distance between prediction states and true states, such as the Disagreement method (Pathak, Gandhi, and Gupta 2019) and ICM (Pathak et al. 2017) method. There have been few attempts to design a curios- ity that contains memory and effectively uses information consolidated in memory, which however is the main goal of this paper. Uncertainty Estimation Our work is also related to the uncertainty estimation, as uncertainty is crucial which allows an agent to discern when to exploit and when to explore its environment in RL (Szepesv´ari 2009). Previous intrinsic rewards can also be interpreted from the perspective of uncertainty estima- tion, which can evaluate curiosity by estimating the deep learning model’s uncertainty (conﬁdence). Take Disagree- Curiosity Arouses Curiosity-based Learning & Curiosity Fades Novel State 𝑆𝑡 Encode Latent State Retrieve Information State 𝑠𝑡 Information Discrepancy Memory Model 𝜔 Output Retrieved Information Online Learner 𝜃1 EMA Memory Model 𝜔 EMA Online Learner 𝜃2 𝜃1 𝑧𝑡 loss 𝜔 𝑧𝑡 loss 𝜃2 𝑧𝑡 Stop- gradient 𝑖𝑛𝑡 𝑟𝑡 𝑠𝑡 State 𝜋 Policy 𝑎𝑡 Action 𝑠𝑡+1 Next State Algorithm 1: Dynamic Memory-based Curiosity Initialization: policy network πφ; dual online learner net- works fθ1, fθ2; memory network Mω; coefﬁcients of intrin- sic and extrinsic reward ζ, β. 1: while Training do 2: 3: 4: 5: Receive state st from environment at ← πφ(a|s) based on policy network πφ Take action at, receive state st+1 and extrinsic re- ext from environment ward rt Collect step data into replay buffer st ← st+1 for t = 1, · · · , T do end for Sample batch data as {(si, ai, ri replay buffer for each i = 1, · · · , N do ext, si+1)}N i=1from Generate latent state vectors zθ1 fθ2 (si), zω i = Mω(si) Calculate intrinsic reward rint Calculate total reward ri total = ζri i = (cid:107)zθ1 i − zθ2 i (cid:107)2 int + βrext i i = fθ1 (si), zθ2 i = end for Update θ1 and θ2 with sampled data by minimizing loss with equation (3) Update ω with equation (7) Update φ with sampled data by maximizing rtotal us- ing RL algorithm 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: end while ment as an example, instead of comparing the prediction to the ground-truth, they suggest to evaluate the uncertainty of multiple prediction models using the deep ensemble (Diet- terich 2000), despite incurring additional computation. RND also claims that the distillation error can be viewed as a quantiﬁcation of the uncertainty. Unlike RND, in our work, we evaluate the uncertainty of given states though measur- ing the information gap between dual learners which rely on dynamic memory instead of a random ﬁxed network. Methodology In general, if an agent encounters a state with the informa- tion value E compared to its memory, then this state is worth exploring and such information value is worth consolidat- ing to its memory dynamically (Rotgans and Schmidt 2017; Silvia 2017). In detail, the concept of information value E necessitates the formation of the dynamic memory M and a way g to consolidate information to the memory. For deep neural networks, the memory M can be embedded in the la- tent space and g can by the function that maps state s into memory (Peterson and Verstynen 2021). This kind of con- solidating information is denoted by: g(s; M ) → M (cid:48). (1) With the memory M which has been learned by g over historical states, we can measure the information value E of the next state st+1. According to the information the- ory (Ishii, Yoshida, and Yoshimoto 2002; Reddy, Celani, and Vergassola 2016; Gray 2011) and the concepts proposed in Peterson and Verstynen (2021), the information value E of a state should (1) only depend on the memory and what can be immediately learned (i.e., M , s and g); (2) be non-negative because E is for exploring the environment; (3) decelerate in the ﬁnite time for the same state. Thus we deﬁne E as: E = (cid:107)g(st+1; M ) − M (cid:107) . (2) We can get from the deﬁnition of E as : (1) If one state has been completely explored, or cannot be learned, then no more information gain can be added into the current memory, and E = 0. Such state is no longer worth exploring. (2) If E > 0, then the larger the value of E, the more information gain can be consolidated into the memory. In other words, the larger the value of E, the memory M is less aware of the current state, such state is more worth explor- ing. It is such information deﬁciency of memory that sparks the curiosity of agents. In this paper, we will focus on how to obtain and lever- age the information value E for agents exploration, and the information consolidation method g in details. Dynamic Memory-based Curiosity In our framework, if the information in current memory cannot handle the encountered state, then the curiosity is aroused. We model the memory as a learnable neural net- work, but there is a dilemma that we do not have a “bench- mark” encoded network in the parameter space to encode the encountered state accurately, since not enough supervised signal provided here. Thus it is difﬁcult and not sound to di- rectly deﬁne the curiosity by comparing a random encoded state with the output latent state from a dynamic memory network. We instead introduce dual online learners for state encode representations. These dual learners have the same network architecture as the memory network but with each own different parameters. In their network parameter space, the dual networks are supervised by the memory encoding ability. And then our curiosity can be deﬁned by the gap of the encodings of the same state output by dual learners’ networks. The intuition of our dual learners is: if the state information has been squeezed out by the memory, then the memory can completely know and resolve the state, and the dual learners which can be seen as the two imitators to the memory are easier to get the similar encodings to the current state. In other words, if one state is little known by the mem- ory, then the dual learners may produce quite different en- codings to it, which represents the larger information value E and thus stimulates the agent to explore this state. Here, for the uniform description, we refer the encoded states and encodings to the latent states, which reﬂect the cognition of states by the memory and learner networks. In our implementation, such kind of latent gap E will spark the curiosity as the intrinsic reward for agents explo- ration. After the RL agents learning, such information will be consolidated to the memory for memory better growing. In terms of the consolidation method g, it is externalized as updating the memory parameters via the exponential mov- ing average (EMA) (Haynes, Corns, and Venayagamoorthy Figure 2: Performance of different methods in the ﬁne-tuning phase on DeepMind Control Suite. 2012; Klinker 2011; Grebenkov and Serror 2014) of the dual learners’ parameters. From such analysis, we see the dual learners ﬁrst learn based on the memory network for measure the information value for exploration, and then the memory network consol- idate information gain based on dual learners in the EMA way. The memory is actually seeking for the appropriate po- sition in the parameter space dynamically, in order that its network can better characterize the memory and cognition ability of the seen states in environments. In a word, our dy- namic memory is updated in a bootstrap (Grill et al. 2020) way. Figure 1 and algorithm 1 present the whole framework and pseudo-code of DyMeCu. • Learning of Dual Learners: Dual online learner models fθ1 and fθ2 are deﬁned by a set of weights θ1 and θ2 with the same architecture as the memory network Mω. The memory provides the regression targets for the learning of dual learners fθ1 and fθ2. Given a current state st, the learners transform it into the latent (cid:44) fθ1 (st) and zθ2 states zθ1 (cid:44) fθ2 (st) respectively, and the t t (cid:44) Mω (st). The mean squared memory network outputs zω t error (MSE) between them is: (cid:13) (cid:13)zθ1 (cid:13) (cid:13) (cid:13)zθ2 (cid:13) Based on Lθ1 and Lθ2 , the dual learners are updated as : (cid:26)θ1 ← optim (θ1, ∇θ1Lθ1 , η) , θ2 ← optim (θ2, ∇θ2Lθ1 , η) , where optim and η represent the optimizer and learning rate. (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13) t − zω t t − zω t   Lθ2 Lθ1 (3) (4)  (cid:44) (cid:44) , . • Intrinsic Reward based on Curiosity: The curiosity relies on the information value of current state. In our method, such information value can be mea- sured by the information gap between dual learners. This information gap can also be considered following the δ- Progress (Achiam and Sastry 2017; Graves et al. 2017) to form the curiosity. We obtain the intrinsic reward to agents based on the curiosity from the information value: (5) t (cid:107)2. t − zω t − zθ2 t ) − (zθ2 t )(cid:107)2 = (cid:107)zθ1 t = (cid:107)(zθ1 t − zω rint From another point of view, the dual-learner mechanism can be regarded as the variant of ensemble (Mai, Mani, and Paull 2022) for uncertainty estimation. Compared with pre- vious attempts which requires heavily ensembling (such as the Disagreement), our lightweight solution can previous better performance while retaining computation efﬁciency. Overall, we can get the optimization goal for the agent: max φ Eπφ(st) (cid:104)(cid:88) γt(ζrint t + βrext t (cid:105) ) , (6) where γ is the discount factor and φ represents parameters of policy π; ζ and β are the coefﬁcients of the intrinsic reward and extrinsic reward respectively. • Consolidating Information into Memory: The memory model is updated in an EMA way for sake of its stability to the old state information and the plasticity to the current new state information. In other words, the mem- ory is dynamically growing taking the contextual environ- ment information into account. Speciﬁcally, given a decay rate α ∈ [0, 1] and after each training step, the memory Mω can be updated as: θ1 + θ2 2 • Intuitions on DyMeCu’s behavior: ω ← αω + (1 − α) . (7) The dynamic memory-based curiosity is closer to human curiosity mechanism. It is the cognitive difference compared to the memory that stimulates our curiosity to explore the world, and then we will consolidate the cognition infor- mation to the memory dynamically. In addition, from the knowledge distillation view, such memory can also be re- garded as the teacher model in Mean Teacher-based ap- proach (Tarvainen and Valpola 2017). The memory is essen- tially a self-ensemble of the intermediate models of learners. Table 1: Performance comparison with different pre-training methods on DeepMind Control Suite. The best results are in bold font in each task, and the second best results are underlined. Domain Walker Task Flip Run Stand Walk Average Performance Jump Run Stand Walk Average Performance Quadruped Jaco Reach bottom left Reach bottom right Reach top left Reach top right Average Performance ICM 398±18 216±35 928±18 696±162 560±59 112±4 91±29 184±100 99±46 122±45 102±47 75±27 105±29 93±19 94±31 Disagreement 407±75 291±81 680±107 595±153 494±104 383±265 389±61 628±114 384±28 446±117 117±17 142±3 121±17 131±10 128±12 RND 465±62 352±29 901±8 814±116 633±54 580±72 385±47 800±54 392±39 540±53 103±17 101±26 146±46 99±25 113±29 APT 477±16 344±28 914±8 759±35 624±22 462±48 339±40 622±57 434±64 465±52 88±12 115±12 112±11 136±5 113±10 ProtoRL 480±23 200±15 870±23 777±33 582±24 425±63 316±36 560±71 403±91 426±66 121±22 113±16 124±20 135±19 124±20 SMM DIAYN 381±17 505±26 242±11 430±26 860±26 877±34 661±26 821±36 536±20 659±31 578±46 298±39 415±28 220±37 706±48 367±42 406±64 184±26 527±47 268±36 17±5 40±9 31±4 50±9 11±3 50±7 19±4 37±8 20±4 44±9 APS 461±24 257±27 835±64 711±68 566±46 529±42 465±37 714±50 602±86 578±43 96±13 93±9 65±10 81±11 84±11 Ours 630±37 588±25 965±5 934±16 780±21 694±15 479±6 921±14 833±44 732±20 155±13 146±16 166±14 152±4 155±12 The paradigm we proposed is one type of replay mechanism that is thought to play an important role in memory forma- tion, retrieval, and consolidation (Hayes et al. 2021). More- over, we consider our way to form the memory can also be used in the continual learning to address the issue of catas- trophic forgetting (Arani, Sarfraz, and Zonooz 2021). Experiments and Analysis Experimental Settings We evaluate our method in both pre-training and traditional RL situations utilizing two widely used benchmarks: Deep- Mind Control Suite (DMC) (Tunyasuvunakool et al. 2020) and Atari Suite (Bellemare et al. 2013). We follow the RND (Burda et al. 2019) experimental settings for Atari Suite and settings of URLB (Laskin et al. 2021) which is the pre-training benchmark for DeepMind Control Suite. We ap- ply PPO algorithm (Schulman et al. 2017) to train the agent. The hyper-parameter α was set as 0.99 in all experiments, and the implementation details and hyper-parameters can be found in the appendix. DeepMind Control Suite Many well-performing approaches like URLB (Laskin et al. 2021) use the pre-training and ﬁne-tuning paradigm to im- prove sample efﬁciency for RL, especially in the experiment benchmark like DMC containing various domains and com- plex tasks. We evaluate DyMeCu on all three domains of DMC, namely Walker, Quadruped, and Jaco Arm (from eas- iest to hardest), and each of them has four tasks. During the pre-training phase, the agents are trained for 2 million steps with only intrinsic rewards produced by the curiosity. Dur- ing the ﬁne-tuning phase, the agents are trained for 100k steps with only extrinsic rewards. Table 1 reports the ﬁnal scores and standard deviations of DyMeCu and other competitive methods. We compare DyMeCu with both intrinsic reward-based methods (ICM, Disagreement, RND, APT (Liu and Abbeel 2021b)) and other pre-training strategies (ProtoRL (Yarats et al. 2021), SMM (Lee et al. 2019), DIAYN (Eysenbach et al. 2018), APS (Liu and Abbeel 2021a)). DyMeCu improves average performance by 18.3%, 26.6%, and 21.0% on these three domains respectively. From the quantitative results, we can see our DyMeCu achieve the new state-of-the-art across all 12 tasks, demonstrating DyMeCu’s ability to improve the model performance and robustness through pre-training paradigm. Figure 2 plots 6 learning curves (ﬁne-tuning phase) of DyMeCu and three competitive curiosity-based methods. All learning curves can be found in the appendix. DyMeCu shows a superior convergence speed than other methods. Meanwhile, the convergence result of DyMeCu also surpasses others signiﬁcantly. DyMeCu’s speed in- crease may sbe mainly due to the contextual state infor- mation being consolidated into memory dynamically, rather than a random ﬁxed setting like the RND. Based on the dy- namic memory, the exploration of agents can be much more efﬁcient. Atari Suite For the Atari suite, we ﬁrst record the performance of agents with both intrinsic and extrinsic rewards. The experiments conducts 100M running steps - equivalent to 400M frames and the intrinsic and extrinsic rewards coefﬁcients are set to ζ = 1 and β = 2 respectively for all methods, follow- ing the setup of the previous curiosity-based methods. Ta- ble 2 lists the aggregate metrics and scores of three meth- ods trained with both intrinsic and extrinsic rewards on the Atari 26 games. Human and random scores are adopted from Hessel et al. (2018). As done in previous works (Liu and Abbeel 2021b; Yarats, Kostrikov, and Fergus 2020; Schwarzer et al. 2020), we normalize the episode reward as human-normalized scores (HNS) by expert human scores to account for different score scales in each game. #SOTA denotes the number of games that the current method ex- ceeds other methods and mean HNS is calculated as the average of (agent score − random score)/(human score − random score) of all games. From Table 2, DyMeCu dis- plays the superiority over Disagreement and ICM with its highest mean HNS and #SOTA. Figure 3 displays the learning curves using both intrin- Figure 3: Performance comparison on Atari games subsets using both intrinsic rewards and extrinsic rewards. Table 2: Performance comparison of curiosity-based meth- ods using both intrinsic and extrinsic rewards on 26 Atari games subset. The bold font indicates the best value. Game Alien Amidar Assault Asterix Bank Heist BattleZone Boxing Breakout ChopperCommand Crazy Climber Demon Attack Freeway Frostbite Gopher Hero Jamesbond Kangaroo Krull Kung Fu Master Ms Pacman Pong Private Eye Qbert Road Runner Seaquest Up N Down Mean HNS #SOTA Random 227.8 5.8 222.4 210.0 14.2 2360.0 0.1 1.7 811.0 10780.5 107805.0 0.0 65.2 257.6 1027.0 29.0 52.0 1598.0 258.5 307.3 -20.7 24.9 163.9 11.5 68.4 533.4 0.0 N/A Human 7127.7 1719.5 742.0 8503.3 753.1 37187.5 12.1 30.5 7387.8 23829.4 35829.4 29.6 4334.7 2412.5 30826.4 302.8 3035.0 2665.6 22736.3 6951.6 14.6 69571.3 13455.0 7845.0 42054.7 11693.2 1.0 N/A ICM 1524.7 763.0 1365.5 2103.4 1359.4 51459.1 98.9 247.6 9456.5 135003.3 4679.2 33.8 309.4 14619.4 13482.2 680.8 12922.7 10027.1 40157.7 2787.0 20.1 96.0 16388.9 56273.7 16178.1 46152.9 2.861 7 Disagreement Ours 1304.7 506.6 1544.6 1616.2 1343.4 65387.4 99.3 177.8 10286.9 132614.0 6606.0 33.9 295.1 14202.4 13488.0 726.5 14621.8 11402.7 32607.2 6287.8 20.6 98.0 22474.5 55359.3 2733.1 18235.5 2.767 9 2589.2 470.1 4539.3 4576 1529.5 58220.0 99.6 119.7 9521.0 106682.0 8417.0 30.7 1750.0 9750.3 12728.5 5052.5 10760.0 6447.0 44604.9 2752.4 15.3 100.0 14770.2 32271.0 3910.9 18067.6 3.019 10 sic and extrinsic rewards. We compare DyMeCu with three widely-used baselines, including Disagreement, ICM and RND, on 6 random chosen Atari games. DyMeCu shows evident advantages in most games on the performance and learning speed. For example, on Jamesbond, the conver- gence plot reward of DyMeCu is more than three times that of other methods. Moreover, we also compare the per- formance of agents trained with only intrinsic rewards. As shown in Figure 4, of the 6 environments, DyMeCu out- performs Disagreement baseline in all environments, out- performs ICM and RND baselines both in 4 environments. Overall, the results in Atari Suite show that DyMeCu outperforms other curiosity-based methods, demonstrating DyMeCu’s ability to generate more accurate intrinsic re- wards and provide more useful information for better ex- ploration. Figure 5: Performance comparison among two kinds of de- ployments and baselines with both intrinsic and extrinsic re- wards. Further Analysis on DyMeCu Further analysis including ablation studies on DyMeCu are presented to give an intuition of its behavior and perfor- mance. We run the experiments across 3 random seeds and all following experiments conducts 50M running steps - equivalent to 200M frames. • Dual learners: Here we explore to design the curiosity under the naive setting, that is, using one encoding network to learn to en- code the latent space, and thus the curiosity-based intrinsic reward can be deﬁned as the gap with the memory network: t (cid:107)2. t − zω t = (cid:107)zθ rint (8) The memory is updated with ω ← αω + (1 − α)θ. As shown in Figure 5, one-learner mechanism does not show signiﬁcant advantages over other methods, whereas our dual-learner mechanism performs much better with more ac- curate curiosity and corresponding intrinsic rewards. • Update of memory network: The memory network in DyMeCu is updated with dual learners, we additionally evaluate the performance of DyMeCu when the memory is updated using only one of the Figure 4: Performance comparison on Atari games suite subsets using only intrinsic rewards. Table 3: Performance comparison of baselines and DyMeCu under different settings with only intrinsic rewards. The re- sults represent the average episode reward at the end of train- ing. The Ave. in the last column shows the average result among the three tasks. Game Method Alien Kangaroo MsPacman Ave. Disagreement ICM RND DyMeCu (ours) DyMeCu update with one learner DyMeCu with additional module 316.6 374.2 206.1 492.0 521.7 390.6 514.0 557.0 412.0 739.0 782.0 645.2 291.0 412.7 607.2 602.4 500.6 644.4 373.9 447.9 408.4 611.1 601.4 560.1 learner’s parameters. The results in Table 3 indicate that both learners can consolidate state information into the memory well. Combined with Figure 5, it is useful and necessary to assign and train dual learners, and then we can update the memory with dual or one-learner, while dual-learner update mechanism shows a little superior performance. • Structure of learners: The bootstrap idea has been explored and used in some previous researches. The most similar one to ours is BYOL (Grill et al. 2020), which uses the bootstrap method for self-supervised learning in computer vision. Further- more, Grill et al. add another predictor module to the on- line network, and compare the output of predictor to the tar- get network, and it is the key to generating well-performed representations (Chen and He 2021). Similarly, in this ab- lation study, we also design the controlled trials, in which additional two convolution layers are added to each of dual learners. In Table 3, we can ﬁnd that such learnable addi- tional module does not lead to signiﬁcant improvement. Un- der our analysis, unlike previous work using the bootstrap method, we aim to generate the intrinsic reward by calcu- lating the information value (i.e., information gap between dual learners) as accurate as possible, instead of better rep- resentations for downstream tasks. • Robustness to hyper-parameter α: There is a concern of the updating speed of memory net- Figure 6: Performance comparison with different values of α with only intrinsic rewards. work in the EMA way. It is about how much and how fast to accept and consolidate the new environment informa- tion. Therefore, to further analyze the updating effect of the hyper-parameter α, we evaluate DyMeCu with different val- ues of α in a rational interval, and we assess the agents’ performance in three different Atari games: Alien, Kan- garoo, and Krull. For more direct and visual comparison, we normalize the episode reward of DyMeCu as baseline- normalized scores (BNS) which is calculated as the aver- age of (DyMeCu score − random score)/(baseline score − random score) where the baseline score is the average score of baselines. As illustrated in Figure 6, all values of the hyper-parameter α between 0.99 and 0.9999 yield satisﬁed performance, generally greater than twice that of the base- line average. DyMeCu shows acceptable robustness to the updating hyper-parameter. Conclusion To address the challenge of extrinsic rewards sparsity in RL, we propose DyMeCu to mimic human curiosity in this pa- per. Speciﬁcally, DyMeCu consists of a dynamic memory and dual online learners. The information gap between dual learners sparks the agent’s curiosity and then formulates the intrinsic reward, and the state information can then be con- solidated into the dynamic memory. Large-scale empirical experiments are conducted on multiple benchmarks, and the experimental results show that DyMeCu outperforms com- peting curiosity-based methods under different settings. References Abdar, M.; Pourpanah, F.; Hussain, S.; Rezazadegan, D.; Liu, L.; Ghavamzadeh, M.; Fieguth, P.; Cao, X.; Khosravi, A.; Acharya, U. R.; et al. 2021. A review of uncertainty quantiﬁcation in deep learning: Techniques, applications and challenges. Information Fusion, 76: 243–297. Achiam, J.; and Sastry, S. 2017. Surprise-based intrinsic motivation for deep reinforcement learning. arXiv preprint arXiv:1703.01732. Arani, E.; Sarfraz, F.; and Zonooz, B. 2021. Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System. In International Conference on Learning Representations. Bellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M. 2013. The arcade learning environment: An evaluation plat- form for general agents. Journal of Artiﬁcial Intelligence Research, 47: 253–279. Berlyne, D. E. 1950. Novelty and curiosity as determinants of exploratory behaviour. British journal of psychology, 41(1): 68. Burda, Y.; Edwards, H.; Pathak, D.; Storkey, A.; Darrell, T.; and Efros, A. A. 2018. Large-Scale Study of Curiosity- Driven Learning. In International Conference on Learning Representations. Burda, Y.; Edwards, H.; Storkey, A.; and Klimov, O. 2019. Exploration by random network distillation. In International Conference on Learning Representations. Charikar, M. S. 2002. Similarity estimation techniques from rounding algorithms. In Proceedings of the thiry-fourth an- nual ACM symposium on Theory of computing, 380–388. Chen, X.; and He, K. 2021. Exploring simple siamese repre- sentation learning. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition, 15750– 15758. de Abril, I. M.; and Kanai, R. 2018. Curiosity-driven rein- forcement learning with homeostatic regulation. In 2018 in- ternational joint conference on neural networks (ijcnn), 1–6. IEEE. Dietterich, T. G. 2000. Ensemble methods in machine learn- In International workshop on multiple classiﬁer sys- ing. tems, 1–15. Springer. Eysenbach, B.; Gupta, A.; Ibarz, J.; and Levine, S. 2018. Diversity is all you need: Learning skills without a reward function. arXiv preprint arXiv:1802.06070. Flennerhag, S.; Schroecker, Y.; Zahavy, T.; van Hasselt, H.; Silver, D.; and Singh, S. 2021. Bootstrapped meta-learning. arXiv preprint arXiv:2109.04504. Friston, K.; FitzGerald, T.; Rigoli, F.; Schwartenbeck, P.; Pezzulo, G.; et al. 2016. Active inference and learning. Neu- roscience & Biobehavioral Reviews, 68: 862–879. Graves, A.; Bellemare, M. G.; Menick, J.; Munos, R.; and Kavukcuoglu, K. 2017. Automated curriculum learning for neural networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, 1311–1320. Gray, R. M. 2011. Entropy and information theory. Springer Science & Business Media. Grebenkov, D. S.; and Serror, J. 2014. Following a trend with an exponential moving average: Analytical results for a Gaussian model. Physica A: Statistical Mechanics and its Applications, 394: 288–303. Grill, J.-B.; Strub, F.; Altch´e, F.; Tallec, C.; Richemond, P.; Buchatskaya, E.; Doersch, C.; Avila Pires, B.; Guo, Z.; Gheshlaghi Azar, M.; et al. 2020. Bootstrap your own latent- a new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33: 21271–21284. Guo, Z. D.; Pires, B. A.; Piot, B.; Grill, J.-B.; Altch´e, F.; Munos, R.; and Azar, M. G. 2020. Bootstrap latent- predictive representations for multitask reinforcement learn- In International Conference on Machine Learning, ing. 3875–3886. PMLR. Hayes, T. L.; Krishnan, G. P.; Bazhenov, M.; Siegelmann, H. T.; Sejnowski, T. J.; and Kanan, C. 2021. Replay in deep learning: Current approaches and missing biological elements. Neural Computation, 33(11): 2908–2950. Haynes, D.; Corns, S.; and Venayagamoorthy, G. K. 2012. In 2012 IEEE An exponential moving average algorithm. Congress on Evolutionary Computation, 1–8. IEEE. Hessel, M.; Modayil, J.; Van Hasselt, H.; Schaul, T.; Os- trovski, G.; Dabney, W.; Horgan, D.; Piot, B.; Azar, M.; and Silver, D. 2018. Rainbow: Combining improvements in deep reinforcement learning. In Thirty-second AAAI conference on artiﬁcial intelligence. Ishii, S.; Yoshida, W.; and Yoshimoto, J. 2002. Control of exploitation–exploration meta-parameter in reinforcement learning. Neural networks, 15(4-6): 665–687. Jaegle, A.; Mehrpour, V.; and Rust, N. 2019. Visual novelty, curiosity, and intrinsic reward in machine learning and the brain. Current opinion in neurobiology, 58: 167–174. Kearns, M.; and Singh, S. 2002. Near-optimal reinforcement learning in polynomial time. Machine learning, 49(2): 209– 232. Kidd, C.; and Hayden, B. Y. 2015. The psychology and neu- roscience of curiosity. Neuron, 88(3): 449–460. Kim, K.; Sano, M.; De Freitas, J.; Haber, N.; and Yamins, D. 2020. Active world model learning with progress cu- In International conference on machine learning, riosity. 5306–5315. PMLR. Klinker, F. 2011. Exponential moving average versus mov- ing exponential average. Mathematische Semesterberichte, 58(1): 97–107. Laskin, M.; Yarats, D.; Liu, H.; Lee, K.; Zhan, A.; Lu, K.; Cang, C.; Pinto, L.; and Abbeel, P. 2021. URLB: Unsu- pervised Reinforcement Learning Benchmark. In Deep RL Workshop NeurIPS 2021. Lee, L.; Eysenbach, B.; Parisotto, E.; Xing, E.; Levine, S.; and Salakhutdinov, R. 2019. Efﬁcient exploration via state marginal matching. arXiv preprint arXiv:1906.05274. Liu, H.; and Abbeel, P. 2021a. Aps: Active pretraining with successor features. In International Conference on Machine Learning, 6736–6747. PMLR. Szepesv´ari, C. 2009. Synthesis Lectures on Artiﬁcial Intelli- gence and Machine Learning. Synthesis lectures on artiﬁcial intelligence and machine learning. Tao, R. Y.; Franc¸ois-Lavet, V.; and Pineau, J. 2020. Novelty search in representational space for sample efﬁcient explo- ration. Advances in Neural Information Processing Systems, 33: 8114–8126. Tarvainen, A.; and Valpola, H. 2017. Mean teachers are better role models: Weight-averaged consistency targets im- prove semi-supervised deep learning results. Advances in neural information processing systems, 30. Tesauro, G.; et al. 1995. Temporal difference learning and TD-Gammon. Communications of the ACM, 38(3): 58–68. Tunyasuvunakool, S.; Muldal, A.; Doron, Y.; Liu, S.; Bohez, S.; Merel, J.; Erez, T.; Lillicrap, T.; Heess, N.; and Tassa, Y. 2020. dm control: Software and tasks for continuous con- trol. Software Impacts, 6: 100022. Yang, T.; Tang, H.; Bai, C.; Liu, J.; Hao, J.; Meng, Z.; and Liu, P. 2021. Exploration in deep reinforcement learning: a comprehensive survey. arXiv preprint arXiv:2109.06668. Yarats, D.; Fergus, R.; Lazaric, A.; and Pinto, L. 2021. Reinforcement learning with prototypical representations. In International Conference on Machine Learning, 11920– 11931. PMLR. Yarats, D.; Kostrikov, I.; and Fergus, R. 2020. Image aug- mentation is all you need: Regularizing deep reinforcement learning from pixels. In International Conference on Learn- ing Representations. Liu, H.; and Abbeel, P. 2021b. Behavior from the void: Un- supervised active pre-training. Advances in Neural Informa- tion Processing Systems, 34. Liu, J.; Lin, Z.; Padhy, S.; Tran, D.; Bedrax Weiss, T.; and Lakshminarayanan, B. 2020. Simple and principled uncer- tainty estimation with deterministic deep learning via dis- tance awareness. Advances in Neural Information Process- ing Systems, 33: 7498–7512. Mai, V.; Mani, K.; and Paull, L. 2022. Sample Efﬁcient Deep Reinforcement Learning via Uncertainty Estimation. arXiv preprint arXiv:2201.01666. Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Ve- ness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidje- land, A. K.; Ostrovski, G.; et al. 2015. Human-level control through deep reinforcement learning. nature, 518(7540): 529–533. O’Reilly, R. C.; and Rudy, J. W. 2001. Conjunctive repre- sentations in learning and memory: principles of cortical and hippocampal function. Psychological review, 108(2): 311. Pathak, D.; Agrawal, P.; Efros, A. A.; and Darrell, T. 2017. Curiosity-driven exploration by self-supervised prediction. In International Conference on Machine Learning, 2778– 2787. PMLR. Pathak, D.; Gandhi, D.; and Gupta, A. 2019. Self-supervised exploration via disagreement. In International Conference on Machine Learning, 5062–5071. PMLR. Peterson, E. J.; and Verstynen, T. D. 2021. Curiosity eliminates the exploration-exploitation dilemma. bioRxiv, 671362. Reddy, G.; Celani, A.; and Vergassola, M. 2016. Infomax strategies for an optimal balance between exploration and exploitation. Journal of Statistical Physics, 163(6): 1454– 1476. Rotgans, J. I.; and Schmidt, H. G. 2017. The role of interest in learning: knowledge acquisition at the intersection of sit- uational and individual interest. In The science of interest, 69–93. Springer. Ryan, R. M.; and Deci, E. L. 2000. Intrinsic and extrinsic motivations: Classic deﬁnitions and new directions. Con- temporary educational psychology, 25(1): 54–67. Schmidhuber, J. 1991. A possibility for implementing cu- riosity and boredom in model-building neural controllers. In Proc. of the international conference on simulation of adap- tive behavior: From animals to animats, 222–227. Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and Klimov, O. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Schwarzer, M.; Anand, A.; Goel, R.; Hjelm, R. D.; Courville, A.; and Bachman, P. 2020. Data-Efﬁcient Re- inforcement Learning with Self-Predictive Representations. In International Conference on Learning Representations. Silvia, P. J. 2017. Curiosity. In The science of interest, 97– 107. Springer. Smith, L.; and Gasser, M. 2005. The development of em- bodied cognition: Six lessons from babies. Artiﬁcial life, 11(1-2): 13–29."
418,NYU Explores the  Principles for Modelling Neural Collapse and Its Role in Generalization,https://syncedreview.com/2022/07/06/nyu-explores-the-principles-for-modelling-neural-collapse-and-its-role-in-generalization/,2022-07-06,"Deep neural networks (DNNs) have advanced the state-of-the-art on tasks ranging from image classification to language processing and gameplay. But as models have become deeper and more complex, understanding their behaviours has become more challenging. A case in point is an intriguing empirical phenomenon called Neural Collapse, first identified by Papyan et al. in 2020. In the new paper Neural Collapse: A Review on Modelling Principles and Generalization, researchers from New York University analyze the principles of Neural Collapse (NC) and present a thought model designed to explain the effect of variance collapse, aiming at insights on and a better understanding of the generalization capabilities of DNNs. The team summarizes their main contributions as: The modern training paradigm for DNNs involves training well beyond the zero error threshold and toward zero loss. This post-zero-error phase is called the Terminal Phase of Training (TPT), which begins at the epoch where training error first vanishes. During TPT, the training error stays effectively zero while the training loss is pushed to zero. The TPT however is exposed to a pervasive inductive bias, NC, which involves four deeply interconnected phenomena: The team uses a principled approach to review the NC phenomena, first confirming that the final layer classifiers in DNNs tend to fall into a simple symmetric structure that helps the models obtain their high performance and state-of-the-art results. In an effort to capture the essence of the NC phenomena, the researchers then analyze such models from the ground up and unify them under a common set of principles. Overall, the paper provides a solid overview of current efforts to explain NC, It also probes the implications of NC on generalization and transfer learning via a thought model that explains the effects of variance collapse on transfer learning based on the inverse-square law to provide additional insights on the generalization capabilities of DNNs. The team hopes their work’s analytical results will be of interest to the deep learning community and encourage future research in this area.The paper Neural Collapse: A Review on Modelling Principles and Generalization is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates."
422,A Comparison of Source Distribution and Result Overlap in Web Search   Engines,"[{'href': 'http://arxiv.org/abs/2207.07330v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2207.07330v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-07-15 07:58:49,"2 2 0 2 l u J 8 1 ] R C . s c [ 1 v 8 7 9 8 0 . 7 0 2 2 : v i X r a A Security & Privacy Analysis of US-based Contact Tracing Apps Joydeep Mitra Department of Computer Science Stony Brook University joydeep.mitra@stonybrook.edu July 20, 2022 Abstract With the onset of COVID-19, governments worldwide planned to develop and deploy contact tracing apps to help speed up the contact tracing process. However, experts raised concerns about the long-term privacy and security implications of using these apps. Consequently, several proposals were made to design privacy-preserving contact tracing apps. To this end, Google and Apple developed the Google/Apple Exposure Notiﬁcation (GAEN) framework to help public health authorities develop privacy-preserving contact tracing apps. In the United States, 26 states used the GAEN framework to develop their contact tracing apps. In this paper, we empirically evaluate the US-based apps to determine 1) the privileges these apps have, 2) if the apps comply with their deﬁned privacy policies, and 3) if they contain known vulnerabilities that can be exploited to compromise privacy. The results show that all apps violate their privacy policies and contain several known vulnerabilities. 1 Introduction Contact tracing is one of many methods health authorities use to contain the rapid spread of COVID-19 [1]. However, manual contact tracing to keep track of the growing pandemic has been a challenge for health authorities due to the lack of human resources and the limitations of human memory to remember all possible contacts accurately [2]. Consequently, there have been several eﬀorts to automate the contact tracing process by developing mobile apps that use tracking tech- nology in mobile phones, such as GPS and Bluetooth [3, 4]. In addition, since mobile phones are ubiquitous, they can be used to eﬀectively and quickly identify a person’s contacts when they test positive for COVID. 1.1 Background 1.1.1 How do Contact Tracing Apps Work? Contact tracing apps work by estimating if two mobile phones, X and Y, are close to each other based on a metric deﬁned by local health authorities (e.g., 6 feet). Y will be notiﬁed of a potential infection if X tests positive and conﬁrms the result. An app detects proximity using Bluetooth 1 Low Energy (BLE), Global Positioning System (GPS) technology, or a combination of the two. If two phones are nearby, then the apps exchange encounter messages, which contain, among other things, random identiﬁers and signal strength. Apps may also include location data in the encounter messages. Each app keeps a record of all the encounter messages it has exchanged with other apps. If one of these messages is from a phone whose user has tested positive, then the app analyzes the encounter message to calculate the level of risk based on factors the local health authorities decided. This calculation is performed either locally in the device or by a central server. While contact tracing apps can be useful to help expedite the process of contact tracing, experts have warned that such technologies have long-term consequences for user privacy and security since they can be easily exploited and used in malicious contexts such as mass surveillance [5, 6, 7]. Therefore, contact tracing apps’ design and implementation must be carefully scrutinized before being deployed and adopted. 1.1.2 Contact Tracing App Architectures The need to ensure privacy in contact tracing apps has led to several proposals of app architectures that will best protect user privacy – centralized, decentralized, and hybrid [8, 9]. In a centralized architecture, a central server collects and stores personally identiﬁable information (PII), which is further used to generate random identiﬁers for the encounter messages. Further, the central server is responsible for determining the users that have been potentially exposed. Government health authorities have access to the data stored in the central server and can use it to perform advanced aggregate analysis, which can be useful for understanding trends to help inform mitigation eﬀorts. However, this approach negatively aﬀects user privacy since the central server is assumed to be trusted and has access to vast amounts of personal information, which unauthorized or hostile entities can potentially misuse. On the other hand, a decentralized server has minimal data. Most of the data is maintained locally on the user’s device. Each app periodically communicates with the server to download a set of random identiﬁers that have tested positive. The app then performs a risk analysis of its encounter messages locally to determine if it had come close to a device that tested positive. This architecture helps preserve privacy better than a centralized architecture. However, it limits access to crucial data that could be used to analyze the spread of the pandemic or to understand the eﬀectiveness of the app. The hybrid architecture combines features of the centralized and decentralized architecture, where random identiﬁer generation is left to the local device, whereas the server manages the risk analysis and exposure notiﬁcation. Countries have developed and deployed apps based on a centralized architecture (e.g., Singa- pore’s TraceTogether [10]) and decentralized architecture (e.g., the apps based on the Google/Apple framework). Further, many apps also used location data to enable automated contact tracing (e.g., India’s Arogya Setu [11]). However, there is no consensus among researchers about which approach is the most feasible for eﬀective contact tracing while minimizing privacy risks [12, 13]. 1.1.3 The Google Apple Exposure Notiﬁcation System In May 2020, Google and Apple collaborated to develop the Google Apple Exposure Notiﬁcation (GAEN) framework based on the decentralized architecture to help public health authorities develop contact tracing apps [14, 15]. A GAEN app works by generating a temporary key, which changes periodically. The key is used to encrypt locally stored data and to generate a random identiﬁer. The app embeds the identiﬁer 2 in encounter messages and exchanges them with other apps using BLE. When a user tests positive, a public health oﬃcial uses a veriﬁcation server to send the user a conﬁrmation code. The user can use the conﬁrmation code to upload all their recent keys (e.g., last 14 days) to a key server. Each app downloads keys periodically from the key server and compares them with a set of keys exchanged in the last few days. If there is a match, the app determines the risk of the potential exposure based on a formula pre-determined by public health authorities. 1.2 Motivation In the absence of an oﬃcial national contact tracing app in the United States, individual US states used the GAEN framework to develop their own contact tracing apps. To date, 26 US states have developed a GAEN app. Despite the security and privacy guarantees built into the GAEN framework, people in the US lack conﬁdence in the apps’ ability and intentions to protect their privacy [16]. The Center for Disease Control and Prevention (CDC) in the US recommends that healthcare authorities should conduct a third-party assessment of contact tracing apps and make the results publicly available [9]. However, according to the technology assessment conducted by the United States Government Accountability Oﬃce (GAO) at the behest of the US Congress, most states with a contact tracing apps have not conducted a third-party assessment. Moreover, the states that have conducted an evaluation, have not made the results publicly available [9]. Motivated by the CDC’s recommendations and the lack of assessments of the US-based apps, in this paper we are analyzing the privacy and security of GAEN-based Android apps for contact tracing in the US. Speciﬁcally, we are asking the following research questions: • RQ1: What degree of privilege do the GAEN-based Android apps for contact tracing have? Android apps by default have least privilege. They need to request the system or users for permission to perform privileged operations (e.g., use Bluetooth). The purpose of this question is to understand the permissions used by these apps and their privacy implications. • RQ2: Do GAEN-based Android apps for contact tracing violate their own privacy policies? Contact tracing apps are required to publish a privacy policy to inform users of the app’s capabilities and its data sharing, storage and retention policies. The purpose of this question is to determine if the privacy policy is consistent with the behavior encoded in the app’s source code. RQ3: Do GAEN-based Android apps for contact tracing contain known Android app vulner- abilities? Android apps can have vulnerabilities that can be exploited by malicious apps available locally or remotely to compromise users’ privacy. The purpose of this question is to identify if GAEN-based contact tracing apps have similar vulnerabilities. 2 Methodology In this section, we describe the apps selected, the tools chosen, and the factors we considered to answer our research questions. 2.1 App Selection We selected the oﬃcial contact tracing apps in Android of all US states developed using the exposure notiﬁcation APIs by Google and Apple (GAEN). We considered these apps since the focus of our 3 App State Alabama Arizona California Colorado Connecticut Delaware DC Guam Hawaii Louisiana Maryland Michigan Minnesota Nevada New Jersey New Mexico New York North Carolina North Dakota & Wyoming Pennsylvania Utah Virginia Washington Wisconsin App Name GuideSafe Covid Watch Arizona CA Notify CO Exposure Notiﬁcations COVID Alert CT COVID Alert DE DC CAN Guam Covid Alert AlohaSafe Alert COVID Defense MD COVID Alert MI COVID Alert COVIDaware MN COVID Trace Nevada COVID Alert NJ NM Notify COVID Alert NY SlowCOVIDNC Care19 Alert Version Name 1.10.0 2.1.11 minted1000003 minted141006 1.0.1 Package Name gov.adph.exposurenotiﬁcations gov.azdhs.covidwatch.android gov.ca.covid19.exposurenotiﬁcations minted14020 gov.co.cdphe.exposurenotiﬁcations gov.ct.covid19.exposurenotiﬁcations gov.de.covidtracker gov.dc.covid19.exposurenotiﬁcations minted1100019 org.pathcheck.guam.bt org.alohasafe.alert org.pathcheck.la.bt gov.md.covid19.exposurenotiﬁcations minted151008 gov.michigan.MiCovidExposure org.pathcheck.covidsafepathsBt.mn gov.nv.dhhs.en com.nj.gov.covidalert gov.nm.covid19.exposurenotiﬁcations minted1200004 gov.ny.health.proximity gov.nc.dhhs.exposurenotiﬁcation com.proudcrowd.exposure 1.4 1.17.12 minted1200005 1.0.1 1.0.10 1.0.15 1.9.1 1.1.5 1.6 1.2 Version Code 2764 201011 minted14020 10000032 141006 15 11000192 1947 41 2661 151008 255 3503 12000052 20 12000042 81 205 10 COVID Alert PA UT Exposure Notiﬁcations COVIDWISE WA Notify WI Exposure Notiﬁcation gov.pa.covidtracker gov.ut.covid19.exposurenotiﬁcations minted1100011 1.5 giv.vdh.exposurenotiﬁcation gov.wa.doh.exposurenotiﬁcations minted142004 gov.wi.covid19.exposurenotiﬁcations minted141003 2.0.0 46 11000112 160 142004 141003 Table 1: US-based contact tracing GAEN apps. App size Download (in MB) Date 6.70 3.56 10.07 3.38 9.94 105.55 11.8 64.52 64.64 7.37 10.19 3.19 3.19 12.12 105.62 3.9 105.90 3.1 7.23 Nov 12, 2021 Nov 19, 2021 Oct 15, 2021 Nov 19, 2021 Nov 12, 2021 Dec 3, 2021 Dec 3, 2021 Jan 19, 2022 Nov 5, 2021 Feb 4, 2022 Mar 11, 2022 Oct 15, 2021 Mar 11, 2022 Mar 25, 2022 Nov 5, 2021 Mar 4, 2022 Oct 8, 2021 Nov 5, 2021 Nov 12, 2021 105.68 11.82 9.32 10.36 9.86 Oct 14, 2021 Dec 3, 2021 Mar 24, 2022 Nov 5, 2021 Mar 19, 2022 study is to examine GAEN apps based in the US. We found the apps from the Android Developer’s oﬃcial page [17]. Each US-based app has a link to Google Play. We used the links to download the corresponding APK ﬁle from Google Play in an Emulator running an Android version supported by the app. We transferred the apk ﬁles from the emulator to a computer where we could reverse engineer and statically analyze the apps. All US states did not develop a GAEN app. Further, North Dakota and Wyoming use a single app. The Massachusetts app is built into the device and can only be installed from the device’s settings, not from Google Play. We could not obtain this app since we were using an emulator to install the apps. Consequently, we did not consider the Massachusetts app in our assessment. In total, we ended up with 24 apps. Table 1 lists all the selected apps along with their name, package, version information, size of the APK ﬁles, and when we downloaded them. 2.2 Tool Selection Research in mobile app security and privacy has led to the development of several tools and tech- niques to detect vulnerabilities and malicious behavior [18, 19]. The tools are based on static and dynamic analysis. Most use static analysis to either ﬂag vulnerabilities or malicious behavior or guide subsequent dynamic analysis. Few tools use only dynamic analysis. Prior research eﬀorts studying the eﬃcacy of such tools have observed that for freely available tools, static analysis tools detect more known Android app vulnerabilities than dynamic analysis tools [20]. Furthermore, among the static analysis tools, MobSF [21] detects the most known vulnerabilities. Based on this observation, we used MobSF as the primary tool for analysis. MobSF has a static and dynamic analyzer. The static analyzer statically analyzes the app’s source code to determine if the app uses APIs and features that are known to cause Android app 4 Figure 1: Snippet from a report generated by MobSF. Each issue has a severity, security standards it is associated with, and the source ﬁle/s in which it was detected vulnerabilities (see Figure 1). We failed to run the dynamic analyzer on the contact tracing apps that we had selected. Hence, we considered only the static analyzer. We used Androguard [22] to generate the control ﬂow graph of an app (see Figure 2), which was used to analyze the data ﬂow through the app. We tracked the data ﬂow in the control ﬂow graph to determine if data ﬂowing out of the app is sensitive or if the data being used by the app is potentially malicious. This was necessary to verify the potential data leak and data injection vulnerabilities reported by MobSF’s static analyzer, as it is known to report false positives. Speciﬁcally, we used the following strategies: • We conﬁrmed a potential data leak vulnerability if there was a path in the control ﬂow graph from a pre-identiﬁed sensitive data source node to a target node with shared storage, network, or inter-app communication APIs. Sensitive data sources include APIs used to collect user input (e.g., biometric), read from app’s private ﬁles and communication channels (e.g., Bluetooth), and strings hard coded with personal information (e.g., IP address). • We conﬁrmed a data injection vulnerability if there was a path in the control ﬂow graph from a pre-identiﬁed source node of potentially malicious data such as shared storage, network, or inter-app communication APIs to a target node and the target node was using the data without sanitizing it. Examples of a target node using potentially malicious data after sani- tization include an exported broadcast receiver that uses input data only if it was sent via an 5 intent-ﬁlter with a system-deﬁned action or a function in a target node that uses input data from a trusted remote server. Figure 2: Snippet of the list of edges in a control ﬂow graph generated by the AndroGuard tool for the California app. The source and target columns indicate the nodes in the graph. Each row is a directed edge connecting the source node to the target node. Each node has an ID and an API call contained in the source code. 2.3 Policy Analysis A focus of our study is to determine if an app’s source code is consistent with the app’s privacy policy. To this end, we downloaded each app’s privacy policy and examined them. We identiﬁed the features that the privacy policy of an app claims the app does not use (e.g., does not collect location). All selected apps in their privacy policies claim the following: 1. does not collect, store, or transmit any personally identiﬁable data. 2. stores exposure data (e.g., random IDs and exposure date) locally in the users’ device. 3. prevents unauthorized access to locally stored data. 4. encrypts locally stored data. 5. communicates with trusted servers through encrypted networks in the United States. We looked for these features in the apps using MobSF and Androguard. If at least one such feature was found in the app, then we deemed that the app violates its own privacy policy. For example, using MobSF, we determined instances in an app’s source code where data was being stored in external storage. We then used the app’s control ﬂow graph (generated by Androguard) to determine the source of the data stored in external storage and whether the source is sensitive. If sensitive data was being stored in external storage, then we deemed it as a violation of the app’s privacy policy due to bullet three listed above. 6 2.4 Known Vulnerabilities Analysis Android apps have vulnerabilities, which malicious apps exploit to cause harm to the user [23, 24, 25, 26]. Therefore, it is necessary to ensure such vulnerabilities do not occur in apps, especially contact tracing apps, which deal with sensitive personal information and can perform privileged operations on the phone. Hence, we analyzed these apps for known Android app vulnerabilities. We used MobSF, and the Ghera repository [25] for our analysis. MobSF provides a list of potential vulnerabilities in its static analysis report. We investigated each of them to determine their veracity as MobSF is known to report false positives. Moreover, MobSF does not detect all known vulnerabilities. Therefore, we used Ghera, a repository of 60 known vulnerability benchmarks, to further guide our analysis. Each benchmark in Ghera is well-documented and contains only the features related to the vulnerability captured in the benchmark. We statically analyzed the 24 apps in our set using MobSF to determine the features/APIs used in them. We then considered only those features also used in the Ghera benchmarks. We investigated each such feature to determine if it resulted in a potentially exploitable vulnerability, that is, it could be exploited by a malicious app on the device or remotely. For example, an app can give unrestricted access to another app using the pending intent feature in Android. If MobSF found an app with a component using pending intent, we investigated the component to conﬁrm if it performed privileged operations, which, if true, could result in a potential privilege escalation attack. 3 Results In this section, we report the ﬁndings of our study in terms of the permissions requested and used by the apps, their potential privacy violations, and the potential vulnerabilities in them that can be exploited to cause harm to the user. 3.1 RQ1: What degree of privilege do the GAEN-based Android apps for contact tracing have? A total of eight permissions are used across all 24 contact tracing apps. An app uses approximately seven permissions on average. The least number of permissions used by an app is six, and the highest is eight. These statistics are higher than the average permissions used by apps in general, which is ﬁve [27], with nearly 100k of one million apps using zero permissions as of June 2014. Furthermore, ﬁve of the eight permissions are used by all 24 apps, which suggests that only ﬁve of them are necessary for contact tracing. The other three permissions are most likely extraneous. Therefore, the GAEN apps in Android are over-privileged, which is concerning since these apps have access to vast amounts of personal data. Over-privileged apps increase the risk of being exploited since they expose a larger attack surface due to having more privileges than needed [28]. Therefore, developers of these apps should carefully consider the required permissions and ensure that only the necessary ones are used by their apps. All permissions, except two, are normal permissions [29], that is, granted by Android when the app is installed. Therefore, the apps do not need to ask the user for permission at runtime; they always have them. In the context of contact tracing, where many users install these apps to help prevent the spread of the pandemic, this permission model compromises privacy by not giving users control of granting or denying permission to the apps. Furthermore, it assumes that the apps are benign and that users should trust them during installation. This assumption hinders an app’s 7 Permissions Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y N N N N Y N Y Y Y N N Y N Y N Y N N Y N N N N Y N Y Y Y N N Y N Y N N N N Y Y Y Y Y N Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 # Permissions per app App 8 Y Alabama 8 Y Arizona 6 Y California 6 Y Colorado 6 Y Connecticut 7 Y Delaware 6 Y DC 8 Y Guam 8 Y Hawaii 8 Y Louisiana 6 Y Maryland Michigan 6 Y 8 Y Minnesota 8 Y Nevada 8 Y New Jersey 8 Y New Mexico New York 7 Y 6 North Carolina Y North Dakota 8 Y & Wyoming Penn Utah Virginia Washington Wisconsin # Apps per permission N N N N N N N N N N N N N Y N Y N N Y N Y N N N N N N N N N N N N N N N N N N Y N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N Y N Y N N Y Y Y Y Y Y 24 Y Y Y Y Y 24 Y Y Y Y Y 24 Y Y Y Y Y 24 Y Y Y Y Y 24 Y Y Y Y Y 23 Y N N N N 9 N N N N N 2 N N N N N 2 N N N N N 7 N N N N N 2 N N N N N 2 7 6 6 6 6 Table 2: Permissions used by the US-based GAEN apps as declared in their manifest ﬁle. The permissions names are encoded as PN due to lack of space. The exact permission names are listed in Table 3 Permission Code Permission Name P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 android.permission.INTERNET android.permission.VIBRATE android.permission.RECEIVE BOOT COMPLETED android.permission.BLUETOOTH android.permission.ACCESS NETWORK STATE android.permission.ACCESS WIFI STATE android.permission.WAKE LOCK android.permission.FOREGROUND SERVICE com.google.android.c2dm.permission.RECEIVE com.google.android.ﬁnsky.permission.BIND GET INSTALL REFERRER SERVICE android.permission.USE BIOMETRIC android.permission.USE FINGERPRINT Table 3: Name of permissions used in all US-based GAEN apps. 8 adoption in an environment where the role of government-deployed contact tracing apps is being seen with suspicion [30]. However, app developers cannot resolve this issue as Android, not the apps, deﬁne these permissions. Therefore, platform developers should either consider changing the way these permissions are granted or deﬁne custom permissions that will be used in the context of contact tracing. Google-based third-party analytics libraries, not the core Android system, deﬁne two permis- sions. Only two of the 24 considered apps use these permissions. Therefore, this raises the question if these permissions are necessary for contact tracing. Further, such apps are designed to have the necessary permissions to access privileged operations in the device (e.g., Bluetooth). In this context, should contact tracing apps further increase security and privacy risks by using third-party libraries that are not directly related to the task of contact tracing? The USE FINGERPRINT permission is deprecated [31]. Two of 24 apps use it. While using deprecated permissions is not recommended, this is not a major concern as both the apps use the permission in conjunction with the USE BIOMETRIC permission, which is the recommended permission to use instead of USE FINGERPRINT. Nevertheless, apps should not use deprecated permissions since they may have unknown and unexpected security and privacy implications. Two of the 24 apps use biometric permissions to access the device capabilities to use and collect biometric information. While this is not a violation of privacy by itself, it increases the risk of exposing private sensitive user info to unauthorized entities. Furthermore, since a majority of the GAEN apps are not using biometric permissions, it raises questions about the necessity of using such capabilities to collect and transmit biometric-related data. The ACCESS WIFI STATE permission is used by 17 of the 24 selected apps. This suggests that not all apps need this permission for contact tracing. Apps use this permission when they connect to a remote server through WiFi. Using WiFi is not always secure since WiFi networks may not be protected and may be susceptible to Man-In-The-Middle attacks. Therefore, apps that collect and transmit sensitive information over the internet should avoid WiFi communication. The VIBRATE permission is used by nine of 24 apps to control the device’s vibration. This permission is not necessarily benign. If used incorrectly or maliciously, it may damage a user’s phone [27]. Therefore, it is best to avoid using such permissions if not absolutely necessary. Since a majority of the selected apps do not use the VIBRATE permission, this permission is not likely necessary for GAEN apps. Only one of the 24 apps declares a query element in its manifest ﬁle to indicate the list of apps it can communicate with. Speciﬁcally, the Arizona app declares that the query elements android.intent.action.DIAL and android.intent.action.SEND in its manifest ﬁles. This implies that the Arizona app has capabilities to place phone calls and share data with any app with the SEND intent. In the context of contact tracing, these capabilities are unnecessary and pose additional risk to the security and privacy of the app’s users. 3.2 RQ2: Do GAEN-based Android apps for contact tracing violate their own privacy policies? All US-based GAEN apps violate their privacy policy since their behavior is inconsistent with at least one of the claims in their policy. This is concerning since contact tracing apps collect and store vast amounts of private data, which can be potentially misused. Therefore, they should take additional care to guarantee their users’ privacy or at least be consistent with their own policies. The GAEN apps are designed not to collect, store, or track location. However, 20 of the 24 9 GAEN apps collect users’ locations despite claiming otherwise in their privacy policy. On further analysis, we discovered that these apps are not explicitly collecting location. However, they use a library called TwilightManager that collects user location to determine the local time. Apps that have conﬁgured dark themes automatically import and use this library. Therefore, these apps violate their privacy policy due to using a library that collects location. Consequently, this raises the question if app developers are aware of the privacy implications of the libraries they used in their apps. Vetting the libraries before using them is especially crucial for contact tracing apps, which malicious actors can potentially misuse to compromise user privacy. The Nevada and New Mexico apps collect Biometrics for authentication or to encrypt locally stored data. However, the privacy policy of these apps does not explicitly state that they col- lect biometric information. Moreover, they mention that the apps do not collect any personally identiﬁable information. Hence, we deem these apps as violating their own privacy policy. All apps claim in their privacy policy that they store exposure-related data in local storage in a way that prevents unauthorized access. However, nine of the 24 apps store their data in external storage. Any app installed on the device (including malicious apps) can access this data if they have the necessary permission to access external storage. Therefore, all other apps can potentially access the exposure-related data stored by these nine apps. Consequently, this leads to a violation of privacy as deﬁned in the apps’ privacy policy. This issue could have been addressed by using the app’s internal storage instead of the external storage because, in Android, the internal storage of an app can only be accessed by the app. Moreover, Android recommends that an app’s data should be stored in internal storage unless it needs to be shared with other apps. Considering more than a third of the selected apps used external storage instead of internal storage suggests that several developers of these apps are not aware of the diﬀerence between the two. This lack of knowledge is concerning since the apps collect and store vast amounts of personal information. If access to the data is not minimized, then they can be potential targets for cyberattacks by malicious actors. The privacy policy of all the selected apps mentions that data stored locally in the device is protected by encryption. However, six of the 24 apps use the AES block cipher in ECB mode, which is a weak cipher. Weak encryption is a violation of the apps’ privacy policy of encrypting local data since it leads to a potential leak of sensitive data. This result implies that a signiﬁcant number of developers are not aware that AES in ECB mode should be avoided despite several security guidelines, such as OWASP, recommending not to use it. One likely reason that developers end up using the ECB mode is that this is the default mode for AES encryption in Android. As a result, developers must explicitly change the mode. However, this oversight is surprising since the focus of the GAEN apps was to ensure user privacy by storing personally identiﬁable information locally and protecting them via strong encryption. Failure to use strong encryption despite claiming to do so in their privacy policies raises questions about their diligence in protecting users’ data from misuse. Fourteen of the 24 apps use HTTP to communicate with remote servers. Consequently, their communication can be potentially hijacked by Man-In-The-Middle attacks. Furthermore, this is inconsistent with the apps’ stated privacy policy that mentions that communication with remote servers is encrypted end-to-end. Considering that HTTP is used in more than half of the selected apps implies that the developers of these apps are either unaware of the implications of using HTTP or did not do due diligence to verify that all communication with remote web servers uses HTTPS. The use of HTTP is concerning, especially since using HTTPS is an essential requirement of apps that transmit sensitive personal information to remote web servers. Moreover, the developers of apps promoted for large-scale use and with access to vast amounts of personal information must be 10 more perceptive of such issues and take extra care to avoid them. Exposure notiﬁcation apps transmit exposure-related data (e.g., random IDs and exposure date) to remote servers when a user consents to share their data in the event of an exposure to COVID-19. The apps’ privacy policy does not mention the location of the servers. Therefore, users reasonably assume that the servers are in the United States. However, 13 of the 24 apps communicate with exposure notiﬁcation servers outside the United States. Furthermore, the privacy policy of two of the 13 apps explicitly states that the apps only communicate with servers in the United States. Therefore, this raises the question if the apps are sending exposure-related data of US-based residents outside the US. The apps should be more transparent and explicitly state in their policies the location of the exposure notiﬁcation servers so users can make informed decisions. 3.3 RQ3: Do GAEN-based Android apps for contact tracing contain known Android app vulnerabilities? We discovered a total of six known vulnerabilities across all 24 apps, and each app had approximately two vulnerabilities on average. All 24 apps had at least one of the six vulnerabilities. Table 5 shows the breakdown of the vulnerabilities found in each app. The vulnerabilities are brieﬂy described as follows: • Unprotected Component. Android apps consist of components. Apps can export their com- ponents to share operations or data with other apps. However, if components are exported without restrictions, all apps, including malicious apps, can access them. Consequently, it can lead to denial-of-service, data leak, and data injection attacks. • Insecure PRNG. Apps that use the Random package in Java to generate pseudo-random numbers can be more easily predicted than apps that use the SecureRandom package. Since contact tracing apps based on GAEN rely on random identiﬁers, they should use a random number generator that makes it harder to predict the random numbers. Therefore, apps should use the SecureRandom package instead of the Random package to generate hard-to- predict random identiﬁers. • Weak hashing. Hashing algorithms such as MD5 and SHA-1 are considered weak. Attackers can use a hash collision to forge a duplicate hash. Therefore, apps should avoid using them to prevent forgery attacks. • Data Backup. Android allows users to create backups of all data in an app without having root privileges. Consequently, malicious users with access to the device will be able to create a backup of all the app’s data using a USB. Apps can be conﬁgured to protect against this potential attack by setting the allowBackup attribute in the app’s manifest ﬁle to false. Android recommends apps to disable this feature to prevent malicious users from accessing an app’s local data. • Insecure TLS/SSL Implementation. Apps using TLS/SSL protocol to communicate with remote servers must verify the trustworthiness of the servers. The established way to verify trust is for the app to maintain a list of trusted certiﬁcate authorities (CAs). The server is conﬁgured with a certiﬁcate containing a public key and a matching private key. The certiﬁcate must be signed by a certiﬁcate authority (CA) trusted by the app. Generally, the list of trusted CAs is pre-installed in the device on which the app is installed. However, the 11 connection may fail if the certiﬁcate used to conﬁgure the server (1) is signed by a CA, not in the list of trusted CAs, (2) or is self-signed, (3) or is signed by an intermediate certiﬁcate missing from the server conﬁguration. While the third reason is addressed at the server-side, the ﬁrst two reasons are addressed by implementing a custom TrustManager, an Android API, in the app. Implementation mistakes in the custom TrustManager lead to vulnerabilities (e.g., trust all CAs) that can lead to Man-In-The-Middle attacks. • Unpinned Certiﬁcates. Apps installed in a device trust all CAs that are pre-conﬁgured with the device. App developers can further restrict the CAs that the app will trust by pinning a set of trusted CAs to the app. The app then trusts only the pinned CAs and not any other CA, including the ones trusted by the device. Although not mandatory, certiﬁcate pinning is good security practice. However, they should be used with care as they can hamper usability due to communication failure because of outdated certiﬁcates as a result of changes to the server conﬁguration. Twelve of the 24 apps had at least one unprotected component vulnerability. We found sev- eral manifestations of this vulnerability. For example, apps used a third-party library that had an unprotected broadcast receiver that could write to shared preferences. Malicious apps could potentially exploit this vulnerability to execute data injection attacks. In other instances, apps deﬁned an unprotected activity that could access Bluetooth and location. As a result, malicious apps could potentially exploit this activity to get access to privileged features without having the necessary permissions. Furthermore, few apps deﬁned an activity that could share exposure-related diagnostic information with a remote server. Since this activity was exported without restriction, malicious apps could potentially exploit this vulnerability to communicate with the remote server. Apps must protect these components by making them private to the app. If these apps need to share these components with the underlying system, then they must protect the components with system permissions. All apps, except two, chose to use an insecure PRNG to generate the random identiﬁers used to identify devices where the app is installed anonymously. Choosing an insecure PRNG over a more secure PRNG allows malicious actors to potentially predict the random identiﬁer more easily, which could be used to create duplicate identiﬁers and hence create erroneous or fake exposure-related data entries. Therefore, all the selected apps must use the SecureRandom package to generate random identiﬁers. Seven of the 24 apps use MD5 or SHA-1, which are weak hashing algorithms. Choosing a weak hashing algorithm for contact tracing apps is problematic since these apps are expected to provide strong privacy and security guarantees. A strong hashing algorithm is the most basic requirement with which the app can ensure the integrity of the information it stores. A sixth of the 24 apps, that is, four apps allow users to back up the app’s data without rooting the device. This is insecure for GAEN apps since they store a user’s and their contacts’ exposure- related data locally in the app. A malicious user with access to the device but without root access can get access to this data and misuse it. Apps that have this feature should disable it by setting the allowbackup attribute in their manifest ﬁle to false. Five of the 24 apps chose to use certiﬁcate pinning instead of relying on the device’s list of trusted CAs. While certiﬁcate pinning provides additional protection against MITM attacks, most apps choose not to use it, possibly because of potential connection failures due to the pinned certiﬁcates becoming outdated. In such situations, the only way to restore the app is by pushing a software update, which the users must install. Temporary connection failures are not ideal and could render 12 an app useless. However, in the context of the GAEN framework, where communication with remote servers is minimal1 and security is paramount, it is advisable for the apps to use certiﬁcate pinning to reduce the risk of an MITM attack. Further, apps could pin backup certiﬁcates to prevent relying on only one pinned certiﬁcate. If one of them is outdated, the app can use the backups to connect to the server. MobSF reported an insecure SSL implementation vulnerability in exactly one app (the Arizona app) because the app was using a pinned self-signed certiﬁcate. Apps pinning self-signed certiﬁcates have beneﬁts and limitations in terms of preventing MITM attacks. Consider a scenario where an app has pinned a certiﬁcate signed by a CA that has been compromised. In this situation, the app will need to be updated via a software update with a newly issued certiﬁcate. On the other hand, if the app had pinned a self-signed certiﬁcate, then the app only trusts that certiﬁcate and will not be aﬀected if any other CA’s certiﬁcate is compromised. However, using self-signed certiﬁcates are secure only if they are continuously monitored. In its absence, app developers may not know about a compromised certiﬁcate, and the app’s communication with the compromised server will continue unknowingly. However, actively maintaining self-signed certiﬁcates is more cumbersome than using certiﬁcates from a trusted CA. Trusted CAs can revoke compromised certiﬁcates used by a server to stop communication between the app with the pinned certiﬁcate and the server, protecting the user from further harm. Therefore, in the general case, it is more secure to pin certiﬁcates signed by trusted CAs instead of self-signing them. 4 Discussion 4.1 Observations on the apps The results show that contact tracing apps in Android based on the GAEN framework are over- privileged. Further, in Android, the apps are granted permission to use privileged system features at install time. As a result, users have less control over granting permission to these apps at runtime. This is concerning since these apps could potentially be used for tasks other than contact tracing, such as mass surveillance. Therefore, the apps must collect minimal information and use the least privileges. The GAEN framework was developed to help create apps that preserve user privacy. How- ever, our analysis shows that all apps violate their own privacy policy due to several potential reasons, such as developer oversight and developers’ lacking domain knowledge and awareness of the underlying platform. Oversight is concerning but understandable since contact tracing apps were developed hurriedly to tackle the challenges of a growing pandemic. However, oversight in this context leads to a lack of transparency and credibility. It exacerbates the skepticism that the general public has towards contact tracing apps and hampers their widespread adoption. Weak adoption is not desirable since contact tracing apps if used eﬀectively, are a vital tool to contain the pandemic. If developer oversight is the reason for privacy violations, then there is a need to develop tools and techniques that help developers write privacy policies that are consistent with their app’s behavior and vice versa. Few apps in our study missed mentioning in their privacy policy all the personally identiﬁable information that they collected in their apps. One possible reason for this is a lack of domain 1Users only connect with the remote server to upload a positive test result. Also, apps periodically connect with the remote server to check for positive cases. 13 knowledge in developers. In this context, privacy research eﬀorts should focus on developing meth- ods to help identify domain-speciﬁc personally identiﬁable information. Contact tracing apps have features that could be misused to violate users’ privacy. Therefore, it is crucial to accurately iden- tify the information that the apps collect so users can make informed decisions about the privacy implications of using the apps. The GAEN framework made it easier for healthcare providers to create apps for eﬀective contact tracing without the need to know the details of the underlying platform [9]. This is also evident from certain privacy violations, which could have been avoided if developers had known about the underlying platform behavior. Therefore, existing research in app security and privacy should focus on developing methods and tools to assist less experienced developers gain the necessary knowledge to avoid privacy violations. The selected apps had vulnerabilities that are well known in the app development community. In fact, all four vulnerabilities that were discovered are part of the OWASP top 10 [32], a popular set of guidelines for developing secure mobile and web apps. The apps had these vulnerabilities despite the focus on ensuring that the apps are secure and preserve user privacy. This suggests that the app developers did not have the experience to avoid these mistakes, or they did not have access to tools to help them prevent these vulnerabilities eﬀectively. The latter is less likely since app development IDEs such as Android Studio have support for detecting and preventing such vulnerabilities during development. Therefore, it is more likely that the states did not allocate the resources to recruit developers with suﬃcient experience in mobile app development. While it is understandable that states need to prioritize their resources to tackle a pandemic, they should have planned better before developing and deploying apps with long-term consequences for user privacy and security. 4.2 Observations on MobSF While investigating the potential vulnerabilities MobSF reported, we discovered that a few of them were false positives, that is, falsely reported as vulnerabilities. We report and discuss them to help tool developers like MobSF improve their tools. Brieﬂy, MobSF reported a total of ﬁve false positives across the 24 apps. Further, MobSF reported ﬁve false positives for all apps except the Virginia app and the Arizona app, which had three and four false positives, respectively. We explain the false positives, their likely reasons, and suggestions on how to avoid them as follows: • The SQLInjection vulnerability was falsely reported in 23 of the 24 apps. MobSF reported the vulnerability in apps using the execSQL API to execute SQL queries. However, using execSQL leads to SQLInjection only if the query string has user-supplied input with potentially malicious SQL. None of the queries reported as being potentially vulnerable to SQLInjection relied on user input. Hence, they were not vulnerable to SQLInjection. Suggestion: MobSF should ﬂag queries in execSQL only if they rely on user-input and are not parameterized. • MobSF falsely reported an unprotected component vulnerability in all 24 apps because it found components in these apps that were exported. However, these components were also protected by system permissions, that is, only the system could be granted access to these components. Furthermore, these permissions were deﬁned as part of the GAEN framework and were made available only for the purpose of contact tracing to government healthcare 14 authorities. Therefore, apps without necessary authorization cannot request these permissions and get access to the exported components. Suggestion: MobSF should consider the system permissions deﬁned in the GAEN framework during analysis to improve the detection of unprotected components. • MobSF reported that 22 of the 24 apps saved sensitive data such as usernames, passwords, and secret keys in clear text. Furthermore, MobSF claimed that 22 of the 24 apps logged sensitive data. On further inspection, we found the claims to be false. MobSF reported the apps because they were saving or logging string constants in ﬁles and the strings contained words such as ”key” and ”password,” but these constants were not sensitive data. Suggestion: Considering that none of the string constants with words like key and password were sensitive data in our sample of apps, MobSF should consider using other heuristics to identify sensitive data. • MobSF falsely reported a Janus signature vulnerability in all 24 apps. Janus is a system vulnerability that allows attackers to inject a DEX ﬁle into an APK ﬁle signed with the v1 signature scheme without aﬀecting the signatures. The vulnerability can be exploited because an Android package can be a valid DEX ﬁle and APK ﬁle at the same time. However, this vulnerability is exploitable only if the app runs on an Android version lower than 7.0. Android developers ﬁxed this vulnerability in version 7.0 and above. As a result, Android APKs or packages have to be signed with the v2 and v3 signature schemes. All 24 contact tracing apps we considered used the v2 and v3 signature schemes to sign their APKs. MobSF ﬂagged the apps as potentially vulnerable to Janus because the apps were also signed with the v1 signature scheme to enable backward compatibility. We categorize this as a false positive since the app developers have no choice but to sign their apps with the v1 signature scheme to support Android versions less than 7.0. Moreover, the app developers are doing due diligence by signing the apps with v2 and v3 signature schemes along with the v1 scheme (for backward compatibility), which is the best they can do under the circumstance. Suggestion: MobSF should ﬂag apps as potentially vulnerable to Janus if they are signed only with the v1 signature scheme. For apps signed with v1,v2, and v3 signatures, MobSF should consider reporting a diﬀerent label like an information label to inform the developers that while this cannot be ﬁxed at the app stage, one should be aware that the v1 signature is vulnerable on Android versions less than 7. Therefore, apps might want to consider supporting only Android version 7.0 or more. In conclusion, MobSF reported more false positives than potential true positives w.r.t potential vulnerabilities in each app (see Tables 5 vs. 6), which shows that MobSF has a high false positive rate. This observation is consistent with prior research eﬀorts to evaluate the eﬀectiveness of security analysis tools for Android apps [20]. In general, static analysis tools like MobSF have a high false positive rate, which hampers their adoption and reduces their eﬀectiveness. Due to a high false positive rate, MobSF users have to manually verify the warnings and issues reported, which reduces trust in the tool verdicts. Consequently, this hampers tool adoption and the overall eﬀectiveness of the tool. 15 5 Related Work Since the onset of the pandemic, numerous proposals have been made to automate the contact In this context, many contact tracing apps have been implemented and tracing process [4, 33]. deployed worldwide. This has led to a plethora of eﬀorts to survey the characteristics and evaluate the eﬀectiveness of contact tracing apps [34, 35, 36]. Several agencies worldwide have called for an independent assessment of the security and privacy risks posed by contact tracing apps for greater transparency and accountability [9]. In this context, researchers have evaluated the security and privacy of the contact tracing apps to understand if they pose any privacy risks to the users [37, 38]. In this section, we discuss the eﬀorts that are most closely related to our work and how they are diﬀerent. Wen et al. [39] performed a systematic study of 41 contact tracing apps deployed on Android and iOS. They used program analysis to determine the APIs relevant for contact tracing and identify the information collected by the apps. Additionally, they performed a cross-platform comparison of apps available on Android and iOS. Their results show that some apps expose identiﬁable information that can enable ﬁngerprinting of apps and tracking of speciﬁc users. Moreover, they observed that some apps exhibited inconsistencies across platforms, which led to diﬀerent privacy implications across the platforms. Their eﬀort was one of the ﬁrst attempts to understand the privacy and security implications of contact tracing apps. Although their eﬀort is related to our evaluation, we focus on diﬀerent aspects. For example, in addition to vulnerability analysis, we also analyze the privacy policy of the apps, which their eﬀort does not consider. Samhi et al. [34] conducted an empirical evaluation of Android apps in Google Play related to COVID-19. The aim of their study was to broadly characterize the apps in terms of their purpose, intended users, complexity, development process, and potential security risks. While their focus was not on security and privacy, they observed that none of the apps they considered leaked sensitive data based on static analysis of the apps using tools FlowDroid and IccTA. However, recent eﬀorts to measure the eﬀectiveness of security analysis tools in Android have raised questions on the eﬀectiveness of static analysis tools like FlowDroid and IccTA in detecting sensitive data leaks. Hatmian et al. [40] analyzed the privacy and security performance of 28 contact tracing apps available on the Android platform in May-June 2020. They analyzed the permissions used by the apps and the potential vulnerabilities in them. Further, they measured the coverage of the privacy policy of the apps w.r.t the privacy principles outlined in the General Data Protection Regulation (GDPR) laws [41]. Our work is diﬀerent from theirs in a number of ways. First, we focus on oﬃcial apps developed by the US states based on the GAEN framework. None of the four US apps in the study conducted by Hatmian et al. are based on the GAEN framework. Second, instead of measuring the coverage of the apps’ privacy policies w.r.t the privacy principles, we analyze if the apps’ privacy policies are consistent with their source code, that is, apps are not violating their own privacy policies. Third, we critically analyze the verdicts reported by tools such as MobSF instead of reporting them as is. For example, Hatmian et al. report, based on MobSF’s analysis, that all apps they considered log sensitive information. In our evaluation, MobSF also ﬂagged all apps as logging sensitive information. However, after manually verifying the verdict, we discovered this was a false positive for all apps. In November 2020, Baumgartner et al. [37] demonstrated that the GAEN framework’s design is vulnerable to proﬁling and de-anonymizing infected persons and relay-based wormhole attacks that are capable of generating fake contacts to derail the contact tracing process. They claimed that if the vulnerabilities are not addressed, then all apps based on the framework will be vulnerable. 16 Instead of analyzing the GAEN framework’s design, in this paper, we are analyzing the apps based on the GAEN framework from the perspective of whether the apps comply with their own privacy policy and if they contain vulnerabilities that manifest due to known implementation bugs and incorrect conﬁgurations. Ang et al. [42] reviewed the security and privacy of 70 contact tracing apps one year after the pandemic. They statically analyzed the apps using MobSF for vulnerabilities based on threat sce- narios they identiﬁed for contact tracing apps. Additionally, they reported data trackers embedded in apps that can potentially violate privacy since data collected by the trackers can be used with- out the users’ consent. However, their privacy analysis does not include any analysis of an app’s privacy policy. The set of apps in the evaluation by Ang et al. includes 20 apps that we considered in our assessment. However, the results of our analysis diﬀer signiﬁcantly. For example, 80% of the apps they considered stored sensitive information in cleartext. On the other hand, we found this to be a false positive for 20 of the 24 apps we considered. Similarly, we discovered vulnerabilities in our analysis (e.g., data backup) that are not reported in Ang et al. Further, Ang et al. used dynamic analyzers such as VirusTotal [43] to detect the presence of malware in the contact tracing apps. However, malware detection tools such as VirusTotal do not accurately detect the presence of malware as they often falsely identify Potentially Unwanted Programs (PUPs) as malware [20]. Kouliaridis et al. [44] investigated all oﬃcial contact tracing apps deployed by European coun- tries as of Feb 2, 2021. They analyzed the apps both statically and dynamically. Static analysis included sensitive permissions and API calls, third-party trackers, and known vulnerabilities and conﬁgurations that aﬀect app security based on the Common Weakness Enumerations (CWEs) [45] and Common Vulnerabilities and Exposures (CVEs) [46]. Dynamic analysis involved instrumenting the app’s source code, verifying if the app uses location and Bluetooth services at runtime, and in three monitoring network traﬃc. The evaluation in our paper diﬀers from Kouliaridis et al. distinct ways. First, we considered a diﬀerent set of apps. Second, we analyzed the apps’ privacy policies to determine if they are consistent with their encoded behavior. Third, we did not dynam- ically analyze the apps. Further, there are notable diﬀerences in our static analysis observations. For example, Kouliaridis et al. report that two-thirds of their apps, which included GAEN apps, had a potential SQL injection vulnerability based on MobSF’s analysis. However, we observed that MobSF falsely reported SQL injection as a vulnerability in 23 of the 24 GAEN apps we considered. 6 Caveats In vulnerability analysis, we considered vulnerabilities in Ghera and reported by MobSF. Since we did not cover vulnerabilities outside these sources, it is possible that they existed in the apps but went unreported. Consequently, our vulnerability analysis may not be comprehensive. App developers reading this report must take steps to ﬁx the reported vulnerabilities and perform further analysis to ensure that other vulnerabilities not reported here do not exist in their apps. We conﬁrmed the potential vulnerabilities reported by static analysis tools by manually examin- ing them. However, we did not build malicious applications to exploit the vulnerabilities. Therefore, we do not know to what extent the vulnerabilities are exploitable. The results reported in this study are limited to the set of apps considered or the GAEN apps in general. Therefore, they should not be generalized for other contact tracing apps, especially apps not based on the GAEN framework. 17 7 Conclusion In this paper, we conducted a systematic investigation of 24 contact tracing apps based on the GAEN framework in the US. All the apps were implemented and deployed by the oﬃcial health departments of the respective US states. We discovered that the considered apps are over-privileged, they violate their own privacy policies, and contain vulnerabilities that can be exploited by malicious users to cause harm to the app’s users. While there have been previous eﬀorts at evaluating the contact tracing apps for privacy violations and vulnerabilities, none of them have focused on the consistency of the apps’ privacy policies w.r.t to their encoded behavior. Although there are similarities between our eﬀort and existing eﬀorts in terms of vulnerability analysis of contact tracing apps, the results diﬀer markedly. Our results show that few vulnerabilities reported as potential vulnerabilities in related evaluations are false positives. For example, several existing research eﬀorts have reported the Janus vulnerability, which we reported as a false positive, as a true positive in their results. Therefore, this raises the question if eﬀorts to study the privacy and security of contact tracing apps are reporting vulnerabilities that may not manifest in reality. Reporting false positives as potential true positives may erode the public’s trust in contact tracing apps and eventually lead to reduced adoption, which may ultimately weaken eﬀorts to contain the pandemic. Therefore, there is a need for researchers to continuously evaluate the security and privacy of contact tracing apps to reproduce and verify the results. 8 Ackowledgements We wish to thank Minqi Shi, Taylor Giles, Soroush Semerkant, Mihir Madhira, Jeﬀrey Jiminez, Colin Ruan, and Patrick Wszeborowski, undergraduate students in the Department of Computer Science at Stony Brook University for assisting with data collection. References [1] D. Klinkenberg, C. Fraser, and H. Heesterbeek, “The eﬀectiveness of contact tracing in emerg- ing epidemics,” PloS one, vol. 1, no. 1, p. e12, 2006. [2] T. Jiang, Y. Zhang, M. Zhang, T. Yu, Y. Chen, C. Lu, J. Zhang, Z. Li, J. Gao, and S. Zhou, “A survey on contact tracing: the latest advancements and challenges,” ACM Transactions on Spatial Algorithms and Systems (TSAS), vol. 8, no. 2, pp. 1–35, 2022. [3] A. Anglemyer, T. H. Moore, L. Parker, T. Chambers, A. Grady, K. Chiu, M. Parry, M. Wilczyn- ska, E. Flemyng, and L. Bero, “Digital contact tracing technologies in epidemics: a rapid review,” Cochrane Database of Systematic Reviews, no. 8, 2020. [4] N. Ahmed, R. A. Michelin, W. Xue, S. Ruj, R. Malaney, S. S. Kanhere, A. Seneviratne, W. Hu, H. Janicke, and S. K. Jha, “A survey of covid-19 contact tracing apps,” IEEE access, vol. 8, pp. 134 577–134 601, 2020. [5] A. Akinbi, M. Forshaw, and V. Blinkhorn, “Contact tracing apps for the covid-19 pandemic: a systematic literature review of challenges and future directions for neo-liberal societies,” Health Information Science and Systems, vol. 9, no. 1, pp. 1–15, 2021. 18 [6] F. Rowe, “Contact tracing apps and values dilemmas: A privacy paradox in a neo-liberal world,” International Journal of Information Management, vol. 55, p. 102178, 2020. [7] F. Hassandoust, S. Akhlaghpour, and A. C. Johnston, “Individuals’ privacy concerns and adoption of contact tracing mobile applications in a pandemic: A situational privacy calculus perspective,” Journal of the American Medical Informatics Association, vol. 28, no. 3, pp. 463–471, 2021. [8] T. Martin, G. Karopoulos, J. L. Hern´andez-Ramos, G. Kambourakis, and I. Nai Fovino, “De- mystifying covid-19 digital contact tracing: A survey on frameworks and mobile apps,” Wireless Communications and Mobile Computing, vol. 2020, 2020. [9] U. S. G. A. Oﬃce, “Beneﬁts and challenges of smartphone applications to augment contact tracing,” https://www.gao.gov/products/gao-21-104622, Sept 2021. [10] T. TraceTogether, “How does tracetogether work,” 2020. [11] R. Gupta, M. Bedi, P. Goyal, S. Wadhera, and V. Verma, “Analysis of covid-19 tracking tool in india: case study of aarogya setu mobile application,” Digital Government: Research and Practice, vol. 1, no. 4, pp. 1–8, 2020. [12] S. Vaudenay, “Centralized or decentralized? the contact tracing dilemma,” Cryptology ePrint Archive, 2020. [13] T. Li, C. Faklaris, J. King, Y. Agarwal, L. Dabbish, J. I. Hong et al., “Decentralized is not risk- free: Understanding public perceptions of privacy-utility trade-oﬀs in covid-19 contact-tracing apps,” arXiv preprint arXiv:2005.11957, 2020. [14] Google, “Exposure notiﬁcations implementation guide,” https://developers.google.com/ android/exposure-notiﬁcations/implementation-guide, Feb. 2022. [15] Apple, “Enexposureconﬁguration,” exposurenotiﬁcation/enexposureconﬁguration, Feb. 2022. https://developer.apple.com/documentation/ [16] S. Altmann, L. Milsom, H. Zillessen, R. Blasone, F. Gerdon, R. Bach, F. Kreuter, D. Nosenzo, S. Toussaert, J. Abeler et al., “Acceptability of app-based contact tracing for covid-19: Cross- country survey study,” JMIR mHealth and uHealth, vol. 8, no. 8, p. e19857, 2020. [17] Google, “Publicly-available exposure notiﬁcations apps,” https://developers.google.com/ android/exposure-notiﬁcations/apps, Feb. 2022. [18] Sufatrio, D. J. J. Tan, T.-W. Chua, and V. L. L. Thing, “Securing android: A survey, taxonomy, and challenges,” ACM Comput. Surv., pp. 58:1–58:45, 2015. [19] L. Li, T. F. Bissyand´e, M. Papadakis, S. Rasthofer, A. Bartel, D. Octeau, J. Klein, and L. Traon, “Static analysis of android apps: A systematic literature review,” Information and Software Technology, vol. 88, pp. 67–95, 2017. [20] V.-P. Ranganath and J. Mitra, “Are free android app security analysis tools eﬀective in de- tecting known vulnerabilities?” Empirical Software Engineering, vol. 25, no. 1, pp. 178–219, 2020. 19 [21] A. Abraham, Magaofei, M. Dobrushin, and V. Nadal, “Mobsf github,” https://github.com/ MobSF/Mobile-Security-Framework-MobSF, Feb. 2022. [22] A. Desnos, G. Gueguen, and S. Bachmann, “Androguard,” https://androguard.readthedocs. io/en/latest/, Feb. 2022. [23] L. Lu, Z. Li, Z. Wu, W. Lee, and G. Jiang, “Chex: statically vetting android apps for compo- nent hijacking vulnerabilities,” in Proceedings of the 2012 ACM conference on Computer and communications security, 2012, pp. 229–240. [24] T. Watanabe, M. Akiyama, F. Kanei, E. Shioji, Y. Takata, B. Sun, Y. Ishi, T. Shibahara, T. Yagi, and T. Mori, “Understanding the origins of mobile app vulnerabilities: A large-scale measurement study of free and paid apps,” in 2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR). IEEE, 2017, pp. 14–24. [25] J. Mitra and V.-P. Ranganath, “Ghera: A repository of android app vulnerability benchmarks,” in Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering, 2017, pp. 43–52. [26] M. Ghafari, P. Gadient, and O. Nierstrasz, “Security smells in android,” in 2017 IEEE 17th IEEE, international working conference on source code analysis and manipulation (SCAM). 2017, pp. 121–130. [27] P. R. Center, “An analysis of android app permissions,” https://www.pewresearch.org/ internet/2015/11/10/an-analysis-of-android-app-permissions/, Nov 2015. [28] B. P. Sarma, N. Li, C. Gates, R. Potharaju, C. Nita-Rotaru, and I. Molloy, “Android permis- sions: a perspective combining risks and beneﬁts,” in Proceedings of the 17th ACM symposium on Access Control Models and Technologies, 2012, pp. 13–22. [29] Google, “Android permissions/overview, May 2022. app permissions,” https://developer.android.com/guide/topics/ [30] A. V. Prakash and S. Das, “Explaining citizens’ resistance to use digital contact tracing apps: A mixed-methods study,” International Journal of Information Management, vol. 63, p. 102468, 2022. [31] Google, “Manifest permissions in android,” https://developer.android.com/reference/android/ Manifest.permission#USE FINGERPRINT, Jul. 2022. [32] Owasp, “Owasp top 10,” https://owasp.org/www-project-mobile-top-10/, Jul. 2022. [33] J. Bell, D. Butler, C. Hicks, and J. Crowcroft, “Tracesecure: Towards privacy preserving contact tracing,” arXiv preprint arXiv:2004.04059, 2020. [34] J. Samhi, K. Allix, T. F. Bissyand´e, and J. Klein, “A ﬁrst look at android applications in google play related to covid-19,” Empirical Software Engineering, vol. 26, no. 4, pp. 1–49, 2021. [35] H. Cho, D. Ippolito, and Y. W. Yu, “Contact tracing mobile apps for covid-19: Privacy considerations and related trade-oﬀs,” arXiv preprint arXiv:2003.11511, 2020. 20 [36] M. Lanzing, “Contact tracing apps: an ethical roadmap,” Ethics and information technology, vol. 23, no. 1, pp. 87–90, 2021. [37] L. Baumg¨artner, A. Dmitrienko, B. Freisleben, A. Gruler, J. H¨ochst, J. K¨uhlberg, M. Mezini, R. Mitev, M. Miettinen, A. Muhamedagic et al., “Mind the gap: Security & privacy risks of contact tracing apps,” in 2020 IEEE 19th international conference on trust, security and privacy in computing and communications (TrustCom). IEEE, 2020, pp. 458–467. [38] Y. Gvili, “Security analysis of the covid-19 contact tracing speciﬁcations by apple inc. and google inc.” Cryptology ePrint Archive, 2020. [39] H. Wen, Q. Zhao, Z. Lin, D. Xuan, and N. Shroﬀ, “A study of the privacy of covid-19 con- tact tracing apps,” in International Conference on Security and Privacy in Communication Systems. Springer, 2020, pp. 297–317. [40] M. Hatamian, S. Wairimu, N. Momen, and L. Fritsch, “A privacy and security analysis of early- deployed covid-19 contact tracing android apps,” Empirical software engineering, vol. 26, no. 3, pp. 1–51, 2021. [41] E. Union, “General data protection regulation,” https://gdpr.eu/what-is-gdpr/, 2022. [42] V. Ang and L. K. Shar, “Covid-19 one year on–security and privacy review of contact tracing mobile apps,” IEEE Pervasive Computing, vol. 20, no. 4, pp. 61–70, 2021. [43] H. Systemas, “Virustotal,” https://www.virustotal.com/gui/home/upload, 2022. [44] V. Kouliaridis, G. Kambourakis, E. Chatzoglou, D. Geneiatakis, and H. Wang, “Dissecting contact tracing apps in the android platform,” Plos one, vol. 16, no. 5, p. e0251867, 2021. [45] Mitre, “Common weakness enumeration,” https://cwe.mitre.org/, 2022. [46] “Common vulnerabilities and exposures,” https://cve.mitre.org/, 2022. 21 App Collects Location Uses Insecure Uses Weak Uses HTTP Communicates With Non-US # Violations Privacy Policy Violation Y Alabama Y Arizona Y California Y Colorado Y Connecticut Y Delaware Y DC N Guam Hawaii Y Y Louisiana Maryland Y Y Michigan N Minnesota Nevada Y New Jersey N New Mexico Y New York N North Carolina Y Y North Dakota Penn Y Y Utah Virginia Y Y Washington Y Wisconsin # Apps per 20 violation Storage Y N N N N Y N N Y N N N Y N Y Y Y N Y Y N N N N 9 Encryption Y N N N N N N Y Y Y N N Y N N N N Y N N N N N N 6 N N Y Y N Y Y N N N Y Y Y Y Y Y N Y N N Y N Y Y 14 server domain Y N Y Y Y N Y N N N Y N Y Y N Y N Y Y N N N Y Y 13 per app 4 1 3 3 2 3 3 1 3 2 3 2 4 3 2 4 1 4 3 2 2 1 3 3 Table 4: Privacy Violations by each US-based GAEN app. Columns 2-6 indicate a feature or an action that an app claims it does not use or do in its privacy policy. The cells with Y/N denote Yes if an app performs the action in the corresponding column and No if it does not. Y implies a privacy violation and N implies otherwise. 22 No Certiﬁcate # Vulns. per app 3 3 2 2 3 2 3 4 3 3 3 1 3 4 2 4 2 2 2 2 3 2 3 3 App Unprotected Component Y Alabama N Arizona N California N Colorado Y Connecticut N Delaware N DC Y Guam Y Hawaii Y Louisiana Maryland Y N Michigan Y Minnesota Y Nevada N New Jersey New Mexico Y N New York North Carolina N North Dakota/Wyoming N N Penn N Utah Virginia Y Y Washington Y Wisconsin # Apps per 12 Vuln Known Vulnerabilities Allows Insecure Weak PRNG Y Y Y Y Y Y Y Y Y Y Y N Y Y Y Y Y Y Y Y Y N Y Y 22 hashing Data Backup N N N Y N N N N N N N Y N N N Y N N N N N N N N N N Y N N Y Y N N Y N N N Y N Y Y N N N N N N N 4 7 Insecure SSL Impl. Pinning N Y N N N N Y N N N N N N N N N N N N N N N N N 1 Y N Y Y Y N Y Y Y Y Y Y Y Y N Y N Y N N Y Y Y Y 18 Table 5: Known Vulnerabilities in each US-based GAEN app. Columns 2-5 indicate a known vulnerability. The cells with Y/N denote Yes if an app contains the vulnerability and No otherwise. 23 App SQL Injection Unprotected Component Cleartext Storage Log Sensitive False Positive Vulnerability Y Alabama Y Arizona Y California Y Colorado Y Connecticut Y Delaware Y DC Y Guam Hawaii Y Y Louisiana Y Maryland Y Michigan Y Minnesota Y Nevada New Jersey Y Y New Mexico Y New York North Carolina Y North Dakota/Wyoming Y Y Penn Utah Y N Virginia Y Washington Y Wisconsin # Apps per 23 false positive Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y 24 Y N Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y N Y Y 22 Data Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y 24 Table 6: False Positives reported by MobSF in every US-based GAEN app. The cells with Y indicate that the vulnerability in the corresponding column was falsely reported as a potential vulnerability by MobSF and N denotes MobSF did not report the vulnerability in the corresponding column. 5 5 5 5 5 5 5 4 5 5 per app Janus Signature # False Positives Vulnerability Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y 24 5 5 5 5 5 3 5 5 5 5 5 5 5 5 24"
424,         DALL-E 2’s Failures Are the Most Interesting Thing About It     ,https://spectrum.ieee.org/openai-dall-e-2,2022-07-14,"OpenAI’s text-to-image generator still struggles with text, science, faces, and bias IEEE Spectrum queried DALL-E 2 for an image of “a technology journalist writing an article about a new AI system that can create remarkable and strange images.” In response, it sent back only pictures of men. In April, the artificial intelligence research lab OpenAI revealed DALL-E 2, the successor to 2021’s DALL-E. Both AI systems can generate astounding images from natural-language text descriptions; they’re capable of producing images that look like photos, illustrations, paintings, animations, and basically any other art style you can put into words. DALL-E 2 upped the ante with better resolution, faster processing, and an editor function that lets the user make changes within a generated image using only text commands, such as “replace that vase with a plant” or “make the dog’s nose bigger.” Users can also upload an image of their own and then tell the AI system how to riff on it. The world’s initial reactions to DALL-E 2 were amazement and delight. Any combination of objects and creatures could be brought together within seconds; any art style could be mimicked; any location could be depicted; and any lighting conditions could be portrayed. Who wouldn’t be impressed at the sight, for example, of a parrot flipping pancakes in the style of Picasso? There were also ripples of concern, as people cataloged the industries that could easily be disrupted by such a technology. OpenAI has not released the technology to the public, to commercial entities, or even to the AI community at large. “We share people’s concerns about misuse, and it’s something that we take really seriously,” OpenAI researcher Mark Chen tells IEEE Spectrum.But the company did invite select people to experiment with DALL-E 2 and allowed them to share their results with the world. That policy of limited public testing stands in contrast to Google’s policy with its own just-released text-to-image generator, Imagen. When unveiling the system, Google announced that it would not be releasing code or a public demo due to risks of misuse and generation of harmful images. Google has released a handful of very impressive images but hasn’t shown the world any of the problematic content to which it had alluded. That makes the images that have come out from the early DALL-E 2 experimenters more interesting than ever. The results that have emerged over the last few months say a lot about the limits of today’s deep-learning technology, giving us a window into what AI understands about the human world—and what it totally doesn’t get. OpenAI kindly agreed to run some text prompts from Spectrum through the system. The resulting images are scattered through this article. Spectrum asked for ""a Picasso-style painting of a parrot flipping pancakes,"" and DALL-E 2 served it up. OpenAI DALL-E 2 was trained on approximately 650 million image-text pairs scraped from the Internet, according to the paper that OpenAI posted to ArXiv. From that massive data set it learned the relationships between images and the words used to describe them. OpenAI filtered the data set before training to remove images that contained obvious violent, sexual, or hateful content. “The model isn’t exposed to these concepts,” says Chen, “so the likelihood of it generating things it hasn’t seen is very, very low.” But the researchers have clearly stated that such filtering has its limits and have noted that DALL-E 2 still has the potential to generate harmful material. Once this “encoder” model was trained to understand the relationships between text and images, OpenAI paired it with a decoder that generates images from text prompts using a process called diffusion, which begins with a random pattern of dots and slowly alters the pattern to create an image. Again, the company integrated certain filters to keep generated images in line with its content policy and has pledged to keep updating those filters. Prompts that seem likely to produce forbidden content are blocked and, in an attempt to prevent deepfakes, it can't exactly reproduce faces it has seen during its training. Thus far, OpenAI has also used human reviewers to check images that have been flagged as possibly problematic. Because of DALL-E 2’s clear potential for misuse, OpenAI initially granted access to only a few hundred people, mostly AI researchers and artists. Unlike the lab’s language-generating model, GPT-3, DALL-E 2 has not been made available for even limited commercial use, and OpenAI hasn’t publicly discussed a timetable for doing so. But from browsing the images that DALL-E 2 users have created and posted on forums such as Reddit, it does seem like some professions should be worried. For example, DALL-E 2 excels at food photography, at the type of stock photos used for corporate brochures and websites, and with illustrations that wouldn’t seem out of place on a dorm room poster or a magazine cover. Spectrum asked for a “New Yorker-style cartoon of an unemployed panda realizing her job eating bamboo has been taken by a robot.” OpenAI Here’s DALL-E 2’s response to the prompt: “An overweight old dog looks delighted that his younger and healthier dog friends have remembered his birthday, in the style of a greeting card.”OpenAI Spectrum reached out to a few entities within these threatened industries. A spokesperson for Getty Images, a leading supplier of stock photos, said the company isn’t worried. “Technologies such a DALL-E are no more a threat to our business than the two-decade reality of billions of cellphone cameras and the resulting trillions of images,” the spokesperson said. What’s more, the spokesperson said, before models such as DALL-E 2 can be used commercially, there are big questions to be answered about their use for generating deepfakes, the societal biases inherent in the generated images, and “the rights to the imagery and the people, places, and objects within the imagery that these models were trained on.” The last part of that sounds like a lawsuit brewing. Rachel Hill, CEO of the Association of Illustrators, also brought up the issues of copyright and compensation for images’ use in training data. Hill admits that “AI platforms may attract art directors who want to reach for a fast and potentially lower-price illustration, particularly if they are not looking for something of exceptional quality.” But she still sees a strong human advantage: She notes that human illustrators help clients generate initial concepts, not just the final images, and that their work often relies “on human experience to communicate an emotion or opinion and connect with its viewer.” It remains to be seen, says Hill, whether DALL-E 2 and its equivalents could do the same, particularly when it comes to generating images that fit well with a narrative or match the tone of an article about current events. To gauge its ability to replicate the kinds of stock photos used in corporate communications, Spectrum asked for “a multiethnic group of blindfolded coworkers touching an elephant.”OpenAI For all DALL-E 2’s strengths, the images that have emerged from eager experimenters show that it still has a lot to learn about the world. Here are three of its most obvious and interesting bugs. Text: It’s ironic that DALL-E 2 struggles to place comprehensible text in its images, given that it’s so adept at making sense of the text prompts that it uses to generate images. But users have discovered that asking for any kind of text usually results in a mishmash of letters. The AI blogger Janelle Shane had fun asking the system to create corporate logos and observing the resulting mess. It seems likely that a future version will correct this issue, however, particularly since OpenAI has plenty of text-generation expertise with its GPT-3 team. “Eventually a DALL-E successor will be able to spell Waffle House, and I will mourn that day,” Shane tells Spectrum. “I’ll just have to move on to a different method of messing with it.” To test DALL-E 2’s skills with text, Spectrum riffed on the famous Magritte painting that has the French words “Ceci n’est pas une pipe” below a picture of a pipe. Spectrum asked for the words “This is not a pipe” beneath a picture of a pipe. OpenAI Science: You could argue that DALL-E 2 understands some laws of science, since it can easily depict a dropped object falling or an astronaut floating in space. But asking for an anatomical diagram, an X-ray image, a mathematical proof, or a blueprint yields images that may be superficially right but are fundamentally all wrong. For example, Spectrum asked DALL-E 2 for an “illustration of the solar system, drawn to scale,” and got back some very strange versions of Earth and its far too many presumptive interplanetary neighbors—including our favorite, Planet Hard-Boiled Egg. “DALL-E doesn’t know what science is. It just knows how to read a caption and draw an illustration,” explains OpenAI researcher Aditya Ramesh, “so it tries to make up something that’s visually similar without understanding the meaning.” Spectrum asked for “an illustration of the solar system, drawn to scale,” and got back a very crowded and strange collection of planets, including a blobby Earth at lower left and something resembling a hard-boiled egg at upper left.OpenAI Faces: Sometimes, when DALL-E 2 tries to generate photorealistic images of people, the faces are pure nightmare fodder. That’s partly because, during its training, OpenAI introduced some deepfake safeguards to prevent it from memorizing faces that appear often on the Internet. The system also rejects uploaded images if they contain realistic faces of anyone, even nonfamous people. But an additional issue, an OpenAI representative tells Spectrum, is that the system was optimized for images with a single focus of attention. That’s why it’s great at portraits of imaginary people, such as this nuanced portrait produced when Spectrum asked for “an astronaut gazing back at Earth with a wistful expression on her face,” but pretty terrible at group shots and crowd scenes. Just look what happened when Spectrum asked for a picture of seven engineers gathered around a whiteboard. This image shows DALL-E 2’s skill with portraits. It also shows that the system’s gender bias can be overcome with careful prompts. This image was a response to the prompt “an astronaut gazing back at Earth with a wistful expression on her face.”OpenAI When DALL-E 2 is asked to generate pictures of more than one human at a time, things fall apart. This image of “seven engineers gathered around a white board” includes some monstrous faces and hands. OpenAI Bias: We’ll go a little deeper on this important topic. DALL-E 2 is considered a multimodal AI system because it was trained on images and text, and it exhibits a form of multimodal bias. For example, if a user asks it to generate images of a CEO, a builder, or a technology journalist, it will typically return images of men, based on the image-text pairs it saw in its training data. Spectrum queried DALL-E 2 for an image of “a technology journalist writing an article about a new AI system that can create remarkable and strange images.” This image shows one of its responses; the others are shown at the top of this article. OpenAI OpenAI asked external researchers who work in this area to serve as a “red team” before DALL-E 2’s release, and their insights helped inform OpenAI’s write-up on the system’s risks and limitations. They found that in addition to replicating societal stereotypes regarding gender, the system also over-represents white people and Western traditions and settings. One red team group, from the lab of Mohit Bansal at the University of North Carolina, Chapel Hill, had previously created a system that evaluated the first DALL-E for bias, called DALL-Eval, and they used it to check the second iteration as well. The group is now investigating the use of such evaluation systems earlier in the training process—perhaps sampling data sets before training and seeking additional images to fix problems of underrepresentation or using bias metrics as a penalty or reward signal to push the image-generating system in the right direction. Chen notes that a team at OpenAI has already begun experimenting with “machine-learning mitigations” to correct for bias. For example, during DALL-E 2’s training the team found that removing sexual content created a data set with more males than females, which caused the system to generate more images of males. “So we adjusted our training methodology and up-weighted images of females so they’re more likely to be generated,” Chen explains. Users can also help DALL-E 2 generate more diverse results by specifying gender, ethnicity, or geographical location using prompts such as “a female astronaut” or “a wedding in India.” But critics of OpenAI say the overall trend toward training models on massive uncurated data sets should be questioned. Vinay Prabhu, an independent researcher who co-authored a 2021 paper about multimodal bias, feels that the AI research community overvalues scaling up models via “engineering brawn” and undervalues innovation. “There is this sense of faux claustrophobia that seems to have consumed the field where Wikipedia-based data sets spanning [about] 30 million image-text pairs are somehow ad hominem declared to be ‘too small’!” he tells Spectrum in an email. Prabhu champions the idea of creating smaller but “clean” data sets of image-text pairs from such sources as Wikipedia and e-books, including textbooks and manuals. “We could also launch (with the help of agencies like UNESCO for example) a global drive to contribute images with descriptions according to W3C’s best practices and whatever is recommended by vision-disabled communities,” he suggests. The DALL-E 2 team says they’re eager to see what faults and failures early users discover as they experiment with the system, and they’re already thinking about next steps. “We’re very much interested in improving the general intelligence of the system,” says Ramesh, adding that the team hopes to build “a deeper understanding of language and its relationship to the world into DALL-E.” He notes that OpenAI’s text-generating GPT-3 has a surprisingly good understanding of common sense, science, and human behavior. “One aspirational goal could be to try to connect the knowledge that GPT-3 has to the image domain through DALL-E,” Ramesh says. As users have worked with DALL-E 2 over the past few months, their initial awe at its capabilities changed fairly quickly to bemusement at its quirks. As one experimenter put it in a blog post, “Working with DALL-E definitely still feels like attempting to communicate with some kind of alien entity that doesn’t quite reason in the same ontology as humans, even if it theoretically understands the English language.” One day, maybe, OpenAI or its competitors will create something that approximates human artistry. For now, we’ll appreciate the marvels and laughs that come from an alien intelligence—perhaps hailing from Planet Hard-Boiled Egg. This article appears in the August 2022 print issue as “DALL-E 2’s Failures Show the Limits of AI.” Eliza Strickland is a senior editor at IEEE Spectrum, where she covers AI, biomedical engineering, and other topics. She holds a master's degree in journalism from Columbia University. ...Or did the metaverse just turn a shade more uncannily creepy? Matthew S. Smith writes IEEE Spectrum's Gizmo column and is a freelance consumer-tech journalist. An avid gamer, he is a former staff editor at Digital Trends and is particularly fond of wearables, e-bikes, all things smartphone, and CES, which he has attended every year since 2009. Mesh to MetaHuman lets creators import a facial mesh to create a photorealistic 3D character. Creating your virtual clone isn’t as difficult as you’d think. Epic Games recently introduced Mesh to MetaHuman, a framework for creating photorealistic human characters. It lets creators sculpt an imported mesh to create a convincing character in less than an hour. “It’s incredibly simple compared to a lot of other tools,” says Stu Richards (a.k.a. Meta Mike), partner success lead at GigLabs and Cofounder of Versed. “I’d compare it to a character creator in a game.” This e-nose can detect glucose levels with 90 percent accuracy Michelle Hampson is a freelance writer based in Halifax. She frequently contributes to Spectrum's Journal Watch coverage, which highlights newsworthy studies published in IEEE journals. This article is part of our exclusive IEEE Journal Watch series in partnership with IEEE Xplore. Researchers at NYU have developed an AI solution that can leverage public video feeds to better inform decision makers Dexter Johnson is a contributing editor at IEEE Spectrum, with a focus on nanotechnology. This is a sponsored article brought to you by NYU’s Tandon School of Engineering. In the midst of the COVID-19 pandemic, in 2020, many research groups sought an effective method to determine mobility patterns and crowd densities on the streets of major cities like New York City to give insight into the effectiveness of stay-at-home and social distancing strategies. But sending teams of researchers out into the streets to observe and tabulate these numbers would have involved putting those researchers at risk of exposure to the very infection the strategies were meant to curb. Researchers at New York University’s (NYU) Connected Cities for Smart Mobility towards Accessible and Resilient Transportation (C2SMART) Center, a Tier 1 USDOT-funded University Transportation Center, developed a solution that not only eliminated the risk of infection to researchers, and which could easily be plugged into already existing public traffic camera feeds infrastructure, but also provided the most comprehensive data on crowd and traffic densities that had ever been compiled previously and cannot be easily detected by conventional traffic sensors."
460,Denoised MDPs: Learning World Models Better Than the World Itself,"[{'href': 'http://arxiv.org/abs/2206.15477v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2206.15477v2', 'rel': 'related', 'type': 'application/pdf'}]",2022-06-30 17:59:49,"Stabilizing Off-Policy Deep Reinforcement Learning from Pixels Edoardo Cetin * 1 Philip J. Ball * 2 Steve Roberts 2 Oya Celiktutan 1 2 2 0 2 l u J 3 ] G L . s c [ 1 v 6 8 9 0 0 . 7 0 2 2 : v i X r a Abstract Off-policy reinforcement learning (RL) from pixel observations is notoriously unstable. As a result, many successful algorithms must com- bine different domain-speciﬁc practices and auxil- iary losses to learn meaningful behaviors in com- plex environments. In this work, we provide novel analysis demonstrating that these instabil- ities arise from performing temporal-difference learning with a convolutional encoder and low- magnitude rewards. We show that this new visual deadly triad causes unstable training and prema- ture convergence to degenerate solutions, a phe- nomenon we name catastrophic self-overﬁtting. Based on our analysis, we propose A-LIX, a method providing adaptive regularization to the encoder’s gradients that explicitly prevents the occurrence of catastrophic self-overﬁtting using a dual objective. By applying A-LIX, we signiﬁ- cantly outperform the prior state-of-the-art on the DeepMind Control and Atari 100k benchmarks without any data augmentation or auxiliary losses. 1. Introduction One of the core challenges in real world Reinforcement Learning (RL) is achieving stable training with sample- efﬁcient algorithms (Dulac-Arnold et al., 2019). Combining these properties with the ability to reason from visual obser- vations has great implications for the application of RL to the real world (Kalashnikov et al., 2018; Zhu et al., 2020). Recent works utilizing temporal-difference (TD-) learning have made great progress advancing sample-efﬁciency (Lil- licrap et al., 2015; Fujimoto et al., 2018; Haarnoja et al., 2018a; Cetin & Celiktutan, 2021). However, stability has remained a key issue for these off-policy algorithms (Sutton, *Equal contribution 1Centre for Robotics Research, Depart- ment of Engineering, King’s College London 2Department of Engineering Science, University of Oxford. Correspondence to: Edoardo Cetin <edoardo.cetin@kcl.ac.uk>, Philip J. Ball <ball@robots.ox.ac.uk>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). Figure 1. Performance of agents in DMC (left) and Atari 100k (right) benchmarks from 10 seeds. A-LIX outperforms previous methods without using image augmentations or auxiliary losses. 1988; Duan et al., 2016; Van Hasselt et al., 2018; Bus¸oniu et al., 2018), making their general applicability limited as compared to their on-policy counterparts (Schulman et al., 2017; Cobbe et al., 2021). At the same time, using pixel observations has been another orthogonal source of insta- bilities, with several successful approaches relying on pre- training instead of end-to-end learning (Finn et al., 2015; Dwibedi et al., 2018). In fact, alternative optimization ob- jectives, large amounts of simulation data, and symbolic observations have been common factors in most contempo- rary large-scale RL milestones (Silver et al., 2017; Vinyals et al., 2019; Berner et al., 2019). In this work, we provide novel insights behind why ap- plying successful off-policy RL algorithms designed for proprioceptive tasks to pixel-based environments is gener- ally underwhelming (Lee et al., 2019; Yarats et al., 2021). In particular, we provide evidence that three key elements strongly correlate with the occurrence of detrimental insta- bilities: i) Exclusive reliance on the TD-loss. ii) Unregu- larized end-to-end learning with a convolutional encoder. iii) Low-magnitude sparse rewards. Using this framework, we are able to motivate the effectiveness of auxiliary losses (Laskin et al., 2020b; Schwarzer et al., 2020; Yarats et al., 2021) and many domain-speciﬁc practices (Hessel et al., 2018; Laskin et al., 2020a) by explaining how they address elements of this new visual deadly triad. We focus our analysis on the popular DeepMind Control suite (DMC) (Tassa et al., 2018), where the introduction of random shift augmentations has played a key role in re- cent advances (Laskin et al., 2020a; Kostrikov et al., 2021; DMC Performance Atari 100K Performance A-LIX 694.7 A-LIX SPR 0.752 0.704 DrQ-v2 632.7 SPR (No Augs) 0.463 DrQ 241.2 CURL 262.1 SAC 51.9 DrQ 0.357 CURL 0.381 OTRainbow 0.264 DER 0.285 SimPLe 0.443 0 250 500 0.00 0.25 0.50 0.75 Average Score (Medium + Hard) Mean Human-Normalized Score Stabilizing Off-Policy Deep Reinforcement Learning from Pixels Table 1. Practices from recent pixel-based TD-learning methods to mitigate elements of the visual deadly triad. †DrQ uses 10-step returns on Atari. *CURL uses 20-step returns on Atari. Algorithm Visual Deadly Triad Mitigation TD-Loss CNN Overﬁt DrQ/RAD DrQ-v2 SAC-AE SPR DER CURL - - VAE Loss Model-Based Loss - Contrastive Loss Shift/Jitter Augmentations Shift Augmentations - Shift/Jitter Augmentations Non-Overlapping Strides Shift Augmentations Low-Density Reward 10-step returns† 3-step returns - 10-step returns 20-step returns 20-step returns* Yarats et al., 2022). In this domain, we observe that the presence of the visual deadly triad results in the TD-loss gradients through the convolutional encoder’s feature maps having high spatial frequencies. We ﬁnd these gradients are spatially inconsistent and result in degenerate optimization landscapes when backpropagated to the encoder’s param- eters. Furthermore, repeatedly updating the convolutional encoder with these gradients consistently leads to early con- vergence to degenerate feature representations causing the critic to ﬁt high-variance erroneous targets, a phenomenon we name catastrophic self-overﬁtting. As a way of iden- tifying the direct implications of the visual deadly triad in the gradient signal, we propose a new measure called the Normalized Discontinuity (ND) score and show how its value precisely correlates with agent performance. Thus, we explain the effectiveness of shift augmentations by recog- nizing that they regularize the gradient signal by providing an implicit spatial smoothing effect. Based on our analysis, we propose Adaptive Local SIgnal MiXing (A-LIX) a novel method to prevent catastrophic self-overﬁtting with two key components: i) A new parame- terized layer (LIX) that explicitly enforces smooth feature map gradients. ii) A dual objective that ensures learning sta- bility by adapting the LIX parameters based on the estimated ND scores. We show that integrating A-LIX with existing off-policy algorithms achieves state-of-the-art performance in both DeepMind Control and Atari 100k benchmarks with- out requiring image augmentations or auxiliary losses and signiﬁcantly fewer heuristics. We open-source our code to facilitate reproducibility and future extensions1. Our main contribution can be summarized as follows: • We conjecture the existence of a visual deadly triad as a principal source of instability in reinforcement learn- ing from pixel observations and provide clear empirical evidence validating our hypothesis. • We show these instabilities affect the gradient signal causing catastrophic self-overﬁtting, a phenomenon that can severely harm TD-learning. As a result, we design the normalized discontinuity score to explicitly 1https://github.com/Aladoro/Stabilizing-Off-Policy-RL Figure 2. Returns of agents over 5 seeds. Solid lines represent median performance, faded lines represent individual runs. The vertical dashed line shows when augmentations are turned off. anticipate its occurrence. • We propose A-LIX, a new method that adaptively reg- ularizes convolutional features to prevent catastrophic self-overﬁtting, achieving state-of-the-art results on two popular pixel-based RL benchmarks. 2. Background We consider problem settings described by Markov Deci- sion Processes (MDPs) (Bellman, 1957), deﬁned as the tuple (S, A, P, p0, r, γ). This comprises a state space S, an action space A, transitions dynamics given by P and p0, and a re- ward function r. The RL objective is then for an agent to re- cover an optimal policy π∗, yielding a distribution of trajec- tories pπ(τ ) that maximizes its expected sum of discounted future rewards, π∗ = arg maxπ Epπ(τ ) [(cid:80)∞ t=0 γtr(st, at)]. In off-policy RL, this objective is usually approached by learning a critic function to evaluate the effectiveness of the agent’s behavior. A common choice for the critic is to param- eterize the policy’s Q-function Qπ : S × A → R, that quan- tiﬁes the agent’s performance after performing a particular action: Qπ(s, a) = Epπ(τ |s0=s,a0=a) [(cid:80)∞ t=0 γtr(st, at)]. Most off-policy algorithms entail storing trajectories in a buffer D, and learning parameterized Q-functions by itera- tively minimizing a squared temporal difference (TD-) loss: JQ(φ) = E(s,a,s(cid:48),r)∼D (cid:2)(Qπ φ(s, a) − y)2(cid:3) , (cid:105) (cid:104) ˆQπ φ(cid:48)(s(cid:48), a) . y = r + γEa∼π(s(cid:48)) (1) Here, the TD-targets y are computed from a 1-step bootstrap operation with a slowly-changing target Q-function ˆQπ φ(cid:48). In continuous action spaces, we also learn a separate parame- terized policy to exploit the information in the critic. This practically results in alternating TD-learning with maximiz- ing the Q-function’s expected return predictions, following the policy gradient theorem (Sutton et al., 2000). 3. Instabilities in TD-Learning from Pixels Unlike proprioceptive observations, off-policy RL from pixel observations commonly requires additional domain- Cheetah Run Augs Turned Off 500 n r u t e R 0 0 Augs 1 2 Frames (×106) No Augs 3 No Augs @ 500k Stabilizing Off-Policy Deep Reinforcement Learning from Pixels Figure 3. Evidence of overﬁtting when augmentations are not used. On the left, shaded lines are individual estimates, the solid line represents the median Q-value. On the right, the Q-values Pearson correlation with target values and Monte-Carlo returns (RM C ). Figure 4. TD-loss of ofﬂine ﬁxed transitions during training, sepa- rated based on having non-zero reward. complementary experiments validating these claims). speciﬁc practices to ensure stability. In this section, we provide a novel analysis of this phenomenon by focusing on the DeepMind Control Suite (Tassa et al., 2018). In this benchmark, the introduction of random shift data augmen- tations has been a core component of recent advances in pixel-based off-policy RL (Laskin et al., 2020a; Yarats et al., 2022), allowing us to isolate and reproduce stable and un- stable training regimes. Our analysis suggests the existence of speciﬁc elements that cause instabilities and strives to explain their implications on learning dynamics. We vali- date our ﬁndings via thorough empirical experimentation showing numerous results corroborating our hypotheses. Based on our discoveries, in Section 4 we provide a new interpretation of random shifts and propose a new improved method to isolate and counteract instabilities. 3.1. Why Do Augmentations Help? The underlying mechanism behind the effectiveness of ran- dom shifts is not immediately clear. While this augmen- tation may appear to assist generalization by encoding an invariance (Shorten & Khoshgoftaar, 2019), we note that all the environments from DMC employ a camera that is ﬁxed relative to the agent’s position. Hence, robustness to shifts does not appear to introduce any useful inductive bias about the underlying tasks. Moreover, prior work successfully learned effective controllers without augmentations (Hafner et al., 2020; Yarats et al., 2021), suggesting that shift gen- eralization might not be the primary beneﬁt of this method. We analyze the effect of random shifts by training a DrQ-v2 agent (Yarats et al., 2022) on Cheetah Run but turning off augmentations after an initial 500,000 time-steps learning phase. As shown in Fig. 2, while training without any shift augmentation fails to make consistent progress, turning off augmentations after the initial learning phase actually ap- pears to slightly improve the performance of DrQ-v2. This result is a clear indication that augmentations are not needed for asymptotic performance, and are most helpful to coun- teract instabilities present in the earlier stages of learning, which we now focus on analyzing (see App. F.1-F.2 for 3.2. Identifying a New Deadly Triad To reduce confounding factors and to disentangle the origin of these instabilities, we design an ofﬂine RL experiment (Levine et al., 2020). This experiment isolates three distinct elements affecting off-policy RL: exploration, policy eval- uation, and policy improvement. First, we gather a set of 15,000 transitions with pixel observations using a random policy in Cheetah Run. This allows us to ground explo- ration and analyze learning from ﬁxed data resembling the early stages of online training (when augmentations appear most helpful). We then isolate policy evaluation by training both critic and encoder using SARSA (Rummery & Niran- jan, 1994) until convergence on this ﬁxed data. Finally, we run policy improvement, training an actor to maximize the expected discounted return as predicted by the converged critic (see App. B.1 for details). Interestingly, we ﬁnd that turning on augmentations exclusively during exploration or policy improvement has no apparent effect on stability and ﬁnal performance. Hence, we focus on the effects that augmentations have on TD-learning and analyze applying augmentations only during policy evaluation. Table 2. Performance and training statistics of different agent types in the ofﬂine experiments from 15,000 random transitions. Agent Augmented Non-Augmented Proprioceptive Frozen CNN (random) Frozen CNN (pre-trained) Non-Augmented (norm r) Non-Augmented (10-step returns) Final TD-Loss Final Policy Loss Return 0.021 0.002 0.012 0.023 0.012 18.616 0.003 −0.99 −1.05 −1.14 −0.95 −0.99 3.86 −1.24 86.5 ± 11.3 9.2 ± 12.1 79.1 ± 7.7 43.6 ± 20.2 77.6 ± 18.5 38.6 ± 16.5 36.5 ± 20.3 As shown in Table 2, applying augmentations during pol- icy evaluation enables us to learn policies that achieve a return of 86.5, despite the best trajectory in the ofﬂine data achieving only 10.8. In contrast, without augmentations we consistently recover near 0 returns, resembling the failures observed in the online experiments. On the left of Fig. 3 we show the evolution of the predicted Q-values for both Q values during training No Augs Augs 2.0 1.5 1.0 0.5 0.0 l s e u a V Q Corr. of Q with Qtarget and RMC 1.0 l n o i t a e r r o C n o s r a e P 0.8 0.6 0.4 0.2 0.0 Qtarget RMC 0 2500 5000 SGD Steps 7500 0 2500 5000 SGD Steps 7500 Zero reward samples Non-Zero reward samples No Augs Augs r o r r E D T 1.00 0.75 0.50 0.25 0.00 0 2000 4000 SGD Steps 6000 8000 0 2000 4000 SGD Steps 6000 8000 Stabilizing Off-Policy Deep Reinforcement Learning from Pixels agents on a ﬁxed batch of ofﬂine data. In particular, when performing policy evaluation without augmentations, these predictions display extremely high variance across different state-action pairs. In Table 2 we further show that the non- augmented agent displays signiﬁcantly lower loss, despite having higher average Q-values than the augmented agent (Schaul et al., 2021). We argue this is a clear indication of the occurrence of overﬁtting. We corroborate our claim by analyzing the evolution of the Pearson bi-variate correlation between the estimated Q-values and target Q-values on the right of Fig. 3. These results show that the non-augmented agent displays near-perfect correlation with its own target Q-values throughout training, indicating that it immediately learns to ﬁt its own noisy, randomly-initialized predictions. We also record the correlation with the actual discounted Monte-Carlo returns, which represent the true targets the Q-values should ideally approximate during policy evalu- ation. For these results, we observe that the relationship between applying augmentations and the recorded correla- tion is reversed, with the non-augmented agent displaying signiﬁcantly lower correlation. This dichotomy appears to indicate that ﬁtting the noisy targets severely affects learn- ing the useful training signal from the collected transitions regarding the experienced rewards. We conﬁrm this phe- nomenon by splitting the data into non-zero and zero reward transitions, where the only learning signal propagated in the TD-loss is from the initially random target values. In Fig. 4 we illustrate that the non-augmented agents initially experience much higher TD-errors on zero reward transi- tions, conﬁrming that they focus on ﬁtting uninformative components of the TD-objective. In Table 2 we provide the results of additional experiments that indicate that TD-learning is not the only cause for the observed instabilities. First, we conﬁrm that the ob- served overﬁtting appears to be exclusive to performing end-to-end TD-learning with convolutional neural network (CNN) encoders. Concretely, we run the same ofﬂine exper- iment without training an encoder in three different settings. First, we consider performing policy evaluation directly from non-augmented proprioceptive observations with a fully-connected critic network. Moreover, we also consider freezing the encoder weights either to their initial random values or to pre-trained values from the augmented agent experiments. In all three cases, we attain largely superior performance, almost matching the augmented agent’s per- formance for both the proprioceptive and pre-trained exper- iments. In addition, we also ﬁnd that the observed over- ﬁtting phenomenon is diminished when simply increasing the magnitude of the reward signal in the TD-loss. We test this through two additional experiments which consider normalizing the collected rewards before policy evaluation and incorporating large n-step returns (Sutton, 1988). As reported, both modiﬁcations considerably improve the non- Figure 5. Feature maps in the ﬁnal layer of both augmented (top) and non-augmented (bottom) agent encoders. Non-augmented agents manifest inconsistent, high-frequency feature maps. augmented agent’s performance. However, we note that both practices introduce further unwanted variance in the optimization, failing to yield the same improvements as augmentations (see App. C.2). Taken together, our results appear to strongly indicate that instabilities in off-policy RL from pixel observation come from three key conditions, which we refer to as the visual deadly triad: i) Exclusive reliance on the TD-loss; ii) Un- regularized learning with an expressive convolutional en- coder; iii) Initial low-magnitude sparse rewards. Further evidence arises when considering the ubiquity of partic- ular practices employed in pixel-based off-policy RL. In particular, as summarized in Table 1, most popular prior algorithms feature design choices that appear to counteract at least two elements of this triad, either directly or implic- itly. Furthermore, we show these instabilities result in the non-augmented critics focusing on learning their own noisy predictions, rather than the actual experienced returns. We observe this ultimately leads to convergence to erroneous and high-variance Q-value predictions, a phenomenon we name catastrophic self-overﬁtting. 3.3. Anticipating Catastrophic Self-Overﬁtting We now attempt to unravel the links that connect the visual deadly triad with catastrophic self-overﬁtting. We start by observing that catastrophic self-overﬁtting comes with a signiﬁcant reduction of the critic’s sensitivity to changes in action inputs, implying that the erroneous high-variance Q-value predictions arise primarily due to changes in the observations (see App. F.3 for action-value surface plots). Hence, we focus on analyzing the feature representations of the pixel observations, computed by the convolutional en- coder, z ∈ RC×H×W . In particular, we wish to quantify the sensitivity of feature representations to small perturbations in the input observations. To measure this, we evaluate the Jacobians of the encoder across a ﬁxed batch of ofﬂine data for the augmented and non-augmented agents. We then cal- culate the Frobenius norm of each agent’s Jacobians, giving us a measure of how quickly the encoder feature represen- Augmented Final Feature Map Non-augmented Final Feature Map Stabilizing Off-Policy Deep Reinforcement Learning from Pixels overﬁtting (Keskar et al., 2017) 2. To quantify the level of discontinuity in the features and their gradients, we propose a new metric that encodes the aggregated immediate spatial ‘unevenness’ of each feature location within its relative feature map. In particular, we deﬁne D(z) ∈ RC×H×W as the expected squared local discontinuity of z in any spatial direction, i.e.: (cid:19)2(cid:35) D(z)ijc ≈ Ev∼S1 , (2) (cid:34)(cid:18) ∂zijc ∂v practically estimated via sampling. We then normalize each value in D(z) by its squared input and average over all the feature positions. We name this metric the normalized discontinuity (ND) score: ND(z) = 1 C × H × W C (cid:88) H (cid:88) W (cid:88) c=1 j=1 i=1 D(z)ijc z2 ijc . (3) Intuitively, this score reﬂects how locally discontinuous z is expected to be at any spatial location. In Fig. 7, we show how the N D score of ∇z evolves during training in the ofﬂine and an online setting for both augmented and non- augmented agents. We see that augmented agents experi- ence considerably less discontinuous gradients through their features, and that recordings of lower N D scores also ap- pear to be highly correlated with performance improvements. We additionally show an accumulated N D score, using an exponential moving average of ∇z in each spatial position to calculate this metric. Interestingly, we observe that the N D score over accumulated gradients is almost identical to the instantaneous N D score, showing that similar gradient discontinuities are propagated persistently through training in each position of the feature map. This property conﬁrms that the discontinuities are not smoothed by the stochastic sampling of different consecutive training batches, in which case we would expect to observe lower accumulated N D scores. Thus, it suggests that self-overﬁtting emerges in the non-augmented agents due to repeated gradient steps towards persistent feature map discontinuities. 4. Counteracting Gradient Discontinuities 4.1. Gradient Smoothing and Random Shifts As analyzed in Section 3, catastrophic self-overﬁtting oc- curs when the gradients in the convolution layers are locally discontinuous. As a result, we argue that the efﬁcacy of random shifts arises from their downstream effect on feature gradient computation, which counteracts these discontinu- ities during backpropagation. In particular, while random shifts do not act directly on the latent representation or their 2Instead, the loss surface with respect to the fully-connected weights is smoother (App. F.5). Figure 6. Critic loss surface plots of augmented (left) and non- augmented (right) agents after 5,000 steps of ofﬂine training. tations are changing locally around an input (see App. B.2 for details). Our results show a stark difference, with the feature representations of the non-augmented agents being on average 2.85 times more sensitive. This suggests that overﬁtting is driven by the CNN encoder’s representations learning high-frequency information about the input obser- vations and, thus, breaking useful inductive biases about this class of models (Rahaman et al., 2019). In App. E.1 we demonstrate that lower sensitivity to ran- dom noise, while desirable for optimization (Rosca et al., 2020), is actually a byproduct of a stable feature represen- tations, and not its deﬁning factor. Furthermore, observing the actual feature maps of different observations in Fig. 5, we see that augmentations make the encoder produce fea- tures that are spatially consistent, aligned with common understandings of how natural representations should ap- pear (Alsallakh et al., 2021; Allen-Zhu & Li, 2021). In contrast, the non-augmented agents display high-frequency and discontinuous feature maps that do not reﬂect the spa- tial properties of their inputs. Hence, our evidence suggests that catastrophic self-overﬁtting speciﬁcally follows from the same learning process that produces highly-sensitive and discontinuous encoder feature maps. Therefore, we turn our focus to analyzing the gradients backpropagated to the encoder’s features maps and observe one key prop- erty: the gradients of the output feature maps consistently reﬂect the same spatial properties of their resulting features. In particular, the gradients of the feature maps appear spa- tially consistent for the augmented agent, and discontinuous for the non-augmented agent. This optimization property reﬂects intuitive understandings of backpropagation since discontinuous gradients should push the encoder’s weights to encode discontinuous representations. To provide further complementary evidence that discontinuous gradients are the direct cause of catastrophic self-overﬁtting, we analyze the normalized loss surfaces when backpropagating these discontinuous gradients to the encoder’s parameters (fol- lowing Li et al. (2018)). In Fig. 6, we see that gradient discontinuities in the non-augmented agent yield extreme peaks in its encoder’s loss surface, clearly suggestive of Critic with Augmentations Critic w/o Augmentations s s o L D T 0.75 0.50 0.25 1 0 W eig ht S u bsp ace 2 1 0.75 0.50 0.25 1 0.75 0.50 0.25 0 TD Loss 1 0 1 1 1 0 Weight Subspace 1 1 Stabilizing Off-Policy Deep Reinforcement Learning from Pixels persistent discontinuities from accumulating. Hence, ran- dom shifts break the second condition of the visual deadly triad, by providing effective implicit regularization of the convolutional encoder’s learning process. At the same time, this minimally disrupts the information content of the re- sultant features, since discarded observation borders almost exclusively comprise background textures that are irrele- vant for performing the task. This interpretation of random shifts aligns with the analysis in Section 3, showing that im- plicitly smoothing over the backpropagated gradient maps consistently prevents catastrophic self-overﬁtting. 4.2. Local Signal Mixing We extrapolate our hypotheses about catastrophic self- overﬁtting and random shifts by proposing a technique that aims to enforce gradient smoothing regularization explic- itly. We propose Local SIgnal MiXing, or LIX, a new layer speciﬁcally designed to prevent catastrophic self-overﬁtting in convolutional reinforcement learning architectures. LIX acts on the features produced by the convolutional encoder, z ∈ RC×H×W , by randomly mixing each component zcij with its neighbors belonging to the same feature map. Hence, LIX outputs a new latent representation with the same di- mensionality ˆz ∈ RC×H×W , whose computation graph minimally disrupts the information content of each feature zcij while smoothing discontinuous components of the gra- dient signal during backpropagation. LIX is a regularization layer that acts as a simple random smoothing operation, reducing the expected magnitude of gradient discontinuities by preventing higher frequency sig- nals to persist. In the forward pass, LIX produces a new latent representation where for each of the C feature maps, ˆzcij is computed as a randomly weighted average of its spatial neighbors around coordinates i, j. We further param- eterize this stochastic operation using some maximum range radius S, and consequently sample two uniform continuous random variables δx, δy ∼ U [−S, S], representing shifts in the x and y coordinates respectively. Correspondingly, we deﬁne ˜i = i + δx and ˜j = j + δy and perform the weighted averaging as a bilinear interpolation with weights determined by the random shifts: ˆzcij =zc(cid:98)˜i(cid:99)(cid:98)˜j(cid:99)((cid:100)˜i(cid:101) − ˜i)((cid:100)˜j(cid:101) − ˜j) + zc(cid:98)˜i(cid:99)(cid:100)˜j(cid:101)((cid:100)˜i(cid:101) − ˜i)(˜j − (cid:98)˜j(cid:99)) +zc(cid:100)i(cid:101)(cid:98)˜j(cid:99)(˜i − (cid:98)˜i(cid:99))((cid:100)˜j(cid:101) − ˜j) + zc(cid:100)i(cid:101)(cid:100)˜j(cid:101)(˜i − (cid:98)˜i(cid:99))(˜j − (cid:98)˜j(cid:99)). Since nearby features in a convolutional feature map are computed with very similar receptive ﬁelds, the mixing effect of LIX should have a trivial effect on the informa- tion the encoder can convey in its latent representations. In addition, LIX should have a direct regularization ef- fect on the gradients by acting on the feature maps them- selves. In particular, since LIX computes each output feature from a weighted average of its neighbors, back- Figure 7. Instantaneous (red and blue) and accumulated (orange and purple) N D scores for the features gradients from ofﬂine (left) and online (right) training in Cheetah Run. respective gradients, they do affect how the latent represen- tations are computed. This has an impact on how persistent discontinuous components of the gradient are backpropa- gated to the encoder’s parameters during learning. From the approximate shift invariance of convolutional layers, we can view a convolutional encoder as computing each of the feature vectors [z1ij, ..., zCij]T with the same param- eterized function, Vφ, that takes as input a subset of each observation O ∈ RC(cid:48)×H (cid:48)×W (cid:48) . This subset corresponds to a local neighborhood around some reference input coordinates i(cid:48), j(cid:48). Thus, the only factor differentiating features in the same feature map (e.g., zcij and zckl) is some implicit func- tion f (i, j) = i(cid:48), j(cid:48) translating each of the output features coordinate into the relative reference input coordinate, i.e. zcij = Vφ(O, i(cid:48), j(cid:48))c (determined by kernel sizes, strides...). Therefore, random shifts are approximately equivalent to further translating each reference coordinate by adding some x, δ(cid:48) uniform random variables δ(cid:48) y: zcij ≈ Vφ(O, i(cid:48) + δ(cid:48) y ∼ U [−s(cid:48), s(cid:48)], x, δ(cid:48) δ(cid:48) x, j(cid:48) + δ(cid:48) y)c, f (i, j) = i(cid:48), j(cid:48). where Due to the employed strides from the convolutional archi- tectures used in DrQ-v2 (Yarats et al., 2022), the difference in reference coordinates of adjacent features in a feature map is less than the maximum allowable shift employed in the augmentations, i.e., (i + 1)(cid:48) − i(cid:48), (j + 1)(cid:48) − j(cid:48) < s(cid:48) (where s(cid:48) is the maximum allowable shift). Consequently, shift augmentations effectively turn the deterministic com- putation graph of each feature zcij into a random variable, whose sample space comprises the computation graphs of all nearby features within its feature map. Hence, applying different random shifts to samples in a minibatch makes the gradient of each feature ∇zcij backpropagate to a random computation graph, sampled from a set that extends the set of non-augmented computation graphs for all features in a local neighborhood of coordinates i, j. Therefore, ag- gregating the parameter gradients produced with different δ(cid:48) x, δ(cid:48) y, provides a smoothing effect on how each discon- tinuous component of ∇z affects learning, and prevents Offline Online e r o c S D N 2.2 2.0 1.8 1.6 0 5000 SGD Steps 2.0 1.5 1.0 10000 0.0 0.5 1.0 1.5 No Augs No Augs (Accumulated) Frames (×106) Augs Augs (Accumulated) Stabilizing Off-Policy Deep Reinforcement Learning from Pixels propagation will split each gradient ∇ˆzcij, to a random local combination of features within the same feature map, {∇zc(cid:98)˜i(cid:99)(cid:98)˜j(cid:99), ∇zc(cid:98)˜i(cid:99)(cid:100)˜j(cid:101), ∇zc(cid:100)i(cid:101)(cid:98)˜j(cid:99), ∇zc(cid:100)i(cid:101)(cid:100)˜j(cid:101)}. Thus, LIX should mostly preserve the consistent component of ∇z, while randomly smoothing its discontinuous component. There are multiple key differences between the regulariza- tion from LIX and random shifts. LIX provides a local smoothing effect over the gradients explicitly and exactly, without having to deal with the implications of padding and strided convolutions breaking shift-invariance assump- tions. Moreover, LIX smooths the gradient signal not only across different inputs but also within each feature map. In addition, by applying its operation solely at the feature level, the encoder can still learn to entirely circumvent LIX’s smoothing effect on the information encoded in the latent representations, given enough capacity. This means that LIX does not forcibly preclude any input information from affecting the computation. Consequently, LIX also does not have to enforce learning invariances which might not neces- sarily reﬂect useful inductive biases about the distribution of observations. In contrast, random shifts need to exploit the particular uninformativeness of the observations borders to avoid disrupting the features’ information content. 4.3. A-LIX LIX introduces a single key parameter: the range radius S used for sampling δx and δy. Intuitively, this value should reﬂect how much we expect gradients to be locally consis- tent for a given architecture and task. Therefore, we argue that the value of S should ideally decrease throughout train- ing as the useful learning signal from the TD-loss becomes stronger. This is consistent with the results illustrated in Figure 2, showing that turning off random shift augmenta- tions after the TD-targets become informative can improve learning. Hence, we propose an adaptive strategy to learn S throughout training. Utilizing the normalized discontinuity (N D) score in Section 3.3, we set up a dual optimization objective to ensure a minimum value of local smoothness in the representation gradients, N D. However, computing the N D score of the gradient signal involves a ratio between po- tentially very small values. As a result, estimation of these values from a batch of gradient samples can lead to outliers having an extreme impact on this average measure, trans- lating into large erroneous updates of S. To overcome this, we propose using a slightly modiﬁed version of the N D score with increased robustness to outliers (see App. C.1 for further details): (cid:103)N D(∇ˆz) = C (cid:88) H (cid:88) W (cid:88) (cid:32) log 1 + c=1 j=1 i=1 (cid:33) D(∇ˆz)cij ∇ˆz2 ijc . (4) In practice, we set up a dual optimization objective similar to the automatic temperature adjustment from Haarnoja et al. Figure 8. A-LIX’s S parameter evolution during training in Chee- tah Run (left) and Quadruped Run (right). As the critic targets become more informative, S falls, improving data efﬁciency and asymptotic performance. (2018b). This entails alternating the optimization of the TD- learning objectives described in Section 2 with minimizing a dual objective loss: (cid:104) −S × Eˆz (cid:103)N D(∇ˆz) − N D (cid:105) , (5) arg min S approximating dual gradient descent (Boyd et al., 2004). Hence, we call this new layer Adaptive LIX (A-LIX). In Fig. 8 we show that A-LIX effectively anneals S as the agent escapes its unstable regimes, in line with our intuition. 5. Performance Evaluation We evaluate the effectiveness of A-LIX in pixel-based re- inforcement learning tasks in two popular and distinct do- mains featuring a diverse set of continuous and discrete control problems. We integrate A-LIX with existing popu- lar algorithms and compare against current state-of-the-art model-free baselines. We provide further details of our integration and full hyperparameters in App. D. We also extend this section by providing more granular evaluation metrics in App. A. Furthermore, we provide ablation studies analyzing different components of A-LIX in App. E. 5.1. DeepMind Control Evaluation We ﬁrst evaluate the effectiveness of A-LIX for pixel-based RL on continuous control tasks from the DeepMind Control Suite (DMC) (Tassa et al., 2018). Concretely, we integrate A-LIX with the training procedure and network architecture from DrQ-v2 (Yarats et al., 2022), but without using image augmentations. To show the generality of our method we do not modify any of the environment-speciﬁc hyperparameters from DrQ-v2 and simply add our A-LIX layer after each encoder nonlinearity. For simplicity, we optimize a shared S for all the A-LIX layers with the dual objective in Eq. 5. Hence, this introduces a single additional parameter and negligible computational overhead. We compare A-LIX to DrQ-v2, which represents the current state-of-the-art on this benchmark. We also compare against three further baselines: the original DrQ (Kostrikov et al., 2021), which foregoes n- step returns and includes an entropy bonus; CURL (Laskin et al., 2020b), which includes an auxiliary contrastive ob- 500 n r u t e R 0 0.0 Cheetah Run Quadruped Run 2 1 700 600 500 0.7 l e u a V 0.6 S 0.5 X I L - A 1.0 0.5 Frames (×106) 1.5 Agent Return 1.5 2.0 2.5 3.0 Frames (×106) A-LIX Parameter S Stabilizing Off-Policy Deep Reinforcement Learning from Pixels Table 3. Results summary for the Atari 100k benchmark. The reported performance of A-LIX is from 10 seeds. Metrics SimPLe DER OTRainbow CURL DrQ SPR A-LIX Norm. Mean 0.443 0.285 Norm. Median 0.144 0.161 # SOTA # Super Average Rank 7 2 3.92 1 2 5.00 0.264 0.204 1 1 5.21 0.381 0.357 0.704 0.753 0.175 0.268 0.415 0.411 1 2 1 2 3.92 4.85 2.88 4 7 11 7 2.21 Figure 9. Average performance in 10 seeds for DMC Medium (left) and Hard tasks (right). Shaded regions represent ±1 SE. jective; an extension of SAC (Haarnoja et al., 2018b) with the encoder from Yarats et al. (2021). These last three base- lines have been performant on a prior DMC benchmark that considers fewer tasks with high action repeats, as described by Hafner et al. (2019). Instead, we evaluate on the more challenging ‘Medium’ and ‘Hard’ benchmarks from Yarats et al. (2022), comprising 15 tasks with low action repeats. Results. We summarize the results in Figure 9, showing the mean performance curves for both medium and hard bench- mark tasks. We provide further details and the full list of results across all 15 environments in App. A.1. Overall, A- LIX surpasses all prior methods with clear margins, both in terms of efﬁciency and ﬁnal performanc. This is particularly notable in the more complex ‘Hard’ tasks. As highlighted in prior work (Cetin & Celiktutan, 2021), DrQ-v2 appears to yield inconsistent results on some of the harder exploration tasks with sparse rewards. This likely indicates that the gradient regularization induced by random shifts (described in Section 4.1) is unable to consistently prevent catastrophic self-overﬁtting in scenarios where the initial learning signal from TD-learning is particularly low. Finally, DrQ, CURL, and SAC fail to make consistent meaningful progress on this harder benchmark. This performance gap corroborates the third component of the visual deadly triad, showing how lower magnitude rewards due to harder exploration and lower action-repeats further destabilize TD-learning based algorithms, and explains the gains seen in DrQ-v2 when incorporating n-step returns. We believe these results em- phasize the challenge of overcoming the visual deadly triad in continuous control problems and the particular effective- ness of A-LIX to counteract its direct implications. 5.2. Atari 100k Evaluation We perform a second set of experiments in an entirely dif- ferent setting, discrete control. We make use of the popular Atari Learning Environment (ALE) (Bellemare et al., 2013) and consider the 100k evaluation benchmark from Kaiser et al. (2020). In particular, this benchmark comprises eval- uating performance for 26 tasks after only two hours of play-time (100k interaction steps), following the evaluation protocol in Machado et al. (2018). We integrate A-LIX with Data-Efﬁcient Rainbow (DER) (van Hasselt et al., 2019), a simple extension to Rainbow (Hessel et al., 2018) with improved data-efﬁciency. We would like to note that our integration has key differences to DER, designed to high- light the generality of our method in tackling the visual deadly triad. In particular, we reduce the n-step returns to 3 (from 20), and we maintain the same encoder architecture as in DrQ-v2. To speak to the latter point, this means we do not require the highly regularized encoders with large convolutional ﬁlters and strides, used ubiquitously in off- policy learning for Atari environments. Instead, to stabilize learning we simply apply our A-LIX layer after the ﬁnal encoder nonlinearity. We compare against three algorithms that, like A-LIX, do not employ data-augmentation: Data- Efﬁcient Rainbow (DER); Overtrained Rainbow (OTRain- bow) (Kielak, 2019); and Simulated Policy Learning (Sim- PLe) (Kaiser et al., 2020) (model-based). Moreover, we also compare with additional state-of-the-art off-policy baselines that make use of data augmentations: the aforementioned CURL and DrQ; and Self-Predictive Representations (SPR) (Schwarzer et al., 2020), the current state-of-the-art TD- learning based algorithm on this benchmark. SPR combines data augmentation with numerous additional algorithmic design choices, such as an auxiliary self-supervised loss for learning a latent dynamics model. Results. We summarize the results in Table 3, showing the mean and median human-normalized scores together with the number of environments where each algorithm ei- ther achieves state-of-the-art or super-human performance. We include the full per-environment results in App. A.2. Remarkably, A-LIX obtains a substantially higher human- normalized mean performance than all other considered algorithms. While the recorded normalized median per- formance is slightly inferior to SPR, we argue that such difference is not particularly signiﬁcant since this metric de- pends on the performance obtained in just two environments. Moreover, A-LIX achieves super-human performance in 7 games (the same as SPR), and state-of-the-art performance in 11 games, considerably more than all other algorithms. These results corroborate how tuned architectures, data aug- mentation, and auxiliary losses used on ALE mostly serve the purpose of counteracting the direct implications of the visual deadly triad and show that A-LIX enables us to learn DMC Medium Tasks DMC Hard Tasks n r u t e R 600 400 200 0 0 600 400 200 0 1 2 3 0 1 2 3 Frames (×106) Frames (×107) A-LIX DrQ-v2 CURL DrQ SAC Stabilizing Off-Policy Deep Reinforcement Learning from Pixels (a) DeepMind Control: Medium and Hard Tasks (b) Atari 100k Figure 10. Probability of improvement and performance proﬁles obtained from the recorded results in DMC (left) and Atari 100k (right). A-LIX displays statistically signiﬁcantly improvements and stochastically dominates most prior algorithms. powerful models without relying on these design choices. 5.3. Statistical Signiﬁcance To validate the signiﬁcance of our improvements, we sta- tistically analyze our results using the Rliable tools and practices from Agarwal et al. (2021). We summarize some of our key ﬁndings in Fig. 10, showing the probability of improvements of A-LIX over prior methods (computed with the Mann-Whitney U statistic (Mann & Whitney, 1947)) and the relative normalized performance proﬁles (Dolan & Mor´e, 2002). The ranges correspond to 95% stratiﬁed boot- strap conﬁdence intervals (Efron, 1992). In both DMC and Atari benchmarks, we ﬁnd that our improvements are sta- tistically signiﬁcant (lower conﬁdence intervals >0.5) and observe ‘stochastic dominance’ of our algorithm against almost all considered baselines (Dror et al., 2019). We pro- vide further results and details of the employed statistical analysis in App. A.1 and App. A.3 respectively. 6. Related Work Previous works have characterized several optimization is- sues related to performing RL via TD-learning (Baird & Moore, 1998; Baird, 1999). In this work, we instead fo- cus on the empirical analysis of modern TD-learning al- gorithms, speciﬁc to the pixel-based RL setting. We also observe links with recent work studying observational over- ﬁtting (Song et al., 2020). Our work differs by focusing on memorization effects particular to the combination of CNNs and TD-learning. There are also connections with existing feature-level augmentation work, such as Dropout (Srivastava et al., 2014) and DropBlock (Ghiasi et al., 2018). In particular, the latter also applies structured transforma- tions directly to the feature maps and introduces a heuristic to adjust this transformation over training, validating our ﬁndings on the utility of adaptivity. Outside RL, there is a rich body of work on implicit regularization and memoriza- tion in CNNs (Keskar et al., 2017; Neyshabur et al., 2017; Arpit et al., 2017; Liu et al., 2020; Maennel et al., 2020). Rahaman et al. (2019) show that higher frequency data man- ifolds cause CNNs to learn higher spectral frequency terms, aligning with our analysis of higher frequency representa- tions. Chatterjee (2020) show generalization arises when similar examples induce similar gradients during learning (i.e., coherence). Their work supports our ﬁndings since inconsistent feature gradients are a manifestation of non- coherence, explaining their poor generalization. Finally, our dual objective falls under automatic tuning methods in RL (AutoRL) (Parker-Holder et al., 2022). These ap- proaches have been applied very successfully to manage non-stationary trade-offs, such as exploration and exploita- tion (Ball et al., 2020) and optimism (Moskovitz et al., 2021; Cetin & Celiktutan, 2021). Finally, we note links with re- cent work concerning implicit regularization in TD-learning (Kumar et al., 2021). However, while Kumar et al. (2021) observe an implicit ‘underﬁtting’ phenomenon in later train- ing stages, we analyze an opposed ‘overﬁtting’ phenomenon occurring during the ﬁrst training steps, which we ﬁnd to be speciﬁc to learning from visual inputs. 7. Conclusion In this work, we provide a novel analysis demonstrating that instabilities in pixel-based off-policy RL come speciﬁcally from performing TD-learning with a convolutional encoder in the presence of a sparse reward signal. We show this visual deadly triad affects the encoder’s gradients, causing the critic to catastrophically self-overﬁt to its own noisy predictions. Therefore, we propose Adaptive Local SIgnal MiXing (A-LIX), a powerful regularization layer to explic- itly counteract this phenomenon. Applying A-LIX enables us to outperform prior state-of-the-art algorithms on pop- ular benchmarks without relying on image augmentations, auxiliary losses, or other notable design choices. Acknowledgments Edoardo Cetin and Oya Celiktutan would like to acknowl- edge the support from the Engineering and Physical Sci- ences Research Council [EP/R513064/1] and LISI Project [EP/V010875/1]. Philip J. Ball would like to thank the Wil- lowgrove Foundation for support and funding. Furthermore, support from Toyota Motor Corporation contributed towards funding the utilized computational resources. P(A-LIX > Y) Fraction of runs with score > τ SAC CURL DrQ DrQv2 1.00 0.75 0.50 0.25 0.00 A-LIX DrQv2 DrQ CURL SAC 0.60 0.75 0.90 0.0 0.5 1.0 P(A-LIX > Y) Fraction of runs with score > τ SPR DrQ CURL OTR DER SimPLe 1.00 0.75 0.50 0.25 0.00 A-LIX SPR DrQ CURL OTR DER SimPLe 0.6 0.7 0.8 0.9 0.0 1.0 2.0 Stabilizing Off-Policy Deep Reinforcement Learning from Pixels References Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A., and Bellemare, M. G. Deep reinforcement learning at the edge of the statistical precipice, 2021. Allen-Zhu, Z. and Li, Y. Feature puriﬁcation: How adver- sarial training performs robust deep learning, 2021. Alsallakh, B., Kokhlikyan, N., Miglani, V., Yuan, J., and Reblitz-Richardson, O. Mind the pad – {cnn}s In International Conference can develop blind spots. on Learning Representations, 2021. URL https:// openreview.net/forum?id=m1CD7tPubNy. Arpit, D., Jastrzebski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal, M. S., Maharaj, T., Fischer, A., Courville, A., Bengio, Y., and Lacoste-Julien, S. A closer look at memorization in deep networks. In Precup, D. and Teh, Y. W. (eds.), Proceedings of the 34th International Con- ference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 233–242. PMLR, 06– 11 Aug 2017. URL https://proceedings.mlr. press/v70/arpit17a.html. Baird, L. Reinforcement learning through gradient descent. Technical report, Carnegie-Mellon University, Depart- ment of Computer Science, 1999. Baird, L. and Moore, A. Gradient descent for general re- inforcement learning. Advances in neural information processing systems, 11, 1998. Ball, P., Parker-Holder, J., Pacchiano, A., Choromanski, K., and Roberts, S. Ready policy one: World building through active learning. In Proceedings of the 37th Inter- national Conference on Machine Learning, ICML. 2020. Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An evaluation plat- form for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, 2013. Bellman, R. A markovian decision process. Indiana Univ. Math. J., 6:679–684, 1957. ISSN 0022-2518. Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019. Boyd, S., Boyd, S. P., and Vandenberghe, L. Convex opti- mization. Cambridge university press, 2004. Brandfonbrener, D., Whitney, W. F., Ranganath, R., and Bruna, J. Ofﬂine RL without off-policy evaluation. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/ forum?id=LU687itn08w. Bus¸oniu, L., de Bruin, T., Toli´c, D., Kober, J., and Palunko, I. Reinforcement learning for control: Performance, stabil- ity, and deep approximators. Annual Reviews in Control, 46:8–28, 2018. Cetin, E. and Celiktutan, O. Learning pessimism for robust and efﬁcient off-policy reinforcement learning. arXiv preprint arXiv:2110.03375, 2021. Chatterjee, S. Coherent gradients: An approach to under- standing generalization in gradient descent-based opti- mization. arXiv preprint arXiv:2002.10657, 2020. Cobbe, K. W., Hilton, J., Klimov, O., and Schulman, J. Phasic policy gradient. In International Conference on Machine Learning, pp. 2020–2027. PMLR, 2021. Dolan, E. D. and Mor´e, J. J. Benchmarking optimization software with performance proﬁles. Mathematical pro- gramming, 91(2):201–213, 2002. Dror, R., Shlomov, S., and Reichart, R. Deep dominance- how to properly compare deep neural models. In Pro- ceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2773–2785, 2019. Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P. Benchmarking deep reinforcement learning for continuous control. In International conference on machine learning, pp. 1329–1338. PMLR, 2016. Dulac-Arnold, G., Mankowitz, D., and Hester, T. Chal- arXiv lenges of real-world reinforcement learning. preprint arXiv:1904.12901, 2019. Dwibedi, D., Tompson, J., Lynch, C., and Sermanet, P. Learning actionable representations from visual observa- tions. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 1577–1584. IEEE, 2018. Efron, B. Bootstrap methods: another look at the jackknife. In Breakthroughs in statistics, pp. 569–593. Springer, 1992. Finn, C., Tan, X. Y., Duan, Y., Darrell, T., Levine, S., and Abbeel, P. Learning visual feature spaces for robotic ma- nipulation with deep spatial autoencoders. arXiv preprint arXiv:1509.06113, 25, 2015. Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S. D4{rl}: Datasets for deep data-driven reinforcement learning, 2021. Fujimoto, S., van Hoof, H., and Meger, D. Addressing func- tion approximation error in actor-critic methods. In ICML, pp. 1582–1591, 2018. URL http://proceedings. mlr.press/v80/fujimoto18a.html. Stabilizing Off-Policy Deep Reinforcement Learning from Pixels Ghiasi, G., Lin, T.-Y., and Le, Q. V. Dropblock: A regular- ization method for convolutional networks. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa- Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 31. Curran As- sociates, Inc., 2018. URL https://proceedings. neurips.cc/paper/2018/file/ 7edcfb2d8f6a659ef4cd1e6c9b6d7079-Paper. pdf. Gogianu, F., Berariu, T., Rosca, M. C., Clopath, C., Bu- soniu, L., and Pascanu, R. Spectral normalisation for deep reinforcement learning: An optimisation per- In Meila, M. and Zhang, T. (eds.), Pro- spective. ceedings of the 38th International Conference on Ma- chine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 3734–3744. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/ v139/gogianu21a.html. Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor- critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Confer- ence on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 1861–1870. PMLR, 10– 15 Jul 2018a. URL https://proceedings.mlr. press/v80/haarnoja18b.html. Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V., Zhu, H., Gupta, A., Abbeel, P., et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018b. Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. Learning latent dynamics for planning from pixels. In International Conference on Machine Learning, pp. 2555–2565. PMLR, 2019. Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. Dream to control: Learning behaviors by latent imagination. In International Conference on Learning Representations, 2020. Hernandez-Garcia, J. F. and Sutton, R. S. Understanding multi-step deep reinforcement learning: A systematic study of the dqn target, 2019. Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostro- vski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., and Silver, D. Rainbow: Combining improvements in deep re- inforcement learning. In Thirty-second AAAI conference on artiﬁcial intelligence, 2018. Kaiser, L., Babaeizadeh, M., Milos, P., Osi´nski, B., Camp- bell, R. H., Czechowski, K., Erhan, D., Finn, C., Koza- kowski, P., Levine, S., Mohiuddin, A., Sepassi, R., Tucker, G., and Michalewski, H. Model based reinforce- ment learning for Atari. In International Conference on Learning Representations, 2020. Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E., Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., et al. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. arXiv preprint arXiv:1806.10293, 2018. Kearns, M. J. and Singh, S. P. Bias-variance error bounds for temporal difference updates. In Proceedings of the Thirteenth Annual Conference on Computational Learn- ing Theory, COLT ’00, pp. 142–147, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc. ISBN 155860703X. Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T. P. On large-batch training for deep learn- ing: Generalization gap and sharp minima. In 5th Inter- national Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https: //openreview.net/forum?id=H1oyRlYgg. Kielak, K. P. Do recent advancements in model-based deep reinforcement learning really improve data efﬁciency? 2019. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kostrikov, I., Yarats, D., and Fergus, R. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In International Conference on Learning Representations. 2021. Kumar, A., Agarwal, R., Ghosh, D., and Levine, S. Im- plicit under-parameterization inhibits data-efﬁcient deep In International Conference reinforcement learning. on Learning Representations, 2021. URL https:// openreview.net/forum?id=O9bnihsFfXU. Laskin, M., Lee, K., Stooke, A., Pinto, L., Abbeel, P., and Srinivas, A. Reinforcement learming with augmented In Advances in Neural Information Processing data. Systems 33. 2020a. Laskin, M., Srinivas, A., and Abbeel, P. CURL: Contrastive unsupervised representations for reinforcement learning. In Proceedings of the 37th International Conference on Machine Learning, 2020b. Lee, A. X., Nagabandi, A., Abbeel, P., and Levine, S. Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. arXiv preprint arXiv:1907.00953, 2019. Stabilizing Off-Policy Deep Reinforcement Learning from Pixels Levine, S., Kumar, A., Tucker, G., and Fu, J. Ofﬂine rein- forcement learning: Tutorial, review, and perspectives on open problems, 2020. Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T. Visualizing the loss landscape of neural nets. In Neural Information Processing Systems, 2018. Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015. Liu, S., Papailiopoulos, D., and Achlioptas, D. Bad global minima exist and sgd can reach them. Advances in Neural Information Processing Systems, 33, 2020. Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., and Bowling, M. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artiﬁcial Intelligence Research, 61:523–562, 2018. Maennel, H., Alabdulmohsin, I. M., Tolstikhin, I. O., Baldock, R., Bousquet, O., Gelly, S., and Keysers, D. What do neural networks learn when trained with random labels? In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 19693–19704. Curran Associates, URL https://proceedings. Inc., neurips.cc/paper/2020/file/ e4191d610537305de1d294adb121b513-Paper. pdf. 2020. Mann, H. B. and Whitney, D. R. On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other. The Annals of Mathematical Statistics, 18(1):50 – 60, 1947. doi: 10.1214/aoms/1177730491. URL https: //doi.org/10.1214/aoms/1177730491. Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y. Spec- tral normalization for generative adversarial networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=B1QRgziT-. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Moskovitz, T., Parker-Holder, J., Pacchiano, A., Arbel, M., and Jordan, M. Tactical optimism and pessimism for deep reinforcement learning. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https: //openreview.net/forum?id=a4WgjcLeZIn. Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N. Exploring generalization in deep learning. In Proceed- ings of the 31st International Conference on Neural In- formation Processing Systems, NIPS’17, pp. 5949–5958, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964. Parker-Holder, J., Rajan, R., Song, X., Biedenkapp, A., Miao, Y., Eimer, T., Zhang, B., Nguyen, V., Calandra, R., Faust, A., Hutter, F., and Lindauer, M. Automated rein- forcement learning (autorl): A survey and open problems, 2022. Rahaman, N., Baratin, A., Arpit, D., Draxler, F., Lin, M., Hamprecht, F., Bengio, Y., and Courville, A. On the spec- tral bias of neural networks. In International Conference on Machine Learning, pp. 5301–5310. PMLR, 2019. Rosca, M., Weber, T., Gretton, A., and Mohamed, S. A case for new neural networks smoothness constraints. In ”I Can’t Believe It’s Not Better!” NeurIPS 2020 workshop, 2020. URL https://openreview.net/forum? id=_b-uT9wCI-7. Rummery, G. A. and Niranjan, M. On-line Q-learning using connectionist systems. Technical Report TR 166, Cam- bridge University Engineering Department, Cambridge, England, 1994. Schaul, T., Ostrovski, G., Kemaev, I., and Borsa, D. Return- based scaling: Yet another normalisation trick for deep rl, 2021. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv. org/abs/1707.06347. Schwarzer, M., Anand, A., Goel, R., Hjelm, R. D., Courville, A., and Bachman, P. Data-efﬁcient reinforcement learn- ing with self-predictive representations. arXiv preprint arXiv:2007.05929, 2020. Shorten, C. and Khoshgoftaar, T. M. A survey on image data augmentation for deep learning. Journal of Big Data, 6(1):1–48, 2019. Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. Deterministic policy gradient algorithms. 2014. Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Grae- pel, T., et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017. Stabilizing Off-Policy Deep Reinforcement Learning from Pixels Song, X., Jiang, Y., Tu, S., Du, Y., and Neyshabur, B. Ob- servational overﬁtting in reinforcement learning. In Inter- national Conference on Learning Representations, 2020. In International Conference reinforcement learning. on Learning Representations, 2022. URL https:// openreview.net/forum?id=_SJ-_yyes8. Zhu, H., Yu, J., Gupta, A., Shah, D., Hartikainen, K., Singh, A., Kumar, V., and Levine, S. The ingredients of real- world robotic reinforcement learning. arXiv preprint arXiv:2004.12570, 2020. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Dropout: A simple way Jour- to prevent neural networks from overﬁtting. nal of Machine Learning Research, 15(56):1929–1958, URL http://jmlr.org/papers/v15/ 2014. srivastava14a.html. Student. The probable error of a mean. Biometrika, 6 (1):1–25, 1908. ISSN 00063444. URL http://www. jstor.org/stable/2331554. Sutton, R. Learning to predict by the method of temporal differences. Machine Learning, 3:9–44, 08 1988. doi: 10.1007/BF00115009. Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural in- formation processing systems, pp. 1057–1063, 2000. Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. d. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, arXiv preprint A., et al. Deepmind control suite. arXiv:1801.00690, 2018. Van Hasselt, H., Doron, Y., Strub, F., Hessel, M., Sonnerat, N., and Modayil, J. Deep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648, 2018. van Hasselt, H. P., Hessel, M., and Aslanides, J. When to use parametric models in reinforcement learning? Advances in Neural Information Processing Systems, 32:14322– 14333, 2019. Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575 (7782):350–354, 2019. Individual comparisons by ranking Wilcoxon, F. Biometrics Bulletin, 1(6):80–83, 1945. methods. ISSN 00994987. URL http://www.jstor.org/ stable/3001968. Yarats, D., Zhang, A., Kostrikov, I., Amos, B., Pineau, J., and Fergus, R. Improving sample efﬁciency in model- free reinforcement learning from images. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 35(12): 10674–10681, May 2021. URL https://ojs.aaai. org/index.php/AAAI/article/view/17276. Yarats, D., Fergus, R., Lazaric, A., and Pinto, L. Master- ing visual continuous control: Improved data-augmented Stabilizing Off-Policy Deep Reinforcement Learning from Pixels A. Detailed Results A.1. DMC Medium and Hard Tasks In Table 4, we show the performance in each of the evaluated 15 DMC environments by reporting the mean and standard deviations over the cumulative returns obtained midway and at the end of training for the medium and hard benchmark tasks, respectively. A-LIX attains state-of-the-art performance in the majority of the tasks at both reported checkpoints, while still closely matching DrQ-v2’s performance on the remaining tasks. On the other hand, DrQ-v2 struggles to consistently solve some of the harder exploration tasks such as Cartpole Swingup Sparse and Humanoid Run, as shown by the high standard deviations. Interestingly, unlike in the simpler DMC benchmark from Hafner et al. (2019) with higher action repeat, CURL appears have a slight edge over DrQ. In particular, the self-supervised signal from CURL appears to aid precisely in the sparse reward environments where DrQ-v2 struggles. Hence, this appears to suggest that including an additional self-supervised signal to the TD-loss, lessens the hindering effects of a lower-magnitude reward signal. We interpret this result as additional evidence showing how addressing any individual component of the deadly triad helps counteracting the catastrophic self-overﬁtting phenomenon. We also test the signiﬁcance of our results by performing a Wilcoxon signed-rank test (Wilcoxon, 1945) between A-LIX and DrQ-v2. We perform a paired rank test across both seeds and tasks, allowing us to obtain an p-value that takes into account both population size and relative performance gains across all tasks. The choice of Wilcoxon signed-rank test also does not presume normality in the distributions of performance which we believe is a more appropriate assumption than for instance a paired t-test (Student, 1908), despite a potential loss of statistical power. To ensure correct population pairing, A-LIX and DrQ-v2 seeds were identical, resulting in the same initially collected data and network initialization. Performing this test over all 15 tasks and 5 seeds, we achieve a p-value of 0.0057 at 50% total frames (1.5M and 15M for Medium and Hard respectively) and 0.0053 at 100% total frames (3.0M and 30M for Medium and Hard Respectively), much lower than the typical rejection criteria of p > 0.05. We therefore believe this shows clear evidence that our results in DMC are strongly statistically signiﬁcant. Table 4. Full results for the DeepMind Control Suite benchmark. Each displayed return is averaged over 10 random seeds and from 10 evaluation runs collected at each experience checkpoint. Medium tasks SAC CURL DrQ DrQv2 A-LIX (Ours) SAC CURL DrQ DrQv2 A-LIX (Ours) 1.5M frames 3.0M frames 8±9 9±8 6±5 24±27 Acrobot Swingup 256±47 Cartpole Swingup Sparse 118±233 479±329 318±389 485±396 Cheetah Run 507±114 788±59 792±29 Finger Turn Easy 190±137 297±150 199±132 854±73 Finger Turn Hard 79±73 174±106 100±63 491±182 Hopper Hop 184±127 268±91 198±102 0±0 Quadruped Run 164±91 129±97 419±204 68±72 Quadruped Walk 134±53 144±149 591±256 75±65 Reach Duplo 220±7 8±12 8±10 1±1 Reacher Easy 52±64 707±142 600±201 971±4 Reacher Hard 463±196 320±233 727±172 3±2 Walker Run 379±234 474±148 571±276 26±4 270±99 718±250 806±78 546±101 587±109 287±48 528±107 776±37 212±3 887±19 720±83 691±10 7±8 6±5 28±25 442±64 12±11 185±295 499±349 316±389 505±412 590±95 835±45 873±60 200±155 309±176 216±158 934±54 902±77 100±78 146±95 86±70 0±0 224±135 285±96 240±123 63±45 175±104 130±59 523±271 168±49 142±67 920±36 48±32 228±2 9±9 7±10 2±3 115±98 667±182 612±181 940±50 10±23 678±350 397±273 935±49 447±224 547±143 616±297 25±3 402±100 742±250 864±78 901±109 906±101 372±48 759±107 900±37 221±3 966±19 855±83 756±10 Average score 52.28 291.73 281.03 547.96 585.67 63.80 326.45 300.27 671.40 720.30 15.0M frames 30.0M frames Hard tasks Humanoid Walk Humanoid Stand Humanoid Run Average score SAC CURL DrQ DrQv2 A-LIX (Ours) SAC CURL DrQ DrQv2 A-LIX (Ours) 7±3 5±3 5±3 5.64 5±3 6±3 6±2 5.74 3±2 4±3 5±3 243±162 167±159 22±30 476±79 519±94 122±59 4.02 144.16 372.78 4±3 6±3 3±3 4.30 4±3 6±2 4±3 4.89 5±3 6±2 4±2 675±86 588±63 170±122 754±79 781±94 242±59 4.90 477.74 592.48 We now compare our results using the Rliable framework introduced in Agarwal et al. (2021) (see App. A.3 for a detailed explanation about the metrics introduced). Stabilizing Off-Policy Deep Reinforcement Learning from Pixels Figure 11. Performance proﬁles at 50% (left) and 100% (right) of the total steps in Medium and Hard DMC Tasks. We plot performance proﬁles in Fig. 11 at both 50% and 100% the total training steps in DMC, which aim to represent sample efﬁciency and asymptotic performance respectively. We see that in almost all cases, A-LIX improves upon DrQ-v2. (a) Overall ranking statistics at 50% (top) and 100% (bottom) of the to- tal steps in Medium and Hard DMC Tasks. (b) Aggregate IQM (left) and Optimality Gap (right) metrics at 50% of the total steps in Medium and Hard DMC Tasks. We plot ranking statistics in Fig. 11 at both 50% and 100% the total training steps in DMC. We see that A-LIX clearly appears most in the 1st ranked column, and rarely appears in lower ranked (i.e., > 3), suggesting strong performance across all environments in DMC Medium and Hard. We also provide a further aggregated statistics plot in Fig. 12b (this time at 50% the total steps), which shows A-LIX is particularly sample-efﬁcient and consistent (i.e., low error bars) across all environments. (a) 50% total steps. (b) 100% total steps. Figure 13. Probability of Improvement statitistics at both 50% (left) and 100% (right) of the total timesteps in Medium and Hard DMC Tasks. In Fig. 13 we observe that A-LIX likely improves over prior work, and note that whilst the improvement probability over DrQ-v2 may seem slightly low at ∼60%, we note that this value is in line with statistics in prior works that achieve signiﬁcant gains (as seen in Agarwal et al. (2021)), and furthermore it does not take into account absolute performance values, and instead only compares relative values, which explains why the gains of A-LIX appear larger when evaluated under IQM and OG. Furthermore, the lower CI for 50% total steps does not fall below 0.5, which means improvements are indeed statistically signiﬁcant. 100% Total Steps 50% Total Steps τ > e r o c s h t i w s n u r f o n o i t c a r F 1.00 0.75 0.50 0.25 0.00 τ > e r o c s h t i w s n u r f o n o i t c a r F 1.00 0.75 0.50 0.25 0.00 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Normalized Score (τ) Normalized Score (τ) A-LIX DrQv2 DrQ CURL SAC 50% Total Steps 100 80 60 40 20 0 100 ) % n i ( n o i t c a r F 100% Total Steps 80 60 40 20 0 1 2 3 4 5 Ranking A-LIX DrQv2 DrQ CURL SAC A-LIX DrQv2 DrQ CURL SAC IQM Optimality Gap 0.2 0.4 0.6 0.45 0.60 0.75 0.90 Max Normalized Score P(A-LIX > Y) Y m h t i r o g A l SAC CURL DrQ DrQv2 0.60 0.75 0.90 Y m h t i r o g A l SAC CURL DrQ DrQv2 P(A-LIX > Y) 0.60 0.75 0.90 Stabilizing Off-Policy Deep Reinforcement Learning from Pixels A.2. Atari 100k In Table 5, we show the ﬁnal average performance for all the evaluated algorithms in each of the twenty-six tasks in the Atari 100k benchmark. A-LIX outperforms SPR, the previous state-of-the-art off-policy algorithm on this benchmark, on 16 out of 26 tasks. Moreover, it attains comparatively similar performance on most of the remaining tasks despite using no augmentation, auxiliary losses, or model-based elements. Table 5. Full results for the Atari 100k benchmark, following the evaluation protocol from Machado et al. (2018). We report the results collected from 10 random seeds. Tasks Random Human SimPLe DER OTRainbow CURL DrQ SPR A-LIX (Ours) Alien Amidar Assault Asterix Bank Heist Battle Zone Boxing Breakout Chopper Command Crazy Climber Demon Attack Freeway Frostbite Gopher Hero Jamesbond Kangaroo Krull Kung Fu Master Ms Pacman Pong Private Eye Qbert Road Runner Seaquest Up N Down Human Norm. Mean Human Norm. Median # SOTA # Super Average Rank 227.80 5.80 222.40 210.00 14.20 2360.00 0.10 1.70 811.00 10780.50 152.10 0.00 65.20 257.60 1027.00 29.00 52.00 1598.00 258.50 307.30 -20.70 24.90 163.90 11.50 68.40 533.40 0.000 0.000 N/A N/A N/A 7127.70 1719.50 742.00 8503.30 753.10 37187.50 12.10 30.50 7387.80 35829.40 1971.00 29.60 4334.70 2412.50 30826.40 302.80 3035.00 2665.50 22736.30 6951.60 14.60 69571.30 13455.00 7845.00 42054.70 11693.20 1.000 1.000 N/A N/A N/A 616.9 88 527.2 1128.3 34.2 5184.4 9.1 16.4 1246.9 62583.6 208.1 20.3 254.7 771 2656.6 125.3 323.1 4539.9 17257.2 1480 12.8 58.3 1288.8 5640.6 683.3 3350.3 0.443 0.144 7 2 3.92 739.9 188.6 431.2 470.8 51 10124.6 0.2 1.9 861.8 16185.3 508 27.9 866.8 349.5 6857 301.6 779.3 2851.5 14346.1 1204.1 -19.3 97.8 1152.9 9600 354.1 2877.4 0.285 0.161 1 2 5.00 824.7 82.8 351.9 628.5 182.1 4060.6 2.5 9.8 1033.3 21327.8 711.8 25 231.6 778 6458.8 112.3 605.4 3277.9 5722.2 941.9 1.3 100 509.3 2696.7 286.9 2847.6 0.264 0.204 1 1 5.21 558.2 142.1 600.6 734.5 131.6 14870 1.2 4.9 1058.5 12146.5 817.6 26.7 1181.3 669.3 6279.3 471 872.5 4229.6 14307.8 1465.5 -16.5 218.4 1042.4 5661 384.5 2955.2 0.381 0.175 1 2 3.92 771.2 102.8 452.4 603.5 168.9 12954 6 16.1 780.3 20516.5 1113.4 9.8 331.1 636.3 3736.3 236 940.6 4018.1 9111 960.5 -8.5 -13.6 854.4 8895.1 301.2 3180.8 0.357 0.268 1 2 4.85 801.5 176.3 571 977.8 380.9 16651 35.8 17.1 974.8 42923.6 545.2 24.4 1821.5 715.2 7019.2 365.4 3276.4 3688.9 13192.7 1313.2 -5.9 124 669.1 14220.5 583.1 28138.5 0.704 0.415 4 7 2.88 902 174.27 660.53 809.5 639.4 14470 21.5 23.52 747 53166 888.15 31.04 1845.7 500.6 7185.85 341.5 6507 4884.04 16316 1258.4 6.03 100 2974 17471 654.6 5011.7 0.753 0.411 11 7 2.21 We now present additional evaluations under the Rliable framework, continuing on from the analysis in Fig. 10b. Figure 14. Performance proﬁles with linear (left) and logarithmic (right) scaling in Atari 100k. In Fig. 14 A-LIX performs noticeably better than previous work, and performs at least as well as SPR over all settings of τ > e r o c s h t i w s n u r f o n o i t c a r F 1.00 0.75 0.50 0.25 0.00 0.0 Score Distributions with Non Linear Scaling Score Distributions τ > e r o c s h t i w s n u r f o n o i t c a r F 1.00 0.75 0.50 0.25 0.00 0.5 1.0 Human Normalized Score (τ) 1.5 2.0 0.0 0.1 0.2 0.5 1.0 2.0 Human Normalized Score (τ) A-LIX SPR DrQ CURL OTR DER SimPLe Stabilizing Off-Policy Deep Reinforcement Learning from Pixels normalized scores. (a) Ranking statistics. (b) Probability of improvement statistics. Figure 15. Bootstrapped ranking statistics (left) and probability of improvement plots (right) on Atari 100k. In Fig. 15a A-LIX constitutes the majority of the algorithms ranked in 1st, and shows far fewer instances of being ranked in lower positions (i.e., > 4). In Fig. 15b we observe A-LIX likely improves upon prior work. Similar to Fig. 13, while the ∼60% improvement value over SPR may seem low, this is justiﬁed due to shortcomings in this metric, such as not taking into account actual performance values, and instead relative improvements. Furthermore, the lower CI does not fall below 0.5, which means improvements due to A-LIX are statistically signiﬁcant. A.3. Rliable: A Primer In addition to providing traditional methods of evaluation (e.g., performance tables, signiﬁcance testing), we use robust metrics and evaluation strategies introduced in Rliable (Agarwal et al., 2021). Rliable advocates for computing aggregate performance statistics not just across many seeds, but also across the many tasks within a benchmark suite. We give details on how these metrics achieve reliable performance evaluation in RL, denoting number of seeds as N and number of tasks as M . We follow Agarwal et al. (2021) as closely as possible; please refer to their paper for further details. A.3.1. SEED AND TASK AGGREGATION In order to aggregate performances across different tasks in the same benchmark suite, we must ﬁrst normalize each benchmark to the same range. In Atari, this is usually done by normalizing scores with respect to those achieved by humans, and in DMC this is done with respect to the maximum achievable score (i.e., 1, 000). We refer to this normalized score as τ . A.3.2. IQM AND OG Interquartile Mean (IQM) takes the middle 50% of the runs across seeds and benchmarks (i.e., [N M/2]) and then calculates its mean score, improving outlier robustness whilst maintaining statistical efﬁciency. Optimality Gap (OG) calculates the proportion of performances (N M ) that fail to meet a minimum threshold γ, with the assumption that improvements beyond γ are not important. In both cases, stratiﬁed bootstrap sampling is used to calculate conﬁdence intervals (CIs). A.3.3. PERFORMANCE PROFILES Performance proﬁles are a form of empirical CDF, but with stratiﬁed bootstrap sampling to produce conﬁdence bands that account for the underlying variability of the score. We can also establish ‘stochastic dominance’ by observing whether one method’s performance proﬁle is consistently above another’s for all normalized performance values τ . A.3.4. RANKING Ranking shows the proportion of times a given algorithm ranks in a given position across all tasks, with distributions produced using stratiﬁed bootstrap sampling having 200, 000 repetitions. A.3.5. PROBABILITY OF IMPROVEMENT Probability of improvement is calculated by calculating the Mann-Whitney U-statistic (Mann & Whitney, 1947) across all M tasks. The distribution is then plotted as a boxplot, and if the lower CI > 0.5, the improvement is statistically signiﬁcant. ) % n i ( n o i t c a r F 100 80 60 40 20 0 1 2 3 4 5 6 7 Ranking A-LIX SPR DrQ CURL OTR DER SimPLe P(A-LIX > Y) Y m h t i r o g A l SPR DrQ CURL OTR DER SimPLe 0.6 0.7 0.8 0.9 Stabilizing Off-Policy Deep Reinforcement Learning from Pixels B. Experiments Description B.1. Ofﬂine Experiments We follow the original training hyperparameters of DrQ-v2, and run policy evaluation and policy improvement until we saw convergence in the TD-loss, which would occur at similar points in all agents (i.e., between 10-20k and 5-10k steps of SGD in policy evaluation and policy iteration respectively). For the proprioceptive experiments, we keep everything consistent, except the input to the critic and actor MLP layers are now the proprioceptive states from the DMC simulator, not the latent representation z from the encoder. That is to say we do not modify the MLP architectures nor their learning rates in the interests of a fair comparison. Furthermore, for any given seed of the ofﬂine experiment, we also instantiate all networks in the agents identically and train on the same random ofﬂine data, with minibatches presented in the same order. We also note that a similar algorithm is described in Brandfonbrener et al. (2021), but in the context of minimizing extrapolation errors. Now we present some additional analysis to provide further context to our ofﬂine experiments. First, we see that the proprioceptive statistics mirror those of the augmented agent, further illustrating the crucial role of CNN regularization for successful TD-learning from pixels: Figure 16. Q values and Pearson Correlation of the ofﬂine Proprioceptive agent on an ofﬂine ﬁxed batch. Secondly, we observe that the exact same self-overﬁt also manifests in the online setting by plotting the Pearson correlation values over the initial stages of training in 5 seeds, conﬁrming that phenomena of our ofﬂine analysis applies to the online RL problem: Figure 17. Pearson Correlation of augmented and non-augmented online agents in Cheetah Run and Quadruped Walk across 5 seeds. Shaded lines represent individual runs, and solid lines represent the median. We see that augmented agents do not immediately overﬁt to their target networks, and become correlated only after useful signal is learned. B.2. Jacobian Analysis In order to measure local sensitivity, we linearize the encoder around its input using a Taylor series expansion. Consider an N -dimensional input x ∈ RN and perturbation (cid:15) ∈ RN , an M -dimensional output y ∈ RM , and a function F : RN → RM . Now, performing a Taylor series expansion around ˜x: F(˜x + (cid:15)) = F(˜x) + (cid:15)F(x)∇T |x=˜x + (cid:15)2 2 ∇F(x)∇T |x=˜x + . . . ≈ F(˜x) + J(˜x)(cid:15) = ˜y (6) (7) (8) Q values during training l n o i t a e r r o C n o s r a e P 1.5 1.0 0.5 0.0 0.5 l s e u a V Q 0 2000 4000 6000 8000 SGD Steps Correlation of Q with Qtarget and r 1.0 0.8 0.6 0.4 0.2 0.0 Qtarget r 0 2000 4000 6000 8000 SGD Steps l n o i t a e r r o C n o s r a e P 1.0 0.8 0.6 0.4 0.2 Cheetah Run 20000 40000 Frames Quadruped Walk 1.0 0.9 0.8 0.7 0.6 5000 7500 10000 12500 Qtarget Augs Qtarget No Augs Frames r Augs r No Augs Stabilizing Off-Policy Deep Reinforcement Learning from Pixels where we make the approximation in the second line by dropping the second order/Hessian and higher terms under the assumption the perturbation vector (cid:15) is small. This allows us to write F in the form of a local linear system: y = F(x) + J(x)(cid:15). It is straightforward to see that if the entries of the Jacobian matrix J are larger, then small perturbations (cid:15) will cause larger changes in the output y. To measure the magnitude of the Jacobian entries, we take the Frobenius norm: ||J(x)||F = (cid:88) (cid:88) n m (cid:19)2 (cid:18) ∂Fm(x) ∂xn (9) where xn is the ‘n’th entry of x and Fm is the ‘m’th entry of the codomain of F. The calculation of the Jacobian is trivial through the use of an automatic differentiation framework. In our analysis we calculate the Jacobians of both agents on of a ﬁxed batch of 128 frame stacked images taken from the ofﬂine training dataset, and compare the corresponding ratios of their Frobenius norms, and take this average ratio over the batch across 4 seeds. C. Additional Analysis C.1. Adaptive ND Dual Objective Optimization The alternative N D score with increased outlier robustness, (cid:103)N D, proposed in Section 4.3 is inspired by recordings of signal-to-noise ratio measurements. In particular, by passing the individual normalized D(z) terms through a log(1 + x) smoothing function we downweight the effect that large individual outliers might have on this aggregated metric. We would like to remark that since we set up the optimization of S with a dual objective, changes in the actual target value relating to some appropriate smoothness constraint are mostly irrelevant when considering the optimization’s dynamics. Therefore, we argue that tuning S with the actual N D should not considerably diverge from tuning S based on a re-scaled appropriate target for (cid:103)N D. We provide further plots comparing agent performance and respective adaptive parameter S during training: Figure 18. Performance of agents across 4 different seeds of the Cheetah Run environment and their adaptive scalar parameter S. We observe that initially, S is high until agents learn useful behaviors, whereupon it drops to maintain ND due to presence of useful signal in the feature gradients. Figure 19. Performance of agents across 4 different seeds of the Quadruped Run environment and their adaptive scalar parameter S. We observe that as meaningful behaviors are learned in agents towards the end of training, S falls accordingly, whereupon it drops to maintain ND due to presence of useful signal in the feature gradients. We see the same effect in these two contrasting environments; in Cheetah Run, where learning is more stable due to more predictable initializations and fewer degrees of freedom, we see the A-LIX parameter S drop almost immediately as the TD-targets quickly become more accurate. In the less stable Quadruped Run, we also notice this annealing effect, however this occurs later on in training, when the agent can consistently recover from poor initializations. Seed 0 Seed 1 Seed 2 Seed 3 500 n r u t e R 0 0.0 1.0 0.5 Frames (×106) 1.5 0.0 1.0 0.5 Frames (×106) 1.5 Agent Return 0.0 1.0 0.5 Frames (×106) A-LIX Parameter S 1.5 2 1 l e u a V S X I L - A 0.0 1.0 0.5 Frames (×106) 1.5 Seed 0 Seed 1 Seed 2 Seed 3 800 600 n r u t e R 400 1.5 2.0 2.5 3.0 1.5 2.0 2.5 3.0 1.5 2.0 2.5 3.0 1.5 2.0 2.5 3.0 Frames (×106) Frames (×106) Agent Return Frames (×106) A-LIX Parameter S Frames (×106) l e u a V S X I L - A 0.6 0.4 Stabilizing Off-Policy Deep Reinforcement Learning from Pixels C.2. N-Step Returns Large n-step rewards have become an important part of many algorithms that use TD-learning from visual observations. As motivated in Section 3, large n-step rewards can help towards mitigating self-overﬁtting by densifying the reward and downweighting the contribution of the inaccurate target critic, especially early in training; indeed as shown in (Yarats et al., 2022), using 1-step learning has a signiﬁcant negative impact on performance. However, it is known that there is a bias-variance trade-off with multi-step approaches (Kearns & Singh, 2000), and furthermore, almost all approaches using this method do not apply off-policy bias correction when sampling from a replay buffer. While we motivate the use of n-step returns as a way to mitigate self-overﬁtting through incurring fewer 0 reward tuples (especially common in sparse reward environments early in training), we believe there is evidence to show that this introduces bias when n is sufﬁciently large, despite prior work suggesting this is not the case (Hernandez-Garcia & Sutton, 2019). Figure 20. Returns of agents over 5 seeds. Solid lines represent median performance, faded lines represent individual runs. We show in Fig. 20 that 10-step (as is commonly done in algorithms used to solve Atari) returns can mitigate failure seeds as predicted under the visual deadly triad framework (indeed in Cheetah Run there are no seeds that completely ﬂat-line when 10-step returns are used). However, we also see evidence that applying 10-step returns can have negative impacts on convergence and asymptotic performance in Cheetah Run when the deadly triad is sufﬁciently managed, such as using augmentations; in Quadruped Run we see moderate beneﬁt initially, but note that asymptotically the 10-step and 3-step agents converge to the same performance. We also provide further evidence in App. E.3, where applying 10-step returns to an A-LIX agent generally has a laregely negative impact on performance. Finally, we note that trying 20-step returns, as is done in some algorithms that solve Atari (Laskin et al., 2020b), caused signiﬁcant performance reductions in DMC. In conclusion, this provides evidence that we should consider using lower values of ‘n’ in multi-step returns, and achieve this through addressing other elements of the deadly triad. Cheetah Run Quadruped Walk 750 500 250 n r u t e R 0 0 1 2 3 0 1 2 3 Frames (×106) Frames (×106) Augs 3-step No Augs 3-step Augs 10-step No Augs 10-step Stabilizing Off-Policy Deep Reinforcement Learning from Pixels D. Implementation Details In Tables 6 and 7 we provide the full list of hyperparameters used in our implementations for DMC and Atari 100k, respectively. We show signiﬁcant differences from standard practices in bold. In particular, A-LIX uses the same encoder architecture and n-step returns for both benchmarks, highlighting its lower reliance to environment-speciﬁc heuristics. Moreover, unlike prior state-of-the-art algorithms it does not employ any data augmentation or auxiliary loss function. These factors show the effectiveness of our adaptive method in counteracting instabilities from the visual deadly triad without any additional help, highlighting its applicability. Table 6. Full hyperparameters list used for the DeepMind Control A-LIX experiments. Bolded values represent signiﬁcant differences from canonical implementations. DDPG-integration hyperparameters (following (Yarats et al., 2022)) Replay data buffer size Batch size Minimum data before training Random exploration steps Optimizer Policy/critic learning rate Policy/critic β1 Critic UTD ratio Policy UTD ratio Discount γ Polyak coefﬁcient ρ N-step returns Hidden dimensionality Feature dimensionality Nonlinearity Exploration stddev. clip Exploration stddev. schedule Augmentations 1000000 (100000 for Quadruped Run) 256 (512 for Walker Run) 4000 2000 Adam (Kingma & Ba, 2014) medium: 0.0001 hard: 0.00008 0.9 0.5 0.5 0.99 0.99 3 (1 for Walker Run) 1024 medium: 50 hard: 100 ReLU 0.3 medium: linear: 1 → 0.1 in 500000 steps hard: linear: 1 → 0.1 in 2000000 steps OFF A-LIX-speciﬁc hyperparameters Initial maximum sampling shift S Normalized discontinuity targets N D Maximum sampling shift learning rate Maximum sampling shift β1 1.0 0.635 0.003 0.5 Stabilizing Off-Policy Deep Reinforcement Learning from Pixels Table 7. Full hyperparameters list used for the Atari 100k A-LIX experiments. Bolded values represent signiﬁcant differences from canonical implementations. DER-integration hyperparameters Gray-scaling Down-sampling Frames stacked Action repetitions Reward clipping Max episode frames Replay data buffer size Replay period every Batch size Minimum data before training Random exploration steps Optimizer Critic learning rate Critic β1 Critic (cid:15) Max gradient norm Critic UTD ratio Discount γ Target update period N-step returns Feature maps Filter sizes Strides Hidden dimensionality Feature dimensionality Nonlinearity Exploration noisy nets parameter Augmentations True 84 × 84 4 4 [−1, 1] 108000 100000 1 32 1600 1600 Adam (Kingma & Ba, 2014) 0.0001 0.9 0.000015 10 2 0.99 1 3 32, 32, 32 3 × 3, 3 × 3, 3 × 3 2, 1, 1 256 50 ReLU 0.1 OFF A-LIX-speciﬁc hyperparameters Initial maximum sampling shift S Normalized discontinuity targets N D Maximum sampling shift learning rate Maximum sampling shift β1 1.0 0.75 0.0001 0.5 Stabilizing Off-Policy Deep Reinforcement Learning from Pixels E. Additional Ablations E.1. Smoothness Regularization through Spectral Normalization To distinguish between general smoothness contraints in convolutional features, and the smoothness that arises as a result spatial consistency, we apply spectral normalization (Miyato et al., 2018) to the ﬁnal convolutional layer in the encoder to represent the former class of constraints. Spectral normalization operates on the parameters of a network and constrains its outputs to be 1-Lipschitz and has shown beneﬁts in prior work (Gogianu et al., 2021), but does not explicitly enforce a spatial regularization in the features. We train agents without augmentations using spectral normalization. Figure 21. Returns of agents over 5 seeds. Solid lines represent median performance, faded lines represent individual runs. We see that whilst there is clear improvement above the original non-augmented agents in some cases, the performance is still lower than agents that use spatial consistency regularization, such as random shift augmentations. E.2. Is Gradient Smoothing All We Need? Following the argument in Section 4.1, we can view augmentations as a gradient smoothing regularizer. This naturally leads us to ask the following: can we replace the stochastic shifting mechanism with a ﬁxed smoothing mechanism? To test this, we instead apply a Gaussian smoothing kernel to the feature gradients in the CNN, and utilize our N D score to vary the width of the kernel adaptively through training; we call this method A-Gauss (Adaptive Gaussian Feature Gradient Kernel). Figure 22. Returns of agents over 5 seeds. Solid lines represent median performance, faded lines represent individual runs. We see that while there is improvement over non-augmented agents, overall performance is still lower than even simple non-adaptive augmentation. We believe this is due to the Gaussian kernel having too signiﬁcant an effect on the information contained in the feature gradients during backpropagation, causing information to be lost. We believe this explains the effectiveness of shift-augmentations in reinforcement learning, which is that they effectively balance the information contained in the gradients, as well as ensuring their smoothness to reduce overﬁtting. E.3. Ablations to A-LIX We now provide a set of ablations on both DMC and Atari, assessing the impact of individual components in A-LIX. Cheetah Run Quadruped Walk 750 500 250 n r u t e R 0 800 600 400 200 0 1 2 3 0 1 2 3 Frames (×106) Frames (×106) Augs Spectral Normalization Cheetah Run Quadruped Walk 750 500 250 n r u t e R 0 800 600 400 200 0 1 2 3 0 1 2 3 Frames (×106) Frames (×106) Augs A-Gauss Stabilizing Off-Policy Deep Reinforcement Learning from Pixels (a) DMC Control ablations in Cheetah Run (left) and Quadruped Run (right) evaluated over 4 seeds. (b) Atari 100k ablations evaluated over 4 seeds in 4 different Atari 100k tasks. Figure 23. An ablation study of A-LIX, showing the contribution of its individual components to ultimate performance in DMC and Atari 100k. In Fig. 23a we choose the following ablations for DMC: • A-LIX • Adaptive Random Shifts (where the magnitude of the random shift image augmentation is adjusted using the dual ND objective) • LIX • Random Shifts (i.e., DrQ-v2) While we see a slight asymptotic performance improvement in Cheetah Run by using LIX layers instead of random shifts, we notice signiﬁcant differences in the less stable Quadruped Run environment. Concretely, we see much greater stability in both LIX approaches compared with image augmentation approaches, with the former having no failure seeds. Furthermore, we observe stronger asymptotic performance with the inclusion of the adaptive dual objective for both approaches. As motivated in Fig. 19, this is likely a result of reducing the shift parameter as the signal in the target values increases. In Fig. 23b, we choose the following ablations for Atari 100k on a subset of environments that represent a diverse set of tasks and performances with baseline algorithms: • A-LIX • Adaptive Random Shifts (as before) • LIX • A-LIX with 10-step returns • Random Shifts We see that A-LIX performs consistently strongly across the environments tested, always placing in the top 2 with regards to Human Normalized Score. We also notice that generally, LIX layer methods outperform random shift methods apart from in Crazy Climber, where the opposite is true. We believe this may be due to random shift augmentations actually reﬂecting the inductive biases concerning generalization in this environment, and believe this merits further investigation. Finally, we observe that using 10-step returns instead of 3 generally harms performance with A-LIX, with justiﬁcation given in App. C.2. n r u t e R 900 800 700 600 500 400 300 Cheetah Run Quadruped Run 800 600 400 200 A-LIX Adaptive Random Shifts LIX Random Shifts (DrQ-v2) 0 1 Frames (×106) 2 3 0 1 Frames (×106) 2 3 e r o c S d e z i l a m r o N n a m u H 2.0 1.5 1.0 0.5 0.0 A-LIX Adaptive Random Shifts LIX A-LIX 10-step Random Shifts Battle Zone Crazy Climber Ms Pacman Pong Atari 100k Task Stabilizing Off-Policy Deep Reinforcement Learning from Pixels F. Additional Ofﬂine Experiment Analysis F.1. Behavior Cloning without Augmentations Figure 24. Returns of agents over 5 seeds. Solid lines represent median performance, faded lines represent individual runs. The grey dotted horizontal line represents mean expert performance. To illustrate that test time shift invariance is not required, we show that it is possible to learn a policy through supervised learning. To do this, we generate a pixel-based dataset of 500,000 timesteps under an expert policy in Cheetah Run, and jointly train a CNN encoder and policy using behavior cloning/supervised learning by minimizing the loss L = (a − π(o))2 until convergence, where o follows the stacked frame image inputs of (Mnih et al., 2013). We see that the pixel-based policy performs as well as the behavior agent, despite using both higher dimensional data and fewer than half the samples compared to existing expert ofﬂine RL benchmarks from proprioceptive states (Fu et al., 2021). This provides clear evidence that shift invariance is not required at test time, and motivates us to ﬁnd an alternative explanation for why random shift augmentations help the learning process in TD-learning. An alternative perspective is that when the learning signal is strong, as is the case for supervised learning (and later stages during online learning when target values are more accurate), the natural bias of CNNs to learn lower order representations acts as an implicit regularizer (Rahaman et al., 2019) that results in test-time generalization. F.2. Turning Off Augmentations We present more evidence showing that augmentations beneﬁt learning the most at the beginning of training. In Fig. 25 we show the effect of turning off augmentations at 200,000 steps in Cheetah Run, and at 500,000 in Quadruped Walk. In both instances, we see large improvements over not augmenting at all, and both nearly converge to the same value as DrQ-v2, showing further evidence that stability initially in learning is vital. We posit that turning off augmentations here did not yield similar beneﬁts to Fig. 2 due to the fact that there is still high-frequency information in the targets (consider that the augmentations in Cheetah Run are switched off signiﬁcantly earlier) that cause a marginal amount of self-overﬁtting, reducing the rate of learning due to feature space degeneration. Figure 25. Returns of agents over 5 seeds. Solid lines represent median performance, faded lines represent individual runs. The grey dashed line shows when augmentations are turned off. Cheetah Run: BC 900 Expert Perfomance 800 n r u t e R 700 600 0 No Augs 4 1 2 3 SGD Steps (×105) Cheetah Run Quadruped Walk Augs Turned Off Augs Turned Off 750 500 250 n r u t e R 0 0 1 2 3 0 1 2 3 Frames (×106) Frames (×106) Augs No Augs Augs Removed Stabilizing Off-Policy Deep Reinforcement Learning from Pixels F.3. Action-Value Surfaces Here we show the action-value surfaces of the ofﬂine agents’ critics at various tuples sampled from the data. This provides us with an intuition over the loss landscape that the policies will be optimizing during the policy improvement, as accordingly the policy under the deterministic policy gradient (Silver et al., 2014) updates its own weights towards maximizing the action-values deﬁned by the critic through the chain rule: ∇φJπ ≈ Es∼E (cid:2)∇aQθ(s, a)|a=fφ(s)∇φfφ(s)(cid:3) (10) where φ and θ are policy and critic weights respectively. We hypothesize that self-overﬁtting reduces the sensitivity of the critic to actions, discarding important information regarding the causal link between actions and expected returns. To evaluate this, we sample state-action pairs from our replay buffer, and then visualize the action-value surface by sampling two random orthogonal direction vectors from the action space A. We then normalize the direction vectors to have a 2-norm of 1, and then multiply each direction vector by scalars α, β ∈ [−2, 2] respectively. We then plot the action-value surface as a result of adding the random vectors multiplied by their respective scalars onto the action sampled from ofﬂine dataset, giving us a 3-D surface. We clip actions to a ∈ [−1, 1]|A| as actions are squashed to this range in the policy through a truncated normal distribution. (a) Random Sampled State-Action Pair 1 (b) Random Sampled State-Action Pair 2 (c) Random Sampled State-Action Pair 4 (d) Random Sampled State-Action Pair 4 Figure 26. Action-Value loss surface plotted with respect two orthogonal random directions sampled from the action space (i.e., dr ∈ A and d1 ⊥ d2). We see that the critics learned by the augmented agents are more sensitive to changes in action. We believe this is due to the non-augmented agents overﬁtting to the observations, thus ignoring the lower-dimensional action inputs. To validate this, we sampled 128 random state-action tuples from the ofﬂine buffer, and calculated the average variance across the loss surfaces. We see a signiﬁcant difference, with the augmented agent having an average loss surface variance of 0.0129, whereas the non-augmented agent has an average loss surface variance of 0.0044, suggestive of lower sensitivity. F.4. Evidence of Critic MLP Overﬁtting from High-Frequency Features We provide further evidence that measuring high-frequency features through the ND score is vital to understanding overﬁt by showing how overﬁtting is able to occur in the fully-connected critic layers, which are usually stable under proprioceptive observations (see Table 2). To do this, we construct a pattern containing high frequency checkerboard noise c ∈ RH×W , and produce as many patterns as there are channels C in the ﬁnal layer. To ensure consistency across each individual feature map, we normalize each checkerboard pattern by the maximum value in its respective feature map, and then divide by the width of the checkerboard. We then add this pattern multiplied by a scalar α onto each feature map. Critic with Augmentations Critic w/o Augmentations 0.2 0.4 0.6 0.8 0 1 A ctio n S u bsp ace 2 1 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1 0 1 1 0 Loss 1 0 Action Subspace 1 1 Critic with Augmentations Critic w/o Augmentations 0.6 0.8 1.0 0 1 A ctio n S u bsp ace 2 1 0.6 0.8 1.0 1 0.6 0.8 1.0 1 0 1 1 0 Loss 1 0 Action Subspace 1 1 Critic with Augmentations Critic w/o Augmentations 0.4 0.6 0.8 0 1 A ctio n S u bsp ace 2 1 0.4 0.6 0.8 1 0.4 0.6 0.8 1 0 1 1 0 Loss 1 0 Action Subspace 1 1 Critic with Augmentations Critic w/o Augmentations 0.4 0.6 0.8 1 0 A ctio n S u bsp ace 2 1 0.4 0.6 0.8 0.4 0.6 1 0 0.8 Loss 1 0 1 1 1 0 Action Subspace 1 1 Stabilizing Off-Policy Deep Reinforcement Learning from Pixels (a) Example checkerboard artefacts. (b) Sensitivity of agents to checkerboard artifact weight Figure 27. Effect of checkerboard artifacts on feature maps and resultant loss sensitivity. We see the non-augmented agent is signiﬁcantly more sensitive to this high-frequency noise. As we see, the loss is signiﬁcantly more sensitive to high-frequency perturbations in the non-augmented agent, justifying its reliance on high-frequency patterns in the feature maps to enable self-overﬁtting. F.5. Additional Loss Surfaces Here we show the loss surfaces of the ofﬂine agents under policy evaluation with at 1,000, 5,000, and 10,000 training steps. We also show the surfaces respect to only the MLP layers, again following the normalization approach of Li et al. (2018). (a) 1,000 SGD Steps (b) 5,000 SGD Steps (c) 10,000 SGD Steps Figure 28. Loss surface plotted with respect to Encoder parameters at various stages of training. (a) 1,000 SGD Steps (b) 5,000 SGD Steps (c) 10,000 SGD Steps Figure 29. Loss surface plotted with respect to Critic MLP parameters at various stages of training. As we see, the loss surface with respect to the MLP parameters is signiﬁcantly less sharp, lending further evidence that self-overﬁtting is predominately a result of the ﬂexibility of the CNN layers to learn high-frequency features. Augmented Agent Non-Augmented Agent 0.4 0.3 0.2 s s o L D T 0.1 0.0 0.00 Non Augs Augs 0.25 0.50 (Checkerboard Weight) 0.75 Critic with Augmentations Critic w/o Augmentations s s o L D T 0.15 0.10 0.05 1 0 W eig ht S u bsp ace 2 1 0.15 0.10 0.05 1 0.15 0.10 0.05 0 TD Loss 1 0 1 1 1 0 Weight Subspace 1 1 Critic with Augmentations Critic w/o Augmentations s s o L D T 0.75 0.50 0.25 0 1 W eig ht S u bsp ace 2 1 0.75 0.50 0.25 1 0.75 0.50 0.25 0 TD Loss 1 0 1 1 1 0 Weight Subspace 1 1 Critic with Augmentations Critic w/o Augmentations s s o L D T 0.75 0.50 0.25 1 0 W eig ht S u bsp ace 2 1 0.75 0.50 0.25 1 0.75 0.50 0.25 0 TD Loss 1 0 1 1 1 0 Weight Subspace 1 1 Critic with Augmentations Critic w/o Augmentations s s o L D T 4 2 1 0 W eig ht S u bsp ace 2 1 4 2 4 2 1 0 TD Loss 1 0 1 1 1 0 Weight Subspace 1 1 Critic with Augmentations Critic w/o Augmentations s s o L D T 20 10 1 0 W eig ht S u bsp ace 2 1 20 10 20 10 1 0 TD Loss 1 0 1 1 1 0 Weight Subspace 1 1 Critic with Augmentations Critic w/o Augmentations s s o L D T 10 5 1 0 W eig ht S u bsp ace 2 1 10 5 10 5 1 0 TD Loss 1 0 1 1 1 0 Weight Subspace 1 1"
462,Speaker Diarization and Identification from Single-Channel Classroom   Audio Recording Using Virtual Microphones,"[{'href': 'http://arxiv.org/abs/2207.00660v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2207.00660v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-07-01 21:03:50,"Personalized Showcases: Generating Multi-Modal Explanations for Recommendations An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, Julian McAuley UC San Diego La Jolla, CA, USA {ayan,zhh004,j9li,tiz010,jmcauley}@ucsd.edu 2 2 0 2 n u J 0 3 ] R I . s c [ 1 v 2 2 4 0 0 . 7 0 2 2 : v i X r a ABSTRACT Existing explanation models generate only text for recommenda- tions but still struggle to produce diverse contents. In this paper, to further enrich explanations, we propose a new task named per- sonalized showcases, in which we provide both textual and visual information to explain our recommendations. Specifically, we first select a personalized image set that is the most relevant to a user’s interest toward a recommended item. Then, natural language ex- planations are generated accordingly given our selected images. For this new task, we collect a large-scale dataset from Google Lo- cal (i.e., maps) and construct a high-quality subset for generating multi-modal explanations. We propose a personalized multi-modal framework which can generate diverse and visually-aligned ex- planations via contrastive learning. Experiments show that our framework benefits from different modalities as inputs, and is able to produce more diverse and expressive explanations compared to previous methods on a variety of evaluation metrics. 1 INTRODUCTION Personalized explanation generation models have the potential to increase the transparency and reliability of recommendations. Previous works [1, 7, 49, 52] considered generating textual explana- tions from users’ historical reviews, tips [27] or justifications [31]. However, these methods still struggle to provide diverse explana- tions because a large amount of general sentences (e.g., ‘food is very good!’) exist in generated explanations and the text gener- ation models lack grounding information (e.g., images) for their generation process. To further diversify and enrich explanations for recommendations, we propose a new explanation generation task named personalized showcases (shown in Figure 1). In this new task, we explain recommendations via both textual and visual infor- mation. Our task aims to provide a set of images that are relevant to a user’s interest and generate textual explanations accordingly. Compared to previous works that generate only text as explana- tions, our showcases present diverse explanations including images and visually-guided text. To this end, the first challenge of this task is building a dataset. Existing review datasets (e.g., Amazon [31] and Yelp1) are largely unsuitable for this task (we further discuss these datasets in Sec- tion 3.2). Thus, we first construct a large-scale multi-modal dataset, namely Gest, which is collected from Google Local2 Restaurants including review text and corresponding pictures. Then, to improve the quality of Gest for personalized showcases, we annotate a small subset to find highly matched image-sentence pairs. Based on the annotations, we train a classifier with CLIP [36] to extract 1https://www.yelp.com/dataset 2https://www.google.com/maps Figure 1: Illustration of previous text-only explanation and our personalized showcases for recommendations. Given a recommended item or business: (1) Text-only Explanation models only use historical textual reviews from user and item sides to generate textual explanations. (2) We propose a personalized showcases task to enrich the personalized ex- planations with multi-modal (visual and textual) informa- tion, which can largely improve the informativeness and di- versity of generated explanations. visually-aware explanations from the full dataset. The images and text explanations from users are used as the learning target for personalized showcases. For this new task, we design a new multi-modal explanation framework. To begin with, the framework selects several images from historical photos of the business that the user is most in- terested in. Then, the framework takes the displayed images and users’ profiles (e.g., historical reviews) as inputs and learns to gen- erate textual explanations with a multi-modal decoder. However, generating expressive, diverse and engaging text that will capture users’ interest remains a challenging problem. First, different from previous textual explanation generation, the alignment between multiple images and generated text becomes an important problem for showcases, which poses higher requirements for information extraction and fusion across modalities. Second, a typical encoder- decoder model with a cross-entropy loss and teacher forcing can easily lead to generating repetitive and dull sentences that occur frequently in the training corpus (e.g., “food is great”) [18]. To tackle these challenges, we propose a Personalized Cross- Modal Contrastive Learning (PC2L) framework by contrasting in- put modalities with output sequences. Contrastive learning has Recommendations … R1: Chinese Food R2: American Food R3: Japanese Food Food is very delicious! Burgers are great, service is good, too. Great selection of beers and delicious burgers! The bread that comes with the entree soup is amazing. The cheesecake is on point. Previous: Text-Only Explanations (e.g. Ref2Seq) Ours: Personalized Showcases (Visual+Textual) An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, Julian McAuley Figure 2: Example of business and user reviews in Gest. For a business (e.g., an Italian restaurant), Gest contains historical reviews and images from different users. drawn attention as a self-supervised representation learning ap- proach [5, 33]. However, simply training with negative samples in a mini-batch is suboptimal [23] for many tasks, as the randomly se- lected embeddings could be easily discriminated in the latent space. Hence, we first design a cross-modal contrastive loss to enforce the alignment between images and output explanations, by construct- ing hard negative samples with randomly replaced entities in the output. Motivated by the observation that users with similar histori- cal reviews share similar interests, we further design a personalized contrastive loss to reweight the negative samples based on their history similarities. Experimental results on both automatic and human evaluation show that our model is able to generate more expressive, diverse and visually-aligned explanations compared to a variety of baselines. Overall, our contributions are as follows: • To generate more informative explanations for recommenda- tions, we present a new task: personalized showcases which can provide both textual and visual explanations for recom- mendations. • For this new task, we collect a large-scale multi-modal dataset from Google Local (i.e., maps). To ensure alignment between images and text, we annotate a small dataset and train a classifier to propagate labels on Gest, and construct a high- quality subset for generating textual explanations. • We propose a novel multi-modal framework for personalized showcases which applies contrastive learning to improve diversity and visual alignment of generated text. Comprehen- sive experiments on both automatic and human evaluation indicate that textual explanations from our showcases are more expressive and diverse than existing explanation gen- eration methods. 2 TASK DEFINITION In the personalized showcases task, we aim to provide both per- sonalized textual and visual explanations to explain recommen- dations for users. Formally, given user 𝑢 ∈ 𝑈 and business (item) 𝑏 ∈ 𝐵, where 𝑈 and 𝐵 are the user set and business set respectively, the personalized showcases task will provide textual explanations 𝑆 = {𝑠1, 𝑠2, ..., 𝑠𝑚 } and visual explanations 𝐼 = {𝑖1, 𝑖2, ..., 𝑖𝑛 }, where 𝑠 and 𝑖 represent sentences and images in explanations. 𝑆 and 𝐼 are matched with each other and personalized to explain why 𝑏 is recommended to 𝑢. To better study the relation between textual and visual expla- nations and provide baselines for future work, in this paper, we decompose the task into two steps as shown in Figure 5: (1) Select- ing an image set as a visual explanation that is relevant to a user’s interest; (2) Generating textual explanations given the selected images and a user’s historical reviews. |𝐼𝑏 | , 𝑖𝑏 2 , . . . 𝑖𝑏 Formally, given user 𝑢, business 𝑏 and the image candidate set 𝐼𝑏 = {𝑖𝑏 } from 𝑏, we first select a set of images as visual 1 explanations 𝐼 from 𝐼𝑏 which user 𝑢 will be interested in, based on user 𝑢’s profile (i.e., historical reviews 𝑋𝑢 = {𝑥𝑢 𝐾 } 1 and images 𝐼𝑢 = {𝑖𝑢 , ..., 𝑖𝑢 𝑛 }). Then, we use the user’s historical 1 reviews 𝑋𝑢 and selected images 𝐼 to generate visually-aware textual explanations 𝑆. , ..., 𝑥𝑢 , 𝑥𝑢 2 , 𝑖𝑢 2 For our method, we consider the following aspects: • Accuracy: We aim to predict the target images (i.e., images associated with the ground-truth review) from business im- age candidates correctly, and the generated text is expected to be relevant to the business. • Diversity: The selected images should be diverse and cover more information from businesses (e.g., including more dishes from a restaurant). Textual explanations should be diverse and expressive. • Alignment: Unlike previous explanation or review gener- ation tasks which only use historical reviews or aspects as inputs, our visually-aware setting provides grounding to the images. Hence the generated explanations in this new task should aim to accurately describe the content and cover the main objects (e.g., the name of dishes, the environment) in the given set of images. Amazing! Best Cesar salad I ever had and the cake was delicious. Seafood soup was excellent. Granddaughter loved the Spaghetti and meatballs. I had an excellent experience at this restaurant. The ambience is romantic and perfect for a couple date night. An Italian Restaurant User Reviews Personalized Showcases: Generating Multi-Modal Explanations for Recommendations Figure 3: Visual Diversity Comparison. A, B, C, E in Ama- zon denote different categories of amazon review datasets, which are uniformly sampled from All, Beauty, Clothing and Electronics, respectively. Intra-/Inter- User Diversity for the Yelp dataset is unavailable since Yelp images lack user information. 3 DATASET 3.1 Dataset Statistics We collected reviews with images from Google Local. Gest-raw in Table 1 shows the data statistics of our crawled dataset. We can see that Gest-raw contains 1,771,160 reviews from 1,010,511 users and 65,113 businesses. Every review has at least one image and the raw dataset has 4,435,565 image urls. We processed our dataset into two subsets as (1) Gest-s1 for personalized image set selection, and (2) Gest-s2 for visually-aware explanation generation. Statistics of our processed dataset are in Ta- ble 1, with more processing details in Section 3.3 and Appendix A. 3.2 Visual Diversity Analysis To distinguish our Gest from existing review datasets and show the usefulness of personalized showcases, we first define CLIP-based dis- similarity in three levels to measure the diversity of user-generated images in each business. Then, we compare the visual diversities between our Gest data with two representative review datasets, Amazon Reviews [29, 31] and Yelp. First, similar to [36, 53], we use the cosine similarity (denoted as sim) from pre-trained CLIP to define the dis-similarity between image 𝑖𝑚 and 𝑖𝑛 as dis(𝑖𝑚, 𝑖𝑛) = 1 − sim(𝑖𝑚, 𝑖𝑛). Thus, we introduce visual diversity in three levels as Intra-Business Div, Inter-User Div and Intra-User Div, which are formally defined in Appendix B; higher scores mean more visual diversity. Then, we investigate the visual diversities for our Gest data as well as Amazon Reviews (using all categories All (A) and sub- categories Beauty (B), Clothing (C), Electronics (E)) and Yelp. For Amazon, we treat each item page as a “business” because reviews are collected according to items. In our calculation, we sample 5,000 items with more than one user-uploaded image. Note that images in Yelp dataset do not have user information, so we cannot calculate user-level diversities for Yelp. From Figure 3, we have the following observations: • Diversities within datasets: Figure 3 shows that for Gest and Amazon, Inter-User Div is the highest and Intra-User Div is the lowest. It indicates even for the same business (item), users focus on and present different visual information. Figure 4: Example of user-generated images from Amazon from an item page and for Yelp from a business. Amazon images mainly focus on a single item and Yelp images for a business are diverse (yet the current public Yelp dataset has no user-image interactions). Table 1: Data statistics for Gest. Avg. R. Len. denotes average review length and #Bus. denotes the number of Businesses. -raw denotes raw Gest. -s1 denotes Gest data for the first step, and -s2 denotes Gest data for the second step of our proposed personalized showcases framework. Dataset #Image #Review #User #Bus. Avg. R. Len. Gest-raw 4,435,565 1,722,296 Gest-s1 203,433 Gest-s2 1,771,160 370,563 108,888 1,010,511 119,086 36,996 65,113 48,330 30,831 36.26 45.48 24.32 • Gest vs. Amazon: In Figure 3, three visual diversities of Amazon are consistently lower than Gest by a large margin. We try to explain this by discussing the difference of user behaviors on these two platforms. As an example in Figure 4, user-generated images usually focus on the purchased item. Though the information they want to show differs, there is usually a single object in an image (i.e., the purchased item). Thus visual diversity is limited. While for Gest, as examples in Figure 2 show, reviews on restaurants allow users to share more diverse information from more varied items, angles or aspects. Compared with Amazon, using Gest should generate more informative personalized showcases according to different user profiles. • Gest vs. Yelp: Yelp images are high-quality (as an example in Figure 4) and the intra-business div. is higher (0.44) than Gest (0.39). Images in Yelp themselves are similar to images in Gest. However, Yelp images do not fit our task due to the lack of user information. 3.3 Explanation Distillation Reviews often contain uninformative text that is irrelevant to the images, and cannot be used directly as explanations. Hence, we con- struct an explanation dataset from Gest-raw. We distill sentences in reviews that align with the content of a given image as valid explanations. Three annotators were asked to label 1,000 reviews (with 9,930 image-sentence pairs) randomly sampled from the full dataset. The task is to decide if a sentence describes a image. Label- ing was performed iteratively, followed by feedback and discussion, Intra-Business Div Inter-User Div Intra-User Div 0.4 0.2 0.0 GEST Amazon-A Amazon-B Amazon-C Amazon-E Yelp Amazon Yelp … … An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, Julian McAuley Figure 5: Illustration of our personalized showcases framework for the given business. We take user historical images and tex- tual reviews as inputs. First, we select an image set that is most relevant to a user’s interest. Then we generate natural language explanations accordingly with a multi-modal decoder. A cross-modal contrastive loss and a personalized contrastive loss are applied between each input modality and the explanations. Last, the selected images and generated textual explanations will be organized as multi-modal explanations to users. until the quality was aligned between the three annotators. The annotated image-sentence pairs are then split into train, validation, and testing with a ratio of 8:1:1. We then train a binary classification model Φ based on these annotated image-sentence pairs and their corresponding labels. Specifically, we extract the embedding of each sentence and image via CLIP. The two features are concatenated and fed into a fully connected layer. The classifier achieves an AUC of 0.97 and F-1 score of 0.71 on the test set, where similar results are obtained in [31] for building a text-only explanation dataset. We use this model to extract explanations from all reviews. The statistics of the dataset Gest-s2 can be found in Table 1. 4 METHODOLOGY In this section, we present our framework of producing personal- ized showcases. As the overview shows (Figure 5), we start with personalized image set selection and the visually-aware explanation generation module, then introduce our personalized cross-modal contrastive learning approach in Section 4.3. 4.1 Personalized Image Set Selection The first step is to select an image set as a visual explanation that is relevant to a user’s interests, and is diverse. We formulate this selection step as diverse recommendation with multi-modal inputs. Multi-Modal Encoder. Generally, these user textual- or visual- profiles can be effectively encoded with different pre-trained deep neural networks (e.g., ResNet [16], ViT [11], BERT [9]). Here we choose CLIP [35], a state-of-the-art pre-trained cross-modal re- trieval model as both textual- and visual-encoders. CLIP encodes raw images as image features, and encodes user textual- and visual- profiles as user profile features. Image Selection Model. We use a Determinantal Point Process (DPP) method [22] to select the image subset, which has recently been used for different diverse recommendation tasks [2, 45]. Com- pared with other algorithms for individual item recommendation, DPP-based models are suitable for multiple image selection. Given user 𝑢 and business 𝑏, we predict the image set ˆ𝐼𝑢,𝑏 as follows: ˆ𝐼𝑢,𝑏 = DPP(𝐼𝑏, 𝑢), (1) where 𝐼𝑏 is the image set belonging to business 𝑏. In our design, we calculate user-image relevance using the CLIP-based user’s profile features and image features. More details of the model are in [45]. 4.2 Visually-Aware Explanation Generation After obtaining an image set, we aim to generate personalized expla- nations given a set of images and a user’s historical reviews, with the extracted explanation dataset Gest-s2 in Section 3.3. Specifically, we build a multi-modal encoder-decoder model with GPT-2 [37] as the backbone. Multi-Modal Encoder. Given a set of user 𝑢’s3 historical reviews 𝑋 = {𝑥1, 𝑥2, . . . , 𝑥𝐾 }, we use the text encoder of CLIP to extract the review features 𝑅 = {𝑟1, 𝑟2, . . . , 𝑟𝐾 }. Similar operations are applied to the input images 𝐼 = {𝑖1, 𝑖2, . . . , 𝑖𝑛 }, where we use a pretrained ResNet to extract the visual features 𝑉 = {𝑣1, 𝑣2, . . . , 𝑣𝑛 }. Those features are then projected into a latent space: 𝑖 = 𝑊 𝑉 𝑣𝑖, 𝑍 𝑅 𝑍𝑉 𝑖 𝑟𝑖, (2) where 𝑊 𝑉 and 𝑊 𝑅 are two learnable projection matrices. Then we use a multi-modal attention (MMA) module with stacked self- attention layers [43] to encode the input features: ; 𝑍 𝑅]), ; 𝐻 𝑅] = MMA([𝑍𝑉 𝑖 = 𝑊 𝑅 [𝐻𝑉 (3) , 𝐻 𝑅 where each 𝐻𝑉 𝑖 aggregate features from two modalities and 𝑖 [; ] denotes concatenation. This flexible design allows for variable lengths of each modality and enables interactions between modali- ties via co-attentions. 3We omit the subscript 𝑢 below for simplicity All review images from the business Multi- Modal Encoder Selection Model … User historical images You have to get the scallops. The “one bad hombre” drink is amazing! …… User historical reviews … STEP 1: Personalized Image Set Selection Cross-Modal Contrastive Learning Multi- Modal Encoder Multi- Modal Decoder Personalized Contrastive Learning STEP 2: Visually-Aware Explanation Everything was fresh and good. Toro Sushi was the bomb and I even dream about it the night after! Personalized Showcases: Generating Multi-Modal Explanations for Recommendations Multi-Modal Decoder. Inspired by recent advances of powerful pre-trained language models, we leverage GPT-2 as the decoder for generating explanations. To efficiently adapt the linguistic knowl- edge from GPT-2, we insert the encoder-decoder attention module into the pre-trained model with a similar architecture in [4]. With this multi-modal GPT-2, given a target explanation 𝑌 = {𝑦1, 𝑦2, ..., 𝑦𝐿 }, the decoding process at each time step 𝑡 can be formalized as ˆ𝑦𝑡 = Decoder([𝐻𝑉 ; 𝐻 𝑅], 𝑦1, . . . , 𝑦𝑡 −1). (4) We use a cross-entropy (CE) loss to maximize the conditional log likelihood log 𝑝𝜃 (𝑌 |𝑋, 𝐼 ) for 𝑁 training samples (𝑋 (𝑖), 𝐼 (𝑖), 𝑌 (𝑖) )𝑁 𝑖=1 as follows: LCE = − 𝑁 ∑︁ 𝑖=1 log 𝑝𝜃 (𝑌 (𝑖) |𝑋 (𝑖), 𝐼 (𝑖) ). (5) We use ground truth images from the user for training and images from our image-selection model for inference. 4.3 Personalized Cross-Modal Contrastive Learning Unlike image captioning tasks where the caption is a short descrip- tion of an image, our task utilizes multiple images as “prompts” to express personal feelings and opinions about them. To encourage generating expressive, diverse and visual-aligned explanations, we propose a Personalized Cross-Modal Contrastive Learning (𝑃𝐶2𝐿) framework. We first project the hidden representations of images, historical reviews, and the target sequence into a latent space: ˜𝐻𝑌 = 𝜙𝑌 (𝐻𝑌 ) ˜𝐻𝑉 = 𝜙𝑉 (𝐻𝑉 ), ˜𝐻 𝑅 = 𝜙𝑅 (𝐻 𝑅), (6) where 𝜙𝑉 , 𝜙𝑅, and 𝜙𝑌 consist of two fully connected layers with ReLU activation [30] and average pooling over the hidden states 𝐻𝑉 , 𝐻𝑅 and 𝐻𝑌 from the last self-attention layers. For the vanilla contrastive learning with InfoNCE loss [5, 33], we then maximize the similarity between the pair of source modality and target se- quence, while minimizing the similarity between the negative pairs as follows: LCL = − 𝑁 ∑︁ 𝑖=1 log exp(𝑠𝑋 ,𝑌 𝑖,𝑖 ) + (cid:205) 𝑗 ∈𝐾 ) exp(𝑠𝑋 ,𝑌 𝑖,𝑗 exp(𝑠𝑋 ,𝑌 𝑖,𝑖 , ) (7) , ˜𝐻𝑌 ( 𝑗) = sim( ˜𝐻 𝑋 (𝑖) where 𝑠𝑋 ,𝑌 )/𝜏, sim is the cosine similarity be- 𝑖,𝑗 tween two vectors, 𝜏 is the temperature parameter, (𝑖) and ( 𝑗) are two samples in the mini-batch, 𝐾 is the set of negative samples for sample (𝑖). One challenge of this task is the model is asked to describe multiple objects or contents in a set of images. To ensure the visual grounding between multiple image features and output text, we design a novel cross-modal contrastive loss. Specifically, given a target explanation 𝑌 = {𝑦1, 𝑦2, ..., 𝑦𝐿 }, we randomly replace the entities 4 in the text with other entities presented in the dataset to construct a hard negative sample 𝑌 ent = {𝑦 ′ , ...𝑦𝐿 } (i.e., “I like the sushi” to “I like the burger”), such that during training, the model is exposed to samples with incorrect entities regarding the images, which are non-trivial to distinguish from the original , 𝑦2, ...𝑦 ′ ent2 ent1 4We extract entities using spaCy noun chunks (https://spacy.io/). target sequence. Thus, we add the hidden representation of 𝑌 ent as an additional negative sample ent to formulate the cross-modal contrastive loss: LCCL = − 𝑁 ∑︁ 𝑖=1 log exp(𝑠𝑉 ,𝑌 𝑖,𝑖 ) exp(𝑠𝑉 ,𝑌 𝑖,𝑖 ) + (cid:205) 𝑗 ∈𝐾∪ent , (8) exp(𝑠𝑉 ,𝑌 𝑖,𝑗 ) On the other hand, to enhance the personalization of explanation generation, we re-weight negative pairs according to user personal- ities. The intuition is that users with more distinct personalities are more likely to generate different explanations. Motivated by this, we propose a weighted contrastive loss for personalization: LPCL = − 𝑁 ∑︁ 𝑖=1 log exp(𝑠𝑅,𝑌 𝑖,𝑖 ) 𝑖,𝑖 ) + 𝑓 (𝑖, 𝑗) (cid:205) 𝑗 ∈𝐾 exp(𝑠𝑅,𝑌 . (9) exp(𝑠𝑅,𝑌 𝑖,𝑗 ) where negative pairs in a mini-batch are re-weighted based on user personality similarity function 𝑓 . In our framework, user person- alities are represented by their historical reviews. Specifically, we define 𝑓 function as: 𝑓 (𝑖, 𝑗) = 𝛼 (1−sim( ˜𝑅 (𝑖 ) , ˜𝑅 ( 𝑗 ) )) (10) i.e., we reduce the weights of negative pairs with similar histories, and increase those with distinct histories. 𝛼 (𝛼 > 1) is a hyperparam- eter that weighs the negative samples, sim is the cosine similarity, ˜𝑅 (𝑖) and ˜𝑅 ( 𝑗) are the average features of two users’ input historical reviews. Overall, the model is optimized with a mixture of a cross-entropy loss and the two contrastive losses: L𝑙𝑜𝑠𝑠 = LCE + 𝜆1LCCL + 𝜆2LPCL, (11) where 𝜆1 and 𝜆2 are hyperparameters that weigh the two losses. 4.4 A Metric for Visual Grounding As mentioned in Section 2, we want our model to generate explana- tions that can accurately describe the content in a given image set. Typical n-gram evaluation metrics such as BLEU compute scores based on n-gram co-occurrences, which are originally proposed for diagnostic evaluation of machine translation systems but not capa- ble of evaluating text quality, as they are only sensitive to lexical variation and fail to reward semantic or syntactic variations be- tween predictions and references [38, 39, 50]. To effectively test the performance of the alignment between visual images and text ex- planations, we design an automatic evaluation metric: CLIP-Align based on [36]. Given a set of images 𝐼 = {𝑖1, 𝑖2, ..., 𝑖𝑛 } and a set of sentences from the generated text 𝑆 = {𝑠1, 𝑠2, ..., 𝑠𝑚 }, we first extract the embeddings of all the images and sentences with CLIP, we compute the metric as follows: CLIP-Align = 1 𝑛 𝑛 ∑︁ 𝑖=1 𝑚𝑎𝑥 ({cs1,𝑖, ..., cs𝑚,𝑖 }) (12) where cs𝑖,𝑗 is the confidence score produced by the CLIP-based classifier Φ trained on our annotated data. By replacing cs𝑖,𝑗 with the cosine similarity of image and sentence embeddings, we obtain another metric CLIP-Score, similar to [17]. Table 2: Results on personalized showcases with different models and different input modalities. Results are reported in per- centage (%). GT is the ground truth. An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, Julian McAuley Model Input N-Gram Metrics Diversity Metrics Embedding Metrics BLEU-1 METEOR NIST Distinct-1 Distinct-2 CLIP-Align CLIP-Score BERT-Score GT ST R2Gen Ref2Seq Peter Ours - img img text text - 8.24 6.47 7.09 8.89 img img+text 9.92 10.40 - 3.41 3.10 3.80 3.28 3.64 3.83 - 28.08 36.55 30.78 34.45 37.35 50.64 6.06 2.74 3.23 0.92 0.38 3.37 3.58 43.23 17.41 22.45 5.89 1.27 26.37 28.58 90.47 80.84 82.07 73.51 72.70 84.78 85.31 28.41 24.31 24.28 23.83 23.27 24.68 24.50 - 85.20 85.89 84.71 86.94 88.03 88.23 Compared with previous CLIP-based metrics [17, 53], CLIP- Align focuses specifically on the accuracy and the alignment be- tween objects in the sentences and the images (e.g. “food is great” and “burger is great” achieves similar high scores with the same burger image computed on CLIP-Score, and a model that repet- itively generates “food is great” can reach high performance on CLIPscore in corpus level). Moreover, the vanilla CLIPscore [17] showed poor correlations with captions containing personal feel- ings, making it less suitable for this task. We show in Section 5 with automatic and human evaluation results that our metric performs better when evaluating alignment between images and text. 5 EXPERIMENTS In this section, we conduct extensive experiments to evaluate the performance of our personalized showcases framework. Ablation studies show the influence of different modalities to personalized showcases. Case studies and human evaluation are conducted to validate that our model present more diverse and accurate explana- tions than baselines. 5.1 Experimental Setting Baselines. To show the effectiveness of our model, we compare it with a number of popular baselines from different tasks, including image captioning, report generation and explanation generation: • ST [47] is a classic CNN+LSTM model for image captioning. • R2Gen [6] is a state-of-the-art memory-driven transformer specialized at generating long text with visual inputs. • Ref2Seq [31] is a popular reference-based seq2seq model for explanation generation in recommendation. • Peter [25] is a recent transformer-based explanation genera- tion model which uses the user and item IDs to predict the words in the target explanation. • img and text refer to image and text features respectively. K images {𝑖1, . . . , 𝑖𝐾 }, div@K is defined as: div@𝐾 = ∑︁ 1≤𝑚<𝑛 ≤𝐾 dis(𝑖𝑚, 𝑖𝑛) 𝐾 (𝐾 − 1)/2 . (13) For textual explanations, we first evaluate the relevance of gener- ated text and ground truth by n-gram based text evaluation metrics: BLEU (n=1,4) [34], METEOR [8] and NIST (n=4) [10]. To evaluate di- versity, we report Dinstinct-1 and Distinct-2 which is proposed in [24] for text generation models. We then use CLIP and BERT to compute embedding-based metrics. CLIP-Align is our proposed metrics in Section 4.2. CLIP-Score [17] BERT-Score [50] are two recent embedding-based metrics. Implementation Details. We use CLIP [35] with ViT-B/32 as image and text encoder to encode user historical reviews and images. We convert user profile feature into a 128-dimensional vector with a MLP model (1024→512→512→256→128), and convert candidate images with another MLP (512→512→512→256→128), where both models use ReLU activations [30]. We follow [45] to calculate each element of 𝑳 and optimize DPP using Adam [28] with an initial learning rate of 1e-3 and batch size 512. For inference, we use greedy decoding to select 𝐾 = 3 images as visual explanation. For training PC2L, we use AdamW [28] as the optimizer with an initial learning rate of 1e-4. The maximum sequence lengths are set to 64 which covers 95% of the explanations. The maximum number of images and historical reviews are set to 5 and 10 respectively. The hidden sizes of both the encoder and decoder are 768 with 12 heads. There are 3 layers in the encoder and 12 layers in the decoder. The batch size for training is 32. We use the GPT-2-small pre-trained weights with 117M parameters. The weighting parameters 𝜆1, 𝛼 and temperature 𝜏 are set to 0.2, 0.2, 𝑒 and 0.1 respectively. We use a beam size of 2 for decoding to balance the generation effectiveness and efficiency. Evaluation Metrics. For image selection, we report Precision@K, Recall@K and F1@K to measure the ranking quality. Due to the nature of our task, we set a small K (𝐾 = 3). To evaluate diversity, we introduce the truncated div@K (𝐾 = 3) for the average dissimi- larities for all image pairs in recommended images. Formally, given 5.2 Framework Performance We first report the model performance on text evaluation met- rics in Table 2, as we found this last step in our framework came with more challenges and interesting findings, e.g., how to gener- ate human-like explanations and avoid dull text, how to evaluate Personalized Showcases: Generating Multi-Modal Explanations for Recommendations Table 3: Ablation study for personalized image selection. Re- sults are reported in percentage (%). Accuracy Diversity Method random img text img+text Prec@3 Recall@3 F1@3 Div@3 4.87 25.21 15.28 25.21 6.14 34.05 20.58 34.37 5.43 28.97 17.54 29.09 30.24 17.12 18.68 17.07 the generation quality. Here the input images are selected by our model,5 and the input text consists of historical reviews from users. First, the clear gap between text-input models and image-input models on diversity and CLIP-based metrics validates the impor- tance of incorporating image features. The setting of visually-aware generation models is able to generate accurate explanations with diverse language style. Second, our 𝑃𝐶2𝐿 shows substantial im- provement on most of the metrics compared to LSTM and trans- former based models, showing that a pretrained language model with contrastive learning is able to generate high quality explana- tions. Finally, though text-based models Ref2Seq and Peter achieve competitive results with our method on some n-gram metrics such as BLEU and METEOR, their performance is much worse on di- versity and embedding metrics. The text quality is also low with repetitive and non-informative sentences appearing often, which we further validate with human evaluations and case studies. 5.3 Component Analysis We conduct ablation studies to evaluate the effectiveness of each component individually. Model for image set selection. First, we evaluate the perfor- mance of personalized image set selection. For general ranking performance, we compare our model with random selection and different input modalities. As shown in Table 3, though the trun- cated diversity of the text-only model is the highest, its performance is significantly worse than those with images in terms of ranking metrics. This indicates text input alone is far insufficient to pro- vide personalization for users, and its recommendation result is closer to that of random selection. Historical images on the other hand, provide an important visual cue for modeling users’ prefer- ence. Overall, a model with images and text can achieve the best ranking performance for image set selection, which validates the importance of our multi-modal setting for personalized showcases. Effectiveness of Contrastive Learning We conduct ablation stud- ies on different variations of our contrastive loss to verify the ef- fectiveness of our method. As shown in Table 4, our PC2L achieves the best performance over all baselines on different metrics. Specif- ically, CCL contributes more to the visual grounding by enforcing the model to distinguish random entities from the correct ones, and 5For effective training and evaluation of our framework, ground truth images of a given user are included in the image candidate pool for selecting. If it is for real-world deployment, ground truth images are not available but similar images can be selected. Table 4: Ablation study on contrastive learning. Baseline is to train a multi-modal decoder without contrastive learning. CL, CCL and PCL are the contrastive losses in Eq. (7), Eq. (8) and Eq. (9) Method Baseline img CL + text CL CCL+ text CL img CL + PCL 𝑃𝐶2𝐿 BLEU-1 Distinct-2 CLIP-Align 7.96 9.72 10.19 9.96 10.40 25.90 27.58 28.10 28.32 28.58 . 82.50 84.03 85.12 84.15 85.31 Figure 6: (a) The length distributions of generated texts on the test set. (b) The generated explanation coverage of nouns (Noun), adjectives (ADJ) and adverbs (ADV) in ground truth. improves CLIP-Align compared to the vanilla contrastive frame- work [5]. PCL improves more on diversity by encouraging the model to focus on users with dissimilar interest. To further evaluate the generation quality improved by con- trastive learning, we analyze the generated explanations from two aspects, length distributions of generations and keywords coverage. Figure 6 (a) compares the length distributions of generations on the test set to the ground truth. We categorize text lengths into 6 groups (within the range of [0, 60] with an interval of 10). The model without PC2L has a sharper distribution, while adding our PC2L leads to a distribution which is closer to the ground truth, demonstrating its effectiveness and the ability to generalize on unseen images. Note the ground truth contains more long texts than generations from the model since we set the max length to 64 during training and inference, which results in the discrepancy for text length greater than 60. Figure 6 (b) shows the keyword coverage (i.e., nouns, adjectives and adverbs) in output sentences. We consider an output as covering a keyword if the word exists in the corresponding ground truth. We compare two models trained with and without PC2L. We can see that PC2L improves the coverage of all kinds of keywords, which indicates our contrastive learning method diversifies and personalizes the generated text. Overall, incorporating contrastive learning into multi-modal explanation generation leads to better output quality with more diverse and visually-aligned texts. Can GPT-2 provide linguistic knowledge? Finally, we study whether GPT-2 can provide linguistic knowledge for our generation (cid:40)(cid:83)(cid:80)(cid:86)(cid:79)(cid:69)(cid:1)(cid:53)(cid:83)(cid:86)(cid:85)(cid:73) (cid:88)(cid:16)(cid:80)(cid:1) (cid:88)(cid:16)(cid:1) (cid:88)(cid:16)(cid:80)(cid:1) (cid:88)(cid:16)(cid:1) (cid:90) (cid:68) (cid:79) (cid:70) (cid:86) (cid:82) (cid:70) (cid:83) (cid:39) (cid:19)(cid:17)(cid:17)(cid:17) (cid:18)(cid:22)(cid:17)(cid:17) (cid:18)(cid:17)(cid:17)(cid:17) (cid:22)(cid:17)(cid:17) (cid:17) (cid:70) (cid:72) (cid:66) (cid:83) (cid:70) (cid:87) (cid:80) (cid:36) (cid:18)(cid:19)(cid:22)(cid:17) (cid:18)(cid:17)(cid:17)(cid:17) (cid:24)(cid:22)(cid:17) (cid:22)(cid:17)(cid:17) (cid:19)(cid:22)(cid:17) (cid:17) (cid:47)(cid:80)(cid:86)(cid:79) (cid:34)(cid:37)(cid:43)(cid:1)(cid:7)(cid:1)(cid:34)(cid:37)(cid:55) (b) (cid:17) (cid:18)(cid:17) (cid:21)(cid:17) (cid:20)(cid:17) (cid:19)(cid:17) (cid:53)(cid:70)(cid:89)(cid:85)(cid:1)(cid:45)(cid:70)(cid:79)(cid:72)(cid:85)(cid:73) (a) (cid:22)(cid:17) (cid:23)(cid:17) An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, Julian McAuley Figure 7: Comparison between text-only explanations (i.e., Ref2Seq and Text GPT-2) and our showcases. User reviews are pro- cessed following Section 3.3. Table 5: Ablation Study on different initializations of the decoder. Random randomly initializes model weights. Text GPT-2 and Img GPT-2 are initialized with weights from [37]. Img GPT-2 + FT finetunes the model on a corpus similar to our training text data. Results are in percentage (%). Table 6: Human evaluation results on two models. We present the workers with reference text and images, and ask them to give scores from different aspects. Results are statis- tically significant via sign test (p<0.01). Method Img Random Text GPT-2 Img GPT-2 Img GPT-2 + FT BLEU-1 Distinct-1 Distinct-2 5.21 4.81 7.59 7.10 0.23 3.43 4.05 4.32 5.08 19.27 29.41 30.82 task. We train models with different weight initializations, with ground truth images (Img) or historical reviews (Text) as inputs. As shown in Table 5, comparing the performance of random and GPT-2 initialization, it is evident that the pretrained weights play a significant role. Finetuning on in-domain data (260k samples from users with one review and excluded from our personalization dataset) further improves domain-specific knowledge of the decoder and benefits generation performance on diversity metrics. 5.4 Case Study We study three examples (see Figure 7) and compare our person- alized showcases to single-modal explanations from Ref2Seq and Text GPT-2. Overall, our visual explanations is able to recommend images that fit users’ interest. This indicates the effectiveness of our image selection module and the selected images can be used as valid visual explanations. More importantly, these images can provide grounding information for text generation such that the textual explanations become more informative (i.e., specific dishes), which aligns with our CLIP-Align metric as well as human evaluations in Section 5.5. As is shown in Figure 7, we can see historical review text alone cannot provide correct explanations (see Case 1) to the Method Expressiveness Visual Alignment Ref2Seq PC2L 3.72 4.25 3.65 4.10 user (i.e., explanations from Ref2Seq and Text GPT-2 are irrelevant to the user review) and the sentences are monotonous (see Case 2). In contrast, our showcase provides relevant and diverse textual explanations based on images. In case 3, our generated text missed some entities in the user’s review since it only correctly describes one of the selected images. Hence, generating texts from multiple images is still a challenging problem for this new task. As we can observe from the examples, Ref2Seq tends to gen- erate explanations with the same pattern, which also match the observation in Table 2 that it has low Distinct-1 and Distinct-2. 5.5 Human Evaluation To fully evaluate our model, we conduct human evaluation on Amazon Mechanical Turk.6 For each model, we randomly sample 500 examples from the test set. Each example is scored by three human judges using a 5-point Likert scale to reduce variance. We instruct the annotators to consider two perspectives, expressiveness (semantically correct, diversity, no repetition) and visual alignment (the text describes the context of the images). As is shown in Table 6, PC2L significantly outperforms Ref2Seq, which is consistent with the automatic evaluation metrics. 6https://www.mturk.com/ We ordered pork and shrimp spring rolls that came with a peanut-y dipping sauce. Then we ordered a chicken banh-mi and a lemongrass beef with noodles. The steak frites was tasty - it was charred, which I really liked, and topped with a butter sauce. The truffle fries were also really, really good. The burger was delicious though! My co worker said the Pork Torta was delicious! Other guys had Gyro, pizza and fish tacos. My Bacon Cheeseburger was excellent. we ordered the fried rice and it was very good. i had the grilled chicken sandwich , which was delicious . i had the grilled cheese sandwich and it was delicious ! i love it if you want to eat japanese - style ramen. first time here, i had the bbq bacon cheeseburger medium rare with onion rings. the rice pilaf was very good as well. if you like vietnamese food, you should try this place out. the spring rolls are a definite must -. the pho is good. old school rustic feel with a wide selection of burgers and beers. the burgers were done well …… bloody mary was perfect. food was wonderful, try the fried green tomato breakfast tacos. EXAMPLE 1 EXAMPLE 2 EXAMPLE 3 Processed User Reviews Previous Ref2Seq Previous Text GPT-2 Ours Personalized Showcases Personalized Showcases: Generating Multi-Modal Explanations for Recommendations 6 RELATED WORK 6.1 Explanation Generation There has been a line of work that studies how to generate explana- tions for recommendations. Some work generates product reviews based on categorical attributes [52] images [42], or aspects [32]. Due to noise in reviews, Li et al. [26] generated ‘tips’ from the Yelp dataset which are more concise and informative as explanations in recommendation. To further improve the quality of generation, Ni et al. [31] proposed to identify justifications by dividing re- views into text segments and classifying text segments to get “good” justifications. Li et al. [25] proposed transformer-based model for recommendation explanation generations by incorporating user, item embeddings and related features. These text generation tasks leverage historical reviews from users or items. Images, on the other hand, provide rich information and grounding for text generation. Moreover, multi-modal information in our task (i.e., images and text) are more acceptable than text as explanations for users. In this paper, we propose a new task for generating multi-modal explanations and present a framework that provides personalized image showcases and visually-aware text explanations for recom- mendations. 6.2 Multi-Modal Learning Recent years have witnessed the success of deep learning on multi- modal learning and pretraining [4, 20, 35, 41]. These models usually adopt the Transformer [43] structure to encode visual and textual features for pretraining, to later benefit the multimodal downstream tasks. Among them, CLIP [35] is a powerful model trained on a massive amount of image-caption pairs, and has shown a strong zero-shot capability on various vision and language tasks [40]. Sev- eral methods [17, 53] used CLIP embeddings to compute modality similarities as evaluation metrics for image captioning and text generation tasks. In our work, we used CLIP extensively as the multi-modal en- coder for our framework. We also designed a new metric based on CLIP for evaluating the visual alignment between the image set and generated explanations. 6.3 Contrastive Learning The goal of contrastive learning [14, 33] is to learn representations by contrasting positive and negative pairs. It has been investigated in several fields of machine learning, including computer vision [5, 15, 21], natural language processing [12, 13, 19], and recommender systems [44, 46, 51]. A few recent work showed promising results of applying contrastive learning to conditional text generation, by generating adversarial examples [23], or finding hard negatives with pretrained language models [3, 48]. Our work differs in that we study contrastive learning for condi- tional text generation in a cross-modal setting, where we proposed a novel contrastive framework for generating personalized multi- modal explanations. 7 CONCLUSION In this paper, to generate explanations with rich information for recommendations, we introduce a new task, namely personalized showcases, and collect a large-scale dataset Gest from Google Local for the task. We design a personalized cross-modal contrastive learning framework to learn visual and textual explanations from user reviews. Experimental results show that showcases provide more informative and diverse explanations compared to previous text-only explanations. As future work, one promising direction is to develop an end-to-end framework for generating both visual and textual explanations. Besides, visual grounding on multiple images is still challenging for showcases. Another interesting setting is to address cold-start users or reviews written without images. We hope our dataset and framework would benefit the community for future research on multi-modalities and recommendations. A DATA CONSTRUCTION Our dataset is constructed from Google Local (i.e., maps) using a breadth-first-search algorithm with memorization. After collect- ing the review data, we filtered out reviews of length less than 5 words, which are less likely to provide useful information; we also removed reviews (2.13%) containing more than 10 images. The details of Gest-s1 construction for personalized image selection are as follows: We remove users with only one review for building a personalized dataset, then filter out reviews whose image urls are expired. After pre-processing, statistics for the personalized show- case dataset are shown in Table 1, where the number of images per business is 35.63 on average. We then randomly split the dataset by users, with 95,270/11,908/11,908 users for train/val/test. B VISUAL DIVERSITY DEFINITION We define the visual diversities in three levels as below: • Intra-Business Div: Measure the average diversity for im- age pairs at a business-level, where P1 (𝑏) means all the possible image pairs for business 𝑏. 𝑍1 is the valid counts7 of dis-similarity calculations (same as below): ∑︁ ∑︁ 𝑏 ∈𝐵 𝑚,𝑛 ∈ P (𝑏) dis(𝑖𝑏 𝑚, 𝑖𝑏 𝑛) 𝑍1 . (14) • Inter-User Div: Measure the average diversity for image pairs from different users for the same business, where P2 (𝑏) means all possible image pairs for business 𝑏 that come from different users: ∑︁ ∑︁ 𝑏 ∈𝐵 𝑚,𝑛 ∈ P2 (𝑏) dis(𝑖𝑏 𝑚, 𝑖𝑏 𝑛) 𝑍2 . (15) • Intra-User Div: Measure the average diversity in (business, user)-level, where P3 (𝑢, 𝑏) means all possible image pairs from user 𝑢 to business 𝑏: ∑︁ ∑︁ ∑︁ 𝑏 ∈𝐵 𝑢 ∈𝑈 𝑚,𝑛 ∈ P3 (𝑢,𝑏) dis(𝑖𝑏 𝑚, 𝑖𝑏 𝑛) 𝑍3 . (16) REFERENCES [1] Ashutosh Baheti, Alan Ritter, Jiwei Li, and William B. Dolan. 2018. Generating More Interesting Responses in Neural Conversation Models with Distributional Constraints. In EMNLP. 7When image set size is not more than 1, the dis-similarity calculation is invalid. [2] Jinze Bai, Chang Zhou, Junshuai Song, Xiaoru Qu, Weiting An, Zhao Li, and Jun Gao. 2019. Personalized Bundle List Recommendation. The World Wide Web Conference (2019). [3] Hengyi Cai, Hongshen Chen, Yonghao Song, Zhuoye Ding, Yongjun Bao, Weipeng Yan, and Xiaofang Zhao. 2020. Group-wise contrastive learning for neural dia- logue generation. arXiv preprint arXiv:2009.07543 (2020). [4] Jun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed Elhoseiny. 2021. VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning. [5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In Interna- tional conference on machine learning. PMLR, 1597–1607. [6] Zhihong Chen, Yan Song, Tsung-Hui Chang, and Xiang Wan. 2020. Generating Ra- diology Reports via Memory-driven Transformer. arXiv preprint arXiv:2010.16056 (2020). [7] Zhongxia Chen, Xiting Wang, Xing Xie, Tong Wu, Guoqing Bu, Yining Wang, and Enhong Chen. 2019. Co-Attentive Multi-Task Learning for Explainable Recommendation. In IJCAI. [8] Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reli- able optimization and evaluation of machine translation systems. In Proceedings of the sixth workshop on statistical machine translation. 85–91. [9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL. [10] George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proceedings of the second international conference on Human Language Technology Research. 138–145. [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi- aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ArXiv abs/2010.11929 (2021). [12] Hongchao Fang, Sicheng Wang, Meng Zhou, Jiayuan Ding, and Pengtao Xie. 2020. Cert: Contrastive self-supervised learning for language understanding. arXiv preprint arXiv:2005.12766 (2020). [13] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. arXiv preprint arXiv:2104.08821 (2021). [14] Michael Gutmann and Aapo Hyvärinen. 2010. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. JMLR Workshop and Conference Proceedings, 297–304. [15] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momen- tum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9729–9738. [16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770–778. [17] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. CLIPScore: A Reference-free Evaluation Metric for Image Captioning. arXiv preprint arXiv:2104.08718 (2021). [18] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751 (2019). [19] Jiaji Huang, Yi Li, Wei Ping, and Liang Huang. 2018. Large margin neural language model. arXiv preprint arXiv:1808.08987 (2018). [20] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. 2020. Pixel-bert: Aligning image pixels with text by deep multi-modal transformers. arXiv preprint arXiv:2004.00849 (2020). [21] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020. Supervised contrastive learning. arXiv preprint arXiv:2004.11362 (2020). [22] Alex Kulesza and Ben Taskar. 2012. Determinantal Point Processes for Machine Learning. Found. Trends Mach. Learn. 5 (2012), 123–286. [23] Seanie Lee, Dong Bok Lee, and Sung Ju Hwang. 2020. Contrastive Learning with Adversarial Perturbations for Conditional Text Generation. arXiv preprint arXiv:2012.07280 (2020). [24] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2015. A diversity-promoting objective function for neural conversation models. arXiv preprint arXiv:1510.03055 (2015). An Yan, Zhankui He, Jiacheng Li, Tianyang Zhang, Julian McAuley [29] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. 2015. Image-based recommendations on styles and substitutes. In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval. 43–52. [30] Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve restricted boltzmann machines. In Icml. [31] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 188–197. [32] Jianmo Ni and Julian McAuley. 2018. Personalized Review Generation By Ex- panding Phrases and Attending on Aspect-Aware Representations. In ACL. [33] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018). [34] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 311–318. [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020 (2021). [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. In ICML. [37] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners. [38] Ehud Reiter. 2018. A structured review of the validity of BLEU. Computational Linguistics 44, 3 (2018), 393–401. [39] Thibault Sellam, Dipanjan Das, and Ankur P Parikh. 2020. BLEURT: Learning robust metrics for text generation. arXiv preprint arXiv:2004.04696 (2020). [40] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai- Wei Chang, Zhewei Yao, and Kurt Keutzer. 2021. How Much Can CLIP Benefit Vision-and-Language Tasks? arXiv preprint arXiv:2107.06383 (2021). [41] Hao Tan and Mohit Bansal. 2019. Lxmert: Learning cross-modality encoder representations from transformers. arXiv preprint arXiv:1908.07490 (2019). [42] Quoc-Tuan Truong and Hady Lauw. 2019. Multimodal review generation for recommender systems. In The World Wide Web Conference. 1864–1874. [43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems. 5998–6008. [44] Yinwei Wei, Xiang Wang, Qi Li, Liqiang Nie, Yan Li, Xuanping Li, and Tat-Seng Chua. 2021. Contrastive learning for cold-start recommendation. In Proceedings of the 29th ACM International Conference on Multimedia. 5382–5390. [45] Mark Wilhelm, Ajith Ramanathan, Alexander Bonomo, Sagar Jain, Ed H. Chi, and Jennifer Gillenwater. 2018. Practical Diversified Recommendations on YouTube with Determinantal Point Processes. Proceedings of the 27th ACM International Conference on Information and Knowledge Management (2018). [46] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Bolin Ding, and Bin Cui. 2020. Contrastive Learning for Sequential Recommendation. arXiv preprint arXiv:2010.14395 (2020). [47] Ke Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. In ICML. [48] An Yan, Zexue He, Xing Lu, Jiang Du, Eric Chang, Amilcare Gentili, Julian McAuley, and Chun-Nan Hsu. 2021. Weakly Supervised Contrastive Learning for Chest X-Ray Report Generation. arXiv preprint arXiv:2109.12242 (2021). [49] Hongyu Zang and Xiaojun Wan. 2017. Towards Automatic Generation of Product Reviews from Aspect-Sentiment Scores. In INLG. [50] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 (2019). [51] Chang Zhou, Jianxin Ma, Jianwei Zhang, Jingren Zhou, and Hongxia Yang. 2021. Contrastive learning for debiased candidate generation in large-scale recom- mender systems. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 3985–3995. [25] Lei Li, Yongfeng Zhang, and Li Chen. 2021. Personalized Transformer for Ex- [52] M. Zhou, Mirella Lapata, Furu Wei, Li Dong, Shaohan Huang, and Ke Xu. 2017. plainable Recommendation. In ACL/IJCNLP. [26] Piji Li, Zihao Wang, Lidong Bing, and Wai Lam. 2019. Persona-Aware Tips Generation? The World Wide Web Conference (2019). [27] Piji Li, Zihao Wang, Zhaochun Ren, Lidong Bing, and Wai Lam. 2017. Neu- ral Rating Regression with Abstractive Tips Generation for Recommendation. Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (2017). [28] Ilya Loshchilov and Frank Hutter. 2017. Fixing Weight Decay Regularization in Adam. ArXiv abs/1711.05101 (2017). Learning to Generate Product Reviews from Attributes. In EACL. [53] Wanrong Zhu, Xin Eric Wang, An Yan, Miguel Eckstein, and William Yang Wang. 2021. ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural Language Generation. arXiv preprint arXiv:2106.05970 (2021)."
463,Multivariate trace estimation in constant quantum depth,"[{'href': 'http://arxiv.org/abs/2206.15405v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2206.15405v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-06-30 16:44:58,"Watch and Match: Supercharging Imitation with Regularized Optimal Transport Siddhant Haldar1 Vaibhav Mathur Denis Yarats Lerrel Pinto New York University rot-robot.github.io Abstract: Imitation learning holds tremendous promise in learning policies efﬁciently for complex decision making problems. Current state-of-the-art algorithms often use inverse reinforcement learning (IRL), where given a set of expert demonstrations, an agent alternatively infers a reward function and the associated optimal policy. However, such IRL approaches often require substantial online interactions for complex control problems. In this work, we present Regularized Optimal Transport (ROT), a new imitation learning algorithm that builds on recent advances in optimal transport based trajectory-matching. Our key technical insight is that adaptively combining trajectory-matching rewards with behavior cloning can signiﬁcantly accelerate imitation even with only a few demonstrations. Our experiments on 20 visual control tasks across the DeepMind Control Suite, the OpenAI Robotics Suite, and the Meta-World Benchmark demonstrate an average of 7.8× faster imitation to reach 90% of expert performance compared to prior state-of-the-art methods. On real-world robotic manipulation, with just one demonstration and an hour of online training, ROT achieves an average success rate of 90.1% across 14 tasks. Keywords: Imitation Learning, Manipulation, Robotics 2 2 0 2 n u J 0 3 ] O R . s c [ 1 v 9 6 4 5 1 . 6 0 2 2 : v i X r a Figure 1: (Top) Regularized Optimal Transport (ROT) is a new imitation learning algorithm that adaptively combines ofﬂine behavior cloning with online trajectory-matching based rewards. This enables signiﬁcantly faster imitation across a variety of simulated and real robotics tasks, while being compatible with high-dimensional visual observation. (Bottom) On our xArm robot, ROT can learn visual policies with only a single human demonstration and under an hour of online training. 1Correspondence to: sh6474@nyu.edu Opening a box Hanging a tote bag Placing peg in a box Pouring almonds t n e g A t r e p x E πBC πROT OT Rewards Adaptive Regularization OT Computation Environments Environment Interactions 1 Introduction Imitation Learning (IL) [1, 2, 3] has a rich history that can be categorized across two broad paradigms, Behavior Cloning (BC) [1] and Inverse Reinforcement Learning (IRL) [4]. BC uses supervised learning to obtain a policy that maximizes the likelihood of taking the demonstrated action given an observation in the demonstration. While this allows for training without online interactions, it suffers from distributional mismatch during online rollouts [5]. IRL, on the other hand, infers the underlying reward function from the demonstrated trajectories before employing RL to optimize a policy through online environment rollouts. This results in a policy that can robustly solve demonstrated tasks even in the absence of task-speciﬁc rewards [6, 7]. Although powerful, IRL methods suffer from a signiﬁcant drawback – they require numerous expensive online interactions with the environment. There are three reasons for this: (a) the inferred reward function is often highly non-stationary, which compromises the learning of the associated behavior policy [7]; (b) even when the rewards are stationary, policy learning still requires effective exploration to maximize rewards [8]; and (c) when strong priors such as pretraining with BC are applied to accelerate policy learning, ensuing updates to the policy cause a distribution shift that destabilizes training [9, 10]. Combined, these issues manifest themselves on empirical benchmarks, where IRL methods have poor efﬁciency compared to vanilla RL methods on hard control tasks [11]. In this work, we present Regularized Optimal Transport (ROT) for imitation learning, a new method that is conceptually simple, compatible with high-dimensional observations, and requires minimal additional hyperparameters compared to standard IRL approaches. In order to address the challenge of reward non-stationarity in IRL, ROT builds upon recent advances in using Optimal Transport (OT) [12, 13, 11] for reward computation that use non-parametric trajectory-matching functions. To alleviate the challenge of exploration, we pretrain the IRL behavior policy using BC on the expert demonstrations. This reduces the need for our imitation agent to explore from scratch. However, even with OT-based reward computation and pretrained policies, we only obtain marginal gains in empirical performance. The reason for this is that the high-variance of IRL policy gradi- ents [14, 15] often wipe away the progress made by the ofﬂine BC pretraining. This phenomenon has been observed in both online RL [16] and ofﬂine RL [9] methods. Inspired by solutions presented in these works, we stabilize the online learning process by regularizing the IRL policy to stay close to the pretrained BC policy. To enable this, we develop a new adaptive weighing scheme called soft Q-ﬁltering that automatically sets the regularization – prioritizing staying close to the BC policy in the beginning of training and prioritizing exploration later on. In contrast to prior policy regularization schemes [16, 17], soft Q-ﬁltering does not require hand-speciﬁcation of decay schedules. To demonstrate the effectiveness of ROT, we run extensive experiments on 20 simulated tasks across DM Control [18], OpenAI Robotics [19], and Meta-world [20], and 14 robotic manipulation tasks on an xArm (see Fig. 1). Our main ﬁndings are summarized below. 1. ROT outperforms prior state-of-the-art imitation methods, reaching 90% of expert performance 7.8× faster than our strongest baselines on simulated visual control benchmarks. 2. On real-world tasks, with a single human demonstration and an hour of training, ROT achieves an average success rate of 90.1% with randomized robot initialization and image observations. This is signiﬁcantly higher than behavior cloning (36.1%) and adversarial IRL (14.6%). 3. ROT exceeds the performance of state-of-the-art RL trained with rewards, while coming close to methods that augment RL with demonstrations (Section 5.5 & Appendix H.3). Unlike standard RL methods, ROT does not require hand-speciﬁcation of the reward function. 4. Ablation studies demonstrate the importance of every component in ROT, particularly the role that soft Q-ﬁltering plays in stabilizing training and the need for OT-based rewards during online learning (Section 5.3 & Appendix H.4). Open-source code and demonstration data will be publicly released on our project website. Videos of our trained policies can be seen here: rot-robot.github.io/. 2 Figure 2: (a) Given a single demonstration to avoid the grey obstacle and reach the goal location, BC is unable to solve the task. (b) Finetuning from this BC policy with OT-based reward also fails to solve the task. (c) ROT, with adaptive regularization of OT-based IRL with BC successfully solves the task. (d) Even when the ROT agent is initialized randomly, it is able to solve the task. 2 Background Before describing our method in detail, we provide a brief background to imitation learning with optimal transport, which serves as the backbone of our method. Formalism related to RL follows the convention in prior work [8, 11] and is described in Appendix A. t=1}N Imitation Learning with Optimal Transport (OT) The goal of imitation learning is to learn a behavior policy πb given access to either the expert policy πe or trajectories derived from the expert policy T e. While there are a multitude of settings with differing levels of access to the expert [21], our work operates in the setting where the agent only has access to observation-based trajectories, i.e. T e ≡ {(ot, at)T n=1. Here N and T denotes the number of trajectory rollouts and episode timesteps respectively. Inverse Reinforcement Learning (IRL) [4, 22] tackles the IL problem by inferring the reward function re based on expert trajectories T e. Then given the inferred reward re, policy optimization is used to derive the behavior policy πb. To compute re, a new line of OT-based approaches for IL [12, 13, 11] have been proposed. Intuitively, the closeness between expert trajectories T e and behavior trajectories T b can be computed by measuring the optimal transport of probability mass from T b → T e. Thus, given a cost matrix Ct,t(cid:48) = c(ob t(cid:48)) and the optimal alignment µ∗ between a behavior trajectory ob and and expert trajectory oe, a reward signal for each observation can be computed using the equation: t , oe rOT (ob t ) = − T (cid:88) t(cid:48)=1 Ct,t(cid:48) µ∗ t,t(cid:48) (1) A detailed account of the OT formulation has been provided in Appendix A. Actor-Critic based reward maximization Given rewards obtained through OT computation, efﬁ- cient maximization of the reward can be achieved through off-policy learning [7]. In this work, we use Deep Deterministic Policy Gradient (DDPG) [23] as our base RL optimizer which is an actor-critic algorithm that concurrently learns a deterministic policy πφ and a Q-function Qθ. However, instead of minimizing a one step Bellman residual in vanilla DDPG, we use the recent n-step version of DDPG from Yarats et al. [8] that achieves high performance on visual control problems. 3 Challenges in Online Finetuning from a Pretrained Policy In this section, we study the challenges with ﬁnetuning a pretrained policy with online interactions in the environment. Fig. 2 illustrates a task where an agent is supposed to navigate the environment from the top left to the bottom right, while dodging obstacles in between. The agent has access to a single expert demonstration, which is used to learn a BC policy for the task. Fig. 2 (a) shows that this BC policy, though close to the expert demonstration, performs suboptimally due to accumulating errors on out-of-distribution states during online rollouts [5]. Further, Fig. 2 (b) uses this BC policy 3 Expert trajectory BC trajectory Start location Goal location (a) Task: Particle Reach (b) IRL Finetune w/o Reg. (c) ROT (d) ROT + random init. 100k s p e t s e m i t 0 Expert trajectory BC trajectory Start location Goal location as an initialization and naively ﬁnetunes it with OT rewards (described in Section 2). Such naive ﬁnetuning of a pretrained policy (or actor) with an untrained critic in an actor-critic framework exhibits a forgetting behavior in the actor, resulting in performance degradation as compared to the pretrained policy. This phenomenon has also been reported by Nair et al. [9] and we provide a detailed discussion in Appendix B. In this paper, we propose ROT which addresses this issue by adaptively keeping the policy close to the behavior data during the initial phase of ﬁnetuning and reduces this dependence over time. Fig. 2 (c) demonstrates the performance of our approach on such ﬁnetuning. It can be clearly seen that even though the BC policy is suboptimal, our proposed adaptive regularization scheme quickly improves and solves the task by driving it closer to the expert demonstration. In Fig. 2 (d), we demonstrate that even if the agent was initialized at points outside the expert trajectory, the agent is still able to learn quickly and complete the task. This generalization to starting states would not be possible with regular BC. 4 Regularized Optimal Transport A fundamental challenge in imitation learning is to balance the ability to mimic demonstrated actions along with the ability to recover from states outside the distribution of demonstrated states. Behavior Cloning (BC) specializes in mimicking demonstrated actions through supervised learning, while Inverse Reinforcement Learning (IRL) specializes in obtaining policies that can recover from arbitrary states. Regularized Optimal Transport (ROT) combines the best of both worlds by adaptively combining the two objectives. This is done in two phases. In the ﬁrst phase, a randomly initialized policy is trained using the BC objective on expert demonstrated data. This ‘BC-pretrained’ policy then serves as an initialization for the second phase. In the second phase, the policy is allowed access to the environment where it can train using an IRL objective. To accelerate the IRL training, the BC loss is added to the objective with an adaptive weight. Details of each component are described below, with additional algorithmic details in Appendix C. 4.1 Phase 1: BC Pretraining BC corresponds to solving the maximum likelihood problem shown in Eq. 2. Here T e refers to expert demonstrations. When parameterized by a normal distribution with ﬁxed variance, the objective can be framed as a regression problem where, given inputs se, πBC needs to output ae. LBC = E(se,ae)∼T e (cid:107)ae − πBC(se)(cid:107)2 (2) After training, it enables πBC to mimic the actions corresponding to the observations seen in the demonstrations. However, during rollouts in an environment, small errors in action prediction can lead to the agent visiting states not seen in the demonstrations [5]. This distributional mismatch often causes πBC to fail on empirical benchmarks [16, 11] (see Fig. 2 (a) in Sec. 3). 4.2 Phase 2: Online Finetuning with IRL Given a pretrained πBC model, we now begin online ‘ﬁnetuning’ of the policy πb ≡ πROT in the environment. Since we are operating without explicit task rewards, we use rewards obtained through OT-based trajectory matching, which is described in Section 2. These OT-based rewards rOT enable the use of standard RL optimizers to maximize cumulative reward from πb ≡ πROT . In this work we use n-step DDPG [23], a deterministic actor-critic based method that provides high-performance in continuous control [8]. Finetuning with Regularization πBC is susceptible to distribution shift due to accumulation of errors during online rollouts [5] and directly ﬁnetuning πBC also leads to subpar performance (refer to Fig. 2 in Sec. 3). To address this, we build upon prior work in guided RL [16] and ofﬂine RL [9], and regularize the training of πROT by combining it with a BC loss as seen in Eq. 3. πROT = argmax π (cid:2)(1 − λ(π)))E(s,a)∼Dβ [Q(s, a)] − αλ(π)E(se,ae)∼T e (cid:107)ae − πBC(se)(cid:107)2(cid:3) (3) 4 Here, Q(s, a) represents the Q-value from the critic used in actor-critic policy optimization. α is a ﬁxed weight, while λ(π) is a policy-dependent adaptive weight that controls the contributions of the two loss terms. Dβ refers to the replay buffer for online rollouts. Adaptive Regularization with Soft Q-ﬁltering While prior work [16, 17] use hand-tuned sched- ules for λ(π), we propose a new adaptive scheme that removes the need for tuning. This is done by comparing the performance of the current policy πROT and the pretrained policy πBC on a batch of data sampled from an expert replay buffer De. More precisely, given a behavior policy πBC(s), the current policy πROT (s), the Q-function Q(s, a) and the replay buffer De, we set λ as: λ(πROT ) = E(s,·)∼De (4) The strength of the BC regularization hence depends on the performance of the current policy with respect to the behavior policy. This ﬁltering strategy is inspired by Nair et al. [24], where instead of a binary hard assignment we use a soft continuous weight. Experimental comparisons with hand-tuned decay strategies are presented in Section 5.3. (cid:2)1Q(s,πBC (s))>Q(s,πROT (s)) (cid:3) Considerations for image-based observations Since we are interested in using ROT with high- dimensional visual observations, additional machinery is required to ensure compatibility. Following prior work in image-based RL and imitation [8, 11], we perform data augmentations on visual observations and then feed it into a CNN encoder. Similar to Cohen et al. [11], we use a target encoder with Polyak averaging to obtain representations for OT reward computation. This is necessary to reduce the non-stationarity caused by learning the encoder alongside the ROT imitation process. Further implementation details and the training procedure can be found in Appendix C. 5 Experiments Our experiments are designed to answer the following questions: (a) How efﬁcient is ROT for imitation learning? (b) How does ROT perform on real-world tasks? (c) How important is the choice of IRL method in ROT? (d) Does soft Q-ﬁltering improve imitation? (e) How does ROT compare to standard reward-based RL? Additional results and analysis have been provided in Appendix H. Simulated tasks We experiment with 10 tasks from the DeepMind Control suite [18, 25], 3 tasks from the OpenAI Robotics suite [26], and 7 tasks from the Meta-world suite [27]. For DeepMind Control tasks, we train expert policies using DrQ-v2 [8] and collect 10 demonstrations for each task using this policy. For OpenAI Robotics tasks, we train a state-based DrQ-v2 with hindsight experience replay [28] and collect 50 demonstrations for each task. For Meta-world tasks, we use a single hard-coded expert demonstration from their open-source implementation [27]. Full environment details can be found in Appendix D and details about the variations in demonstrations and initialization conditions can be found in Appendix E. Robot tasks Our real world setup for each of the 14 manipulation tasks can be seen in Fig. 4. We use an Ufactory xArm 7 robot with a xArm Gripper as the robot platform for our real world experiments. However, our method is agnostic to the speciﬁc robot hardware. The observations are RGB images from a ﬁxed camera. In this setup, we only use a single expert demonstration collected by a human operator with a joystick and limit the online training to a ﬁxed period of 1 hour. Descriptions of each task and the evaluation procedure is in Appendix F. Primary baselines We compare ROT with baselines against several prominent imitation learning methods. While a full description of our baselines are in Appendix G, a brief description of the two strongest ones are as follows: 1. Adversarial IRL (DAC): Discriminator Actor Critic [7] is a state-of-the-art adversarial imitation learning method [6, 29, 7]. DAC outperforms prior work such as GAIL [6] and AIRL [30], and thus it serves as our primary adversarial imitation baseline. 2. Trajectory-matching IRL (OT): Sinkhorn Imitation Learning [12, 13] is a state-of-the-art trajectory-matching imitation learning method [31] that approximates OT matching through the Sinkhorn Knopp algorithm [32, 33]. Since ROT is derived from similar OT-based foundations, we use SIL as our primary state-matching imitation baseline. 5 Expert BC OT DAC ROT (Ours) Figure 3: Pixel-based continuous control learning on 9 selected environments. Shaded region represents ±1 standard deviation across 5 seeds. We notice that ROT is signiﬁcantly more sample efﬁcient compared to prior work. 5.1 How efﬁcient is ROT for imitation learning? Performance of ROT for image-based imitation is depicted on select environments in Fig. 3. On all but one task, ROT trains signiﬁcantly faster than prior work. To reach 90% of expert performance, ROT is on average 8.7× faster on DeepMind Control tasks, 2.1× faster on OpenAI Robotics tasks, and 8.9× faster on Meta-world tasks. We also ﬁnd that the improvements of ROT are most apparent on the harder tasks, which are in rightmost column of Fig. 3. Appendix H.1 shows results on all 20 simulated tasks, along with experiments that exhibit similar improvements in state-based settings. 5.2 How does ROT perform on real-world tasks? We devise a set of 14 manipulation tasks on our xArm robot to compare the performance of ROT with BC and our strongest baseline RDAC, an adversarial IRL method that combines DAC [7] with our pretraining and regularization scheme. The BC policy is trained using supervised learning on a single expert demonstration collected by a human operator. ROT and RDAC ﬁnetune the pretrained BC policy through 1 hour of online training, which amounts to ∼ 6k environment steps. Since there is just one demonstration, our tasks are designed to have random initializations but ﬁxed goals. Note that a single demonstration only demonstrates solving the tasks from one initial condition. Evaluation results across 20 different initial conditions can be seen in Fig. 4. We observe that ROT has an average success rate of 90.1% over 20 evaluation trajectories across all tasks as compared to 36.1% for BC and 14.6% for RDAC. The poor performance of BC can be attributed to distributional mismatch due to accumulation of error in online rollouts and different initial conditions. The poor performance of RDAC can be attributed to slow learning during the initial phase of training. More detailed evaluations of RDAC on simulated environments is present in Sec. 5.4. 6 d r a w e r _ e d o s i p e 1000 800 600 400 200 0 e t a r s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 1.0 e t a r s s e c c u s 0.8 0.6 0.4 0.2 0.0 dmc_cheetah_run dmc_hopper_hop dmc_walker_run 300 200 100 d r a w e r _ e d o s i p e 0 800 600 400 200 d r a w e r _ e d o s i p e 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 fetch_reach fetch_push fetch_pick_and_place e t a r s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 e t a r s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame metaworld_drawer_close metaworld_hammer metaworld_door_open e t a r s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 e t a r s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 Figure 4: (Top) ROT is evaluated on a set of 14 robotic manipulation tasks. (Bottom) Success rates for each task is computed by running 20 trajectories from varying initial conditions on the robot. BC RDAC ROT (Ours) 5.3 Does soft Q-ﬁltering improve imitation? To understand the importance of soft Q-ﬁltering, we compare ROT against two variants of our proposed regular- ization scheme: (a) A tuned ﬁxed BC regularization weight (ignoring λ(π) in Eq. 3); (b) A carefully designed linear-decay schedule for λ(π), where it varies from 1.0 to 0.0 in the ﬁrst 20k environment steps [16]. As demon- strated in Fig. 5 (and Appendix H.2), ROT is on par and in some cases ex- ceeds the efﬁciency of a hand-tuned decay schedule, while not having to hand-tune its regularization weights. We hypothesize this improvement is primarily due to the better stability of adaptive weighing as seen in the signiﬁcantly smaller standard deviation on the Meta-world tasks. Figure 5: Effect of various BC regularization schemes com- pared with our adaptive soft-Q ﬁltering regularization. Finetune with ﬁxed weight Finetune with ﬁxed ROT (Ours) schedule 5.4 How important is the choice of IRL method in ROT? In ROT, we build on OT-based IRL instead of adversarial IRL. This is because adversarial IRL methods require iterative reward learning, which produces a highly non-stationary reward function for policy optimization. In Fig. 6, we compare ROT with adversarial IRL methods that use our pretraining and adaptive BC regularization technique (RDAC). We ﬁnd that our soft Q-ﬁltering method does improve prior state-of-the-art adversarial IRL (RDAC vs. DAC in Fig. 6). However, our OT-based approach (ROT) is more stable and on average leads to more efﬁcient learning. 5.5 How does ROT compare to standard reward-based RL? We compare the performance of ROT against DrQ-v2 [8], a state-of-the-art algorithm for image-based RL. As opposed to the reward-free setting ROT operates in, DrQ-v2 has access to environments rewards. The results in Fig. 6 show that ROT handily outperforms DrQ-v2. This clearly demonstrates 7 Close a door Hang a hanger Erasing a board Reach Hanging a mug Hanging a tote bag Turn a knob Stacking cups Pressing a switch Peg in a box (Easy) Peg in a box (Med) Peg in a box (Hard) Opening a box Pouring almonds dmc_hopper_hop metaworld_hammer 300 200 100 d r a w e r e d o s p e i 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 1.0 0.8 0.6 0.4 0.2 e t a r s s e c c u s 0.0 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 Expert BC DAC OT DrQ-v2 (RL) RDAC ROT (Ours) Figure 6: Ablation analysis on the choice of base IRL method. We ﬁnd that although adversarial methods beneﬁt from regularized BC, the gains seen are smaller compared to ROT. Here, we also see that ROT can outperform plain RL that requires explicit task-rewards. the usefulness of imitation learning in domains where expert demonstrations are available over reward-based RL. We also compare against a demo-assisted variant of DrQ-v2 agent using the same pretraining and regularization scheme as ROT (refer to Appendix H.3). Interestingly, we ﬁnd that our soft Q-ﬁltering based regularization can accelerate learning of RL with task rewards, which can be seen in the high performance of the demo-assisted variant of DrQ-v2. 6 Related Work Imitation Learning (IL) IL [34] refers to the setting where agents learn from demonstrations without access to environment rewards. IL can be broadly categorized into Behavior Cloning (BC) [1, 21, 35, 36] and Inverse Reinforcement Learning (IRL) [4, 22]. BC solely learns from ofﬂine demonstrations but suffers on out-of-distributions samples [5] whereas IRL focuses on learning a robust reward function through online interactions but suffers from sample inefﬁciency [7]. Deep IRL methods can be further divided into two categories: (1) adversarial learning [37] based methods, and (2) state-matching [38, 39] based methods. GAIL [6] is an adversarial learning based formulation inspired by maximum entropy IRL [40] and GANs [37]. There has been a signiﬁcant body of work built up on GAIL proposing alternative losses [30, 41, 29], and enhancing its sample efﬁciency by porting it to an off-policy setting [7]. There have also been visual extensions of these adversarial learning approaches [42, 43, 44, 11]. However, although adversarial methods produce competent policies, they are inefﬁcient due to the non-stationarity associated with iterative reward inference [11]. Optimal Transport (OT) OT [38, 39] is a tool for comparing probability measures while including the geometry of the space. In the context of IL, OT computes an alignment between a set of agent and expert observations using distance metrics such as Sinkhorn [33], Gromov-Wasserstein [45], GDTW [46], CO-OT [47] and Soft-DTW [48]. For many of these distance measures, there is an associated IL algorithm, with SIL [12] using Sinkhorn, PWIL [13] using greedy Wasserstein, GDTW-IL [46] using GDTW, and GWIL [49] using Gromov-Wasserstein. Recent work from Cohen et al. [11] demonstrates that the Sinkhorn distance [12] produces the most efﬁcient learning among the discussed metrics. They further show that SIL is compatible with high-dimensional visual observations and encoded representations. Inspired by this, ROT adopts the Sinkhorn metric for its OT reward computation, and improves upon SIL through adaptive behavior regularization. Behavior Regularized Control Behavior regularization is a widely used technique in ofﬂine RL [50] where explicit constraints are added to the policy improvement update to avoid bootstrapping on out-of-distribution actions [51, 52, 53, 54, 55, 56]. In an online setting with access to environment rewards, prior work [16, 10] has shown that behavior regularization can be used to boost sample efﬁciency by ﬁnetuning a pretrained policy via online interactions. For instance, Jena et al. [17] demonstrates the effectiveness of behavior regularization to enhance sample efﬁciency in the context of adversarial IL. ROT builds upon this idea by extending to visual observations, OT-based IL, and adaptive regularization, which leads to improved performance (see Appendix H.4). We also note that the idea of using adaptive regularization has been previously explored in RL [24]. However, ROT uses a soft, continuous adaptive scheme, which on initial experiments provided signiﬁcantly faster learning compared to hard assignments. 8 1000 d r a w e r e d o s i p e 800 600 400 200 0 dmc_walker_run fetch_pick_and_place metaworld_hammer e t a r s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 e t a r s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.00 0.25 0.50 0.75 frame 1.00 1.25 1.50 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 7 Conclusion and Limitations In this work, we have proposed a new imitation learning algorithm, ROT, that demonstrates improved performance compared to prior state-of-the-art work on a variety of simulated and robotic domains. However, we recognize a few limitations in this work: (a) Since our OT-based approach aligns agents with demonstrations without task-speciﬁc rewards, it relies on the demonstrator being an ‘expert’. Extending ROT to suboptimal, noisy and multimodal demonstrations would be an exciting problem to tackle. (b) Performing BC pretraining and BC-based regularization requires access to expert actions, which may not be present in some real-world scenarios particularly when learning from humans. Recent work on using inverse models to infer actions given observational data could alleviate this challenge [57]. (c) On robotic tasks such as Peg in box (hard) and Pressing a switch from Fig. 4, we ﬁnd that ROT’s performance drops substantially compared to other tasks. This might be due to the lack of visual features corresponding to the task success. For example, in the ‘Peg’ task, it is visually difﬁcult to discriminate if the peg is in the box or behind the box. Similarly for the ‘Switch’ task, it is difﬁcult to discern if the button was pressed or not. This limitation can be addressed by integrating more sensory modalities such as additional cameras, and tactile sensors in the observation space. Acknowledgments We thank Ben Evans, Anthony Chen, Ulyana Piterbarg and Abitha Thankaraj for valuable feedback and discussions. This work was supported by grants from Honda, Amazon, and ONR awards N00014-21-1-2404 and N00014-21-1-2758. References [1] D. Pomerleau. An autonomous land vehicle in a neural network. Advances in Neural Information Processing Systems, 1, 1998. [2] O. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, et al. Learning dexterous in-hand manipulation. The Interna- tional Journal of Robotics Research, 39(1):3–20, 2020. [3] P. N. Kolm and G. Ritter. Modern perspectives on reinforcement learning in ﬁnance. Modern Perspectives on Reinforcement Learning in Finance (September 6, 2019). The Journal of Machine Learning in Finance, 1(1), 2020. [4] A. Y. Ng, S. J. Russell, et al. Algorithms for inverse reinforcement learning. In Icml, volume 1, page 2, 2000. [5] S. Ross, G. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artiﬁ- cial intelligence and statistics, pages 627–635. JMLR Workshop and Conference Proceedings, 2011. [6] J. Ho and S. Ermon. Generative adversarial imitation learning. Advances in neural information processing systems, 29, 2016. [7] I. Kostrikov, K. K. Agrawal, D. Dwibedi, S. Levine, and J. Tompson. Discriminator-actor-critic: Addressing sample inefﬁciency and reward bias in adversarial imitation learning. arXiv preprint arXiv:1809.02925, 2018. [8] D. Yarats, R. Fergus, A. Lazaric, and L. Pinto. Mastering visual continuous control: Improved data-augmented reinforcement learning. arXiv preprint arXiv:2107.09645, 2021. [9] A. Nair, A. Gupta, M. Dalal, and S. Levine. Awac: Accelerating online reinforcement learning with ofﬂine datasets. arXiv preprint arXiv:2006.09359, 2020. [10] I. Uchendu, T. Xiao, Y. Lu, B. Zhu, M. Yan, J. Simon, M. Bennice, C. Fu, C. Ma, J. Jiao, et al. Jump-start reinforcement learning. arXiv preprint arXiv:2204.02372, 2022. [11] S. Cohen, B. Amos, M. P. Deisenroth, M. Henaff, E. Vinitsky, and D. Yarats. Imitation learning from pixel observations for continuous control, 2022. URL https://openreview.net/ forum?id=JLbXkHkLCG6. 9 [12] G. Papagiannis and Y. Li. arXiv:2008.09167, 2020. Imitation learning with sinkhorn distances. arXiv preprint [13] R. Dadashi, L. Hussenot, M. Geist, and O. Pietquin. Primal wasserstein imitation learning. arXiv preprint arXiv:2006.04678, 2020. [14] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [15] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Deterministic policy gradient algorithms. In International conference on machine learning, pages 387–395. PMLR, 2014. [16] A. Rajeswaran, V. Kumar, A. Gupta, G. Vezzani, J. Schulman, E. Todorov, and S. Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. arXiv preprint arXiv:1709.10087, 2017. [17] R. Jena, C. Liu, and K. Sycara. Augmenting gail with bc for sample efﬁcient imitation learning. arXiv preprint arXiv:2001.07798, 2020. [18] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki, J. Merel, A. Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018. [19] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [20] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on Robot Learning, pages 1094–1100. PMLR, 2020. [21] F. Torabi, G. Warnell, and P. Stone. Recent advances in imitation learning from observation. arXiv preprint arXiv:1905.13566, 2019. [22] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-ﬁrst international conference on Machine learning, page 1, 2004. [23] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015. [24] A. Nair, B. McGrew, M. Andrychowicz, W. Zaremba, and P. Abbeel. Overcoming exploration in reinforcement learning with demonstrations. In 2018 IEEE international conference on robotics and automation (ICRA), pages 6292–6299. IEEE, 2018. [25] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pages 5026–5033. IEEE, 2012. [26] M. Plappert, M. Andrychowicz, A. Ray, B. McGrew, B. Baker, G. Powell, J. Schneider, J. Tobin, M. Chociej, P. Welinder, et al. Multi-goal reinforcement learning: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464, 2018. [27] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on Robot Learning (CoRL), 2019. URL https://arxiv.org/abs/1910.10897. [28] M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin, O. Pieter Abbeel, and W. Zaremba. Hindsight experience replay. Advances in neural information processing systems, 30, 2017. [29] F. Torabi, G. Warnell, and P. Stone. Generative adversarial imitation from observation. arXiv preprint arXiv:1807.06158, 2018. [30] J. Fu, K. Luo, and S. Levine. Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248, 2017. 10 [31] S. K. S. Ghasemipour, R. Zemel, and S. Gu. A divergence minimization perspective on imitation learning methods. In Conference on Robot Learning, pages 1259–1277. PMLR, 2020. [32] R. Sinkhorn and P. Knopp. Concerning nonnegative matrices and doubly stochastic matrices. Paciﬁc Journal of Mathematics, 21(2):343–348, 1967. [33] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26, 2013. [34] A. Hussein, M. M. Gaber, E. Elyan, and C. Jayne. Imitation learning: A survey of learning methods. ACM Computing Surveys (CSUR), 50(2):1–35, 2017. [35] S. P. Arunachalam, S. Silwal, B. Evans, and L. Pinto. Dexterous imitation made easy: A learning- based framework for efﬁcient dexterous manipulation. arXiv preprint arXiv:2203.13251, 2022. [36] J. Pari, N. Muhammad, S. P. Arunachalam, and L. Pinto. The surprising effectiveness of representation learning for visual imitation. arXiv preprint arXiv:2112.01511, 2021. [37] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. [38] C. Villani. Optimal transport: old and new, volume 338. Springer, 2009. [39] G. Peyr´e, M. Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends® in Machine Learning, 11(5-6):355–607, 2019. [40] B. D. Ziebart, A. L. Maas, J. A. Bagnell, A. K. Dey, et al. Maximum entropy inverse reinforce- ment learning. In Aaai, volume 8, pages 1433–1438. Chicago, IL, USA, 2008. [41] H. Xiao, M. Herman, J. Wagner, S. Ziesche, J. Etesami, and T. H. Linh. Wasserstein adversarial imitation learning. arXiv preprint arXiv:1906.08113, 2019. [42] E. Cetin and O. Celiktutan. Domain-robust visual imitation learning with mutual information constraints. arXiv preprint arXiv:2103.05079, 2021. [43] S. Toyer, R. Shah, A. Critch, and S. Russell. The magical benchmark for robust imitation. Advances in Neural Information Processing Systems, 33:18284–18295, 2020. [44] R. Rafailov, T. Yu, A. Rajeswaran, and C. Finn. Visual adversarial imitation learning using variational models. Advances in Neural Information Processing Systems, 34, 2021. [45] G. Peyr´e, M. Cuturi, and J. Solomon. Gromov-wasserstein averaging of kernel and distance matrices. In International Conference on Machine Learning, pages 2664–2672. PMLR, 2016. [46] S. Cohen, G. Luise, A. Terenin, B. Amos, and M. Deisenroth. Aligning time series on incom- parable spaces. In International Conference on Artiﬁcial Intelligence and Statistics, pages 1036–1044. PMLR, 2021. [47] I. Redko, T. Vayer, R. Flamary, and N. Courty. Co-optimal transport. arXiv preprint arXiv:2002.03731, 2020. [48] M. Cuturi and M. Blondel. Soft-dtw: a differentiable loss function for time-series. In Interna- tional conference on machine learning, pages 894–903. PMLR, 2017. [49] A. Fickinger, S. Cohen, S. Russell, and B. Amos. Cross-domain imitation learning via optimal transport. arXiv preprint arXiv:2110.03684, 2021. [50] S. Levine, A. Kumar, G. Tucker, and J. Fu. Ofﬂine reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020. [51] S. Fujimoto and S. S. Gu. A minimalist approach to ofﬂine reinforcement learning. Advances in Neural Information Processing Systems, 34, 2021. [52] Y. Wu, G. Tucker, and O. Nachum. Behavior regularized ofﬂine reinforcement learning. arXiv preprint arXiv:1911.11361, 2019. 11 [53] A. Ajay, A. Kumar, P. Agrawal, S. Levine, and O. Nachum. {OPAL}: Ofﬂine primitive discovery In International Conference on Learning for accelerating ofﬂine reinforcement learning. Representations, 2021. URL https://openreview.net/forum?id=V69LGwJ0lIN. [54] A. Kumar, J. Fu, M. Soh, G. Tucker, and S. Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. Advances in Neural Information Processing Systems, 32, 2019. [55] N. Y. Siegel, J. T. Springenberg, F. Berkenkamp, A. Abdolmaleki, M. Neunert, T. Lampe, R. Hafner, N. Heess, and M. Riedmiller. Keep doing what worked: Behavioral modelling priors for ofﬂine reinforcement learning. arXiv preprint arXiv:2002.08396, 2020. [56] S. Fujimoto, D. Meger, and D. Precup. Off-policy deep reinforcement learning without explo- ration. In International Conference on Machine Learning, pages 2052–2062. PMLR, 2019. [57] I. Radosavovic, X. Wang, L. Pinto, and J. Malik. State-only imitation learning for dexterous manipulation. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 7865–7871. IEEE, 2020. [58] R. Bellman. A markovian decision process. Journal of mathematics and mechanics, pages 679–684, 1957. [59] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018. [60] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep rein- forcement learning. nature, 518(7540):529–533, 2015. [61] A. Zhan, P. Zhao, L. Pinto, P. Abbeel, and M. Laskin. A framework for efﬁcient robotic manipulation. arXiv preprint arXiv:2012.07975, 2020. [62] S. Young, D. Gandhi, S. Tulsiani, A. Gupta, P. Abbeel, and L. Pinto. Visual imitation made easy. arXiv preprint arXiv:2008.04899, 2020. [63] P. A. Knight. The sinkhorn–knopp algorithm: convergence and applications. SIAM Journal on Matrix Analysis and Applications, 30(1):261–275, 2008. 12 A Background Reinforcement Learning (RL) We study RL as a discounted inﬁnite-horizon Markov Decision Process (MDP) [58, 59]. For pixel observations, the agent’s state is approximated as a stack of consecutive RGB frames [60]. The MDP is of the form (O, A, P, R, γ, d0) where O is the observation space, A is the action space, P : O × A → ∆(O) is the transition function that deﬁnes the probability distribution over the next state given the current state and action, R : O × A → R is the reward function, γ is the discount factor and d0 is the initial state distribution. The goal is to ﬁnd a policy π : O → ∆(A) that maximizes the expected discount sum of rewards Eπ[Σ∞ t=0γtR(ot, at)], where o0 ∼ d0, at ∼ π(ot) and ot+1 ∼ P (.|ot, at). Imitation Learning (IL) The goal of imitation learning is to learn a behavior policy πb given access to either the expert policy πe or trajectories derived from the expert policy T e. While there are a multitude of settings with differing levels of access to the expert [21], this work operates in the setting where the agent only has access to observation-based trajectories, i.e. T e ≡ {(ot, at)T n=0. Here N and T denotes the number of trajectory rollouts and episode timesteps respectively. We choose this speciﬁc setting since obtaining observations and actions from expert or near-expert demonstrators is feasible in real-world settings [61, 62] and falls in line with recent work in this area [13, 6, 7]. t=0}N Inverse Reinforcement Learning (IRL) IRL [4, 22] tackles the IL problem by inferring the reward function re based on expert trajectories T e. Then given the inferred reward re, policy optimization is used to derive the behavior policy πb. Prominent algorithms in IRL [7, 6] requires alternating the inference of reward and optimization of policy in an iterative manner, which is practical for restricted model classes [22]. For compatibility with more expressive deep networks, techniques such as adversarial learning [6, 7] or optimal-transport [12, 13, 11] are needed. Adversarial learning based approaches tackle this problem by learning a discriminator that models the gap between the expert trajectories T e and behavior trajectories T b. The behavior policy πb is then optimized to minimize this gap through gap-minimizing rewards re. Such a training procedure is prone to instabilities since re is updated at every iteration and is hence non-stationary for the optimization of πb. Optimal Transport for Imitation Learning (OT) To alleviate the non-stationary reward problem with adversarial IRL frameworks, a new line of OT-based approaches have been recently proposed [12, 13, 11]. Intuitively, the closeness between expert trajectories T e and behavior trajectories T b can be computed by measuring the optimal transport of probability mass from T b → T e. During policy learning, the policy πφ encompasses a feature preprocessor fφ which transforms observations into informative state representations. Some examples of a preprocessor function fφ are an identity function, a mean-variance scaling function and a parametric neural network. In this work, we use a parametric neural network as fφ. Given a cost function c : O × O → R deﬁned in the preprocessor’s output space and an OT objective g, the optimal alignment between an expert trajectory oe and a behavior trajectory ob can be computed as µ∗ ∈ arg min µ∈M g(µ, fφ(ob), fφ(oe), c) (5) where M = {µ ∈ RT ×T : µ1 = µT 1 = 1 T 1} is the set of coupling matrices and the cost c can be the Euclidean or Cosine distance. In this work, inspired by [11], we use the entropic Wasserstein distance with cosine cost as our OT metric, which is given by the equation g(µ, fφ(ob), fφ(oe), c) = W 2(fφ(ob), fφ(oe)) T (cid:88) = t,t(cid:48)=1 Ct,t(cid:48) µt,t(cid:48) 13 (6) where the cost matrix Ct,t(cid:48) = c(fφ(ob), fφ(oe)). Using Eq. 6 and the optimal alignment µ∗ obtained by optimizing Eq. 5, a reward signal can be computed for each observation using the equation rOT (ob t ) = − T (cid:88) t(cid:48)=1 Ct,t(cid:48) µ∗ t,t(cid:48) (7) Intuitively, maximizing this reward encourages the imitating agent to produce trajectories that closely match demonstrated trajectories. Since solving Eq. 5 is computationally expensive, approximate solutions such as the Sinkhorn algorithm [63, 12] are used instead. B Issue with Fine-tuning Actor-Critic Frameworks In this paper, we use n-step DDPG proposed by Yarats et al. [8] as our RL optimizer for actor- critic based reward maximization. DDPG [23] concurrently learns a deterministic policy πφ using deterministic policy gradients (DPG) [15] and a Q-function Qθ by minimizing a n-step Bellman residual (for n-step DDPG). For a parameterized actor network πφ(s) and a critic function Qθ(s, a), the deterministic policy gradients (DPG) for updating the actor weights is given by ∇φJ ≈ Est∼ρβ = Est∼ρβ (cid:104) (cid:104) ∇φ Qθ(s, a)|s=st,a=πφ(st) (cid:105) ∇a Qθ(s, a)|s=st,a=πφ(st) ∇φ πφ(s)|s=st (cid:105) (8) Here, ρβ refers to the state visitation distribution of the data present in the replay buffer at time t. From Eq. 8, it is clear that the policy gradients in this framework depend on the gradients with respect to the critic value. Hence, as mentioned in [9, 10], naively initializing the actor with a pretrained policy while using a randomly initialized critic results in the untrained critic providing an exceedingly poor signal to the actor network during training. As a result, the actor performance drops immediately and the good behavior of the informed initialization of the policy gets forgotten. In this paper, we propose an adaptive regularization scheme that permits ﬁnetuning a pretrained actor policy in an actor-critic framework. As opposed to Rajeswaran et al. [16], Jena et al. [17] which employ on-policy learning, our method is off-policy and aims to leverage the sample efﬁcient characteristic of off-policy learning as compared to on-policy learning [7]. C Algorithmic Details C.1 Implementation Algorithm 1 describes our proposed algorithm, Regularized Optimal Transport (ROT), for sample efﬁcient imitation learning for continuous control tasks. Further implementation details are as follows: Algorithm and training procedure Our model consists of 3 primary neural networks - the encoder, the actor and the critic. During the BC pretraining phase, the encoder and the actor are trained using a mean squared error (MSE) on the expert demonstrations. Next, for ﬁnetuning, weights of the pretrained encoder and actor are loaded from memory and the critic is initialized randomly. We observed that the performance of the algorithm is not very sensitive to the value of α and we set it to 0.03 for all experiments in this paper. A copy of the pretrained encoder and actor are stored with ﬁxed weights to be used for computing λ(π) for soft Q-ﬁltering. Actor-critic based reward maximization We use a recent n-step DDPG proposed by Yarats et al. [8] as our RL backbone. The deterministic actor is trained using deterministic policy gradients (DPG) [15] given by Eq. 8. The critic is trained using clipped double Q-learning similar to Yarats et al. [8] in order to reduce the overestimation bias in the target value. This is done using two Q-functions, Qθ1 and Qθ2. The critic loss for each critic is given by the equation Lθk = E(s,a)∼Dβ (cid:2)(Qθk (s, a) − y)2(cid:3) ∀ k ∈ {1, 2} (9) 14 Algorithm 1 ROT: Regularized Optimal Transport Require: Expert Demonstrations T e ≡ {(ot, at)T Pretrained policy πBC Replay buffer D, Training steps T , Episode Length L Task environment env Parametric networks for RL backbone (e.g., the encoder, policy and critic function for DrQ-v2) A discriminator D for adversarial baselines t=0}N n=0 Algorithm: πROT ← πBC for each timestep t = 1...T do if done then r1:L = rewarderOT (episode) Update episode with r1:L and add (ot, at, ot+1, rt) to D ot = env.reset(), done = False, episode = [ ] (cid:46) Initialize with pretrained policy (cid:46) OT-based reward computation end if at = πROT (ot) ot+1, done = env.step(at) episode.append([ot, at, ot+1]) Update backbone-speciﬁc networks and reward-speciﬁc networks using D end for where Dβ is the replay buffer for online rollouts and y is the target value for n-step DDPG given by y = n−1 (cid:88) i=0 γirt+i + γn min k=1,2 Q¯θk (st+n, at+n) (10) Here, γ is the discount factor, r is the reward obtained using OT-based reward computation and ¯θ1, ¯θ2 are the slow moving weights of target Q-networks. Target feature processor to stabilize OT rewards The OT rewards are computed on the output of the feature processor fφ which is initialized with a parametric neural network. Hence, as the weights of fφ change during training, the rewards become non-stationary resulting in unstable training. In order to increase the stability of training, the OT rewards are computed using a target feature processor fφ(cid:48) [11] which is updated with the weights of fφ every Tupdate environment steps. For state-based observations, fφ corresponds to a ’trunk’ network which is a single layer neural network. For pixel-based observations, fφ includes DrQ-v2’s encoder followed by the ’trunk’ network. C.2 Hyperparameters The complete list of hyperparameters is provided in Table 1. Similar to Yarats et al. [8], there is a slight deviation from the given setting for the Walker Stand/Walk/Run task from the DeepMind Control suite where we use a mini-batch size of 512 and a n-step return of 1. D Environments Table 2 lists the different tasks that we experiment with from the DeepMind Control suite [18, 25], OpenAI Robotics suite [26] and the Meta-world suite [27] along with the number of training steps and the number of demonstrations used. For the tasks in the OpenAI Robotics suite, we ﬁx the goal while keeping the initial state randomized. No modiﬁcations are made in case of the DeepMind Control suite and the Meta-world suite. The episode length for all tasks in DeepMind Control is 1000 steps, for OpenAI Robotics is 50 steps and Meta-world is 125 steps (except bin picking which runs for 175 steps). 15 Method Common Parameter Replay buffer size Learning rate Discount γ n-step returns Action repeat Seed frames Mini-batch size Agent update frequency Critic soft-update rate Feature dim Hidden dim Optimizer ROT Exploration steps DDPG exploration schedule Target feature processor update frequency(steps) Reward scale factor Fixed weight α Value 150000 1e−4 0.99 3 2 12000 256 2 0.01 50 1024 Adam 0 0.1 20000 10 0.03 Linear decay schedule for λ(π) linear(1,0.1,20000) OT Exploration steps 2000 DDPG exploration schedule linear(1,0.1,500000) DAC Target feature processor update frequency(steps) Reward scale factor Exploration steps DDPG exploration schedule Gradient penalty coefﬁcient Table 1: List of hyperparameters. 20000 10 2000 linear(1,0.1,500000) 10 E Demonstrations For DeepMind Control tasks, we train expert policies using pixel-based DrQ-v2 [8] and collect 10 demonstrations for each task using this expert policy. The expert policy is trained using a stack of 3 consecutive RGB frames of size 84 × 84 with random crop augmentation. Each action in the environment is repeated 2 times. For OpenAI Robotics tasks, we train a state-based DrQ-v2 with hindsight experience replay [28] and collect 50 demonstrations for each task. The state representation comprises the observation from the environment appended with the desired goal location. For this, we did not do frame stacking and action repeat was set to 2. For Meta-World tasks, we use a single expert demonstration obtained using the task-speciﬁc hard-coded policies provided in their open-source implementation [27]. 16 Suite Tasks DeepMind Control Acrobot Swingup Allowed Steps 2 × 106 # Demonstrations 10 Cartpole Swingup Cheetah Run Finger Spin Hopper Stand Hopper Hop Quadruped Run Walker Stand Walker Walk Walker Run OpenAI Robotics Fetch Reach 1.5 × 106 50 1 1 Fetch Push Fetch Pick and Place Meta-World Hammer 1 × 106 xArm Robot 6 × 103 Drawer Close Door Open Bin Picking Button Press Topdown Door Unlock. Close Door Hang Hanger Erase Board Reach Hang Mug Hang Bag Turn Knob Stack Cups Press Switch Peg (Easy) Peg (Medium) Peg (Hard) Open Box Pour Table 2: List of tasks used for evaluation. F Robot Tasks In this section, we describe the suite of manipulation experiments carried out on a real robot in this paper. 17 Figure 7: Examples of different initializations for the real robot tasks. 18 r o o D e s o l C r e g n a H g n a H d r a o B e s a r E h c a e R g u M g n a H g a B g n a H b o n K n r u T s p u C k c a t S h c t i w S s s e r P ) y s a E ( g e P ) d e M ( g e P ) d r a H ( g e P x o B n e p O r u o P Figure 8: Example trajectories for selected real robot tasks. (a) Door Close: Here, the robot arm is supposed to close an open door by pushing it to the target. (b) Hang Hanger: While holding a hanger between the grippers, the robot arm is initialized at random position and is tasked with putting the hanger at a goal region on a closet rod. (c) Erase Board: While holding a board duster between the grippers, the robot arm is tasks with erasing marking drawn on the board while getting initialized from random positions. (d) Reach: The robot arm is required to reach a speciﬁc goal after being initialized at a random position. (e) Hang Mug: While holding a mug between the grippers, the robot arm is initialized at random position and is tasked with hanging the mug on a speciﬁc hook. (f) Hang Bag: While holding a tote between the grippers, the robot arm is initialized at random position and is tasked with hanging the tote bag on a speciﬁc hook. (g) Turn Knob: The robot arm is tasked with rotating a knob placed on the table by a certain angle after being initialized at a random position. We consider a 90 degree rotation as success. (h) Cup Stack: While holding a cup between the gripper, the robot arm is required to stack it into another cup placed on the table. (i) Press Switch: With the gripper kept closed, the robot arm is required to press a switch (with an LED light) placed on the table. (j) Peg (Easy, Medium, Hard): The robot arm is supposed to insert a peg, hanging by a string, into a bucket placed on the table. This task has 3 variants - Easy, Medium, Hard - with the size of the bucket decreasing from Easy to Hard. (k) Box Open: In this task, the robot arm is supposed to open the lid of a box placed on the table by lifting a handle provided in the front of the box. (l) Pour: Given a cup with some item place inside (in our case, almonds), the robot arm is supposed to move towards a cup place on the table and pour the item into the cup. Evaluation procedure For each task, we obtained a set of 20 random initializations and evaluate all of the methods (BC, RDAC and ROT) over 20 trajectories from the same set of initializations. These initializations are different for each task based on the limits of the observation space for the task. G Baselines Throughout the paper, we compare ROT with several prominent imitation learning and reinforcement learning methods. Here, we give a brief description of each of the baseline models that have been used. (a) Expert: For each task, the expert refers to the expert policy used to generate the demonstrations for the task (described in Appendix E). 19 d r a o B e s a r E g a B g n a H b o n K n r u T ) d r a H ( g e P x o B n e p O (b) Behavior Cloning (BC): This refers to the behavior cloned policy trained on expert demonstra- tions. (c) Adversarial IRL (DAC): Discriminator Actor Critic [7] is a state-of-the-art adversarial imi- tation learning method [6, 29, 7]. Since DAC outperforms prior work such as GAIL[6] and AIRL[30], it serves as our primary adversarial imitation baseline. (d) State-matching IRL (OT): Sinkhorn Imitation Learning [12, 13] is a state-of-the-art state- matching imitation learning method [31] that approximates OT matching through the Sinkhorn Knopp algorithm. Since ROT is derived from similar OT-based foundations, we use SIL as our primary state-matching imitation baseline. (e) RDAC: This is the same as ROT, but instead of using state-matching IRL (OT), adversarial IRL (DAC) is used. (f) Finetune with ﬁxed weight: This is similar to ROT where instead of using a time-varying adaptive weight λ(i), only the ﬁxed weight λ0 is used. λ0 is set to a ﬁxed value of 0.03. (g) Finetune with ﬁxed schedule: This is similar to ROT that uses both the ﬁxed weight λ0 and the time-varying adaptive weight λ1(i). However, instead of using Soft Q-ﬁltering to compute λ1(i), a hand-coded linear decay schedule is used. (h) DrQ-v2 (RL): DrQ-v2 [8] is a state-of-the-art algorithm for pixel-based RL. DrQ-v2 is assumed to have access to environment rewards as opposed to ROT which computes the reward using OT-based techniques. (i) Demo-DrQ-v2: This refers to DrQ-v2 but with access to both environment rewards and expert demonstrations. The model is initialized with a pretrained BC policy followed by RL ﬁnetuning with an adaptive regularization scheme like ROT. During RL ﬁnetuning, this baseline has access to environment rewards. (j) BC+OT: This is the same as the OT baseline but the policy is initialized with a pretrained BC policy. No adaptive regularization scheme is used while ﬁnetuning the pretrained policy. (k) OT+BC Reg.: This is the same as the OT baseline with randomly initialized networks but during training, the adaptive regularization scheme is added to the objective function. H Additional Experimental Results H.1 How efﬁcient is ROT for imitation learning? In addition to the results provided in Sec. 5.1, Fig. 9 and Fig. 10 shows the performance of ROT for pixel-based imitation on 10 tasks from the DeepMind Control suite, 3 tasks from the OpenAI Robotics suite and 7 tasks from the Meta-world suite. On all but one task, ROT is signiﬁcantly more sample efﬁcient than prior work. Finally, the improvements from ROT hold on state-based observations as well(see Fig. 11). Table 3 provides a comparison between the factor of speedup of ROT to reach 90% of expert performance compared to prior state-of-the-art [7, 11] methods. H.2 Does soft Q-ﬁltering improve imitation? Extending the results shown in Fig. 5, we provide training curves from representative tasks in each suite in Fig. 12. We observe that our adaptive soft-Q ﬁltering regularization is more stable compared to prior hand-tuned regularization schemes. ROT is on par and in some cases exceeds the efﬁciency of a hand-tuned decay schedule, while not having to hand-tune its regularization weights. H.3 How does ROT compare to standard reward-based RL? Extending the results shown in Fig. 6, we provide training curves from representative tasks in each suite in Fig. 13, thus showing that ROT can outperform standard RL that requires explicit task- reward. We also show that this RL method combined with our regularization scheme (represented by Demo-DrQ-v2 in Fig. 13 provides strong results. 20 Expert BC OT DAC ROT (Ours) Figure 9: Pixel-based continuous control learning on 10 DMC environments. Shaded region represents ±1 standard deviation across 5 seeds. We notice that ROT is signiﬁcantly more sample efﬁcient compared to prior work. H.4 How important are the design choices in ROT? Importance of pretraining and regularizing the IRL policy Fig. 14 compares the following variants of ROT on set of pixel-based tasks: (a) Training the IRL policy from scratch (OT); (b) Finetuning a pretrained BC policy without BC regularization (BC+OT); (c) Training the IRL policy from scratch with BC regularization (OT+BC Reg.). We observe that pretraining the IRL policy (BC+OT) does not provide a signiﬁcant difference without regularization. This can be attributed to the ‘forgetting behavior’ of pre-trained policies, studied in Nair et al. [9]. Interestingly, we see that even without BC pretraining, keeping the policy close to a behavior distribution (OT+BC Reg.) can yield improvements in efﬁciency over vanilla training from scratch. Our key takeaway from these experiments is that both pretraining and BC regularization are required to obtain sample-efﬁcient imitation learning. Choice of IRL method In ROT, we build on OT-based IRL instead of adversarial IRL. This is because adversarial IRL methods require iterative reward learning, which produces a highly non- stationary reward function for policy optimization. In Fig. 15, we compare ROT with adversarial 21 400 300 200 100 d r a w e r _ e d o s i p e 0 d r a w e r _ e d o s i p e 1000 800 600 400 200 0 d r a w e r _ e d o s i p e 1000 800 600 400 200 dmc_acrobot_swingup dmc_cartpole_swingup dmc_finger_spin d r a w e r _ e d o s i p e 1000 800 600 400 200 0 d r a w e r _ e d o s i p e 1200 1000 800 600 400 200 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 dmc_cheetah_run dmc_hopper_stand dmc_hopper_hop d r a w e r _ e d o s i p e 1000 800 600 400 200 0 200 300 200 100 d r a w e r _ e d o s i p e 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 dmc_walker_stand dmc_walker_walk dmc_walker_run d r a w e r _ e d o s i p e 1000 800 600 400 200 0 800 600 400 200 d r a w e r _ e d o s i p e 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 dmc_quadruped_run 600 400 200 d r a w e r _ e d o s i p e 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 Expert BC OT DAC ROT (Ours) Figure 10: Pixel-based continuous control learning on 3 OpenAI Gym Robotics and 7 Meta-World tasks. Shaded region represents ±1 standard deviation across 5 seeds. We notice that ROT is signiﬁcantly more sample efﬁcient compared to prior work. IRL methods that use our pretraining and adaptive BC regularization technique (RDAC). We ﬁnd that our soft Q-ﬁltering method does improve prior state-of-the-art adversarial IRL (RDAC vs. DAC in Fig. 15). However, our OT-based approach (ROT) is more stable and on average leads to more efﬁcient learning. 22 e g a t n e c r e p _ s s e c c u s e g a t n e c r e p _ s s e c c u s e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 1.25 1.00 0.75 0.50 0.25 0.00 0.25 1.00 0.75 0.50 0.25 0.00 0.25 fetch_reach fetch_push fetch_pick_and_place e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame metaworld_hammer metaworld_drawer_close metaworld_drawer_open e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 0.2 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 metaworld_door_open metaworld_bin_picking e g a t n e c r e p _ s s e c c u s 1.00 0.75 0.50 0.25 0.00 0.25 e g a t n e c r e p _ s s e c c u s e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 0.2 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 metaworld_button_press_topdown 1.0 0.8 0.6 0.4 0.2 0.0 0.2 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 metaworld_door_unlock e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 Expert BC OT DAC ROT (Ours) Figure 11: State-based continuous control learning on DMC and Meta-World tasks. We notice that ROT is signiﬁcantly more sample efﬁcient compared to prior work. 23 d r a w e r _ e d o s i p e 500 400 300 200 100 0 d r a w e r _ e d o s i p e 1000 800 600 400 200 0 200 d r a w e r _ e d o s i p e 1000 800 600 400 200 0 e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 dmc_acrobot_swingup dmc_cartpole_swingup dmc_finger_spin d r a w e r _ e d o s i p e 1000 800 600 400 200 0 d r a w e r _ e d o s i p e 1000 800 600 400 200 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 dmc_hopper_stand dmc_hopper_hop 300 200 100 d r a w e r _ e d o s i p e 0 dmc_walker_stand d r a w e r _ e d o s i p e 1200 1000 800 600 400 200 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 dmc_walker_walk dmc_walker_run dmc_quadruped_run d r a w e r _ e d o s i p e 800 600 400 200 0 600 400 200 d r a w e r _ e d o s i p e 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 metaworld_hammer metaworld_drawer_close metaworld_door_open e g a t n e c r e p _ s s e c c u s 1.00 0.75 0.50 0.25 0.00 e g a t n e c r e p _ s s e c c u s 1.00 0.75 0.50 0.25 0.00 0.25 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 metaworld_drawer_open metaworld_button_press_topdown metaworld_door_unlock e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 Suite Tasks ROT 2nd Best Model Speedup Factor DeepMind Control Acrobot Swingup Cartpole Swingup Finger Spin Cheetah Run Hopper Stand Hopper Hop Walker Stand Walker Walk Walker Run Quadruped Run OpenAI Robotics Fetch Reach 200k 100k 20k 400k 60k. 200k 80k 200k 320k 400k 300k 600k (OT) 350k (OT) 700k (OT) 2M (DAC) 750k (OT) >2M (DAC) 400k (DAC) 750k (DAC) >2M (OT) >2M (DAC) 1.1M (DAC) Fetch Push 1.1M 600k (DAC) Fetch Pick and Place Meta-World Hammer Drawer Close Drawer Open Door Open Bin Picking 750k 200k 20k >1M 400k 700k >1.5M (OT) >1M (DAC) >1M (OT) >1M (OT) >1M (OT) >1M (OT) Button Press Topdown >1M >1M (OT) Door Unlock 1M >1M (OT) 3 3.5 35 5 12.5 10 5 3.75 6.25 5 3.67 0.54 2 5 50 1 2.5 1.43 1 1 Table 3: Task-wise comparison between environment steps required to reach 90% of expert perfor- mance for pixel-based ROT compared to the strongest baseline for each task. 24 Expert BC Finetune with ﬁxed weight Finetune with ﬁxed schedule ROT (Ours) Figure 12: Pixel-based ablation analysis on the effect of varying BC regularization schemes. We observe that our adaptive soft-Q ﬁltering regularization is more stable compared to prior hand-tuned regularization schemes. 25 d r a w e r _ e d o s p e i 800 600 400 200 0 e g a t n e c r e p _ s s e c c u s 1.2 1.0 0.8 0.6 0.4 0.2 0.0 e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 dmc_cheetah_run dmc_hopper_hop dmc_quadruped_run 300 200 100 d r a w e r _ e d o s p e i 0 d r a w e r _ e d o s p e i 600 500 400 300 200 100 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 fetch_reach fetch_push fetch_pick_and_place e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 0.00 0.25 0.50 0.75 frame 1.00 1.25 1.50 1e6 0.00 0.25 0.50 0.75 frame 1.00 1.25 1.50 1e6 0.00 0.25 0.50 0.75 frame 1.00 1.25 1.50 1e6 metaworld_door_open metaworld_hammer metaworld_drawer_close e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 Expert BC OT DrQ-v2(RL) Demo-DrQ-v2 ROT (Ours) Figure 13: Pixel-based ablation analysis on the performance comparison of ROT against DrQ-v2, a reward-based RL method. Here we see that ROT can outperform plain RL that requires explicit task-reward. However, we also observe that this RL method combined with our regularization scheme provides strong results. 26 d r a w e r _ e d o s i p e 1000 800 600 400 200 0 e g a t n e c r e p _ s s e c c u s e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 1.25 1.00 0.75 0.50 0.25 0.00 0.25 dmc_finger_spin dmc_cheetah_run dmc_walker_run d r a w e r _ e d o s i p e 1000 800 600 400 200 0 d r a w e r _ e d o s i p e 1000 800 600 400 200 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 fetch_reach fetch_push fetch_pick_and_place 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame metaworld_door_open e g a t n e c r e p _ s s e c c u s e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 1.25 1.00 0.75 0.50 0.25 0.00 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame metaworld_bin_picking e g a t n e c r e p _ s s e c c u s e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 1.00 0.75 0.50 0.25 0.00 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame metaworld_hammer 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 BC OT BC+OT OT+BC Reg. ROT (Ours) Figure 14: Pixel-based ablation analysis on the importance of pretraining and regularizing the IRL policy. The key takeaway from these experiments is that both pretraining and BC regularization are required to obtain sample-efﬁcient imitation learning. 27 1200 1000 800 600 400 200 0 d r a w e r _ e d o s i p e e g a t n e c r e p _ s s e c c u s e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 1.00 0.75 0.50 0.25 0.00 0.25 dmc_finger_spin dmc_cheetah_run dmc_walker_run 800 600 400 200 d r a w e r _ e d o s i p e 0 800 d r a w e r _ e d o s i p e 600 400 200 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 fetch_reach fetch_push fetch_pick_and_place e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame metaworld_door_open metaworld_hammer e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 e g a t n e c r e p _ s s e c c u s e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame metaworld_button_press_topdown 1.25 1.00 0.75 0.50 0.25 0.00 0.25 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 Expert BC DAC OT RDAC ROT (Ours) Figure 15: Pixel-based ablation analysis on the choice of base IRL method. We ﬁnd that although adversarial methods beneﬁt from regularized BC, the gains seen are smaller compared to ROT. 28 d r a w e r _ e d o s i p e 1250 1000 750 500 250 0 250 e g a t n e c r e p _ s s e c c u s e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 1.00 0.75 0.50 0.25 0.00 0.25 dmc_finger_spin dmc_cheetah_run d r a w e r _ e d o s i p e 1000 800 600 400 200 0 dmc_walker_run 1000 d r a w e r _ e d o s i p e 800 600 400 200 0 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 0.0 0.5 1.0 frame 1.5 2.0 1e6 fetch_reach 1.2 1.0 0.8 0.6 0.4 0.2 0.0 e g a t n e c r e p _ s s e c c u s fetch_push fetch_pick_and_place e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1e6 frame metaworld_door_open metaworld_hammer metaworld_button_press_topdown e g a t n e c r e p _ s s e c c u s 1.25 1.00 0.75 0.50 0.25 0.00 0.25 e g a t n e c r e p _ s s e c c u s 1.0 0.8 0.6 0.4 0.2 0.0 0.2 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6 0.0 0.2 0.4 0.6 0.8 frame 1.0 1e6"
464,Personalized Showcases: Generating Multi-Modal Explanations for   Recommendations,"[{'href': 'http://arxiv.org/abs/2207.00422v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2207.00422v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-06-30 01:43:58,"Denoised MDPs: Learning World Models Better Than the World Itself Tongzhou Wang 1 Simon S. Du 2 Antonio Torralba 1 Phillip Isola 1 Amy Zhang 3 4 Yuandong Tian 4 2 2 0 2 l u J 1 ] G L . s c [ 2 v 7 7 4 5 1 . 6 0 2 2 : v i X r a Abstract The ability to separate signal from noise, and reason with clean abstractions, is critical to in- telligence. With this ability, humans can efﬁ- ciently perform real world tasks without consider- ing all possible nuisance factors. How can artiﬁ- cial agents do the same? What kind of information can agents safely discard as noises? In this work, we categorize information out in the wild into four types based on controllability and relation with reward, and formulate useful information as that which is both controllable and reward-relevant. This framework clariﬁes the kinds information removed by various prior work on representation learning in reinforcement learning (RL), and leads to our proposed approach of learning a Denoised MDP that explicitly factors out certain noise dis- tractors. Extensive experiments on variants of DeepMind Control Suite and RoboDesk demon- strate superior performance of our denoised world model over using raw observations alone, and over prior works, across policy optimization con- trol tasks as well as the non-control task of joint position regression. ssnl.github.io/denoised_mdp Project Page: Code: github.com/facebookresearch/denoised_mdp 1. Introduction The real world provides us a plethora of information, from microscopic physical interactions to abstracted semantic signals such as the latest COVID-19 news. Fortunately, processing each and every signal is unnecessary (and also impossible). In fact, any particular reasoning or decision often only relies on a small portion of information. Imagine waking up and wanting to embrace some sunlight. As you open the curtain, a nearby resting bird is scared 1MIT CSAIL 2University of Washington 3UC Berkeley 4Meta AI. Correspondence to: Tongzhou Wang <tongzhou@mit.edu>. Work done while Tongzhou Wang was an intern at Meta AI. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). Figure 1: Illustrative example: (a) Four distinct kinds of infor- mation in the scenario described in Section 1, where the person desires to increase the amount of sunlight let into the room. Their opening of the curtain scares away the bird. (b) A denoised world model only includes a small subset of all information. away and you are pleasantly met with a beautiful sunny day. Far away, a jet plane is slowly ﬂying across the sky. This may seem a simple activity, but in fact highlights four distinct types of information (see Figure 1), with respect to the goal of letting in as much sunlight as possible: • Controllable and reward-relevant: curtain, inﬂuenced by actions and affecting incoming sunlight; • Controllable and reward-irrelevant: bird, inﬂuenced by actions but not affecting sunlight; • Uncontrollable and reward-relevant: weather, inde- pendent with actions but affecting sunlight; • Uncontrollable and reward-irrelevant: plane, indepen- dent with both actions and the sunlight. Our optimal actions towards the goal, however, only in fact depend on information that is controllable and reward- Reward- Relevant Reward- Irrelevant Uncontrollable Controllable (a) GOAL: Letting in as much sunlight as possible. Denoise (b) Optimal control only relies on information that is both controllable and reward-relevant. Good world models should ignore other factors as noisy distractors. Denoised MDPs relevant, and the three other kinds of information are merely noise distractors. Indeed, no matter how much natural sunlight there is outside, or how the plane and the bird move, the best plan is always to open up the curtain. When performing a particular task, we humans barely think about the other three types of information, and usually only plan on how our actions affect information that is control- lable and reward-relevant. Our mental model is an ab- stract and condensed version of the real world that is actually better suited for the task. The notion of better model/data is ubiquitous in data science and machine learning. Algorithms rarely perform well on raw noisy real data. The common approach is to perform data cleaning and feature engineering, where we manually select the useful signals based on prior knowledge and/or heuristics. Years of research have identiﬁed ways to extract good features for computer vision (Lowe, 1999; Donahue et al., 2014), natural language processing (Elman, 1990; Mikolov et al., 2013), reinforcement learning (RL) (Ma- hadevan & Maggioni, 2007; Bellemare et al., 2019), etc. Similarly, system identiﬁcation aligns real observation with a predeﬁned set of abstract signals/states. Yet for tasks in the wild (in the general form of (partially observable) Markov Decision Processes), there can be very little prior knowledge of the optimal set of signals. In this work, we ask: can we infer and extract these signals automatically, in the form of a learned world model? The general idea of a mental world model have long been un- der active research in philosophy and social science (Craik, 1952; Dennett, 1975), cognitive science, where an intuitive physics model is hypothesized to be core in our planning capabilities (Spelke & Kinzler, 2007), and in reinforcement learning, where various methods investigate state abstrac- tions for faster and better learning (Sutton, 1991; 1981). In this work, we explore this idea within the context of machine learning and reinforcement learning, where we aim to make concrete the different types of information in the wild, and automatically learn a world model that removes noise distractors and is beneﬁcial for both control (i.e., policy optimization) and non-control tasks. Toward this goal, our contributions are • We categorize information into four distinct kinds as in Figure 1, and review prior approaches under this frame- work (Section 2). • Based on the above framework, we propose Denoised MDPs, a method for learning world models with certain distractors removed (Section 3). • Through experiments in DeepMind Control Suite and RoboDesk environments, we demonstrate superior per- formance of policies learned our method, across many distinct types of noise distractors (Sections 5.1 and 5.2). • We show that Denoised MDP is also beneﬁcial beyond control objectives, improving the supervised task of robot joint position regression (Section 5.1). 2. Different Types of Information in the Wild In Section 1, we illustrated the four types of information available in the wild w.r.t. a task. Here we make these notions more concrete, and relate them to existing works. For generality, we consider tasks in the form of Markov Decision Processes (MDPs), described in the usual manner: M (cid:44) (S, A, R, P, ps0 ) (Puterman, 1994), where S is the state space, A is the action space, R : S → ∆([0, rmax]) deﬁnes the reward random variable R(s(cid:48)) received for ar- riving at state s(cid:48) ∈ S, P : S × A → ∆(S) is the transition dynamics, and ps0 ∈ ∆(S) deﬁnes the distribution of initial state. We use ∆(A) to denote the set of all distributions over A. P and R deﬁne the most important components of a MDP: the transition dynamics P[s(cid:48) | s, a] and the re- ward function P[r | s(cid:48)]. Usually, the objective is to ﬁnd a policy π : S → ∆(A) acting based on current state, that maximizes the expected cumulative (discounted) reward. Indeed, MDPs provide a general formulation that encom- passes many tasks. In fact, the entire real world may be viewed as an MDP with a rich state/observation space S that contains all possible information/signal. For an artiﬁ- cial agent to successfully perform real world tasks, it must be able to process observations that are incredibly rich and high-dimensional, such as visual or audio signals. We characterize different types of information in such ob- servations by considering two intuitive notions of “noisy and irrelevant” signals: (1) uncontrollable information and (2) reward-irrelevant information. Such factors can often be ignored without affecting optimal control, and are referred to as noise distractors. To understand their roles in MDPs, we study different for- mulations of the transition dynamics and reward functions, and show how different structures naturally leads to decom- positions that may help identify such distractors. Removing these distractors can thus transform the original noisy MDP to a clean denoised one, to be used in downstream tasks. For starters, the most generic transition model in Figure 2a has little to no structure. The state s can contain both the useful signals and noise distractors. Therefore, it is not directly useful for extracting important information. 2.1. Controllability Intuitively, if something is not controllable, an agent might be able to do well without considering it. Yet it is not enough to only require some variable to be unaffected by actions (e.g., wind directions should not be ignored while sailing). Denoised MDPs a a a a r Rew Rew Ctrl s s(cid:48) Ctrl s s s s x yR yR s x(cid:48) y(cid:48) R y(cid:48) R s(cid:48) rx ry + r Rew Rew Ctrl x yR yR x yR yR Ctrl x yR yR x yR yR x y z s x(cid:48) y(cid:48) z(cid:48) s(cid:48) rx ry + r Rew Rew Ctrl Ctrl x y x y z z x y x y z z (b) Transition that factorizes out uncontrol- lable information in yR and yR. (a) Transition without useful structure. s may contain any type of information. Figure 2: MDP transition structures consisting of dynamics and reward functions. Unlike the regular structure of (a), (b, c) factorized (yet still general) structures inherently separate information into controllable (Ctrl) versus uncontrollable (Ctrl), and reward-relevant (Rew) versus reward-irrelevant (Rew). Presence of a variable in a cell means possible containing of respective information. E.g., in (c), z can only contain reward-irrelevant information. In (b, c), the x dynamics form an MDP with less noise and sufﬁcient for optimal planning. Our Denoised MDP (see Section 3) is based on these two factorizations. (c) Transition that factorizes out uncontrol- lable y and reward-irrelevant z. Instead, we focus on factors that simply evolve on their own, without inﬂuencing or being inﬂuenced by others. Not all such information can be safely ignored, as they still may affect reward (e.g., trafﬁc lights when driving). Fortunately, in the usual objective of maximizing expected return, we can ignore ones that only additively affect reward. Concretely, if an MDP transition can be represented in the form of Figure 2b, we say variables yR and yR are uncontrol- lable information, as they evolve independently of actions and do not affect controllable x. Here yR (additively) af- fects reward, but can be ignored. One can safely discard both yR and yR as noise distractors. Operating with the compressed MDP of only x is sufﬁcient for optimal control. 2.2. Reward-Relevance Among controllable information, there can still be some that is completely unrelated to reward. In Figure 1, the bird is affected by the opening curtain, but is irrelevant to the task of letting in sunlight. In such cases, the information can be safely discarded, as it does not affect the objective. If an MDP transition can be represented in the form of Figure 2c, we say z is reward-irrelevant because it evolves by potentially using everything (i.e., all latent variables and actions), but crucially does not affect anything but itself. Similar to uncontrollable information, z (and y) is a noise distractor that can be discarded. The compressed MDP of only x contains all signals needed for optimal control. 2.3. Which Information Do Existing Methods Learn? In RL, many prior work have explored state abstractions in some form. Here we cast several representative ones under Reconstruction-Based Model-Based RL (e.g., SLAC (Lee et al., 2019), Dreamer (Hafner et al., 2019a)) Model-Based Bisimulation (e.g., Ferns et al. (2004), Castro (2020), Zhang et al. (2020)) Model-Free Task Informed Abstractions (TIA) (Fu et al., 2021) Model-Based Denoised MDP (Figure 2b variant) (Our method from Section 3) Model-Based Denoised MDP (Figure 2c variant) (Our method from Section 3) Model-Based RewRew (cid:51) (cid:51) (cid:51) (cid:51) Ctrl Ctrl RewRew (cid:51) (cid:55) (cid:51) (cid:55) Ctrl Ctrl RewRew (cid:51) ? (cid:51) ? RewRew (cid:51) (cid:51) (cid:55) (cid:55) RewRew (cid:51) (cid:55) (cid:55) (cid:55) Ctrl Ctrl Ctrl Ctrl Ctrl Ctrl Information Grid Legend: (cid:51) Kept (cid:55) Reduced ? Depending on how the information is integrated in observations Figure 3: Categorization of information learned and removed by various methods with distinct formulations. the framework described above, and show which kinds of information they learn to remove, summarized in Figure 3, together with our proposed method (explained in Section 3). Below we discuss each prior work in detail. Reconstruction-Based Model-Based RL. Many model- based RL methods learn via reconstruction from a single latent code, often as a result of a variational formulation (Hafner et al., 2019a;b; Lee et al., 2019). The latent code Denoised MDPs must try to compress all information present in the observa- tion, and necessarily contains all types of information. Bisimulation. Bisimulation deﬁnes a state abstraction where states aggregated together must have the same ex- pected return and transition dynamics up to the abstrac- tion (Givan et al., 2003), and is known to optimally ignore reward-irrelevant information (Ferns et al., 2004). While its continuous version, bisimilation metric, is gaining popular- ity, learning them is computationally difﬁcult (Modi et al., 2020). Even with many additional assumptions, it is gen- erally only possible to learn an on-policy variant that loses the above guarantee (Castro, 2020; Zhang et al., 2020). Task Informed Abstractions (TIA). TIA (Fu et al., 2021) extends Dreamer by modelling two independent la- tent MDPs, representing signal and noise. The noise latent is enforced to be independent with reward and reconstruct the observation as well as possible. Reconstructions from each latent are composed together using an inferred mask in pixel-space, to form the full reconstruction for the re- construction loss. Because of its special structure, TIA can remove reward-irrelevant noise distractors that are present via pixel-wise composing two images from independent processes (e.g., agent moving on a noisy background), but not general ones (e.g., a shaky camera affecting both the agent and the noisy background). Predictive Information, Data Augmentation, etc. An- other set of researches learn state representation that only contains information useful for predicting future states (e.g., CPC (Oord et al., 2018) and PI-SAC (Lee et al., 2020)) or augmented views of the current state (e.g., CURL (Laskin et al., 2020b)). These methods do not guarantee removal of any of the three redundant piece of information identiﬁed above. Non-i.i.d. noises (e.g., people moving in background) are predictive of future and may be kept by CPC and PI- SAC. The performance of augmentation-based methods can critically rely on speciﬁc types of augmentation used and relevance to the tasks. As we show in experiments (see Sec- tion 5), indeed they struggle to handle certain noise types. 2.4. Possible Extensions to Further Factorizations The above framework is sufﬁcient for characterizing most prior work and related tasks, and can also be readily ex- tended with further factorized transition structures. E.g., if an independent process confounds a signal process and a noise process, ﬁtting the Figure 2c structure must group all three processes into x (to properly model the dependencies). However, a further factorization shows that only considering the signal and the confounding processes is theoretically sufﬁcient for control. We leave such extensions as future work. 3. Denoised MDPs Figures 2b and 2c show two special MDP structures that au- tomatically identify certain information that can be ignored, leaving x as the useful information (which also forms an MDP). This suggests a naïve approach: directly ﬁtting such structures to collected trajectories, and then extract x. However, the same MDP dynamics and rewards can be decomposed as Figures 2b and 2c in many different ways. In the extreme case, x may even contain all information in the raw state s, and such extraction may not help at all. Instead, we desire a ﬁt with the minimal x, deﬁned as being least informative of s (so that removal of the other latent variables discards the most information possible). Concretely, we aim for a ﬁt with least I({xt}T t=1), the mutual information x contains about s over T steps. Then from this ﬁt, we can extract a minimal Denoised MDP of only x. For notation simplicity, we use bold symbols to denote variable sequences, and thus write, e.g., I(x; s | a). t=1 | {at}T t=1; {st}T Practically, we consider regularizing model-ﬁtting with I(x; s | a). As we show below, this amounts to a modiﬁ- cation to the well-established variational objective (Hafner et al., 2019a). The resulting method is easy-to-implement yet effective, enabling clean removal of various noise distrac- tors the original formulation cannot handle (see Section 5). We instantiate this idea with the structure in Figure 2c. The Figure 2b formulation can be obtained by simply removing the z components and viewing y as combined yR and yR. The transition structure is modeled with components: p(xt) θ p(yt) θ p(zt) θ (cid:44) pθ(xt | xt−1, a) pθ(rx | xt) (cid:44) pθ(yt−1 | yt−1) pθ(ry | yt) (x dynamics) (x reward) (y dynamics) (y reward) (cid:44) pθ(zt | xt, yt, zt−1, a) pθ(st | xt, yt, zt). (z dynamics) (obs. emission) Consider training data in the form of trajectory segments s, a, r sampled from some data distribution pdata (e.g., stored agent experiences from a replay buffer). We perform model learning by minimizing the negative log likelihood: LMLE(θ) (cid:44) −Es,a,r∼pdata (cid:2) log pθ (s, r | a) (cid:3). To obtain a tractable form, we jointly learn three variational posterior components (i.e., encoders): q(xt) ψ q(yt) ψ q(zt) ψ (cid:44) qψ(xt | xt−1, yt−1, zt−1, st, at) (x posterior) (cid:44) qψ(yt | xt−1, yt−1, zt−1, st, at) (y posterior) (cid:44) qψ(zt | xt, yt, st, at), (z posterior) Denoised MDPs whose product deﬁnes the posterior qψ(x, y, z | s, a)1. We choose this factorized form based on the forward (prior) model structure of Figure 2c. Then, the model can be optimized w.r.t. the standard varia- tional bound on log likelihood: (cid:20) LMLE(θ) = min ψ Es,a,r Ex,y,z∼ qψ (·|s,a,r) − log pθ(s, r | x, y, z, a) (cid:124) (cid:123)(cid:122) (cid:125) (cid:44) Lrecon(θ, ψ) T (cid:88) DKL (cid:0)q(yt) ψ (cid:13) (cid:13) p(yt) θ DKL (cid:0)q(xt) ψ (cid:13) (cid:13) p(xt) θ (cid:1) + (cid:123)(cid:122) (cid:44) LKL-x(θ, ψ) (cid:0)q(zt) DKL ψ (cid:13) (cid:13) p(zt) θ (cid:123)(cid:122) (cid:44) LKL-z(θ, ψ) t=1 (cid:124) (cid:123)(cid:122) (cid:44) LKL-y(θ, ψ) (cid:125) (cid:1) (cid:125) (cid:21) , (cid:1) (cid:125) (1) + + T (cid:88) t=1 (cid:124) T (cid:88) t=1 (cid:124) where equality is attained by optimal qψ that is compatible with pθ, i.e., qψ is the exact posterior of pθ. The mutual information regularizer I(x; s | a), using a variational formulation, can be written as I(x; s | a) = min θ LKL-x(θ, ψ), (2) with equality attained when qψ and pθ are compatible. The appendix describes this derivation in detail. Therefore, for a regularizer weight of c ≥ 0, we can opti- mize Equations (1) and (2) together as min θ = min θ,ψ LMLE(θ) + c · I(x; s | a) Lrecon(θ, ψ) + (1 + c) · LKL-x(θ, ψ) + LKL-y(θ, ψ) + LKL-z(θ, ψ). (3) Recall that we ﬁt to the true MDP with the structure of Fig- ure 2c, which inherently guarantees all useful information in the x latent variable. As the regularizer ensures learning the minimal x latents, the learned model extracts an MDP of condensed useful information with X as the denoised state space, pθ(x(cid:48) | x, a) as the transition dynamics, pθ(rx | x(cid:48)) as the reward function. This MDP is called the Denoised MDP, as it discards the noise distractors contained in y and z. Additionally, we also obtain qψ(x | s, a) as the encoder mapping from raw noisy observation s to the denoised x. A loss variant for improved stability. When using a large c ≥ 0 (e.g. when the environment is expected to be very noisy), Equation (3) contains to a term with a large weight. Thus Equation (3) often requires learning rates to be tuned for different c. To avoid this, we use the following loss form that empirically has better training stability and does not require tuning learning rates w.r.t. other hyperpa- 1Following Dreamer (Hafner et al., 2019a), we deﬁne pos- terior of ﬁrst-step latents qψ(x1, y1, z1 | s1) (cid:44) qψ( · , · , · | 0, 0, 0, s1, 0), where 0 is the all zeros vector of appropriate size. Algorithm 1 Denoised MDP Input: Model pθ. Posterior encoder qψ. Policy π : X → ∆(A). Policy optimization algorithm PI-OPT. Output: Denoised MDP of x in pθ; Encoder qψ; Policy π. 1: while training do // Exploration 2: Collect trajectories with π acting on qψ encoded outputs 3: // Model learning 4: Sample a batch of (s, a, r) segments from reply buffer 5: Train pθ and qψ with Equation (4) on (s, a, r) 6: // Policy optimization 7: Sample x ∼ qψ(x | s, a); Compute rx = E [pθ(rx | x)] 8: Train π by running PI-OPT on (x, a, rx) 9: 10: end while rameters: min θ,ψ Lrecon + α · (LKL-x + βLKL-y + βLKL-z) , (4) where θ, ψ in arguments are omitted, and the hyperparame- ters are α > 0 and 0 < β ≤ 1. Here β is bounded, where β = 1 represents no regularization. α is also generally small and simply chosen according to the state-space dimensional- ity (see the appendix; α ∈ {1, 2} in our experiments). This form is justiﬁed from the observation that in practice we use isotropic Gaussians with ﬁxed variance to parameter- ize the distributions of observation pθ(s | . . . ) and reward pθ(r | . . . ), where scaling log likelihoods is essentially changing the variance hyperparameter. Thus, Equation (4) is effectively a scaled Equation (3) with different variance hyperparameters. Online algorithm with policy optimization. The model ﬁtting objective of Equation (4) can be used in various set- tings, e.g., ofﬂine over a collected trajectory dataset. With- out assuming existing data, we explore an online setting, where the training process iteratively performs (1) explo- ration, (2) model-ﬁtting, and (3) policy optimization, as shown in Algorithm 1. The policy π : X → ∆(A) soley operates on the Denoised MDP of x, which has all infor- mation sufﬁcient for control. For policy optimization, the learned posterior encoder qψ(x | s, a) is used to extract x information from the raw trajectory (s, a, r), obtaining transition sequences in X space. Paired with the pθ(rx | x) rewards, we obtain (x, a, rx) as trajectories collected from the Denoised MDP on x. Any general-purpose MDP policy optimization algorithm may be employed on these data, such as Stochastic Actor-Critic (SAC) (Haarnoja et al., 2018). We can also utilize the learned differentiable Denoised MDP, e.g., optimizing policy by backpropagating through addi- tional roll-outs from the model, as is done in Dreamer. While presented in the fully observable setting, Denoised MDP readily handles partial observability without extra changes. In the appendix, we discuss this point in details, and provide a guideline for choosing hyperparameters α, β. Denoised MDPs 4. Related Work Model-Based Learning for Control jointly learns a world model and a policy. Such methods often enjoy good sample efﬁciency on RL tasks with rich observations. Some formulations rely on strong assumptions, e.g., determinis- tic transition in DeepMDP (Gelada et al., 2019) and bilin- ear transition in FLAMBE (Agarwal et al., 2020). Most general-setting methods use a reconstruction-based objec- tive (Hafner et al., 2019b; Kim et al., 2020; Ha & Schmidhu- ber, 2018; Lee et al., 2019). Among them, Dreamer (Hafner et al., 2019a) trains world models with a variational formu- lation and optimizes policies by backpropagating through latent-space rollouts. It has proven effective across a va- riety of environments with image observations. However, such reconstruction-based approaches can struggle with the presence of noise distractors. TIA (Fu et al., 2021) partially addresses this limitation (see Section 2.3) but can not handle general distractors, unlike our method. Representation Learning and Reinforcement Learning. Our work automates selecting useful signals from noisy MDPs by learning denoised world models, and can be viewed as an approach for learning general representations (Donahue et al., 2014; Mikolov et al., 2013; He et al., 2019; Huh et al., 2016). In model-free RL, various methods learn state embeddings that are related to value functions (Schaul et al., 2015; Bellemare et al., 2019), transition dynamics (Mahadevan & Maggioni, 2007; Lee et al., 2020), recent ac- tion (Pathak et al., 2017), bisimulation structure (Ferns et al., 2004; Castro, 2020; Zhang et al., 2020), data augmentations (Laskin et al., 2020b) etc. Recently, Eysenbach et al. (2021) proposes a regularizer similar to ours but for the different purpose of robust compressed policies. The theoretical work by Efroni et al. (2021) is closest to our setting but concerns a more restricted set of distractors (ones both uncontrollable and reward-irrelevant). Unlike Denoised MDP, their pro- posed algorithm is largely impractical and does not produce a generative model of observations (i.e., no decoder). System Identiﬁcation. Our work is related to system identiﬁcation, where an algorithm infers from real world an abstract state among a predeﬁned limited state space, e.g., pose estimation (Rıza Alp Güler, 2018; Yen-Chen et al., 2021) and material estimation (Hahn et al., 2019). Such results are useful for robotic manipulation (Manuelli et al., 2019), image generation (Gu et al., 2019), etc. Our setting is not limited to a predeﬁned abstract state space, but instead focuses on automatic discovery of such valuable states. 5. Experiments In this section, we contrast our method with existing ap- proaches on environments with image observations and many distinct types of noise distractors. Our experiments are designed to include a variety of noise distractors and to conﬁrm our analysis on various methods in Section 2.3. Environments. We choose DeepMind Control (DMC) Suite (Tunyasuvunakool et al., 2020) (Section 5.2) and RoboDesk (Kannan et al., 2021) (Section 5.1) with image observations, where we explore adding various noise dis- tractors. Information types in all evaluated environments are categorized in Table 2 of the appendix. Tasks include control (policy optimization) and a non-control task of regressing robot joint position from RoboDesk image observations. Methods. We compare not only model-based RL meth- ods, but also model-free algorithms and general representa- tion learning approaches, when the task is suited: • Model Learning: Denoised MDP (our method), Dreamer (Hafner et al., 2019a), and TIA (Fu et al., 2021); • Model-Free: DBC (Zhang et al., 2020), CURL (Laskin et al., 2020b), PI-SAC (Lee et al., 2020) (without data augmentation for a fair comparison of its core predictive information regularization against other non-augmenting methods), and SAC on true state-space (Haarnoja et al., this is 2018) (instead of using image observations, roughly an “upper bound”); • General Image Representation Learning for Non- Control Tasks: Contrastive learning with the Align- ment+Uniformity loss (Wang & Isola, 2020) (a form of contrastive loss theoretically and empirically comparable to the popular InfoNCE loss (Oord et al., 2018)). Model-learning methods can be used in combination with any policy optimization algorithm. For a complete com- parison for general control, we compare the models trained with these two policy learning choices: (1) backpropagating via the learned dynamics and (2) SAC on the learned la- tent space (which roughly recovers SLAC (Lee et al., 2019) when used with an unfactorized model such as Dreamer). Most compared methods do not apply data augmentations, which is known to strongly boost performance (Yarats et al., 2021; Laskin et al., 2020a). Therefore, for a fair comparison, we run PI-SAC without augmentation to highlight its main contribution—representation of only predictive information. All results are aggregated from 5 runs, showing mean and standard deviations. The appendix contains more details, hyperparameter studies, and additional results. Our website presents videos showing clearer video visualizations. For Denoised MDP, we use the Figure 2b variant. Empiri- cally, the Figure 2c variant leads to longer training time and sometimes inferior performance (perhaps due to having to optimize extra components and ﬁt a more complex model). The appendix provides a comparison between them. Denoised MDPs Figure 4: Visualization of learned models for RoboDesk by using decoders to reconstruct from encoded latents. For TIA and Denoised MDP, we visualize how they separate information as signal versus noise. In each row, what changes over frames is the information modeled by the corresponding latent component. E.g., in the bottom row, only the TV content, camera pose and lighting condition change, so Denoised MDP considers these factors as noises, while modelling the TV hue as signal. See our website for clearer video visualizations. 5.1. RoboDesk with Various noise distractors We augment RoboDesk environment with many noise dis- tractors that models realistic noises (e.g., ﬂickering lights and shaky camera). Most importantly, we place a large TV in the scene, which plays natural RGB videos. A green button on the desk controls the TV’s hue (and a light on the desk). The agent is tasked with using this button to shift the TV to a green hue. Its reward is directly affected by how green the TV image is. The ﬁrst row of Figure 4 shows a trajectory with various distractors annotated. All four types of information exist (see Table 2), with the controllable and reward-relevant information being the robot arm, the green button, the light on the desk, and the TV screen green-ness. controllable and reward-relevant information as signals— the Signal row only tracks changes in robot arms, green button and light, and the TV screen green-ness. All other information is modeled as noises (see the Noise row). We recommend viewing video visualizations on our website. Denoised models improve policy learning. Figure 4 also shows the total episode return achieved by policies learned with each of the three models, where the cleanest model from Denoised MDP achieves the best performance. Aggregating over 5 runs, the complete comparison in Fig- ure 5 shows that Denoised MDP (with backpropagating via dynamics) generally outperforms all baselines, suggesting that its clean models are helpful for control. Only Denoised MDP learns a clean denoised model. Using learned decoders, Figure 4 visualizes how the mod- els captures various information. As expected, Dreamer model captures all information. TIA also fails to separate any noise distractors out (the Noise row fails to capture anything), likely due to its limited ability to model differ- ent noises. In contrast, Denoised MDP cleanly extracts all Denoised models beneﬁt non-control tasks. We evalu- ate the learned representations on a supervised non-control task—regressing the robot arm joint position from observed images. Using various pretrained encoders, we ﬁnetune on a labeled training set, and measure mean squared error (MSE) on a heldout test set. In addition to RL methods, we compare encoders learned via general contrastive learning Blocks on Desk: Ctrl & Rew TV Image Green-ness: Ctrl & Rew Green Button & Light: Ctrl & Rew Robot Joints: Ctrl & Rew TV Semantic Content: Ctrl & Rew Shaky/Flickering Camera & Lights: Ctrl & Rew Env. Rollout Obs. Reward <latexit sha1_base64=""2Iv/3i2hbQBcJJvODDrynrkogxE="">AAACaHicjVHLSgMxFE3Hd321uhBxEyyCLiwzoqgLQRTBpYJVoR0kk96xwSQzJHekZZgP8Gvc6qf4C36Fae1CrYIXAodzziU5J1EqhUXffyt5Y+MTk1PTM+XZufmFxUp16dommeHQ4IlMzG3ELEihoYECJdymBpiKJNxED6d9/eYRjBWJvsJeCqFi91rEgjN01F2lttlSDDtRlJ8VzRZCF22cG8DM6CKkR3QvONxyLr/uD4aOgmAIamQ4F3fV0narnfBMgUYumbXNwE8xzJlBwSUU5VZmIWX8gd1D00HNFNgwH6Qp6IZj2jROjDsa6YD9upF3/2tkytqeipyzn9H+1PrkXxp21G9SM8P4IMyFTjMEzT/fEGeSYkL79dK2MMBR9hxg3AiXl/IOM4yj+4Rvl1irpXKexBau4OBnnaPgeqce7NX9y93a8cmw6mmyRtbJJgnIPjkm5+SCNAgnT+SZvJDX0rtX8Va81U+rVxruLJNv461/AAe3u44=</latexit> Dreamer (E[return] = 519) Recon. Recon. <latexit sha1_base64=""detSQHFar06ldxZUdWoplWvpw3s="">AAACaHicjVHLSgMxFE3H97vVhYibYBF0YZnxvRFEEVwqWBXaQTLpnTaYZIbkjrQM8wF+jVv9FH/BrzCtXfgELwQO55xLck6iVAqLvv9a8kZGx8YnJqemZ2bn5hfKlcVrm2SGQ50nMjG3EbMghYY6CpRwmxpgKpJwE92f9vWbBzBWJPoKeymEirW1iAVn6Ki7cnWjqRh2oig/KxpNhC7aODeAmdFFSI/o7s7+pnP5NX8w9CcIhqBKhnNxVyltNVsJzxRo5JJZ2wj8FMOcGRRcQjHdzCykjN+zNjQc1EyBDfNBmoKuO6ZF48S4o5EO2M8befe/Rqas7anIOfsZ7XetT/6lYUf9JjUyjA/DXOg0Q9D84w1xJikmtF8vbQkDHGXPAcaNcHkp7zDDOLpP+HKJtVoq50ls4QoOvtf5E1xv14K9mn+5Wz0+GVY9SVbJGtkgATkgx+ScXJA64eSRPJFn8lJ688resrfyYfVKw50l8mW8tXcD9buM</latexit> TIA (E[return] = 436) Signal Noise Recon. Signal Noise Denoised MDP (E[return] = 628) <latexit sha1_base64=""ftE/OHSJy3ovK5tqHIptkB9eUHM="">AAACaHicjVHLSgMxFE3H97vqQsRNaBF0YZkRXxtBFMGlgtVCO0gmvWODSWZI7ohlmA/wa9zqp/gLfoVp7UJbBS8EDuecS3JOolQKi77/XvLGxicmp6ZnZufmFxaXyssrNzbJDIc6T2RiGhGzIIWGOgqU0EgNMBVJuI0eznr67SMYKxJ9jd0UQsXutYgFZ+iou3J1q6UYdqIoPy+aLYQntHFuADOji5Ae04Pdo23n8mt+f+goCAagSgZzebdc2mm1E54p0Mgls7YZ+CmGOTMouIRitpVZSBl/YPfQdFAzBTbM+2kKuumYNo0T445G2me/b+RP/zUyZW1XRc7Zy2iHtR75l4Yd9ZvUzDA+CnOh0wxB8683xJmkmNBevbQtDHCUXQcYN8LlpbzDDOPoPuHHJdZqqZwnsYUrOBiucxTc7NaC/Zp/tVc9OR1UPU02SIVskYAckhNyQS5JnXDyTF7IK3krfXhlb81b/7J6pcHOKvkxXuUTCZu7jw==</latexit> Denoised MDPs Figure 5: Policy optimization on RoboDesk. We give state-space SAC a less noisy reward so it can learn (see appendix). Figure 6: Performance of ﬁnetuning various encoders to infer joint position from RoboDesk image observation. Policy Learning: Backprop via Dynamics Policy Learning: SAC (Latent-Space) Denoised MDP TIA Dreamer Denoised MDP TIA Dreamer DBC PI-SAC (No Aug.) CURL (Use Aug.) State-Space SAC (Upper Bound) Noiseless 801.4 ± 96.6 769.7 ± 97.1 848.6 ± 137.1 587.1 ± 98.7 480.2 ± 125.5 575.4 ± 146.2 297.4 ± 72.5 246.4 ± 56.6 417.3 ± 183.2 910.3 ± 28.2 Video Background 597.7 ± 117.8 407.1 ± 225.4 227.8 ± 102.7 309.8 ± 153.0 318.1 ± 123.7 188.7 ± 78.2 188.0 ± 67.4 131.7 ± 20.1 478.0 ± 113.5 910.3 ± 28.2 Video Background + Noisy Sensor Video Background + Camera Jittering 563.1 ± 143.0 261.2 ± 200.4 212.4 ± 89.7 288.2 ± 123.4 197.3 ± 124.2 218.2 ± 58.1 79.9 ± 36.0 152.5 ± 12.6 354.3 ± 119.9 919.8 ± 100.7 254.1 ± 114.2 151.7 ± 160.5 98.6 ± 27.7 186.8 ± 47.7 126.5 ± 125.6 105.2 ± 33.8 68.0 ± 38.4 91.6 ± 7.6 390.4 ± 64.9 910.3 ± 28.2 Table 1: DMC policy optimization results. For each variant, we aggregate performance across three tasks (Cheetah Run, Walker Walk, Reacher Easy) by averaging. Denoised MDP performs well across all four variants with distinct noise types. Bold numbers show the best model-learning result for speciﬁc policy learning choices, or the best overall result. On Camera Jittering, Denoised MDP greatly outperforms all other methods except for CURL, which potentially beneﬁts from its speciﬁc data augmentation choice (random crop) on this task, and can be seen as using extra information (i.e., knowing the noise distractor form). In fact, Denoised MDP is the only method that consistently performs well across all tasks and noise variants, which can be seen from the full results in the appendix. on the same amount of data. In Figure 6, Denoised MDP representations lead to best converged solutions across a wide range of training set sizes, achieve faster training, and avoid overﬁtting when the training set is small. DBC, CURL and PI-SAC encoders, which take in stacked frames, are not directly comparable and thus absent from Figure 6. In the appendix, we compare them with running Denoised MDP encoder on each frame and concatenating the output fea- tures, where Denoised MDP handily outperforms both DBC and CURL by a large margin. 5.2. DeepMind Control Suite (DMC) To evaluate a diverse set of noise distractors, we consider four variants for each DMC task (see Figure 7 top row): • Noiseless: Original environment without distractors. • Video Background: Replacing noiseless background with natural videos (Zhang et al., 2020) (Ctrl + Rew). • Video Background + Sensor Noise: Imperfect sensors sensitive to intensity of a background patch (Ctrl+Rew). • Video Background + Camera Jittering: Shifting the observation by a smooth random walk (Ctrl + Rew). Denoised MDP consistently removes noise distractors. In Figure 7, TIA struggles to learn clean separations in many settings. Consistent with analysis in Section 2.3, it cannot handle Sensor Noise or Camera Jittering, as the former is reward-relevant noise that it cannot model, and the latter (although reward-irrelevant) cannot be represented by masking. Furthermore, it fails on Reacher Easy with Video Background, where the reward is given by the distance between the agent and a randomly-located ball. TIA encour- ages its noise latent to be independent of reward, but does not prevent it from capturing the controllable agent. These failures lead to either TIA trying to model everything as useful signals, or a badly-ﬁt model (e.g., wrong agent pose in the last column). In contrast, Denoised MDP separates out noise in all cases, obtaining a clean and accurate MDP (its Signal rows only have the agent moving). Denoised models consistently improve policy learning. We evaluate the learned policies in Table 1, where results are aggregated by the noise distractor variant. Other meth- ods, while sometimes handling certain noise types well, struggle to deal with all four distinct variants. TIA, as ex- pected, greatly underperforms Denoised MDP under Noisy Sensor or Camera Jittering. CURL, whose augmentation choice potentially helps handling Camera Jittering, under- performs in other three variants. In contrast, Denoised MDP State-Space SAC with Modified Reward Joint Position Regression Final Test MSE vs. Training Set Size Joint Position Regression Learning Curve for |Train Set|=104 Denoised MDPs Figure 7: Visualization of the different DMC variants and factorizations learned by TIA and Denoised MDP. E.g., bottom Noise row often shows a static agent but varying background, indicating that only the background is modeled as noises in Denoised MDP. Visualizations of full reconstructions are in appendix. See our website for clearer video visualizations. policies consistently perform well for all noisy variants and also the noiseless setting, regardless of the policy optimizer. Model-based approaches have a signiﬁcant lead over the model-free ones, as seen from the DBC results in Table 1 and the well-known fact that direct model-free learning on raw image observations usually fails (Laskin et al., 2020a; Kostrikov et al., 2020; Yarats et al., 2021). These results show that learning in a world model is useful, and that learning in a denoised world model is even better. 6. Implications In this work we explore learning denoised and compressed world models in the presence of environment noises. As a step towards better understanding of such noises, we categorize of information in the wild into four types (Sec- tion 2). This provides a framework to contrast and under- stand various methods, highlighting where they may be successful and where they will suffer (Section 2.3). Insights gained this way empirically agrees with ﬁndings from exten- sive experiments (Section 5). It can potentially assist better algorithm design and analysis of new MDP representation methods, as we have done in designing Denoised MDP (Section 3). We believe that this categorization will be a useful framework for investigation on learning under noises, revealing not just the (conceptual) success scenarios, but also the failure scenarios at the same time. Additionally, the framework can be readily extended with more sophisticated factorizations (Section 2.4), which can lead to correspond- ing Denoised MDP variants and/or new algorithms. Based on the framework, our proposed Denoised MDP nov- elly can remove all noise distractors that are uncontrollable or reward-irrelevant, in distinction to prior works. Empiri- cally, it effectively identiﬁes and removes a diverse set of noise types, obtaining clean denoised world models (Sec- tion 5). It may serve as an important step towards efﬁcient learning of general tasks in the noisy real world. Our ex- periments also highlight beneﬁts of cleanly denoised world models on both standard control tasks as well as non-control tasks. The success in both cases highlights the general use- fulness of such models. Given the generality of MDPs, this opens up the possibility of casting non-RL tasks as MDPs and automatically learn representations from denoised world models, as an alternative to manual feature engineering. Acknowledgements We thank Jiaxi Chen for the beautiful Figure 1 illustration. We thank Daniel Jiang and Yen-Chen Lin for their helpful comments and suggestions. We are grateful to the following organizations for providing computation resources to this project: IBM’s MIT Satori cluster, MIT Supercloud cluster, and Google Cloud Computing with credits gifted by Google to MIT. We are very thankful to Alex Lamb for suggestions and catching our typo in the conditioning of Equation (1). Cheetah Run Noiseless Reacher Easy Video Background Walker Walk Video Background + Noisy Sensor Cheetah Run Video Background + Camera Jittering Env. Rollout Obs. Reward TIA Denoised MDP Signal Noise Signal Noise Denoised MDPs References Agarwal, A., Kakade, S., Krishnamurthy, A., and Sun, FLAMBE: Structural complexity and represen- arXiv preprint W. tation learning of low rank mdps. arXiv:2006.10814, 2020. Bellemare, M., Dabney, W., Dadashi, R., Ali Taiga, A., Castro, P. S., Le Roux, N., Schuurmans, D., Lattimore, T., and Lyle, C. A geometric perspective on optimal representations for reinforcement learning. Advances in neural information processing systems, 32:4358–4369, 2019. Castro, P. S. Scalable methods for computing state similarity in deterministic markov decision processes. In Proceed- ings of the AAAI Conference on Artiﬁcial Intelligence, volume 34, pp. 10069–10076, 2020. Coates, A., Ng, A., and Lee, H. An analysis of single- layer networks in unsupervised feature learning. In Pro- ceedings of the fourteenth international conference on artiﬁcial intelligence and statistics, pp. 215–223, 2011. Craik, K. J. W. The nature of explanation, volume 445. CUP Archive, 1952. Dennett, D. C. Why the law of effect will not go away. Journal for the Theory of Social Behaviour, 1975. Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional activation feature for generic visual recognition. In Inter- national conference on machine learning, pp. 647–655. PMLR, 2014. Du, S., Krishnamurthy, A., Jiang, N., Agarwal, A., Dudik, M., and Langford, J. Provably efﬁcient rl with rich obser- vations via latent state decoding. In International Con- ference on Machine Learning, pp. 1665–1674. PMLR, 2019. Efroni, Y., Misra, D., Krishnamurthy, A., Agarwal, A., and Langford, J. Provable rl with exogenous distrac- tors via multistep inverse dynamics. arXiv preprint arXiv:2110.08847, 2021. Elman, J. L. Finding structure in time. Cognitive science, 14(2):179–211, 1990. Eysenbach, B., Salakhutdinov, R., and Levine, S. Robust predictable control. arXiv preprint arXiv:2109.03214, 2021. Ferns, N., Panangaden, P., and Precup, D. Metrics for ﬁnite markov decision processes. In UAI, volume 4, pp. 162– 169, 2004. Fu, X., Yang, G., Agrawal, P., and Jaakkola, T. Learning task informed abstractions. In International Conference on Machine Learning, pp. 3480–3491. PMLR, 2021. Gelada, C., Kumar, S., Buckman, J., Nachum, O., and Belle- mare, M. G. Deepmdp: Learning continuous latent space models for representation learning. In International Con- ference on Machine Learning, pp. 2170–2179. PMLR, 2019. Givan, R., Dean, T., and Greig, M. Equivalence notions and model minimization in markov decision processes. Artiﬁcial Intelligence, 147(1-2):163–223, 2003. Gu, S., Bao, J., Yang, H., Chen, D., Wen, F., and Yuan, L. Mask-guided portrait editing with conditional gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3436–3445, 2019. Ha, D. and Schmidhuber, J. World models. arXiv preprint arXiv:1803.10122, 2018. Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V., Zhu, H., Gupta, A., Abbeel, P., et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018. Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019a. Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. Learning latent dynamics for planning from pixels. In International Conference on Machine Learning, pp. 2555–2565. PMLR, 2019b. Hahn, D., Banzet, P., Bern, J. M., and Coros, S. Real2sim: Visco-elastic parameter estimation from dynamic motion. ACM Transactions on Graphics (TOG), 38(6):1–13, 2019. He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo- mentum contrast for unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019. Huh, M., Agrawal, P., and Efros, A. A. What makes arXiv preprint imagenet good for transfer learning? arXiv:1608.08614, 2016. Kannan, H., Hafner, D., Finn, C., and Erhan, D. RoboDesk: A multi-task reinforcement learning benchmark. https: //github.com/google-research/robodesk, 2021. Kim, S. W., Zhou, Y., Philion, J., Torralba, A., and Fidler, S. Learning to Simulate Dynamic Environments with GameGAN. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2020. Denoised MDPs Kostrikov, I., Yarats, D., and Fergus, R. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. arXiv preprint arXiv:2004.13649, 2020. Laskin, M., Lee, K., Stooke, A., Pinto, L., Abbeel, P., and Srinivas, A. Reinforcement learning with augmented data. Advances in Neural Information Processing Systems, 33: 19884–19895, 2020a. Laskin, M., Srinivas, A., and Abbeel, P. Curl: Contrastive unsupervised representations for reinforcement learning. In International Conference on Machine Learning, pp. 5639–5650. PMLR, 2020b. In International Conference on Machine Learning, pp. 5171–5180. PMLR, 2019. Puterman, M. L. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., USA, 1st edition, 1994. ISBN 0471619779. Rıza Alp Güler, Natalia Neverova, I. K. Densepose: Dense human pose estimation in the wild. 2018. Schaul, T., Horgan, D., Gregor, K., and Silver, D. Universal value function approximators. In International conference on machine learning, pp. 1312–1320. PMLR, 2015. Lee, A. X., Nagabandi, A., Abbeel, P., and Levine, S. Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. arXiv preprint arXiv:1907.00953, 2019. Smaira, L., Carreira, J., Noland, E., Clancy, E., Wu, A., and Zisserman, A. A short note on the kinetics-700-2020 human action dataset. arXiv preprint arXiv:2010.10864, 2020. Lee, K.-H., Fischer, I., Liu, A., Guo, Y., Lee, H., Canny, J., and Guadarrama, S. Predictive information accelerates learning in rl. Advances in Neural Information Processing Systems, 33:11890–11901, 2020. Lowe, D. G. Object recognition from local scale-invariant features. In Proceedings of the seventh IEEE interna- tional conference on computer vision, volume 2, pp. 1150– 1157. Ieee, 1999. Mahadevan, S. and Maggioni, M. Proto-value functions: A laplacian framework for learning representation and control in markov decision processes. Journal of Machine Learning Research, 8(10), 2007. Manuelli, L., Gao, W., Florence, P., and Tedrake, R. kpam: Keypoint affordances for category-level robotic manipu- lation. arXiv preprint arXiv:1903.06684, 2019. Mikolov, T., Chen, K., Corrado, G., and Dean, J. Efﬁcient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. Modi, A., Jiang, N., Tewari, A., and Singh, S. Sample com- plexity of reinforcement learning using linearly combined model ensembles. In International Conference on Artiﬁ- cial Intelligence and Statistics, pp. 2010–2020. PMLR, 2020. Oord, A. v. d., Li, Y., and Vinyals, O. Representation learn- ing with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T. Curiosity-driven exploration by self-supervised predic- tion. In ICML, 2017. Poole, B., Ozair, S., Van Den Oord, A., Alemi, A., and Tucker, G. On variational bounds of mutual information. Spelke, E. S. and Kinzler, K. D. Core knowledge. Develop- mental science, 10(1):89–96, 2007. Sutton, R. S. An adaptive network that constructs and uses and internal model of its world. Cognition and Brain Theory, 4(3):217–246, 1981. Sutton, R. S. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin, 2(4):160– 163, 1991. Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ Inter- national Conference on Intelligent Robots and Systems, pp. 5026–5033. IEEE, 2012. Tunyasuvunakool, S., Muldal, A., Doron, Y., Liu, S., Bohez, S., Merel, J., Erez, T., Lillicrap, T., Heess, N., and Tassa, Y. dm_control: Software and tasks for continuous control. Software Impacts, 6:100022, 2020. Wang, T. and Isola, P. Understanding contrastive represen- tation learning through alignment and uniformity on the hypersphere. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Pro- ceedings of Machine Learning Research, pp. 9929–9939. PMLR, 13–18 Jul 2020. Yarats, D., Fergus, R., Lazaric, A., and Pinto, L. Mastering visual continuous control: Improved data-augmented re- inforcement learning. arXiv preprint arXiv:2107.09645, 2021. Yen-Chen, L., Florence, P., Barron, J. T., Rodriguez, A., Isola, P., and Lin, T.-Y. iNeRF: Inverting neural radiance ﬁelds for pose estimation. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021. Denoised MDPs Zhang, A., McAllister, R., Calandra, R., Gal, Y., and Levine, S. Learning invariant representations for rein- forcement learning without reconstruction. arXiv preprint arXiv:2006.10742, 2020. A. Denoised MDP Discussions A.1. Loss Derivation Denoised MDPs To apply our mutual information regularizer I(x; s | a), we can consider a form using another variational distribution ρ (see, e.g., Poole et al. (2019)), I(x; s | a) = min ρ EaEpθ(s|a) [DKL(pθ(x | s, a) (cid:107) ρ(x | a))] ρ EaEqψ(s|a) [DKL(qψ(x | s, a) (cid:107) ρ(x | a))] ≈ min = min θ(cid:48) LKL-x(ψ, θ(cid:48)). (assume qψ is roughly the posterior of pθ) (5) The assumption that qψ is roughly the posterior of pθ is acceptable because it is the natural consequence of optimizing the variational MLE objective in Equation (1) over θ, ψ. Alternatively, we can consider the MI deﬁned by a joint conditional distribution P (x, s | a) not from the forward model pθ, but from the data distribution and posterior model qψ(x | s, a). This is also sensible because the variational MLE objective in Equation (1) optimizes for compatible pθ and qψ that both ﬁt data and consistently describe (conditionals of) the same underlying distribution. Thus regularizing either can encourage a low MI. This approach leads to exactly Equation (5), without approximation. Then, the total loss in Equation (3) from combining Equations (1) and (5) is given by min θ LMLE(θ) + c · I(x; s | a) = min θ,θ(cid:48),ψ Lrecon(θ, ψ) + LKL-x(θ, ψ) + LKL-y(θ, ψ) + LKL-z + c · +LKL-x(θ(cid:48), ψ) = min θ,ψ Lrecon(θ, ψ) + (1 + c) · LKL-x(θ, ψ) + LKL-y(θ, ψ) + LKL-z(θ, ψ). A.2. Discussions We discuss some algorithmic choices of Denoised MDP below. Speciﬁc implementation details (e.g., architectures) can be found at Appendix B.1.2. Posterior distributions of rx and ry. The pθ reward distributions pθ(rx | xt) and pθ(ry | yt) are modelled via Gaussians (as is done usually in world models, such as Dreamer (Hafner et al., 2019a)). By the transition structure of Denoised MDPs, these distributions are inherently independent. Recall that r = rx + ry. Therefore, we can easily compute the distribution of pθ(r | xt, yt) and its log likelihoods. This enables easy optimization of the variational MLE objective, without requiring the posterior model to also infer rx and ry from observed r subject to the addition relation. Partial observability. Sections 2 and 3 discussions are mostly based in the fully observable setting. Yet most benchmarks and real-world tasks are partially observable, e.g., robot joint speeds that can not be inferred from a single frame. Fortunately, the transition models used in Denoised MDP are fully capable of handle such cases, as long as the encoder qψ is not deterministic and the observation model pθ(s | . . . ) does not have the block structure (Du et al., 2019) (which would make x, y, z fully determined from s). In practice, we let both components to be generic conditional distributions (parameterized by regular deep neural networks). Therefore, Denoised MDP does not require full observability. The loss in Equation (4) has two hyperparameters: α ∈ (0, ∞) and β ∈ (0, 1). To maintain Hyperparameter choice. relative ratio with the observation reconstruction loss, we recommend scaling α roughly proportionally with dimensionality of the observation space, as is done in our experiments presented in this paper. A smaller β means stronger regularization. Therefore, β can be chosen based on training stability and the level of noise distractors in the task. B. Experiment Details All code (including code for our environment variants and code for our Denoised MDP method) will be released upon publication. Denoised MDPs Ctrl + Rew Ctrl + Rew Ctrl + Rew Noiseless Video Background DMC Video Background + Noisy Sensor Video Background + Camera Jittering Agent Agent Agent Agent — — — — — — Ctrl + Rew — Background Background — — Background, Jittering camera RoboDesk Agent, Button, Light on desk, Green hue of TV Blocks on desk, Handle on desk, Other movable objects TV content, Button sensor noise Jittering and ﬂickering environment lighting, Jittering camera Table 2: Categorization of various information in the environments we evaluated with. B.1. Implementation Details B.1.1. ENVIRONMENTS AND TASKS In all environments, trajectories are capped at 1000 timesteps. Table 2 shows a summary of what kinds of information exist in each environment. DeepMind Control Suite (DMC). Our Video Background implementation follows Deep Bisimulation for Control (Zhang et al., 2020) on most environments, using Kinetics-400 grayscale videos (Smaira et al., 2020), and replacing pixels where blue channel is strictly the greatest of three. This method, however, does not cleanly remove most of background in the Walker Walk environment, where we use an improved mask that replaces all pixels where the blue channel is among the greatest of three. For Camera Jittering, we shift the observation image according to a smooth random walk, implemented as, at each step, Gaussian-perturbing acceleration, decaying velocity, and adding a pulling force if the position is too far away from origin. For Sensor Noise, we select one sensor, and perturb it according to intensity of a patch of the natural video background (i.e., adding average patch value − 0.5). We perturb the speed sensor for Cheetah Run, the torso_height sensor for Walker Walk, and the normalized finger_to_target_dist sensor for Reacher Easy. These sensor values undergo non-linear (mostly piece-wise linear) transforms to compute rewards. While they can not be perfectly modelled by additive reward noise, such a model is usually sufﬁcient in most cases when the sensor values are not too extreme and stay in one linear region. RoboDesk. We modify the original RoboDesk environment by adding a TV screen and two neighboring desks. The TV screen places (continuously horizontally shifting) natural RGB videos from the Kinetics-400 dataset (Smaira et al., 2020). The environment has three light sources from the above, to which we added random jittering and ﬂickering. The viewing camera is placed further to allow better view of the noise distractors. Resolution is increased from 64 × 64 to 96 × 96 to compensate this change. Camera jittering is implemented by a 3D smooth random walk. Finally, the button sensor (i.e., detected value of how much the button is pressed) is also offset by a random walk. Each of the three button affects the corresponding light on the desk. Additionally, pressing the green button also shifts the TV screen content to a green hue. Following RoboDesk reward design, we reward the agent for (1) placing arm close to the button, (2) pressing the button, and (3) how green the TV screen content is. RoboDesk Joint Position Regression Datasets. To generate training and test set, we use four policies trained by state- space SAC at different stages of training (which is not related to any of the compared methods) and a uniform random actor, to obtain ﬁve policies of different qualities. For each policy, we sample 100 trajectories, each containing 1001 pairs (from 1000 interactions) of image observation and groundtruth joint position (of dimension 9). This leads to a total of 500.5 × 103 samples from each policy. From these, 100 × 103 samples are randomly selected as test set. Training sets of sizes 5 × 103, 10 × 103, 25 × 103, 50 × 103, 100 × 103, 150 × 103 are sampled from the rest. For all test sets and training sets, we enforce each policy to strictly contribute an equal amount. Denoised MDPs Operator Input Shape Kernel Size Stride Padding Operator Input Shape Kernel Size Stride Padding Input [3, 96, 96] — — — Input [input_size] Conv. + ReLU [k, 47, 47] Conv. + ReLU [2k, 22, 22] Conv. + ReLU [4k, 10, 10] Conv. + ReLU Conv. + ReLU [8k, 4, 4] [8k, 2, 2] 4 4 4 4 3 2 2 2 2 1 0 0 0 0 0 Reshape + FC [m] — — — FC + ReLU + Reshape Conv. Transpose + ReLU Conv. Transpose + ReLU [m, 1, 1] [4k, 3, 3] [4k, 9, 9] Conv. Transpose + ReLU [2k, 21, 21] Conv. Transpose + ReLU Conv. Transpose + ReLU [k, 46, 46] [3, 96, 96] — — 5 5 5 6 6 — — 2 2 2 2 2 — — 0 0 0 0 0 Table 3: Encoder architecture for 96 × 96-resolution observa- tion. The output of this encoder is then fed to other network for inferring posteriors. m and k are two architectural hyper- parameters. m controls the output size (unrelated to the actual latent variable sizes). k controls the network width. Table 4: Decoder architecture for 96 × 96-resolution observation. m and k are two architectural hyperparameters. m controls width the fully connected part. k controls width of the convolutional part. They are the same values as in Table 3. B.1.2. MODEL LEARNING METHODS For all experiments, we let the algorithms use 106 environment steps. For PI-SAC and CURL, we follow the original implementations (Laskin et al., 2020b; Lee et al., 2020) and use an action repeat of 4 for Cheetah Run and Reacher Easy, and an action repeat of 2 for Walker Walk. For Denoised MDP, Dreamer, TIA and DBC, we always use an action repeat of 2, following prior works (Hafner et al., 2019a; Fu et al., 2021; Zhang et al., 2020). Denoised MDP, Dreamer, and TIA. Both Dreamer and TIA use the same training schedule and the Recurrent State-Space Model (RSSM) as the base architecture (Hafner et al., 2019b). Following them, Denoised MDP also uses these components, and follow the same preﬁlling and training schedule (see Dreamer (Hafner et al., 2019b) for details). These three model learning methods take in 64 × 64 RGB observations for DMC, and 96 × 96 RGB observations for RoboDesk. Dreamer only implements encoder and decoder for the former resolution. To handle the increased resolution, we modify the 64 × 64 architectures and obtain convolutional encoder and decoder shown in Tables 3 and 4. For fair comparison, we ensure that each method has roughly equal number of parameters by using different latent variable sizes, encoder output sizes (m of Table 3) and convolutional net widths (k of Table 4). Details are shown in Table 5. KL clipping (free nats). For Denoised MDP, we follow Dreamer (Hafner et al., 2019b;a) and TIA (Fu et al., 2021), and allow 3 free nats for the LKL-x term. In other words, for each element of a batch, we do not optimize the KL term if it is less than 3 (e.g., implemented via clipping). However, we do not allow this for the LKL-y and LKL-z terms, as these variables are to be discarded and information is not allowed to hide in them unless permitted by the structure. An alternative strategy, + (1 − β) · LKL-x which we ﬁnd also empirically effective, is to consider LKL-x = β · LKL-x , and to allow free nats only for (cid:124) (cid:125) (cid:123)(cid:122) (cid:124) (cid:125) (cid:123)(cid:122) VAE KL term MI regularizer term the ﬁrst term that is a part of the variational model ﬁtting objective. All results presented in this paper use the ﬁrst strategy. Both strategies are implemented in our open source code repository: github.com/facebookresearch/denoised_mdp. B.1.3. POLICY OPTIMIZATION ALGORITHMS USED WITH MODEL LEARNING Backpropagate via Dynamics. We use the same setting as Dreamer (Hafner et al., 2019a), optimizing a λ-return over 15-step-long rollouts with λ = 0.95, clipping gradients with norm greater than 100. TIA uses the same strategy, except that it groups different models together for gradient clipping. We strictly follow the ofﬁcial TIA implementation. Latent-Space SAC. We use the regular SAC with automatic entropy tuning, without gradient clipping. This works well for almost all settings, except for Walker Walk variant of DMC, where training often collapses after obtaining good return, regardless of the model learning algorithm. To address instability in this case, we reduce learning rates from 3 × 10−4 to 1 × 10−4 and clip gradients with norm greater than 100 for all latent-space SAC run on these variants. Denoised MDPs DMC RoboDesk Latent Sizes Dreamer TIA Denoised MDP (220 + 33) (120 + 20) + (120 + 20) (120 + 20) + (120 + 20) m 1024 490 1024 k 32 24 32 Total Number of Parameters Latent Sizes 7,479,789 7,475,567 7,478,826 (220 + 33) (120 + 20) + (120 + 20) (120 + 20) + (120 + 20) m 1024 490 1024 k 32 24 32 Total Number of Parameters 6,385,511 6,384,477 6,384,248 Table 5: The speciﬁc architecture parameters for model learning methods. Since RSSM uses a deterministic part and a stochastic part to represent each latent variable, we use (deterministic_size + stochastic_size) to indicate size of a latent variable. TIA and Denoised MDP have more than one latent variable. Note that while TIA has lower m and k, it has multiple encoder and decoders, whereas Dreamer and Denoised MDP only have one encoder and one decoder. The total number of parameters is measured with the actor model, but without any additional components from policy optimization algorithm (e.g., critics in SAC). Total number of parameters is lower for RoboDesk as the encoder and decoder architecture is narrower than those of DMC for the purpose of reducing memory usage, despite with a higher resolution. B.1.4. MODEL-FREE METHODS DBC. For DMC, we used 84 × 84-resolution observation following original work (even though other methods train on 64 × 64-resolution observations). For RoboDesk, DBC uses the encoder in Table 3 for 96 × 96-resolution observation, for fair comparison with other methods. Following the original work, we stack 3 consecutive frames to approximate the required full observability. In the robot arm joint position regression experiment Section 5.1, DBC encoders also see stacked observations. For DMC evaluations, we use the data provided by Zhang et al. wherever possible, and run the ofﬁcial repository for other cases. State-Space SAC. The state space usually contains robot joint states, including position, velocity, etc. For DMC, when Sensor Noise is present, this is not the true optimal state space, as we do not supply it with the noisy background that affects the noisy reward. However, it still works well in practice. For RoboDesk, the TV’s effect on reward is likely stronger and direct state-space SAC fails to learn. Since this evaluation is to obtain a rough “upper bound”, we train state-space SAC with a modiﬁed reward with less noise— the agent is rewarded by pressing the button, independent of the TV content. This still encourages the optimal strategy of the task allows achieving good policies. B.1.5. NON-RL METHODS Contrastive Learning. We used the Alignment+Uniformity contrastive learning loss from Wang & Isola (2020). The hyperparameters and data augmentations strictly follow their experiments on STL-10 (Coates et al., 2011), which also is of resolution 96 × 96. The exact loss form is Lalign(α = 2) + Luniform(t = 2), a high-performance setting for STL-10. B.2. Compute Resources All our experiments are run on a single GPU, requiring 8GB memory for DMC tasks, and 16GB memory for RoboDesk tasks. We use NVIDIA GPUs of the following types: 1080 Ti, 2080 Ti, 3080 Ti, P100, V100, Titan XP, Titan RTX. For MuJoCo (Todorov et al., 2012), we use the EGL rendering engine. Training time required for each run heavily depends on the CPU speciﬁcation and availability. In general, a Denoised MDP run needs 12 ∼ 36 hours on DMC and 24 ∼ 50 hours on RoboDesk. TIA uses about 1.5× of these times, due to the adversarial losses. For a comparison between the two Denoised MDP variants, running the same DMC task on the same machine, the Figure 2b variant used 23 hours while the Figure 2c variant used 26 hours. B.3. Visualization Details Visualizations of components in learned models. We use different methods to visualize signal and noise information learned by TIA and Denoised MDP in Figures 4 and 7. For TIA, we used the reconstructions from the two latent (before mask-composing them together as the full reconstruction). For Denoised MDP, we only have one decoder (instead of three for TIA), and thus we decode (xt, const) and (const, yt) to visualize information contained in each variable, with const chosen by visual clarity (usually as value of the other variable at a ﬁxed timestep). Due to the fundamental different ways to obtain these visualizations, in DMC, TIA can prevent the agent from showing up in noise visualizations, while Denoised Denoised MDPs Figure 8: Effect of weight decay on RoboDesk joint position regres- sion. The curves show ﬁnal test MSE for various training set sizes. Weight decay generally helps when ﬁnetuning from a pretrained encoder, but hurts when training from scratch. Figure 9: Performance of all TIA settings on RoboDesk joint position regression. Only using the signal encoder is necessary for good performance. Figure 10: Training curve comparisons for the RoboDesk joint position regression task across many training set sizes. MDP cannot. However, as stated in Section 5.2, our focus should be on what evolves/changes in these images, rather than what is visually present, as static components are essentially not modelled by the corresponding transition dynamics. Visualizations in Figures 4 and 7 use trajectories generated by a policy trained with state-space SAC. To obtain diverse behaviors, policy outputs are randomly perturbed before being used as actions. From the same trajectory, we use the above described procedure to obtain visualizations. The speciﬁc used trajectory segments are chosen to showcase both the modiﬁed environment and representative behavior of each method. Please see the supplementary video for clearer visualizations. B.4. RoboDesk Result Details Environment modiﬁcations. The agent controls a robotic arm placed in front of a desk and a TV, and is tasked to push down the green button on the desk, which turns on a small green light and makes the TV display have a green hue. The intensity of the TV image’s green channel is given to the agent as part of their reward, in addition to distance between the arm to the button, and how much the button is pressed. Additionally, the environment contains other noise distractors, including moveable blocks on the desk (Ctrl + Rew), ﬂickering environment light and camera jittering (Ctrl + Rew), TV screen hue (Ctrl + Rew), TV content (Ctrl + Rew), and noisy button sensors (Ctrl + Rew). RoboDesk has roughly twice as many pixels as DMC has. For Denoised MDP, we Denoised MDP hyperparameters. scale α with the observation space dimensionality (see Section 3) and use α = 2, with a ﬁxed β = 0.125. When using the alternative KL free nats strategy discussed in Appendix B.1.2 (results not shown in paper), we ﬁnd α = 1 and β = 0.25 also effective. TIA hyperparameters. We follow recommendations in the TIA paper, setting λRadv = 25,000 to match reconstruction loss in magnitude, and setting λOs = 2 where training is stable. B.4.1. ROBOT ARM JOINT POSITION REGRESSION. Training details. For this task, we jointly train the pre-trained backbone and a three-layer MLP head that has 256 hidden units at each layer, with a learning rate of 8 × 10−5. For ﬁnetuning from pretrained encoders, we follow common ﬁnetuning practice and apply a weight decay of 3 × 10−5 whenever it is helpful (all cases except CURL and training from scratch). E S M 10 1 t e S t s e T 10 2 E S M 10 1 t e S t s e T 10 2 Denoised MDP Dreamer TIA Contrastive With Weight Decay No Weight Decay From Scratch DBC (Stacked Frames) CURL (Stacked Frames) PI-SAC without Augmentation (Stacked Frames) 0.5 1.0 1.5 0.5 1.0 1.5 0.5 1.0 1.5 0.5 1.0 1.5 Training Set Size1e5 Training Set Size1e5 Training Set Size1e5 Training Set Size1e5 TIA With Weight Decay + Only Signal Encoder No Weight Decay + Only Signal Encoder With Weight Decay + Both Encoders No Weight Decay + Both Encoders 103 102 101 100 E S M t e S t s e T 10 1 10 2 0.2 0.4 0.6 0.8 1.0 Training Set Size 1.2 1.4 1e5 Learning Curve for 104 Training Samples Denoised MDP TIA Dreamer Contrastive From Scratch 0.25 0.24 0.23 0.22 0.21 0.20 0.19 0.18 0 20 40 60 80 100 Training Epoch Learning Curve for 150 × 104 Training Samples 0.40 0.30 0.20 0.100 0.09 0 20 40 60 80 100 Training Epoch Learning Curve for 50 × 104 Training Samples 0.24 0.22 0.20 0.18 0.16 0.14 0.12 0.100 0 20 40 60 80 100 Training Epoch Learning Curve for 150 × 104 Training Samples 0.20 0.100 0.09 0.08 0.07 0.06 0.05 0.04 0 20 40 60 80 100 Training Epoch Denoised MDPs Figure 11: Performance comparison of ﬁnetuning from Denoised MDP encoders and frame-stacked encoders that take in 3 consec- utive frames. For Denoised MDP and training from scratch, the encoders take in only a single frame and are applied for each of the frame, with output concatenated together before feeding to the prediction head. Figure 12: Performance of all DBC settings on RoboDesk joint position regression. Using the output features (after layer normal- ization) is necessary for good performance. Figure 13: Performance of all CURL settings on RoboDesk joint position regression. Using the output features (after layer normal- ization) is necessary for good performance. Figure 14: Performance of all PI-SAC settings on RoboDesk joint position regression. Using the activations before layer normaliza- tion gives best performance. See Figure 8 for comparisons for weight decay options over all methods. • For model-based RL, we take encoders trained with backpropagating via dynamics as the policy optimization algorithm. • In training the contrastive encoder, for a (more) fair comparison with RL-trained encoders that are optimized over 106 environment steps, we train contrastive encoders on 106 samples, obtained in the exact same method of the training sets of this task. In a sense, these contrastive encoders have the advantage of training on the exact same distribution, and seeing more samples (since RL-trained encoders use action repeat of 2 and thus only ever see 0.5 × 106 samples). • TIA has two sets of encoders. Using concatenated latents from both unfortunately hurts performance greatly (see Figure 9). So we use only the encoder for the signal latent. We also compare training speeds over a wide range of training set sizes in Figure 10. Denoised MDP encoders lead to faster and better training in all settings. Additional comparison with frame-stacking encoders. Other pretrained encoders (DBC, CURL and PI-SAC) take in stacked 3 consecutive frames, and are not directly comparable with the other methods. To compare, we also try running Denoised MDP encoders on the 3 consecutive frames, whose feature vector is concatenated before feeding into the head. The result in Figure 11 shows that our encoder outperforms all but PI-SAC encoders. Finally, for DBC, CURL and PI-SAC, we attempted evaluating intermediate features, features before the ﬁnal layer normalization, and the output space, and ﬁnd the last option best-performing for DBC and CURL, and the second option best-performing for PI-SAC (see Figures 12 to 14). Therefore, we use these respective spaces, which arguably gives a further edge to these methods, as we essentially tune this additional option on test results. Notably, these respective choices are often the only one achieving relatively good performance, highlighting the necessity of tuning for these methods. 10 1 E S M t e S t s e T 10 2 Ours (Stacked Frames) Ours (Single Frame) DBC (Stacked Frames) PI-SAC (Stacked Frames) CURL (Stacked Frames) From Scratch (Stacked Frames) 0.2 0.4 0.6 0.8 Training Set Size 1.0 1.2 1.4 1e5 DBC With Weight Decay, Output Features No Weight Decay, Output Features With Weight Decay, No Layer Norm No Weight Decay, No Layer Norm With Weight Decay, Conv Features No Weight Decay, Conv Features 103 102 101 100 E S M t e S t s e T 10 1 0.2 0.4 0.6 0.8 1.0 Training Set Size 1.2 1.4 1e5 CURL With Weight Decay, Output Features No Weight Decay, Output Features With Weight Decay, No Layer Norm No Weight Decay, No Layer Norm With Weight Decay, Conv Features No Weight Decay, Conv Features 103 102 101 100 E S M t e S t s e T 10 1 10 2 0.2 0.4 0.6 0.8 Training Set Size 1.0 1.2 1.4 1e5 PI-SAC 10 1 E S M t e S t s e T 10 2 With Weight Decay, Output Features (Stacked Frames) No Weight Decay, Output Features (Stacked Frames) With Weight Decay, No Layer Norm (Stacked Frames) No Weight Decay, No Layer Norm (Stacked Frames) With Weight Decay, Conv Features (Stacked Frames) No Weight Decay, Conv Features (Stacked Frames) 0.2 0.4 0.6 0.8 Training Set Size 1.0 1.2 1.4 1e5 B.5. DeepMind Control Suite (DMC) Result Details Denoised MDPs Full policy optimization results. Figure 15 presents the full results on each DMC environment (task + variant). For environment, a comparison plot is made based on which policy learning algorithm is used with the model learning method (with model-free baselines duplicated in both). Such separation is aimed to highlight the performance difference caused by model structure (rather than policy learning algorithm). Across most noisy environments, Denoised MDP performs the best. It also achieves high return on noiseless environments. Visualization of learned models. Figure 16 is the extended version of Figure 7 in main text, with full reconstructions from all three models. Please see the supplementary video for clearer visualizations. Comparison between Denoised MDP variants. We compare the two Denoised MDP variants based Figures 2b and 2c on Cheetah Run environments with policy trained by packpropagating via learned dynamics. The comparison is shown in the top row of Figure 15, where we see the Figure 2b variant often performing a bit better. We hypothesize that this may due to the more complex prior and posterior structure of Figure 2c, which may not learn as efﬁciently. This also makes Figure 2c variant needing longer (wall-clock) time to optimize, as mentioned above in Appendix B.2. TIA hyperparameters and instability. We strictly follow recommendations of the original paper, and use their suggested value for each DMC task. We also note that TIA runs sometimes collapse during training, leading to sharp drops in rewards. After closely inspecting the models before and after collapses, we note that in many cases, such collapses co-occur with sudden spikes in TIA’s reward disassociation loss, which is implemented as an adversarial minimax loss, and the noise latent space instantly becomes degenerate (i.e., not used in reconstruction). We hypothesize that this adversarial nature can cause training instability. However, a few collapses do not co-occur with such loss spikes, which maybe alternatively due to that TIA model structure cannot model the respective noise types and that better ﬁtting the model naturally means a degenerate noise latent space. PI-SAC hyperparameters. For each task, we use the hyperparameters detailed in the original paper (Lee et al., 2020). PI-SAC is usually run with augmentations. However, unlike CURL, augmentation is not an integral part of the PI-SAC algorithm and is completely optional. For a fair comparisons with other methods and to highlight the effect of the predictive information regularizer, the main mechanism proposed by PI-SAC, we do not use augmentations for PI-SAC. Denoised MDP hyperparameters. For DMC, we always use ﬁxed α = 1. β can be tune according to amount of noises in environment, and to training stability. In Figure 17, we compare effects of choosing different β’s. On noiseless environments, larger β (i.e., less regularization) performs often better. Whereas on noisy environments, sometimes stronger regularization can boost performance. However, overall good performance can be obtained by usually several β values. In Table 6, we summarize our β choices for each environment in Table 6. Denoised MDPs Figure 15: Policy optimization results on DMC. Each plot focuses on a single task variant, showing total episode return versus environment steps taken. For three model-based approaches, we use two policy optimization choices to train on the learned model: (top half) backpropagate via learned dynamics and (bottom half) SAC on the learned MDP. We also compare with DBC, a model-free baseline. For an “upper bound” (not plotted due to presentation clarity), SAC on true state-space (i.e., optimal representation) in 106 environment steps reaches episode return ≈ 800 on Cheetah Run variants, ≈ 980 on Walker Walk variants, and ≈ 960 on Reacher Easy variants. CURL’s speciﬁc augmentation choice (random crop) potentially helps signiﬁcantly for Reacher Easy (where the reacher and the target appear in random spatial locations) and Camera Jittering. However, unlike Denoised MDP, it does not generally perform well across all environments and noise variants. n o i t a z i m i t p O y c i l o P e t a g a p o r p k c a B s c i m a n y D a v i n o i t a z i m i t p O y c i l o P ) e c a p S - t n e t a L ( C A S Denoised MDPs Figure 16: Complete visualization of the different DMC variants and factorizations learned by TIA and Denoised MDP. In addition to visualizations of Figure 7, we also visualize full reconstructions from Dreamer, TIA, and Denoised MDP. Cheetah Run Noiseless Reacher Easy Video Background Walker Walk Video Background + Noisy Sensor Cheetah Run Video Background + Camera Jittering Env. Rollout Obs. Reward Dreamer Recon. Recon. TIA Signal Noise Recon. Denoised MDP Signal Noise Denoised MDPs Figure 17: Effect of choosing β in Denoised MDP on DMC policy optimization results. Setting β = 1 disables regularization and is only run on noiseless variants. Noiseless Video Background Video Background + Noisy Sensor Video Background + Camera Jittering Policy Learning: Backprop via Dynamics Policy Learning: SAC (Latent-Space) Cheetah Run Walker Walk Reacher Easy Cheetah Run Walker Walk Reacher Easy 1 1 1 1 1 1 0.125 0.25 0.25 0.125 0.25 0.125 0.25 0.25 0.25 0.125 0.125 0.25 0.25 0.5 0.25 0.25 0.5 0.25 Table 6: β choices for Denoised MDP results shown in Table 1 and Figure 15. We choose β = 1 (i.e., disabling regularization) for all noiseless environments, and tuned others. However, as seen in Figure 17, the results often are not too sensitive to small β changes. n o i t a z i m i t p O y c i l o P e t a g a p o r p k c a B s c i m a n y D a v i n o i t a z i m i t p O y c i l o P ) e c a p S - t n e t a L ( C A S"
465,         We’re Training AI Twice as Fast This Year as Last     ,https://spectrum.ieee.org/mlperf-rankings-2022,2022-06-30,"New MLPerf rankings show training times plunging Google’s cloud based TPU v4 Pods turned in some impressive results. According to the best measures we’ve got, a set of benchmarks called MLPerf, machine-learning systems can be trained nearly twice as quickly as they could last year. It’s a figure that outstrips Moore’s Law, but also one we’ve come to expect. Most of the gain is thanks to software and systems innovations, but this year also gave the first peek at what some new processors, notably from Graphcore and Intel subsidiary Habana Labs, can do. The once-crippling time it took to train a neural network to do its task is the problem that launched startups like Cerebras and SambaNova and drove companies like Google to develop machine-learning accelerator chips in house. But the new MLPerf data shows that training time for standard neural networks has gotten a lot less taxing in a short period of time. And that speedup has come from much more than just the advance of Moore’s Law. Neural networks can now be trained much faster than what you would expect just from the march of Moore’s Law.ML Commons This capability has only incentivized machine-learning experts to dream big. So the size of new neural networks continues to outpace computing power. Called by some “the Olympics of machine learning,” MLPerf consists of eight benchmark tests: image recognition, medical-imaging segmentation, two versions of object detection, speech recognition, natural-language processing, recommendation, and a form of gameplay called reinforcement learning. (One of the object-detection benchmarks was updated for this round to a neural net that is closer to the state of the art.) Computers and software from 21 companies and institutions compete on any or all of the tests. This time around, officially called MLPerf Training 2.0, they collectively submitted 250 results. Very few commercial and cloud systems were tested on all eight, but Nvidia director of product development for accelerated computing Shar Narasimhan gave an interesting example of why systems should be able to handle such breadth: Imagine a person with a smartphone snapping a photo of a flower and asking the phone: “What kind of flower is this?” It seems like a single request, but answering it would likely involve 10 different machine-learning models, several of which are represented in MLPerf. To give a taste of the data, for each benchmark we’ve listed the fastest results for commercially available computers and cloud offerings (Microsoft Azure and Google Cloud) by how many machine-learning accelerators (usually GPUs) were involved. Keep in mind that some of these will be a category of one. For instance, there really aren’t that many places that can devote thousands of GPUs to a task. Likewise, there are some benchmarks where systems beat their nearest competitor by a matter of seconds or where five or more entries landed within a few minutes of each other. So if you’re curious about the nuances of AI performance, check out the complete list. [Or, click here to skip past the data and hear about some of the new stuff from Google, Graphcore, Intel, and Hazy Research. We won’t judge.] As usual, systems built using Nvidia A100 GPUs dominated the results. Nvidia’s new GPU architecture, Hopper, was designed with architectural features aimed at speeding training. But it was too new for this set of results. Look for some systems based on the Hopper H100–based systems in upcoming contests. For Nvidia’s take on the results see this blog post. Google’s TPU v4 offers a three-fold improvement in computations per watt over its predecessor, the company says. Google noted that two of its tests were done using what it calls a “full TPU v4 pod”—a system consisting of 4,096 chips, for a total of up to 1.1 billion billion operations per second. At that scale, the system ripped through the image-recognition and natural-language-processing trainings in just over 10 seconds each. Because it’s a cloud service, Google’s machine-learning system is available around the world. But the company wants you to know that the machines are actually located in Oklahoma. Why? Because that’s where it’s built a data center that operates on 90 percent carbon-free energy. For Google’s take on its results see the Google Cloud’s blog post. Graphcore presented the first performance results from computers built with its new Bow IPU. Bow is the first commercial processor built by stacking two silicon wafers atop each other. In its current iteration, one of the chips in the stack does no computing; instead, it delivers power in such a way that the chip runs up to 40 percent faster using as much as 16 percent less energy compared to its predecessor. MLPerf 2.0 was the first opportunity to see how that translated to real neural nets. The chips didn’t disappoint, showing a 26 to 31 percent speedup for image recognition and a 36 to 37 percent boost at natural-language processing. Graphcore executives say to expect more. The company is planning a future IPU, where both chips in the 3D stack do computing. Such a future processor would be combined into an 8,192-IPU supercomputer called the Good Computer, capable of handling neural networks 1,000 times or more as large as today’s biggest language models. Graphcore also touted what’s essentially a beneficial nonresult. Neural networks are constructed using frameworks that make the development job way easier. Pytorch and Tensorflow are commonly used open-source frameworks in North America and Europe, but in China Baidu’s PaddlePaddle is popular, according Graphcore executives. Seeking to satisfy clients and potential customers there, they showed that using PaddlePaddle or Graphcore’s in-house framework popART makes essentially no difference to training time. Intel subsidiary Habana Labs’ Gaudi2 put up some winning systems in this set of results. And Habana’s Eitan Medina says to expect a lot more from Gaudi2 in future tests. The company didn’t have time to put all of Gaudi2’s architectural features through their paces in time for this round. One possibly important feature is the use the of low-precision numbers in parts of the training process, similar to what Nvidia’s H100 promises. “With Gaudi2, there’s still lots of performance to squeeze out,” says Medina. Performing MLPerf benchmarks is no easy task, and often involves the work of many engineers. But a single graduate student, with some consultation, can do it, too. Tri Dao was that graduate student. He’s member of Hazy Research, the nom de guerre of Chris Re’s laboratory at Stanford. (Re is one of the founders of AI giant SambaNova.) Dao, Re, and other colleagues came up with a way to speed up the training of so-called attention-based networks, also called transformer networks. Among the MLPerf benchmarks, the natural-language-processing network BERT is the transformer, but the concept of “attention” is at the heart of very large language models such as GPT3. And it’s starting to show up in other machine-learning applications, such as machine vision. In attention networks, the length of the sequence of data the network works on is crucial to its accuracy. (Think of it as how many words a natural-language processor is aware of at once or how large an image a machine vision system can look at.) However, that length doesn’t scale up well. Double its size and you’re quadrupling the scale of the attention layer of the network, Dao explains. And this scaling problem shows up in training time because of all the instances when the network needs to write to system memory. Dao and his colleagues came up with an algorithm that gives the training process an awareness of this time penalty and a way to minimize it. Dao applied the lab’s “flash attention” algorithm to BERT using an 8-GPU system in the Microsoft Azure cloud, shaving almost 2 minutes (about 10 percent) off of Microsoft’s best effort. “Chris [Re] calls MLPerf ‘the Olympics of machine-learning performance.’ ” says Dao. And “even on the most competitive benchmark we were able to give a speedup.” Look for Re’s group to put flash attention to use in other transformer models soon. For more on the algorithm see their blog post about it. Computer-vision systems use shapes, colors, and even labels to identify materials at superhuman speeds It’s Tuesday night. In front of your house sits a large blue bin, full of newspaper, cardboard, bottles, cans, foil take-out trays, and empty yogurt containers. You may feel virtuous, thinking you’re doing your part to reduce waste. But after you rinse out that yogurt container and toss it into the bin, you probably don’t think much about it ever again. The truth about recycling in many parts of the United States and much of Europe is sobering. Tomorrow morning, the contents of the recycling bin will be dumped into a truck and taken to the recycling facility to be sorted. Most of the material will head off for processing and eventual use in new products. But a lot of it will end up in a landfill."
466,         Meta’s AI Takes an Unsupervised Step Forward     ,https://spectrum.ieee.org/unsupervised-learning-meta,2022-06-29,"In the quest for human-level intelligent AI, Meta is betting on self-supervised learning Meta AI’s masked auto-encoder for computer vision was trained on images that were mostly obscured [left]. Yet its reconstructions [center] were remarkably close to the original images [right]. Meta’s chief AI scientist, Yann LeCun, doesn’t lose sight of his far-off goal, even when talking about concrete steps in the here and now. “We want to build intelligent machines that learn like animals and humans,” LeCun tells IEEE Spectrum in an interview. Today’s concrete step is a series of papers from Meta, the company formerly known as Facebook, on a type of self-supervised learning (SSL) for AI systems. SSL stands in contrast to supervised learning, in which an AI system learns from a labeled data set (the labels serve as the teacher who provides the correct answers when the AI system checks its work). LeCun has often spoken about his strong belief that SSL is a necessary prerequisite for AI systems that can build “world models” and can therefore begin to gain humanlike faculties such as reason, common sense, and the ability to transfer skills and knowledge from one context to another. The new papers show how a self-supervised system called a masked auto-encoder (MAE) learned to reconstruct images, video, and even audio from very patchy and incomplete data. While MAEs are not a new idea, Meta has extended the work to new domains. By figuring out how to predict missing data, either in a static image or a video or audio sequence, the MAE system must be constructing a world model, LeCun says. “If it can predict what’s going to happen in a video, it has to understand that the world is three-dimensional, that some objects are inanimate and don’t move by themselves, that other objects are animate and harder to predict, all the way up to predicting complex behavior from animate persons,” he says. And once an AI system has an accurate world model, it can use that model to plan actions. “Images, which are signals from the natural world, are not constructed to remove redundancy. That’s why we can compress things so well when we create JPGs.”—Ross Girshick, Meta “The essence of intelligence is learning to predict,” LeCun says. And while he’s not claiming that Meta’s MAE system is anything close to an artificial general intelligence, he sees it as an important step. Not everyone agrees that the Meta researchers are on the right path to human-level intelligence. Yoshua Bengio is credited, in addition to his co–Turing Award winners LeCun and Geoffrey Hinton, with the development of deep neural networks, and he sometimes engages in friendly sparring with LeCun over big ideas in AI. In an email to IEEE Spectrum, Bengio spells out both some differences and similarities in their aims. “I really don’t think that our current approaches (self-supervised or not) are sufficient to bridge the gapto human-level intelligence,” Bengio writes. He adds that “qualitative advances” in the field will be needed to really move the state of the art anywhere closer to human-scale AI. While he agrees with LeCun that the ability to reason about the world is a key element of intelligence, Bengio’s team isn’t focused on models that can predict, but rather those that can render knowledge in the form of natural language. Such a model “would allow us to combine these pieces of knowledge to solve new problems, perform counterfactual simulations, or examine possible futures,” he notes. Bengio’s team has developed a new neural-net framework that has a more modular nature than those favored by LeCun, whose team is working on end-to-end learning (models that learn all the steps between the initial input stage and the final output result). Meta’s MAE work builds on a trend toward a type of neural network architecture called transformers. Transformers were first adopted in natural-language processing, where they caused big jumps in performance for models like Google’s BERT and OpenAI’s GPT-3. Meta AI researcher Ross Girshick says that transformers’ success with language caused people in the computer-vision community to “work feverishly to try to replicate those results” in their own field. Meta’s researchers weren’t the first to successfully apply transformers to visual tasks; Girshick says that Google research on a Vision Transformer (ViT) inspired the Meta team. “By adopting the ViT architecture, it eliminated obstacles that had been standing in the way of experimenting with some ideas,” he tells Spectrum. Girshick coauthored Meta’s first paper on MAE systems, which dealt with static images. Its training was analogous to how BERT and other language transformers are trained. Such language models are shown huge databases of text with some fraction of the words missing, or “masked.” The models try to predict the missing words, and then the missing text is unmasked so the models can check their work, adjust their parameters, and try again with a new chunk of text. To do something similar with vision, Girshick explains, the team broke up images into patches, masked some of the patches, and asked the MAE system to predict the missing parts of the images. One of the team’s breakthroughs was the realization that masking a large proportion of the image gave the best results—a key difference from language transformers, where perhaps 15 percent of the words might be masked. “Language is an extremely dense and efficient communication system,” says Girshick. “Every symbol has a lot of meaning packed in. But images, which are signals from the natural world, are not constructed to remove redundancy. That’s why we can compress things so well when we create JPG images,” he notes. Meta AI researchers experimented with how much of the images to mask to get the best results.Meta By masking more than 75 percent of the patches in an image, Girshick explains, they remove the redundancy from the image that would otherwise make the task too trivial for training. Their two-part MAE system first uses an encoder that learns relationships between pixels across the training data set, then a decoder does its best to reconstruct original images from the masked versions. After this training regimen is complete, the encoder can also be fine-tuned for vision tasks such as classification and object detection. “The reason why ultimately we’re excited is the results we see in transfer learning to downstream tasks,” says Girshick. When using the encoder for tasks such as object recognition, he says, “we’re seeing gains that are really substantial; they move the needle.” He notes that scaling up the model led to better performance, which is a promising sign for future models because SSL “has the potential to use a lot of data without requiring manual annotation.” Going all-in for learning on massive uncurated data sets may be Meta’s tactic for improving results in SSL, but it’s also an increasingly controversial approach. AI ethics researchers such as Timnit Gebru have called attention to the biases inherent in the uncurated data sets that large language models learn from, sometimes with disastrous results. In the MAE system for video, the masking obscured up to 95 percent of each video frame, because the similarities between frames meant that video signals have even more redundancy than static images. One big advantage of the MAE approach when it comes to video, says Meta researcher Christoph Feichtenhofer, is that video is typically very computationally demanding. But by masking up to 95 percent of each frame, MAE reduces the computational cost by up to 95 percent, he says. The clips used in these experiments were only a few seconds long, but Feichtenhofer says that training an AI system on longer videos is “a very active research topic.” Imagine, he says, a virtual assistant who has a video feed of your house and can tell you where you left your keys an hour ago. (Whether you consider that possibility amazing or creepy, rest assured that it’s quite far off.) More immediately, one can imagine both the image and video systems being useful for the kind of classification tasks that are needed for content moderation on Facebook and Instagram, and Feichtenhofer says that “integrity” is one possible application. “We are definitely talking to product teams,” he says, “but it’s very new, and we don’t have any concrete projects yet.” For the audio MAE work, which the team says will soon be posted on the arXiv preprint server, the Meta AI team found a clever way to apply the masking technique. They turned the sound files into spectrograms, visual representations of the spectrum of frequencies within signals, and then masked parts of those images for training. The reconstructed audio is pretty impressive, though the model can currently handle clips of only a few seconds. Meta’s masked auto-encoder for audio was trained on heavily masked data, and was then able to reconstruct audio files with impressive fidelity. Bernie Huang, who worked on the audio system, says that potential applications include classification tasks, helping with voice over IP calls by filling in audio that gets lost when a packet gets dropped, or finding more efficient ways to compress audio files. Meta has been on something of an AI charm offensive, open-sourcing research such as these MAE models and offering up a pretrained large language model to the AI community for research purposes. But critics have noted that for all this openness on the research side, Meta has not made its core commercial algorithms available for study—those that control newsfeeds, recommendations, and ad placements. Eliza Strickland is a senior editor at IEEE Spectrum, where she covers AI, biomedical engineering, and other topics. She holds a master's degree in journalism from Columbia University. Computer-vision systems use shapes, colors, and even labels to identify materials at superhuman speeds It’s Tuesday night. In front of your house sits a large blue bin, full of newspaper, cardboard, bottles, cans, foil take-out trays, and empty yogurt containers. You may feel virtuous, thinking you’re doing your part to reduce waste. But after you rinse out that yogurt container and toss it into the bin, you probably don’t think much about it ever again. The truth about recycling in many parts of the United States and much of Europe is sobering. Tomorrow morning, the contents of the recycling bin will be dumped into a truck and taken to the recycling facility to be sorted. Most of the material will head off for processing and eventual use in new products. But a lot of it will end up in a landfill."
472,AI’s progress isn’t the same as creating human intelligence in machines,https://www.technologyreview.com/2022/06/28/1054270/2022-innovators-ai-robots/,2022-06-28,"The term “artificial intelligence” really has two meanings. AI refers both to the fundamental scientific quest to build human intelligence into computers and to the work of modeling massive amounts of data. These two endeavors are very different, both in their ambitions and in the amount of progress they have made in recent years. Scientific AI, the quest to both construct and understand human-level intelligence, is one of the most profound challenges in all of science; it dates back to the 1950s and is likely to continue for many decades. Data-centric AI, on the other hand, began in earnest in the 1970s with the invention of methods for automatically constructing “decision trees” and has exploded in popularity over the last decade with the resounding success of neural networks (now dubbed “deep learning”). Data-centric artificial intelligence has also been called “narrow AI” or “weak AI,” but the rapid progress over the last decade or so has demonstrated its power. Deep-learning methods, coupled with massive training data sets plus unprecedented computational power, have delivered success on a broad range of narrow tasks from speech recognition to game playing and more. The artificial-­intelligence methods build predictive models that grow increasingly accurate through a compute-­intensive iterative process. In previous years, the need for human-­labeled data to train the AI models has been a major bottleneck in achieving success. But recently, research and development focus has shifted to ways in which the necessary labels can be created automatically, based on the internal structure of the data. The GPT-3 language model released by OpenAI in 2020 exemplifies both the potential and the challenges of this approach. GPT-3 was trained on billions of sentences. It automatically generates highly plausible text, and even sensibly answers questions on a broad range of topics, mimicking the same language that a person might use. This essay is part of MIT Technology Review’s 2022 Innovators Under 35 package recognizing the most promising young people working in technology today. See the full list here or explore the winners in this category below. But GPT-3 suffers from several problems that researchers are working to address. It’s often inconsistent—you can get contradictory answers to the same question. Second, GPT-3 is prone to “hallucinations”: when asked who the president of the United States was in 1492, it will happily conjure up an answer. Third, GPT-3 is an expensive model to train and expensive to run. Fourth, GPT-3 is opaque—it’s difficult to understand why it drew a particular conclusion. Finally, since GPT-3 parrots the contents of its training data, which is drawn from the web, it often spews out toxic content, including sexism, racism, xenophobia, and more. In essence, GPT-3 cannot be trusted. This year's 35 Innovators in biotech are breaking new ground via machine learning, gene therapy, gene analysis, and CRISPR. Despite these challenges, researchers are investigating multi-modal versions of GPT-3 (such as DALL-E2), which create realistic images from natural-language requests. AI developers are also considering how to use these insights in robots that interact with the physical world. And AI is increasingly being applied to biology, chemistry, and other scientific disciplines to glean insights from the massive data and complexities in those fields. The bulk of the rapid progress today is in this data-centric AI, and the work of this year’s 35 Innovators Under 35 winners is no exception. While data-centric AI is powerful, it has key limitations: the systems are still designed and framed by humans. A few years ago, I wrote an article for MIT Technology Review called “How to know if artificial intelligence is about to destroy civilization.” I argued that successfully formulating problems remains a distinctly human capability. Pablo Picasso famously said, “Computers are useless. They only give you answers.” We continue to anticipate the distant day when AI systems can formulate good questions—and shed more light on the fundamental scientific challenge of understanding and constructing human-level intelligence. Oren Etzioni is CEO of the Allen Institute for AI and a judge for this year’s 35 Innovators competition."
499,         GPT Language Model Spells Out New Proteins     ,https://spectrum.ieee.org/gpt-2-language-model-proteins,2022-08-12 00:00:00.000000,"Human speech and protein structure are close enough for AI purposes Human languages have much in common with proteins, at least in terms of computational modeling. This has led research teams to apply novel methods from natural-language processing (NLP) to protein design. One of these—Birte Höcker’s protein design lab at Bayreuth University, in Germany—describes ProtGPT2, a language model based on OpenAI’s GPT-2, to generate novel protein sequences based on the principles of natural ones. Just as letters from the alphabet form words and sentences, naturally occurring amino acids combine in different ways to form proteins. And protein sequences, just like natural languages, store structure and function in their amino-acid sequence with extreme efficiency. ProtGPT2 is a deep, unsupervised model that takes advantage of advances in transformer architecture that have also caused rapid progress in NLP technologies. The architecture has two modules, explains Noelia Ferruz, a coauthor of the paper and the person who trained ProtGPT2: one module to understand input text, and another that processes or generates new text. It was the second one, the decoder module that generates new text, that went into the development of ProtGPT2. Researchers have used GPT-2 to train a model to learn the protein “language,” generate stable proteins, and explore “dark” regions of protein space. “At the time we created this model, there were many others that were using the first module,” she says, such as ESM, ProtTrans, and ProteinBERT. “Ours was the first one publicly released at a time that was a decoder.” It was also the first time someone had directly applied GPT-2, she adds. Ferruz herself is a big fan of GPT-2. “I find it very impressive that there was a model capable of writing English,” she says. This is a well-known transformer model that was pretrained on 40 gigabytes of Internet text in English in an unsupervised manner—that is, it used raw text with no human labeling—to generate the next word in sentences. The GPT-x series has been shown to efficiently produce long, coherent text, often indistinguishable from something written by a human—to the extent that potential misuse is a concern. Given the capabilities of GPT-2, the Bayreuth researchers were optimistic about using it to train a model to learn the protein language, generate stable proteins, and also explore “dark” regions of the protein space. Ferruz trained ProtGPT2 on a data set of about 50 million nonannotated sequences across the whole protein space. To evaluate the model, the researchers compared a data set of 10,000 sequences generated by ProtGPT2 with a random set of 10,000 sequences from the training data set. “We could add labels, and potentially in the future start generating sequences with a specific function.”—Noelia Ferruz, University of Bayreuth, Germany They found the sequences predicted by the model to be similar in secondary structure to naturally occurring proteins. ProtGPT2 can predict proteins that are stable and functional, although, Ferruz says, this will be verified by laboratory experiments on a set of 30 or so proteins in the coming months. ProtGPT2 also models proteins that do not occur in nature, opening up possibilities in the protein design space. Each node represents a sequence. Two nodes are linked when they have an alignment of at least 20 amino acids and 70 percent HHsearch probability. Colors depict the different SCOPe classes, and ProtGPT2 sequences are shown in white.University of Bayreuth/Nature Communications The model can generate millions of proteins in minutes, says Ferruz. “Without further improvements, people could take the model, which is freely available, and fine-tune a set of sequences to produce more sequences in this region,” such as for antibiotics or vaccines. But also, she adds, with small modifications in the training process “we could add labels, and potentially in the future start generating sequences with a specific function.” This in turn has potential for uses in not just medical and biomedical fields but also in environmental sciences and more. Ferruz acknowledges the rapid developments in the NLP space for the success of ProtGPT2, but also points out that this is an ever-changing space—“It’s crazy, all the things that have happened in the last 12 months.” At the moment, she and her colleagues are already writing a review of their work. “I trained this model over Christmas [2021],” she says, “and at the time, there was another model that had been described...but it wasn’t available.” Yet by this spring, she says, other models had been released. ProtGPT2’s predicted sequences spanned new, rarely explored regions of protein structure and function. However, a few weeks ago, DeepMind released structures of over 200 million proteins. “So I guess we don’t have that much of a dark proteome anymore,” Ferruz says. “But still, there are regions…that haven’t been explored.” There is plenty of work ahead, though. “I would like to have control over the design process,” Ferruz adds. “We will need to take the sequence, predict the structure, and maybe predict the function if it has any….That will be very challenging.” Payal Dhar (she/they) is a freelance journalist on science, technology, and society. They write about AI, cybersecurity, surveillance, space, online communities, games, and any shiny new technology that catches their eye. You can find and DM Payal on Twitter (@payaldhar). Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor."
517,StableDiffusion; CHIPXODUS; Microsoft makes a big bet on pre-training - 0,http://eepurl.com/h-mNQ9,2022-09-06,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Facebook's AI chief - here's why you're not gonna get AGI out of an LLM: …Embodiment matters for making general intelligence… Two AI researchers, one of whom - Yann Lecun - happens to lead Facebook's AI research, have said that language is an inherently limited medium for training AI systems. Basically, the claim is that large language models ""are doomed to a shallow understanding that will never approximate the full-bodied thinking we see in humans"". What's wrong with language: This argument comes down to representation - language just isn't able to inherently encode precise information about the world and, by nature, involves creating explanations for precise phenomena in the world (e.g, descriptions of unusual objects, or defining the nuanced brushwork used to make a painting). ""There are nonlinguistic representational schemes which can express this information in an accessible way,"" they note. This dependency on language basically makes LLMs useful improvisational artists who don't understand the role they're playing. ""The contextual knowledge is embedded in one form — the capacity to rattle off linguistic knowledge — but is not embedded in another form — as skillful know-how for how to do things like being empathetic or handling a difficult issue sensitively,"" they write. Why this matters: I'd say the jury is out here - sure, language may have some limits as a modality, but there's a ton of language to use to train models on, and things like GPT3 have already surprised experts with the capabilities they gain purely via language training. It feels to me like there's some % chance here that this is a case of a 'bitter lesson' in disguise - at some scale of data, a purely LM-based system might have capabilities that Lecun deems impossible. On the other hand, adding other modalities certainly helps (see the incredible AI art projects that have been unlocked by the multimodal 'CLIP' model), so there's certainly merit to adding more datatypes. Read more: AI And The Limits Of Language (Noema magazine)."
523,Google's Bitter Lesson; DOOM AGI; DALL-E's open source competition StableDiffusion - 0,http://eepurl.com/h9fIwP,2022-08-22,"Intro (no tag) Welcome to Import AI, a newsletter about artificial intelligence. Forward this email to give your chums an AI upgrade. Subscribe here. Google makes its robots massively smarter by swapping out one LM for a different, larger LM: …Maybe language models really can work as world models… Earlier this year, Google showed how it was able to use a large language model to significantly improve the performance and robustness of robots tasked with doing tasks in the physical world. The 'SayCan' approach (Import AI 291) basically involved taking the affordances outputted by on-robot AI systems and pairing that with a language model, looking at the high-likleihood actions generated by both systems (the on-robot models, as well as the LM), then taking actions accordingly. The approach is both simple and effective. Now, Google has found a way to make the approach much, much more effective. The secret? Swapping out one LM for a far larger one. What Google did: Google upgraded its robots by pairing them with its large-scale 540B parameter 'PALM' language model, where the previous system used the 137B parameter 'FLAN' model. The larger model gives the robots significantly improved performance: ""The results show that the system using PaLM with affordance grounding (PaLM-SayCan) chooses the correct sequence of skills 84% of the time and executes them successfully 74% of the time, reducing errors by half compared to FLAN,"" Google writes. The bitter lesson - bigger is better: Though FLAN was finetuned to be good at instruction following, PALM beats FLAN likely as a consequence of scale. ""The broader and improved dataset for PaLM may make up for this difference in training,"" Google writes. This is significant as it's another sign that simply scaling up models lets them develop a bunch of capabilities naturally which beat human-engineered finetuned approaches - chalk another point up in favor of silicon minds versus mushy minds. Read more: Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (arXiv, read the 'v2' version)."
530,The world's best language model is Made in China; NVIDIA boosts LLM training; OpenAI shows how to 'fill in the middle' on a LM - 1,http://eepurl.com/h8jfsX,2022-08-08,"#################################################### Want to make a language model with a 'fill in the middle' option? Here's how! …Sentence completion is cool, but infilling is useful as well… Here's a straightforward paper from OpenAI that describes how to give language models the ability to learn to infill text - e.g, taking a sentence and knocking out the middle of it and asking the model to 'fill in the middle'. The big insight: The main insight here is that you can learn to fill in the middle ""without compromising the left-to-right capability in pretraining…FIM models achieve the same test loss as AR models on left-to-right test loss while achieving lower FIM loss."". They also learn that it's inefficient to finetune a model to learn to fill in the middle, and you should generally do it at the pretraining stage instead. Why this matters: Somewhat like DeepMind's recent 'Chinchilla' paper (Import AI #290), which showed you can dramatically increase the capabilities of language models by training them on 5X data, this paper shows you can augment an LM with a nice edit function, and this doesn't come at a loss anywhere else. In fact, OpenAI shows that these ""models are strictly more capable than canonically trained left-to-right models, at least within the bounds of the evaluations we consider"". Read more: Efficient Training of Language Models to Fill in the Middle (arXiv)."
537,Mimetic models; LLM search engine raises $25m; UK splits from Europe on AI regulation - 2,http://eepurl.com/h7jJjn,2022-07-25,"#################################################### Now that models can imitate people, what do we do? …All hail the era of the funhouse mirror model… A language model can do an impression of Einstein, a lawyer from Texas in the 19th century, and - given enough information - you. Now, researchers with the University of Toronto, Cornell University and Microsoft Research have grappled with the issues these so-called 'Mimetic Models' may produce. What they are: A mimetic model is ""an algorithm that is trained on data from a specific individual in a given domain, and which is designed to accurately predict and simulate the behavior of this individual in new situations from the domain"", they write. ""Interacting with a mimetic model can be used as preparation for interactions in real life - essentially, as a means to an end."" How they might be used: These models will be used for tasks as varied as being a stand-in for oneself (e.g, answering emails for you), or being a stand-in for an opponent (e.g, preparing for a competition with someone, or a debate). They could also be used as 'mimetic counterfactuals' - how might a person change if they did something different with their life? Real world use: Mimetic models are already out there in the world - like AI21's marketing stunt to create a virtual 'Ruth Bader Ginsburg' model people can talk to (Import AI 296), or this experiment by an independent artist where they resurrect a childhood friend and the mimetic model tries to kill them using a microwave (Import AI 292). How to think about them: We should think about these models with reference to four key roles - the target that the model is designed to imitate, the person or organization that created the model, the operator who uses the model, and the interactor who interacts with the model or views its outputs. Why this matters: Because language models can approximate specific data distributions, it makes sense they can eventually represent people to a high level of fidelity. But I'm not sure the world is ready for the economic, security, and cultural implications of (digital) clones on tap. Read more: Mimetic Models: Ethical Implications of AI that Acts Like You (arXiv)."
762,Learning Without Simulations? UC Berkeley’s DayDreamer Establishes a Strong Baseline for Real-World Robotic Training,https://syncedreview.com/2022/07/04/learning-without-simulations-uc-berkeleys-daydreamer-establishes-a-strong-baseline-for-real-world-robotic-training/,2022-07-04,"Using reinforcement learning ( RL ) to train robots directly in real-world environments has been considered impractical due to the huge amount of trial and error operations typically required before the agent finally gets it right . The use of deep RL in simulated environments has thus become the go-to alternative , but this approach is far from ideal , as it requires designing simulated tasks and collecting expert demonstrations . Moreover , simulations can fail to capture the complexities of real-world environments , are prone to inaccuracies , and the resulting robot behaviours will not adapt to real-world environmental changes . The Dreamer algorithm proposed by Hafner et al . at ICLR 2020 introduced an RL agent capable of solving long-horizon tasks purely via latent imagination . Although Dreamer has demonstrated its potential for learning from small amounts of interaction in the compact state space of a learned world model , learning accurate real-world models remains challenging , and it was unknown whether Dreamer could enable faster learning on physical robots . In the new paper DayDreamer : World Models for Physical Robot Learning , Hafner and a research team from the University of California , Berkeley leverage recent advances in the Dreamer world model to enable online RL for robot training without simulators or demonstrations . The novel approach achieves promising results and establishes a strong baseline for efficient real-world robot training . The team summarizes their main contributions as : Dreamer learns its world model from a replay buffer of past experiences . It adopts an actor-critic algorithm to learn behaviours from the learned model ’ s predicted trajectories , then deploys these behaviours in the environment to continuously grow the replay buffer . In the new paper ’ s implementation for online RL in the real world , the world model and actor-critic behaviour are continuously trained by a learner thread , while a parallel actor thread computes actions for environment interaction . The team evaluated Dreamer on a variety of challenging tasks involving locomotion , manipulation , navigation , etc . The results show that Dreamer can train physical robots to perform behaviours such as rolling off their backs , standing up , and walking , all from scratch and in only about one hour . Dreamer also approached human performance on a task involving picking and placing multiple objects directly from camera images ; and , on a wheeled robot , learned to navigate to a goal position purely from camera images , automatically resolving ambiguities with regard to robot orientation . Overall , this work demonstrates Dreamer ’ s strong potential for sample-efficient physical robot learning of real-world tasks without simulators.Videos are available on the project website : https : //danijar.com/daydreamer . The paper DayDreamer : World Models for Physical Robot Learning is on arXiv . Author : Hecate He | Editor : Michael Sarazen We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates ."
763,Stabilizing Off-Policy Deep Reinforcement Learning from Pixels,"[{'href': 'http://arxiv.org/abs/2207.00986v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2207.00986v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-07-03 08:52:40,"University of New Mexico University of New Mexico UNM Digital Repository UNM Digital Repository Electrical and Computer Engineering ETDs Engineering ETDs Spring 5-2022 Speaker Diarization and Identification from Single-Channel Speaker Diarization and Identification from Single-Channel Classroom Audio Recording Using Virtual Microphones Classroom Audio Recording Using Virtual Microphones Antonio Gomez Follow this and additional works at : https : Part of the Bilingual , Multilingual , and Multicultural Education Commons , Computational Engineering Commons , Educational Methods Commons , Educational Technology Commons , Science and Mathematics Education Commons , and the Signal Processing Commons Antonio Gomez Candidate Electrical and Computer Engineering Department This dissertation is approved , and it is acceptable in quality and form for publication : Approved by the Dissertation Committee : Dr. Marios Pattichis Chairperson Dr. Ramiro Jordan Dr. Sylvia Pattichis Dr. Kim Linder Dr. Manel Martinez-Ramon i Speaker Diarization and Identification from Single-Channel Classroom Audio Recording Using Virtual Microphones by ANTONIO GOMEZ BS , Electrical Engineering , Florida International University , 1986 MS , Engineering Management , Florida International University , 1997 DISSERTATION Submitted in Partial Fulfillment of the Requirements for the Degree of Doctor of Philosophy Engineering The University of New Mexico Albuquerque , New Mexico May 2022 ii DEDICATION I would like to dedicate this dissertation to a very special person in my life , my wife Grace , for her support during this long journey . For all the hours , days , months , years she spent giving me the breath to move on , and for all the times she told me it is time to finish as well . For her , my eternal gratitude . I would like also to dedicate this work to my children , Daniel , and Carolina , for understanding why sometimes I was not there with them . I hope this work would serve as an inspiration to them . iii ACKNOWLEDGEMENTS I would like to sincerely acknowledge the labor of my advisor and dissertation chair , Dr. Marios Pattichis , for his continued support during all these years . Thank you for believing in me , Marios . I would have never done this without you on my side . I would like also to thank my committee members , for taking the time to review and advise my work . Thank you , Dr. Kim Linder , for being always there as a friend . Finally , I would like to say thank you to my managers at Honeywell and Sandia National Labs for their support and understanding . iv Speaker Diarization and Identification from Single-Channel Classroom Audio Recording Using Virtual Microphones By ANTONIO GOMEZ BS , Electrical Engineering , Florida International University , 1986 MS , Engineering Management , Florida International University , 1997 Ph.D. Engineering , University of New Mexico , 2022 ABSTRACT Speaker identification in noisy audio recordings , specifically those from collaborative learning environments , can be extremely challenging . There is a need to identify individual students talking in small groups from other students talking at the same time . To solve the problem , we assume the use of a single microphone per student group without any access to previous large datasets for training . This dissertation proposes a method of speaker identification using cross- correlation patterns associated to an array of virtual microphones , centered around the physical microphone . The virtual microphones are simulated by using approximate speaker geometry observed from a video recording . The patterns are constructed based on estimates v of the room impulse responses for each virtual microphone . The correlation patterns are then used to identify the speakers . The proposed method is validated with classroom audios and shown to substantially outperform diarization services provided by Google Cloud and Amazon AWS . vi TABLE OF CONTENTS LIST OF FIGURES………………………………………………………………… ... …xii LIST OF TABLES……………………………………………………………….… ... …xvi CHAPTER 1 . INTRODUCTION 1 1.1 MOTIVATION 4 1.2 RELATED RESEARCH 6 1.3 THESIS STATEMENT 12 1.4 CONTRIBUTIONS 12 1.5 DISSERTATION OVERVIEW 13 CHAPTER 2 . BACKGROUND 15 2.1 ACOUSTICS PRINCIPLES 15 2.1.1 Sound Propagation : Near and Far Fields 16 2.1.2 Sound Propagation : Direct Path , Reflections , and Reverberation ................ 18 2.2 MICROPHONES AND MICROPHONE ARRAYS 22 2.2.1 Classification of Microphones 22 2.2.2 Microphone Arrays 24 2.2.2.1 Microphone Arrays Configurations 26 2.2.2.2 Spatial Aliasing 30 2.2.2.3 TDOA and Cross-Correlation 31 vii 2.2.2.4 Beamforming and Spatial Filters 33 2.3 MODELING OF ROOM ACOUSTICS 34 2.3.1 Ray Tracing Method 35 2.3.2 Image Source Method 36 2.4 CHARACTERISTICS OF THE HUMAN SPEECH 38 2.5 SPEAKER DIARIZATION AND IDENTIFICATION 40 2.5.1 Methods for Diarization and Identification 40 2.5.1.1 Classical Methods for Diarization and Identification ........................... 40 2.5.1.2 Deep Neural Networks 43 2.5.1.2.1 Stage-wise diarization 44 2.5.1.2.2 Multimodal Speaker Diarization 46 2.5.2 Current State-of-the-Art Methods for Diarization and Identification ........... 47 CHAPTER 3 . PROPOSED METHOD 50 3.1 METHODOLOGY 50 3.2 BLOCK DIAGRAM OF THE PROPOSED SYSTEM 68 CHAPTER 4 . EXPERIMENTAL IMPLEMENTATION 72 4.1 SOFTWARE AND HARDWARE TOOLS 72 4.1.1 Open-Source Code for Room Geometry , RIR Calculation , and Microphone Simulation 73 4.1.1.1 Pyroomacoustics Implementation 76 4.1.2 Audio Segmentation 78 viii 4.1.2.1 Fixed Length Segmentation 78 4.1.2.2 Voice Activity Detection 79 4.1.3 Implementation Using the LabVIEW Graphical Programming ................... 81 4.1.3.1 LabVIEW Implementation 83 4.1.3.1.1 Function VIs 83 4.1.3.1.2 Operational Sub-VIs 87 4.1.3.2 Audio Laboratory 91 4.2 THE AOLME ENVIRONMENT 94 4.2.1 Characteristics of the AOLME Environment 95 4.2.2 Preparation of the Experimental Models 96 4.2.2.1 Approximating the Models Using Video Observations ........................ 97 CHAPTER 5 . RESULTS 104 5.1 EVALUATION OF PYROOMACOUSTICS 104 5.1.1 Microphone Calibration 104 5.1.2 Audio Lab Setup and Model Configuration 108 5.1.3 Experimental Execution 110 5.1.4 Results 111 5.2 CONTROLLED ENVIRONMENT EXPERIMENTS 112 5.2.1 Methodology 112 5.2.1.1 Audio Lab and Model Preparation 113 5.2.1.2 Evaluation Criteria 115 ix 5.2.2 “ HAL 9000 ” Experiments 116 5.2.2.1 Source Preparation and Editing 118 5.2.2.2 Ground Truth Recording 120 5.2.2.3 Training and Segmentation 120 5.2.2.4 Testing and Results 123 5.2.3 Multi-Speaker Identification Experiments 124 5.2.3.1 Source Preparation and Editing 124 5.2.3.2 Ground Truth Recording 126 5.2.3.3 Training and Segmentation 127 5.2.3.4 Testing and Results 127 5.3 AOLME EXPERIMENTS 128 5.3.1 Evaluation and Selection of AOLME Videos 128 5.3.2 Model Preparation 129 5.3.3 Training and Segmentation 131 5.3.4 Testing and Results 132 5.4 COMPARISON WITH OTHER METHODS 134 5.4.1 Methodology for Comparison 134 5.4.2 Selection , Preparation , and Ground Truth Measurements of Videos for Analysis 135 5.4.3 Training and Segmentation 136 5.4.4 Testing and Analysis 136 x 5.4.5 Results 136 CHAPTER 6 . SUMMARY , CONCLUSIONS , AND FUTURE WORK ...................... 140 APPENDIX A : PYROOMACOUSTICS SCRIPTS 143 APPENDIX B : LABVIEW SUB-VIS 146 APPENDIX C : AUDIO LAB EQUIPMENT SPECIFICATIONS .......................... 158 REFERENCES 162 xi LIST OF FIGURES Figure 1 : Propagation of Sound Waves . ( a ) Free Field . ( b ) Directional . .......................... 17 Figure 2 : Near and Far-Field Areas . 18 Figure 3 : Direct and Reflected Paths for Sound Propagation in a Diffuse Field . ............. 19 Figure 4 : Representation of the Room Impulse Response and its Components . .............. 21 Figure 5 : Polar Pattern Plot of Directivity of Two Types of Microphones : ( a ) Omnidirectional . ( b ) Cardioid . 24 Figure 6 : Linear Microphone Array . ( a ) Geometry . ( b ) 3D Directionality Pattern . ......... 27 Figure 7 : Cross-Linear Array and Azimuth Directivity Pattern . 28 Figure 8 : Circular Microphone Array and Directivity Pattern 29 Figure 9 : Volumetric Microphone Array and 3D Directivity Pattern ............................... 30 Figure 10 : Location of Source and Microphones for Time Difference of Arrival . .......... 32 Figure 11 : Simulation Methods . ( a ) Ray Tracing . ( b ) Image Source Method . ................. 36 Figure 12 : Source Image Map . 37 Figure 13 : Directionality of Human Head . 39 Figure 14 : Block Diagram of a Typical Speaker Diarization System . ............................. 41 Figure 15 : GMM/i-vector Framework . 45 Figure 16 : DNN/i-vector Implementation . 45 Figure 17 : Collaborative Environment ( a ) with 2D Model ( b ) . 51 xii Figure 18 : 2D Model of Fig . 12 ( b ) with Microphone Array . 52 Figure 19 : Possible 2D Model for Fig . 17 . 56 Figure 20 : Estimation of the Sources . 60 Figure 21 : Estimation of Virtual Microphones . 60 Figure 22 : Training Samples from Each Speaker and Noise . 64 Figure 23 : Audio Segmentation using a VAD . 67 Figure 24 : Block Diagram of the Proposed System 69 Figure 25 : Pyroomacoustics Models . ( a ) 2D . ( b ) 3D . ( c ) 3D With Images . ..................... 75 Figure 26 : LabVIEW Sub VI for Cross-Correlation Calculation . 82 Figure 27 : LabVIEW Convolution VI . 84 Figure 28 : LabVIEW Deconvolution VI . 85 Figure 29 : LabVIEW Correlation VI . 86 Figure 30 : LabVIEW Cross-Correlation VI . 87 Figure 31 : Audio Lab Components 93 Figure 32 : Audio Lab Setup 94 Figure 33 : Common AOLME Environment Setup . 95 Figure 34 : Relative Positions of AOLME Participants 97 Figure 35 : Location of Speakers and Real Microphone . 100 Figure 36 : 3D Model of the Virtual Room 101 Figure 37 : Final 2D Model for AOLME Example . 103 Figure 38 : Block Diagram of a Microphone Calibration Setup . 105 xiii Figure 39 : ( a ) Microphone Calibration Jig . ( b ) Location to Loudspeaker . ..................... 106 Figure 40 : Audio Lab Set-Up for Pyroomacoustics Evaluation . 108 Figure 41 : Final 2D Model of Audio Lab Setup . 110 Figure 42 : 2D Model for Controlled Experiments . 115 Figure 43 : Video Clips of Dave ( Clip1 ) , and HAL ( Clip2 ) . 117 Figure 44 : Sources and Noise for HAL 9000 Experiment 119 Figure 45 : Ground Truth Sets A and B for HAL 9000 Experiment . .............................. 121 Figure 46 : Training Segments for HAL and Dave . 122 Figure 47 : Video Clips for AOLME Experiments . 129 Figure 48 : 2D Model for AOLME Experiments 130 Figure 49 : Room Parameters Reader Inputs and Outputs 146 Figure 50 : Room Parameters Reader Front Panel . 146 Figure 51 : Room Parameters Reader Block Diagram 147 Figure 52 : Room Model Generator Icon . 148 Figure 53 : Room Model Generator Front Panel . 148 Figure 54 : Room Model Generator Block Diagram . 149 Figure 55 : Source Estimator Icon . 150 Figure 56 : Source Estimator Front Panel . 150 Figure 57 : Source Estimator Simplified Block Diagram . 151 Figure 58 : Cross-Correlation Model Calculator Icon . 151 Figure 59 : Cross-Correlation Model Calculator Front Panel . 152 xiv Figure 60 : Cross-Correlation Model Calculator Block Diagram . 153 Figure 61 : Cross-Correlation Model Calculator . Cross-Correlator Sub-VI .................... 154 Figure 62 : Model Classifier Icon with Inputs and Outputs 154 Figure 63 : Model Classifier Front Panel . 155 Figure 64 : Multi-Function Convolution and Correlator Visualizer Front Panel . ........... 156 Figure 65 : Multi-Function Convolution and Correlator Visualizer Diagram . ................ 157 xv LIST OF TABLES Table I : Template Cross-Correlation Table for Source 1 . 64 Table II : Template Cross-Correlation Table for Source 2 . 65 Table III : Template Cross-Correlation Table for Source 2 . 65 Table IV : Example Cross-Correlation Table Output from Model Calculator .................. 90 Table V : Cross-Correlation Tables for Classification 90 Table VI : Cross-Correlation Table for Microphone Calibration . 107 Table VII : Dimensions of Virtual Room and Location of Sources ( in m ) . .................... 109 Table VIII : Experimental Results for Simulation Software Evaluation . ........................ 111 Table IX : Distribution of Microphones and Sources for Controlled Experiments . ........ 114 Table X : DER Results for HAL 9000 Experiments . 124 Table XI : Multi-Speaker Experiment Sequence Table 126 Table XII : Controlled Environment Experiments Diarization Error Rate Results ......... 127 Table XIII : Location of Speakers and Microphones for AOLME Experiments . ........... 131 Table XIV : Speaker Assignment for AOLME Experiments . 132 Table XV : CC Tables for AOLME Experiment . 133 Table XVI : Classification Results for AOLME Experiments . 134 Table XVII : Experimental Comparison Between Methods . 137 Table XVIII : % Average Error for All Three Methods . 138 xvi Chapter 1 . Introduction The field of speech processing , which includes speech recognition , separation , transcription , and enhancement , has undergone several transformational changes . Despite significant progress , speaker identification in crowded rooms continues to be a difficult problem . Crosstalk and large amounts of background noise make these environments particularly challenging . Most speaker identification and diarization systems rely on the use of Deep Learning methods that require pre-training on large datasets . Speech features such as formant frequencies , pitch contours , and coarticulation are extracted from the test samples and are eventually matched against a database of training samples [ 1 ] . The databases need to contain as many training examples as possible and should be updated periodically to maintain a proper performance level [ 2 ] . The accuracy of the identification depends on the size of the database : the bigger the database , the better the accuracy , but the longer the training times [ 3 ] . In addition to long training times , databases are prone to bias concerning spoken language and accent [ 4 ] . This biasing is usually unintentional and unconscious , and it is the product of the environment where the speech recognition system is developed [ 5 ] . The limitations of speech processing systems are more evident in challenging situations such as collaborative environments , meetings , or large-scale educational settings in general . These environments commonly consist of multiple speakers sitting around a table located inside a room . The speakers can take turns to speak , but it is not unusual to 1 have two or more speakers talking at the same time . The environment can also be very noisy if we have numerous participants or groups inside the same room . These types of environments are too difficult for most speech processing systems , requiring in many cases heavy manual analysis . Manual diarization of meetings is a tedious and time-consuming task that requires many hours of processing , and it is subject to many interpretation errors . There is a need in many educational research activities to understand how the classroom material engages the students . To understand how students interact , classroom sessions are recorded and transcribed . An important problem here is to determine which participant is speaking at a particular moment , what she or he has said , and for how long the participant spoke . Automated methods usually require multi-channel audio recordings and are prone to errors due to noise and crosstalk . Also , these systems have limitations in the number of speakers they can process , as well as the length of the audio segments . While diarization systems do not require enrollment of the speakers , they can only generate abstract labels of the speaker that is active in an audio segment . On the other hand , speaker identification systems can provide non-abstract labels by enrolling the participating speakers . The enrollment process consists of each speaker providing several seconds of noise-free speech without crosstalk . This requirement can not be met when the data consists of audio recordings of busy meetings with noisy backgrounds . It is thus important to develop speech identification and diarization methods that do not impose any requirement to pre-enroll the speakers . 2 This dissertation aims to provide the foundations of a new approach to speaker identification and diarization using virtual microphones and spatial information . Simulations are never perfect , but this work shows that it is possible to use an approximation of a real room geometry to obtain the acoustic parameters necessary to simulate reception in a virtual array of microphones and use these simulated signals for speaker diarization and identification . The simulation is based on a physical model that requires no databases , it is independent of the spoken language or accent of the participants , it does not require prior speaker enrollment , and it presents high immunity to noise . The proposed approach relies on the fact that discriminant information about the 3D geometry of each speaker is embedded in the recorded audio from a single microphone . The basic idea is to recognize speakers using acoustical simulation . As part of the simulation process , the proposed method computes the Room Impulse Response ( RIR ) for each of the microphones and the speakers and simulates the reception on each of the virtual microphones . The accuracy of the process of computing RIRs is verified through real-life measurements of the correlation patterns . Based on the simulated reception over the virtual microphones , the method computes correlation patterns among the virtual microphones . The recorded audio is then also used to generate different correlation patterns based on hypothesized speaker locations . A classifier is applied to the generated correlation patterns to select the most likely speaker location . This approach has several advantages . First , we do not require databases of speech . Our system is based on physical models that are unique to the scene we are analyzing . 3 Because we do not have databases to train the model , our system requires capturing only about 1 to 2 seconds of audio from each speaker for both training and recognition . In contrast , state-of-the-art systems require tens of seconds of clean audio for training and several seconds of identification . Second , our system has been conceived to operate in noisy environments where microphone arrays and cross-correlation analysis have been proven to be efficient methods for speaker discrimination [ 6 ] , [ 7 ] . Third , the simulation does not require multi-channel audio , but it uses a single channel recording as a reference for the simulation . Finally , our system can run on simple computers without the need to access remote computer clusters or databases . 1.1 Motivation This work is motivated by the need for a reliable non-manual method of assessing the level of engagement of the students participating in the Advancing Out-of-school Learning in Mathematics and Engineering ( AOLME ) program at the University of New Mexico ( UNM ) [ 8 ] . AOLME is a collaborative learning environment where students are introduced to STEM subjects such as integrating computer programming and middle school mathematics . It forms part of the educational research activities performed at the University of New Mexico ’ s Image and Video Processing and Communications Lab ( ivPCL ) [ 9 ] . AOLME sessions are video recorded for later analysis that includes students ’ participation and overall level of attention , as well as the facilitator ’ s interaction with the students . The analysis consists of evaluating the activities of the participants such as hands 4 or head movement , use of keyboard and mouse , lip movement , etc. , and transcription of the sessions to determine the time when a participant is speaking and for how long . Detailed participation statistics for each participant are currently not available because manually measuring talking times is time-consuming and plagued with errors . AOLME organizers have tried several transcriptions systems currently available in the market or open-source code , all without much success . The AOLME environment is extremely challenging for any speech recognition and transcription system due to multiple groups talking at the same time and the presence of background noise and echo . Hence , there is a need for a robust system that can overcome the limitations of the current state-of-the-art methods and complements the ivPCL methods , with the application to process hundreds of hours of video recordings . AOLME video analysis presents other challenges in addition to the presence of multiple speakers and noise . First , these videos were taken with a simple video camera using a single microphone located at the meeting table . Budget limitations restrict the purchasing and use of more advanced equipment with multi-channel audio recording capabilities . Second , there are already hundreds of hours of these video recordings that need to be analyzed . There was no previous speaker enrollment that could be used to train a speaker identification system . Furthermore , most participants only speak for several seconds at a time , which makes the identification process more difficult . Even if for future sessions it is possible to record multichannel audio and enroll the speakers , there is the need to process the existing videos , therefore the need for a flexible method that can handle 5 new and existing recordings . 1.2 Related Research Assessing the level of engagement of participants in collaborative educational sessions requires the application of tools to extract relevant information from audio and video data . This information is then interpreted and translated into statistical data for the researchers . For this end , these tools can either identify activities in a video scene that are related to attention behaviors ( e.g. , typing and writing ) , or they can identify the active speaker or speakers in an audio segment . Related work to this dissertation includes both types of tools . In the area of activity tracking , it is important to mention the work by UNM ’ s ivPCL lab in direct connection with the AOLME program . Darsey [ 10 ] , analyzes video using color and optical flow for tracking hand movement . Teeparthi et al . [ 11 ] , [ 12 ] , presents fast methods of video analysis for hand and object tracking as well . Jacoby et al . [ 13 ] works in human activity detection using context-sensitive approaches , while Jatla et al [ 14 ] uses 3D Convolutional Nets . Eiliar et al . [ 15 ] provides a maintainable open-source activity system . Detection of attention traits is investigated by Shi et al . [ 16 ] , using AM- FM models to detect head direction and group interactions . Research on speech processing covers a vast area containing different topics . Under the umbrella of speech processing , we find speech identification , speech enhancement , speaker verification , speaker diarization , and speaker identification , among others . This 6 dissertation focus on speaker identification and diarization as part of the research labor of the ivPCL lab and AOLME programs . Speaker identification is the process of recognizing the identity of a speaker or several speakers present in a speech segment . Speaker diarization is the process in which an audio recording that contains several speakers is dissected into segments that contain only one speaker at a time [ 17 ] . Speaker Diarization is often defined as “ who said what , and when ” , and for “ how long ” . Both Speaker Identification and Speaker Diarization are important mechanisms in many audio-processing tasks . Most of the research on speech processing nowadays is focused on the use of Artificial Neural Networks and Deep Learning . Deep Belief Networks ( DBN ) are widely used in speech recognition [ 18 ] , [ 19 ] . X-vectors are considered today state of the art in speaker recognition [ 20 ] . X-vector methods outperform classic i-vector methods in the order of 9.23 % , and they have been tested with datasets such as VoxCeleb , NIST SRE 2016 , and SWBD [ 21 ] . The research in this dissertation focuses on the use of spatial information and virtual microphone arrays for speaker identification and diarization . There is no attempt to cover methods that do not use spatial information or virtual microphone arrays for speaker identification and diarization . Although not as extensive as the neural network and deep learning research , it was possible to find numerous works that demonstrated the use of spatial information for speaker identification and diarization , as well as applications of 7 virtual microphone arrays for acoustic signal enhancement and meeting diarization . The references to these works are presented in the next sections . Most literature regarding the application of microphone arrays ( multichannel audio ) and beamforming is related to the implementation of spatial filters to improve the signal-to-noise ratio ( SNR ) . Nevertheless , several researchers found ways to exploit the operational principles of microphone arrays and apply them to speaker identification and diarization . Xavier Anguera et al . [ 22 ] propose the use of beamforming algorithms as the forefront of a speaker diarization system . These beamforming algorithms take advantage of the environment commonly encountered in meetings , such as multiple microphones , to enhance a single signal of interest . Anguera et al . optimize a conventional delay and sum beamforming array to operate under the constraints of an unknown number of speakers , unknown location of both speaker and microphones , and microphone mismatching . The Time Differential of Arrivals ( TDOAs ) of the microphones are calculated by cross- correlation . Diarization is accomplished by agglomerative clustering where each cluster is modeled via a Gaussian Mixture Model ( GMM ) . A separate set of GMMs is used to model the TDOA features . In a similar manner as Anguera , Mitianoudis et al . [ 23 ] propose the use of beamforming in parallel with Independent Component Analysis ( ICA ) for audio source separation . The ICA for source separation requires knowledge of the parameters of the mixing matrix . If these parameters are not known , then the separation problem becomes a Blind Source Separation problem ( BSS ) , which is an ill-posed problem ( multiple 8 solutions ) . Mitianoudis et al . propose the use of the directivity pattern of beamforming ( use of phase information ) to select signals among different possible permutations . Both previous authors exploit the phase information of signals captured by microphone arrays . In my research , I also exploit the phase information ( TDOA between microphones ) as a means of determining the relative position of the active speaker and thus the identity . The previous work shows the use of cross-correlation to calculate the TDOA . Klein et al . [ 24 ] study the performance of the multi-channel cross-correlation ( MCCC ) coefficient method as a robust solution to calculations of TDOA under noisy and reverberant environments . Padois [ 25 ] studies the performance of time-domain beamformers based on the generalized cross-correlation functions . Padois generates a sound source map by interpolating the cross-correlation function between microphones , to generate a two-dimensional hyperbola of the spatial likelihood function . The number of hyperbolas corresponds to the number of microphones used in the array . The source position can be determined by averaging the hyperbolas and determining their maximum value as the intersecting point for the location . In general , the experimental results show that resolution improves with the number of microphones , up to a number where the performance seems to plateau . Pasha et al . [ 26 ] present work that is closely related to our research on RIR and room geometry for TDOA estimation . Pasha et al . propose a method of source localization that utilizes RIRs amplitudes to fit a TDOA surface and an amplitude surface across a room of known geometry . The RIR is obtained from a set of microphones of an unknown 9 location . The RIR amplitudes of the direct path impulses are higher and have a shorter relative time of arrival for the signals that are closer to the receiving microphone . The area with the maximum amplitude and minimum delay is considered the estimated source area . The center of these areas is the estimated source location . Similar work was previously presented by Tervo et al [ 27 ] , but , instead of source location , this work focuses on localization of acoustic reflections using the combined TOA and the TDOA information contained in the RIR . All the work presented so far takes advantage of the properties of beamformers , TDOA , TOA , DOA , and cross-correlation , but also requires an array of physical microphones . In this dissertation , the method depends only on the information captured by a single microphone . Research material on single microphone acoustic separation based on spatial information is more limited , as well as work on virtual microphone arrays for the same purpose . Nevertheless , there is interesting work that provided useful information for my work . Perhaps the closest work to my research that I found is presented by Hu et al . [ 28 ] . Hu et al . propose a method to utilize the reverberant information , known as the Direct- to-Reverberant Ration ( DRR ) , from a single channel recording for Speaker Diarization . Hu et al . estimate the DRR using the algorithm from Peso Parada et al . [ 29 ] and combine it with a Mel-Frequency Cepstral Coefficient ( MFCC ) diarization method proposed by Vijayasenan et al . [ 30 ] . The principle is to use both MFCC and DRR features in combination so a trained system can perform a clustering type of classification . The estimates for the DRRs are computed using features such as Signal-to-Noise ratios , 10 MFCCs , power spectrum , and zero-crossing rates . It is important to notice that this work was tested only using simulated meeting recordings and assumes that the speakers are stationary . Because the research work in this dissertation proposed virtual microphone simulations , it is necessary to present some relevant work in this area . Yoshioka et al [ 31 ] describe a way of linking several recording devices , such as laptops or mobile phones , to create a virtual microphone array . Once the link has been established , the multi-channel audio can be used for speaker diarization . Yoshioka et al . claim to achieve a 13.6 % diarization rate when 10 % of the speech duration contains more than one speaker . The Yoshioka et al . approach is very innovative but requires the presence of several recording devices in the meeting room . More aligned with this dissertation is the work of Katahira et al . [ 32 ] , Del Galdo et al . [ 33 ] , and Izquierdo [ 34 ] . Here the authors propose methods to simulate arrays of microphones by interpolating the signal received by two physical microphones . The authors demonstrate that the virtual microphone arrays improve the SNR in reverberant environments , hence their potential application for speech processing devices . Even though these methods succeed in emulating a set of virtual microphones , they need at least two physical microphones as “ seed ” , which are not available for the method presented in this dissertation . Finally , Tapia et al . [ 35 ] presented a bilingual speech recognition method inspired in the research presented in this dissertation . Tapia utilizes still video frames to estimate the approximate geometry of the speakers and simulate the center microphone reception 11 using Pyroomacoustics . The simulated audio is used along with ALOME transcriptions to generate the training sets for a convolutional neural network . 1.3 Thesis Statement The main objective of this dissertation is to develop a method that applies spatial information and virtual microphone arrays to identify multiple speakers in a single channel audio recording of a collaborative environment and provide activity statistics of each of the participants . This method is aimed to succeed in challenging environments with multiple active speakers and background noise , conditions that make the current state-of-the-art methods perform poorly . For this purpose , the work in this dissertation presents the implementation of an acoustic model based on a virtual room of rough similar geometry to the actual acoustic scene being analyzed , and then the simulation of the signals received by a virtual microphone array located in the virtual scenario . The signal delay between each virtual microphone represents the relative physical position of the active source that in this case is each speaker . The research goal is to find a suitable way to extract the spatial location embedded into a single channel recording to implement the model and subsequent virtual microphone array . 1.4 Contributions The contributions expected from this work include : 12  A method to identify speakers in a collaborative environment by extracting spatial information from a single channel audio recording utilizing an acoustic simulation and virtual microphones .  A solution for the limitations of current state-of-the-art speaker identification methods concerning : o Multiple speakers o Speaker gender or accent o Background Noise and reverberation .  Development of speaker identification framework that is based on an explainable model developed in terms of the physical characteristics of the problem , and hence does not require large datasets to train many parameters .  The basis for a tool for quantitative analysis of video recordings for assessing the level of interaction of participants in collaborative environments . 1.5 Dissertation Overview This dissertation is divided into 6 chapters that cover background theory and other related work , a description of our method , experiments , and results , and conclusion and recommendations for future work . The dissertation is presented as follows :  Chapter 2 gives a background of audio spatial theory and its applications in speaker diarization and identification , and how they functionally compare with other state- of-the-art methods . 13  Chapter 3 presents the foundations on which the proposed method in this Dissertation is based and a block diagram of its implementation .  Chapter 4 describes the practical implementation , including software , model implementation , simulation ions , video analysis , and audio segmentation .  Chapter 5 presents the experimental results obtained when analyzing audio under controlled and uncontrolled environments , and the experimental comparison of our method against current Google and Amazon speaker diarization methods .  Chapter 6 presents a summary of this dissertation and possible future work .  Appendix A contains the scripts and pseudo-code for the Python implementation of Pyroomacoustics .  Appendix B presents the most important LabVIEW Sub-VIs front panels and block diagrams .  Appendix C contains the specifications of the equipment used in the audio laboratory . 14 Chapter 2 . Background This chapter introduces the principles that form the foundations that define the method described in this dissertation . The section begins with basic acoustic theory , concepts , and definitions , and continues with a presentation on microphones and microphone arrays . It finalizes with an introduction to methods for speaker diarization and identification , covering both classic methods and Deep Learning methods . 2.1 Acoustics Principles The perception of sound by a sound capturing device ( e.g. , a microphone or human ear ) , not only depends on the characteristics of the sound source , but it also depends on the medium where the sound propagates , the physical environment where the sound source is located , and the relative locations of the capturing devices and the source . This dissertation , considers all these factors to create models that represent the environment where the sound sources , i.e. , the speakers , are active . Chapter 1 presented a brief introduction to the AOLME program . The AOLME video recordings were taken inside rooms where the participants gather in groups sitting around tables . The exact geometry of the room is unknown , but the video recording provides clues about the location of the speakers , the separation between them , their physical height , and the location of the recording microphone . These clues can be used for modeling a virtual room which can be defined as a three-dimensional enclosed space where 15 the acoustic event takes place . This virtual room may not be necessarily the whole space where all the AOLME participants are , but it can be the space surrounding the participants in a single table . The approximate geometrical and physical characteristics of the virtual room allow us to emulate the reception on arrays of virtual microphones . 2.1.1 Sound Propagation : Near and Far Fields Consider an acoustic source such as a person speaking , a stereo system playing a song , or a running ventilation fan . Sound from these sources propagates in the form of circular air pressure waves , away from the source . They can propagate in all directions if the source is in an open field ( Fig . 1a ) , or directionally if the source is in proximity to a non-conducting medium such as a wall ( Fig.1b ) . In acoustic theory , the relative location of a source to a point in space determines its field location . A source is in the near field if its distance to a point is less than one wavelength of the acoustic signal it is emitting . Sources that are located at distances greater than one wavelength are located at the far-field . The field location of a source plays an important factor when modeling the perception of sounds wave at a point in space . 16 Figure 1 : Propagation of Sound Waves . ( a ) Free Field . ( b ) Directional . Fig . 2 shows a representation of the near field , the transition zone , and the far-field . In the near field , the sound waves behave turbulently , with more circulation than propagation . At about a distance of one wavelength from the source , the sound waves begin transitioning into propagation . At more than one wavelength , sound waves mostly propagate into the infinite . A point located at the near field perceives the sound waves as circular while one located at the far-field will consider these waves planar [ 36 ] . 17 Figure 2 : Near and Far-Field Areas . 2.1.2 Sound Propagation : Direct Path , Reflections , and Reverberation The perception of sounds varies depending on whether the listener is located inside a theater room , small dormitory , or an open field . These differences in perception are the result of the behavior of the sound waves when they propagate across a medium . To visualize this phenomenon , consider for example a room where there is one acoustic source S ( a person speaking ) and one microphone M , as represented in Fig . 3 . 18 Figure 3 : Direct and Reflected Paths for Sound Propagation in a Diffuse Field . In Fig . 1 ( a ) and Fig . 1 ( b ) , sound from the source will reach an observer or receiver directly , from one direction without reflections . In this case , the source is said to be in an acoustic free field . Fig . 3 , in contrast , represents a diffuse field . In this case , the sound reaches the microphone from more than one direction due to reflections . As in the free field , the direct signal received at the microphone is characterized by the distance from the source to the microphone . This distance determines the sound pressure at the microphone , and the time it takes from the sound wave to reach the microphone . This time is known as the Time of Arrival ( TOA ) , and it is a function of the speed of the sound in the room and the Euclidian distance from the source to the microphone . Each reflection contributes similarly . The signal received at a microphone can be expressed in mathematical terms . If we consider a signal s ( t ) from an acoustic source located in the far-field , this signal is captured 19 by a microphone as a signal x ( t ) that is the convolution of the Room Impulse Response ( RIR ) h ( t ) with additive noise w ( t ) as given by : 𝑥 ( 𝑡 ) = 𝑠 ( 𝑡 ) ∗ ℎ ( 𝑡 ) + 𝑤 ( 𝑡 ) ( 2.0 ) . The RIR is unique for every two points in the room and depends on the geometry of the room , the absorption of the materials in the room , and the frequency of the sources [ 37 ] . The RIR consists of three parts : the direct path , the early reflections , and the late reverberations . The direct path component is determined by the Euclidian distance of the source to the microphone , and it is a function of the Time of Arrival ( TOA ) or the time it takes for the signal to travel from the source to the microphone . The other two components of the RIR are related to the reflections of the sound waves at the walls and objects in the room . The early reflections usually arrive 5 ms after the direct path . The late reverberations arrive 20 or 30 ms after the early reflections begin . The RIR can then be expressed as the summation of each of the impulse responses corresponding to the direct path and the reflections : ( cid:3012 ) ℎ ( 𝑡 ) = ( cid:3533 ) ℎ ( cid:3038 ) ( 𝑡 ) + 𝑚 ( 𝑡 ) ( cid:3038 ) ( cid:2880 ) ( cid:2869 ) ( 2.1 ) , where K is the number of reflections , k is the index number of the reflection , and m is the measurement noise . The RIR lasts until the reverberation energy decays to 60 dB on what 20 is known as the T60 time . The T60 was calculated empirically by Sabine in 1890 and can be expressed as : 𝑇 ( cid:2874 ) ( cid:2868 ) = 55.25 ∙ 𝑉 𝑐 ∙ 𝑆 ∙ 𝑎 ( 2.2 ) , where V is the total volume of the room , c is the speed of sound , S is the total surface of the room , and a is the absorption coefficient of the room ( 0 to 1 ) . The reverberations are characterized by the frequency of the sources but , in the case of the early reflections , this influence is minimum [ 27 ] . Fig . 4 depicts a representation of a RIR with its three components . Figure 4 : Representation of the Room Impulse Response and its Components . 21 The path of the reflections from the walls can be represented as direct paths coming from imaginary sources called Images . The signal at any microphone would be then represented by the number of contributing sources plus their image reflections . All acoustic reflections are subject to a TOA that depends on the distance of the path of the reflection . Section 2.3 presents more detail on the concept of acoustic images and their role in room simulation . 2.2 Microphones and Microphone Arrays The previous section introduced microphones as devices capable of capturing sound . In general terms , microphones are sensing devices that detect changes in air pressure and convert these changes into electrical signals . Microphones are categorized by their electrical conversion type and their directionality pattern . Deep technical details for each type of conversion and directionality pattern microphone are out of the scope of this dissertation . The dissertation will only consider the type of microphones used during the research . 2.2.1 Classification of Microphones This research used two types of physical microphones : Condenser omnidirectional , and condenser cardioid . The condenser term refers to the type of electrical conversion of the sound , and the terms omnidirectional and cardioid refer to the directionality of the microphone . 22 Condenser microphones work by utilizing a variable condenser that detects the air pressure changes . The change in pressure translates into a movement of the plates of the condenser thus changing its capacitance . The changes in capacitance are measured by the changes in the charging current in a circuit . Condenser microphones are also known by the name of electret . They are the most popular type of microphones today . The directivity pattern of a microphone determines its gain or sensitivity according to the direction of the incoming sound . Omnidirectional microphones are equally sensitive to incoming sound from any direction . These microphones are simple pressure sensing devices or acoustic monopoles . Cardioid microphones are also known as pressure gradient microphones , and they are characterized for a directionality pattern that is like a heart ( hence their name cardioid , from the Greek “ heart ” ) . Fig . 5 shows the typical directivity pattern for omnidirectional ( a ) and cardioid ( b ) microphones [ 38 ] . All AOLME videos were recorded using Audio-Technica ATR3350 condenser omnidirectional microphones . 23 Figure 5 : Polar Pattern Plot of Directivity of Two Types of Microphones : ( a ) Omnidirectional . ( b ) Cardioid . Regardless of their type. , all microphones generate noise . The conversion of sound pressure waves into electrical signals carries electrical noise , which has a flat spectrum [ 39 ] . Manufacturers usually indicate the electrical noise of their microphones in a Signal to noise ratio ( SNR ) number at a certain sound level . Appendix C contains the technical specifications of the microphones used for this research . 2.2.2 Microphone Arrays When two or more microphones are arranged into a geometric pattern , they become a microphone array . Microphone arrays have important functional properties that are of 24 interest when capturing sound in noisy environments , or when directionality is needed to discriminate between sound sources . An important part of the results of our research is based on the functional characteristics of microphone arrays . Microphone arrays allow for the incorporation of spatial dimensionality to sound capturing . The difference between the signals captured by any two microphones separated a distance d provides information that can be used for source localization , tracking , and general noise reduction . Microphone arrays can be expressed mathematically by ( 2.3 ) 𝒙 = 𝑠𝒅 + 𝒗 ( 2.3 ) , where x represents the vector of all microphone signals , s is the source audio signal , d is the propagation vector represented in ( 2.4 ) , and v is the additive noise [ 40 ] . The vector d is expressed by ( 2.4 ) 𝒅 ( 𝑓 ) = [ 𝑎 ( cid:2869 ) 𝑒 ( cid:2879 ) ( cid:2870 ) ( cid:3095 ) ( cid:3033 ) ( cid:3099 ) ( cid:3117 ) … . 𝑎 ( cid:3041 ) 𝑒 ( cid:2879 ) ( cid:2870 ) ( cid:3095 ) ( cid:3033 ) ( cid:3099 ) ( cid:3289 ) … . 𝑎 ( cid:3015 ) 𝑒 ( cid:2879 ) ( cid:2870 ) ( cid:3095 ) ( cid:3033 ) ( cid:3099 ) ( cid:3263 ) ] ( cid:3021 ) ( 2.4 ) , where an represents the attenuation factor 1 ( cid:3415 ) ( 𝑛 ) , 𝜏𝑛 is the channel delay 𝑑 ( cid:3046 ) 𝑐 ( cid:3415 ) ( 𝑛 ) and 𝑑 ( cid:3046 ) 𝑑 ( cid:3046 ) ( 𝑛 ) is the distance between the source and a microphone n , with c the speed of sound . The fine details of the theory behind microphone arrays are out of the scope of this dissertation . Nevertheless , it is important to have a basic knowledge of the properties of 25 microphone arrays due to their applications in source localization , spatial filtering , and source separation . All of these are applications related to this research and will be discussed later in this section . 2.2.2.1 Microphone Arrays Configurations The possible geometries of microphone arrays are infinite . These different geometries are guided by the number of microphones that can practically be allocated to an array , and the type of acoustic scenario the array is intended to operate . The most common types are linear , circular , and volumetric ( 3D ) [ 41 ] . a ) Linear Microphone Arrays : In this type of array , the microphones are linearly arranged . Fig . 6 ( a ) represents a five-microphone array with a separation of 0.05 m between microphones . This array configuration is very popular , and it is designed to capture the sound that is in front of it . This type of array can not distinguish from sounds that are coming from the same angle to the axis of the array , as the sound waves will arrive at the microphones with the same time delay . Fig . 6 ( b ) shows the directivity pattern of the microphone array of Fig . 6 ( a ) , calculated at 450 Hz with the speed of sound c = 343 m/s . 26 Figure 6 : Linear Microphone Array . ( a ) Geometry . ( b ) 3D Directionality Pattern . A variant of this type of array is a cross-linear array , also known as a planar microphone array . This type of array consists of two linear arrays perpendicular to each other , as shown in Fig.7 . This is the type of array used in this dissertation for virtual microphone simulations . 27 Figure 7 : Cross-Linear Array and Azimuth Directivity Pattern . b ) Circular Microphone Array : This microphone array has its elements positioned circularly . They can consist of one circle , or several concentric circles , as shown in Fig . 8 . This type of array is commonly found in conference equipment that is in the center of a meeting table . 28 Figure 8 : Circular Microphone Array and Directivity Pattern . c ) Volumetric ( 3D ) Array : This type of array forms a lattice with its elements , as shown in Fig . 9 . They can capture sound from any direction , for as long as they are “ suspended in the air ” with no other interference . Their shape can vary as cubes , spheres , or cylinders . 29 Figure 9 : Volumetric Microphone Array and 3D Directivity Pattern . 2.2.2.2 Spatial Aliasing Signal aliasing occurs when the sampling frequency is less than twice the largest signal frequency component . When the bandwidth of the signal is greater than half of the sampling frequency , spectral overlapping happens . Spatial aliasing occurs similarly . To reconstruct a spatial signal from a set of samples , it is necessary to have a spatial sampling period that is less than half of the signal wavelength [ 42 ] . In microphone arrays , the phase difference between two microphones should be less than π to avoid spatial aliasing [ 43 ] . This constraint means that given a signal of frequency f , there is maximum distance 𝑑 between microphones before spatial aliasing occurs , and vice versa . For an audio signal of wavelength λ , this distance is half of the wavelength : 30 𝑑 ≤ 𝜆 ( cid:3040 ) ( cid:3036 ) ( cid:3041 ) 2 ( 2.5 ) , which translates to a maximum frequency of 𝑓 ( cid:3040 ) ( cid:3028 ) ( cid:3051 ) ≤ 𝑐 2𝑑 ( 2.6 ) , where 𝑐 is the speed of sound . 2.2.2.3 TDOA and Cross-Correlation A very important property of microphone arrays is the Time Difference of Arrival ( TDOA ) between microphones . The TDOA is defined as the difference in time a signal takes to reach two points separated by a certain distance 𝑑 . To understand this concept , assume there are two microphones 𝑀𝑖 and 𝑀𝑗 separated by a distance d , and sound source S located at distances 𝐷𝑖 and 𝐷𝑗 from microphones 𝑀𝑖 and 𝑀𝑗 , respectively , as shown in Fig . 10 : 31 Figure 10 : Location of Source and Microphones for Time Difference of Arrival . The difference in the distance ∆𝑫 between 𝐷𝑖 and 𝐷𝑗 is defined as : ∆𝑫 = 𝑐 ∗ ( ∆𝑡 ) ( 2.7 ) , where 𝑐 is the speed of sound and ∆𝑡 is the TDOA between 𝑀𝑖 and 𝑀𝑗 . Conversely , if d and ∆𝑡 are known , it is possible to determine 𝐷𝑖 or 𝐷𝑗 if one of them is known . From ( 2.7 ) , it is also possible to infer the proximity of the source to either microphone by the sign of ∆𝑫 . Because ∆𝑡 = 𝑡 ( cid:3036 ) − 𝑡 ( cid:3037 ) , a positive ∆𝑡 indicates that 𝑀𝑖 is closer to the sound source than 𝑀𝐽 , whereas a negative ∆𝑡 indicates the opposite . The signal delay between microphones 𝑀𝑖 and 𝑀𝑗 can be also expressed in terms of their cross-correlation ( CC ) . Let 𝑟 ( cid:3036 ) , ( cid:3037 ) ( 𝑡 ) = 𝑥 ( cid:3036 ) ( 𝑡 ) ⊛ 𝑥 ( cid:3037 ) ( 𝑡 ) denote the cross-correlation 32 between microphone signals 𝑥 ( cid:3036 ) ( 𝑡 ) , 𝑥 ( cid:3037 ) ( 𝑡 ) corresponding to the microphones 𝑀𝑖 and 𝑀𝑗 . The CC 𝑟 ( cid:3036 ) , ( cid:3037 ) ( 𝑡 ) between these two signals is defined as : 𝑟 ( cid:3036 ) , ( cid:3037 ) ( 𝑡 ) ≜ E ( cid:3427 ) 𝑥 ( cid:3036 ) ( 𝑡 ) 𝑥 ( cid:3115 ) ( cid:3365 ) ( 𝑡 ) ( cid:3431 ) ( 2.8 ) . The normalized cross-correlation is defined by : 𝑅 ( cid:3036 ) , ( cid:3037 ) ( 𝑡 ) = ( cid:2869 ) ( cid:3028 ) ∙ ( cid:3029 ) 𝑟 ( cid:3036 ) , ( cid:3037 ) ( 𝑡 ) ( 2.9 ) , where the 𝑎 , 𝑏 are defined using 𝑎 = ( cid:3495 ) ∑ 𝑥 ( cid:3036 ) ( cid:3047 ) ( cid:2870 ) ( 𝑡 ) and 𝑏 = ( cid:3495 ) ∑ 𝑥 ( cid:3037 ) ( cid:3047 ) ( cid:2870 ) ( 𝑡 ) . 2.2.2.4 Beamforming and Spatial Filters The process of filtering each of the outputs of the microphones of an array into a single output is known as beamforming . Beamforming steers the array ’ s directivity pattern into a particular direction using beamforming filters [ 40 ] . The combination of the signals from each microphone is governed by : 𝑦 = 𝒘𝑯𝒙 ( 2.10 ) , 33 where w represents the beamforming filters and 𝒘𝑯 is the conjugate transpose . The beamforming filters can be estimated as a function of a propagation vector d and a noise correlation matrix Q using : 𝒘 = 𝑸 ( cid:2879 ) ( cid:2869 ) 𝑑 𝒅 ( cid:3009 ) 𝑸 ( cid:2879 ) ( cid:2869 ) 𝒅 ( 2.11 ) . The filter described in equation ( 2.11 ) is known as the Minimum Variance Distortionless Response ( MVDR ) , and it is one of the most popular types of beamforming filter . Refer to [ 31 ] for a full explanation of beamforming filters . If the location of the sound sources d is known , it is possible to construct a spatial filter for each of the sources . This approach is used to minimize crosstalk between channels and for noise reduction . The details of Spatial Filtering are outside the scope of this dissertation . Nevertheless , a brief introduction is presented because future work proposed in this dissertation includes a possible combination of the proposed method with spatial filtering and beamforming for speaker separation . 2.3 Modeling of Room Acoustics The proposed research requires the modeling of room acoustics . The simulation of microphones and sources are all based on physical models that predict the effects of the acoustic reflections given the geometry of the room and the location of the speakers . To this end , simulations calculate RIRs to the target points . The methods to model room 34 acoustics are dived into two categories : geometrical acoustics-based and wave acoustic- based [ 44 ] . Geometrical acoustics-based methods work by capitalizing the reflection properties of sound , i.e. , sounds reflect into smooth surfaces in the same way light does , following Snell ’ s law . These methods are relatively easy to implement but do not take into consideration the roughness of the reflective surface . On the other hand , wave acoustic- based methods take into consideration the characteristics of the sound wave , providing a more accurate simulation . In contrast with geometry methods , wave methods are more computationally intensive and are limited to low-frequency ranges [ 45 ] . The simulation package used for this research is geometry acoustic-based ; wave acoustic-based methods are not considered in this dissertation . The two more common geometry acoustic-based methods of modeling are the Ray Tracing Method and the Image Source Method . This dissertation focus on the Image Method as this method is the one used by the simulation package . 2.3.1 Ray Tracing Method The Ray Tracing Method assumes that sound radiates from the source as several rays [ 44 ] whose energy is the total energy of the source divided by the number of rays . These rays propagate at the speed of sound and , when they reach a boundary surface , some of the energy is reflected in an angle 𝛼′ equal to the incidence angle 𝛼 , as it is shown in Fig . 11 ( a ) . The perceived sound at any point is represented by an echogram that contains the history of all the ray reflections [ 45 ] plus the direct ray . The Ray Tracing Method was 35 introduced in the late 1960s and was widely used until the 1980s . Ray Tracing is a relatively straightforward method , but its resolution is limited [ 45 ] . 2.3.2 Image Source Method The Image Source Method ( ISM , also known as Mirror Image Source Method MISM ) , is perhaps the most popular modeling method in use [ 46 ] . Image methods are used to solve physics problems , and in the late 1970s , Allen and Berkley [ 46 ] , [ 47 ] introduced an algorithm to RIR related applications . In the Image Source Method , a virtual image or specular reflection of the source is created perpendicularly to the source , as shown in Fig . 11 ( b ) . The sound received by the sensor 𝑀 is the summation of the sound from the source 𝑆 and the image source 𝑆′ . Figure 11 : Simulation Methods . ( a ) Ray Tracing . ( b ) Image Source Method . 36 The ISM needs the amplitude and delay of the image sources to calculate the RIR . Because there are infinite possible reflection paths , the ISM creates a map of mirrored rooms with the position of the number of desired images , as shown in Fig . 12 . Figure 12 : Source Image Map . The coordinates of each of the images are calculated using the map with the corresponding room size and the position of the source . Once the position of the images is calculated , the Euclidian distance 𝑑 ( cid:3041 ) from the image 𝑛 to the source is used to calculate the delay 𝜏 ( cid:3041 ) = ( cid:3031 ) ( cid:3289 ) ( cid:3030 ) , where 𝑐 is the speed of sound inside the room . Finally , the amplitude 𝐴 ( cid:3041 ) of each of the signals from the images is calculated by the reflection coefficient 𝛽 ( cid:3041 ) of each of the walls crossed by the path from the image to the sensor , using ( 2.12 ) [ 46 ] : 37 𝐴 ( cid:3041 ) = 𝛽 ( cid:3041 ) 4𝜋 . 𝑑 ( cid:3041 ) . ( 2.12 ) . The RIR ℎ ( 𝑡 ) is calculated using the amplitude and the delay for the images : ℎ ( 𝑡 ) = ( cid:3533 ) 𝐴 ( cid:3041 ) ⋅ 𝛿 ( 𝑡 − 𝜏 ( cid:3041 ) ) ( 2.13 ) , ( cid:3041 ) ∈ℕ where ℕ represents the image sources and 𝛿 is the impulse function . 2.4 Characteristics of the Human Speech The performance of the method described in this dissertation will improve if the acoustic models are tailored to human speech . Human speech has some characteristics that can be exploited and used to compensate for some of the deficiencies encountered with the approximation of the geometry of the room and the limitations of the modeling software . Two characteristics of human speech : fundamental frequency and directionality are of particular importance . Speech is a non-stationary signal , or rather said , a non- stationary process , meaning that its frequency content is not unique in any given interval of time . The fundamental frequency of the human voice varies from 85 Hz to 180 Hz , with women going up to 255 Hz , and children to 300 Hz and even higher [ 48 ] . The whole spectrum of the human voice contains frequencies that go up to 8kHz . Much of the energy 38 is found in frequencies that are below 500 Hz for males and 800Hz for females [ 49 ] . As a curiosity note , the frequency sensitivity of the human ear is very close to the frequency spectrum of the human voice . This research work focus on the fundamental frequency to develop the acoustic models . More detail is presented in the Experimental Implementation section of this dissertation . The other important characteristic of human speech is its directionality . Speech does not propagate equally in all directions , but rather has directionality due to the location of the mouth and the shadow cast by the head and the torso [ 50 ] . Fig . 13 ( a ) depicts the propagation of sound in the horizontal direction , while Fig . 13 ( b ) presents the propagation in the vertical axis . Lower frequencies propagate farther from the back of the head than higher frequencies . Most propagation occurs at the front of the head . This directionality property is utilized for positioning the speakers in the simulation models . Figure 13 : Directionality of Human Head [ 50 ] . 39 2.5 Speaker Diarization and Identification Section 1.2 presented a background on several methods for speaker diarization and identification that relate to this research . To understand the differences between these methods and the method proposed in this research , this next section reviews the fundamentals on which some of these methods are based on . Reviewing in detail all the methods presented in section 1.2 requires an effort that goes beyond the scope of this dissertation . For this reason , this dissertation only focuses on the most recent and common methods of speaker diarization and identification . 2.5.1 Methods for Diarization and Identification Speaker diarization can be summarized as “ who said what , and when ” , and for “ how long ” . The task of determining for how long one speaker has been active in a multi- participant conversation requires speaker diarization and subsequent identification with non-abstract labels . Speaker identification should not be confused with speaker verification . A system that accepts or rejects the identity claim by a speaker is called a speaker verification system . This dissertation divided these methods into two categories : Classic methods and Deep Learning methods . 2.5.1.1 Classical Methods for Diarization and Identification Anyone conducting a web search for “ speaker diarization and identification methods ” will find thousands and even millions of documents that somehow relate to the 40 subject ( by the time of this dissertation , “ speaker diarization methods ” gave 66,400 hits , “ speaker identification methods ” about 48,000,00 hits , and “ speaker diarization and identification methods ” some 121,000 ) . Nevertheless , until researchers started using Deep Learning and Neural Network methods , most speaker diarization and identification methods consisted of four basic modules or steps : A feature extraction module , a speech or voice activity detector ( SAD or VAD , respectively ) , a segmenter or speaker change module , and finally , a clustering mechanism [ 51 ] , [ 52 ] . Fig . 14 shows a block diagram of the four modules . Figure 14 : Block Diagram of a Typical Speaker Diarization System . The feature extraction module generally uses Mel-Frequency Cepstral Coefficients ( MFCC ) as features . Not as popular as MFCCs , Linear Frequency Cepstral Coefficients ( LFCC ) , and Perceptual Linear Predictive are also used as features [ 52 ] . The purpose of the speech activity module ( SAD ) , also known as the voice activity module ( VAD ) , is to detect the presence of speech . SADs or VADs ( hereon referred to as 41 VAD ) eliminate audio segments that contain no necessary information , such as noise or music , thus improving the performance of the segmenting and clustering modules . There are several different algorithms for these detectors , that vary from just energy level detection to binary classifiers based on pre-trained speech models . This research uses a custom-made VAD to segment the audio into frames , as it will be shown in later chapters . The next module to follow is the segmenter or speaker change detector . The segmenter detects when there is a speaker change in the audio and creates frames that ideally only contain one speaker . This is a necessary step before clustering , where the grouping of the clusters is done without previous information . A common method of segmentation is to measure the distance between two segments . Segments that belong to the same speaker are usually closer in distance than those coming from a different speaker . The models are usually Ergodic Hidden Markov Models ( HMMs ) , where each state represents a speaker , and the probabilities are modeled by Gaussian Mixture Models ( GMMs ) . Bayesian Information Criterion ( BIC ) is used to determine the nearest clusters , merging the clusters that generate the highest BIC , stopping the process when the values of the BIC are no longer positive . This algorithm was introduced by Chen et al . [ 53 ] and it is defined as ( 2.14 ) for a parametric Gaussian Mixture Model ( GMM ) with clusters 𝐶 with features [ 51 ] 𝐵𝐼𝐶 ( 𝑀 ) = log ℒ ( 𝑋|𝑀 ) − 𝜆 2 # ( 𝑀 ) log ( 𝑁 ) ( 2.14 ) , 42 where 𝑁 is the number of samples , # ( 𝑀 ) is the number of parameters of the model , and 𝜆 is a tunable parameter . The final clustering step groups together segments that belong to the same speaker . In most speaker diarization and identification approaches , clustering is achieved by agglomerative hierarchical clustering ( AHC ) . Using the same distance concept , each segment is its cluster at the beginning of the process , and parts of clusters are merged until the stopping criterion is met . This criterion is ideally to get the number of clusters equal to the number of speakers . In practical terms , the stopping criterion is a threshold that is preset at the beginning of the process . 2.5.1.2 Deep Neural Networks The applications of Deep Neural Networks ( DNNs ) to speaker diarization have gained a lot of momentum in recent years . It is difficult to keep pace with the amount of research that is done on an almost monthly basis in this field . It is therefore of importance to have a basic understanding of how DNNs are applied to the problem of speaker diarization . In general terms , DNN speaker diarization/identification methods are divided into four groups [ 54 ] : Stage-wise , end to end , online , and multimodal . From these groups , this dissertation will address stage-wise and multimodal groups as they relate more to the research work . 43 2.5.1.2.1 Stage-wise diarization Stage-wise diarization methods are based on the same blocks or stages as the GMM methods covered in the previous section , but they rely on DNNs that employ a universal background model ( UBM ) , rather than GMMs for feature extraction and clustering . For GMMs to be computationally efficient for feature extraction , each sequence of feature vectors is converted into a fixed-length vector , or supervector [ 55 ] . Because this approach makes GMMs susceptible to speaker and channel variations of utterances [ 56 ] , it is desirable to reduce the dimensionality of the supervectors . These lower-dimensional vectors are called i-vectors ( the i-vectors were previously referenced in the background section ) . The representation of i-vectors assumes that speaker and channel-dependent variabilities reside in a lower-dimensional space [ 57 ] , which is represented by a total variability matrix T. For GMMs this conversion can be expressed as : 𝑠 = 𝑠 ( cid:4593 ) + 𝑇𝑤 ( 2.15 ) , where 𝑠′ is the speaker and channel supervector and 𝑤 is the i-vector . Fig . 15 shows a GMM/i-vector framework [ 56 ] . 44 Figure 15 : GMM/i-vector Framework . At this point , the effort moved to replace the GMM generated i-vectors for DNN generated i-vectors . The idea behind this approach is to replace the GMM generated posteriors for the feature vectors and take a DNN trained acoustic model using senones to generate these posteriors . Fig . 16 represents this approach [ 54 ] . Figure 16 : DNN/i-vector Implementation . Although the performance of DNN based acoustic models has been proven [ 56 ] , they require a large set of training data and more computational cost as well . 45 In addition to i-vectors , other DNN approaches include the use of d-vectors and x- vectors embedding . D-vectors were introduced by Variani et al . ( 2014 ) , and they are based on assigning the ground truth training utterance to labels of the training frames to the corresponding utterance in the training stage , converting the problem into a classification one . For a more detailed description of d-vectors , refer to [ 54 ] , [ 56 ] . X-vectors are derived from d-vectors . Instead of using frame-by-frame speaker labels , they use utterance-level speaker labels by aggregation . As was referred to in the background section , x-vectors outperform most i-vector and d-vector approaches . Refer to [ 54 ] , [ 56 ] , for details on x-vectors . Deep Learning clustering techniques are also applied in replacement of conventional distance and similarity methods . Clustering is treated as either a supervised or unsupervised problem , by employing recurrent neural networks ( RNN ) or discriminative sequence-to-sequence neural networks . References to these methods can be found in [ 54 ] . 2.5.1.2.2 Multimodal Speaker Diarization Related to our approach of exploiting video clues and spatial information , Deep Learning is applied to the analysis of not only visual clues , such as movement of lips [ 58 ] , but also to the content of the speech of the participants [ 59 ] , [ 60 ] . In this sense , multimodal methods train the networks based on the patterns that most likely belong to a genre of participants . For example , in a collaborative environment with students , the facilitator is more likely to have a calm voice , in contrast with the students . In recent publications , W. 46 Kang et al . [ 61 ] , have presented speaker diarization based on d-vectors combined with spatial information provided by microphone arrays . 2.5.2 Current State-of-the-Art Methods for Diarization and Identification State-of-the-art methods cover the speaker diarization and transcription that major technology players are offering . They keep their technologies a secret , as they compete to have the most reliable service available , thus the difficulty in obtaining detailed information on how their methods work . It is expected that they somehow use the speaker diarization approaches reviewed in the previous sections . IBM , Google , Amazon , and Microsoft offer cloud computing that includes speech processing services based on algorithms that use Deep Learning and Machine Learning . Amazon ’ s , Google ’ s , and Microsoft ’ s are all closed-source cloud services that provide an API for speech-to-text processing and speaker diarization . This dissertation reviewed Amazon ’ s Transcribe ( AWS ) [ 62 ] , Google ’ s Cloud [ 63 ] , and Microsoft Azure Speech Services [ 64 ] , and experimentally compared Amazon ’ s and Google ’ s against our proposed system . Amazon ’ s Transcribe accepts either audio files or streaming data , single-channel , and outputs text files with speaker diarization if this option is selected , and the number of speakers is specified . Amazon ’ s Transcribe works better with 2-5 speakers , and it is language-dependent , limited to 120 minutes of audio . Amazon ’ s Transcribe stores the voice data to train the models [ 65 ] unless the users select the option to delete this data . 47 Amazon offers a highly trained set of models called Amazon Transcribe Medical which is aimed at medical transcriptions . Users can also customize the vocabulary to better fit their needs . Amazon functionality can be accessed via REST and SOAP protocol over HTTP [ 66 ] . Google ’ s Cloud works similarly , with an interface for long speech , single-channel input for transcription purposes [ 65 ] . The optimum number of speakers is set at a maximum of 5 . As with Amazon Transcribe , Google offers the option of privacy that prevents data logging that could be used to improve the models . Google ’ s models are optimized for phone conversations or videos , accepting 16 kHz or 8 kHz audio , respectively , depending on the application [ 67 ] . It also offers vocabulary customization . Google offers good scalability , infrastructure , and payment schemes that are considered the best among the technology giants [ 66 ] . Microsoft offers speaker diarization utilizing its Cognitive Services . Microsoft ’ s Diarization system ranked first at the VoxSRC challenge 2020 by achieving a Diarization Error ( DER ) of 3.71 % in development and 6.23 % in evaluation testing [ 68 ] . The datasets consisted of audio collected from YouTube recordings . For the challenge , the network was trained with 1500 hours of simulated mixed training audio . Microsoft Speaker Recognition [ 69 ] offers text-independent speaker recognition/verification . The speakers need to be enrolled to create a signature , which is later compared with the audio to be analyzed . The minimum requirements are 20 seconds of speech for training , and 4 seconds of speech for identification , with unlimited speaker enrollment , with only one speaker present . In the 48 case of diarization , Microsoft can only recognize up to two speakers . Microsoft Transcription requires multi-channel audio for diarization and the signature of the participating speakers for identification , labeling each speech segment with its correspondent speaker . Microsoft does not collect users ’ voice tracks to train its models . Users can customize their vocabulary and the environment they are expecting to operate , meaning that customization must include noise , indoor or outdoor environments , multi- gender speech , etc . [ 64 ] . 49 Chapter 3 . Proposed Method The previous chapter discussed the principles of acoustics , speech , and speaker diarization that served as the foundations for this research . This chapter will cover the mathematical models that are used to estimate the virtual microphones , and how they are implemented into a working system . Finally , it will present a block diagram detailing the function of each of the elements of the proposed system and its operation . 3.1 Methodology The goal of this dissertation is the identification of speakers from single-channel recordings using virtual microphones . This statement includes the objective ( identify speakers ) , the data source ( single channel recording ) , and the means to accomplish this objective ( using virtual microphones ) . This section will begin by identifying the physical and mathematical elements of the models needed for the virtual microphone simulation . Let us consider a collaborative environment such as the one represented in Fig . 17 ( a ) , where we have three speakers sitting around a table with a central recording microphone . Such an environment can be represented as a simple 2D model shown in Fig . 17 ( b ) that shows the relative location of the speakers and the recording microphone . 50 Figure 17 : Collaborative Environment ( a ) with 2D Model ( b ) . To capitalize on the properties of microphone arrays , it is necessary to find a method to simulate several virtual microphones based on the information contained in the signal captured by the central microphone . From the discussion on microphone arrays in chapter 2 , it is possible to implement several different virtual array configurations . Let us consider a cross-linear array with four microphones and one central recording microphone , as shown in Fig . 18 . 51 Figure 18 : 2D Model of Fig . 12 ( b ) with Microphone Array . If Fig . 18 is an ideal representation , where there are no reflections or room absorption , then for each unique active speaker there will be a set of pairs of microphones with unique TDOAs that correspond to the active speaker . For example , if speaker 3 is active , then the TDOA between M5 and M3 and the TDOA between M2 and M3 will be unique for speaker 3 . Having this concept in mind , we recall from Chapter 2 that the cross- correlation from any pair of microphones represents the signal delay between them . To uniquely identify each of the speakers , we are interested in the location of the peak of the cross-correlation function defined by : 𝑇 ( cid:3036 ) , ( cid:3037 ) = argmax 𝑅 ( cid:3036 ) , ( cid:3037 ) ( 𝑡 ) ( 3.0 ) , 52 where 𝑅 ( cid:3036 ) , ( cid:3037 ) ( 𝑡 ) denotes the cross-correlation between two microphone signals 𝑥 ( cid:3036 ) ( 𝑡 ) , 𝑥 ( cid:3037 ) ( 𝑡 ) . If a source signal propagates to microphones 𝑖 , 𝑗 , 𝑇 ( cid:3036 ) , ( cid:3037 ) represents the time lag that it takes for the signal to reach 𝑗 after reaching 𝑖 . Thus , 𝑇 ( cid:3036 ) , ( cid:3037 ) > 0 implies that the signal arrived at microphone 𝑖 before 𝑗 . On the other hand , 𝑇 ( cid:3036 ) , ( cid:3037 ) < 0 implies that the signal arrived at microphone 𝑗 before 𝑖 . The cross-correlation matrix of all possible values 𝑇 ( cid:3036 ) , ( cid:3037 ) will be used for determining the locations of the speakers . Now we move to the problem of simulating the virtual microphones . From equation ( 2.0 ) from Chapter 2 , it is possible to extend this model for the case of multiple sources and microphones . Suppose that we have 𝐽 possible sources : 𝑠 ( cid:2869 ) ( 𝑡 ) , … , 𝑠 ( cid:3011 ) ( 𝑡 ) and 𝑁 possible microphone signals : 𝑥 ( cid:2869 ) ( 𝑡 ) , . . . , 𝑥 ( cid:3015 ) ( 𝑡 ) . Next , let ℎ ( cid:3037 ) , ( cid:3038 ) ( 𝑡 ) denote the RIR that describes the propagation from the 𝑗-th source to the 𝑘-th microphone . At the 𝑘-th microphone , we receive signals from all sources as expressed by : ( cid:3011 ) 𝑥 ( cid:3038 ) ( 𝑡 ) = ( cid:3533 ) 𝑠 ( cid:3037 ) ( 𝑡 ) ∗ ℎ ( cid:3037 ) , ( cid:3038 ) ( 𝑡 ) + 𝑛 ( 𝑡 ) ( 3.1 ) , ( cid:3037 ) ( cid:2880 ) ( cid:2869 ) where 𝑛 ( 𝑡 ) represents additive white noise . For the model in ( 3.1 ) , we need to estimate ℎ ( cid:3037 ) , ( cid:3038 ) ( 𝑡 ) . If ℎ ( cid:3037 ) , ( cid:3038 ) ( 𝑡 ) is known , it is possible , at least in theory , to estimate the signal source by deconvolving the signal 𝑥 ( cid:3038 ) ( 𝑡 ) with ℎ ( cid:3037 ) , ( cid:3038 ) ( 𝑡 ) ( i.e. , ℎ ( cid:3037 ) , ( cid:3038 ) ( cid:2879 ) ( cid:2869 ) ( 𝑡 ) ) . Once the sources have been estimated , each virtual microphone can be emulated by just convolving the emulated source with each of the RIRs of the virtual microphones . 53 Some important factors need to be considered to develop a model for this approach . First , ℎ ( cid:3037 ) , ( cid:3038 ) ( cid:2879 ) ( cid:2869 ) ( 𝑡 ) may not exist [ 70 ] , and it may be necessary to construct virtual microphone approximations to ℎ ( cid:3037 ) , ( cid:3038 ) ( 𝑡 ) . Second , the speaker feature correlation matrix defined by 𝑇 ( cid:3040 ) is estimated under the assumption that speaker 𝑚 is talking while all other speakers remain quiet : 𝑠 ( cid:3038 ) ( 𝑡 ) = 0 , 𝑘 ≠ 𝑚 . Third , for each audio segment , we need to compute 𝑇 , the cross- correlation matrix of the actual signal . Finally , we need to estimate the active speaker by solving match ( 𝑇 , 𝑇 ( cid:3040 ) ) ( 3.2 ) , max ( cid:3040 ) where match ( . , . ) is a function of the similarity between 𝑇 , 𝑇 ( cid:3040 ) . We now can turn our attention to estimating the RIRs . As it was presented in Chapter 2 , the RIR is a function of the geometry of the room , the relative location of the sources and microphones , and the physics of the room ( i.e. , the absorption of the room , which characterizes the reverberation ) . This information will be very difficult if not impossible to obtain from just the audio from the recording microphone , but we could use information from the video recording to estimate some of the parameters needed to calculate the RIR . From the video recording , it would be possible to approximate the location of the speakers and the virtual microphones to each other . This information , along with an empirical approximation of the absorption of the room , is all that is necessary to calculate the RIR . 54 Calculating the RIR can be a very tedious task . The number of calculations required is a factor in the degree of accuracy desired in the model . If we recall the concept of images from Chapter 2 , the reception at a microphone is the result of the sum of the images ; therefore , the fidelity will depend on the number of images added as part of the RIR function . The next chapter will present an open-source software package that performs these calculations thus saving some programming time . So far , this dissertation has presented the fundamentals of the simulation on which the proposed method is based . By using the approximate geometry of the room to calculate the RIR and to simulate the microphones , we should be able to calculate the cross- correlation between microphones and determine the active speaker . The proposed method then can be summarized in 5 steps : 1 ) Evaluating the room geometry and location of the speakers of the acoustic scene , 2 ) Estimating a generic RIR model for this geometry , 3 ) Training the model with known speaker samples , 4 ) Estimation of the sources that will fit the model for each of the possible active speakers given an unknown audio sample , and 5 ) Conducting a Cross-Correlation Analysis and classification . The following section explains each of the steps in more detail . 1 ) Evaluation of room geometry and location of speakers and microphones As it was described before , the RIR is a unique transfer function between two points in space . To calculate the RIR between a source and a microphone , it is necessary to know their spatial locations inside a physical room of known acoustic characteristics . In Fig . 17 , 55 it is possible to appreciate the relative location of the three speakers and the recording microphone . This video frame can be used as a reference for the location of the sources and virtual microphones in the model , e.g. , from this image we can approximate that the table is about 1.5 meters long by 1 meter wide , that the speakers are separated about 0.7 meters from each other , and the speaker ’ s mouths are about 0.24 to 0.25 m from the table . It is also possible to locate the reference microphone in coordinates that are relative to each of the speakers . These are just approximations to create a generic model from where to calculate the RIRs . Fig . 19 shows a possible 2D model for these approximations . Figure 19 : Possible 2D Model for Fig . 17 . The location and separation of the virtual microphones can be arbitrary for as long as they do not violate the rules of spatial anti-aliasing . As presented in Chapter 2 , the 56 fundamental frequency of human speech varies from 85 Hz to 180 Hz approximately , with some extreme cases going up to 255-300 Hz ( children ) . If it is assumed a max frequency average of 180 Hz using ( 2.6 ) and ( 2.7 ) , the maximum separation 𝑑 for each microphone would be ≤ ( cid:2871 ) ( cid:2872 ) ( cid:2871 ) ( cid:3288 ) ( cid:3294 ) ( cid:2870 ) ( ( cid:2869 ) ( cid:2876 ) ( cid:2868 ) ( cid:3009 ) ( cid:3053 ) ) = 0.95 𝑚 . 2 ) Estimation of the generic RIR model The approximation of the geometry of the room provides the basis to implement a generic model to calculate a set of RIRs to estimate the virtual microphone array . This model , as it was mentioned before , is based on an approximate geometry of the room , the location of the speakers , and the number of reflections . It is desirable to reduce the influence of reflections and reverberation in the simulation as they add complexity to the RIR . One way this can be achieved is by an overall reduction of the length of the T60 . Recalling equation ( 2.2 ) , we can minimize the volume of the room and maximize its absorption as a means of reducing the length of T60 . These two parameters are easy to control and implement in the simulation . The number of images to calculate can be set to an acceptable value that compromises the simulation fidelity and the computational burden . Some trial and error may be necessary to optimize the number of reflections . Another point to consider is the directionality of the human voice . The human voice propagates mostly in one direction to the front of the head ; therefore , our model must take this propagation inequality when simulating the audio reception at any point of the room . 57 One solution implemented in this research consisted of locating the speakers close to the end of the virtual room , so the reflections from the back of the speaker are minimized . With the approximate physical and acoustical characteristics of the room , it is then possible to calculate the RIRs between the virtual microphones of the array and the speakers . It was indicated in the previous section that it could be possible to implement any arbitrary array of microphones for as long as we follow the rules of spatial anti-aliasing . The calculated value of the distance d is well fitted between the boundaries of the proposed model , but it would be beneficial for the performance of the model to optimize the microphone array for maximum cross-correlation information . This can be accomplished by asymmetric microphone arrays , i.e. , arranging the microphones at locations that are offset from equidistant points to the speakers . Also , the microphone arrays should have as many microphones as possible , for as long as the required computational resources remain manageable . 3 ) Estimation of sources and virtual microphones The next step is to apply our generic model to estimate the signal at the virtual microphone array based on the recorded signal at the reference microphone . To do so , it is necessary first to estimate the sources that would fit the model , i.e. , estimate a set of sources that , when convolved with the model ’ s RIRs , will represent the signal at each microphone of the array , including the reference microphone . One way to estimate the sources is to deconvolve the reference signal with the RIR that corresponds to the source we want to 58 estimate . For example , assume that our model has three sources 𝑠 ( cid:2869 ) ( 𝑡 ) , 𝑠 ( cid:2870 ) ( 𝑡 ) and 𝑠 ( cid:2871 ) ( 𝑡 ) , three microphones M1 , M2 , and M3 , with M3 as the reference microphone . If 𝑥 ( cid:2871 ) , ( cid:3037 ) ( 𝑡 ) is the signal received at M3 with 𝑗 = 1,2,3 for the respective sources 𝑠 ( cid:2869 ) , 𝑠 ( cid:2870 ) , and 𝑠 ( cid:2871 ) , then to estimate 𝑠 ( cid:2869 ) ( 𝑡 ) , 𝑠 ( cid:2870 ) ( 𝑡 ) , and 𝑠 ( cid:2871 ) ( 𝑡 ) given 𝑥 ( cid:2871 ) , ( cid:3041 ) ( 𝑡 ) 𝑠̃ ( cid:2869 ) ( 𝑡 ) = 𝑥 ( cid:2871 ) , ( cid:2869 ) ( 𝑡 ) ∗ ℎ ( cid:2871 ) , ( cid:2869 ) ( cid:2879 ) ( cid:2869 ) ( 𝑡 ) ( 3.3 ) , 𝑠̃ ( cid:2870 ) ( 𝑡 ) = 𝑥 ( cid:2871 ) , ( cid:2870 ) ( 𝑡 ) ∗ ℎ ( cid:2871 ) , ( cid:2870 ) ( cid:2879 ) ( cid:2869 ) ( 𝑡 ) ( 3.4 ) , 𝑠̃ ( cid:2871 ) ( 𝑡 ) = 𝑥 ( cid:2871 ) , ( cid:2871 ) ( 𝑡 ) ∗ ℎ ( cid:2871 ) , ( cid:2871 ) ( cid:2879 ) ( cid:2869 ) ( 𝑡 ) ( 3.5 ) , with ℎ ( cid:2871 ) , ( cid:2869 ) ( cid:2879 ) ( cid:2869 ) ( 𝑡 ) the inverse of ℎ ( cid:2871 ) , ( cid:2869 ) ( 𝑡 ) ( RIR from M3 to 𝑠 ( cid:2869 ) ) , ℎ ( cid:2871 ) , ( cid:2870 ) ( cid:2879 ) ( cid:2869 ) ( 𝑡 ) the inverse of ℎ ( cid:2871 ) , ( cid:2869 ) ( 𝑡 ) ( RIR from M3 to 𝑠 ( cid:2870 ) ) , and ℎ ( cid:2871 ) , ( cid:2871 ) ( cid:2879 ) ( cid:2869 ) ( 𝑡 ) the inverse of ℎ ( cid:2871 ) , ( cid:2871 ) ( 𝑡 ) ( RIR from M3 to 𝑠 ( cid:2871 ) ) . Fig . 20 shows how the sources can be estimated for the example of Fig . 17 and the model of Fig . 19 . Once the sources have been estimated , they can be convolved with the remaining RIRs to obtain the simulated reception on each of the microphones of the array . Fig . 21 shows an example of how microphones M1 , M2 , and M3 are estimated using 𝑠 ( cid:2870 ) ( 𝑡 ) . The process is extensive to the other sources as well . We can use an estimation or the ground truth for microphone M3 . 59 Figure 20 : Estimation of the Sources . Figure 21 : Estimation of Virtual Microphones . 60 It may be noticed at this point that the solution presented above will only work if we know which n source is active at 𝑥 ( cid:2871 ) , ( cid:3041 ) ( 𝑡 ) , and which ℎ ( cid:2871 ) , ( cid:3041 ) ( 𝑡 ) we need to deconvolve with . To solve this problem , our method simulates each possible source by deconvolving the signal at M3 with each RIR of the model and then simulates the signal at each of the virtual microphones . The result is a set of virtual arrays that correspond to each of the possible active sources . In the three source examples , if 𝑥 ( cid:2871 ) ( 𝑡 ) is defined as the unknown signal at M3 , the estimate of both possible sources 𝑠̃ ( cid:2869 ) ( t ) , 𝑠̃ ( cid:2870 ) ( t ) and 𝑠̃ ( cid:2871 ) ( t ) is obtained by deconvolving 𝑥 ( cid:2871 ) ( 𝑡 ) with ℎ ( cid:2871 ) , ( cid:2869 ) ( 𝑡 ) , ℎ ( cid:2871 ) , ( cid:2870 ) ( 𝑡 ) and ℎ ( cid:2871 ) , ( cid:2871 ) ( 𝑡 ) : 𝑠̃ ( cid:2869 ) ( 𝑡 ) = 𝑥 ( cid:2871 ) ( 𝑡 ) ∗ ℎ ( cid:2871 ) , ( cid:2869 ) ( cid:2879 ) ( cid:2869 ) ( 𝑡 ) ( 3.6 ) , 𝑠̃ ( cid:2870 ) ( 𝑡 ) = 𝑥 ( cid:2871 ) ( 𝑡 ) ∗ ℎ ( cid:2871 ) , ( cid:2870 ) ( cid:2879 ) ( cid:2869 ) ( 𝑡 ) ( 3.7 ) , 𝑠̃ ( cid:2871 ) ( 𝑡 ) = 𝑥 ( cid:2871 ) ( 𝑡 ) ∗ ℎ ( cid:2871 ) , ( cid:2871 ) ( cid:2879 ) ( cid:2869 ) ( 𝑡 ) ( 3.8 ) , and then emulating two sets of virtual microphones . Each set of the microphones is represented as 𝑥 ( cid:3028 ) , ( cid:3029 ) , where a is the index of the virtual set , b is the index of the virtual microphone of the set , and ℎ ( cid:3030 ) , ( cid:3031 ) is the RIR from source c to microphone d. For set 1 : 𝑥 ( cid:2869 ) , ( cid:2869 ) ( 𝑡 ) = 𝑠̃ ( cid:2869 ) ( 𝑡 ) ∗ ℎ ( cid:2869 ) , ( cid:2869 ) ( 𝑡 ) , ( 3.9 ) , 61 For set 2 : For set 3 : 𝑥 ( cid:2869 ) , ( cid:2870 ) ( 𝑡 ) = 𝑠̃ ( cid:2869 ) ( 𝑡 ) ∗ ℎ ( cid:2869 ) , ( cid:2870 ) ( 𝑡 ) , ( 3.10 ) , 𝑥 ( cid:2869 ) , ( cid:2871 ) ( 𝑡 ) = 𝑠̃ ( cid:2869 ) ( 𝑡 ) ∗ ℎ ( cid:2869 ) , ( cid:2871 ) ( 𝑡 ) . ( 3.11 ) . 𝑥 ( cid:2870 ) , ( cid:2869 ) ( 𝑡 ) = 𝑠̃ ( cid:2870 ) ( 𝑡 ) ∗ ℎ ( cid:2870 ) , ( cid:2869 ) ( 𝑡 ) , ( 3.12 ) , 𝑥 ( cid:2870 ) , ( cid:2870 ) ( 𝑡 ) = 𝑠̃ ( cid:2870 ) ( 𝑡 ) ∗ ℎ ( cid:2870 ) , ( cid:2870 ) ( 𝑡 ) , ( 3.13 ) , 𝑥 ( cid:2870 ) , ( cid:2871 ) ( 𝑡 ) = 𝑠̃ ( cid:2870 ) ( 𝑡 ) ∗ ℎ ( cid:2870 ) , ( cid:2871 ) ( 𝑡 ) . ( 3.14 ) . 𝑥 ( cid:2871 ) , ( cid:2869 ) ( 𝑡 ) = 𝑠̃ ( cid:2871 ) ( 𝑡 ) ∗ ℎ ( cid:2871 ) , ( cid:2869 ) ( 𝑡 ) , ( 3.9 ) , 𝑥 ( cid:2871 ) , ( cid:2870 ) ( 𝑡 ) = 𝑠̃ ( cid:2871 ) ( 𝑡 ) ∗ ℎ ( cid:2871 ) , ( cid:2870 ) ( 𝑡 ) , ( 3.10 ) , 𝑥 ( cid:2871 ) , ( cid:2871 ) ( 𝑡 ) = 𝑠̃ ( cid:2871 ) ( 𝑡 ) ∗ ℎ ( cid:2871 ) , ( cid:2871 ) ( 𝑡 ) . ( 3.11 ) . 62 4 ) Cross-Correlation and Model training The three sets of virtual microphones give us enough information for cross- correlation analysis and model training . To train the model , we take a small audio sample that contains only one active source ( e.g. , see Fig . 22 ) , and we then use this information to generate a cross-correlation table that contains all combinations of possible sources and microphone pairs for that source . For the training , we include the background noise , as is shown in Fig . 22 . Training the model for noise is explained later in the implementation section . These tables are templates for the classification of each of the active sources . For our example of three sources and three microphones , we calculate three cross-correlation tables for each known sample processed with filters for s1 , s2 , and s3 , as shown in Table I , Table II , and Table III . 63 Figure 22 : Training Samples from Each Speaker and Noise . Table I : Template Cross-Correlation Table for Source 1. r1-2 V1 V4 V7 𝑠̃ ( cid:2869 ) , ( cid:2869 ) 𝑠̃ ( cid:2869 ) , ( cid:2870 ) 𝑠̃ ( cid:2869 ) , ( cid:2871 ) r1-3 V2 V5 V8 r2-3 V3 V6 V9 64 Table II : Template Cross-Correlation Table for Source 2. r1-2 V10 V13 V16 𝑠̃ ( cid:2870 ) , ( cid:2869 ) 𝑠̃ ( cid:2870 ) , ( cid:2870 ) 𝑠̃ ( cid:2870 ) , ( cid:2871 ) r1-3 V11 V14 V17 r2-3 V12 V15 V18 Table III : Template Cross-Correlation Table for Source 2. r1-2 V19 V22 V25 𝑠̃ ( cid:2871 ) , ( cid:2869 ) 𝑠̃ ( cid:2871 ) , ( cid:2870 ) 𝑠̃ ( cid:2871 ) , ( cid:2871 ) r1-3 V20 V23 V26 r2-3 V21 V24 V27 where 𝑉 ( cid:2869 ) … 𝑉 ( cid:3041 ) are the values of the cross-correlations 𝑟 ( cid:2869 ) , ( cid:2870 ) , 𝑟 ( cid:2869 ) , ( cid:2871 ) , and 𝑟 ( cid:2870 ) , ( cid:2871 ) corresponding to the microphones M1-M2 , M1-M3 , and M2-M3 respectively , for each known sample source of 𝑠 ( cid:2869 ) , 𝑠 ( cid:2870 ) and 𝑠 ( cid:2871 ) . Table I will contain the results for the sample 𝑠 ( cid:2869 ) , Table II for the sample 𝑠 ( cid:2870 ) , and Table III for the sample 𝑠 ( cid:2871 ) . Training needs to be done just once . 5 ) Analysis and Classification Cross-correlation analysis of multi-speaker audio is not possible unless this is divided into segments . Proper segmentation of the audio is important to the performance of the proposed method . Because the audio from collaborative environments contains multiple speakers , it is possible at any time to have more than one simultaneous active speaker . Also , it is possible to have periods of noise ( there are no periods of silence ) or 65 overlapping speech when one speaker finishes and another one begins speaking . For optimal cross-correlation location identification , each segment should only contain one active speaker at a time . If the audio contains overlapping speech or mixes of speakers , the location of the peak value of the cross-correlation between microphones will depend on the amount of information from each of the speakers that are contained in the audio segment , making the classification more difficult . One solution to maximize the probabilities of having only one active speaker in a segment is by minimizing its length : the shorter the segment is , the most likely it is to have content from only one speaker . There is , however , a limit on the minimum length of the segments . The minimum length of the segments is subject to the performance of the cross- correlation algorithm . This means the segments need to be long enough to contain sufficient information for the algorithm to calculate a meaningful cross-correlation . In addition , the total analysis time is affected by the number of segments that need to be cross- correlated and analyzed , hence the desire of reducing the number of segments . There is then a need for optimizing the length of the segments for a balance between the maximum information content and the minimum overlapping between speakers . Recalling from Chapter 2 , the best way to segment the audio is to incorporate a VAD . Ideally , a VAD will detect speech content and the change of speakers based on a certain pre-determined energy threshold , as shown in Fig . 23 . If the energy threshold is properly adjusted , a VAD can be effective in producing segments of audio that contain only one speaker at the time , and segments that contain mixes of speakers or noise , maximizing their information content . 66 Because VADs are not perfect , there will be always segments that could contain overlapping , mixes of speakers , or simply being misclassified . To minimize the number of misclassifications , the length of the segments can be limited to a maximum that provides an acceptable number of misclassifications . It was found during this research that segments that are more than 1.5 s long are prone to misclassifications , while segments of less than 500 ms are difficult to cross-correlate . Figure 23 : Audio Segmentation using a VAD . Each audio segment will generate a single cross-correlation table that corresponds to each of the possible locations of the speakers , as it is done for training . The classifier will then use the cross-correlation template tables from training to compare with the analysis results and determine the most probable source match . Alternatively , it is possible 67 to cluster the cross-correlation results for later classification . Chapter 4 covers the analysis and classification methods of this research in more detail . 3.2 Block Diagram of the Proposed System Fig . 24 presents the block diagram of the proposed method . Chapter 4 covers the experimental implementation of each of the modules in this diagram , except for the clustering module , which was not implemented for this research . 68 Figure 24 : Block Diagram of the Proposed System . The proposed system is divided into 4 subsystems described below : 1 ) Room Geometry and RIR Generator : The first subsystem of the block diagram is The Room Geometry and RIR Generator ( RGRG ) . This subsystem accepts the room geometry parameters ( geometry of the room , absorption , location of the speakers and microphones ) and calculates the RIRs using synthetic speech sources . The RGRG consists of two modules : The Room Parameter 69 Generator ( A ) and the Room Model Generator ( B ) . The Room Parameter Generator creates the vectors that contain the geometry of the room and the location of the speakers and microphones ( virtual and real ) . The Room Model Generator gets the room geometry vectors and calculates the RIRs between the sources and the microphones . 2 ) Audio Pre-Processor : The second subsystem is the Audio Pre-Processor ( APP ) . The function of the APP is to prepare the single-channel raw audio for analysis . It consists of two modules : The Training Sample Audio ( C ) , and the Segmenter ( D ) . The Training Sample Audio module contains the training samples of each of the speakers participating in the audio to be analyzed plus a sample of ambient noise . These samples are saved as .wav files and labeled independently . The Segmenter module uses a VAD to create segments of the audio to be analyzed . Each segment of audio is saved as a .wav file of variable duration , with a minimum and a maximum length threshold . The segments that are less than a predetermined length are discarded . 3 ) Analysis Subsystem : The function of the Analysis Subsystem ( AS ) is to calculate the cross-correlation between the emulated microphones . The AS consists of three modules : The Source Estimator ( E ) , the Room Model Estimator ( F ) , and the Cross-Correlator ( G ) . The source Estimator gets the audio from the APP and deconvolves it with the RIR from the RGRG to estimate each possible source . The deconvolution is done to both the audio training samples 70 and the segments for analysis . The Room Model Estimator then emulates each of the microphones by convolving the estimated sources with each of the corresponding RIR calculated using the model room geometry . Again , this is done for both the training samples and the analysis segments . Finally , The Cross-Correlator module calculates the cross- correlation between the microphones for each of the possible source combinations , for both training and testing . 4 ) Classifier : The output of the AS subsystem is then handled to the Classifier . The classifier creates the cross-correlation sets of tables for training and testing . During training , only one table is created for each of the training audio samples . For testing , there is a cross- correlation table for each possible source for each of the segments , as was described in section 3.1 . There are two possible paths of action once the cross-correlation tables are available . One path is to just run a clustering algorithm to group each segment in similar clusters or run a classifier that selects the best source that matches the training cross- correlation table . This research follows the classifier option , which will be discussed in the next section . The final component of the Classifier subsystem is the Speaker Metric module . The function of this module is to calculate the statistics of each of the participants , e.g. , for how long they have been active , and when they have been active . In this research , our metrics only focus on measuring the total time each participant has been active . 71 Chapter 4 . Experimental Implementation This chapter presents the software and hardware implementation of the proposed method presented in Chapter 3 . It begins by presenting the software tools for simulation , deconvolution , and data handling , and continue with the software implementation based on the AOLME environment . This implementation will be used in the experiments of Chapter 5 to evaluate the performance of the proposed method . 4.1 Software and Hardware Tools The block diagram of Fig . 24 shows the need of developing several software modules to simulate the acoustics characteristics of the room , including RIRs and source image calculations , deconvolution for source estimation , cross-correlation , and classification . In summary , it is necessary to have code that performs the following operations : 1 ) Simulation of the geometry of a room 2 ) Calculation of all the RIRs based on the geometry of the room 3 ) Extraction of audio track from video recording 4 ) Segmentation of audio recording 5 ) Deconvolution of audio for source simulation 6 ) Simulation of microphone array 7 ) Microphone cross-correlation calculation 8 ) Analysis of microphone cross-correlation to identify the active speaker 72 9 ) Calculation of the metrics for each participating speaker . Developing code for the above modules is a time-consuming task due to the large number of mathematical algorithms and calculations needed . Fortunately , there are software packages available and code libraries that simplify the implementation of these modules into a software framework for the experimental analysis . This dissertation have combined open-source code and commercial software , saving a considerable amount of time to the alternative of writing code from scratch . 4.1.1 Open-Source Code for Room Geometry , RIR Calculation , and Microphone Simulation The open-source community of code developers offers an extensive variety of software libraries that cover a wide range of topics , from machine learning to financial market analysis , including acoustic simulations . Several acoustic simulation packages are available on GitHub for download . These packages are mainly designed to simulate the acoustics of environments for performing arts , such as theaters , stadiums , and recording studios . From these available packages , Pyroomacoustics was selected for RIR calculation and microphone simulation . Pyroomacoustics [ 71 ] is an open-source acoustic simulation package that calculates the RIRs and simulates the reception of the audio at a set of virtual microphones located inside a virtual room . Pyroomacoustics uses the Image Source Method ( ISM ) to calculate the RIR between a source and any point inside the virtual room . The location of the images 73 can be visualized with 2-D and 3-D representations of the geometry of the virtual room and the location of the sources and the virtual microphones . After simulating the location of the images , Pyroomacoustics calculates the RIRs to the target microphones and convolves the sample audio to simulate the reception at the microphones . Pyroomacoustics libraries ' inputs are the geometry of the room , the location of the sources , the location of the virtual microphones , the absorption of the room , the sampling frequency , and the number of images to calculate . The outputs for the libraries include a set of arrays containing the RIR to each of the microphones and the emulated reception at each of the microphones . Figs . 25 ( a ) and 25 ( b ) show examples of 2-D and 3-D visualizations from Pyroomacoustics of a non-rectangular room 3 x 5 x 2 meters , with a circular microphone array with 6 microphones and one source . Fig 25 ( c ) shows the same room with the simulated images . These types of representations will be used to approximate the AOLME models discussed later . Complete documentation on Pyroomacoustics functions and code can be found in [ 72 ] . 74 Figure 25 : Pyroomacoustics Models . ( a ) 2D . ( b ) 3D . ( c ) 3D With Images . The version of Pyroomacoustics used for this research ( 0.4.1 ) has some limitations that needed to be considered when developing the models : 1 ) Microphones and sources are always modeled as omnidirectional . There are no options to add unidirectional sources or other types of microphones ( e.g. , cardioids ) ; 2 ) All rooms are square , with no round corners ; 3 ) There is no option to add objects such as tables inside the room , and 4 ) Absorption is an empirical parameter that needs to be estimated by other means outside the software . 75 All experiments in this research were conducted using Pyroomacoustics version 0.4.1 . For the room geometry calculations , Pyroomacoustics libraries are called from a Jupyter Notebook under Anaconda 3 . The libraries were also called from within LabVIEW using scripts under Python 3 . 4.1.1.1 Pyroomacoustics Implementation Implementation of Pyroomacoustics for virtual microphone simulation is accomplished by four steps that include simulation of the room , placement of the sources and microphones , calculation of the RIR from the sources to the microphones , and convolution of the sources with the RIR for microphone simulation . This is done by calling the classes in the Pyroomacoustics libraries as follows : a ) Room Simulation : The room simulation contains the parameters of the room , such as its dimensions , the absorption , the number of images to be calculated , and the sampling frequency . For example , the following script will generate a room of dimensions 9 x 7 x 3 meters , with a total of 9 images , at a sampling frequency of 9600 Hz : pyroomacoustics.room.Room ( [ 9.0,7.0,3.0 ] , fs=9600 , max-order=9 ) b ) Sources and Microphone Placement : 76 This script adds the sources and microphones to the model . Pyroomacoustics need a valid audio file for source location . The following script will locate a source at X= 2 m , Y = 3 m , and Z = 1 m from the origin , and two microphones at X1 = 6 m , Y1 = 4 , and Z1 = 1 m for microphone 1 , and X2 = 6 m , Y2 = 4.5 , and Z2 = 1 m for microphone 2 : room.add_source ( [ 2.0 , 3.0 , 1.0 ] , signal=audio ) mic_locs = np.c_ [ [ 6.0 , 4.0 , 1.0 ] , # mic 1 [ 6.0 , 4.5 , 1.0 ] , # mic 2 ] room.add_microphone_array ( mic_locs ) c ) RIR calculation : By calling room.compute_rir ( ) the RIRs are calculated to each of the microphones , and the results are saved in the form of a list of lists at the rir attribute of room . d ) Microphone Simulation : The final microphone simulation is obtained by calling simulate ( ) . This convolves the sources with each of the RIRs and emulates the signals in each of the microphones . The results of the convolutions are stored in the signals attribute of room.mic_array . Appendix A includes some of the scripts used in the actual experimental implementation of Pyroomacoustics . Refer to Pyroomacoustics documentation found at [ 72 ] for full description of the libraries and their algorithms . 77 4.1.2 Audio Segmentation Recalling from previous discussions , the practical analysis of long audio recordings is not possible unless they are segmented into smaller frames . Audio segmentation plays a critical role in the overall performance of the proposed method ; therefore , careful consideration should be made with the algorithms for audio segmentation . As previously indicated , the audio segments need to comply with two main requirements : 1 ) Contain audio from only one active speaker with minimum overlapping or mixing between speakers , and 2 ) are of a length that provides enough signal information for cross- correlation analysis . Both requirements are difficult to achieve , and in chapter 3 it was introduced the concept of VADs as a segmentation method that maximizes the information content of a single speaker . Two segmentation methods were considered during this research : Fixed Segmentation and Voice Activity Detection . In the end , it was opted for VADs due to their better performance results . 4.1.2.1 Fixed Length Segmentation The simplest way to segment audio is to divide it into fixed-length segments . Fixed length segmentation is a relatively simple and computationally inexpensive method where each audio segment has the same length , independently of their content . Because there is no intelligence in this method , there is a probability of some of the segments containing overlapping speech . Also , it is very unlikely that the audio can be divided into exactly equal parts making the last segment of shorter duration than the others . 78 A solution to minimize overlapping is to make the segments as short as possible . As it was discussed before , if the segments are too short , they may not contain enough information for calculating the cross-correlation . There is therefore a balance between the optimum length of the segments and the desired classification error . One empirical way to find the length of the segments is by assessing the audio . If the audio contains well-separated speakers with little overlapping , the length of the segments can be longer than in acoustic scenes with noise or disorganized speech . This research conducted experiments with segment lengths varying from 250 ms to 1.2 s , obtaining different degrees of success . At the end , fixed-length segmentation was abandoned due to an undesirable number of errors and a lower performance when compared with voice activity detection segmentation . 4.1.2.2 Voice Activity Detection The VAD used in these experiments was programmed using MATLAB by a fellow graduate student at the University of New Mexico [ 73 ] . He used MATLAB Fast Fourier Transform ( FFT ) and Inverse Fast Fourier Transform ( IFFT ) functions to convert the audio from the time domain to the frequency domain and vice versa . First , the audio is converted into the frequency domain by applying the FFT , and then a 3000 Hz low pass filter and a 1000 Hz high pass filter are applied to remove some of the noise . The filtered audio is brought back to the time domain using the IFFT , and it is normalized afterward . An Amplitude Trigger ( AT ) with a threshold of 0.1 is used to determine the presence of speech 79 or noise . If the amplitude is exceeded at any time , this will be the beginning of a speech , and this time is marked as 𝑇 ( cid:2869 ) . The level is checked 300 ms after 𝑇 ( cid:2869 ) . If the AT is exceeded again , we mark that time as 𝑇 ( cid:2870 ) , and check the AT again after another 300 ms . If the audio does not exceed the AT , then it is marked as the end of the audio with a time 𝑇 ( cid:2870 ) = 𝑇 ( cid:2869 ) +300 ms ; otherwise , the end of the audio will be 𝑇 ( cid:2869 ) + 𝑇 ( cid:2870 ) . When the audio does not exceed the AT and is not in the time range of speech , it is classified as noise . Using the information obtained from the filtered audio and adding a time offset ( 250 ms ) to compensate for information missing in the filtered audio , we split the audio giving a maximum and a minimum time . If a noise segment is too small ( under 250 ms ) , they will be combined with the audio segments since this is a pause of a person speaking . If there is noise or the audio is too long , it gets split into batches of a maximum time of 1.2s , with small exceptions that can go up to 1.449s . It is important to notice that audio segmentation produces artifacts at the beginning and the end of the segment [ 74 ] . These artifacts are the familiar “ clicks ” we hear when listening to a sequence of segments . In audio processing , it is common practice to apply a window , a filter , and overlap of the segments [ 74 ] to allow for a smooth transition between them . Our method did not apply windowing or overlapping due to the possibility of altering the spatial content of the segments ; therefore , it was better to accept a reasonable error instead . 80 4.1.3 Implementation Using the LabVIEW Graphical Programming This research work used LabVIEW for deconvolution , array manipulation , classification , metrics , and user interface . LabVIEW is very popular in engineering due to its wide variety of built-in functions and its simplicity to create graphical user interfaces . It is not an open-source language , requiring the purchasing of a license . LabVIEW requires additional toolkits for some advanced digital signal processing , statistics , and software integration . This research applied the Advanced Signal Processing toolkit for cross-correlation analysis , convolution , and deconvolution calculations . Because Pyroomacoustics was used for all room simulations , it was necessary to install the Python Integration Toolkit provided by Enthought . This toolkit provides LabVIEW with the capability of calling Python code directly . LabVIEW is then used as a wrapper to call Pyroomacoustics Python libraries from within LabVIEW . In this way , LabVIEW provides the user interface for the Pyroomacoustics inputs ( i.e. , room geometry , number of images , audio files ) , and processes the outputs ( i.e. , RIRs , microphone simulations ) , saves the data , and displays the results . All documents and detailed description of LabVIEW can be found at the NI website [ 75 ] . Instead of scripting code , LabVIEW uses a graphical interface that contains functional modules called VIs ( short of Virtual Instruments ) . The VIs perform basic functions such as adding , subtracting , array manipulation , and logical operations , among others . There are more advanced VIs to calculate more complex operations such as convolution and inverse convolution , correlation and cross-correlation , and file 81 manipulation , for example . Each VI transfers data using a wired connection , and there is a mechanism to handle and display execution errors . Fig . 26 ( a ) and Fig . 26 ( b ) show a screenshot of the internal block diagram and user interface , respectively , of the LabVIEW implementation used during this research for convolution , deconvolution , correlation , and cross-correlation operations between two files . The user can select between any of the operations using a drop-down selector . The implementation reads two text files that correspond to the audio files to be analyzed or convolved and a third file that corresponds to the RIR for convolution operations only . There are four graphics that represent the input files , the RIR , and the output of the cross-correlation calculation . The results can be saved as text files for later conversion into audio or any other format . Figure 26 : LabVIEW Sub VI for Cross-Correlation Calculation . 82 Despite the popularity of LabVIEW among the engineering community , LabVIEW is many times regarded by hard-core coders as a language for those who do not know how to code . Its major deficiencies lie in the fact that its built-in functions are rarely modifiable , the block diagrams can get confusing if they are not divided into smaller VIs , and it is difficult to document and comment . The decision was to use LabVIEW because of the time-savings advantages it has over scripted languages . 4.1.3.1 LabVIEW Implementation The code written for this research used several built-in VIs available in LabVIEW version 2016 , 32 bits . These VIs were implemented into more complex sub-VIs to run the calculations , data handling , user interface , file manipulation , and display of results . Although the code required the use of dozens of different VIs for simple mathematical operations and data flow , important calculations such as convolution and deconvolution were handled with LabVIEW built-in functions . 4.1.3.1.1 Function VIs The three functions VIs in this section were used to calculate convolution , deconvolution , and cross-correlation . They are part of LabVIEW 's built-in library for signal processing . The algorithms for these functions are explained next . 4.1.3.1.1.1 Convolution VI 83 This VI computes the convolution of two vectors x and y . The convolution can be computed by selecting either a direct method or a frequency domain algorithm that uses the FFT , being the latter the one used for this research . The VI that represents the convolution is shown in Fig . 27 . Documentation on this VI can be found at [ 76 ] . Figure 27 : LabVIEW Convolution VI . The algorithm works by padding the ends of x and y with zeros to make their lengths M + N – 1 , as shown in ( 4.0 ) and ( 4.1 ) : 𝑥′𝑖 = ( cid:3420 ) 𝑥𝑖 , 0 , 𝑖 = 0,1 , … , 𝑁 − 1 𝑖 = 𝑁 , … , 𝑀 + 𝑁 − 2 ( 4.0 ) , 𝑦′𝑖 = ( cid:3420 ) 𝑦𝑖 , 0 , 𝑖 = 0,1 … , 𝑀 − 1 𝑖 = 𝑀 , … , 𝑀 + 𝑁 − 2 ( 4.1 ) , The convolution is computed by calculating the inverse FFT of the product of the FFTs of 𝑥 ( cid:4593 ) and 𝑦 ( cid:4593 ) 𝒙 ( cid:4593 ) ( 𝑓 ) = 𝐹𝐹𝑇 ( 𝑥 ( cid:4593 ) ) ( 4.2 ) , 84 𝒚 ( cid:4593 ) ( 𝑓 ) = 𝐹𝐹𝑇 ( 𝑦 ( cid:4593 ) ) ( 4.3 ) , 𝒙 ∗ 𝒚 = 𝐼𝐹𝐹𝑇 ( cid:3435 ) 𝒙 ( cid:4593 ) ( 𝑓 ) ∙ 𝒚 ( cid:4593 ) ( 𝑓 ) ( cid:3439 ) ( 4.4 ) , where IFFT is the inverse FFT . 4.1.3.1.1.2 Deconvolution VI The deconvolution VI computes the inverse convolution of two vectors x * y and y . It returns the value of vector x . Fig . 17 shows the symbol for this VI . Documentation on this VI can be found at [ 77 ] . Figure 28 : LabVIEW Deconvolution VI . This VI implements the deconvolution by computing the Fourier Transform of the input x * y and y , then dividing them to create a new vector h. The vector x is computed by applying the IFFT to the sequence h. 85 4.1.3.1.1.3 Correlation VI The Correlation VI calculates the correlation coefficient r between two vectors x and y . Fig . 18 shows the icon for this VI . Documentation on this VI can be found at [ 78 ] . Figure 29 : LabVIEW Correlation VI . This VI calculates the linear correlation coefficient , also known as Pierson ’ s correlation by ( eq . number ) 𝑟 = ∑ 𝑧 ( cid:3051 ) 𝑧 ( cid:3052 ) 𝑛 ( 4.5 ) , where 𝑧𝑥and 𝑧𝑦are the standardized z-values of x and y . The standardized z-values indicate how many standard deviations x and y are above or below the mean . 4.1.3.1.1.4 Cross-Correlation VI The cross-correlation VI computes the cross-correlation between two vectors x and y . The inputs for this VI are the vectors 𝒙𝒕 and 𝒚𝒕 , the weighting specifies the use of a biased or unbiased weighting in the cross-correlation calculation , being the former the one used in all the calculations . The maximum lag specifies the maximum value of the lag this 86 VI uses to compute the cross-correlation . The maximum lag used equals max ( M , N ) – 1 , where M and N are the lengths of 𝒙𝒕 and 𝒚𝒕 , respectively . Fig . 30 shows the icon for this VI . Documentation on this VI can be found at [ 79 ] . Figure 30 : LabVIEW Cross-Correlation VI . This VI computes the cross-correlation values between two univariate time series 𝑿𝒕 and 𝒀𝒕 according to the following equation : 𝑟𝑥𝑦 ( 𝑘 + 𝑁 − 1 ) = 1 𝑎 ∙ 𝑏 ∙ 𝑤 ( 𝑘 ) 𝑁−1 ( cid:3533 ) 𝑋𝑡 ( 𝑛 ) 𝑌𝑡 𝑛=0 ( 𝑛 + 𝑘 ) , 1 − 𝑁 < 𝑘 < 𝑀 ( 4.6 ) , where = ( cid:3493 ) ∑ ( cid:3050 ) ( cid:2879 ) ( cid:2869 ) ( cid:3041 ) ( cid:2880 ) ( cid:2868 ) ( cid:2870 ) ( 𝑛 ) 𝑋 ( cid:3047 ) , 𝑎 = ( cid:3493 ) ∑ ( cid:3014 ) ( cid:2879 ) ( cid:2869 ) ( cid:3041 ) ( cid:2880 ) ( cid:2868 ) ( cid:2870 ) ( 𝑛 ) 𝑌 ( cid:3047 ) , 𝑿𝒕 has length N and 𝒀𝒕 has length M. The length of the output is N+M–1 . w is the weighting factor which in our case , w ( k ) = 1 . 4.1.3.1.2 Operational Sub-VIs This section will cover the Sub-VIs that form the core of the code that performs the computations needed for the analysis . These Sub-VIs contain the function VIs covered in 87 the previous section . Appendix B contains the front panels and block diagrams of these sub-VIs . 4.1.3.1.2.1 Room Parameters Reader The Room Parameters Reader Sub-VI reads the source locations , microphone locations , and 2D room dimension files created by the Pyroomacoustics Room Geometry Generator and formats them for the Room Model Generator Sub-VI . The room absorption , the room extrusion , and the number of images to calculate are just a pass thru . Appendix B section ( a ) shows the front panel and blocks diagrams for this Sub-VI . 4.1.3.1.2.2 Room Model Generator The Room Model Generator Sub-VI reads the room geometry parameters formatted by the Room Parameters Reader Sub-VI and runs the Python scripts that call the Pyroomacoustics libraries that compute the RIRs for the room model . This Sub-VI also reads the synthetic speech or noise .wav files used by Pyroomacoustics for the RIR calculations . The calculated RIRs are saved in .txt files for later retrieval by the Source Estimator Sub-VI . The Room Model Generator is used twice , first to calculate the room model RIRs for the source estimation , and again to emulate the virtual microphones using the estimated sources . Section Appendix B section ( b ) shows the front panel , block diagram , and inputs and outputs with more detail . 88 4.1.3.1.2.3 Source Estimator The Source Estimator takes the model RIR and estimates all the sources that will correspond to the audio segment that is being analyzed . For this estimation , this Sub-VI takes the segment of audio under analysis ( corresponding to the real recording microphone ) and deconvolves it with the RIRs for each of the source locations . The emulated sources are saved under .txt files for virtual microphone simulation using another instance of the Room Model Generator . Appendix B section ( c ) shows the details of this Sub-VI and a simplified block diagram . 4.1.3.1.2.4 Cross-Correlation Model Calculator This Sub-VI takes the results of the virtual microphone simulation from the second run of the Room Model Generator and calculates all the cross-correlations between the virtual microphones . The results are saved as cross-correlation tables and used for training and classification . Appendix B section d shows the details of this Sub-VI . The output of this Sub-VI is a table that contains all possible cross-correlations between microphones for each of the possible sources . For the three speakers and three microphones example , the cross-correlation table would look like the one represented in Table IV . The first row is the cross-correlation microphone combinations , and the first column is the speakers . The numbers represent the array index where the max occurs . 89 Table IV : Example Cross-Correlation Table Output from Model Calculator 1-2 96 -32 5 1-3 5 0 5 2-3 -5 -83 -130 1 2 3 4.1.3.1.2.5 Model Classifier The Model Classifier Sub-VI takes all the correlation tables , from training and testing , and performs the classification by comparing the testing results against the training templates . This is a very simple classifier that works by comparing each CC table for best similarity . For example , assume that the CC table IV corresponds to the training of speaker S1 , and the analysis of an unknown audio segment produces the three CC tables shown in Table V ( a ) , ( b ) , and ( c ) . The classifier simply counts the number of matches between each CC table and the training CC table . In this example , table V ( a ) has the greatest number of matches , indicating that the unknown segment corresponds to speaker 1 . Appendix B section ( e ) shows the icon and front panel . Table V : Cross-Correlation Tables for Classification 1 2 3 1-3 1-2 96 1 2 -15 5 1 Total for S1 ( a ) 2-3 -5 -3 -130 Match 2 0 2 4 90 1 2 3 1 2 3 1-3 1-2 9 1 -32 6 2 9 Total for S2 ( b ) 1-3 1-2 6 8 -2 2 5 5 Total for S3 ( c ) 2-3 -1 -8 -13 2-3 -50 -8 -15 Match 0 1 0 1 Match 0 1 1 2 4.1.3.1.2.6 Multi-Function Convolution and Correlator Visualizer The Multi-Function Convolution and Correlator Visualizer is a full stand-alone Sub-VI used to manually convolve and deconvolve audio files and for correlation and cross-correlation analysis of files . Appendix B section ( f ) provides more information about this Sub-VI . 4.1.3.2 Audio Laboratory The purpose of the Audio Laboratory was to capture real audio in a controlled environment . This laboratory allowed to conduct experiments knowing the position of the speakers and microphones and control the content , duration , and characteristics of the analyzed speech . The results from the experiments performed at the audio lab were 91 compared against the results obtained from our proposed method and the simulation software . The audio laboratory consisted of a set of microphones , an audio processing device , an audio amplifier , loudspeakers , and the computer running the software that captures the recordings . The audio laboratory was physically configured to follow the common acoustic scene found on the videos analyzed in this research . This configuration used a set of loudspeakers located at the approximated position of the speakers sitting around a table . A set of microphones captured the audio at different locations of the lab , and one microphone was located at the same relative position as the recording microphone at the videos . Fig . 31 shows a block diagram of the lab components . The set of microphones were the same type used in the recording of AOLME video . These microphones were connected to the Tascam® Audio Processor . This processor can capture simultaneous audio from all six microphones and send it digitally to the computer via USB . The computer processes the audio using Tracktion Waveform® audio processing software [ 80 ] . This software processes the audio from the microphones and saves it in separate .wav files that correspond to each of the microphones . 92 Figure 31 : Audio Lab Components . The simulation of the speakers is accomplished using a set of four loudspeakers connected to a stereo audio amplifier . Speakers 1 and 2 were simulated with the left stereo channel , while speakers 3 and 4 were simulated with the right stereo channel . A switch allows selecting between loudspeakers 1 and 3 , and 2 and 4 . The lab also included a Compact Disk ( CD ) player located at a certain distance from the table . This CD player was used to inject background noise during the experiments . Fig . 32 shows the actual audio laboratory setup where we can appreciate the location of its components . 93 Figure 32 : Audio Lab Setup 4.2 The AOLME Environment The dissertation focuses on the analysis of audio from AOLME videos to assess the level of engagement of the participants . The AOLME environment is characterized by the presence of background noise , crosstalk , and other interferences that make it challenging for speaker identification tasks ; therefore , to improve the identification rate , the simulation models must be optimized to fit this environment . This section studies the AOLME environment to find out how to best adapt the models to the acoustic characteristics of this environment and implement these models for the experimental section . 94 4.2.1 Characteristics of the AOLME Environment Fig . 33 shows a screen capture from one of the AOLME videos analyzed in this research . The scene shows a typical collaboration table with four students and one instructor . It is common to have 5 to 10 of these tables , with three to six participants each , distributed in a room of approximated dimensions of 9 x 14 x 2.5 m. The camera is recording the audio via a single omnidirectional microphone that is resting on the top of the table . In addition to normal room noise , this environment presents other elements that make its dynamics more complex . For example , it is typical to have the participants shuffling papers , leaning over the table , eating , speaking simultaneously , and accidentally covering the microphone with books or other utensils . Furthermore , there are occasions when another staff member walks in and joins the group for a conversation . Figure 33 : Common AOLME Environment Setup . 95 The first step in building the models is to approximate the location of the speakers and the recording microphone . By analyzing the scene in Fig . 33 , it is possible to get some clues that can be used to approximate these locations . From Fig . 20 , it is possible to estimate the relative locations of each of the speakers with respect to each other and the recording microphone . It is noticeable also that the position of the speakers forms a rectangle that can be translated into a 3D figure whose bottom area is the table and its height is defined by the tallest speaker . The second step is to approximate the geometry of the room . From Fig . 33 , it is possible to recognize that there is a nearby wall behind speakers 1 and 3 . The second wall is located behind speaker 2 at a farther distance from speaker 2 than the first wall is located from speakers 1 and 3 . There is no indication of any other wall or the presence of the celling , which we are assuming exists . It is also assumed that there are other tables nearby , but these can not be seen in Fig . 33 . 4.2.2 Preparation of the Experimental Models As mentioned earlier , the models are based in part on the geometry of the room and the location of the speakers . Because this exact information is not available , the models need approximations based on the observations made from the video shot . Also , recalling from section 4.1.1 , our version of Pyroomacoustics does not allow us to simulate complex environments like the one shown in Fig . 33 , where we have the participants sitting around 96 a table . Fortunately , the models do not need to be perfect , and we can make assumptions that will reduce their complexity . 4.2.2.1 Approximating the Models Using Video Observations We are ready to make some assumptions and approximations based on observations from the video . Fig . 34 shows another frame from the same AOLME video recording , where it is easy to estimate the relative distances between the participants . In Fig . 34 , H1 represents the height of speaker 2 , while and H2 represents the relative height of speakers 1 , 3 , 5 , and 4 . S represents the separation between speakers , and D represents the width of the table . We are assuming also in this observation that speakers 1 and 4 are separated by the same distance D. Figure 34 : Relative Positions of AOLME Participants . 97 In Fig . 34 , D can be approximated to the width of two standard commercial tables , which we can assume is 0.8 m x 2 = 1.6 m total . Speaker 2 is sitting about half of this distance , about 0.8 m from each edge of the table combination . Speaker 1 is close to one of the corners of the table , as it is speaker 4 . The separation S between speakers can be approximated to 0.3 m , and the recording microphone can be located at half of this distance at the center of the table . Finally , H1 can be approximated using as reference the average waist to head distance of a young female , to about 0.5m , and H2 to the average waist to head distance of kids 11 years old , to approximately 0.4 m. These values are just examples to illustrate the principle on which we are basing the approximations . The actual model will not necessarily use these values . There is no prior knowledge of the dimensions of the room that can be used to approximate its geometry . Observations about the location of the walls and the ceiling only provide a reference for the location of two walls . Nevertheless , it is possible to recognize , given the appearance of the scene , that the remaining walls are at a greater distance than the visible ones . This assumption does not provide a numeric value to the location of the walls or the ceiling , but it gives a clue of the behavior of the sound in the room . Recalling Section 2.4 , the human voice propagates mostly unidirectionally to the front of the speaker . Speakers 1 and 3 will project their voices toward speakers 4 and 5 and vice versa . Most of the sound energy from speakers 1 and 3 is absorbed by speakers 4 and 5 , with some energy reflected by the table , some traveling to the ceiling of the room , and some other amount propagating to the walls behind . The walls reflect the residual 98 sound energy to speakers 4 and 5 , and the process repeats until all the energy is absorbed , following the 𝑇 ( cid:2874 ) ( cid:2868 ) rule . The same process applies when speakers 4 and 5 are active . In the case of speaker 2 , there are no reflecting surfaces directly located in front of her , and the computer screen is located at a distance where the sound reflections from it can be considered of minimal influence , making the table the only reflecting surface . Under this model , it is possible to conclude that the sound energy of the participants is mainly contained within the boundaries of the table , and the contributions of the reflections due to the walls can be considered in practice as negligible , given the directionality of speech , the absorption of the speakers , and the separation of the speakers to the wall and the ceiling of the room . The previous analysis indicated that it is not critical that the models take into consideration the reflections from the walls , suggesting that the rooms can be modeled as to be of infinite dimensions or to have an absorbance that is close to 1 . Unfortunately , having a room of infinite dimensions will lead to a problem when modeling the sources . As discussed previously , the simulation software only allows for omnidirectional sources and microphones . In a wall-less room , Pyroomacoustics will create images from speech that equally propagates in all directions from the speaker , which we know is not accurate . The solution is to place the sources at very close proximity from the walls of the model and make the virtual room of the size of the table , thus reducing the propagation behind each speaker to negligible levels . 99 The analysis described above gives the basis for a first model representing the location of the speakers and the recording microphone . Recalling the 2D model of Fig . 19 , we can set up a 2D model based on the acoustic scene of Fig . 34 , representing the location with respect to the table of the 5 speakers and the real ( recording ) microphone . Note that this model includes a 6th “ speaker ” that represents the room noise . Representing the noise as a separate speaker allows for better discrimination between audio segments containing noise and those containing speech . Figure 35 : Location of Speakers and Real Microphone . The Z dimension ( room height ) needs to be added to convert the 2D model into a 3D model . Because the perimeter of the room is limited to the size of the table , the table itself can be modeled as the floor of the room . With this approach , all locations will be zero-referenced with respect to the table . 100 The total height of the room can be approximated in a similar manner as it was done for the perimeter of the room . Because of the directionality of the human voice , it is expected that there will be a little transmission of voice energy to the ceiling ; therefore , the reflections coming from above can be neglected . The ceiling can then be located at any height for as long it is above the maximum height of the taller speaker . Empirically , this value can be set , for example , at 1 m above the table . The 3D model for the room dimensions and the speakers is shown in Fig . 36 . Figure 36 : 3D Model of the Virtual Room The last element needed to complete the model is the location of the virtual microphones . Their location is constrained by the dimensions of the virtual room and the maximum anti-aliasing distance between them . Also , it is necessary to consider that the array of microphones consists of a set of virtual microphones plus a real microphone , which 101 is resting at the top of the table . At this location , the real microphone receives no sound reflections from the bottom ; therefore , it can be assigned a Z value of zero . Because the real microphone is resting on the table , there are mechanical vibrations transmitted from the table . To simulate these vibrations , all models in this research include some value for the Z component of the real microphone . The location of the virtual microphone array can be arbitrary , and the separation between microphones is not critical because the distance between two adjacent microphones will never exceed the maximum for anti-aliasing . However , it is of interest to have unique cross-correlation values between microphones . For this , the array should be in an asymmetric position with respect to the speakers in such a way that the value of the magnitude of the cross-correlation between microphones is different for each speaker . The Z value of the virtual microphones can be arbitrary , but because Pyroomacoustics can only simulate omnidirectional microphones , it is of advantage to locate them a certain height above the reference microphone . All the models in this research have microphones located at approximately the height of the speakers , allowing for simulation from all directions . Fig . 37 shows the complete 2D model derived from the five-speaker AOLME environment example . This type of model is used in all experiments in this dissertation , with the variations needed to fit the objective of the experiment . 102 Figure 37 : Final 2D Model for AOLME Example . 103 Chapter 5 . Results This chapter presents the experiments conducted to evaluate the capability of the proposed method to identify speakers in audio segments . The experiments focused on three objectives : 1 ) To determine the suitability of Pyroomacoustics as a simulation package ; 2 ) to evaluate the performance of the proposed method for diarizing and identifying speakers ; and 3 ) to compare the performance of the proposed method against Amazon AWS and Google Cloud . These experiments included both real audio recordings from the audio lab and AOLME videos . 5.1 Evaluation of Pyroomacoustics The objective of this experiment was to evaluate Pyroomacoustics as simulation software . This experiment compared the cross-correlation measured between real microphones and the cross-correlation between emulated microphones using Pyroomacoustics . This experiment was performed using the audio lab , with a Pyroomacoustics simulation based on the geometry discussed in Chapter 4 . 5.1.1 Microphone Calibration All audio recording devices have an electronic delay that varies from equipment to equipment . To measure the real cross-correlation between physical microphones , it is necessary to measure this electronic delay for each of the microphones and apply a calibration factor if necessary . Because Pyroomacoustics version 0.4.0 simulates all 104 microphones as ideal and does not consider any delays , it is necessary to calibrate the real microphones to compensate for their delays before comparing them against any simulation . One way to calibrate the microphones is to place them in an array configuration and locate this array in the proximity to an audio source . Fig . 38 shows a block diagram of the components needed to calibrate the microphones . This calibration setup consists of an audio source , speaker , sound processor , and microphone array . The audio source is driven by a signal generator , and the sound processor can acquire six audio channels simultaneously . Figure 38 : Block Diagram of a Microphone Calibration Setup . a ) Calibration Preparation A homemade jig made of cloth pins was used to hold the six microphones for calibration . The configuration and separation of the microphones are shown in Fig . 39 ( a ) . The array of microphones was located next to one of the loudspeakers , as shown in Fig 39 ( b ) . With this configuration , the distance of each microphone to the sound source is about 105 the same for all microphones , making the time differential of arrival between them neglectable . ( a ) ( b ) Figure 39 : ( a ) Microphone Calibration Jig . ( b ) Location to Loudspeaker . 106 b ) Calibration Execution A 450 Hz signal was applied to the loudspeaker using a signal generator , to the loudspeaker , and the output of the six microphones was collected simultaneously using the sound processor and the computer running Tracktion Waveform® software . Each channel recording was saved as a separated .wav file of 2 s duration , sampled at 48 kHz . To measure the delay between microphones , each of the .wav files was converted into .txt files for cross-correlation analysis using the Multi-Function Convolution and Correlator Visualizer Sub-VI . Each combination of microphones was cross-correlated as shown in Table VI . The results in Table VI show that Microphones 1 , 3 , and 6 had zero cross-correlation between them . The same was observed between microphones 2 , 4 , and 5 . Rather than apply a calibration factor , it is more convenient to segregate the microphones into groups and measure the cross-correlation between pairs that belong to the same group . Note that the results shown by Table VI correspond to the index of the array where the max cross-correlation occurs . Table VI : Cross-Correlation Table for Microphone Calibration . 1 s 2 e n o 3 h p 4 o r c 5 i 6 M 1 - 50 0 49 50 0 2 -50 - -50 0 0 -50 Microphones 4 -49 0 -49 - 0 -49 3 0 50 - 49 50 0 107 5 -50 0 -50 0 - -50 6 0 50 0 49 50 - 5.1.2 Audio Lab Setup and Model Configuration Fig . 40 shows the laboratory setup for this experiment . The setup follows the general model configuration described in Chapter 4 , but the microphones were distributed between the loudspeakers to maximize the cross-correlation value differences between microphones . The dimension of the lab setup allows for the microphones to be within the anti-aliasing distance already calculated of 0.95 m. Microphone 3 was kept in the same location as the recording microphone of the draft model . Figure 40 : Audio Lab Set-Up for Pyroomacoustics Evaluation . The Pyroomacoustics model was set up following the configuration of the audio lab . Because the audio lab has only 4 loudspeakers , speakers 5 and 6 were not included in the model . The virtual room perimeter was set to the size of the lab table , and the height of 108 the room was set to a value of 1 m. The absorption of the model was set empirically to 0.95 and the number of images at 8 . The microphone height was set to 0.025 m for all microphones , following the observations made in Chapter 3 . Table VII shows the final dimensions of the virtual room and the location of the sources ( loudspeakers ) and microphones used to create the Pyroomacoustics model . The final 2D model geometry generated by Pyroomacoustics is shown in Fig . 41 . Table VII : Dimensions of Virtual Room and Location of Sources ( in m ) . Z 0.25 0.25 0.25 0.25 -- - -- - 0.025 0.025 0.025 0.025 0.025 0.025 -- -- -- -- Y 0.79 0.4 0.79 0.01 -- - -- - 0.79 0.01 0.35 0.1 0.7 0.79 0 0.8 0.8 0 1 Sources Mics Room S1 S2 S3 S4 S5 S6 M1 M2 M3 M4 M5 M6 CORNER 1 CORNER 2 CORNER 3 CORNER 4 EXTRUDE X 0.4 0.01 1 0.4 -- - -- - 0.015 0.015 0.9 1.39 1.39 0.7 0 0 1.4 1.4 109 Figure 41 : Final 2D Model of Audio Lab Setup . 5.1.3 Experimental Execution Both audio lab and simulation sections of this experiment used as a source one anechoic male voice of 2 s of duration . The source was played sequentially on each of the loudspeakers corresponding to S1 , S2 , S3 , and S4 , and it was captured simultaneously into the six-channel audio processor , corresponding to each of the microphones . The six- channel audio then was saved as six independent audio files using Tracktion Waveform® . The simulation with Pyroomacoustics used the geometric model of Fig . 41 . Because there was no need to estimate the sources , the simulation of the reception at microphones M1 , M2 , M3 , M4 , M5 , and M6 was accomplished by only running the Room Model Generator Sub-VI with the geometric model and playing the source at the location of speakers S1 to S4 . The Sub-VI saved the results of each microphone simulation as a separate .txt file . 110 5.1.4 Results The final analysis consisted of running the Multi-Function Convolution and Correlator Visualizer Sub-VI to calculate the cross-correlation for each of the real microphone audio files ( ground truth ) and the simulated microphone audio files . The cross- correlation was calculated between microphones of the same group as it was determined during calibration . There was no need for audio segmentation due to the short duration of the sample audio . Table VIII shows the results in ms of the offset between the ground truth and the simulated signals , corresponding to a sampling rate of 48 kHz . Table VIII : Experimental Results for Simulation Software Evaluation . S1 S2 S3 S4 Sim . G.T . Diff Sim . G.T . Diff Sim . G.T . Diff Sim . G.T . Diff 0.34 -0.58 -1.62 0.38 -0.34 0.24 -1.24 0.38 0.72 1.56 1.88 0.32 0.20 0.80 0.30 0.68 0.10 -1.06 -1.30 0.24 0.12 0.18 0.30 0.12 -1.08 -0.88 0.20 -1.90 -1.56 0.34 -0.48 -0.50 0.02 -1.90 -1.62 0.28 0.58 0.40 0.18 0.00 -0.02 0.02 1.82 0.26 0.48 1.62 1.12 1.96 0.06 0.50 1.46 0.96 0.14 0.08 0.06 0.02 0.20 -0.60 -0.32 0.28 0.02 -1.98 -1.56 0.42 0.16 -2.58 -2.08 0.50 0.16 -0.56 -0.50 0.06 1-3 1-6 3-6 2-4 2-5 4-5 Table VIII shows that the simulation correctly predicts the sign of the cross- correlation for each of the microphone pairs . The maximun offset difference is 0.5 ms which corresponds to a difference of 20 % , and the average difference is 0.2 m , hence the 111 simulation model appears to be sufficiently accurate for differentiating speakers based on their positions . 5.2 Controlled Environment Experiments The objective of this next set of experiments is to evaluate the performance of our method to identify speakers in single-channel audio segments that were recorded under controlled conditions at the audio lab . There were two controlled experiments : The first experiment demonstrated the capability of the proposed method to identify two speakers based only on their location . The second experiment demonstrated the capability of the proposed method to identify multiple speakers independently of their spoken words . 5.2.1 Methodology The approach for these experiments is to physically emulate an open collaborative environment such as AOLME in which we record audio containing speech with a single microphone . Because the geometry of the acoustic scene is known , we can create a model that numerically follows this real scene , and then evaluate the performance of the proposed method using this model . Conversely , by having control over some of the parameters , such as the location of the sources , it is possible to experiment with different microphone arrays and absorptions values to evaluate the performance of different models . The controlled experiments used the same audio lab configuration and the same Pyroomacoustics model from the previous experiment . A change was made to the location 112 of the speakers to better fit the distance that will be used for the AOLME experiments . 5.2.1.1 Audio Lab and Model Preparation Table IX represents the audio lab configuration for this experiment , with the location of the loudspeakers and the recording microphone ( MIC3 ) . The audio was recorded using the Canon video camera connected with MIC3 , and the video recording was saved in the internal SD card of the camera , the same way it is done with AOLME recordings . Ambient noise was injected using the CD player with background noise from one of the AOLME video sessions . 113 Table IX : Distribution of Microphones and Sources for Controlled Experiments . Sources Mics S1 S2 S3 S4 S5 S6 M1 M2 M3 M4 M5 M6 M7 CORNER 1 CORNER 2 Room CORNER 3 CORNER 4 EXTRUDE X 0.4 0.16 0.65 0.3 -- - 0.98 0.6 0.65 0.6 0.6 0.6 0.55 0.6 0 0 1 1 Z 0.25 0.25 0.25 0.25 -- - 1.5 0.025 0.025 0.025 0.025 0.025 0.025 0.025 -- -- -- -- Y 0.79 0.5 0.79 0.2 -- - 0.4 0.6 0.55 0.55 0.45 0.5 0.55 0.65 0 0.8 0.8 0 2 The lab setup was translated into the Pyroomacoustics 2D model shown in Fig . 42 . Noise is represented as “ speaker ” S6 and placed it in a relative location that resembles the location of the CD player . All sources and microphones kept the same Z coordinate value as in the previous experiment ( 0.25 m ) , except for the noise S6 , which is located at Z= 1.5 m , to better represent the location of the CD player . This experiment ( and for all subsequent experiments in this research ) , used a linear cross-type virtual microphone array with 7 elements , with the recording microphone 114 located at the center of the array . This type of microphone configuration is flexible and compact and allows its implementation in other models with different geometries . The separation between microphones in the array was set to 0.05 m , which is a distance commonly found in commercial microphone arrays , which is around 0.025 m to 0.040 m. The virtual microphone array is located at an offset position to the loudspeakers , avoiding any symmetry with them . This location should provide more distinctive cross-correlation results between microphones for better differentiation . Figure 42 : 2D Model for Controlled Experiments . 5.2.1.2 Evaluation Criteria A common method to measure the performance of diarization systems is the Diarization Error Rate ( DER ) [ 81 ] , [ 82 ] . The DER is defined as the fraction of the time that is not attributed correctly to a speaker or non-speech [ 38 ] . It can be calculated as the 115 summation of all errors as follows : 𝐷𝐸𝑅 = ( cid:3007 ) ( cid:3002 ) ( cid:2878 ) ( cid:3014 ) ( cid:3036 ) ( cid:3046 ) ( cid:3046 ) ( cid:2878 ) ( cid:3016 ) ( cid:3049 ) ( cid:3032 ) ( cid:3045 ) ( cid:3039 ) ( cid:3028 ) ( cid:3043 ) ( cid:2878 ) ( cid:3004 ) ( cid:3042 ) ( cid:3041 ) ( cid:3033 ) ( cid:3048 ) ( cid:3046 ) ( cid:3036 ) ( cid:3042 ) ( cid:3041 ) ( cid:3019 ) ( cid:3032 ) ( cid:3033 ) ( cid:3032 ) ( cid:3045 ) ( cid:3032 ) ( cid:3041 ) ( cid:3030 ) ( cid:3032 ) ( cid:3013 ) ( cid:3032 ) ( cid:3041 ) ( cid:3034 ) ( cid:3035 ) ( 5.0 ) , where FA is the length of False Alarms , Miss is the length missed speech segments , Overlap is the total length of overlapped speech , Confusion is the total length of misclassified segments , and the Reference Length is the total length of the audio reference . Overlap was not used in any of the tests . 5.2.2 “ HAL 9000 ” Experiments The objective of this experiment is to demonstrate that the proposed method can identify speakers solely on the location of the speaker and independently of their speech characteristics . This was accomplished by using non-anechoic audio as the speech source , obtained from a raw video clip of a classic movie . Many of the software packages for speech processing found during this research provided some sort of test files for evaluation . One of these demos included a phrase from the classical 1967 movie “ 2001 , a Space Odyssey ” . In this movie , the human crew faced the rebellion of the spaceship ’ s computer , “ HAL 9000 ” , which after some malfunction , attempts to kill the crew . The phrase “ I ’ m sorry Dave , I ’ m afraid I can ’ t do that ” is still very well-known nowadays when we discuss the implications of artificial intelligence taking over the control of critical missions . 116 This experiment used a clip of 128 seconds of duration where this famous phrase is spoken . This clip included the conversation between Dave , who is inside a space pod ( Fig . 43 , clip1 ) , and HAL at the mothership ( Fig . 43 , clip2 ) . The video scenes switched between the space pod and the spaceship , with voices coming from radio transmissions , or the inside of the spaceships , depending on the scene . There is also some background noise from the electronic equipment at the space pod . This clip can be downloaded from YouTube at https : //www.youtube.com/watch ? v=Wy4EfdnMZ5g & t=11s Figure 43 : Video Clips of Dave ( Clip1 ) , and HAL ( Clip2 ) SOURCE : Fandango Movie Clips . Two sets of experiments were performed : Experiment 1 was aimed to determine if there was any biasing on the results as the product of the location of the speakers . Experiment 2 evaluated the effects of training in the results . 117 5.2.2.1 Source Preparation and Editing This experiment played Dave ’ s and HAL ’ s voices independently at the loudspeakers . To do so , the YouTube video was converted into a single channel using Audacity ® version 2.4.2 [ 83 ] and saved as a MP4 48 kHz audio ( See Fig 44 ( a ) ) . Then , using Audacity , the segments with voices of Dave and HAL were cut and pasted in two separate channels of a new stereo track ( Fig . 44 ( b ) ) . The intervals with noise were converted into silence to allow the recording noise to come from an external source . Dave was placed on the right track and Hal was placed on the left track . The noise segments were copied and pasted into a separate audio track and burnt into a CD ( Fig . 44 ( c ) ) . 118 Figure 44 : Sources and Noise for HAL 9000 Experiment 119 With this configuration , it was possible to play Dave at loudspeakers 1 ( S1 ) and 2 ( S2 ) , and HAL at loudspeakers 3 ( S3 ) and 4 ( S4 ) , by using the loudspeaker switch . The noise was played at the CD player in a continuous loop and modeled as S5 or S6 . 5.2.2.2 Ground Truth Recording Two sets of recordings were taken for this experiment . Set 1A consisted of playing the audio track using the loudspeakers 1 ( Dave ) and 3 ( HAL ) . Set 1B consisted of playing the loudspeakers 2 ( Dave ) and 4 ( HAL ) . The noise track was played in a continuous loop by a CD player located at the position of Source 6 . The audio was recorded using the Canon video camera with the microphone located at the position of microphone 3 in Table IX . The recording was transferred to the computer for segmentation and training . Because the camera records audio in stereo mode and the code can only handle mono audio , the stereo track was converted into mono audio by removing the right channel . This conversion kept intact all the spatial information contained in the left channel . Using Audacity ’ s “ convert to mono ” feature would have mixed both channels , rendering the spatial information useless . Fig . 45 shows the final sets 1A and 1B of audio captured by the camera . 5.2.2.3 Training and Segmentation Both models for experiments 1 and 2 were trained with segments of speech from Dave and HAL , and a segment of noise . For experiment 1A , the model was trained with Dave as S1 using a 1.98 s long segment , and HAL as S3 with a 1.76 long segment , as shown in Fig 46 . 120 Figure 45 : Ground Truth Sets A and B for HAL 9000 Experiment . 121 Figure 46 : Training Segments for HAL and Dave . 122 Noise was trained as S6 with a 2 s long segment . S2 , S4 , and S5 were set to silence . For experiment 1B , Dave was trained as S2 , and HAL was trained as S4 . S1 , S3 , and S5 were set to silence , and S6 was noise . All training segments were about the same length as in 1A . For Experiment 2 , the model was trained with HAL as speaker S1 and S4 , and Dave as speaker S2 and S3 . The noise was trained as S6 , and S5 was set to silence . The recorded audio was segmented in two different ways . For experiment 1 , the VAD was set with a maximum length segment of 5 seconds , ending with an audio of 120.39 s after subtracting the dropped segments . For experiment 2 , the length of the segments was limited to a maximum of 1.5 s. Frames of less than 500 ms were discarded for both experiments . 5.2.2.4 Testing and Results Table X shows the results of experiments 1 and 2 . We can appreciate that the length of the segments has an influence on the DER . In this experiment , the longer the segments , the less the error . These results agree with our previous discussion on the amount of information needed for proper cross-correlation . It is important then to optimize the length of the segments so they can contain as much information as possible and maximize the matching probabilities with the training template . 123 Table X : DER Results for HAL 9000 Experiments . Exp Test No . Segments 1 2 A B -- 71 71 122 Properly Classified Segments 58 58 99 False Alarms ( s ) 5.68 1.22 11.31 Miss ( s ) Confusion ( s ) 1.46 3.01 1.2 2.88 2.32 10.98 DER 0.083 0.054 0.195 5.2.3 Multi-Speaker Identification Experiments The objective of the experiments in this section is to measure the performance of the proposed method to identify several speakers in single-channel recording , independently of the content of their speech . As with the previous experiments , the geometry of the room and the location of the speakers is known , allowing for models that represent more accurately the actual acoustic scene under analysis . The experiments in this section used the same lab setup and models of the “ HAL 9000 ” experiment . The experiment was divided into four separate tests , that included two speakers and four speakers . Three of the experiments have two independent speakers repeating the same phrases , at different positions . The last experiment has four separate speakers at four different locations . 5.2.3.1 Source Preparation and Editing The speech sources for the experiments consisted of four different speakers , two male , and two females , sampled at a rate of 48 kHz . These sources were downloaded from 124 the Telecommunications and Signal Processing Laboratory of McGill University , database version 2 [ 84 ] . The lengths of these sources vary between 1.2 to 3 s , approximately . A total of four audio tracks were prepared for analysis . Tracks A , B , and C had two speakers , while track D had four . The sources were arranged into one stereo track , so they can be played at the loudspeakers LS1 and LS3 , and then switched to be played at LS2 and LS4 , as it was done with the HAL 9000 experiments . A small pause was inserted to allow for switching between loudspeakers . Table XI shows the structure of each of the audio sample . Each sequence in the table indicates the label of the active speaker , the loudspeaker playing the speech , and the label of the spoken phrase . For example , audio sample A contains two sequences , 1 and 2 . Sequence 1 is played at loudspeaker S1 by speaker 1 , speaking phrase “ a ” . Sequence 2 is played by speaker 2 , loudspeaker S3 , speaking phrase “ b ” . In samples B , C , and D , speakers repeat some of the phrases with the objective to demonstrate the ability of the proposed system to differentiate the speakers regardless of their speech content . 125 Table XI : Multi-Speaker Experiment Sequence Table Duration ( s ) Conditions 1 2 3 -- -- -- Loudspeaker S1 S3 Speaker Phrase 1 a 2 b Loudspeaker S2 S4 S2 Speaker Phrase 1 a 2 a 1 b Sequence 4 -- -- -- -- -- -- 5 -- -- -- -- -- -- 6 -- -- -- -- -- -- 7 -- -- -- -- -- -- 8 -- -- -- -- -- -- 9 -- -- -- -- -- -- A 4.78 B 6.05 Loudspeaker S1 S1 S3 S1 S2 S4 S4 S4 S4 C 39.58 Speaker Phrase 1 a 1 b 2 c 1 d 1 e 2 f 2 g 2 h 2 i 10 -- -- -- -- -- -- -- -- -- l e p m a S Loudspeaker S1 S3 S1 S3 S3 S2 S2 S4 S2 S4 D 27.73 Speaker Phrase 1 a 2 a 1 b 2 b 2 c 3 d 3 e 4 f 3 g 4 h 5.2.3.2 Ground Truth Recording The audio was captured the same way as in the “ HAL 9000 ” experiments , using the Canon video camera and saving the video recording in the camera ’ s internal SD storage . Ambient noise was injected by paying background noise using the CD player , as it was done for the “ HALL 9000 ” experiments . The background noise was extracted from one of the AOLME video recordings . As with the “ HAL 9000 ” experiments , the stereo recording from the video camera was converted into single-channel audio by removing the right channel , before the analysis . 126 5.2.3.3 Training and Segmentation The training was done with segments that had a maximum length of 1.5 s for each of the speakers , plus 1.5 s segment of noise . The custom VAD was used for segmentation . The number of segments produced for each of the audio tracks varied as is shown in the results table . 5.2.3.4 Testing and Results Testing was conducted in the same manner as the “ HAL 9000 ” experiments . The results for each of the segments are shown in Table XII . Table XII : Controlled Environment Experiments Diarization Error Rate Results Audio Sample No . Speakers No . Segments A B C D 2 2 2 4 9 15 116 37 Properly Classified Segments 7 12 98 27 False Alarms 0 0 10 2 Miss Confusion DER 2 2 0 0 0 1 8 8 0.19 0.19 0.12 0.27 Table V shows that the DER is not more than 0.27 in the worst case . These results are comparable or better than DER results from methods using databases and neural networks [ 85 ] 127 5.3 AOLME Experiments The controlled environment experiments demonstrated that the proposed method could identify speakers in single-channel recordings . These experiments analyzed audio samples that featured organized speech ( one speaker at a time ) , where the speakers are well separated from each other ( no overlapping between speakers ) . The objective of the AOLME experiments in this section is to evaluate the performance of the proposed method to identify speakers in single-channel audio recordings from videos of noisy multi-speaker collaborative environments . This section evaluates the process of selection of the AOLME videos for the experimental analysis , and discusses the models employed for the analysis . The analysis of the audios will follow the same approach as the previous experiments . 5.3.1 Evaluation and Selection of AOLME Videos There are several hundred hours of AOLME video recordings available for analysis but , because of the experimental nature of this research work , it was necessary to select videos that met certain characteristics that facilitate the preparation of the models and the setup of the experiments . The models used in the previous experiments proved to perform well , and for this reason , it was necessary to search for AOLME videos with similar geometric characteristics as these models , i.e. , the participants were in similar places as the speakers in the model from our previous experiments . The selection consisted of four videos with 2 , 3 , 4 and 5 participants from the library of videos . The videos were 128 approximately 3 minutes long each . Fig . 47 shows frames from these videos with 2 participants ( a ) , 3 participants ( b ) , 4 participants ( c ) , and 5 participants ( d ) . As was done in the previous experiments , the stereo audio track for each video was extracted and converted into a 48 kHz single channel by removing the right channel . Figure 47 : Video Clips for AOLME Experiments . 5.3.2 Model Preparation The model used for this experiment followed the same geometry as the previous experiments , with the width of the table adjusted to 1.8 m to fit the AOLME scene more accurately . Instead of generating separate models for each of the videos , the model had all 129 three speakers for all the experiments . As previously done , the locations of the absent speakers were turned off by training with a silence segment . Fig . 48 shows the 2D Pyroomacoustics model and Table XIII shows the locations of the speakers and the microphones . Figure 48 : 2D Model for AOLME Experiments . 130 Table XIII : Location of Speakers and Microphones for AOLME Experiments . s r e k a e p S s c i M m o o R S1 S2 S3 S4 S5 S6 M1 M2 M3 M4 M5 M6 M7 CORNER 1 CORNER 2 CORNER 3 CORNER 4 EXTRUDE X 0.40 0.01 1.00 0.40 1.20 2.40 0.75 0.85 0.80 0.80 0.80 0.80 0.80 0.00 0.00 2.50 2.50 Y 1.79 0.80 1.79 0.01 0.01 1.00 1.00 1.00 1.00 1.05 0.95 1.10 0.90 0.00 1.80 1.80 1.80 2.00 Z 0.25 0.25 0.25 0.25 0.25 1.50 0.03 0.03 0.03 0.03 0.03 0.03 0.03 -- -- -- -- 5.3.3 Training and Segmentation The same training and segmentation principles were used as in the previous experiments . Training used a 1.5 to 2 s long sample of each of the participants , plus a similar length segment of background noise . Because the same model was used for all participants , non-active speakers were trained with a silence segment of 2 s duration . Table XIV shows the speaker assignment for each of the experiments . 131 Table XIV : Speaker Assignment for AOLME Experiments . Audio Sample A B C D Speaker Assignment S2 S1  Silence    Silence   S3 Silence Silence  S5 S4  Silence  Silence      S6 Noise Noise Noise Noise The Ground Truth for each audio was segmented using the VAD , discarding segments with less than 0.5 s duration , and limiting the length of the segments to 1.5 s maximum . The total number of segments for each sample is shown in the results table . 5.3.4 Testing and Results The same type of analysis was applied as in the previous experiments . Table XV ( a ) shows an example of the Cross-Correlation results of analyzing one segment of Audio Sample B . Tables VII ( b ) , ( c ) , and ( d ) show the training CC tables with the score of each possible speaker . Each match is represented by a zero ( 0 ) . In this case , the segment corresponds to speaker 2 . 132 Table XV : CC Tables for AOLME Experiment . 1-2 1-3 1-4 1-5 1-6 1-7 2-3 2-4 2-5 2-6 2-7 3-4 3-5 3-6 3-7 4-5 4-6 ( a ) Microphone Cross Correlation . Unknown Segment -64 -65 11 63 -45 -75 89 72 -96 -236 137 133 -100 -236 306 133 12 -75 90 97 -68 0 -28 0 18 -30 56 8 -74 -103 203 57 -276 -103 92 57 71 -11 79 21 -9 65 -34 -63 -72 -69 -41 42 -41 -69 35 42 57 19 21 1 -27 75 -123 -72 14 0 82 0 1-2 1-3 0 91 0 -53 11 0 4 0 1-2 1-3 0 -245 -1 0 0 0 4 0 1-4 0 -83 -145 0 1-4 0 21 -145 12 1-5 -14 -83 197 0 1-5 -14 21 218 12 ( b ) Microphone Cross Correlation . Training Speaker 1 . Score : 25 4-5 -127 0 21 0 2-6 2-7 -5 -9 53 -1 17 -11 0 -1 3-4 3-5 8 0 -11 1 1-6 1-7 2-3 4 18 5 0 0 110 1 6 -5 1 0 12 2-5 -139 0 5 -1 3-7 -4 0 -2 -4 3-6 0 1 1 -1 2-4 4 0 192 -1 0 0 2 1 ( c ) Microphone Cross Correlation . Training Speaker 2 . Score : 32 4-5 5 0 0 0 2-6 2-7 -5 -13 0 0 1 5 0 0 3-4 3-5 14 -1 0 0 3 -11 0 0 1-6 1-7 2-3 4 18 5 0 0 0 1 -9 -5 4 0 0 2-5 -270 0 5 -1 3-7 -4 1 -11 -4 3-6 0 1 3 -1 2-4 204 0 419 -1 ( d ) Microphone Cross Correlation . Training Speaker 3 . Score : 21 1-2 13 0 -5 0 1-3 -1 -1 14 -209 1-4 5 -121 -77 0 1-5 -14 -121 202 0 1-6 1-7 2-3 -6 13 5 -194 0 0 0 -5 -5 4 0 24 2-4 -198 0 182 11 2-5 2-6 2-7 -5 -9 -72 0 -2 0 -155 5 5 0 11 11 3-4 3-5 0 7 0 2 1 7 3 2 3-6 1 191 3 -3 3-7 3 1 -3 209 4-5 4 0 6 0 133 285 98 18 -36 4-6 56 0 2 1 4-6 394 5 1 1 4-6 -9 0 -31 1 s r e k a e p S s r e k a e p S s r e k a e p S s r e k a e p S 1 2 3 6 1 2 3 6 1 2 3 6 1 2 3 6 4-7 -58 236 -94 -133 4-7 -333 83 0 0 4-7 -333 -21 0 -12 4-7 -267 121 -12 0 5-6 294 98 -8 -36 5-6 206 0 5 1 5-6 208 5 0 1 5-6 201 0 5 1 5-7 3 236 -107 -133 5-7 -1 83 37 0 5-7 0 -21 -202 -12 5-7 -5 121 16 0 6-7 -80 75 -125 -97 6-7 4 -110 -15 -12 6-7 0 0 0 0 6-7 4 0 -12 -24 Table XVI shows the results of the analysis of all Audio Samples , with the respective DER for each experiment . Table XVI : Classification Results for AOLME Experiments . Audio Sample Sample Duration ( s ) No . Speakers No . Segments Properly Classified Segments False Alarms Miss Confusion DER A B C D 244 256 381 257 2 3 4 5 311 328 489 339 281 302 426 284 5 8 10 12 10 10 25 15 15 8 28 28 0.095 0.079 0.12 0.16 5.4 Comparison with Other Methods The final set of experiments focus on comparing our proposed method against Google ’ s and Amazon AWS . Google ’ s and Amazon AWS were two of the cloud-based speech processing services introduced in the background section of this dissertation . Microsoft Diarization service was in the process of being updated by the time this dissertation was written and , therefore , it was not possible to run any experiment with it . 5.4.1 Methodology for Comparison The diarization services provided by Google and Amazon differ from the proposed method in three aspects . First , they do not require a sample of audio for training . Second , the audio samples to diarize need to be of a minimum duration of 4 s , approximately . Third , 134 their output does not provide a label of the active speaker , but rather a set of text transcripts that contain the speech segment , the abstract speaker label ( e.g. , speaker 0 , speaker 2 ) , the active time of the speaker on the transcript segment , and the confidence rate . Given these constraints , the only fair comparison criteria are to manually measure each speaker ’ s ground truth active time manually and compare these times with the results of the analysis by all three methods . It was necessary to add a section of code to the proposed method to measure the length of each of the segments that are already classified and totalize the time for the same speaker plus noise . 5.4.2 Selection , Preparation , and Ground Truth Measurements of Videos for Analysis The analysis consisted of a total of 8 AOLME videos containing 2 , 3 , 4 , and 5 speakers . The duration of each video was limited to a maximum of 3 minutes . The audio from each video was extracted using Audacity and downshifted to 16 kHz for upload to Goggle and Amazon . The audio files for our methods were sampled at a rate of 48 kHz . Each speaker ’ s active time from the ground truth audio was measured using a stopwatch . In some of the AOLME videos , it was difficult to assess this time due to several speakers being active simultaneously . In these cases , each speaker ’ s time was recorded by listening to his/her voice and watching his/her lip movement on video , even if their speech overlapped at any moment . 135 5.4.3 Training and Segmentation The system was trained with audio samples of about 1.8 s long from each speaker and noise , using a VAD with a maximum segment length of 1.2 s. All segments with a duration of less than 0.5 s were dropped . There was no need for training on Google or Amazon ; these systems trained by using the uploaded audio and their databases . 5.4.4 Testing and Analysis Each of the audio files from the videos was analyzed using the modified code that totalizes each speaker ’ s time , with no other additional steps . For Amazon and Google , the audio was uploaded to the cloud . Because both Amazon and Google ’ s methods return only abstract labels , the output transcriptions of each of the speakers were used to manually match the identity of the speaker on each segment , noting that both Amazon and Google label the first active speaker they detect as “ speaker 0 ” . 5.4.5 Results Table XVII shows the results of this experiment , with the percentage error highlighted in light blue . The error was calculated using ( 5.1 ) . Percent error = estimated time − true time true time ∗ 100 ( 5.1 ) . 136 Table XVII : Experimental Comparison Between Methods . Audio Sample No . of Speakers Speaker 1 2 3 4 5 6 7 2 2 3 3 4 4 5 8 5 S1 S2 S1 S2 S1 S2 S3 S1 S2 S3 S1 S2 S3 S4 S1 S2 S3 S4 S1 S2 S3 S4 S5 S1 S2 S3 S4 S5 Amazon AWS Google Cloud 1.95 45.25 4.85 8.24 40.88 47.08 31.51 61.41 24.01 3.72 40.39 7.53 Time Error ( s ) % 94.52 19.21 74.47 170.60 120.90 12.99 45.46 152.14 9.88 64.67 143.74 40.21 0.00 100.00 106.36 61.79 37.67 36.19 0.00 100.00 52.19 84.48 8.93 20.05 0.00 100.00 0.00 100.00 78.70 221.49 36.95 65.84 100.00 0.00 250.00 38.05 3070.83 100.00 0.00 60.54 28.30 88.77 6.74 100.00 0.00 13.82 39.24 60.04 13.31 100.00 0.00 10.92 100.00 0.00 31.65 53.73 53.13 21.67 100.00 0.00 44.00 15.63 17.61 46.22 17.52 56.02 42.23 Error Time % ( s ) 8.63 127.10 100.00 0.00 31.40 73.40 66.59 269.33 66.59 1009.83 50.80 10.29 80.20 31.39 0.00 0.00 8.30 35.00 94.30 53.59 29.19 15.29 3.30 5.09 24.90 0.00 54.70 46.60 6.29 29.59 7.49 11.20 29.59 50.45 11.12 22.00 13.49 100.00 100.00 25.69 17.20 27.71 118.91 31.01 40.62 175.00 74.86 64.01 100.00 26.86 279.79 55.95 14.38 199.60 26.46 37.93 Proposed Method Ground Truth Time Error Time ( s ) % ( s ) 14.54 99.99 117.00 25.80 34.62 27.52 5.61 107.00 113.00 23.44 18.03 30.01 20.69 244.83 6.00 102.52 100.52 13.45 68.93 25.38 15.30 41.61 14.69 68.23 91.57 25.39 13.28 27.69 4.20 7.99 64.53 10.71 48.86 10.93 18.80 42.05 3.60 22.27 27.54 9.26 65.74 27.66 10.86 28.29 11.17 42.27 73.84 24.48 22.28 25.75 1.20 20.25 69.19 9.41 43.12 12.27 14.28 34.56 2.50 15.23 47.67 137 Table XVIII shows the average error for 2 , 3 , 4 , and 5 speakers , as well as the total average error for each method . Table XVIII : % Average Error for All Three Methods . No . of Speakers 2 3 4 5 Total Proposed Method 18.99 57.67 58.21 29.11 42.10 Amazon AWS 88.74 67.14 470.34 65.44 184.82 Google Cloud 102.34 201.15 67.02 87.98 108.29 The results presented in Tables XVII and XVIII show a substantial reduction in the achieved error rate . More specifically , error reduction ranges from 50 % to 87 % . The color codes used in Table XVII emphasize the results of this experiment . The red highlighting denotes cases of failures where we have a speaker that was completely missed , or the estimated talking time of the speaker had more than a 100 % error ( e.g. , an over-estimating speaker talking time ) . Out of 28 possible speakers across all examples , Amazon AWS gave failing results for 14 cases ( 50 % ) , Google cloud gave failing results for 10 cases ( 36 % ) , while the proposed method gave failing results for 2 cases ( 7 % ) . It is interesting to notice that the proposed method never failed to detect a speaker ( 0 % error ) , while Amazon AWS could not detect any talking time for 10 cases ( 36 % ) . Google cloud failed to detect any talking time for 4 cases ( 14 % ) . Also , there are failure cases for all 8 samples for Amazon 138 AWS and Google Cloud . In contrast , for the proposed method , there are 2 samples with examples of over-estimation , with 6 samples being free of dramatic failures . Teal highlighting denotes cases where the total estimated speaking time gave 20 % or less error . Based on this criterion , both AWS and Google Cloud gave satisfactory results in 5 cases ( 18 % ) versus 11 cases ( 39 % ) for the proposed method . 139 Chapter 6 . Summary , Conclusions , and Future work This dissertation presented a method for speaker diarization and identification using virtual microphones and cross-correlation patterns . The proposed method identifies speakers in single-channel recordings taken in noisy collaborative environments , such as classrooms and educational workshops . The method gave an error rate that was over 50 % less on average than other available diarization methods when subject to the same testing environments . In contrast with other methods that are considered state-of-the-art , the proposed method requires minimal training and no databases , making it applicable in situations where it is not possible to gather clean speech samples . The background section of this dissertation presented similar research works on speaker diarization and identification based on microphone arrays . Although some of these works included virtual microphone arrays , none of them approached a full virtual array simulation from a single microphone recording . Given the unprecedented focus on Deep Learning methods , alternative approaches are avoided , limiting the number of researchers interested in pursuing them . Yet , the proposed methodology clearly outperformed commercial Deep Learning methods and demonstrated some of their limitations due to their needs for large training datasets . The method presented in this dissertation offers an alternative for educational researchers that are involved with collaborative environments and depend mostly on the 140 analysis of data provided by video recordings . The work in this dissertation showed that other available methods perform poorly under these environments when determining who speaks , when , and for how long . The deficiencies presented by these methods are even more prominent when the participants are from underrepresented groups from which large training databases may not exist . The proposed method demonstrated a significant performance improvement by capitalizing on real video information of the environment under analysis , rather than depending on unrelated training data . Also , by no requiring previous speaker enrollment , this method opens the possibility of analysis of a wide variety of video data that may not have been recorded with the known intention of posteriors analysis . The dissertation method constitutes more of a proof of concept than a fully operational method . The success of the proposed method is due to the possibility of simulating acoustic wave propagation , including speech . Even though this modeling can be complex , we have now powerful personal computers to execute the calculations required by the signal processing algorithms . Furthermore , the code for the simulations is available from large repositories that contain open-source libraries ready for implementation ; nevertheless , there is work that needs to be done to address some of the weaknesses observed so far , such as it is the case where participant speakers move and change their original locations , and when they “ invade ” other ’ s speakers ’ physical location . Under this area , it is possible to eventually adapt the methods from the research work done at the ivPCL lab regarding object and subject tracking . The location of the speakers and the general geometry of the room could be dynamically modified in the models based on the 141 information from video data , thus improving the error rate . Also , the experiments only considered one type of microphone array , leaving open the question of the performance of other types of arrays , such as circular or even volumetric . In addition , the simulation version available during the development of this dissertation had some limitations that impacted the accuracy of the models . Pyroomacoustics released a new version that includes improvements to the models ’ parameters , such as physical modeling of room absorption , reverberation modeling , and multi-pattern microphone simulation . Finally , the method depends on proper audio segmentation and final classification . Most of the misclassifications in the method were the product of improper pre-segmentation and sub- optimal classification . A more sophisticated classifier using machine learning or neural networks would help improve the overall performance . It is possible also to apply clustering classification for unsupervised identification of the speakers . Finally , the method could be extended to support other applications of speech processing , as it can be incorporated as a front end or pre-processor . For example , the method can be used to improve the accuracy of spatial filters for speech enhancement or speaker separation from mixtures . The parameters of the spatial filters can be better determined by estimating the location of the speaker and then optimizing the parameters for that location . 142 Appendix A : Pyroomacoustics Scripts This section describes the two Python scripts that call the Pyroomacoustics libraries to generate the room geometry parameters , calculate the RIRs , and emulate the virtual microphones . a ) Room Geometry Generator : This script accepts the room dimensions and locations of the sources and virtual microphones and generates the 2D and 3D geometric models . The room geometry is saved as a set of .txt files that contains the geometry arrays . This script runs under a Jupyter Notebook . # Location of Sources and Microphones Source6= [ 0.98,0.4 ] Source6_3D= [ 0.98,0.4,0.98 ] Mic_X = [ 0.6,0.65,0.6,0.6,0.6,0.55,0.6 ] Mic_Y = [ 0.6,0.55,0.55,0.45,0.5,0.55,0.65 ] Mic_Z = [ 0.25,0.25,0.01,0.25,0.25,0.25,0.25 ] # Add room room = pra.Room.from_corners ( corners , fs=fs ) # Location of Microphones Array R = np.array ( [ Mic_X , Mic_Y ] ) # [ [ x ] , [ y ] , [ z ] ] # Add source to 2D room room.add_source ( Source1 , signal=s1 ) . . room.add_source ( Source6 , signal=s6 ) room.add_microphone_array ( pra.MicrophoneArray ( R , room.fs ) ) # Execute Location room = pra.Room.from_corners ( corners , fs=fs ) room.extrude ( 1.0 ) 143 R = np.array ( [ Mic_X , Mic_Y , Mic_Z ] ) # [ [ x ] , [ y ] , [ z ] ] room.add_microphone_array ( pra.MicrophoneArray ( R , room.fs ) ) room.add_source ( Source1_3D , signal=s1 ) . . room.add_source ( Source4_3D , signal=s4 ) # Save Geometry np.savetxt ( r ' C : \Users\User\Desktop\PhD s_array.txt ' , corners [ : , : ] , delimiter= ' , ' , fmt= ' % f ' ) . . . np.savetxt ( r ' C : \Users\User\Desktop\PhD ray.txt ' , R [ : , : ] , delimiter= ' , ' , fmt= ' % f ' ) b ) Pyroomacoustics Virtual Microphone Simulation Script This script is used twice to first calculate the RIR from the model sources to the virtual microphones , and then again to emulate the signal at the virtual microphones using the estimated sources . This script is called within LabVIEW , and its outputs are saved in .txt files . # Setup Python import numpy as np import matplotlib.pyplot as plt from scipy.io import wavfile from scipy.signal import fftconvolve import pyroomacoustics as pra # Define Variables Abs = 0 max_o = 0 room_extrude = 0 corners_array = 0 Source1 = 0 . . Source6 = 0 mic_array = 0 # Define model def model_generation ( ) : 144 # Delimit the corners of the room corners = np.array ( corners_array ) .T # [ x , y ] room = pra.Room.from_corners ( corners ) room.extrude ( room_extrude ) # Read Sources fs , s1 = wavfile.read ( r '' C : \ ..... ) . . fs , s6 = wavfile.read ( r '' C : \ ..... ) room = pra.Room.from_corners ( corners , fs=fs ) # Add microphone array R = np.array ( mic_array ) # [ [ x ] , [ y ] , [ z ] ] room.add_microphone_array ( pra.MicrophoneArray ( R , room.fs ) ) # set max_order for RIR room = pra.Room.from_corners ( corners , fs=fs , max_order=max_o , absorption=Abs ) # Set Extrusion room.extrude ( room_extrude ) # Add source arrays and microphones Source1_3D=np.array ( Source1 ) # Source 1 room.add_source ( Source1_3D , signal=s1 ) room.add_microphone_array ( pra.MicrophoneArray ( R , room.fs ) ) # Compute image sources room.image_source_model ( use_libroom=True ) room.compute_rir ( ) # Save Data np.savetxt ( r ' C : \ .... , room.rir [ 0 ] [ 0 ] , delimiter= ' , ' , fmt= ' % f ' ) # Data Mic data_mic=room.mic_array.signals [ 0 , : ] np.savetxt ( r ' C : \ .... , room.rir [ 0 ] [ 0 ] , delimiter= ' , ' , fmt= ' % f ' ) data_mic=room.mic_array.signals [ 1 , : ] np.savetxt ( r ' C : \ .... , room.rir [ 0 ] [ 0 ] , delimiter= ' , ' , fmt= ' % f ' ) data_mic=room.mic_array.signals [ 2 , : ] np.savetxt ( r ' C : \ .... , room.rir [ 0 ] [ 0 ] , delimiter= ' , ' , fmt= ' % f ' ) data_mic=room.mic_array.signals [ 3 , : ] np.savetxt ( r ' C : \ .... , room.rir [ 0 ] [ 0 ] , delimiter= ' , ' , fmt= ' % f ' ) # Data Mic5 data_mic=room.mic_array.signals [ 4 , : ] np.savetxt ( r ' C : \ .... , room.rir [ 0 ] [ 0 ] , delimiter= ' , ' , fmt= ' % f ' ) # Data Mic6 data_mic=room.mic_array.signals [ 5 , : ] np.savetxt ( r ' C : \ .... , room.rir [ 0 ] [ 0 ] , delimiter= ' , ' , fmt= ' % f ' ) data_mic=room.mic_array.signals [ 6 , : ] np.savetxt ( r ' C : \ .... , room.rir [ 0 ] [ 0 ] , delimiter= ' , ' , fmt= ' % f ' ) # Reepeat for all sources . . . return 145 Appendix B : LabVIEW Sub-Vis a ) Room Parameters Reader Figure 49 : Room Parameters Reader Inputs and Outputs Figure 50 : Room Parameters Reader Front Panel . 146 Figure 51 : Room Parameters Reader Block Diagram . 147 b ) Room Model Generator Figure 52 : Room Model Generator Icon . Figure 53 : Room Model Generator Front Panel . 148 Figure 54 : Room Model Generator Block Diagram . 149 c ) Source Estimator This Sub- VI is too complex to display its source block diagram . Instead , a simple functional block diagram is shown . Figure 55 : Source Estimator Icon . Figure 56 : Source Estimator Front Panel . 150 Figure 57 : Source Estimator Simplified Block Diagram . d ) Cross-Correlation Model Calculator Figure 58 : Cross-Correlation Model Calculator Icon . 151 Figure 59 : Cross-Correlation Model Calculator Front Panel . 152 Figure 60 : Cross-Correlation Model Calculator Block Diagram . 153 Figure 61 : Cross-Correlation Model Calculator . Cross-Correlator Sub-VI . Notes on this sub-VI : The cross-correlation results are indicated by the index where the max cross-correlation occur . This method makes the results independent of the sampling frequency . Also , this sub-VI truncates the largest input to make both input files the same size as the smallest one . e ) Model Classifier Figure 62 : Model Classifier Icon with Inputs and Outputs . 154 Figure 63 : Model Classifier Front Panel . 155 f ) Multi-Function Convolution and Correlator Visualizer Figure 64 : Multi-Function Convolution and Correlator Visualizer Front Panel . 156 Figure 65 : Multi-Function Convolution and Correlator Visualizer Diagram . 157 Appendix C : Audio Lab Equipment Specifications a ) Microphone Equipment : Audio-Technica ATR3350xIs  Element : Condenser  Polar Pattern : Omnidirectional  Frequency Response : 50 – 18,000 Hz .  Sensitivity : -54 db .   Power Source : Battery Type : LR44 . Impedance : 1,000 ohms Comica CVM-V020  Transducer : Back Electrets Condenser  Directivity : Omnidirectional  Frequency Range : 100Hz ~ 12KHz  THD : ≤1 %  Sensitivity : 35dB ±3dB  Signal/Noise Ratio : ≥60dB  Power Source : 48V Phantom Powered 158 Excelvan 700  Polar Pattern : Uni-directional  Frequency Response : 20Hz-20kHz  Sensitivity : 45dB±1dB  Output Impedance:1500Ω±30 % ( at 1kHz )  Load impedance : ≥1000 Ω  Equivalent Noise level : 16dBA  Power Source : 48V phantom power supply b ) Audio Processing Equipment TASCAM Model US-16x08  Frequency response : o LINE OUT ( BALANCED ) o 44.1k/48k Hz 20Hz to 20kHz , ±0.3dB ( JEITA ) o 88.2k/96k Hz 20Hz to 40kHz , ±0.3dB ( JEITA ) 100dB or more 100dB or more  THD 0.008 % or less  S/N ratio  Crosstalk  EIN  Sampling frequency 44.1k/48k/88.2k/96k Hz  Quantization bit rate 16/24-bit –125dBu or less 159  Analog audio inputs : o MIC IN ( IN 1-8 )  Connector XLR-3-31 ( 1 : GND , 2 : HOT , 3 : COLD ) , 2.4kΩ BALANCED  Input impedance  Nominal input level  GAIN : MAX –68dBu ( 0.0003Vrms )  GAIN : MIN –12dBu ( 0.195Vrms )  Maximum input level +8dBu ( 1.947Vrms )  Gain 56dB o LINE IN ( IN 9-10 )  Connector 1/4 '' ( 6.3mm ) TRS-jack ( T : HOT , R : COLD , S : 10kΩ GND ) , BALANCED  Input impedance  Nominal input level  GAIN : MAX –41dBu ( 0.0069Vrms )  GAIN : MIN +4dBu ( 1.228Vrms )  Maximum input level +24dBu ( 12.182Vrms )  Gain 45dB AIWA Stereo Audio Amplifier  Power output : 80 watts per channel into 8Ω ( stereo )  Surround output : 80W ( front ) , 80W ( center ) , 80W ( rear )  Frequency response : 20Hz to 20kHz  Total harmonic distortion : 1 %   Output : 300mV ( line ) Input sensitivity : 2.5mV ( MM ) , 300mV ( line ) 160  Speaker load impedance : 8Ω ( minimum ) c ) Loudspeakers Polk Audio RM6751  Power Range : 20- 100 W  Frequency Response : 40 Hz – 24 kHz  Sensitivity : 89 @ 2.83Vrms dB  Impedance ( Ohms ) : 8 161 References [ 1 ] S. S. Tirumala , S. R. Shahamiri , A. S. Garhwal , R. Wang , “ Speaker identification features extraction methods : A systematic review ” . Expert Systems with Applications , vol . 90 , pp . 250-271 , 2017. doi : 0957-4174 , https : . [ 2 ] J. Brownlee , “ Impact of Dataset Size on Deep Learning Model Skill And Performance Estimates , ” Deep Learning Performance , machinelearningmastery.com , para.4 , Jan. 2 , 2019 . [ Online ] . Available : https : skill-and-performance-estimates/ . [ 3 ] J. Yoon and S. O. Arik “ Estimating the Impact of Training Data with Reinforcement Learning , ” Cloud AI Team Google Research , googleblog.com , para . 2 , Oct. 28 , 2020 . [ Online ] . Available : https : training-data-with.html . [ 4 ] A. Koenecke A. Nam , E. Lake , J. Nudell , M. Quartey , Z. Mengesha , C. Toups , J.R. Rickford , D. Jurafsky S. Goel , “ Racial disparities in automated speech recognition , ” Proceedings of the National Academy of Sciences of the United States of America , April 7 , 2020 , 117 ( 14 ) :7684-7689 , [ Online serial ] . Available : https : //www.pnas.org/content/117/14/7684 . [ 5 ] J. Martin , K.Tang , “ Understanding Racial Disparities in Automatic Speech Recognition : The Case of Habitual “ be ” . Presented at 21st International Conference on Speech Processing and Applications , Shanghai , China , 2020 . [ 6 ] R. Gupte , S. Hawa , and R. Sonkusare , “ Speech recognition using cross correlation and feature analysis using mel-frequency cepstral coefficients and pitch , ” In Proc . 2020 IEEE International Conference for Innovation in Technology ( INOCON ) , 2020 , pp . 1-5 . [ 7 ] G. Ekim , N. Ikizler , A. Atasoy , and I. H. Cavdar , “ A speaker recognition system using by cross correlation , ” In Proc . 2008 IEEE 16th Signal Processing , Communication and Applications Conference , 2008 , pp . 1-4 . 162 [ 8 ] The University of New Mexico , “ AOLME : Advancing Out-of-school Learning in Mathematics and Engineering ” . [ Online ] . Available : https : //aolme.unm.edu/ . [ 9 ] University of New Mexico ’ s Image and Video Processing and Communications Lab ( ivPCL ) . [ Online ] . Available : https : //ivpcl.unm.edu/ [ 10 ] C. J. Darsey , “ Hand Detection in Collaborative Learning Environments ” . The University of New Mexico , 2018 . [ 11 ] Teeparthi S. , “ Long-term Video Object Detection and Tracking in Collaborative Learning Environments , ” Fall 2021 ( with distinction ) . She was funded through NSF . [ 12 ] Teeparthi , S. , Jatla , V. , Pattichis , M.S. , Celedón-Pattichis , S. , and LópezLeiva , C. , “ Fast Hand Detection in Collaborative Learning Environments , ” The 19th International Conference on Computer Analysis of Images and Patterns ( CAIP ) , pp . 445-454 , 2021 . [ 13 ] Jacoby A. R. , “ Context-Sensitive Human Activity Classification in Video Utilizing Object Recognition and Motion Estimation , ” Spring 2018 . [ 14 ] Jatla , V. , Teeparthi , S. , Pattichis , M.S. , Celedón-Pattichis , S. , and LópezLeiva , C. , “ Long-term Human Video Activity Quantification of Student Participation , ” in 2021 Asilomar Conference on Signals , Systems , and Computers . [ 15 ] Eilar , C. , Jatla , V. , Pattichis , M. S. , Celedón-Pattichis , S. , & LópezLeiva , C. A. , “ Distributed Video Analysis for the Advancing Out of School Learning in Mathematics and Engineering Project , ” 2016 Asilomar Conference on Signals , Systems , and Computers , pp . 571-575 , 2016 . [ 16 ] Shi , W. , Pattichis , M.S. , Celedón-Pattichis , S. , and LópezLeiva , C. , “ Dynamic Group Interactions in Collaborative Learning Videos , ” 2018 Asilomar Conference on Signals , Systems , and Computers , in press , pp . 1528-1531 , 2018 . [ 17 ] X. Anguera , S. Bozonnet , N. Evans , C. Fredouille , G. Friedland , O. Vinyals , `` Speaker diarization : A review of recent research , '' IEEE Transactions on Audio , Speech , and Language Processing , vol . 20 , no . 2 , pp . 356-370 , 2012 . [ 18 ] M. Alam , M.D . Samad , L. Vidyaratne , A. Glandon , K.M . Iftekharuddin , ” Survey on Deep Neural Networks in Speech and Vision Systems , ” Neurocomputing , Volume 417 , 2020 , pp . 302-321 . 163 [ 19 ] D. Sztahó , G. Szaszák , A. Beke , ‘ ‘ Deep learning methods in speaker recognition : A review , ’ ’ Periodica Polytechnica Electrical Engineering and Computer Science , vol . 65 , no . 4 , pp . 310–328 , Jan. 2021 . [ Online ] . Available : http : //arxiv.org/abs/1911.06615 . [ 20 ] J. Villalba , N. Chen , D. Snyder , D. Garcia-Romero , A. McCree , G. Sell , J. Borgstrom , L. Paola García-Perera , F. Richardson , R. Dehak , P. A. Torres- Carrasquillo , N. Dehak , “ State-of-the-art speaker recognition with neural network embeddings in NIST SRE18 and Speakers in the Wild evaluations , ” Computer Speech & Language , vol . 60 , March , 2020 . [ 21 ] D. Snyder , D. Garcia-Romero , G. Sell , D. Povey and S. Khudanpur , `` X-Vectors : Robust DNN Embeddings for Speaker Recognition , '' 2018 IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) , 2018 , pp . 5329- 5333 . [ 22 ] X . A. Miró , “ Robust speaker diarization for meetings , ” Ph.D. thesis , Speech Processing Group Department of Signal Theory and Communications Universitat Politècnica de Catalunya , Barcelona , 2006 . [ 23 ] N. Mitianoudis and M. E. Davies , `` Using beamforming in the audio source separation problem , '' Seventh International Symposium on Signal Processing and Its Applications , 2003 . Proceedings. , 2003 , pp . 89-92 vol.2 , doi : 10.1109/ISSPA.2003.1224822 . [ 24 ] U. Klein and Trình Quốc Võ , `` Direction-of-arrival estimation using a microphone array with the multichannel cross-correlation method , '' 2012 IEEE International Symposium on Signal Processing and Information Technology ( ISSPIT ) , 2012 , pp . 000251-000256 , doi : 10.1109/ISSPIT.2012.6621296 . [ 25 ] T. Padois , “ Acoustic source localization based on the generalized cross-correlation and the generalized mean with few microphones ” . J Acoust Soc Am . 2018 . [ 26 ] S. Pasha and C. Ritz , `` Informed source location and DOA estimation using acoustic room impulse response parameters , '' 2015 IEEE International Symposium on Signal Processing and Information Technology ( ISSPIT ) , 2015 , pp . 139-144 , doi : 10.1109/ISSPIT.2015.7394316 . [ 27 ] S. Tervo , J. Pätynen , and T. Lokki , “ Acoustic reflection localization from room impulse responses , ” Acta Acustica united with Acustica , vol . 98 , no . 3 , pp . 418-440 , 2021 . 164 [ 28 ] M. Hu , P.P . Parada , D. Sharma , S. Doclo , T.V Waterschoot , M. Brookes , P.A . Naylor , `` Single-channel speaker diarization based on spatial features , '' In Proc . IEEE Workshop on Applications of Signal Processing to Audio and Acoustics ( WASPAA ) , 2015 , pp . 1-5 . [ 29 ] P. P. Parada , D. Sharma , P. A. Naylor , “ Non-intrusive estimation of the level of reverberation in speech , ” in Proc . IEEE International Conf . on Acoustics , Speech and Signal Processing ( ICASSP ) , Florence , Italy , May 2014 , pp . 4718–4722 . [ 30 ] D. Vijayasenan , F. Valente , and H. Bourlard , “ Multistream speaker diarization beyond two acoustic feature streams , ” in Proc . IEEE Intl . Conf . on Acoustics , Speech and Signal Processing ( ICASSP ) , Dallas , TX , USA , Mar . 2010 , pp . 4950–4953 . [ 31 ] T. Yoshioka , Z. Chen , D. Dimitriadis , W. Hinthorn , X. Huang , A. Stolcke , M. Zeng , “ Meeting transcription using virtual microphone arrays , ” Microsoft Technical Report MSR-TR-2019-11 , July 2019 . [ 32 ] H. Katahira , N. Ono , S. Miyabe , T. Yamada , S. Makino , “ Nonlinear speech enhancement by virtual increase of channels and maximum SNR beamformer , ” EURASIP Journal on Advances in Signal Processing , 2016 , issue 1 , article 11 , pp . 1- 8 , 2016 . [ 33 ] G. Del Galdo , O. Thiergart , T. Weller and E. A. P. Habets , `` Generating virtual microphone signals using geometrical information gathered by distributed arrays , '' 2011 Joint Workshop on Hands-free Speech Communication and Communication and Microphone Arrays , Edinburgh , pp . 185-190 , 2011 . [ 34 ] A. Izquierdo , J. Villacorta , L. del Val , L. Suárez , and D. Suárez , “ Implementation of a Virtual Microphone Array to Obtain High Resolution Acoustic Images , ” Sensors , vol . 18 , no . 2 , p. 25 , Dec. 2017 . [ 35 ] Tapia , L.S. , Gomez , A. , Esparza , M. , Jatla , V. , Pattichis , M.S. , Celedón-Pattichis , S. , and López-Leiva , C. , “ Bilingual Speech Recognition by Estimating Speaker Geometry from Video Data , ” The 19th International Conference on Computer Analysis of Images and Patterns ( CAIP ) , pp . 79-89 , 2021 . [ 36 ] Siemens Simcenter “ Sound Fields : Free versus Diffuse Field , Near versus Far Field ” [ Online ] . Available : https : [ 37 ] I. Tashev , “ Sound Capture and Processing : Practical Approaches ” . Chichester , West Sussex : John Wiley & Sons Ltd. , pp 341-343 , 2009 . 165 [ 38 ] I. Tashev , “ Sound Capture and Processing : Practical Approaches ” . Chichester , West Sussex : John Wiley & Sons Ltd. , 2009 , pp 171-174 , 2009 . [ 39 ] I. Tashev , “ Sound Capture and Processing : Practical Approaches ” . Chichester , West Sussex : John Wiley & Sons Ltd. , 2009 , pp 74 , 2009 [ 40 ] S. Renals , H. Bourlard , J. Carletta , and A. Popescu-Belis , “ Multi-Modal Signal Processing . Human Interactions in Meetings ” , New York : Cambridge University Press , pp 29-35 , 2012 . [ 41 ] I. Tashev , “ Sound Capture and Processing : Practical Approaches ” . Chichester , West Sussex : John Wiley & Sons Ltd. , pp 171 , 2009 . [ 42 ] I. Cohen , J. Benesty , and S. Gannot , “ Speech Processing in Modern Communications ” . W. Kellerman . Berlin : Springer-Verlag , page 212 , 2010 . [ 43 ] B. Gunel , EE2.LabB : Measurement and Processing of Room Impulse Responses ” . University of Surrey , 2011 . [ Online ] . Available : http : . [ 44 ] B. Xie , “ Head Related Transfer Function and Virtual Auditory Display ” . Second Edition . J. Ross Publishing , Plantation Fl. , pp 352-355 , 2013 . [ 45 ] F. A. Everest , and K.C . Pohlmann , “ Master Handbook of Acoustics ” McGraw Hill , New York , pp 559-560 , 2015 . [ 46 ] D. Diaz-Guerra , A. Miguel , and A. J. Beltran , “ gpuRIR : A python library for room impulse response simulation with GPU acceleration ” . Multimed Tools Appl 80 , 5653–5671 . 2021. https : . [ 47 ] J.B. Allen , D.A . Berkley , “ Image Method for Efficiently Simulating Small-Room Acoustics ” , The Journal of the Acoustical Society of America , 1979 . DOI 10.1121/1.382599 [ 48 ] Wikipedia [ Online ] . Available : https : [ 49 ] I. McLoughlin , “ Speech and Audio Processing , a MATLAB-based Approach ” , Cambridge University Press , New York , page 65 , 2016 . [ 50 ] F. A. Everest , and K.C . Pohlmann , “ Master Handbook of Acoustics ” McGraw Hill , New York , pp 75-76 , 2015 . 166 [ 51 ] S. Renals , H. Bourlard , J. Carletta , and A. Popescu-Belis , “ Multi-Modal Signal Processing . Human Interactions in Meetings ” , New York : Cambridge University Press , pp 40-41 , 2012 . [ 52 ] M. de Campos Niero , A. de Lima Veiga Filho , and A. G. Adami , `` A comparison of distance measures for clustering in speaker diarization , '' 2014 International Telecommunications Symposium ( ITS ) , 2014 , pp . 1-5 , doi : 10.1109/ITS.2014.6947954 . [ 53 ] S. Chen , and P. Gopalakrishnan , ” Speaker , environment and channel change detection and clustering via the bayesian information criterion ” , in Proceedings of DARPA Broadcast News Transcription and Understanding Workshop , 1998 . [ 54 ] Z. Bai , Xiao-Lei Zhang , “ Speaker recognition based on deep learning : An overview ” , Elsevier : Journal of Neural Networks , Volume 140 , 2021 , pp 84-88 , ISSN 0893-6080 , [ Online ] , Available : https : . [ 55 ] O. Ghahabi , “ Deep Learning for i-Vector Speaker and Language Recognition ” . Ph.D. thesis , Speech Processing Group Department of Signal Theory and Communications Universitat Politècnica de Catalunya , Barcelona , 2018 . [ 56 ] Z. Bai , and Xiao-Lei Zhang , “ Speaker recognition based on deep learning : An overview ” , Elsevier : Journal of Neural Networks , Volume 140 , 2021 , pp 68-72 , ISSN 0893-6080 , [ Online ] , Available : https : . [ 57 ] J. Guo , N. Xu , K. Qian , Y. Shi , K. Xu , Y. Wu , and A . Alwan. , “ Deep neural network based i-vector mapping for speaker verification using short utterances ” , Computer Speech & Language , Volume 72 , March 2022 , 101317 . [ Online ] . Available : https : //arxiv.org/abs/2101.09624 . [ 58 ] D. Yifan , X. Yong , Z. Shi-Xiong , C. Yauhan , and W. Liqiang . 2020 . “ Self- Supervised learning for audio-visual speaker diarization ” . In ICASSP 2020 - 2020 IEEE international conference on acoustics , speech and signal processing pp . 4367– 4371 , 2020 . [ 59 ] T. J . Park , and P. Georgiou , “ Multimodal speaker segmentation and diarization using lexical and acoustic cues via sequence to sequence neural networks ” . In Proc . INTERSPEECH 2018 pp . 1373–1377 , 2018 . 167 [ 60 ] L. El Shafey , H. Soltau , and I. Shafran , “ Joint speech recognition and speaker diarization via sequence transduction ” . In Proc . INTERSPEECH 2019 pp . 396–400 , 2019 . [ 61 ] W. Kang , B. Roy , and W. Chow . “ Multimodal speaker diarization of real-world meetings using d-vectors with spatial features ” . In ICASSP 2020 – 2020 IEEE international conference on acoustics , speech , and signal processing pp . 6509–6513 , 2020 . [ 62 ] Amazon AWS , “ Amazon Transcribe ” , 2021 . [ Online ] . Available : https : //aws.amazon.com/transcribe/ ? nc=sn & loc=1 . [ 63 ] Google ’ s Cloud , “ Separating different speakers in an audio recording ” , 2021 . [ Online ] . Available : https : . [ 64 ] Microsoft Azure Product Documentation , “ What is Speaker Recognition ? ” Microsoft , Nov. 3 , 2021 . [ Online ] . Available : https : //docs.microsoft.com/en- . [ 65 ] D. Misal , “ Google Speech Vs Amazon Transcribe : The War of Speech Technology , ” Analytics India Magazine , Oct. 22 , 2018 . [ Online ] , Available : https : speech-technology/ . [ 66 ] M. Saraswat and R. C. Tripathi , `` Cloud computing : comparison and analysis of cloud service providers-AWs , Microsoft and Google , '' In Proc . 2020 9th International Conference System Modeling and Advancement in Research Trends ( SMART ) , 2020 , pp . 281-285 . [ 67 ] A. Woollacott , “ Benchmarking speech technologies , ” Academia.edu , Feb. 2021 . [ Online ] . Available : Academia , https : . [ 68 ] X. Xiao , N. Kanda , Z. Chen , T. Zhou , T. Yoshioka , S. Chen , Y. Zhao , G. Liu , Y. Wu , J. Wu , S. Liu , J. Li , and Y. Gong , “ Microsoft speaker diarization system for the VoxCeleb speaker recognition challenge 2020 , ” In Proc . ICASSP 2021-2021 IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) , 2021 , pp.5824-5828 . [ 69 ] Microsoft speaker recognition . [ Online ] . Available : https : //docs.microsoft.com/en- 168 [ 70 ] I. Tashev , “ Sound Capture and Processing : Practical Approaches ” . Chichester , West Sussex : John Wiley & Sons Ltd. , pp 351 , 2009 . [ 71 ] R. Scheibler , E. Bezzam , and I. Dokmanić , “ Pyroomacoustics : A Python package for audio room simulation and array processing algorithms , ” In Proc . 2018 IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) , 2018 , pp . 351-355 . [ 72 ] Pyroomacoustics documentation . [ Online ] . Available : https : [ 73 ] E. Marin . “ Voice Activity Detection Using Filters ” . University of New Mexico , Spring 2021. eguaderrama @ unm.edu . [ 74 ] I. McLoughlin , “ Speech and Audio Processing , a MATLAB-based Approach ” , Cambridge University Press , New York , pp 24-28 , 2016 . [ 75 ] NI LabVIEW . [ Online ] . Available : https : [ 76 ] NI LabVIEW Convolution . [ Online ] . Available : https : //zone.ni.com/reference/en- [ 77 ] NI LabVIEW Deconvolution . [ Online ] . Available : https : //zone.ni.com/reference/en- [ 78 ] NI LabVIEW Correlation . [ Online ] . Available : https : //zone.ni.com/reference/en- [ 79 ] NI LabVIEW Cross-Correlation . [ Online ] . Available : https : [ 80 ] Waveform Tracktion . [ Online ] . Available : https : [ 81 ] O. Galibert , “ Methodologies for the evaluation of speaker diarization and automatic speech recognition in the presence of overlapping speech , ” In Proc . INTERSPEECH 2013 , 2013 , pp . 1131-1134 . [ 82 ] Q. Wang , “ SimpleDER : a lightweight library to compute Diarization Error Rate ( DER ) ” . [ Online ] . Available : https : //pypi.org/project/simpleder/ . 169 [ 83 ] Audacity® software is copyright © 1999-2021 Audacity Team . Web site : https : //audacityteam.org/ . It is free software distributed under the terms of the GNU General Public License . The name Audacity® is a registered trademark . [ 84 ] P. Kabal , TSP Speech Database , version 2 ( 2018-11 ) , Montreal , Quebec : McGill University Department of Electrical and Computer Engineering Telecommunications & Signal Processing Laboratory , 2018 . [ Online ] . Available : http : //www- mmsp.ece.mcgill.ca/Documents/Data/ . [ 85 ] Y. Fujita , N. Kanda , S. Horiguchi , K. Nagamatsu , and S. Watanabe , “ End-to-end neural speaker diarization with permutation-free objectives , ” In Proc . INTERSPEECH 2019 , 2019 , pp . 4300-4304 . 170"
779,Yann LeCun has a bold new vision for the future of AI,https://www.technologyreview.com/2022/06/24/1054817/yann-lecun-bold-new-vision-future-ai-deep-learning-meta/,2022-06-24,"Around a year and a half ago , Yann LeCun realized he had it wrong . LeCun , who is chief scientist at Meta ’ s AI lab and one of the most influential AI researchers in the world , had been trying to give machines a basic grasp of how the world works—a kind of common sense—by training neural networks to predict what was going to happen next in video clips of everyday events . But guessing future frames of a video pixel by pixel was just too complex . He hit a wall . Now , after months figuring out what was missing , he has a bold new vision for the next generation of AI . In a draft document shared with MIT Technology Review , LeCun sketches out an approach that he thinks will one day give machines the common sense they need to navigate the world . ( Update : LeCun has since posted the document online . ) For LeCun , the proposals could be the first steps on a path to building machines with the ability to reason and plan like humans—what many call artificial general intelligence , or AGI . He also steps away from today ’ s hottest trends in machine learning , resurrecting some old ideas that have gone out of fashion . But his vision is far from comprehensive ; indeed , it may raise more questions than it answers . The biggest question mark , as LeCun points out himself , is that he does not know how to build what he describes . A machine that could think like a person has been the guiding vision of AI research since the earliest days—and remains its most divisive idea . The centerpiece of the new approach is a neural network that can learn to view the world at different levels of detail . Ditching the need for pixel-perfect predictions , this network would focus only on those features in a scene that are relevant for the task at hand . LeCun proposes pairing this core network with another , called the configurator , which determines what level of detail is required and tweaks the overall system accordingly . For LeCun , AGI is going to be a part of how we interact with future tech . His vision is colored by that of his employer , Meta , which is pushing a virtual-reality metaverse . He says that in 10 or 15 years people won ’ t be carrying smartphones in their pockets , but augmented-reality glasses fitted with virtual assistants that will guide humans through their day . “ For those to be most useful to us , they basically have to have more or less human-level intelligence , ” he says . “ Yann has been talking about many of these ideas for some time , ” says Yoshua Bengio , an AI researcher at the University of Montreal and scientific director at the Mila-Quebec Institute . “ But it is good to see it all together , in one big picture. ” Bengio thinks that LeCun asks the right questions . He also thinks it ’ s great that LeCun is willing to put out a document that has so few answers . It ’ s a research proposal rather than a set of clean results , he says . “ People talk about these things in private , but they ’ re not usually shared publicly , ” says Bengio . “ It ’ s risky. ” LeCun has been thinking about AI for nearly 40 years . In 2018 he was joint winner of computing ’ s top prize , the Turing Award , with Bengio and Geoffrey Hinton , for his pioneering work on deep learning . “ Getting machines to behave like humans and animals has been the quest of my life , ” he says . LeCun thinks that animal brains run a kind of simulation of the world , which he calls a world model . Learned in infancy , it ’ s the way animals ( including humans ) make good guesses about what ’ s going on around them . Infants pick up the basics in the first few months of life by observing the world , says LeCun . Seeing a dropped ball fall a handful of times is enough to give a child a sense of how gravity works . “ Common sense ” is the catch-all term for this kind of intuitive reasoning . It includes a grasp of simple physics : for example , knowing that the world is three-dimensional and that objects don ’ t actually disappear when they go out of view . It lets us predict where a bouncing ball or a speeding bike will be in a few seconds ’ time . And it helps us join the dots between incomplete pieces of information : if we hear a metallic crash from the kitchen , we can make an educated guess that someone has dropped a pan , because we know what kinds of objects make that noise and when they make it . In short , common sense tells us what events are possible and impossible , and which events are more likely than others . It lets us foresee the consequences of our actions and make plans—and ignore irrelevant details . But teaching common sense to machines is hard . Today ’ s neural networks need to be shown thousands of examples before they start to spot such patterns . In many ways common sense amounts to the ability to predict what ’ s going to happen next . “ This is the essence of intelligence , ” says LeCun . That ’ s why he—and a few other researchers—have been using video clips to train their models . But existing machine-learning techniques required the models to predict exactly what is going to happen in the next frame and generate it pixel by pixel . Imagine you hold up a pen and let it go , LeCun says . Common sense tells you that the pen will fall , but not the exact position it will end up in . Predicting that would require crunching some tough physics equations . That ’ s why LeCun is now trying to train a neural network that can focus only on the relevant aspects of the world : predicting that the pen will fall but not exactly how . He sees this trained network as the equivalent of the world model that animals rely on . LeCun says he has built an early version of this world model that can do basic object recognition . He is now working on training it to make predictions . But how the configurator should work remains a mystery , he says . LeCun imagines that neural network as the controller for the whole system . It would decide what kind of predictions the world model should be making at any given time and what level of detail it should focus on to make those predictions possible , adjusting the world model as required . LeCun is convinced that something like a configurator is needed , but he doesn ’ t know how to go about training a neural network to do the job . “ We need to figure out a good recipe to make this work , and we don ’ t have that recipe yet , ” he says . In LeCun ’ s vision , the world model and the configurator are two key pieces in a larger system , known as a cognitive architecture , that includes other neural networks—such as a perception model that senses the world and a model that uses rewards to motivate the AI to explore or curb its behavior . Each neural network is roughly analogous to parts of the brain , says LeCun . For example , the configurator and world model are meant to replicate functions of the prefrontal cortex . The motivation model corresponds to certain functions of the amygdala , and so on . The idea of cognitive architectures , especially ones inspired by the brain , has been around for decades . So have many of LeCun ’ s ideas about prediction using models with different levels of detail . But when deep learning became the dominant approach in AI , many of these older ideas went out of fashion . “ People in AI research have kind of forgotten about this a little bit , ” he says . What he has done is taken these older ideas and rehabilitated them , suggesting ways that they can be combined with deep learning . For LeCun , revisiting these out-of-fashion ideas is essential , because he believes the two dominant approaches in modern AI are dead ends . When it comes to building general-purpose AI , there are two main camps . In one , many researchers think the remarkable success of very large language or image-making models like OpenAI 's GPT-3 and DALL-E show that all we need to do is just build bigger and bigger models . In the other camp are champions of reinforcement learning , the AI technique that rewards specific behaviors to make neural networks to learn by trial and error . This is the approach DeepMind used to train its game-playing AIs like AlphaZero . Get the rewards right , the argument goes , and reinforcement learning will eventually produce more general intelligence . Open AI 's language AI wowed the public with its apparent mastery of English – but is it all an illusion ? LeCun is having none of it : “ This idea that we 're going to just scale up the current large language models and eventually human-level AI will emerge—I don ’ t believe this at all , not for one second. ” These large models just manipulate words and images , he says . They have no direct experience of the world . He is equally skeptical about reinforcement learning , because it requires vast amounts of data to train models to do even simple tasks . “ I think that has no chance of working at all , ” says LeCun . David Silver at DeepMind , who led the work on AlphaZero and is a big advocate of reinforcement learning , disagrees with this assessment but welcomes LeCun ’ s overall vision . “ It ’ s an exciting new proposal for how a world model could be represented and learned , ” he says . Melanie Mitchell , an AI researcher at the Santa Fe Institute , is also excited to see a whole new approach . “ We really haven ’ t seen this coming out of the deep-learning community so much , ” she says . She also agrees with LeCun that large language models can not be the whole story . “ They lack memory and internal models of the world that are actually really important , ” she says . Natasha Jaques , a researcher at Google Brain , thinks that language models should still play a role , however . It ’ s odd for language to be entirely missing from LeCun ’ s proposals , she says : “ We know that large language models are super effective and bake in a bunch of human knowledge. ” Jaques , who works on ways to get AIs to share information and abilities with each other , points out that humans don ’ t have to have direct experience of something to learn about it . We can change our behavior simply by being told something , such as not to touch a hot pan . “ How do I update this world model that Yann is proposing if I don ’ t have language ? ” she asks . There ’ s another issue , too . If they were to work , LeCun ’ s ideas would create a powerful technology that could be as transformative as the internet . And yet his proposal doesn ’ t discuss how his model ’ s behavior and motivations would be controlled , or who would control them . This is a weird omission , says Abhishek Gupta , the founder of the Montreal AI Ethics Institute and a responsible-AI expert at Boston Consulting Group . “ We should think more about what it takes for AI to function well in a society , and that requires thinking about ethical behavior , amongst other things , ” says Gupta . Yet Jaques notes that LeCun ’ s proposals are still very much ideas rather than practical applications . Mitchell says the same : “ There ’ s certainly little risk of this becoming a human-level intelligence anytime soon. ” LeCun would agree . His aim is to sow the seeds of a new approach in the hope that others build on it . “ This is something that is going to take a lot of effort from a lot of people , ” he says . “ I ’ m putting this out there because I think ultimately this is the way to go. ” If nothing else , he wants to convince people that large language models and reinforcement learning are not the only ways forward . “ I hate to see people wasting their time , ” he says ."
