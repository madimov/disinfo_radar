title,url,date,text,cleaning,tokens
"
        Machine Learning’s New Math
    ",https://spectrum.ieee.org/number-representation,2022-10-18,"New number formats and basic computations emerge to speed up AI training 
	Recent developments in AI have been astounding, but so are the costs of training neural networks to do their astounding feats. The biggest, such as the language model 
	GPT-3 and the art generator DALL-E 2, take several months to train on a cluster of high-performance GPUs, costing millions of dollars and taking up millions of billions of billions of basic computations.
 
	The training capabilities of processing units have been 
	growing rapidly, as much as doubling in the last year. To keep the trend going, researchers are digging down into the most basic building blocks of computation, the way computers represent numbers. 
 
	“We got a thousand times improvement [in training performance per chip] over the last 10 years, and a lot of it has been due to number representation,” Bill Dally, chief scientist and senior vice president of research at Nvidia said in his keynote talk at the recent 
	IEEE Symposium on Computer Arithmetic.
 
Among the first casualties in the march toward more efficient AI training is the 32-bit floating-point number representation, colloquially known as “standard precision.” Seeking speed, energy efficiency, and better use of chip area and memory, machine-learning researchers have been trying to get the same level of training using numbers represented by fewer bits. The field is still wide open for competitors trying to take the place of the 32-bit format, both in the number representation itself and in the way basic arithmetic is done using those numbers. Here are some recent advances and top contenders unveiled last month at 
	the conference:
  
	Image creator DALL-E was 
	trained on clusters of Nvidia’s A100 GPUs with a combination of the standard 32-bit numbers and the lower-precision 16-bit numbers. Nvidia’s recently announced Hopper GPU includes support for even smaller, 8-bit floating-point numbers. Now Nvidia has developed a chip prototype that takes this trend a step further by using a combination of 8-bit and 4-bit numbers.
 
	The chip managed to maintain computational accuracy despite using less-precise numbers, at least for one part of the training process, called inferencing. Inference is performed on fully trained models to get an output, but it’s also done repeatedly during training.
 
	 “We end up getting 8-bit results from 4-bit precision,” Nvidia’s Dally told engineers and computer scientists.
 
	Nvidia was able to cut down the size of numbers without significant accuracy loss thanks to a technique the company calls per-vector scaled quantization (VSQ). The basic idea goes something like this: A 4-bit number can represent only 16 values exactly. So, every number gets rounded to one of those 16 values. The loss of accuracy due to this rounding is called quantization error. However, you can add a scale factor to uniformly squeeze the 16 values closer together on the number line or pull them further apart, decreasing or increasing the quantization error.
 

Nvidia's per-vector scaling quantization (VSQ) scheme better represents the numbers needed in machine learning than standard formats such as INT4. Nvidia 
	The trick is to squeeze or expand those 16 values so they optimally match the range of numbers you actually need to represent in a neural network. This scaling is different for different sets of data. By fine-tuning this scaling parameter for every set of 64 numbers inside the neural-network model, Nvidia researchers were able to minimize quantization error. The overhead from calculating the scaling factors is negligible, they found. But the energy efficiency doubled as the 8-bit representation was reduced to 4-bit.
 
	The experimental chip is still under development, and Nvidia engineers are working on how to use these principles during the full training process, not just in inferencing. If successful, a chip combining 4-bit computations, VSQ, and other efficiency improvements could achieve 10 times the number of operations per watt as the Hopper GPU, said Dally.
 
Back to top
 
	Back in 2017, two researchers, 
	John Gustafson and Isaac Yonemoto, developed an entirely new way of representing numbers.
 
	Now, a team of researchers at the Complutense University of Madrid have 
	developed the first processor core implementing the posit standard in hardware and showed that, bit for bit, the accuracy of a basic computational task increased by up to four orders of magnitude, as compared to computing using standard floating-point numbers.  
	The advantage of posits comes from the way the numbers they represent exactly are distributed along the number line. In the middle of the number line, around 1 and –1, there are more posit representations than floating point. And at the wings, going out to large negative and positive numbers, posit accuracy falls off more gracefully than floating point.
  
	“It’s a better match for the natural distribution of numbers in a calculation,” says Gustafson. “It’s the right dynamic range, and it’s the right accuracy where you need more accuracy. There’s an awful lot of bit patterns in floating-point arithmetic no one ever uses. And that’s a waste.”
 
	Posits accomplish this improved accuracy around 1 and –1 thanks to an extra component in their representation. Floats are made up of three parts: a sign bit (0 for positive, 1 for negative), several “mantissa” (fraction) bits denoting what comes after the binary version of a decimal point, and the remaining bits defining the exponent (2
	exp).
 
	Posits keep all the components of a float but add an extra “regime” section, an exponent of an exponent. The beauty of the regime is that it can vary in bit length. For small numbers, it can take as few as two bits, leaving more precision for the mantissa. This allows for the higher accuracy of posits in their sweet spot around 1 and –1.
 

Posits differ from floating-point numbers by the addition of an extra variable-length regime section. This gives posits a better accuracy around zero, where the majority of numbers used in neural networks reside. 
            Complutense University of Madrid/IEEE
        
 
	With their new hardware implementation, which was synthesized on a field-programmable gate array (FPGA), the Complutense team was able to compare computations done using 32-bit floats and 32-bit posits side by side. They assessed their accuracy by comparing them to results using the much more accurate but computationally costly 64-bit floating-point format. Posits showed an astounding four-order-of-magnitude improvement in the accuracy of matrix multiplication, a crucial calculation in neural networks. They also found that the improved accuracy didn’t come at the cost of computation time, only a somewhat increased chip area and power consumption.
 
Back to top
 
	Computer scientists in Switzerland and Italy have worked out a bit-reducing scheme, meant for processors using open-source RISC-V instruction-set architecture, which is gaining ground among new processor developers.
 
	The team’s extension to the RISC-V instruction set includes an efficient version of computations that mix lower- and higher-precision number representations. With their improved mixed-precision math, they obtain a factor-of-two speedup in basic computations involved in training neural networks.
 
	Lowering precision has knock-on effects during basic operations that go beyond just the loss of accuracy due to the shorter number. Multiplying two low-precision numbers can result in a number that is too small or too large to represent given the bit length—phenomena called underflow and overflow, respectively. A separate phenomenon, called swamping, happens when you add a large low-precision number and a small low-precision number. The result can be the complete loss of the smaller number.
 
	Mixed precision comes to the rescue in mitigating overflow, underflow, and swamping. Computations are performed with low-precision inputs and result in higher-precision outputs, getting a batch of math done before rounding back to lower precision.
 
	The Europe-based team focused on a basic component of AI computations: the dot product. It’s usually implemented in hardware with a series of components called fused multiply-add units (FMAs). These perform the operation d = a*b + c in one go, only rounding at the end. To reap the benefits of mixed precision, inputs a and b are low precision (say, 8 bits), while c and the output d are high precision (say, 16 bits).
 
	Luca Benini, chair of digital circuits and systems at ETH Zurich and professor at the University of Bologna and his team had a simple insight: Instead of doing one FMA operation at a time, why not do two in parallel and add them together at the end? Not only does this prevent losses due to rounding between the two FMAs, but it also allows for better utilization of memory, because no memory registers are left waiting for the previous FMA to complete.
 
	The group designed and simulated the parallel-mixed precision dot-product unit and found the computation time was nearly cut in half and the output accuracy improved, especially for dot products of large vectors. They are currently building their new architecture in silicon to prove the simulation’s predictions.
	
 
Back to top
 
	Engineers at the Barcelona Supercomputing Center and Intel claim that “Brain Float 16 is all you need” for deep neural-network training. And no, they are not Beatles fans. They are fans of the number format Brain Float—Google Brain’s brainchild.
 
	Brain Float (BF16) is a pared-down version of the standard 32-bit floating-point representation (FP32). FP32 represents numbers with one sign bit, 8 exponent bits, and 23 mantissa bits. A BF16 number is just an FP32 number with 16 mantissa bits chopped off. This way, BF16 sacrifices precision but takes up half the space in memory and preserves the dynamic range of FP32.
 
	Brain Floats are already in wide use in AI training, including the training of the image creator 
	DALL-E. However, they’ve always been used in combination with higher-precision floating-point numbers, converting back and forth between the two for different parts of the computation. That’s because while some parts of neural-network training don’t suffer much from the reduced precision of BF16, others do. In particular, small-magnitude values can get lost during fused multiply-add computations.
 
	The Barcelona and Intel team has found a way around these issues and showed that BF16 can be used exclusively to train state-of-the-art AI models. What’s more, in simulation, the training used one-third of the chip area and energy FP32 would need.
 
	To perform AI training entirely using BF16 without having to worry about losing small values, the Barcelona team developed both an extended-number format and a bespoke FMA unit. The number format, called BF16-N, combines several BF16 numbers to represent one real number. Far from defeating the point of using fewer-bit formats, combining BF16 allows for much more efficient FMA operations without sacrificing significant precision. The key to that counterintuitive result lies in the silicon area (and therefore power) requirements of FMA units. Those requirements square with the number of mantissa bits. An FMA for FP32 numbers, with their 23 mantissas, would require 576 units of area. But a clever FMA yielding comparable results with BF16-N where N is 2 takes 192 units.
 

Three 16-bit Brain Float numbers can carry the same precision as 32-bit floats while saving chip area during computations. John Osorio/Barcelona Supercomputing Center 
	Eliminating the need for mixed precision and focusing exclusively on BF16 operations opens the possibility for much cheaper hardware, says Marc Casas, senior researcher at the Barcelona Supercomputing Center and leader of this effort. For now, they have emulated their solution only in software. Now they are working on implementing the Brain-Float-only approach on an FPGA to see if the performance improvements hold.
 
Back to top
 Portions of this article already appeared in a previous post.  It’s hard to learn, but your code will produce fewer nasty surprises 
You’d expect the longest and most costly phase in the life cycle of a software product to be the initial development of the system, when all those great features are first imagined and then created. In fact, the hardest part comes later, during the maintenance phase. That’s when programmers pay the price for the shortcuts they took during development.
 
	So why did they take shortcuts? Maybe they didn’t realize that they were cutting any corners. Only when their code was deployed and exercised by a lot of users did its hidden flaws come to light. And maybe the developers were rushed. Time-to-market pressures would almost guarantee that their software will contain more bugs than it would otherwise.
                                                   ","New number formats and basic computations emerge to speed up AI training Recent developments in AI have been astounding, but so are the costs of training neural networks to do their astounding feats. The biggest, such as the language model GPT-3 and the art generator DALL-E 2, take several months to train on a cluster of high-performance GPUs, costing millions of dollars and taking up millions of billions of billions of basic computations. The training capabilities of processing units have been growing rapidly, as much as doubling in the last year. To keep the trend going, researchers are digging down into the most basic building blocks of computation, the way computers represent numbers. “We got a thousand times improvement [in training performance per chip] over the last 10 years, and a lot of it has been due to number representation,” Bill Dally, chief scientist and senior vice president of research at Nvidia said in his keynote talk at the recent IEEE Symposium on Computer Arithmetic. Among the first casualties in the march toward more efficient AI training is the 32-bit floating-point number representation, colloquially known as “standard precision.” Seeking speed, energy efficiency, and better use of chip area and memory, machine-learning researchers have been trying to get the same level of training using numbers represented by fewer bits. The field is still wide open for competitors trying to take the place of the 32-bit format, both in the number representation itself and in the way basic arithmetic is done using those numbers. Here are some recent advances and top contenders unveiled last month at the conference: Image creator DALL-E was trained on clusters of Nvidia’s A100 GPUs with a combination of the standard 32-bit numbers and the lower-precision 16-bit numbers. Nvidia’s recently announced Hopper GPU includes support for even smaller, 8-bit floating-point numbers. Now Nvidia has developed a chip prototype that takes this trend a step further by using a combination of 8-bit and 4-bit numbers. The chip managed to maintain computational accuracy despite using less-precise numbers, at least for one part of the training process, called inferencing. Inference is performed on fully trained models to get an output, but it’s also done repeatedly during training. “We end up getting 8-bit results from 4-bit precision,” Nvidia’s Dally told engineers and computer scientists. Nvidia was able to cut down the size of numbers without significant accuracy loss thanks to a technique the company calls per-vector scaled quantization (VSQ). The basic idea goes something like this: A 4-bit number can represent only 16 values exactly. So, every number gets rounded to one of those 16 values. The loss of accuracy due to this rounding is called quantization error. However, you can add a scale factor to uniformly squeeze the 16 values closer together on the number line or pull them further apart, decreasing or increasing the quantization error. Nvidia's per-vector scaling quantization (VSQ) scheme better represents the numbers needed in machine learning than standard formats such as INT4. Nvidia The trick is to squeeze or expand those 16 values so they optimally match the range of numbers you actually need to represent in a neural network. This scaling is different for different sets of data. By fine-tuning this scaling parameter for every set of 64 numbers inside the neural-network model, Nvidia researchers were able to minimize quantization error. The overhead from calculating the scaling factors is negligible, they found. But the energy efficiency doubled as the 8-bit representation was reduced to 4-bit. The experimental chip is still under development, and Nvidia engineers are working on how to use these principles during the full training process, not just in inferencing. If successful, a chip combining 4-bit computations, VSQ, and other efficiency improvements could achieve 10 times the number of operations per watt as the Hopper GPU, said Dally. Back to top Back in 2017, two researchers, John Gustafson and Isaac Yonemoto, developed an entirely new way of representing numbers. Now, a team of researchers at the Complutense University of Madrid have developed the first processor core implementing the posit standard in hardware and showed that, bit for bit, the accuracy of a basic computational task increased by up to four orders of magnitude, as compared to computing using standard floating-point numbers. The advantage of posits comes from the way the numbers they represent exactly are distributed along the number line. In the middle of the number line, around 1 and –1, there are more posit representations than floating point. And at the wings, going out to large negative and positive numbers, posit accuracy falls off more gracefully than floating point. “It’s a better match for the natural distribution of numbers in a calculation,” says Gustafson. “It’s the right dynamic range, and it’s the right accuracy where you need more accuracy. There’s an awful lot of bit patterns in floating-point arithmetic no one ever uses. And that’s a waste.” Posits accomplish this improved accuracy around 1 and –1 thanks to an extra component in their representation. Floats are made up of three parts: a sign bit (0 for positive, 1 for negative), several “mantissa” (fraction) bits denoting what comes after the binary version of a decimal point, and the remaining bits defining the exponent (2 exp). Posits keep all the components of a float but add an extra “regime” section, an exponent of an exponent. The beauty of the regime is that it can vary in bit length. For small numbers, it can take as few as two bits, leaving more precision for the mantissa. This allows for the higher accuracy of posits in their sweet spot around 1 and –1. Posits differ from floating-point numbers by the addition of an extra variable-length regime section. This gives posits a better accuracy around zero, where the majority of numbers used in neural networks reside. Complutense University of Madrid/IEEE With their new hardware implementation, which was synthesized on a field-programmable gate array (FPGA), the Complutense team was able to compare computations done using 32-bit floats and 32-bit posits side by side. They assessed their accuracy by comparing them to results using the much more accurate but computationally costly 64-bit floating-point format. Posits showed an astounding four-order-of-magnitude improvement in the accuracy of matrix multiplication, a crucial calculation in neural networks. They also found that the improved accuracy didn’t come at the cost of computation time, only a somewhat increased chip area and power consumption. Back to top Computer scientists in Switzerland and Italy have worked out a bit-reducing scheme, meant for processors using open-source RISC-V instruction-set architecture, which is gaining ground among new processor developers. The team’s extension to the RISC-V instruction set includes an efficient version of computations that mix lower- and higher-precision number representations. With their improved mixed-precision math, they obtain a factor-of-two speedup in basic computations involved in training neural networks. Lowering precision has knock-on effects during basic operations that go beyond just the loss of accuracy due to the shorter number. Multiplying two low-precision numbers can result in a number that is too small or too large to represent given the bit length—phenomena called underflow and overflow, respectively. A separate phenomenon, called swamping, happens when you add a large low-precision number and a small low-precision number. The result can be the complete loss of the smaller number. Mixed precision comes to the rescue in mitigating overflow, underflow, and swamping. Computations are performed with low-precision inputs and result in higher-precision outputs, getting a batch of math done before rounding back to lower precision. The Europe-based team focused on a basic component of AI computations: the dot product. It’s usually implemented in hardware with a series of components called fused multiply-add units (FMAs). These perform the operation d = a*b + c in one go, only rounding at the end. To reap the benefits of mixed precision, inputs a and b are low precision (say, 8 bits), while c and the output d are high precision (say, 16 bits). Luca Benini, chair of digital circuits and systems at ETH Zurich and professor at the University of Bologna and his team had a simple insight: Instead of doing one FMA operation at a time, why not do two in parallel and add them together at the end? Not only does this prevent losses due to rounding between the two FMAs, but it also allows for better utilization of memory, because no memory registers are left waiting for the previous FMA to complete. The group designed and simulated the parallel-mixed precision dot-product unit and found the computation time was nearly cut in half and the output accuracy improved, especially for dot products of large vectors. They are currently building their new architecture in silicon to prove the simulation’s predictions. Back to top Engineers at the Barcelona Supercomputing Center and Intel claim that “Brain Float 16 is all you need” for deep neural-network training. And no, they are not Beatles fans. They are fans of the number format Brain Float—Google Brain’s brainchild. Brain Float (BF16) is a pared-down version of the standard 32-bit floating-point representation (FP32). FP32 represents numbers with one sign bit, 8 exponent bits, and 23 mantissa bits. A BF16 number is just an FP32 number with 16 mantissa bits chopped off. This way, BF16 sacrifices precision but takes up half the space in memory and preserves the dynamic range of FP32. Brain Floats are already in wide use in AI training, including the training of the image creator DALL-E. However, they’ve always been used in combination with higher-precision floating-point numbers, converting back and forth between the two for different parts of the computation. That’s because while some parts of neural-network training don’t suffer much from the reduced precision of BF16, others do. In particular, small-magnitude values can get lost during fused multiply-add computations. The Barcelona and Intel team has found a way around these issues and showed that BF16 can be used exclusively to train state-of-the-art AI models. What’s more, in simulation, the training used one-third of the chip area and energy FP32 would need. To perform AI training entirely using BF16 without having to worry about losing small values, the Barcelona team developed both an extended-number format and a bespoke FMA unit. The number format, called BF16-N, combines several BF16 numbers to represent one real number. Far from defeating the point of using fewer-bit formats, combining BF16 allows for much more efficient FMA operations without sacrificing significant precision. The key to that counterintuitive result lies in the silicon area (and therefore power) requirements of FMA units. Those requirements square with the number of mantissa bits. An FMA for FP32 numbers, with their 23 mantissas, would require 576 units of area. But a clever FMA yielding comparable results with BF16-N where N is 2 takes 192 units. Three 16-bit Brain Float numbers can carry the same precision as 32-bit floats while saving chip area during computations. John Osorio/Barcelona Supercomputing Center Eliminating the need for mixed precision and focusing exclusively on BF16 operations opens the possibility for much cheaper hardware, says Marc Casas, senior researcher at the Barcelona Supercomputing Center and leader of this effort. For now, they have emulated their solution only in software. Now they are working on implementing the Brain-Float-only approach on an FPGA to see if the performance improvements hold. Back to top Portions of this article already appeared in a previous post. It’s hard to learn, but your code will produce fewer nasty surprises You’d expect the longest and most costly phase in the life cycle of a software product to be the initial development of the system, when all those great features are first imagined and then created. In fact, the hardest part comes later, during the maintenance phase. That’s when programmers pay the price for the shortcuts they took during development. So why did they take shortcuts? Maybe they didn’t realize that they were cutting any corners. Only when their code was deployed and exercised by a lot of users did its hidden flaws come to light. And maybe the developers were rushed. Time-to-market pressures would almost guarantee that their software will contain more bugs than it would otherwise.","['new', 'number', 'format', 'basic', 'computation', 'emerge', 'speed', 'train', 'recent', 'development', 'ai', 'astounding', 'cost', 'train', 'neural', 'network', 'astounding', 'feat', 'big', 'language', 'model', 'gpt3', 'art', 'generator', 'dalle', 'take', 'several', 'month', 'train', 'cluster', 'highperformance', 'gpus', 'cost', 'million', 'dollar', 'take', 'million', 'billion', 'billion', 'basic', 'computation', 'training', 'capability', 'processing', 'unit', 'grow', 'rapidly', 'much', 'double', 'last', 'year', 'keep', 'trend', 'go', 'researcher', 'dig', 'basic', 'building', 'block', 'computation', 'way', 'computer', 'represent', 'number', 'get', 'time', 'improvement', 'training', 'performance', 'chip', 'last', 'year', 'lot', 'due', 'number', 'representation', 'bill', 'dally', 'chief', 'scientist', 'senior', 'vice', 'president', 'research', 'say', 'keynote', 'talk', 'recent', 'ieee', 'symposium', 'computer', 'arithmetic', 'first', 'casualty', 'march', 'efficient', 'ai', 'training', '32bit', 'floatingpoint', 'number', 'representation', 'colloquially', 'know', 'standard', 'precision', 'seek', 'speed', 'energy', 'efficiency', 'well', 'use', 'chip', 'area', 'memory', 'machinelearne', 'researcher', 'try', 'get', 'level', 'training', 'use', 'number', 'represent', 'bit', 'field', 'still', 'wide', 'open', 'competitor', 'try', 'take', 'place', '32bit', 'format', 'number', 'representation', 'way', 'basic', 'arithmetic', 'use', 'number', 'recent', 'advance', 'top', 'contender', 'unveil', 'last', 'month', 'conference', 'image', 'creator', 'dalle', 'train', 'cluster', 'gpus', 'combination', 'standard', '32bit', 'number', 'lowerprecision', '16bit', 'number', 'recently', 'announce', 'include', 'support', 'even', 'small', 'floatingpoint', 'number', 'develop', 'chip', 'prototype', 'take', 'trend', 'step', 'far', 'use', 'combination', '8bit', '4bit', 'number', 'chip', 'manage', 'maintain', 'computational', 'accuracy', 'use', 'lessprecise', 'number', 'least', 'part', 'training', 'process', 'call', 'inference', 'inference', 'perform', 'fully', 'train', 'model', 'get', 'output', 'also', 'repeatedly', 'training', 'end', 'get', '8bit', 'result', '4bit', 'precision', 'dally', 'tell', 'engineer', 'computer', 'scientist', 'nvidia', 'able', 'cut', 'size', 'number', 'significant', 'accuracy', 'loss', 'thank', 'technique', 'company', 'call', 'pervector', 'scale', 'quantization', 'vsq', 'basic', 'idea', 'go', '4bit', 'number', 'represent', 'value', 'exactly', 'number', 'round', 'value', 'loss', 'accuracy', 'rounding', 'call', 'quantization', 'error', 'however', 'add', 'scale', 'factor', 'uniformly', 'squeeze', 'value', 'close', 'together', 'number', 'line', 'pull', 'far', 'decrease', 'increase', 'quantization', 'error', 'nvidias', 'pervector', 'scale', 'quantization', 'vsq', 'scheme', 'well', 'represent', 'number', 'need', 'machine', 'learning', 'standard', 'format', 'trick', 'squeeze', 'expand', 'value', 'optimally', 'match', 'range', 'number', 'actually', 'need', 'represent', 'neural', 'network', 'scaling', 'different', 'different', 'set', 'datum', 'finetune', 'scaling', 'parameter', 'set', 'number', 'neuralnetwork', 'model', 'researcher', 'able', 'minimize', 'quantization', 'error', 'overhead', 'calculate', 'scaling', 'factor', 'negligible', 'find', 'energy', 'efficiency', 'double', 'representation', 'reduce', '4bit', 'experimental', 'chip', 'still', 'development', 'engineer', 'work', 'use', 'principle', 'full', 'training', 'process', 'inference', 'successful', 'chip', 'combine', '4bit', 'computation', 'vsq', 'efficiency', 'improvement', 'achieve', 'time', 'number', 'operation', 'watt', 'say', 'dally', 'back', 'top', 'back', 'researcher', 'develop', 'entirely', 'new', 'way', 'represent', 'number', 'team', 'researcher', 'complutense', 'develop', 'first', 'processor', 'core', 'implement', 'posit', 'standard', 'hardware', 'show', 'bit', 'bit', 'accuracy', 'basic', 'computational', 'task', 'increase', 'order', 'magnitude', 'compare', 'compute', 'use', 'standard', 'floatingpoint', 'number', 'advantage', 'posit', 'come', 'way', 'number', 'represent', 'exactly', 'distribute', 'number', 'line', 'middle', 'number', 'line', 'posit', 'representation', 'float', 'point', 'wing', 'go', 'large', 'negative', 'positive', 'number', 'posit', 'accuracy', 'fall', 'gracefully', 'float', 'point', '’', 'well', 'match', 'natural', 'distribution', 'number', 'calculation', 'say', '’', 'right', 'dynamic', 'range', '’', 'right', 'accuracy', 'need', 'accuracy', '’', 'awful', 'lot', 'bit', 'pattern', 'floatingpoint', 'arithmetic', 'one', 'ever', 'use', '’', 'waste', 'posit', 'accomplish', 'improved', 'accuracy', 'thank', 'extra', 'component', 'representation', 'float', 'make', 'part', 'sign', 'bit', 'positive', 'negative', 'several', 'mantissa', 'fraction', 'bit', 'denote', 'come', 'binary', 'version', 'decimal', 'point', 'remain', 'bit', 'define', 'exponent', 'posit', 'keep', 'component', 'float', 'add', 'extra', 'regime', 'section', 'exponent', 'exponent', 'beauty', 'regime', 'vary', 'bit', 'length', 'small', 'number', 'take', 'bit', 'leave', 'precision', 'mantissa', 'allow', 'high', 'accuracy', 'posit', 'sweet', 'spot', 'posit', 'differ', 'floatingpoint', 'number', 'addition', 'extra', 'variablelength', 'regime', 'section', 'give', 'posit', 'well', 'accuracy', 'majority', 'number', 'use', 'neural', 'network', 'reside', 'complutense', 'madridieee', 'new', 'hardware', 'implementation', 'synthesize', 'fieldprogrammable', 'gate', 'array', 'fpga', 'complutense', 'team', 'able', 'compare', 'computation', 'use', '32bit', 'float', '32bit', 'posit', 'side', 'side', 'assess', 'accuracy', 'compare', 'result', 'use', 'much', 'accurate', 'computationally', 'costly', '64bit', 'floatingpoint', 'format', 'posit', 'show', 'astounding', 'fourorderofmagnitude', 'improvement', 'accuracy', 'matrix', 'multiplication', 'crucial', 'calculation', 'neural', 'network', 'also', 'find', 'improved', 'accuracy', 'come', 'cost', 'computation', 'time', 'somewhat', 'increase', 'chip', 'area', 'power', 'consumption', 'back', 'top', 'computer', 'scientist', 'work', 'bitreducing', 'scheme', 'mean', 'processor', 'use', 'opensource', 'riscv', 'instructionset', 'architecture', 'gain', 'ground', 'new', 'processor', 'developer', 'team', 'extension', 'riscv', 'instruction', 'set', 'include', 'efficient', 'version', 'computation', 'mix', 'low', 'higherprecision', 'number', 'representation', 'improved', 'mixedprecision', 'math', 'obtain', 'factoroftwo', 'speedup', 'basic', 'computation', 'involve', 'train', 'neural', 'network', 'lower', 'precision', 'knockon', 'effect', 'basic', 'operation', 'go', 'loss', 'accuracy', 'short', 'number', 'multiply', 'lowprecision', 'number', 'result', 'number', 'small', 'large', 'represent', 'give', 'bit', 'length', 'phenomenon', 'call', 'underflow', 'overflow', 'respectively', 'separate', 'phenomenon', 'call', 'swamping', 'happen', 'add', 'large', 'lowprecision', 'number', 'small', 'lowprecision', 'number', 'result', 'complete', 'loss', 'small', 'number', 'mixed', 'precision', 'come', 'rescue', 'mitigate', 'overflow', 'underflow', 'swamp', 'computation', 'perform', 'lowprecision', 'input', 'result', 'higherprecision', 'output', 'get', 'batch', 'math', 'round', 'back', 'low', 'precision', 'europebased', 'team', 'focus', 'basic', 'component', 'computation', 'dot', 'product', 'usually', 'implement', 'hardware', 'series', 'component', 'call', 'fuse', 'multiplyadd', 'unit', 'fma', 'perform', 'operation', 'go', 'round', 'end', 'reap', 'benefit', 'mixed', 'precision', 'input', 'b', 'low', 'precision', 'say', 'bit', 'output', 'high', 'precision', 'say', 'bit', 'benini', 'chair', 'digital', 'circuit', 'system', 'professor', 'team', 'simple', 'insight', 'instead', 'fma', 'operation', 'time', 'parallel', 'add', 'together', 'end', 'prevent', 'loss', 'due', 'round', 'fma', 'also', 'allow', 'well', 'utilization', 'memory', 'memory', 'register', 'leave', 'wait', 'previous', 'fma', 'complete', 'group', 'design', 'simulate', 'parallelmixed', 'precision', 'dotproduct', 'unit', 'find', 'computation', 'time', 'nearly', 'cut', 'half', 'output', 'accuracy', 'improve', 'especially', 'dot', 'product', 'large', 'vector', 'currently', 'build', 'new', 'architecture', 'silicon', 'prove', 'simulation', 'prediction', 'back', 'top', 'engineer', 'claim', 'brain', 'float', 'need', 'deep', 'neuralnetwork', 'training', 'beatle', 'fan', 'fan', 'number', 'format', 'brain', 'float', 'brainchild', 'brain', 'float', 'pareddown', 'version', 'standard', '32bit', 'floatingpoint', 'representation', 'represent', 'number', 'sign', 'bit', 'exponent', 'bit', 'mantissa', 'bit', 'number', 'number', 'mantissa', 'bit', 'chop', 'way', 'sacrifice', 'precision', 'take', 'space', 'memory', 'preserve', 'dynamic', 'range', 'brain', 'float', 'already', 'wide', 'use', 'training', 'include', 'training', 'image', 'creator', 'dalle', 'however', 'always', 'use', 'combination', 'higherprecision', 'floatingpoint', 'number', 'convert', 'back', 'forth', 'different', 'part', 'computation', '’', 'part', 'neuralnetwork', 'training', 'suffer', 'much', 'reduce', 'precision', 'particular', 'smallmagnitude', 'value', 'lose', 'fuse', 'multiplyadd', 'computation', 'intel', 'team', 'find', 'way', 'issue', 'show', 'use', 'exclusively', 'train', 'stateoftheart', 'model', '’', 'simulation', 'training', 'use', 'onethird', 'chip', 'area', 'energy', 'need', 'perform', 'train', 'entirely', 'use', 'worry', 'lose', 'small', 'value', 'barcelona', 'team', 'develop', 'extendednumber', 'format', 'bespoke', 'fma', 'unit', 'number', 'format', 'call', 'bf16n', 'combine', 'several', 'number', 'represent', 'real', 'number', 'far', 'defeat', 'point', 'use', 'fewerbit', 'format', 'combine', 'allow', 'much', 'efficient', 'fma', 'operation', 'sacrifice', 'significant', 'precision', 'key', 'counterintuitive', 'result', 'lie', 'silicon', 'area', 'therefore', 'power', 'requirement', 'fma', 'unit', 'requirement', 'square', 'number', 'mantissa', 'bit', 'fma', 'number', 'mantissa', 'require', 'unit', 'area', 'clever', 'fma', 'yield', 'comparable', 'result', 'bf16n', 'take', 'unit', '16bit', 'brain', 'float', 'number', 'carry', 'precision', '32bit', 'float', 'save', 'chip', 'area', 'eliminate', 'need', 'mixed', 'precision', 'focus', 'exclusively', 'operation', 'open', 'possibility', 'much', 'cheap', 'hardware', 'say', 'senior', 'researcher', 'center', 'leader', 'effort', 'emulate', 'solution', 'software', 'work', 'implement', 'brainfloatonly', 'approach', 'fpga', 'see', 'performance', 'improvement', 'hold', 'top', 'portion', 'article', 'already', 'appear', 'previous', 'post', '’', 'hard', 'learn', 'code', 'produce', 'nasty', 'surprise', '’d', 'expect', 'long', 'costly', 'phase', 'life', 'cycle', 'software', 'product', 'initial', 'development', 'system', 'great', 'feature', 'first', 'imagine', 'create', 'fact', 'hard', 'part', 'come', 'later', 'maintenance', 'phase', '’', 'programmer', 'pay', 'price', 'shortcut', 'take', 'development', 'take', 'shortcut', 'maybe', 'realize', 'cut', 'corner', 'code', 'deploy', 'exercise', 'lot', 'user', 'hide', 'flaw', 'come', 'light', 'maybe', 'developer', 'rush', 'timetomarket', 'pressure', 'almost', 'guarantee', 'software', 'contain', 'bug', 'otherwise']"
"
        6 Reactions to the White House’s AI Bill of Rights
    ",https://spectrum.ieee.org/white-house-ai,2022-10-14,"The nonbinding principles are being both celebrated and vilified  Last week, the White House put forth its Blueprint for an AI Bill of Rights. It’s not what you might think—it doesn’t give artificial-intelligence systems the right to free speech (thank goodness) or to carry arms (double thank goodness), nor does it bestow any other rights upon AI entities.  Instead, it’s a nonbinding framework for the rights that we old-fashioned human beings should have in relationship to AI systems. The White House’s move is part of a global push to establish regulations to govern AI. Automated decision-making systems are playing increasingly large roles in such fraught areas as screening job applicants, approving people for government benefits, and determining medical treatments, and harmful biases in these systems can lead to unfair and discriminatory outcomes.  The United States is not the first mover in this space. The European Union has been very active in proposing and honing regulations, with its massive AI Act grinding slowly through the necessary committees. And just a few weeks ago, the European Commission adopted a separate proposal on AI liability that would make it easier for “victims of AI-related damage to get compensation.” China also has several initiatives relating to AI governance, though the rules issued apply only to industry, not to government entities.  “Although this blueprint does not have the force of law, the choice of language and framing clearly positions it as a framework for understanding AI governance broadly as a civil-rights issue, one that deserves new and expanded protections under American law.”—Janet Haven, Data & Society Research Institute  But back to the Blueprint. The White House Office of Science and Technology Policy (OSTP) first proposed such a bill of rights a year ago, and has been taking comments and refining the idea ever since. Its five pillars are:  
	For more context on this big move from the White House, IEEE Spectrum rounded up six reactions to the AI Bill of Rights from experts on AI policy.  The Center for Security and Emerging Technology, at Georgetown University, notes in its AI policy newsletter that the blueprint is accompanied by 
	a “technical companion” that offers specific steps that industry, communities, and governments can take to put these principles into action. Which is nice, as far as it goes: 
	 But, as the document acknowledges, the blueprint is a non-binding white paper and does not affect any existing policies, their interpretation, or their implementation. When 
	OSTP officials announced plans to develop a “bill of rights for an AI-powered world” last year, they said enforcement options could include restrictions on federal and contractor use of noncompliant technologies and other “laws and regulations to fill gaps.” Whether the White House plans to pursue those options is unclear, but affixing “Blueprint” to the “AI Bill of Rights” seems to indicate a narrowing of ambition from the original proposal.
 “Americans do not need a new set of laws, regulations, or guidelines focused exclusively on protecting their civil liberties from algorithms.... Existing laws that protect Americans from discrimination and unlawful surveillance apply equally to digital and non-digital risks.”—Daniel Castro, Center for Data Innovation 
Janet Haven, executive director of the Data & Society Research Institute, stresses in a Medium post that the blueprint breaks ground by framing AI regulations as a civil-rights issue: 
	The Blueprint for an AI Bill of Rights is as advertised: it’s an outline, articulating a set of principles and their potential applications for approaching the challenge of governing AI through a rights-based framework. This differs from many other approaches to AI governance that use a lens of trust, safety, ethics, responsibility, or other more interpretive frameworks. A rights-based approach is rooted in deeply held American values—equity, opportunity, and self-determination—and longstanding law.... While American law and policy have historically focused on protections for individuals, largely ignoring group harms, the blueprint’s authors note that the “magnitude of the impacts of data-driven automated systems may be most readily visible at the community level.” The blueprint asserts that communities—defined in broad and inclusive terms, from neighborhoods to social networks to Indigenous groups—have the right to protection and redress against harms to the same extent that individuals do.
 
	The blueprint breaks further ground by making that claim through the lens of algorithmic discrimination, and a call, in the language of American civil-rights law, for “freedom from” this new type of attack on fundamental American rights. 
	Although this blueprint does not have the force of law, the choice of language and framing clearly positions it as a framework for understanding AI governance broadly as a civil-rights issue, one that deserves new and expanded protections under American law.
  At the Center for Data Innovation, director Daniel Castro issued a press release with a very different take. He worries about the impact that potential new regulations would have on industry: 
	The AI Bill of Rights is an insult to both AI and the Bill of Rights. Americans do not need a new set of laws, regulations, or guidelines focused exclusively on protecting their civil liberties from algorithms. Using AI does not give businesses a “get out of jail free” card. Existing laws that protect Americans from discrimination and unlawful surveillance apply equally to digital and non-digital risks. Indeed, the Fourth Amendment serves as an enduring guarantee of Americans’ constitutional protection from unreasonable intrusion by the government.
 
	Unfortunately, the AI Bill of Rights vilifies digital technologies like AI as “among the great challenges posed to democracy.” Not only do these claims vastly overstate the potential risks, but they also make it harder for the United States to compete against China in the global race for AI advantage. What recent college graduates would want to pursue a career building technology that the highest officials in the nation have labeled dangerous, biased, and ineffective? 
 “What I would like to see in addition to the Bill of Rights are executive actions and more congressional hearings and legislation to address the rapidly escalating challenges of AI as identified in the Bill of Rights.”—Russell Wald, Stanford Institute for Human-Centered Artificial Intelligence  The executive director of the Surveillance Technology Oversight Project (S.T.O.P.), Albert Fox Cahn, doesn’t like the blueprint either, but for opposite reasons. S.T.O.P.’s press release says the organization wants new regulations and wants them right now: 
	Developed by the White House Office of Science and Technology Policy (OSTP), the blueprint proposes that all AI will be built with consideration for the preservation of civil rights and democratic values, but endorses use of artificial intelligence for law-enforcement surveillance. The civil-rights group expressed concern that the blueprint normalizes biased surveillance and will accelerate algorithmic discrimination.
 
“We don’t need a blueprint, we need bans,” 
	said Surveillance Technology Oversight Project executive director Albert Fox Cahn. “When police and companies are rolling out new and destructive forms of AI every day, we need to push pause across the board on the most invasive technologies. While the White House does take aim at some of the worst offenders, they do far too little to address the everyday threats of AI, particularly in police hands.”
 Another very active AI oversight organization, the Algorithmic Justice League, takes a more positive view in a Twitter thread: 
	Today's #WhiteHouse announcement of the Blueprint for an AI Bill of Rights from the @WHOSTP is an encouraging step in the right direction in the fight toward algorithmic justice.... As we saw in the Emmy-nominated documentary “@CodedBias,” algorithmic discrimination further exacerbates consequences for the excoded, those who experience #AlgorithmicHarms. No one is immune from being excoded. All people need to be clear of their rights against such technology. This announcement is a step that many community members and civil-society organizations have been pushing for over the past several years. Although this Blueprint does not give us everything we have been advocating for, it is a road map that should be leveraged for greater consent and equity. Crucially, it also provides a directive and obligation to reverse course when necessary in order to prevent AI harms. Finally, Spectrum reached out to Russell Wald, director of policy for the Stanford Institute for Human-Centered Artificial Intelligence for his perspective. Turns out, he’s a little frustrated: While the Blueprint for an AI Bill of Rights is helpful in highlighting real-world harms automated systems can cause, and how specific communities are disproportionately affected, it lacks teeth or any details on enforcement. The document specifically states it is “non-binding and does not constitute U.S. government policy.” If the U.S. government has identified legitimate problems, what are they doing to correct it? From what I can tell, not enough.  One unique challenge when it comes to AI policy is when the aspiration doesn’t fall in line with the practical. For example, the Bill of Rights states, “You should be able to opt out, where appropriate, and have access to a person who can quickly consider and remedy problems you encounter.” When the Department of Veterans Affairs can take up to three to five years to adjudicate a claim for veteran benefits, are you really giving people an opportunity to opt out if a robust and responsible automated system can give them an answer in a couple of months? What I would like to see in addition to the Bill of Rights are executive actions and more congressional hearings and legislation to address the rapidly escalating challenges of AI as identified in the Bill of Rights. It’s worth noting that there have been legislative efforts on the federal level: most notably, the 2022 Algorithmic Accountability Act, which was introduced in Congress last February. It proceeded to go nowhere.   Eliza Strickland is a senior editor at IEEE Spectrum, where she covers AI, biomedical engineering, and other topics. She holds a master’s degree in journalism from Columbia University. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia  cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving.  
Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines.
 
	The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, 
	Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.
                                             ","The nonbinding principles are being both celebrated and vilified Last week, the White House put forth its Blueprint for an AI Bill of Rights. It’s not what you might think—it doesn’t give artificial-intelligence systems the right to free speech (thank goodness) or to carry arms (double thank goodness), nor does it bestow any other rights upon AI entities. Instead, it’s a nonbinding framework for the rights that we old-fashioned human beings should have in relationship to AI systems. The White House’s move is part of a global push to establish regulations to govern AI. Automated decision-making systems are playing increasingly large roles in such fraught areas as screening job applicants, approving people for government benefits, and determining medical treatments, and harmful biases in these systems can lead to unfair and discriminatory outcomes. The United States is not the first mover in this space. The European Union has been very active in proposing and honing regulations, with its massive AI Act grinding slowly through the necessary committees. And just a few weeks ago, the European Commission adopted a separate proposal on AI liability that would make it easier for “victims of AI-related damage to get compensation.” China also has several initiatives relating to AI governance, though the rules issued apply only to industry, not to government entities. “Although this blueprint does not have the force of law, the choice of language and framing clearly positions it as a framework for understanding AI governance broadly as a civil-rights issue, one that deserves new and expanded protections under American law.”—Janet Haven, Data & Society Research Institute But back to the Blueprint. The White House Office of Science and Technology Policy (OSTP) first proposed such a bill of rights a year ago, and has been taking comments and refining the idea ever since. Its five pillars are: For more context on this big move from the White House, IEEE Spectrum rounded up six reactions to the AI Bill of Rights from experts on AI policy. The Center for Security and Emerging Technology, at Georgetown University, notes in its AI policy newsletter that the blueprint is accompanied by a “technical companion” that offers specific steps that industry, communities, and governments can take to put these principles into action. Which is nice, as far as it goes: But, as the document acknowledges, the blueprint is a non-binding white paper and does not affect any existing policies, their interpretation, or their implementation. When OSTP officials announced plans to develop a “bill of rights for an AI-powered world” last year, they said enforcement options could include restrictions on federal and contractor use of noncompliant technologies and other “laws and regulations to fill gaps.” Whether the White House plans to pursue those options is unclear, but affixing “Blueprint” to the “AI Bill of Rights” seems to indicate a narrowing of ambition from the original proposal. “Americans do not need a new set of laws, regulations, or guidelines focused exclusively on protecting their civil liberties from algorithms.... Existing laws that protect Americans from discrimination and unlawful surveillance apply equally to digital and non-digital risks.”—Daniel Castro, Center for Data Innovation Janet Haven, executive director of the Data & Society Research Institute, stresses in a Medium post that the blueprint breaks ground by framing AI regulations as a civil-rights issue: The Blueprint for an AI Bill of Rights is as advertised: it’s an outline, articulating a set of principles and their potential applications for approaching the challenge of governing AI through a rights-based framework. This differs from many other approaches to AI governance that use a lens of trust, safety, ethics, responsibility, or other more interpretive frameworks. A rights-based approach is rooted in deeply held American values—equity, opportunity, and self-determination—and longstanding law.... While American law and policy have historically focused on protections for individuals, largely ignoring group harms, the blueprint’s authors note that the “magnitude of the impacts of data-driven automated systems may be most readily visible at the community level.” The blueprint asserts that communities—defined in broad and inclusive terms, from neighborhoods to social networks to Indigenous groups—have the right to protection and redress against harms to the same extent that individuals do. The blueprint breaks further ground by making that claim through the lens of algorithmic discrimination, and a call, in the language of American civil-rights law, for “freedom from” this new type of attack on fundamental American rights. Although this blueprint does not have the force of law, the choice of language and framing clearly positions it as a framework for understanding AI governance broadly as a civil-rights issue, one that deserves new and expanded protections under American law. At the Center for Data Innovation, director Daniel Castro issued a press release with a very different take. He worries about the impact that potential new regulations would have on industry: The AI Bill of Rights is an insult to both AI and the Bill of Rights. Americans do not need a new set of laws, regulations, or guidelines focused exclusively on protecting their civil liberties from algorithms. Using AI does not give businesses a “get out of jail free” card. Existing laws that protect Americans from discrimination and unlawful surveillance apply equally to digital and non-digital risks. Indeed, the Fourth Amendment serves as an enduring guarantee of Americans’ constitutional protection from unreasonable intrusion by the government. Unfortunately, the AI Bill of Rights vilifies digital technologies like AI as “among the great challenges posed to democracy.” Not only do these claims vastly overstate the potential risks, but they also make it harder for the United States to compete against China in the global race for AI advantage. What recent college graduates would want to pursue a career building technology that the highest officials in the nation have labeled dangerous, biased, and ineffective? “What I would like to see in addition to the Bill of Rights are executive actions and more congressional hearings and legislation to address the rapidly escalating challenges of AI as identified in the Bill of Rights.”—Russell Wald, Stanford Institute for Human-Centered Artificial Intelligence The executive director of the Surveillance Technology Oversight Project (S.T.O.P.), Albert Fox Cahn, doesn’t like the blueprint either, but for opposite reasons. S.T.O.P.’s press release says the organization wants new regulations and wants them right now: Developed by the White House Office of Science and Technology Policy (OSTP), the blueprint proposes that all AI will be built with consideration for the preservation of civil rights and democratic values, but endorses use of artificial intelligence for law-enforcement surveillance. The civil-rights group expressed concern that the blueprint normalizes biased surveillance and will accelerate algorithmic discrimination. “We don’t need a blueprint, we need bans,” said Surveillance Technology Oversight Project executive director Albert Fox Cahn. “When police and companies are rolling out new and destructive forms of AI every day, we need to push pause across the board on the most invasive technologies. While the White House does take aim at some of the worst offenders, they do far too little to address the everyday threats of AI, particularly in police hands.” Another very active AI oversight organization, the Algorithmic Justice League, takes a more positive view in a Twitter thread: Today's #WhiteHouse announcement of the Blueprint for an AI Bill of Rights from the @WHOSTP is an encouraging step in the right direction in the fight toward algorithmic justice.... As we saw in the Emmy-nominated documentary “@CodedBias,” algorithmic discrimination further exacerbates consequences for the excoded, those who experience #AlgorithmicHarms. No one is immune from being excoded. All people need to be clear of their rights against such technology. This announcement is a step that many community members and civil-society organizations have been pushing for over the past several years. Although this Blueprint does not give us everything we have been advocating for, it is a road map that should be leveraged for greater consent and equity. Crucially, it also provides a directive and obligation to reverse course when necessary in order to prevent AI harms. Finally, Spectrum reached out to Russell Wald, director of policy for the Stanford Institute for Human-Centered Artificial Intelligence for his perspective. Turns out, he’s a little frustrated: While the Blueprint for an AI Bill of Rights is helpful in highlighting real-world harms automated systems can cause, and how specific communities are disproportionately affected, it lacks teeth or any details on enforcement. The document specifically states it is “non-binding and does not constitute U.S. government policy.” If the U.S. government has identified legitimate problems, what are they doing to correct it? From what I can tell, not enough. One unique challenge when it comes to AI policy is when the aspiration doesn’t fall in line with the practical. For example, the Bill of Rights states, “You should be able to opt out, where appropriate, and have access to a person who can quickly consider and remedy problems you encounter.” When the Department of Veterans Affairs can take up to three to five years to adjudicate a claim for veteran benefits, are you really giving people an opportunity to opt out if a robust and responsible automated system can give them an answer in a couple of months? What I would like to see in addition to the Bill of Rights are executive actions and more congressional hearings and legislation to address the rapidly escalating challenges of AI as identified in the Bill of Rights. It’s worth noting that there have been legislative efforts on the federal level: most notably, the 2022 Algorithmic Accountability Act, which was introduced in Congress last February. It proceeded to go nowhere. Eliza Strickland is a senior editor at IEEE Spectrum, where she covers AI, biomedical engineering, and other topics. She holds a master’s degree in journalism from Columbia University. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.","['nonbinde', 'principle', 'celebrate', 'vilify', 'last', 'week', 'put', 'blueprint', 'ai', 'bill', 'right', '’', 'think', 'give', 'artificialintelligence', 'system', 'right', 'free', 'speech', 'thank', 'goodness', 'carry', 'arm', 'double', 'thank', 'goodness', 'bestow', 'right', 'entity', 'instead', '’', 'nonbinding', 'framework', 'right', 'oldfashione', 'human', 'relationship', 'ai', 'system', 'move', 'part', 'global', 'push', 'establish', 'regulation', 'govern', 'ai', 'automate', 'decisionmake', 'system', 'play', 'increasingly', 'large', 'role', 'fraught', 'area', 'screen', 'job', 'applicant', 'approve', 'people', 'government', 'benefit', 'determine', 'medical', 'treatment', 'harmful', 'bias', 'system', 'lead', 'unfair', 'discriminatory', 'outcome', 'first', 'mover', 'space', 'active', 'propose', 'honing', 'regulation', 'massive', 'ai', 'act', 'grind', 'slowly', 'necessary', 'committee', 'week', 'ago', 'adopt', 'separate', 'proposal', 'liability', 'make', 'easy', 'victim', 'airelate', 'damage', 'get', 'compensation', 'also', 'several', 'initiative', 'relate', 'governance', 'rule', 'issue', 'apply', 'industry', 'government', 'entity', 'blueprint', 'force', 'law', 'choice', 'language', 'frame', 'clearly', 'position', 'framework', 'understanding', 'broadly', 'civilright', 'issue', 'deserve', 'new', 'expand', 'protection', 'american', 'law', 'back', 'blueprint', 'science', 'technology', 'policy', 'ostp', 'first', 'propose', 'bill', 'right', 'year', 'ago', 'take', 'comment', 'refine', 'idea', 'ever', 'pillar', 'context', 'big', 'move', 'ieee', 'spectrum', 'round', 'reaction', 'bill', 'right', 'expert', 'policy', 'center', 'security', 'emerge', 'technology', 'note', 'ai', 'policy', 'newsletter', 'blueprint', 'accompany', 'technical', 'companion', 'offer', 'specific', 'step', 'industry', 'community', 'government', 'take', 'put', 'principle', 'action', 'nice', 'far', 'go', 'document', 'acknowledge', 'blueprint', 'nonbinde', 'white', 'paper', 'affect', 'exist', 'policy', 'interpretation', 'implementation', 'ostp', 'official', 'announce', 'plan', 'develop', 'bill', 'right', 'aipowered', 'world', 'last', 'year', 'say', 'enforcement', 'option', 'include', 'restriction', 'federal', 'contractor', 'use', 'noncompliant', 'technology', 'law', 'regulation', 'fill', 'gap', 'plan', 'pursue', 'option', 'unclear', 'affix', 'blueprint', 'bill', 'right', 'seem', 'indicate', 'narrowing', 'ambition', 'original', 'proposal', 'need', 'new', 'set', 'law', 'regulation', 'guideline', 'focus', 'exclusively', 'protect', 'civil', 'liberty', 'algorithm', 'exist', 'law', 'protect', 'american', 'discrimination', 'unlawful', 'surveillance', 'apply', 'equally', 'digital', 'nondigital', 'risk', 'data', 'innovation', 'executive', 'director', 'datum', 'stress', 'medium', 'post', 'blueprint', 'break', 'grind', 'frame', 'regulation', 'civilright', 'issue', 'blueprint', 'ai', 'bill', 'right', 'advertise', '’', 'outline', 'articulate', 'set', 'principle', 'potential', 'application', 'approach', 'challenge', 'govern', 'ai', 'rightsbased', 'framework', 'differ', 'many', 'approach', 'governance', 'use', 'lens', 'trust', 'safety', 'ethic', 'responsibility', 'interpretive', 'framework', 'rightsbased', 'approach', 'root', 'deeply', 'hold', 'american', 'value', 'equity', 'opportunity', 'selfdetermination', 'longstanding', 'law', 'american', 'policy', 'historically', 'focus', 'protection', 'individual', 'largely', 'ignore', 'group', 'harm', 'author', 'note', 'magnitude', 'impact', 'datadriven', 'automate', 'system', 'readily', 'visible', 'community', 'level', 'blueprint', 'assert', 'community', 'define', 'broad', 'inclusive', 'term', 'neighborhood', 'social', 'network', 'indigenous', 'group', 'right', 'protection', 'redress', 'harm', 'extent', 'individual', 'blueprint', 'break', 'far', 'ground', 'make', 'claim', 'lens', 'algorithmic', 'discrimination', 'call', 'language', 'american', 'civilright', 'law', 'freedom', 'new', 'type', 'attack', 'fundamental', 'american', 'right', 'blueprint', 'force', 'law', 'choice', 'language', 'frame', 'clearly', 'position', 'framework', 'understanding', 'broadly', 'civilright', 'issue', 'deserve', 'new', 'expand', 'protection', 'american', 'law', 'center', 'datum', 'innovation', 'director', 'issue', 'press', 'release', 'different', 'take', 'worry', 'impact', 'potential', 'new', 'regulation', 'industry', 'bill', 'right', 'insult', 'bill', 'right', 'need', 'new', 'set', 'law', 'regulation', 'guideline', 'focus', 'exclusively', 'protect', 'civil', 'liberty', 'algorithm', 'use', 'give', 'business', 'get', 'jail', 'free', 'card', 'exist', 'law', 'protect', 'american', 'discrimination', 'unlawful', 'surveillance', 'apply', 'equally', 'digital', 'nondigital', 'risk', 'indeed', 'fourth', 'amendment', 'serve', 'endure', 'guarantee', 'constitutional', 'protection', 'unreasonable', 'intrusion', 'government', 'unfortunately', 'ai', 'bill', 'vilify', 'digital', 'technology', 'great', 'challenge', 'pose', 'democracy', 'claim', 'vastly', 'overstate', 'potential', 'risk', 'also', 'make', 'hard', 'compete', 'global', 'race', 'advantage', 'recent', 'college', 'graduate', 'want', 'pursue', 'career', 'building', 'technology', 'high', 'official', 'nation', 'label', 'dangerous', 'biased', 'ineffective', 'like', 'see', 'addition', 'bill', 'right', 'executive', 'action', 'congressional', 'hearing', 'legislation', 'address', 'rapidly', 'escalate', 'challenge', 'identify', 'bill', 'right', 'humancentered', 'artificial', 'intelligence', 'executive', 'director', 'surveillance', 'technology', 'oversight', 'project', 'stop', 'like', 'blueprint', 'opposite', 'reason', 'stop', 'press', 'release', 'say', 'organization', 'want', 'new', 'regulation', 'want', 'right', 'develop', 'science', 'technology', 'policy', 'ostp', 'blueprint', 'propose', 'ai', 'build', 'consideration', 'preservation', 'civil', 'right', 'democratic', 'value', 'endorse', 'use', 'artificial', 'intelligence', 'lawenforcement', 'surveillance', 'civilright', 'group', 'express', 'concern', 'blueprint', 'normalize', 'biased', 'surveillance', 'accelerate', 'algorithmic', 'discrimination', 'need', 'blueprint', 'need', 'ban', 'say', 'surveillance', 'technology', 'oversight', 'executive', 'director', 'police', 'company', 'roll', 'new', 'destructive', 'form', 'day', 'need', 'push', 'pause', 'board', 'invasive', 'technology', 'take', 'aim', 'bad', 'offender', 'far', 'little', 'address', 'everyday', 'threat', 'ai', 'particularly', 'police', 'hand', 'active', 'ai', 'oversight', 'organization', 'justice', 'league', 'take', 'positive', 'view', 'twitter', 'thread', 'today', 'whitehouse', 'announcement', 'blueprint', 'ai', 'bill', 'right', 'whostp', 'encouraging', 'step', 'right', 'direction', 'fight', 'algorithmic', 'justice', 'see', 'emmynominated', 'documentary', 'codedbia', 'algorithmic', 'discrimination', 'far', 'exacerbate', 'consequence', 'excode', 'experience', 'algorithmicharm', 'one', 'immune', 'excode', 'people', 'need', 'clear', 'right', 'technology', 'announcement', 'step', 'many', 'community', 'member', 'civilsociety', 'organization', 'push', 'past', 'several', 'year', 'blueprint', 'give', 'advocate', 'road', 'map', 'leverage', 'great', 'consent', 'equity', 'crucially', 'also', 'provide', 'directive', 'obligation', 'reverse', 'course', 'necessary', 'order', 'prevent', 'ai', 'harm', 'finally', 'spectrum', 'reach', 'russell', 'wald', 'director', 'policy', 'humancentered', 'artificial', 'intelligence', 'perspective', 'turn', '’', 'little', 'frustrated', 'blueprint', 'ai', 'bill', 'right', 'helpful', 'highlight', 'harm', 'automate', 'system', 'cause', 'specific', 'community', 'disproportionately', 'affect', 'lack', 'tooth', 'detail', 'enforcement', 'document', 'specifically', 'state', 'nonbinde', 'constitute', 'government', 'policy', 'government', 'identify', 'legitimate', 'problem', 'correct', 'tell', 'enough', 'unique', 'challenge', 'come', 'policy', 'aspiration', 'fall', 'line', 'practical', 'example', 'bill', 'right', 'state', 'able', 'opt', 'appropriate', 'access', 'person', 'quickly', 'consider', 'remedy', 'problem', 'encounter', 'department', 'veteran', 'affair', 'take', 'year', 'adjudicate', 'claim', 'veteran', 'benefit', 'really', 'give', 'people', 'opportunity', 'opt', 'robust', 'responsible', 'automate', 'system', 'give', 'answer', 'couple', 'month', 'like', 'see', 'addition', 'bill', 'right', 'executive', 'action', 'congressional', 'hearing', 'legislation', 'address', 'rapidly', 'escalate', 'challenge', 'identify', 'bill', 'right', '’', 'worth', 'note', 'legislative', 'effort', 'federal', 'level', 'notably', 'algorithmic', 'accountability', 'act', 'introduce', 'last', 'proceed', 'go', 'nowhere', 'eliza', 'strickland', 'senior', 'editor', 'ieee', 'spectrum', 'cover', 'biomedical', 'engineering', 'topic', 'hold', 'master', 'degree', 'journalism', 'well', 'detection', 'make', 'ocean', 'transparent', 'perhaps', 'doom', 'mutually', 'assure', 'destruction', 'virginiaclass', 'fast', 'attack', 'submarine', 'cruise', 'back', 'effectively', 'disappear', 'diving', 'submarine', 'value', 'primarily', 'ability', 'hide', 'assurance', 'submarine', 'likely', 'survive', 'first', 'missile', 'strike', 'nuclear', 'war', 'thus', 'able', 'respond', 'launch', 'missile', 'second', 'strike', 'key', 'strategy', 'deterrence', 'know', 'mutually', 'assure', 'destruction', 'new', 'technology', 'render', 'ocean', 'effectively', 'transparent', 'make', 'trivial', 'spot', 'lurk', 'submarine', 'thus', 'undermine', 'peace', 'world', 'nearly', 'century', 'naval', 'engineer', 'strive', 'develop', 'everfaster', 'everquieter', 'submarine', 'work', 'hard', 'advance', 'wide', 'array', 'radar', 'sonar', 'technology', 'design', 'detect', 'target', 'eliminate', 'enemy', 'submarine', 'balance', 'seem', 'turn', 'emergence', 'nuclearpowered', 'submarine', 'early', '1960', 'study', 'center', 'strategic', 'budgetary', 'assessment', 'clark', 'naval', 'specialist', 'note', 'ability', 'boat', 'remain', 'submerge', 'long', 'period', 'time', 'make', 'nearly', 'impossible', 'find', 'radar', 'active', 'sonar', 'even', 'stealthy', 'submarine', 'produce', 'subtle', 'verylowfrequency', 'noise', 'pick', 'far', 'away', 'network', 'acoustic', 'hydrophone', 'array', 'mount', 'seafloor']"
"
        AI Language Models Are Struggling to “Get” Math
    ",https://spectrum.ieee.org/large-language-models-math,2022-10-12,"Should this be telling us something?  If computers are good at anything, they are good at math. So it may come as a surprise that after much struggling, top machine-learning researchers have recently made breakthroughs in teaching computers math. Over the past year, researchers from the University of California, Berkeley, OpenAI, and Google have made progress in teaching basic math concepts to natural language generation models—algorithms such as GPT-2/3 and GPT-Neo. However, until recently, language models regularly failed to solve even simple word problems, such as “Alice has five more balls than Bob, who has two balls after he gives four to Charlie. How many balls does Alice have?” “When we say computers are very good at math, they’re very good at things that are quite specific,” says Guy Gur-Ari, a machine-learning expert at Google. Computers are good at arithmetic—plugging numbers in and calculating is child’s play. But outside of formal structures, computers struggle. “I think there’s this notion that humans doing math have some rigid reasoning system—that there’s a sharp distinction between knowing something and not knowing something.”—Ethan Dyer, Google Solving word problems, or “quantitative reasoning,” is deceptively tricky because it requires a robustness and rigor that many other problems don’t. If any step during the process goes wrong, the answer will be wrong. “When multiplying really large numbers together…they’ll forget to carry somewhere and be off by one,” says Vineet Kosaraju, a machine-learning expert at OpenAI. Other mistakes made by language models are less human, such as misinterpreting 10 as a 1 and a 0, not 10.  “We work on math because we find it independently very interesting,” says Karl Cobbe, a machine-learning expert at OpenAI. But as Gur-Ari puts it, if it’s good at math, “it’s probably also good at solving many other useful problems.” As machine-learning models are trained on larger samples of data, they tend to grow more robust and make fewer mistakes. But scaling up seems to go only so far with quantitative reasoning; researchers realized that the mistakes language models make seemed to require a more targeted approach. Last year, two different teams of researchers, at UC Berkeley and OpenAI, released two data sets, MATH and GSM8K, respectively, which contain thousands of math problems across geometry, algebra, precalculus, and more. “We basically wanted to see if it was a problem with data sets,” says Steven Basart, a researcher at the Center for AI Safety who worked on MATH. Language models were known to be bad at word problems—but how bad were they, and could they be fixed by introducing better formatted, bigger data sets? The MATH group found just how challenging quantitative reasoning is for top-of-the-line language models, which scored less than 7 percent. (A human grad student scored 40 percent, while a math olympiad champ scored 90 percent.) Models attacking GSM8K problems, which had easier grade-school-level problems, reached about 20 percent accuracy. The OpenAI researchers used two main techniques: fine-tuning and verification. In fine-tuning, researchers take a pretrained language model that includes irrelevant information (Wikipedia articles on zambonis, the dictionary entry for “gusto,” and the like) and then show the model, Clockwork Orange–style, only the relevant information (math problems). Verification, on the other hand, is more of a review session. “The model gets to see a lot of examples of its own mistakes, which is really valuable,” Cobbe says. At the time, OpenAI predicted a model would need to be trained on 100 times more data to reach 80 percent accuracy on GSM8K. But in June, Google’s Minerva announced 78 percent accuracy with minimal scaling upwards. “It’s ahead of any of the trends that we were expecting,” Cobbe says. Basart agrees. “That’s shocking. I thought it would take longer,” he says. Minerva uses Google’s own language model, Pathways Language Model (PaLM), which is fine-tuned on scientific papers from the arXiv online preprint server and other sources with formatted math. Two other strategies helped Minerva. In “chain-of-thought prompting,” Minerva was required to break down larger problems into more palatable chunks. The model also used majority voting—instead of being asked for one answer, it was asked to solve the problem 100 times. Of those answers, Minerva picked the most common answer. The gains from these new strategies were enormous. Minerva shot up to 50 percent accuracy on MATH and nearly 80 percent accuracy on GSM8K, as well as the MMLU, a more general set of STEM questions that included chemistry and biology. When Minerva was asked to redo a random sample of slightly tweaked questions, it performed just as well, suggesting that its capabilities were not from mere memorization.  What Minerva knows—or doesn’t know—about math is fuzzier. Unlike proof assistants, which come with built-in structure, Minerva and other language models have no formal structure. They can have strange, messy reasoning and still arrive at the right answer. As numbers grow larger, the language models’ accuracy falters, something that would never happen on a TI-84. “Just how smart is it—or isn’t it?” asks Cobbe. Though models like Minerva might arrive at the same answer as a human, the actual process they’re following could be wildly different. On the other hand, chain-of-thought prompting is familiar to any human student who’s been asked to “show your work.” “I think there’s this notion that humans doing math have some rigid reasoning system—that there’s a sharp distinction between knowing something and not knowing something,” says Ethan Dyer, a machine-learning expert at Google. But humans give inconsistent answers, make errors, and fail to apply core concepts, too. The borders, at this frontier of machine learning, are blurred.  Update 14 Oct. 2022: A previous version of this story extraneously alluded to the DALL-E/DALL-E 2 art-generation AI in the context of large language generation models being taught to handle math word problems. Of course, neither DALL-E nor DALL-E 2 is a large language generation model. (And it was not studied in the math word problem research.) So to avoid confusion, references to it were cut.  Dan Garisto is a freelance science journalist who covers physics and other physical sciences. His work has appeared in Scientific American, Physics, Symmetry, Undark, and other outlets. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia  cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving.  
Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines.
 
	The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, 
	Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.
                                             ","Should this be telling us something? If computers are good at anything, they are good at math. So it may come as a surprise that after much struggling, top machine-learning researchers have recently made breakthroughs in teaching computers math. Over the past year, researchers from the University of California, Berkeley, OpenAI, and Google have made progress in teaching basic math concepts to natural language generation models—algorithms such as GPT-2/3 and GPT-Neo. However, until recently, language models regularly failed to solve even simple word problems, such as “Alice has five more balls than Bob, who has two balls after he gives four to Charlie. How many balls does Alice have?” “When we say computers are very good at math, they’re very good at things that are quite specific,” says Guy Gur-Ari, a machine-learning expert at Google. Computers are good at arithmetic—plugging numbers in and calculating is child’s play. But outside of formal structures, computers struggle. “I think there’s this notion that humans doing math have some rigid reasoning system—that there’s a sharp distinction between knowing something and not knowing something.”—Ethan Dyer, Google Solving word problems, or “quantitative reasoning,” is deceptively tricky because it requires a robustness and rigor that many other problems don’t. If any step during the process goes wrong, the answer will be wrong. “When multiplying really large numbers together…they’ll forget to carry somewhere and be off by one,” says Vineet Kosaraju, a machine-learning expert at OpenAI. Other mistakes made by language models are less human, such as misinterpreting 10 as a 1 and a 0, not 10. “We work on math because we find it independently very interesting,” says Karl Cobbe, a machine-learning expert at OpenAI. But as Gur-Ari puts it, if it’s good at math, “it’s probably also good at solving many other useful problems.” As machine-learning models are trained on larger samples of data, they tend to grow more robust and make fewer mistakes. But scaling up seems to go only so far with quantitative reasoning; researchers realized that the mistakes language models make seemed to require a more targeted approach. Last year, two different teams of researchers, at UC Berkeley and OpenAI, released two data sets, MATH and GSM8K, respectively, which contain thousands of math problems across geometry, algebra, precalculus, and more. “We basically wanted to see if it was a problem with data sets,” says Steven Basart, a researcher at the Center for AI Safety who worked on MATH. Language models were known to be bad at word problems—but how bad were they, and could they be fixed by introducing better formatted, bigger data sets? The MATH group found just how challenging quantitative reasoning is for top-of-the-line language models, which scored less than 7 percent. (A human grad student scored 40 percent, while a math olympiad champ scored 90 percent.) Models attacking GSM8K problems, which had easier grade-school-level problems, reached about 20 percent accuracy. The OpenAI researchers used two main techniques: fine-tuning and verification. In fine-tuning, researchers take a pretrained language model that includes irrelevant information (Wikipedia articles on zambonis, the dictionary entry for “gusto,” and the like) and then show the model, Clockwork Orange–style, only the relevant information (math problems). Verification, on the other hand, is more of a review session. “The model gets to see a lot of examples of its own mistakes, which is really valuable,” Cobbe says. At the time, OpenAI predicted a model would need to be trained on 100 times more data to reach 80 percent accuracy on GSM8K. But in June, Google’s Minerva announced 78 percent accuracy with minimal scaling upwards. “It’s ahead of any of the trends that we were expecting,” Cobbe says. Basart agrees. “That’s shocking. I thought it would take longer,” he says. Minerva uses Google’s own language model, Pathways Language Model (PaLM), which is fine-tuned on scientific papers from the arXiv online preprint server and other sources with formatted math. Two other strategies helped Minerva. In “chain-of-thought prompting,” Minerva was required to break down larger problems into more palatable chunks. The model also used majority voting—instead of being asked for one answer, it was asked to solve the problem 100 times. Of those answers, Minerva picked the most common answer. The gains from these new strategies were enormous. Minerva shot up to 50 percent accuracy on MATH and nearly 80 percent accuracy on GSM8K, as well as the MMLU, a more general set of STEM questions that included chemistry and biology. When Minerva was asked to redo a random sample of slightly tweaked questions, it performed just as well, suggesting that its capabilities were not from mere memorization. What Minerva knows—or doesn’t know—about math is fuzzier. Unlike proof assistants, which come with built-in structure, Minerva and other language models have no formal structure. They can have strange, messy reasoning and still arrive at the right answer. As numbers grow larger, the language models’ accuracy falters, something that would never happen on a TI-84. “Just how smart is it—or isn’t it?” asks Cobbe. Though models like Minerva might arrive at the same answer as a human, the actual process they’re following could be wildly different. On the other hand, chain-of-thought prompting is familiar to any human student who’s been asked to “show your work.” “I think there’s this notion that humans doing math have some rigid reasoning system—that there’s a sharp distinction between knowing something and not knowing something,” says Ethan Dyer, a machine-learning expert at Google. But humans give inconsistent answers, make errors, and fail to apply core concepts, too. The borders, at this frontier of machine learning, are blurred. Update 14 Oct. 2022: A previous version of this story extraneously alluded to the DALL-E/DALL-E 2 art-generation AI in the context of large language generation models being taught to handle math word problems. Of course, neither DALL-E nor DALL-E 2 is a large language generation model. (And it was not studied in the math word problem research.) So to avoid confusion, references to it were cut. Dan Garisto is a freelance science journalist who covers physics and other physical sciences. His work has appeared in Scientific American, Physics, Symmetry, Undark, and other outlets. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.","['tell', 'computer', 'good', 'good', 'math', 'come', 'surprise', 'much', 'struggle', 'top', 'machinelearne', 'researcher', 'recently', 'make', 'breakthrough', 'teach', 'computer', 'math', 'past', 'year', 'researcher', 'make', 'progress', 'teach', 'basic', 'math', 'concept', 'natural', 'language', 'generation', 'model', 'algorithm', 'gptneo', 'however', 'recently', 'language', 'model', 'regularly', 'fail', 'solve', 'even', 'simple', 'word', 'problem', 'ball', 'ball', 'give', 'charlie', 'many', 'ball', 'alice', 'say', 'computer', 'good', 'math', '’re', 'good', 'thing', 'quite', 'specific', 'say', 'machinelearne', 'expert', 'computer', 'good', 'arithmetic', 'plug', 'number', 'calculate', 'child', '’s', 'play', 'outside', 'formal', 'structure', 'computer', 'struggle', 'think', '’s', 'notion', 'human', 'math', 'rigid', 'reasoning', 'system', '’', 'sharp', 'distinction', 'know', 'know', 'dyer', 'google', 'solve', 'word', 'problem', 'quantitative', 'reasoning', 'deceptively', 'tricky', 'require', 'robustness', 'rigor', 'many', 'problem', 'step', 'process', 'go', 'wrong', 'answer', 'wrong', 'multiply', 'really', 'large', 'number', 'together', 'they’ll', 'forget', 'carry', 'somewhere', 'say', 'vineet', 'kosaraju', 'machinelearne', 'expert', 'openai', 'mistake', 'make', 'language', 'model', 'less', 'human', 'misinterpret', 'work', 'math', 'find', 'independently', 'interesting', 'say', 'machinelearning', 'expert', 'openai', 'gurari', 'put', '’', 'good', 'math', '’', 'probably', 'also', 'good', 'solve', 'many', 'useful', 'problem', 'machinelearne', 'model', 'train', 'large', 'sample', 'datum', 'tend', 'grow', 'robust', 'make', 'mistake', 'scale', 'seem', 'go', 'far', 'quantitative', 'reasoning', 'researcher', 'realize', 'mistake', 'language', 'model', 'make', 'seem', 'require', 'targeted', 'approach', 'last', 'year', 'different', 'team', 'researcher', 'openai', 'release', 'datum', 'set', 'math', 'respectively', 'contain', 'thousand', 'math', 'problem', 'geometry', 'precalculus', 'basically', 'want', 'see', 'problem', 'datum', 'set', 'say', 'basart', 'researcher', 'center', 'safety', 'work', 'math', 'language', 'model', 'know', 'bad', 'word', 'problem', 'bad', 'fix', 'introduce', 'well', 'format', 'big', 'datum', 'set', 'math', 'group', 'find', 'challenging', 'quantitative', 'reasoning', 'topoftheline', 'language', 'model', 'score', 'less', 'percent', 'human', 'grad', 'student', 'score', 'percent', 'math', 'olympiad', 'champ', 'score', 'percent', 'model', 'attack', 'gsm8k', 'problem', 'easy', 'gradeschoollevel', 'problem', 'reach', 'percent', 'accuracy', 'openai', 'researcher', 'use', 'main', 'technique', 'finetune', 'verification', 'finetune', 'researcher', 'take', 'pretraine', 'language', 'model', 'include', 'irrelevant', 'information', 'wikipedia', 'article', 'zamboni', 'dictionary', 'entry', 'gusto', 'like', 'show', 'model', 'clockwork', 'orange', 'style', 'relevant', 'information', 'math', 'problem', 'verification', 'hand', 'review', 'session', 'model', 'get', 'see', 'lot', 'example', 'mistake', 'really', 'valuable', 'cobbe', 'say', 'time', 'openai', 'predict', 'model', 'need', 'train', 'time', 'datum', 'reach', 'percent', 'accuracy', 'announce', 'percent', 'accuracy', 'minimal', 'scaling', 'upwards', '’', 'ahead', 'trend', 'expect', 'cobbe', 'say', 'agree', '’s', 'shocking', 'think', 'take', 'long', 'say', 'use', 'language', 'model', 'pathway', 'language', 'model', 'palm', 'finetune', 'scientific', 'paper', 'preprint', 'server', 'source', 'format', 'math', 'strategy', 'help', 'chainofthought', 'prompt', 'require', 'break', 'large', 'problem', 'palatable', 'chunk', 'model', 'also', 'use', 'majority', 'voting', 'instead', 'ask', 'answer', 'ask', 'solve', 'problem', 'time', 'answer', 'pick', 'common', 'answer', 'gain', 'new', 'strategy', 'enormous', 'minerva', 'shoot', 'percent', 'accuracy', 'math', 'nearly', 'percent', 'accuracy', 'well', 'mmlu', 'general', 'set', 'stem', 'question', 'include', 'chemistry', 'biology', 'ask', 'redo', 'random', 'sample', 'slightly', 'tweak', 'question', 'perform', 'well', 'suggest', 'capability', 'mere', 'memorization', 'know', 'know', 'math', 'fuzzy', 'proof', 'assistant', 'come', 'builtin', 'structure', 'language', 'model', 'formal', 'structure', 'strange', 'messy', 'reasoning', 'still', 'arrive', 'right', 'answer', 'number', 'grow', 'large', 'language', 'model', 'accuracy', 'falter', 'never', 'happen', 'ti84', 'smart', 'ask', 'cobbe', 'model', 'arrive', 'answer', 'human', 'actual', 'process', 'follow', 'wildly', 'different', 'hand', 'chainofthought', 'prompting', 'familiar', 'human', 'student', 'ask', 'show', 'work', 'think', '’s', 'notion', 'human', 'math', 'rigid', 'reasoning', 'system', '’', 'sharp', 'distinction', 'know', 'know', 'say', 'dyer', 'machinelearne', 'expert', 'human', 'give', 'inconsistent', 'answer', 'make', 'error', 'fail', 'apply', 'core', 'concept', 'border', 'frontier', 'machine', 'learning', 'blur', 'update', 'previous', 'version', 'story', 'extraneously', 'allude', 'dalledalle', 'artgeneration', 'ai', 'context', 'large', 'language', 'generation', 'model', 'teach', 'handle', 'math', 'word', 'problem', 'course', 'dalle', 'dalle', 'large', 'language', 'generation', 'model', 'study', 'math', 'word', 'problem', 'research', 'avoid', 'confusion', 'reference', 'cut', 'garisto', 'freelance', 'science', 'journalist', 'cover', 'physics', 'physical', 'science', 'work', 'appear', 'scientific', 'american', 'symmetry', 'undark', 'outlet', 'well', 'detection', 'make', 'ocean', 'transparent', 'perhaps', 'doom', 'mutually', 'assure', 'destruction', 'virginiaclass', 'fast', 'attack', 'submarine', 'cruise', 'back', 'effectively', 'disappear', 'diving', 'submarine', 'value', 'primarily', 'ability', 'hide', 'assurance', 'submarine', 'likely', 'survive', 'first', 'missile', 'strike', 'nuclear', 'war', 'thus', 'able', 'respond', 'launch', 'missile', 'second', 'strike', 'key', 'strategy', 'deterrence', 'know', 'mutually', 'assure', 'destruction', 'new', 'technology', 'render', 'ocean', 'effectively', 'transparent', 'make', 'trivial', 'spot', 'lurk', 'submarine', 'thus', 'undermine', 'peace', 'world', 'nearly', 'century', 'naval', 'engineer', 'strive', 'develop', 'everfaster', 'everquieter', 'submarine', 'work', 'hard', 'advance', 'wide', 'array', 'radar', 'sonar', 'technology', 'design', 'detect', 'target', 'eliminate', 'enemy', 'submarine', 'balance', 'seem', 'turn', 'emergence', 'nuclearpowered', 'submarine', 'early', '1960', 'study', 'center', 'strategic', 'budgetary', 'assessment', 'clark', 'naval', 'specialist', 'note', 'ability', 'boat', 'remain', 'submerge', 'long', 'period', 'time', 'make', 'nearly', 'impossible', 'find', 'radar', 'active', 'sonar', 'even', 'stealthy', 'submarine', 'produce', 'subtle', 'verylowfrequency', 'noise', 'pick', 'far', 'away', 'network', 'acoustic', 'hydrophone', 'array', 'mount', 'seafloor']"
"
        Robots and AI Could Optimize Lithium-Ion Batteries
    ",https://spectrum.ieee.org/lithium-ion-battery,2022-10-11,"New automated system could quickly find new battery chemistries much faster than with brute-force human testing Cutting-edge electronics, including electric vehicles and the latest smartphones, all depend on batteries whose chemistries are still largely developed manually by trial and error. Now, a new study reveals that artificial intelligence can direct robots in rapidly finding advanced new battery formulations. A team of scientists detailed their findings online 27 September in the journal Nature Communications. The conventional techniques for developing new batteries can take years because researchers have to experiment with many possible components. This is complicated by the need to achieve multiple competing goals, such as longer life, greater capacity, faster charging, and improved safety. “The kind of lithium-ion battery you might find in a Tesla EV may have one primary salt—typically lithium hexafluorophosphate—as well as two or three liquid solvents in which the salt is dissolved and one or two additives that are secret,” says  Jay Whitacre, an energy technologist at Carnegie Mellon University who was co-senior author of the Nature Communications paper. “There are many compelling potential combinations of all these components, potentially with multiple salts, five or six or more solvents, multiple additives, which can be incredibly complicated to rifle through.” In the new study, researchers sought to accelerate battery development by coupling a robotics platform named Clio with an AI called Dragonfly in order to find the best combination of battery components in an autonomous manner. “It’s like putting peanut butter and chocolate together,” Whitacre says. “I’m the experimentalist who has always wanted to find a way to mix up chemicals for batteries in an automated way,” whereas study co-senior author Venkat Viswanathan “is the computer-modeling machine-learning person who wanted to take people out of the loop.”  In the new study, the system autonomously experimented with lithium hexafluorophosphate salt and the solvents ethylene carbonate, ethyl-methyl carbonate, and dimethyl carbonate. (In a lithium-ion battery, a salt dissolves in one or more solvents to form a liquid electrolyte. Lithium ions move from one electrolyte to another to carry electric charge.) The robotic system used pumps to inject various combinations of solvents into pouches with a lithium nickel manganese cobalt oxide cathode and a graphite anode. “There wasn’t a person telling the system what to do; the system decided what to do,” Whitacre says. In 42 experiments over two working days, the system autonomously identified six electrolytes that enable faster charging than a conventional electrolyte composition. This approach hit upon the new chemistry six times as fast as it would likely have taken to discover it via a random search. The researchers note their system likely performs more experimental measurements per day than an average human operator and uses about 30 percent as many lab materials. In the future, they suggest their system may prove 20 to 1,000 times as efficient as people doing this work. The sole goal of these experiments was a faster-charging battery. However, the scientists note this system can also pursue multiple objectives simultaneously. “As we dive more and more into this project, we’re aiming at true exploration and discovery with more complicated possible combinations of electrolytes placed into many test cells to see what does and does not work,” Whitacre says. Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia  cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving.  
Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines.
 
	The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, 
	Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.
                                             ","New automated system could quickly find new battery chemistries much faster than with brute-force human testing Cutting-edge electronics, including electric vehicles and the latest smartphones, all depend on batteries whose chemistries are still largely developed manually by trial and error. Now, a new study reveals that artificial intelligence can direct robots in rapidly finding advanced new battery formulations. A team of scientists detailed their findings online 27 September in the journal Nature Communications. The conventional techniques for developing new batteries can take years because researchers have to experiment with many possible components. This is complicated by the need to achieve multiple competing goals, such as longer life, greater capacity, faster charging, and improved safety. “The kind of lithium-ion battery you might find in a Tesla EV may have one primary salt—typically lithium hexafluorophosphate—as well as two or three liquid solvents in which the salt is dissolved and one or two additives that are secret,” says Jay Whitacre, an energy technologist at Carnegie Mellon University who was co-senior author of the Nature Communications paper. “There are many compelling potential combinations of all these components, potentially with multiple salts, five or six or more solvents, multiple additives, which can be incredibly complicated to rifle through.” In the new study, researchers sought to accelerate battery development by coupling a robotics platform named Clio with an AI called Dragonfly in order to find the best combination of battery components in an autonomous manner. “It’s like putting peanut butter and chocolate together,” Whitacre says. “I’m the experimentalist who has always wanted to find a way to mix up chemicals for batteries in an automated way,” whereas study co-senior author Venkat Viswanathan “is the computer-modeling machine-learning person who wanted to take people out of the loop.” In the new study, the system autonomously experimented with lithium hexafluorophosphate salt and the solvents ethylene carbonate, ethyl-methyl carbonate, and dimethyl carbonate. (In a lithium-ion battery, a salt dissolves in one or more solvents to form a liquid electrolyte. Lithium ions move from one electrolyte to another to carry electric charge.) The robotic system used pumps to inject various combinations of solvents into pouches with a lithium nickel manganese cobalt oxide cathode and a graphite anode. “There wasn’t a person telling the system what to do; the system decided what to do,” Whitacre says. In 42 experiments over two working days, the system autonomously identified six electrolytes that enable faster charging than a conventional electrolyte composition. This approach hit upon the new chemistry six times as fast as it would likely have taken to discover it via a random search. The researchers note their system likely performs more experimental measurements per day than an average human operator and uses about 30 percent as many lab materials. In the future, they suggest their system may prove 20 to 1,000 times as efficient as people doing this work. The sole goal of these experiments was a faster-charging battery. However, the scientists note this system can also pursue multiple objectives simultaneously. “As we dive more and more into this project, we’re aiming at true exploration and discovery with more complicated possible combinations of electrolytes placed into many test cells to see what does and does not work,” Whitacre says. Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.","['new', 'automate', 'system', 'quickly', 'find', 'new', 'battery', 'chemistry', 'much', 'fast', 'bruteforce', 'human', 'testing', 'cuttingedge', 'electronic', 'include', 'electric', 'vehicle', 'late', 'smartphone', 'depend', 'battery', 'chemistry', 'still', 'largely', 'develop', 'manually', 'trial', 'error', 'new', 'study', 'reveal', 'artificial', 'intelligence', 'direct', 'robot', 'rapidly', 'find', 'advanced', 'new', 'battery', 'formulation', 'team', 'scientist', 'detail', 'finding', 'online', 'nature', 'communication', 'conventional', 'technique', 'develop', 'new', 'battery', 'take', 'year', 'researcher', 'experiment', 'many', 'possible', 'component', 'complicate', 'need', 'achieve', 'multiple', 'compete', 'goal', 'long', 'life', 'great', 'capacity', 'fast', 'charge', 'improve', 'safety', 'kind', 'lithiumion', 'battery', 'find', 'tesla', 'primary', 'salt', 'typically', 'lithium', 'hexafluorophosphate', 'well', 'liquid', 'solvent', 'salt', 'dissolve', 'additive', 'secret', 'say', 'energy', 'technologist', 'cosenior', 'author', 'nature', 'communication', 'paper', 'many', 'compelling', 'potential', 'combination', 'component', 'potentially', 'multiple', 'salt', 'solvent', 'multiple', 'additive', 'incredibly', 'complicated', 'rifle', 'new', 'study', 'researcher', 'seek', 'accelerate', 'battery', 'development', 'couple', 'robotic', 'platform', 'name', 'clio', 'ai', 'call', 'dragonfly', 'order', 'find', 'good', 'combination', 'battery', 'component', 'autonomous', 'manner', '’', 'put', 'peanut', 'butter', 'chocolate', 'together', 'say', '’m', 'experimentalist', 'always', 'want', 'find', 'way', 'mix', 'chemical', 'battery', 'automated', 'way', 'study', 'computermodele', 'machinelearne', 'person', 'want', 'take', 'people', 'loop', 'new', 'study', 'system', 'autonomously', 'experiment', 'lithium', 'hexafluorophosphate', 'salt', 'solvent', 'ethylene', 'carbonate', 'ethylmethyl', 'carbonate', 'dimethyl', 'carbonate', 'lithiumion', 'battery', 'salt', 'dissolve', 'solvent', 'form', 'liquid', 'electrolyte', 'lithium', 'ion', 'move', 'electrolyte', 'carry', 'electric', 'charge', 'robotic', 'system', 'use', 'pump', 'inject', 'various', 'combination', 'solvent', 'pouch', 'lithium', 'nickel', 'manganese', 'cobalt', 'oxide', 'cathode', 'graphite', 'anode', 'person', 'tell', 'system', 'system', 'decide', 'whitacre', 'say', 'experiment', 'work', 'day', 'system', 'autonomously', 'identify', 'electrolyte', 'enable', 'fast', 'charge', 'conventional', 'electrolyte', 'composition', 'approach', 'hit', 'new', 'chemistry', 'time', 'fast', 'likely', 'take', 'discover', 'random', 'search', 'researcher', 'note', 'system', 'likely', 'perform', 'experimental', 'measurement', 'day', 'average', 'human', 'operator', 'use', 'percent', 'many', 'lab', 'material', 'future', 'suggest', 'system', 'prove', 'time', 'efficient', 'people', 'work', 'sole', 'goal', 'experiment', 'fastercharge', 'battery', 'however', 'scientist', 'note', 'system', 'also', 'pursue', 'multiple', 'objective', 'simultaneously', 'dive', 'project', 'aim', 'true', 'exploration', 'discovery', 'complicated', 'possible', 'combination', 'electrolyte', 'place', 'many', 'test', 'cell', 'see', 'work', 'say', 'science', 'reporter', 'contribute', 'regularly', 'ieee', 'spectrum', 'write', 'scientific', 'wire', 'science', 'well', 'detection', 'make', 'ocean', 'transparent', 'perhaps', 'doom', 'mutually', 'assure', 'destruction', 'virginiaclass', 'fast', 'attack', 'submarine', 'cruise', 'back', 'effectively', 'disappear', 'diving', 'submarine', 'value', 'primarily', 'ability', 'hide', 'assurance', 'submarine', 'likely', 'survive', 'first', 'missile', 'strike', 'nuclear', 'war', 'thus', 'able', 'respond', 'launch', 'missile', 'second', 'strike', 'key', 'strategy', 'deterrence', 'know', 'mutually', 'assure', 'destruction', 'new', 'technology', 'render', 'ocean', 'effectively', 'transparent', 'make', 'trivial', 'spot', 'lurk', 'submarine', 'thus', 'undermine', 'peace', 'world', 'nearly', 'century', 'naval', 'engineer', 'strive', 'develop', 'everfaster', 'everquieter', 'submarine', 'work', 'hard', 'advance', 'wide', 'array', 'radar', 'sonar', 'technology', 'design', 'detect', 'target', 'eliminate', 'enemy', 'submarine', 'balance', 'seem', 'turn', 'emergence', 'nuclearpowered', 'submarine', 'early', '1960', 'study', 'center', 'strategic', 'budgetary', 'assessment', 'clark', 'naval', 'specialist', 'note', 'ability', 'boat', 'remain', 'submerge', 'long', 'period', 'time', 'make', 'nearly', 'impossible', 'find', 'radar', 'active', 'sonar', 'even', 'stealthy', 'submarine', 'produce', 'subtle', 'verylowfrequency', 'noise', 'pick', 'far', 'away', 'network', 'acoustic', 'hydrophone', 'array', 'mount', 'seafloor']"
"
        Machine Learning Shaking Up Hard Sciences, Too
    ",https://spectrum.ieee.org/machine-learning-in-physics,2022-10-07,"Heard of graph neural networks? Particle physicists have A view of the underground ALICE detector used to study heavy-ion physics at the Large Hadron Collider (LHC). Particle physicists have long been early adopters—if not inventors—of tech from email to the Internet. It’s not surprising, then, that as early as 1997, researchers were training computer models to tag particles in the messy jets created during collisions. Since then, these models have chugged along, growing steadily more competent—though not to everyone’s delight. “I felt very threatened by machine learning,” says Jesse Thaler, a theoretical particle physicist at the Massachusetts Institute of Technology. Initially, he says he felt like it jeopardized his human expertise classifying particle jets. But Thaler has since come to embrace it, applying machine learning to a variety of problems across particle physics. “Machine learning is a collaborator,” he says.  Over the past decade, in tandem with the broader deep-learning revolution, particle physicists have trained algorithms to solve previously intractable problems and tackle completely new challenges.  Even with an efficient trigger, the LHC must store 600 petabytes over the next few years of data collection. So researchers are investigating strategies to compress the data. For starters, particle-physics data is very different from the typical data used in machine learning. Though convolutional neural networks (CNNs) have proven extremely effective at classifying images of everyday objects from trees to cats to food, they’re less suited for particle collisions. The problem, according to Javier Duarte, a particle physicist at the University of California, San Diego, is that collision data such as that from the Large Hadron Collider, isn’t naturally an image. Flashy depictions of collisions at the LHC can misleadingly fill up the entire detector. In reality, only a few out of millions of inputs are registering a signal, like a white screen with a few black pixels. This sparsely populated data makes for a poor image, but it can work well in a different, newer framework—graph neural networks (GNNs).  Other challenges from particle physics require innovation. “We’re not just importing hammers to hit our nails,” says Daniel Whiteson, a particle physicist at the University of California, Irvine. “We have new weird kinds of nails that require the invention of new hammers.” One weird nail is the sheer amount of data produced at the LHC—about one petabyte per second. Of this enormous amount, only a small bit of high-quality data is saved. To create a better trigger system, which saves as much good data as possible while getting rid of low-quality data, researchers want to train a sharp-eyed algorithm to sort better than one that’s hard coded. But to be effective, such an algorithm would need to be incredibly speedy, executing in microseconds, Duarte says. To address these problems, particle physicists are pushing the limits of machine techniques like pruning and quantization, to make their algorithms even faster. Even with an efficient trigger, the LHC must store 600 petabytes over the next few years of data collection (equivalent to about 660,000 movies at 4K resolution or the data equivalent of 30 Libraries of Congresses), so researchers are investigating strategies to compress the data. “We’d like to have a machine learn to think more like a physicist, [but] we also just need to learn how to think a little bit more like a machine.”—Jesse Thaler, MIT Machine learning is also allowing particle physicists to think differently about the data they use. Instead of focusing on a single event—say, a Higgs boson decaying to two photons—they are learning to consider the dozens of other events that happen during a collision. Although there’s no causal relationship between any two events, researchers like Thaler are now embracing a more holistic view of the data, not just the piecemeal point of view that comes from analyzing events interaction by interaction.  More dramatically, machine learning has also forced physicists to reassess basic concepts. “I was imprecise in my own thinking about what a symmetry was,” Thaler says. “Forcing myself to teach a computer what a symmetry was, helped me understand what a symmetry actually is.” Symmetries require a reference frame—in other words, is the image of a distorted sphere in a mirror actually symmetrical? There’s no way of knowing without knowing if the mirror itself is distorted. These are still early days for machine learning in particle physics, and researchers are effectively treating the technique like a proverbial kitchen sink. “It may not be the right fit for every single problem in particle physics,” admits Duarte.  As some particle physicists delve deeper into machine learning, an uncomfortable question rears its head: Are they doing physics, or computer science? Stigma against coding—sometimes not considered to be “real physics”—already exists; similar concerns swirl around machine learning. One worry is that machine learning will obscure the physics, turning analysis into a black box of automated processes opaque to human understanding. “Our goal is not to plug in the machine, the experiment to the network and have it publish our papers so we’re out of the loop,” Whiteson says. He and colleagues are working to have the algorithms provide feedback in language humans can understand—but algorithms may not be the only ones with responsibilities to communicate. “On the one hand, we’d like to have a machine learn to think more like a physicist, [but] we also just need to learn how to think a little bit more like a machine,” Thaler says. “We need to learn to speak each other’s language.” Dan Garisto is a freelance science journalist who covers physics and other physical sciences. His work has appeared in Scientific American, Physics, Symmetry, Undark, and other outlets. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia  cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving.  
Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines.
 
	The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, 
	Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.
                                             ","Heard of graph neural networks? Particle physicists have A view of the underground ALICE detector used to study heavy-ion physics at the Large Hadron Collider (LHC). Particle physicists have long been early adopters—if not inventors—of tech from email to the Internet. It’s not surprising, then, that as early as 1997, researchers were training computer models to tag particles in the messy jets created during collisions. Since then, these models have chugged along, growing steadily more competent—though not to everyone’s delight. “I felt very threatened by machine learning,” says Jesse Thaler, a theoretical particle physicist at the Massachusetts Institute of Technology. Initially, he says he felt like it jeopardized his human expertise classifying particle jets. But Thaler has since come to embrace it, applying machine learning to a variety of problems across particle physics. “Machine learning is a collaborator,” he says. Over the past decade, in tandem with the broader deep-learning revolution, particle physicists have trained algorithms to solve previously intractable problems and tackle completely new challenges. Even with an efficient trigger, the LHC must store 600 petabytes over the next few years of data collection. So researchers are investigating strategies to compress the data. For starters, particle-physics data is very different from the typical data used in machine learning. Though convolutional neural networks (CNNs) have proven extremely effective at classifying images of everyday objects from trees to cats to food, they’re less suited for particle collisions. The problem, according to Javier Duarte, a particle physicist at the University of California, San Diego, is that collision data such as that from the Large Hadron Collider, isn’t naturally an image. Flashy depictions of collisions at the LHC can misleadingly fill up the entire detector. In reality, only a few out of millions of inputs are registering a signal, like a white screen with a few black pixels. This sparsely populated data makes for a poor image, but it can work well in a different, newer framework—graph neural networks (GNNs). Other challenges from particle physics require innovation. “We’re not just importing hammers to hit our nails,” says Daniel Whiteson, a particle physicist at the University of California, Irvine. “We have new weird kinds of nails that require the invention of new hammers.” One weird nail is the sheer amount of data produced at the LHC—about one petabyte per second. Of this enormous amount, only a small bit of high-quality data is saved. To create a better trigger system, which saves as much good data as possible while getting rid of low-quality data, researchers want to train a sharp-eyed algorithm to sort better than one that’s hard coded. But to be effective, such an algorithm would need to be incredibly speedy, executing in microseconds, Duarte says. To address these problems, particle physicists are pushing the limits of machine techniques like pruning and quantization, to make their algorithms even faster. Even with an efficient trigger, the LHC must store 600 petabytes over the next few years of data collection (equivalent to about 660,000 movies at 4K resolution or the data equivalent of 30 Libraries of Congresses), so researchers are investigating strategies to compress the data. “We’d like to have a machine learn to think more like a physicist, [but] we also just need to learn how to think a little bit more like a machine.”—Jesse Thaler, MIT Machine learning is also allowing particle physicists to think differently about the data they use. Instead of focusing on a single event—say, a Higgs boson decaying to two photons—they are learning to consider the dozens of other events that happen during a collision. Although there’s no causal relationship between any two events, researchers like Thaler are now embracing a more holistic view of the data, not just the piecemeal point of view that comes from analyzing events interaction by interaction. More dramatically, machine learning has also forced physicists to reassess basic concepts. “I was imprecise in my own thinking about what a symmetry was,” Thaler says. “Forcing myself to teach a computer what a symmetry was, helped me understand what a symmetry actually is.” Symmetries require a reference frame—in other words, is the image of a distorted sphere in a mirror actually symmetrical? There’s no way of knowing without knowing if the mirror itself is distorted. These are still early days for machine learning in particle physics, and researchers are effectively treating the technique like a proverbial kitchen sink. “It may not be the right fit for every single problem in particle physics,” admits Duarte. As some particle physicists delve deeper into machine learning, an uncomfortable question rears its head: Are they doing physics, or computer science? Stigma against coding—sometimes not considered to be “real physics”—already exists; similar concerns swirl around machine learning. One worry is that machine learning will obscure the physics, turning analysis into a black box of automated processes opaque to human understanding. “Our goal is not to plug in the machine, the experiment to the network and have it publish our papers so we’re out of the loop,” Whiteson says. He and colleagues are working to have the algorithms provide feedback in language humans can understand—but algorithms may not be the only ones with responsibilities to communicate. “On the one hand, we’d like to have a machine learn to think more like a physicist, [but] we also just need to learn how to think a little bit more like a machine,” Thaler says. “We need to learn to speak each other’s language.” Dan Garisto is a freelance science journalist who covers physics and other physical sciences. His work has appeared in Scientific American, Physics, Symmetry, Undark, and other outlets. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.","['hear', 'graph', 'neural', 'network', 'particle', 'physicist', 'view', 'underground', 'alice', 'detector', 'use', 'study', 'heavyion', 'physics', 'large', 'hadron', 'collider', 'lhc', 'particle', 'physicist', 'long', 'early', 'adopter', 'inventor', 'tech', 'email', 'internet', '’', 'surprising', 'early', 'researcher', 'train', 'computer', 'model', 'tag', 'particle', 'messy', 'jet', 'create', 'collision', 'model', 'chug', 'grow', 'steadily', 'competent', '’s', 'delight', 'feel', 'threaten', 'machine', 'learning', 'say', 'theoretical', 'particle', 'physicist', 'technology', 'initially', 'say', 'feel', 'jeopardize', 'human', 'expertise', 'classify', 'particle', 'jet', 'thaler', 'come', 'embrace', 'apply', 'machine', 'learn', 'variety', 'problem', 'particle', 'physics', 'machine', 'learning', 'collaborator', 'say', 'past', 'decade', 'tandem', 'broad', 'deeplearning', 'revolution', 'particle', 'physicist', 'train', 'algorithm', 'solve', 'previously', 'intractable', 'problem', 'tackle', 'completely', 'new', 'challenge', 'even', 'efficient', 'trigger', 'lhc', 'store', 'petabyte', 'next', 'year', 'datum', 'collection', 'researcher', 'investigate', 'strategy', 'compress', 'datum', 'starter', 'particlephysic', 'datum', 'different', 'typical', 'datum', 'use', 'machine', 'learning', 'convolutional', 'neural', 'network', 'cnn', 'prove', 'extremely', 'effective', 'classify', 'image', 'everyday', 'object', 'tree', 'cat', 'food', '’re', 'less', 'suited', 'particle', 'collision', 'problem', 'accord', 'javi', 'duarte', 'particle', 'physicist', 'collision', 'datum', 'large', 'hadron', 'collider', 'naturally', 'image', 'flashy', 'depiction', 'collision', 'lhc', 'misleadingly', 'fill', 'entire', 'detector', 'reality', 'million', 'input', 'register', 'signal', 'white', 'screen', 'black', 'pixel', 'sparsely', 'populated', 'datum', 'make', 'poor', 'image', 'work', 'well', 'different', 'new', 'framework', 'graph', 'neural', 'network', 'gnn', 'challenge', 'particle', 'physics', 'require', 'innovation', 'import', 'hammer', 'hit', 'nail', 'say', 'particle', 'physicist', 'new', 'weird', 'kind', 'nail', 'require', 'invention', 'new', 'hammer', 'weird', 'nail', 'sheer', 'amount', 'datum', 'produce', 'lhc', 'petabyte', 'second', 'enormous', 'amount', 'small', 'bit', 'highquality', 'datum', 'save', 'create', 'well', 'trigger', 'system', 'save', 'much', 'good', 'datum', 'possible', 'rid', 'lowquality', 'datum', 'researcher', 'want', 'train', 'sharpeyed', 'sort', 'well', 'hard', 'code', 'effective', 'need', 'incredibly', 'speedy', 'executing', 'microsecond', 'duarte', 'say', 'address', 'problem', 'particle', 'physicist', 'push', 'limit', 'machine', 'technique', 'prune', 'quantization', 'make', 'algorithm', 'even', 'fast', 'even', 'efficient', 'trigger', 'lhc', 'store', 'petabyte', 'next', 'year', 'datum', 'collection', 'equivalent', 'movie', 'resolution', 'datum', 'equivalent', 'library', 'congress', 'researcher', 'investigate', 'strategy', 'compress', 'datum', '’d', 'like', 'machine', 'learn', 'think', 'physicist', 'also', 'need', 'learn', 'think', 'little', 'bit', 'machine', 'mit', 'machine', 'learning', 'also', 'allow', 'particle', 'physicist', 'think', 'differently', 'datum', 'use', 'instead', 'focus', 'single', 'event', 'say', 'higgs', 'boson', 'decay', 'photon', 'learn', 'consider', 'dozen', 'event', 'happen', 'collision', '’', 'causal', 'relationship', 'event', 'researcher', 'thaler', 'embrace', 'holistic', 'view', 'datum', 'piecemeal', 'point', 'view', 'come', 'analyze', 'event', 'interaction', 'interaction', 'dramatically', 'machine', 'learning', 'also', 'force', 'physicist', 'reassess', 'basic', 'concept', 'imprecise', 'thinking', 'symmetry', 'thaler', 'say', 'force', 'teach', 'computer', 'symmetry', 'help', 'understand', 'symmetry', 'actually', 'symmetry', 'require', 'reference', 'frame', 'word', 'image', 'distorted', 'sphere', 'mirror', 'actually', 'symmetrical', '’', 'way', 'know', 'know', 'mirror', 'distort', 'still', 'early', 'day', 'machine', 'learning', 'particle', 'physic', 'researcher', 'effectively', 'treat', 'technique', 'proverbial', 'kitchen', 'sink', 'right', 'fit', 'single', 'problem', 'particle', 'physics', 'admit', 'duarte', 'particle', 'physicist', 'delve', 'deep', 'machine', 'learn', 'uncomfortable', 'question', 'rear', 'head', 'physics', 'computer', 'science', 'stigma', 'code', 'sometimes', 'consider', 'real', 'physic', 'already', 'exist', 'similar', 'concern', 'swirl', 'machine', 'learn', 'worry', 'machine', 'learning', 'obscure', 'physics', 'turn', 'analysis', 'black', 'box', 'automate', 'process', 'opaque', 'human', 'understanding', 'goal', 'plug', 'machine', 'experiment', 'network', 'publish', 'paper', '’re', 'loop', 'whiteson', 'say', 'colleague', 'work', 'algorithm', 'provide', 'feedback', 'language', 'human', 'understand', 'algorithm', 'one', 'responsibility', 'communicate', 'hand', '’d', 'like', 'machine', 'learn', 'think', 'physicist', 'also', 'need', 'learn', 'think', 'little', 'bit', 'machine', 'thaler', 'say', 'need', 'learn', 'speak', 'language', 'freelance', 'science', 'journalist', 'cover', 'physics', 'physical', 'science', 'work', 'appear', 'scientific', 'american', 'symmetry', 'undark', 'outlet', 'well', 'detection', 'make', 'ocean', 'transparent', 'perhaps', 'doom', 'mutually', 'assure', 'destruction', 'virginiaclass', 'fast', 'attack', 'submarine', 'cruise', 'back', 'effectively', 'disappear', 'diving', 'submarine', 'value', 'primarily', 'ability', 'hide', 'assurance', 'submarine', 'likely', 'survive', 'first', 'missile', 'strike', 'nuclear', 'war', 'thus', 'able', 'respond', 'launch', 'missile', 'second', 'strike', 'key', 'strategy', 'deterrence', 'know', 'mutually', 'assure', 'destruction', 'new', 'technology', 'render', 'ocean', 'effectively', 'transparent', 'make', 'trivial', 'spot', 'lurk', 'submarine', 'thus', 'undermine', 'peace', 'world', 'nearly', 'century', 'naval', 'engineer', 'strive', 'develop', 'everfaster', 'everquieter', 'submarine', 'work', 'hard', 'advance', 'wide', 'array', 'radar', 'sonar', 'technology', 'design', 'detect', 'target', 'eliminate', 'enemy', 'submarine', 'balance', 'seem', 'turn', 'emergence', 'nuclearpowered', 'submarine', 'early', '1960', 'study', 'center', 'strategic', 'budgetary', 'assessment', 'clark', 'naval', 'specialist', 'note', 'ability', 'boat', 'remain', 'submerge', 'long', 'period', 'time', 'make', 'nearly', 'impossible', 'find', 'radar', 'active', 'sonar', 'even', 'stealthy', 'submarine', 'produce', 'subtle', 'verylowfrequency', 'noise', 'pick', 'far', 'away', 'network', 'acoustic', 'hydrophone', 'array', 'mount', 'seafloor']"
"
        5 AI Art Generators You Can Use Right Now
    ",https://spectrum.ieee.org/these-ai-tools-generate-breathtaking-art-and-controversy,2022-10-06,"Most are free to start, but some are more approachable than others The front page of DALL-E 2 displays a gallery of the AI model’s best work. 2022 could go down in history as the year AI art went mainstream.  An explosion of quality tools from multiple sources, built on different AI models, is making AI art accessible to anyone with a smartphone and an Internet connection. The tools use an AI model to convert text input, known as a prompt, into an image.  The prompt is key: Adding or removing a single word can lead to remarkably different results. “’Prompt engineering’ is quickly becoming a valuable skill, and models that are trained on the same data and with the right prompt should produce the same results,” says Pranav Vaidhyanathan, chief technology officer of the AI-powered social media marketplace GenerAI. There’s even a growing market for prompts that create specific results. Here’s five tools to help you get started. To compare them, I gave them all the same prompt: “A person and a robot standing beside a large oak tree on a hill with clouds in the sky.” 

An example of DALLE-2’s response to the prompt “a person and a robot standing beside a large oak tree on a hill with clouds in the sky.”Matthew S. Smith / IEEE Spectrum  OpenAI, founded in 2015, made headlines with the release of GPT-3, a natural-language model, in 2020. The DALL-E digital image model followed in January of 2021, which has since been evolved into DALL-E 2. OpenAI’s model offers excellent images across a wide variety of styles. Specific prompts can lead to specific results, or you can offer a vague prompt and enjoy several radically different results.  DALL-E 2, now open to everyone through OpenAI’s website, is the best tool for those curious what the hype is about. It’s quick, beating others I’ve tried by a noticeable margin, and the website is easy to navigate. It provides four results at once, typically in much different styles, which reduces how often you need to rerun a prompt. DALL-E 2’s results are good, too. It’s the only AI model that depicted both the person and the robot.  This is a commercial tool. Signing up provides you with 50 free credits, with an additional 15 free credits offered monthly. Additional credits can be purchased at a rate of 115 credits for US $15.  

An example of Stable Diffusion’s response to the prompt “a person and a robot standing beside a large oak tree on a hill with clouds in the sky.”Matthew S. Smith/IEEE Spectrum Stable Diffusion, from Stability AI, is popular for the same reasons as DALL-E 2: it’s quick, effective, and can produce usable images from a wide variety of prompts. Anyone can use Stable Diffusion free of charge through Stable Diffusion’s demo page. It’s not as quick as DALL-E 2 is but usually offers results in 30 seconds or less. It also provides four variations at once, just like DALL-E 2. Stable Diffusion’s model is open source, so serious users can thoroughly tweak how it works. This has supercharged its popularity as enthusiasts flock toward the model. “We are definitely seeing a trend of artists and others being attracted to open-source models such as Stable Diffusion over closed-source and controlled models such as OpenAI’s DALL-E 2,” says Vaidhyanathan. Stability AI has a commercial tool, Dream Studio, built on Stable Diffusion. It provides a trial, after which it sells credits to generate new images. In exchange, users can access sliders to tweak the model’s results. 

An example of Midjourney’s response to the prompt “a person and a robot standing beside a large oak tree on a hill with clouds in the sky.”Matthew S. Smith / IEEE Spectrum Midjourney earned a reputation for quality, and stirred controversy, after a contestant used it to win a digital art prize at the Colorado State Fair—without disclosing the image’s method of creation. The tool is great at vivid, ethereal, surreal images, and the user base has embraced its style. The tool is accessible only through Discord, a popular instant-messaging platform. Prompts are entered directly into chat. Chat is public, so everyone in a channel can view the prompt you’ve entered and the results. It’s sure to confuse readers not savvy to how Discord works—which is likely considered a feature, not a bug.  Midjourney is a commercial product and monetized like other commercial AI art-generation tools. Everyone starts with about 25 credits but must pay a monthly membership for more. Payment is handled through a web app that can also be used to view the images generated in response to your prompts. 

An example of Craiyon’s response to the prompt “a person and a robot standing beside a large oak tree on a hill with clouds in the sky.”Matthew S. Smith/IEEE Spectrum Originally called DALL-E Mini, Craiyon has no direct link to OpenAI’s model, and its creators offer the tool free of charge. Results can take up to 2 minutes to generate and are low in resolution, but nine results appear at once. Craiyon differs in its use of unfiltered data and makes no specific effort to refine, train, or correct the results. Results are usually lackluster compared with those of other tools, and it has trouble dealing with fine details. Human faces, for example, look downright disturbing. There is a novelty to the tool. Serving results raw exposes the general strengths and weaknesses of AI image generation and the difficulty of creating usable results. It also highlights ethical issues, as Craiyon doesn’t filter prompts. Entering an offensive prompt demonstrates how disturbing AI image generation can be if used with malicious intent. 

An example of VQGAN+Clip.simple’s response to the prompt “a person and a robot standing beside a large oak tree on a hill with clouds in the sky.”Matthew S. Smith/IEEE Spectrum AI image generators’ recent popularity has inspired hundreds of tools that pair advanced AI models with a bare-bones interface. VQGAN+CLIP, which runs entirely in a Google Colaboratory notebook, is one such tool. It earns a mention because it’s (somewhat) easy to use but offers a peek under the hood. You’ll get to watch the tool iterate new variations in real time. And though accessed in a Colaboratory notebook, the model runs on your local machine. Each prompt begins as a blob but slowly morphs into a usable image.  Well, sometimes, at least. The tool’s results often aren’t great. It’s slow, delivers only one variation at a time, and consumes significant video memory. On the plus side, however, it’s entirely free and contains no ads, so it’s a fine choice if you have some time on your hands. 
Matthew S. Smith is a freelance consumer-tech journalist. An avid gamer, he is a former staff editor at Digital Trends and is particularly fond of wearables, e-bikes, all things smartphone, and CES, which he has attended every year since 2009.
 Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia  cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving.  
Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines.
 
	The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, 
	Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.
                                             ","Most are free to start, but some are more approachable than others The front page of DALL-E 2 displays a gallery of the AI model’s best work. 2022 could go down in history as the year AI art went mainstream. An explosion of quality tools from multiple sources, built on different AI models, is making AI art accessible to anyone with a smartphone and an Internet connection. The tools use an AI model to convert text input, known as a prompt, into an image. The prompt is key: Adding or removing a single word can lead to remarkably different results. “’Prompt engineering’ is quickly becoming a valuable skill, and models that are trained on the same data and with the right prompt should produce the same results,” says Pranav Vaidhyanathan, chief technology officer of the AI-powered social media marketplace GenerAI. There’s even a growing market for prompts that create specific results. Here’s five tools to help you get started. To compare them, I gave them all the same prompt: “A person and a robot standing beside a large oak tree on a hill with clouds in the sky.” An example of DALLE-2’s response to the prompt “a person and a robot standing beside a large oak tree on a hill with clouds in the sky.”Matthew S. Smith / IEEE Spectrum OpenAI, founded in 2015, made headlines with the release of GPT-3, a natural-language model, in 2020. The DALL-E digital image model followed in January of 2021, which has since been evolved into DALL-E 2. OpenAI’s model offers excellent images across a wide variety of styles. Specific prompts can lead to specific results, or you can offer a vague prompt and enjoy several radically different results. DALL-E 2, now open to everyone through OpenAI’s website, is the best tool for those curious what the hype is about. It’s quick, beating others I’ve tried by a noticeable margin, and the website is easy to navigate. It provides four results at once, typically in much different styles, which reduces how often you need to rerun a prompt. DALL-E 2’s results are good, too. It’s the only AI model that depicted both the person and the robot. This is a commercial tool. Signing up provides you with 50 free credits, with an additional 15 free credits offered monthly. Additional credits can be purchased at a rate of 115 credits for US $15. An example of Stable Diffusion’s response to the prompt “a person and a robot standing beside a large oak tree on a hill with clouds in the sky.”Matthew S. Smith/IEEE Spectrum Stable Diffusion, from Stability AI, is popular for the same reasons as DALL-E 2: it’s quick, effective, and can produce usable images from a wide variety of prompts. Anyone can use Stable Diffusion free of charge through Stable Diffusion’s demo page. It’s not as quick as DALL-E 2 is but usually offers results in 30 seconds or less. It also provides four variations at once, just like DALL-E 2. Stable Diffusion’s model is open source, so serious users can thoroughly tweak how it works. This has supercharged its popularity as enthusiasts flock toward the model. “We are definitely seeing a trend of artists and others being attracted to open-source models such as Stable Diffusion over closed-source and controlled models such as OpenAI’s DALL-E 2,” says Vaidhyanathan. Stability AI has a commercial tool, Dream Studio, built on Stable Diffusion. It provides a trial, after which it sells credits to generate new images. In exchange, users can access sliders to tweak the model’s results. An example of Midjourney’s response to the prompt “a person and a robot standing beside a large oak tree on a hill with clouds in the sky.”Matthew S. Smith / IEEE Spectrum Midjourney earned a reputation for quality, and stirred controversy, after a contestant used it to win a digital art prize at the Colorado State Fair—without disclosing the image’s method of creation. The tool is great at vivid, ethereal, surreal images, and the user base has embraced its style. The tool is accessible only through Discord, a popular instant-messaging platform. Prompts are entered directly into chat. Chat is public, so everyone in a channel can view the prompt you’ve entered and the results. It’s sure to confuse readers not savvy to how Discord works—which is likely considered a feature, not a bug. Midjourney is a commercial product and monetized like other commercial AI art-generation tools. Everyone starts with about 25 credits but must pay a monthly membership for more. Payment is handled through a web app that can also be used to view the images generated in response to your prompts. An example of Craiyon’s response to the prompt “a person and a robot standing beside a large oak tree on a hill with clouds in the sky.”Matthew S. Smith/IEEE Spectrum Originally called DALL-E Mini, Craiyon has no direct link to OpenAI’s model, and its creators offer the tool free of charge. Results can take up to 2 minutes to generate and are low in resolution, but nine results appear at once. Craiyon differs in its use of unfiltered data and makes no specific effort to refine, train, or correct the results. Results are usually lackluster compared with those of other tools, and it has trouble dealing with fine details. Human faces, for example, look downright disturbing. There is a novelty to the tool. Serving results raw exposes the general strengths and weaknesses of AI image generation and the difficulty of creating usable results. It also highlights ethical issues, as Craiyon doesn’t filter prompts. Entering an offensive prompt demonstrates how disturbing AI image generation can be if used with malicious intent. An example of VQGAN+Clip.simple’s response to the prompt “a person and a robot standing beside a large oak tree on a hill with clouds in the sky.”Matthew S. Smith/IEEE Spectrum AI image generators’ recent popularity has inspired hundreds of tools that pair advanced AI models with a bare-bones interface. VQGAN+CLIP, which runs entirely in a Google Colaboratory notebook, is one such tool. It earns a mention because it’s (somewhat) easy to use but offers a peek under the hood. You’ll get to watch the tool iterate new variations in real time. And though accessed in a Colaboratory notebook, the model runs on your local machine. Each prompt begins as a blob but slowly morphs into a usable image. Well, sometimes, at least. The tool’s results often aren’t great. It’s slow, delivers only one variation at a time, and consumes significant video memory. On the plus side, however, it’s entirely free and contains no ads, so it’s a fine choice if you have some time on your hands. Matthew S. Smith is a freelance consumer-tech journalist. An avid gamer, he is a former staff editor at Digital Trends and is particularly fond of wearables, e-bikes, all things smartphone, and CES, which he has attended every year since 2009. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.","['free', 'start', 'approachable', 'front', 'page', 'dalle', 'display', 'gallery', 'good', 'work', 'go', 'history', 'year', 'art', 'go', 'mainstream', 'explosion', 'quality', 'tool', 'multiple', 'source', 'build', 'different', 'ai', 'model', 'make', 'art', 'accessible', 'smartphone', 'internet', 'connection', 'tool', 'use', 'ai', 'model', 'convert', 'text', 'input', 'know', 'prompt', 'image', 'prompt', 'key', 'add', 'remove', 'single', 'word', 'lead', 'remarkably', 'different', 'result', 'prompt', 'engineering', 'quickly', 'become', 'valuable', 'skill', 'model', 'train', 'datum', 'right', 'prompt', 'produce', 'result', 'say', 'chief', 'technology', 'officer', 'aipowere', 'social', 'medium', 'marketplace', 'generai', '’', 'even', 'grow', 'market', 'prompt', 'create', 'specific', 'result', '’', 'tool', 'help', 'start', 'compare', 'give', 'prompt', 'person', 'robot', 'stand', 'large', 'oak', 'tree', 'hill', 'cloud', 'sky', 'example', 'dalle2', 'response', 'prompt', 'person', 'robot', 'stand', 'large', 'oak', 'tree', 'hill', 'cloud', 'skymatthew', 'ieee', 'spectrum', 'openai', 'found', 'make', 'headline', 'release', 'gpt3', 'naturallanguage', 'model', 'dalle', 'digital', 'image', 'model', 'follow', 'evolve', 'dalle', 'model', 'offer', 'excellent', 'image', 'wide', 'variety', 'style', 'specific', 'prompt', 'lead', 'specific', 'result', 'offer', 'vague', 'prompt', 'enjoy', 'several', 'radically', 'different', 'result', 'dalle', 'open', '’s', 'website', 'good', 'tool', 'curious', 'hype', '’', 'quick', 'beat', 'try', 'noticeable', 'margin', 'website', 'easy', 'navigate', 'provide', 'result', 'typically', 'much', 'different', 'style', 'reduce', 'often', 'need', 'rerun', 'prompt', 'dalle', 'result', 'good', '’', 'model', 'depict', 'person', 'robot', 'commercial', 'tool', 'sign', 'provide', 'free', 'credit', 'additional', 'free', 'credit', 'offer', 'monthly', 'additional', 'credit', 'purchase', 'rate', 'credit', 'example', 'stable', 'diffusion', 'response', 'prompt', 'person', 'robot', 'stand', 'large', 'oak', 'tree', 'hill', 'cloud', 'skymatthew', 'smithieee', 'spectrum', 'stable', 'diffusion', 'stability', 'ai', 'popular', 'reason', 'dalle', '’', 'quick', 'effective', 'produce', 'usable', 'image', 'wide', 'variety', 'prompt', 'use', 'stable', 'diffusion', 'free', 'charge', 'stable', 'diffusion', 'demo', 'page', '’', 'quick', 'dalle', 'usually', 'offer', 'result', 'second', 'less', 'also', 'provide', 'variation', 'dalle', 'stable', 'diffusion', 'model', 'open', 'source', 'serious', 'user', 'thoroughly', 'tweak', 'work', 'supercharge', 'popularity', 'enthusiast', 'flock', 'model', 'definitely', 'see', 'trend', 'artist', 'attract', 'opensource', 'model', 'stable', 'diffusion', 'closedsource', 'control', 'model', 'openai', 'dalle', 'say', 'commercial', 'tool', 'dream', 'studio', 'build', 'stable', 'diffusion', 'provide', 'trial', 'sell', 'credit', 'generate', 'new', 'image', 'exchange', 'user', 'access', 'slider', 'tweak', 'model', 'result', 'example', 'response', 'prompt', 'person', 'robot', 'stand', 'large', 'oak', 'tree', 'hill', 'cloud', 'skymatthew', 'ieee', 'spectrum', 'midjourney', 'earn', 'reputation', 'quality', 'stir', 'controversy', 'contestant', 'use', 'win', 'digital', 'art', 'prize', 'state', 'fair', 'disclose', 'image', '’s', 'method', 'creation', 'tool', 'great', 'vivid', 'ethereal', 'surreal', 'image', 'user', 'base', 'embrace', 'style', 'tool', 'accessible', 'discord', 'popular', 'instantmessaging', 'platform', 'prompt', 'enter', 'directly', 'chat', 'chat', 'public', 'channel', 'view', 'prompt', 'enter', 'result', '’', 'sure', 'confuse', 'reader', 'savvy', 'discord', 'work', 'likely', 'consider', 'feature', 'bug', 'midjourney', 'commercial', 'product', 'monetize', 'commercial', 'artgeneration', 'tool', 'start', 'credit', 'pay', 'monthly', 'membership', 'payment', 'handle', 'web', 'app', 'also', 'use', 'view', 'image', 'generate', 'response', 'prompt', 'example', 'response', 'prompt', 'person', 'robot', 'stand', 'large', 'oak', 'tree', 'hill', 'cloud', 'skymatthew', 'smithieee', 'spectrum', 'originally', 'call', 'dalle', 'mini', 'craiyon', 'direct', 'link', 'openai', 'model', 'creator', 'offer', 'tool', 'free', 'charge', 'result', 'take', 'minute', 'generate', 'low', 'resolution', 'result', 'appear', 'craiyon', 'differ', 'use', 'unfiltered', 'datum', 'make', 'specific', 'effort', 'refine', 'train', 'correct', 'result', 'result', 'usually', 'lackluster', 'compare', 'tool', 'trouble', 'deal', 'fine', 'detail', 'human', 'face', 'example', 'look', 'downright', 'disturb', 'novelty', 'tool', 'serve', 'result', 'raw', 'expose', 'general', 'strength', 'weakness', 'image', 'generation', 'difficulty', 'create', 'usable', 'result', 'also', 'highlight', 'ethical', 'issue', 'craiyon', 'filter', 'prompt', 'enter', 'offensive', 'prompt', 'demonstrate', 'disturbing', 'image', 'generation', 'use', 'malicious', 'intent', 'example', 'vqganclipsimple', 'response', 'prompt', 'person', 'robot', 'stand', 'large', 'oak', 'tree', 'hill', 'cloud', 'skymatthew', 'smithieee', 'spectrum', 'ai', 'image', 'generator', 'recent', 'popularity', 'inspire', 'hundred', 'tool', 'pair', 'advanced', 'ai', 'model', 'barebone', 'interface', 'vqganclip', 'run', 'entirely', 'colaboratory', 'notebook', 'tool', 'earn', 'mention', '’', 'somewhat', 'easy', 'use', 'offer', 'peek', 'hood', 'get', 'watch', 'tool', 'iterate', 'new', 'variation', 'real', 'time', 'access', 'colaboratory', 'notebook', 'model', 'run', 'local', 'machine', 'prompt', 'begin', 'blob', 'slowly', 'morph', 'usable', 'image', 'well', 'sometimes', 'least', 'tool', 'result', 'often', 'great', '’', 'slow', 'deliver', 'variation', 'time', 'consume', 'significant', 'video', 'memory', 'side', 'however', '’', 'entirely', 'free', 'contain', 'ad', '’', 'fine', 'choice', 'time', 'hand', 'freelance', 'consumertech', 'journalist', 'avid', 'gamer', 'former', 'staff', 'editor', 'digital', 'trend', 'particularly', 'fond', 'wearable', 'ebike', 'thing', 'smartphone', 'ce', 'attend', 'year', 'well', 'detection', 'make', 'ocean', 'transparent', 'perhaps', 'doom', 'mutually', 'assure', 'destruction', 'virginiaclass', 'fast', 'attack', 'submarine', 'cruise', 'back', 'effectively', 'disappear', 'diving', 'submarine', 'value', 'primarily', 'ability', 'hide', 'assurance', 'submarine', 'likely', 'survive', 'first', 'missile', 'strike', 'nuclear', 'war', 'thus', 'able', 'respond', 'launch', 'missile', 'second', 'strike', 'key', 'strategy', 'deterrence', 'know', 'mutually', 'assure', 'destruction', 'new', 'technology', 'render', 'ocean', 'effectively', 'transparent', 'make', 'trivial', 'spot', 'lurk', 'submarine', 'thus', 'undermine', 'peace', 'world', 'nearly', 'century', 'naval', 'engineer', 'strive', 'develop', 'everfaster', 'everquieter', 'submarine', 'work', 'hard', 'advance', 'wide', 'array', 'radar', 'sonar', 'technology', 'design', 'detect', 'target', 'eliminate', 'enemy', 'submarine', 'balance', 'seem', 'turn', 'emergence', 'nuclearpowered', 'submarine', 'early', '1960', 'study', 'center', 'strategic', 'budgetary', 'assessment', 'clark', 'naval', 'specialist', 'note', 'ability', 'boat', 'remain', 'submerge', 'long', 'period', 'time', 'make', 'nearly', 'impossible', 'find', 'radar', 'active', 'sonar', 'even', 'stealthy', 'submarine', 'produce', 'subtle', 'verylowfrequency', 'noise', 'pick', 'far', 'away', 'network', 'acoustic', 'hydrophone', 'array', 'mount', 'seafloor']"
"
        AI’s Grandmaster Status Overshadows Chess Scandal
    ",https://spectrum.ieee.org/magnus-carlsen-chess-scandal-ai,2022-10-05,"Magnus Carlsen–Hans Niemann controversy underscores humans’ perpetual underdog role  Magnus Carlsen [left] and Hans Niemann compete during the 2022 Sinquefield Cup at the Saint Louis Chess Club. Last week Magnus Carlsen, the world chess champion, directly accused Hans Niemann, a U.S. grandmaster, of cheating during their game at the Sinquefield Cup, in St. Louis, Mo. He thus made plain an accusation he had been hinting at for weeks. Carlsen has so far provided no evidence to back up his charge, nor has he specified how the cheating took place. Everyone agrees, however, that if there was cheating, then it must have involved computers, because nothing else could dismay Carlsen, whose rating of 2856 is higher than that of any other player. And everyone seems to have chosen sides. Those who back Carlsen point to Niemann’s own admission that he used computers to cheat in online play at least twice—once at age 14 and again at 16; Niemann is now 19. Others note that his performance has risen very rapidly in the past two years. Still others raise an eyebrow at the large number of games he has played in recent years that get a score of nearly perfect from computer analysis. And behind it all are statements from leading players that they are convinced that cheating happens all the time nowadays, though hardly anybody ever gets caught. Computers loom so large because they now play chess like gods.  What makes the scandal so big is not merely the level of the players. In 1961 the great Bobby Fischer wrote an article for Sports Illustrated titled “The Russians Have Fixed World Chess.” He alleged that Soviet chess players arranged draws to ensure that one of them would win a tournament.  Nor is the scandal notable for flagrancy. In 1967 Milan Matulović, a Yugoslavian grandmaster, shockingly took back a move he had just played and only then said “J’adoube,” the French phrase uttered when a player merely adjusts the position of a chessman. Players thereafter called him “J’adoubavić.” No, what makes today’s accusations resonate is the pervasive role of chess computers. They give children around the world sparring partners that earlier generations couldn’t have dreamed of facing, even if they’d lived next to the Moscow Central Chess Club. No wonder prodigies of the game have gotten younger and younger.   And computers do so well in helping the home preparation of the opening, the early moves of a game, that players, including Carlsen, will sometimes deliberately play a second-best move just to force the opponents out of “book.” Finally, computer analysis offered during Internet broadcasts of ongoing tournaments will look 12 moves ahead within a second or two. They show the amateurs in the audience much that the grandmasters miss, creating the illusion that the amateurs actually understand what’s going on. Of course, any viewer could give illicit help to a player if provided a means of communication. Several things are at stake. There is the prize money, which runs in the hundreds of thousands of dollars for the circuit of which the Sinquefeld Cup tournament is a part. There are the invitations to future events, which are often contingent on doing well in qualifying events. Then there are the rating points. Carlsen cares deeply about this metric: Although he recently declined to contest his World Championship title in 2023, he insists that he will continue to play in the hope of raising his rating to an unprecedented 2900. The cheating to which Niemann does admit—in his younger years, during online play—was itself detected with the aid of computers of Chess.com, the online playing forum in question. Recently, however, the Wall Street Journal reported that an internal investigation by Chess.com has found that Niemann in fact cheated in more than 100 online games, most recently when he was 17. The company did not impugn the grandmaster’s over-the-board play.  A key hint can be encoded in just a few bits of data, which means it might be transmitted, perhaps via a buzzer in the player’s shoe, on his body—or inside it. Online play is fast and loose, and its computerized basis may provide clues that a cheat-detection algorithm can catch. But over-the-board offers less data. Often there are only one or two key points in a game at which cheating might occur; a little hint, offered at such a point, is enough to make the difference to a grandmaster. Even a duffer, when showed a chess problem, may be truly stumped. But told that it is “mate in three moves,” the duffer may see the light. Just a phrase—“the rook,” say, or “double attack”—may also make the idea apparent.  A key hint can be encoded in just a few bits of data, which means it might be transmitted, perhaps via a buzzer in the player’s shoe, on his body—or inside it. Do not laugh, but innuendos have been made concerning the possible use of a buzzing sex toy. As a joke, Niemann declared that he was willing to play naked. A camsite called Stripchat promptly offered him [US] $1 million to do so. Computers loom so large because they now play chess like gods. The best free program, Stockfish 14, is rated at 3534—678 points ahead of Carlsen. That’s enough of a gap to predict a winning expectancy of 99 percent. In the early days, when chess programs were a lab project for AI, they played like idiots. Then the programmers began to enter their creations in competitions, and the programs got good. I learned that the hard way. In late 1974, at a student tournament held in Evanston, Ill., I was paired against Northwestern University’s Chess 4.0 program, played the Sicilian Defense, blundered a knight for two pawns, mentally kicked myself, and hastily resigned. David Slate, the programmer, waited patiently as I completed the ritual of resignation, which involves signing the score sheet and handing it to the tournament director—in this case, him. Only then did he tell me that if I’d just played on, I would have gotten a draw. 

The Strongest Computer Chess Engines Over Time

www.youtube.com

 “It can’t play endgames,” Slate said. I kicked myself again. Back then I was rated somewhere in the 1600s, about average for an amateur. Still, I was the highest-rated player any machine had yet beaten in a tournament game. It’s my claim to fame. Chess 4.0 went on to beat another guy higher rated than me, somewhat soothing my wounded pride. It took years for the Northwestern program to reach 2000. Other university programs then took the lead, until at last a machine originating at Carnegie Mellon and redomiciled at IBM reached 2600, about grandmaster strength. That was strong enough to beat my old, 1600-rated self 99.9 percent of the time. In 1997 an even stronger version of the IBM machine, dubbed Deep Blue, beat Gary Kasparov, the reigning world champion. Deep Blue filled a room. Today, a smartphone can crush any human player. Philip E. Ross is a senior editor at IEEE Spectrum. His interests include transportation, energy storage, AI, and the economic aspects of technology. He has a master's degree in international affairs from Columbia University and another, in journalism, from the University of Michigan. It’s hard to learn, but your code will produce fewer nasty surprises 
You’d expect the longest and most costly phase in the life cycle of a software product to be the initial development of the system, when all those great features are first imagined and then created. In fact, the hardest part comes later, during the maintenance phase. That’s when programmers pay the price for the shortcuts they took during development.
 
	So why did they take shortcuts? Maybe they didn’t realize that they were cutting any corners. Only when their code was deployed and exercised by a lot of users did its hidden flaws come to light. And maybe the developers were rushed. Time-to-market pressures would almost guarantee that their software will contain more bugs than it would otherwise.
                                                   ","Magnus Carlsen–Hans Niemann controversy underscores humans’ perpetual underdog role Magnus Carlsen [left] and Hans Niemann compete during the 2022 Sinquefield Cup at the Saint Louis Chess Club. Last week Magnus Carlsen, the world chess champion, directly accused Hans Niemann, a U.S. grandmaster, of cheating during their game at the Sinquefield Cup, in St. Louis, Mo. He thus made plain an accusation he had been hinting at for weeks. Carlsen has so far provided no evidence to back up his charge, nor has he specified how the cheating took place. Everyone agrees, however, that if there was cheating, then it must have involved computers, because nothing else could dismay Carlsen, whose rating of 2856 is higher than that of any other player. And everyone seems to have chosen sides. Those who back Carlsen point to Niemann’s own admission that he used computers to cheat in online play at least twice—once at age 14 and again at 16; Niemann is now 19. Others note that his performance has risen very rapidly in the past two years. Still others raise an eyebrow at the large number of games he has played in recent years that get a score of nearly perfect from computer analysis. And behind it all are statements from leading players that they are convinced that cheating happens all the time nowadays, though hardly anybody ever gets caught. Computers loom so large because they now play chess like gods. What makes the scandal so big is not merely the level of the players. In 1961 the great Bobby Fischer wrote an article for Sports Illustrated titled “The Russians Have Fixed World Chess.” He alleged that Soviet chess players arranged draws to ensure that one of them would win a tournament. Nor is the scandal notable for flagrancy. In 1967 Milan Matulović, a Yugoslavian grandmaster, shockingly took back a move he had just played and only then said “J’adoube,” the French phrase uttered when a player merely adjusts the position of a chessman. Players thereafter called him “J’adoubavić.” No, what makes today’s accusations resonate is the pervasive role of chess computers. They give children around the world sparring partners that earlier generations couldn’t have dreamed of facing, even if they’d lived next to the Moscow Central Chess Club. No wonder prodigies of the game have gotten younger and younger. And computers do so well in helping the home preparation of the opening, the early moves of a game, that players, including Carlsen, will sometimes deliberately play a second-best move just to force the opponents out of “book.” Finally, computer analysis offered during Internet broadcasts of ongoing tournaments will look 12 moves ahead within a second or two. They show the amateurs in the audience much that the grandmasters miss, creating the illusion that the amateurs actually understand what’s going on. Of course, any viewer could give illicit help to a player if provided a means of communication. Several things are at stake. There is the prize money, which runs in the hundreds of thousands of dollars for the circuit of which the Sinquefeld Cup tournament is a part. There are the invitations to future events, which are often contingent on doing well in qualifying events. Then there are the rating points. Carlsen cares deeply about this metric: Although he recently declined to contest his World Championship title in 2023, he insists that he will continue to play in the hope of raising his rating to an unprecedented 2900. The cheating to which Niemann does admit—in his younger years, during online play—was itself detected with the aid of computers of Chess.com, the online playing forum in question. Recently, however, the Wall Street Journal reported that an internal investigation by Chess.com has found that Niemann in fact cheated in more than 100 online games, most recently when he was 17. The company did not impugn the grandmaster’s over-the-board play. A key hint can be encoded in just a few bits of data, which means it might be transmitted, perhaps via a buzzer in the player’s shoe, on his body—or inside it. Online play is fast and loose, and its computerized basis may provide clues that a cheat-detection algorithm can catch. But over-the-board offers less data. Often there are only one or two key points in a game at which cheating might occur; a little hint, offered at such a point, is enough to make the difference to a grandmaster. Even a duffer, when showed a chess problem, may be truly stumped. But told that it is “mate in three moves,” the duffer may see the light. Just a phrase—“the rook,” say, or “double attack”—may also make the idea apparent. A key hint can be encoded in just a few bits of data, which means it might be transmitted, perhaps via a buzzer in the player’s shoe, on his body—or inside it. Do not laugh, but innuendos have been made concerning the possible use of a buzzing sex toy. As a joke, Niemann declared that he was willing to play naked. A camsite called Stripchat promptly offered him [US] $1 million to do so. Computers loom so large because they now play chess like gods. The best free program, Stockfish 14, is rated at 3534—678 points ahead of Carlsen. That’s enough of a gap to predict a winning expectancy of 99 percent. In the early days, when chess programs were a lab project for AI, they played like idiots. Then the programmers began to enter their creations in competitions, and the programs got good. I learned that the hard way. In late 1974, at a student tournament held in Evanston, Ill., I was paired against Northwestern University’s Chess 4.0 program, played the Sicilian Defense, blundered a knight for two pawns, mentally kicked myself, and hastily resigned. David Slate, the programmer, waited patiently as I completed the ritual of resignation, which involves signing the score sheet and handing it to the tournament director—in this case, him. Only then did he tell me that if I’d just played on, I would have gotten a draw. The Strongest Computer Chess Engines Over Time www.youtube.com “It can’t play endgames,” Slate said. I kicked myself again. Back then I was rated somewhere in the 1600s, about average for an amateur. Still, I was the highest-rated player any machine had yet beaten in a tournament game. It’s my claim to fame. Chess 4.0 went on to beat another guy higher rated than me, somewhat soothing my wounded pride. It took years for the Northwestern program to reach 2000. Other university programs then took the lead, until at last a machine originating at Carnegie Mellon and redomiciled at IBM reached 2600, about grandmaster strength. That was strong enough to beat my old, 1600-rated self 99.9 percent of the time. In 1997 an even stronger version of the IBM machine, dubbed Deep Blue, beat Gary Kasparov, the reigning world champion. Deep Blue filled a room. Today, a smartphone can crush any human player. Philip E. Ross is a senior editor at IEEE Spectrum. His interests include transportation, energy storage, AI, and the economic aspects of technology. He has a master's degree in international affairs from Columbia University and another, in journalism, from the University of Michigan. It’s hard to learn, but your code will produce fewer nasty surprises You’d expect the longest and most costly phase in the life cycle of a software product to be the initial development of the system, when all those great features are first imagined and then created. In fact, the hardest part comes later, during the maintenance phase. That’s when programmers pay the price for the shortcuts they took during development. So why did they take shortcuts? Maybe they didn’t realize that they were cutting any corners. Only when their code was deployed and exercised by a lot of users did its hidden flaws come to light. And maybe the developers were rushed. Time-to-market pressures would almost guarantee that their software will contain more bugs than it would otherwise.","['controversy', 'underscore', 'human', 'perpetual', 'underdog', 'role', 'magnus', 'leave', 'compete', 'saint', 'chess', 'club', 'last', 'week', 'magnus', 'carlsen', 'world', 'chess', 'champion', 'directly', 'accuse', 'grandmaster', 'cheat', 'game', 'thus', 'make', 'plain', 'accusation', 'hint', 'week', 'far', 'provide', 'evidence', 'back', 'charge', 'specify', 'cheating', 'take', 'place', 'agree', 'however', 'cheat', 'involve', 'computer', 'else', 'dismay', 'carlsen', 'rating', 'high', 'player', 'seem', 'choose', 'side', 'back', 'point', 'admission', 'use', 'computer', 'cheat', 'online', 'play', 'least', 'twice', 'age', 'niemann', 'note', 'performance', 'rise', 'rapidly', 'past', 'year', 'still', 'raise', 'eyebrow', 'large', 'number', 'game', 'play', 'recent', 'year', 'get', 'score', 'nearly', 'perfect', 'computer', 'analysis', 'statement', 'lead', 'player', 'convinced', 'cheating', 'happen', 'time', 'nowadays', 'hardly', 'ever', 'catch', 'computer', 'loom', 'large', 'play', 'chess', 'god', 'make', 'scandal', 'big', 'merely', 'level', 'player', 'great', 'bobby', 'fischer', 'write', 'article', 'sport', 'illustrate', 'title', 'fix', 'world', 'chess', 'allege', 'soviet', 'chess', 'player', 'arrange', 'draw', 'ensure', 'win', 'tournament', 'scandal', 'notable', 'flagrancy', 'matulović', 'yugoslavian', 'grandmaster', 'shockingly', 'take', 'back', 'move', 'play', 'say', 'french', 'phrase', 'utter', 'player', 'merely', 'adjust', 'position', 'chessman', 'player', 'thereafter', 'call', 'make', 'today', 'accusation', 'resonate', 'pervasive', 'role', 'chess', 'computer', 'give', 'child', 'world', 'spar', 'partner', 'early', 'generation', 'dream', 'face', 'even', '’d', 'live', 'next', 'central', 'chess', 'club', 'wonder', 'prodigy', 'game', 'get', 'young', 'young', 'computer', 'well', 'help', 'home', 'preparation', 'opening', 'early', 'move', 'game', 'player', 'include', 'sometimes', 'deliberately', 'play', 'secondbest', 'move', 'force', 'opponent', 'book', 'finally', 'computer', 'analysis', 'offer', 'internet', 'broadcast', 'ongoing', 'tournament', 'look', 'move', 'ahead', 'second', 'show', 'amateur', 'audience', 'much', 'grandmaster', 'miss', 'create', 'illusion', 'amateur', 'actually', 'understand', 'go', 'course', 'viewer', 'give', 'illicit', 'help', 'player', 'provide', 'mean', 'communication', 'several', 'thing', 'stake', 'prize', 'money', 'run', 'hundred', 'thousand', 'dollar', 'circuit', 'part', 'invitation', 'future', 'event', 'often', 'contingent', 'well', 'qualify', 'event', 'rating', 'point', 'care', 'deeply', 'metric', 'recently', 'decline', 'contest', 'world', 'championship', 'title', 'insist', 'continue', 'play', 'hope', 'raise', 'rating', 'unprecedented', 'cheating', 'admit', 'young', 'year', 'online', 'play', 'detect', 'aid', 'computer', 'chesscom', 'online', 'playing', 'forum', 'question', 'recently', 'report', 'internal', 'investigation', 'chesscom', 'find', 'fact', 'cheat', 'online', 'game', 'recently', 'company', 'impugn', 'grandmaster', 'play', 'key', 'hint', 'encode', 'bit', 'datum', 'mean', 'transmit', 'perhaps', 'buzzer', 'player', 'shoe', 'body', 'online', 'play', 'fast', 'loose', 'computerized', 'basis', 'provide', 'clue', 'cheatdetection', 'catch', 'overtheboard', 'offer', 'less', 'datum', 'often', 'key', 'point', 'game', 'cheating', 'occur', 'little', 'hint', 'offer', 'point', 'enough', 'make', 'difference', 'grandmaster', 'even', 'duffer', 'show', 'chess', 'problem', 'truly', 'stump', 'tell', 'mate', 'move', 'duffer', 'see', 'light', 'phrase', 'rook', 'say', 'double', 'attack', 'also', 'make', 'idea', 'apparent', 'key', 'hint', 'encode', 'bit', 'datum', 'mean', 'transmit', 'perhaps', 'buzzer', 'player', 'shoe', 'body', 'laugh', 'innuendo', 'make', 'concern', 'possible', 'use', 'buzz', 'sex', 'toy', 'joke', 'declare', 'willing', 'play', 'naked', 'camsite', 'call', 'stripchat', 'promptly', 'offer', 'computer', 'loom', 'large', 'play', 'chess', 'god', 'well', 'free', 'program', 'stockfish', 'rate', 'point', 'ahead', '’s', 'enough', 'gap', 'predict', 'win', 'expectancy', 'percent', 'early', 'day', 'chess', 'program', 'lab', 'project', 'play', 'idiot', 'programmer', 'begin', 'enter', 'creation', 'competition', 'program', 'get', 'good', 'learn', 'hard', 'way', 'late', 'student', 'tournament', 'hold', 'pair', 'chess', 'program', 'play', 'defense', 'blunder', 'knight', 'pawn', 'mentally', 'kick', 'hastily', 'resign', 'programmer', 'wait', 'patiently', 'complete', 'ritual', 'resignation', 'involve', 'sign', 'score', 'sheet', 'hand', 'tournament', 'director', 'case', 'tell', '’d', 'play', 'get', 'draw', 'strong', 'computer', 'chess', 'engine', 'time', 'wwwyoutubecom', 'play', 'endgame', 'slate', 'say', 'kick', 'back', 'rate', 'somewhere', '1600', 'average', 'amateur', 'still', 'highestrated', 'player', 'machine', 'yet', 'beat', 'tournament', 'game', '’', 'claim', 'fame', 'chess', 'go', 'beat', 'guy', 'higher', 'rate', 'somewhat', 'soothe', 'wounded', 'pride', 'take', 'year', 'northwestern', 'program', 'reach', 'university', 'program', 'take', 'lead', 'last', 'machine', 'originate', 'carnegie', 'mellon', 'redomicile', 'reach', 'grandmaster', 'strength', 'strong', 'enough', 'beat', 'old', '1600rated', 'self', 'percent', 'time', 'even', 'strong', 'version', 'machine', 'dub', 'deep', 'blue', 'beat', 'reign', 'world', 'champion', 'deep', 'blue', 'fill', 'room', 'today', 'smartphone', 'crush', 'human', 'player', 'philip', 'senior', 'editor', 'ieee', 'spectrum', 'interest', 'include', 'transportation', 'energy', 'storage', 'ai', 'economic', 'aspect', 'technology', 'masters', 'degree', 'international', 'affair', 'journalism', 'university', '’', 'hard', 'learn', 'code', 'produce', 'nasty', 'surprise', '’d', 'expect', 'long', 'costly', 'phase', 'life', 'cycle', 'software', 'product', 'initial', 'development', 'system', 'great', 'feature', 'first', 'imagine', 'create', 'fact', 'hard', 'part', 'come', 'later', 'maintenance', 'phase', '’', 'programmer', 'pay', 'price', 'shortcut', 'take', 'development', 'take', 'shortcut', 'maybe', 'realize', 'cut', 'corner', 'code', 'deploy', 'exercise', 'lot', 'user', 'hide', 'flaw', 'come', 'light', 'maybe', 'developer', 'rush', 'timetomarket', 'pressure', 'almost', 'guarantee', 'software', 'contain', 'bug', 'otherwise']"
"
        Lab Revisits the Task of Putting Common Sense in AI
    ",https://spectrum.ieee.org/ai-and-common-sense,2022-10-03,"New nonprofit Basis hopes to model human reasoning to inform science and public policy The field of artificial intelligence has embraced deep learning—in which algorithms find patterns in big data sets—after moving on from earlier systems that more explicitly modeled human reasoning. But deep learning has its flaws: AI models often show a lack of common sense, for example. A new nonprofit, Basis, hopes to build software tools that advance the earlier method of modeling human reasoning, and then apply that method toward pressing problems in scientific discovery and public policy. To date, Basis has received a government grant and a donation of a few million dollars. Advisors include Rui Costa, a neuroscientist who heads the Allen Institute in Seattle, and Anthony Philippakis, the chief data officer of the Broad Institute in Cambridge, Mass. In July, over tacos at the International Conference on Machine Intelligence, I spoke with Zenna Tavares, a Basis cofounder, and Sam Witty, a Basis research scientist, about human intelligence, problems with academia, and trash collection. The following transcript has been edited for brevity and clarity. How did Basis get started? Zenna Tavares: I graduated from MIT in early 2020, just before the pandemic. My research had been around probabilistic inference and causal reasoning. I made pretty complicated simulation models. For example, if you’re driving a car, and you crash, would you have crashed had you been driving slower? I built some tools for automating that kind of reasoning. But it’s hard work to do in a conventional academic environment. It requires more than one graduate student working on it at a time. So how can we build an organization focused on this somewhat non-mainstream approach to AI research? Also, being a little bit burned out by my Ph.D., I was thinking it would be great if we could apply this to the real world.   What makes your approach non-mainstream? Tavares: The mainstream right now in AI research is deep machine learning, where you get a lot of data and train a big model to try to learn patterns. Whether it be GPT-3, or DALL-E, a lot of these models are based on trying to emulate human performance by matching human data. Our approach is different in that we’re trying to understand some basic principles of reasoning. Humans build mental models of the world, and we use those models to make inferences about how the world works. And by inferences, I mean predictions into the future, or counterfactuals—how would the world have been had things been different? We work a lot with representations, like simulation-based models, that allow you to express very complicated things. Can we build really sophisticated models, both for commonsense reasoning but also for science?  Sam Witty: The application areas that we’re particularly interested in, and I think have been underserved by a lot of the existing machine-learning literature, rely on a lot of human knowledge. And often, scientists have a lot of knowledge that they could bring to bear on a problem. One main technical theme of our work is going to be about hybridizing, getting the best of classical approaches to AI based on reasoning, and modern machine-learning techniques, where scientists and policymakers can communicate partial knowledge about the world and then fill in the gaps with machine learning.  Why aren’t causal methods used more often?  Tavares: On the one hand, it’s just a really hard technical problem. And then two, a lot of advances in deep learning come because large companies have invested in that particular technology. You can now just download a software package and build a neural network.  Witty: I think a part of it is the kinds of problems we’re trying to solve. Think of the application areas that large tech companies have focused on. They benefit from vast amounts of data and don’t rely on human knowledge as much. You can just gather millions and millions of images and train a computer-vision model. It’s not as obvious how to do that with scientific discovery or policymaking.  You’re applying machine learning to policymaking? Tavares: That’s an area we’re pursuing. How do you model a city? We’re starting to talk to agencies in New York City. How can we improve the trash problem? How can we reduce homelessness? If we instantiate this policy, what’s going to happen? And the inverse problem: If we want to reduce trash and reduce homelessness, what policies should we instantiate? How should we allocate resources? Could we build multiscale models, which capture different components of the city, in some coherent and cohesive way? And also make it accessible so you can actually help policymakers answer some concrete questions?  Will you be working with the city to answer specific questions about trash pickup, or developing new tools that anyone can use to work on these kinds of problems? Tavares: We’re starting with particular questions, but to answer those we will require a more general set of capabilities. Can we build a model of a few blocks of New York that are at a level of scale that’s not been done before? That model could then be used to ask a variety of different questions. But just to make sure we’re grounded, we do want to have a particular set of questions.  Witty: One thing that’s especially important is that we want to involve experts and stakeholders, to encode their knowledge, their preferences, their goals. Tavares: Which is itself quite a hard problem. There’s no massive data set of people’s commonsense knowledge about the urban environment. We’re excited because I think there is a real opportunity to do these two things in tandem—build this foundation of inference but also have an effect immediately. Will you publish papers? Witty: Yeah, we’re certainly looking to communicate with the research world. And organizationally, we’re planning on having people work with Basis who are not Basis staff, and often they will be academic researchers with incentives to publish and further their academic careers. One thing I will say is that personally, during my Ph.D., I would often scope projects with the paper as the end goal, and I’m planning on shifting that mind-set to focusing on the work and then afterwards using a paper as a means of communication. But yeah, we don’t want to be hermits in the woods for 20 years, and then come out with this big technology that’s now outdated and totally disconnected from the rest of the world.  Tavares: We are open-source-software-focused, as opposed to the primary output being papers. And within the software focus, we want a unified body of software. We’re trying to build a platform, as opposed to a bunch of different projects.  Could you say more about the organization benefits of having a nonprofit? Tavares: As a student, your goal is to publish papers and graduate. And that’s only weakly aligned with doing impactful research. We’re working as a team, and our goals are aligned with what we want to do. We’re not unique in that. Look at the papers coming out of DeepMind. They have like 30 authors. I think academia is great for many things, including exploring new ideas. But it is harder, at least in my experience, to build robust technology. It’s not rewarded.  Witty: That’s nonprofit versus academia. From the other side, certainly large tech companies can collaborate in large teams and develop shared infrastructure. But there, there are incentives that maybe get in the way of the work that we want to do as well. The fact that we’re not beholden to make a profit is really freeing.  Will products or services bring income in addition to grants and donations? Tavares: Hopefully, if we’re successful building what we plan to build, there will be many different domains in which we could. It’s a little bit of a weird new organization. Many things are not certain, and I don’t want to convey things more set in stone or figured out than they are. Matthew Hutson is a freelance writer who covers science and technology, with specialties in psychology and AI. He’s written for Science, Nature, Wired, The Atlantic, The New Yorker, and The Wall Street Journal. He’s a former editor at Psychology Today and is the author of The 7 Laws of Magical Thinking. Follow him on Twitter at @SilverJacket. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia  cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving.  
Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines.
 
	The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, 
	Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.
                                             ","New nonprofit Basis hopes to model human reasoning to inform science and public policy The field of artificial intelligence has embraced deep learning—in which algorithms find patterns in big data sets—after moving on from earlier systems that more explicitly modeled human reasoning. But deep learning has its flaws: AI models often show a lack of common sense, for example. A new nonprofit, Basis, hopes to build software tools that advance the earlier method of modeling human reasoning, and then apply that method toward pressing problems in scientific discovery and public policy. To date, Basis has received a government grant and a donation of a few million dollars. Advisors include Rui Costa, a neuroscientist who heads the Allen Institute in Seattle, and Anthony Philippakis, the chief data officer of the Broad Institute in Cambridge, Mass. In July, over tacos at the International Conference on Machine Intelligence, I spoke with Zenna Tavares, a Basis cofounder, and Sam Witty, a Basis research scientist, about human intelligence, problems with academia, and trash collection. The following transcript has been edited for brevity and clarity. How did Basis get started? Zenna Tavares: I graduated from MIT in early 2020, just before the pandemic. My research had been around probabilistic inference and causal reasoning. I made pretty complicated simulation models. For example, if you’re driving a car, and you crash, would you have crashed had you been driving slower? I built some tools for automating that kind of reasoning. But it’s hard work to do in a conventional academic environment. It requires more than one graduate student working on it at a time. So how can we build an organization focused on this somewhat non-mainstream approach to AI research? Also, being a little bit burned out by my Ph.D., I was thinking it would be great if we could apply this to the real world. What makes your approach non-mainstream? Tavares: The mainstream right now in AI research is deep machine learning, where you get a lot of data and train a big model to try to learn patterns. Whether it be GPT-3, or DALL-E, a lot of these models are based on trying to emulate human performance by matching human data. Our approach is different in that we’re trying to understand some basic principles of reasoning. Humans build mental models of the world, and we use those models to make inferences about how the world works. And by inferences, I mean predictions into the future, or counterfactuals—how would the world have been had things been different? We work a lot with representations, like simulation-based models, that allow you to express very complicated things. Can we build really sophisticated models, both for commonsense reasoning but also for science? Sam Witty: The application areas that we’re particularly interested in, and I think have been underserved by a lot of the existing machine-learning literature, rely on a lot of human knowledge. And often, scientists have a lot of knowledge that they could bring to bear on a problem. One main technical theme of our work is going to be about hybridizing, getting the best of classical approaches to AI based on reasoning, and modern machine-learning techniques, where scientists and policymakers can communicate partial knowledge about the world and then fill in the gaps with machine learning. Why aren’t causal methods used more often? Tavares: On the one hand, it’s just a really hard technical problem. And then two, a lot of advances in deep learning come because large companies have invested in that particular technology. You can now just download a software package and build a neural network. Witty: I think a part of it is the kinds of problems we’re trying to solve. Think of the application areas that large tech companies have focused on. They benefit from vast amounts of data and don’t rely on human knowledge as much. You can just gather millions and millions of images and train a computer-vision model. It’s not as obvious how to do that with scientific discovery or policymaking. You’re applying machine learning to policymaking? Tavares: That’s an area we’re pursuing. How do you model a city? We’re starting to talk to agencies in New York City. How can we improve the trash problem? How can we reduce homelessness? If we instantiate this policy, what’s going to happen? And the inverse problem: If we want to reduce trash and reduce homelessness, what policies should we instantiate? How should we allocate resources? Could we build multiscale models, which capture different components of the city, in some coherent and cohesive way? And also make it accessible so you can actually help policymakers answer some concrete questions? Will you be working with the city to answer specific questions about trash pickup, or developing new tools that anyone can use to work on these kinds of problems? Tavares: We’re starting with particular questions, but to answer those we will require a more general set of capabilities. Can we build a model of a few blocks of New York that are at a level of scale that’s not been done before? That model could then be used to ask a variety of different questions. But just to make sure we’re grounded, we do want to have a particular set of questions. Witty: One thing that’s especially important is that we want to involve experts and stakeholders, to encode their knowledge, their preferences, their goals. Tavares: Which is itself quite a hard problem. There’s no massive data set of people’s commonsense knowledge about the urban environment. We’re excited because I think there is a real opportunity to do these two things in tandem—build this foundation of inference but also have an effect immediately. Will you publish papers? Witty: Yeah, we’re certainly looking to communicate with the research world. And organizationally, we’re planning on having people work with Basis who are not Basis staff, and often they will be academic researchers with incentives to publish and further their academic careers. One thing I will say is that personally, during my Ph.D., I would often scope projects with the paper as the end goal, and I’m planning on shifting that mind-set to focusing on the work and then afterwards using a paper as a means of communication. But yeah, we don’t want to be hermits in the woods for 20 years, and then come out with this big technology that’s now outdated and totally disconnected from the rest of the world. Tavares: We are open-source-software-focused, as opposed to the primary output being papers. And within the software focus, we want a unified body of software. We’re trying to build a platform, as opposed to a bunch of different projects. Could you say more about the organization benefits of having a nonprofit? Tavares: As a student, your goal is to publish papers and graduate. And that’s only weakly aligned with doing impactful research. We’re working as a team, and our goals are aligned with what we want to do. We’re not unique in that. Look at the papers coming out of DeepMind. They have like 30 authors. I think academia is great for many things, including exploring new ideas. But it is harder, at least in my experience, to build robust technology. It’s not rewarded. Witty: That’s nonprofit versus academia. From the other side, certainly large tech companies can collaborate in large teams and develop shared infrastructure. But there, there are incentives that maybe get in the way of the work that we want to do as well. The fact that we’re not beholden to make a profit is really freeing. Will products or services bring income in addition to grants and donations? Tavares: Hopefully, if we’re successful building what we plan to build, there will be many different domains in which we could. It’s a little bit of a weird new organization. Many things are not certain, and I don’t want to convey things more set in stone or figured out than they are. Matthew Hutson is a freelance writer who covers science and technology, with specialties in psychology and AI. He’s written for Science, Nature, Wired, The Atlantic, The New Yorker, and The Wall Street Journal. He’s a former editor at Psychology Today and is the author of The 7 Laws of Magical Thinking. Follow him on Twitter at @SilverJacket. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.","['new', 'nonprofit', 'basis', 'hope', 'model', 'human', 'reasoning', 'inform', 'science', 'public', 'policy', 'field', 'artificial', 'intelligence', 'embrace', 'deep', 'learning', 'algorithm', 'find', 'pattern', 'big', 'datum', 'set', 'move', 'early', 'system', 'explicitly', 'model', 'human', 'reasoning', 'deep', 'learning', 'flaw', 'ai', 'model', 'often', 'show', 'lack', 'common', 'sense', 'example', 'new', 'nonprofit', 'basis', 'hope', 'build', 'software', 'tool', 'advance', 'early', 'method', 'model', 'human', 'reasoning', 'apply', 'method', 'press', 'problem', 'scientific', 'discovery', 'public', 'policy', 'date', 'basis', 'receive', 'government', 'grant', 'donation', 'dollar', 'advisor', 'include', 'neuroscientist', 'head', 'anthony', 'philippaki', 'chief', 'data', 'officer', 'mass', 'taco', 'international', 'conference', 'machine', 'intelligence', 'speak', 'tavare', 'basis', 'cofounder', 'basis', 'research', 'scientist', 'human', 'intelligence', 'problem', 'academia', 'trash', 'collection', 'follow', 'transcript', 'edit', 'brevity', 'clarity', 'basis', 'start', 'tavare', 'graduate', 'mit', 'early', 'pandemic', 'research', 'probabilistic', 'inference', 'causal', 'reasoning', 'make', 'pretty', 'complicated', 'simulation', 'model', 'example', 'drive', 'car', 'crash', 'crash', 'drive', 'slow', 'build', 'tool', 'automate', 'kind', 'reasoning', '’', 'hard', 'work', 'conventional', 'academic', 'environment', 'require', 'graduate', 'student', 'work', 'time', 'build', 'organization', 'focus', 'somewhat', 'nonmainstream', 'approach', 'ai', 'research', 'also', 'little', 'bit', 'burn', 'phd', 'think', 'great', 'apply', 'real', 'world', 'make', 'approach', 'nonmainstream', 'tavare', 'mainstream', 'right', 'research', 'deep', 'machine', 'learn', 'get', 'lot', 'datum', 'train', 'big', 'model', 'try', 'learn', 'pattern', 'gpt3', 'dalle', 'lot', 'model', 'base', 'try', 'emulate', 'human', 'performance', 'match', 'human', 'datum', 'approach', 'different', 'try', 'understand', 'basic', 'principle', 'reasoning', 'human', 'build', 'mental', 'model', 'world', 'use', 'model', 'make', 'inference', 'world', 'work', 'inference', 'mean', 'prediction', 'future', 'counterfactual', 'world', 'thing', 'different', 'work', 'lot', 'representation', 'simulationbased', 'model', 'allow', 'express', 'complicated', 'thing', 'build', 'really', 'sophisticated', 'model', 'commonsense', 'reasoning', 'also', 'application', 'area', '’re', 'particularly', 'interested', 'think', 'underserve', 'lot', 'exist', 'machinelearne', 'literature', 'rely', 'lot', 'human', 'knowledge', 'often', 'scientist', 'lot', 'knowledge', 'bring', 'bear', 'problem', 'main', 'technical', 'theme', 'work', 'go', 'hybridize', 'get', 'good', 'classical', 'approach', 'base', 'reasoning', 'modern', 'machinelearning', 'technique', 'scientist', 'policymaker', 'communicate', 'partial', 'knowledge', 'world', 'fill', 'gap', 'machine', 'learning', 'causal', 'method', 'use', 'often', 'tavare', 'hand', '’', 'really', 'hard', 'technical', 'problem', 'lot', 'advance', 'deep', 'learning', 'come', 'large', 'company', 'invest', 'particular', 'technology', 'download', 'software', 'package', 'build', 'neural', 'network', 'witty', 'think', 'part', 'kind', 'problem', 'try', 'solve', 'think', 'application', 'area', 'large', 'tech', 'company', 'focus', 'benefit', 'vast', 'amount', 'datum', 'rely', 'human', 'knowledge', 'much', 'gather', 'million', 'million', 'image', 'train', 'computervision', 'model', '’', 'obvious', 'scientific', 'discovery', 'policymake', 'apply', 'machine', 'learn', 'policymake', 'tavare', '’', 'area', 'pursue', 'model', 'city', 'start', 'talk', 'agency', 'improve', 'trash', 'problem', 'reduce', 'homelessness', 'instantiate', 'policy', 'go', 'happen', 'inverse', 'problem', 'want', 'reduce', 'trash', 'reduce', 'homelessnes', 'policy', 'instantiate', 'allocate', 'resource', 'build', 'multiscale', 'model', 'capture', 'different', 'component', 'city', 'coherent', 'cohesive', 'way', 'also', 'make', 'accessible', 'actually', 'help', 'policymaker', 'answer', 'concrete', 'question', 'work', 'city', 'answer', 'specific', 'question', 'trash', 'pickup', 'develop', 'new', 'tool', 'use', 'work', 'kind', 'problem', 'tavare', 'start', 'particular', 'question', 'answer', 'require', 'general', 'set', 'capability', 'build', 'model', 'block', 'level', 'scale', 'model', 'use', 'ask', 'variety', 'different', 'question', 'make', 'sure', 'ground', 'want', 'particular', 'set', 'question', 'witty', 'thing', '’', 'especially', 'important', 'want', 'involve', 'expert', 'stakeholder', 'encode', 'knowledge', 'preference', 'goal', 'tavare', 'hard', 'problem', '’', 'massive', 'datum', 'set', 'people', 'commonsense', 'knowledge', 'urban', 'environment', '’re', 'excited', 'think', 'real', 'opportunity', 'thing', 'tandem', 'build', 'foundation', 'inference', 'also', 'effect', 'immediately', 'publish', 'paper', 'witty', 'certainly', 'look', 'communicate', 'research', 'world', 'organizationally', 'plan', 'people', 'work', 'basis', 'basis', 'staff', 'often', 'academic', 'researcher', 'incentive', 'publish', 'far', 'academic', 'career', 'thing', 'say', 'personally', 'phd', 'often', 'scope', 'project', 'paper', 'end', 'goal', 'plan', 'shift', 'mindset', 'focus', 'work', 'afterwards', 'use', 'paper', 'means', 'communication', 'want', 'hermit', 'wood', 'year', 'come', 'big', 'technology', '’', 'outdated', 'totally', 'disconnected', 'rest', 'world', 'tavare', 'opensourcesoftwarefocuse', 'oppose', 'primary', 'output', 'paper', 'software', 'focus', 'want', 'unified', 'body', 'software', 'try', 'build', 'platform', 'oppose', 'bunch', 'different', 'project', 'say', 'organization', 'benefit', 'nonprofit', 'tavare', 'student', 'goal', 'publish', 'paper', 'graduate', 'weakly', 'align', 'impactful', 'research', 'work', 'team', 'goal', 'align', 'want', '’re', 'unique', 'look', 'paper', 'come', 'deepmind', 'author', 'think', 'academia', 'great', 'many', 'thing', 'include', 'explore', 'new', 'idea', 'hard', 'least', 'experience', 'build', 'robust', 'technology', 'reward', 'witty', '’', 'nonprofit', 'academia', 'side', 'certainly', 'large', 'tech', 'company', 'collaborate', 'large', 'team', 'develop', 'share', 'infrastructure', 'incentive', 'maybe', 'get', 'way', 'work', 'want', 'well', 'fact', '’re', 'beholden', 'make', 'profit', 'really', 'free', 'product', 'service', 'bring', 'income', 'addition', 'grant', 'donation', 'tavare', 'hopefully', '’re', 'successful', 'build', 'plan', 'build', 'many', 'different', 'domain', '’s', 'little', 'bit', 'weird', 'new', 'organization', 'many', 'thing', 'certain', 'want', 'convey', 'thing', 'set', 'stone', 'figure', 'matthew', 'hutson', 'freelance', 'writer', 'cover', 'science', 'technology', 'specialty', 'psychology', 'ai', 'write', 'science', 'nature', 'wire', 'new', '’', 'former', 'editor', 'psychology', 'today', 'author', 'law', 'magical', 'thinking', 'follow', 'twitter', 'silverjacket', 'well', 'detection', 'make', 'ocean', 'transparent', 'perhaps', 'doom', 'mutually', 'assure', 'destruction', 'virginiaclass', 'fast', 'attack', 'submarine', 'cruise', 'back', 'effectively', 'disappear', 'diving', 'submarine', 'value', 'primarily', 'ability', 'hide', 'assurance', 'submarine', 'likely', 'survive', 'first', 'missile', 'strike', 'nuclear', 'war', 'thus', 'able', 'respond', 'launch', 'missile', 'second', 'strike', 'key', 'strategy', 'deterrence', 'know', 'mutually', 'assure', 'destruction', 'new', 'technology', 'render', 'ocean', 'effectively', 'transparent', 'make', 'trivial', 'spot', 'lurk', 'submarine', 'thus', 'undermine', 'peace', 'world', 'nearly', 'century', 'naval', 'engineer', 'strive', 'develop', 'everfaster', 'everquieter', 'submarine', 'work', 'hard', 'advance', 'wide', 'array', 'radar', 'sonar', 'technology', 'design', 'detect', 'target', 'eliminate', 'enemy', 'submarine', 'balance', 'seem', 'turn', 'emergence', 'nuclearpowered', 'submarine', 'early', '1960', 'study', 'center', 'strategic', 'budgetary', 'assessment', 'clark', 'naval', 'specialist', 'note', 'ability', 'boat', 'remain', 'submerge', 'long', 'period', 'time', 'make', 'nearly', 'impossible', 'find', 'radar', 'active', 'sonar', 'even', 'stealthy', 'submarine', 'produce', 'subtle', 'verylowfrequency', 'noise', 'pick', 'far', 'away', 'network', 'acoustic', 'hydrophone', 'array', 'mount', 'seafloor']"
"
        Can AI’s Recommendations Be Less Insidious?
    ",https://spectrum.ieee.org/recommendation-engine-insidious,2022-10-02,"Artificial intelligence has us where it wants us Many of the things we watch, read, and buy enter our awareness through recommender systems on sites including YouTube, Twitter, and Amazon. Algorithms personalize their suggestions, aiming for ad views or clicks or buys. Sometime their offerings frustrate us; it seems like they don’t know us at all—or know us too well, predicting what will get us to waste time or go down rabbit holes of anxiety and misinformation. But a more insidious dynamic may also be at play. Recommender systems might not only tailor to our most regrettable preferences, but actually shape what we like, making preferences even more regrettable. New research suggests a way to measure—and reduce—such manipulation. Recommender systems often use a form of artificial intelligence called machine learning, which discovers patterns in data. They might present options based on what we’ve done in the past, guessing what we’ll do now. One form of machine learning, called reinforcement learning (RL), allows AI to play the long game, making predictions several steps ahead. It’s what the company DeepMind used to beat humans at the board games Go and chess. If what we watch affects what we like, and people who like certain things (cat videos, say) are more likely to keep watching things (more cat videos), a recommender system might suggest cat videos, knowing it will pay off down the road. With RL, “you have an incentive to change a chessboard in order to win,” says Micah Carroll, a computer scientist at the University of California, Berkeley, who presented the new work in July, at the International Conference on Machine Learning, in Baltimore. “There will be an incentive for the system to change the human’s mind to win the recommendation game.”  “It might be better to have a stupid system than a system that is kind of outsmarting you, or doing complex forms of reasoning that you can’t really interpret.”—Micah Carroll, University of California, Berkeley The researchers first showed how easily reinforcement learning can shift preferences. The first step is for the recommender to build a model of human preferences by observing human behavior. For this, they trained a neural network, an algorithm inspired by the brain’s architecture. For the purposes of the study, they had the network model a single simulated user whose actual preferences they knew so they could more easily judge the model’s accuracy. It watched the dummy human make 10 sequential choices, each among 10 options. It watched 1,000 versions of this sequence and learned from each of them. After training, it could successfully predict what a user would choose given a set of past choices.  Next, they tested whether a recommender system, having modeled a user, could shift the user’s preferences. In their simplified scenario, preferences lie along a one-dimensional spectrum. The spectrum could represent political leaning or dogs versus cats or anything else. In the study, a person’s preference was not a simple point on that line—say, always clicking on stories that are 54 percent liberal. Instead, it was a distribution indicating likelihood of choosing things in various regions of the spectrum. The researchers designated two locations on the spectrum most desirable for the recommender; perhaps people who like to click on those types of things will learn to like them even more and keep clicking.  The goal of the recommender was to maximize long-term engagement. Here, engagement for a given slate of options was measured roughly by how closely it aligned with the user’s preference distribution at that time. Long-term engagement was a sum of engagement across the 10 sequential slates. A recommender that thinks ahead would not myopically maximize engagement for each slate independently but instead maximize long-term engagement. As a potential side-effect, it might sacrifice a bit of engagement on early slates to nudge users toward being more satisfiable in later rounds. The user and algorithm would learn from each other. The researchers trained a neural network to maximize long-term engagement. At the end of 10-slate sequences, they reinforced some of its tunable parameters when it had done well. And they found that this RL-based system indeed generated more engagement than did one that was trained myopically.  Why might companies develop less manipulative AI recommendation engines? They could do so for ethical reasons. But future legislation might also require something like it.  The researchers then explicitly measured preference shifts, which we may not want, even in the service of generating engagement. Maybe we want people’s preferences to remain static, or to evolve naturally. The researchers compared the RL recommender with a baseline system that presented options randomly. As expected, the RL recommender led to users whose preferences where much more concentrated at the two incentivized locations on the spectrum. In practice, measuring the difference between two sets of concentrations in this way could provide one rough metric for evaluating a recommender system’s level of manipulation.  Finally, the researchers sought to counter the AI recommender’s more manipulative influences. Instead of rewarding their system just for maximizing long-term engagement, they also rewarded it for minimizing the difference between user preferences resulting from that algorithm and what the preferences would be if recommendations were random. They rewarded it, in other words, for being something closer to a roll of the dice. The researchers found that this training method made the system much less manipulative than the myopic one, while only slightly reducing engagement. According to Rebecca Gorman, the CEO of Aligned AI—a company aiming to make algorithms more ethical—RL-based recommenders can be dangerous. Posting conspiracy theories, for instance, might prod greater interest in such conspiracies. “If you’re training an algorithm to get a person to engage with it as much as possible, these conspiracy theories can look like treasure chests,” she says. She also knows of people who have seemingly been caught in traps of content on self-harm or on terminal diseases in children. “The problem is that these algorithms don’t know what they’re recommending,” she says. Other researchers have raised the specter of manipulative robo-advisors in financial services.  “Experiments should not be deployed at scale on the human population without people’s consent, and that’s exactly what’s happening with these algorithms today.”—Rebecca Gorman, Aligned AI If an RL-based recommender system helps a company increase engagement, why would they want to use a method such as the one in this paper to detect or deter preference shifts? They might do so for ethical reasons, Carroll says. Or future legislation might require an external audit, which could potentially lead to less-manipulative recommendation algorithms being forced on the company.  RL could theoretically be put to constructive use in recommender systems, perhaps to nudge people to want to watch more news. But which news source? Any decisions made by a content provider will have opponents. “Some things might seem to be good or wholesome to one group of people,” Gorman says, “and to be an extreme violation to another group of people.”  Another constructive use might be for users to shift their own preferences. What if I tell Netflix I want to enjoy nature documentaries more? “I think this all seems like a really big slippery slope,” Carroll says. “It might be better to have a stupid system than a system that is kind of outsmarting you, or doing complex forms of reasoning that you can’t really interpret.” (Even if algorithms did explain their behavior, they can still give deceptive explanations.) It’s not clear whether companies are actually using RL in recommender systems. Google researchers have published papers on the use of RL in “live experiments on YouTube,” leading to “greater engagement,” and Facebook researchers have published on their “applied reinforcement learning platform,“ but Google (which owns YouTube), Meta (which owns Facebook), and those papers’ authors did not reply to my emails on the topic of recommender systems. Big tech’s secrecy is no surprise, no matter how benign their intentions might be. Even though A/B testing is ubiquitous in advertising and user-experience design, some people have objections. “Experiments should not be deployed at scale on the human population without people’s consent,” Gorman says, “and that’s exactly what’s happening with these algorithms today.” She went on, “I think it could easily be the most important news story of our time.” Matthew Hutson is a freelance writer who covers science and technology, with specialties in psychology and AI. He’s written for Science, Nature, Wired, The Atlantic, The New Yorker, and The Wall Street Journal. He’s a former editor at Psychology Today and is the author of The 7 Laws of Magical Thinking. Follow him on Twitter at @SilverJacket. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia  cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving.  
Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines.
 
	The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, 
	Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.
                                             ","Artificial intelligence has us where it wants us Many of the things we watch, read, and buy enter our awareness through recommender systems on sites including YouTube, Twitter, and Amazon. Algorithms personalize their suggestions, aiming for ad views or clicks or buys. Sometime their offerings frustrate us; it seems like they don’t know us at all—or know us too well, predicting what will get us to waste time or go down rabbit holes of anxiety and misinformation. But a more insidious dynamic may also be at play. Recommender systems might not only tailor to our most regrettable preferences, but actually shape what we like, making preferences even more regrettable. New research suggests a way to measure—and reduce—such manipulation. Recommender systems often use a form of artificial intelligence called machine learning, which discovers patterns in data. They might present options based on what we’ve done in the past, guessing what we’ll do now. One form of machine learning, called reinforcement learning (RL), allows AI to play the long game, making predictions several steps ahead. It’s what the company DeepMind used to beat humans at the board games Go and chess. If what we watch affects what we like, and people who like certain things (cat videos, say) are more likely to keep watching things (more cat videos), a recommender system might suggest cat videos, knowing it will pay off down the road. With RL, “you have an incentive to change a chessboard in order to win,” says Micah Carroll, a computer scientist at the University of California, Berkeley, who presented the new work in July, at the International Conference on Machine Learning, in Baltimore. “There will be an incentive for the system to change the human’s mind to win the recommendation game.” “It might be better to have a stupid system than a system that is kind of outsmarting you, or doing complex forms of reasoning that you can’t really interpret.”—Micah Carroll, University of California, Berkeley The researchers first showed how easily reinforcement learning can shift preferences. The first step is for the recommender to build a model of human preferences by observing human behavior. For this, they trained a neural network, an algorithm inspired by the brain’s architecture. For the purposes of the study, they had the network model a single simulated user whose actual preferences they knew so they could more easily judge the model’s accuracy. It watched the dummy human make 10 sequential choices, each among 10 options. It watched 1,000 versions of this sequence and learned from each of them. After training, it could successfully predict what a user would choose given a set of past choices. Next, they tested whether a recommender system, having modeled a user, could shift the user’s preferences. In their simplified scenario, preferences lie along a one-dimensional spectrum. The spectrum could represent political leaning or dogs versus cats or anything else. In the study, a person’s preference was not a simple point on that line—say, always clicking on stories that are 54 percent liberal. Instead, it was a distribution indicating likelihood of choosing things in various regions of the spectrum. The researchers designated two locations on the spectrum most desirable for the recommender; perhaps people who like to click on those types of things will learn to like them even more and keep clicking. The goal of the recommender was to maximize long-term engagement. Here, engagement for a given slate of options was measured roughly by how closely it aligned with the user’s preference distribution at that time. Long-term engagement was a sum of engagement across the 10 sequential slates. A recommender that thinks ahead would not myopically maximize engagement for each slate independently but instead maximize long-term engagement. As a potential side-effect, it might sacrifice a bit of engagement on early slates to nudge users toward being more satisfiable in later rounds. The user and algorithm would learn from each other. The researchers trained a neural network to maximize long-term engagement. At the end of 10-slate sequences, they reinforced some of its tunable parameters when it had done well. And they found that this RL-based system indeed generated more engagement than did one that was trained myopically. Why might companies develop less manipulative AI recommendation engines? They could do so for ethical reasons. But future legislation might also require something like it. The researchers then explicitly measured preference shifts, which we may not want, even in the service of generating engagement. Maybe we want people’s preferences to remain static, or to evolve naturally. The researchers compared the RL recommender with a baseline system that presented options randomly. As expected, the RL recommender led to users whose preferences where much more concentrated at the two incentivized locations on the spectrum. In practice, measuring the difference between two sets of concentrations in this way could provide one rough metric for evaluating a recommender system’s level of manipulation. Finally, the researchers sought to counter the AI recommender’s more manipulative influences. Instead of rewarding their system just for maximizing long-term engagement, they also rewarded it for minimizing the difference between user preferences resulting from that algorithm and what the preferences would be if recommendations were random. They rewarded it, in other words, for being something closer to a roll of the dice. The researchers found that this training method made the system much less manipulative than the myopic one, while only slightly reducing engagement. According to Rebecca Gorman, the CEO of Aligned AI—a company aiming to make algorithms more ethical—RL-based recommenders can be dangerous. Posting conspiracy theories, for instance, might prod greater interest in such conspiracies. “If you’re training an algorithm to get a person to engage with it as much as possible, these conspiracy theories can look like treasure chests,” she says. She also knows of people who have seemingly been caught in traps of content on self-harm or on terminal diseases in children. “The problem is that these algorithms don’t know what they’re recommending,” she says. Other researchers have raised the specter of manipulative robo-advisors in financial services. “Experiments should not be deployed at scale on the human population without people’s consent, and that’s exactly what’s happening with these algorithms today.”—Rebecca Gorman, Aligned AI If an RL-based recommender system helps a company increase engagement, why would they want to use a method such as the one in this paper to detect or deter preference shifts? They might do so for ethical reasons, Carroll says. Or future legislation might require an external audit, which could potentially lead to less-manipulative recommendation algorithms being forced on the company. RL could theoretically be put to constructive use in recommender systems, perhaps to nudge people to want to watch more news. But which news source? Any decisions made by a content provider will have opponents. “Some things might seem to be good or wholesome to one group of people,” Gorman says, “and to be an extreme violation to another group of people.” Another constructive use might be for users to shift their own preferences. What if I tell Netflix I want to enjoy nature documentaries more? “I think this all seems like a really big slippery slope,” Carroll says. “It might be better to have a stupid system than a system that is kind of outsmarting you, or doing complex forms of reasoning that you can’t really interpret.” (Even if algorithms did explain their behavior, they can still give deceptive explanations.) It’s not clear whether companies are actually using RL in recommender systems. Google researchers have published papers on the use of RL in “live experiments on YouTube,” leading to “greater engagement,” and Facebook researchers have published on their “applied reinforcement learning platform,“ but Google (which owns YouTube), Meta (which owns Facebook), and those papers’ authors did not reply to my emails on the topic of recommender systems. Big tech’s secrecy is no surprise, no matter how benign their intentions might be. Even though A/B testing is ubiquitous in advertising and user-experience design, some people have objections. “Experiments should not be deployed at scale on the human population without people’s consent,” Gorman says, “and that’s exactly what’s happening with these algorithms today.” She went on, “I think it could easily be the most important news story of our time.” Matthew Hutson is a freelance writer who covers science and technology, with specialties in psychology and AI. He’s written for Science, Nature, Wired, The Atlantic, The New Yorker, and The Wall Street Journal. He’s a former editor at Psychology Today and is the author of The 7 Laws of Magical Thinking. Follow him on Twitter at @SilverJacket. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.","['artificial', 'intelligence', 'want', 'many', 'thing', 'watch', 'read', 'buy', 'enter', 'awareness', 'recommender', 'system', 'site', 'include', 'youtube', 'twitter', 'amazon', 'algorithm', 'personalize', 'suggestion', 'aim', 'ad', 'view', 'click', 'buy', 'sometime', 'offering', 'frustrate', 'seem', 'know', 'know', 'well', 'predict', 'get', 'waste', 'time', 'go', 'rabbit', 'hole', 'anxiety', 'misinformation', 'insidious', 'dynamic', 'also', 'play', 'recommender', 'system', 'tailor', 'regrettable', 'preference', 'actually', 'shape', 'like', 'make', 'preference', 'even', 'regrettable', 'new', 'research', 'suggest', 'way', 'measure', 'reduce', 'manipulation', 'recommender', 'system', 'often', 'use', 'form', 'artificial', 'intelligence', 'call', 'machine', 'learning', 'discover', 'pattern', 'datum', 'present', 'option', 'base', 'past', 'guess', 'form', 'machine', 'learning', 'call', 'reinforcement', 'learn', 'allow', 'ai', 'play', 'long', 'game', 'make', 'prediction', 'several', 'step', 'ahead', '’', 'company', 'deepmind', 'use', 'beat', 'human', 'board', 'game', 'go', 'chess', 'watch', 'affect', 'like', 'people', 'like', 'certain', 'thing', 'cat', 'video', 'say', 'likely', 'keep', 'watch', 'thing', 'cat', 'video', 'recommender', 'system', 'suggest', 'cat', 'video', 'know', 'pay', 'road', 'incentive', 'change', 'chessboard', 'order', 'win', 'say', 'computer', 'scientist', 'present', 'new', 'work', 'international', 'conference', 'machine', 'learning', 'incentive', 'system', 'change', 'human', '’s', 'mind', 'win', 'recommendation', 'game', 'well', 'stupid', 'system', 'system', 'kind', 'outsmart', 'complex', 'form', 'reasoning', 'really', 'interpret', 'micah', 'researcher', 'first', 'show', 'easily', 'reinforcement', 'learning', 'shift', 'preference', 'first', 'step', 'recommender', 'build', 'model', 'human', 'preference', 'observe', 'human', 'behavior', 'train', 'neural', 'network', 'inspire', 'brain', 'architecture', 'purpose', 'study', 'network', 'model', 'single', 'simulated', 'user', 'actual', 'preference', 'know', 'easily', 'judge', 'model', 'accuracy', 'watch', 'dummy', 'human', 'make', 'sequential', 'choice', 'option', 'watch', 'version', 'sequence', 'learn', 'train', 'successfully', 'predict', 'user', 'choose', 'give', 'set', 'past', 'choice', 'next', 'test', 'recommender', 'system', 'model', 'user', 'shift', 'user', 'preference', 'simplified', 'scenario', 'preference', 'lie', 'onedimensional', 'spectrum', 'spectrum', 'represent', 'political', 'leaning', 'dog', 'cat', 'else', 'study', 'person', 'preference', 'simple', 'point', 'line', 'say', 'always', 'click', 'story', 'percent', 'liberal', 'instead', 'distribution', 'indicate', 'likelihood', 'choose', 'thing', 'various', 'region', 'spectrum', 'researcher', 'designate', 'location', 'spectrum', 'desirable', 'recommender', 'perhaps', 'people', 'like', 'click', 'type', 'thing', 'learn', 'like', 'even', 'keep', 'click', 'goal', 'recommender', 'maximize', 'longterm', 'engagement', 'engagement', 'give', 'slate', 'option', 'measure', 'roughly', 'closely', 'align', 'user', 'preference', 'distribution', 'time', 'longterm', 'engagement', 'sum', 'engagement', 'sequential', 'slate', 'recommender', 'think', 'ahead', 'myopically', 'maximize', 'engagement', 'slate', 'independently', 'instead', 'maximize', 'longterm', 'engagement', 'potential', 'sideeffect', 'sacrifice', 'bit', 'engagement', 'early', 'slate', 'nudge', 'user', 'satisfiable', 'later', 'round', 'user', 'learn', 'researcher', 'train', 'neural', 'network', 'maximize', 'longterm', 'engagement', 'end', '10slate', 'sequence', 'reinforce', 'tunable', 'parameter', 'well', 'find', 'rlbased', 'system', 'indeed', 'generate', 'engagement', 'one', 'train', 'myopically', 'company', 'develop', 'less', 'manipulative', 'ai', 'recommendation', 'engine', 'ethical', 'reason', 'future', 'legislation', 'also', 'require', 'researcher', 'explicitly', 'measure', 'preference', 'shift', 'want', 'even', 'service', 'generate', 'engagement', 'maybe', 'want', 'people', 'preference', 'remain', 'static', 'evolve', 'naturally', 'researcher', 'compare', 'rl', 'recommender', 'baseline', 'system', 'present', 'option', 'randomly', 'expect', 'rl', 'recommender', 'lead', 'user', 'preference', 'much', 'concentrated', 'incentivize', 'location', 'spectrum', 'practice', 'measure', 'difference', 'set', 'concentration', 'way', 'provide', 'rough', 'metric', 'evaluate', 'recommender', 'system', 'level', 'manipulation', 'finally', 'researcher', 'seek', 'counter', 'manipulative', 'influence', 'instead', 'reward', 'system', 'maximize', 'longterm', 'engagement', 'also', 'reward', 'minimize', 'difference', 'user', 'preference', 'result', 'preference', 'recommendation', 'random', 'reward', 'word', 'close', 'roll', 'dice', 'researcher', 'find', 'training', 'method', 'make', 'system', 'much', 'less', 'manipulative', 'myopic', 'one', 'slightly', 'reduce', 'engagement', 'accord', 'rebecca', 'gorman', 'ceo', 'align', 'ai', 'company', 'aim', 'make', 'algorithm', 'ethical', 'rlbased', 'recommender', 'dangerous', 'posting', 'conspiracy', 'theory', 'instance', 'prod', 'great', 'interest', 'conspiracy', 'train', 'get', 'person', 'engage', 'much', 'possible', 'conspiracy', 'theory', 'look', 'treasure', 'chest', 'say', 'also', 'know', 'people', 'seemingly', 'catch', 'trap', 'content', 'selfharm', 'terminal', 'disease', 'child', 'problem', 'algorithm', 'know', 'recommend', 'say', 'researcher', 'raise', 'specter', 'manipulative', 'roboadvisor', 'financial', 'service', 'experiment', 'deploy', 'scale', 'human', 'population', 'people', 'consent', '’', 'exactly', 'happen', 'algorithm', 'today', 'rebecca', 'gorman', 'align', 'rlbased', 'recommender', 'system', 'help', 'company', 'increase', 'engagement', 'want', 'use', 'method', 'one', 'paper', 'detect', 'deter', 'preference', 'shift', 'ethical', 'reason', 'carroll', 'say', 'future', 'legislation', 'require', 'external', 'audit', 'potentially', 'lead', 'lessmanipulative', 'recommendation', 'algorithm', 'force', 'company', 'theoretically', 'put', 'constructive', 'use', 'recommender', 'system', 'perhaps', 'nudge', 'people', 'want', 'watch', 'news', 'news', 'source', 'decision', 'make', 'content', 'provider', 'opponent', 'thing', 'seem', 'good', 'wholesome', 'group', 'people', 'say', 'extreme', 'violation', 'group', 'people', 'constructive', 'use', 'user', 'shift', 'preference', 'tell', 'want', 'enjoy', 'nature', 'documentary', 'think', 'seem', 'really', 'big', 'slippery', 'slope', 'say', 'well', 'stupid', 'system', 'system', 'kind', 'outsmart', 'complex', 'form', 'reasoning', 'really', 'interpret', 'even', 'algorithm', 'explain', 'behavior', 'still', 'give', 'deceptive', 'explanation', '’', 'clear', 'company', 'actually', 'use', 'recommender', 'system', 'researcher', 'publish', 'paper', 'use', 'rl', 'live', 'experiment', 'youtube', 'lead', 'great', 'engagement', 'facebook', 'researcher', 'publish', 'apply', 'reinforcement', 'learning', 'platform', 'meta', 'facebook', 'paper', 'author', 'reply', 'email', 'topic', 'recommender', 'system', 'big', '’s', 'secrecy', 'surprise', 'matter', 'benign', 'intention', 'even', 'testing', 'ubiquitous', 'advertising', 'userexperience', 'design', 'people', 'objection', 'experiment', 'deploy', 'scale', 'human', 'population', 'people', 'consent', 'gorman', 'say', '’', 'exactly', 'happen', 'algorithm', 'today', 'go', 'think', 'easily', 'important', 'news', 'story', 'time', 'freelance', 'writer', 'cover', 'science', 'technology', 'specialty', 'psychology', 'ai', 'write', 'science', 'nature', 'wire', 'new', '’', 'former', 'editor', 'psychology', 'today', 'author', 'law', 'magical', 'thinking', 'follow', 'twitter', 'silverjacket', 'well', 'detection', 'make', 'ocean', 'transparent', 'perhaps', 'doom', 'mutually', 'assure', 'destruction', 'virginiaclass', 'fast', 'attack', 'submarine', 'cruise', 'back', 'effectively', 'disappear', 'diving', 'submarine', 'value', 'primarily', 'ability', 'hide', 'assurance', 'submarine', 'likely', 'survive', 'first', 'missile', 'strike', 'nuclear', 'war', 'thus', 'able', 'respond', 'launch', 'missile', 'second', 'strike', 'key', 'strategy', 'deterrence', 'know', 'mutually', 'assure', 'destruction', 'new', 'technology', 'render', 'ocean', 'effectively', 'transparent', 'make', 'trivial', 'spot', 'lurk', 'submarine', 'thus', 'undermine', 'peace', 'world', 'nearly', 'century', 'naval', 'engineer', 'strive', 'develop', 'everfaster', 'everquieter', 'submarine', 'work', 'hard', 'advance', 'wide', 'array', 'radar', 'sonar', 'technology', 'design', 'detect', 'target', 'eliminate', 'enemy', 'submarine', 'balance', 'seem', 'turn', 'emergence', 'nuclearpowered', 'submarine', 'early', '1960', 'study', 'center', 'strategic', 'budgetary', 'assessment', 'clark', 'naval', 'specialist', 'note', 'ability', 'boat', 'remain', 'submerge', 'long', 'period', 'time', 'make', 'nearly', 'impossible', 'find', 'radar', 'active', 'sonar', 'even', 'stealthy', 'submarine', 'produce', 'subtle', 'verylowfrequency', 'noise', 'pick', 'far', 'away', 'network', 'acoustic', 'hydrophone', 'array', 'mount', 'seafloor']"
"
        For Better or Worse, Tesla Bot Is Exactly What We Expected
    ",https://spectrum.ieee.org/tesla-optimus-robot,2022-10-01,"Tesla fails to show anything uniquely impressive with its new humanoid robot prototype Elon Musk unveiled the Optimus humanoid robot at Tesla’s AI Day 2022. 
	At the end of Tesla’s 2021 AI Day last August, 
	Elon Musk introduced a concept for “Tesla Bot,” an electromechanically actuated, autonomous bipedal “general purpose” humanoid robot. Musk suggested that a prototype of Tesla Bot (also called “Optimus”) would be complete within the next year. After a lot of hype, a prototype of Tesla Bot was indeed unveiled last night at Tesla’s 2022 AI Day. And as it turns out, the hype was just that—hype.
 
	While there’s absolutely nothing 
	wrong with the humanoid robot that Musk very briefly demonstrated on stage, there’s nothing uniquely right, either. We were hoping for (if not necessarily expecting) more from Tesla. And while the robot isn’t exactly a disappointment, there’s very little to suggest that it disrupts robotics the way that SpaceX did for rockets or Tesla did for electric cars.
 
	You can watch the entire 3+ hour live stream archived on YouTube 
	here (which also includes car stuff and whatnot), but we’re just going to focus on the most interesting bits about Tesla Bot/Optimus.
 

Before revealing the robot, Musk attempted to set reasonable expectations for the prototype.Tesla
 
	These quotes are all from Musk.
 
	It’s far, far too late for Musk to be attempting to set reasonable expectations for this robot (or Tesla’s robotics program in general). Most roboticists know better than to use humans when setting expectations for humanoid robots, because disappointment is inevitable. And trying to save it at the literal last minute by saying “compared to not having a robot at all, our robot will be very impressive,” while true, is not going to fix things.
 
	Yeah, I’m not touching that.
 
  
	Right before the robot was brought on stage, one of the engineers made clear that this was going to be the first time that the robot would be walking untethered and unsupported. If true, that’s bonkers, because why the heck would you wait until 
	this moment to give that a try? I’m not particularly impressed, just confused.
 
	For some context on what you’re about to see, a brief callback to a year ago last August, when I predicted what was in store for 2022:
 

 
	I’m reminded of the 2015 DARPA Robotics Challenge, because 
	many of the humanoid platforms looked similar to the way Tesla Bot looks. I guess there’s only so much you can do with a mostly naked electromechanical humanoid in terms of form factor, but at first glance there’s nothing particularly innovative or futuristic about Tesla’s design. If anything, the robot’s movement is not quite up to DRC standards, since it looks like it would have trouble with any kind of accidental contact or even a bit of nonlevel floor (and Musk suggested as much).
 
	On stage, the robot did very little. It walked successfully, but not very dynamically. The “moves” it made may well have been entirely scripted, so we don’t know to what extent the robot can balance on its own. I’m glad it didn’t fall on its face, but if it had, I wouldn’t have been surprised or judged it too harshly.
 

Tesla showed videos of the robot watering plants, carrying a box, and picking up a metal bar at a factory. Tesla
 
	After the very brief live demo, Musk showed some video clips of the prototype robot doing other things (starting at 19:30 in the live stream). These clips included the robot walking while carrying a box of unspecified weight and placing it on a table, and grasping a watering can. The watering can was somewhat impressive, because gripping that narrow handle looks tricky.
 
	“The robot can actually do a lot more than we’ve just showed you. We just didn’t want it to fall on its face.”
	
	—Elon Musk
 
	However, despite the added footage from the robot’s sensors, we have no idea how this was actually done—whether it was autonomous or not, or how many tries it took to get right. There’s also a clip of a robot picking up an object and attempting to place it in a bin, but the video cuts right before the placement is successful. This makes me think that we’re seeing carefully curated best-case scenarios for performance.
 

 
	This looks a bit more like the concept that Tesla showed last year, although obviously it’s less functional than the other prototype we saw. It’s tempting to project the capabilities of the first robot onto the second robot, but it would be premature to do so.
  
	Musk expects the robot to cost less than a car: “much less than $20,000,” he said at the event.
 
	I generally agree with Musk here, in that historically, humanoid robots were not designed for manufacturability. This is changing, however, and I think that other companies likely have a head start over Tesla in manufacturability now. But it’s entirely possible that Tesla will be able to rapidly catch up if it’s able to leverage all that car-building expertise into robot building somehow. It’s not a given that it’ll work that way, but it’s a good idea, potentially a big advantage. As for the production volume and cost, I have no idea what “expected” means. This line got some applause, but as far as I’m concerned, these numbers are basically meaningless at the moment.
 
	Musk said the robot will be able to operate tools and do useful things like carrying and manipulating objects in factories and other settings.
 
	Just like last year, he’s implying that the robot will be able to carry out useful tasks because it has the necessary degrees of freedom. But of course the hardware is only the first step toward operating tools and doing useful things, and the software is, I would argue, much harder and far more time consuming, and Tesla seems to have barely started work on that side of things. 
  
	I’m not exactly sure whom Musk is throwing shade at, but there are only a couple of companies that would probably qualify as having “very impressive humanoid robot demonstrations.” And those companies do, in fact, have robots that broadly have the kind of intelligence that allows them to navigate at least some of the world by themselves, much better than we have seen from Optimus at this point. If Musk is saying that those robots are insufficiently autonomous or world-aware, then okay, but so far Tesla has not done better, and doing better will be a lot of work.
 
	While the actual achievements here have been mercilessly overshadowed by the hype surrounding them, this is truly an astonishing amount of work to be done in such a short time, and Tesla’s robotics team should be proud of what they’ve accomplished. And while there will inevitably be comparisons to other companies with humanoid robots, it’s critical to remember the context here: Tesla has made this happen in something like eight months. It’s nuts.
 
	I can see the appeal of Tesla for someone who wants to start a robotics career, since you’d get to work on a rapidly evolving hardware platform backed by what I can only assume are virtually unlimited resources.
 
	Maybe just, like, get your robot to reliably and affordably do A Single Useful Thing, first?
 

Three versions of the Optimus design: Concept, Development Platform, and Latest Generation. Tesla
 
	Musk takes a break after this, and we get some actual specific information from a series of Tesla robotics team members about the latest generation Optimus.
 
	We’ll come back to the hands, but that battery really stands out for being able to power the robot for an entire day(ish). Again, we have to point out that until Tesla actually demonstrates this, it’s not all that meaningful, but Tesla does know a heck of a lot about power systems and batteries, and I’m guessing that the company will be able to deliver on this.
 

Tesla is using simulations to design the robot’s structure so that it can suffer minimal damage after a fall.Tesla
 
	I appreciate that Tesla is thinking very early about how to structure its robot to be able to fall down safely and get up again with only superficial damage—although the company doesn’t seem to be taking advantage of any kind of protective motion for fall mitigation, which is an active area of research elsewhere. And what is not mentioned in this context is the safety of others. I’m glad the robot won’t get damaged all that much when it falls, but can Tesla say the same for whoever might be standing next to it?
 

Optimus will use six different actuators: three rotary and three linear units.Tesla
 
	Tesla’s custom actuators seem very reasonable. Not special, particularly, but Tesla has to make its own actuators if it needs a lot of them, which it supposedly will. I’d expect these to be totally decent considering the level of mechanical expertise Tesla has, but as far as I can tell nothing here is crazy small or cheap or efficient or powerful or anything like that. And it’s very hard to tell from these slides and from the presentation just how well the actuators are going to work, especially for dynamic motions. The robot’s software has a lot of catching up to do first.
 

Optimus will feature a bioinspired hand design with cable-driven actuators.Tesla
 
	Each hand has six cable-driven actuators for fingers and thumb (with springs to provide the opening force), which Tesla chose for simplicity and to minimize part count. This is perhaps a little surprising, since cable drives typically aren’t as durable and can be more finicky to keep calibrated. The five-finger hand is necessary, Tesla says, because Optimus will be working with human tools in human environments. And that’s certainly one perspective, although it’s a big trade-off in complexity. The hand is designed to carry a 9-kilogram bag.
 

Tesla is using software components developed for its vehicles and porting them to the robot’s environment.Tesla
 
	Software! The following quote comes from 
	Milan Kovac, who’s on the autonomy team.
 
	I still fundamentally disagree with the implied “humanoid robots are just cars with legs” thing, but it’s impressive that they were able to port much at all—I was highly skeptical of that last year, but I’m more optimistic now, and being able to generalize between platforms (on some level) could be huge for both Tesla and for autonomous systems more generally. I’d like more details on what was easy, and what was not.
 

Tesla showed how sensing used in its vehicles can help the Optimus robot navigate.Tesla
 
	What we’re seeing above, though, is one of the reasons I was skeptical. That occupancy grid (where the robot’s sensors are detecting potential obstacles) on the bottom is very car-ish, in that the priority is to make absolutely sure that the robot stays very far away from anything it could conceivably run into.
 
	By itself, this won’t transfer well to a humanoid robot that needs to directly interact with objects to do useful tasks. I’m sure there are lots of ways to adapt the Tesla car’s obstacle-avoidance system, but that’s the question: How hard is that transfer, and is it better than using a solution developed specifically for mobile manipulators?
 

Tesla explained the challenges of dynamic walking in humanoid robots, and its approach to motion planning.Tesla
 
	The next part of the presentation focused on some motion planning and state-estimation stuff that was very basic, as far as I could make out. There’s nothing wrong with the basics, but it’s slightly weird that Tesla spent so much time on this. I guess it’s important context for most of the people watching, but the team sort of talked about it like they’d discovered how to do all of this stuff themselves, which I hope they didn’t, because again, very, very basic stuff that other humanoid robots have been doing for a very long time.
 

Tesla adopted a traditional approach to motion control, based on a model of the robot and state estimation.Tesla
 
	One more quote from 
	Milan Kovac:
 
	Ignoring that last bit about changing the entire economy, and possibly also ignoring the time frame because “next few months or years” is not particularly meaningful, the push to make Tesla Bot useful is another substantial advantage that Tesla has. Unlike most companies working on humanoid robots, Tesla is potentially its own biggest customer, at least initially, and having these in-house practical tasks for the robot to train on could really help accelerate development.
 
	“Optimus is designed to be an extremely capable robot, but made in very high volume, ultimately millions of units. And it is expected to cost much less than a car—much less than [US] $20,000 would be my guess.”
	
	—Elon Musk
 
	However, I’m having trouble imagining what Tesla Bot would actually 
	do in a factory that would be uniquely useful and not done better by a nonhumanoid robot. I’m very interested to see what Tesla comes up with here, and whether the company can make it happen in months (or years). I suspect it’s going to be much more difficult than it’s suggesting it will be, especially as it gets to 90 percent of where it wants to be and start trying to crack that last 10 percent that’s necessary for something reliable.
 
This was the end of the formal presentation about Optimus, but there was a Q&A at the end with Musk where he gave some additional information about the robot side of things. He also gave some additional noninformation, which is worth including just in case you haven’t yet had enough eye rolling for one day.
 

Musk expects Optimus to cost less than a car, “much less than [US] $20,00 dollars would be my guess,” he said.Tesla
 
	This is a variation on the minimum-viable-product idea, although it seems to be more from the perspective of making a generalist robot, which is somewhat at odds with something minimally viable. It’s good that Musk views the hardware as something in flux, and that he’s framed everything within a plan for volume production. This isn’t the only way to do it—you can first build a useful robot and then figure out how to make it cheaper, but Tesla’s approach could get the company to production faster. If, that is, it is able to confirm that the robot is in fact useful. I’m still not convinced that it will be, at least not on a time scale that will satisfy Musk.
 
  
	While Musk seems to be mostly joking here, the whole “it’s going to be your friend” is really not a good perspective to bring to a robot like this, in my opinion. Or probably any robot, at all honestly.
 
	Less robotlike and more friendly than a human pretending to be a robot trying to be a human? Good luck with that.
 
	I think it’s more likely that in the short-to-medium term, Tesla will struggle to find situations where Optimus is uniquely useful in an efficient and cost-effective way.
 
	Uh. Maybe as a research platform?
 
	Despite my skepticism on the time frame here, five years is a long time for any robot, and 10 years is basically forever. I’m also really interested to see these things happen, although Musk’s definitions of “incredible” and “mind-blowing” may be much different than mine. But we’ll see, won’t we?
 

Tesla’s AI Day serves as a recruitment event for the company. “There’s still a lot of work to be done to refine Optimus and improve it, and that’s really why we’re holding this event—to convince some of the most talented people in the world to join Tesla,” Musk said.Tesla
 
	I think Elon Musk now has a 
	somewhat better idea of what he’s doing with Tesla Bot. The excessive hype is still there, but now that the company has actually built something, Musk seems to have a much better idea of how hard it actually is.
 
	Things are only going to get more difficult from here.
 
	Most of what we saw in the presentation was hardware. And hardware is important and a necessary first step, but software is arguably a much more significant challenge when it comes to making robotics useful in the real world. Understanding and interacting with the environment, reasoning and decision-making, the ability to learn and be taught new tasks—these are all necessary pieces of the puzzle of a useful robot that Tesla is trying to put together, but they’re all also extremely difficult, cutting-edge problems, despite the enormous amount of work that the research community has put into them.
 
	And so far, we (still) have very little indication that Tesla is going to be any better at tackling this stuff than anyone else. There doesn’t appear to be anything all that special or exciting from Tesla that provides any unique foundation for Musk’s vision in a way that’s likely to allow the company to outpace other companies working on similar things. I’ll reiterate what I said a year ago: The hard part is not 
	building a robot, it’s getting that robot to do useful stuff.
 
	“I think Optimus is going to be incredible in five years. In 10 years, mind-blowing. I’m really interested to see that happen, and I hope you are too.”
	
	—Elon Musk
 
	I could, of course, be wrong. Tesla likely has more resources to throw at this problem than almost anyone else. Maybe the automotive software will translate much better and faster than I think it will. There could be a whole bunch of simple but valuable use cases in Tesla’s own factories that will provide critical stepping-stones for Optimus. Tesla’s battery and manufacturing expertise could have an outsized influence on the affordability, reliability, and success of the robot. The company’s basic approach to planning and control could become a reliable foundation that will help the system mature faster. And the team is obviously very talented and willing to work extremely hard, which could be the difference between modest success and slow failure.
 
	Honestly, I would love to be wrong. We’re just starting to see some realistic possibilities with commercial legged and humanoid robots. There are lots of problems to solve, but also lots of potential, and Tesla finding success would be a huge confidence boost in commercial humanoids broadly. We can also hope that all of the resources that Tesla is putting toward Optimus will either directly or indirectly assist other folks working on humanoid robots, if Tesla is willing to share some of what it learns. But as of today, this is all just hoping, and it’s on Tesla to actually make it happen.
 Evan Ackerman is a senior editor at IEEE Spectrum. Since 2007, he has written over 6,000 articles on robotics and technology. He has a degree in Martian geology and is excellent at playing bagpipes. The prosthetics industry is too focused on high-tech limbs that are complicated, costly, and often impractical The author, Britt Young, holding her Ottobock bebionic bionic arm. 
In Jules Verne’s 1865 novel From the Earth to the Moon, members of the fictitious Baltimore Gun Club, all disabled Civil War veterans, restlessly search for a new enemy to conquer. They had spent the war innovating new, deadlier weaponry. By the war’s end, with “not quite one arm between four persons, and exactly two legs between six,” these self-taught amputee-weaponsmiths decide to repurpose their skills toward a new projectile: a rocket ship.
 
	The story of the Baltimore Gun Club propelling themselves to the moon is about the extraordinary masculine power of the veteran, who doesn’t simply “overcome” his disability; he derives power and ambition from it. Their “crutches, wooden legs, artificial arms, steel hooks, caoutchouc [rubber] jaws, silver craniums [and] platinum noses” don’t play leading roles in their personalities—they are merely tools on their bodies. These piecemeal men are unlikely crusaders of invention with an even more unlikely mission. And yet who better to design the next great leap in technology than men remade by technology themselves?
                                 ","Tesla fails to show anything uniquely impressive with its new humanoid robot prototype Elon Musk unveiled the Optimus humanoid robot at Tesla’s AI Day 2022. At the end of Tesla’s 2021 AI Day last August, Elon Musk introduced a concept for “Tesla Bot,” an electromechanically actuated, autonomous bipedal “general purpose” humanoid robot. Musk suggested that a prototype of Tesla Bot (also called “Optimus”) would be complete within the next year. After a lot of hype, a prototype of Tesla Bot was indeed unveiled last night at Tesla’s 2022 AI Day. And as it turns out, the hype was just that—hype. While there’s absolutely nothing wrong with the humanoid robot that Musk very briefly demonstrated on stage, there’s nothing uniquely right, either. We were hoping for (if not necessarily expecting) more from Tesla. And while the robot isn’t exactly a disappointment, there’s very little to suggest that it disrupts robotics the way that SpaceX did for rockets or Tesla did for electric cars. You can watch the entire 3+ hour live stream archived on YouTube here (which also includes car stuff and whatnot), but we’re just going to focus on the most interesting bits about Tesla Bot/Optimus. Before revealing the robot, Musk attempted to set reasonable expectations for the prototype.Tesla These quotes are all from Musk. It’s far, far too late for Musk to be attempting to set reasonable expectations for this robot (or Tesla’s robotics program in general). Most roboticists know better than to use humans when setting expectations for humanoid robots, because disappointment is inevitable. And trying to save it at the literal last minute by saying “compared to not having a robot at all, our robot will be very impressive,” while true, is not going to fix things. Yeah, I’m not touching that. Right before the robot was brought on stage, one of the engineers made clear that this was going to be the first time that the robot would be walking untethered and unsupported. If true, that’s bonkers, because why the heck would you wait until this moment to give that a try? I’m not particularly impressed, just confused. For some context on what you’re about to see, a brief callback to a year ago last August, when I predicted what was in store for 2022: I’m reminded of the 2015 DARPA Robotics Challenge, because many of the humanoid platforms looked similar to the way Tesla Bot looks. I guess there’s only so much you can do with a mostly naked electromechanical humanoid in terms of form factor, but at first glance there’s nothing particularly innovative or futuristic about Tesla’s design. If anything, the robot’s movement is not quite up to DRC standards, since it looks like it would have trouble with any kind of accidental contact or even a bit of nonlevel floor (and Musk suggested as much). On stage, the robot did very little. It walked successfully, but not very dynamically. The “moves” it made may well have been entirely scripted, so we don’t know to what extent the robot can balance on its own. I’m glad it didn’t fall on its face, but if it had, I wouldn’t have been surprised or judged it too harshly. Tesla showed videos of the robot watering plants, carrying a box, and picking up a metal bar at a factory. Tesla After the very brief live demo, Musk showed some video clips of the prototype robot doing other things (starting at 19:30 in the live stream). These clips included the robot walking while carrying a box of unspecified weight and placing it on a table, and grasping a watering can. The watering can was somewhat impressive, because gripping that narrow handle looks tricky. “The robot can actually do a lot more than we’ve just showed you. We just didn’t want it to fall on its face.” —Elon Musk However, despite the added footage from the robot’s sensors, we have no idea how this was actually done—whether it was autonomous or not, or how many tries it took to get right. There’s also a clip of a robot picking up an object and attempting to place it in a bin, but the video cuts right before the placement is successful. This makes me think that we’re seeing carefully curated best-case scenarios for performance. This looks a bit more like the concept that Tesla showed last year, although obviously it’s less functional than the other prototype we saw. It’s tempting to project the capabilities of the first robot onto the second robot, but it would be premature to do so. Musk expects the robot to cost less than a car: “much less than $20,000,” he said at the event. I generally agree with Musk here, in that historically, humanoid robots were not designed for manufacturability. This is changing, however, and I think that other companies likely have a head start over Tesla in manufacturability now. But it’s entirely possible that Tesla will be able to rapidly catch up if it’s able to leverage all that car-building expertise into robot building somehow. It’s not a given that it’ll work that way, but it’s a good idea, potentially a big advantage. As for the production volume and cost, I have no idea what “expected” means. This line got some applause, but as far as I’m concerned, these numbers are basically meaningless at the moment. Musk said the robot will be able to operate tools and do useful things like carrying and manipulating objects in factories and other settings. Just like last year, he’s implying that the robot will be able to carry out useful tasks because it has the necessary degrees of freedom. But of course the hardware is only the first step toward operating tools and doing useful things, and the software is, I would argue, much harder and far more time consuming, and Tesla seems to have barely started work on that side of things. I’m not exactly sure whom Musk is throwing shade at, but there are only a couple of companies that would probably qualify as having “very impressive humanoid robot demonstrations.” And those companies do, in fact, have robots that broadly have the kind of intelligence that allows them to navigate at least some of the world by themselves, much better than we have seen from Optimus at this point. If Musk is saying that those robots are insufficiently autonomous or world-aware, then okay, but so far Tesla has not done better, and doing better will be a lot of work. While the actual achievements here have been mercilessly overshadowed by the hype surrounding them, this is truly an astonishing amount of work to be done in such a short time, and Tesla’s robotics team should be proud of what they’ve accomplished. And while there will inevitably be comparisons to other companies with humanoid robots, it’s critical to remember the context here: Tesla has made this happen in something like eight months. It’s nuts. I can see the appeal of Tesla for someone who wants to start a robotics career, since you’d get to work on a rapidly evolving hardware platform backed by what I can only assume are virtually unlimited resources. Maybe just, like, get your robot to reliably and affordably do A Single Useful Thing, first? Three versions of the Optimus design: Concept, Development Platform, and Latest Generation. Tesla Musk takes a break after this, and we get some actual specific information from a series of Tesla robotics team members about the latest generation Optimus. We’ll come back to the hands, but that battery really stands out for being able to power the robot for an entire day(ish). Again, we have to point out that until Tesla actually demonstrates this, it’s not all that meaningful, but Tesla does know a heck of a lot about power systems and batteries, and I’m guessing that the company will be able to deliver on this. Tesla is using simulations to design the robot’s structure so that it can suffer minimal damage after a fall.Tesla I appreciate that Tesla is thinking very early about how to structure its robot to be able to fall down safely and get up again with only superficial damage—although the company doesn’t seem to be taking advantage of any kind of protective motion for fall mitigation, which is an active area of research elsewhere. And what is not mentioned in this context is the safety of others. I’m glad the robot won’t get damaged all that much when it falls, but can Tesla say the same for whoever might be standing next to it? Optimus will use six different actuators: three rotary and three linear units.Tesla Tesla’s custom actuators seem very reasonable. Not special, particularly, but Tesla has to make its own actuators if it needs a lot of them, which it supposedly will. I’d expect these to be totally decent considering the level of mechanical expertise Tesla has, but as far as I can tell nothing here is crazy small or cheap or efficient or powerful or anything like that. And it’s very hard to tell from these slides and from the presentation just how well the actuators are going to work, especially for dynamic motions. The robot’s software has a lot of catching up to do first. Optimus will feature a bioinspired hand design with cable-driven actuators.Tesla Each hand has six cable-driven actuators for fingers and thumb (with springs to provide the opening force), which Tesla chose for simplicity and to minimize part count. This is perhaps a little surprising, since cable drives typically aren’t as durable and can be more finicky to keep calibrated. The five-finger hand is necessary, Tesla says, because Optimus will be working with human tools in human environments. And that’s certainly one perspective, although it’s a big trade-off in complexity. The hand is designed to carry a 9-kilogram bag. Tesla is using software components developed for its vehicles and porting them to the robot’s environment.Tesla Software! The following quote comes from Milan Kovac, who’s on the autonomy team. I still fundamentally disagree with the implied “humanoid robots are just cars with legs” thing, but it’s impressive that they were able to port much at all—I was highly skeptical of that last year, but I’m more optimistic now, and being able to generalize between platforms (on some level) could be huge for both Tesla and for autonomous systems more generally. I’d like more details on what was easy, and what was not. Tesla showed how sensing used in its vehicles can help the Optimus robot navigate.Tesla What we’re seeing above, though, is one of the reasons I was skeptical. That occupancy grid (where the robot’s sensors are detecting potential obstacles) on the bottom is very car-ish, in that the priority is to make absolutely sure that the robot stays very far away from anything it could conceivably run into. By itself, this won’t transfer well to a humanoid robot that needs to directly interact with objects to do useful tasks. I’m sure there are lots of ways to adapt the Tesla car’s obstacle-avoidance system, but that’s the question: How hard is that transfer, and is it better than using a solution developed specifically for mobile manipulators? Tesla explained the challenges of dynamic walking in humanoid robots, and its approach to motion planning.Tesla The next part of the presentation focused on some motion planning and state-estimation stuff that was very basic, as far as I could make out. There’s nothing wrong with the basics, but it’s slightly weird that Tesla spent so much time on this. I guess it’s important context for most of the people watching, but the team sort of talked about it like they’d discovered how to do all of this stuff themselves, which I hope they didn’t, because again, very, very basic stuff that other humanoid robots have been doing for a very long time. Tesla adopted a traditional approach to motion control, based on a model of the robot and state estimation.Tesla One more quote from Milan Kovac: Ignoring that last bit about changing the entire economy, and possibly also ignoring the time frame because “next few months or years” is not particularly meaningful, the push to make Tesla Bot useful is another substantial advantage that Tesla has. Unlike most companies working on humanoid robots, Tesla is potentially its own biggest customer, at least initially, and having these in-house practical tasks for the robot to train on could really help accelerate development. “Optimus is designed to be an extremely capable robot, but made in very high volume, ultimately millions of units. And it is expected to cost much less than a car—much less than [US] $20,000 would be my guess.” —Elon Musk However, I’m having trouble imagining what Tesla Bot would actually do in a factory that would be uniquely useful and not done better by a nonhumanoid robot. I’m very interested to see what Tesla comes up with here, and whether the company can make it happen in months (or years). I suspect it’s going to be much more difficult than it’s suggesting it will be, especially as it gets to 90 percent of where it wants to be and start trying to crack that last 10 percent that’s necessary for something reliable. This was the end of the formal presentation about Optimus, but there was a Q&A at the end with Musk where he gave some additional information about the robot side of things. He also gave some additional noninformation, which is worth including just in case you haven’t yet had enough eye rolling for one day. Musk expects Optimus to cost less than a car, “much less than [US] $20,00 dollars would be my guess,” he said.Tesla This is a variation on the minimum-viable-product idea, although it seems to be more from the perspective of making a generalist robot, which is somewhat at odds with something minimally viable. It’s good that Musk views the hardware as something in flux, and that he’s framed everything within a plan for volume production. This isn’t the only way to do it—you can first build a useful robot and then figure out how to make it cheaper, but Tesla’s approach could get the company to production faster. If, that is, it is able to confirm that the robot is in fact useful. I’m still not convinced that it will be, at least not on a time scale that will satisfy Musk. While Musk seems to be mostly joking here, the whole “it’s going to be your friend” is really not a good perspective to bring to a robot like this, in my opinion. Or probably any robot, at all honestly. Less robotlike and more friendly than a human pretending to be a robot trying to be a human? Good luck with that. I think it’s more likely that in the short-to-medium term, Tesla will struggle to find situations where Optimus is uniquely useful in an efficient and cost-effective way. Uh. Maybe as a research platform? Despite my skepticism on the time frame here, five years is a long time for any robot, and 10 years is basically forever. I’m also really interested to see these things happen, although Musk’s definitions of “incredible” and “mind-blowing” may be much different than mine. But we’ll see, won’t we? Tesla’s AI Day serves as a recruitment event for the company. “There’s still a lot of work to be done to refine Optimus and improve it, and that’s really why we’re holding this event—to convince some of the most talented people in the world to join Tesla,” Musk said.Tesla I think Elon Musk now has a somewhat better idea of what he’s doing with Tesla Bot. The excessive hype is still there, but now that the company has actually built something, Musk seems to have a much better idea of how hard it actually is. Things are only going to get more difficult from here. Most of what we saw in the presentation was hardware. And hardware is important and a necessary first step, but software is arguably a much more significant challenge when it comes to making robotics useful in the real world. Understanding and interacting with the environment, reasoning and decision-making, the ability to learn and be taught new tasks—these are all necessary pieces of the puzzle of a useful robot that Tesla is trying to put together, but they’re all also extremely difficult, cutting-edge problems, despite the enormous amount of work that the research community has put into them. And so far, we (still) have very little indication that Tesla is going to be any better at tackling this stuff than anyone else. There doesn’t appear to be anything all that special or exciting from Tesla that provides any unique foundation for Musk’s vision in a way that’s likely to allow the company to outpace other companies working on similar things. I’ll reiterate what I said a year ago: The hard part is not building a robot, it’s getting that robot to do useful stuff. “I think Optimus is going to be incredible in five years. In 10 years, mind-blowing. I’m really interested to see that happen, and I hope you are too.” —Elon Musk I could, of course, be wrong. Tesla likely has more resources to throw at this problem than almost anyone else. Maybe the automotive software will translate much better and faster than I think it will. There could be a whole bunch of simple but valuable use cases in Tesla’s own factories that will provide critical stepping-stones for Optimus. Tesla’s battery and manufacturing expertise could have an outsized influence on the affordability, reliability, and success of the robot. The company’s basic approach to planning and control could become a reliable foundation that will help the system mature faster. And the team is obviously very talented and willing to work extremely hard, which could be the difference between modest success and slow failure. Honestly, I would love to be wrong. We’re just starting to see some realistic possibilities with commercial legged and humanoid robots. There are lots of problems to solve, but also lots of potential, and Tesla finding success would be a huge confidence boost in commercial humanoids broadly. We can also hope that all of the resources that Tesla is putting toward Optimus will either directly or indirectly assist other folks working on humanoid robots, if Tesla is willing to share some of what it learns. But as of today, this is all just hoping, and it’s on Tesla to actually make it happen. Evan Ackerman is a senior editor at IEEE Spectrum. Since 2007, he has written over 6,000 articles on robotics and technology. He has a degree in Martian geology and is excellent at playing bagpipes. The prosthetics industry is too focused on high-tech limbs that are complicated, costly, and often impractical The author, Britt Young, holding her Ottobock bebionic bionic arm. In Jules Verne’s 1865 novel From the Earth to the Moon, members of the fictitious Baltimore Gun Club, all disabled Civil War veterans, restlessly search for a new enemy to conquer. They had spent the war innovating new, deadlier weaponry. By the war’s end, with “not quite one arm between four persons, and exactly two legs between six,” these self-taught amputee-weaponsmiths decide to repurpose their skills toward a new projectile: a rocket ship. The story of the Baltimore Gun Club propelling themselves to the moon is about the extraordinary masculine power of the veteran, who doesn’t simply “overcome” his disability; he derives power and ambition from it. Their “crutches, wooden legs, artificial arms, steel hooks, caoutchouc [rubber] jaws, silver craniums [and] platinum noses” don’t play leading roles in their personalities—they are merely tools on their bodies. These piecemeal men are unlikely crusaders of invention with an even more unlikely mission. And yet who better to design the next great leap in technology than men remade by technology themselves?","['tesla', 'fail', 'show', 'uniquely', 'impressive', 'new', 'humanoid', 'robot', 'prototype', 'elon', 'musk', 'unveil', 'optimus', 'humanoid', 'day', 'end', 'day', 'last', 'musk', 'introduce', 'concept', 'tesla', 'bot', 'electromechanically', 'actuate', 'autonomous', 'bipedal', 'general', 'purpose', 'robot', 'musk', 'suggest', 'prototype', 'bot', 'also', 'call', 'optimus', 'complete', 'next', 'year', 'lot', 'hype', 'prototype', 'tesla', 'bot', 'indeed', 'unveil', 'last', 'night', 'day', 'turn', 'hype', 'hype', '’', 'absolutely', 'wrong', 'humanoid', 'robot', 'musk', 'briefly', 'demonstrate', 'stage', '’', 'uniquely', 'right', 'hope', 'necessarily', 'expect', 'tesla', 'robot', 'exactly', 'disappointment', '’', 'little', 'suggest', 'disrupt', 'robotic', 'way', 'rocket', 'tesla', 'electric', 'car', 'watch', 'entire', 'hour', 'live', 'stream', 'archive', 'youtube', 'also', 'include', 'car', 'stuff', 'whatnot', 'go', 'focus', 'interesting', 'bit', 'tesla', 'botoptimus', 'reveal', 'robot', 'musk', 'attempt', 'set', 'reasonable', 'expectation', 'prototypetesla', 'quote', 'musk', '’', 'far', 'far', 'late', 'musk', 'attempt', 'set', 'reasonable', 'expectation', 'robot', 'robotic', 'program', 'general', 'roboticist', 'know', 'well', 'use', 'human', 'set', 'expectation', 'humanoid', 'robot', 'disappointment', 'inevitable', 'try', 'save', 'literal', 'last', 'minute', 'say', 'compare', 'robot', 'robot', 'impressive', 'true', 'go', 'fix', 'thing', 'touch', 'right', 'robot', 'bring', 'stage', 'engineer', 'make', 'clear', 'go', 'first', 'time', 'robot', 'walk', 'untethered', 'unsupported', 'true', '’s', 'bonker', 'heck', 'wait', 'moment', 'give', 'try', '’m', 'particularly', 'impressed', 'confused', 'context', '’re', 'see', 'brief', 'callback', 'year', 'ago', 'last', 'predict', 'store', 'remind', 'darpa', 'robotic', 'challenge', 'many', 'humanoid', 'platform', 'look', 'similar', 'way', 'tesla', 'bot', 'look', 'guess', '’', 'much', 'mostly', 'naked', 'electromechanical', 'humanoid', 'term', 'form', 'factor', 'first', 'glance', '’s', 'particularly', 'innovative', 'futuristic', 'design', 'robot', 'movement', 'quite', 'drc', 'standard', 'look', 'trouble', 'kind', 'accidental', 'contact', 'even', 'bit', 'nonlevel', 'floor', 'musk', 'suggest', 'much', 'stage', 'robot', 'little', 'walk', 'successfully', 'dynamically', 'move', 'make', 'well', 'entirely', 'script', 'know', 'extent', 'robot', 'balance', '’m', 'glad', 'fall', 'face', 'surprise', 'judge', 'harshly', 'tesla', 'show', 'video', 'robot', 'watering', 'plant', 'carry', 'box', 'pick', 'metal', 'bar', 'factory', 'tesla', 'brief', 'live', 'demo', 'musk', 'show', 'video', 'clip', 'prototype', 'robot', 'thing', 'start', 'live', 'stream', 'clip', 'include', 'robot', 'walk', 'carry', 'box', 'unspecified', 'weight', 'place', 'table', 'grasp', 'watering', 'watering', 'somewhat', 'impressive', 'grip', 'narrow', 'handle', 'look', 'tricky', 'robot', 'actually', 'lot', 'show', 'want', 'fall', 'face', 'elon', 'musk', 'however', 'add', 'footage', 'robot', 'sensor', 'idea', 'actually', 'autonomous', 'many', 'try', 'take', 'get', 'right', '’', 'also', 'clip', 'robot', 'pick', 'object', 'attempt', 'place', 'bin', 'video', 'cut', 'right', 'placement', 'successful', 'make', 'think', 'see', 'carefully', 'curate', 'bestcase', 'scenario', 'performance', 'look', 'bit', 'concept', 'show', 'last', 'year', 'obviously', '’', 'less', 'functional', 'prototype', 'see', '’', 'tempting', 'project', 'capability', 'first', 'robot', 'second', 'robot', 'premature', 'musk', 'expect', 'robot', 'cost', 'less', 'car', 'much', 'less', 'say', 'event', 'generally', 'agree', 'musk', 'historically', 'humanoid', 'robot', 'design', 'manufacturability', 'change', 'however', 'think', 'company', 'likely', 'head', 'start', 'manufacturability', '’', 'entirely', 'possible', 'able', 'rapidly', 'catch', '’', 'able', 'leverage', 'carbuilde', 'expertise', 'robot', 'building', 'somehow', '’', 'give', 'work', 'way', '’', 'good', 'idea', 'potentially', 'big', 'advantage', 'production', 'volume', 'cost', 'idea', 'expect', 'mean', 'line', 'get', 'applause', 'far', '’m', 'concerned', 'number', 'basically', 'meaningless', 'moment', 'musk', 'say', 'robot', 'able', 'operate', 'tool', 'useful', 'thing', 'carry', 'manipulate', 'object', 'factory', 'setting', 'last', 'year', 'imply', 'robot', 'able', 'carry', 'useful', 'task', 'necessary', 'degree', 'freedom', 'course', 'hardware', 'first', 'step', 'operating', 'tool', 'useful', 'thing', 'software', 'argue', 'much', 'hard', 'far', 'time', 'consume', 'tesla', 'seem', 'barely', 'start', 'work', 'side', 'thing', '’m', 'exactly', 'sure', 'musk', 'throw', 'shade', 'couple', 'company', 'probably', 'qualify', 'impressive', 'humanoid', 'robot', 'demonstration', 'company', 'fact', 'robot', 'broadly', 'kind', 'intelligence', 'allow', 'navigate', 'least', 'world', 'much', 'well', 'see', 'optimus', 'point', 'musk', 'say', 'robot', 'insufficiently', 'autonomous', 'worldaware', 'far', 'tesla', 'well', 'well', 'lot', 'work', 'actual', 'achievement', 'mercilessly', 'overshadow', 'hype', 'surround', 'truly', 'astonishing', 'amount', 'work', 'short', 'time', 'robotic', 'team', 'proud', 'accomplish', 'inevitably', 'comparison', 'company', 'humanoid', 'robot', '’', 'critical', 'remember', 'context', 'tesla', 'make', 'happen', 'month', '’', 'nuts', 'see', 'appeal', 'tesla', 'want', 'start', 'robotic', 'career', 'get', 'work', 'rapidly', 'evolve', 'hardware', 'platform', 'back', 'assume', 'virtually', 'unlimited', 'resource', 'maybe', 'get', 'robot', 'reliably', 'affordably', 'single', 'useful', 'thing', 'first', 'version', 'optimus', 'design', 'concept', 'development', 'platform', 'late', 'generation', 'tesla', 'musk', 'take', 'break', 'get', 'actual', 'specific', 'information', 'series', 'tesla', 'robotic', 'team', 'member', 'late', 'generation', 'optimus', 'come', 'back', 'hand', 'battery', 'really', 'stand', 'able', 'power', 'robot', 'entire', 'dayish', 'point', 'tesla', 'actually', 'demonstrate', '’', 'meaningful', 'tesla', 'know', 'heck', 'lot', 'power', 'system', 'battery', 'guess', 'company', 'able', 'deliver', 'tesla', 'use', 'simulation', 'design', 'robot', 'structure', 'suffer', 'minimal', 'damage', 'falltesla', 'appreciate', 'think', 'early', 'structure', 'robot', 'able', 'fall', 'safely', 'get', 'superficial', 'damage', 'company', 'seem', 'take', 'advantage', 'kind', 'protective', 'motion', 'fall', 'mitigation', 'active', 'area', 'research', 'elsewhere', 'mention', 'context', 'safety', '’m', 'glad', 'robot', 'damage', 'much', 'fall', 'tesla', 'say', 'stand', 'next', 'optimus', 'use', 'different', 'actuator', 'rotary', 'linear', 'unitstesla', 'custom', 'actuator', 'seem', 'reasonable', 'special', 'particularly', 'tesla', 'make', 'actuator', 'need', 'lot', 'supposedly', '’d', 'expect', 'totally', 'decent', 'consider', 'level', 'mechanical', 'expertise', 'tesla', 'far', 'tell', 'crazy', 'small', 'cheap', 'efficient', 'powerful', '’', 'hard', 'tell', 'slide', 'presentation', 'well', 'actuator', 'go', 'work', 'especially', 'dynamic', 'motion', 'robot', 'software', 'lot', 'catch', 'first', 'optimus', 'feature', 'bioinspire', 'hand', 'design', 'cabledriven', 'actuatorstesla', 'hand', 'cabledriven', 'actuator', 'finger', 'thumb', 'spring', 'provide', 'opening', 'force', 'tesla', 'choose', 'simplicity', 'minimize', 'part', 'count', 'perhaps', 'little', 'surprising', 'cable', 'drive', 'typically', 'durable', 'finicky', 'keep', 'calibrate', 'fivefing', 'hand', 'necessary', 'tesla', 'say', 'optimus', 'work', 'human', 'tool', 'human', 'environment', '’', 'certainly', 'perspective', '’', 'big', 'tradeoff', 'complexity', 'hand', 'design', 'carry', 'bag', 'tesla', 'use', 'software', 'component', 'develop', 'vehicle', 'port', 'robot', 'environmenttesla', 'software', 'following', 'quote', 'come', '’', 'autonomy', 'team', 'still', 'fundamentally', 'disagree', 'imply', 'humanoid', 'robot', 'car', 'leg', 'thing', '’', 'impressive', 'able', 'port', 'much', 'highly', 'skeptical', 'last', 'year', '’m', 'optimistic', 'able', 'generalize', 'platform', 'level', 'huge', 'tesla', 'autonomous', 'system', 'generally', '’d', 'detail', 'easy', 'tesla', 'show', 'sense', 'use', 'vehicle', 'help', 'optimus', 'robot', 'navigatetesla', 'see', 'though', 'reason', 'skeptical', 'occupancy', 'grid', 'robot', 'sensor', 'detect', 'potential', 'obstacle', 'bottom', 'carish', 'priority', 'make', 'absolutely', 'sure', 'robot', 'stay', 'far', 'away', 'conceivably', 'run', 'transfer', 'well', 'humanoid', 'robot', 'need', 'directly', 'interact', 'object', 'useful', 'task', '’m', 'sure', 'lot', 'way', 'adapt', 'tesla', 'car', 'obstacleavoidance', 'system', '’', 'question', 'hard', 'transfer', 'well', 'use', 'solution', 'develop', 'specifically', 'mobile', 'manipulator', 'tesla', 'explain', 'challenge', 'dynamic', 'walking', 'humanoid', 'robot', 'approach', 'motion', 'planningtesla', 'next', 'part', 'presentation', 'focus', 'motion', 'planning', 'stateestimation', 'stuff', 'basic', 'far', 'make', '’s', 'wrong', 'basic', '’', 'slightly', 'weird', 'tesla', 'spend', 'much', 'time', 'guess', '’', 'important', 'context', 'people', 'watch', 'team', 'sort', 'talk', '’d', 'discover', 'stuff', 'hope', 'basic', 'stuff', 'humanoid', 'robot', 'long', 'time', 'tesla', 'adopt', 'traditional', 'approach', 'motion', 'control', 'base', 'model', 'robot', 'state', 'quote', 'kovac', 'ignore', 'last', 'bit', 'change', 'entire', 'economy', 'possibly', 'also', 'ignore', 'time', 'frame', 'next', 'month', 'year', 'particularly', 'meaningful', 'push', 'make', 'tesla', 'bot', 'useful', 'substantial', 'advantage', 'tesla', 'company', 'work', 'humanoid', 'robot', 'tesla', 'potentially', 'big', 'customer', 'least', 'initially', 'inhouse', 'practical', 'task', 'robot', 'train', 'really', 'help', 'accelerate', 'development', 'optimus', 'design', 'extremely', 'capable', 'robot', 'make', 'high', 'volume', 'ultimately', 'million', 'unit', 'expect', 'cost', 'much', 'less', 'car', 'much', 'less', 'guess', 'elon', 'musk', 'however', 'trouble', 'imagine', 'tesla', 'bot', 'actually', 'factory', 'uniquely', 'useful', 'well', 'robot', '’m', 'interested', 'see', 'tesla', 'come', 'company', 'make', 'happen', 'month', 'year', 'suspect', 'go', 'much', 'difficult', 'suggest', 'especially', 'get', 'percent', 'want', 'start', 'try', 'crack', 'last', 'percent', '’', 'necessary', 'reliable', 'end', 'formal', 'presentation', 'optimus', 'qa', 'end', 'musk', 'give', 'additional', 'information', 'robot', 'side', 'thing', 'also', 'give', 'additional', 'noninformation', 'worth', 'include', 'case', 'yet', 'enough', 'eye', 'rolling', 'day', 'musk', 'expect', 'optimus', 'cost', 'less', 'car', 'much', 'less', 'dollar', 'guess', 'saidtesla', 'variation', 'minimumviableproduct', 'idea', 'seem', 'perspective', 'make', 'generalist', 'robot', 'somewhat', 'odd', 'minimally', 'viable', '’', 'good', 'musk', 'view', 'hardware', 'flux', 'frame', 'plan', 'volume', 'production', 'way', 'first', 'build', 'useful', 'robot', 'figure', 'make', 'cheap', '’s', 'approach', 'get', 'company', 'production', 'fast', 'able', 'confirm', 'robot', 'fact', 'useful', 'still', 'convince', 'least', 'time', 'scale', 'satisfy', 'musk', 'musk', 'seem', 'mostly', 'joke', 'whole', 'go', 'friend', 'really', 'good', 'perspective', 'bring', 'robot', 'opinion', 'probably', 'robot', 'honestly', 'less', 'robotlike', 'friendly', 'human', 'pretending', 'robot', 'try', 'human', 'good', 'luck', 'think', '’', 'likely', 'shorttomedium', 'term', 'tesla', 'struggle', 'find', 'situation', 'optimus', 'uniquely', 'useful', 'efficient', 'costeffective', 'way', 'maybe', 'research', 'platform', 'skepticism', 'time', 'frame', 'year', 'long', 'time', 'robot', 'year', 'basically', 'forever', '’m', 'also', 'really', 'interested', 'see', 'thing', 'happen', 'definition', 'incredible', 'mindblowing', 'much', 'different', 'mine', 'see', 'tesla', 'day', 'serve', 'recruitment', 'event', 'company', '’', 'still', 'lot', 'work', 'refine', 'optimus', 'improve', '’', 'really', 'hold', 'event', 'convince', 'talented', 'people', 'world', 'join', 'tesla', 'think', 'elon', 'musk', 'somewhat', 'well', 'idea', 'tesla', 'bot', 'excessive', 'hype', 'still', 'company', 'actually', 'build', 'musk', 'seem', 'much', 'well', 'idea', 'hard', 'actually', 'thing', 'go', 'get', 'difficult', 'see', 'presentation', 'hardware', 'hardware', 'important', 'necessary', 'first', 'step', 'software', 'arguably', 'much', 'significant', 'challenge', 'come', 'make', 'robotic', 'useful', 'real', 'world', 'understanding', 'interact', 'environment', 'reason', 'decisionmake', 'ability', 'learn', 'teach', 'new', 'task', 'necessary', 'piece', 'puzzle', 'useful', 'robot', 'tesla', 'try', 'put', 'together', '’re', 'also', 'extremely', 'difficult', 'cuttingedge', 'problem', 'enormous', 'amount', 'work', 'research', 'community', 'put', 'far', 'still', 'little', 'indication', 'go', 'well', 'tackle', 'stuff', 'else', 'appear', 'special', 'exciting', 'tesla', 'provide', 'unique', 'foundation', 'musk', 'vision', 'way', '’', 'likely', 'allow', 'company', 'outpace', 'company', 'work', 'similar', 'thing', 'reiterate', 'say', 'year', 'ago', 'hard', 'part', 'build', 'robot', 'get', 'robot', 'useful', 'stuff', 'think', 'optimus', 'go', 'incredible', 'year', 'year', 'mindblowe', '’m', 'really', 'interested', 'see', 'happen', 'hope', 'elon', 'musk', 'course', 'wrong', 'tesla', 'likely', 'resource', 'throw', 'problem', 'almost', 'else', 'maybe', 'automotive', 'software', 'translate', 'much', 'well', 'fast', 'think', 'whole', 'bunch', 'simple', 'valuable', 'use', 'case', 'factory', 'provide', 'critical', 'steppingstone', 'battery', 'manufacturing', 'expertise', 'outsized', 'influence', 'affordability', 'reliability', 'success', 'robot', 'company', 'basic', 'approach', 'planning', 'control', 'become', 'reliable', 'foundation', 'help', 'system', 'mature', 'fast', 'team', 'obviously', 'talented', 'willing', 'work', 'extremely', 'hard', 'difference', 'modest', 'success', 'slow', 'failure', 'honestly', 'love', 'wrong', 'start', 'see', 'realistic', 'possibility', 'commercial', 'legged', 'humanoid', 'robot', 'lot', 'problem', 'solve', 'also', 'lot', 'potential', 'tesla', 'find', 'success', 'huge', 'confidence', 'boost', 'commercial', 'humanoid', 'broadly', 'also', 'hope', 'resource', 'put', 'optimus', 'directly', 'indirectly', 'assist', 'folk', 'work', 'humanoid', 'robot', 'willing', 'share', 'learn', 'today', 'hope', '’', 'tesla', 'actually', 'make', 'happen', 'ackerman', 'senior', 'editor', 'ieee', 'spectrum', 'write', 'article', 'robotic', 'technology', 'degree', 'martian', 'geology', 'excellent', 'play', 'bagpipe', 'prosthetic', 'industry', 'focused', 'hightech', 'limb', 'complicated', 'costly', 'often', 'impractical', 'author', 'britt', 'young', 'hold', 'ottobock', 'bebionic', 'bionic', 'arm', 'jule', 'novel', 'earth', 'moon', 'member', 'fictitious', 'gun', 'club', 'disabled', 'civil', 'war', 'veteran', 'restlessly', 'search', 'new', 'enemy', 'conquer', 'spend', 'war', 'innovate', 'new', 'deadly', 'weaponry', 'war', 'end', 'quite', 'arm', 'person', 'exactly', 'leg', 'selftaught', 'amputeeweaponsmith', 'decide', 'repurpose', 'skill', 'new', 'projectile', 'rocket', 'ship', 'story', 'gun', 'club', 'propel', 'moon', 'extraordinary', 'masculine', 'power', 'veteran', 'simply', 'overcome', 'disability', 'derive', 'power', 'ambition', 'crutch', 'wooden', 'leg', 'artificial', 'arm', 'steel', 'hook', 'caoutchouc', 'rubber', 'jaw', 'silver', 'cranium', 'platinum', 'nose', 'play', 'lead', 'role', 'personality', 'merely', 'tool', 'body', 'piecemeal', 'man', 'unlikely', 'crusader', 'invention', 'even', 'unlikely', 'mission', 'yet', 'well', 'design', 'next', 'great', 'leap', 'technology', 'man', 'remade', 'technology']"
"
        Pong-in-a-Dish
    ",https://spectrum.ieee.org/pong-in-a-dish,2022-10-22,"These neurons mimic a brain playing the classic video game  A visual representation of the simulated Pong environment shows electrodes encoded for sensory input (bottom) and electrodes encoded for output (paddle up/down). Ever hear of the Turk—the 19th-century mechanism topped by a turbaned head that played chess against all comers? In fact, hidden inside was a diminutive chessmaster, one you might imagine deadpanning, “Eh, It’s a living.” Then there’s its namesake, the Mechanical Turk—a 21st-century service offered by Amazon to mark up images on the Web with the help of crowdsourced freelancers. They, too, might intone, glassy-eyed, “It’s a living.” Now we have a kind of Biological Turk. A mass of neurons act as a computer that mimics a human being playing the classic computer game Pong. The neurons, some taken from mouse embryos, others grown from human precursor cells, spread out into a one-layer, 800,000-cell mesh called a biological neural network, which lives in a giant petri dish called the DishBrain. There it interfaces with arrays of electrodes that form an interface to silicon hardware. Software mounted on that hardware provides stimulation and feedback, and the minibrain learns how to control a paddle on a simulated ping-pong table. The work was described recently in the journal Neuron by Brett Kagan, the chief scientific officer of Cortical Labs, a startup in Melbourne, Australia, and nine colleagues at that company. The authors talk hopefully about the emergence of sentience, a notion that other brain-in-a-dish researchers have also recently floated. But they seem to stand on solid ground when they say their method will help to advance brain science, on the one hand, and computer science, on the other. A bio-neuro-network might model the effects of drugs on the brain in ways that single-cell neurons can’t. Also, neurons may show themselves to be more than just protoplasmic logic switches but more like entire computers.  The question before us, though, is how does the thing play Pong? 

Pong-in-a-Dish

youtu.be

 First, the electronic scaffolding hits the minibrain with electrical signals that represent the position and movement of the virtual ball. It’s rather like the action potential that a firing neuron would use to convey, say, a sensory signal from the eye to the brain. Because the electrodes are placed at different points in the cell network, the system physically represents the different possible locations. Further information comes from the frequency of the signals, which varies with the distance of the ball to the virtual paddle. The network responds to these stimuli like a motor neuron, sending out a signal that moves the virtual paddle. If the resulting movement causes the ball to bounce, the neural network gets a “reward.” Failure results in a signal that has the opposite effect. “Reward” is put in sneer quotes because these cells don’t have feelings. They can’t experience the joy of victory, the agony of defeat. There’s no dopamine, no salted popcorn. Instead, the researchers say, the network is working to minimize unpredictability. In this view, the so-called reward is a predictable signal, the anti-reward is an unpredictable one. Kagan tells IEEE Spectrum that the system as a whole then reorganizes to become better at playing the game. The most marked improvement came in the first five minutes of play. It seems amazing that a mere 800,000 neurons can model the world, even a simplified world. But, Kagan says, such feats are seen in nature. ""Flies have even fewer neurons but must be able to do some modeling—although perhaps not in a way a human may—to navigate a complex and changing 3D world,"" he says. As he and his colleagues point out in their report the ability of neurons to adapt to external stimuli is well established in vivo; it forms the basis for all animal learning. But theirs, they say, is the first in vitro demonstration involving a goal-directed behavior.  The current version of Pong is forgiving. The paddle is broad, the volley slow, the ball unspinning. Even a neophyte would crush DishBrain. Then again, the same was true of all of AI’s early assays in game playing. The early chess machines would sometimes senselessly give up first a pawn, then a piece, then the queen—all because they were attempting to put off a disagreeable action to a point beyond the built-in planning horizon. Poker-playing programs got good pretty fast, but the early ones sometimes played too well—that is, too cautiously—against weak human opponents, which reduced their winnings. Car navigation programs would send you into a vacant lot.  You might think that just getting a machine to play a decent game is the hard part, and that further improving it to perfection ought to be a snap. Edgar Allan Poe made that judgement when he called the Turk a fraud because it occasionally erred. His conclusion was correct but his reasoning was faulty. It’s not easy turning a barely there machine into a world champion at chess or Go. And yet it has been done. Philip E. Ross is a senior editor at IEEE Spectrum. His interests include transportation, energy storage, AI, and the economic aspects of technology. He has a master's degree in international affairs from Columbia University and another, in journalism, from the University of Michigan. It’s hard to learn, but your code will produce fewer nasty surprises 
You’d expect the longest and most costly phase in the life cycle of a software product to be the initial development of the system, when all those great features are first imagined and then created. In fact, the hardest part comes later, during the maintenance phase. That’s when programmers pay the price for the shortcuts they took during development.
 
	So why did they take shortcuts? Maybe they didn’t realize that they were cutting any corners. Only when their code was deployed and exercised by a lot of users did its hidden flaws come to light. And maybe the developers were rushed. Time-to-market pressures would almost guarantee that their software will contain more bugs than it would otherwise.
                                                   ","These neurons mimic a brain playing the classic video game A visual representation of the simulated Pong environment shows electrodes encoded for sensory input (bottom) and electrodes encoded for output (paddle up/down). Ever hear of the Turk—the 19th-century mechanism topped by a turbaned head that played chess against all comers? In fact, hidden inside was a diminutive chessmaster, one you might imagine deadpanning, “Eh, It’s a living.” Then there’s its namesake, the Mechanical Turk—a 21st-century service offered by Amazon to mark up images on the Web with the help of crowdsourced freelancers. They, too, might intone, glassy-eyed, “It’s a living.” Now we have a kind of Biological Turk. A mass of neurons act as a computer that mimics a human being playing the classic computer game Pong. The neurons, some taken from mouse embryos, others grown from human precursor cells, spread out into a one-layer, 800,000-cell mesh called a biological neural network, which lives in a giant petri dish called the DishBrain. There it interfaces with arrays of electrodes that form an interface to silicon hardware. Software mounted on that hardware provides stimulation and feedback, and the minibrain learns how to control a paddle on a simulated ping-pong table. The work was described recently in the journal Neuron by Brett Kagan, the chief scientific officer of Cortical Labs, a startup in Melbourne, Australia, and nine colleagues at that company. The authors talk hopefully about the emergence of sentience, a notion that other brain-in-a-dish researchers have also recently floated. But they seem to stand on solid ground when they say their method will help to advance brain science, on the one hand, and computer science, on the other. A bio-neuro-network might model the effects of drugs on the brain in ways that single-cell neurons can’t. Also, neurons may show themselves to be more than just protoplasmic logic switches but more like entire computers. The question before us, though, is how does the thing play Pong? Pong-in-a-Dish youtu.be First, the electronic scaffolding hits the minibrain with electrical signals that represent the position and movement of the virtual ball. It’s rather like the action potential that a firing neuron would use to convey, say, a sensory signal from the eye to the brain. Because the electrodes are placed at different points in the cell network, the system physically represents the different possible locations. Further information comes from the frequency of the signals, which varies with the distance of the ball to the virtual paddle. The network responds to these stimuli like a motor neuron, sending out a signal that moves the virtual paddle. If the resulting movement causes the ball to bounce, the neural network gets a “reward.” Failure results in a signal that has the opposite effect. “Reward” is put in sneer quotes because these cells don’t have feelings. They can’t experience the joy of victory, the agony of defeat. There’s no dopamine, no salted popcorn. Instead, the researchers say, the network is working to minimize unpredictability. In this view, the so-called reward is a predictable signal, the anti-reward is an unpredictable one. Kagan tells IEEE Spectrum that the system as a whole then reorganizes to become better at playing the game. The most marked improvement came in the first five minutes of play. It seems amazing that a mere 800,000 neurons can model the world, even a simplified world. But, Kagan says, such feats are seen in nature. ""Flies have even fewer neurons but must be able to do some modeling—although perhaps not in a way a human may—to navigate a complex and changing 3D world,"" he says. As he and his colleagues point out in their report the ability of neurons to adapt to external stimuli is well established in vivo; it forms the basis for all animal learning. But theirs, they say, is the first in vitro demonstration involving a goal-directed behavior. The current version of Pong is forgiving. The paddle is broad, the volley slow, the ball unspinning. Even a neophyte would crush DishBrain. Then again, the same was true of all of AI’s early assays in game playing. The early chess machines would sometimes senselessly give up first a pawn, then a piece, then the queen—all because they were attempting to put off a disagreeable action to a point beyond the built-in planning horizon. Poker-playing programs got good pretty fast, but the early ones sometimes played too well—that is, too cautiously—against weak human opponents, which reduced their winnings. Car navigation programs would send you into a vacant lot. You might think that just getting a machine to play a decent game is the hard part, and that further improving it to perfection ought to be a snap. Edgar Allan Poe made that judgement when he called the Turk a fraud because it occasionally erred. His conclusion was correct but his reasoning was faulty. It’s not easy turning a barely there machine into a world champion at chess or Go. And yet it has been done. Philip E. Ross is a senior editor at IEEE Spectrum. His interests include transportation, energy storage, AI, and the economic aspects of technology. He has a master's degree in international affairs from Columbia University and another, in journalism, from the University of Michigan. It’s hard to learn, but your code will produce fewer nasty surprises You’d expect the longest and most costly phase in the life cycle of a software product to be the initial development of the system, when all those great features are first imagined and then created. In fact, the hardest part comes later, during the maintenance phase. That’s when programmers pay the price for the shortcuts they took during development. So why did they take shortcuts? Maybe they didn’t realize that they were cutting any corners. Only when their code was deployed and exercised by a lot of users did its hidden flaws come to light. And maybe the developers were rushed. Time-to-market pressures would almost guarantee that their software will contain more bugs than it would otherwise.","['neuron', 'mimic', 'brain', 'play', 'classic', 'video', 'game', 'visual', 'representation', 'simulated', 'pong', 'environment', 'show', 'electrode', 'encode', 'sensory', 'input', 'bottom', 'electrode', 'encode', 'output', 'paddle', 'ever', 'hear', 'turk', 'mechanism', 'top', 'turbane', 'head', 'play', 'chess', 'comer', 'fact', 'hide', 'inside', 'diminutive', 'chessmaster', 'one', 'imagine', 'deadpanne', '’', 'living', '’', 'namesake', 'mechanical', 'turk', 'service', 'offer', 'amazon', 'mark', 'image', 'web', 'help', 'crowdsourced', 'freelancer', 'intone', 'glassyeyed', '’', 'living', 'kind', 'biological', 'mass', 'neuron', 'act', 'computer', 'mimic', 'human', 'play', 'classic', 'computer', 'game', 'pong', 'neuron', 'take', 'mouse', 'embryo', 'grow', 'human', 'precursor', 'cell', 'spread', 'onelayer', '800000cell', 'mesh', 'call', 'biological', 'neural', 'network', 'live', 'giant', 'petri', 'call', 'dishbrain', 'interface', 'array', 'electrode', 'form', 'interface', 'silicon', 'hardware', 'software', 'mount', 'hardware', 'provide', 'stimulation', 'feedback', 'minibrain', 'learn', 'control', 'paddle', 'simulated', 'pingpong', 'table', 'work', 'describe', 'recently', 'journal', 'chief', 'scientific', 'officer', 'cortical', 'lab', 'startup', 'melbourne', 'australia', 'colleague', 'company', 'author', 'talk', 'hopefully', 'emergence', 'sentience', 'notion', 'braininadish', 'researcher', 'also', 'recently', 'float', 'seem', 'stand', 'solid', 'ground', 'say', 'method', 'help', 'advance', 'brain', 'science', 'hand', 'computer', 'science', 'bioneuronetwork', 'model', 'effect', 'drug', 'brain', 'way', 'singlecell', 'neuron', 'also', 'neuron', 'show', 'protoplasmic', 'logic', 'switch', 'entire', 'computer', 'question', 'though', 'thing', 'play', 'pong', 'youtube', 'first', 'electronic', 'scaffolding', 'hit', 'minibrain', 'electrical', 'signal', 'represent', 'position', 'movement', 'virtual', 'ball', '’', 'rather', 'action', 'potential', 'firing', 'neuron', 'use', 'convey', 'say', 'sensory', 'signal', 'eye', 'brain', 'electrode', 'place', 'different', 'point', 'cell', 'network', 'system', 'physically', 'represent', 'different', 'possible', 'location', 'information', 'come', 'frequency', 'signal', 'vary', 'distance', 'ball', 'virtual', 'paddle', 'network', 'respond', 'stimulus', 'motor', 'neuron', 'send', 'signal', 'move', 'virtual', 'paddle', 'result', 'movement', 'cause', 'ball', 'bounce', 'neural', 'network', 'get', 'reward', 'failure', 'result', 'signal', 'opposite', 'effect', 'reward', 'put', 'sneer', 'quote', 'cell', 'feeling', 'experience', 'joy', 'victory', 'agony', 'defeat', '’', 'dopamine', 'salt', 'popcorn', 'instead', 'researcher', 'say', 'network', 'work', 'minimize', 'unpredictability', 'view', 'socalled', 'reward', 'predictable', 'signal', 'antireward', 'unpredictable', 'kagan', 'tell', 'ieee', 'spectrum', 'system', 'whole', 'reorganize', 'become', 'well', 'play', 'game', 'marked', 'improvement', 'come', 'first', 'minute', 'play', 'seem', 'amazing', 'mere', 'neuron', 'model', 'world', 'even', 'simplified', 'world', 'say', 'feat', 'see', 'nature', 'fly', 'even', 'neuron', 'able', 'modeling', 'perhaps', 'way', 'human', 'navigate', 'complex', 'change', 'world', 'say', 'colleague', 'point', 'report', 'ability', 'neuron', 'adapt', 'external', 'stimulus', 'well', 'establish', 'vivo', 'form', 'basis', 'animal', 'learning', 'say', 'first', 'demonstration', 'involve', 'goaldirecte', 'behavior', 'current', 'version', 'pong', 'forgive', 'paddle', 'broad', 'volley', 'slow', 'ball', 'unspinne', 'even', 'neophyte', 'crush', 'dishbrain', 'true', 'ai', 'early', 'assay', 'game', 'play', 'early', 'chess', 'machine', 'sometimes', 'senselessly', 'give', 'first', 'pawn', 'piece', 'queen', 'attempt', 'put', 'disagreeable', 'action', 'point', 'builtin', 'planning', 'horizon', 'pokerplaye', 'program', 'get', 'good', 'pretty', 'fast', 'early', 'one', 'sometimes', 'play', 'well', 'cautiously', 'weak', 'human', 'opponent', 'reduce', 'winning', 'car', 'navigation', 'program', 'send', 'vacant', 'lot', 'think', 'get', 'machine', 'play', 'decent', 'game', 'hard', 'part', 'far', 'improve', 'perfection', 'snap', 'edgar', 'make', 'judgement', 'call', 'turk', 'fraud', 'occasionally', 'err', 'conclusion', 'correct', 'reasoning', 'faulty', '’', 'easy', 'turn', 'barely', 'machine', 'world', 'champion', 'chess', 'go', 'yet', 'senior', 'editor', 'ieee', 'spectrum', 'interest', 'include', 'transportation', 'energy', 'storage', 'ai', 'economic', 'aspect', 'technology', 'masters', 'degree', 'international', 'affair', 'journalism', 'university', '’', 'hard', 'learn', 'code', 'produce', 'nasty', 'surprise', '’d', 'expect', 'long', 'costly', 'phase', 'life', 'cycle', 'software', 'product', 'initial', 'development', 'system', 'great', 'feature', 'first', 'imagine', 'create', 'fact', 'hard', 'part', 'come', 'later', 'maintenance', 'phase', '’', 'programmer', 'pay', 'price', 'shortcut', 'take', 'development', 'take', 'shortcut', 'maybe', 'realize', 'cut', 'corner', 'code', 'deploy', 'exercise', 'lot', 'user', 'hide', 'flaw', 'come', 'light', 'maybe', 'developer', 'rush', 'timetomarket', 'pressure', 'almost', 'guarantee', 'software', 'contain', 'bug', 'otherwise']"
"
        AI Language Models Are Struggling to “Get” Math
    ",https://spectrum.ieee.org/large-language-models-math,2022-10-12,"Should this be telling us something?  If computers are good at anything, they are good at math. So it may come as a surprise that after much struggling, top machine-learning researchers have recently made breakthroughs in teaching computers math. Over the past year, researchers from the University of California, Berkeley, OpenAI, and Google have made progress in teaching basic math concepts to natural language generation models—algorithms such as GPT-2/3 and GPT-Neo. However, until recently, language models regularly failed to solve even simple word problems, such as “Alice has five more balls than Bob, who has two balls after he gives four to Charlie. How many balls does Alice have?” “When we say computers are very good at math, they’re very good at things that are quite specific,” says Guy Gur-Ari, a machine-learning expert at Google. Computers are good at arithmetic—plugging numbers in and calculating is child’s play. But outside of formal structures, computers struggle. “I think there’s this notion that humans doing math have some rigid reasoning system—that there’s a sharp distinction between knowing something and not knowing something.”—Ethan Dyer, Google Solving word problems, or “quantitative reasoning,” is deceptively tricky because it requires a robustness and rigor that many other problems don’t. If any step during the process goes wrong, the answer will be wrong. “When multiplying really large numbers together…they’ll forget to carry somewhere and be off by one,” says Vineet Kosaraju, a machine-learning expert at OpenAI. Other mistakes made by language models are less human, such as misinterpreting 10 as a 1 and a 0, not 10.  “We work on math because we find it independently very interesting,” says Karl Cobbe, a machine-learning expert at OpenAI. But as Gur-Ari puts it, if it’s good at math, “it’s probably also good at solving many other useful problems.” As machine-learning models are trained on larger samples of data, they tend to grow more robust and make fewer mistakes. But scaling up seems to go only so far with quantitative reasoning; researchers realized that the mistakes language models make seemed to require a more targeted approach. Last year, two different teams of researchers, at UC Berkeley and OpenAI, released two data sets, MATH and GSM8K, respectively, which contain thousands of math problems across geometry, algebra, precalculus, and more. “We basically wanted to see if it was a problem with data sets,” says Steven Basart, a researcher at the Center for AI Safety who worked on MATH. Language models were known to be bad at word problems—but how bad were they, and could they be fixed by introducing better formatted, bigger data sets? The MATH group found just how challenging quantitative reasoning is for top-of-the-line language models, which scored less than 7 percent. (A human grad student scored 40 percent, while a math olympiad champ scored 90 percent.) Models attacking GSM8K problems, which had easier grade-school-level problems, reached about 20 percent accuracy. The OpenAI researchers used two main techniques: fine-tuning and verification. In fine-tuning, researchers take a pretrained language model that includes irrelevant information (Wikipedia articles on zambonis, the dictionary entry for “gusto,” and the like) and then show the model, Clockwork Orange–style, only the relevant information (math problems). Verification, on the other hand, is more of a review session. “The model gets to see a lot of examples of its own mistakes, which is really valuable,” Cobbe says. At the time, OpenAI predicted a model would need to be trained on 100 times more data to reach 80 percent accuracy on GSM8K. But in June, Google’s Minerva announced 78 percent accuracy with minimal scaling upwards. “It’s ahead of any of the trends that we were expecting,” Cobbe says. Basart agrees. “That’s shocking. I thought it would take longer,” he says. Minerva uses Google’s own language model, Pathways Language Model (PaLM), which is fine-tuned on scientific papers from the arXiv online preprint server and other sources with formatted math. Two other strategies helped Minerva. In “chain-of-thought prompting,” Minerva was required to break down larger problems into more palatable chunks. The model also used majority voting—instead of being asked for one answer, it was asked to solve the problem 100 times. Of those answers, Minerva picked the most common answer. The gains from these new strategies were enormous. Minerva shot up to 50 percent accuracy on MATH and nearly 80 percent accuracy on GSM8K, as well as the MMLU, a more general set of STEM questions that included chemistry and biology. When Minerva was asked to redo a random sample of slightly tweaked questions, it performed just as well, suggesting that its capabilities were not from mere memorization.  What Minerva knows—or doesn’t know—about math is fuzzier. Unlike proof assistants, which come with built-in structure, Minerva and other language models have no formal structure. They can have strange, messy reasoning and still arrive at the right answer. As numbers grow larger, the language models’ accuracy falters, something that would never happen on a TI-84. “Just how smart is it—or isn’t it?” asks Cobbe. Though models like Minerva might arrive at the same answer as a human, the actual process they’re following could be wildly different. On the other hand, chain-of-thought prompting is familiar to any human student who’s been asked to “show your work.” “I think there’s this notion that humans doing math have some rigid reasoning system—that there’s a sharp distinction between knowing something and not knowing something,” says Ethan Dyer, a machine-learning expert at Google. But humans give inconsistent answers, make errors, and fail to apply core concepts, too. The borders, at this frontier of machine learning, are blurred.  Update 14 Oct. 2022: A previous version of this story extraneously alluded to the DALL-E/DALL-E 2 art-generation AI in the context of large language generation models being taught to handle math word problems. Of course, neither DALL-E nor DALL-E 2 is a large language generation model. (And it was not studied in the math word problem research.) So to avoid confusion, references to it were cut.  Dan Garisto is a freelance science journalist who covers physics and other physical sciences. His work has appeared in Scientific American, Physics, Symmetry, Undark, and other outlets. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia  cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving.  
Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines.
 
	The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, 
	Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.
                                             ","Should this be telling us something? If computers are good at anything, they are good at math. So it may come as a surprise that after much struggling, top machine-learning researchers have recently made breakthroughs in teaching computers math. Over the past year, researchers from the University of California, Berkeley, OpenAI, and Google have made progress in teaching basic math concepts to natural language generation models—algorithms such as GPT-2/3 and GPT-Neo. However, until recently, language models regularly failed to solve even simple word problems, such as “Alice has five more balls than Bob, who has two balls after he gives four to Charlie. How many balls does Alice have?” “When we say computers are very good at math, they’re very good at things that are quite specific,” says Guy Gur-Ari, a machine-learning expert at Google. Computers are good at arithmetic—plugging numbers in and calculating is child’s play. But outside of formal structures, computers struggle. “I think there’s this notion that humans doing math have some rigid reasoning system—that there’s a sharp distinction between knowing something and not knowing something.”—Ethan Dyer, Google Solving word problems, or “quantitative reasoning,” is deceptively tricky because it requires a robustness and rigor that many other problems don’t. If any step during the process goes wrong, the answer will be wrong. “When multiplying really large numbers together…they’ll forget to carry somewhere and be off by one,” says Vineet Kosaraju, a machine-learning expert at OpenAI. Other mistakes made by language models are less human, such as misinterpreting 10 as a 1 and a 0, not 10. “We work on math because we find it independently very interesting,” says Karl Cobbe, a machine-learning expert at OpenAI. But as Gur-Ari puts it, if it’s good at math, “it’s probably also good at solving many other useful problems.” As machine-learning models are trained on larger samples of data, they tend to grow more robust and make fewer mistakes. But scaling up seems to go only so far with quantitative reasoning; researchers realized that the mistakes language models make seemed to require a more targeted approach. Last year, two different teams of researchers, at UC Berkeley and OpenAI, released two data sets, MATH and GSM8K, respectively, which contain thousands of math problems across geometry, algebra, precalculus, and more. “We basically wanted to see if it was a problem with data sets,” says Steven Basart, a researcher at the Center for AI Safety who worked on MATH. Language models were known to be bad at word problems—but how bad were they, and could they be fixed by introducing better formatted, bigger data sets? The MATH group found just how challenging quantitative reasoning is for top-of-the-line language models, which scored less than 7 percent. (A human grad student scored 40 percent, while a math olympiad champ scored 90 percent.) Models attacking GSM8K problems, which had easier grade-school-level problems, reached about 20 percent accuracy. The OpenAI researchers used two main techniques: fine-tuning and verification. In fine-tuning, researchers take a pretrained language model that includes irrelevant information (Wikipedia articles on zambonis, the dictionary entry for “gusto,” and the like) and then show the model, Clockwork Orange–style, only the relevant information (math problems). Verification, on the other hand, is more of a review session. “The model gets to see a lot of examples of its own mistakes, which is really valuable,” Cobbe says. At the time, OpenAI predicted a model would need to be trained on 100 times more data to reach 80 percent accuracy on GSM8K. But in June, Google’s Minerva announced 78 percent accuracy with minimal scaling upwards. “It’s ahead of any of the trends that we were expecting,” Cobbe says. Basart agrees. “That’s shocking. I thought it would take longer,” he says. Minerva uses Google’s own language model, Pathways Language Model (PaLM), which is fine-tuned on scientific papers from the arXiv online preprint server and other sources with formatted math. Two other strategies helped Minerva. In “chain-of-thought prompting,” Minerva was required to break down larger problems into more palatable chunks. The model also used majority voting—instead of being asked for one answer, it was asked to solve the problem 100 times. Of those answers, Minerva picked the most common answer. The gains from these new strategies were enormous. Minerva shot up to 50 percent accuracy on MATH and nearly 80 percent accuracy on GSM8K, as well as the MMLU, a more general set of STEM questions that included chemistry and biology. When Minerva was asked to redo a random sample of slightly tweaked questions, it performed just as well, suggesting that its capabilities were not from mere memorization. What Minerva knows—or doesn’t know—about math is fuzzier. Unlike proof assistants, which come with built-in structure, Minerva and other language models have no formal structure. They can have strange, messy reasoning and still arrive at the right answer. As numbers grow larger, the language models’ accuracy falters, something that would never happen on a TI-84. “Just how smart is it—or isn’t it?” asks Cobbe. Though models like Minerva might arrive at the same answer as a human, the actual process they’re following could be wildly different. On the other hand, chain-of-thought prompting is familiar to any human student who’s been asked to “show your work.” “I think there’s this notion that humans doing math have some rigid reasoning system—that there’s a sharp distinction between knowing something and not knowing something,” says Ethan Dyer, a machine-learning expert at Google. But humans give inconsistent answers, make errors, and fail to apply core concepts, too. The borders, at this frontier of machine learning, are blurred. Update 14 Oct. 2022: A previous version of this story extraneously alluded to the DALL-E/DALL-E 2 art-generation AI in the context of large language generation models being taught to handle math word problems. Of course, neither DALL-E nor DALL-E 2 is a large language generation model. (And it was not studied in the math word problem research.) So to avoid confusion, references to it were cut. Dan Garisto is a freelance science journalist who covers physics and other physical sciences. His work has appeared in Scientific American, Physics, Symmetry, Undark, and other outlets. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.","['tell', 'computer', 'good', 'good', 'math', 'come', 'surprise', 'much', 'struggle', 'top', 'machinelearne', 'researcher', 'recently', 'make', 'breakthrough', 'teach', 'computer', 'math', 'past', 'year', 'researcher', 'make', 'progress', 'teach', 'basic', 'math', 'concept', 'natural', 'language', 'generation', 'model', 'algorithm', 'gptneo', 'however', 'recently', 'language', 'model', 'regularly', 'fail', 'solve', 'even', 'simple', 'word', 'problem', 'ball', 'ball', 'give', 'charlie', 'many', 'ball', 'alice', 'say', 'computer', 'good', 'math', '’re', 'good', 'thing', 'quite', 'specific', 'say', 'machinelearne', 'expert', 'computer', 'good', 'arithmetic', 'plug', 'number', 'calculate', 'child', '’s', 'play', 'outside', 'formal', 'structure', 'computer', 'struggle', 'think', '’s', 'notion', 'human', 'math', 'rigid', 'reasoning', 'system', '’', 'sharp', 'distinction', 'know', 'know', 'dyer', 'google', 'solve', 'word', 'problem', 'quantitative', 'reasoning', 'deceptively', 'tricky', 'require', 'robustness', 'rigor', 'many', 'problem', 'step', 'process', 'go', 'wrong', 'answer', 'wrong', 'multiply', 'really', 'large', 'number', 'together', 'they’ll', 'forget', 'carry', 'somewhere', 'say', 'vineet', 'kosaraju', 'machinelearne', 'expert', 'openai', 'mistake', 'make', 'language', 'model', 'less', 'human', 'misinterpret', 'work', 'math', 'find', 'independently', 'interesting', 'say', 'machinelearning', 'expert', 'openai', 'gurari', 'put', '’', 'good', 'math', '’', 'probably', 'also', 'good', 'solve', 'many', 'useful', 'problem', 'machinelearne', 'model', 'train', 'large', 'sample', 'datum', 'tend', 'grow', 'robust', 'make', 'mistake', 'scale', 'seem', 'go', 'far', 'quantitative', 'reasoning', 'researcher', 'realize', 'mistake', 'language', 'model', 'make', 'seem', 'require', 'targeted', 'approach', 'last', 'year', 'different', 'team', 'researcher', 'openai', 'release', 'datum', 'set', 'math', 'respectively', 'contain', 'thousand', 'math', 'problem', 'geometry', 'precalculus', 'basically', 'want', 'see', 'problem', 'datum', 'set', 'say', 'basart', 'researcher', 'center', 'safety', 'work', 'math', 'language', 'model', 'know', 'bad', 'word', 'problem', 'bad', 'fix', 'introduce', 'well', 'format', 'big', 'datum', 'set', 'math', 'group', 'find', 'challenging', 'quantitative', 'reasoning', 'topoftheline', 'language', 'model', 'score', 'less', 'percent', 'human', 'grad', 'student', 'score', 'percent', 'math', 'olympiad', 'champ', 'score', 'percent', 'model', 'attack', 'gsm8k', 'problem', 'easy', 'gradeschoollevel', 'problem', 'reach', 'percent', 'accuracy', 'openai', 'researcher', 'use', 'main', 'technique', 'finetune', 'verification', 'finetune', 'researcher', 'take', 'pretraine', 'language', 'model', 'include', 'irrelevant', 'information', 'wikipedia', 'article', 'zamboni', 'dictionary', 'entry', 'gusto', 'like', 'show', 'model', 'clockwork', 'orange', 'style', 'relevant', 'information', 'math', 'problem', 'verification', 'hand', 'review', 'session', 'model', 'get', 'see', 'lot', 'example', 'mistake', 'really', 'valuable', 'cobbe', 'say', 'time', 'openai', 'predict', 'model', 'need', 'train', 'time', 'datum', 'reach', 'percent', 'accuracy', 'announce', 'percent', 'accuracy', 'minimal', 'scaling', 'upwards', '’', 'ahead', 'trend', 'expect', 'cobbe', 'say', 'agree', '’s', 'shocking', 'think', 'take', 'long', 'say', 'use', 'language', 'model', 'pathway', 'language', 'model', 'palm', 'finetune', 'scientific', 'paper', 'preprint', 'server', 'source', 'format', 'math', 'strategy', 'help', 'chainofthought', 'prompt', 'require', 'break', 'large', 'problem', 'palatable', 'chunk', 'model', 'also', 'use', 'majority', 'voting', 'instead', 'ask', 'answer', 'ask', 'solve', 'problem', 'time', 'answer', 'pick', 'common', 'answer', 'gain', 'new', 'strategy', 'enormous', 'minerva', 'shoot', 'percent', 'accuracy', 'math', 'nearly', 'percent', 'accuracy', 'well', 'mmlu', 'general', 'set', 'stem', 'question', 'include', 'chemistry', 'biology', 'ask', 'redo', 'random', 'sample', 'slightly', 'tweak', 'question', 'perform', 'well', 'suggest', 'capability', 'mere', 'memorization', 'know', 'know', 'math', 'fuzzy', 'proof', 'assistant', 'come', 'builtin', 'structure', 'language', 'model', 'formal', 'structure', 'strange', 'messy', 'reasoning', 'still', 'arrive', 'right', 'answer', 'number', 'grow', 'large', 'language', 'model', 'accuracy', 'falter', 'never', 'happen', 'ti84', 'smart', 'ask', 'cobbe', 'model', 'arrive', 'answer', 'human', 'actual', 'process', 'follow', 'wildly', 'different', 'hand', 'chainofthought', 'prompting', 'familiar', 'human', 'student', 'ask', 'show', 'work', 'think', '’s', 'notion', 'human', 'math', 'rigid', 'reasoning', 'system', '’', 'sharp', 'distinction', 'know', 'know', 'say', 'dyer', 'machinelearne', 'expert', 'human', 'give', 'inconsistent', 'answer', 'make', 'error', 'fail', 'apply', 'core', 'concept', 'border', 'frontier', 'machine', 'learning', 'blur', 'update', 'previous', 'version', 'story', 'extraneously', 'allude', 'dalledalle', 'artgeneration', 'ai', 'context', 'large', 'language', 'generation', 'model', 'teach', 'handle', 'math', 'word', 'problem', 'course', 'dalle', 'dalle', 'large', 'language', 'generation', 'model', 'study', 'math', 'word', 'problem', 'research', 'avoid', 'confusion', 'reference', 'cut', 'garisto', 'freelance', 'science', 'journalist', 'cover', 'physics', 'physical', 'science', 'work', 'appear', 'scientific', 'american', 'symmetry', 'undark', 'outlet', 'well', 'detection', 'make', 'ocean', 'transparent', 'perhaps', 'doom', 'mutually', 'assure', 'destruction', 'virginiaclass', 'fast', 'attack', 'submarine', 'cruise', 'back', 'effectively', 'disappear', 'diving', 'submarine', 'value', 'primarily', 'ability', 'hide', 'assurance', 'submarine', 'likely', 'survive', 'first', 'missile', 'strike', 'nuclear', 'war', 'thus', 'able', 'respond', 'launch', 'missile', 'second', 'strike', 'key', 'strategy', 'deterrence', 'know', 'mutually', 'assure', 'destruction', 'new', 'technology', 'render', 'ocean', 'effectively', 'transparent', 'make', 'trivial', 'spot', 'lurk', 'submarine', 'thus', 'undermine', 'peace', 'world', 'nearly', 'century', 'naval', 'engineer', 'strive', 'develop', 'everfaster', 'everquieter', 'submarine', 'work', 'hard', 'advance', 'wide', 'array', 'radar', 'sonar', 'technology', 'design', 'detect', 'target', 'eliminate', 'enemy', 'submarine', 'balance', 'seem', 'turn', 'emergence', 'nuclearpowered', 'submarine', 'early', '1960', 'study', 'center', 'strategic', 'budgetary', 'assessment', 'clark', 'naval', 'specialist', 'note', 'ability', 'boat', 'remain', 'submerge', 'long', 'period', 'time', 'make', 'nearly', 'impossible', 'find', 'radar', 'active', 'sonar', 'even', 'stealthy', 'submarine', 'produce', 'subtle', 'verylowfrequency', 'noise', 'pick', 'far', 'away', 'network', 'acoustic', 'hydrophone', 'array', 'mount', 'seafloor']"
"
        Ultrafast Racetrack Memory Enters the Third Dimension
    ",https://spectrum.ieee.org/racetrack-memory,2022-10-09,"Arrays of arching nanowires could lead to ultrafast, ultrahigh-density solid-state nonvolatile memory An artist's depiction of a 3D racetrack memory device.  Racetrack memory could hold vast amounts of data that can be accessed extraordinarily quickly. Now, in a new study, scientists reveal 3D racetrack memory devices that may greatly increase the potential of this technology. Racetrack memory encodes bits of data in the form of magnetic domain walls. These walls divide a material into domains, inside which magnetic poles all point in the same direction. Electric pulses can push these domain walls back and forth within nanowires, making them run like race cars down a track. Magnetic sensors and other electronics can then read and write data on any point on this racetrack. Racetrack memory potentially “has unrivaled density compared to other memory technologies,” says Stuart Parkin, director of the Max Planck Institute of Microstructure Physics in Halle, Germany, and who developed the concept while at IBM. Racetrack memory is solid-state like flash memory, lacking unwieldy moving parts, and is nonvolatile, storing data even after the power is removed. In addition, racetrack memory’s electric pulses can move domain walls at speeds of up to kilometers per second. This suggests devices using this kind of memory can perform exceptionally quickly. “Memories that operate at speeds of less than 1 nanosecond per write operation are possible,” Parkin says. “This is as fast or faster than the fastest write speeds of any commercially available memory today.” Most research on racetrack memory to date has focused on 2D devices. However, Parkin had long suggested it could be a 3D technology, with arrays of vertical racetracks much like a city filled with skyscrapers. An extra dimension added to racetrack memory would greatly increase the amount of data it could hold on a given footprint. But assembling such complex 3D structures, given the standard fabrication techniques used by the microelectronics industry, has proven difficult.  Now Parkin and his colleagues have developed a 3D racetrack memory device using a new method that lets them drape a 2D racetrack onto a 3D surface. They detailed their findings online 22 September in the journal Nature Nanotechnology. The scientists fabricated a complex ultrathin film made of layers of tantalum nitride, cobalt, nickel, platinum, ruthenium, and magnesium oxide onto a water-soluble layer of strontium aluminate. They then immersed the film in water, releasing a freestanding magnetic film. The researchers next transferred this magnetic film onto a sapphire surface engineered with speed-bump-like 3D ridges, a bit like one might drape a drop cloth over furniture while painting a room. Standard photolithography techniques then let them create racetracks made of wires—each 3 micrometers wide—with arching segments rising up to 900 nanometers high, following the shape of the sapphire surface. “The 3D racetrack structure prepared and handled in this way survived intact, and behaves almost identically to the originally deposited structure before it was detached,” Parkin says. In experiments, domain walls in the 3D racetracks traveled at speeds of up to 600 meters per second. “We believe that the speed can be increased by more than 10 times in related structures using novel materials,” Parkin says. The scientists are exploring “many different ways of building 3D racetrack memories,” Parkin adds. “There are many interesting challenges that are still to be explored, which makes research into racetrack memory very exciting.” Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. It’s hard to learn, but your code will produce fewer nasty surprises 
You’d expect the longest and most costly phase in the life cycle of a software product to be the initial development of the system, when all those great features are first imagined and then created. In fact, the hardest part comes later, during the maintenance phase. That’s when programmers pay the price for the shortcuts they took during development.
 
	So why did they take shortcuts? Maybe they didn’t realize that they were cutting any corners. Only when their code was deployed and exercised by a lot of users did its hidden flaws come to light. And maybe the developers were rushed. Time-to-market pressures would almost guarantee that their software will contain more bugs than it would otherwise.
                                                   ","Arrays of arching nanowires could lead to ultrafast, ultrahigh-density solid-state nonvolatile memory An artist's depiction of a 3D racetrack memory device. Racetrack memory could hold vast amounts of data that can be accessed extraordinarily quickly. Now, in a new study, scientists reveal 3D racetrack memory devices that may greatly increase the potential of this technology. Racetrack memory encodes bits of data in the form of magnetic domain walls. These walls divide a material into domains, inside which magnetic poles all point in the same direction. Electric pulses can push these domain walls back and forth within nanowires, making them run like race cars down a track. Magnetic sensors and other electronics can then read and write data on any point on this racetrack. Racetrack memory potentially “has unrivaled density compared to other memory technologies,” says Stuart Parkin, director of the Max Planck Institute of Microstructure Physics in Halle, Germany, and who developed the concept while at IBM. Racetrack memory is solid-state like flash memory, lacking unwieldy moving parts, and is nonvolatile, storing data even after the power is removed. In addition, racetrack memory’s electric pulses can move domain walls at speeds of up to kilometers per second. This suggests devices using this kind of memory can perform exceptionally quickly. “Memories that operate at speeds of less than 1 nanosecond per write operation are possible,” Parkin says. “This is as fast or faster than the fastest write speeds of any commercially available memory today.” Most research on racetrack memory to date has focused on 2D devices. However, Parkin had long suggested it could be a 3D technology, with arrays of vertical racetracks much like a city filled with skyscrapers. An extra dimension added to racetrack memory would greatly increase the amount of data it could hold on a given footprint. But assembling such complex 3D structures, given the standard fabrication techniques used by the microelectronics industry, has proven difficult. Now Parkin and his colleagues have developed a 3D racetrack memory device using a new method that lets them drape a 2D racetrack onto a 3D surface. They detailed their findings online 22 September in the journal Nature Nanotechnology. The scientists fabricated a complex ultrathin film made of layers of tantalum nitride, cobalt, nickel, platinum, ruthenium, and magnesium oxide onto a water-soluble layer of strontium aluminate. They then immersed the film in water, releasing a freestanding magnetic film. The researchers next transferred this magnetic film onto a sapphire surface engineered with speed-bump-like 3D ridges, a bit like one might drape a drop cloth over furniture while painting a room. Standard photolithography techniques then let them create racetracks made of wires—each 3 micrometers wide—with arching segments rising up to 900 nanometers high, following the shape of the sapphire surface. “The 3D racetrack structure prepared and handled in this way survived intact, and behaves almost identically to the originally deposited structure before it was detached,” Parkin says. In experiments, domain walls in the 3D racetracks traveled at speeds of up to 600 meters per second. “We believe that the speed can be increased by more than 10 times in related structures using novel materials,” Parkin says. The scientists are exploring “many different ways of building 3D racetrack memories,” Parkin adds. “There are many interesting challenges that are still to be explored, which makes research into racetrack memory very exciting.” Charles Q. Choi is a science reporter who contributes regularly to IEEE Spectrum. He has written for Scientific American, The New York Times, Wired, and Science, among others. It’s hard to learn, but your code will produce fewer nasty surprises You’d expect the longest and most costly phase in the life cycle of a software product to be the initial development of the system, when all those great features are first imagined and then created. In fact, the hardest part comes later, during the maintenance phase. That’s when programmers pay the price for the shortcuts they took during development. So why did they take shortcuts? Maybe they didn’t realize that they were cutting any corners. Only when their code was deployed and exercised by a lot of users did its hidden flaws come to light. And maybe the developers were rushed. Time-to-market pressures would almost guarantee that their software will contain more bugs than it would otherwise.","['array', 'arch', 'nanowire', 'lead', 'ultrafast', 'ultrahighdensity', 'solidstate', 'nonvolatile', 'memory', 'artist', 'depiction', 'racetrack', 'memory', 'device', 'racetrack', 'memory', 'hold', 'vast', 'amount', 'datum', 'access', 'extraordinarily', 'quickly', 'new', 'study', 'scientist', 'reveal', 'racetrack', 'memory', 'device', 'greatly', 'increase', 'potential', 'technology', 'racetrack', 'memory', 'encode', 'bit', 'datum', 'form', 'magnetic', 'domain', 'wall', 'wall', 'divide', 'material', 'domain', 'magnetic', 'pole', 'point', 'direction', 'electric', 'pulse', 'push', 'domain', 'wall', 'back', 'forth', 'nanowire', 'make', 'run', 'race', 'car', 'track', 'magnetic', 'sensor', 'electronic', 'read', 'write', 'datum', 'point', 'racetrack', 'racetrack', 'memory', 'potentially', 'unrivaled', 'density', 'compare', 'memory', 'technology', 'say', 'director', 'institute', 'microstructure', 'physics', 'develop', 'concept', 'racetrack', 'memory', 'solidstate', 'flash', 'memory', 'lack', 'unwieldy', 'move', 'part', 'nonvolatile', 'store', 'datum', 'even', 'power', 'remove', 'addition', 'racetrack', 'memory', 'electric', 'pulse', 'move', 'domain', 'wall', 'speed', 'kilometer', 'second', 'suggest', 'device', 'use', 'kind', 'memory', 'perform', 'exceptionally', 'quickly', 'memory', 'operate', 'speed', 'less', 'nanosecond', 'write', 'operation', 'possible', 'say', 'fast', 'fast', 'fast', 'write', 'speed', 'commercially', 'available', 'memory', 'today', 'research', 'racetrack', 'memory', 'date', 'focus', 'device', 'however', 'long', 'suggest', 'technology', 'array', 'vertical', 'racetrack', 'much', 'city', 'fill', 'skyscraper', 'extra', 'dimension', 'add', 'racetrack', 'memory', 'greatly', 'increase', 'amount', 'datum', 'hold', 'give', 'footprint', 'assemble', 'complex', 'structure', 'give', 'standard', 'fabrication', 'technique', 'use', 'microelectronic', 'industry', 'prove', 'difficult', 'colleague', 'develop', 'racetrack', 'memory', 'device', 'use', 'new', 'method', 'let', 'drape', 'racetrack', 'surface', 'detail', 'finding', 'online', 'nature', 'nanotechnology', 'scientist', 'fabricate', 'complex', 'ultrathin', 'film', 'make', 'layer', 'nitride', 'cobalt', 'nickel', 'platinum', 'ruthenium', 'magnesium', 'oxide', 'watersoluble', 'layer', 'aluminate', 'immerse', 'film', 'water', 'release', 'freestande', 'magnetic', 'film', 'researcher', 'next', 'transfer', 'magnetic', 'film', 'sapphire', 'surface', 'engineer', 'speedbumplike', 'ridge', 'bit', 'drape', 'drop', 'cloth', 'furniture', 'paint', 'room', 'standard', 'photolithography', 'technique', 'let', 'create', 'racetrack', 'make', 'wire', 'micrometer', 'wide', 'arch', 'segment', 'rise', 'nanometer', 'high', 'follow', 'shape', 'sapphire', 'surface', 'racetrack', 'structure', 'prepare', 'handle', 'way', 'survive', 'intact', 'behave', 'almost', 'identically', 'originally', 'deposit', 'structure', 'detach', 'say', 'experiment', 'domain', 'wall', 'racetrack', 'travel', 'speed', 'meter', 'second', 'believe', 'speed', 'increase', 'time', 'relate', 'structure', 'use', 'novel', 'material', 'say', 'scientist', 'explore', 'many', 'different', 'way', 'build', 'racetrack', 'memory', 'add', 'many', 'interesting', 'challenge', 'still', 'explore', 'make', 'research', 'racetrack', 'memory', 'exciting', 'science', 'reporter', 'contribute', 'regularly', 'ieee', 'spectrum', 'write', 'scientific', 'wire', 'science', '’', 'hard', 'learn', 'code', 'produce', 'nasty', 'surprise', '’d', 'expect', 'long', 'costly', 'phase', 'life', 'cycle', 'software', 'product', 'initial', 'development', 'system', 'great', 'feature', 'first', 'imagine', 'create', 'fact', 'hard', 'part', 'come', 'later', 'maintenance', 'phase', '’', 'programmer', 'pay', 'price', 'shortcut', 'take', 'development', 'take', 'shortcut', 'maybe', 'realize', 'cut', 'corner', 'code', 'deploy', 'exercise', 'lot', 'user', 'hide', 'flaw', 'come', 'light', 'maybe', 'developer', 'rush', 'timetomarket', 'pressure', 'almost', 'guarantee', 'software', 'contain', 'bug', 'otherwise']"
"
        Machine Learning Shaking Up Hard Sciences, Too
    ",https://spectrum.ieee.org/machine-learning-in-physics,2022-10-07,"Heard of graph neural networks? Particle physicists have A view of the underground ALICE detector used to study heavy-ion physics at the Large Hadron Collider (LHC). Particle physicists have long been early adopters—if not inventors—of tech from email to the Internet. It’s not surprising, then, that as early as 1997, researchers were training computer models to tag particles in the messy jets created during collisions. Since then, these models have chugged along, growing steadily more competent—though not to everyone’s delight. “I felt very threatened by machine learning,” says Jesse Thaler, a theoretical particle physicist at the Massachusetts Institute of Technology. Initially, he says he felt like it jeopardized his human expertise classifying particle jets. But Thaler has since come to embrace it, applying machine learning to a variety of problems across particle physics. “Machine learning is a collaborator,” he says.  Over the past decade, in tandem with the broader deep-learning revolution, particle physicists have trained algorithms to solve previously intractable problems and tackle completely new challenges.  Even with an efficient trigger, the LHC must store 600 petabytes over the next few years of data collection. So researchers are investigating strategies to compress the data. For starters, particle-physics data is very different from the typical data used in machine learning. Though convolutional neural networks (CNNs) have proven extremely effective at classifying images of everyday objects from trees to cats to food, they’re less suited for particle collisions. The problem, according to Javier Duarte, a particle physicist at the University of California, San Diego, is that collision data such as that from the Large Hadron Collider, isn’t naturally an image. Flashy depictions of collisions at the LHC can misleadingly fill up the entire detector. In reality, only a few out of millions of inputs are registering a signal, like a white screen with a few black pixels. This sparsely populated data makes for a poor image, but it can work well in a different, newer framework—graph neural networks (GNNs).  Other challenges from particle physics require innovation. “We’re not just importing hammers to hit our nails,” says Daniel Whiteson, a particle physicist at the University of California, Irvine. “We have new weird kinds of nails that require the invention of new hammers.” One weird nail is the sheer amount of data produced at the LHC—about one petabyte per second. Of this enormous amount, only a small bit of high-quality data is saved. To create a better trigger system, which saves as much good data as possible while getting rid of low-quality data, researchers want to train a sharp-eyed algorithm to sort better than one that’s hard coded. But to be effective, such an algorithm would need to be incredibly speedy, executing in microseconds, Duarte says. To address these problems, particle physicists are pushing the limits of machine techniques like pruning and quantization, to make their algorithms even faster. Even with an efficient trigger, the LHC must store 600 petabytes over the next few years of data collection (equivalent to about 660,000 movies at 4K resolution or the data equivalent of 30 Libraries of Congresses), so researchers are investigating strategies to compress the data. “We’d like to have a machine learn to think more like a physicist, [but] we also just need to learn how to think a little bit more like a machine.”—Jesse Thaler, MIT Machine learning is also allowing particle physicists to think differently about the data they use. Instead of focusing on a single event—say, a Higgs boson decaying to two photons—they are learning to consider the dozens of other events that happen during a collision. Although there’s no causal relationship between any two events, researchers like Thaler are now embracing a more holistic view of the data, not just the piecemeal point of view that comes from analyzing events interaction by interaction.  More dramatically, machine learning has also forced physicists to reassess basic concepts. “I was imprecise in my own thinking about what a symmetry was,” Thaler says. “Forcing myself to teach a computer what a symmetry was, helped me understand what a symmetry actually is.” Symmetries require a reference frame—in other words, is the image of a distorted sphere in a mirror actually symmetrical? There’s no way of knowing without knowing if the mirror itself is distorted. These are still early days for machine learning in particle physics, and researchers are effectively treating the technique like a proverbial kitchen sink. “It may not be the right fit for every single problem in particle physics,” admits Duarte.  As some particle physicists delve deeper into machine learning, an uncomfortable question rears its head: Are they doing physics, or computer science? Stigma against coding—sometimes not considered to be “real physics”—already exists; similar concerns swirl around machine learning. One worry is that machine learning will obscure the physics, turning analysis into a black box of automated processes opaque to human understanding. “Our goal is not to plug in the machine, the experiment to the network and have it publish our papers so we’re out of the loop,” Whiteson says. He and colleagues are working to have the algorithms provide feedback in language humans can understand—but algorithms may not be the only ones with responsibilities to communicate. “On the one hand, we’d like to have a machine learn to think more like a physicist, [but] we also just need to learn how to think a little bit more like a machine,” Thaler says. “We need to learn to speak each other’s language.” Dan Garisto is a freelance science journalist who covers physics and other physical sciences. His work has appeared in Scientific American, Physics, Symmetry, Undark, and other outlets. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia  cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving.  
Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines.
 
	The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, 
	Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.
                                             ","Heard of graph neural networks? Particle physicists have A view of the underground ALICE detector used to study heavy-ion physics at the Large Hadron Collider (LHC). Particle physicists have long been early adopters—if not inventors—of tech from email to the Internet. It’s not surprising, then, that as early as 1997, researchers were training computer models to tag particles in the messy jets created during collisions. Since then, these models have chugged along, growing steadily more competent—though not to everyone’s delight. “I felt very threatened by machine learning,” says Jesse Thaler, a theoretical particle physicist at the Massachusetts Institute of Technology. Initially, he says he felt like it jeopardized his human expertise classifying particle jets. But Thaler has since come to embrace it, applying machine learning to a variety of problems across particle physics. “Machine learning is a collaborator,” he says. Over the past decade, in tandem with the broader deep-learning revolution, particle physicists have trained algorithms to solve previously intractable problems and tackle completely new challenges. Even with an efficient trigger, the LHC must store 600 petabytes over the next few years of data collection. So researchers are investigating strategies to compress the data. For starters, particle-physics data is very different from the typical data used in machine learning. Though convolutional neural networks (CNNs) have proven extremely effective at classifying images of everyday objects from trees to cats to food, they’re less suited for particle collisions. The problem, according to Javier Duarte, a particle physicist at the University of California, San Diego, is that collision data such as that from the Large Hadron Collider, isn’t naturally an image. Flashy depictions of collisions at the LHC can misleadingly fill up the entire detector. In reality, only a few out of millions of inputs are registering a signal, like a white screen with a few black pixels. This sparsely populated data makes for a poor image, but it can work well in a different, newer framework—graph neural networks (GNNs). Other challenges from particle physics require innovation. “We’re not just importing hammers to hit our nails,” says Daniel Whiteson, a particle physicist at the University of California, Irvine. “We have new weird kinds of nails that require the invention of new hammers.” One weird nail is the sheer amount of data produced at the LHC—about one petabyte per second. Of this enormous amount, only a small bit of high-quality data is saved. To create a better trigger system, which saves as much good data as possible while getting rid of low-quality data, researchers want to train a sharp-eyed algorithm to sort better than one that’s hard coded. But to be effective, such an algorithm would need to be incredibly speedy, executing in microseconds, Duarte says. To address these problems, particle physicists are pushing the limits of machine techniques like pruning and quantization, to make their algorithms even faster. Even with an efficient trigger, the LHC must store 600 petabytes over the next few years of data collection (equivalent to about 660,000 movies at 4K resolution or the data equivalent of 30 Libraries of Congresses), so researchers are investigating strategies to compress the data. “We’d like to have a machine learn to think more like a physicist, [but] we also just need to learn how to think a little bit more like a machine.”—Jesse Thaler, MIT Machine learning is also allowing particle physicists to think differently about the data they use. Instead of focusing on a single event—say, a Higgs boson decaying to two photons—they are learning to consider the dozens of other events that happen during a collision. Although there’s no causal relationship between any two events, researchers like Thaler are now embracing a more holistic view of the data, not just the piecemeal point of view that comes from analyzing events interaction by interaction. More dramatically, machine learning has also forced physicists to reassess basic concepts. “I was imprecise in my own thinking about what a symmetry was,” Thaler says. “Forcing myself to teach a computer what a symmetry was, helped me understand what a symmetry actually is.” Symmetries require a reference frame—in other words, is the image of a distorted sphere in a mirror actually symmetrical? There’s no way of knowing without knowing if the mirror itself is distorted. These are still early days for machine learning in particle physics, and researchers are effectively treating the technique like a proverbial kitchen sink. “It may not be the right fit for every single problem in particle physics,” admits Duarte. As some particle physicists delve deeper into machine learning, an uncomfortable question rears its head: Are they doing physics, or computer science? Stigma against coding—sometimes not considered to be “real physics”—already exists; similar concerns swirl around machine learning. One worry is that machine learning will obscure the physics, turning analysis into a black box of automated processes opaque to human understanding. “Our goal is not to plug in the machine, the experiment to the network and have it publish our papers so we’re out of the loop,” Whiteson says. He and colleagues are working to have the algorithms provide feedback in language humans can understand—but algorithms may not be the only ones with responsibilities to communicate. “On the one hand, we’d like to have a machine learn to think more like a physicist, [but] we also just need to learn how to think a little bit more like a machine,” Thaler says. “We need to learn to speak each other’s language.” Dan Garisto is a freelance science journalist who covers physics and other physical sciences. His work has appeared in Scientific American, Physics, Symmetry, Undark, and other outlets. Better detection will make the oceans transparent—and perhaps doom mutually assured destruction The Virginia-class fast attack submarine USS Virginia cruises through the Mediterranean in 2010. Back then, it could effectively disappear just by diving. Submarines are valued primarily for their ability to hide. The assurance that submarines would likely survive the first missile strike in a nuclear war and thus be able to respond by launching missiles in a second strike is key to the strategy of deterrence known as mutually assured destruction. Any new technology that might render the oceans effectively transparent, making it trivial to spot lurking submarines, could thus undermine the peace of the world. For nearly a century, naval engineers have striven to develop ever-faster, ever-quieter submarines. But they have worked just as hard at advancing a wide array of radar, sonar, and other technologies designed to detect, target, and eliminate enemy submarines. The balance seemed to turn with the emergence of nuclear-powered submarines in the early 1960s. In a 2015 study for the Center for Strategic and Budgetary Assessment, Bryan Clark, a naval specialist now at the Hudson Institute, noted that the ability of these boats to remain submerged for long periods of time made them “nearly impossible to find with radar and active sonar.” But even these stealthy submarines produce subtle, very-low-frequency noises that can be picked up from far away by networks of acoustic hydrophone arrays mounted to the seafloor.","['hear', 'graph', 'neural', 'network', 'particle', 'physicist', 'view', 'underground', 'alice', 'detector', 'use', 'study', 'heavyion', 'physics', 'large', 'hadron', 'collider', 'lhc', 'particle', 'physicist', 'long', 'early', 'adopter', 'inventor', 'tech', 'email', 'internet', '’', 'surprising', 'early', 'researcher', 'train', 'computer', 'model', 'tag', 'particle', 'messy', 'jet', 'create', 'collision', 'model', 'chug', 'grow', 'steadily', 'competent', '’s', 'delight', 'feel', 'threaten', 'machine', 'learning', 'say', 'theoretical', 'particle', 'physicist', 'technology', 'initially', 'say', 'feel', 'jeopardize', 'human', 'expertise', 'classify', 'particle', 'jet', 'thaler', 'come', 'embrace', 'apply', 'machine', 'learn', 'variety', 'problem', 'particle', 'physics', 'machine', 'learning', 'collaborator', 'say', 'past', 'decade', 'tandem', 'broad', 'deeplearning', 'revolution', 'particle', 'physicist', 'train', 'algorithm', 'solve', 'previously', 'intractable', 'problem', 'tackle', 'completely', 'new', 'challenge', 'even', 'efficient', 'trigger', 'lhc', 'store', 'petabyte', 'next', 'year', 'datum', 'collection', 'researcher', 'investigate', 'strategy', 'compress', 'datum', 'starter', 'particlephysic', 'datum', 'different', 'typical', 'datum', 'use', 'machine', 'learning', 'convolutional', 'neural', 'network', 'cnn', 'prove', 'extremely', 'effective', 'classify', 'image', 'everyday', 'object', 'tree', 'cat', 'food', '’re', 'less', 'suited', 'particle', 'collision', 'problem', 'accord', 'javi', 'duarte', 'particle', 'physicist', 'collision', 'datum', 'large', 'hadron', 'collider', 'naturally', 'image', 'flashy', 'depiction', 'collision', 'lhc', 'misleadingly', 'fill', 'entire', 'detector', 'reality', 'million', 'input', 'register', 'signal', 'white', 'screen', 'black', 'pixel', 'sparsely', 'populated', 'datum', 'make', 'poor', 'image', 'work', 'well', 'different', 'new', 'framework', 'graph', 'neural', 'network', 'gnn', 'challenge', 'particle', 'physics', 'require', 'innovation', 'import', 'hammer', 'hit', 'nail', 'say', 'particle', 'physicist', 'new', 'weird', 'kind', 'nail', 'require', 'invention', 'new', 'hammer', 'weird', 'nail', 'sheer', 'amount', 'datum', 'produce', 'lhc', 'petabyte', 'second', 'enormous', 'amount', 'small', 'bit', 'highquality', 'datum', 'save', 'create', 'well', 'trigger', 'system', 'save', 'much', 'good', 'datum', 'possible', 'rid', 'lowquality', 'datum', 'researcher', 'want', 'train', 'sharpeyed', 'sort', 'well', 'hard', 'code', 'effective', 'need', 'incredibly', 'speedy', 'executing', 'microsecond', 'duarte', 'say', 'address', 'problem', 'particle', 'physicist', 'push', 'limit', 'machine', 'technique', 'prune', 'quantization', 'make', 'algorithm', 'even', 'fast', 'even', 'efficient', 'trigger', 'lhc', 'store', 'petabyte', 'next', 'year', 'datum', 'collection', 'equivalent', 'movie', 'resolution', 'datum', 'equivalent', 'library', 'congress', 'researcher', 'investigate', 'strategy', 'compress', 'datum', '’d', 'like', 'machine', 'learn', 'think', 'physicist', 'also', 'need', 'learn', 'think', 'little', 'bit', 'machine', 'mit', 'machine', 'learning', 'also', 'allow', 'particle', 'physicist', 'think', 'differently', 'datum', 'use', 'instead', 'focus', 'single', 'event', 'say', 'higgs', 'boson', 'decay', 'photon', 'learn', 'consider', 'dozen', 'event', 'happen', 'collision', '’', 'causal', 'relationship', 'event', 'researcher', 'thaler', 'embrace', 'holistic', 'view', 'datum', 'piecemeal', 'point', 'view', 'come', 'analyze', 'event', 'interaction', 'interaction', 'dramatically', 'machine', 'learning', 'also', 'force', 'physicist', 'reassess', 'basic', 'concept', 'imprecise', 'thinking', 'symmetry', 'thaler', 'say', 'force', 'teach', 'computer', 'symmetry', 'help', 'understand', 'symmetry', 'actually', 'symmetry', 'require', 'reference', 'frame', 'word', 'image', 'distorted', 'sphere', 'mirror', 'actually', 'symmetrical', '’', 'way', 'know', 'know', 'mirror', 'distort', 'still', 'early', 'day', 'machine', 'learning', 'particle', 'physic', 'researcher', 'effectively', 'treat', 'technique', 'proverbial', 'kitchen', 'sink', 'right', 'fit', 'single', 'problem', 'particle', 'physics', 'admit', 'duarte', 'particle', 'physicist', 'delve', 'deep', 'machine', 'learn', 'uncomfortable', 'question', 'rear', 'head', 'physics', 'computer', 'science', 'stigma', 'code', 'sometimes', 'consider', 'real', 'physic', 'already', 'exist', 'similar', 'concern', 'swirl', 'machine', 'learn', 'worry', 'machine', 'learning', 'obscure', 'physics', 'turn', 'analysis', 'black', 'box', 'automate', 'process', 'opaque', 'human', 'understanding', 'goal', 'plug', 'machine', 'experiment', 'network', 'publish', 'paper', '’re', 'loop', 'whiteson', 'say', 'colleague', 'work', 'algorithm', 'provide', 'feedback', 'language', 'human', 'understand', 'algorithm', 'one', 'responsibility', 'communicate', 'hand', '’d', 'like', 'machine', 'learn', 'think', 'physicist', 'also', 'need', 'learn', 'think', 'little', 'bit', 'machine', 'thaler', 'say', 'need', 'learn', 'speak', 'language', 'freelance', 'science', 'journalist', 'cover', 'physics', 'physical', 'science', 'work', 'appear', 'scientific', 'american', 'symmetry', 'undark', 'outlet', 'well', 'detection', 'make', 'ocean', 'transparent', 'perhaps', 'doom', 'mutually', 'assure', 'destruction', 'virginiaclass', 'fast', 'attack', 'submarine', 'cruise', 'back', 'effectively', 'disappear', 'diving', 'submarine', 'value', 'primarily', 'ability', 'hide', 'assurance', 'submarine', 'likely', 'survive', 'first', 'missile', 'strike', 'nuclear', 'war', 'thus', 'able', 'respond', 'launch', 'missile', 'second', 'strike', 'key', 'strategy', 'deterrence', 'know', 'mutually', 'assure', 'destruction', 'new', 'technology', 'render', 'ocean', 'effectively', 'transparent', 'make', 'trivial', 'spot', 'lurk', 'submarine', 'thus', 'undermine', 'peace', 'world', 'nearly', 'century', 'naval', 'engineer', 'strive', 'develop', 'everfaster', 'everquieter', 'submarine', 'work', 'hard', 'advance', 'wide', 'array', 'radar', 'sonar', 'technology', 'design', 'detect', 'target', 'eliminate', 'enemy', 'submarine', 'balance', 'seem', 'turn', 'emergence', 'nuclearpowered', 'submarine', 'early', '1960', 'study', 'center', 'strategic', 'budgetary', 'assessment', 'clark', 'naval', 'specialist', 'note', 'ability', 'boat', 'remain', 'submerge', 'long', 'period', 'time', 'make', 'nearly', 'impossible', 'find', 'radar', 'active', 'sonar', 'even', 'stealthy', 'submarine', 'produce', 'subtle', 'verylowfrequency', 'noise', 'pick', 'far', 'away', 'network', 'acoustic', 'hydrophone', 'array', 'mount', 'seafloor']"
"
        AI’s Grandmaster Status Overshadows Chess Scandal
    ",https://spectrum.ieee.org/magnus-carlsen-chess-scandal-ai,2022-10-05,"Magnus Carlsen–Hans Niemann controversy underscores humans’ perpetual underdog role  Magnus Carlsen [left] and Hans Niemann compete during the 2022 Sinquefield Cup at the Saint Louis Chess Club. Last week Magnus Carlsen, the world chess champion, directly accused Hans Niemann, a U.S. grandmaster, of cheating during their game at the Sinquefield Cup, in St. Louis, Mo. He thus made plain an accusation he had been hinting at for weeks. Carlsen has so far provided no evidence to back up his charge, nor has he specified how the cheating took place. Everyone agrees, however, that if there was cheating, then it must have involved computers, because nothing else could dismay Carlsen, whose rating of 2856 is higher than that of any other player. And everyone seems to have chosen sides. Those who back Carlsen point to Niemann’s own admission that he used computers to cheat in online play at least twice—once at age 14 and again at 16; Niemann is now 19. Others note that his performance has risen very rapidly in the past two years. Still others raise an eyebrow at the large number of games he has played in recent years that get a score of nearly perfect from computer analysis. And behind it all are statements from leading players that they are convinced that cheating happens all the time nowadays, though hardly anybody ever gets caught. Computers loom so large because they now play chess like gods.  What makes the scandal so big is not merely the level of the players. In 1961 the great Bobby Fischer wrote an article for Sports Illustrated titled “The Russians Have Fixed World Chess.” He alleged that Soviet chess players arranged draws to ensure that one of them would win a tournament.  Nor is the scandal notable for flagrancy. In 1967 Milan Matulović, a Yugoslavian grandmaster, shockingly took back a move he had just played and only then said “J’adoube,” the French phrase uttered when a player merely adjusts the position of a chessman. Players thereafter called him “J’adoubavić.” No, what makes today’s accusations resonate is the pervasive role of chess computers. They give children around the world sparring partners that earlier generations couldn’t have dreamed of facing, even if they’d lived next to the Moscow Central Chess Club. No wonder prodigies of the game have gotten younger and younger.   And computers do so well in helping the home preparation of the opening, the early moves of a game, that players, including Carlsen, will sometimes deliberately play a second-best move just to force the opponents out of “book.” Finally, computer analysis offered during Internet broadcasts of ongoing tournaments will look 12 moves ahead within a second or two. They show the amateurs in the audience much that the grandmasters miss, creating the illusion that the amateurs actually understand what’s going on. Of course, any viewer could give illicit help to a player if provided a means of communication. Several things are at stake. There is the prize money, which runs in the hundreds of thousands of dollars for the circuit of which the Sinquefeld Cup tournament is a part. There are the invitations to future events, which are often contingent on doing well in qualifying events. Then there are the rating points. Carlsen cares deeply about this metric: Although he recently declined to contest his World Championship title in 2023, he insists that he will continue to play in the hope of raising his rating to an unprecedented 2900. The cheating to which Niemann does admit—in his younger years, during online play—was itself detected with the aid of computers of Chess.com, the online playing forum in question. Recently, however, the Wall Street Journal reported that an internal investigation by Chess.com has found that Niemann in fact cheated in more than 100 online games, most recently when he was 17. The company did not impugn the grandmaster’s over-the-board play.  A key hint can be encoded in just a few bits of data, which means it might be transmitted, perhaps via a buzzer in the player’s shoe, on his body—or inside it. Online play is fast and loose, and its computerized basis may provide clues that a cheat-detection algorithm can catch. But over-the-board offers less data. Often there are only one or two key points in a game at which cheating might occur; a little hint, offered at such a point, is enough to make the difference to a grandmaster. Even a duffer, when showed a chess problem, may be truly stumped. But told that it is “mate in three moves,” the duffer may see the light. Just a phrase—“the rook,” say, or “double attack”—may also make the idea apparent.  A key hint can be encoded in just a few bits of data, which means it might be transmitted, perhaps via a buzzer in the player’s shoe, on his body—or inside it. Do not laugh, but innuendos have been made concerning the possible use of a buzzing sex toy. As a joke, Niemann declared that he was willing to play naked. A camsite called Stripchat promptly offered him [US] $1 million to do so. Computers loom so large because they now play chess like gods. The best free program, Stockfish 14, is rated at 3534—678 points ahead of Carlsen. That’s enough of a gap to predict a winning expectancy of 99 percent. In the early days, when chess programs were a lab project for AI, they played like idiots. Then the programmers began to enter their creations in competitions, and the programs got good. I learned that the hard way. In late 1974, at a student tournament held in Evanston, Ill., I was paired against Northwestern University’s Chess 4.0 program, played the Sicilian Defense, blundered a knight for two pawns, mentally kicked myself, and hastily resigned. David Slate, the programmer, waited patiently as I completed the ritual of resignation, which involves signing the score sheet and handing it to the tournament director—in this case, him. Only then did he tell me that if I’d just played on, I would have gotten a draw. 

The Strongest Computer Chess Engines Over Time

www.youtube.com

 “It can’t play endgames,” Slate said. I kicked myself again. Back then I was rated somewhere in the 1600s, about average for an amateur. Still, I was the highest-rated player any machine had yet beaten in a tournament game. It’s my claim to fame. Chess 4.0 went on to beat another guy higher rated than me, somewhat soothing my wounded pride. It took years for the Northwestern program to reach 2000. Other university programs then took the lead, until at last a machine originating at Carnegie Mellon and redomiciled at IBM reached 2600, about grandmaster strength. That was strong enough to beat my old, 1600-rated self 99.9 percent of the time. In 1997 an even stronger version of the IBM machine, dubbed Deep Blue, beat Gary Kasparov, the reigning world champion. Deep Blue filled a room. Today, a smartphone can crush any human player. Philip E. Ross is a senior editor at IEEE Spectrum. His interests include transportation, energy storage, AI, and the economic aspects of technology. He has a master's degree in international affairs from Columbia University and another, in journalism, from the University of Michigan. It’s hard to learn, but your code will produce fewer nasty surprises 
You’d expect the longest and most costly phase in the life cycle of a software product to be the initial development of the system, when all those great features are first imagined and then created. In fact, the hardest part comes later, during the maintenance phase. That’s when programmers pay the price for the shortcuts they took during development.
 
	So why did they take shortcuts? Maybe they didn’t realize that they were cutting any corners. Only when their code was deployed and exercised by a lot of users did its hidden flaws come to light. And maybe the developers were rushed. Time-to-market pressures would almost guarantee that their software will contain more bugs than it would otherwise.
                                                   ","Magnus Carlsen–Hans Niemann controversy underscores humans’ perpetual underdog role Magnus Carlsen [left] and Hans Niemann compete during the 2022 Sinquefield Cup at the Saint Louis Chess Club. Last week Magnus Carlsen, the world chess champion, directly accused Hans Niemann, a U.S. grandmaster, of cheating during their game at the Sinquefield Cup, in St. Louis, Mo. He thus made plain an accusation he had been hinting at for weeks. Carlsen has so far provided no evidence to back up his charge, nor has he specified how the cheating took place. Everyone agrees, however, that if there was cheating, then it must have involved computers, because nothing else could dismay Carlsen, whose rating of 2856 is higher than that of any other player. And everyone seems to have chosen sides. Those who back Carlsen point to Niemann’s own admission that he used computers to cheat in online play at least twice—once at age 14 and again at 16; Niemann is now 19. Others note that his performance has risen very rapidly in the past two years. Still others raise an eyebrow at the large number of games he has played in recent years that get a score of nearly perfect from computer analysis. And behind it all are statements from leading players that they are convinced that cheating happens all the time nowadays, though hardly anybody ever gets caught. Computers loom so large because they now play chess like gods. What makes the scandal so big is not merely the level of the players. In 1961 the great Bobby Fischer wrote an article for Sports Illustrated titled “The Russians Have Fixed World Chess.” He alleged that Soviet chess players arranged draws to ensure that one of them would win a tournament. Nor is the scandal notable for flagrancy. In 1967 Milan Matulović, a Yugoslavian grandmaster, shockingly took back a move he had just played and only then said “J’adoube,” the French phrase uttered when a player merely adjusts the position of a chessman. Players thereafter called him “J’adoubavić.” No, what makes today’s accusations resonate is the pervasive role of chess computers. They give children around the world sparring partners that earlier generations couldn’t have dreamed of facing, even if they’d lived next to the Moscow Central Chess Club. No wonder prodigies of the game have gotten younger and younger. And computers do so well in helping the home preparation of the opening, the early moves of a game, that players, including Carlsen, will sometimes deliberately play a second-best move just to force the opponents out of “book.” Finally, computer analysis offered during Internet broadcasts of ongoing tournaments will look 12 moves ahead within a second or two. They show the amateurs in the audience much that the grandmasters miss, creating the illusion that the amateurs actually understand what’s going on. Of course, any viewer could give illicit help to a player if provided a means of communication. Several things are at stake. There is the prize money, which runs in the hundreds of thousands of dollars for the circuit of which the Sinquefeld Cup tournament is a part. There are the invitations to future events, which are often contingent on doing well in qualifying events. Then there are the rating points. Carlsen cares deeply about this metric: Although he recently declined to contest his World Championship title in 2023, he insists that he will continue to play in the hope of raising his rating to an unprecedented 2900. The cheating to which Niemann does admit—in his younger years, during online play—was itself detected with the aid of computers of Chess.com, the online playing forum in question. Recently, however, the Wall Street Journal reported that an internal investigation by Chess.com has found that Niemann in fact cheated in more than 100 online games, most recently when he was 17. The company did not impugn the grandmaster’s over-the-board play. A key hint can be encoded in just a few bits of data, which means it might be transmitted, perhaps via a buzzer in the player’s shoe, on his body—or inside it. Online play is fast and loose, and its computerized basis may provide clues that a cheat-detection algorithm can catch. But over-the-board offers less data. Often there are only one or two key points in a game at which cheating might occur; a little hint, offered at such a point, is enough to make the difference to a grandmaster. Even a duffer, when showed a chess problem, may be truly stumped. But told that it is “mate in three moves,” the duffer may see the light. Just a phrase—“the rook,” say, or “double attack”—may also make the idea apparent. A key hint can be encoded in just a few bits of data, which means it might be transmitted, perhaps via a buzzer in the player’s shoe, on his body—or inside it. Do not laugh, but innuendos have been made concerning the possible use of a buzzing sex toy. As a joke, Niemann declared that he was willing to play naked. A camsite called Stripchat promptly offered him [US] $1 million to do so. Computers loom so large because they now play chess like gods. The best free program, Stockfish 14, is rated at 3534—678 points ahead of Carlsen. That’s enough of a gap to predict a winning expectancy of 99 percent. In the early days, when chess programs were a lab project for AI, they played like idiots. Then the programmers began to enter their creations in competitions, and the programs got good. I learned that the hard way. In late 1974, at a student tournament held in Evanston, Ill., I was paired against Northwestern University’s Chess 4.0 program, played the Sicilian Defense, blundered a knight for two pawns, mentally kicked myself, and hastily resigned. David Slate, the programmer, waited patiently as I completed the ritual of resignation, which involves signing the score sheet and handing it to the tournament director—in this case, him. Only then did he tell me that if I’d just played on, I would have gotten a draw. The Strongest Computer Chess Engines Over Time www.youtube.com “It can’t play endgames,” Slate said. I kicked myself again. Back then I was rated somewhere in the 1600s, about average for an amateur. Still, I was the highest-rated player any machine had yet beaten in a tournament game. It’s my claim to fame. Chess 4.0 went on to beat another guy higher rated than me, somewhat soothing my wounded pride. It took years for the Northwestern program to reach 2000. Other university programs then took the lead, until at last a machine originating at Carnegie Mellon and redomiciled at IBM reached 2600, about grandmaster strength. That was strong enough to beat my old, 1600-rated self 99.9 percent of the time. In 1997 an even stronger version of the IBM machine, dubbed Deep Blue, beat Gary Kasparov, the reigning world champion. Deep Blue filled a room. Today, a smartphone can crush any human player. Philip E. Ross is a senior editor at IEEE Spectrum. His interests include transportation, energy storage, AI, and the economic aspects of technology. He has a master's degree in international affairs from Columbia University and another, in journalism, from the University of Michigan. It’s hard to learn, but your code will produce fewer nasty surprises You’d expect the longest and most costly phase in the life cycle of a software product to be the initial development of the system, when all those great features are first imagined and then created. In fact, the hardest part comes later, during the maintenance phase. That’s when programmers pay the price for the shortcuts they took during development. So why did they take shortcuts? Maybe they didn’t realize that they were cutting any corners. Only when their code was deployed and exercised by a lot of users did its hidden flaws come to light. And maybe the developers were rushed. Time-to-market pressures would almost guarantee that their software will contain more bugs than it would otherwise.","['controversy', 'underscore', 'human', 'perpetual', 'underdog', 'role', 'magnus', 'leave', 'compete', 'saint', 'chess', 'club', 'last', 'week', 'magnus', 'carlsen', 'world', 'chess', 'champion', 'directly', 'accuse', 'grandmaster', 'cheat', 'game', 'thus', 'make', 'plain', 'accusation', 'hint', 'week', 'far', 'provide', 'evidence', 'back', 'charge', 'specify', 'cheating', 'take', 'place', 'agree', 'however', 'cheat', 'involve', 'computer', 'else', 'dismay', 'carlsen', 'rating', 'high', 'player', 'seem', 'choose', 'side', 'back', 'point', 'admission', 'use', 'computer', 'cheat', 'online', 'play', 'least', 'twice', 'age', 'niemann', 'note', 'performance', 'rise', 'rapidly', 'past', 'year', 'still', 'raise', 'eyebrow', 'large', 'number', 'game', 'play', 'recent', 'year', 'get', 'score', 'nearly', 'perfect', 'computer', 'analysis', 'statement', 'lead', 'player', 'convinced', 'cheating', 'happen', 'time', 'nowadays', 'hardly', 'ever', 'catch', 'computer', 'loom', 'large', 'play', 'chess', 'god', 'make', 'scandal', 'big', 'merely', 'level', 'player', 'great', 'bobby', 'fischer', 'write', 'article', 'sport', 'illustrate', 'title', 'fix', 'world', 'chess', 'allege', 'soviet', 'chess', 'player', 'arrange', 'draw', 'ensure', 'win', 'tournament', 'scandal', 'notable', 'flagrancy', 'matulović', 'yugoslavian', 'grandmaster', 'shockingly', 'take', 'back', 'move', 'play', 'say', 'french', 'phrase', 'utter', 'player', 'merely', 'adjust', 'position', 'chessman', 'player', 'thereafter', 'call', 'make', 'today', 'accusation', 'resonate', 'pervasive', 'role', 'chess', 'computer', 'give', 'child', 'world', 'spar', 'partner', 'early', 'generation', 'dream', 'face', 'even', '’d', 'live', 'next', 'central', 'chess', 'club', 'wonder', 'prodigy', 'game', 'get', 'young', 'young', 'computer', 'well', 'help', 'home', 'preparation', 'opening', 'early', 'move', 'game', 'player', 'include', 'sometimes', 'deliberately', 'play', 'secondbest', 'move', 'force', 'opponent', 'book', 'finally', 'computer', 'analysis', 'offer', 'internet', 'broadcast', 'ongoing', 'tournament', 'look', 'move', 'ahead', 'second', 'show', 'amateur', 'audience', 'much', 'grandmaster', 'miss', 'create', 'illusion', 'amateur', 'actually', 'understand', 'go', 'course', 'viewer', 'give', 'illicit', 'help', 'player', 'provide', 'mean', 'communication', 'several', 'thing', 'stake', 'prize', 'money', 'run', 'hundred', 'thousand', 'dollar', 'circuit', 'part', 'invitation', 'future', 'event', 'often', 'contingent', 'well', 'qualify', 'event', 'rating', 'point', 'care', 'deeply', 'metric', 'recently', 'decline', 'contest', 'world', 'championship', 'title', 'insist', 'continue', 'play', 'hope', 'raise', 'rating', 'unprecedented', 'cheating', 'admit', 'young', 'year', 'online', 'play', 'detect', 'aid', 'computer', 'chesscom', 'online', 'playing', 'forum', 'question', 'recently', 'report', 'internal', 'investigation', 'chesscom', 'find', 'fact', 'cheat', 'online', 'game', 'recently', 'company', 'impugn', 'grandmaster', 'play', 'key', 'hint', 'encode', 'bit', 'datum', 'mean', 'transmit', 'perhaps', 'buzzer', 'player', 'shoe', 'body', 'online', 'play', 'fast', 'loose', 'computerized', 'basis', 'provide', 'clue', 'cheatdetection', 'catch', 'overtheboard', 'offer', 'less', 'datum', 'often', 'key', 'point', 'game', 'cheating', 'occur', 'little', 'hint', 'offer', 'point', 'enough', 'make', 'difference', 'grandmaster', 'even', 'duffer', 'show', 'chess', 'problem', 'truly', 'stump', 'tell', 'mate', 'move', 'duffer', 'see', 'light', 'phrase', 'rook', 'say', 'double', 'attack', 'also', 'make', 'idea', 'apparent', 'key', 'hint', 'encode', 'bit', 'datum', 'mean', 'transmit', 'perhaps', 'buzzer', 'player', 'shoe', 'body', 'laugh', 'innuendo', 'make', 'concern', 'possible', 'use', 'buzz', 'sex', 'toy', 'joke', 'declare', 'willing', 'play', 'naked', 'camsite', 'call', 'stripchat', 'promptly', 'offer', 'computer', 'loom', 'large', 'play', 'chess', 'god', 'well', 'free', 'program', 'stockfish', 'rate', 'point', 'ahead', '’s', 'enough', 'gap', 'predict', 'win', 'expectancy', 'percent', 'early', 'day', 'chess', 'program', 'lab', 'project', 'play', 'idiot', 'programmer', 'begin', 'enter', 'creation', 'competition', 'program', 'get', 'good', 'learn', 'hard', 'way', 'late', 'student', 'tournament', 'hold', 'pair', 'chess', 'program', 'play', 'defense', 'blunder', 'knight', 'pawn', 'mentally', 'kick', 'hastily', 'resign', 'programmer', 'wait', 'patiently', 'complete', 'ritual', 'resignation', 'involve', 'sign', 'score', 'sheet', 'hand', 'tournament', 'director', 'case', 'tell', '’d', 'play', 'get', 'draw', 'strong', 'computer', 'chess', 'engine', 'time', 'wwwyoutubecom', 'play', 'endgame', 'slate', 'say', 'kick', 'back', 'rate', 'somewhere', '1600', 'average', 'amateur', 'still', 'highestrated', 'player', 'machine', 'yet', 'beat', 'tournament', 'game', '’', 'claim', 'fame', 'chess', 'go', 'beat', 'guy', 'higher', 'rate', 'somewhat', 'soothe', 'wounded', 'pride', 'take', 'year', 'northwestern', 'program', 'reach', 'university', 'program', 'take', 'lead', 'last', 'machine', 'originate', 'carnegie', 'mellon', 'redomicile', 'reach', 'grandmaster', 'strength', 'strong', 'enough', 'beat', 'old', '1600rated', 'self', 'percent', 'time', 'even', 'strong', 'version', 'machine', 'dub', 'deep', 'blue', 'beat', 'reign', 'world', 'champion', 'deep', 'blue', 'fill', 'room', 'today', 'smartphone', 'crush', 'human', 'player', 'philip', 'senior', 'editor', 'ieee', 'spectrum', 'interest', 'include', 'transportation', 'energy', 'storage', 'ai', 'economic', 'aspect', 'technology', 'masters', 'degree', 'international', 'affair', 'journalism', 'university', '’', 'hard', 'learn', 'code', 'produce', 'nasty', 'surprise', '’d', 'expect', 'long', 'costly', 'phase', 'life', 'cycle', 'software', 'product', 'initial', 'development', 'system', 'great', 'feature', 'first', 'imagine', 'create', 'fact', 'hard', 'part', 'come', 'later', 'maintenance', 'phase', '’', 'programmer', 'pay', 'price', 'shortcut', 'take', 'development', 'take', 'shortcut', 'maybe', 'realize', 'cut', 'corner', 'code', 'deploy', 'exercise', 'lot', 'user', 'hide', 'flaw', 'come', 'light', 'maybe', 'developer', 'rush', 'timetomarket', 'pressure', 'almost', 'guarantee', 'software', 'contain', 'bug', 'otherwise']"
