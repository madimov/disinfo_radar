title,url,date,summary,text,cleaning,tokens
Google Leverages Transformers to Vastly Simplify Neural Video Compression With SOTA Results,https://syncedreview.com/2022/06/17/google-leverages-transformers-to-vastly-simplify-neural-video-compression-with-sota-results/,2022-06-17,"
In the new paper VCT: A Video Compression Transformer, a Google Research team presents an elegantly simple but powerful video compression transformer (VCT) that does not require architectural biases and priors and learns totally from data without any hand-crafting. VCT is easy to implement and outperforms conventional video compression approaches.

 ","Neural network-based approaches have made significant progress on video compression over the last several years, reaching performance on par with classical codec-based methods. These novel neural approaches however are challenging to implement, as they tend to require complex hand-crafted connections between their many sub-components and struggle when the input data does not match their architectural biases and priors. In the new paper VCT: A Video Compression Transformer, a Google Research team presents an “elegantly simple” but powerful video compression transformer (VCT) that eliminates the architectural biases and priors of previous approaches (such as motion prediction and warping operations), and instead learns totally from data without any hand-crafting. VCT is easy to implement and outperforms existing video compression methods on standard datasets. The proposed VCT is based on the original language translation transformer (Vaswani et al., 2017) and is tasked with translating the previous two frames of a video input into the current frame. It first uses lossy transform coding to project frames from the image space to quantized representations. A transformer then leverages temporal redundancies to model the representation distributions. These predicted distributions are then used to compress the quantized representations via entropy coding. In their empirical studies, the team trained VCT on one million Internet video clips and compared it to video compression approaches such as the classical HEVC (High-Efficiency Video Coding) and neural methods such as SSF (Scale-Space Flow, Agustsson et al., 2020) and ELF-VC (Efficient Learned Flexible-Rate Video Coding, Rippel et al., 2021). The evaluations were conducted on the MCL-JCV and UVG benchmark datasets, with PSNR (peak signal-to-noise ratio) and MS-SSIM (multi-scale structural similarity index for motion detection) as metrics. Despite its simplicity — and not using flow prediction, warping or residual compensation — VCT surpassed all methods in both PSNR and MS-SSIM in the evaluations. Moreover, experiments on synthetic data showed that VCT can also learn to handle complex motion patterns such as panning, blurring and fading, purely from data. The team says VCT can reduce bandwidth requirements for video conferencing and streaming and enable better utilization of storage space, and hope it can serve as a foundation for a new generation of video codecs. The VCT code has been released on the project’s GitHub. The paper VCT: A Video Compression Transformer is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Neural network-based approaches have made significant progress on video compression over the last several years, reaching performance on par with classical codec-based methods. These novel neural approaches however are challenging to implement, as they tend to require complex hand-crafted connections between their many sub-components and struggle when the input data does not match their architectural biases and priors. In the new paper VCT: A Video Compression Transformer, a Google Research team presents an “elegantly simple” but powerful video compression transformer (VCT) that eliminates the architectural biases and priors of previous approaches (such as motion prediction and warping operations), and instead learns totally from data without any hand-crafting. VCT is easy to implement and outperforms existing video compression methods on standard datasets. The proposed VCT is based on the original language translation transformer (Vaswani et al., 2017) and is tasked with translating the previous two frames of a video input into the current frame. It first uses lossy transform coding to project frames from the image space to quantized representations. A transformer then leverages temporal redundancies to model the representation distributions. These predicted distributions are then used to compress the quantized representations via entropy coding. In their empirical studies, the team trained VCT on one million Internet video clips and compared it to video compression approaches such as the classical HEVC (High-Efficiency Video Coding) and neural methods such as SSF (Scale-Space Flow, Agustsson et al., 2020) and ELF-VC (Efficient Learned Flexible-Rate Video Coding, Rippel et al., 2021). The evaluations were conducted on the MCL-JCV and UVG benchmark datasets, with PSNR (peak signal-to-noise ratio) and MS-SSIM (multi-scale structural similarity index for motion detection) as metrics. Despite its simplicity — and not using flow prediction, warping or residual compensation — VCT surpassed all methods in both PSNR and MS-SSIM in the evaluations. Moreover, experiments on synthetic data showed that VCT can also learn to handle complex motion patterns such as panning, blurring and fading, purely from data. The team says VCT can reduce bandwidth requirements for video conferencing and streaming and enable better utilization of storage space, and hope it can serve as a foundation for a new generation of video codecs. The VCT code has been released on the project’s GitHub. The paper VCT: A Video Compression Transformer is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['neural', 'networkbased', 'approach', 'make', 'significant', 'progress', 'video', 'compression', 'last', 'several', 'year', 'reach', 'performance', 'par', 'classical', 'codecbase', 'method', 'novel', 'neural', 'approach', 'however', 'challenge', 'implement', 'tend', 'require', 'complex', 'handcraft', 'connection', 'many', 'subcomponent', 'struggle', 'input', 'datum', 'match', 'architectural', 'bias', 'prior', 'new', 'paper', 'vct', 'video', 'compression', 'transformer', 'research', 'team', 'present', 'elegantly', 'simple', 'powerful', 'video', 'compression', 'transformer', 'vct', 'eliminate', 'architectural', 'bias', 'prior', 'previous', 'approach', 'motion', 'prediction', 'warp', 'operation', 'instead', 'learn', 'totally', 'datum', 'handcraft', 'vct', 'easy', 'implement', 'outperform', 'exist', 'video', 'compression', 'method', 'standard', 'dataset', 'propose', 'vct', 'base', 'original', 'language', 'translation', 'transformer', 'vaswani', 'task', 'translate', 'previous', 'frame', 'video', 'input', 'current', 'frame', 'first', 'use', 'lossy', 'transform', 'code', 'project', 'frame', 'image', 'space', 'quantize', 'representation', 'transformer', 'leverage', 'temporal', 'redundancy', 'model', 'representation', 'distribution', 'predict', 'distribution', 'use', 'compress', 'quantize', 'representation', 'entropy', 'coding', 'empirical', 'study', 'team', 'train', 'vct', 'internet', 'video', 'clip', 'compare', 'video', 'compression', 'approach', 'classical', 'hevc', 'highefficiency', 'video', 'coding', 'neural', 'method', 'ssf', 'agustsson', 'elfvc', 'efficient', 'learn', 'flexiblerate', 'video', 'coding', 'rippel', 'evaluation', 'conduct', 'mcljcv', 'uvg', 'benchmark', 'dataset', 'psnr', 'peak', 'signaltonoise', 'ratio', 'msssim', 'multiscale', 'structural', 'similarity', 'index', 'motion', 'detection', 'metric', 'simplicity', 'use', 'flow', 'prediction', 'warping', 'residual', 'compensation', 'vct', 'surpass', 'method', 'psnr', 'msssim', 'evaluation', 'moreover', 'experiment', 'synthetic', 'datum', 'show', 'vct', 'also', 'learn', 'handle', 'complex', 'motion', 'pattern', 'pan', 'blurring', 'fade', 'purely', 'datum', 'team', 'say', 'vct', 'reduce', 'bandwidth', 'requirement', 'video', 'conferencing', 'streaming', 'enable', 'well', 'utilization', 'storage', 'space', 'hope', 'serve', 'foundation', 'new', 'generation', 'video', 'codec', 'vct', 'code', 'release', 'project', 'paper', 'vct', 'video', 'compression', 'transformer', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
Wav2Vec 2.0 Learns Brain-Like Representations From Just 600 Hours of Unlabeled Speech Data in New Study,https://syncedreview.com/2022/06/16/wav2vec-2-0-learns-brain-like-representations-from-just-600-hours-of-unlabeled-speech-data-in-new-study/,2022-06-16,"
In the new paper Toward a Realistic Model of Speech Processing in the Brain with Self-supervised Learning, researchers show that self-supervised architectures such as Wav2Vec 2.0 can learn brain-like representations from as little as 600 hours of unlabelled speech; and can also learn sound-generic and speech- and language-specific representations similar to those of the prefrontal and temporal cortices. 
","Deep neural networks have recently hinted at their potential for processing speech in a manner more like the human brain and generating activations similar to those of the brain in response to the same inputs. The development of such algorithms however remains difficult as they require massive training data, supervised labels, textual data rather than more realistic raw sensory data, and prohibitively large memory. In the new paper Toward a Realistic Model of Speech Processing in the Brain with Self-supervised Learning, a research team from Meta AI, PSL University, Université Paris Cité, Université Paris-Saclay, University of Toronto and INSERM shows that self-supervised architectures such as Wav2Vec 2.0 (Baevski et al., 2020) that stack convolutional and transformer layers to predict a quantization of the latent representations of speech waveforms can learn brain-like representations from as little as 600 hours of unlabelled speech; and can also learn sound-generic and speech- and language-specific representations similar to those of the prefrontal and temporal cortices. The team summarizes their study’s main contributions as: The Wav2Vec 2.0 architecture comprises three modules: 1) a feature encoder that transforms raw mono speech waveform inputs into latent representations, 2) a quantization module that discretizes the latent representations into a dictionary of discrete and latent representations of sounds, and 3) a “context network” that uses the previously generated outputs to produce contextualized embeddings. The team trained several variants of Wav2Vec 2.0 on different datasets with both self-supervised and supervised learning objectives and extracted the activations of each layer from both the feature encoder and the context network. In their empirical studies, the team compared the Wav2Vec 2.0 learned representations to those in the brains of 412 human volunteers (351 English speakers, 28 French speakers and 33 Mandarin speakers) recorded with functional magnetic resonance imaging (fMRI) while they passively listened to approximately one hour of audio books in their native language. The experimental results show that Wav2Vec 2.0 model activations can predict brain activity in nearly all cortical areas, self-supervised learning leads to slightly better performance than supervised learning, the hierarchy of Wav2Vec 2.0 maps onto the hierarchy of the cortex, and 600 hours of self-supervised learning suffices for Wav2Vec 2.0 to learn brain-like language-specific representations. Overall, this work demonstrates that applying self-supervised learning to a limited amount of speech data can enable the learning of representations similar to the human brain’s speech perception, taking a step toward a realistic model of speech processing in the brain with self-supervised learning.The paper Toward a Realistic Model of Speech Processing in the Brain with Self-supervised Learning is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Deep neural networks have recently hinted at their potential for processing speech in a manner more like the human brain and generating activations similar to those of the brain in response to the same inputs. The development of such algorithms however remains difficult as they require massive training data, supervised labels, textual data rather than more realistic raw sensory data, and prohibitively large memory. In the new paper Toward a Realistic Model of Speech Processing in the Brain with Self-supervised Learning, a research team from Meta AI, PSL University, Université Paris Cité, Université Paris-Saclay, University of Toronto and INSERM shows that self-supervised architectures such as Wav2Vec 2.0 (Baevski et al., 2020) that stack convolutional and transformer layers to predict a quantization of the latent representations of speech waveforms can learn brain-like representations from as little as 600 hours of unlabelled speech; and can also learn sound-generic and speech- and language-specific representations similar to those of the prefrontal and temporal cortices. The team summarizes their study’s main contributions as: The Wav2Vec 2.0 architecture comprises three modules: 1) a feature encoder that transforms raw mono speech waveform inputs into latent representations, 2) a quantization module that discretizes the latent representations into a dictionary of discrete and latent representations of sounds, and 3) a “context network” that uses the previously generated outputs to produce contextualized embeddings. The team trained several variants of Wav2Vec 2.0 on different datasets with both self-supervised and supervised learning objectives and extracted the activations of each layer from both the feature encoder and the context network. In their empirical studies, the team compared the Wav2Vec 2.0 learned representations to those in the brains of 412 human volunteers (351 English speakers, 28 French speakers and 33 Mandarin speakers) recorded with functional magnetic resonance imaging (fMRI) while they passively listened to approximately one hour of audio books in their native language. The experimental results show that Wav2Vec 2.0 model activations can predict brain activity in nearly all cortical areas, self-supervised learning leads to slightly better performance than supervised learning, the hierarchy of Wav2Vec 2.0 maps onto the hierarchy of the cortex, and 600 hours of self-supervised learning suffices for Wav2Vec 2.0 to learn brain-like language-specific representations. Overall, this work demonstrates that applying self-supervised learning to a limited amount of speech data can enable the learning of representations similar to the human brain’s speech perception, taking a step toward a realistic model of speech processing in the brain with self-supervised learning.The paper Toward a Realistic Model of Speech Processing in the Brain with Self-supervised Learning is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['deep', 'neural', 'network', 'recently', 'hint', 'potential', 'processing', 'speech', 'manner', 'human', 'brain', 'generating', 'activation', 'similar', 'brain', 'response', 'input', 'development', 'algorithm', 'however', 'remain', 'difficult', 'require', 'massive', 'training', 'datum', 'supervise', 'label', 'textual', 'datum', 'rather', 'realistic', 'raw', 'sensory', 'datum', 'prohibitively', 'large', 'memory', 'new', 'paper', 'realistic', 'model', 'speech', 'processing', 'brain', 'selfsupervise', 'learn', 'research', 'team', 'university', 'show', 'selfsupervise', 'architecture', 'wav2vec', 'baevski', 'et', 'stack', 'convolutional', 'transformer', 'layer', 'predict', 'quantization', 'latent', 'representation', 'speech', 'waveform', 'learn', 'brainlike', 'representation', 'little', 'hour', 'unlabelled', 'speech', 'also', 'learn', 'soundgeneric', 'speech', 'languagespecific', 'representation', 'similar', 'prefrontal', 'temporal', 'cortex', 'team', 'summarize', 'study', 'main', 'contribution', 'wav2vec', 'architecture', 'comprise', 'module', 'feature', 'encoder', 'transform', 'raw', 'speech', 'waveform', 'input', 'latent', 'representation', 'quantization', 'module', 'discretize', 'latent', 'representation', 'dictionary', 'discrete', 'latent', 'representation', 'sound', 'context', 'network', 'use', 'previously', 'generate', 'output', 'produce', 'contextualized', 'embedding', 'team', 'train', 'several', 'variant', 'wav2vec', 'different', 'dataset', 'selfsupervise', 'supervised', 'learning', 'objective', 'extract', 'activation', 'layer', 'feature', 'encoder', 'context', 'network', 'empirical', 'study', 'team', 'compare', 'wav2vec', 'learn', 'representation', 'brain', 'human', 'volunteer', 'english', 'speaker', 'french', 'speaker', 'mandarin', 'speaker', 'record', 'functional', 'magnetic', 'resonance', 'imaging', 'fmri', 'passively', 'listen', 'approximately', 'hour', 'audio', 'book', 'native', 'language', 'experimental', 'result', 'show', 'wav2vec', 'model', 'activation', 'predict', 'brain', 'activity', 'cortical', 'area', 'selfsupervise', 'learning', 'lead', 'slightly', 'well', 'performance', 'supervised', 'learn', 'hierarchy', 'wav2vec', 'map', 'hierarchy', 'cortex', 'hour', 'selfsupervised', 'learning', 'suffice', 'wav2vec', 'learn', 'languagespecific', 'representation', 'overall', 'work', 'demonstrate', 'apply', 'selfsupervised', 'learning', 'limited', 'amount', 'speech', 'datum', 'enable', 'learning', 'representation', 'similar', 'human', 'speech', 'perception', 'take', 'step', 'realistic', 'model', 'speech', 'processing', 'brain', 'selfsupervise', 'learningthe', 'paper', 'realistic', 'model', 'speech', 'processing', 'brain', 'selfsupervised', 'learning', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
Apple’s MobileOne Backbone Reduces Inference Time to Under One Millisecond on an iPhone12 and Reaches 75.9% Top-1 Accuracy on ImageNet,https://syncedreview.com/2022/06/15/apples-mobileone-backbone-reduces-inference-time-to-under-one-millisecond-on-an-iphone12-and-reaches-75-9-top-1-accuracy-on-imagenet/,2022-06-15,"
 In the new paper An Improved One millisecond Mobile Backbone, an Apple research team presents MobileOne, a novel mobile backbone that cuts inference time to under one millisecond on an iPhone12 and reaches 75.9 percent top-1 accuracy on ImageNet.
","As AI systems increasingly move from the cloud to devices, identifying suitable neural network backbones for mobile device deployment has become a hot research area. While decreasing floating-point operations (FLOPs) and parameter counts have produced efficient mobile architectures with high accuracy, factors such as memory access and degree of parallelism continue to have a negative effect with regard to latency cost during inference. In the new paper An Improved One Millisecond Mobile Backbone, an Apple research team presents MobileOne, a novel and efficient neural network backbone for mobile devices that cuts inference time to under one millisecond on an iPhone12 and reaches 75.9 percent top-1 accuracy on ImageNet. The team summarizes their main contributions as: The paper first introduces MobileOne’s architectural blocks, which are designed for convolutional layers factorized into depthwise and pointwise layers. The basic block is built on Google’s small MobileNet-V1 block of 3×3 depthwise convolution followed by 1×1 pointwise convolutions. Over-parameterization branches are also used to improve model performance. MobileOne uses a depth scaling approach similar to MobileNet-V2 — with shallower early stages where input resolution is larger and the layers are slower. Because this setup does not require a multi-branched architecture at inference time, no data movement costs are incurred. This allows the researchers to aggressively scale model parameters compared to multi-branched architectures without introducing significant latency costs. The team evaluated MobileOne on the ImageNet benchmark using mobile devices. In the tests, the MobileOne-S1 variant achieved a lightning-quick inference time of under one millisecond on an iPhone12 while scoring 75.9 percent top-1 accuracy. The researchers also demonstrated MobileOne’s versatility on other computer vision tasks, successfully applying it as a backbone feature extractor for a single shot object detector and in a Deeplab V3 segmentation network. Overall, the study validates the proposed MobileOne as an efficient, general-purpose backbone that achieves state-of-the-art results compared to existing efficient architectures while being many times faster on mobile devices. The paper An Improved One millisecond Mobile Backbone is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","As AI systems increasingly move from the cloud to devices, identifying suitable neural network backbones for mobile device deployment has become a hot research area. While decreasing floating-point operations (FLOPs) and parameter counts have produced efficient mobile architectures with high accuracy, factors such as memory access and degree of parallelism continue to have a negative effect with regard to latency cost during inference. In the new paper An Improved One Millisecond Mobile Backbone, an Apple research team presents MobileOne, a novel and efficient neural network backbone for mobile devices that cuts inference time to under one millisecond on an iPhone12 and reaches 75.9 percent top-1 accuracy on ImageNet. The team summarizes their main contributions as: The paper first introduces MobileOne’s architectural blocks, which are designed for convolutional layers factorized into depthwise and pointwise layers. The basic block is built on Google’s small MobileNet-V1 block of 3×3 depthwise convolution followed by 1×1 pointwise convolutions. Over-parameterization branches are also used to improve model performance. MobileOne uses a depth scaling approach similar to MobileNet-V2 — with shallower early stages where input resolution is larger and the layers are slower. Because this setup does not require a multi-branched architecture at inference time, no data movement costs are incurred. This allows the researchers to aggressively scale model parameters compared to multi-branched architectures without introducing significant latency costs. The team evaluated MobileOne on the ImageNet benchmark using mobile devices. In the tests, the MobileOne-S1 variant achieved a lightning-quick inference time of under one millisecond on an iPhone12 while scoring 75.9 percent top-1 accuracy. The researchers also demonstrated MobileOne’s versatility on other computer vision tasks, successfully applying it as a backbone feature extractor for a single shot object detector and in a Deeplab V3 segmentation network. Overall, the study validates the proposed MobileOne as an efficient, general-purpose backbone that achieves state-of-the-art results compared to existing efficient architectures while being many times faster on mobile devices. The paper An Improved One millisecond Mobile Backbone is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['system', 'increasingly', 'move', 'cloud', 'device', 'identify', 'suitable', 'neural', 'network', 'backbone', 'mobile', 'device', 'deployment', 'become', 'hot', 'research', 'area', 'decrease', 'floatingpoint', 'operation', 'flop', 'parameter', 'count', 'produce', 'efficient', 'mobile', 'architecture', 'high', 'accuracy', 'factor', 'memory', 'access', 'degree', 'parallelism', 'continue', 'negative', 'effect', 'regard', 'latency', 'cost', 'inference', 'new', 'paper', 'improve', 'millisecond', 'mobile', 'backbone', 'apple', 'research', 'team', 'present', 'mobileone', 'novel', 'efficient', 'neural', 'network', 'backbone', 'mobile', 'device', 'cut', 'inference', 'time', 'millisecond', 'iphone12', 'reach', 'percent', 'top1', 'accuracy', 'imagenet', 'team', 'summarize', 'main', 'contribution', 'paper', 'first', 'introduce', 'mobileone', 'architectural', 'block', 'design', 'convolutional', 'layer', 'factorize', 'depthwise', 'pointwise', 'layer', 'basic', 'block', 'build', 'small', 'mobilenetv1', 'block', 'depthwise', 'convolution', 'follow', 'pointwise', 'convolution', 'overparameterization', 'branch', 'also', 'use', 'improve', 'model', 'performance', 'mobileone', 'use', 'depth', 'scale', 'approach', 'similar', 'mobilenetv2', 'shallow', 'early', 'stage', 'input', 'resolution', 'large', 'layer', 'slow', 'setup', 'require', 'multibranched', 'architecture', 'inference', 'time', 'datum', 'movement', 'cost', 'incur', 'allow', 'researcher', 'aggressively', 'scale', 'model', 'parameter', 'compare', 'multibranched', 'architecture', 'introduce', 'significant', 'latency', 'cost', 'team', 'evaluate', 'mobileone', 'imagenet', 'benchmark', 'use', 'mobile', 'device', 'test', 'mobileones1', 'variant', 'achieve', 'lightningquick', 'inference', 'time', 'millisecond', 'iphone12', 'score', 'percent', 'top1', 'accuracy', 'researcher', 'also', 'demonstrate', 'versatility', 'computer', 'vision', 'task', 'successfully', 'apply', 'backbone', 'feature', 'extractor', 'single', 'shot', 'object', 'detector', 'overall', 'study', 'validate', 'propose', 'mobileone', 'efficient', 'generalpurpose', 'backbone', 'achieve', 'stateoftheart', 'result', 'compare', 'exist', 'efficient', 'architecture', 'many', 'time', 'fast', 'mobile', 'device', 'paper', 'improve', 'millisecond', 'mobile', 'backbone', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
444 Authors From 132 Institutions Release BIG-bench: A 204-Task ‘Extremely Difficult and Diverse’ Benchmark for Large Language Models,https://syncedreview.com/2022/06/14/444-authors-from-132-institutions-release-big-bench-a-204-task-extremely-difficult-and-diverse-benchmark-for-large-language-models/,2022-06-14,"
In the new paper Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models, 444 authors from 132 institutions introduce Beyond the Imitation Game (BIG-bench), a large-scale, extremely difficult and diverse benchmark that includes 204 tasks for predicting the potentially transformative effects of large language models.
","Powered by their ever-increasing scale, today’s large language models have shown breakthrough capabilities beyond natural language processing (NLP), in areas such as writing computer code, diagnosing medical conditions and playing competitive games. As the development and deployment of large-scale language models continues, it is important that the AI community understands their current and near-future capabilities and limitations. In the new paper Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models, 444 authors from 132 institutions introduce Beyond the Imitation Game (BIG-bench), a large-scale, extremely difficult and diverse benchmark that includes 204 tasks for predicting the potentially transformative effects of large language models. BIG-bench was named in homage to Alan Turing’s imitation game (Turing, 1950); and designed for analyzing dense and sparse transformer models such as those from Google and OpenAI, whose scales range from millions to hundreds of billions of parameters. The team summarizes their BIG-Bench suite as follows: BIG-bench supports two types of tasks: JSON (JavaScript Object Notation) and programmatic. The JSON file contains a list of input-target pairs, and performance is evaluated by comparing the outputs and the targets. The programmatic tasks are written in Python and are evaluated by measuring the generated text continuations for given inputs and computing conditional log probabilities of target given inputs. The BIG-bench task scope ranges from writing codes, playing competitive games and common-sense reasoning to social bias, linguistics, software development and beyond. It can also measure progress well beyond the current state of the art. The researchers’ experiments with BIG-bench revealed a number of behavioural characteristics of large language models, such as: 1) Aggregate performance improves with model size but can’t compete with human performance; 2) Model predictions grow better calibrated with increased scale; 3) Model classes behave similarly, with benefits from sparsity; 4) Breakthrough behaviour is sensitive to details of task specification; and 5) Even programmatic measures of model capability can be highly subjective. The team also tackled the thorny topic of social biases in large language models. They observed that biases often increase with scale in settings with broad or ambiguous context and can decrease with scale in settings with narrow unambiguous context; and that biases can potentially be steered through appropriately chosen prompting. The team considers BIG-bench a “living benchmark” and will continue to accept new task submissions for peer review on a rolling basis. They hope BIG-bench can help identify additional breakthrough capabilities and enable researchers to better understand the power and potential of current and future large language models. The BIG-bench project was collaboratively developed on the GitHub repository, where the code is now open-sourced. The paper Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Powered by their ever-increasing scale, today’s large language models have shown breakthrough capabilities beyond natural language processing (NLP), in areas such as writing computer code, diagnosing medical conditions and playing competitive games. As the development and deployment of large-scale language models continues, it is important that the AI community understands their current and near-future capabilities and limitations. In the new paper Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models, 444 authors from 132 institutions introduce Beyond the Imitation Game (BIG-bench), a large-scale, extremely difficult and diverse benchmark that includes 204 tasks for predicting the potentially transformative effects of large language models. BIG-bench was named in homage to Alan Turing’s imitation game (Turing, 1950); and designed for analyzing dense and sparse transformer models such as those from Google and OpenAI, whose scales range from millions to hundreds of billions of parameters. The team summarizes their BIG-Bench suite as follows: BIG-bench supports two types of tasks: JSON (JavaScript Object Notation) and programmatic. The JSON file contains a list of input-target pairs, and performance is evaluated by comparing the outputs and the targets. The programmatic tasks are written in Python and are evaluated by measuring the generated text continuations for given inputs and computing conditional log probabilities of target given inputs. The BIG-bench task scope ranges from writing codes, playing competitive games and common-sense reasoning to social bias, linguistics, software development and beyond. It can also measure progress well beyond the current state of the art. The researchers’ experiments with BIG-bench revealed a number of behavioural characteristics of large language models, such as: 1) Aggregate performance improves with model size but can’t compete with human performance; 2) Model predictions grow better calibrated with increased scale; 3) Model classes behave similarly, with benefits from sparsity; 4) Breakthrough behaviour is sensitive to details of task specification; and 5) Even programmatic measures of model capability can be highly subjective. The team also tackled the thorny topic of social biases in large language models. They observed that biases often increase with scale in settings with broad or ambiguous context and can decrease with scale in settings with narrow unambiguous context; and that biases can potentially be steered through appropriately chosen prompting. The team considers BIG-bench a “living benchmark” and will continue to accept new task submissions for peer review on a rolling basis. They hope BIG-bench can help identify additional breakthrough capabilities and enable researchers to better understand the power and potential of current and future large language models. The BIG-bench project was collaboratively developed on the GitHub repository, where the code is now open-sourced. The paper Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['power', 'everincreasing', 'scale', 'today', 'large', 'language', 'model', 'show', 'breakthrough', 'capability', 'natural', 'language', 'processing', 'nlp', 'area', 'write', 'computer', 'code', 'diagnose', 'medical', 'condition', 'play', 'competitive', 'game', 'development', 'deployment', 'largescale', 'language', 'model', 'continue', 'important', 'community', 'understand', 'current', 'nearfuture', 'capability', 'limitation', 'new', 'paper', 'imitation', 'game', 'quantify', 'extrapolate', 'capability', 'language', 'model', 'author', 'institution', 'introduce', 'imitation', 'game', 'bigbench', 'largescale', 'extremely', 'difficult', 'diverse', 'benchmark', 'include', 'task', 'predict', 'potentially', 'transformative', 'effect', 'large', 'language', 'model', 'bigbench', 'name', 'homage', 'ture', '’s', 'imitation', 'game', 'ture', 'design', 'analyze', 'dense', 'sparse', 'transformer', 'model', 'openai', 'scale', 'range', 'million', 'hundred', 'billion', 'parameter', 'team', 'summarize', 'bigbench', 'suite', 'follow', 'bigbench', 'support', 'type', 'task', 'javascript', 'object', 'notation', 'programmatic', 'json', 'file', 'contain', 'list', 'inputtarget', 'pair', 'performance', 'evaluate', 'compare', 'output', 'target', 'programmatic', 'task', 'write', 'evaluate', 'measure', 'generate', 'text', 'continuation', 'give', 'input', 'compute', 'conditional', 'log', 'probability', 'target', 'give', 'input', 'bigbench', 'task', 'scope', 'range', 'write', 'code', 'play', 'competitive', 'game', 'commonsense', 'reasoning', 'social', 'bias', 'linguistic', 'software', 'development', 'also', 'measure', 'progress', 'well', 'current', 'state', 'art', 'researcher', 'experiment', 'bigbench', 'reveal', 'number', 'behavioural', 'characteristic', 'large', 'language', 'model', 'aggregate', 'performance', 'improve', 'model', 'size', 'compete', 'human', 'performance', 'model', 'prediction', 'grow', 'well', 'calibrate', 'increase', 'scale', 'model', 'class', 'behave', 'similarly', 'benefit', 'sparsity', 'breakthrough', 'behaviour', 'sensitive', 'detail', 'task', 'specification', 'even', 'programmatic', 'measure', 'model', 'capability', 'highly', 'subjective', 'team', 'also', 'tackle', 'thorny', 'topic', 'social', 'bias', 'large', 'language', 'model', 'observe', 'bias', 'often', 'increase', 'scale', 'setting', 'broad', 'ambiguous', 'context', 'decrease', 'scale', 'setting', 'narrow', 'unambiguous', 'context', 'bias', 'potentially', 'steer', 'appropriately', 'choose', 'prompt', 'team', 'consider', 'bigbench', 'living', 'benchmark', 'continue', 'accept', 'new', 'task', 'submission', 'peer', 'review', 'rolling', 'basis', 'hope', 'bigbench', 'identify', 'additional', 'breakthrough', 'capability', 'enable', 'researcher', 'well', 'understand', 'power', 'potential', 'current', 'future', 'large', 'language', 'model', 'bigbench', 'project', 'collaboratively', 'develop', 'repository', 'code', 'opensource', 'paper', 'imitation', 'game', 'quantify', 'extrapolate', 'capability', 'language', 'model', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
"Cambridge, Google & Secondmind’s Neural Diffusion Processes Challenge Gaussian Processes for Describing Rich Distributions Over Functions",https://syncedreview.com/2022/06/13/cambridge-google-secondminds-neural-diffusion-processes-challenge-gaussian-processes-for-describing-rich-distributions-over-functions/,2022-06-13,"
In the new paper Neural Diffusion Processes, a research team from the University of Cambridge, Secondmind, and Google Research presents Neural Diffusion Processes (NDPs), a novel framework that learns to sample from rich distributions over functions at a lower computational cost than the true Bayesian posterior of a conventional Gaussian process.
","While researchers have traditionally employed Gaussian processes (GP) for specifying prior and posterior distributions over functions, this approach becomes computationally expensive when scaled, is limited by the expressivity of its covariance function, and struggles with adapting a point estimation for the hyperparameters. A research team from the University of Cambridge, Secondmind, and Google Research addresses these issues in the new paper Neural Diffusion Processes, proposing Neural Diffusion Processes (NDPs). The novel framework learns to sample from rich distributions over functions at a lower computational cost and capture distributions that are close to the true Bayesian posterior of a conventional Gaussian process. The paper’s lead author, Vincent Dutordoir, explains, “Bayesian inference for regression is great, but it is often very costly and requires making a priori modelling assumptions. What if we can train a big neural net to sample plausible posterior samples over functions? This is the premise of our Neural Diffusion Processes.” The team summarizes their main contributions as: The proposed NDP is a denoising diffusion model-based approach for learning probabilities from a function and producing prior and conditional samples of functions. It allows full marginalization over the GP hyperparameters while reducing the computational burden compared to GPs. The team first examined existing state-of-the-art neural network-based generative models in terms of sample quality. Based on their findings, they designed NDP to generalize diffusion models to infinite-dimensional function spaces by enabling the indexing of random variables onto which the model diffuses. The researchers also adopted a novel bi-dimensional attention block to guarantee equivariance over the input dimensionality and sequence and enable the model to draw samples from a stochastic process. As such, NDP can leverage the benefits of stochastic processes, such as exchangeability. In their empirical study, the team evaluated the proposed NDP’s ability to produce high-quality conditional samples and marginalize over kernel hyperparameters; and on its input dimensionality invariance. The results show that NDP is able to capture functional distributions that are close to the true Bayesian posterior while reducing computational burdens. The researchers note that while NDP sample quality improves with the number of diffusion steps, this also results in slower inference times. They suggest inference acceleration or sample parameterizing techniques could be explored in future studies to address this issue. The paper Neural Diffusion Processes is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","While researchers have traditionally employed Gaussian processes (GP) for specifying prior and posterior distributions over functions, this approach becomes computationally expensive when scaled, is limited by the expressivity of its covariance function, and struggles with adapting a point estimation for the hyperparameters. A research team from the University of Cambridge, Secondmind, and Google Research addresses these issues in the new paper Neural Diffusion Processes, proposing Neural Diffusion Processes (NDPs). The novel framework learns to sample from rich distributions over functions at a lower computational cost and capture distributions that are close to the true Bayesian posterior of a conventional Gaussian process. The paper’s lead author, Vincent Dutordoir, explains, “Bayesian inference for regression is great, but it is often very costly and requires making a priori modelling assumptions. What if we can train a big neural net to sample plausible posterior samples over functions? This is the premise of our Neural Diffusion Processes.” The team summarizes their main contributions as: The proposed NDP is a denoising diffusion model-based approach for learning probabilities from a function and producing prior and conditional samples of functions. It allows full marginalization over the GP hyperparameters while reducing the computational burden compared to GPs. The team first examined existing state-of-the-art neural network-based generative models in terms of sample quality. Based on their findings, they designed NDP to generalize diffusion models to infinite-dimensional function spaces by enabling the indexing of random variables onto which the model diffuses. The researchers also adopted a novel bi-dimensional attention block to guarantee equivariance over the input dimensionality and sequence and enable the model to draw samples from a stochastic process. As such, NDP can leverage the benefits of stochastic processes, such as exchangeability. In their empirical study, the team evaluated the proposed NDP’s ability to produce high-quality conditional samples and marginalize over kernel hyperparameters; and on its input dimensionality invariance. The results show that NDP is able to capture functional distributions that are close to the true Bayesian posterior while reducing computational burdens. The researchers note that while NDP sample quality improves with the number of diffusion steps, this also results in slower inference times. They suggest inference acceleration or sample parameterizing techniques could be explored in future studies to address this issue. The paper Neural Diffusion Processes is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['researcher', 'traditionally', 'employ', 'gaussian', 'process', 'gp', 'specify', 'prior', 'posterior', 'distribution', 'function', 'approach', 'become', 'computationally', 'expensive', 'scale', 'limit', 'expressivity', 'covariance', 'function', 'struggle', 'adapt', 'point', 'estimation', 'hyperparameter', 'research', 'team', 'research', 'address', 'issue', 'new', 'paper', 'neural', 'diffusion', 'process', 'propose', 'neural', 'diffusion', 'process', 'ndps', 'novel', 'framework', 'learn', 'sample', 'rich', 'distribution', 'function', 'low', 'computational', 'cost', 'capture', 'distribution', 'close', 'true', 'bayesian', 'posterior', 'conventional', 'gaussian', 'process', 'paper', 'explain', 'inference', 'regression', 'great', 'often', 'costly', 'require', 'make', 'priori', 'modelling', 'assumption', 'train', 'big', 'neural', 'net', 'sample', 'plausible', 'posterior', 'sample', 'function', 'premise', 'neural', 'diffusion', 'process', 'team', 'summarize', 'main', 'contribution', 'propose', 'ndp', 'denoise', 'diffusion', 'modelbase', 'approach', 'learn', 'probability', 'function', 'produce', 'prior', 'conditional', 'sample', 'function', 'allow', 'full', 'marginalization', 'gp', 'hyperparameter', 'reduce', 'computational', 'burden', 'compare', 'gps', 'team', 'first', 'examine', 'exist', 'stateoftheart', 'neural', 'networkbase', 'generative', 'model', 'term', 'sample', 'quality', 'base', 'finding', 'design', 'ndp', 'generalize', 'diffusion', 'model', 'infinitedimensional', 'function', 'space', 'enable', 'indexing', 'random', 'variable', 'model', 'diffuse', 'researcher', 'also', 'adopt', 'novel', 'bidimensional', 'attention', 'block', 'guarantee', 'equivariance', 'input', 'dimensionality', 'sequence', 'enable', 'model', 'draw', 'sample', 'stochastic', 'process', 'ndp', 'leverage', 'benefit', 'stochastic', 'process', 'exchangeability', 'empirical', 'study', 'team', 'evaluate', 'propose', 'ndp', 'ability', 'produce', 'highquality', 'conditional', 'sample', 'marginalize', 'kernel', 'hyperparameter', 'input', 'dimensionality', 'invariance', 'result', 'show', 'ndp', 'able', 'capture', 'functional', 'distribution', 'close', 'true', 'bayesian', 'posterior', 'reduce', 'computational', 'burden', 'researcher', 'note', 'ndp', 'sample', 'quality', 'improve', 'number', 'diffusion', 'step', 'also', 'result', 'slow', 'inference', 'time', 'suggest', 'inference', 'acceleration', 'sample', 'parameterizing', 'technique', 'explore', 'future', 'study', 'address', 'issue', 'paper', 'neural', 'diffusion', 'process', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
Yoshua Bengio Team’s Large-Scale Analysis Reveals the Benefits of Modularity and Sparsity for DNNs,https://syncedreview.com/2022/06/10/yoshua-bengio-teams-large-scale-analysis-reveals-the-benefits-of-modularity-and-sparsity-for-dnns/,2022-06-10,"
In the new paper Is a Modular Architecture Enough?, a research team from Mila and the Université de Montréal conducts a rigorous and thorough quantitative assessment of common modular architectures that reveals the benefits of modularity and sparsity for deep neural networks and the sub-optimality of existing end-to-end learned modular systems.
","Deep neural networks (DNNs) have drawn much inspiration from the human cognitive process, evidenced recently in their incorporation of modular structures and attention mechanisms. By representing knowledge in a modular manner and selecting relevant information via attention mechanisms, DNN models can develop meaningful inductive biases, boost their out-of-distribution generalization abilities, and manipulate concepts at higher levels of cognition. While modular architectures provide proven advantages for DNNs, there currently exists no rigorous quantitative assessment method for them due to the complexity and unknown nature of real-world data distributions. As such, it is unclear whether or to what extent the performance gains obtained by modular systems are actually attributable to good modular architecture design. In the new paper Is a Modular Architecture Enough, a research team from Mila and the Université de Montréal conducts a rigorous and thorough quantitative assessment of common modular architectures that reveals the benefits of modularity and sparsity for DNNs and the sub-optimality of existing end-to-end learned modular systems. The team summarizes their main contributions as: “ The team considers four model types with different levels of specialization: Monolithic, a large neural network that takes the entire data as input; Modular, a number of modules, each of which is a neural network that takes the data as input; Modular-op, similar to the modular system but with activation decided only by the rule context; and GT-Modular, which serves as an oracle benchmark, i.e., a modular system that specializes perfectly. They conduct a step-by-step analysis of the benefits of each system and contrast simple end-to-end trained modular systems with monolithic systems. The team explores both in-distribution and out-of-distribution performance and evaluates how different models perform on a variety of tasks. They also introduce two metrics — Collapse-Avg and Collapse-Worst — to measure the amount of collapse suffered by a modular system; and use alignment, adaptation and inverse mutual information metrics to quantify the amount of specialization obtained. In the experiments, the GT-Modular system generally had the highest performance, confirming the advantages of perfect specialization. Although standard end-to-end trained modular systems slightly outperformed monolithic systems, the team notes that these systems’ reliance on backpropagation of the task losses does not enable them to discover perfect specialization. Both the Modular and Modular-op systems were shown to have collapse issues, but Modular-op generally suffered fewer. The team suggests a deeper investigation into forms of regularization may help alleviate these collapse problems. Overall, this work shows that modular models outperform monolithic models. Although modular networks can obtain perfectly specialized solutions, end-to-end training does not recover them, and additional inductive biases are required to learn adequately specialized solutions. The team hopes their work will motivate future research into the design and development of modular architectures.Open-sourced implementation is available on the project’s GitHub. The paper Is a Modular Architecture Enough? is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Deep neural networks (DNNs) have drawn much inspiration from the human cognitive process, evidenced recently in their incorporation of modular structures and attention mechanisms. By representing knowledge in a modular manner and selecting relevant information via attention mechanisms, DNN models can develop meaningful inductive biases, boost their out-of-distribution generalization abilities, and manipulate concepts at higher levels of cognition. While modular architectures provide proven advantages for DNNs, there currently exists no rigorous quantitative assessment method for them due to the complexity and unknown nature of real-world data distributions. As such, it is unclear whether or to what extent the performance gains obtained by modular systems are actually attributable to good modular architecture design. In the new paper Is a Modular Architecture Enough, a research team from Mila and the Université de Montréal conducts a rigorous and thorough quantitative assessment of common modular architectures that reveals the benefits of modularity and sparsity for DNNs and the sub-optimality of existing end-to-end learned modular systems. The team summarizes their main contributions as: “ The team considers four model types with different levels of specialization: Monolithic, a large neural network that takes the entire data as input; Modular, a number of modules, each of which is a neural network that takes the data as input; Modular-op, similar to the modular system but with activation decided only by the rule context; and GT-Modular, which serves as an oracle benchmark, i.e., a modular system that specializes perfectly. They conduct a step-by-step analysis of the benefits of each system and contrast simple end-to-end trained modular systems with monolithic systems. The team explores both in-distribution and out-of-distribution performance and evaluates how different models perform on a variety of tasks. They also introduce two metrics — Collapse-Avg and Collapse-Worst — to measure the amount of collapse suffered by a modular system; and use alignment, adaptation and inverse mutual information metrics to quantify the amount of specialization obtained. In the experiments, the GT-Modular system generally had the highest performance, confirming the advantages of perfect specialization. Although standard end-to-end trained modular systems slightly outperformed monolithic systems, the team notes that these systems’ reliance on backpropagation of the task losses does not enable them to discover perfect specialization. Both the Modular and Modular-op systems were shown to have collapse issues, but Modular-op generally suffered fewer. The team suggests a deeper investigation into forms of regularization may help alleviate these collapse problems. Overall, this work shows that modular models outperform monolithic models. Although modular networks can obtain perfectly specialized solutions, end-to-end training does not recover them, and additional inductive biases are required to learn adequately specialized solutions. The team hopes their work will motivate future research into the design and development of modular architectures.Open-sourced implementation is available on the project’s GitHub. The paper Is a Modular Architecture Enough? is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['deep', 'neural', 'network', 'dnn', 'draw', 'much', 'inspiration', 'human', 'cognitive', 'process', 'evidence', 'recently', 'incorporation', 'modular', 'structure', 'attention', 'mechanism', 'represent', 'knowledge', 'modular', 'manner', 'select', 'relevant', 'information', 'attention', 'mechanism', 'dnn', 'model', 'develop', 'meaningful', 'inductive', 'bias', 'boost', 'outofdistribution', 'generalization', 'ability', 'manipulate', 'concept', 'high', 'level', 'cognition', 'modular', 'architecture', 'provide', 'prove', 'advantage', 'dnn', 'currently', 'exist', 'rigorous', 'quantitative', 'assessment', 'method', 'complexity', 'unknown', 'nature', 'realworld', 'datum', 'distribution', 'unclear', 'extent', 'performance', 'gain', 'obtain', 'modular', 'system', 'actually', 'attributable', 'good', 'modular', 'architecture', 'design', 'new', 'paper', 'modular', 'architecture', 'enough', 'research', 'team', 'mila', 'conduct', 'rigorous', 'thorough', 'quantitative', 'assessment', 'common', 'modular', 'architecture', 'reveal', 'benefit', 'modularity', 'sparsity', 'dnn', 'suboptimality', 'exist', 'endtoend', 'learn', 'modular', 'system', 'team', 'summarize', 'main', 'contribution', 'team', 'consider', 'model', 'type', 'different', 'level', 'specialization', 'monolithic', 'large', 'neural', 'network', 'take', 'entire', 'datum', 'input', 'modular', 'number', 'module', 'neural', 'network', 'take', 'datum', 'input', 'modularop', 'similar', 'modular', 'system', 'activation', 'decide', 'rule', 'context', 'gtmodular', 'serve', 'oracle', 'benchmark', 'modular', 'system', 'specialize', 'perfectly', 'conduct', 'stepbystep', 'analysis', 'benefit', 'system', 'contrast', 'simple', 'endtoend', 'train', 'modular', 'system', 'monolithic', 'system', 'team', 'explore', 'indistribution', 'outofdistribution', 'performance', 'evaluate', 'different', 'model', 'perform', 'variety', 'task', 'also', 'introduce', 'metric', 'collapseavg', 'collapseworst', 'measure', 'amount', 'collapse', 'suffer', 'modular', 'system', 'use', 'alignment', 'adaptation', 'inverse', 'mutual', 'information', 'metric', 'quantify', 'amount', 'specialization', 'obtain', 'experiment', 'gtmodular', 'system', 'generally', 'high', 'performance', 'confirm', 'advantage', 'perfect', 'specialization', 'standard', 'endtoend', 'train', 'modular', 'system', 'slightly', 'outperform', 'monolithic', 'system', 'team', 'note', 'system', 'reliance', 'backpropagation', 'task', 'loss', 'enable', 'discover', 'perfect', 'specialization', 'modular', 'modularop', 'system', 'show', 'collapse', 'issue', 'modularop', 'generally', 'suffer', 'team', 'suggest', 'deep', 'investigation', 'form', 'regularization', 'help', 'alleviate', 'collapse', 'problem', 'overall', 'work', 'show', 'modular', 'model', 'outperform', 'monolithic', 'model', 'modular', 'network', 'obtain', 'perfectly', 'specialized', 'solution', 'endtoend', 'training', 'recover', 'additional', 'inductive', 'bias', 'require', 'learn', 'adequately', 'specialized', 'solution', 'team', 'hope', 'work', 'motivate', 'future', 'research', 'design', 'development', 'modular', 'architecturesopensource', 'implementation', 'available', 'project', 'paper', 'modular', 'architecture', 'enough', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
Microsoft’s XTC Extreme Lightweight Compression Method for Pretrained Transformers Achieves SOTA Results and 50x Smaller Model Sizes,https://syncedreview.com/2022/06/09/microsofts-xtc-extreme-lightweight-compression-method-for-pretrained-transformers-achieves-sota-results-and-50x-smaller-model-sizes/,2022-06-09,"
In the new paper Extreme Compression for Pre-trained Transformers Made Simple and Efficient, a Microsoft research team introduces XTC, a simple yet effective extreme compression pipeline for pretrained transformers that can achieve  state-of-the-art results while reducing model size by 50x.
","Pretrained transformer models have grown dramatically in recent years and now reach hundreds of billions of parameters. Although these behemoths are achieving unprecedented performance on natural language processing (NLP) tasks, their ever-expanding size has limited their real-world deployment on resource-constrained edge or embedded devices. In the new paper Extreme Compression for Pre-trained Transformers Made Simple and Efficient, a Microsoft research team proposes XTC, a simple yet effective extreme compression pipeline for pre-trained transformers. XTC can skip the compute-heavy pretraining knowledge distillation (KD) process to obtain a 5-layer BERT model with better performance than previous state-of-the-art distillation methods, and its extreme quantization and layer reduction can cut model sizes by 50x. The team summarizes their main contributions as: The proposed XTC pipeline comprises two steps: 1) Lightweight layer reduction. Instead of adopting computationally expensive pretraining distillation, the researchers first employ a subset of the fine-tuned teacher weights as a lightweight layer reduction method to initialize a layer-reduced model. When combined with the team’s other training strategies, this lightweight approach reduces computational cost and achieves a much higher compression ratio than other existing methods. 2) 1-bit quantization by applying 1S-KD with DA and long training. The team applies quantize-aware 1S-KD (one-step knowledge distillation), using an ultra-low bit (1-bit/2-bit) quantizer to compress the layer-reduced model weights obtained in step 1 for a forward pass, then uses a straight-through estimator (STE) in a backward pass for passing gradients. The team minimizes the single-stage deep KD objective with data augmentation (DA) and longer training, such that the training loss is close to zero. In their empirical study, the team applied their novel compression approach to the BERT large language model, using the standard General Language Understanding Evaluation (GLUE) benchmark. The experimental results show that the proposed XTC can compress BERTbase to a 5-layer BERTbase while outperforming previous state-of-the-art distillation methods such as the 6-layer TinyBERT without incurring the computationally expensive pretraining distillation. The method’s robust extreme quantization can also reduce model size by 50x with better accuracy than prior extreme quantization methods; and achieve state-of-the-art results on GLUE tasks. Overall, this work introduces a simple yet effective compression pipeline for extreme compression in pretrained transformers, providing a possible solution for deploying such models on resource-constrained devices. The code will be released on the Microsoft DeepSpeed GitHub. The paper Extreme Compression for Pre-trained Transformers Made Simple and Efficient is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Pretrained transformer models have grown dramatically in recent years and now reach hundreds of billions of parameters. Although these behemoths are achieving unprecedented performance on natural language processing (NLP) tasks, their ever-expanding size has limited their real-world deployment on resource-constrained edge or embedded devices. In the new paper Extreme Compression for Pre-trained Transformers Made Simple and Efficient, a Microsoft research team proposes XTC, a simple yet effective extreme compression pipeline for pre-trained transformers. XTC can skip the compute-heavy pretraining knowledge distillation (KD) process to obtain a 5-layer BERT model with better performance than previous state-of-the-art distillation methods, and its extreme quantization and layer reduction can cut model sizes by 50x. The team summarizes their main contributions as: The proposed XTC pipeline comprises two steps: 1) Lightweight layer reduction. Instead of adopting computationally expensive pretraining distillation, the researchers first employ a subset of the fine-tuned teacher weights as a lightweight layer reduction method to initialize a layer-reduced model. When combined with the team’s other training strategies, this lightweight approach reduces computational cost and achieves a much higher compression ratio than other existing methods. 2) 1-bit quantization by applying 1S-KD with DA and long training. The team applies quantize-aware 1S-KD (one-step knowledge distillation), using an ultra-low bit (1-bit/2-bit) quantizer to compress the layer-reduced model weights obtained in step 1 for a forward pass, then uses a straight-through estimator (STE) in a backward pass for passing gradients. The team minimizes the single-stage deep KD objective with data augmentation (DA) and longer training, such that the training loss is close to zero. In their empirical study, the team applied their novel compression approach to the BERT large language model, using the standard General Language Understanding Evaluation (GLUE) benchmark. The experimental results show that the proposed XTC can compress BERTbase to a 5-layer BERTbase while outperforming previous state-of-the-art distillation methods such as the 6-layer TinyBERT without incurring the computationally expensive pretraining distillation. The method’s robust extreme quantization can also reduce model size by 50x with better accuracy than prior extreme quantization methods; and achieve state-of-the-art results on GLUE tasks. Overall, this work introduces a simple yet effective compression pipeline for extreme compression in pretrained transformers, providing a possible solution for deploying such models on resource-constrained devices. The code will be released on the Microsoft DeepSpeed GitHub. The paper Extreme Compression for Pre-trained Transformers Made Simple and Efficient is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['pretraine', 'transformer', 'model', 'grow', 'dramatically', 'recent', 'year', 'reach', 'hundred', 'billion', 'parameter', 'behemoth', 'achieve', 'unprecedented', 'performance', 'natural', 'language', 'processing', 'task', 'everexpande', 'size', 'limit', 'realworld', 'deployment', 'resourceconstraine', 'edge', 'embed', 'device', 'new', 'paper', 'extreme', 'compression', 'pretraine', 'transformer', 'make', 'simple', 'efficient', 'research', 'team', 'propose', 'simple', 'yet', 'effective', 'extreme', 'compression', 'pipeline', 'pretraine', 'transformer', 'skip', 'computeheavy', 'pretraine', 'knowledge', 'distillation', 'kd', 'process', 'obtain', 'bert', 'model', 'well', 'performance', 'previous', 'stateoftheart', 'distillation', 'method', 'extreme', 'quantization', 'layer', 'reduction', 'cut', 'model', 'size', '50x', 'team', 'summarize', 'main', 'contribution', 'propose', 'pipeline', 'comprise', 'step', 'lightweight', 'layer', 'reduction', 'instead', 'adopt', 'computationally', 'expensive', 'pretraining', 'distillation', 'researcher', 'first', 'employ', 'subset', 'finetune', 'teacher', 'weight', 'lightweight', 'layer', 'reduction', 'method', 'initialize', 'layerreduced', 'model', 'combine', 'team', 'training', 'strategy', 'lightweight', 'approach', 'reduce', 'computational', 'cost', 'achieve', 'much', 'high', 'compression', 'ratio', 'exist', 'method', '1bit', 'quantization', 'apply', 'da', 'long', 'training', 'team', 'apply', 'quantizeaware', 'onestep', 'knowledge', 'distillation', 'use', 'ultralow', 'bit', 'quantizer', 'compress', 'layerreduced', 'model', 'weight', 'obtain', 'step', 'forward', 'pass', 'use', 'straightthrough', 'estimator', 'ste', 'backward', 'pass', 'pass', 'gradient', 'team', 'minimize', 'singlestage', 'deep', 'kd', 'objective', 'datum', 'augmentation', 'da', 'long', 'train', 'training', 'loss', 'close', 'empirical', 'study', 'team', 'apply', 'novel', 'compression', 'approach', 'bert', 'large', 'language', 'model', 'use', 'standard', 'general', 'language', 'understand', 'evaluation', 'glue', 'benchmark', 'experimental', 'result', 'show', 'propose', 'xtc', 'compress', 'bertbase', 'bertbase', 'outperform', 'previous', 'stateoftheart', 'distillation', 'method', 'tinybert', 'incur', 'computationally', 'expensive', 'pretraining', 'distillation', 'method', 'robust', 'extreme', 'quantization', 'also', 'reduce', 'model', 'size', '50x', 'well', 'accuracy', 'prior', 'extreme', 'quantization', 'method', 'achieve', 'stateoftheart', 'result', 'glue', 'task', 'overall', 'work', 'introduce', 'simple', 'yet', 'effective', 'compression', 'pipeline', 'extreme', 'compression', 'pretraine', 'transformer', 'provide', 'possible', 'solution', 'deploy', 'model', 'resourceconstraine', 'device', 'code', 'release', 'deepspee', 'paper', 'extreme', 'compression', 'pretraine', 'transformer', 'make', 'simple', 'efficient', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
Gem-Miner: Finding Lottery Tickets at Initialization and Bettering All Baselines at 19x Faster Speeds,https://syncedreview.com/2022/06/08/gem-miner-finding-lottery-tickets-at-initialization-and-bettering-all-baselines-at-19x-faster-speeds/,2022-06-08,"
In the new paper Rare Gems: Finding Lottery Tickets at Initialization, a research team from Carnegie Mellon University, MBZUAI, Petuum, Inc and the University of Wisconsin-Madison proposes GEM-MINER, an algorithm that finds sparse subnetworks at initialization trainable to accuracy that is comparable or better than iterative magnitude pruning (IMP) with warm-up.
","As artificial neural networks continue expanding in size, machine learning researchers are increasingly keen to find ways to compress them while incurring minimal performance trade-offs. Although standard pruning techniques can reduce large-scale networks’ parameter counts without sacrificing their predictive accuracy, this approach requires repeated rounds of computationally expensive retraining. In 2019, MIT researchers Frankle & Carbin won the ICLR Best Paper Award with The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks, which proposed that dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that, when trained in isolation, can reach test accuracy comparable to the original network in a similar number of iterations. How to most effectively find these winning lottery tickets however remains an open question. A research team from Carnegie Mellon University, MBZUAI, Petuum, Inc and the University of Wisconsin-Madison tackles this problem in their new paper Rare Gems: Finding Lottery Tickets at Initialization, proposing GEM-MINER, an algorithm that finds lottery tickets at initialization that are trainable to accuracy comparable or better than iterative magnitude pruning (IMP) at speeds up to 19x faster. GEM-MINER is designed to find rare gems: subnetworks with sparsity and non-trivial pretraining accuracy that can be finetuned to reach accuracy close to the original fully trained dense network. GEM-MINER uses a form of backpropagation, where each random weight is associated with a normalized score, and these normalized scores are used as optimization variables for computing the supermask, i.e. the pruning pattern of the network at initialization. In each iteration, GEM-MINER samples a set of training data and performs backpropagation on the loss of the effective weights to automatically find an optimal sparsity subnetwork. The team evaluated GEM-MINER on CIFAR-10 image classification against baselines that included dense weight training and four pruning algorithms (IMP, Learning Rate Rewinding, Edge-Popup and Smart-Ratio). In the experiments, the proposed GEM-MINER bettered all baselines, reaching high accuracy even in the early training stages. When finetuned, GEM-Miner outperformed IMP with warmup training at speeds up to 19x faster. The researchers say their work resolves the open question of pruning at initialization, finding lottery tickets at initialization that have non-trivial accuracy even before finetuning and accuracy rivalling prune-after-train methods after finetuning. The code is available on the project’s GitHub. The paper Rare Gems: Finding Lottery Tickets at Initialization is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","As artificial neural networks continue expanding in size, machine learning researchers are increasingly keen to find ways to compress them while incurring minimal performance trade-offs. Although standard pruning techniques can reduce large-scale networks’ parameter counts without sacrificing their predictive accuracy, this approach requires repeated rounds of computationally expensive retraining. In 2019, MIT researchers Frankle & Carbin won the ICLR Best Paper Award with The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks, which proposed that dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that, when trained in isolation, can reach test accuracy comparable to the original network in a similar number of iterations. How to most effectively find these winning lottery tickets however remains an open question. A research team from Carnegie Mellon University, MBZUAI, Petuum, Inc and the University of Wisconsin-Madison tackles this problem in their new paper Rare Gems: Finding Lottery Tickets at Initialization, proposing GEM-MINER, an algorithm that finds lottery tickets at initialization that are trainable to accuracy comparable or better than iterative magnitude pruning (IMP) at speeds up to 19x faster. GEM-MINER is designed to find rare gems: subnetworks with sparsity and non-trivial pretraining accuracy that can be finetuned to reach accuracy close to the original fully trained dense network. GEM-MINER uses a form of backpropagation, where each random weight is associated with a normalized score, and these normalized scores are used as optimization variables for computing the supermask, i.e. the pruning pattern of the network at initialization. In each iteration, GEM-MINER samples a set of training data and performs backpropagation on the loss of the effective weights to automatically find an optimal sparsity subnetwork. The team evaluated GEM-MINER on CIFAR-10 image classification against baselines that included dense weight training and four pruning algorithms (IMP, Learning Rate Rewinding, Edge-Popup and Smart-Ratio). In the experiments, the proposed GEM-MINER bettered all baselines, reaching high accuracy even in the early training stages. When finetuned, GEM-Miner outperformed IMP with warmup training at speeds up to 19x faster. The researchers say their work resolves the open question of pruning at initialization, finding lottery tickets at initialization that have non-trivial accuracy even before finetuning and accuracy rivalling prune-after-train methods after finetuning. The code is available on the project’s GitHub. The paper Rare Gems: Finding Lottery Tickets at Initialization is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['artificial', 'neural', 'network', 'continue', 'expand', 'size', 'machine', 'learning', 'researcher', 'increasingly', 'keen', 'find', 'way', 'compress', 'incur', 'minimal', 'performance', 'tradeoff', 'standard', 'pruning', 'technique', 'reduce', 'largescale', 'network', 'parameter', 'count', 'sacrifice', 'predictive', 'accuracy', 'approach', 'require', 'repeat', 'round', 'computationally', 'expensive', 'retraining', 'mit', 'researcher', 'frankle', 'win', 'good', 'paper', 'award', 'lottery', 'ticket', 'hypothesis', 'find', 'sparse', 'trainable', 'neural', 'network', 'propose', 'dense', 'randomlyinitialize', 'feedforward', 'network', 'contain', 'subnetwork', 'win', 'ticket', 'train', 'isolation', 'reach', 'test', 'accuracy', 'comparable', 'original', 'network', 'similar', 'number', 'iteration', 'effectively', 'find', 'win', 'lottery', 'ticket', 'however', 'remain', 'open', 'question', 'research', 'team', 'wisconsinmadison', 'tackle', 'problem', 'new', 'paper', 'rare', 'gem', 'find', 'lottery', 'ticket', 'initialization', 'propose', 'gemminer', 'find', 'lottery', 'ticket', 'initialization', 'trainable', 'accuracy', 'comparable', 'well', 'iterative', 'magnitude', 'prune', 'imp', 'speed', 'fast', 'gemminer', 'design', 'find', 'rare', 'gem', 'subnetwork', 'sparsity', 'nontrivial', 'pretraining', 'accuracy', 'finetune', 'reach', 'accuracy', 'close', 'original', 'fully', 'train', 'dense', 'network', 'gemminer', 'use', 'form', 'backpropagation', 'random', 'weight', 'associate', 'normalize', 'score', 'normalize', 'score', 'use', 'optimization', 'variable', 'compute', 'prune', 'pattern', 'network', 'initialization', 'iteration', 'gemminer', 'sample', 'set', 'training', 'datum', 'perform', 'backpropagation', 'loss', 'effective', 'weight', 'automatically', 'find', 'optimal', 'sparsity', 'subnetwork', 'team', 'evaluate', 'gemminer', 'cifar10', 'image', 'classification', 'baseline', 'include', 'dense', 'weight', 'training', 'pruning', 'algorithm', 'imp', 'learning', 'rate', 'rewinde', 'edgepopup', 'smartratio', 'experiment', 'propose', 'gemminer', 'better', 'baseline', 'reach', 'high', 'accuracy', 'even', 'early', 'training', 'stage', 'finetune', 'gemminer', 'outperform', 'warmup', 'training', 'speed', 'fast', 'researcher', 'say', 'work', 'resolve', 'open', 'question', 'prune', 'initialization', 'find', 'lottery', 'ticket', 'initialization', 'nontrivial', 'accuracy', 'even', 'finetune', 'accuracy', 'rival', 'pruneaftertrain', 'method', 'finetune', 'code', 'available', 'project', 'paper', 'rare', 'gem', 'find', 'lottery', 'ticket', 'initialization', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
"200+ World-Class AI Experts at BAAI 2022: ‘AI Life’, Multimodal Models, AI for Science, Autonomous Driving and More!",https://syncedreview.com/2022/06/07/200-world-class-ai-experts-at-baai-2022-ai-life-multimodal-models-ai-for-science-autonomous-driving-and-more/,2022-06-07,"
The BAAI Conference 2022 kicked off at 9:00 am on May 31 in Beijing and ran through June 2. AI experts, industry leaders, young talents and international delegates joined the virtual gathering and live stream for three busy days of high-level keynotes, tech talks, parallel forums and networking.
","The BAAI Conference 2022 kicked off at 9:00 am on May 31 in Beijing and ran through June 2. AI experts, industry leaders, young talents and international delegates joined the virtual gathering and live stream for three busy days of high-level keynotes, tech talks, parallel forums and networking. The conference addressed a variety of contemporary AI challenges, from large-scale multimodal models and neurocomputers to AI for science and autonomous driving and much more. The host, Beijing Academy of Artificial Intelligence (BAAI), is a well-established non-profit research institute that promotes strategic collaborations between academia and industry to bridge the gap between pioneering AI research and cutting-edge applications in complex real-world scenarios.China’s most influential AI academic conference, BAAI 2022 welcomed over 200 domestic and international experts and scholars, including Israeli cryptographer and 2002 Turing Award Laureate Adi Shamir, DeepMind Distinguished Research Scientist Richard Sutton, Gödel Prize Laureate Cynthia Dwork, Head of Hugging Face Research Douwe Kiela, Head of OpenAI Research Jeff Clune, University College London neuroscientist Karl Friston, and UC Berkeley computer scientist Michael I. Jordan. Following the conference’s opening remarks, BAAI Dean and Peking University Professor TieJun Huang took the stage to introduce three important BAAI advancements: MetaWorm 1.0 (Tian Bao), the AI Chip Ecosystem Laboratory & Jiuding AI-Computing Platform, and the latest applications powered by China’s first homegrown super-scale intelligent model, WUDAO big model. BAAI Life Simulation Research Center Director Lei Ma described MetaWorm 1.0 as a computational model of the Caenorhabditis elegans (C. elegans) nematode with the “most detailed” nervous system in synergy with a digital body built on 96 muscles and interactive with a three-dimensional fluid simulation environment in real-time. This digital “Worm” has achieved forward worming in the simulation environment and represents a milestone in the eVolution project. MetaWorm 1.0 exhibits behaviours that parallel C. elegans in the real world, and the next goal on the eVolution project roadmap will be to have it demonstrate more complex intelligent behaviours such as avoidance response and optimal foraging. As MetaWorm 1.0 evolves to 2.0 and 3.0, the goal is to have it “gradually becoming intelligent.” Using neuron circuits from small animals to investigate what we can learn from nature to improve artificial intelligence technologies has always been a promising approach. For example, a 2020 study published in Nature Machine Intelligence showed how a novel AI system with 19 control neurons inspired by the brains of C. elegans could enable high-fidelity autonomy for task-specific parts of complex autonomous systems such as steering commands. In summary, MetaWorm 1.0 is the most biologically accurate computational model of C. elegans and has achieved the following breakthroughs: Significant AI technical innovations will often occur along with breakthroughs in infrastructure technologies. Reflecting this, BAAI announced the AI chip Ecosystem Lab and Jiuding AI-Computing Platform, which researchers can use to boost AI evolution with cross-layer innovation. Jiuding has the world’s largest Chinese dataset for AI training, and will provide 1000P computation capacity and 400Gbps high-speed interconnection per server to support AI research activities. To enable Jiuding to effectively run on AI chipsets from different vendors, BAAI researchers are exploring an adaptation layer with self-learning and self-adaptation capabilities which can automatically find the most suitable computing resource for different tasks. BAAI Chief Engineer Yonghua Lin explained, “With more and more AI chips being delivered into the market, AI platform infrastructure will be required to support AI chips of various architectures and different capabilities. So we need to explore automatic technologies to help the industry solve the adaptation problem. It will be a kind of AI for AI system innovation. And it will be critical for the AI chip industry as well.” At the conference, BAAI announced the open-sourcing of FlagAI (FeiZhi), an algorithm and tools project designed to support mainstream large-scale multimodal foundation models and simplify their trial and development. BAAI strongly believes in the benefits of open science and has consistently promoted an open-source approach to the innovation of AI algorithms. The FlagAI project has been launched on GitHub with plenty of tutorials and examples to support research in the global open-source community. Additional FlagAI features and algorithms will be added in future releases. The focus of the Wu Dao project for this year is to continue close collaborations with various companies in the fields of lifestyle, art and language to expedite its application on complex real-world tasks. After working with OPPO, TAL Education Group, Taobao and Sogou, the Wu Dao project’s partnership with Meituan produced a large-scale multimodal model that provides some 700 million users daily with conveniences in search ads, smart assistants and fine-grained sentiment analysis, boosting Meituan’s search ads revenue by 2.7%. Last month, BAAI released CogView2 (Synced coverage). At the conference, the CogView team showcased CogVideo as its next big breakthrough CogVideo is a 9B-parameter large-scale pretrained text-to-video transformer model, trained by inheriting a pretrained text-to-image model, CogView2. The Wu Dao project team is also carrying out a number of strategic collaborations with world-renowned organizations and institutes. It is currently working with the Arab Academy for Science, Technology & Maritime Transport and the Bibliotheca Alexandrina to build the world’s largest Arabic language dataset. The joint effort is aimed at developing large-scale Arab language models and applications for solving complex real-world challenges. Synced‘s BAAI 2022 keynote and other presentation highlights are summarized below. Richard Sutton | DeepMind Distinguished Research Scientist Sutton started his talk with an essential question: Will intelligence ultimately be explained in objective terms such as states of the external world, objects, and people; or in experiential terms such as sensations, actions, and rewards? He noted that experience has played an increasing role in this consideration over AI’s seven decades and identified four significant steps in which AI has turned toward experience to be more grounded, learnable, and scalable. Sutton posited that the alternative to the objective state is the experiential state, where the world is defined entirely in terms of experiences that are useful for predicting and controlling future experiences. He proposed the experiential state be recursively updated, and that by combining all experiential steps, a standard model of the experiential agent can be obtained. Sutton also offered a philosophical take on the matter: “Experience offers a path to knowing the world: if any fact about the world is a fact about the experience, then it can be learned and verified from experience.” Despite Steps 3 and 4 being far from complete, he noted that many AI research opportunities remain, and the story of intelligence may one day be told in terms of sensorimotor experience. Jeff Clune | OpenAI Research Team Lead Jeff Clune shared work that essentially represents two different approaches to reinforcement learning (RL): But how might we accomplish the grandest ambitions of very powerful AI? Clune’s answer was a novel paradigm: AI-Generating Algorithms. Clune suggested we look at the history of how science progresses via innovations. It is critical that science and technological innovation generate problems and simultaneously carry out goal switching. But what exactly does this mean? Clune explained that the only way to solve complex problems is by creating problems while we simultaneously solve them and performing goal switching between them. For example, when the only known cooking method was heating a hanging pot, if scientists were rewarded only if they could produce the fastest hot food with less smoke, then the microwave oven would never have been invented because its related radar technology would not have been developed.How can we get our algorithms to do “goal switching” and realize such “serendipitous discoveries”? A family of algorithms called Quality Diversity Algorithms stands out. Presently, one of the most popular algorithms of this type is MAP-Elites, which returns an entire set of high-quality solutions that Clune says are “the best you can find.” An illustrative outcome and takeaway from the MAP-Elites algorithm? The 2015 paper published in Nature,Robots that can adapt like animals, which introduces “an intelligent trial-and-error algorithm that allows robots to adapt to damage in less than two minutes, without requiring self-diagnosis or pre-specified contingency plans.” When facing the dual challenges of detachment and derailment, the Go-Explore algorithm comes into play. Go-Explore’s strategy is carried out in two phases. It first initializes itself by taking random actions, storing the states visited, and starting a simple loop until the problem is solved. It then robustifies the solutions into a deep neural network via imitation learning. “In my ideal world, algorithms do not require domain knowledge but will take advantage of it,” Clune added. This view reflects the improved results of the Go-Explore algorithm, which continues to demonstrate the value of Quality Diversity algorithms for hard-exploration problems and inspire new and novel research directions. “Collecting a diverse repertoire of high-quality entities gives you a lot of fuel to power other algorithms downstream.” “Traditional machine learning usually relies on human efforts to pick challenges for algorithms to solve,” Clune explained. “But it is way more interesting to me to focus on the notion of open-endless, which means algorithms are truly, endlessly innovating. If you can run them forever, they will keep doing interesting new things. An example of that would be the natural evolution, which has been going on for 3.5 billion years. The question is, can we make algorithms to do that? Another way to think about it is, could you make an algorithm to run for billion years?” Clune proposed researchers learn from natural evolution and human culture, where innovating means generating new problems, then solving them to generate more new problems. He enthusiastically introduced the results obtained by the enhanced POET algorithm — a phylogenetic tree of the first 100 environments of a POET run. “This is one of my most favourite plots and results in my entire scientific career because I have been trying to generate this plot since I was a PhD student on Day One.” Clune said the results left him with an unforgettable impression since they resemble those from nature, exhibiting a clear signature of open-ended algorithms: multiple, deep, hierarchically nested branches. Clune concluded by noting that although we have a long way to go before achieving artificial general intelligence (AGI), there are three pillars that could support research toward that goal: Clune believes AI-generating algorithms would likely be the fastest path to AGI. The hypothesis is that pieces of AI are learnable, and learned solutions ultimately win out, so we should be going all-in on learning the solutions to AGI. “These algorithms are worthwhile even if they are not the fastest path because they shed light on our origins, and they tend to be very creative and could surprise us,” Clune told the audience. “They can create an entirely new intelligence that we could never dream of and teach us what it means to be intelligent.” Luke Zettlemoyer | Meta AI “Will large language models keep getting bigger?” asked Luke Zettlemoyer, addressing a hot topic in machine learning research. As Synced previously reported, the large-scale multimodal Wu Dao model has a whopping 1.75 trillion parameters, roughly ten times that of OpenAI’s powerful GPT-3. Today’s data-hungry language models are not just getting a lot bigger; they have also become adept zero-shot learners. Will researchers be able to keep scaling them up, or are we approaching the physical limitations of computing hardware? How can we best use these big models? And what about alternative forms of supervision? Zettlemoyer introduced two ideas he and his research team have been investigating to enable further scaling: Zettlemoyer noted that in expert specializations, an assignment often depends only on the previous word and corresponds to simple word clusters. Could even simpler routing work? “When you’re trying to scale, you’d want to go simple,” Zettlemoyer suggested, noting that the DeMix Layers approach is “much much simpler, and we are currently actively trying to scale up.” Each domain has its expert, and the modular model can mix, add, or remove experts as necessary, with rapid adaptation throughout training and testing. Zettlemoyer showcased recent studies on alternatives to fine-tuning for natural language generation tasks that keep language model parameters frozen, such as the lightweight “Prefix-Tuning” paradigm proposed by Stanford University researchers Li and Liang. These can deliver a “much better result based on a little bit of compute and careful framing,” and also enable directly learning a noisy channel prompting model for even better performance. Zettlemoyer stressed the significance of open science to the research community and pointed out that limited access to models and restricted availability through APIs is “deeply problematic for doing good science.” He encouraged the audience to thoroughly explore research papers and look at the parameters to “see what the models are doing as much as you can.” Zettlemoyer summarized another project Facebook AI has been actively scaling up and trying to generalize for newer methods and suggested new types of training supervisions, with images, text, and more as discrete tokens. “Text is not all you need, consider the structure and other modalities,” he concluded. Cynthia Dwork | Radcliffe Institute for Advanced Study “What are risk prediction algorithms actually producing?” asked Professor Dwork to start her talk. She characterized this as a defining problem of artificial intelligence because it seeks to understand what the produced numbers mean and, more specifically, the “probability” of a non-repeatable event. Dwork walked the audience through the landscape of previous relevant scientific research, then introduced Outcome Indistinguishability, a recent study she worked on with researchers from UC Berkeley, Stanford University, and the Weizmann Institute of Science. With regard to this “probability” function, the team’s paper argued that “prediction algorithms assign numbers to individuals that are popularly understood as individual ‘probabilities’ — e.g. what is the probability of 5-year survival after a cancer diagnosis? — and which increasingly form the basis for life-altering decisions.” The study established the first scientific grounds for a “political argument” recommending auditors inspecting algorithmic risk prediction instruments be granted oracle access to the algorithm rather than simple historical predictions. The talk was followed by an insightful Q&A session between Professor Dwork and BAAI Chairman of the Board HongJiang Zhang. Professor Dwork mentioned that after working on differential privacy for many years, she wanted to come up with a new problem. Then, ten years ago, an intense and inspirational brainstorming session with a colleague from Tel Aviv University that spanned various topics drew her attention to the fairness of machine learning.We are now at a time when computer science is increasingly involved in social science and AI ethics. For young scholars, Dwork advised, “One of my key pieces of advice is to read the news and follow what’s going on in the world. See what you like and what you don’t like. And then, for everything that you don’t like, my approach would be, how on earth can theoretical computer science contribute to this? And for the things you like, how can theoretical computer science develop this? So constantly look for how the tools you have could be relevant to the major questions you see around you.”Dwork says paying close attention to societal affairs will enable young scholars to stay informed about the world and “ask how what you’ve learned that day in your technical studies could be relevant to the world.” After all, she added, once a scholar is trained, they become a precious resource, and their efforts and technical skills also bring an essential responsibility to society.As a strong believer in basic research with a deep understanding of and working experiences in academia and industry, Dwork said both career paths could be great as long as they support basic research. She suggested young scholars evaluate their pursuits, “For people who are more theoretical, what’s wonderful about industry research is the opportunity to see your theoretical idea deployed on a massive scale. Think about your security and where you want to work in your career, considering the job security and risk-taking.”Dwork believes it is vital to include algorithmic fairness and privacy among critical machine learning metrics such as accuracy. “I really believe that underlying everything is a metric. There is a notion somewhere, for a particular task, of how similar or dissimilar are each pair of individuals for this task? I would like to see a lot more work on that. I would also like to see a lot more work on the question of whether data representation itself is unfair. Everything begins with the data. So I suppose that would be the next big topic that I would like to look at, and I really encourage people watching this talk to think about that problem.“ In the Annual Young Scholars Forum and Meetup, HongJiang Zhang and UC Berkeley Professor Michael Jordan, who also serves on the BAAI Academic Advisory Committee, had a remarkable conversation on young scholars and career choices. Professor Jordan cautioned young scholars not to simply chase the next big thing in machine learning research. “I prefer to go a little slower, not to have a big competition, and to work with my students at my own pace. I always try to somewhat orthogonalize with respect to where everyone else is going. We can think about decisions and uncertainty, uncertainty qualification, out-of-sample kinds of things where the distribution changes, economic incentives for data collection and sharing data, and competitive mechanisms for making decisions when there are multiple decision-makers. These are all things that happen in the real world, and they are things that there’s much less work on, but there’s not zero work. One of the reasons I choose to work in such areas is because I see that they are really important, have real-world consequences, and they’ve been neglected, so not that many people are thinking about them.“Similar to Professor Dwork’s advice to young scholars, Jordan stressed that we must remember researchers are also real-world problem solvers. He added, “You really should think about a lifetime career. You should realize you’re not going to work on one thing. I spend at least 30% of my time learning new things or learning about topics that seem like they will be relevant to me sometime in the future.“There are lots of great videos now, and lots of great, even undergraduate-level books on some really interesting topics. I don’t look for them to give immediately me research ideas or a payoff. But after five years or ten years, almost always, they come to be relevant.”“So you have to be a problem solver. You can’t just come in and apply the technology and walk away and move on. You have to spend the time understanding the problem and bringing the technology to bear. I’ve done a lot of applied math, I’ve got a lot of control theory, and I’ve done a lot of statistics. So my brain has a partial understanding of all those things — and I can tell you that’s been the biggest secret of my success.” The first BAAI conference was held in October 2019. Informed by the organizers’ growing experience in fostering a welcoming environment for top talents and with a consistent focus on exploring fundamental AI technologies, BAAI 2022 proved an outstanding Expert AI Conference, attracting 30,000 registrations and 100,000 online attendees from various Chinese provinces and 47 international countries and regions.. BAAI provides valuable platforms for the ever-growing global community of scholars, students, and industry experts pursuing advancements in AI research and real-world applications, helping them to learn, share and flourish. Author: Fangyu Cai| Editor: Michael Sarazen","The BAAI Conference 2022 kicked off at 9:00 am on May 31 in Beijing and ran through June 2. AI experts, industry leaders, young talents and international delegates joined the virtual gathering and live stream for three busy days of high-level keynotes, tech talks, parallel forums and networking. The conference addressed a variety of contemporary AI challenges, from large-scale multimodal models and neurocomputers to AI for science and autonomous driving and much more. The host, Beijing Academy of Artificial Intelligence (BAAI), is a well-established non-profit research institute that promotes strategic collaborations between academia and industry to bridge the gap between pioneering AI research and cutting-edge applications in complex real-world scenarios.China’s most influential AI academic conference, BAAI 2022 welcomed over 200 domestic and international experts and scholars, including Israeli cryptographer and 2002 Turing Award Laureate Adi Shamir, DeepMind Distinguished Research Scientist Richard Sutton, Gödel Prize Laureate Cynthia Dwork, Head of Hugging Face Research Douwe Kiela, Head of OpenAI Research Jeff Clune, University College London neuroscientist Karl Friston, and UC Berkeley computer scientist Michael I. Jordan. Following the conference’s opening remarks, BAAI Dean and Peking University Professor TieJun Huang took the stage to introduce three important BAAI advancements: MetaWorm 1.0 (Tian Bao), the AI Chip Ecosystem Laboratory & Jiuding AI-Computing Platform, and the latest applications powered by China’s first homegrown super-scale intelligent model, WUDAO big model. BAAI Life Simulation Research Center Director Lei Ma described MetaWorm 1.0 as a computational model of the Caenorhabditis elegans (C. elegans) nematode with the “most detailed” nervous system in synergy with a digital body built on 96 muscles and interactive with a three-dimensional fluid simulation environment in real-time. This digital “Worm” has achieved forward worming in the simulation environment and represents a milestone in the eVolution project. MetaWorm 1.0 exhibits behaviours that parallel C. elegans in the real world, and the next goal on the eVolution project roadmap will be to have it demonstrate more complex intelligent behaviours such as avoidance response and optimal foraging. As MetaWorm 1.0 evolves to 2.0 and 3.0, the goal is to have it “gradually becoming intelligent.” Using neuron circuits from small animals to investigate what we can learn from nature to improve artificial intelligence technologies has always been a promising approach. For example, a 2020 study published in Nature Machine Intelligence showed how a novel AI system with 19 control neurons inspired by the brains of C. elegans could enable high-fidelity autonomy for task-specific parts of complex autonomous systems such as steering commands. In summary, MetaWorm 1.0 is the most biologically accurate computational model of C. elegans and has achieved the following breakthroughs: Significant AI technical innovations will often occur along with breakthroughs in infrastructure technologies. Reflecting this, BAAI announced the AI chip Ecosystem Lab and Jiuding AI-Computing Platform, which researchers can use to boost AI evolution with cross-layer innovation. Jiuding has the world’s largest Chinese dataset for AI training, and will provide 1000P computation capacity and 400Gbps high-speed interconnection per server to support AI research activities. To enable Jiuding to effectively run on AI chipsets from different vendors, BAAI researchers are exploring an adaptation layer with self-learning and self-adaptation capabilities which can automatically find the most suitable computing resource for different tasks. BAAI Chief Engineer Yonghua Lin explained, “With more and more AI chips being delivered into the market, AI platform infrastructure will be required to support AI chips of various architectures and different capabilities. So we need to explore automatic technologies to help the industry solve the adaptation problem. It will be a kind of AI for AI system innovation. And it will be critical for the AI chip industry as well.” At the conference, BAAI announced the open-sourcing of FlagAI (FeiZhi), an algorithm and tools project designed to support mainstream large-scale multimodal foundation models and simplify their trial and development. BAAI strongly believes in the benefits of open science and has consistently promoted an open-source approach to the innovation of AI algorithms. The FlagAI project has been launched on GitHub with plenty of tutorials and examples to support research in the global open-source community. Additional FlagAI features and algorithms will be added in future releases. The focus of the Wu Dao project for this year is to continue close collaborations with various companies in the fields of lifestyle, art and language to expedite its application on complex real-world tasks. After working with OPPO, TAL Education Group, Taobao and Sogou, the Wu Dao project’s partnership with Meituan produced a large-scale multimodal model that provides some 700 million users daily with conveniences in search ads, smart assistants and fine-grained sentiment analysis, boosting Meituan’s search ads revenue by 2.7%. Last month, BAAI released CogView2 (Synced coverage). At the conference, the CogView team showcased CogVideo as its next big breakthrough CogVideo is a 9B-parameter large-scale pretrained text-to-video transformer model, trained by inheriting a pretrained text-to-image model, CogView2. The Wu Dao project team is also carrying out a number of strategic collaborations with world-renowned organizations and institutes. It is currently working with the Arab Academy for Science, Technology & Maritime Transport and the Bibliotheca Alexandrina to build the world’s largest Arabic language dataset. The joint effort is aimed at developing large-scale Arab language models and applications for solving complex real-world challenges. Synced‘s BAAI 2022 keynote and other presentation highlights are summarized below. Richard Sutton | DeepMind Distinguished Research Scientist Sutton started his talk with an essential question: Will intelligence ultimately be explained in objective terms such as states of the external world, objects, and people; or in experiential terms such as sensations, actions, and rewards? He noted that experience has played an increasing role in this consideration over AI’s seven decades and identified four significant steps in which AI has turned toward experience to be more grounded, learnable, and scalable. Sutton posited that the alternative to the objective state is the experiential state, where the world is defined entirely in terms of experiences that are useful for predicting and controlling future experiences. He proposed the experiential state be recursively updated, and that by combining all experiential steps, a standard model of the experiential agent can be obtained. Sutton also offered a philosophical take on the matter: “Experience offers a path to knowing the world: if any fact about the world is a fact about the experience, then it can be learned and verified from experience.” Despite Steps 3 and 4 being far from complete, he noted that many AI research opportunities remain, and the story of intelligence may one day be told in terms of sensorimotor experience. Jeff Clune | OpenAI Research Team Lead Jeff Clune shared work that essentially represents two different approaches to reinforcement learning (RL): But how might we accomplish the grandest ambitions of very powerful AI? Clune’s answer was a novel paradigm: AI-Generating Algorithms. Clune suggested we look at the history of how science progresses via innovations. It is critical that science and technological innovation generate problems and simultaneously carry out goal switching. But what exactly does this mean? Clune explained that the only way to solve complex problems is by creating problems while we simultaneously solve them and performing goal switching between them. For example, when the only known cooking method was heating a hanging pot, if scientists were rewarded only if they could produce the fastest hot food with less smoke, then the microwave oven would never have been invented because its related radar technology would not have been developed.How can we get our algorithms to do “goal switching” and realize such “serendipitous discoveries”? A family of algorithms called Quality Diversity Algorithms stands out. Presently, one of the most popular algorithms of this type is MAP-Elites, which returns an entire set of high-quality solutions that Clune says are “the best you can find.” An illustrative outcome and takeaway from the MAP-Elites algorithm? The 2015 paper published in Nature,Robots that can adapt like animals, which introduces “an intelligent trial-and-error algorithm that allows robots to adapt to damage in less than two minutes, without requiring self-diagnosis or pre-specified contingency plans.” When facing the dual challenges of detachment and derailment, the Go-Explore algorithm comes into play. Go-Explore’s strategy is carried out in two phases. It first initializes itself by taking random actions, storing the states visited, and starting a simple loop until the problem is solved. It then robustifies the solutions into a deep neural network via imitation learning. “In my ideal world, algorithms do not require domain knowledge but will take advantage of it,” Clune added. This view reflects the improved results of the Go-Explore algorithm, which continues to demonstrate the value of Quality Diversity algorithms for hard-exploration problems and inspire new and novel research directions. “Collecting a diverse repertoire of high-quality entities gives you a lot of fuel to power other algorithms downstream.” “Traditional machine learning usually relies on human efforts to pick challenges for algorithms to solve,” Clune explained. “But it is way more interesting to me to focus on the notion of open-endless, which means algorithms are truly, endlessly innovating. If you can run them forever, they will keep doing interesting new things. An example of that would be the natural evolution, which has been going on for 3.5 billion years. The question is, can we make algorithms to do that? Another way to think about it is, could you make an algorithm to run for billion years?” Clune proposed researchers learn from natural evolution and human culture, where innovating means generating new problems, then solving them to generate more new problems. He enthusiastically introduced the results obtained by the enhanced POET algorithm — a phylogenetic tree of the first 100 environments of a POET run. “This is one of my most favourite plots and results in my entire scientific career because I have been trying to generate this plot since I was a PhD student on Day One.” Clune said the results left him with an unforgettable impression since they resemble those from nature, exhibiting a clear signature of open-ended algorithms: multiple, deep, hierarchically nested branches. Clune concluded by noting that although we have a long way to go before achieving artificial general intelligence (AGI), there are three pillars that could support research toward that goal: Clune believes AI-generating algorithms would likely be the fastest path to AGI. The hypothesis is that pieces of AI are learnable, and learned solutions ultimately win out, so we should be going all-in on learning the solutions to AGI. “These algorithms are worthwhile even if they are not the fastest path because they shed light on our origins, and they tend to be very creative and could surprise us,” Clune told the audience. “They can create an entirely new intelligence that we could never dream of and teach us what it means to be intelligent.” Luke Zettlemoyer | Meta AI “Will large language models keep getting bigger?” asked Luke Zettlemoyer, addressing a hot topic in machine learning research. As Synced previously reported, the large-scale multimodal Wu Dao model has a whopping 1.75 trillion parameters, roughly ten times that of OpenAI’s powerful GPT-3. Today’s data-hungry language models are not just getting a lot bigger; they have also become adept zero-shot learners. Will researchers be able to keep scaling them up, or are we approaching the physical limitations of computing hardware? How can we best use these big models? And what about alternative forms of supervision? Zettlemoyer introduced two ideas he and his research team have been investigating to enable further scaling: Zettlemoyer noted that in expert specializations, an assignment often depends only on the previous word and corresponds to simple word clusters. Could even simpler routing work? “When you’re trying to scale, you’d want to go simple,” Zettlemoyer suggested, noting that the DeMix Layers approach is “much much simpler, and we are currently actively trying to scale up.” Each domain has its expert, and the modular model can mix, add, or remove experts as necessary, with rapid adaptation throughout training and testing. Zettlemoyer showcased recent studies on alternatives to fine-tuning for natural language generation tasks that keep language model parameters frozen, such as the lightweight “Prefix-Tuning” paradigm proposed by Stanford University researchers Li and Liang. These can deliver a “much better result based on a little bit of compute and careful framing,” and also enable directly learning a noisy channel prompting model for even better performance. Zettlemoyer stressed the significance of open science to the research community and pointed out that limited access to models and restricted availability through APIs is “deeply problematic for doing good science.” He encouraged the audience to thoroughly explore research papers and look at the parameters to “see what the models are doing as much as you can.” Zettlemoyer summarized another project Facebook AI has been actively scaling up and trying to generalize for newer methods and suggested new types of training supervisions, with images, text, and more as discrete tokens. “Text is not all you need, consider the structure and other modalities,” he concluded. Cynthia Dwork | Radcliffe Institute for Advanced Study “What are risk prediction algorithms actually producing?” asked Professor Dwork to start her talk. She characterized this as a defining problem of artificial intelligence because it seeks to understand what the produced numbers mean and, more specifically, the “probability” of a non-repeatable event. Dwork walked the audience through the landscape of previous relevant scientific research, then introduced Outcome Indistinguishability, a recent study she worked on with researchers from UC Berkeley, Stanford University, and the Weizmann Institute of Science. With regard to this “probability” function, the team’s paper argued that “prediction algorithms assign numbers to individuals that are popularly understood as individual ‘probabilities’ — e.g. what is the probability of 5-year survival after a cancer diagnosis? — and which increasingly form the basis for life-altering decisions.” The study established the first scientific grounds for a “political argument” recommending auditors inspecting algorithmic risk prediction instruments be granted oracle access to the algorithm rather than simple historical predictions. The talk was followed by an insightful Q&A session between Professor Dwork and BAAI Chairman of the Board HongJiang Zhang. Professor Dwork mentioned that after working on differential privacy for many years, she wanted to come up with a new problem. Then, ten years ago, an intense and inspirational brainstorming session with a colleague from Tel Aviv University that spanned various topics drew her attention to the fairness of machine learning.We are now at a time when computer science is increasingly involved in social science and AI ethics. For young scholars, Dwork advised, “One of my key pieces of advice is to read the news and follow what’s going on in the world. See what you like and what you don’t like. And then, for everything that you don’t like, my approach would be, how on earth can theoretical computer science contribute to this? And for the things you like, how can theoretical computer science develop this? So constantly look for how the tools you have could be relevant to the major questions you see around you.”Dwork says paying close attention to societal affairs will enable young scholars to stay informed about the world and “ask how what you’ve learned that day in your technical studies could be relevant to the world.” After all, she added, once a scholar is trained, they become a precious resource, and their efforts and technical skills also bring an essential responsibility to society.As a strong believer in basic research with a deep understanding of and working experiences in academia and industry, Dwork said both career paths could be great as long as they support basic research. She suggested young scholars evaluate their pursuits, “For people who are more theoretical, what’s wonderful about industry research is the opportunity to see your theoretical idea deployed on a massive scale. Think about your security and where you want to work in your career, considering the job security and risk-taking.”Dwork believes it is vital to include algorithmic fairness and privacy among critical machine learning metrics such as accuracy. “I really believe that underlying everything is a metric. There is a notion somewhere, for a particular task, of how similar or dissimilar are each pair of individuals for this task? I would like to see a lot more work on that. I would also like to see a lot more work on the question of whether data representation itself is unfair. Everything begins with the data. So I suppose that would be the next big topic that I would like to look at, and I really encourage people watching this talk to think about that problem.“ In the Annual Young Scholars Forum and Meetup, HongJiang Zhang and UC Berkeley Professor Michael Jordan, who also serves on the BAAI Academic Advisory Committee, had a remarkable conversation on young scholars and career choices. Professor Jordan cautioned young scholars not to simply chase the next big thing in machine learning research. “I prefer to go a little slower, not to have a big competition, and to work with my students at my own pace. I always try to somewhat orthogonalize with respect to where everyone else is going. We can think about decisions and uncertainty, uncertainty qualification, out-of-sample kinds of things where the distribution changes, economic incentives for data collection and sharing data, and competitive mechanisms for making decisions when there are multiple decision-makers. These are all things that happen in the real world, and they are things that there’s much less work on, but there’s not zero work. One of the reasons I choose to work in such areas is because I see that they are really important, have real-world consequences, and they’ve been neglected, so not that many people are thinking about them.“Similar to Professor Dwork’s advice to young scholars, Jordan stressed that we must remember researchers are also real-world problem solvers. He added, “You really should think about a lifetime career. You should realize you’re not going to work on one thing. I spend at least 30% of my time learning new things or learning about topics that seem like they will be relevant to me sometime in the future.“There are lots of great videos now, and lots of great, even undergraduate-level books on some really interesting topics. I don’t look for them to give immediately me research ideas or a payoff. But after five years or ten years, almost always, they come to be relevant.”“So you have to be a problem solver. You can’t just come in and apply the technology and walk away and move on. You have to spend the time understanding the problem and bringing the technology to bear. I’ve done a lot of applied math, I’ve got a lot of control theory, and I’ve done a lot of statistics. So my brain has a partial understanding of all those things — and I can tell you that’s been the biggest secret of my success.” The first BAAI conference was held in October 2019. Informed by the organizers’ growing experience in fostering a welcoming environment for top talents and with a consistent focus on exploring fundamental AI technologies, BAAI 2022 proved an outstanding Expert AI Conference, attracting 30,000 registrations and 100,000 online attendees from various Chinese provinces and 47 international countries and regions.. BAAI provides valuable platforms for the ever-growing global community of scholars, students, and industry experts pursuing advancements in AI research and real-world applications, helping them to learn, share and flourish. Author: Fangyu Cai| Editor: Michael Sarazen","['baai', 'conference', 'kick', 'run', 'ai', 'expert', 'industry', 'leader', 'young', 'talent', 'international', 'delegate', 'join', 'virtual', 'gathering', 'live', 'stream', 'busy', 'day', 'highlevel', 'keynote', 'tech', 'talk', 'parallel', 'forum', 'network', 'conference', 'address', 'variety', 'contemporary', 'ai', 'challenge', 'largescale', 'multimodal', 'model', 'neurocomputer', 'ai', 'science', 'autonomous', 'driving', 'much', 'host', 'artificial', 'intelligence', 'baai', 'wellestablished', 'nonprofit', 'research', 'institute', 'promote', 'strategic', 'collaboration', 'academia', 'industry', 'bridge', 'gap', 'pioneer', 'research', 'cuttingedge', 'application', 'complex', 'influential', 'academic', 'conference', 'baai', 'welcome', 'domestic', 'international', 'expert', 'scholar', 'include', 'israeli', 'ture', 'award', 'laureate', 'research', 'scientist', 'laureate', 'head', 'hug', 'face', 'research', 'head', 'openai', 'research', 'scientist', 'follow', 'conference', 'opening', 'remark', 'professor', 'take', 'stage', 'introduce', 'important', 'baai', 'advancement', 'tian', 'bao', 'chip', 'ecosystem', 'laboratory', 'jiude', 'aicompute', 'platform', 'late', 'application', 'power', 'first', 'superscale', 'intelligent', 'model', 'wudao', 'big', 'center', 'director', 'describe', 'metaworm', 'computational', 'model', 'caenorhabditis', 'elegan', 'elegan', 'nematode', 'detailed', 'nervous', 'system', 'synergy', 'digital', 'body', 'build', 'muscle', 'interactive', 'threedimensional', 'fluid', 'simulation', 'environment', 'realtime', 'digital', 'worm', 'achieve', 'forward', 'worming', 'simulation', 'environment', 'represent', 'milestone', 'evolution', 'project', 'exhibit', 'behaviour', 'parallel', 'real', 'world', 'next', 'goal', 'evolution', 'project', 'demonstrate', 'complex', 'intelligent', 'behaviour', 'avoidance', 'response', 'optimal', 'foraging', 'metaworm', 'evolve', 'goal', 'gradually', 'become', 'intelligent', 'use', 'neuron', 'circuit', 'small', 'animal', 'investigate', 'learn', 'nature', 'improve', 'artificial', 'intelligence', 'technology', 'always', 'promising', 'approach', 'example', 'study', 'publish', 'nature', 'machine', 'intelligence', 'show', 'novel', 'ai', 'system', 'control', 'neuron', 'inspire', 'brain', 'elegan', 'enable', 'highfidelity', 'autonomy', 'taskspecific', 'part', 'complex', 'autonomous', 'system', 'steer', 'command', 'summary', 'metaworm', 'biologically', 'accurate', 'computational', 'model', 'achieve', 'follow', 'breakthrough', 'significant', 'ai', 'technical', 'innovation', 'often', 'occur', 'breakthrough', 'infrastructure', 'technology', 'reflect', 'baai', 'announce', 'chip', 'ecosystem', 'lab', 'jiude', 'aicompute', 'platform', 'researcher', 'use', 'boost', 'ai', 'evolution', 'crosslayer', 'innovation', 'jiuding', 'world', 'large', 'chinese', 'dataset', 'ai', 'training', 'provide', 'computation', 'capacity', 'highspeed', 'interconnection', 'server', 'support', 'ai', 'research', 'activity', 'enable', 'jiude', 'effectively', 'run', 'ai', 'chipset', 'different', 'vendor', 'baai', 'researcher', 'explore', 'adaptation', 'layer', 'selflearning', 'selfadaptation', 'capability', 'automatically', 'find', 'suitable', 'computing', 'resource', 'different', 'task', 'baai', 'chief', 'explain', 'ai', 'chip', 'deliver', 'market', 'ai', 'platform', 'infrastructure', 'require', 'support', 'ai', 'chip', 'various', 'architecture', 'different', 'capability', 'need', 'explore', 'automatic', 'technology', 'help', 'industry', 'solve', 'adaptation', 'problem', 'kind', 'ai', 'system', 'innovation', 'critical', 'chip', 'industry', 'well', 'conference', 'baai', 'announce', 'opensourcing', 'algorithm', 'tool', 'project', 'design', 'support', 'mainstream', 'largescale', 'multimodal', 'foundation', 'model', 'simplify', 'trial', 'development', 'strongly', 'believe', 'benefit', 'open', 'science', 'consistently', 'promote', 'opensource', 'approach', 'innovation', 'algorithms', 'flagai', 'project', 'launch', 'github', 'plenty', 'tutorial', 'example', 'support', 'research', 'global', 'opensource', 'community', 'additional', 'flagai', 'feature', 'algorithm', 'add', 'future', 'release', 'focus', 'project', 'year', 'continue', 'close', 'collaboration', 'various', 'company', 'field', 'lifestyle', 'art', 'language', 'expedite', 'application', 'complex', 'realworld', 'task', 'work', 'oppo', 'tal', 'education', 'group', 'sogou', 'partnership', 'produce', 'largescale', 'multimodal', 'model', 'provide', 'user', 'daily', 'convenience', 'search', 'ad', 'smart', 'assistant', 'finegraine', 'sentiment', 'analysis', 'boost', 'search', 'ad', 'revenue', 'last', 'month', 'baai', 'release', 'cogview2', 'synced', 'coverage', 'conference', 'cogview', 'team', 'showcase', 'cogvideo', 'next', 'big', 'breakthrough', 'cogvideo', 'largescale', 'pretraine', 'texttovideo', 'transformer', 'model', 'train', 'inherit', 'pretraine', 'texttoimage', 'model', 'cogview2', 'project', 'team', 'also', 'carry', 'number', 'strategic', 'collaboration', 'worldrenowned', 'organization', 'institute', 'currently', 'work', 'science', 'technology', 'maritime', 'transport', 'bibliotheca', 'alexandrina', 'build', 'world', 'large', 'arabic', 'language', 'dataset', 'joint', 'effort', 'aim', 'develop', 'largescale', 'arab', 'language', 'model', 'application', 'solve', 'complex', 'realworld', 'challenge', 'keynote', 'presentation', 'highlight', 'summarize', 'deepmind', 'distinguished', 'research', 'scientist', 'start', 'talk', 'essential', 'question', 'intelligence', 'ultimately', 'explain', 'objective', 'term', 'state', 'external', 'world', 'object', 'people', 'experiential', 'term', 'sensation', 'action', 'reward', 'note', 'experience', 'play', 'increase', 'role', 'consideration', 'ai', 'decade', 'identify', 'significant', 'step', 'ai', 'turn', 'experience', 'ground', 'learnable', 'scalable', 'posit', 'alternative', 'objective', 'state', 'experiential', 'state', 'world', 'define', 'entirely', 'term', 'experience', 'useful', 'predict', 'control', 'future', 'experience', 'propose', 'experiential', 'state', 'recursively', 'update', 'combine', 'experiential', 'step', 'standard', 'model', 'experiential', 'agent', 'obtain', 'also', 'offer', 'philosophical', 'take', 'matter', 'experience', 'offer', 'path', 'know', 'world', 'fact', 'world', 'fact', 'experience', 'learn', 'verify', 'experience', 'step', 'far', 'complete', 'note', 'many', 'research', 'opportunity', 'remain', 'story', 'intelligence', 'day', 'tell', 'term', 'sensorimotor', 'experience', 'research', 'team', 'lead', 'share', 'work', 'essentially', 'represent', 'different', 'approach', 'reinforcement', 'learn', 'rl', 'accomplish', 'grand', 'ambition', 'powerful', 'answer', 'novel', 'paradigm', 'aigenerate', 'suggest', 'look', 'history', 'science', 'progress', 'innovation', 'critical', 'science', 'technological', 'innovation', 'generate', 'problem', 'simultaneously', 'carry', 'goal', 'switching', 'exactly', 'mean', 'clune', 'explain', 'way', 'solve', 'complex', 'problem', 'create', 'problem', 'simultaneously', 'solve', 'perform', 'goal', 'switching', 'example', 'know', 'cooking', 'method', 'heat', 'hanging', 'pot', 'scientist', 'reward', 'produce', 'fast', 'hot', 'food', 'less', 'smoke', 'microwave', 'oven', 'never', 'invent', 'related', 'radar', 'technology', 'developedhow', 'get', 'algorithm', 'goal', 'switching', 'realize', 'serendipitous', 'discovery', 'family', 'algorithm', 'call', 'quality', 'diversity', 'algorithm', 'stand', 'presently', 'popular', 'algorithm', 'type', 'mapelite', 'return', 'entire', 'set', 'highquality', 'solution', 'clune', 'say', 'good', 'find', 'illustrative', 'outcome', 'takeaway', 'mapelite', 'paper', 'publish', 'naturerobot', 'adapt', 'animal', 'introduce', 'intelligent', 'trialanderror', 'allow', 'robot', 'adapt', 'damage', 'less', 'minute', 'require', 'selfdiagnosis', 'prespecifie', 'contingency', 'plan', 'face', 'dual', 'challenge', 'detachment', 'derailment', 'goexplore', 'come', 'strategy', 'carry', 'phase', 'first', 'initialize', 'take', 'random', 'action', 'store', 'state', 'visit', 'start', 'simple', 'loop', 'problem', 'solve', 'robustifie', 'solution', 'deep', 'neural', 'network', 'imitation', 'learning', 'ideal', 'world', 'algorithm', 'require', 'domain', 'knowledge', 'take', 'advantage', 'clune', 'add', 'view', 'reflect', 'improved', 'result', 'goexplore', 'continue', 'demonstrate', 'value', 'quality', 'diversity', 'algorithm', 'hardexploration', 'problem', 'inspire', 'new', 'novel', 'research', 'direction', 'collect', 'diverse', 'repertoire', 'highquality', 'entity', 'give', 'lot', 'fuel', 'power', 'algorithm', 'downstream', 'traditional', 'machine', 'learning', 'usually', 'rely', 'human', 'effort', 'pick', 'challenge', 'algorithm', 'solve', 'clune', 'explain', 'way', 'interesting', 'focus', 'notion', 'openendless', 'mean', 'algorithm', 'truly', 'endlessly', 'innovate', 'run', 'forever', 'keep', 'interesting', 'new', 'thing', 'example', 'natural', 'evolution', 'go', 'year', 'question', 'make', 'algorithm', 'way', 'think', 'make', 'run', 'year', 'clune', 'propose', 'researcher', 'learn', 'natural', 'evolution', 'human', 'culture', 'innovate', 'mean', 'generate', 'new', 'problem', 'solve', 'generate', 'new', 'problem', 'enthusiastically', 'introduce', 'result', 'obtain', 'enhance', 'poet', 'phylogenetic', 'tree', 'first', 'environment', 'poet', 'run', 'favourite', 'plot', 'result', 'entire', 'scientific', 'career', 'try', 'generate', 'plot', 'phd', 'student', 'day', 'clune', 'say', 'result', 'leave', 'unforgettable', 'impression', 'resemble', 'nature', 'exhibit', 'clear', 'signature', 'openende', 'algorithm', 'multiple', 'deep', 'hierarchically', 'nest', 'branch', 'clune', 'conclude', 'note', 'long', 'way', 'go', 'achieve', 'artificial', 'general', 'intelligence', 'agi', 'pillar', 'support', 'research', 'goal', 'clune', 'believe', 'aigenerate', 'algorithm', 'likely', 'fast', 'path', 'agi', 'hypothesis', 'piece', 'ai', 'learnable', 'learn', 'solution', 'ultimately', 'win', 'go', 'allin', 'learn', 'solution', 'agi', 'algorithm', 'worthwhile', 'even', 'fast', 'path', 'shed', 'light', 'origin', 'tend', 'creative', 'surprise', 'clune', 'tell', 'audience', 'create', 'entirely', 'new', 'intelligence', 'never', 'dream', 'teach', 'mean', 'intelligent', 'luke', 'zettlemoyer', 'ai', 'large', 'language', 'model', 'keep', 'get', 'big', 'ask', 'zettlemoyer', 'address', 'hot', 'topic', 'machine', 'learning', 'research', 'synced', 'previously', 'report', 'whopping', 'parameter', 'roughly', 'time', 'openai', 'powerful', 'gpt3', 'today', 'datahungry', 'language', 'model', 'get', 'lot', 'big', 'also', 'become', 'adept', 'learner', 'researcher', 'able', 'keep', 'scale', 'approach', 'physical', 'limitation', 'compute', 'hardware', 'well', 'use', 'big', 'model', 'alternative', 'form', 'supervision', 'zettlemoyer', 'introduce', 'idea', 'research', 'team', 'investigate', 'enable', 'far', 'scale', 'zettlemoyer', 'note', 'expert', 'specialization', 'assignment', 'often', 'depend', 'previous', 'word', 'correspond', 'simple', 'word', 'cluster', 'even', 'simple', 'routing', 'work', 'try', 'scale', '’d', 'want', 'go', 'simple', 'zettlemoyer', 'suggest', 'note', 'demix', 'layer', 'approach', 'much', 'much', 'simple', 'currently', 'actively', 'try', 'scale', 'domain', 'expert', 'modular', 'model', 'mix', 'add', 'remove', 'expert', 'necessary', 'rapid', 'adaptation', 'training', 'testing', 'zettlemoyer', 'showcase', 'recent', 'study', 'alternative', 'finetune', 'natural', 'language', 'generation', 'task', 'keep', 'language', 'model', 'parameter', 'freeze', 'lightweight', 'prefixtuning', 'paradigm', 'propose', 'researcher', 'deliver', 'much', 'well', 'result', 'base', 'little', 'bit', 'compute', 'careful', 'framing', 'also', 'enable', 'directly', 'learn', 'noisy', 'channel', 'prompt', 'model', 'even', 'well', 'performance', 'zettlemoyer', 'stress', 'significance', 'open', 'science', 'research', 'community', 'point', 'limited', 'access', 'model', 'restrict', 'availability', 'deeply', 'problematic', 'good', 'science', 'encourage', 'audience', 'thoroughly', 'explore', 'research', 'paper', 'look', 'parameter', 'see', 'model', 'much', 'zettlemoyer', 'summarize', 'project', 'facebook', 'ai', 'actively', 'scale', 'try', 'generalize', 'new', 'method', 'suggest', 'new', 'type', 'training', 'supervision', 'image', 'text', 'discrete', 'tokens', 'text', 'need', 'consider', 'structure', 'modality', 'conclude', 'advanced', 'study', 'risk', 'prediction', 'algorithm', 'actually', 'produce', 'ask', 'professor', 'dwork', 'start', 'talk', 'characterize', 'defining', 'problem', 'artificial', 'intelligence', 'seek', 'understand', 'produce', 'number', 'mean', 'specifically', 'probability', 'nonrepeatable', 'event', 'dwork', 'walk', 'audience', 'landscape', 'previous', 'relevant', 'scientific', 'research', 'introduce', 'outcome', 'indistinguishability', 'recent', 'study', 'work', 'researcher', 'science', 'regard', 'probability', 'function', 'team', 'paper', 'argue', 'prediction', 'algorithm', 'assign', 'number', 'individual', 'popularly', 'understand', 'individual', 'probability', 'eg', 'probability', 'survival', 'cancer', 'diagnosis', 'increasingly', 'form', 'basis', 'lifealtering', 'decision', 'study', 'establish', 'first', 'scientific', 'ground', 'political', 'argument', 'recommend', 'auditor', 'inspect', 'algorithmic', 'risk', 'prediction', 'instrument', 'grant', 'oracle', 'access', 'rather', 'simple', 'historical', 'prediction', 'talk', 'follow', 'insightful', 'qa', 'session', 'professor', 'dwork', 'baai', 'chairman', 'professor', 'mention', 'work', 'differential', 'privacy', 'many', 'year', 'want', 'come', 'new', 'problem', 'year', 'ago', 'intense', 'inspirational', 'brainstorming', 'session', 'colleague', 'span', 'various', 'topic', 'draw', 'attention', 'fairness', 'machine', 'time', 'computer', 'science', 'increasingly', 'involve', 'social', 'science', 'ai', 'ethic', 'young', 'scholar', 'dwork', 'advise', 'key', 'piece', 'advice', 'read', 'news', 'follow', 'go', 'world', 'see', 'like', 'like', 'like', 'approach', 'earth', 'theoretical', 'computer', 'science', 'contribute', 'thing', 'like', 'theoretical', 'computer', 'science', 'develop', 'constantly', 'look', 'tool', 'relevant', 'major', 'question', 'see', 'youdwork', 'say', 'pay', 'close', 'attention', 'societal', 'affair', 'enable', 'young', 'scholar', 'stay', 'informed', 'world', 'ask', 'learn', 'day', 'technical', 'study', 'relevant', 'world', 'add', 'scholar', 'train', 'become', 'precious', 'resource', 'effort', 'technical', 'skill', 'also', 'bring', 'essential', 'responsibility', 'strong', 'believer', 'basic', 'research', 'deep', 'understanding', 'working', 'experience', 'academia', 'industry', 'dwork', 'say', 'career', 'path', 'great', 'long', 'support', 'basic', 'research', 'suggest', 'young', 'scholar', 'evaluate', 'pursuit', 'people', 'theoretical', '’s', 'wonderful', 'industry', 'research', 'opportunity', 'see', 'theoretical', 'idea', 'deploy', 'massive', 'scale', 'think', 'security', 'want', 'work', 'career', 'consider', 'job', 'security', 'risktakingdwork', 'believe', 'vital', 'include', 'algorithmic', 'fairness', 'privacy', 'critical', 'machine', 'learning', 'metric', 'accuracy', 'really', 'believe', 'underlie', 'metric', 'notion', 'somewhere', 'particular', 'task', 'similar', 'dissimilar', 'pair', 'individual', 'task', 'like', 'see', 'lot', 'work', 'also', 'like', 'see', 'lot', 'work', 'question', 'data', 'representation', 'unfair', 'begin', 'datum', 'suppose', 'next', 'big', 'topic', 'like', 'look', 'really', 'encourage', 'people', 'watch', 'talk', 'think', 'problem', 'annual', 'young', 'scholar', 'meetup', 'also', 'serve', 'committee', 'remarkable', 'conversation', 'young', 'scholar', 'career', 'choice', 'professor', 'caution', 'young', 'scholar', 'simply', 'chase', 'next', 'big', 'thing', 'machine', 'learning', 'research', 'prefer', 'go', 'little', 'slow', 'big', 'competition', 'work', 'student', 'pace', 'always', 'try', 'somewhat', 'orthogonalize', 'respect', 'else', 'go', 'think', 'decision', 'uncertainty', 'uncertainty', 'qualification', 'outofsample', 'kind', 'thing', 'distribution', 'change', 'economic', 'incentive', 'data', 'collection', 'share', 'datum', 'competitive', 'mechanism', 'make', 'decision', 'multiple', 'decisionmaker', 'thing', 'happen', 'real', 'world', 'thing', '’', 'much', 'less', 'work', '’', 'work', 'reason', 'choose', 'work', 'area', 'see', 'really', 'important', 'realworld', 'consequence', 'neglect', 'many', 'people', 'think', 'themsimilar', 'professor', 'advice', 'young', 'scholar', 'stress', 'remember', 'researcher', 'also', 'problem', 'solver', 'add', 'really', 'think', 'lifetime', 'career', 'realize', 'go', 'work', 'thing', 'spend', 'least', 'time', 'learn', 'new', 'thing', 'learn', 'topic', 'seem', 'relevant', 'sometime', 'futurethere', 'lot', 'great', 'video', 'lot', 'great', 'even', 'undergraduatelevel', 'book', 'really', 'interesting', 'topic', 'look', 'give', 'immediately', 'research', 'idea', 'payoff', 'year', 'year', 'almost', 'always', 'come', 'relevantso', 'problem', 'solver', 'come', 'apply', 'technology', 'walk', 'away', 'move', 'spend', 'time', 'understand', 'problem', 'bring', 'technology', 'bear', 'lot', 'apply', 'math', 'get', 'lot', 'control', 'theory', 'lot', 'statistic', 'brain', 'partial', 'understanding', 'thing', 'tell', 'big', 'secret', 'success', 'first', 'baai', 'conference', 'hold', 'inform', 'organizer', 'grow', 'experience', 'foster', 'welcoming', 'environment', 'top', 'talent', 'consistent', 'focus', 'explore', 'fundamental', 'prove', 'outstanding', 'expert', 'ai', 'conference', 'attract', 'registration', 'online', 'attendee', 'various', 'chinese', 'province', 'international', 'country', 'region', 'baai', 'provide', 'valuable', 'platform', 'evergrowing', 'global', 'community', 'scholar', 'student', 'industry', 'expert', 'pursue', 'advancement', 'research', 'realworld', 'application', 'help', 'learn', 'share', 'flourish', 'author', 'editor']"
Snap & NEU’s  EfficientFormer Models Push ViTs to MobileNet Speeds While Maintaining High Performance,https://syncedreview.com/2022/06/06/snap-neus-efficientformer-models-push-vits-to-mobilenet-speeds-while-maintaining-high-performance/,2022-06-06,"
In the new paper EfficientFormer: Vision Transformers at MobileNet, a research team from Snap Inc. and Northeastern University proposes EfficientFormer, a vision transformer that runs as fast as MobileNet while maintaining high performance.
","First proposed in 2020, vision transformers (ViT) have demonstrated promising performance across a variety of computer vision tasks. These breakthroughs however have come at the cost of speed, as ViTs run much slower than convolutional neural networks (CNNs). This latency issue and their extremely high computational costs have made it challenging to deploy ViTs on resource-constrained hardware such as mobile devices, limiting their real-world application. A research team from Snap Inc. and Northeastern University addresses this issue in the new paper EfficientFormer: Vision Transformers at MobileNet, which identifies inefficient operators in ViT architectures and proposes a new ViT design paradigm. The team’s resulting EfficientFormer models run as fast as lightweight MobileNet CNNs while maintaining the high performance of transformer architectures. The researchers summarize their study’s main contributions as: The proposed EfficientFormer comprises patch embedding and a stack of meta transformer blocks, where each block contains an unspecified token mixer followed by a multilayer perceptron block. The network has four stages, each serving as an embedding operation that maps the embedding dimensions and downsamples token length. EfficientFormer thus remains a fully transformer-based model that does not use MobileNet structures. The team also introduces a simple yet effective gradient-based search algorithm that obtains candidate networks to optimize EfficientFormer’s inference speed. In their empirical study, the team compared EfficientFormer with widely used CNN-based models and existing ViTs on image classification, object detection, and segmentation tasks. EfficientFormer outperformed existing transformer models and most competitive CNNs in the experiments, with the fastest variant, EfficientFormer-L1, achieving 79.2 percent top-1 accuracy on ImageNet-1K with only 1.6 ms inference latency on an iPhone 12; and the largest variant, EfficientFormer-L7, reaching 83.3 percent accuracy with only 7.0 ms latency. The study shows that ViTs can reach MobileNet speeds on mobile devices while maintaining transformers’ high performance. The team’s future research will explore EfficientFormer’s potential on other resource-constrained hardware.The EfficientFormer code and models are available on the project’s GitHub. The paper EfficientFormer: Vision Transformers at MobileNet Speed is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","First proposed in 2020, vision transformers (ViT) have demonstrated promising performance across a variety of computer vision tasks. These breakthroughs however have come at the cost of speed, as ViTs run much slower than convolutional neural networks (CNNs). This latency issue and their extremely high computational costs have made it challenging to deploy ViTs on resource-constrained hardware such as mobile devices, limiting their real-world application. A research team from Snap Inc. and Northeastern University addresses this issue in the new paper EfficientFormer: Vision Transformers at MobileNet, which identifies inefficient operators in ViT architectures and proposes a new ViT design paradigm. The team’s resulting EfficientFormer models run as fast as lightweight MobileNet CNNs while maintaining the high performance of transformer architectures. The researchers summarize their study’s main contributions as: The proposed EfficientFormer comprises patch embedding and a stack of meta transformer blocks, where each block contains an unspecified token mixer followed by a multilayer perceptron block. The network has four stages, each serving as an embedding operation that maps the embedding dimensions and downsamples token length. EfficientFormer thus remains a fully transformer-based model that does not use MobileNet structures. The team also introduces a simple yet effective gradient-based search algorithm that obtains candidate networks to optimize EfficientFormer’s inference speed. In their empirical study, the team compared EfficientFormer with widely used CNN-based models and existing ViTs on image classification, object detection, and segmentation tasks. EfficientFormer outperformed existing transformer models and most competitive CNNs in the experiments, with the fastest variant, EfficientFormer-L1, achieving 79.2 percent top-1 accuracy on ImageNet-1K with only 1.6 ms inference latency on an iPhone 12; and the largest variant, EfficientFormer-L7, reaching 83.3 percent accuracy with only 7.0 ms latency. The study shows that ViTs can reach MobileNet speeds on mobile devices while maintaining transformers’ high performance. The team’s future research will explore EfficientFormer’s potential on other resource-constrained hardware.The EfficientFormer code and models are available on the project’s GitHub. The paper EfficientFormer: Vision Transformers at MobileNet Speed is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['first', 'propose', 'vision', 'transformer', 'demonstrate', 'promising', 'performance', 'variety', 'computer', 'vision', 'task', 'breakthrough', 'however', 'come', 'cost', 'speed', 'vit', 'run', 'much', 'slow', 'convolutional', 'neural', 'network', 'cnn', 'latency', 'issue', 'extremely', 'high', 'computational', 'cost', 'make', 'challenge', 'deploy', 'vit', 'resourceconstrained', 'hardware', 'mobile', 'device', 'limit', 'realworld', 'application', 'research', 'team', 'northeastern', 'university', 'address', 'issue', 'new', 'paper', 'efficientformer', 'vision', 'transformer', 'mobilenet', 'identify', 'inefficient', 'operator', 'architecture', 'propose', 'new', 'design', 'paradigm', 'team', 'result', 'efficientformer', 'model', 'run', 'fast', 'lightweight', 'mobilenet', 'cnn', 'maintain', 'high', 'performance', 'transformer', 'architecture', 'researcher', 'summarize', 'study', 'main', 'contribution', 'propose', 'efficientformer', 'comprise', 'patch', 'embed', 'stack', 'meta', 'transformer', 'block', 'block', 'contain', 'unspecified', 'token', 'mixer', 'follow', 'multilayer', 'perceptron', 'block', 'network', 'stage', 'serve', 'embed', 'operation', 'map', 'embed', 'dimension', 'downsample', 'token', 'length', 'efficientformer', 'thus', 'remain', 'fully', 'transformerbase', 'model', 'use', 'mobilenet', 'structure', 'team', 'also', 'introduce', 'simple', 'yet', 'effective', 'gradientbased', 'search', 'algorithm', 'obtain', 'candidate', 'network', 'optimize', 'efficientformer', 'inference', 'speed', 'empirical', 'study', 'team', 'compare', 'efficientformer', 'widely', 'use', 'cnnbased', 'model', 'exist', 'vit', 'image', 'classification', 'object', 'detection', 'segmentation', 'task', 'efficientformer', 'outperform', 'exist', 'transformer', 'model', 'competitive', 'cnn', 'experiment', 'fast', 'variant', 'efficientformerl1', 'achieve', 'percent', 'top1', 'accuracy', 'inference', 'latency', 'iphone', 'large', 'variant', 'efficientformerl7', 'reach', 'percent', 'accuracy', 'latency', 'study', 'show', 'vit', 'reach', 'mobilenet', 'speed', 'mobile', 'device', 'maintain', 'transformer', 'high', 'performance', 'team', 'future', 'research', 'explore', 'potential', 'resourceconstraine', 'efficientformer', 'code', 'model', 'available', 'project', 'paper', 'efficientformer', 'vision', 'transformer', 'mobilenet', 'speed', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
NVIDIA & UW Introduce Factory: A Set of Physics Simulation Methods and Learning Tools for Contact-Rich Robotic Assembly,https://syncedreview.com/2022/06/03/nvidia-uw-introduce-factory-a-set-of-physics-simulation-methods-and-learning-tools-for-contact-rich-robotic-assembly/,2022-06-03,"
In the new paper Factory: Fast Contact for Robotic Assembly, a research team from NVIDIA and the University of Washington introduces Factory, a set of physics simulation methods and robot learning tools for simulating contact-rich interactions in assembly with high accuracy, efficiency, and robustness.
","When we think of modern industrial assembly lines, we can imagine a tireless ensemble of task-specific robots efficiently cutting, stamping, connecting, inserting, tightening or soldering whatever product the factory is manufacturing. While assembly is one of the oldest and widest applications of robotics, effectively simulating its many high-precision and contact-rich interactions remains a challenging task. In the new paper Factory: Fast Contact for Robotic Assembly, a research team from NVIDIA Corporation and the University of Washington introduces Factory, a set of physics simulation methods and robot learning tools for simulating contact-rich interactions in assembly with high accuracy, efficiency, and robustness. The team summarizes their study’s main contributions as: The researchers set out to build a tool that enables fast, accurate, and robust robotic assembly simulation with three key considerations: 1) geometric representations, 2) contact reduction schemes, and 3) numerical solvers. For geometric representation, the team adopted discrete, voxel-based SDFs to map points to distance-to-a-surface and ensure efficient, robust collision detection. For contact reduction, they combined normal similarity, penetration depth, and an area-based metric to reduce contacts and demonstrate the desired dynamics properties across various evaluation scenes. The team chose the Gauss-Seidel method as their numerical solver, as it can be accelerated via contact reduction to achieve better performance than other popular solvers such as Jacobi. The team’s resulting suite of physics simulation methods and robot learning tools — which they dub “Factory” — is able to simulate thousands of contact-rich interactions in PhysX and Isaac Gym environments in real-time on a single GPU. The researchers also provide 60 carefully designed, ISO-standard or manufacturer-based assets from the NIST Assembly Task Board 1 for high-accuracy simulation; and train proof-of-concept reinforcement learning (RL) policies in Isaac Gym for contact-rich nut-and-bolt assembly. Although Factory was designed to establish a state-of-the-art for contact-rich simulation in robotic assembly, the researchers say it can also be applied to additional robotics tasks such as grasping of complex non-convex shapes in home environments, locomotion on uneven outdoor terrain, and non-prehensile manipulation of aggregates of objects. The researchers hope that their work can help accelerate the efficiency of robotic assembly, and invite the machine learning community to establish benchmarks for solving the provided scenes and extend Factory to their own contact-rich applications. A Factory demo video is available on Vimeo. The paper Factory: Fast Contact for Robotic Assembly is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","When we think of modern industrial assembly lines, we can imagine a tireless ensemble of task-specific robots efficiently cutting, stamping, connecting, inserting, tightening or soldering whatever product the factory is manufacturing. While assembly is one of the oldest and widest applications of robotics, effectively simulating its many high-precision and contact-rich interactions remains a challenging task. In the new paper Factory: Fast Contact for Robotic Assembly, a research team from NVIDIA Corporation and the University of Washington introduces Factory, a set of physics simulation methods and robot learning tools for simulating contact-rich interactions in assembly with high accuracy, efficiency, and robustness. The team summarizes their study’s main contributions as: The researchers set out to build a tool that enables fast, accurate, and robust robotic assembly simulation with three key considerations: 1) geometric representations, 2) contact reduction schemes, and 3) numerical solvers. For geometric representation, the team adopted discrete, voxel-based SDFs to map points to distance-to-a-surface and ensure efficient, robust collision detection. For contact reduction, they combined normal similarity, penetration depth, and an area-based metric to reduce contacts and demonstrate the desired dynamics properties across various evaluation scenes. The team chose the Gauss-Seidel method as their numerical solver, as it can be accelerated via contact reduction to achieve better performance than other popular solvers such as Jacobi. The team’s resulting suite of physics simulation methods and robot learning tools — which they dub “Factory” — is able to simulate thousands of contact-rich interactions in PhysX and Isaac Gym environments in real-time on a single GPU. The researchers also provide 60 carefully designed, ISO-standard or manufacturer-based assets from the NIST Assembly Task Board 1 for high-accuracy simulation; and train proof-of-concept reinforcement learning (RL) policies in Isaac Gym for contact-rich nut-and-bolt assembly. Although Factory was designed to establish a state-of-the-art for contact-rich simulation in robotic assembly, the researchers say it can also be applied to additional robotics tasks such as grasping of complex non-convex shapes in home environments, locomotion on uneven outdoor terrain, and non-prehensile manipulation of aggregates of objects. The researchers hope that their work can help accelerate the efficiency of robotic assembly, and invite the machine learning community to establish benchmarks for solving the provided scenes and extend Factory to their own contact-rich applications. A Factory demo video is available on Vimeo. The paper Factory: Fast Contact for Robotic Assembly is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['think', 'modern', 'industrial', 'assembly', 'line', 'imagine', 'tireless', 'ensemble', 'taskspecific', 'robot', 'efficiently', 'cut', 'stamp', 'connect', 'inserting', 'tighten', 'solder', 'product', 'factory', 'manufacture', 'assembly', 'old', 'widest', 'application', 'robotic', 'effectively', 'simulate', 'many', 'highprecision', 'contactrich', 'interaction', 'remain', 'challenging', 'task', 'new', 'paper', 'factory', 'fast', 'contact', 'robotic', 'assembly', 'research', 'team', 'corporation', 'introduce', 'factory', 'set', 'physics', 'simulation', 'method', 'robot', 'learning', 'tool', 'simulate', 'contactrich', 'interaction', 'assembly', 'high', 'accuracy', 'efficiency', 'robustness', 'team', 'summarize', 'study', 'main', 'contribution', 'researcher', 'set', 'build', 'tool', 'enable', 'fast', 'accurate', 'robust', 'robotic', 'assembly', 'simulation', 'key', 'consideration', 'geometric', 'representation', 'contact', 'reduction', 'scheme', 'numerical', 'solver', 'geometric', 'representation', 'team', 'adopt', 'discrete', 'voxelbased', 'sdfs', 'map', 'point', 'distancetoasurface', 'ensure', 'efficient', 'robust', 'collision', 'detection', 'contact', 'reduction', 'combine', 'normal', 'similarity', 'penetration', 'depth', 'areabase', 'metric', 'reduce', 'contact', 'demonstrate', 'desire', 'dynamic', 'property', 'various', 'evaluation', 'scene', 'team', 'choose', 'gaussseidel', 'method', 'numerical', 'solver', 'accelerate', 'contact', 'reduction', 'achieve', 'well', 'performance', 'popular', 'solver', 'team', 'result', 'suite', 'physics', 'simulation', 'method', 'robot', 'learning', 'tool', 'dub', 'factory', 'able', 'simulate', 'thousand', 'contactrich', 'interaction', 'physx', 'isaac', 'gym', 'environment', 'realtime', 'single', 'gpu', 'researcher', 'also', 'provide', 'carefully', 'design', 'isostandard', 'manufacturerbased', 'asset', 'nist', 'assembly', 'task', 'board', 'highaccuracy', 'simulation', 'train', 'proofofconcept', 'reinforcement', 'learn', 'policy', 'gym', 'contactrich', 'nutandbolt', 'assembly', 'factory', 'design', 'establish', 'stateoftheart', 'contactrich', 'simulation', 'robotic', 'assembly', 'researcher', 'say', 'also', 'apply', 'additional', 'robotic', 'task', 'grasp', 'complex', 'nonconvex', 'shape', 'home', 'environment', 'locomotion', 'uneven', 'outdoor', 'terrain', 'nonprehensile', 'manipulation', 'aggregate', 'object', 'researcher', 'hope', 'work', 'help', 'accelerate', 'efficiency', 'robotic', 'assembly', 'invite', 'machine', 'learn', 'community', 'establish', 'benchmark', 'solve', 'provide', 'scene', 'extend', 'factory', 'contactrich', 'application', 'factory', 'demo', 'video', 'available', 'vimeo', 'paper', 'factory', 'fast', 'contact', 'robotic', 'assembly', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
Google Brain’s UViM: A Unified Approach for Modelling Diverse Vision Tasks Without Modifications,https://syncedreview.com/2022/06/02/google-brains-uvim-a-unified-approach-for-modelling-diverse-vision-tasks-without-modifications/,2022-06-02,"
In the new paper UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes, a Google Brain research team proposes UViM, a unified approach that leverages language modelling and discrete representation learning to enable the modelling of a wide range of computer vision tasks without task-specific modifications.
","Deep neural networks have revolutionized the field of computer vision, achieving unprecedented performance across a wide range of tasks. The production of high-dimensional structured outputs for vision tasks such as image segmentation, monocular depth estimation, object detection, etc. however requires human handcrafting of network architectures and tailoring of training procedures for each specific task. These are time-consuming processes that can also introduce the need for expert knowledge with regard to the task at hand. A Google Brain research team challenges this “fragmented” vision modelling paradigm in their new paper UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes, proposing UViM (Unified Vision Model), a novel approach that leverages language modelling and discrete representation learning to enable the modelling of diverse computer vision tasks without any task-specific modifications. In the field of natural language processing (NLP), autoregressive sequence models parameterized by transformer architectures have emerged as a prominent unified model that enjoys advantages such as theoretical soundness, expressiveness, and robustness. This motivated the Google researchers to design a similar general solution for computer vision. The proposed UViM is a unified computer vision model that combines a standard feedforward base model and an autoregressive language model. It can handle vision tasks that deal with extremely high dimensional and structured outputs with much lower computational costs. The UViM optimization procedure comprises two training stages: learning with a guiding code and learning to model the guiding code. In the first stage, a restricted oracle model produces a short discrete sequence (guiding code) to help the base model solve complex vision tasks and reduce the cost of high-dimensional structured prediction. In the second stage, the team trains a language model to output a guiding code by learning to “mimic” the oracle using only the image input. The resulting UViM is thus equipped to model highly structured outputs for diverse vision tasks. In their empirical study, the team applied UViM to three diverse vision tasks: general scene understanding panoptic segmentation, conditional generative image colorization, and 3D scene depth prediction understanding. In the evaluations, the proposed UViM achieved results competitive with the state-of-the-art on all three tasks, confirming its ability to handle diverse vision tasks in a unified manner. The team regards UViM as a “brave new prototype” for a general-purpose unified computer vision model and hopes their paper will motivate future research on the generation of better guiding codes and the design of more efficient training procedures. The paper UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Deep neural networks have revolutionized the field of computer vision, achieving unprecedented performance across a wide range of tasks. The production of high-dimensional structured outputs for vision tasks such as image segmentation, monocular depth estimation, object detection, etc. however requires human handcrafting of network architectures and tailoring of training procedures for each specific task. These are time-consuming processes that can also introduce the need for expert knowledge with regard to the task at hand. A Google Brain research team challenges this “fragmented” vision modelling paradigm in their new paper UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes, proposing UViM (Unified Vision Model), a novel approach that leverages language modelling and discrete representation learning to enable the modelling of diverse computer vision tasks without any task-specific modifications. In the field of natural language processing (NLP), autoregressive sequence models parameterized by transformer architectures have emerged as a prominent unified model that enjoys advantages such as theoretical soundness, expressiveness, and robustness. This motivated the Google researchers to design a similar general solution for computer vision. The proposed UViM is a unified computer vision model that combines a standard feedforward base model and an autoregressive language model. It can handle vision tasks that deal with extremely high dimensional and structured outputs with much lower computational costs. The UViM optimization procedure comprises two training stages: learning with a guiding code and learning to model the guiding code. In the first stage, a restricted oracle model produces a short discrete sequence (guiding code) to help the base model solve complex vision tasks and reduce the cost of high-dimensional structured prediction. In the second stage, the team trains a language model to output a guiding code by learning to “mimic” the oracle using only the image input. The resulting UViM is thus equipped to model highly structured outputs for diverse vision tasks. In their empirical study, the team applied UViM to three diverse vision tasks: general scene understanding panoptic segmentation, conditional generative image colorization, and 3D scene depth prediction understanding. In the evaluations, the proposed UViM achieved results competitive with the state-of-the-art on all three tasks, confirming its ability to handle diverse vision tasks in a unified manner. The team regards UViM as a “brave new prototype” for a general-purpose unified computer vision model and hopes their paper will motivate future research on the generation of better guiding codes and the design of more efficient training procedures. The paper UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['deep', 'neural', 'network', 'revolutionize', 'field', 'computer', 'vision', 'achieve', 'unprecedented', 'performance', 'wide', 'range', 'task', 'production', 'highdimensional', 'structure', 'output', 'vision', 'task', 'image', 'segmentation', 'monocular', 'depth', 'estimation', 'object', 'detection', 'however', 'require', 'human', 'handcrafting', 'network', 'architecture', 'tailoring', 'training', 'procedure', 'specific', 'task', 'timeconsuming', 'process', 'also', 'introduce', 'need', 'expert', 'knowledge', 'regard', 'task', 'hand', 'brain', 'research', 'team', 'challenge', 'fragmented', 'vision', 'model', 'paradigm', 'new', 'paper', 'uvim', 'unified', 'modeling', 'approach', 'vision', 'learned', 'guide', 'code', 'propose', 'uvim', 'unified', 'vision', 'model', 'novel', 'approach', 'leverage', 'language', 'modelling', 'discrete', 'representation', 'learn', 'enable', 'modelling', 'diverse', 'computer', 'vision', 'task', 'taskspecific', 'modification', 'field', 'natural', 'language', 'processing', 'autoregressive', 'sequence', 'model', 'parameterize', 'transformer', 'architecture', 'emerge', 'prominent', 'unified', 'model', 'enjoy', 'advantage', 'theoretical', 'soundness', 'expressiveness', 'robustness', 'motivate', 'researcher', 'design', 'similar', 'general', 'solution', 'computer', 'vision', 'propose', 'uvim', 'unified', 'computer', 'vision', 'model', 'combine', 'standard', 'feedforward', 'base', 'model', 'autoregressive', 'language', 'model', 'handle', 'vision', 'task', 'deal', 'extremely', 'high', 'dimensional', 'structure', 'output', 'much', 'low', 'computational', 'cost', 'uvim', 'optimization', 'procedure', 'comprise', 'training', 'stage', 'learn', 'guide', 'code', 'learn', 'model', 'guide', 'code', 'first', 'stage', 'restricted', 'oracle', 'model', 'produce', 'short', 'discrete', 'sequence', 'guide', 'code', 'help', 'base', 'model', 'solve', 'complex', 'vision', 'task', 'reduce', 'cost', 'highdimensional', 'structured', 'prediction', 'second', 'stage', 'team', 'train', 'language', 'model', 'output', 'guide', 'code', 'learn', 'mimic', 'oracle', 'use', 'image', 'input', 'result', 'uvim', 'thus', 'equip', 'model', 'highly', 'structured', 'output', 'diverse', 'vision', 'task', 'empirical', 'study', 'team', 'apply', 'uvim', 'diverse', 'vision', 'task', 'general', 'scene', 'understand', 'panoptic', 'segmentation', 'conditional', 'generative', 'image', 'colorization', 'scene', 'depth', 'prediction', 'understanding', 'evaluation', 'propose', 'uvim', 'achieve', 'result', 'competitive', 'stateoftheart', 'task', 'confirm', 'ability', 'handle', 'diverse', 'vision', 'task', 'unified', 'manner', 'team', 'regard', 'uvim', 'brave', 'new', 'prototype', 'generalpurpose', 'unify', 'computer', 'vision', 'model', 'hope', 'paper', 'motivate', 'future', 'research', 'generation', 'well', 'guide', 'code', 'design', 'efficient', 'training', 'procedure', 'paper', 'unified', 'modeling', 'approach', 'vision', 'learned', 'guide', 'code', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
