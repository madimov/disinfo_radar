title,url,date,text,cleaning,tokens
The CLRS Algorithmic Reasoning Benchmark,"[{'href': 'http://arxiv.org/abs/2205.15659v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2205.15659v2', 'rel': 'related', 'type': 'application/pdf'}]",2022-05-31 09:56:44,"2
2
0
2

n
u
J

7
1

]
E
M

.
t
a
t
s
[

1
v
6
1
6
8
0
.
6
0
2
2
:
v
i
X
r
a

Dynamical Modeling for non-Gaussian Data
with High-dimensional Sparse Ordinary
Diﬀerential Equations

Muye Nanshan, Nan Zhang
School of Data Science, Fudan University
and
Xiaolei Xun
Global Statistics and Data Science, BeiGene
and
Jiguo Cao
Department of Statistics and Actuarial Science, Simon Fraser University

Abstract

Ordinary diﬀerential equations (ODE) have been widely used for modeling dy-
namical complex systems. For high-dimensional ODE models where the number of
diﬀerential equations is large, it remains challenging to estimate the ODE parame-
ters and to identify the sparse structure of the ODE models. Most existing methods
exploit the least-square based approach and are only applicable to Gaussian obser-
vations. However, as discrete data are ubiquitous in applications, it is of practical
importance to develop dynamic modeling for non-Gaussian observations. New meth-
ods and algorithms are developed for both parameter estimation and sparse structure
identiﬁcation in high-dimensional linear ODE systems. First, the high-dimensional
generalized proﬁling method is proposed as a likelihood-based approach with ODE
ﬁdelity and sparsity-inducing regularization, along with eﬃcient computation based
on parameter cascading. Second, two versions of the two-step collocation methods
are extended to the non-Gaussian set-up by incorporating the iteratively reweighted
least squares technique. Simulations show that the proﬁling procedure has excellent
performance in latent process and derivative ﬁtting and ODE parameter estimation,
while the two-step collocation approach excels in identifying the sparse structure of
the ODE system. The usefulness of the proposed methods is also demonstrated by
analyzing three real datasets from Google trends, stock market sectors, and yeast cell
cycle studies.

Keywords: Dynamic system; Generalized linear model; Ordinary diﬀerential equations;
Parameter cascade; Penalized likelihood; Proﬁled estimation.

1

 
 
 
 
 
 
1

Introduction

Ordinary diﬀerential equations (ODE) are widely used for complex dynamic system mod-

eling in biology, engineering, econometrics, and other scientiﬁc and social applications. For

example, massive gene expression proﬁles are available with the advancement of second-

generation sequencing technology. Modeling their dynamics using gene regulatory networks

has drawn signiﬁcant interest from both biomedical and statistical research communities

(Stuart et al., 2003; Yuan and Kendziorski, 2006; Hecker et al., 2009; Polynikis et al., 2009;

Lu et al., 2011; Wu et al., 2014). In computational sociology, public opinion sensing and

trend analysis have emerged from the advent of the big data revolution (Dodds et al., 2011;

Sloan and Morgan, 2015). Massive datasets, such as Google searches or Twitter posts, are

collected daily or even hourly, which enables social scientists to extract interesting temporal

or spatial patterns via dynamic modeling. The main purpose of this article is to propose

new methods and algorithms to estimate the ODE parameters and to identify the sparse

structure for high-dimensional ODE models with non-Gaussian observations.

A general ﬁrst-order ODE system can be described as

θ′(t) = f (θ(t), β),

(1.1)

where the vector θ(t) = (θ1(t), . . . , θp(t))⊤ collects p processes while θ′(t) is the ﬁrst-order

derivative of θ(t), function f = (f1, . . . , fp) describes the dependence between processes

and their derivatives, β is the vector of ODE parameters to be estimated. Typically, the

processes are indexed with time t and some initial conditions, for example, θ(0) = θ0, are

assumed for the ODE system (1.1) as well.

In practice, observations from the dynamic system are measured according to the re-

alizations of latent processes θ(t) at discrete time points. Estimation of ODE parame-

ters from noisy data remains a challenging problem (Ramsay et al., 2007; Wu et al., 2014;

Hall and Ma, 2014; Chen et al., 2017; Wu et al., 2019; Dai and Li, 2021). In general, pa-

rameter estimation procedures fall into three categories. The ﬁrst approach is based on

a data ﬁtting process by nonlinear least squares. Given a set of initial ODE parameters,

the ODE solutions are approximated by numerical methods, for example, the Runge-Kutta

algorithm. Then the ODE parameters are updated with the nonlinear least squares. This

2

approach is computationally intensive and can be potentially inaccurate due to iterative

numerical approximations. The second approach is the two-step collocation, where the

basis expansions are exploited to approximate the ODE solutions. Varah (1982) proposed

to ﬁt the processes via data smoothing methods, followed by a second stage of minimizing a

least-square criterion based on the ODE system to estimate the ODE parameters. Because

of its computational advantage, two-step collocation gains much popularity in the develop-

ment of methodology and applications (Liang and Wu, 2008; Lu et al., 2011; Brunel et al.,

2014; Wu et al., 2014; Dattner and Klaassen, 2015) and is further improved by iterative

principal diﬀerential analysis (Ramsay, 1996; Poyton et al., 2006). However, the perfor-

mance of two-step procedures relies heavily on the smoothing step, while the amount of

roughness regularization is hard to control. The third approach is the generalized proﬁling

procedure (Ramsay et al., 2007), which also represents ODE solutions with basis expan-

sion as with two-step collocation methods. The essential diﬀerence is the inclusion of an

ODE-induced penalty that controls the ﬁdelity of the processes to the ODE system. The

basis coeﬃcients and ODE parameters are then estimated simultaneously from a penalized

criterion using the parameter cascading algorithm (Cao and Ramsay, 2007). From a the-

oretical perspective, Qi and Zhao (2010) derived an upper bound on the uniform norm of

the diﬀerence between the true underlying solutions and their approximations, and proved

the consistency and asymptotic normality of the estimation procedure.

More recently, there has been growing interest in high-dimensional ODE systems where

the number of processes p is large. For instance, the high-dimensional time-course gene

expression data enables biomedical researchers to model the regulatory behaviors via a

large-scale directed graphical network model. Such a task is called network recovery. The

ODE system (1.1) naturally serves for this purpose by relating the dynamics of each process

with all the processes in the system, and a sparse network structure can be further imposed.

Lu et al. (2011) considered the high-dimensional linear ODE for dynamic gene regulatory

network identiﬁcation and applied the smoothly clipped absolute deviation (Fan and Li,

2001) approach for variable selection. Wu et al. (2014) further relaxed the linear assump-

tion and investigated a sparse additive ODE model using a two-stage procedure coupled

with the adaptive group Lasso technique (Wang and Leng, 2008) to deal with nonlinear

3

eﬀects. Chen et al. (2017) proposed an eﬃcient procedure using the integrated form of the

ODE to bypass numerical diﬃculty in the derivative estimation and adopted the group

Lasso (Yuan and Lin, 2006) for variable selection. Wu et al. (2019) recently developed a

matrix factorization based approach to ultra-high dimensional linear ODE models for pa-

rameter estimation and variable selection. To our best knowledge, existing procedures for

high-dimensional ODE models are two-stage approaches.

Besides, most of the existing work assumes that observations of the ODE system are

contaminated with Gaussian noises. Therefore, least-squares estimation is conveniently

adopted. However, non-Gaussian observations are commonly encountered in real appli-

cations, for example, short read count data from RNA sequencing (Nagalakshmi et al.,

2008), bisulﬁte sequencing data for DNA methylation analysis (Cokus et al., 2008), and

direction of change in the stock price over time (Huang et al., 2005). The literature on

non-Gaussian data analysis with the ODE system is rare. Miao et al. (2014) developed

a likelihood-based parameter estimation and inference for generalized ODE models.

Its

extension to high-dimensional ODE models, however, is still unknown.

Motivated by network recovery tasks for time-course non-Gaussian data, this paper

focuses on the parameter estimation and sparse structure identiﬁcation for high-dimensional

linear ODE systems with a likelihood-based approach. To facilitate versatile analysis of

non-Gaussian data, we assume the observations follow a distribution from the exponential

family, where θj(t) is known as the canonical parameter in the context of generalized linear

models (McCullagh and Nelder, 1989; Wood, 2017). Assume that t ∈ [0, 1] without loss of

generality. Given a set of discrete time points t1, . . . , tn, denote by yij the measurement

according to the jth latent process θj(t) at time t = ti, j = 1, . . . , p. Then, the conditional

distribution of yij given θj(ti) admits a density function as

f (yij | θj(ti)) = exp

yijθj(ti) − b(θj(ti))
a(φ)

(cid:26)

+ c(yij, φ)

,

(cid:27)

where a > 0, b, c are known functions, φ is either known or considered as a nuisance
parameter. Let (y1j, . . . , ynj)⊤ be the vector of observations from the latent process θj(t),
and correspondingly the canonical parameter vector be (θj(t1), . . . , θj(tn))⊤. Imposing a

linear structure on the general model (1.1), we investigate in this work the modeling of

the dynamics among latent processes {θj(t) : j = 1, . . . , p} with a high-dimensional linear

4

ODE system, that is

′

θ
j(t) = γj0 +

p

k=1
X

γjkθk(t),

j = 1, . . . , p.

(1.2)

In this article, we develop new methods and algorithms for both parameter estima-

tion and sparse structure identiﬁcation in high-dimensional linear ODE systems. First, we

propose the high-dimensional generalized proﬁling method along with a computationally

eﬃcient procedure based on parameter cascading (Ramsay et al., 2007; Cao and Ramsay,

2007). It solves a hierarchical optimization for parameter estimation and variable selec-

tion: an outer optimization concerning the ODE parameters under sparsity regularization

is performed subject to an inner optimization where latent processes expanded with basis

functions are ﬁtted by minimizing a weighted sum of data ﬁtting and ODE ﬁdelity crite-

ria given ODE parameters.

In particular, we regularize the structural ODE parameters

based on individual diﬀerential equation and mitigate the computational burden for pa-

rameter estimation in the high-dimensional ODE system. Moreover, there are two tuning

parameters involved in our procedure: one controls the balance between data ﬁtting and

ODE ﬁdelity in the inner optimization while the other regularizes the sparsity or model

complexity in the outer optimization. Their interaction may aﬀect the overall convergence

performance of the procedure in a complicated way. Due to the non-convexity nature

of our objective function, we carefully design the tuning and stopping rules according to

the performance of parameter estimation to help escape local minima (Carey and Ramsay,

2021). The global convergence of the proposed algorithm is analyzed. Next, we extend the

two-step collocation methods (Wu et al., 2014; Chen et al., 2017), which are recently pro-

posed for high-dimensional ODE models with Gaussian observations, to the non-Gaussian

set-up. Two versions, corresponding to the vanilla collocation (Varah, 1982) and the graph

reconstruction via additive diﬀerential equations (GRADE) (Chen et al., 2017), are devel-

oped under the likelihood-based framework. Eﬃcient computation is feasible by applying

the iteratively reweighted least squares technique (Wood, 2017). Finally, we apply the

proposed methods to simulated and real data sets. In general, the proﬁling method is more

eﬃcient than two-step collocation methods in estimating the latent processes, their deriva-

tives, and the structural ODE parameters, while one two-step collocation method excels

5

in identifying the sparse structure of the ODE system. To sum up, the proposed methods

present a versatile toolbox for parameter estimation and sparse structure identiﬁcation in

high-dimensional linear ODE systems.

The remainder of the article is organized as follows. Our proﬁled estimation approach

is developed in Section 2. Detailed computational procedure and its global convergence

are discussed in Section 3. In Section 4, we extend two-step collocation methods to model

non-Gaussian observations. Section 5 compares empirical performance of the proposed

methods. We analyze three real data examples in Section 6 with dynamical modeling

approaches. Section 7 concludes the article and Appendix collects some technical details.

2 High-dimensional Generalized Proﬁling

This section introduces the proposed approach for simultaneous parameter estimation and

sparse structure identiﬁcation in a high-dimensional linear ODE model for non-Gaussian

data under the penalized likelihood estimation framework.

Denote by Γ = (γ1, . . . , γp) the parameter matrix of the ODE model (1.2), where γj =
(γj0, . . . , γjp)⊤ ∈ Rp+1, for j = 1, . . . , p. These ODE parameters Γ are of primary interest

in order to understand the network structure, called structural parameters hereafter. On

the other hand, the latent processes θj’s are treated as nuisance parameters. Denote by

yij and θj(ti) the observation and the canonical parameter of the jth latent process at

time ti, respectively. Under the proﬁling scheme (Ramsay et al., 2007), an intermediate

ﬁt of latent processes

θ(t; Γ) = (

θ1(t; Γ), . . . ,

θp(t; Γ)) minimizes the following penalized

likelihood criterion,

b

b

b

p

1

n

p

{yijθj(ti) − b(θj(ti))} + λθ

−

1
np

p

2

′

θ
j(t) − γj0 −
0 (

γjkθk(t)

dt,

(2.1)

)

i=1
X

j=1 Z
X
where the likelihood part measures ﬁdelity to data, the ODE ﬁdelity part measures the

j=1
X

k=1
X

extent to which latent processes fail to satisfy the ODE system, and the tuning parameter

λθ controls the amount of regularization. Furthermore, with

θ(t; Γ) plugged in, an estimate

of the structural parameters can be obtained by minimizing a data ﬁtting criterion with

b

6

respect to Γ,

−

1
np

i=1
X

j=1
X

n

p

{yij

θj(ti; Γ) − b(

θj(ti; Γ))}.

(2.2)

The generalized proﬁling procedure proceed iteratively with a non-decreasing sequence of

b

b

λθ under certain rules such that the ﬁtted processes adhere to the ODE. Identiﬁable issue

and asymptotic behavior of the estimation procedure are studied by Ramsay et al. (2007)

and Qi and Zhao (2010).

Although the generalized proﬁling method provides a computationally eﬃcient treat-

ment for the challenging ODE parameter estimation, it can only handle relatively small-

scale models (Wu et al., 2019). On the one hand, for a p-dimensional linear ODE system,

we have p2 + p ODE parameters to estimate in (2.2). If we further approximate the latent
process θj(t) by basis expansion c⊤

j hj(t), where hj(t) is an mj-dimensional basis vector and

cj is the coeﬃcient vector, then (2.1) becomes

−

1
np

n

p

yijc⊤

j hj(ti) − b(c⊤

j hj(ti))

i=1
X

j=1
X

(cid:8)

p

1

+ λθ

0 (

j=1 Z
X

(cid:9)
c⊤
j h

′

j(t) − γj0 −

p

γjkc⊤

j hk(t)

2

)

dt,

k=1
X

and the total number of nuisance parameters

p
j=1 mj can be huge. Therefore, a di-
rect application of the standard generalized proﬁling procedure to parameter estimation

P

for high-dimensional linear ODE is computationally demanding. On the other hand, the

structural parameters obtained from (2.2) indeed infer an interaction network among the

latent processes, in the sense that a nonzero γjk implies that θk(t) has an eﬀect on the

change of θj(t). For better interpretation and to avoid potential over-ﬁtting, it is reason-

able to introduce some sparsity for the structural parameters. For example, the Lasso

and its variants (Tibshirani, 1996; Yuan and Lin, 2006; Zou, 2006), the smoothly clipped

absolute deviation (SCAD) (Fan and Li, 2001) and the minimax concave penalty (MCP)

(Zhang, 2010) have been extensively studied and used to recover probabilistic graphical

structures (Yuan and Lin, 2007; Fan et al., 2009; Voorman et al., 2014).

To address the above computational issues, we ﬁrst notice that the data ﬁdelity term

in the penalized criterion (2.1) can be decomposed into sums of the likelihood for p in-

dividual processes. Meanwhile, the penalty term, being a squared L2 norm of diﬀerential

7

equations, does not admit such decomposable property. Therefore, we propose to regularize

the estimate of θj only by the corresponding jth diﬀerential equation. Speciﬁcally, when

estimating θj given other {θk : k 6= j} at their most recent updates, we obtain

θj(t; γj) by

minimizing

Gj(θj; γj) = −

1
n

n

i=1
X

{yijθj(ti) − b(θj(ti))} + λθ,j

1

′

j(t) − γj0 −
θ
0 (cid:26)
Z

b
2

γjkθk(t)

dt, (2.3)

(cid:27)

p

k=1
X

for j = 1, . . . , p. For simplicity, we use the same tuning parameter for individual sub-

problems, that is λθ,j = λθ for j = 1, . . . , p. Optimizing Gj involves only p + 1 structural

parameters in the vector γj and hence the computational complexity is greatly reduced.

The beneﬁt of using (2.3) is justiﬁed from two aspects. First, it is computationally in-

feasible to estimate a large number of ODE parameters jointly by directly applying the

original generalized proﬁling criterion (2.1) to the high-dimensional ODE system. Our new

formulation decouples the dependency of

θ(t; Γ) on the matrix Γ into individual depen-

dencies of θj(t; γj) on the vector γj. Second, from the perspective of penalized estimation,

b

it improves the estimation for the latent process and the ODE structural parameters by

employing diﬀerential equations to regularize data smoothing.

We remark on the potential risk of employing (2.3) instead of (2.1) when estimating

the latent processes. Note that (2.1) aggregates all the diﬀerential equations to update the

latent processes altogether such that the estimates will follow the ODE system jointly. In

contrast, our method uses a single diﬀerential equation to regularize the estimation of each

latent process. When the tuning parameter λθ increases, the parallel updating procedure

(2.3) over j = 1, . . . , p, is expected to achieve an approximation in a marginal way to

the joint estimation by (2.1). The simulation example introduced in Section S1 of the

Supplementary Material shows that the approximation by (2.3) performs reasonably well,

although the joint method (2.1) has a more accurate estimate for ODE parameters.

Next, to induce sparsity to the structural parameter matrix, we estimate γj by mini-

mizing

Hj(γj) = −

1
n

i=1
X

n

{yij

θj(ti; γj) − b(

θj(ti; γj))} + PENλγ,j (γj),

(2.4)

where the penalty function PENλγ,j (γj) with tuning parameter λγ,j > 0 induces sparsity for

b

b

the structural parameter of the jth diﬀerential equation. Here we also assume for simplicity

8

that λγ,j = λγ for j = 1, . . . , p. If the ﬁtted structural parameter vector

γj is zero, then we

γj implies that the
b

b

say other latent processes have no impact on θj(t). Any zero element in

corresponding process has no inﬂuence on θj(t). The amount of sparsity regularization is

typically determined by Bayesian information criterion (BIC) type principles, which have

been adopted in other ODE parameter estimation approaches (Wu et al., 2014; Chen et al.,

2017).

Our new proﬁling estimation procedure for high-dimensional linear ODE systems con-

sists of two objective functions (2.3) and (2.4), which are referred to as inner and outer

criteria, respectively. Such a multi-criterion optimization problem is challenging due to

non-convexity and non-diﬀerentiability. Speciﬁcally, we approximate the latent processes

with basis expansion in the inner optimization, and basis coeﬃcients can be solved eﬃ-

ciently with the Newton-Raphson method. However, the dependence of

θj(t; γj) on γj is

complicated and in general non-linear, which leads to the non-convexity of Hj. Moreover,

b

the sparsity-inducing penalty in Hj is non-diﬀerentiable at zero, making the Gauss-Newton

scheme adopted by Ramsay et al. (2007) invalid under this scenario.

Recent advances of derivative-free optimization algorithms (Powell, 2006; Zhang et al.,

2010) may provide a viable solution. Nevertheless, they are in spirit joint optimization

algorithms designed for general purpose and are thus not tailored for our speciﬁc problem.

In contrast, our proﬁling procedure enjoys not only estimation eﬃciency but also algo-

rithmic eﬃciency due to the use of analytical expressions of derivatives. Computational

details are presented in the next section.

In brief, after obtaining an estimate

θj(t; γj)

given the structural parameters, we linearize the likelihood component in (2.4) and formu-

b

late the outer optimization as a parameter estimation problem for a penalized generalized

linear model. Therefore, the structural parameters can be readily updated by the iterative

reweighted least-squares (IRLS). Through an iterative scheme between inner and outer op-

timizations, our proﬁling procedure provides ODE parameter estimates and latent process

ﬁts and identiﬁes the sparse structure of the ODE model.

9

3 Computation

In this section, we provide computational details of our proﬁling procedure for high-

dimensional linear ODE and analyze its global convergence. Minimizing criteria in (2.3)

and (2.4) are referred as inner and outer optimizations. The structural parameters Γ =

(γ1, . . . , γp) is of our primary interest, while the latent process ﬁts by the inner optimiza-

tion is regarded as a nuisance parameter. In our proﬁling scheme, whenever γj changes

by minimizing Hj in the outer, latent process ﬁts are then updated by solving the inner

criterion Gj. Details are provided in Algorithm 1. In addition, two tuning parameters are

involved in the proﬁling procedure, and their complex interaction aﬀects the overall algo-

rithmic performance because of the non-convexity of the optimization. In the following,

we split the discussion into inner and outer parts. Then we discuss the practical strategy

of tuning parameter selection and the global convergence of the proposed algorithm.

Algorithm 1: High-dimensional linear ODE for non-Gaussian data

Input: Observations {yij : i = 1, . . . , n; j = 1, . . . , p}, initial ODE paramters

Γ(0) = (γ(0)

1 , . . . , γ(0)

p ), and ﬁxed tuning parameters λθ and λγ.

Output: Estimated ODE parameters

Γ = (

γ1, . . . ,

γp).

repeat

b
At step s ≥ 1, the current estimate is

b
Γ(s) = (

b
γ(s)
1 , . . . ,

γ(s)
p ).

for 1 ≤ j ≤ p do

Update γj via the proﬁling procedure.

b

b

b

repeat

1. Given current

γj, obtain the basis coeﬃcient estimate c∗
j (

γj) for

the jth latent process in the inner optimization.

e

e

2. Apply basis expansion and update γj via minimizing the penalized

reweighted least squares.

until

γj converges, and set γ(s+1)

j

=

γj.

end

e

until Estimated ODE parameter

e
Γ converges.

b

10

3.1

Inner Optimization

The inner procedure aims at ﬁnding an accurate estimate for latent processes given the

structural parameter Γ. Similar to the two-step collocation method (Varah, 1982) and the

generalized proﬁling (Ramsay et al., 2007), we represent latent processes by basis expan-

sion. Suppose hj(t) = (φj1(t), . . . , φjmj (t)) is a set of basis functions for the jth process
such that θj(t) = c⊤

j hj(t). Choices of basis functions include polynomials, truncated power

functions and splines. In our numerical study, we use B-spline due to its numerical stability

and excellent empirical performance. For notation simplicity, we use the same basis h(t)

for all latent processes.

Although critical in optimization, the basis coeﬃcients cj, j = 1, . . . , p, are often not

of direct concern and thus considered as nuisance parameters. Observing that Gj(θj; γj) is

convex with respect to the basis coeﬃcients cj, we can apply the Newton-Raphson scheme

directly. When the Hessian of Gj(θj; γj) is invertible, we can start with an initial guess of

cj and iteratively obtain

c(r+1)
j

= c(r)

j −

(cid:18)

−1

c(r)
j (cid:19)

(cid:18)

∂2Gj
∂cj∂c⊤
j (cid:12)
(cid:12)
(cid:12)
(cid:12)

∂Gj
,
∂cj (cid:12)
c(r)
j (cid:19)
(cid:12)
(cid:12)
(cid:12)

r ≥ 1.

Analytical expressions of the derivatives involved in the above updating rule are given in

A.

3.2 Outer Optimization

The outer optimization is designed for updating γj with a regularized likelihood objective
function (2.4). Denote by c∗
the inner optimization given the current γj. Observing that the dependence of c∗

j (γj) the optimal basis coeﬃcients for θj(t; γj) obtained from

j (γj) on

γj is implicit and possibly complicated, we propose to linearize the likelihood component

in (2.4) and transform the optimization to ﬁnding the maximum likelihood estimate of

a generalized linear model. The solution can then be readily obtained by the iteratively

reweighted least squares (IRLS), see Wood (2017) for more detail.

Let

γj be the most recent update of γj. First, we linearize the c∗

j (γj) at

γj which,

e

j (γj) ≈ c∗
c∗
j (

γj) +

∂c∗

j (γj)
∂γ⊤
j

e

11

(γj −

γj),

e

(3.1)

eγj
(cid:12)
(cid:12)
(cid:12)
(cid:12)

e

where the derivative ∂c∗

j /∂γj is explicitly derived using the implicit function theorem in A.

Hence,

θj(t; γj) in its basis expansion form can be approximated by a linear function of γj.

As a result, the outer objective function (2.4) now becomes a penalized likelihood function

b

of a generalized linear model. Second, we apply the IRLS and update our estimate of γj. Let

θj(t) =

θj(t;

γj) be latent process ﬁt given the structural parameter

γj. Based on the theory

of generalized linear models, the observation Yj according to the latent process
e
b
properties of E(Yj|

µj(t) and var(Yj|

θj(t)) = b

θj(t)) = b

θj(t)) =

e

(

′′

′

θj(t) admits

vj(t)a(φ),

e
θj(t))a(φ) =
(
e

where functions a, b and parameter φ follow from the exponential family speciﬁcation. Write

uij = −yij + b

′

(

e

e
θj(ti)) = −yij +

e

µj(ti) and

′′

wij = b

(

e
θj(ti)) =

e

vj(ti). The IRLS algorithm

e

applies a quadratic approximation to the log-likelihood, that is, at θj =
e

e

e

e

e

e

θj,

−yij

θj(ti; γj) + b(

θj(ti; γj)) ≈

wij

yij −

θj(ti; γj)

2

e
+ Cij,

1
2

where

yij =

θj(ti) −

b
uij/

n
wij and Cij is independent of
e

e

b

o

θj(ti). In conjunction with the linear

b

approximation of θj(ti; γj), it amounts to solving a penalized linear least squares to update

e

e

e

e

e

the estimate for structural parameter γj. Eﬃcient algorithms are available for diﬀerent

sparsity penalty choices PEN(·).

3.3 Tuning Parameter Selection

There are two tuning parameters involved in our proﬁling procedure, which jointly aﬀect

the algorithmic performance. On the one hand, λθ in the inner optimization controls the

amount of regularization regarding the diﬀerential equations. We deﬁne the aggregated

ODE ﬁdelity criterion as

p

1

′

θ
j(t) − γj0 −
0 (

j=1 Z
X

γjkθk(t)

2

)

dt.

p

k=1
X

(3.2)

Small λθ makes optimizing Hj(γj) with respect to γj more robust to initial guesses, but

yields bad approximations to ODE solutions. Large λθ gives rise to a diﬃcult opti-

mization problem where Hj(γj) is usually not convex and can have many local optima

(Ramsay et al., 2007; Qi and Zhao, 2010; Carey and Ramsay, 2021). On the other hand,

λγ in the outer optimization induces a sparse network structure for latent processes with

better interpretation, and existing methods such as information criteria can be adopted for

12

tuning. Based on the above discussion, we propose to ﬁx λγ in the outer optimization ﬁrst,

iteratively select a proper λθ in the inner optimization, and then determine the best λγ

via the Bayesian information criterion. In detail, suppose we choose λγ from a sequence of

candidate values. Then, we initialize λθ with a small value and moderately increase it via

an iterative scheme. At each iteration,

θ and

Γ are repeatedly estimated for the current

λθ, which are then used as initial values in the ﬁtting procedure with the next larger λθ.

b

b

The iterative scheme stops when the estimated ODE parameters converge, and thus λθ is

decided. The change of the estimated ODE parameters should be small when there is only

a moderate increase in λθ. Therefore, with a conservatively increasing sequence of λθ, every

estimated

Γ is much likely to be a proper initialization for the next iteration. Details of

the iterative selecting scheme for λθ given a ﬁxed λγ are as follows.

b

(1) Start with a small positive λ(0)

θ . Choose ∆(0) as an initial incremental factor.

(2) At the uth iteration where u ≥ 0, obtain the ﬁtted latent processes

θ(u) and

Γ(u) via

our proﬁling procedure, and evaluate the ODE ﬁdelity (3.2) based on the estimates.

b

b

(a) If the absolute percentage of change in the ODE ﬁdelity (3.2) is below a threshold

constant, then we update λ(u+1)

θ

= λ(u)

θ × ∆(u).

(b) Otherwise, we need to downsize the incremental factor, for example, set ∆(u) =
∆(u−1)/2, which ensures that the ODE ﬁdelity (3.2) varies little among iterations.

(3) When the successive ODE parameter estimates are closed enough, we stop iteration;

otherwise, repeat previous steps.

Our iterative tuning strategy treats λθ as a function of λγ. Hence, after λθ is selected

for each ﬁxed λγ from a sequence of candidate values, we can evaluate the following BIC

and choose the best λγ,

BIC(λγ) = −

1
np

n

p

yij

θj(ti; λγ) − b(θj(ti; λγ))

+ k(λ) log(n),

i=1
X

j=1 n
X

b

o

where

θj(t; λγ) emphasizes the dependence on λγ, and k(λγ) denotes the number of non-zero

elements in the resultant ODE parameter

b

Γ(λγ).

b

13

3.4 Global Convergence

Suppose that the estimated latent process

θj(t; γj) from the inner optimization is a smooth

function of γj, where j = 1, . . . , p. Let H(Γ) =

p
j=1 Hj(γj) be the objective function in the
outer optimization for a given tuning parameter λγ, where Γ = (γ1, . . . , γp). Algorithm 1

b

P

is essentially a block coordinate descent method because it minimizes H(Γ) by iteratively

updating γj. Write Hj(γj) = ℓj(γj) + PENλγ (γj), where ℓj(γj) is the likelihood term and

PENλγ (γj) is assumed to be convex. As described in Section 3.2, the outer optimization is

equivalent to updating γj to γj + dj(γj), where the descent direction dj(γj) is the solution

to

∇ℓj(γj)⊤d +

min
d

1
2

d⊤Qj(γj)d + PENλγ (γj + d),

where ∇ℓj(γj) is the gradient of ℓj(γj) and

Qj(γj) =

1
n

n




i=1
X

b′′(

θj(ti; γj))

∂

θj(ti; γj)
∂γj

b

∂

θj(ti; γj)

∂γj !

b

b

is a positive deﬁnite matrix approximating the Hessian ∇2ℓj(γj).


⊤






We follow Tseng and Yun (2009) to establish the global convergence. Because the

actual value of the Hessian ∇2ℓj(γj) is identical to its expected value under canonical links

(McCullagh and Nelder, 1989), the IRLS method described in Section 3.2 remains the same

when the Hessian is replaced by the expected Hessian. Then it follows from Lemma S1 in

the Supplementary Material that

Hj(γj + dj(γj)) − Hj(γj) ≤ −d⊤

j (γj)

Qj(γj) −
(cid:20)

1
2

E{∇2ℓj(γj)}

dj(γj) + o(kdj(γj)k2).

(cid:21)

(3.3)

Some algebra yields that

Qj(γj) −

1
2

E{∇2ℓj(γj)} =

1
2

Qj(γj) +

1
2n

n

b′(θ∗

j (ti)) − b′(

θj(ti, γj))

i=1 n
X

b

∂2
θj(ti, γj)
∂γj∂γ⊤
j
b

,

o

where θ∗

j (t) is the true latent process. The above matrix is positive deﬁnite because b′(·)
It follows from

θj(ti, γj) is suﬃciently close to the truth.

is continuous, provided that

(3.3) that Hj(γj) decreases along the iterations and will eventually converge because it is

b

lower-bounded. Moreover, the sequence of descent directions converges to zero due to (3.3).

14

 
According to Theorem 1(e) and Lemma 2 of Tseng and Yun (2009), every cluster point of

the iterative estimates by Algorithm 1 exhibits exact zero descent direction, which implies

it is indeed a stationary point of H(Γ).

Finally, we remark that the above analysis cannot be directly applied to a non-convex

PENλγ (·) such as the SCAD penalty. However, the non-convex penalty can be numerically

approximated by local linear or quadratic functions (Fan et al., 2020). We would anticipate

a similar convergence result but with more involved technical details, which is not pursued

in this paper.

4 Two-step Collocation Methods for non-Gaussian Data

Collocation methods have been exploited for both parameter estimation and network re-

construction for various ODE models. In this section, we extend the popular two-step col-

location method for high-dimensional linear ODE with non-Gaussian observations. In the

large literature on collocation, Varah (1982); Ramsay et al. (2007); Dattner and Klaassen

(2015), and Wu et al. (2019) consider the linear case while recently the nonparametric

additive structure is investigated by Henderson and Michailidis (2014); Wu et al. (2014)

and Chen et al. (2017). Most existing methods are proposed for Gaussian observations

and adopt the least square loss function for estimation. In the following, we present two

versions of the two-step collocation method for high-dimensional ODE models with non-

Gaussian observations: the vanilla collocation based on Varah (1982) and an extension from

graph reconstruction via additive diﬀerential equations (GRADE) by Chen et al. (2017).

The vanilla two-step method ﬁrst ﬁts smoothing estimates

θ(t) to the latent processes

with maximum likelihood estimation, and then obtain the structural parameter γ with the

b

estimated processes and their derivatives plugged in. The procedure solves the following

optimization problems,

1

with

b

γj = arg min

γj0,γj Z

0 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
θj(t) = arg min

θ∈H

b

−

1
n

n

i=1
X

d

θj(t)
dt
b

p

− γj0 −

γjk

θk(t)

k=1
X

b

dt + PENλγ (γj),

(4.1)

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

{yijθ(ti) − b(θ(ti))},

1 ≤ j ≤ p,

(4.2)

15

where H is a proper reproducing kernel Hilbert space, and the exponential family smoothing

splines can be adopted (Wahba et al., 1995; Gu, 2013; Ma et al., 2017). The performance

of the vanilla two-step collocation method relies on the estimation accuracy of

θj(t) and its

derivatives. Although statistical convergence has been established, it is in practice hard to

b

tune the smoothing procedure to achieve the optimality (Liang and Wu, 2008; Brunel et al.,

2014).

Another extension is based on the GRADE method (Chen et al., 2017). It avoids the

derivative estimation issue in the vanilla collocation method, and instead considers the

ODE ﬁdelity term in its integral form. Similar to the vanilla two-step method, the GRADE

method ﬁrst obtains the smoothing estimates of latent processes from observations as in

(4.2). Using integrated basis functions

Θj(t) =

t
0

θj(t) dt, j = 1, . . . , p, one can express

R
θj(t) = Cj0 + γj0 t +

b

p
b

γjk

Θk(t),

according to the integrated diﬀerential equations. Finally, we solve the following optimiza-

e

k=1
X

b

tion problems to obtain

γj = arg min
Cj0,γj0,γj

1
n

n

yij

θj(ti) − b(

θj(ti))

+ PENλγ (γj).

(4.3)

The GRADE method is initially developed for nonparametric additive ODE models and

b

i=1 n
X

e

e

o

naturally adapts to the linear case. The use of an integrated form of ODE facilitates in-

vestigating the asymptotic behavior of the estimator and enhancing its robustness to the

smoothing eﬀect in the ﬁrst step (Dattner and Klaassen, 2015; Chen et al., 2017). Both

the two-step collocation methods proposed in this section involve maximizing the likeli-

hood function for exponential family distributions, which can be eﬃciently solved with the

iteratively reweighted least squares technique as in Section 3.2.

We compare the two-step collocation methods with the high-dimensional generalized

proﬁling (HDGP) procedure in Section 5. For process and derivative estimation, since

HDGP balances both the data and ODE ﬁdelities, it usually results in reasonable ﬁts and

more accurate ODE parameter estimates due to the more accurate derivatives. For sparse

structure identiﬁcation, GRADE achieves the best accuracy, which is consistent with the

motivation of GRADE for network reconstruction (Chen et al., 2017). In summary, HDGP

16

is a better choice for process ﬁtting and ODE parameter estimation, while GRADE excels

in sparse structure identiﬁcation.

5 Simulation Studies

This section compares the empirical performance of three dynamical modeling approaches:

the high-dimensional generalized proﬁling (HDGP) procedure and the two-step collocation

methods proposed in Section 4, namely the GRADE and the vanilla two-step method,

respectively.

Consider the ODE system studied by Chen et al. (2017) which consists of eight processes

in four pairs, for k = 1, . . . , 4,

θ′
2k−1(t) = 2kπ θ2k(t)

θ′
2k(t) = 2kπ θ2k−1(t)




, t ∈ [0, 1].

It is clear that the ODE solutions take the form of sine and cosine functions with vary-



ing frequencies, whereas no interaction exists across pairs. For the kth pair, the initial

state is sin(yk) and cos(yk), where yk is sampled from N(0, 1). The latent processes
θ(t) = (θ1(t), . . . , θ8(t))⊤ described by the above ordinary diﬀerential equations are used

to generate observations from Gaussian, Poisson and Bernoulli distributions. Denote by

t1, . . . , tn time points from [0, 1]. For Gaussian distribution, yij is sampled from N(θj(ti), σ2)

with known variance σ2, and the sample size n for each process is set to be 100 and 500.

For Poisson distribution, we draw 500 and 1000 samples from Poisson(λj(ti)) where the

intensity process λj(t) = exp{θj(t)}. For Bernoulli distribution, 1500 and 2500 samples

are generated with probability of success pj(t) = exp{θj(t)}/[1 + exp{θj(t)}]. Sample sizes

for Poisson and Bernoulli distributions are larger than Gaussian, as in those cases more

observations are generally required to ensure reasonable estimates according to the theory

of generalized linear model.

We use the smoothing spline ﬁtting as an initialization for the proﬁling procedure, which

also corresponds to the ﬁrst stage of two-step collocation methods. The order of B-spline

functions in HDGP is set as 6, and the number of knots is half of that of time points. Both

HDGP and GRADE require numerical integration to evaluate ODE ﬁdelity and integrated

17

basis representations, respectively. For sparsity penalty choices, we consider the Lasso

penalty PENλγ (γj) = λγkγjk1 and the SCAD penalty PENλγ (γj) =

p
k=1 pλγ (|γjk|), where

the function pλ(·) is deﬁned on [0, ∞) as

λu,

P

if 0 ≤ u ≤ λ

−(u2 − 2aλu + λ2)/(a − 1),

if λ < u < aλ

(a + 1)λ2/2

if u ≥ aλ,

pλ(u) = 



and a suggested value for a is 3.7 according to Fan and Li (2001). Algorithmic convergence

is demonstrated when the diﬀerence between successive ODE parameter estimates is small

enough.

It works well for two-step collocation methods. However, due to the complex

interaction between inner and outer optimizations, HDGP may not yield sparse ODE pa-

rameter estimates at the declaration of convergence. To address this numerical issue, we

manually set ODE parameter estimates below a constant threshold as zero. Based on our

empirical studies, a recommended value for the threshold is the root-mean-square of the

initial estimate

Γ multiplied by a factor 0.01.

Simulation results are evaluated using three types of criteria. The ﬁrst two criteria

b

concern about process and derivative estimates, which are evaluated by the mean squared
errors (MSE) of θ(t) and θ′(t),

MSE(

θ(t)) =

b
θ′(t)) =

MSE(

b

1
np

1
np

p

n

j=1
X
p

i=1 n
X
n

j=1
X

i=1 n
X

b

θj(ti) − θj(ti)

b
j(ti) − θ′
θ′

j(ti)

,

.

2

o
2

o

Second, we measure how well the structural parameters are estimated by their root-mean-

square error (RMSE). Third, true positive rate (TPR) and false positive rate (FPR) are

used to quantify how well the sparse structure is identiﬁed, where we refer to non-zero

structural parameters as positive cases and otherwise as negative cases.

Table 1 displays the averaged evaluations over 50 repeated experiments using the Lasso

penalty, while the true positive rates are omitted because they are all equal to one for all

three methods. Under each simulation set-up, increasing the number of observations always

leads to reduced errors and tighter conﬁdence intervals in terms of the process ﬁt and the

parameter estimation. For process and derivative ﬁtting, the smoothing splines method,

18

Table 1: Performance of HDGP, GRADE and the vanilla two-step method evaluated based
θ′(t))), non-zero pa-

θ(t))), derivative estimates (MSE(

on the process estimates (MSE(

rameter estimation (RMSE), and sparse structure estimates (FPR). The 95% conﬁdence

b

b

intervals are given in parentheses.

N

Method

MSE (

bθ(t))

MSE (

bθ′(t))

n
a
i
s
s
u
a
G

n
o
s
s
i
o
P

i
l
l
u
o
n
r
e
B

100

500

500

1000

1500

2500

HDGP

GRADE

vanilla

HDGP

GRADE

vanilla

HDGP

GRADE

vanilla

HDGP

GRADE

vanilla

HDGP

GRADE

vanilla

HDGP

GRADE

vanilla

0.011
(0.0097,0.0124)

0.005
(0.0042,0.0049)

0.005
(0.0043,0.0049)

0.002
(0.0017,0.0023)

0.001
(0.0010,0.0011)

0.001
(0.0010,0.0011)

0.024
(0.0222,0.0259)

0.024
(0.0232,0.0252)

0.024
(0.0232,0.0252)

0.011
(0.0105,0.0121)

0.013
(0.0128,0.0141)

0.013
(0.0127,0.0141)

0.031
(0.0260,0.0357)

0.031
(0.0277,0.0333)

0.032
(0.0285,0.0376)

0.019
(0.0169,0.0216)

0.020
(0.0193,0.0210)

0.020
(0.0193,0.0211)

3.01
(2.48, 3.53)

5.23
(4.67, 5.87)

5.23
(4.74, 5.93)

0.48
(0.41, 0.57)

1.82
(1.75, 1.88)

1.82
(1.75, 1.88)

6.24
(5.60, 6.93)

12.27
(11.66,13.06)

12.27
(11.63,13.00)

2.70
(2.43, 2.97)

8.20
(7.70, 8.73)

8.20
(7.71, 8.67)

8.18
(6.74, 9.73)

15.97
(14.77,16.99)

22.28
(17.21,32.11)

5.05
(4.33, 6.03)

12.71
(11.91,13.67)

12.71
(11.88,13.69)

19

RMSE (

b
Γ)

0.58
(0.52,0.66)

2.97
(2.89,3.05)

0.62
(0.54,0.72)

0.28
(0.25,0.30)

0.84
(0.82,0.85)

0.34
(0.32,0.37)

1.70
(1.54,1.91)

2.03
(1.86,2.19)

1.86
(1.70,2.07)

1.04
(0.94,1.14)

1.41
(1.31,1.53)

1.18
(1.09,1.29)

1.77
(1.48,1.97)

3.32
(3.01,3.61)

2.28
(1.70,3.22)

1.55
(1.39,1.74)

2.57
(2.34,2.79)

1.67
(1.46,1.87)

FPR

0.44
(0.43,0.46)

0.00
(-,-)

0.89
(0.84,0.92)

0.44
(0.42,0.45)

0.01
(0.01,0.02)

0.67
(0.64,0.71)

0.58
(0.56,0.61)

0.37
(0.34,0.41)

0.98
(0.97,0.98)

0.57
(0.55,0.59)

0.32
(0.28,0.36)

0.97
(0.96,0.97)

0.54
(0.52,0.58)

0.24
(0.21,0.28)

0.94
(0.90,0.96)

0.59
(0.55,0.62)

0.20
(0.16,0.24)

0.98
(0.97,0.99)

as the ﬁrst stage of two-step collocation methods, often produces accurate estimates of

the latent process itself, but is less eﬃcient in the derivative ﬁtting.

In contrast, the

inner optimization of HDGP balances the data and ODE ﬁdelities, resulting in reasonable

process ﬁtting and improved derivative ﬁtting. For ODE parameter estimation, HDGP

delivers the smallest error due to the more accurate derivatives. Interestingly, GRADE has

much worse performance than the other two under this criterion. One partial reason is that

GRADE only uses structural parameters in the integrated basis representation (4.3) instead

of the explicit form of diﬀerential equations. For sparse structure identiﬁcation, GRADE

achieves the best accuracy, as it discovers all non-zero structural parameters with the fewest

false positives. It is consistent with the motivation of GRADE for network reconstruction

(Chen et al., 2017). In summary, HDGP is a better choice for process ﬁtting and ODE

parameter estimation, while GRADE excels in sparse structure identiﬁcation.

We next investigate the eﬀects of diﬀerent noise levels and choices of sparse penalty.

Under the above Gaussian set-up with 500 observations for each process. The signal-

to-noise ratio (SNR) is deﬁned as the ratio between the sample standard deviation of

{θj(ti)}n

i=1 and the noise standard deviation σ. We set the signal-to-noise ratio as 3, 10, 30,

and inﬁnity, where the inﬁnite ratio means that no noise is added. Both Lasso and SCAD

penalties are considered. Figure 1 presents the performance evaluations over 50 repeated

experiments. In general, all methods perform better over all criteria when the signal-to-

noise ratio increases. The top row of Figure 1 corresponding to the Lasso penalty provides

the consistent result as in Table 1, which indicates that HDGP has a comparable process ﬁt

and better derivative estimation, especially when the noise level is low. Moreover, HDGP

performs the best for estimating structural parameters, while the vanilla two-step method

also provides satisfactory results. In contrast, even when there is no noise, the bias of ODE

parameter estimates by GRADE is still large and RMSE is almost constant. For sparse

structure identiﬁcation, GRADE outperforms the other methods under a wide range of

noise levels. HDGP and the vanilla two-step method only have high accuracy when the

signal level is high. The bottom row of Figure 1 displays simulation results when the SCAD

penalty is used for inducing sparsity for the ODE system. Compared with the results with

Lasso, overall performances in process, derivative, and ODE parameter estimations are

20

Processes

Derivatives

Non-zero Parameters

FPR

0.012

0.009

0.006

E
S
M

0.003

0.000

0.0015

0.0010

E
S
M

0.0005

0.0000

3

10

30

Inf

SNR

Processes

E
S
M

E
S
M

4

3

2

1

0

3

2

1

0

E
S
M
R

2.0

1.5

1.0

0.5

0.0

0.75

0.50

0.25

0.00

3

10

30

Inf

3

10

30

Inf

3

10

30

Inf

SNR

SNR

SNR

(a) With Lasso penalty.

Derivatives

Non-zero Parameters

FPR

E
S
M
R

0.5

0.4

0.3

0.2

0.1

0.0

0.8

0.6

0.4

0.2

0.0

3

10

30

Inf

3

10

30

Inf

3

10

30

Inf

3

10

30

Inf

SNR

SNR

SNR

SNR

(b) With SCAD penalty.

Figure 1: Performance of HDGP (purple solid), GRADE (blue dashed), and the vanilla

two-step method (yellow dotted) for Gaussian observations at diﬀerent noise levels. The

boxes identify the medians and the quartiles of each criterion for 50 repeated experiments.

Top and bottom rows correspond to Lasso and SCAD penalties, respectively.

21

improved mainly due to the unbiasedness property of SCAD penalty (Fan and Li, 2001).

More interestingly, the poor performance of GRADE in ODE parameter estimation is

greatly enhanced, and now it delivers comparable estimation results as the other two. Due

to the oracle property enjoyed by the SCAD penalty (Fan and Li, 2001), we recommend it

for better performance in parameter estimation.

6 Real Data Analysis

6.1 Google Trends Data Analysis

Google Trends provides a publicly accessible online portal to analyze the popularity of

search queries. In this study, we attempt to apply our method to model the interactions

among a number of trending keywords during the recent pandemic of Coronavirus disease

2019 (COVID-19). In Table 2, we list 24 keywords and cluster them into three categories.

The ﬁrst category consists of ﬁve keywords about speciﬁc terminologies such as mask and

quarantine. The second category includes not only the countries with the most conﬁrmed

cases as of January 2021, such as the United States, India, and Brazil but also the districts

like Antarctica, which is the last continent to report conﬁrmed cases due to the remoteness

and sparse population. We also include the last category of noise keywords with no apparent

relationship to the pandemic.

Table 2: Three categories of keywords selected for the analysis of Google Trends data.

Category

Keyword

COVID-19 related

coronavirus, mask, quarantine, vaccine, WHO (5 words)

Countries or districts Africa, Antarctica, Arctic, Australia, Brazil, Canada, China, India, Iran,

Italy, Japan, Russia, the United States (13 words)

Noise words

cat, cloud, desert, dog, game, sun (6 words)

The Google Trends data used in our study cover the range from January 20 to Septem-

ber 20 in 2020. The keyword popularity is measured by an integer index calculated by

22

normalizing and rounding the keyword count in an unbiased searching requests sample.

We observe that the daily trend indices have several sharp peaks, see Figure 2 for an illus-

trative example. Direct modeling for the mean trends will result in abrupt high values near

the peaks and undersmooth other relatively ﬂat regions. Therefore, it is more appropriate

to assume the indices follow Poisson distributions, and we apply the proposed method to

model the latent processes of intensity parameters with ODEs.

Figure 2: Daily Google Trends indices of keyword Russia and vaccine from January to

September 2020.

To better exhibit diﬀerent stages of the pandemic, we consider three time periods: from

January 20 to March 19, from March 20 to June 19, and from June 20 to September 20.

For each period, our method is applied to ﬁt the trending processes with a series of sparsity

parameter λγ’s. Figures 3a and 3b display two networks with diﬀerent sparsity parameters

in the ﬁrst period (from January to March). Keyword quarantine has the highest degree

in both networks. During the COVID-19 pandemic, quarantines or self-quarantines are

enacted by multiple governmental actors to prevent the rapid spread of the virus.

It is

of no surprise to become the top-ranked trending keyword. The other three keywords

in Figure 3a are coronavirus, China and vaccine, which stand for the virus’s name, the

country where the ﬁrst case was identiﬁed, and the immunization method. The top four

keywords represent the major trending focus at the early stage of the pandemic. In Figure

23

Desert

Russia

cat

dog

WHO

India

the United States

Iran

mask

Arctic

cloud

Italy

cat

game

Brazil

Japan

sun

Brazil

cloud

India

Africa

Antarctica

game

vaccine

Coronavirus

Canada

Arctic

Antarctica

Italy

Australia

Africa

quarantine

Desert

Coronavirus

China

quarantine

WHO

dog

Australia

sun

China

the United States

Russia

vaccine

Japan

Canada

(a) λγ = 10−0.5

Iran

mask

(b) λγ = 10−1

Figure 3: Recovered networks of the trending keywords during the ﬁrst period (from Jan-

uary 20 to March 19) with diﬀerent values of sparsity parameters.

3b, more aﬀected countries such as Australia, Italy, and the United States, are involved

when the sparsity parameter is decreased. In contrast, noise keywords are isolated in both

networks, indicating no connection to the trending topics. More interestingly, we investigate

Table 3: Top four keywords in the recovered networks during three periods. The keyword

of the highest degree is in boldface.

Period

Keywords

January 20 – March 19

quarantine, China, coronavirus, vaccine

March 20 – June 19

Italy, China, Iran, Russia

June 20 – September 20

coronavirus, the United States, vaccine, mask

the evolution of network structure for the trending keywords along the progression of the

COVID-19 pandemic. Table 3 lists the top four keywords in three time periods where the

keyword with the highest degree is in boldface. From the ﬁrst period to the second, the

keyword Italy emerges as the new top word. According to the WHO report, on March

19, Italy overtook China as the country with the most reported deaths, and announced

24

Table 4: Companies selected in eight categories for stock price data analysis.

Group Category

Companies

1

2

3

4

5

6

7

8

Information Technology

Adobe, Apple, Microsoft, Salesforce, Zoom

Electric Vehicle

Pharmaceutical

BYD, Kandi, Nio, Tesla, Workhorse

AbbVie, Eli lilly, Moderna, Novartis, Pﬁzer

Consumer Services & Retail Ascena, J. C. Penney, Kohl’s, Macy’s, Nordstrom

Online Retail Shopping

Amazon, Best Buy, Target, Walmart, Wayfair

Hotels

Hilton, Marriott, Wyndham, Wynn, Park

Air Transportation

Boeing, Airbus, Delta Air Lines, Southwest Airlines,

United Airline

Energy

Chevron, Conocophillips, Exxon Mobil, Schlumberger,

Valero Energy

the national lockdown in March. Turning to the third period, China and Italy drop out

of the top list. Both countries had successfully slowed down the domestic infections and

reduced daily new cases signiﬁcantly. As preventive measures including wearing face masks

in public are advised and several promising vaccines are being developed, mask and vaccine

are among the top trending keywords.

6.2 Analysis of Stock Price Change Directions

In the year 2020, the stock market experienced enormous volatility due to the coronavirus

pandemic. Many companies have suﬀered massive price drops, while others have witnessed

substantial increases. We collect the stock price indices for 40 companies during 251 trad-

ing days spanning from January 1 to December 30, 2020. Our goal in this study is to

characterize the change direction patterns of stock prices, taking into account the dynamic

interactions among the stocks. To this end, the original price indices are coded as binary

data to denote an increase or decrease. We group the companies into eight categories based

on the Global Industry Classiﬁcation Standard. Details are provided in Table 4.

The high-dimensional ODE system is built up for the latent success probability pro-

25

(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)

(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)

0.550

0(cid:0)(cid:1)(cid:2)(cid:3)

0.500

0.54

(cid:129)(cid:130)(cid:131)(cid:132)

0.50

Information Technology

Electric Vehicle

Consumer Services & Retail

Pharmaceutical

+,-./

0.550

&’()*

0.500

!""#$%

0.55

0.50

0.45

0.40

0.550

fghij

0.500

abcde

0.450

\]^_‘

(cid:14)(cid:15)(cid:16)(cid:17)(cid:18) (cid:19)(cid:20)(cid:21)(cid:22)(cid:23) (cid:24)(cid:25)(cid:26)(cid:27)(cid:28) 1(cid:29)(cid:30)(cid:31) 

23456 789:; <=>?@ ABCDE

GHIJK

LMNOP

QRSTU

VWXYZ

klmno qrstu vwxyz {|}~(cid:127)

Online Retail Shopping

Hotels

Air Transportation

Energy

0.51

0.48

0.45

0.50

0.48

‡·(cid:181)¶

0.44

ﬂ(cid:176)–†

0.50

0.45

0.40

0.35

(cid:133)(cid:134)(cid:135)(cid:136)(cid:137)

(cid:138)(cid:139)(cid:140)(cid:141)(cid:142)

(cid:143)(cid:144)(cid:145)(cid:146)(cid:147)

(cid:148)(cid:149)(cid:150)(cid:151)(cid:152)

(cid:154)(cid:155)(cid:156)(cid:157)(cid:158)

(cid:159)(cid:160)¡¢£

⁄¥ƒ§¤

'“«‹›

•‚„”»

…‰(cid:190)¿(cid:192)

`´ˆ˜¯

˘˙¨(cid:201)˚

(cid:204)˝˛ˇ—

(cid:209)(cid:210)(cid:211)(cid:212)(cid:213)

(cid:214)(cid:215)(cid:216)(cid:217)(cid:218)

(cid:219)(cid:220)(cid:221)(cid:222)(cid:223)

Figure 4: The ﬁtted probability processes of daily price increase for the eight categories.

The red dashed line denotes p = 0.5.

cesses. Our sparsity tuning procedure leads to λγ = 10−2.1 and the ﬁtted model achieves
an ODE ﬁdelity below 10−6. Figure 4 displays the ﬁtted probabilities of a daily stock

price increase for all categories. We notice some interesting results from the result. First,

all categories have the low ﬁtted probabilities around March. It corresponds to the 2020

stock market crash, during which multiple circuit breakers were triggered on fears of the

COVID-19 coronavirus. Since the crash, some sectors recovered and re-entered a bull

market through December. Online retail companies made huge proﬁts as health concerns

changed customers’ shopping habits. Information technology companies beneﬁted from the

growing demands for information services and electronics devices. For example, the shifts

towards remote working had raised the number of Zoom’s daily users to an unprecedented

one. In contrast, sectors like energy, hotels, and air transportation experienced the most

severe hit by the COVID-19 pandemic. Although there were signs of recovery in the fourth

quarter, these industries are still under the tremendous impact of the COVID-19 recession.

6.3 Analysis of Yeast Cell Cycle-regulated Genes

The cell cycle is a fundamental biological process consisting of cell growth, duplication

of genetic information, distribution of chromosomes, and cell division (Cho et al., 1998).

Spellman et al. (1998) analyzed the expression levels of 6,178 yeast genes at 7-minute in-

tervals for 119 minutes. The experiments were carried out in the cell cultures with three

independent synchronization methods. A score was calculated for each gene to indicate

26

p
F
[
(cid:128)
(cid:153)
ﬁ
¸
(cid:224)
14

56

72

71

48

52

41

38

6

60

45

59

24

3

17

2

31

32

33

63

51

21

9

28

42

47

30

13

54

70

39

10

16

1

12

8

61

62

66

5

20

65

68

4

27

7

26

37

35

69

57

40

58

11

55

46

53

49

36

15

Figure 5: The recovered network of the yeast cell cycle. Yellow nodes represent genes, and

the green-solid or red-dashed edges indicate potential promotion or suppression eﬀects.

their similarities to those cell-cycle regulated genes already known. Due to missingness

in data, we choose 72 out of 93 genes identiﬁed by Spellman et al. (1998) in the alpha

factor-based synchronized experiment, and model the dynamic relationship between the

mean proﬁles of these 72 genes using an ODE system under Gaussian assumption for gene

expression level. The proposed method is applied to identify the sparse structure of the

gene regulatory network. The result is shown in Figure 5, which excludes 12 isolated genes.

This suggests that although those genes get involved in the cell cycle, their regulated tran-

scriptions are not absolutely required. Among the 60 genes in Figure 5, 116 regulations

(i.e., directed edges) are discovered. The average number of regulations for each gene is

around three, while more than 80% genes have regulations fewer than ﬁve. Genes with

high network degrees are identiﬁed as central hub nodes. For example, CLN3 (node 1) has

the largest number of regulations in Figure 5. According to the Saccharomyces Genome

Database (Cherry et al., 1998), the encoded protein CLN3p is known as a cell cycle regu-

27

lator and promotes the G1/S transition (Nasmyth, 1993). More interestingly, the positive

or negative signs of our estimated structural parameters naturally imply the potential pro-

motion or inhibition between genes, respectively. Our result suggests that CHS1 (node 62)

promotes the expression of POL30 (node 12), which regulates DNA replication in the G1

phase. Meanwhile, it suppresses the expression of FAR1 (node 30), which is a CDC28p

kinase inhibitor functioning in the G2/M transition.

7 Conclusions and Discussion

In this article, we have proposed a new proﬁling procedure for both parameter estima-

tion and sparse structure identiﬁcation for high-dimensional linear ODE models with non-

Gaussian observations. Our method involves a hierarchical optimization scheme: the inner

optimization balances the data ﬁtting and ODE ﬁdelity to improve estimation eﬃciency,

while the outer optimization induces a sparse structure for better model interpretation.

Besides, we extend two-step collocation methods to the non-Gaussian observation setting

and compare them with the proposed proﬁling procedure via comprehensive studies.

One limitation of our work is that only the linear ODE system is under consideration.

We are aware of the recent development of two-step collocation to sparse additive ODE

systems (Henderson and Michailidis, 2014; Wu et al., 2014; Chen et al., 2017) and a more

general functional ANOVA extension (Dai and Li, 2021). Although our hierarchical opti-

mization is not restricted to the linear ODE, the extension to nonlinear ODE systems is

not straightforward. For instance, a common strategy to handle additive ODE models is

to expand the nonlinear components with basis function. However, due to the proﬁling

nature, the range of collocation bases for latent processes needs to be controlled within a

compact interval, which may not be easily overcome. Another future research is on the

statistical properties such as uniform bound on the approximations to the true solutions,

asymptotic normality of the estimators. Despite existing theory established for the stan-

dard generalized proﬁling (Qi and Zhao, 2010), it is still a challenging problem due to high

dimensionality, and we leave the systematic study to future work.

28

A Derivatives

We provide the analytical expressions of the derivatives used in the computation (Section

3).

Derivatives of Gj in inner optimization

Write Gj(θj; γj) in the inner optimization

Gj = −

1
n

n

i=1
X

{yijθj(ti) − b(θj(ti))} + λθ

θ′
j(t) − γj0 −

ZT (

γjk θk(t)

2

)

dt,

p

k=1
X

then the ﬁrst derivative is

∂Gj
∂cj

= −

1
n

n

{yijh(ti) − b′(θj(ti))h(ti)}

i=1
X
+ 2λθ

dθj(t)
dt

ZT (cid:26)

p

− γj0 −

γjk θk(t)

k=1
X

dh(t)
dt

(cid:27)(cid:26)

− γjj h(t)

dt,

(cid:27)

and the second derivative is

∂2Gj
∂cj∂c⊤
j

=

1
n

n

i=1
X

{b′′(θj(ti))h(ti)h(ti)⊤}

+ 2λθ

For k = 0,

dh(t)
dt

ZT (cid:26)

− γjjh(t)

dh(t)
dt

(cid:27)(cid:26)

⊤

− γjjh(t)

dt.

(cid:27)

∂2Gj
∂cj∂γj0

= −2λθ

dh(t)
dt

ZT (cid:26)

− γjjh(t)

dt,

(cid:27)

for k = 1, . . . , p and k 6= j ,

∂2Gj
∂cj∂γjk

= −2λθ

dh(t)
dt

ZT (cid:26)

− γjjh(t)

θk(t) dt,

(cid:27)

for k = j,

∂2Gj
∂cj∂γjj

= −2λθ

ZT (cid:26)

dh(t)
dt

− γjjh(t)

θj(t) dt

− 2λθ

ZT (cid:26)

dθj(t)
dt

(cid:27)

p

− γj0 −

γjkθk(t)

h(t) dt.

k=1
X

(cid:27)

29

Derivative of c∗

j in outer optimization

Write c∗

j (γj) as c∗

j for simplicity. Since Gj has zero-gradient at c∗

j , then

Taking the derivative with respect to γj on both sides, we have

= 0.

∂Gj
∂cj (cid:12)
c∗
j
(cid:12)
(cid:12)
(cid:12)

j (cid:18)
Suppose ∂2Gj/(∂cj∂c⊤

d
dγ⊤

=

∂2Gj
∂Gj
∂cj∂c⊤
∂cj (cid:12)
j !
c∗
c∗
j (cid:12)
j (cid:19)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
j )|c∗
j is non-singular, we have the following expression of the deriva-
(cid:12)
(cid:12)

∂2Gj
∂cj∂γ⊤
j (cid:12)
(cid:12)
(cid:12)
(cid:12)

j (γj)
∂γ⊤
j

= 0.

+

c∗
j

∂c∗

tive

∂c∗

∂2Gj
∂cj∂c⊤
j (cid:12)
(cid:12)
Both matrices on the right-hand side have been explicitly derived, the the derivative of c∗
(cid:12)
j
(cid:12)

∂2Gj
.
∂cj∂γ⊤
c∗
j (cid:12)
j (cid:19)
(cid:12)
(cid:12)
(cid:12)

j (γj)
∂γ⊤
j

c∗
j (cid:19)

= −

(cid:18)

(cid:18)

−1

follows.

SUPPLEMENTARY MATERIALS

Supplementary Material contains additional numerical results.

References

Brunel, N. J., Q. Clairon, and F. d’Alch´e Buc (2014). Parametric estimation of ordinary

diﬀerential equations with orthogonality conditions. Journal of the American Statistical

Association 109 (505), 173–185.

Cao, J. and J. O. Ramsay (2007). Parameter cascades and proﬁling in functional data

analysis. Computational Statistics 22 (3), 335–351.

Carey, M. and J. O. Ramsay (2021). Fast stable parameter estimation for linear dynamical

systems. Computational Statistics & Data Analysis 156, 107124.

Chen, S., A. Shojaie, and D. M. Witten (2017). Network reconstruction from high-

dimensional ordinary diﬀerential equations. Journal of the American Statistical Associ-

ation 112 (520), 1697–1707.

30

 
Cherry, J. M., C. Adler, C. Ball, S. A. Chervitz, S. S. Dwight, E. T. Hester, Y. Jia, G. Juvik,

T. Roe, and M. Schroeder (1998). Sgd: Saccharomyces genome database. Nucleic Acids

Research 26 (1), 73–79.

Cho, R. J., M. J. Campbell, E. A. Winzeler, L. Steinmetz, A. Conway, L. Wodicka, T. G.

Wolfsberg, A. E. Gabrielian, D. Landsman, and D. J. Lockhart (1998). A genome-wide

transcriptional analysis of the mitotic cell cycle. Molecular cell 2 (1), 65–73.

Cokus, S. J., S. Feng, X. Zhang, Z. Chen, B. Merriman, C. D. Haudenschild, S. Pradhan,

S. F. Nelson, M. Pellegrini, and S. E. Jacobsen (2008). Shotgun bisulphite sequencing of

the arabidopsis genome reveals dna methylation patterning. Nature 452 (7184), 215–219.

Dai, X. and L. Li (2021). Kernel ordinary diﬀerential equations. Journal of the American

Statistical Association.

Dattner, I. and C. A. Klaassen (2015). Optimal rate of direct estimators in systems of

ordinary diﬀerential equations linear in functions of the parameters. Electronic Journal

of Statistics 9 (2), 1939–1973.

Dodds, P. S., K. D. Harris, I. M. Kloumann, C. A. Bliss, and C. M. Danforth (2011). Tem-

poral patterns of happiness and information in a global social network: Hedonometrics

and twitter. PLoS ONE 6 (12), e26752.

Fan, J., Y. Feng, and Y. Wu (2009, 06). Network exploration via the adaptive lasso and

scad penalties. The Annals of Applied Statistics 3 (2), 521–541.

Fan, J. and R. Li (2001). Variable selection via nonconcave penalized likelihood and its

oracle properties. Journal of the American statistical Association 96 (456), 1348–1360.

Fan, J., R. Li, C.-H. Zhang, and H. Zou (2020). Statistical foundations of data science.

Chapman and Hall/CRC.

Gu, C. (2013). Smoothing Spline ANOVA Models (2nd ed.), Volume 297. New York:

Springer.

31

Hall, P. and Y. Ma (2014). Quick and easy one-step parameter estimation in diﬀeren-

tial equations. Journal of the Royal Statistical Society: Series B (Statistical Methodol-

ogy) 76 (4), 735–748.

Hecker, M., S. Lambeck, S. Toepfer, E. Van Someren, and R. Guthke (2009). Gene regula-

tory network inference: data integration in dynamic models—a review. Biosystems 96 (1),

86–103.

Henderson, J. and G. Michailidis (2014). Network reconstruction using nonparametric

additive ODE models. PLoS ONE 9 (4), e94003.

Huang, W., Y. Nakamori, and S.-Y. Wang (2005). Forecasting stock market movement

direction with support vector machine. Computers & operations research 32 (10), 2513–

2522.

Liang, H. and H. Wu (2008). Parameter estimation for diﬀerential equation models using a

framework of measurement error in regression models. Journal of the American Statistical

Association 103 (484), 1570–1583.

Lu, T., H. Liang, H. Li, and H. Wu (2011). High-dimensional odes coupled with mixed-

eﬀects modeling techniques for dynamic gene regulatory network identiﬁcation. Journal

of the American Statistical Association 106 (496), 1242–1258.

Ma, P., N. Zhang, J. Z. Huang, and W. Zhong (2017). Adaptive basis selection for exponen-

tial family smoothing splines with application in joint modeling of multiple sequencing

samples. Statistica Sinica 27 (4), 1757–1777.

McCullagh, P. and J. Nelder (1989). Generalized Linear Models, Second Edition. Chapman

& Hall/CRC Monographs on Statistics & Applied Probability. Taylor & Francis.

Miao, H., H. Wu, and H. Xue (2014). Generalized ordinary diﬀerential equation models.

Journal of the American Statistical Association 109 (508), 1672–1682.

Nagalakshmi, U., Z. Wang, K. Waern, C. Shou, D. Raha, M. Gerstein, and M. Snyder

(2008). The transcriptional landscape of the yeast genome deﬁned by RNA sequencing.

Science 320 (5881), 1344–1349.

32

Nasmyth, K. (1993). Control of the yeast cell cycle by the Cdc28 protein kinase. Current

Opinion in Cell Biology 5 (2), 166–179.

Polynikis, A., S. Hogan, and M. di Bernardo (2009). Comparing diﬀerent ODE modelling

approaches for gene regulatory networks. Journal of Theoretical Biology 261 (4), 511–530.

Powell, M. J. (2006). The NEWUOA software for unconstrained optimization without

derivatives. In Large-scale nonlinear optimization, pp. 255–297. Springer.

Poyton, A., M. S. Varziri, K. B. McAuley, P. J. McLellan, and J. O. Ramsay (2006).

Parameter estimation in continuous-time dynamic models using principal diﬀerential

analysis. Computers & Chemical Engineering 30 (4), 698–708.

Qi, X. and H. Zhao (2010). Asymptotic eﬃciency and ﬁnite-sample properties of the

generalized proﬁling estimation of parameters in ordinary diﬀerential equations. The

Annals of Statistics 38 (1), 435–481.

Ramsay, J. O. (1996). Principal diﬀerential analysis: Data reduction by diﬀerential opera-

tors. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 58 (3),

495–508.

Ramsay, J. O., G. Hooker, D. Campbell, and J. Cao (2007). Parameter estimation for

diﬀerential equations: a generalized smoothing approach. Journal of the Royal Statistical

Society: Series B (Statistical Methodology) 69 (5), 741–796.

Sloan, L. and J. Morgan (2015). Who tweets with their location? understanding the rela-

tionship between demographic characteristics and the use of geoservices and geotagging

on twitter. PLoS ONE 10 (11), e0142209.

Spellman, P. T., G. Sherlock, M. Q. Zhang, V. R. Iyer, K. Anders, M. B. Eisen, P. O. Brown,

D. Botstein, and B. Futcher (1998). Comprehensive identiﬁcation of cell cycle-regulated

genes of the yeast Saccharomyces cerevisiae by microarray hybridization. Molecular

Biology of the Cell 9 (12), 3273–3297.

Stuart, J. M., E. Segal, D. Koller, and S. K. Kim (2003). A gene-coexpression network for

global discovery of conserved genetic modules. Science 302 (5643), 249–255.

33

Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the

Royal Statistical Society: Series B (Statistical Methodology) 58 (1), 267–288.

Tseng, P. and S. Yun (2009). A coordinate gradient descent method for nonsmooth sepa-

rable minimization. Mathematical Programming 117 (1), 387–423.

Varah, J. M. (1982). A spline least squares method for numerical parameter estimation

in diﬀerential equations. SIAM Journal on Scientiﬁc and Statistical Computing 3 (1),

28–46.

Voorman, A., A. Shojaie, and D. Witten (2014). Graph estimation with joint additive

models. Biometrika 101 (1), 85–101.

Wahba, G., Y. Wang, C. Gu, R. Klein, and B. Klein (1995, 12). Smoothing spline ANOVA

for exponential families, with application to the Wisconsin epidemiological study of dia-

betic retinopathy : the 1994 Neyman memorial lecture. The Annals of Statistics 23 (6),

1865–1895.

Wang, H. and C. Leng (2008). A note on adaptive group lasso. Computational Statistics

& Data Analysis 52 (12), 5277–5286.

Wood, S. N. (2017). Generalized additive models: an introduction with R. CRC press.

Wu, H., T. Lu, H. Xue, and H. Liang (2014). Sparse additive ordinary diﬀerential equations

for dynamic gene regulatory network modeling. Journal of the American Statistical

Association 109 (506), 700–716.

Wu, L., X. Qiu, Y.-x. Yuan, and H. Wu (2019). Parameter estimation and variable selec-

tion for big systems of linear ordinary diﬀerential equations: a matrix-based approach.

Journal of the American Statistical Association 114 (526), 657–667.

Yuan, M. and C. Kendziorski (2006). Hidden Markov models for microarray time course

data in multiple biological conditions. Journal of the American Statistical Associa-

tion 101 (476), 1323–1332.

34

Yuan, M. and Y. Lin (2006). Model selection and estimation in regression with grouped vari-

ables. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 68 (1),

49–67.

Yuan, M. and Y. Lin (2007). Model selection and estimation in the Gaussian graphical

model. Biometrika 94 (1), 19–35.

Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty.

The Annals of Statistics 38 (2), 894–942.

Zhang, H., A. R. Conn, and K. Scheinberg (2010). A derivative-free algorithm for least-

squares minimization. SIAM Journal on Optimization 20 (6), 3555–3576.

Zou, H. (2006). The adaptive lasso and its oracle properties. Journal of the American

Statistical Association 101 (476), 1418–1429.

35

","2 2 0 2 n u J 7 1 ] E M . t a t s [ 1 v 6 1 6 8 0 . 6 0 2 2 : v i X r a Dynamical Modeling for non-Gaussian Data with High-dimensional Sparse Ordinary Diﬀerential Equations Muye Nanshan, Nan Zhang School of Data Science, Fudan University and Xiaolei Xun Global Statistics and Data Science, BeiGene and Jiguo Cao Department of Statistics and Actuarial Science, Simon Fraser University Abstract Ordinary diﬀerential equations (ODE) have been widely used for modeling dy- namical complex systems. For high-dimensional ODE models where the number of diﬀerential equations is large, it remains challenging to estimate the ODE parame- ters and to identify the sparse structure of the ODE models. Most existing methods exploit the least-square based approach and are only applicable to Gaussian obser- vations. However, as discrete data are ubiquitous in applications, it is of practical importance to develop dynamic modeling for non-Gaussian observations. New meth- ods and algorithms are developed for both parameter estimation and sparse structure identiﬁcation in high-dimensional linear ODE systems. First, the high-dimensional generalized proﬁling method is proposed as a likelihood-based approach with ODE ﬁdelity and sparsity-inducing regularization, along with eﬃcient computation based on parameter cascading. Second, two versions of the two-step collocation methods are extended to the non-Gaussian set-up by incorporating the iteratively reweighted least squares technique. Simulations show that the proﬁling procedure has excellent performance in latent process and derivative ﬁtting and ODE parameter estimation, while the two-step collocation approach excels in identifying the sparse structure of the ODE system. The usefulness of the proposed methods is also demonstrated by analyzing three real datasets from Google trends, stock market sectors, and yeast cell cycle studies. Keywords: Dynamic system; Generalized linear model; Ordinary diﬀerential equations; Parameter cascade; Penalized likelihood; Proﬁled estimation. 1 1 Introduction Ordinary diﬀerential equations (ODE) are widely used for complex dynamic system mod- eling in biology, engineering, econometrics, and other scientiﬁc and social applications. For example, massive gene expression proﬁles are available with the advancement of second- generation sequencing technology. Modeling their dynamics using gene regulatory networks has drawn signiﬁcant interest from both biomedical and statistical research communities (Stuart et al., 2003; Yuan and Kendziorski, 2006; Hecker et al., 2009; Polynikis et al., 2009; Lu et al., 2011; Wu et al., 2014). In computational sociology, public opinion sensing and trend analysis have emerged from the advent of the big data revolution (Dodds et al., 2011; Sloan and Morgan, 2015). Massive datasets, such as Google searches or Twitter posts, are collected daily or even hourly, which enables social scientists to extract interesting temporal or spatial patterns via dynamic modeling. The main purpose of this article is to propose new methods and algorithms to estimate the ODE parameters and to identify the sparse structure for high-dimensional ODE models with non-Gaussian observations. A general ﬁrst-order ODE system can be described as θ′(t) = f (θ(t), β), (1.1) where the vector θ(t) = (θ1(t), . . . , θp(t))⊤ collects p processes while θ′(t) is the ﬁrst-order derivative of θ(t), function f = (f1, . . . , fp) describes the dependence between processes and their derivatives, β is the vector of ODE parameters to be estimated. Typically, the processes are indexed with time t and some initial conditions, for example, θ(0) = θ0, are assumed for the ODE system (1.1) as well. In practice, observations from the dynamic system are measured according to the re- alizations of latent processes θ(t) at discrete time points. Estimation of ODE parame- ters from noisy data remains a challenging problem (Ramsay et al., 2007; Wu et al., 2014; Hall and Ma, 2014; Chen et al., 2017; Wu et al., 2019; Dai and Li, 2021). In general, pa- rameter estimation procedures fall into three categories. The ﬁrst approach is based on a data ﬁtting process by nonlinear least squares. Given a set of initial ODE parameters, the ODE solutions are approximated by numerical methods, for example, the Runge-Kutta algorithm. Then the ODE parameters are updated with the nonlinear least squares. This 2 approach is computationally intensive and can be potentially inaccurate due to iterative numerical approximations. The second approach is the two-step collocation, where the basis expansions are exploited to approximate the ODE solutions. Varah (1982) proposed to ﬁt the processes via data smoothing methods, followed by a second stage of minimizing a least-square criterion based on the ODE system to estimate the ODE parameters. Because of its computational advantage, two-step collocation gains much popularity in the develop- ment of methodology and applications (Liang and Wu, 2008; Lu et al., 2011; Brunel et al., 2014; Wu et al., 2014; Dattner and Klaassen, 2015) and is further improved by iterative principal diﬀerential analysis (Ramsay, 1996; Poyton et al., 2006). However, the perfor- mance of two-step procedures relies heavily on the smoothing step, while the amount of roughness regularization is hard to control. The third approach is the generalized proﬁling procedure (Ramsay et al., 2007), which also represents ODE solutions with basis expan- sion as with two-step collocation methods. The essential diﬀerence is the inclusion of an ODE-induced penalty that controls the ﬁdelity of the processes to the ODE system. The basis coeﬃcients and ODE parameters are then estimated simultaneously from a penalized criterion using the parameter cascading algorithm (Cao and Ramsay, 2007). From a the- oretical perspective, Qi and Zhao (2010) derived an upper bound on the uniform norm of the diﬀerence between the true underlying solutions and their approximations, and proved the consistency and asymptotic normality of the estimation procedure. More recently, there has been growing interest in high-dimensional ODE systems where the number of processes p is large. For instance, the high-dimensional time-course gene expression data enables biomedical researchers to model the regulatory behaviors via a large-scale directed graphical network model. Such a task is called network recovery. The ODE system (1.1) naturally serves for this purpose by relating the dynamics of each process with all the processes in the system, and a sparse network structure can be further imposed. Lu et al. (2011) considered the high-dimensional linear ODE for dynamic gene regulatory network identiﬁcation and applied the smoothly clipped absolute deviation (Fan and Li, 2001) approach for variable selection. Wu et al. (2014) further relaxed the linear assump- tion and investigated a sparse additive ODE model using a two-stage procedure coupled with the adaptive group Lasso technique (Wang and Leng, 2008) to deal with nonlinear 3 eﬀects. Chen et al. (2017) proposed an eﬃcient procedure using the integrated form of the ODE to bypass numerical diﬃculty in the derivative estimation and adopted the group Lasso (Yuan and Lin, 2006) for variable selection. Wu et al. (2019) recently developed a matrix factorization based approach to ultra-high dimensional linear ODE models for pa- rameter estimation and variable selection. To our best knowledge, existing procedures for high-dimensional ODE models are two-stage approaches. Besides, most of the existing work assumes that observations of the ODE system are contaminated with Gaussian noises. Therefore, least-squares estimation is conveniently adopted. However, non-Gaussian observations are commonly encountered in real appli- cations, for example, short read count data from RNA sequencing (Nagalakshmi et al., 2008), bisulﬁte sequencing data for DNA methylation analysis (Cokus et al., 2008), and direction of change in the stock price over time (Huang et al., 2005). The literature on non-Gaussian data analysis with the ODE system is rare. Miao et al. (2014) developed a likelihood-based parameter estimation and inference for generalized ODE models. Its extension to high-dimensional ODE models, however, is still unknown. Motivated by network recovery tasks for time-course non-Gaussian data, this paper focuses on the parameter estimation and sparse structure identiﬁcation for high-dimensional linear ODE systems with a likelihood-based approach. To facilitate versatile analysis of non-Gaussian data, we assume the observations follow a distribution from the exponential family, where θj(t) is known as the canonical parameter in the context of generalized linear models (McCullagh and Nelder, 1989; Wood, 2017). Assume that t ∈ [0, 1] without loss of generality. Given a set of discrete time points t1, . . . , tn, denote by yij the measurement according to the jth latent process θj(t) at time t = ti, j = 1, . . . , p. Then, the conditional distribution of yij given θj(ti) admits a density function as f (yij | θj(ti)) = exp yijθj(ti) − b(θj(ti)) a(φ) (cid:26) + c(yij, φ) , (cid:27) where a > 0, b, c are known functions, φ is either known or considered as a nuisance parameter. Let (y1j, . . . , ynj)⊤ be the vector of observations from the latent process θj(t), and correspondingly the canonical parameter vector be (θj(t1), . . . , θj(tn))⊤. Imposing a linear structure on the general model (1.1), we investigate in this work the modeling of the dynamics among latent processes {θj(t) : j = 1, . . . , p} with a high-dimensional linear 4 ODE system, that is ′ θ j(t) = γj0 + p k=1 X γjkθk(t), j = 1, . . . , p. (1.2) In this article, we develop new methods and algorithms for both parameter estima- tion and sparse structure identiﬁcation in high-dimensional linear ODE systems. First, we propose the high-dimensional generalized proﬁling method along with a computationally eﬃcient procedure based on parameter cascading (Ramsay et al., 2007; Cao and Ramsay, 2007). It solves a hierarchical optimization for parameter estimation and variable selec- tion: an outer optimization concerning the ODE parameters under sparsity regularization is performed subject to an inner optimization where latent processes expanded with basis functions are ﬁtted by minimizing a weighted sum of data ﬁtting and ODE ﬁdelity crite- ria given ODE parameters. In particular, we regularize the structural ODE parameters based on individual diﬀerential equation and mitigate the computational burden for pa- rameter estimation in the high-dimensional ODE system. Moreover, there are two tuning parameters involved in our procedure: one controls the balance between data ﬁtting and ODE ﬁdelity in the inner optimization while the other regularizes the sparsity or model complexity in the outer optimization. Their interaction may aﬀect the overall convergence performance of the procedure in a complicated way. Due to the non-convexity nature of our objective function, we carefully design the tuning and stopping rules according to the performance of parameter estimation to help escape local minima (Carey and Ramsay, 2021). The global convergence of the proposed algorithm is analyzed. Next, we extend the two-step collocation methods (Wu et al., 2014; Chen et al., 2017), which are recently pro- posed for high-dimensional ODE models with Gaussian observations, to the non-Gaussian set-up. Two versions, corresponding to the vanilla collocation (Varah, 1982) and the graph reconstruction via additive diﬀerential equations (GRADE) (Chen et al., 2017), are devel- oped under the likelihood-based framework. Eﬃcient computation is feasible by applying the iteratively reweighted least squares technique (Wood, 2017). Finally, we apply the proposed methods to simulated and real data sets. In general, the proﬁling method is more eﬃcient than two-step collocation methods in estimating the latent processes, their deriva- tives, and the structural ODE parameters, while one two-step collocation method excels 5 in identifying the sparse structure of the ODE system. To sum up, the proposed methods present a versatile toolbox for parameter estimation and sparse structure identiﬁcation in high-dimensional linear ODE systems. The remainder of the article is organized as follows. Our proﬁled estimation approach is developed in Section 2. Detailed computational procedure and its global convergence are discussed in Section 3. In Section 4, we extend two-step collocation methods to model non-Gaussian observations. Section 5 compares empirical performance of the proposed methods. We analyze three real data examples in Section 6 with dynamical modeling approaches. Section 7 concludes the article and Appendix collects some technical details. 2 High-dimensional Generalized Proﬁling This section introduces the proposed approach for simultaneous parameter estimation and sparse structure identiﬁcation in a high-dimensional linear ODE model for non-Gaussian data under the penalized likelihood estimation framework. Denote by Γ = (γ1, . . . , γp) the parameter matrix of the ODE model (1.2), where γj = (γj0, . . . , γjp)⊤ ∈ Rp+1, for j = 1, . . . , p. These ODE parameters Γ are of primary interest in order to understand the network structure, called structural parameters hereafter. On the other hand, the latent processes θj’s are treated as nuisance parameters. Denote by yij and θj(ti) the observation and the canonical parameter of the jth latent process at time ti, respectively. Under the proﬁling scheme (Ramsay et al., 2007), an intermediate ﬁt of latent processes θ(t; Γ) = ( θ1(t; Γ), . . . , θp(t; Γ)) minimizes the following penalized likelihood criterion, b b b p 1 n p {yijθj(ti) − b(θj(ti))} + λθ − 1 np p 2 ′ θ j(t) − γj0 − 0 ( γjkθk(t) dt, (2.1) ) i=1 X j=1 Z X where the likelihood part measures ﬁdelity to data, the ODE ﬁdelity part measures the j=1 X k=1 X extent to which latent processes fail to satisfy the ODE system, and the tuning parameter λθ controls the amount of regularization. Furthermore, with θ(t; Γ) plugged in, an estimate of the structural parameters can be obtained by minimizing a data ﬁtting criterion with b 6 respect to Γ, − 1 np i=1 X j=1 X n p {yij θj(ti; Γ) − b( θj(ti; Γ))}. (2.2) The generalized proﬁling procedure proceed iteratively with a non-decreasing sequence of b b λθ under certain rules such that the ﬁtted processes adhere to the ODE. Identiﬁable issue and asymptotic behavior of the estimation procedure are studied by Ramsay et al. (2007) and Qi and Zhao (2010). Although the generalized proﬁling method provides a computationally eﬃcient treat- ment for the challenging ODE parameter estimation, it can only handle relatively small- scale models (Wu et al., 2019). On the one hand, for a p-dimensional linear ODE system, we have p2 + p ODE parameters to estimate in (2.2). If we further approximate the latent process θj(t) by basis expansion c⊤ j hj(t), where hj(t) is an mj-dimensional basis vector and cj is the coeﬃcient vector, then (2.1) becomes − 1 np n p yijc⊤ j hj(ti) − b(c⊤ j hj(ti)) i=1 X j=1 X (cid:8) p 1 + λθ 0 ( j=1 Z X (cid:9) c⊤ j h ′ j(t) − γj0 − p γjkc⊤ j hk(t) 2 ) dt, k=1 X and the total number of nuisance parameters p j=1 mj can be huge. Therefore, a di- rect application of the standard generalized proﬁling procedure to parameter estimation P for high-dimensional linear ODE is computationally demanding. On the other hand, the structural parameters obtained from (2.2) indeed infer an interaction network among the latent processes, in the sense that a nonzero γjk implies that θk(t) has an eﬀect on the change of θj(t). For better interpretation and to avoid potential over-ﬁtting, it is reason- able to introduce some sparsity for the structural parameters. For example, the Lasso and its variants (Tibshirani, 1996; Yuan and Lin, 2006; Zou, 2006), the smoothly clipped absolute deviation (SCAD) (Fan and Li, 2001) and the minimax concave penalty (MCP) (Zhang, 2010) have been extensively studied and used to recover probabilistic graphical structures (Yuan and Lin, 2007; Fan et al., 2009; Voorman et al., 2014). To address the above computational issues, we ﬁrst notice that the data ﬁdelity term in the penalized criterion (2.1) can be decomposed into sums of the likelihood for p in- dividual processes. Meanwhile, the penalty term, being a squared L2 norm of diﬀerential 7 equations, does not admit such decomposable property. Therefore, we propose to regularize the estimate of θj only by the corresponding jth diﬀerential equation. Speciﬁcally, when estimating θj given other {θk : k 6= j} at their most recent updates, we obtain θj(t; γj) by minimizing Gj(θj; γj) = − 1 n n i=1 X {yijθj(ti) − b(θj(ti))} + λθ,j 1 ′ j(t) − γj0 − θ 0 (cid:26) Z b 2 γjkθk(t) dt, (2.3) (cid:27) p k=1 X for j = 1, . . . , p. For simplicity, we use the same tuning parameter for individual sub- problems, that is λθ,j = λθ for j = 1, . . . , p. Optimizing Gj involves only p + 1 structural parameters in the vector γj and hence the computational complexity is greatly reduced. The beneﬁt of using (2.3) is justiﬁed from two aspects. First, it is computationally in- feasible to estimate a large number of ODE parameters jointly by directly applying the original generalized proﬁling criterion (2.1) to the high-dimensional ODE system. Our new formulation decouples the dependency of θ(t; Γ) on the matrix Γ into individual depen- dencies of θj(t; γj) on the vector γj. Second, from the perspective of penalized estimation, b it improves the estimation for the latent process and the ODE structural parameters by employing diﬀerential equations to regularize data smoothing. We remark on the potential risk of employing (2.3) instead of (2.1) when estimating the latent processes. Note that (2.1) aggregates all the diﬀerential equations to update the latent processes altogether such that the estimates will follow the ODE system jointly. In contrast, our method uses a single diﬀerential equation to regularize the estimation of each latent process. When the tuning parameter λθ increases, the parallel updating procedure (2.3) over j = 1, . . . , p, is expected to achieve an approximation in a marginal way to the joint estimation by (2.1). The simulation example introduced in Section S1 of the Supplementary Material shows that the approximation by (2.3) performs reasonably well, although the joint method (2.1) has a more accurate estimate for ODE parameters. Next, to induce sparsity to the structural parameter matrix, we estimate γj by mini- mizing Hj(γj) = − 1 n i=1 X n {yij θj(ti; γj) − b( θj(ti; γj))} + PENλγ,j (γj), (2.4) where the penalty function PENλγ,j (γj) with tuning parameter λγ,j > 0 induces sparsity for b b the structural parameter of the jth diﬀerential equation. Here we also assume for simplicity 8 that λγ,j = λγ for j = 1, . . . , p. If the ﬁtted structural parameter vector γj is zero, then we γj implies that the b b say other latent processes have no impact on θj(t). Any zero element in corresponding process has no inﬂuence on θj(t). The amount of sparsity regularization is typically determined by Bayesian information criterion (BIC) type principles, which have been adopted in other ODE parameter estimation approaches (Wu et al., 2014; Chen et al., 2017). Our new proﬁling estimation procedure for high-dimensional linear ODE systems con- sists of two objective functions (2.3) and (2.4), which are referred to as inner and outer criteria, respectively. Such a multi-criterion optimization problem is challenging due to non-convexity and non-diﬀerentiability. Speciﬁcally, we approximate the latent processes with basis expansion in the inner optimization, and basis coeﬃcients can be solved eﬃ- ciently with the Newton-Raphson method. However, the dependence of θj(t; γj) on γj is complicated and in general non-linear, which leads to the non-convexity of Hj. Moreover, b the sparsity-inducing penalty in Hj is non-diﬀerentiable at zero, making the Gauss-Newton scheme adopted by Ramsay et al. (2007) invalid under this scenario. Recent advances of derivative-free optimization algorithms (Powell, 2006; Zhang et al., 2010) may provide a viable solution. Nevertheless, they are in spirit joint optimization algorithms designed for general purpose and are thus not tailored for our speciﬁc problem. In contrast, our proﬁling procedure enjoys not only estimation eﬃciency but also algo- rithmic eﬃciency due to the use of analytical expressions of derivatives. Computational details are presented in the next section. In brief, after obtaining an estimate θj(t; γj) given the structural parameters, we linearize the likelihood component in (2.4) and formu- b late the outer optimization as a parameter estimation problem for a penalized generalized linear model. Therefore, the structural parameters can be readily updated by the iterative reweighted least-squares (IRLS). Through an iterative scheme between inner and outer op- timizations, our proﬁling procedure provides ODE parameter estimates and latent process ﬁts and identiﬁes the sparse structure of the ODE model. 9 3 Computation In this section, we provide computational details of our proﬁling procedure for high- dimensional linear ODE and analyze its global convergence. Minimizing criteria in (2.3) and (2.4) are referred as inner and outer optimizations. The structural parameters Γ = (γ1, . . . , γp) is of our primary interest, while the latent process ﬁts by the inner optimiza- tion is regarded as a nuisance parameter. In our proﬁling scheme, whenever γj changes by minimizing Hj in the outer, latent process ﬁts are then updated by solving the inner criterion Gj. Details are provided in Algorithm 1. In addition, two tuning parameters are involved in the proﬁling procedure, and their complex interaction aﬀects the overall algo- rithmic performance because of the non-convexity of the optimization. In the following, we split the discussion into inner and outer parts. Then we discuss the practical strategy of tuning parameter selection and the global convergence of the proposed algorithm. Algorithm 1: High-dimensional linear ODE for non-Gaussian data Input: Observations {yij : i = 1, . . . , n; j = 1, . . . , p}, initial ODE paramters Γ(0) = (γ(0) 1 , . . . , γ(0) p ), and ﬁxed tuning parameters λθ and λγ. Output: Estimated ODE parameters Γ = ( γ1, . . . , γp). repeat b At step s ≥ 1, the current estimate is b Γ(s) = ( b γ(s) 1 , . . . , γ(s) p ). for 1 ≤ j ≤ p do Update γj via the proﬁling procedure. b b b repeat 1. Given current γj, obtain the basis coeﬃcient estimate c∗ j ( γj) for the jth latent process in the inner optimization. e e 2. Apply basis expansion and update γj via minimizing the penalized reweighted least squares. until γj converges, and set γ(s+1) j = γj. end e until Estimated ODE parameter e Γ converges. b 10 3.1 Inner Optimization The inner procedure aims at ﬁnding an accurate estimate for latent processes given the structural parameter Γ. Similar to the two-step collocation method (Varah, 1982) and the generalized proﬁling (Ramsay et al., 2007), we represent latent processes by basis expan- sion. Suppose hj(t) = (φj1(t), . . . , φjmj (t)) is a set of basis functions for the jth process such that θj(t) = c⊤ j hj(t). Choices of basis functions include polynomials, truncated power functions and splines. In our numerical study, we use B-spline due to its numerical stability and excellent empirical performance. For notation simplicity, we use the same basis h(t) for all latent processes. Although critical in optimization, the basis coeﬃcients cj, j = 1, . . . , p, are often not of direct concern and thus considered as nuisance parameters. Observing that Gj(θj; γj) is convex with respect to the basis coeﬃcients cj, we can apply the Newton-Raphson scheme directly. When the Hessian of Gj(θj; γj) is invertible, we can start with an initial guess of cj and iteratively obtain c(r+1) j = c(r) j − (cid:18) −1 c(r) j (cid:19) (cid:18) ∂2Gj ∂cj∂c⊤ j (cid:12) (cid:12) (cid:12) (cid:12) ∂Gj , ∂cj (cid:12) c(r) j (cid:19) (cid:12) (cid:12) (cid:12) r ≥ 1. Analytical expressions of the derivatives involved in the above updating rule are given in A. 3.2 Outer Optimization The outer optimization is designed for updating γj with a regularized likelihood objective function (2.4). Denote by c∗ the inner optimization given the current γj. Observing that the dependence of c∗ j (γj) the optimal basis coeﬃcients for θj(t; γj) obtained from j (γj) on γj is implicit and possibly complicated, we propose to linearize the likelihood component in (2.4) and transform the optimization to ﬁnding the maximum likelihood estimate of a generalized linear model. The solution can then be readily obtained by the iteratively reweighted least squares (IRLS), see Wood (2017) for more detail. Let γj be the most recent update of γj. First, we linearize the c∗ j (γj) at γj which, e j (γj) ≈ c∗ c∗ j ( γj) + ∂c∗ j (γj) ∂γ⊤ j e 11 (γj − γj), e (3.1) eγj (cid:12) (cid:12) (cid:12) (cid:12) e where the derivative ∂c∗ j /∂γj is explicitly derived using the implicit function theorem in A. Hence, θj(t; γj) in its basis expansion form can be approximated by a linear function of γj. As a result, the outer objective function (2.4) now becomes a penalized likelihood function b of a generalized linear model. Second, we apply the IRLS and update our estimate of γj. Let θj(t) = θj(t; γj) be latent process ﬁt given the structural parameter γj. Based on the theory of generalized linear models, the observation Yj according to the latent process e b properties of E(Yj| µj(t) and var(Yj| θj(t)) = b θj(t)) = b θj(t)) = e ( ′′ ′ θj(t) admits vj(t)a(φ), e θj(t))a(φ) = ( e where functions a, b and parameter φ follow from the exponential family speciﬁcation. Write uij = −yij + b ′ ( e e θj(ti)) = −yij + e µj(ti) and ′′ wij = b ( e θj(ti)) = e vj(ti). The IRLS algorithm e applies a quadratic approximation to the log-likelihood, that is, at θj = e e e e e e θj, −yij θj(ti; γj) + b( θj(ti; γj)) ≈ wij yij − θj(ti; γj) 2 e + Cij, 1 2 where yij = θj(ti) − b uij/ n wij and Cij is independent of e e b o θj(ti). In conjunction with the linear b approximation of θj(ti; γj), it amounts to solving a penalized linear least squares to update e e e e e the estimate for structural parameter γj. Eﬃcient algorithms are available for diﬀerent sparsity penalty choices PEN(·). 3.3 Tuning Parameter Selection There are two tuning parameters involved in our proﬁling procedure, which jointly aﬀect the algorithmic performance. On the one hand, λθ in the inner optimization controls the amount of regularization regarding the diﬀerential equations. We deﬁne the aggregated ODE ﬁdelity criterion as p 1 ′ θ j(t) − γj0 − 0 ( j=1 Z X γjkθk(t) 2 ) dt. p k=1 X (3.2) Small λθ makes optimizing Hj(γj) with respect to γj more robust to initial guesses, but yields bad approximations to ODE solutions. Large λθ gives rise to a diﬃcult opti- mization problem where Hj(γj) is usually not convex and can have many local optima (Ramsay et al., 2007; Qi and Zhao, 2010; Carey and Ramsay, 2021). On the other hand, λγ in the outer optimization induces a sparse network structure for latent processes with better interpretation, and existing methods such as information criteria can be adopted for 12 tuning. Based on the above discussion, we propose to ﬁx λγ in the outer optimization ﬁrst, iteratively select a proper λθ in the inner optimization, and then determine the best λγ via the Bayesian information criterion. In detail, suppose we choose λγ from a sequence of candidate values. Then, we initialize λθ with a small value and moderately increase it via an iterative scheme. At each iteration, θ and Γ are repeatedly estimated for the current λθ, which are then used as initial values in the ﬁtting procedure with the next larger λθ. b b The iterative scheme stops when the estimated ODE parameters converge, and thus λθ is decided. The change of the estimated ODE parameters should be small when there is only a moderate increase in λθ. Therefore, with a conservatively increasing sequence of λθ, every estimated Γ is much likely to be a proper initialization for the next iteration. Details of the iterative selecting scheme for λθ given a ﬁxed λγ are as follows. b (1) Start with a small positive λ(0) θ . Choose ∆(0) as an initial incremental factor. (2) At the uth iteration where u ≥ 0, obtain the ﬁtted latent processes θ(u) and Γ(u) via our proﬁling procedure, and evaluate the ODE ﬁdelity (3.2) based on the estimates. b b (a) If the absolute percentage of change in the ODE ﬁdelity (3.2) is below a threshold constant, then we update λ(u+1) θ = λ(u) θ × ∆(u). (b) Otherwise, we need to downsize the incremental factor, for example, set ∆(u) = ∆(u−1)/2, which ensures that the ODE ﬁdelity (3.2) varies little among iterations. (3) When the successive ODE parameter estimates are closed enough, we stop iteration; otherwise, repeat previous steps. Our iterative tuning strategy treats λθ as a function of λγ. Hence, after λθ is selected for each ﬁxed λγ from a sequence of candidate values, we can evaluate the following BIC and choose the best λγ, BIC(λγ) = − 1 np n p yij θj(ti; λγ) − b(θj(ti; λγ)) + k(λ) log(n), i=1 X j=1 n X b o where θj(t; λγ) emphasizes the dependence on λγ, and k(λγ) denotes the number of non-zero elements in the resultant ODE parameter b Γ(λγ). b 13 3.4 Global Convergence Suppose that the estimated latent process θj(t; γj) from the inner optimization is a smooth function of γj, where j = 1, . . . , p. Let H(Γ) = p j=1 Hj(γj) be the objective function in the outer optimization for a given tuning parameter λγ, where Γ = (γ1, . . . , γp). Algorithm 1 b P is essentially a block coordinate descent method because it minimizes H(Γ) by iteratively updating γj. Write Hj(γj) = ℓj(γj) + PENλγ (γj), where ℓj(γj) is the likelihood term and PENλγ (γj) is assumed to be convex. As described in Section 3.2, the outer optimization is equivalent to updating γj to γj + dj(γj), where the descent direction dj(γj) is the solution to ∇ℓj(γj)⊤d + min d 1 2 d⊤Qj(γj)d + PENλγ (γj + d), where ∇ℓj(γj) is the gradient of ℓj(γj) and Qj(γj) = 1 n n   i=1 X b′′( θj(ti; γj)) ∂ θj(ti; γj) ∂γj b ∂ θj(ti; γj) ∂γj ! b b is a positive deﬁnite matrix approximating the Hessian ∇2ℓj(γj).  ⊤    We follow Tseng and Yun (2009) to establish the global convergence. Because the actual value of the Hessian ∇2ℓj(γj) is identical to its expected value under canonical links (McCullagh and Nelder, 1989), the IRLS method described in Section 3.2 remains the same when the Hessian is replaced by the expected Hessian. Then it follows from Lemma S1 in the Supplementary Material that Hj(γj + dj(γj)) − Hj(γj) ≤ −d⊤ j (γj) Qj(γj) − (cid:20) 1 2 E{∇2ℓj(γj)} dj(γj) + o(kdj(γj)k2). (cid:21) (3.3) Some algebra yields that Qj(γj) − 1 2 E{∇2ℓj(γj)} = 1 2 Qj(γj) + 1 2n n b′(θ∗ j (ti)) − b′( θj(ti, γj)) i=1 n X b ∂2 θj(ti, γj) ∂γj∂γ⊤ j b , o where θ∗ j (t) is the true latent process. The above matrix is positive deﬁnite because b′(·) It follows from θj(ti, γj) is suﬃciently close to the truth. is continuous, provided that (3.3) that Hj(γj) decreases along the iterations and will eventually converge because it is b lower-bounded. Moreover, the sequence of descent directions converges to zero due to (3.3). 14 According to Theorem 1(e) and Lemma 2 of Tseng and Yun (2009), every cluster point of the iterative estimates by Algorithm 1 exhibits exact zero descent direction, which implies it is indeed a stationary point of H(Γ). Finally, we remark that the above analysis cannot be directly applied to a non-convex PENλγ (·) such as the SCAD penalty. However, the non-convex penalty can be numerically approximated by local linear or quadratic functions (Fan et al., 2020). We would anticipate a similar convergence result but with more involved technical details, which is not pursued in this paper. 4 Two-step Collocation Methods for non-Gaussian Data Collocation methods have been exploited for both parameter estimation and network re- construction for various ODE models. In this section, we extend the popular two-step col- location method for high-dimensional linear ODE with non-Gaussian observations. In the large literature on collocation, Varah (1982); Ramsay et al. (2007); Dattner and Klaassen (2015), and Wu et al. (2019) consider the linear case while recently the nonparametric additive structure is investigated by Henderson and Michailidis (2014); Wu et al. (2014) and Chen et al. (2017). Most existing methods are proposed for Gaussian observations and adopt the least square loss function for estimation. In the following, we present two versions of the two-step collocation method for high-dimensional ODE models with non- Gaussian observations: the vanilla collocation based on Varah (1982) and an extension from graph reconstruction via additive diﬀerential equations (GRADE) by Chen et al. (2017). The vanilla two-step method ﬁrst ﬁts smoothing estimates θ(t) to the latent processes with maximum likelihood estimation, and then obtain the structural parameter γ with the b estimated processes and their derivatives plugged in. The procedure solves the following optimization problems, 1 with b γj = arg min γj0,γj Z 0 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) θj(t) = arg min θ∈H b − 1 n n i=1 X d θj(t) dt b p − γj0 − γjk θk(t) k=1 X b dt + PENλγ (γj), (4.1) 2 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) {yijθ(ti) − b(θ(ti))}, 1 ≤ j ≤ p, (4.2) 15 where H is a proper reproducing kernel Hilbert space, and the exponential family smoothing splines can be adopted (Wahba et al., 1995; Gu, 2013; Ma et al., 2017). The performance of the vanilla two-step collocation method relies on the estimation accuracy of θj(t) and its derivatives. Although statistical convergence has been established, it is in practice hard to b tune the smoothing procedure to achieve the optimality (Liang and Wu, 2008; Brunel et al., 2014). Another extension is based on the GRADE method (Chen et al., 2017). It avoids the derivative estimation issue in the vanilla collocation method, and instead considers the ODE ﬁdelity term in its integral form. Similar to the vanilla two-step method, the GRADE method ﬁrst obtains the smoothing estimates of latent processes from observations as in (4.2). Using integrated basis functions Θj(t) = t 0 θj(t) dt, j = 1, . . . , p, one can express R θj(t) = Cj0 + γj0 t + b p b γjk Θk(t), according to the integrated diﬀerential equations. Finally, we solve the following optimiza- e k=1 X b tion problems to obtain γj = arg min Cj0,γj0,γj 1 n n yij θj(ti) − b( θj(ti)) + PENλγ (γj). (4.3) The GRADE method is initially developed for nonparametric additive ODE models and b i=1 n X e e o naturally adapts to the linear case. The use of an integrated form of ODE facilitates in- vestigating the asymptotic behavior of the estimator and enhancing its robustness to the smoothing eﬀect in the ﬁrst step (Dattner and Klaassen, 2015; Chen et al., 2017). Both the two-step collocation methods proposed in this section involve maximizing the likeli- hood function for exponential family distributions, which can be eﬃciently solved with the iteratively reweighted least squares technique as in Section 3.2. We compare the two-step collocation methods with the high-dimensional generalized proﬁling (HDGP) procedure in Section 5. For process and derivative estimation, since HDGP balances both the data and ODE ﬁdelities, it usually results in reasonable ﬁts and more accurate ODE parameter estimates due to the more accurate derivatives. For sparse structure identiﬁcation, GRADE achieves the best accuracy, which is consistent with the motivation of GRADE for network reconstruction (Chen et al., 2017). In summary, HDGP 16 is a better choice for process ﬁtting and ODE parameter estimation, while GRADE excels in sparse structure identiﬁcation. 5 Simulation Studies This section compares the empirical performance of three dynamical modeling approaches: the high-dimensional generalized proﬁling (HDGP) procedure and the two-step collocation methods proposed in Section 4, namely the GRADE and the vanilla two-step method, respectively. Consider the ODE system studied by Chen et al. (2017) which consists of eight processes in four pairs, for k = 1, . . . , 4, θ′ 2k−1(t) = 2kπ θ2k(t) θ′ 2k(t) = 2kπ θ2k−1(t)   , t ∈ [0, 1]. It is clear that the ODE solutions take the form of sine and cosine functions with vary-  ing frequencies, whereas no interaction exists across pairs. For the kth pair, the initial state is sin(yk) and cos(yk), where yk is sampled from N(0, 1). The latent processes θ(t) = (θ1(t), . . . , θ8(t))⊤ described by the above ordinary diﬀerential equations are used to generate observations from Gaussian, Poisson and Bernoulli distributions. Denote by t1, . . . , tn time points from [0, 1]. For Gaussian distribution, yij is sampled from N(θj(ti), σ2) with known variance σ2, and the sample size n for each process is set to be 100 and 500. For Poisson distribution, we draw 500 and 1000 samples from Poisson(λj(ti)) where the intensity process λj(t) = exp{θj(t)}. For Bernoulli distribution, 1500 and 2500 samples are generated with probability of success pj(t) = exp{θj(t)}/[1 + exp{θj(t)}]. Sample sizes for Poisson and Bernoulli distributions are larger than Gaussian, as in those cases more observations are generally required to ensure reasonable estimates according to the theory of generalized linear model. We use the smoothing spline ﬁtting as an initialization for the proﬁling procedure, which also corresponds to the ﬁrst stage of two-step collocation methods. The order of B-spline functions in HDGP is set as 6, and the number of knots is half of that of time points. Both HDGP and GRADE require numerical integration to evaluate ODE ﬁdelity and integrated 17 basis representations, respectively. For sparsity penalty choices, we consider the Lasso penalty PENλγ (γj) = λγkγjk1 and the SCAD penalty PENλγ (γj) = p k=1 pλγ (|γjk|), where the function pλ(·) is deﬁned on [0, ∞) as λu, P if 0 ≤ u ≤ λ −(u2 − 2aλu + λ2)/(a − 1), if λ < u < aλ (a + 1)λ2/2 if u ≥ aλ, pλ(u) =    and a suggested value for a is 3.7 according to Fan and Li (2001). Algorithmic convergence is demonstrated when the diﬀerence between successive ODE parameter estimates is small enough. It works well for two-step collocation methods. However, due to the complex interaction between inner and outer optimizations, HDGP may not yield sparse ODE pa- rameter estimates at the declaration of convergence. To address this numerical issue, we manually set ODE parameter estimates below a constant threshold as zero. Based on our empirical studies, a recommended value for the threshold is the root-mean-square of the initial estimate Γ multiplied by a factor 0.01. Simulation results are evaluated using three types of criteria. The ﬁrst two criteria b concern about process and derivative estimates, which are evaluated by the mean squared errors (MSE) of θ(t) and θ′(t), MSE( θ(t)) = b θ′(t)) = MSE( b 1 np 1 np p n j=1 X p i=1 n X n j=1 X i=1 n X b θj(ti) − θj(ti) b j(ti) − θ′ θ′ j(ti) , . 2 o 2 o Second, we measure how well the structural parameters are estimated by their root-mean- square error (RMSE). Third, true positive rate (TPR) and false positive rate (FPR) are used to quantify how well the sparse structure is identiﬁed, where we refer to non-zero structural parameters as positive cases and otherwise as negative cases. Table 1 displays the averaged evaluations over 50 repeated experiments using the Lasso penalty, while the true positive rates are omitted because they are all equal to one for all three methods. Under each simulation set-up, increasing the number of observations always leads to reduced errors and tighter conﬁdence intervals in terms of the process ﬁt and the parameter estimation. For process and derivative ﬁtting, the smoothing splines method, 18 Table 1: Performance of HDGP, GRADE and the vanilla two-step method evaluated based θ′(t))), non-zero pa- θ(t))), derivative estimates (MSE( on the process estimates (MSE( rameter estimation (RMSE), and sparse structure estimates (FPR). The 95% conﬁdence b b intervals are given in parentheses. N Method MSE ( bθ(t)) MSE ( bθ′(t)) n a i s s u a G n o s s i o P i l l u o n r e B 100 500 500 1000 1500 2500 HDGP GRADE vanilla HDGP GRADE vanilla HDGP GRADE vanilla HDGP GRADE vanilla HDGP GRADE vanilla HDGP GRADE vanilla 0.011 (0.0097,0.0124) 0.005 (0.0042,0.0049) 0.005 (0.0043,0.0049) 0.002 (0.0017,0.0023) 0.001 (0.0010,0.0011) 0.001 (0.0010,0.0011) 0.024 (0.0222,0.0259) 0.024 (0.0232,0.0252) 0.024 (0.0232,0.0252) 0.011 (0.0105,0.0121) 0.013 (0.0128,0.0141) 0.013 (0.0127,0.0141) 0.031 (0.0260,0.0357) 0.031 (0.0277,0.0333) 0.032 (0.0285,0.0376) 0.019 (0.0169,0.0216) 0.020 (0.0193,0.0210) 0.020 (0.0193,0.0211) 3.01 (2.48, 3.53) 5.23 (4.67, 5.87) 5.23 (4.74, 5.93) 0.48 (0.41, 0.57) 1.82 (1.75, 1.88) 1.82 (1.75, 1.88) 6.24 (5.60, 6.93) 12.27 (11.66,13.06) 12.27 (11.63,13.00) 2.70 (2.43, 2.97) 8.20 (7.70, 8.73) 8.20 (7.71, 8.67) 8.18 (6.74, 9.73) 15.97 (14.77,16.99) 22.28 (17.21,32.11) 5.05 (4.33, 6.03) 12.71 (11.91,13.67) 12.71 (11.88,13.69) 19 RMSE ( b Γ) 0.58 (0.52,0.66) 2.97 (2.89,3.05) 0.62 (0.54,0.72) 0.28 (0.25,0.30) 0.84 (0.82,0.85) 0.34 (0.32,0.37) 1.70 (1.54,1.91) 2.03 (1.86,2.19) 1.86 (1.70,2.07) 1.04 (0.94,1.14) 1.41 (1.31,1.53) 1.18 (1.09,1.29) 1.77 (1.48,1.97) 3.32 (3.01,3.61) 2.28 (1.70,3.22) 1.55 (1.39,1.74) 2.57 (2.34,2.79) 1.67 (1.46,1.87) FPR 0.44 (0.43,0.46) 0.00 (-,-) 0.89 (0.84,0.92) 0.44 (0.42,0.45) 0.01 (0.01,0.02) 0.67 (0.64,0.71) 0.58 (0.56,0.61) 0.37 (0.34,0.41) 0.98 (0.97,0.98) 0.57 (0.55,0.59) 0.32 (0.28,0.36) 0.97 (0.96,0.97) 0.54 (0.52,0.58) 0.24 (0.21,0.28) 0.94 (0.90,0.96) 0.59 (0.55,0.62) 0.20 (0.16,0.24) 0.98 (0.97,0.99) as the ﬁrst stage of two-step collocation methods, often produces accurate estimates of the latent process itself, but is less eﬃcient in the derivative ﬁtting. In contrast, the inner optimization of HDGP balances the data and ODE ﬁdelities, resulting in reasonable process ﬁtting and improved derivative ﬁtting. For ODE parameter estimation, HDGP delivers the smallest error due to the more accurate derivatives. Interestingly, GRADE has much worse performance than the other two under this criterion. One partial reason is that GRADE only uses structural parameters in the integrated basis representation (4.3) instead of the explicit form of diﬀerential equations. For sparse structure identiﬁcation, GRADE achieves the best accuracy, as it discovers all non-zero structural parameters with the fewest false positives. It is consistent with the motivation of GRADE for network reconstruction (Chen et al., 2017). In summary, HDGP is a better choice for process ﬁtting and ODE parameter estimation, while GRADE excels in sparse structure identiﬁcation. We next investigate the eﬀects of diﬀerent noise levels and choices of sparse penalty. Under the above Gaussian set-up with 500 observations for each process. The signal- to-noise ratio (SNR) is deﬁned as the ratio between the sample standard deviation of {θj(ti)}n i=1 and the noise standard deviation σ. We set the signal-to-noise ratio as 3, 10, 30, and inﬁnity, where the inﬁnite ratio means that no noise is added. Both Lasso and SCAD penalties are considered. Figure 1 presents the performance evaluations over 50 repeated experiments. In general, all methods perform better over all criteria when the signal-to- noise ratio increases. The top row of Figure 1 corresponding to the Lasso penalty provides the consistent result as in Table 1, which indicates that HDGP has a comparable process ﬁt and better derivative estimation, especially when the noise level is low. Moreover, HDGP performs the best for estimating structural parameters, while the vanilla two-step method also provides satisfactory results. In contrast, even when there is no noise, the bias of ODE parameter estimates by GRADE is still large and RMSE is almost constant. For sparse structure identiﬁcation, GRADE outperforms the other methods under a wide range of noise levels. HDGP and the vanilla two-step method only have high accuracy when the signal level is high. The bottom row of Figure 1 displays simulation results when the SCAD penalty is used for inducing sparsity for the ODE system. Compared with the results with Lasso, overall performances in process, derivative, and ODE parameter estimations are 20 Processes Derivatives Non-zero Parameters FPR 0.012 0.009 0.006 E S M 0.003 0.000 0.0015 0.0010 E S M 0.0005 0.0000 3 10 30 Inf SNR Processes E S M E S M 4 3 2 1 0 3 2 1 0 E S M R 2.0 1.5 1.0 0.5 0.0 0.75 0.50 0.25 0.00 3 10 30 Inf 3 10 30 Inf 3 10 30 Inf SNR SNR SNR (a) With Lasso penalty. Derivatives Non-zero Parameters FPR E S M R 0.5 0.4 0.3 0.2 0.1 0.0 0.8 0.6 0.4 0.2 0.0 3 10 30 Inf 3 10 30 Inf 3 10 30 Inf 3 10 30 Inf SNR SNR SNR SNR (b) With SCAD penalty. Figure 1: Performance of HDGP (purple solid), GRADE (blue dashed), and the vanilla two-step method (yellow dotted) for Gaussian observations at diﬀerent noise levels. The boxes identify the medians and the quartiles of each criterion for 50 repeated experiments. Top and bottom rows correspond to Lasso and SCAD penalties, respectively. 21 improved mainly due to the unbiasedness property of SCAD penalty (Fan and Li, 2001). More interestingly, the poor performance of GRADE in ODE parameter estimation is greatly enhanced, and now it delivers comparable estimation results as the other two. Due to the oracle property enjoyed by the SCAD penalty (Fan and Li, 2001), we recommend it for better performance in parameter estimation. 6 Real Data Analysis 6.1 Google Trends Data Analysis Google Trends provides a publicly accessible online portal to analyze the popularity of search queries. In this study, we attempt to apply our method to model the interactions among a number of trending keywords during the recent pandemic of Coronavirus disease 2019 (COVID-19). In Table 2, we list 24 keywords and cluster them into three categories. The ﬁrst category consists of ﬁve keywords about speciﬁc terminologies such as mask and quarantine. The second category includes not only the countries with the most conﬁrmed cases as of January 2021, such as the United States, India, and Brazil but also the districts like Antarctica, which is the last continent to report conﬁrmed cases due to the remoteness and sparse population. We also include the last category of noise keywords with no apparent relationship to the pandemic. Table 2: Three categories of keywords selected for the analysis of Google Trends data. Category Keyword COVID-19 related coronavirus, mask, quarantine, vaccine, WHO (5 words) Countries or districts Africa, Antarctica, Arctic, Australia, Brazil, Canada, China, India, Iran, Italy, Japan, Russia, the United States (13 words) Noise words cat, cloud, desert, dog, game, sun (6 words) The Google Trends data used in our study cover the range from January 20 to Septem- ber 20 in 2020. The keyword popularity is measured by an integer index calculated by 22 normalizing and rounding the keyword count in an unbiased searching requests sample. We observe that the daily trend indices have several sharp peaks, see Figure 2 for an illus- trative example. Direct modeling for the mean trends will result in abrupt high values near the peaks and undersmooth other relatively ﬂat regions. Therefore, it is more appropriate to assume the indices follow Poisson distributions, and we apply the proposed method to model the latent processes of intensity parameters with ODEs. Figure 2: Daily Google Trends indices of keyword Russia and vaccine from January to September 2020. To better exhibit diﬀerent stages of the pandemic, we consider three time periods: from January 20 to March 19, from March 20 to June 19, and from June 20 to September 20. For each period, our method is applied to ﬁt the trending processes with a series of sparsity parameter λγ’s. Figures 3a and 3b display two networks with diﬀerent sparsity parameters in the ﬁrst period (from January to March). Keyword quarantine has the highest degree in both networks. During the COVID-19 pandemic, quarantines or self-quarantines are enacted by multiple governmental actors to prevent the rapid spread of the virus. It is of no surprise to become the top-ranked trending keyword. The other three keywords in Figure 3a are coronavirus, China and vaccine, which stand for the virus’s name, the country where the ﬁrst case was identiﬁed, and the immunization method. The top four keywords represent the major trending focus at the early stage of the pandemic. In Figure 23 Desert Russia cat dog WHO India the United States Iran mask Arctic cloud Italy cat game Brazil Japan sun Brazil cloud India Africa Antarctica game vaccine Coronavirus Canada Arctic Antarctica Italy Australia Africa quarantine Desert Coronavirus China quarantine WHO dog Australia sun China the United States Russia vaccine Japan Canada (a) λγ = 10−0.5 Iran mask (b) λγ = 10−1 Figure 3: Recovered networks of the trending keywords during the ﬁrst period (from Jan- uary 20 to March 19) with diﬀerent values of sparsity parameters. 3b, more aﬀected countries such as Australia, Italy, and the United States, are involved when the sparsity parameter is decreased. In contrast, noise keywords are isolated in both networks, indicating no connection to the trending topics. More interestingly, we investigate Table 3: Top four keywords in the recovered networks during three periods. The keyword of the highest degree is in boldface. Period Keywords January 20 – March 19 quarantine, China, coronavirus, vaccine March 20 – June 19 Italy, China, Iran, Russia June 20 – September 20 coronavirus, the United States, vaccine, mask the evolution of network structure for the trending keywords along the progression of the COVID-19 pandemic. Table 3 lists the top four keywords in three time periods where the keyword with the highest degree is in boldface. From the ﬁrst period to the second, the keyword Italy emerges as the new top word. According to the WHO report, on March 19, Italy overtook China as the country with the most reported deaths, and announced 24 Table 4: Companies selected in eight categories for stock price data analysis. Group Category Companies 1 2 3 4 5 6 7 8 Information Technology Adobe, Apple, Microsoft, Salesforce, Zoom Electric Vehicle Pharmaceutical BYD, Kandi, Nio, Tesla, Workhorse AbbVie, Eli lilly, Moderna, Novartis, Pﬁzer Consumer Services & Retail Ascena, J. C. Penney, Kohl’s, Macy’s, Nordstrom Online Retail Shopping Amazon, Best Buy, Target, Walmart, Wayfair Hotels Hilton, Marriott, Wyndham, Wynn, Park Air Transportation Boeing, Airbus, Delta Air Lines, Southwest Airlines, United Airline Energy Chevron, Conocophillips, Exxon Mobil, Schlumberger, Valero Energy the national lockdown in March. Turning to the third period, China and Italy drop out of the top list. Both countries had successfully slowed down the domestic infections and reduced daily new cases signiﬁcantly. As preventive measures including wearing face masks in public are advised and several promising vaccines are being developed, mask and vaccine are among the top trending keywords. 6.2 Analysis of Stock Price Change Directions In the year 2020, the stock market experienced enormous volatility due to the coronavirus pandemic. Many companies have suﬀered massive price drops, while others have witnessed substantial increases. We collect the stock price indices for 40 companies during 251 trad- ing days spanning from January 1 to December 30, 2020. Our goal in this study is to characterize the change direction patterns of stock prices, taking into account the dynamic interactions among the stocks. To this end, the original price indices are coded as binary data to denote an increase or decrease. We group the companies into eight categories based on the Global Industry Classiﬁcation Standard. Details are provided in Table 4. The high-dimensional ODE system is built up for the latent success probability pro- 25 (cid:9)(cid:10)(cid:11)(cid:12)(cid:13) (cid:4)(cid:5)(cid:6)(cid:7)(cid:8) 0.550 0(cid:0)(cid:1)(cid:2)(cid:3) 0.500 0.54 (cid:129)(cid:130)(cid:131)(cid:132) 0.50 Information Technology Electric Vehicle Consumer Services & Retail Pharmaceutical +,-./ 0.550 &’()* 0.500 !""#$% 0.55 0.50 0.45 0.40 0.550 fghij 0.500 abcde 0.450 \]^_‘ (cid:14)(cid:15)(cid:16)(cid:17)(cid:18) (cid:19)(cid:20)(cid:21)(cid:22)(cid:23) (cid:24)(cid:25)(cid:26)(cid:27)(cid:28) 1(cid:29)(cid:30)(cid:31) 23456 789:; <=>?@ ABCDE GHIJK LMNOP QRSTU VWXYZ klmno qrstu vwxyz {|}~(cid:127) Online Retail Shopping Hotels Air Transportation Energy 0.51 0.48 0.45 0.50 0.48 ‡·(cid:181)¶ 0.44 ﬂ(cid:176)–† 0.50 0.45 0.40 0.35 (cid:133)(cid:134)(cid:135)(cid:136)(cid:137) (cid:138)(cid:139)(cid:140)(cid:141)(cid:142) (cid:143)(cid:144)(cid:145)(cid:146)(cid:147) (cid:148)(cid:149)(cid:150)(cid:151)(cid:152) (cid:154)(cid:155)(cid:156)(cid:157)(cid:158) (cid:159)(cid:160)¡¢£ ⁄¥ƒ§¤ '“«‹› •‚„”» …‰(cid:190)¿(cid:192) `´ˆ˜¯ ˘˙¨(cid:201)˚ (cid:204)˝˛ˇ— (cid:209)(cid:210)(cid:211)(cid:212)(cid:213) (cid:214)(cid:215)(cid:216)(cid:217)(cid:218) (cid:219)(cid:220)(cid:221)(cid:222)(cid:223) Figure 4: The ﬁtted probability processes of daily price increase for the eight categories. The red dashed line denotes p = 0.5. cesses. Our sparsity tuning procedure leads to λγ = 10−2.1 and the ﬁtted model achieves an ODE ﬁdelity below 10−6. Figure 4 displays the ﬁtted probabilities of a daily stock price increase for all categories. We notice some interesting results from the result. First, all categories have the low ﬁtted probabilities around March. It corresponds to the 2020 stock market crash, during which multiple circuit breakers were triggered on fears of the COVID-19 coronavirus. Since the crash, some sectors recovered and re-entered a bull market through December. Online retail companies made huge proﬁts as health concerns changed customers’ shopping habits. Information technology companies beneﬁted from the growing demands for information services and electronics devices. For example, the shifts towards remote working had raised the number of Zoom’s daily users to an unprecedented one. In contrast, sectors like energy, hotels, and air transportation experienced the most severe hit by the COVID-19 pandemic. Although there were signs of recovery in the fourth quarter, these industries are still under the tremendous impact of the COVID-19 recession. 6.3 Analysis of Yeast Cell Cycle-regulated Genes The cell cycle is a fundamental biological process consisting of cell growth, duplication of genetic information, distribution of chromosomes, and cell division (Cho et al., 1998). Spellman et al. (1998) analyzed the expression levels of 6,178 yeast genes at 7-minute in- tervals for 119 minutes. The experiments were carried out in the cell cultures with three independent synchronization methods. A score was calculated for each gene to indicate 26 p F [ (cid:128) (cid:153) ﬁ ¸ (cid:224) 14 56 72 71 48 52 41 38 6 60 45 59 24 3 17 2 31 32 33 63 51 21 9 28 42 47 30 13 54 70 39 10 16 1 12 8 61 62 66 5 20 65 68 4 27 7 26 37 35 69 57 40 58 11 55 46 53 49 36 15 Figure 5: The recovered network of the yeast cell cycle. Yellow nodes represent genes, and the green-solid or red-dashed edges indicate potential promotion or suppression eﬀects. their similarities to those cell-cycle regulated genes already known. Due to missingness in data, we choose 72 out of 93 genes identiﬁed by Spellman et al. (1998) in the alpha factor-based synchronized experiment, and model the dynamic relationship between the mean proﬁles of these 72 genes using an ODE system under Gaussian assumption for gene expression level. The proposed method is applied to identify the sparse structure of the gene regulatory network. The result is shown in Figure 5, which excludes 12 isolated genes. This suggests that although those genes get involved in the cell cycle, their regulated tran- scriptions are not absolutely required. Among the 60 genes in Figure 5, 116 regulations (i.e., directed edges) are discovered. The average number of regulations for each gene is around three, while more than 80% genes have regulations fewer than ﬁve. Genes with high network degrees are identiﬁed as central hub nodes. For example, CLN3 (node 1) has the largest number of regulations in Figure 5. According to the Saccharomyces Genome Database (Cherry et al., 1998), the encoded protein CLN3p is known as a cell cycle regu- 27 lator and promotes the G1/S transition (Nasmyth, 1993). More interestingly, the positive or negative signs of our estimated structural parameters naturally imply the potential pro- motion or inhibition between genes, respectively. Our result suggests that CHS1 (node 62) promotes the expression of POL30 (node 12), which regulates DNA replication in the G1 phase. Meanwhile, it suppresses the expression of FAR1 (node 30), which is a CDC28p kinase inhibitor functioning in the G2/M transition. 7 Conclusions and Discussion In this article, we have proposed a new proﬁling procedure for both parameter estima- tion and sparse structure identiﬁcation for high-dimensional linear ODE models with non- Gaussian observations. Our method involves a hierarchical optimization scheme: the inner optimization balances the data ﬁtting and ODE ﬁdelity to improve estimation eﬃciency, while the outer optimization induces a sparse structure for better model interpretation. Besides, we extend two-step collocation methods to the non-Gaussian observation setting and compare them with the proposed proﬁling procedure via comprehensive studies. One limitation of our work is that only the linear ODE system is under consideration. We are aware of the recent development of two-step collocation to sparse additive ODE systems (Henderson and Michailidis, 2014; Wu et al., 2014; Chen et al., 2017) and a more general functional ANOVA extension (Dai and Li, 2021). Although our hierarchical opti- mization is not restricted to the linear ODE, the extension to nonlinear ODE systems is not straightforward. For instance, a common strategy to handle additive ODE models is to expand the nonlinear components with basis function. However, due to the proﬁling nature, the range of collocation bases for latent processes needs to be controlled within a compact interval, which may not be easily overcome. Another future research is on the statistical properties such as uniform bound on the approximations to the true solutions, asymptotic normality of the estimators. Despite existing theory established for the stan- dard generalized proﬁling (Qi and Zhao, 2010), it is still a challenging problem due to high dimensionality, and we leave the systematic study to future work. 28 A Derivatives We provide the analytical expressions of the derivatives used in the computation (Section 3). Derivatives of Gj in inner optimization Write Gj(θj; γj) in the inner optimization Gj = − 1 n n i=1 X {yijθj(ti) − b(θj(ti))} + λθ θ′ j(t) − γj0 − ZT ( γjk θk(t) 2 ) dt, p k=1 X then the ﬁrst derivative is ∂Gj ∂cj = − 1 n n {yijh(ti) − b′(θj(ti))h(ti)} i=1 X + 2λθ dθj(t) dt ZT (cid:26) p − γj0 − γjk θk(t) k=1 X dh(t) dt (cid:27)(cid:26) − γjj h(t) dt, (cid:27) and the second derivative is ∂2Gj ∂cj∂c⊤ j = 1 n n i=1 X {b′′(θj(ti))h(ti)h(ti)⊤} + 2λθ For k = 0, dh(t) dt ZT (cid:26) − γjjh(t) dh(t) dt (cid:27)(cid:26) ⊤ − γjjh(t) dt. (cid:27) ∂2Gj ∂cj∂γj0 = −2λθ dh(t) dt ZT (cid:26) − γjjh(t) dt, (cid:27) for k = 1, . . . , p and k 6= j , ∂2Gj ∂cj∂γjk = −2λθ dh(t) dt ZT (cid:26) − γjjh(t) θk(t) dt, (cid:27) for k = j, ∂2Gj ∂cj∂γjj = −2λθ ZT (cid:26) dh(t) dt − γjjh(t) θj(t) dt − 2λθ ZT (cid:26) dθj(t) dt (cid:27) p − γj0 − γjkθk(t) h(t) dt. k=1 X (cid:27) 29 Derivative of c∗ j in outer optimization Write c∗ j (γj) as c∗ j for simplicity. Since Gj has zero-gradient at c∗ j , then Taking the derivative with respect to γj on both sides, we have = 0. ∂Gj ∂cj (cid:12) c∗ j (cid:12) (cid:12) (cid:12) j (cid:18) Suppose ∂2Gj/(∂cj∂c⊤ d dγ⊤ = ∂2Gj ∂Gj ∂cj∂c⊤ ∂cj (cid:12) j ! c∗ c∗ j (cid:12) j (cid:19) (cid:12) (cid:12) (cid:12) (cid:12) j )|c∗ j is non-singular, we have the following expression of the deriva- (cid:12) (cid:12) ∂2Gj ∂cj∂γ⊤ j (cid:12) (cid:12) (cid:12) (cid:12) j (γj) ∂γ⊤ j = 0. + c∗ j ∂c∗ tive ∂c∗ ∂2Gj ∂cj∂c⊤ j (cid:12) (cid:12) Both matrices on the right-hand side have been explicitly derived, the the derivative of c∗ (cid:12) j (cid:12) ∂2Gj . ∂cj∂γ⊤ c∗ j (cid:12) j (cid:19) (cid:12) (cid:12) (cid:12) j (γj) ∂γ⊤ j c∗ j (cid:19) = − (cid:18) (cid:18) −1 follows. SUPPLEMENTARY MATERIALS Supplementary Material contains additional numerical results. References Brunel, N. J., Q. Clairon, and F. d’Alch´e Buc (2014). Parametric estimation of ordinary diﬀerential equations with orthogonality conditions. Journal of the American Statistical Association 109 (505), 173–185. Cao, J. and J. O. Ramsay (2007). Parameter cascades and proﬁling in functional data analysis. Computational Statistics 22 (3), 335–351. Carey, M. and J. O. Ramsay (2021). Fast stable parameter estimation for linear dynamical systems. Computational Statistics & Data Analysis 156, 107124. Chen, S., A. Shojaie, and D. M. Witten (2017). Network reconstruction from high- dimensional ordinary diﬀerential equations. Journal of the American Statistical Associ- ation 112 (520), 1697–1707. 30 Cherry, J. M., C. Adler, C. Ball, S. A. Chervitz, S. S. Dwight, E. T. Hester, Y. Jia, G. Juvik, T. Roe, and M. Schroeder (1998). Sgd: Saccharomyces genome database. Nucleic Acids Research 26 (1), 73–79. Cho, R. J., M. J. Campbell, E. A. Winzeler, L. Steinmetz, A. Conway, L. Wodicka, T. G. Wolfsberg, A. E. Gabrielian, D. Landsman, and D. J. Lockhart (1998). A genome-wide transcriptional analysis of the mitotic cell cycle. Molecular cell 2 (1), 65–73. Cokus, S. J., S. Feng, X. Zhang, Z. Chen, B. Merriman, C. D. Haudenschild, S. Pradhan, S. F. Nelson, M. Pellegrini, and S. E. Jacobsen (2008). Shotgun bisulphite sequencing of the arabidopsis genome reveals dna methylation patterning. Nature 452 (7184), 215–219. Dai, X. and L. Li (2021). Kernel ordinary diﬀerential equations. Journal of the American Statistical Association. Dattner, I. and C. A. Klaassen (2015). Optimal rate of direct estimators in systems of ordinary diﬀerential equations linear in functions of the parameters. Electronic Journal of Statistics 9 (2), 1939–1973. Dodds, P. S., K. D. Harris, I. M. Kloumann, C. A. Bliss, and C. M. Danforth (2011). Tem- poral patterns of happiness and information in a global social network: Hedonometrics and twitter. PLoS ONE 6 (12), e26752. Fan, J., Y. Feng, and Y. Wu (2009, 06). Network exploration via the adaptive lasso and scad penalties. The Annals of Applied Statistics 3 (2), 521–541. Fan, J. and R. Li (2001). Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American statistical Association 96 (456), 1348–1360. Fan, J., R. Li, C.-H. Zhang, and H. Zou (2020). Statistical foundations of data science. Chapman and Hall/CRC. Gu, C. (2013). Smoothing Spline ANOVA Models (2nd ed.), Volume 297. New York: Springer. 31 Hall, P. and Y. Ma (2014). Quick and easy one-step parameter estimation in diﬀeren- tial equations. Journal of the Royal Statistical Society: Series B (Statistical Methodol- ogy) 76 (4), 735–748. Hecker, M., S. Lambeck, S. Toepfer, E. Van Someren, and R. Guthke (2009). Gene regula- tory network inference: data integration in dynamic models—a review. Biosystems 96 (1), 86–103. Henderson, J. and G. Michailidis (2014). Network reconstruction using nonparametric additive ODE models. PLoS ONE 9 (4), e94003. Huang, W., Y. Nakamori, and S.-Y. Wang (2005). Forecasting stock market movement direction with support vector machine. Computers & operations research 32 (10), 2513– 2522. Liang, H. and H. Wu (2008). Parameter estimation for diﬀerential equation models using a framework of measurement error in regression models. Journal of the American Statistical Association 103 (484), 1570–1583. Lu, T., H. Liang, H. Li, and H. Wu (2011). High-dimensional odes coupled with mixed- eﬀects modeling techniques for dynamic gene regulatory network identiﬁcation. Journal of the American Statistical Association 106 (496), 1242–1258. Ma, P., N. Zhang, J. Z. Huang, and W. Zhong (2017). Adaptive basis selection for exponen- tial family smoothing splines with application in joint modeling of multiple sequencing samples. Statistica Sinica 27 (4), 1757–1777. McCullagh, P. and J. Nelder (1989). Generalized Linear Models, Second Edition. Chapman & Hall/CRC Monographs on Statistics & Applied Probability. Taylor & Francis. Miao, H., H. Wu, and H. Xue (2014). Generalized ordinary diﬀerential equation models. Journal of the American Statistical Association 109 (508), 1672–1682. Nagalakshmi, U., Z. Wang, K. Waern, C. Shou, D. Raha, M. Gerstein, and M. Snyder (2008). The transcriptional landscape of the yeast genome deﬁned by RNA sequencing. Science 320 (5881), 1344–1349. 32 Nasmyth, K. (1993). Control of the yeast cell cycle by the Cdc28 protein kinase. Current Opinion in Cell Biology 5 (2), 166–179. Polynikis, A., S. Hogan, and M. di Bernardo (2009). Comparing diﬀerent ODE modelling approaches for gene regulatory networks. Journal of Theoretical Biology 261 (4), 511–530. Powell, M. J. (2006). The NEWUOA software for unconstrained optimization without derivatives. In Large-scale nonlinear optimization, pp. 255–297. Springer. Poyton, A., M. S. Varziri, K. B. McAuley, P. J. McLellan, and J. O. Ramsay (2006). Parameter estimation in continuous-time dynamic models using principal diﬀerential analysis. Computers & Chemical Engineering 30 (4), 698–708. Qi, X. and H. Zhao (2010). Asymptotic eﬃciency and ﬁnite-sample properties of the generalized proﬁling estimation of parameters in ordinary diﬀerential equations. The Annals of Statistics 38 (1), 435–481. Ramsay, J. O. (1996). Principal diﬀerential analysis: Data reduction by diﬀerential opera- tors. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 58 (3), 495–508. Ramsay, J. O., G. Hooker, D. Campbell, and J. Cao (2007). Parameter estimation for diﬀerential equations: a generalized smoothing approach. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 69 (5), 741–796. Sloan, L. and J. Morgan (2015). Who tweets with their location? understanding the rela- tionship between demographic characteristics and the use of geoservices and geotagging on twitter. PLoS ONE 10 (11), e0142209. Spellman, P. T., G. Sherlock, M. Q. Zhang, V. R. Iyer, K. Anders, M. B. Eisen, P. O. Brown, D. Botstein, and B. Futcher (1998). Comprehensive identiﬁcation of cell cycle-regulated genes of the yeast Saccharomyces cerevisiae by microarray hybridization. Molecular Biology of the Cell 9 (12), 3273–3297. Stuart, J. M., E. Segal, D. Koller, and S. K. Kim (2003). A gene-coexpression network for global discovery of conserved genetic modules. Science 302 (5643), 249–255. 33 Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 58 (1), 267–288. Tseng, P. and S. Yun (2009). A coordinate gradient descent method for nonsmooth sepa- rable minimization. Mathematical Programming 117 (1), 387–423. Varah, J. M. (1982). A spline least squares method for numerical parameter estimation in diﬀerential equations. SIAM Journal on Scientiﬁc and Statistical Computing 3 (1), 28–46. Voorman, A., A. Shojaie, and D. Witten (2014). Graph estimation with joint additive models. Biometrika 101 (1), 85–101. Wahba, G., Y. Wang, C. Gu, R. Klein, and B. Klein (1995, 12). Smoothing spline ANOVA for exponential families, with application to the Wisconsin epidemiological study of dia- betic retinopathy : the 1994 Neyman memorial lecture. The Annals of Statistics 23 (6), 1865–1895. Wang, H. and C. Leng (2008). A note on adaptive group lasso. Computational Statistics & Data Analysis 52 (12), 5277–5286. Wood, S. N. (2017). Generalized additive models: an introduction with R. CRC press. Wu, H., T. Lu, H. Xue, and H. Liang (2014). Sparse additive ordinary diﬀerential equations for dynamic gene regulatory network modeling. Journal of the American Statistical Association 109 (506), 700–716. Wu, L., X. Qiu, Y.-x. Yuan, and H. Wu (2019). Parameter estimation and variable selec- tion for big systems of linear ordinary diﬀerential equations: a matrix-based approach. Journal of the American Statistical Association 114 (526), 657–667. Yuan, M. and C. Kendziorski (2006). Hidden Markov models for microarray time course data in multiple biological conditions. Journal of the American Statistical Associa- tion 101 (476), 1323–1332. 34 Yuan, M. and Y. Lin (2006). Model selection and estimation in regression with grouped vari- ables. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 68 (1), 49–67. Yuan, M. and Y. Lin (2007). Model selection and estimation in the Gaussian graphical model. Biometrika 94 (1), 19–35. Zhang, C.-H. (2010). Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics 38 (2), 894–942. Zhang, H., A. R. Conn, and K. Scheinberg (2010). A derivative-free algorithm for least- squares minimization. SIAM Journal on Optimization 20 (6), 3555–3576. Zou, H. (2006). The adaptive lasso and its oracle properties. Journal of the American Statistical Association 101 (476), 1418–1429. 35","['dynamical', 'modeling', 'nongaussian', 'datum', 'highdimensional', 'sparse', 'ordinary', 'diﬀerential', 'equation', 'datum', 'science', 'global', 'statistic', 'data', 'science', 'beigene', 'statistic', 'actuarial', 'science', 'abstract', 'ordinary', 'diﬀerential', 'equation', 'widely', 'use', 'model', 'dy', 'namical', 'complex', 'system', 'highdimensional', 'ode', 'model', 'number', 'diﬀerential', 'equation', 'large', 'challenge', 'estimate', 'ode', 'parame', 'ter', 'identify', 'sparse', 'structure', 'ode', 'model', 'exist', 'method', 'exploit', 'leastsquare', 'base', 'approach', 'applicable', 'gaussian', 'obser', 'vation', 'however', 'discrete', 'datum', 'ubiquitous', 'application', 'practical', 'importance', 'develop', 'dynamic', 'modeling', 'nongaussian', 'observation', 'new', 'od', 'algorithm', 'develop', 'parameter', 'estimation', 'sparse', 'structure', 'identiﬁcation', 'system', 'first', 'highdimensional', 'generalize', 'proﬁle', 'method', 'propose', 'likelihoodbase', 'approach', 'ode', 'ﬁdelity', 'sparsityinduce', 'regularization', 'eﬃcient', 'computation', 'base', 'parameter', 'cascade', 'second', 'version', 'twostep', 'collocation', 'method', 'extend', 'nongaussian', 'setup', 'incorporate', 'iteratively', 'reweighte', 'least', 'square', 'technique', 'simulation', 'show', 'proﬁle', 'procedure', 'excellent', 'performance', 'latent', 'process', 'derivative', 'ﬁtting', 'ode', 'parameter', 'estimation', 'twostep', 'collocation', 'approach', 'excel', 'identify', 'sparse', 'structure', 'ode', 'system', 'usefulness', 'propose', 'method', 'also', 'demonstrate', 'analyze', 'real', 'dataset', 'trend', 'stock', 'market', 'sector', 'yeast', 'cell', 'cycle', 'study', 'keyword', 'dynamic', 'system', 'generalize', 'linear', 'model', 'ordinary', 'diﬀerential', 'equation', 'parameter', 'cascade', 'penalize', 'likelihood', 'proﬁle', 'estimation', 'introduction', 'ordinary', 'diﬀerential', 'equation', 'ode', 'widely', 'use', 'complex', 'dynamic', 'system', 'mod', 'eling', 'biology', 'engineering', 'econometric', 'scientiﬁc', 'social', 'application', 'example', 'massive', 'gene', 'expression', 'proﬁle', 'available', 'advancement', 'second', 'generation', 'sequence', 'technology', 'model', 'dynamic', 'use', 'gene', 'regulatory', 'network', 'draw', 'signiﬁcant', 'interest', 'biomedical', 'statistical', 'research', 'community', 'stuart', 'yuan', 'kendziorski', 'hecker', 'polyniki', 'computational', 'sociology', 'public', 'opinion', 'sense', 'trend', 'analysis', 'emerge', 'advent', 'big', 'datum', 'revolution', 'dodd', 'sloan', 'massive', 'dataset', 'search', 'twitter', 'post', 'collect', 'daily', 'even', 'hourly', 'enable', 'social', 'scientist', 'extract', 'interesting', 'temporal', 'spatial', 'pattern', 'dynamic', 'modeling', 'main', 'purpose', 'article', 'propose', 'new', 'method', 'algorithm', 'estimate', 'ode', 'parameter', 'identify', 'sparse', 'structure', 'highdimensional', 'ode', 'model', 'nongaussian', 'observation', 'general', 'ﬁrstorder', 'ode', 'system', 'describe', 'θ′t', 'vector', 'collect', 'p', 'process', 'θ′t', 'ﬁrstorder', 'derivative', 'describe', 'dependence', 'process', 'derivative', 'β', 'vector', 'ode', 'parameter', 'estimate', 'typically', 'process', 'index', 'time', 'initial', 'condition', 'example', 'assume', 'ode', 'system', 'well', 'practice', 'observation', 'dynamic', 'system', 'measure', 'accord', 'alization', 'latent', 'process', 'θt', 'discrete', 'time', 'point', 'estimation', 'ode', 'parame', 'ter', 'noisy', 'datum', 'remain', 'challenge', 'problem', 'ramsay', 'general', 'rameter', 'estimation', 'procedure', 'fall', 'category', 'ﬁrst', 'approach', 'base', 'data', 'ﬁtting', 'process', 'nonlinear', 'least', 'square', 'give', 'set', 'initial', 'ode', 'parameter', 'ode', 'solution', 'approximate', 'numerical', 'method', 'example', 'rungekutta', 'ode', 'parameter', 'update', 'nonlinear', 'least', 'square', 'approach', 'computationally', 'intensive', 'potentially', 'inaccurate', 'iterative', 'numerical', 'approximation', 'second', 'approach', 'twostep', 'collocation', 'basis', 'expansion', 'exploit', 'approximate', 'ode', 'solution', 'propose', 'ﬁt', 'process', 'datum', 'smoothing', 'method', 'follow', 'second', 'stage', 'minimize', 'leastsquare', 'criterion', 'base', 'ode', 'system', 'estimate', 'ode', 'parameter', 'computational', 'advantage', 'twostep', 'collocation', 'gain', 'much', 'popularity', 'develop', 'ment', 'methodology', 'application', 'brunel', 'dattner', 'klaassen', 'far', 'improve', 'iterative', 'principal', 'diﬀerential', 'analysis', 'ramsay', 'however', 'perfor', 'mance', 'twostep', 'procedure', 'rely', 'heavily', 'smoothing', 'step', 'amount', 'roughness', 'regularization', 'hard', 'control', 'third', 'approach', 'generalized', 'proﬁle', 'procedure', 'ramsay', 'also', 'represent', 'ode', 'solution', 'basis', 'expan', 'twostep', 'collocation', 'method', 'essential', 'diﬀerence', 'inclusion', 'odeinduced', 'penalty', 'control', 'ﬁdelity', 'process', 'ode', 'system', 'basis', 'coeﬃcient', 'ode', 'parameter', 'estimate', 'simultaneously', 'penalize', 'criterion', 'use', 'parameter', 'cascade', 'cao', 'ramsay', 'oretical', 'perspective', 'qi', 'derive', 'upper', 'bound', 'uniform', 'norm', 'diﬀerence', 'true', 'underlie', 'solution', 'approximation', 'prove', 'consistency', 'asymptotic', 'normality', 'estimation', 'procedure', 'recently', 'grow', 'interest', 'highdimensional', 'system', 'number', 'process', 'p', 'large', 'instance', 'highdimensional', 'timecourse', 'gene', 'expression', 'data', 'enable', 'biomedical', 'researcher', 'model', 'regulatory', 'behavior', 'largescale', 'direct', 'graphical', 'network', 'model', 'task', 'call', 'network', 'recovery', 'ode', 'system', 'naturally', 'serve', 'purpose', 'relate', 'dynamic', 'process', 'process', 'system', 'sparse', 'network', 'structure', 'far', 'impose', 'consider', 'highdimensional', 'linear', 'dynamic', 'gene', 'regulatory', 'network', 'identiﬁcation', 'apply', 'smoothly', 'clip', 'absolute', 'deviation', 'fan', 'approach', 'variable', 'selection', 'far', 'relax', 'linear', 'assump', 'tion', 'investigate', 'sparse', 'additive', 'ode', 'model', 'use', 'twostage', 'procedure', 'couple', 'adaptive', 'group', 'deal', 'nonlinear', 'eﬀect', 'propose', 'eﬃcient', 'procedure', 'use', 'integrate', 'form', 'ode', 'bypass', 'numerical', 'diﬃculty', 'derivative', 'estimation', 'adopt', 'group', 'variable', 'selection', 'recently', 'develop', 'matrix', 'factorization', 'base', 'approach', 'ultrahigh', 'dimensional', 'linear', 'ode', 'model', 'rameter', 'estimation', 'variable', 'selection', 'good', 'knowledge', 'exist', 'procedure', 'highdimensional', 'ode', 'model', 'twostage', 'approach', 'exist', 'work', 'assume', 'observation', 'ode', 'system', 'contaminate', 'gaussian', 'noise', 'therefore', 'leastsquare', 'estimation', 'conveniently', 'adopt', 'however', 'nongaussian', 'observation', 'commonly', 'encounter', 'real', 'appli', 'cation', 'example', 'short', 'read', 'count', 'datum', 'rna', 'sequence', 'bisulﬁte', 'sequence', 'datum', 'dna', 'methylation', 'analysis', 'direction', 'change', 'stock', 'price', 'time', 'literature', 'nongaussian', 'datum', 'analysis', 'ode', 'system', 'rare', 'miao', 'develop', 'likelihoodbase', 'parameter', 'estimation', 'inference', 'generalized', 'ode', 'model', 'extension', 'highdimensional', 'ode', 'model', 'however', 'still', 'unknown', 'motivate', 'network', 'recovery', 'task', 'timecourse', 'nongaussian', 'datum', 'paper', 'focus', 'parameter', 'estimation', 'sparse', 'structure', 'identiﬁcation', 'highdimensional', 'system', 'likelihoodbase', 'approach', 'facilitate', 'versatile', 'analysis', 'nongaussian', 'datum', 'assume', 'observation', 'follow', 'distribution', 'exponential', 'family', 'θjt', 'know', 'canonical', 'parameter', 'context', 'generalize', 'linear', 'model', 'mccullagh', 'nelder', 'wood', 'assume', 'loss', 'generality', 'give', 'set', 'discrete', 'time', 'point', 't1', 'tn', 'denote', 'measurement', 'accord', 'latent', 'process', 'θjt', 'time', 'p', 'conditional', 'distribution', 'give', 'admit', 'density', 'function', '−', 'bθjti', 'cyij', 'b', 'c', 'know', 'function', 'know', 'consider', 'nuisance', 'parameter', 'let', 'vector', 'observation', 'latent', 'process', 'θjt', 'correspondingly', 'canonical', 'parameter', 'vector', 'θjt1', 'impose', 'linear', 'structure', 'general', 'investigate', 'work', 'modeling', 'dynamic', 'latent', 'process', 'p', 'highdimensional', 'linear', 'ode', 'system', 'p', 'γjkθkt', 'p', 'article', 'develop', 'new', 'method', 'algorithm', 'parameter', 'estima', 'tion', 'sparse', 'structure', 'identiﬁcation', 'system', 'first', 'propose', 'highdimensional', 'generalize', 'proﬁle', 'method', 'computationally', 'eﬃcient', 'procedure', 'base', 'parameter', 'cascading', 'ramsay', 'cao', 'ramsay', 'solve', 'hierarchical', 'optimization', 'parameter', 'estimation', 'variable', 'selec', 'outer', 'optimization', 'concern', 'ode', 'parameter', 'sparsity', 'regularization', 'perform', 'subject', 'inner', 'optimization', 'latent', 'process', 'expand', 'basis', 'function', 'ﬁtte', 'minimize', 'weighted', 'sum', 'datum', 'ﬁtte', 'ode', 'ﬁdelity', 'crite', 'ria', 'give', 'ode', 'parameter', 'particular', 'regularize', 'structural', 'ode', 'parameter', 'base', 'individual', 'diﬀerential', 'equation', 'mitigate', 'computational', 'burden', 'rameter', 'estimation', 'highdimensional', 'ode', 'system', 'moreover', 'tune', 'parameter', 'involve', 'procedure', 'control', 'balance', 'datum', 'ﬁtte', 'ode', 'ﬁdelity', 'inner', 'optimization', 'regularize', 'sparsity', 'model', 'complexity', 'outer', 'optimization', 'interaction', 'aﬀect', 'overall', 'convergence', 'performance', 'procedure', 'complicated', 'way', 'nonconvexity', 'nature', 'objective', 'function', 'carefully', 'design', 'tuning', 'stop', 'rule', 'accord', 'performance', 'parameter', 'estimation', 'help', 'escape', 'local', 'minima', 'carey', 'ramsay', 'global', 'convergence', 'propose', 'analyze', 'next', 'extend', 'twostep', 'collocation', 'method', 'recently', 'pro', 'pose', 'highdimensional', 'ode', 'model', 'gaussian', 'observation', 'nongaussian', 'setup', 'version', 'correspond', 'vanilla', 'collocation', 'graph', 'reconstruction', 'additive', 'diﬀerential', 'equation', 'grade', 'devel', 'ope', 'likelihoodbase', 'framework', 'eﬃcient', 'computation', 'feasible', 'apply', 'iteratively', 'reweighte', 'least', 'square', 'technique', 'wood', 'finally', 'apply', 'propose', 'method', 'simulated', 'real', 'datum', 'set', 'general', 'proﬁle', 'method', 'eﬃcient', 'twostep', 'collocation', 'method', 'estimate', 'latent', 'process', 'deriva', 'tive', 'structural', 'ode', 'parameter', 'twostep', 'collocation', 'method', 'excel', 'identify', 'sparse', 'structure', 'ode', 'system', 'sum', 'propose', 'method', 'present', 'versatile', 'toolbox', 'parameter', 'estimation', 'sparse', 'structure', 'identiﬁcation', 'system', 'remainder', 'article', 'organize', 'follow', 'proﬁle', 'estimation', 'approach', 'develop', 'section', 'detailed', 'computational', 'procedure', 'global', 'convergence', 'discuss', 'section', 'section', 'extend', 'twostep', 'collocation', 'method', 'model', 'nongaussian', 'observation', 'section', 'compare', 'empirical', 'performance', 'propose', 'method', 'analyze', 'real', 'datum', 'example', 'section', 'dynamical', 'modeling', 'approach', 'section', 'conclude', 'article', 'collect', 'technical', 'detail', 'highdimensional', 'generalize', 'proﬁle', 'section', 'introduce', 'propose', 'approach', 'simultaneous', 'parameter', 'estimation', 'sparse', 'structure', 'identiﬁcation', 'highdimensional', 'linear', 'ode', 'model', 'nongaussian', 'datum', 'penalized', 'likelihood', 'estimation', 'framework', 'denote', 'γp', 'parameter', 'matrix', 'model', 'γjp⊤', '∈', 'p', 'ode', 'parameter', 'γ', 'primary', 'interest', 'order', 'understand', 'network', 'structure', 'call', 'structural', 'parameter', 'hereafter', 'hand', 'latent', 'process', 'θj', '’s', 'treat', 'nuisance', 'parameter', 'denote', 'θjti', 'observation', 'canonical', 'parameter', 'latent', 'process', 'time', 'respectively', 'proﬁle', 'scheme', 'ramsay', 'intermediate', 'ﬁt', 'latent', 'process', 'θpt', 'minimize', 'following', 'penalize', 'likelihood', 'criterion', 'b', 'p', 'p', 'yijθjti', 'bθjti', 'p', '−', 'γjkθkt', 'j1', 'z', 'likelihood', 'part', 'measure', 'ﬁdelity', 'data', 'ﬁdelity', 'part', 'measure', 'j1', 'extent', 'latent', 'process', 'fail', 'satisfy', 'ode', 'system', 'tuning', 'parameter', 'λθ', 'control', 'amount', 'regularization', 'furthermore', 'plug', 'estimate', 'structural', 'parameter', 'obtain', 'minimize', 'datum', 'ﬁtte', 'criterion', 'respect', 'γ', 'j1', 'p', 'θjti', 'b', 'θjti', 'generalize', 'proﬁle', 'procedure', 'proceed', 'iteratively', 'nondecrease', 'sequence', 'b', 'λθ', 'certain', 'rule', 'ﬁtted', 'process', 'adhere', 'ode', 'identiﬁable', 'issue', 'asymptotic', 'behavior', 'estimation', 'procedure', 'study', 'ramsay', 'generalize', 'proﬁle', 'method', 'provide', 'computationally', 'eﬃcient', 'treat', 'ment', 'challenge', 'parameter', 'estimation', 'handle', 'relatively', 'small', 'scale', 'model', 'hand', 'pdimensional', 'linear', 'ode', 'system', 'p', 'ode', 'parameter', 'estimate', 'far', 'approximate', 'latent', 'process', 'θjt', 'basis', 'expansion', 'c⊤', 'mjdimensional', 'basis', 'vector', 'cj', 'coeﬃcient', 'vector', 'become', 'np', 'hjti', '−', 'bc⊤', 'j1', 'p', 'cid9', 'c⊤', 'γj0', '−', 'p', 'x', 'total', 'number', 'nuisance', 'parameter', 'p', 'j1', 'mj', 'huge', 'therefore', 'rect', 'application', 'standard', 'generalize', 'proﬁle', 'procedure', 'parameter', 'estimation', 'p', 'highdimensional', 'computationally', 'demand', 'hand', 'structural', 'parameter', 'obtain', 'indeed', 'infer', 'interaction', 'network', 'latent', 'process', 'sense', 'nonzero', 'γjk', 'imply', 'θkt', 'eﬀect', 'change', 'θjt', 'well', 'interpretation', 'avoid', 'potential', 'overﬁtting', 'reason', 'able', 'introduce', 'sparsity', 'structural', 'parameter', 'example', 'lasso', 'variant', 'tibshirani', 'yuan', 'zou', 'smoothly', 'clip', 'absolute', 'deviation', 'fan', 'minimax', 'concave', 'extensively', 'study', 'use', 'recover', 'probabilistic', 'graphical', 'structure', 'yuan', 'fan', 'voorman', 'address', 'computational', 'issue', 'ﬁrst', 'notice', 'data', 'ﬁdelity', 'term', 'penalize', 'criterion', 'decompose', 'sum', 'likelihood', 'p', 'dividual', 'process', 'meanwhile', 'penalty', 'term', 'squared', 'l2', 'norm', 'diﬀerential', 'equation', 'admit', 'decomposable', 'property', 'therefore', 'propose', 'regularize', 'estimate', 'θj', 'correspond', 'diﬀerential', 'equation', 'speciﬁcally', 'estimate', 'θj', 'give', 'j', 'recent', 'update', 'obtain', 'θjt', 'γj', 'minimize', '−', 'bθjti', 'λθj', '′', '−', 'θ', 'γjkθkt', 'dt', 'p', 'k1', 'p', 'simplicity', 'use', 'tuning', 'parameter', 'individual', 'sub', 'problem', 'λθ', 'p', 'optimize', 'gj', 'involve', 'structural', 'parameter', 'vector', 'γj', 'hence', 'computational', 'complexity', 'greatly', 'reduce', 'beneﬁt', 'use', 'justiﬁe', 'aspect', 'first', 'computationally', 'feasible', 'estimate', 'large', 'number', 'ode', 'parameter', 'jointly', 'directly', 'apply', 'original', 'generalized', 'proﬁle', 'criterion', 'highdimensional', 'ode', 'system', 'new', 'formulation', 'decouple', 'dependency', 'matrix', 'γ', 'individual', 'depen', 'dencie', 'θjt', 'γj', 'vector', 'γj', 'second', 'perspective', 'penalized', 'estimation', 'improve', 'estimation', 'latent', 'process', 'ode', 'structural', 'parameter', 'employ', 'diﬀerential', 'equation', 'regularize', 'datum', 'smooth', 'remark', 'potential', 'risk', 'employ', 'instead', 'estimate', 'latent', 'process', 'note', 'aggregate', 'diﬀerential', 'equation', 'update', 'latent', 'process', 'altogether', 'estimate', 'follow', 'ode', 'system', 'jointly', 'contrast', 'method', 'use', 'single', 'diﬀerential', 'equation', 'regularize', 'estimation', 'latent', 'process', 'tuning', 'parameter', 'increase', 'parallel', 'updating', 'procedure', 'p', 'expect', 'achieve', 'approximation', 'marginal', 'way', 'joint', 'estimation', 'simulation', 'example', 'introduce', 'section', 'supplementary', 'material', 'show', 'approximation', 'perform', 'reasonably', 'well', 'joint', 'method', 'accurate', 'estimate', 'ode', 'parameter', 'next', 'induce', 'sparsity', 'structural', 'parameter', 'matrix', 'estimate', 'γj', 'mini', 'mize', '−', 'b', 'θjti', 'penalty', 'function', 'induce', 'sparsity', 'structural', 'parameter', 'diﬀerential', 'equation', 'also', 'assume', 'simplicity', 'λγj', 'p', 'ﬁtte', 'structural', 'parameter', 'vector', 'γj', 'γj', 'imply', 'b', 'b', 'say', 'latent', 'process', 'impact', 'element', 'correspond', 'process', 'inﬂuence', 'amount', 'sparsity', 'regularization', 'typically', 'determine', 'information', 'criterion', 'type', 'principle', 'adopt', 'parameter', 'estimation', 'approach', 'new', 'proﬁle', 'estimation', 'procedure', 'highdimensional', 'linear', 'system', 'con', 'sist', 'objective', 'function', 'refer', 'inner', 'outer', 'criterion', 'respectively', 'multicriterion', 'optimization', 'problem', 'challenge', 'nonconvexity', 'nondiﬀerentiability', 'speciﬁcally', 'approximate', 'latent', 'process', 'basis', 'expansion', 'inner', 'optimization', 'basis', 'coeﬃcient', 'solve', 'ciently', 'method', 'however', 'dependence', 'θjt', 'γj', 'γj', 'complicated', 'general', 'nonlinear', 'lead', 'nonconvexity', 'moreover', 'sparsityinduce', 'penalty', 'nondiﬀerentiable', 'make', 'gaussnewton', 'scheme', 'adopt', 'ramsay', 'invalid', 'scenario', 'recent', 'advance', 'derivativefree', 'optimization', 'algorithm', 'provide', 'viable', 'solution', 'nevertheless', 'spirit', 'joint', 'optimization', 'algorithm', 'design', 'general', 'purpose', 'thus', 'tailor', 'speciﬁc', 'problem', 'contrast', 'proﬁle', 'procedure', 'enjoy', 'estimation', 'eﬃciency', 'also', 'algo', 'rithmic', 'eﬃciency', 'use', 'analytical', 'expression', 'derivative', 'computational', 'detail', 'present', 'next', 'section', 'brief', 'obtain', 'estimate', 'θjt', 'γj', 'give', 'structural', 'parameter', 'linearize', 'likelihood', 'component', 'b', 'late', 'outer', 'optimization', 'parameter', 'estimation', 'problem', 'penalize', 'generalize', 'linear', 'model', 'therefore', 'structural', 'parameter', 'readily', 'update', 'iterative', 'reweighte', 'leastsquare', 'irl', 'iterative', 'scheme', 'inner', 'outer', 'op', 'timization', 'proﬁle', 'procedure', 'provide', 'ode', 'parameter', 'estimate', 'latent', 'process', 'ﬁts', 'identiﬁes', 'sparse', 'structure', 'model', 'computation', 'section', 'provide', 'computational', 'detail', 'proﬁle', 'procedure', 'high', 'dimensional', 'linear', 'ode', 'analyze', 'global', 'convergence', 'minimizing', 'criterion', 'refer', 'inner', 'outer', 'optimization', 'structural', 'parameter', 'γp', 'primary', 'interest', 'latent', 'process', 'ﬁts', 'inner', 'optimiza', 'tion', 'regard', 'nuisance', 'parameter', 'proﬁle', 'scheme', 'γj', 'change', 'minimize', 'outer', 'latent', 'process', 'ﬁts', 'update', 'solve', 'inner', 'criterion', 'gj', 'detail', 'provide', 'addition', 'tune', 'parameter', 'involve', 'proﬁle', 'procedure', 'complex', 'interaction', 'aﬀect', 'overall', 'algo', 'rithmic', 'performance', 'nonconvexity', 'optimization', 'following', 'split', 'discussion', 'inner', 'outer', 'part', 'discuss', 'practical', 'strategy', 'tune', 'parameter', 'selection', 'global', 'convergence', 'propose', 'highdimensional', 'linear', 'nongaussian', 'datum', 'input', 'observation', 'p', 'initial', 'ode', 'paramter', 'ﬁxe', 'tuning', 'parameter', 'λθ', 'λγ', 'output', 'estimate', 'ode', 'parameter', 'repeat', 'b', 'current', 'estimate', 'b', 'p', 'update', 'γj', 'proﬁle', 'procedure', 'b', 'repeat', 'give', 'current', 'γj', 'obtain', 'basis', 'coeﬃcient', 'estimate', 'c∗', 'γj', 'latent', 'process', 'inner', 'optimization', 'e', 'e', 'apply', 'basis', 'expansion', 'update', 'γj', 'minimize', 'penalized', 'reweighte', 'least', 'square', 'γj', 'converge', 'set', 'γj', 'end', 'e', 'estimate', 'ode', 'parameter', 'e', 'converge', 'inner', 'optimization', 'inner', 'procedure', 'aim', 'ﬁnde', 'accurate', 'estimate', 'latent', 'process', 'give', 'structural', 'parameter', 'γ', 'similar', 'twostep', 'collocation', 'method', 'generalized', 'proﬁling', 'ramsay', 'represent', 'latent', 'process', 'basis', 'suppose', 'set', 'basis', 'function', 'process', 'θjt', 'c⊤', 'choice', 'basis', 'function', 'include', 'polynomial', 'truncate', 'power', 'function', 'spline', 'numerical', 'study', 'use', 'bspline', 'numerical', 'stability', 'excellent', 'empirical', 'performance', 'notation', 'simplicity', 'use', 'basis', 'ht', 'latent', 'process', 'critical', 'optimization', 'basis', 'coeﬃcient', 'cj', 'p', 'often', 'direct', 'concern', 'thus', 'consider', 'nuisance', 'parameter', 'observe', 'convex', 'respect', 'basis', 'coeﬃcient', 'cj', 'apply', 'scheme', 'directly', 'hessian', 'invertible', 'start', 'initial', 'guess', 'iteratively', 'obtain', 'r', '≥', 'analytical', 'expression', 'derivative', 'involve', 'updating', 'rule', 'give', 'outer', 'optimization', 'outer', 'optimization', 'design', 'update', 'γj', 'regularize', 'likelihood', 'objective', 'function', 'denote', 'c∗', 'inner', 'optimization', 'give', 'current', 'γj', 'observe', 'dependence', 'c∗', 'optimal', 'basis', 'coeﬃcient', 'θjt', 'γj', 'obtain', 'γj', 'implicit', 'possibly', 'complicate', 'propose', 'linearize', 'likelihood', 'component', 'transform', 'optimization', 'ﬁnde', 'maximum', 'likelihood', 'estimate', 'generalize', 'linear', 'model', 'solution', 'readily', 'obtain', 'iteratively', 'reweighte', 'least', 'square', 'irl', 'see', 'wood', 'detail', 'let', 'γj', 'recent', 'update', 'γj', 'first', 'linearize', 'c∗', 'γj', 'γj', 'e', 'c∗', 'c∗', 'γj', '∂c∗', 'γj', 'γj', 'e', 'e', 'derivative', '∂c∗', 'explicitly', 'derive', 'use', 'implicit', 'function', 'theorem', 'hence', 'θjt', 'γj', 'basis', 'expansion', 'form', 'approximate', 'linear', 'function', 'γj', 'result', 'outer', 'objective', 'function', 'become', 'penalized', 'likelihood', 'function', 'b', 'generalize', 'linear', 'model', 'second', 'apply', 'irl', 'update', 'estimate', 'γj', 'let', 'θjt', 'γj', 'latent', 'process', 'ﬁt', 'give', 'structural', 'parameter', 'γj', 'base', 'theory', 'generalize', 'linear', 'model', 'observation', 'accord', 'latent', 'process', 'e', 'b', 'property', 'varyj', 'θjt', 'θjt', 'admit', 'vjtaφ', 'e', 'θjtaφ', 'e', 'function', 'b', 'parameter', 'φ', 'follow', 'exponential', 'family', 'speciﬁcation', 'write', '′', 'e', 'θjti', 'e', 'µjti', 'wij', 'θjti', 'e', 'vjti', 'irl', 'e', 'apply', 'quadratic', 'approximation', 'loglikelihood', 'e', 'e', 'e', 'e', 'e', 'e', 'θj', '−yij', 'θjti', 'θjti', 'γj', 'e', 'cij', 'b', 'uij', 'wij', 'cij', 'independent', 'e', 'e', 'b', 'θjti', 'conjunction', 'linear', 'b', 'approximation', 'θjti', 'γj', 'amount', 'solve', 'penalized', 'linear', 'least', 'square', 'update', 'e', 'e', 'e', 'e', 'e', 'estimate', 'structural', 'parameter', 'γj', 'eﬃcient', 'algorithm', 'available', 'diﬀerent', 'sparsity', 'penalty', 'choice', 'pen', 'tuning', 'parameter', 'selection', 'tune', 'parameter', 'involve', 'proﬁle', 'procedure', 'jointly', 'aﬀect', 'algorithmic', 'performance', 'hand', 'λθ', 'inner', 'optimization', 'control', 'amount', 'regularization', 'regard', 'diﬀerential', 'equation', 'deﬁne', 'aggregated', 'ﬁdelity', 'criterion', 'p', '−', 'z', 'γjkθkt', 'p', 'small', 'make', 'optimize', 'hjγj', 'respect', 'γj', 'robust', 'initial', 'guess', 'yield', 'bad', 'approximation', 'ode', 'solution', 'large', 'give', 'rise', 'diﬃcult', 'mization', 'problem', 'usually', 'convex', 'many', 'local', 'optima', 'ramsay', 'qi', 'carey', 'ramsay', 'hand', 'λγ', 'outer', 'optimization', 'induce', 'sparse', 'network', 'structure', 'latent', 'process', 'well', 'interpretation', 'exist', 'method', 'information', 'criterion', 'adopt', 'tuning', 'base', 'discussion', 'propose', 'λγ', 'outer', 'optimization', 'ﬁrst', 'iteratively', 'select', 'proper', 'λθ', 'inner', 'optimization', 'determine', 'good', 'λγ', 'information', 'criterion', 'detail', 'suppose', 'choose', 'λγ', 'sequence', 'candidate', 'value', 'initialize', 'λθ', 'small', 'value', 'moderately', 'increase', 'iterative', 'scheme', 'iteration', 'γ', 'repeatedly', 'estimate', 'current', 'use', 'initial', 'value', 'ﬁtte', 'procedure', 'next', 'large', 'λθ', 'iterative', 'scheme', 'stop', 'estimate', 'ode', 'parameter', 'converge', 'thus', 'decide', 'change', 'estimate', 'ode', 'parameter', 'small', 'moderate', 'increase', 'therefore', 'conservatively', 'increase', 'sequence', 'estimate', 'γ', 'much', 'likely', 'proper', 'initialization', 'next', 'iteration', 'detail', 'iterative', 'select', 'scheme', 'λθ', 'give', 'ﬁxed', 'λγ', 'follow', 'start', 'small', 'positive', 'λ0', 'choose', 'initial', 'incremental', 'factor', 'uth', 'iteration', 'obtain', 'ﬁtted', 'latent', 'process', 'γu', 'proﬁle', 'procedure', 'evaluate', 'ode', 'ﬁdelity', 'base', 'estimate', 'absolute', 'percentage', 'change', 'ﬁdelity', 'threshold', 'constant', 'update', '×', 'b', 'otherwise', 'need', 'downsize', 'incremental', 'factor', 'example', 'set', '∆u−12', 'ensure', 'ode', 'ﬁdelity', 'vary', 'little', 'iteration', 'successive', 'parameter', 'estimate', 'close', 'enough', 'stop', 'iteration', 'otherwise', 'repeat', 'previous', 'step', 'iterative', 'tuning', 'strategy', 'treat', 'λθ', 'function', 'λγ', 'hence', 'select', 'ﬁxe', 'λγ', 'sequence', 'candidate', 'value', 'evaluate', 'following', 'bic', 'choose', 'good', 'λγ', 'bicλγ', '−', 'p', 'θjti', 'bθjti', 'j1', 'θjt', 'emphasize', 'dependence', 'denote', 'number', 'nonzero', 'element', 'resultant', 'parameter', 'b', 'γλγ', 'global', 'convergence', 'suppose', 'estimate', 'latent', 'process', 'θjt', 'γj', 'inner', 'optimization', 'smooth', 'function', 'γj', 'p', 'let', 'p', 'j1', 'objective', 'function', 'outer', 'optimization', 'give', 'tuning', 'parameter', 'b', 'p', 'essentially', 'block', 'coordinate', 'descent', 'method', 'minimize', 'iteratively', 'update', 'write', 'ℓjγj', 'likelihood', 'term', 'penλγ', 'γj', 'assume', 'convex', 'describe', 'section', 'outer', 'optimization', 'equivalent', 'update', 'γj', 'descent', 'direction', 'djγj', 'solution', 'penλγ', 'γj', 'gradient', 'n', 'θjti', 'γj', 'θjti', 'γj', 'b', 'positive', 'deﬁnite', 'matrix', 'approximate', 'hessian', '\uf8f3', 'follow', 'establish', 'global', 'convergence', 'actual', 'value', 'hessian', 'identical', 'expect', 'value', 'canonical', 'link', 'mccullagh', 'nelder', 'irl', 'method', 'describe', 'section', 'remain', 'hessian', 'replace', 'expect', 'hessian', 'follow', 'supplementary', 'material', 'okdjγjk2', 'cid21', 'algebra', 'yield', 'qjγj', 'n', 'γj', '∂2', 'θjti', 'θ∗', 'true', 'latent', 'process', 'matrix', 'positive', 'deﬁnite', 'follow', 'θjti', 'γj', 'suﬃciently', 'close', 'truth', 'continuous', 'provide', 'decrease', 'iteration', 'eventually', 'converge', 'lowerbounded', 'moreover', 'sequence', 'descent', 'direction', 'converge', 'accord', 'theorem', 'cluster', 'point', 'iterative', 'estimate', 'exhibit', 'exact', 'descent', 'direction', 'imply', 'indeed', 'stationary', 'point', 'finally', 'remark', 'analysis', 'directly', 'apply', 'nonconvex', 'penλγ', 'scad', 'penalty', 'however', 'nonconvex', 'penalty', 'numerically', 'approximate', 'local', 'linear', 'quadratic', 'function', 'fan', 'anticipate', 'similar', 'convergence', 'result', 'involve', 'technical', 'detail', 'pursue', 'paper', 'twostep', 'collocation', 'method', 'nongaussian', 'datum', 'collocation', 'method', 'exploit', 'parameter', 'estimation', 'network', 'construction', 'various', 'ode', 'model', 'section', 'extend', 'popular', 'twostep', 'col', 'location', 'method', 'highdimensional', 'linear', 'nongaussian', 'observation', 'large', 'literature', 'collocation', 'ramsay', 'dattner', 'klaassen', 'consider', 'linear', 'case', 'recently', 'additive', 'structure', 'investigate', 'exist', 'method', 'propose', 'gaussian', 'observation', 'adopt', 'least', 'square', 'loss', 'function', 'estimation', 'following', 'present', 'version', 'twostep', 'collocation', 'method', 'highdimensional', 'ode', 'model', 'gaussian', 'observation', 'vanilla', 'collocation', 'base', 'extension', 'graph', 'reconstruction', 'additive', 'diﬀerential', 'equation', 'grade', 'vanilla', 'twostep', 'ﬁts', 'smoothing', 'estimate', 'θt', 'latent', 'process', 'maximum', 'likelihood', 'estimation', 'obtain', 'structural', 'parameter', 'b', 'estimate', 'process', 'derivative', 'plug', 'procedure', 'solve', 'follow', 'optimization', 'problem', 'θjt', 'i1', 'p', '−', '−', 'γjk', 'θkt', 'k1', 'yijθti', '−', 'bθti', 'p', 'h', 'proper', 'reproducing', 'kernel', 'hilbert', 'space', 'exponential', 'family', 'smoothing', 'spline', 'adopt', 'performance', 'vanilla', 'twostep', 'collocation', 'method', 'rely', 'estimation', 'accuracy', 'θjt', 'derivative', 'statistical', 'convergence', 'establish', 'practice', 'hard', 'tune', 'smoothing', 'procedure', 'achieve', 'optimality', 'brunel', 'extension', 'base', 'grade', 'method', 'avoid', 'derivative', 'estimation', 'issue', 'vanilla', 'collocation', 'method', 'instead', 'consider', 'ode', 'ﬁdelity', 'term', 'integral', 'form', 'similar', 'vanilla', 'twostep', 'method', 'grade', 'method', 'ﬁrst', 'obtain', 'smooth', 'estimate', 'latent', 'process', 'observation', 'use', 'integrate', 'basis', 'function', 'p', 'express', 'r', 'b', 'γjk', 'θkt', 'accord', 'integrated', 'diﬀerential', 'equation', 'finally', 'solve', 'follow', 'optimiza', 'e', 'b', 'tion', 'problem', 'obtain', 'b', 'grade', 'method', 'initially', 'develop', 'ode', 'model', 'i1', 'e', 'e', 'naturally', 'adapt', 'linear', 'case', 'use', 'integrate', 'form', 'ode', 'facilitate', 'vestigate', 'asymptotic', 'behavior', 'estimator', 'enhance', 'robustness', 'smoothing', 'eﬀect', 'ﬁrst', 'step', 'dattner', 'klaassen', 'twostep', 'collocation', 'method', 'propose', 'section', 'involve', 'maximize', 'likeli', 'hood', 'function', 'exponential', 'family', 'distribution', 'eﬃciently', 'solve', 'iteratively', 'reweighte', 'least', 'square', 'technique', 'section', 'compare', 'twostep', 'collocation', 'method', 'highdimensional', 'generalize', 'proﬁle', 'hdgp', 'procedure', 'section', 'process', 'derivative', 'estimation', 'hdgp', 'balance', 'datum', 'ode', 'ﬁdelitie', 'usually', 'result', 'reasonable', 'ﬁts', 'accurate', 'parameter', 'estimate', 'accurate', 'derivative', 'sparse', 'structure', 'identiﬁcation', 'grade', 'achieve', 'good', 'accuracy', 'consistent', 'motivation', 'grade', 'network', 'reconstruction', 'summary', 'hdgp', 'well', 'choice', 'process', 'ﬁtting', 'ode', 'parameter', 'estimation', 'grade', 'excel', 'sparse', 'structure', 'identiﬁcation', 'simulation', 'study', 'section', 'compare', 'empirical', 'performance', 'dynamical', 'modeling', 'approach', 'highdimensional', 'generalize', 'proﬁle', 'hdgp', 'procedure', 'twostep', 'collocation', 'method', 'propose', 'section', 'namely', 'grade', 'vanilla', 'twostep', 'method', 'respectively', 'consider', 'ode', 'system', 'study', 'consist', 'process', 'pair', '2kπ', 'θ′', '2kt', '2kπ', 'θ2k−1', '\uf8f4\uf8f2', 'clear', 'ode', 'solution', 'take', 'form', 'sine', 'cosine', 'function', 'vary', '\uf8f4\uf8f3', 'e', 'frequency', 'interaction', 'exist', 'pair', 'kth', 'pair', 'initial', 'state', 'sinyk', 'cosyk', 'sample', 'latent', 'process', 'describe', 'ordinary', 'diﬀerential', 'equation', 'use', 'generate', 'observation', 'gaussian', 'distribution', 'denote', 'tn', 'time', 'point', 'gaussian', 'distribution', 'sample', 'know', 'variance', 'sample', 'size', 'process', 'set', 'poisson', 'distribution', 'draw', 'sample', 'poissonλjti', 'intensity', 'process', 'expθjt', 'distribution', 'sample', 'generate', 'probability', 'success', 'expθjt', 'sample', 'size', 'poisson', 'distribution', 'large', 'gaussian', 'case', 'observation', 'generally', 'require', 'ensure', 'reasonable', 'estimate', 'accord', 'theory', 'generalize', 'linear', 'model', 'use', 'smooth', 'spline', 'ﬁtte', 'initialization', 'proﬁle', 'procedure', 'also', 'correspond', 'ﬁrst', 'stage', 'twostep', 'collocation', 'method', 'order', 'bspline', 'function', 'hdgp', 'set', 'number', 'knot', 'half', 'time', 'point', 'hdgp', 'grade', 'require', 'numerical', 'integration', 'evaluate', 'ode', 'ﬁdelity', 'integrate', 'basis', 'representation', 'respectively', 'sparsity', 'penalty', 'choice', 'consider', 'γj', 'λγkγjk1', 'scad', 'penalty', 'penλγ', 'γj', 'p', 'pλγ', 'γjk', 'function', 'pλ', 'deﬁne', '−u2', '−', '2aλu', '−', '\uf8f1', 'suggested', 'value', 'accord', 'fan', 'algorithmic', 'convergence', 'demonstrate', 'diﬀerence', 'successive', 'parameter', 'estimate', 'small', 'enough', 'work', 'well', 'twostep', 'collocation', 'method', 'however', 'complex', 'interaction', 'inner', 'outer', 'optimization', 'hdgp', 'yield', 'sparse', 'rameter', 'estimate', 'declaration', 'convergence', 'address', 'numerical', 'issue', 'manually', 'set', 'parameter', 'estimate', 'constant', 'threshold', 'base', 'empirical', 'study', 'recommend', 'value', 'threshold', 'rootmeansquare', 'initial', 'estimate', 'γ', 'multiply', 'factor', 'simulation', 'result', 'evaluate', 'use', 'type', 'criterion', 'ﬁrst', 'criterion', 'b', 'concern', 'process', 'derivative', 'estimate', 'evaluate', 'mean', 'squared', 'error', 'mse', 'θ′t', 'mse', 'θ′t', 'mse', 'b', 'j1', 'p', 'i1', 'j1', 'b', 'θjti', '−', 'θjti', 'b', 'θ′', 'θ′', 'jti', 'second', 'measure', 'well', 'structural', 'parameter', 'estimate', 'rootmean', 'square', 'error', 'rmse', 'third', 'true', 'positive', 'rate', 'tpr', 'false', 'positive', 'rate', 'fpr', 'use', 'quantify', 'well', 'sparse', 'structure', 'identiﬁed', 'refer', 'nonzero', 'structural', 'parameter', 'positive', 'case', 'otherwise', 'negative', 'case', 'table', 'display', 'average', 'evaluation', 'repeat', 'experiment', 'use', 'penalty', 'true', 'positive', 'rate', 'omit', 'equal', 'method', 'simulation', 'setup', 'increase', 'number', 'observation', 'always', 'lead', 'reduce', 'error', 'tight', 'conﬁdence', 'interval', 'term', 'process', 'ﬁt', 'parameter', 'estimation', 'process', 'derivative', 'ﬁtte', 'smoothing', 'spline', 'method', 'table', 'performance', 'hdgp', 'grade', 'vanilla', 'twostep', 'method', 'evaluate', 'base', 'θ′t', 'derivative', 'estimate', 'mse', 'process', 'estimate', 'mse', 'rameter', 'estimation', 'rmse', 'sparse', 'structure', 'estimate', 'fpr', 'conﬁdence', 'b', 'interval', 'give', 'parenthesis', 'method', 'mse', 'mse', 'u', 'g', 'p', 'l', 'l', 'r', 'e', 'b', 'hdgp', 'grade', 'vanilla', 'hdgp', 'grade', 'vanilla', 'hdgp', 'grade', 'vanilla', 'hdgp', 'grade', 'vanilla', 'hdgp', 'grade', 'vanilla', 'hdgp', 'grade', 'rmse', 'γ', 'fpr', 'ﬁrst', 'stage', 'twostep', 'collocation', 'method', 'often', 'produce', 'accurate', 'estimate', 'latent', 'process', 'less', 'eﬃcient', 'derivative', 'ﬁtte', 'contrast', 'inner', 'optimization', 'hdgp', 'balance', 'datum', 'ode', 'ﬁdelitie', 'result', 'reasonable', 'process', 'ﬁtte', 'improve', 'derivative', 'ﬁtte', 'parameter', 'estimation', 'hdgp', 'deliver', 'small', 'error', 'accurate', 'derivative', 'interestingly', 'grade', 'much', 'bad', 'performance', 'criterion', 'partial', 'reason', 'grade', 'use', 'structural', 'parameter', 'integrated', 'basis', 'representation', 'instead', 'explicit', 'form', 'diﬀerential', 'equation', 'sparse', 'structure', 'identiﬁcation', 'grade', 'achieve', 'good', 'accuracy', 'discover', 'nonzero', 'structural', 'parameter', 'false', 'positive', 'consistent', 'motivation', 'grade', 'network', 'reconstruction', 'summary', 'hdgp', 'well', 'choice', 'process', 'ﬁtting', 'ode', 'parameter', 'estimation', 'grade', 'excel', 'sparse', 'structure', 'identiﬁcation', 'next', 'investigate', 'eﬀect', 'diﬀerent', 'noise', 'level', 'choice', 'sparse', 'penalty', 'gaussian', 'setup', 'observation', 'process', 'signal', 'tonoise', 'ratio', 'snr', 'deﬁne', 'ratio', 'sample', 'standard', 'deviation', 'i1', 'noise', 'standard', 'deviation', 'σ', 'set', 'signaltonoise', 'ratio', 'inﬁnity', 'inﬁnite', 'ratio', 'mean', 'noise', 'add', 'scad', 'penalty', 'consider', 'figure', 'present', 'performance', 'evaluation', 'repeat', 'experiment', 'general', 'method', 'perform', 'well', 'criterion', 'noise', 'ratio', 'increase', 'top', 'row', 'figure', 'correspond', 'penalty', 'provide', 'consistent', 'result', 'table', 'indicate', 'hdgp', 'comparable', 'process', 'ﬁt', 'well', 'derivative', 'estimation', 'especially', 'noise', 'level', 'low', 'moreover', 'hdgp', 'perform', 'good', 'estimate', 'structural', 'parameter', 'vanilla', 'twostep', 'method', 'also', 'provide', 'satisfactory', 'result', 'contrast', 'even', 'noise', 'bias', 'ode', 'parameter', 'estimate', 'grade', 'still', 'large', 'rmse', 'almost', 'constant', 'sparse', 'structure', 'identiﬁcation', 'grade', 'outperform', 'method', 'wide', 'range', 'noise', 'level', 'hdgp', 'vanilla', 'twostep', 'method', 'high', 'accuracy', 'signal', 'level', 'high', 'bottom', 'row', 'figure', 'display', 'simulation', 'result', 'scad', 'penalty', 'use', 'induce', 'sparsity', 'ode', 'system', 'compare', 'result', 'overall', 'performance', 'process', 'derivative', 'ode', 'parameter', 'estimation', 'process', 'derivative', 'nonzero', 'parameter', 'fpr', 'e', 'e', 'inf', 'snr', 'process', 'e', 'r', 'inf', 'inf', 'lasso', 'penalty', 'derivative', 'nonzero', 'parameter', 'fpr', 'e', 'r', 'inf', 'inf', 'inf', 'penalty', 'figure', 'performance', 'hdgp', 'purple', 'solid', 'grade', 'blue', 'dash', 'vanilla', 'twostep', 'method', 'yellow', 'dot', 'gaussian', 'observation', 'diﬀerent', 'noise', 'level', 'box', 'identify', 'quartile', 'criterion', 'repeat', 'experiment', 'top', 'bottom', 'row', 'correspond', 'scad', 'penalty', 'respectively', 'improve', 'mainly', 'unbiasedness', 'property', 'interestingly', 'poor', 'performance', 'grade', 'parameter', 'estimation', 'greatly', 'enhance', 'deliver', 'comparable', 'estimation', 'result', 'due', 'oracle', 'property', 'enjoy', 'recommend', 'well', 'performance', 'parameter', 'estimation', 'real', 'datum', 'analysis', 'trend', 'datum', 'analysis', 'google', 'trend', 'provide', 'publicly', 'accessible', 'online', 'portal', 'analyze', 'popularity', 'search', 'query', 'study', 'attempt', 'apply', 'method', 'model', 'interaction', 'number', 'trend', 'keyword', 'recent', 'pandemic', 'covid19', 'table', 'list', 'keyword', 'cluster', 'category', 'ﬁrst', 'category', 'consist', 'ﬁve', 'keyword', 'speciﬁc', 'terminology', 'mask', 'quarantine', 'second', 'category', 'include', 'country', 'conﬁrme', 'case', 'also', 'district', 'last', 'continent', 'report', 'conﬁrme', 'case', 'remoteness', 'sparse', 'population', 'also', 'include', 'last', 'category', 'noise', 'keyword', 'apparent', 'relationship', 'pandemic', 'table', 'category', 'keyword', 'select', 'analysis', 'datum', 'category', 'related', 'mask', 'quarantine', 'vaccine', 'word', 'country', 'district', 'word', 'noise', 'word', 'desert', 'dog', 'game', 'word', 'trend', 'datum', 'use', 'study', 'cover', 'range', 'septem', 'ber', 'popularity', 'measure', 'integer', 'index', 'calculate', 'normalize', 'round', 'count', 'unbiased', 'searching', 'request', 'sample', 'observe', 'daily', 'trend', 'index', 'several', 'sharp', 'peak', 'see', 'figure', 'illus', 'trative', 'example', 'direct', 'modeling', 'mean', 'trend', 'result', 'abrupt', 'high', 'value', 'peak', 'relatively', 'ﬂat', 'region', 'therefore', 'appropriate', 'assume', 'index', 'follow', 'poisson', 'distribution', 'apply', 'propose', 'method', 'model', 'latent', 'process', 'intensity', 'parameter', 'ode', 'figure', 'daily', 'trend', 'index', 'vaccine', 'well', 'exhibit', 'diﬀerent', 'stage', 'pandemic', 'consider', 'time', 'period', 'period', 'method', 'apply', 'ﬁt', 'trend', 'process', 'series', 'sparsity', 'parameter', 'figure', 'display', 'network', 'diﬀerent', 'sparsity', 'parameter', 'ﬁrst', 'period', 'high', 'degree', 'network', 'covid19', 'pandemic', 'quarantine', 'selfquarantine', 'enact', 'multiple', 'governmental', 'actor', 'prevent', 'rapid', 'spread', 'virus', 'surprise', 'become', 'topranke', 'trend', 'keyword', 'figure', 'vaccine', 'stand', 'virus', 'name', 'country', 'ﬁrst', 'case', 'identiﬁed', 'immunization', 'method', 'top', 'keyword', 'represent', 'major', 'trend', 'focus', 'early', 'stage', 'pandemic', 'figure', 'desert', 'dog', 'desert', 'dog', 'λγ', '10−1', 'figure', 'recover', 'network', 'trending', 'keyword', 'ﬁrst', 'period', 'diﬀerent', 'value', 'sparsity', 'parameter', 'aﬀecte', 'country', 'involve', 'sparsity', 'parameter', 'decrease', 'contrast', 'noise', 'keyword', 'isolate', 'network', 'indicate', 'connection', 'trend', 'topic', 'interestingly', 'investigate', 'table', 'top', 'keyword', 'recover', 'network', 'period', 'keyword', 'high', 'degree', 'boldface', 'period', 'keyword', 'quarantine', 'coronavirus', 'mask', 'evolution', 'network', 'structure', 'trend', 'keyword', 'progression', 'covid19', 'pandemic', 'table', 'list', 'top', 'keyword', 'time', 'period', 'keyword', 'high', 'degree', 'boldface', 'ﬁrst', 'period', 'second', 'emerge', 'new', 'top', 'word', 'accord', 'report', 'overtake', 'country', 'report', 'death', 'announce', 'table', 'company', 'select', 'category', 'stock', 'price', 'datum', 'analysis', 'group', 'category', 'company', 'information', 'technology', 'adobe', 'apple', 'salesforce', 'zoom', 'electric', 'vehicle', 'pharmaceutical', 'novartis', 'pﬁzer', 'consumer', 'service', 'retail', 'online', 'retail', 'shopping', 'amazon', 'well', 'buy', 'target', 'air', 'line', 'energy', 'chevron', 'conocophillip', 'national', 'turn', 'third', 'period', 'drop', 'top', 'list', 'country', 'successfully', 'slow', 'domestic', 'infection', 'reduce', 'daily', 'new', 'case', 'signiﬁcantly', 'preventive', 'measure', 'include', 'wear', 'face', 'mask', 'public', 'advise', 'several', 'promising', 'vaccine', 'develop', 'mask', 'vaccine', 'top', 'trending', 'keyword', 'analysis', 'stock', 'price', 'change', 'direction', 'year', 'stock', 'market', 'experience', 'enormous', 'volatility', 'coronavirus', 'pandemic', 'many', 'company', 'suﬀere', 'massive', 'price', 'drop', 'witness', 'substantial', 'increase', 'collect', 'stock', 'price', 'index', 'company', 'trad', 'ing', 'day', 'span', 'goal', 'study', 'characterize', 'change', 'direction', 'pattern', 'stock', 'price', 'take', 'account', 'dynamic', 'interaction', 'stock', 'end', 'original', 'price', 'index', 'code', 'binary', 'datum', 'denote', 'increase', 'decrease', 'group', 'company', 'category', 'base', 'global', 'industry', 'classiﬁcation', 'standard', 'detail', 'provide', 'table', 'highdimensional', 'ode', 'system', 'build', 'latent', 'success', 'probability', 'cid4cid5cid6cid7cid8', 'cid129cid130cid131cid132', 'information', 'technology', 'electric', 'vehicle', 'consumer', 'service', 'retail', 'pharmaceutical', 'fghij', 'abcde', 'cid19cid20cid21cid22cid23', 'online', 'retail', 'shopping', 'hotel', 'air', '‡·cid181¶', 'ﬂcid176–†', 'cid133cid134cid135cid136cid137', 'cid154cid155cid156cid157cid158', '⁄¥ƒ§¤', '‹›', '´', 'cid204˝˛ˇ', 'cid214cid215cid216cid217cid218', 'cid219cid220cid221cid222cid223', 'figure', 'ﬁtte', 'probability', 'process', 'daily', 'price', 'increase', 'category', 'red', 'dash', 'line', 'denote', 'cesse', 'sparsity', 'tuning', 'procedure', 'lead', 'ﬁtted', 'model', 'achieve', 'ode', 'ﬁdelity', 'figure', 'display', 'ﬁtted', 'probability', 'daily', 'stock', 'price', 'increase', 'category', 'notice', 'interesting', 'result', 'result', 'first', 'category', 'low', 'ﬁtte', 'probability', 'correspond', 'stock', 'market', 'crash', 'multiple', 'circuit', 'breaker', 'trigger', 'fear', 'covid19', 'coronavirus', 'crash', 'sector', 'recover', 'reentere', 'bull', 'market', 'online', 'retail', 'company', 'make', 'huge', 'proﬁts', 'health', 'concern', 'change', 'customer', 'shopping', 'habit', 'information', 'technology', 'company', 'beneﬁte', 'grow', 'demand', 'information', 'service', 'electronic', 'device', 'example', 'shift', 'remote', 'working', 'raise', 'number', 'zoom', 'daily', 'user', 'unprecedented', 'one', 'contrast', 'sector', 'energy', 'hotel', 'air', 'transportation', 'experience', 'severe', 'hit', 'covid19', 'pandemic', 'sign', 'recovery', 'fourth', 'quarter', 'industry', 'still', 'tremendous', 'impact', 'covid19', 'recession', 'analysis', 'yeast', 'cell', 'cycleregulate', 'gene', 'cell', 'cycle', 'fundamental', 'biological', 'process', 'consist', 'cell', 'growth', 'duplication', 'genetic', 'information', 'distribution', 'chromosome', 'cell', 'division', 'spellman', 'analyze', 'expression', 'level', 'yeast', 'gene', 'terval', 'minute', 'experiment', 'carry', 'cell', 'culture', 'independent', 'synchronization', 'method', 'score', 'calculate', 'gene', 'indicate', 'p', 'cid128', 'cid153', 'ﬁ', 'figure', 'recover', 'network', 'yeast', 'cell', 'cycle', 'yellow', 'node', 'represent', 'gene', 'greensolid', 'reddashed', 'edge', 'indicate', 'potential', 'promotion', 'suppression', 'eﬀect', 'similarity', 'cellcycle', 'regulate', 'gene', 'already', 'know', 'missingness', 'datum', 'choose', 'gene', 'identiﬁe', 'spellman', 'alpha', 'factorbase', 'synchronize', 'experiment', 'model', 'dynamic', 'relationship', 'mean', 'proﬁle', 'gene', 'use', 'ode', 'system', 'gaussian', 'assumption', 'gene', 'expression', 'level', 'propose', 'method', 'apply', 'identify', 'sparse', 'structure', 'gene', 'regulatory', 'network', 'result', 'show', 'figure', 'exclude', 'isolated', 'gene', 'suggest', 'gene', 'involve', 'cell', 'cycle', 'regulated', 'tran', 'scription', 'absolutely', 'require', 'gene', 'figure', 'regulation', 'direct', 'edge', 'discover', 'average', 'number', 'regulation', 'gene', 'around', 'gene', 'regulation', 'ﬁve', 'gene', 'high', 'network', 'degree', 'identiﬁed', 'central', 'hub', 'node', 'example', 'large', 'number', 'regulation', 'figure', 'accord', 'saccharomyce', 'genome', 'database', 'cherry', 'et', 'encode', 'protein', 'know', 'cell', 'cycle', 'regu', 'lator', 'promote', 'transition', 'nasmyth', 'interestingly', 'positive', 'negative', 'sign', 'estimate', 'structural', 'parameter', 'naturally', 'imply', 'potential', 'pro', 'motion', 'inhibition', 'gene', 'respectively', 'result', 'suggest', 'promote', 'expression', 'node', 'regulate', 'dna', 'replication', 'g1', 'phase', 'meanwhile', 'suppress', 'expression', 'kinase', 'inhibitor', 'function', 'transition', 'conclusion', 'discussion', 'article', 'propose', 'new', 'proﬁle', 'procedure', 'parameter', 'estima', 'tion', 'sparse', 'structure', 'identiﬁcation', 'highdimensional', 'linear', 'ode', 'model', 'non', 'gaussian', 'observation', 'method', 'involve', 'hierarchical', 'optimization', 'scheme', 'inner', 'optimization', 'balance', 'datum', 'ﬁtte', 'ode', 'ﬁdelity', 'improve', 'estimation', 'eﬃciency', 'outer', 'optimization', 'induce', 'sparse', 'structure', 'well', 'model', 'interpretation', 'extend', 'twostep', 'collocation', 'method', 'nongaussian', 'observation', 'setting', 'compare', 'propose', 'proﬁle', 'procedure', 'comprehensive', 'study', 'limitation', 'work', 'linear', 'ode', 'system', 'consideration', 'aware', 'recent', 'development', 'twostep', 'collocation', 'sparse', 'additive', 'ode', 'system', 'michailidis', 'general', 'functional', 'hierarchical', 'mization', 'restrict', 'linear', 'ode', 'extension', 'nonlinear', 'ode', 'system', 'straightforward', 'instance', 'common', 'strategy', 'handle', 'additive', 'ode', 'model', 'expand', 'nonlinear', 'component', 'basis', 'function', 'however', 'proﬁle', 'nature', 'range', 'collocation', 'basis', 'latent', 'process', 'need', 'control', 'compact', 'interval', 'easily', 'overcome', 'future', 'research', 'statistical', 'property', 'uniform', 'bind', 'approximation', 'true', 'solution', 'asymptotic', 'normality', 'estimator', 'exist', 'theory', 'establish', 'generalize', 'proﬁle', 'still', 'challenging', 'problem', 'high', 'dimensionality', 'leave', 'systematic', 'study', 'future', 'work', 'derivative', 'provide', 'analytical', 'expression', 'derivative', 'use', 'computation', 'section', 'derivative', 'gj', 'inner', 'optimization', 'write', 'inner', 'optimization', 'gj', '−', 'bθjti', 'θ′', 'jt', '−', '−', 'zt', 'γjk', 'θkt', 'p', 'ﬁrst', 'derivative', '−', 'yijhti', '−', 'b′θjtihti', '2λθ', 'dθjt', 'zt', 'cid26', 'p', '−', '−', 'γjk', 'θkt', 'cid27cid26', '−', 'γjj', 'second', 'derivative', 'i1', '2λθ', '−', 'γjjht', 'cid27cid26', '−', 'γjjht', '−', 'γjjht', 'p', '−', 'γjjht', 'θkt', 'γjjht', 'θjt', '−', '2λθ', 'zt', 'dθjt', 'p', '−', '−', 'γjkθkt', 'derivative', 'c∗', 'outer', 'optimization', 'write', 'c∗', 'c∗', 'simplicity', 'zerogradient', 'c∗', 'take', 'derivative', 'respect', 'γj', 'side', 'c∗', 'suppose', '∂cj∂c⊤', 'c∗', 'c∗', 'c∗', 'nonsingular', 'following', 'expression', 'deriva', '∂2gj', '∂cj∂γ⊤', 'c∗', 'j', '∂c∗', 'tive', '∂c∗', 'matrix', 'righthand', 'side', 'explicitly', 'derive', 'derivative', 'c∗', '∂cj∂γ⊤', '−1', 'follow', 'supplementary', 'material', 'supplementary', 'material', 'contain', 'additional', 'numerical', 'result', 'reference', 'brunel', 'j', 'q', 'clairon', 'd’alch´e', 'buc', 'parametric', 'estimation', 'ordinary', 'diﬀerential', 'equation', 'orthogonality', 'condition', 'journal', 'ramsay', 'parameter', 'cascade', 'proﬁle', 'functional', 'datum', 'analysis', 'computational', 'statistic', 'carey', 'ramsay', 'fast', 'stable', 'parameter', 'estimation', 'linear', 'dynamical', 'system', 'statistic', 'datum', 'analysis', 'shojaie', 'network', 'reconstruction', 'high', 'dimensional', 'ordinary', 'diﬀerential', 'equation', 'journal', 'cherry', 'chervitz', 'dwight', 'e', 'hester', 'schroeder', 'sgd', 'saccharomyce', 'genome', 'database', 'cho', 'e', 'winzeler', 'l', 'steinmetz', 'conway', 'l', 'wodicka', 'gabrielian', 'landsman', 'genomewide', 'transcriptional', 'analysis', 'mitotic', 'cell', 'cycle', 'molecular', 'cell', 'cokus', 'haudenschild', 'e', 'shotgun', 'bisulphite', 'sequence', 'arabidopsis', 'genome', 'reveal', 'dna', 'methylation', 'pattern', 'nature', 'dai', 'kernel', 'ordinary', 'diﬀerential', 'equation', 'journal', 'association', 'dattner', 'klaassen', 'optimal', 'rate', 'direct', 'estimator', 'system', 'ordinary', 'diﬀerential', 'equation', 'linear', 'function', 'parameter', 'electronic', 'journal', 'statistic', 'dodd', 'p', 'bliss', 'danforth', 'poral', 'pattern', 'happiness', 'information', 'global', 'social', 'network', 'hedonometric', 'twitter', 'plo', 'fan', 'network', 'exploration', 'adaptive', 'lasso', 'scad', 'penalty', 'annal', 'apply', 'statistic', 'fan', 'r', 'variable', 'selection', 'nonconcave', 'penalize', 'likelihood', 'oracle', 'property', 'journal', 'fan', 'h', 'zou', 'statistical', 'foundation', 'datum', 'science', 'chapman', 'hallcrc', 'smooth', 'spline', 'model', 'quick', 'easy', 'onestep', 'parameter', 'estimation', 'tial', 'equation', 'journal', 'royal', 'statistical', 'society', 'series', 'statistical', 'methodol', 'ogy', 'hecker', 'toepfer', 'r', 'guthke', 'gene', 'regula', 'network', 'inference', 'datum', 'integration', 'dynamic', 'model', 'review', 'biosystem', 'michailidis', 'network', 'reconstruction', 'use', 'additive', 'ode', 'model', 'plo', 'forecast', 'stock', 'market', 'movement', 'direction', 'support', 'vector', 'machine', 'computer', 'operation', 'research', 'parameter', 'estimation', 'diﬀerential', 'equation', 'model', 'use', 'framework', 'measurement', 'error', 'regression', 'model', 'journal', 'highdimensional', 'ode', 'couple', 'mixed', 'eﬀect', 'model', 'technique', 'dynamic', 'gene', 'regulatory', 'network', 'identiﬁcation', 'journal', 'adaptive', 'basis', 'selection', 'exponen', 'tial', 'family', 'smooth', 'spline', 'application', 'joint', 'modeling', 'multiple', 'sequence', 'sample', 'mccullagh', 'nelder', 'generalize', 'linear', 'model', 'chapman', 'hallcrc', 'monograph', 'statistic', 'apply', 'probability', 'generalize', 'ordinary', 'diﬀerential', 'equation', 'model', 'journal', 'nagalakshmi', 'snyder', 'transcriptional', 'landscape', 'yeast', 'genome', 'deﬁne', 'rna', 'sequence', 'science', 'nasmyth', 'control', 'yeast', 'cell', 'cycle', 'protein', 'kinase', 'current', 'opinion', 'cell', 'biology', 'polyniki', 'hogan', 'compare', 'diﬀerent', 'ode', 'modelling', 'approach', 'gene', 'regulatory', 'network', 'journal', 'theoretical', 'biology', 'powell', 'newuoa', 'software', 'unconstrained', 'optimization', 'derivative', 'largescale', 'nonlinear', 'optimization', 'mclellan', 'ramsay', 'parameter', 'estimation', 'continuoustime', 'dynamic', 'model', 'use', 'principal', 'diﬀerential', 'analysis', 'computer', 'chemical', 'engineering', 'qi', 'asymptotic', 'eﬃciency', 'ﬁnitesample', 'property', 'generalized', 'proﬁle', 'estimation', 'parameter', 'ordinary', 'diﬀerential', 'equation', 'annal', 'statistic', 'ramsay', 'principal', 'diﬀerential', 'analysis', 'data', 'reduction', 'diﬀerential', 'opera', 'tor', 'journal', 'royal', 'statistical', 'society', 'series', 'statistical', 'methodology', 'ramsay', 'j', 'g', 'hooker', 'parameter', 'estimation', 'diﬀerential', 'equation', 'generalize', 'smoothing', 'approach', 'journal', 'royal', 'statistical', 'society', 'series', 'statistical', 'methodology', 'sloan', 'l', 'tweet', 'location', 'understand', 'rela', 'tionship', 'demographic', 'characteristic', 'use', 'geoservice', 'geotagge', 'twitter', 'plo', 'e0142209', 'spellman', 'p', 'g', 'sherlock', 'zhang', 'r', 'iyer', 'ander', 'eisen', 'p', 'brown', 'botstein', 'futcher', 'comprehensive', 'identiﬁcation', 'cell', 'cycleregulate', 'gene', 'yeast', 'saccharomyce', 'cerevisiae', 'molecular', 'biology', 'cell', 'genecoexpression', 'network', 'global', 'discovery', 'conserved', 'genetic', 'module', 'science', 'tibshirani', 'r', 'regression', 'shrinkage', 'selection', 'royal', 'statistical', 'society', 'series', 'statistical', 'methodology', 'gradient', 'descent', 'method', 'nonsmooth', 'sepa', 'rable', 'minimization', 'mathematical', 'programming', 'spline', 'least', 'square', 'method', 'numerical', 'parameter', 'estimation', 'diﬀerential', 'equation', 'scientiﬁc', 'statistical', 'computing', 'voorman', 'shojaie', 'witten', 'graph', 'estimation', 'joint', 'additive', 'model', 'r', 'klein', 'klein', 'smooth', 'spline', 'exponential', 'family', 'application', 'epidemiological', 'study', 'betic', 'retinopathy', 'memorial', 'lecture', 'annal', 'statistic', 'note', 'adaptive', 'group', 'statistic', 'data', 'analysis', 'wood', 'generalize', 'additive', 'model', 'introduction', 'r', 'additive', 'ordinary', 'diﬀerential', 'equation', 'dynamic', 'gene', 'regulatory', 'network', 'modeling', 'journal', 'yx', 'yuan', 'parameter', 'estimation', 'variable', 'selec', 'tion', 'big', 'system', 'linear', 'ordinary', 'diﬀerential', 'equation', 'matrixbase', 'approach', 'journal', 'yuan', 'kendziorski', 'hide', 'markov', 'model', 'microarray', 'time', 'course', 'datum', 'multiple', 'biological', 'condition', 'journal', 'statistical', 'yuan', 'selection', 'estimation', 'regression', 'group', 'able', 'journal', 'royal', 'statistical', 'society', 'series', 'statistical', 'methodology', 'yuan', 'model', 'selection', 'estimation', 'ch', 'nearly', 'unbiased', 'variable', 'selection', 'concave', 'penalty', 'annal', 'statistic', 'h', 'r', 'derivativefree', 'algorithm', 'least', 'square', 'minimization', 'optimization', 'zou', 'h', 'adaptive', 'lasso', 'oracle', 'property', 'journal']"
"Discovering Policies with DOMiNO: Diversity Optimization Maintaining
  Near Optimality","[{'href': 'http://arxiv.org/abs/2205.13521v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2205.13521v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-05-26 17:40:52,"The CLRS Algorithmic Reasoning Benchmark

Petar Veliˇckovi´c 1 Adri`a Puigdom`enech Badia 1 David Budden 1
Razvan Pascanu 1 Andrea Banino 1 Misha Dashevskiy 1 Raia Hadsell 1 Charles Blundell 1

2
2
0
2

n
u
J

4

]

G
L
.
s
c
[

2
v
9
5
6
5
1
.
5
0
2
2
:
v
i
X
r
a

Abstract

Learning representations of algorithms is an
emerging area of machine learning, seeking to
bridge concepts from neural networks with clas-
sical algorithms. Several important works have
investigated whether neural networks can effec-
tively reason like algorithms, typically by learning
to execute them. The common trend in the area,
however, is to generate targeted kinds of algorith-
mic data to evaluate speciﬁc hypotheses, making
results hard to transfer across publications, and
increasing the barrier of entry. To consolidate
progress and work towards uniﬁed evaluation, we
propose the CLRS Algorithmic Reasoning Bench-
mark, covering classical algorithms from the In-
troduction to Algorithms textbook. Our bench-
mark spans a variety of algorithmic reasoning
procedures, including sorting, searching, dynamic
programming, graph algorithms, string algorithms
and geometric algorithms. We perform extensive
experiments to demonstrate how several popular
algorithmic reasoning baselines perform on these
tasks, and consequently, highlight links to several
open challenges. Our library is readily available at
https://github.com/deepmind/clrs.

1. Introduction

Neural networks and classical algorithms are two techniques
that operate on diametrically opposite (and complementary)
sides of problem-solving: neural networks can adapt and
generalise to raw inputs, automatically extracting appro-
priate features and a single neural network setup is often
applicable to many separate tasks (Zamir et al., 2018). How-
ever, they are hard to interpret, notoriously unreliable when
extrapolating outside of the dataset they have been trained
on, and rely on massive quantities of training data. On

1DeepMind.

Correspondence to:

Petar Veliˇckovi´c

<petarv@deepmind.com>.

Proceedings of the 39 th International Conference on Machine
Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-
right 2022 by the author(s).

the other hand, algorithms trivially strongly generalise to
inputs of arbitrary sizes, and can be veriﬁed or proven to
be correct, with interpretable step-wise operations. Their
shortcoming is that inputs must be made to conform to a par-
ticular algorithm speciﬁcation, and looking at a separate task
often requires coming up with an entirely new algorithm
(Veliˇckovi´c & Blundell, 2021).

Bringing the two sides closer together can therefore yield
the kinds of improvements to performance, generalisation
and interpretability that are unlikely to occur through archi-
tectural gains alone. Accordingly, algorithmic modelling
as a domain for testing neural networks has been gaining
popularity over the last few years (Zaremba & Sutskever,
2014; Kaiser & Sutskever, 2015; Trask et al., 2018; Vinyals
et al., 2015; Kool et al., 2018; Freivalds et al., 2019; Dwivedi
et al., 2020; Chen et al., 2020; Tang et al., 2020; Veliˇckovi´c
et al., 2019; Yan et al., 2020; Deac et al., 2020) due to its
ability to highlight various reasoning limitations of existing
architectures.

Earlier work (Zaremba & Sutskever, 2014; Kaiser &
Sutskever, 2015) focused on the need of long-term mem-
ory capabilities when executing algorithms, which offered
a good test-bed for various recurrent and memory architec-
tures. Recently, algorithmic tasks have been used to high-
light the efﬁciency of graph neural networks (Dwivedi et al.,
2020; Chen et al., 2020; Veliˇckovi´c et al., 2019; Yan et al.,
2020; Corso et al., 2020; Tang et al., 2020; Georgiev & Li´o,
2020; Veliˇckovi´c et al., 2020) and to distinguish between
different variations of them, typically through the lens of
algorithmic alignment—architectures that align better with
the underlying algorithm can be proven to have better sam-
ple complexity (Xu et al., 2019). Unfortunately, many of
these works remain disconnected in terms of the algorithms
they target, how the data is presented to the model or through
the training and testing protocols they use, making direct
comparison somewhat difﬁcult.

To make a ﬁrst step towards a uniﬁed benchmark for al-
gorithmic reasoning tasks, we propose a comprehensive
dataset which we will refer to as The CLRS Algorithmic
Reasoning Benchmark, in homage to the Introduction to Al-
gorithms textbook by Cormen, Leiserson, Rivest and Stein
(Cormen et al., 2009).

 
 
 
 
 
 
The CLRS Algorithmic Reasoning Benchmark

Within this benchmark, we propose and evaluate on CLRS-
30: a dataset containing trajectories—a trajectory is formed
of inputs, the corresponding outputs and optional interme-
diary targets—of 30 classical algorithms covering various
forms of reasoning, including sorting, searching, dynamic
programming, geometry, graphs and strings. Some of these
algorithms are depicted in Figure 1. The appeal and moti-
vation for such a benchmark goes beyond unifying or pro-
viding a common ground for previous works, as we will
describe. We believe that CLRS-30 is well positioned to ex-
plore out-of-distribution (OOD) generalization and transfer
(as potentially part of a meta-learning setting) given the ex-
plicit and known relationship between different algorithms
(e.g. what subroutines are shared and so forth).

2. Motivation

Figure 1. Example of four algorithms within CLRS-30. A) in-
sertion sort; B) string matching; C) greedy task scheduling; D)
shortest paths.

Timely posed benchmarks have led to a signiﬁcant progress
in the ﬁeld, from the impact of ImageNet (Russakovsky
et al., 2015) on the vision community, to that of Wikipedia
and Penn Treebank in popularizing neural networks for lan-
guage modelling (Merity et al., 2016; Mikolov et al., 2011)
or Atari-2600 for deep reinforcement learning (Bellemare
et al., 2013). The prevalence of recent works focusing on al-
gorithmic reasoning1, as well as a history of disparate work
on a variety of bespoke benchmarks (Graves et al., 2014;
Zaremba & Sutskever, 2014; Kaiser & Sutskever, 2015;
Trask et al., 2018), suggests signiﬁcant utility in a bench-
mark covering a wide-range of classical CS algorithms.

Learning to mimic an algorithm also provides an opportu-
nity to extensively test the limitations of architectures both
in terms of their representation capacity and processing.
This can then be related back directly onto underlying oper-
ations and qualities of the well-studied CS algorithms being
mimicked as we are aware of both the process used to gen-
erate the inputs and the speciﬁcs of the underlying function

1Concurrent works published at the same venue include: (Xu
et al., 2019; Veliˇckovi´c et al., 2019) at ICLR’20 and (Veliˇckovi´c
et al., 2020; Corso et al., 2020; Tang et al., 2020) at NeurIPS’20.

producing the corresponding outputs. Hence, benchmarking
in this area can be used to better understand the limitations
of current architectures and the optimisation schemes used.
This benchmarking can come in many forms:

Data can be easily generated, allowing the neural network
behaviour to be probed under different regimes: from few-
shot learning all the way to inﬁnite-data.

Algorithms can be used to understand the efﬁciency of dif-
ferent inductive biases and neural components. For example,
a recent study (Tang et al., 2020) has demonstrated the direct
beneﬁts of choosing inductive biases that align well with
iterative algorithms. Algorithms have also been used to high-
light the importance of attention mechanisms (Graves et al.,
2014) or to disambiguate various message passing mecha-
nisms for graph neural networks (Richter & Wattenhofer,
2020; Joshi et al., 2020; Veliˇckovi´c et al., 2019).

Algorithms can require repeated computation, recursion,
or performing very different forms of computations con-
ditioned on the input, providing an excellent test-bed for
evaluating compositionality; i.e. whether an algorithm ex-
ecutor can effectively exploit these repeated computations.

One can control the amount of memory required to solve
a problem instance, hence test the memorization ability of
neural networks. Moreover, one can build a curriculum
of tasks of increasing memory requirements (Zaremba &
Sutskever, 2014).

Control over the difﬁculty of problem instances also allows
the behaviour of a trained model to be tested on OOD sam-
ples. While neural networks are highly efﬁcient on solving
complex perceptual tasks, current theoretical understanding
suggests that their power relies on their ability to interpo-
late (Liu et al., 2020; Belkin et al., 2019; Jacot et al., 2018),
limiting them to in-distribution generalisation. General rea-
soning systems, however, need to be able to expand beyond
this type of generalization. OOD generalization (Li et al.,
2020) is paramount, as generally one can not control the
distribution a model will face over time when deployed.

Understanding how algorithms operate on corner cases is
a standard approach for analysing their correctness. Sim-
ilarly, understanding the behaviour of a trained model on
larger instances of the problem, or instances that expose
such corner cases that were not covered in the training set,
can elucidate to what degree the model has truly learned the
algorithm (as opposed to overﬁtting to speciﬁc statistics of
the training data). Particularly, we can control how far from
the training distribution a test instance is, potentially allow-
ing us to understand to what extent the model generalizes
OOD, and under which circumstances. In turn, this can offer
insight into the effectiveness of different inductive biases,
highlighting what kinds of inductive biases are useful for
mimicking reasoning processes.

2 3 4 5 6
1
1 2 4 5 6 3
A)                                                    B)

b  a  c  b  a  b  a  b  a  a  b  c  b  a  b   T

s

a  b  a  b  a  c  a   S
q

C)                                                          D)

a2

a3

a1

a1

a1

a4

0

6

7

2

8

7

5

-2

2

9

-3

-4

4

7

-2

The CLRS Algorithmic Reasoning Benchmark

One would also expect a general reasoning system to be
able to reuse parts of learned computations when learning
a new task, and to compose learnt computational subrou-
tines (Lake, 2019; Grifﬁths et al., 2019; Alet et al., 2018).
These forms of generalization have been the aim of several
learning paradigms from transfer learning to meta-learning
and continual learning or domain adaptation. However,
many of these paradigms rely on the concept of a task, and
measuring or understanding the ability of a learned sys-
tem to reuse or compose requires the ability to decompose
a task into sub-tasks and to be able to relate tasks among
themselves. In many scenarios, such decompositions are am-
biguous. Without a clear segmentation into sub-tasks, there
can be no clearly deﬁned distance metric between tasks (Du
et al., 2018). Conversely, algorithms are built based on
subroutines that tend to be extensively shared, providing a
good playground for formalizing and measuring reuse and
composition, making an algorithmic reasoning benchmark
potentially attractive to meta-learning practitioners.

Lastly and fundamentally, computer scientists rely on a rela-
tively small2 number of algorithms to address an extremely
vast set of problems. They can be seen as a very powerful
basis that spans most forms of reasoning processes. On one
hand, this means that any generic reasoning system likely
has to be able to reproduce all such kinds of procedures,
hence, building a system that properly learns all of them
is a major stepping stone towards generic reasoning. On
the other hand, this means that they can be used to discover
inductive biases that will enable tackling more complex
problems. This is either because these complex problems
can be seen as a combination of several algorithms, or be-
cause learning certain algorithms can provide a reliable way
for the model to learn how to access its own memory or how
to attend to its input or other such internal mechanisms. So
by ﬁrst training on algorithms—potentially controlling the
difﬁculty of training instances—one can pre-train for tasks
where full trajectories may not be available (Veliˇckovi´c et al.,
2021). One such example is discovering novel polynomial-
time heuristics for combinatorial optimisation (Bengio et al.,
2020; Cappart et al., 2021; Khalil et al., 2017) or reinforce-
ment learning (Deac et al., 2021). Note that our focus with
this benchmark lies in learning the basic algorithms them-
selves only–this in itself proves sufﬁciently challenging for
neural networks, and is itself a useful outcome for the rea-
sons highlighted above. However, we speculate that once a
neural network can learn not only individual algorithms but
novel combinations of multiple algorithms or even discover
new algorithms, such networks will be useful in a wide
variety of problems from scientiﬁc problems such as pro-
tein folding and genomics to simulated environments such
as those used by reinforcement learning and control–much

2The entire Introduction to Algorithms textbook (Cormen et al.,

2009) proposes and discusses ∼100 algorithms in total.

as classic CS algorithms already make in-roads into these
domains but lack the ability to learn from data.

Guided by these observations, we regard CLRS-30 as a ﬁrst
step towards a pragmatic setting to test many of these dif-
ferent aspects of current architectures. While we do not
directly target all of the scenarios outlined above, the bench-
mark was built with ease of expansion in mind; enabling
for extensive tweaking of training/testing setups, kinds of
information captured in algorithm trajectories, as well as
including additional algorithms, which we aim to do consis-
tently over time.

3. CLRS Algorithmic Reasoning Benchmark

Owing to its name, CLRS-30 consists only of algorithms
which may be encountered in the CLRS textbook (Cormen
et al., 2009). Further, all algorithm trajectories and relevant
variables have been designed to match the pseudocode in
the textbook as closely as possible. We begin by describing
the selection criteria we applied when determining which
algorithms to include in CLRS-30.

Our initial survey of the textbook yielded 94 algorithms and
data structures of interest. From this point, we set out to
ﬁlter this set to algorithms suitable for inclusion in the initial
version of our benchmark. The criteria we applied, with
justiﬁcation and remarks, are as follows:

We want to be able to reliably generate ground-truth outputs
for large inputs. As such, NP-hard tasks (and approximation
algorithms thereof) have been excluded. Our decision is
backed up by theoretical work suggesting impossibility of
accurately modelling NP-hard problems using polynomial-
time samplers, unless NP=co-NP (Yehuda et al., 2020).

Tasks requiring numerical outputs have been excluded. Eval-
uating their performance is ambiguous, and may be depen-
dent on the way architectures choose to represent numbers.
For example, Yan et al. (2020) (which represents numbers
in binary) and Veliˇckovi´c et al. (2019) (which represents
them in ﬂoating-point) report different metrics on predicting
shortest-path lengths. This excludes most number-theoretic
algorithms, linear programming, and max-ﬂow3. It does not
exclude shortest-path algorithms: we can treat them as tasks
of ﬁnding edges belonging to the shortest path, as was done
in Veliˇckovi´c et al. (2019); Tang et al. (2020). The numeri-
cal values of path lengths are then treated as intermediate
parts of the trajectory, and not directly evaluated on.

Standalone data structures do not directly represent a task4.

3It should be noted that, by the max-ﬂow min-cut theorem
(Ford Jr & Fulkerson, 2015), any max-ﬂow problem can be cast as
ﬁnding the minimum cut containing the source vertex. This is a
discrete decision problem over input vertices, which hence doesn’t
violate our constraints, and could be included in future iterations.
4In programming language terms, their algorithms tend to be

The CLRS Algorithmic Reasoning Benchmark

Rather, their target is appropriately updating the internal
state of the data structure. Hence, we don’t include their
operations, unless they appear as components of algorithms.
We, of course, look forward to including them in subsequent
versions of the dataset, as they can provide useful building
blocks for learning complex algorithms.

Lastly, there are representational issues associated with dy-
namically allocated memory—it may be unclear what is the
best way to represent the internal memory storage and its
usage in algorithm trajectories. One example of the ambi-
guity is in asking whether the algorithm executor should
start with a “scratch space” deﬁned by the space complexity
of the problem that gets ﬁlled up, or dynamically generate
such space5 (Strathmann et al., 2021). As such, we for now
exclude all algorithms that require allocating memory which
cannot be directly attached to the set of objects provided
at input time. This excludes algorithms like merge sort,
Hierholzer’s algorithm for ﬁnding Euler tours (Hierholzer
& Wiener, 1873), or string matching using ﬁnite automata.

All of the above applied, we arrive at the 30 algorithms that
are selected into CLRS-30, which we categorize as follows:

Sorting: Insertion sort, bubble sort, heapsort (Williams,
1964), quicksort (Hoare, 1962).

Searching: Minimum, binary search, quickselect (Hoare,
1961).

Divide and Conquer (D&C): Maximum subarray
(Kadane’s variant (Bentley, 1984)).

Greedy: Activity selection (Gavril, 1972), task scheduling
(Lawler, 1985).

Dynamic Programming: Matrix chain multiplication,
longest common subsequence, optimal binary search tree
(Aho et al., 1974).

Graphs: Depth-ﬁrst and breadth-ﬁrst search (Moore,
1959), topological sorting (Knuth, 1973), articulation points,
bridges, Kosaraju’s strongly-connected components algo-
rithm (Aho et al., 1974), Kruskal’s and Prim’s algorithms
for minimum spanning trees (Kruskal, 1956; Prim, 1957),
Bellman-Ford and Dijkstra’s algorithms for single-source
shortest paths (Bellman, 1958; Dijkstra et al., 1959) (+ di-
rected acyclic graphs version), Floyd-Warshall algorithm
for all-pairs shortest paths (Floyd, 1962).

Strings: Na¨ıve string matching, Knuth-Morris-Pratt (KMP)
string matcher (Knuth et al., 1977).

Geometry: Segment intersection, Convex hull algorithms:
Graham scan (Graham, 1972), Jarvis’ march (Jarvis, 1973).

The chosen algorithms span a wide variety of reasoning

of the void type.

5Akin to malloc-like calls in C++.

procedures, and hence can serve as a good basis for algorith-
mic reasoning evaluation, as well as extrapolation to more
challenging problems.

3.1. Implementation, probes and representation

We have implemented the selected 30 algorithms in an id-
iomatic way, which aligns as closely as possible to the origi-
nal pseudocode from Cormen et al. (2009). This allows us
to automatically generate input/output pairs for all of them,
enabling full control over the input data distribution, so long
as it conforms to the preconditions of the algorithm. Further,
we capture the intermediate algorithm trajectory in the form
of “hints” (detailed in section 3.2), which allow insight into
the inner workings of the algorithm. Such trajectories have
already been extensively used in related work (Veliˇckovi´c
et al., 2019; 2020; Georgiev & Li´o, 2020; Deac et al., 2020)
and are typically crucial for OOD generalisation.

In the most generic sense, algorithms can be seen as ma-
nipulating sets of objects, along with any relations between
them (which can themselves be decomposed into binary
relations). If the sets are (partially) ordered (e.g. arrays or
rooted trees), this can be imposed by including predecessor
links. Therefore, algorithms generally operate over graphs.
Motivated by existing theoretical results showing that graph
neural networks align well with dynamic programming-style
computations (Xu et al., 2019; Dudzik & Veliˇckovi´c, 2022),
we propose a graph-oriented way to encode the data.

Generally, our data is represented as a set of n vertices6,
where n is a hyperparameter that is provided as part of the
dataset generation process.

When the semantics of these nodes are not immediately clear
from the task (e.g. graph algorithms naturally operate over a
graph of n nodes), we make an appropriate modiﬁcation to
derive nodes. For example, in sorting algorithms, we treat
every input list element as a separate node, and in string
matching, we treat each character of the two input strings as
a separate node.

All information over these graphs falls under the following
categorisation:

Stage: Every feature, i.e. observation in the trajectory, is
either part of the input, output, or the hints. As we do not
cover algorithms that perform on-line querying, for all 30
algorithms there will be exactly one snapshot of the input
and output values, whereas hints will be a time-series of
intermediate algorithm states.

Location: Every feature is either present within the nodes,
edges (pairs of nodes) or the graph7.

6Edges are only present to represent the predecessor vertex if

the input is a partially ordered.

7This also determines shapes of each feature, e.g. node features

The CLRS Algorithmic Reasoning Benchmark

Type: Every feature can be of ﬁve possible types, which can
determine the appropriate method for encoding/decoding it,
and the appropriate loss function to use when learning to
predict it:

• scalar: Floating-point scalar8 feature. This would

typically be ﬁt using mean-squared error.

• categorical: Categorical feature over K possi-
ble classes. The type corresponds typically to cross-
entropy loss over the classes.

• mask: Categorical feature over two classes. This can

be ﬁt using binary cross-entropy.

• mask one: Categorical feature over two classes,
where exactly one node is active (“one-hot”). One
would generally optimise this argmax operation using
categorical cross-entropy.

• pointer: Categorical feature over the n nodes.
To predict “similarity” score against every node,
and typically optimised using categorical cross en-
tropy (as introduced in Pointer Graph Networks
(PGN) (Veliˇckovi´c et al., 2020)).

Specifying a feature’s stage, location and type fully deter-
mines its role in the dataﬂow. A tuple (stage, loc,
type, values) is referred to as a probe. Each of the
30 algorithms has a static (w.r.t. stage, location and type)
set of probes, which are considered to be a spec for the
algorithm. We will later describe how these specs may be
used to construct baseline architectures for the benchmark.

Every node is always endowed with a position scalar input
probe, which uniquely indexes it—the values are linearly
spaced between 0 and 1 along the node index. This allows
not only representing the data sequentially (when this is
appropriate), but also serves as a useful tie-breaker when
algorithms could make an arbitrary choice on which node
to explore next—we force the algorithms to favour nodes
with smaller position values.

To illustrate these concepts further, at the end of this section
we will describe the probes in detail for a popular algorithm
(insertion sort).

Note that, while we format the data in a way that clearly
favours graph neural network executors, it can be easily
adapted for different types of neural architectures; for exam-
ple, sequence to sequence models (Sutskever et al., 2014).

are of shape n × f ; edge features are of shape n × n × f ; graph
features are of shape f , where f is the dimension of this feature
(excluding batch axis).

8Given our current restriction on numerical predictions, scalar

types will never be given in the output stage.

Overall, CLRS-30 requires

1h to generate, and occupies

4.5GB when uncompressed, across all 30 tasks.

∼

∼

3.2. Hints

Hints are an important component of our benchmark, which
we ﬁnd fundamental in order to make progress on algorith-
mic reasoning. As we previously argued, the advantage of
algorithms as a task is our understanding of their behaviour,
and our ability to decompose them into useful subroutines
that can be shared or repeatedly applied.

While, implicitly, we hope that such a decomposition would
happen in any learned system, even when trained just using
inputs and outputs (as studied in Xu et al. (2019)), the degree
to which we can measure or encourage this is limited in the
typical end-to-end learning process, and often most of the
generalisation happens only in-distribution (as observed by
Veliˇckovi´c et al. (2019); Xu et al. (2020); Bevilacqua et al.
(2021)). The underlying algorithm may not be statistically
identiﬁable from a small set of input/output pairs.

Conversely, a perfect decomposition of a task into small
subtasks can be generated for algorithmic problems. Then,
individual models for each subtask may be trained and re-
composed into a solution. Such an approach will, by con-
struction, provide strong decompositional beneﬁts: as stud-
ied by Yan et al. (2020), perfect OOD generalisation can
be observed with such models, and they can even gener-
alise zero-shot to test algorithms that reuse their modules.
However, the downstream applicability of this is potentially
limited; when faced with a novel task which cannot be easily
decomposed into subtasks, it can be hard to decide how to
reuse the learnt modules.

We believe hints to lie in-between these two approaches. On
one hand, they represent intermediate targets which the net-
work should be able to predict if it performs reasoning simi-
lar9 to the ground truth algorithm it is supposed to mimic.
Indeed, several lines of recent work (Veliˇckovi´c et al., 2019;
Georgiev & Li´o, 2020; Veliˇckovi´c et al., 2020; Deac et al.,
2020) make favourable conclusions about using them, when
it comes to achieving stronger OOD generalisation. Further-
more, models leveraging hints are still end-to-end models;
when faced with a novel task at test-time, we don’t need
explicit knowledge of that task’s hints in order to re-use the
weights learnt on a task which had them.

Algorithms specify one way of attacking a problem, that is
explicitly detailed through the hints. In this sense, insertion
sort (to be presented shortly) is one way of implementing

9Note that architectures supervised in this way usually don’t
model the hints perfectly, and will deviate from the target algorithm
in subtle ways—Veliˇckovi´c et al. (2020) perform a qualitative study
which shows GPU-specialised data structures could emerge as a
result of such setups.

The CLRS Algorithmic Reasoning Benchmark

Figure 2. A sequence of hints for insertion sorting a list [5, 2, 4, 3, 1]. Green pointers correspond to the predecessor pointers (specifying
the list’s state throughout the algorithm’s execution. Note how the head of the list always points to itself, by convention. Further, note how,
at every step, the list is rewired such that the node selected by the blue pointer (slot) will point to the current iterator (pointed in red).

a sorting function: all sorting algorithms model sorting
functions, and will hence have identical outputs for identical
inputs. The aspects that set the different sorting algorithms
apart are exposed through their hints.

Being mindful of the fact that neural networks commonly
run on parallelisable architectures, we have made efforts
to “compress” the hints as much as possible. For example,
if a single for loop is used to sweep the data and detect
the node which optimises a certain quantity (without doing
any order-sensitive computations), that for loop can typi-
cally be entirely “skipped” when recording hints: as parallel
architectures may typically examine all the nodes at once.
Further, we make every effort possible that the hint at step
t + 1 will be predictable from the hints at step t by using
only a single step of message passing.

3.3. Worked example: insertion sort

To illustrate all of the concepts outlined above, we observe
the trajectories extracted by our data collection procedure
on an example: insertion sorting the array [5, 2, 4, 3, 1].

Insertion sort uses one pointer (j) to scan through the array,
and then another pointer (i) to slot the j-th item into the
correct place within [0..j]. This ascertains the invariant that,
after k steps, the subarray of the ﬁrst k elements is com-
pletely sorted. Hence the trajectory (with i and j marked)
[2i, 5j, 4, 3, 1]
is: [5i,j, 2, 4, 3, 1]
→
→
[2,3i, 4, 5j, 1]
[1i, 2, 3, 4, 5j]. Here, at each step, j scans
along the array, and i indicates the correct place for the
element that was j-th at the start of each iteration.

[2, 4i, 5j, 3, 1]

→

→

Converting this trajectory into a graph representation re-
quires some considerations. Requiring the model to perform
explicit swapping of node values would, ultimately, require
numerical predictions. To avoid it, we ask the model to
predict the predecessor pointer of each node (by conven-
tion, the head of the array points to itself). Hence the actual
recorded trajectory can be realised as depicted in Figure 2.
In this ﬁgure, green pointers correspond to the predecessor
pointers, red ones point to j, and blue ones point to i. i and
j are realised as type mask one, whereas predecessors are
of type pointer—and all three are stored in the nodes.
The red and blue pointers represent the “hints” for this task.

Finally, note that the original insertion sort pseudocode
mandates that, at each iteration, i starts at position j and
shifts backward until the right position is found. However,
this procedure can be performed in one step by a GNN, as
it can locate the correct position by examining all relevant
positions, and we can omit all of those intermediate steps.

In order to further illustrate how these hints are collected,
we also provide an informal pseudocode for collecting hints
for insertion sort in Algorithm 1:

Algorithm 1 Hint updates for Insertion Sort
Input :Input array val, Positions pos
Hints :Predecessors pred, Iterator iter, swap slot slot

i = 0
1 i > 0

; // Initialise list

pred[i]

(cid:40)

0
i

←

−
0, iter

slot

←

←
while iter < n do

iter

iter + 1

0

←

max node

argmax
j : pos[j]<pos[iter]

←

val[j]

if val[max node] < val[iter] then
max node

slot

←

(cid:40)

slot
pred[i]

i = iter
otherwise

pred[i]

←

else

slot

argmin
j : pos[j]<pos[iter],val[j]≥val[iter]

←

val[j]

pred[i]

←






iter
iter
pred[slot]
max node
pred[i]

i = slot

i=iter∧pred[slot]=slot

i=iter∧pred[slot](cid:54)=slot

pred[i] = iter
otherwise

end

end
return pred ;

// Return final list

In the interest of illustrating the hint structures further, we
provide worked examples of trajectories for three more al-

5

2

4

3

1

5

2

4

3

1

5

2

4

3

1

5

2

4

3

1

5

2

4

3

1

The CLRS Algorithmic Reasoning Benchmark

gorithms (dynamic programming, path-ﬁnding and string
matching) in Appendix B. It should be remarked that we
directly expose all of the hint collection routines as Python
code inside the CLRS library, allowing for direct inspection.

4. Empirical evaluation

Having surveyed the speciﬁcs of CLRS-30, we now present
experimental results on it for several proposed algorithmic
reasoning models. We primarily investigate whether a natu-
ral ladder of model performance will emerge when extrapo-
lating to larger inputs. Beyond this, we believe the bench-
mark will be useful for empirically examining many other
properties of algorithmic models, such as evaluating gener-
alisation across different graph types, task types, or various
multi-task (Xhonneux et al., 2021) or continual learning
setups. We make available complete implementations of
our data generating, probing and model training subroutines,
which should make evaluating on such settings simple to
deploy10. We survey several key ways of interacting with
the benchmark (e.g.
implementing baselines, modifying
datasets, adding new algorithms) in Appendix A.

4.1. Baseline models

Encode-process-decode For our experimental validation,
we adopt the encode-process-decode paradigm of Hamrick
et al. (2018), which is a common direction for several hint-
based architectures (Veliˇckovi´c et al., 2019; Georgiev & Li´o,
2020; Veliˇckovi´c et al., 2020; Deac et al., 2020).

Namely, we consider a setup with inputs xi in nodes, eij
in edges, and g in the graph. We ﬁrst encode each of these
using linear layers fn, fe, fg, to obtain encodings

hi = fn(xi)

hij = fe(eij)

hg = fg(g)

(1)

We then feed these latents through a processor network to
perform one step of computation. As we are focusing on
graph representation learning in the current data format,
most of our processors will be realised as graph neural net-
works (Gilmer et al., 2017). Most generally, along every
edge (i, j), a message from node i to node j, mij is com-
puted (using a message function fm), and these messages
are then aggregated across all neighbouring nodes using a
permutation-invariant aggregation function, (cid:76). Finally, a
readout network fr transforms these aggregated messages
and the node encodings into processed node encodings:

mij = fm(hi, hj, hij, hg)

mi =

(cid:77)

i∈Nj

mji

h(cid:48)

i = fr(hi, mi)

(2)

(3)

Once node encodings are updated, we can decode them to
make various predictions for this step of reasoning, depend-

ing on the type of the prediction required (using relevant
decoder functions g·), as prescribed in Section 3.1. Further,
we keep track of previous-step node encodings h(t−1)
, to
explicitly use in a recurrent cell update (exactly as done by
Veliˇckovi´c et al. (2019)). We opt to provide this recurrent
update in order to provide long-range capacity to the model.

i

Lastly, we need to decide in what capacity will hints be
used. We provide results for the option where hints are
both decoded (used for computing the loss function) and
encoded (considered as part of x, eij and g). At testing
time, the encoded hint is equal to the hints decoded by the
previous step, whereas we can stabilise these trajectories at
training time by performing noisy teacher forcing—inspired
by Noisy Nodes (Godwin et al., 2021), at each step we feed
back ground-truth hints with probability 0.5. The quantity
of hints is still used to determine the number of processor
steps to perform at evaluation time. This requirement of
knowing the hint-size can be lifted by, e.g., using termina-
tion networks (Veliˇckovi´c et al., 2019; Banino et al., 2021)
or aligning to iterative algorithms (Tang et al., 2020).

Processor networks The only remaining component to
specify is the processor network used by our models. As
this component carries the most computational load, it is
also the most obvious module to sweep over. We provide all
implementations and hyperparameters within our codebase.

Unless otherwise speciﬁed, we assume fully-connected
graphs, i.e.
, hence every node is con-
}
nected to every other node. We consider the following
baseline processor networks:

1, 2, . . . , n
{

i =

N

Deep Sets (Zaheer et al., 2017); where each node is only
(i.e., choice of (cid:76) is irrele-
i
connected to itself:
}
{
vant). Such a model is popular for summary statistic tasks.

i =

N

Graph Attention Networks (Veliˇckovi´c et al., 2017),
where the aggregation function (cid:76) is self-attention (Vaswani
et al., 2017), and the message function fm merely extracts
the sender features: fm(hi, hj, hij, hg) = Whi. We report
the best performance across GAT (Veliˇckovi´c et al., 2017)
and GATv2 (Brody et al., 2021) attention mechanisms.

Message-passing Neural Networks (Gilmer et al., 2017),
which correspond exactly to the formulation in Equation 2,
with (cid:76) = max, as prescribed by previous work (Veliˇckovi´c
et al., 2019). As a sanity check, we also attempted (cid:76) = (cid:80)
ﬁnding it underperformed on all tasks compared to max.

Pointer Graph Networks (Veliˇckovi´c et al., 2020), which
use only graph neighbourhoods
i speciﬁed by a union
of all node pointer and edge mask hints, and (cid:76) =
max. This restricts the model to only reason over the edges
deemed important by the inputs and hints.

N

10https://github.com/deepmind/clrs

Memory Networks (Sukhbaatar et al., 2015) have been

The CLRS Algorithmic Reasoning Benchmark

Figure 3. Validation results on eight representative algorithms in CLRS-30 (activity selector, Bellman-Ford, binary search, ﬁnd maximum
subarray, Graham scan, insertion sort, matrix chain order, na¨ıve string matcher), averaged over three seeds. In all cases the y-axis is
between [0, 100]%. Legend: MPNN red, PGN purple, Deep Sets blue, GAT orange, Memory Networks green. Validation results for all
30 individual algorithms can be found in Appendix D.

used in the past as baseline for investigating reasoning in
neural networks (e.g. Banino et al., 2020), as they provide
an alternative way to use structural dependencies in a graph
by treating edges as memories and nodes as queries. Here
we used latents representing node features hi as queries
and latents representing edge features hij (where there is a
connecting edge and 0 otherwise) as memory inputs.

4.2. Dataset statistics

For each algorithm in CLRS-30, we provide a canonical set
of training, validation and test trajectories for benchmarking
in- and out-of-distribution generalisation. We obtain these
trajectories by running the algorithms on randomly sampled
inputs that conform to their input speciﬁcation. This implies,
e.g., that the inputs to most graph algorithms are Erd˝os-
R´enyi graphs (Erd¨os & R´enyi, 2011) with a certain edge
probability. All scalar inputs are sampled from U (0, 1).

For validation, our aim is to measure in-distribution gener-
alisation. Hence we sample inputs of 16 nodes for both, and
generate 1,000 trajectories for training and 32 for validation.
For testing, we measure out-of-distribution generalisation,
and sample 32 trajectories for inputs of 64 nodes. For algo-
rithms where the output is on the graph stage (rather than
node/edge), we generate 64
more trajectories, in order to
equalise the number of targets across tasks.

×

We optimise our models on the training trajectories in a
teacher-forced fashion, with a batch size of 32, using the
Adam optimiser (Kingma & Ba, 2014) with an initial learn-
ing rate of η = 0.001. We train for 10, 000 steps, early stop-
ping on the validation performance. Our models are trained
on one V100 Volta GPU, requiring roughly between 1h and
30h to train, depending on the algorithm’s time complexity.
For example, linear-time algorithms have signiﬁcantly fewer
hints—hence message passing steps—than cubic-time ones.

4.3. Validation (in-distribution) performance

We provide the in-distribution performance throughout train-
ing in Figure 3, for eight representative tasks in CLRS-30
(one per each algorithm type); see Appendix D for the full
results on all 30 algorithms. In this regime, the MPNN
appears to dominate for most tasks: achieving over 90% F1
score for nearly all of them.

While this might seem like strong evidence in favour of
the fully-connected MPNNs, their added degrees of free-
dom may also make MPNNs more prone to overﬁtting
to speciﬁcs of the input (e.g.
the input graphs’ sizes),
rather than truly learning the underlying reasoning rule. We
present the out-of-distribution results next, in order to make
this distinction clear.

The CLRS Algorithmic Reasoning Benchmark

Table 1. Average test micro-F1 score of all models on all algorithm classes. The full test results for all 30 algorithms, along with a
breakdown of the “win/tie/loss” metric, are given in Appendix C.

Algorithm

Deep Sets

GAT

Memnet

MPNN

PGN

Divide & Conquer
Dynamic Prog.
Geometry
Graphs
Greedy
Search
Sorting
Strings

Overall average
Win/Tie/Loss counts

0.67
7.79
6.60
8.09
6.81
18.29
7.19
0.68

12.48%
66.05%
64.08%
37.65%
75.47%
43.79%
39.60%
2.64%

±
±
±
±
±
±
±
±
42.72%
0/3/27

0.74
5.33
11.18
8.66
4.59
19.81
4.64
1.08

24.43%
67.19%
73.27%
46.80%
78.96%
37.35%
14.35%
3.02%

±
±
±
±
±
±
±
±
43.17%
1/5/24

0.00
7.75
11.65
5.20
20.73
21.67
1.09
0.21

13.05%
67.94%
45.14%
24.12%
53.42%
34.35%
71.53%
1.51%

±
±
±
±
±
±
±
±
38.88%
4/2/24

20.30%
65.10%
73.11%
62.79%
82.39%
41.20%
11.83%
3.21%

0.85
6.44
17.19
8.75
3.01
19.87
2.78
0.94

±
±
±
±
±
±
±
±

44.99%
8/3/19

4.44
6.48
7.01
8.42
6.59
21.56
8.46
0.20

65.23%
70.58%
61.19%
60.25%
75.84%
56.11%
15.45%
2.04%

±
±
±
±
±
±
±
±
50.84%
8/6/16

4.4. Test (out-of-distribution) performance

5. Conclusion

The averaged out-of-distribution performance (using the
early-stopped model on validation) across each of the eight
algorithm types is provided in Table 1; see Appendix C for
the full results on all 30 algorithms. MPNNs are unable to
transfer their impressive gains to graphs that are four times
larger: in fact, the PGN takes over as the most performant
model when averaged across task types—this aligns well
with prior research (Veliˇckovi´c et al., 2020). The outperfor-
mance is also observed when we count how frequently each
model is among the best-performing models for a given
algorithm, as per our “win/tie/loss” metric, which we ex-
plain in Appendix C. GNN models, additionally, outperform
models like Deep Sets and Memory Nets, reinforcing that
GNNs are a useful primitive for algorithmic reasoning (Xu
et al., 2019; Dudzik & Veliˇckovi´c, 2022).

Aside from all of the above, we note that the OOD version of
the CLRS-30 benchmark is highly challenging and far from
solved for most tasks, making it a meaningful informant of
future progress in the area. In particular, PGNs struggled
on tasks requiring long-range rollouts (such as DFS), or
recursive reasoning (such as Quicksort and Quickselect).
This invites further research in algorithmic reasoners that
can support such computation. It is further revealed that
more specialised inductive biases and training regimes may
be required to deal with string matching algorithms (such as
KMP), and that the processor studied here tended to perform
the best on tasks which were of favourable (sublinear) com-
plexity in terms of hint counts (such as BFS, Bellman-Ford,
and task scheduling).

The speciﬁc results we obtain with our baselines validate
several bits of prior research in the area, but also demon-
strate we still have a long way to go, with even simple OOD
scenarios only being ﬁt to about 50% micro-F1 performance.

We introduce CLRS-30, a dataset that contains trajectories
from 30 classical algorithms. This benchmark constitutes
an effective way to test out-of-distribution generalization
and transfer, and brings a means to evaluate algorithmic
reasoning learnt by neural network models. The dataset
provides input/output pairs for all algorithms, as well as
intermediate trajectory information (“hints”).

It is our hope that CLRS-30 will be a useful tool to shepherd
future research in algorithmic reasoning, as prior art in the
area largely generated their own datasets, making progress
tracking challenging. Further, we hope that CLRS-30 will
make algorithmic reasoning a more accessible area: one
does not need a background in theoretical computer science
to generate the dataset, and can focus on the modelling.

If we convinced you to try out our library, please consult
Appendix A for detailed instructions on most common ways
to interact with our platform. CLRS is in constant develop-
ment, and we welcome any and all feedback.

Acknowledgements

CLRS-30 was developed over a long time-frame, with many
useful contributions, which we kindly acknowledge here.

We would like to particularly thank Borja Ibarz for nu-
merous ﬁxes and additions, and laying foundation for fu-
ture iterations. Additionally, we warmly thank Jonathan
Godwin, Sadegh Mahdavi, Euan Ong, MohamedElfatih
Salah, Ahmed Elhag, Andreea Deac, Frederik Nijweide,
Andrew Dudzik, Thomas Kipf, Amin Barekatain and Do-
brik Georgiev for their support, and identifying numerous
bugs during development. Finally, we thank Kim Stachen-
feld, Nate Kushman and Daan Wierstra for reviewing the
paper prior to submission, and anonymous reviewers for
their careful feedback, strengthening the paper signiﬁcantly.

The CLRS Algorithmic Reasoning Benchmark

References

Aho, A. V., Hopcroft, J. E., and Ullman, J. D. The design
and analysis of computer algorithms. Reading, 1974.

Alet, F., Lozano-Perez, T., and Kaelbling, L. P. Modular
meta-learning. volume 87 of Proceedings of Machine
Learning Research. PMLR, 2018.

Banino, A., Badia, A. P., K¨oster, R., Chadwick, M. J.,
Zambaldi, V., Hassabis, D., Barry, C., Botvinick, M.,
Kumaran, D., and Blundell, C. Memo: A deep net-
work for ﬂexible combination of episodic memories. In
International Conference on Learning Representations,
2020. URL https://openreview.net/forum?
id=rJxlc0EtDr.

Banino, A., Balaguer, J., and Blundell, C. Pondernet: Learn-
ing to ponder. arXiv preprint arXiv:2107.05407, 2021.

Cormen, T. H., Leiserson, C. E., Rivest, R. L., and Stein, C.

Introduction to algorithms. MIT press, 2009.

Corso, G., Cavalleri, L., Beaini, D., Li`o, P., and Veliˇckovi´c,
P. Principal neighbourhood aggregation for graph nets.
arXiv preprint arXiv:2004.05718, 2020.

Deac, A., Bacon, P.-L., and Tang, J. Graph neural induc-
tion of value iteration. arXiv preprint arXiv:2009.12604,
2020.

Deac, A.-I., Veliˇckovi´c, P., Milinkovic, O., Bacon, P.-L.,
Tang, J., and Nikolic, M. Neural algorithmic reasoners
are implicit planners. Advances in Neural Information
Processing Systems, 34, 2021.

Dijkstra, E. W. et al. A note on two problems in connex-
ion with graphs. Numerische mathematik, 1(1):269–271,
1959.

Belkin, M., Hsu, D., and Xu, J. Two models of double de-
scent for weak features. arXiv preprint arXiv:1903.07571,
2019.

Du, Y., Czarnecki, W. M., Jayakumar, S. M., Pascanu, R.,
and Lakshminarayanan, B. Adapting auxiliary losses
using gradient similarity, 2018.

Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M.
The arcade learning environment: An evaluation plat-
form for general agents. Journal of Artiﬁcial Intelligence
Research, 47:253–279, 2013.

Bellman, R. On a routing problem. Quarterly of applied

mathematics, 16(1):87–90, 1958.

Bengio, Y., Lodi, A., and Prouvost, A. Machine learning
for combinatorial optimization: a methodological tour
d’horizon. European Journal of Operational Research,
2020.

Bentley, J. Programming pearls: algorithm design tech-
niques. Communications of the ACM, 27(9):865–873,
1984.

Bevilacqua, B., Zhou, Y., and Ribeiro, B. Size-invariant
graph representations for graph classiﬁcation extrapola-
tions. In International Conference on Machine Learning,
pp. 837–851. PMLR, 2021.

Brody, S., Alon, U., and Yahav, E. How attentive are graph
attention networks? arXiv preprint arXiv:2105.14491,
2021.

Cappart, Q., Ch´etelat, D., Khalil, E., Lodi, A., Morris,
C., and Veliˇckovi´c, P. Combinatorial optimization and
reasoning with graph neural networks. arXiv preprint
arXiv:2102.09544, 2021.

Dudzik, A. and Veliˇckovi´c, P. Graph neural networks are
dynamic programmers. arXiv preprint arXiv:2203.15544,
2022.

Dwivedi, V. P., Joshi, C. K., Laurent, T., Bengio, Y., and
Bresson, X. Benchmarking graph neural networks. arXiv
preprint arXiv:2003.00982, 2020.

Erd¨os, P. and R´enyi, A. On the evolution of random graphs.
In The structure and dynamics of networks, pp. 38–82.
Princeton University Press, 2011.

Floyd, R. W. Algorithm 97: shortest path. Communications

of the ACM, 5(6):345, 1962.

Ford Jr, L. R. and Fulkerson, D. R. Flows in networks.

Princeton university press, 2015.

Freivalds, K., Ozolin¸ ˇs, E., and ˇSostaks, A. Neural shufﬂe-
exchange networks-sequence processing in o (n log n)
In Advances in Neural Information Processing
time.
Systems, pp. 6630–6641, 2019.

Gavril, F. Algorithms for minimum coloring, maximum
clique, minimum covering by cliques, and maximum
independent set of a chordal graph. SIAM Journal on
Computing, 1(2):180–187, 1972.

Georgiev, D. and Li´o, P. Neural bipartite matching. arXiv

preprint arXiv:2005.11304, 2020.

Chen, Z., Chen, L., Villar, S., and Bruna, J. Can graph
arXiv preprint

neural networks count substructures?
arXiv:2002.04025, 2020.

Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and
Dahl, G. E. Neural message passing for quantum chem-
istry. arXiv preprint arXiv:1704.01212, 2017.

The CLRS Algorithmic Reasoning Benchmark

Godwin, J., Schaarschmidt, M., Gaunt, A. L., Sanchez-
Gonzalez, A., Rubanova, Y., Veliˇckovi´c, P., Kirkpatrick,
J., and Battaglia, P. Simple gnn regularisation for 3d
molecular property prediction and beyond. In Interna-
tional Conference on Learning Representations, 2021.

Graham, R. L. An efﬁcient algorithm for determining the
Info. Pro. Lett., 1:

convex hull of a ﬁnite planar set.
132–133, 1972.

Graves, A., Wayne, G., and Danihelka, I. Neural turing

machines. arXiv preprint arXiv:1410.5401, 2014.

Grifﬁths, T., Callaway, F., Chang, M., Grant, E., Krueger, P.,
and Lieder, F. Doing more with less: meta-reasoning and
meta-learning in humans and machines. Current Opinion
in Behavioral Sciences, October 2019.

Hamrick, J. B., Allen, K. R., Bapst, V., Zhu, T., McKee,
K. R., Tenenbaum, J. B., and Battaglia, P. W. Relational
inductive bias for physical construction in humans and
machines. arXiv preprint arXiv:1806.01203, 2018.

Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.

Knuth, D. E. Fundamental algorithms. 1973.

Knuth, D. E., Morris, Jr, J. H., and Pratt, V. R. Fast pattern
matching in strings. SIAM journal on computing, 6(2):
323–350, 1977.

Kool, W., van Hoof, H., and Welling, M. Attention,
arXiv preprint

learn to solve routing problems!
arXiv:1803.08475, 2018.

Kruskal, J. B. On the shortest spanning subtree of a graph
and the traveling salesman problem. Proceedings of the
American Mathematical society, 7(1):48–50, 1956.

Lake, B. M. Compositional generalization through meta
sequence-to-sequence learning. In Advances in Neural In-
formation Processing Systems 32, pp. 9791–9801. 2019.

Lawler, E. L. The traveling salesman problem: a guided
tour of combinatorial optimization. Wiley-Interscience
Series in Discrete Mathematics, 1985.

Hennigan, T., Cai, T., Norman, T., and Babuschkin, I. Haiku:
Sonnet for JAX, 2020. URL http://github.com/
deepmind/dm-haiku.

Li, Y., Gimeno, F., Kohli, P., and Vinyals, O. Strong general-
ization and efﬁciency in neural programs. arXiv preprint
arXiv:2007.03629, 2020.

Hierholzer, C. and Wiener, C. ¨Uber die m¨oglichkeit, einen
linienzug ohne wiederholung und ohne unterbrechung zu
umfahren. Mathematische Annalen, 6(1):30–32, 1873.

Hoare, C. A. Algorithm 65: ﬁnd. Communications of the

ACM, 4(7):321–322, 1961.

Hoare, C. A. Quicksort. The Computer Journal, 5(1):10–16,

1962.

Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel:
Convergence and generalization in neural networks. In
Advances in Neural Information Processing Systems 31.
2018.

Jarvis, R. A. On the identiﬁcation of the convex hull of a
ﬁnite set of points in the plane. Information processing
letters, 2(1):18–21, 1973.

Joshi, C. K., Cappart, Q., Rousseau, L.-M., Laurent, T., and
Bresson, X. Learning tsp requires rethinking generaliza-
tion. arXiv preprint arXiv:2006.07054, 2020.

Kaiser, Ł. and Sutskever, I. Neural gpus learn algorithms.

arXiv preprint arXiv:1511.08228, 2015.

Khalil, E., Dai, H., Zhang, Y., Dilkina, B., and Song,
L. Learning combinatorial optimization algorithms over
graphs. In Advances in Neural Information Processing
Systems, pp. 6348–6358, 2017.

Liu, C., Zhu, L., and Belkin, M. Toward a theory of
optimization for over-parameterized systems of non-
linear equations: the lessons of deep learning. CoRR,
abs/2003.00307, 2020.

Merity, S., Xiong, C., Bradbury, J., and Socher, R.
arXiv preprint

Pointer sentinel mixture models.
arXiv:1609.07843, 2016.

Mikolov, T., Deoras, A., Kombrink, S., Burget, L., and
Cernock´y, J. Empirical evaluation and combination of
In INTER-
advanced language modeling techniques.
SPEECH, pp. 605–608, 2011.

Moore, E. F. The shortest path through a maze. In Proc. Int.

Symp. Switching Theory, 1959, pp. 285–292, 1959.

Prim, R. C. Shortest connection networks and some gen-
eralizations. The Bell System Technical Journal, 36(6):
1389–1401, 1957.

Richter, O. and Wattenhofer, R. Normalized attention with-
out probability cage. arXiv preprint arXiv:2005.09561,
2020.

Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S.,
Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,
M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale
Visual Recognition Challenge. International Journal of
Computer Vision (IJCV), 115(3):211–252, 2015. doi:
10.1007/s11263-015-0816-y.

The CLRS Algorithmic Reasoning Benchmark

Strathmann, H., Barekatain, M., Blundell, C., and
Veliˇckovi´c, P. Persistent message passing. arXiv preprint
arXiv:2103.01043, 2021.

Xu, K., Li, J., Zhang, M., Du, S. S., Kawarabayashi, K.-i.,
and Jegelka, S. What can neural networks reason about?
arXiv preprint arXiv:1905.13211, 2019.

Sukhbaatar, S., Szlam, A., Weston, J., and Fergus, R. End-to-
end memory networks. arXiv preprint arXiv:1503.08895,
2015.

Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to se-
quence learning with neural networks. In Advances in
neural information processing systems, pp. 3104–3112,
2014.

Tang, H., Huang, Z., Gu, J., Lu, B., and Su, H. Towards
scale-invariant graph-related problem solving by itera-
tive homogeneous gnns. the 34th Annual Conference on
Neural Information Processing Systems (NeurIPS), 2020.

Xu, K., Li, J., Zhang, M., Du, S. S., ichi Kawarabayashi, K.,
and Jegelka, S. How neural networks extrapolate: From
feedforward to graph neural networks. arXiv preprint
arXiv:2009.11848, 2020.

Yan, Y., Swersky, K., Koutra, D., Ranganathan, P., and
Heshemi, M. Neural execution engines: Learning to
execute subroutines. arXiv preprint arXiv:2006.08084,
2020.

Yehuda, G., Gabel, M., and Schuster, A.

It’s not what
machines can learn, it’s what we cannot teach. arXiv
preprint arXiv:2002.09398, 2020.

Trask, A., Hill, F., Reed, S. E., Rae, J., Dyer, C., and Blun-
som, P. Neural arithmetic logic units. In Advances in
Neural Information Processing Systems, pp. 8035–8044,
2018.

Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B.,
Salakhutdinov, R. R., and Smola, A. J. Deep sets. In
Advances in neural information processing systems, pp.
3391–3401, 2017.

Zamir, A. R., Sax, A., Shen, W., Guibas, L. J., Malik, J.,
and Savarese, S. Taskonomy: Disentangling task trans-
fer learning. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 3712–3722,
2018.

Zaremba, W. and Sutskever, I. Learning to execute. arXiv

preprint arXiv:1410.4615, 2014.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-
tion is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017.

Veliˇckovi´c, P. and Blundell, C. Neural algorithmic reasoning.

arXiv preprint arXiv:2105.02761, 2021.

Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A.,
Lio, P., and Bengio, Y. Graph attention networks. arXiv
preprint arXiv:1710.10903, 2017.

Veliˇckovi´c, P., Ying, R., Padovano, M., Hadsell, R., and
Blundell, C. Neural execution of graph algorithms. arXiv
preprint arXiv:1910.10593, 2019.

Veliˇckovi´c, P., Buesing, L., Overlan, M. C., Pascanu, R.,
Vinyals, O., and Blundell, C. Pointer graph networks.
arXiv preprint arXiv:2006.06380, 2020.

Veliˇckovi´c, P., Boˇsnjak, M., Kipf, T., Lerchner, A., Hadsell,
R., Pascanu, R., and Blundell, C. Reasoning-modulated
representations. arXiv preprint arXiv:2107.08881, 2021.

Vinyals, O., Fortunato, M., and Jaitly, N. Pointer networks.
In Advances in Neural Information Processing Systems,
pp. 2692–2700, 2015.

Williams, J. W. J. Algorithm 232: heapsort. Commun. ACM,

7:347–348, 1964.

Xhonneux, L.-P., Deac, A.-I., Veliˇckovi´c, P., and Tang, J.
How to transfer algorithmic reasoning knowledge to learn
new algorithms? Advances in Neural Information Pro-
cessing Systems, 34, 2021.

A. Interfacing with the CLRS benchmark

The CLRS Algorithmic Reasoning Benchmark

The CLRS benchmark is publicly hosted on GitHub: https://github.com/deepmind/clrs. All code and artifacts
are released under an Apache 2.0 license, which is highly permissive.

Within clrs/examples/run.py, we demonstrate an extensively conﬁgurable example script that evaluates a speciﬁc
baseline on CLRS-30.

Our baselines are provided in JAX and Haiku (Hennigan et al., 2020), but the dataset is generated using NumPy, making it
possible to create learning pipelines in virtually any framework, including PyTorch and TensorFlow.

We will now highlight three key ways in which researchers can interface with the library.

A.1. Evaluating a new baseline on CLRS-30

To support a new baseline, the recommended path depends on how fundamentally different the baseline is to an encode-
process-decode GNN.

In most cases, we anticipate that only the processor network needs changing, and the remainder of the architec-
ture can match our baselines.
In this case, it is only necessary to implement the new processor network within
clrs/ src/processors.py and appropriately set self.mpnn within the construct processor method in
clrs/ src/baselines.py.

For more fundamentally different baselines, it is necessary to create a new class that extends the Model API (as found within
clrs/ src/model.py). clrs/ src/baselines.py provides one example of how this can be done efﬁciently, for
the case of our baselines.

A.2. Modifying the data distribution of CLRS-30

If users want to train and/or evaluate the models on different versions of the tasks given in CLRS-30, the key routines to
modify are located in clrs/ src/samplers.py.

The easiest modiﬁcation concerns the graph sizes and/or numbers of trajectories. They can be directly changed by modifying
the CLRS30 dictionary near the top of the ﬁle.

For more elaborate modiﬁcations, e.g. to the speciﬁc data sampling distributions, the users would need to modify and/or
extend the relevant sampler class. As a guiding example, we provide a SortingSampler class which is convenient for
generating inputs for sorting algorithms. The speciﬁc sampler used for each task is provided in the SAMPLERS dictionary
towards the end of the ﬁle.

A.3. Adding new algorithms to CLRS

As the most elaborate of the three workﬂows, adding a new algorithm to the task suite requires following several steps,
which are potentially comprehensive, depending on the complexity of the algorithm. However, the CLRS benchmark code
still provides may helper routines for probing and batching that facilitate inclusion of novel algorithms. The steps are as
follows:

1. First, determine the input/hint/output speciﬁcation of your algorithm, and include it within the SPECS dictionary of

clrs/ src/specs.py.

2. Implement the desired algorithm in an abstractiﬁed form. Examples of this can be found throughout the

clrs/ src/algorithms/ folder.

3. Next, choose appropriate moments within the algorithm’s execution to create probes that capture the inputs, outputs

and all intermediate state (using the probing.push function).

4. Once generated, probes can be prepared using the probing.finalize method, and should be returned together

with the algorithm output.

5. Lastly, implement an appropriate input data sampler for your algorithm, and include it within the SAMPLERS dictionary

within clrs/ src/samplers.py.

B. Additional worked examples of algorithm trajectories

The CLRS Algorithmic Reasoning Benchmark

Matrix Chain Order As a representative dynamic programming algorithm, we visualise the steps of the procedure for
optimising the order of multiplications in a chain of matrices, for multiplying matrices of size (10
60),
assuming a O(n3)-time multiplication algorithm.

30)(30

5)(5

×

×

×

The algorithm proceeds by ﬁlling up an “upper-triangular” part of a dynamic programming matrix, where cell [i, j]
corresponds to the optimal number of operations when multiplying all the matrices between the ith and jth. Such an
algorithm may also be represented in a “pyramidal” form as below:

Additionally, the algorithm maintains (and returns) the optimal way to recursively divide each subsequence into two (by
5) (yielding 1, 500
storing the optimal dividing point, in green). Here, it is optimal to ﬁrst multiply (10
operations), then multiply the remaning matrices as (10

×
60) (yielding 3, 000 operations; 4, 500 in total).

30)(30

5)(5

×

×

×

Note that every pointer points into one of the original n input nodes (at the lowest level), and how each cell of the pyramid
corresponds to a pair of input nodes (specifying the corresponding range). Therefore, rather than creating O(n2) auxiliary
nodes, we instead record all relevant values above as edge scalars and edge pointers, and store nodes only for the lowest
level of the pyramid. Further, whether or not a particular edge has been populated yet (the “
” indicator above) is stored as
an additional binary ﬂag.

∞

Bellman-Ford As a representative graph algorithm, we visualise the steps of the Bellman-Ford algorithm for ﬁnding
single-source shortest paths in a given graph.

Initially, the source node is labelled with distance zero, and all other nodes with distance “
” (which, once again, is
represented as a binary node hint). The algorithm then iteratively relaxes all edges as follows, until convergence is achieved:

∞

Besides updating the distance values, the algorithm also maintains, and returns, the predicted shortest path tree – for each
node, a pointer to its predecessor along the optimal path from the source. By convention, the source node points to itself.
These pointers are visualised in green.

Na¨ıve String Matcher As a representative string algorithm, we visualise the steps of the na¨ıve string matcher, for detecting
string ""ab"" inside the string ""aab"".

∞

∞ ∞

∞ ∞ ∞

∞

∞ ∞

∞

4500

1500 9000

1500 9000

300

150 300

300

150 300

300

150 300

10

30

5

60

10

30

5

60

10

30

5

60

10

30

5

60

0

1

2

∞

2

2

∞

8

∞

3

∞

0

1

2

1

2

2

2

∞

8

3

∞

0

1

2

1

2

2

2

3

3

5

8

The CLRS Algorithmic Reasoning Benchmark

In this case, each character of the two strings is given a separate node, and three sets of indices are maintained: indicating
the start of the current candidate match (in blue); and the current position being checked in both the haystack (red) and the
needle (purple). The algorithm scans candidate positions left-to-right until a full match is detected for the ﬁrst time.

Additionally, each character is tagged with its predecessor in the string (in green), and a binary ﬂag indicating which of the
two strings it belongs to (not shown here).

C. Test results for all algorithms

Test performance for all 30 algorithms in CLRS-30 may be found in Table 2. In addition, we provide a “win-tie-loss” metric
as another way of differentiating model performance, which is less sensitive to outliers. The resulting counts are provided in
Table 3, and are computed as follows:

• Let µA(
Table 2).

M

) and σA(

) be the mean and standard deviation of model

M

’s test performance on algorithm A (as in

M

• We say that model

• If

=

.
A

∀X (cid:54)
• Otherwise, if

A (cid:31)

A

.

outperforms model

B

on algorithm A—denoted by

A

A (cid:31)

B

—if µA(

)
A

−

, then model

A

wins on algorithm A.

A

X

σA(

) > µA(

A

).

B

A

, then model

loses on algorithm A.

∃X
• Otherwise, model

X (cid:31)

A

A
is tied on algorithm A.

A

The win/tie/loss counts are then aggregated across all algorithms A to obtain a metric for each model. As already mentioned,
the details of this on a per-algorithm level are given in Table 3.

D. Validation results individual plots

Validation performance for all 30 algorithms in CLRS-30 may be found in Figure 4. For convenience, we also report the
early-stopped validation performance in Table 4.

a

a

b

a

b

a

a

b

a

b

a

a

b

a

b

a

a

b

a

b

The CLRS Algorithmic Reasoning Benchmark

Algorithm

Deep Sets

GAT

Memnet

MPNN

PGN

Table 2. Test performance of all models on all algorithms.

Activity Selector
Articulation Points
Bellman-Ford
BFS
Binary Search
Bridges
Bubble Sort
DAG Shortest Paths
DFS
Dijkstra
Find Max. Subarray
Floyd-Warshall
Graham Scan
Heapsort
Insertion Sort
Jarvis’ March
KMP Matcher
LCS Length
Matrix Chain Order
Minimum
MST-Kruskal
MST-Prim
Na¨ıve String Match
Optimal BST
Quickselect
Quicksort
Segments Intersect
SCC
Task Scheduling
Topological Sort

Overall average

1.67
4.04
0.85
0.38
0.88
2.65
3.24
2.42
0.73
3.10
0.39
0.90
2.75
12.57
4.65
0.81
0.54
5.25
3.58
2.08
4.71
5.47
0.29
1.36
1.33
2.16
0.60
2.61
0.70
3.57

66.09%
39.06%
51.33%
98.63%
47.97%
32.43%
50.73%
73.21%
7.44%
36.12%
12.48%
7.22%
64.71%
28.94%
40.98%
50.25%
3.22%
50.10%
78.36%
80.19%
60.58%
12.17%
2.05%
69.71%
3.21%
37.74%
77.29%
17.81%
84.84%
15.84%

±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
43.36%

1.37
1.62
1.19
0.21
3.12
6.60
1.77
1.37
2.04
0.79
0.43
3.14
2.70
1.83
1.87
10.25
0.36
1.02
3.31
2.95
0.99
4.34
1.20
1.75
0.95
0.98
0.04
3.12
2.09
6.92

73.23%
37.76%
87.91%
99.04%
23.50%
25.64%
9.91%
81.14%
11.78%
58.01%
24.43%
16.66%
77.89%
10.35%
29.52%
51.51%
3.03%
57.88%
78.19%
84.20%
65.72%
38.20%
3.01%
65.49%
4.36%
7.60%
90.41%
12.70%
84.69%
27.03%

±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
44.69%

2.22
0.61
1.46
0.04
0.46
0.05
0.78
1.92
1.61
2.39
0.08
0.13
2.31
1.57
0.86
3.87
0.00
4.34
1.03
0.11
0.61
3.77
0.48
1.21
0.03
0.67
0.90
4.78
0.04
0.11

24.10%
1.50%
40.04%
43.34%
14.37%
30.26%
73.58%
66.15%
13.36%
22.48%
13.05%
14.17%
40.62%
68.00%
71.42%
22.99%
1.81%
49.84%
81.96%
86.93%
28.84%
10.29%
1.22%
72.03%
1.74%
73.10%
71.81%
16.32%
82.74%
2.73%

±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
38.03%

3.16
2.18
0.28
0.05
0.26
4.78
0.60
0.56
0.51
0.50
0.49
1.77
0.31
0.84
2.08
12.39
0.86
0.36
1.40
0.88
1.50
7.56
0.30
0.44
0.69
0.10
0.10
4.88
0.32
6.24

80.66%
50.91%
92.01%
99.89%
36.83%
72.69%
5.27%
96.24%
6.54%
91.50%
20.30%
26.74%
91.04%
10.94%
19.81%
34.86%
2.49%
53.23%
79.84%
85.34%
70.97%
69.08%
3.92%
62.23%
1.43%
11.30%
93.44%
24.37%
84.11%
52.60%

±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
51.02%

1.62
2.09
0.34
0.29
0.13
7.82
1.95
0.16
0.24
1.75
2.56
0.51
1.61
0.18
2.43
1.07
0.12
0.21
0.49
0.52
1.36
0.98
0.20
1.82
0.42
0.15
0.75
0.64
0.91
2.69

66.80%
49.53%
92.99%
99.63%
76.95%
51.42%
6.01%
96.94%
8.71%
83.45%
65.23%
28.76%
56.87%
5.27%
44.37%
49.19%
2.00%
56.82%
83.91%
87.71%
66.96%
63.33%
2.08%
71.01%
3.66%
6.17%
77.51%
20.80%
84.89%
60.45%

±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
52.31%

The CLRS Algorithmic Reasoning Benchmark

Figure 4. Validation results on all 30 algorithms in CLRS-30, averaged over three seeds.

The CLRS Algorithmic Reasoning Benchmark

Table 3. Win/Tie/Loss counts of all models on all algorithms. Legend: W: win, T: tie, L: loss.

Algorithm

Deep Sets GAT Memnet MPNN PGN

Activity Selector
Articulation Points
Bellman-Ford
BFS
Binary Search
Bridges
Bubble Sort
DAG Shortest Paths
DFS
Dijkstra
Find Max. Subarray
Floyd-Warshall
Graham Scan
Heapsort
Insertion Sort
Jarvis’ March
KMP Matcher
LCS Length
Matrix Chain Order
Minimum
MST-Kruskal
MST-Prim
Na¨ıve String Match
Optimal BST
Quickselect
Quicksort
Segments Intersect
SCC
Task Scheduling
Topological Sort

L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
T
T
L
L
L
L
L
L
L
L
L
L
L
T
L

L
L
L
L
L
L
L
L
T
L
L
L
L
L
L
T
T
W
L
L
L
L
L
L
T
L
L
L
T
L

L
L
L
L
L
L
W
L
T
L
L
L
L
W
W
L
L
L
L
L
L
L
L
T
L
W
L
L
L
L

W
T
L
W
L
W
L
L
L
W
L
L
W
L
L
L
L
L
L
L
W
T
W
L
L
L
W
T
L
L

L
T
W
L
W
L
L
W
L
L
W
W
L
L
L
L
L
L
W
W
L
T
L
T
T
L
L
T
T
W

Overall counts

0/3/27

1/5/24

4/2/24

8/3/19

8/6/16

The CLRS Algorithmic Reasoning Benchmark

Algorithm

Deep Sets

GAT

Memnet

MPNN

PGN

Table 4. Early-stopped validation results of all models on all algorithms.

Activity Selector
Articulation Points
Bellman-Ford
BFS
Binary Search
Bridges
Bubble Sort
DAG Shortest Paths
DFS
Dijkstra
Find Max. Subarray
Floyd-Warshall
Graham Scan
Heapsort
Insertion Sort
Jarvis’ March
KMP Matcher
LCS Length
Matrix Chain Order
Minimum
MST-Kruskal
MST-Prim
Na¨ıve String Match
Optimal BST
Quickselect
Quicksort
Segments Intersect
SCC
Task Scheduling
Topological Sort

Overall average

0.17
0.31
0.14
0.00
0.41
0.05
1.02
0.28
1.26
0.42
0.22
0.04
0.24
0.33
0.28
0.42
0.21
0.36
0.02
0.11
2.01
0.32
0.15
0.14
0.92
1.12
0.12
1.23
0.04
0.81

83.50%
99.63%
81.12%
100.00%
93.34%
99.36%
81.51%
92.25%
62.76%
80.34%
91.41%
35.79%
87.66%
81.84%
89.58%
72.82%
98.03%
69.24%
94.46%
97.59%
83.79%
74.61%
49.80%
92.02%
42.30%
79.69%
77.49%
89.52%
99.16%
47.23%

±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
80.93%

0.50
0.00
0.14
0.00
0.17
0.00
1.01
0.05
0.64
0.40
0.32
0.09
0.11
2.23
0.58
0.16
0.08
0.19
0.03
0.21
0.25
0.14
0.00
0.49
1.86
0.40
0.16
0.00
0.04
0.00

92.40%
100.00%
99.28%
100.00%
95.72%
100.00%
95.44%
96.81%
99.22%
99.22%
95.00%
87.28%
97.85%
87.24%
95.18%
98.38%
99.76%
77.00%
99.37%
97.74%
97.93%
98.37%
100.00%
93.30%
83.82%
92.97%
90.82%
100.00%
99.80%
100.00%

±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
95.66%

2.15
1.03
0.42
0.09
0.28
1.13
0.14
0.05
0.45
0.70
0.08
0.04
1.58
0.28
0.14
6.61
0.00
0.24
0.10
0.10
0.95
0.28
0.20
0.40
0.25
0.24
1.08
1.43
0.09
0.50

34.59%
16.84%
68.75%
70.70%
20.33%
96.46%
92.64%
81.90%
47.72%
67.38%
27.91%
31.29%
53.53%
54.04%
94.40%
37.92%
9.67%
67.69%
93.91%
95.56%
64.65%
74.09%
9.91%
90.86%
6.56%
93.16%
71.57%
70.57%
84.80%
8.30%

±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
57.92%

0.39
0.00
0.05
0.00
0.12
0.00
1.84
0.05
0.00
0.14
0.37
0.03
0.15
0.11
0.19
0.25
0.05
0.42
0.04
0.05
0.17
0.09
0.00
0.11
0.78
0.40
0.20
0.00
0.00
0.00

93.89%
100.00%
99.48%
100.00%
94.19%
100.00%
94.53%
99.93%
100.00%
99.67%
95.13%
89.14%
98.45%
94.27%
96.74%
97.94%
99.87%
77.88%
99.12%
97.64%
99.71%
99.02%
100.00%
93.88%
88.74%
95.70%
93.84%
100.00%
100.00%
100.00%

±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
96.63%

0.19
0.00
0.05
0.00
0.08
0.00
5.46
0.00
0.00
0.05
0.16
0.15
0.27
0.67
0.82
0.36
0.99
0.04
0.03
0.14
0.08
0.14
0.08
0.27
0.17
1.42
0.18
0.05
0.08
0.00

82.26%
100.00%
99.35%
100.00%
94.17%
100.00%
87.17%
99.80%
100.00%
99.28%
95.30%
88.70%
89.06%
90.36%
84.57%
88.34%
94.14%
69.19%
99.21%
97.07%
99.12%
97.79%
50.33%
93.20%
54.02%
54.30%
78.32%
99.93%
99.06%
100.00%

±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
89.47%

","The CLRS Algorithmic Reasoning Benchmark Petar Veliˇckovi´c 1 Adri`a Puigdom`enech Badia 1 David Budden 1 Razvan Pascanu 1 Andrea Banino 1 Misha Dashevskiy 1 Raia Hadsell 1 Charles Blundell 1 2 2 0 2 n u J 4 ] G L . s c [ 2 v 9 5 6 5 1 . 5 0 2 2 : v i X r a Abstract Learning representations of algorithms is an emerging area of machine learning, seeking to bridge concepts from neural networks with clas- sical algorithms. Several important works have investigated whether neural networks can effec- tively reason like algorithms, typically by learning to execute them. The common trend in the area, however, is to generate targeted kinds of algorith- mic data to evaluate speciﬁc hypotheses, making results hard to transfer across publications, and increasing the barrier of entry. To consolidate progress and work towards uniﬁed evaluation, we propose the CLRS Algorithmic Reasoning Bench- mark, covering classical algorithms from the In- troduction to Algorithms textbook. Our bench- mark spans a variety of algorithmic reasoning procedures, including sorting, searching, dynamic programming, graph algorithms, string algorithms and geometric algorithms. We perform extensive experiments to demonstrate how several popular algorithmic reasoning baselines perform on these tasks, and consequently, highlight links to several open challenges. Our library is readily available at https://github.com/deepmind/clrs. 1. Introduction Neural networks and classical algorithms are two techniques that operate on diametrically opposite (and complementary) sides of problem-solving: neural networks can adapt and generalise to raw inputs, automatically extracting appro- priate features and a single neural network setup is often applicable to many separate tasks (Zamir et al., 2018). How- ever, they are hard to interpret, notoriously unreliable when extrapolating outside of the dataset they have been trained on, and rely on massive quantities of training data. On 1DeepMind. Correspondence to: Petar Veliˇckovi´c <petarv@deepmind.com>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). the other hand, algorithms trivially strongly generalise to inputs of arbitrary sizes, and can be veriﬁed or proven to be correct, with interpretable step-wise operations. Their shortcoming is that inputs must be made to conform to a par- ticular algorithm speciﬁcation, and looking at a separate task often requires coming up with an entirely new algorithm (Veliˇckovi´c & Blundell, 2021). Bringing the two sides closer together can therefore yield the kinds of improvements to performance, generalisation and interpretability that are unlikely to occur through archi- tectural gains alone. Accordingly, algorithmic modelling as a domain for testing neural networks has been gaining popularity over the last few years (Zaremba & Sutskever, 2014; Kaiser & Sutskever, 2015; Trask et al., 2018; Vinyals et al., 2015; Kool et al., 2018; Freivalds et al., 2019; Dwivedi et al., 2020; Chen et al., 2020; Tang et al., 2020; Veliˇckovi´c et al., 2019; Yan et al., 2020; Deac et al., 2020) due to its ability to highlight various reasoning limitations of existing architectures. Earlier work (Zaremba & Sutskever, 2014; Kaiser & Sutskever, 2015) focused on the need of long-term mem- ory capabilities when executing algorithms, which offered a good test-bed for various recurrent and memory architec- tures. Recently, algorithmic tasks have been used to high- light the efﬁciency of graph neural networks (Dwivedi et al., 2020; Chen et al., 2020; Veliˇckovi´c et al., 2019; Yan et al., 2020; Corso et al., 2020; Tang et al., 2020; Georgiev & Li´o, 2020; Veliˇckovi´c et al., 2020) and to distinguish between different variations of them, typically through the lens of algorithmic alignment—architectures that align better with the underlying algorithm can be proven to have better sam- ple complexity (Xu et al., 2019). Unfortunately, many of these works remain disconnected in terms of the algorithms they target, how the data is presented to the model or through the training and testing protocols they use, making direct comparison somewhat difﬁcult. To make a ﬁrst step towards a uniﬁed benchmark for al- gorithmic reasoning tasks, we propose a comprehensive dataset which we will refer to as The CLRS Algorithmic Reasoning Benchmark, in homage to the Introduction to Al- gorithms textbook by Cormen, Leiserson, Rivest and Stein (Cormen et al., 2009). The CLRS Algorithmic Reasoning Benchmark Within this benchmark, we propose and evaluate on CLRS- 30: a dataset containing trajectories—a trajectory is formed of inputs, the corresponding outputs and optional interme- diary targets—of 30 classical algorithms covering various forms of reasoning, including sorting, searching, dynamic programming, geometry, graphs and strings. Some of these algorithms are depicted in Figure 1. The appeal and moti- vation for such a benchmark goes beyond unifying or pro- viding a common ground for previous works, as we will describe. We believe that CLRS-30 is well positioned to ex- plore out-of-distribution (OOD) generalization and transfer (as potentially part of a meta-learning setting) given the ex- plicit and known relationship between different algorithms (e.g. what subroutines are shared and so forth). 2. Motivation Figure 1. Example of four algorithms within CLRS-30. A) in- sertion sort; B) string matching; C) greedy task scheduling; D) shortest paths. Timely posed benchmarks have led to a signiﬁcant progress in the ﬁeld, from the impact of ImageNet (Russakovsky et al., 2015) on the vision community, to that of Wikipedia and Penn Treebank in popularizing neural networks for lan- guage modelling (Merity et al., 2016; Mikolov et al., 2011) or Atari-2600 for deep reinforcement learning (Bellemare et al., 2013). The prevalence of recent works focusing on al- gorithmic reasoning1, as well as a history of disparate work on a variety of bespoke benchmarks (Graves et al., 2014; Zaremba & Sutskever, 2014; Kaiser & Sutskever, 2015; Trask et al., 2018), suggests signiﬁcant utility in a bench- mark covering a wide-range of classical CS algorithms. Learning to mimic an algorithm also provides an opportu- nity to extensively test the limitations of architectures both in terms of their representation capacity and processing. This can then be related back directly onto underlying oper- ations and qualities of the well-studied CS algorithms being mimicked as we are aware of both the process used to gen- erate the inputs and the speciﬁcs of the underlying function 1Concurrent works published at the same venue include: (Xu et al., 2019; Veliˇckovi´c et al., 2019) at ICLR’20 and (Veliˇckovi´c et al., 2020; Corso et al., 2020; Tang et al., 2020) at NeurIPS’20. producing the corresponding outputs. Hence, benchmarking in this area can be used to better understand the limitations of current architectures and the optimisation schemes used. This benchmarking can come in many forms: Data can be easily generated, allowing the neural network behaviour to be probed under different regimes: from few- shot learning all the way to inﬁnite-data. Algorithms can be used to understand the efﬁciency of dif- ferent inductive biases and neural components. For example, a recent study (Tang et al., 2020) has demonstrated the direct beneﬁts of choosing inductive biases that align well with iterative algorithms. Algorithms have also been used to high- light the importance of attention mechanisms (Graves et al., 2014) or to disambiguate various message passing mecha- nisms for graph neural networks (Richter & Wattenhofer, 2020; Joshi et al., 2020; Veliˇckovi´c et al., 2019). Algorithms can require repeated computation, recursion, or performing very different forms of computations con- ditioned on the input, providing an excellent test-bed for evaluating compositionality; i.e. whether an algorithm ex- ecutor can effectively exploit these repeated computations. One can control the amount of memory required to solve a problem instance, hence test the memorization ability of neural networks. Moreover, one can build a curriculum of tasks of increasing memory requirements (Zaremba & Sutskever, 2014). Control over the difﬁculty of problem instances also allows the behaviour of a trained model to be tested on OOD sam- ples. While neural networks are highly efﬁcient on solving complex perceptual tasks, current theoretical understanding suggests that their power relies on their ability to interpo- late (Liu et al., 2020; Belkin et al., 2019; Jacot et al., 2018), limiting them to in-distribution generalisation. General rea- soning systems, however, need to be able to expand beyond this type of generalization. OOD generalization (Li et al., 2020) is paramount, as generally one can not control the distribution a model will face over time when deployed. Understanding how algorithms operate on corner cases is a standard approach for analysing their correctness. Sim- ilarly, understanding the behaviour of a trained model on larger instances of the problem, or instances that expose such corner cases that were not covered in the training set, can elucidate to what degree the model has truly learned the algorithm (as opposed to overﬁtting to speciﬁc statistics of the training data). Particularly, we can control how far from the training distribution a test instance is, potentially allow- ing us to understand to what extent the model generalizes OOD, and under which circumstances. In turn, this can offer insight into the effectiveness of different inductive biases, highlighting what kinds of inductive biases are useful for mimicking reasoning processes. 2 3 4 5 6 1 1 2 4 5 6 3 A) B) b a c b a b a b a a b c b a b T s a b a b a c a S q C) D) a2 a3 a1 a1 a1 a4 0 6 7 2 8 7 5 -2 2 9 -3 -4 4 7 -2 The CLRS Algorithmic Reasoning Benchmark One would also expect a general reasoning system to be able to reuse parts of learned computations when learning a new task, and to compose learnt computational subrou- tines (Lake, 2019; Grifﬁths et al., 2019; Alet et al., 2018). These forms of generalization have been the aim of several learning paradigms from transfer learning to meta-learning and continual learning or domain adaptation. However, many of these paradigms rely on the concept of a task, and measuring or understanding the ability of a learned sys- tem to reuse or compose requires the ability to decompose a task into sub-tasks and to be able to relate tasks among themselves. In many scenarios, such decompositions are am- biguous. Without a clear segmentation into sub-tasks, there can be no clearly deﬁned distance metric between tasks (Du et al., 2018). Conversely, algorithms are built based on subroutines that tend to be extensively shared, providing a good playground for formalizing and measuring reuse and composition, making an algorithmic reasoning benchmark potentially attractive to meta-learning practitioners. Lastly and fundamentally, computer scientists rely on a rela- tively small2 number of algorithms to address an extremely vast set of problems. They can be seen as a very powerful basis that spans most forms of reasoning processes. On one hand, this means that any generic reasoning system likely has to be able to reproduce all such kinds of procedures, hence, building a system that properly learns all of them is a major stepping stone towards generic reasoning. On the other hand, this means that they can be used to discover inductive biases that will enable tackling more complex problems. This is either because these complex problems can be seen as a combination of several algorithms, or be- cause learning certain algorithms can provide a reliable way for the model to learn how to access its own memory or how to attend to its input or other such internal mechanisms. So by ﬁrst training on algorithms—potentially controlling the difﬁculty of training instances—one can pre-train for tasks where full trajectories may not be available (Veliˇckovi´c et al., 2021). One such example is discovering novel polynomial- time heuristics for combinatorial optimisation (Bengio et al., 2020; Cappart et al., 2021; Khalil et al., 2017) or reinforce- ment learning (Deac et al., 2021). Note that our focus with this benchmark lies in learning the basic algorithms them- selves only–this in itself proves sufﬁciently challenging for neural networks, and is itself a useful outcome for the rea- sons highlighted above. However, we speculate that once a neural network can learn not only individual algorithms but novel combinations of multiple algorithms or even discover new algorithms, such networks will be useful in a wide variety of problems from scientiﬁc problems such as pro- tein folding and genomics to simulated environments such as those used by reinforcement learning and control–much 2The entire Introduction to Algorithms textbook (Cormen et al., 2009) proposes and discusses ∼100 algorithms in total. as classic CS algorithms already make in-roads into these domains but lack the ability to learn from data. Guided by these observations, we regard CLRS-30 as a ﬁrst step towards a pragmatic setting to test many of these dif- ferent aspects of current architectures. While we do not directly target all of the scenarios outlined above, the bench- mark was built with ease of expansion in mind; enabling for extensive tweaking of training/testing setups, kinds of information captured in algorithm trajectories, as well as including additional algorithms, which we aim to do consis- tently over time. 3. CLRS Algorithmic Reasoning Benchmark Owing to its name, CLRS-30 consists only of algorithms which may be encountered in the CLRS textbook (Cormen et al., 2009). Further, all algorithm trajectories and relevant variables have been designed to match the pseudocode in the textbook as closely as possible. We begin by describing the selection criteria we applied when determining which algorithms to include in CLRS-30. Our initial survey of the textbook yielded 94 algorithms and data structures of interest. From this point, we set out to ﬁlter this set to algorithms suitable for inclusion in the initial version of our benchmark. The criteria we applied, with justiﬁcation and remarks, are as follows: We want to be able to reliably generate ground-truth outputs for large inputs. As such, NP-hard tasks (and approximation algorithms thereof) have been excluded. Our decision is backed up by theoretical work suggesting impossibility of accurately modelling NP-hard problems using polynomial- time samplers, unless NP=co-NP (Yehuda et al., 2020). Tasks requiring numerical outputs have been excluded. Eval- uating their performance is ambiguous, and may be depen- dent on the way architectures choose to represent numbers. For example, Yan et al. (2020) (which represents numbers in binary) and Veliˇckovi´c et al. (2019) (which represents them in ﬂoating-point) report different metrics on predicting shortest-path lengths. This excludes most number-theoretic algorithms, linear programming, and max-ﬂow3. It does not exclude shortest-path algorithms: we can treat them as tasks of ﬁnding edges belonging to the shortest path, as was done in Veliˇckovi´c et al. (2019); Tang et al. (2020). The numeri- cal values of path lengths are then treated as intermediate parts of the trajectory, and not directly evaluated on. Standalone data structures do not directly represent a task4. 3It should be noted that, by the max-ﬂow min-cut theorem (Ford Jr & Fulkerson, 2015), any max-ﬂow problem can be cast as ﬁnding the minimum cut containing the source vertex. This is a discrete decision problem over input vertices, which hence doesn’t violate our constraints, and could be included in future iterations. 4In programming language terms, their algorithms tend to be The CLRS Algorithmic Reasoning Benchmark Rather, their target is appropriately updating the internal state of the data structure. Hence, we don’t include their operations, unless they appear as components of algorithms. We, of course, look forward to including them in subsequent versions of the dataset, as they can provide useful building blocks for learning complex algorithms. Lastly, there are representational issues associated with dy- namically allocated memory—it may be unclear what is the best way to represent the internal memory storage and its usage in algorithm trajectories. One example of the ambi- guity is in asking whether the algorithm executor should start with a “scratch space” deﬁned by the space complexity of the problem that gets ﬁlled up, or dynamically generate such space5 (Strathmann et al., 2021). As such, we for now exclude all algorithms that require allocating memory which cannot be directly attached to the set of objects provided at input time. This excludes algorithms like merge sort, Hierholzer’s algorithm for ﬁnding Euler tours (Hierholzer & Wiener, 1873), or string matching using ﬁnite automata. All of the above applied, we arrive at the 30 algorithms that are selected into CLRS-30, which we categorize as follows: Sorting: Insertion sort, bubble sort, heapsort (Williams, 1964), quicksort (Hoare, 1962). Searching: Minimum, binary search, quickselect (Hoare, 1961). Divide and Conquer (D&C): Maximum subarray (Kadane’s variant (Bentley, 1984)). Greedy: Activity selection (Gavril, 1972), task scheduling (Lawler, 1985). Dynamic Programming: Matrix chain multiplication, longest common subsequence, optimal binary search tree (Aho et al., 1974). Graphs: Depth-ﬁrst and breadth-ﬁrst search (Moore, 1959), topological sorting (Knuth, 1973), articulation points, bridges, Kosaraju’s strongly-connected components algo- rithm (Aho et al., 1974), Kruskal’s and Prim’s algorithms for minimum spanning trees (Kruskal, 1956; Prim, 1957), Bellman-Ford and Dijkstra’s algorithms for single-source shortest paths (Bellman, 1958; Dijkstra et al., 1959) (+ di- rected acyclic graphs version), Floyd-Warshall algorithm for all-pairs shortest paths (Floyd, 1962). Strings: Na¨ıve string matching, Knuth-Morris-Pratt (KMP) string matcher (Knuth et al., 1977). Geometry: Segment intersection, Convex hull algorithms: Graham scan (Graham, 1972), Jarvis’ march (Jarvis, 1973). The chosen algorithms span a wide variety of reasoning of the void type. 5Akin to malloc-like calls in C++. procedures, and hence can serve as a good basis for algorith- mic reasoning evaluation, as well as extrapolation to more challenging problems. 3.1. Implementation, probes and representation We have implemented the selected 30 algorithms in an id- iomatic way, which aligns as closely as possible to the origi- nal pseudocode from Cormen et al. (2009). This allows us to automatically generate input/output pairs for all of them, enabling full control over the input data distribution, so long as it conforms to the preconditions of the algorithm. Further, we capture the intermediate algorithm trajectory in the form of “hints” (detailed in section 3.2), which allow insight into the inner workings of the algorithm. Such trajectories have already been extensively used in related work (Veliˇckovi´c et al., 2019; 2020; Georgiev & Li´o, 2020; Deac et al., 2020) and are typically crucial for OOD generalisation. In the most generic sense, algorithms can be seen as ma- nipulating sets of objects, along with any relations between them (which can themselves be decomposed into binary relations). If the sets are (partially) ordered (e.g. arrays or rooted trees), this can be imposed by including predecessor links. Therefore, algorithms generally operate over graphs. Motivated by existing theoretical results showing that graph neural networks align well with dynamic programming-style computations (Xu et al., 2019; Dudzik & Veliˇckovi´c, 2022), we propose a graph-oriented way to encode the data. Generally, our data is represented as a set of n vertices6, where n is a hyperparameter that is provided as part of the dataset generation process. When the semantics of these nodes are not immediately clear from the task (e.g. graph algorithms naturally operate over a graph of n nodes), we make an appropriate modiﬁcation to derive nodes. For example, in sorting algorithms, we treat every input list element as a separate node, and in string matching, we treat each character of the two input strings as a separate node. All information over these graphs falls under the following categorisation: Stage: Every feature, i.e. observation in the trajectory, is either part of the input, output, or the hints. As we do not cover algorithms that perform on-line querying, for all 30 algorithms there will be exactly one snapshot of the input and output values, whereas hints will be a time-series of intermediate algorithm states. Location: Every feature is either present within the nodes, edges (pairs of nodes) or the graph7. 6Edges are only present to represent the predecessor vertex if the input is a partially ordered. 7This also determines shapes of each feature, e.g. node features The CLRS Algorithmic Reasoning Benchmark Type: Every feature can be of ﬁve possible types, which can determine the appropriate method for encoding/decoding it, and the appropriate loss function to use when learning to predict it: • scalar: Floating-point scalar8 feature. This would typically be ﬁt using mean-squared error. • categorical: Categorical feature over K possi- ble classes. The type corresponds typically to cross- entropy loss over the classes. • mask: Categorical feature over two classes. This can be ﬁt using binary cross-entropy. • mask one: Categorical feature over two classes, where exactly one node is active (“one-hot”). One would generally optimise this argmax operation using categorical cross-entropy. • pointer: Categorical feature over the n nodes. To predict “similarity” score against every node, and typically optimised using categorical cross en- tropy (as introduced in Pointer Graph Networks (PGN) (Veliˇckovi´c et al., 2020)). Specifying a feature’s stage, location and type fully deter- mines its role in the dataﬂow. A tuple (stage, loc, type, values) is referred to as a probe. Each of the 30 algorithms has a static (w.r.t. stage, location and type) set of probes, which are considered to be a spec for the algorithm. We will later describe how these specs may be used to construct baseline architectures for the benchmark. Every node is always endowed with a position scalar input probe, which uniquely indexes it—the values are linearly spaced between 0 and 1 along the node index. This allows not only representing the data sequentially (when this is appropriate), but also serves as a useful tie-breaker when algorithms could make an arbitrary choice on which node to explore next—we force the algorithms to favour nodes with smaller position values. To illustrate these concepts further, at the end of this section we will describe the probes in detail for a popular algorithm (insertion sort). Note that, while we format the data in a way that clearly favours graph neural network executors, it can be easily adapted for different types of neural architectures; for exam- ple, sequence to sequence models (Sutskever et al., 2014). are of shape n × f ; edge features are of shape n × n × f ; graph features are of shape f , where f is the dimension of this feature (excluding batch axis). 8Given our current restriction on numerical predictions, scalar types will never be given in the output stage. Overall, CLRS-30 requires 1h to generate, and occupies 4.5GB when uncompressed, across all 30 tasks. ∼ ∼ 3.2. Hints Hints are an important component of our benchmark, which we ﬁnd fundamental in order to make progress on algorith- mic reasoning. As we previously argued, the advantage of algorithms as a task is our understanding of their behaviour, and our ability to decompose them into useful subroutines that can be shared or repeatedly applied. While, implicitly, we hope that such a decomposition would happen in any learned system, even when trained just using inputs and outputs (as studied in Xu et al. (2019)), the degree to which we can measure or encourage this is limited in the typical end-to-end learning process, and often most of the generalisation happens only in-distribution (as observed by Veliˇckovi´c et al. (2019); Xu et al. (2020); Bevilacqua et al. (2021)). The underlying algorithm may not be statistically identiﬁable from a small set of input/output pairs. Conversely, a perfect decomposition of a task into small subtasks can be generated for algorithmic problems. Then, individual models for each subtask may be trained and re- composed into a solution. Such an approach will, by con- struction, provide strong decompositional beneﬁts: as stud- ied by Yan et al. (2020), perfect OOD generalisation can be observed with such models, and they can even gener- alise zero-shot to test algorithms that reuse their modules. However, the downstream applicability of this is potentially limited; when faced with a novel task which cannot be easily decomposed into subtasks, it can be hard to decide how to reuse the learnt modules. We believe hints to lie in-between these two approaches. On one hand, they represent intermediate targets which the net- work should be able to predict if it performs reasoning simi- lar9 to the ground truth algorithm it is supposed to mimic. Indeed, several lines of recent work (Veliˇckovi´c et al., 2019; Georgiev & Li´o, 2020; Veliˇckovi´c et al., 2020; Deac et al., 2020) make favourable conclusions about using them, when it comes to achieving stronger OOD generalisation. Further- more, models leveraging hints are still end-to-end models; when faced with a novel task at test-time, we don’t need explicit knowledge of that task’s hints in order to re-use the weights learnt on a task which had them. Algorithms specify one way of attacking a problem, that is explicitly detailed through the hints. In this sense, insertion sort (to be presented shortly) is one way of implementing 9Note that architectures supervised in this way usually don’t model the hints perfectly, and will deviate from the target algorithm in subtle ways—Veliˇckovi´c et al. (2020) perform a qualitative study which shows GPU-specialised data structures could emerge as a result of such setups. The CLRS Algorithmic Reasoning Benchmark Figure 2. A sequence of hints for insertion sorting a list [5, 2, 4, 3, 1]. Green pointers correspond to the predecessor pointers (specifying the list’s state throughout the algorithm’s execution. Note how the head of the list always points to itself, by convention. Further, note how, at every step, the list is rewired such that the node selected by the blue pointer (slot) will point to the current iterator (pointed in red). a sorting function: all sorting algorithms model sorting functions, and will hence have identical outputs for identical inputs. The aspects that set the different sorting algorithms apart are exposed through their hints. Being mindful of the fact that neural networks commonly run on parallelisable architectures, we have made efforts to “compress” the hints as much as possible. For example, if a single for loop is used to sweep the data and detect the node which optimises a certain quantity (without doing any order-sensitive computations), that for loop can typi- cally be entirely “skipped” when recording hints: as parallel architectures may typically examine all the nodes at once. Further, we make every effort possible that the hint at step t + 1 will be predictable from the hints at step t by using only a single step of message passing. 3.3. Worked example: insertion sort To illustrate all of the concepts outlined above, we observe the trajectories extracted by our data collection procedure on an example: insertion sorting the array [5, 2, 4, 3, 1]. Insertion sort uses one pointer (j) to scan through the array, and then another pointer (i) to slot the j-th item into the correct place within [0..j]. This ascertains the invariant that, after k steps, the subarray of the ﬁrst k elements is com- pletely sorted. Hence the trajectory (with i and j marked) [2i, 5j, 4, 3, 1] is: [5i,j, 2, 4, 3, 1] → → [2,3i, 4, 5j, 1] [1i, 2, 3, 4, 5j]. Here, at each step, j scans along the array, and i indicates the correct place for the element that was j-th at the start of each iteration. [2, 4i, 5j, 3, 1] → → Converting this trajectory into a graph representation re- quires some considerations. Requiring the model to perform explicit swapping of node values would, ultimately, require numerical predictions. To avoid it, we ask the model to predict the predecessor pointer of each node (by conven- tion, the head of the array points to itself). Hence the actual recorded trajectory can be realised as depicted in Figure 2. In this ﬁgure, green pointers correspond to the predecessor pointers, red ones point to j, and blue ones point to i. i and j are realised as type mask one, whereas predecessors are of type pointer—and all three are stored in the nodes. The red and blue pointers represent the “hints” for this task. Finally, note that the original insertion sort pseudocode mandates that, at each iteration, i starts at position j and shifts backward until the right position is found. However, this procedure can be performed in one step by a GNN, as it can locate the correct position by examining all relevant positions, and we can omit all of those intermediate steps. In order to further illustrate how these hints are collected, we also provide an informal pseudocode for collecting hints for insertion sort in Algorithm 1: Algorithm 1 Hint updates for Insertion Sort Input :Input array val, Positions pos Hints :Predecessors pred, Iterator iter, swap slot slot i = 0 1 i > 0 ; // Initialise list pred[i] (cid:40) 0 i ← − 0, iter slot ← ← while iter < n do iter iter + 1 0 ← max node argmax j : pos[j]<pos[iter] ← val[j] if val[max node] < val[iter] then max node slot ← (cid:40) slot pred[i] i = iter otherwise pred[i] ← else slot argmin j : pos[j]<pos[iter],val[j]≥val[iter] ← val[j] pred[i] ←    iter iter pred[slot] max node pred[i] i = slot i=iter∧pred[slot]=slot i=iter∧pred[slot](cid:54)=slot pred[i] = iter otherwise end end return pred ; // Return final list In the interest of illustrating the hint structures further, we provide worked examples of trajectories for three more al- 5 2 4 3 1 5 2 4 3 1 5 2 4 3 1 5 2 4 3 1 5 2 4 3 1 The CLRS Algorithmic Reasoning Benchmark gorithms (dynamic programming, path-ﬁnding and string matching) in Appendix B. It should be remarked that we directly expose all of the hint collection routines as Python code inside the CLRS library, allowing for direct inspection. 4. Empirical evaluation Having surveyed the speciﬁcs of CLRS-30, we now present experimental results on it for several proposed algorithmic reasoning models. We primarily investigate whether a natu- ral ladder of model performance will emerge when extrapo- lating to larger inputs. Beyond this, we believe the bench- mark will be useful for empirically examining many other properties of algorithmic models, such as evaluating gener- alisation across different graph types, task types, or various multi-task (Xhonneux et al., 2021) or continual learning setups. We make available complete implementations of our data generating, probing and model training subroutines, which should make evaluating on such settings simple to deploy10. We survey several key ways of interacting with the benchmark (e.g. implementing baselines, modifying datasets, adding new algorithms) in Appendix A. 4.1. Baseline models Encode-process-decode For our experimental validation, we adopt the encode-process-decode paradigm of Hamrick et al. (2018), which is a common direction for several hint- based architectures (Veliˇckovi´c et al., 2019; Georgiev & Li´o, 2020; Veliˇckovi´c et al., 2020; Deac et al., 2020). Namely, we consider a setup with inputs xi in nodes, eij in edges, and g in the graph. We ﬁrst encode each of these using linear layers fn, fe, fg, to obtain encodings hi = fn(xi) hij = fe(eij) hg = fg(g) (1) We then feed these latents through a processor network to perform one step of computation. As we are focusing on graph representation learning in the current data format, most of our processors will be realised as graph neural net- works (Gilmer et al., 2017). Most generally, along every edge (i, j), a message from node i to node j, mij is com- puted (using a message function fm), and these messages are then aggregated across all neighbouring nodes using a permutation-invariant aggregation function, (cid:76). Finally, a readout network fr transforms these aggregated messages and the node encodings into processed node encodings: mij = fm(hi, hj, hij, hg) mi = (cid:77) i∈Nj mji h(cid:48) i = fr(hi, mi) (2) (3) Once node encodings are updated, we can decode them to make various predictions for this step of reasoning, depend- ing on the type of the prediction required (using relevant decoder functions g·), as prescribed in Section 3.1. Further, we keep track of previous-step node encodings h(t−1) , to explicitly use in a recurrent cell update (exactly as done by Veliˇckovi´c et al. (2019)). We opt to provide this recurrent update in order to provide long-range capacity to the model. i Lastly, we need to decide in what capacity will hints be used. We provide results for the option where hints are both decoded (used for computing the loss function) and encoded (considered as part of x, eij and g). At testing time, the encoded hint is equal to the hints decoded by the previous step, whereas we can stabilise these trajectories at training time by performing noisy teacher forcing—inspired by Noisy Nodes (Godwin et al., 2021), at each step we feed back ground-truth hints with probability 0.5. The quantity of hints is still used to determine the number of processor steps to perform at evaluation time. This requirement of knowing the hint-size can be lifted by, e.g., using termina- tion networks (Veliˇckovi´c et al., 2019; Banino et al., 2021) or aligning to iterative algorithms (Tang et al., 2020). Processor networks The only remaining component to specify is the processor network used by our models. As this component carries the most computational load, it is also the most obvious module to sweep over. We provide all implementations and hyperparameters within our codebase. Unless otherwise speciﬁed, we assume fully-connected graphs, i.e. , hence every node is con- } nected to every other node. We consider the following baseline processor networks: 1, 2, . . . , n { i = N Deep Sets (Zaheer et al., 2017); where each node is only (i.e., choice of (cid:76) is irrele- i connected to itself: } { vant). Such a model is popular for summary statistic tasks. i = N Graph Attention Networks (Veliˇckovi´c et al., 2017), where the aggregation function (cid:76) is self-attention (Vaswani et al., 2017), and the message function fm merely extracts the sender features: fm(hi, hj, hij, hg) = Whi. We report the best performance across GAT (Veliˇckovi´c et al., 2017) and GATv2 (Brody et al., 2021) attention mechanisms. Message-passing Neural Networks (Gilmer et al., 2017), which correspond exactly to the formulation in Equation 2, with (cid:76) = max, as prescribed by previous work (Veliˇckovi´c et al., 2019). As a sanity check, we also attempted (cid:76) = (cid:80) ﬁnding it underperformed on all tasks compared to max. Pointer Graph Networks (Veliˇckovi´c et al., 2020), which use only graph neighbourhoods i speciﬁed by a union of all node pointer and edge mask hints, and (cid:76) = max. This restricts the model to only reason over the edges deemed important by the inputs and hints. N 10https://github.com/deepmind/clrs Memory Networks (Sukhbaatar et al., 2015) have been The CLRS Algorithmic Reasoning Benchmark Figure 3. Validation results on eight representative algorithms in CLRS-30 (activity selector, Bellman-Ford, binary search, ﬁnd maximum subarray, Graham scan, insertion sort, matrix chain order, na¨ıve string matcher), averaged over three seeds. In all cases the y-axis is between [0, 100]%. Legend: MPNN red, PGN purple, Deep Sets blue, GAT orange, Memory Networks green. Validation results for all 30 individual algorithms can be found in Appendix D. used in the past as baseline for investigating reasoning in neural networks (e.g. Banino et al., 2020), as they provide an alternative way to use structural dependencies in a graph by treating edges as memories and nodes as queries. Here we used latents representing node features hi as queries and latents representing edge features hij (where there is a connecting edge and 0 otherwise) as memory inputs. 4.2. Dataset statistics For each algorithm in CLRS-30, we provide a canonical set of training, validation and test trajectories for benchmarking in- and out-of-distribution generalisation. We obtain these trajectories by running the algorithms on randomly sampled inputs that conform to their input speciﬁcation. This implies, e.g., that the inputs to most graph algorithms are Erd˝os- R´enyi graphs (Erd¨os & R´enyi, 2011) with a certain edge probability. All scalar inputs are sampled from U (0, 1). For validation, our aim is to measure in-distribution gener- alisation. Hence we sample inputs of 16 nodes for both, and generate 1,000 trajectories for training and 32 for validation. For testing, we measure out-of-distribution generalisation, and sample 32 trajectories for inputs of 64 nodes. For algo- rithms where the output is on the graph stage (rather than node/edge), we generate 64 more trajectories, in order to equalise the number of targets across tasks. × We optimise our models on the training trajectories in a teacher-forced fashion, with a batch size of 32, using the Adam optimiser (Kingma & Ba, 2014) with an initial learn- ing rate of η = 0.001. We train for 10, 000 steps, early stop- ping on the validation performance. Our models are trained on one V100 Volta GPU, requiring roughly between 1h and 30h to train, depending on the algorithm’s time complexity. For example, linear-time algorithms have signiﬁcantly fewer hints—hence message passing steps—than cubic-time ones. 4.3. Validation (in-distribution) performance We provide the in-distribution performance throughout train- ing in Figure 3, for eight representative tasks in CLRS-30 (one per each algorithm type); see Appendix D for the full results on all 30 algorithms. In this regime, the MPNN appears to dominate for most tasks: achieving over 90% F1 score for nearly all of them. While this might seem like strong evidence in favour of the fully-connected MPNNs, their added degrees of free- dom may also make MPNNs more prone to overﬁtting to speciﬁcs of the input (e.g. the input graphs’ sizes), rather than truly learning the underlying reasoning rule. We present the out-of-distribution results next, in order to make this distinction clear. The CLRS Algorithmic Reasoning Benchmark Table 1. Average test micro-F1 score of all models on all algorithm classes. The full test results for all 30 algorithms, along with a breakdown of the “win/tie/loss” metric, are given in Appendix C. Algorithm Deep Sets GAT Memnet MPNN PGN Divide & Conquer Dynamic Prog. Geometry Graphs Greedy Search Sorting Strings Overall average Win/Tie/Loss counts 0.67 7.79 6.60 8.09 6.81 18.29 7.19 0.68 12.48% 66.05% 64.08% 37.65% 75.47% 43.79% 39.60% 2.64% ± ± ± ± ± ± ± ± 42.72% 0/3/27 0.74 5.33 11.18 8.66 4.59 19.81 4.64 1.08 24.43% 67.19% 73.27% 46.80% 78.96% 37.35% 14.35% 3.02% ± ± ± ± ± ± ± ± 43.17% 1/5/24 0.00 7.75 11.65 5.20 20.73 21.67 1.09 0.21 13.05% 67.94% 45.14% 24.12% 53.42% 34.35% 71.53% 1.51% ± ± ± ± ± ± ± ± 38.88% 4/2/24 20.30% 65.10% 73.11% 62.79% 82.39% 41.20% 11.83% 3.21% 0.85 6.44 17.19 8.75 3.01 19.87 2.78 0.94 ± ± ± ± ± ± ± ± 44.99% 8/3/19 4.44 6.48 7.01 8.42 6.59 21.56 8.46 0.20 65.23% 70.58% 61.19% 60.25% 75.84% 56.11% 15.45% 2.04% ± ± ± ± ± ± ± ± 50.84% 8/6/16 4.4. Test (out-of-distribution) performance 5. Conclusion The averaged out-of-distribution performance (using the early-stopped model on validation) across each of the eight algorithm types is provided in Table 1; see Appendix C for the full results on all 30 algorithms. MPNNs are unable to transfer their impressive gains to graphs that are four times larger: in fact, the PGN takes over as the most performant model when averaged across task types—this aligns well with prior research (Veliˇckovi´c et al., 2020). The outperfor- mance is also observed when we count how frequently each model is among the best-performing models for a given algorithm, as per our “win/tie/loss” metric, which we ex- plain in Appendix C. GNN models, additionally, outperform models like Deep Sets and Memory Nets, reinforcing that GNNs are a useful primitive for algorithmic reasoning (Xu et al., 2019; Dudzik & Veliˇckovi´c, 2022). Aside from all of the above, we note that the OOD version of the CLRS-30 benchmark is highly challenging and far from solved for most tasks, making it a meaningful informant of future progress in the area. In particular, PGNs struggled on tasks requiring long-range rollouts (such as DFS), or recursive reasoning (such as Quicksort and Quickselect). This invites further research in algorithmic reasoners that can support such computation. It is further revealed that more specialised inductive biases and training regimes may be required to deal with string matching algorithms (such as KMP), and that the processor studied here tended to perform the best on tasks which were of favourable (sublinear) com- plexity in terms of hint counts (such as BFS, Bellman-Ford, and task scheduling). The speciﬁc results we obtain with our baselines validate several bits of prior research in the area, but also demon- strate we still have a long way to go, with even simple OOD scenarios only being ﬁt to about 50% micro-F1 performance. We introduce CLRS-30, a dataset that contains trajectories from 30 classical algorithms. This benchmark constitutes an effective way to test out-of-distribution generalization and transfer, and brings a means to evaluate algorithmic reasoning learnt by neural network models. The dataset provides input/output pairs for all algorithms, as well as intermediate trajectory information (“hints”). It is our hope that CLRS-30 will be a useful tool to shepherd future research in algorithmic reasoning, as prior art in the area largely generated their own datasets, making progress tracking challenging. Further, we hope that CLRS-30 will make algorithmic reasoning a more accessible area: one does not need a background in theoretical computer science to generate the dataset, and can focus on the modelling. If we convinced you to try out our library, please consult Appendix A for detailed instructions on most common ways to interact with our platform. CLRS is in constant develop- ment, and we welcome any and all feedback. Acknowledgements CLRS-30 was developed over a long time-frame, with many useful contributions, which we kindly acknowledge here. We would like to particularly thank Borja Ibarz for nu- merous ﬁxes and additions, and laying foundation for fu- ture iterations. Additionally, we warmly thank Jonathan Godwin, Sadegh Mahdavi, Euan Ong, MohamedElfatih Salah, Ahmed Elhag, Andreea Deac, Frederik Nijweide, Andrew Dudzik, Thomas Kipf, Amin Barekatain and Do- brik Georgiev for their support, and identifying numerous bugs during development. Finally, we thank Kim Stachen- feld, Nate Kushman and Daan Wierstra for reviewing the paper prior to submission, and anonymous reviewers for their careful feedback, strengthening the paper signiﬁcantly. The CLRS Algorithmic Reasoning Benchmark References Aho, A. V., Hopcroft, J. E., and Ullman, J. D. The design and analysis of computer algorithms. Reading, 1974. Alet, F., Lozano-Perez, T., and Kaelbling, L. P. Modular meta-learning. volume 87 of Proceedings of Machine Learning Research. PMLR, 2018. Banino, A., Badia, A. P., K¨oster, R., Chadwick, M. J., Zambaldi, V., Hassabis, D., Barry, C., Botvinick, M., Kumaran, D., and Blundell, C. Memo: A deep net- work for ﬂexible combination of episodic memories. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=rJxlc0EtDr. Banino, A., Balaguer, J., and Blundell, C. Pondernet: Learn- ing to ponder. arXiv preprint arXiv:2107.05407, 2021. Cormen, T. H., Leiserson, C. E., Rivest, R. L., and Stein, C. Introduction to algorithms. MIT press, 2009. Corso, G., Cavalleri, L., Beaini, D., Li`o, P., and Veliˇckovi´c, P. Principal neighbourhood aggregation for graph nets. arXiv preprint arXiv:2004.05718, 2020. Deac, A., Bacon, P.-L., and Tang, J. Graph neural induc- tion of value iteration. arXiv preprint arXiv:2009.12604, 2020. Deac, A.-I., Veliˇckovi´c, P., Milinkovic, O., Bacon, P.-L., Tang, J., and Nikolic, M. Neural algorithmic reasoners are implicit planners. Advances in Neural Information Processing Systems, 34, 2021. Dijkstra, E. W. et al. A note on two problems in connex- ion with graphs. Numerische mathematik, 1(1):269–271, 1959. Belkin, M., Hsu, D., and Xu, J. Two models of double de- scent for weak features. arXiv preprint arXiv:1903.07571, 2019. Du, Y., Czarnecki, W. M., Jayakumar, S. M., Pascanu, R., and Lakshminarayanan, B. Adapting auxiliary losses using gradient similarity, 2018. Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An evaluation plat- form for general agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, 2013. Bellman, R. On a routing problem. Quarterly of applied mathematics, 16(1):87–90, 1958. Bengio, Y., Lodi, A., and Prouvost, A. Machine learning for combinatorial optimization: a methodological tour d’horizon. European Journal of Operational Research, 2020. Bentley, J. Programming pearls: algorithm design tech- niques. Communications of the ACM, 27(9):865–873, 1984. Bevilacqua, B., Zhou, Y., and Ribeiro, B. Size-invariant graph representations for graph classiﬁcation extrapola- tions. In International Conference on Machine Learning, pp. 837–851. PMLR, 2021. Brody, S., Alon, U., and Yahav, E. How attentive are graph attention networks? arXiv preprint arXiv:2105.14491, 2021. Cappart, Q., Ch´etelat, D., Khalil, E., Lodi, A., Morris, C., and Veliˇckovi´c, P. Combinatorial optimization and reasoning with graph neural networks. arXiv preprint arXiv:2102.09544, 2021. Dudzik, A. and Veliˇckovi´c, P. Graph neural networks are dynamic programmers. arXiv preprint arXiv:2203.15544, 2022. Dwivedi, V. P., Joshi, C. K., Laurent, T., Bengio, Y., and Bresson, X. Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020. Erd¨os, P. and R´enyi, A. On the evolution of random graphs. In The structure and dynamics of networks, pp. 38–82. Princeton University Press, 2011. Floyd, R. W. Algorithm 97: shortest path. Communications of the ACM, 5(6):345, 1962. Ford Jr, L. R. and Fulkerson, D. R. Flows in networks. Princeton university press, 2015. Freivalds, K., Ozolin¸ ˇs, E., and ˇSostaks, A. Neural shufﬂe- exchange networks-sequence processing in o (n log n) In Advances in Neural Information Processing time. Systems, pp. 6630–6641, 2019. Gavril, F. Algorithms for minimum coloring, maximum clique, minimum covering by cliques, and maximum independent set of a chordal graph. SIAM Journal on Computing, 1(2):180–187, 1972. Georgiev, D. and Li´o, P. Neural bipartite matching. arXiv preprint arXiv:2005.11304, 2020. Chen, Z., Chen, L., Villar, S., and Bruna, J. Can graph arXiv preprint neural networks count substructures? arXiv:2002.04025, 2020. Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. Neural message passing for quantum chem- istry. arXiv preprint arXiv:1704.01212, 2017. The CLRS Algorithmic Reasoning Benchmark Godwin, J., Schaarschmidt, M., Gaunt, A. L., Sanchez- Gonzalez, A., Rubanova, Y., Veliˇckovi´c, P., Kirkpatrick, J., and Battaglia, P. Simple gnn regularisation for 3d molecular property prediction and beyond. In Interna- tional Conference on Learning Representations, 2021. Graham, R. L. An efﬁcient algorithm for determining the Info. Pro. Lett., 1: convex hull of a ﬁnite planar set. 132–133, 1972. Graves, A., Wayne, G., and Danihelka, I. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014. Grifﬁths, T., Callaway, F., Chang, M., Grant, E., Krueger, P., and Lieder, F. Doing more with less: meta-reasoning and meta-learning in humans and machines. Current Opinion in Behavioral Sciences, October 2019. Hamrick, J. B., Allen, K. R., Bapst, V., Zhu, T., McKee, K. R., Tenenbaum, J. B., and Battaglia, P. W. Relational inductive bias for physical construction in humans and machines. arXiv preprint arXiv:1806.01203, 2018. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Knuth, D. E. Fundamental algorithms. 1973. Knuth, D. E., Morris, Jr, J. H., and Pratt, V. R. Fast pattern matching in strings. SIAM journal on computing, 6(2): 323–350, 1977. Kool, W., van Hoof, H., and Welling, M. Attention, arXiv preprint learn to solve routing problems! arXiv:1803.08475, 2018. Kruskal, J. B. On the shortest spanning subtree of a graph and the traveling salesman problem. Proceedings of the American Mathematical society, 7(1):48–50, 1956. Lake, B. M. Compositional generalization through meta sequence-to-sequence learning. In Advances in Neural In- formation Processing Systems 32, pp. 9791–9801. 2019. Lawler, E. L. The traveling salesman problem: a guided tour of combinatorial optimization. Wiley-Interscience Series in Discrete Mathematics, 1985. Hennigan, T., Cai, T., Norman, T., and Babuschkin, I. Haiku: Sonnet for JAX, 2020. URL http://github.com/ deepmind/dm-haiku. Li, Y., Gimeno, F., Kohli, P., and Vinyals, O. Strong general- ization and efﬁciency in neural programs. arXiv preprint arXiv:2007.03629, 2020. Hierholzer, C. and Wiener, C. ¨Uber die m¨oglichkeit, einen linienzug ohne wiederholung und ohne unterbrechung zu umfahren. Mathematische Annalen, 6(1):30–32, 1873. Hoare, C. A. Algorithm 65: ﬁnd. Communications of the ACM, 4(7):321–322, 1961. Hoare, C. A. Quicksort. The Computer Journal, 5(1):10–16, 1962. Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems 31. 2018. Jarvis, R. A. On the identiﬁcation of the convex hull of a ﬁnite set of points in the plane. Information processing letters, 2(1):18–21, 1973. Joshi, C. K., Cappart, Q., Rousseau, L.-M., Laurent, T., and Bresson, X. Learning tsp requires rethinking generaliza- tion. arXiv preprint arXiv:2006.07054, 2020. Kaiser, Ł. and Sutskever, I. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228, 2015. Khalil, E., Dai, H., Zhang, Y., Dilkina, B., and Song, L. Learning combinatorial optimization algorithms over graphs. In Advances in Neural Information Processing Systems, pp. 6348–6358, 2017. Liu, C., Zhu, L., and Belkin, M. Toward a theory of optimization for over-parameterized systems of non- linear equations: the lessons of deep learning. CoRR, abs/2003.00307, 2020. Merity, S., Xiong, C., Bradbury, J., and Socher, R. arXiv preprint Pointer sentinel mixture models. arXiv:1609.07843, 2016. Mikolov, T., Deoras, A., Kombrink, S., Burget, L., and Cernock´y, J. Empirical evaluation and combination of In INTER- advanced language modeling techniques. SPEECH, pp. 605–608, 2011. Moore, E. F. The shortest path through a maze. In Proc. Int. Symp. Switching Theory, 1959, pp. 285–292, 1959. Prim, R. C. Shortest connection networks and some gen- eralizations. The Bell System Technical Journal, 36(6): 1389–1401, 1957. Richter, O. and Wattenhofer, R. Normalized attention with- out probability cage. arXiv preprint arXiv:2005.09561, 2020. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y. The CLRS Algorithmic Reasoning Benchmark Strathmann, H., Barekatain, M., Blundell, C., and Veliˇckovi´c, P. Persistent message passing. arXiv preprint arXiv:2103.01043, 2021. Xu, K., Li, J., Zhang, M., Du, S. S., Kawarabayashi, K.-i., and Jegelka, S. What can neural networks reason about? arXiv preprint arXiv:1905.13211, 2019. Sukhbaatar, S., Szlam, A., Weston, J., and Fergus, R. End-to- end memory networks. arXiv preprint arXiv:1503.08895, 2015. Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to se- quence learning with neural networks. In Advances in neural information processing systems, pp. 3104–3112, 2014. Tang, H., Huang, Z., Gu, J., Lu, B., and Su, H. Towards scale-invariant graph-related problem solving by itera- tive homogeneous gnns. the 34th Annual Conference on Neural Information Processing Systems (NeurIPS), 2020. Xu, K., Li, J., Zhang, M., Du, S. S., ichi Kawarabayashi, K., and Jegelka, S. How neural networks extrapolate: From feedforward to graph neural networks. arXiv preprint arXiv:2009.11848, 2020. Yan, Y., Swersky, K., Koutra, D., Ranganathan, P., and Heshemi, M. Neural execution engines: Learning to execute subroutines. arXiv preprint arXiv:2006.08084, 2020. Yehuda, G., Gabel, M., and Schuster, A. It’s not what machines can learn, it’s what we cannot teach. arXiv preprint arXiv:2002.09398, 2020. Trask, A., Hill, F., Reed, S. E., Rae, J., Dyer, C., and Blun- som, P. Neural arithmetic logic units. In Advances in Neural Information Processing Systems, pp. 8035–8044, 2018. Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R., and Smola, A. J. Deep sets. In Advances in neural information processing systems, pp. 3391–3401, 2017. Zamir, A. R., Sax, A., Shen, W., Guibas, L. J., Malik, J., and Savarese, S. Taskonomy: Disentangling task trans- fer learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3712–3722, 2018. Zaremba, W. and Sutskever, I. Learning to execute. arXiv preprint arXiv:1410.4615, 2014. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten- tion is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017. Veliˇckovi´c, P. and Blundell, C. Neural algorithmic reasoning. arXiv preprint arXiv:2105.02761, 2021. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. Veliˇckovi´c, P., Ying, R., Padovano, M., Hadsell, R., and Blundell, C. Neural execution of graph algorithms. arXiv preprint arXiv:1910.10593, 2019. Veliˇckovi´c, P., Buesing, L., Overlan, M. C., Pascanu, R., Vinyals, O., and Blundell, C. Pointer graph networks. arXiv preprint arXiv:2006.06380, 2020. Veliˇckovi´c, P., Boˇsnjak, M., Kipf, T., Lerchner, A., Hadsell, R., Pascanu, R., and Blundell, C. Reasoning-modulated representations. arXiv preprint arXiv:2107.08881, 2021. Vinyals, O., Fortunato, M., and Jaitly, N. Pointer networks. In Advances in Neural Information Processing Systems, pp. 2692–2700, 2015. Williams, J. W. J. Algorithm 232: heapsort. Commun. ACM, 7:347–348, 1964. Xhonneux, L.-P., Deac, A.-I., Veliˇckovi´c, P., and Tang, J. How to transfer algorithmic reasoning knowledge to learn new algorithms? Advances in Neural Information Pro- cessing Systems, 34, 2021. A. Interfacing with the CLRS benchmark The CLRS Algorithmic Reasoning Benchmark The CLRS benchmark is publicly hosted on GitHub: https://github.com/deepmind/clrs. All code and artifacts are released under an Apache 2.0 license, which is highly permissive. Within clrs/examples/run.py, we demonstrate an extensively conﬁgurable example script that evaluates a speciﬁc baseline on CLRS-30. Our baselines are provided in JAX and Haiku (Hennigan et al., 2020), but the dataset is generated using NumPy, making it possible to create learning pipelines in virtually any framework, including PyTorch and TensorFlow. We will now highlight three key ways in which researchers can interface with the library. A.1. Evaluating a new baseline on CLRS-30 To support a new baseline, the recommended path depends on how fundamentally different the baseline is to an encode- process-decode GNN. In most cases, we anticipate that only the processor network needs changing, and the remainder of the architec- ture can match our baselines. In this case, it is only necessary to implement the new processor network within clrs/ src/processors.py and appropriately set self.mpnn within the construct processor method in clrs/ src/baselines.py. For more fundamentally different baselines, it is necessary to create a new class that extends the Model API (as found within clrs/ src/model.py). clrs/ src/baselines.py provides one example of how this can be done efﬁciently, for the case of our baselines. A.2. Modifying the data distribution of CLRS-30 If users want to train and/or evaluate the models on different versions of the tasks given in CLRS-30, the key routines to modify are located in clrs/ src/samplers.py. The easiest modiﬁcation concerns the graph sizes and/or numbers of trajectories. They can be directly changed by modifying the CLRS30 dictionary near the top of the ﬁle. For more elaborate modiﬁcations, e.g. to the speciﬁc data sampling distributions, the users would need to modify and/or extend the relevant sampler class. As a guiding example, we provide a SortingSampler class which is convenient for generating inputs for sorting algorithms. The speciﬁc sampler used for each task is provided in the SAMPLERS dictionary towards the end of the ﬁle. A.3. Adding new algorithms to CLRS As the most elaborate of the three workﬂows, adding a new algorithm to the task suite requires following several steps, which are potentially comprehensive, depending on the complexity of the algorithm. However, the CLRS benchmark code still provides may helper routines for probing and batching that facilitate inclusion of novel algorithms. The steps are as follows: 1. First, determine the input/hint/output speciﬁcation of your algorithm, and include it within the SPECS dictionary of clrs/ src/specs.py. 2. Implement the desired algorithm in an abstractiﬁed form. Examples of this can be found throughout the clrs/ src/algorithms/ folder. 3. Next, choose appropriate moments within the algorithm’s execution to create probes that capture the inputs, outputs and all intermediate state (using the probing.push function). 4. Once generated, probes can be prepared using the probing.finalize method, and should be returned together with the algorithm output. 5. Lastly, implement an appropriate input data sampler for your algorithm, and include it within the SAMPLERS dictionary within clrs/ src/samplers.py. B. Additional worked examples of algorithm trajectories The CLRS Algorithmic Reasoning Benchmark Matrix Chain Order As a representative dynamic programming algorithm, we visualise the steps of the procedure for optimising the order of multiplications in a chain of matrices, for multiplying matrices of size (10 60), assuming a O(n3)-time multiplication algorithm. 30)(30 5)(5 × × × The algorithm proceeds by ﬁlling up an “upper-triangular” part of a dynamic programming matrix, where cell [i, j] corresponds to the optimal number of operations when multiplying all the matrices between the ith and jth. Such an algorithm may also be represented in a “pyramidal” form as below: Additionally, the algorithm maintains (and returns) the optimal way to recursively divide each subsequence into two (by 5) (yielding 1, 500 storing the optimal dividing point, in green). Here, it is optimal to ﬁrst multiply (10 operations), then multiply the remaning matrices as (10 × 60) (yielding 3, 000 operations; 4, 500 in total). 30)(30 5)(5 × × × Note that every pointer points into one of the original n input nodes (at the lowest level), and how each cell of the pyramid corresponds to a pair of input nodes (specifying the corresponding range). Therefore, rather than creating O(n2) auxiliary nodes, we instead record all relevant values above as edge scalars and edge pointers, and store nodes only for the lowest level of the pyramid. Further, whether or not a particular edge has been populated yet (the “ ” indicator above) is stored as an additional binary ﬂag. ∞ Bellman-Ford As a representative graph algorithm, we visualise the steps of the Bellman-Ford algorithm for ﬁnding single-source shortest paths in a given graph. Initially, the source node is labelled with distance zero, and all other nodes with distance “ ” (which, once again, is represented as a binary node hint). The algorithm then iteratively relaxes all edges as follows, until convergence is achieved: ∞ Besides updating the distance values, the algorithm also maintains, and returns, the predicted shortest path tree – for each node, a pointer to its predecessor along the optimal path from the source. By convention, the source node points to itself. These pointers are visualised in green. Na¨ıve String Matcher As a representative string algorithm, we visualise the steps of the na¨ıve string matcher, for detecting string ""ab"" inside the string ""aab"". ∞ ∞ ∞ ∞ ∞ ∞ ∞ ∞ ∞ ∞ 4500 1500 9000 1500 9000 300 150 300 300 150 300 300 150 300 10 30 5 60 10 30 5 60 10 30 5 60 10 30 5 60 0 1 2 ∞ 2 2 ∞ 8 ∞ 3 ∞ 0 1 2 1 2 2 2 ∞ 8 3 ∞ 0 1 2 1 2 2 2 3 3 5 8 The CLRS Algorithmic Reasoning Benchmark In this case, each character of the two strings is given a separate node, and three sets of indices are maintained: indicating the start of the current candidate match (in blue); and the current position being checked in both the haystack (red) and the needle (purple). The algorithm scans candidate positions left-to-right until a full match is detected for the ﬁrst time. Additionally, each character is tagged with its predecessor in the string (in green), and a binary ﬂag indicating which of the two strings it belongs to (not shown here). C. Test results for all algorithms Test performance for all 30 algorithms in CLRS-30 may be found in Table 2. In addition, we provide a “win-tie-loss” metric as another way of differentiating model performance, which is less sensitive to outliers. The resulting counts are provided in Table 3, and are computed as follows: • Let µA( Table 2). M ) and σA( ) be the mean and standard deviation of model M ’s test performance on algorithm A (as in M • We say that model • If = . A ∀X (cid:54) • Otherwise, if A (cid:31) A . outperforms model B on algorithm A—denoted by A A (cid:31) B —if µA( ) A − , then model A wins on algorithm A. A X σA( ) > µA( A ). B A , then model loses on algorithm A. ∃X • Otherwise, model X (cid:31) A A is tied on algorithm A. A The win/tie/loss counts are then aggregated across all algorithms A to obtain a metric for each model. As already mentioned, the details of this on a per-algorithm level are given in Table 3. D. Validation results individual plots Validation performance for all 30 algorithms in CLRS-30 may be found in Figure 4. For convenience, we also report the early-stopped validation performance in Table 4. a a b a b a a b a b a a b a b a a b a b The CLRS Algorithmic Reasoning Benchmark Algorithm Deep Sets GAT Memnet MPNN PGN Table 2. Test performance of all models on all algorithms. Activity Selector Articulation Points Bellman-Ford BFS Binary Search Bridges Bubble Sort DAG Shortest Paths DFS Dijkstra Find Max. Subarray Floyd-Warshall Graham Scan Heapsort Insertion Sort Jarvis’ March KMP Matcher LCS Length Matrix Chain Order Minimum MST-Kruskal MST-Prim Na¨ıve String Match Optimal BST Quickselect Quicksort Segments Intersect SCC Task Scheduling Topological Sort Overall average 1.67 4.04 0.85 0.38 0.88 2.65 3.24 2.42 0.73 3.10 0.39 0.90 2.75 12.57 4.65 0.81 0.54 5.25 3.58 2.08 4.71 5.47 0.29 1.36 1.33 2.16 0.60 2.61 0.70 3.57 66.09% 39.06% 51.33% 98.63% 47.97% 32.43% 50.73% 73.21% 7.44% 36.12% 12.48% 7.22% 64.71% 28.94% 40.98% 50.25% 3.22% 50.10% 78.36% 80.19% 60.58% 12.17% 2.05% 69.71% 3.21% 37.74% 77.29% 17.81% 84.84% 15.84% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 43.36% 1.37 1.62 1.19 0.21 3.12 6.60 1.77 1.37 2.04 0.79 0.43 3.14 2.70 1.83 1.87 10.25 0.36 1.02 3.31 2.95 0.99 4.34 1.20 1.75 0.95 0.98 0.04 3.12 2.09 6.92 73.23% 37.76% 87.91% 99.04% 23.50% 25.64% 9.91% 81.14% 11.78% 58.01% 24.43% 16.66% 77.89% 10.35% 29.52% 51.51% 3.03% 57.88% 78.19% 84.20% 65.72% 38.20% 3.01% 65.49% 4.36% 7.60% 90.41% 12.70% 84.69% 27.03% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 44.69% 2.22 0.61 1.46 0.04 0.46 0.05 0.78 1.92 1.61 2.39 0.08 0.13 2.31 1.57 0.86 3.87 0.00 4.34 1.03 0.11 0.61 3.77 0.48 1.21 0.03 0.67 0.90 4.78 0.04 0.11 24.10% 1.50% 40.04% 43.34% 14.37% 30.26% 73.58% 66.15% 13.36% 22.48% 13.05% 14.17% 40.62% 68.00% 71.42% 22.99% 1.81% 49.84% 81.96% 86.93% 28.84% 10.29% 1.22% 72.03% 1.74% 73.10% 71.81% 16.32% 82.74% 2.73% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 38.03% 3.16 2.18 0.28 0.05 0.26 4.78 0.60 0.56 0.51 0.50 0.49 1.77 0.31 0.84 2.08 12.39 0.86 0.36 1.40 0.88 1.50 7.56 0.30 0.44 0.69 0.10 0.10 4.88 0.32 6.24 80.66% 50.91% 92.01% 99.89% 36.83% 72.69% 5.27% 96.24% 6.54% 91.50% 20.30% 26.74% 91.04% 10.94% 19.81% 34.86% 2.49% 53.23% 79.84% 85.34% 70.97% 69.08% 3.92% 62.23% 1.43% 11.30% 93.44% 24.37% 84.11% 52.60% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 51.02% 1.62 2.09 0.34 0.29 0.13 7.82 1.95 0.16 0.24 1.75 2.56 0.51 1.61 0.18 2.43 1.07 0.12 0.21 0.49 0.52 1.36 0.98 0.20 1.82 0.42 0.15 0.75 0.64 0.91 2.69 66.80% 49.53% 92.99% 99.63% 76.95% 51.42% 6.01% 96.94% 8.71% 83.45% 65.23% 28.76% 56.87% 5.27% 44.37% 49.19% 2.00% 56.82% 83.91% 87.71% 66.96% 63.33% 2.08% 71.01% 3.66% 6.17% 77.51% 20.80% 84.89% 60.45% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 52.31% The CLRS Algorithmic Reasoning Benchmark Figure 4. Validation results on all 30 algorithms in CLRS-30, averaged over three seeds. The CLRS Algorithmic Reasoning Benchmark Table 3. Win/Tie/Loss counts of all models on all algorithms. Legend: W: win, T: tie, L: loss. Algorithm Deep Sets GAT Memnet MPNN PGN Activity Selector Articulation Points Bellman-Ford BFS Binary Search Bridges Bubble Sort DAG Shortest Paths DFS Dijkstra Find Max. Subarray Floyd-Warshall Graham Scan Heapsort Insertion Sort Jarvis’ March KMP Matcher LCS Length Matrix Chain Order Minimum MST-Kruskal MST-Prim Na¨ıve String Match Optimal BST Quickselect Quicksort Segments Intersect SCC Task Scheduling Topological Sort L L L L L L L L L L L L L L L T T L L L L L L L L L L L T L L L L L L L L L T L L L L L L T T W L L L L L L T L L L T L L L L L L L W L T L L L L W W L L L L L L L L T L W L L L L W T L W L W L L L W L L W L L L L L L L W T W L L L W T L L L T W L W L L W L L W W L L L L L L W W L T L T T L L T T W Overall counts 0/3/27 1/5/24 4/2/24 8/3/19 8/6/16 The CLRS Algorithmic Reasoning Benchmark Algorithm Deep Sets GAT Memnet MPNN PGN Table 4. Early-stopped validation results of all models on all algorithms. Activity Selector Articulation Points Bellman-Ford BFS Binary Search Bridges Bubble Sort DAG Shortest Paths DFS Dijkstra Find Max. Subarray Floyd-Warshall Graham Scan Heapsort Insertion Sort Jarvis’ March KMP Matcher LCS Length Matrix Chain Order Minimum MST-Kruskal MST-Prim Na¨ıve String Match Optimal BST Quickselect Quicksort Segments Intersect SCC Task Scheduling Topological Sort Overall average 0.17 0.31 0.14 0.00 0.41 0.05 1.02 0.28 1.26 0.42 0.22 0.04 0.24 0.33 0.28 0.42 0.21 0.36 0.02 0.11 2.01 0.32 0.15 0.14 0.92 1.12 0.12 1.23 0.04 0.81 83.50% 99.63% 81.12% 100.00% 93.34% 99.36% 81.51% 92.25% 62.76% 80.34% 91.41% 35.79% 87.66% 81.84% 89.58% 72.82% 98.03% 69.24% 94.46% 97.59% 83.79% 74.61% 49.80% 92.02% 42.30% 79.69% 77.49% 89.52% 99.16% 47.23% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 80.93% 0.50 0.00 0.14 0.00 0.17 0.00 1.01 0.05 0.64 0.40 0.32 0.09 0.11 2.23 0.58 0.16 0.08 0.19 0.03 0.21 0.25 0.14 0.00 0.49 1.86 0.40 0.16 0.00 0.04 0.00 92.40% 100.00% 99.28% 100.00% 95.72% 100.00% 95.44% 96.81% 99.22% 99.22% 95.00% 87.28% 97.85% 87.24% 95.18% 98.38% 99.76% 77.00% 99.37% 97.74% 97.93% 98.37% 100.00% 93.30% 83.82% 92.97% 90.82% 100.00% 99.80% 100.00% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 95.66% 2.15 1.03 0.42 0.09 0.28 1.13 0.14 0.05 0.45 0.70 0.08 0.04 1.58 0.28 0.14 6.61 0.00 0.24 0.10 0.10 0.95 0.28 0.20 0.40 0.25 0.24 1.08 1.43 0.09 0.50 34.59% 16.84% 68.75% 70.70% 20.33% 96.46% 92.64% 81.90% 47.72% 67.38% 27.91% 31.29% 53.53% 54.04% 94.40% 37.92% 9.67% 67.69% 93.91% 95.56% 64.65% 74.09% 9.91% 90.86% 6.56% 93.16% 71.57% 70.57% 84.80% 8.30% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 57.92% 0.39 0.00 0.05 0.00 0.12 0.00 1.84 0.05 0.00 0.14 0.37 0.03 0.15 0.11 0.19 0.25 0.05 0.42 0.04 0.05 0.17 0.09 0.00 0.11 0.78 0.40 0.20 0.00 0.00 0.00 93.89% 100.00% 99.48% 100.00% 94.19% 100.00% 94.53% 99.93% 100.00% 99.67% 95.13% 89.14% 98.45% 94.27% 96.74% 97.94% 99.87% 77.88% 99.12% 97.64% 99.71% 99.02% 100.00% 93.88% 88.74% 95.70% 93.84% 100.00% 100.00% 100.00% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 96.63% 0.19 0.00 0.05 0.00 0.08 0.00 5.46 0.00 0.00 0.05 0.16 0.15 0.27 0.67 0.82 0.36 0.99 0.04 0.03 0.14 0.08 0.14 0.08 0.27 0.17 1.42 0.18 0.05 0.08 0.00 82.26% 100.00% 99.35% 100.00% 94.17% 100.00% 87.17% 99.80% 100.00% 99.28% 95.30% 88.70% 89.06% 90.36% 84.57% 88.34% 94.14% 69.19% 99.21% 97.07% 99.12% 97.79% 50.33% 93.20% 54.02% 54.30% 78.32% 99.93% 99.06% 100.00% ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± ± 89.47%","['clrs', 'algorithmic', 'reasoning', 'benchmark', 'petar', 'veliˇckovi´c', 'adria', 'puigdomenech', 'hadsell', 'charle', 'blundell', 'x', 'abstract', 'learn', 'representation', 'algorithm', 'emerge', 'area', 'machine', 'learning', 'seek', 'bridge', 'concept', 'neural', 'network', 'sical', 'algorithm', 'several', 'important', 'work', 'investigate', 'neural', 'network', 'effec', 'tively', 'reason', 'algorithm', 'typically', 'learn', 'execute', 'common', 'trend', 'area', 'however', 'generate', 'targeted', 'kind', 'mic', 'datum', 'evaluate', 'speciﬁc', 'hypothesis', 'make', 'result', 'hard', 'transfer', 'publication', 'increase', 'barrier', 'entry', 'consolidate', 'progress', 'work', 'uniﬁed', 'evaluation', 'propose', 'clrs', 'algorithmic', 'reasoning', 'bench', 'mark', 'cover', 'classical', 'algorithm', 'troduction', 'algorithms', 'textbook', 'bench', 'mark', 'span', 'variety', 'algorithmic', 'reasoning', 'procedure', 'include', 'sort', 'search', 'dynamic', 'programming', 'graph', 'algorithm', 'string', 'algorithm', 'geometric', 'algorithm', 'perform', 'extensive', 'experiment', 'demonstrate', 'several', 'popular', 'algorithmic', 'reasoning', 'baseline', 'perform', 'task', 'consequently', 'highlight', 'link', 'several', 'open', 'challenge', 'library', 'readily', 'available', 'introduction', 'neural', 'network', 'classical', 'algorithm', 'technique', 'operate', 'diametrically', 'opposite', 'complementary', 'side', 'problemsolving', 'neural', 'network', 'adapt', 'generalise', 'raw', 'input', 'automatically', 'extract', 'appro', 'priate', 'feature', 'single', 'neural', 'network', 'setup', 'often', 'applicable', 'many', 'separate', 'task', 'zamir', 'et', 'ever', 'hard', 'interpret', 'notoriously', 'unreliable', 'extrapolate', 'dataset', 'train', 'rely', 'massive', 'quantity', 'training', 'datum', 'correspondence', 'petar', 'petarvdeepmindcom', 'proceeding', 'international', 'conference', 'machine', 'learn', 'copy', 'right', 'author', 'hand', 'algorithm', 'trivially', 'strongly', 'generalise', 'input', 'arbitrary', 'size', 'veriﬁe', 'prove', 'correct', 'interpretable', 'stepwise', 'operation', 'shortcoming', 'input', 'make', 'conform', 'par', 'ticular', 'algorithm', 'speciﬁcation', 'look', 'separate', 'task', 'often', 'require', 'come', 'entirely', 'new', 'blundell', 'bring', 'side', 'close', 'together', 'therefore', 'yield', 'kind', 'improvement', 'performance', 'generalisation', 'interpretability', 'unlikely', 'occur', 'archi', 'tectural', 'gain', 'alone', 'accordingly', 'algorithmic', 'modelling', 'domain', 'test', 'neural', 'network', 'gain', 'popularity', 'last', 'year', 'zaremba', 'sutskever', 'kaiser', 'sutskever', 'trask', 'vinyal', 'freivald', 'ability', 'highlight', 'various', 'reasoning', 'limitation', 'exist', 'architecture', 'early', 'work', 'zaremba', 'sutskever', 'kaiser', 'sutskever', 'focus', 'need', 'longterm', 'mem', 'ory', 'capability', 'execute', 'algorithm', 'offer', 'good', 'testbed', 'various', 'recurrent', 'memory', 'architec', 'ture', 'recently', 'algorithmic', 'task', 'use', 'high', 'light', 'efﬁciency', 'graph', 'neural', 'network', 'dwivedi', 'georgiev', 'veliˇckovi´c', 'distinguish', 'different', 'variation', 'typically', 'lens', 'algorithmic', 'alignment', 'architecture', 'align', 'well', 'underlying', 'prove', 'well', 'unfortunately', 'many', 'work', 'remain', 'disconnected', 'term', 'algorithm', 'target', 'datum', 'present', 'model', 'training', 'testing', 'protocol', 'use', 'make', 'direct', 'comparison', 'somewhat', 'difﬁcult', 'make', 'ﬁrst', 'step', 'uniﬁed', 'benchmark', 'reasoning', 'task', 'propose', 'comprehensive', 'dataset', 'refer', 'clrs', 'algorithmic', 'reasoning', 'benchmark', 'homage', 'introduction', 'textbook', 'corman', 'leiserson', 'rivest', 'stein', 'corman', 'clrs', 'algorithmic', 'reasoning', 'benchmark', 'benchmark', 'propose', 'evaluate', 'clrs', 'dataset', 'contain', 'trajectory', 'trajectory', 'form', 'input', 'correspond', 'output', 'optional', 'interme', 'diary', 'target', 'classical', 'algorithm', 'cover', 'various', 'form', 'reasoning', 'include', 'sort', 'search', 'dynamic', 'programming', 'geometry', 'graph', 'string', 'algorithm', 'depict', 'figure', 'appeal', 'benchmark', 'go', 'unifying', 'pro', 'vide', 'common', 'ground', 'previous', 'work', 'describe', 'believe', 'well', 'positioned', 'ex', 'plore', 'outofdistribution', 'generalization', 'transfer', 'potentially', 'part', 'metalearning', 'setting', 'give', 'plicit', 'know', 'relationship', 'different', 'algorithm', 'eg', 'subroutine', 'share', 'forth', 'motivation', 'figure', 'example', 'algorithm', 'sertion', 'sort', 'string', 'match', 'c', 'greedy', 'task', 'scheduling', 'short', 'path', 'timely', 'pose', 'benchmark', 'lead', 'signiﬁcant', 'progress', 'ﬁeld', 'impact', 'imagenet', 'russakovsky', 'vision', 'community', 'wikipedia', 'penn', 'treebank', 'popularize', 'neural', 'network', 'modelling', 'merity', 'mikolov', 'et', 'deep', 'reinforcement', 'learning', 'bellemare', 'prevalence', 'recent', 'work', 'focus', 'reasoning1', 'well', 'history', 'disparate', 'work', 'variety', 'bespoke', 'benchmark', 'grave', 'sutskever', 'kaiser', 'sutskever', 'trask', 'suggest', 'signiﬁcant', 'utility', 'bench', 'mark', 'cover', 'widerange', 'classical', 'cs', 'algorithm', 'learn', 'mimic', 'also', 'provide', 'opportu', 'nity', 'extensively', 'test', 'limitation', 'architecture', 'term', 'representation', 'capacity', 'processing', 'relate', 'back', 'directly', 'underlie', 'oper', 'ation', 'quality', 'wellstudied', 'cs', 'algorithm', 'mimic', 'aware', 'process', 'use', 'erate', 'input', 'speciﬁcs', 'underlie', 'function', 'work', 'publish', 'venue', 'include', 'neurips’20', 'produce', 'correspond', 'output', 'hence', 'benchmarke', 'area', 'use', 'well', 'understand', 'limitation', 'current', 'architecture', 'optimisation', 'scheme', 'use', 'benchmarking', 'come', 'many', 'form', 'datum', 'easily', 'generate', 'allow', 'neural', 'network', 'behaviour', 'probe', 'different', 'regime', 'shot', 'learn', 'way', 'inﬁnitedata', 'algorithm', 'use', 'understand', 'efﬁciency', 'dif', 'ferent', 'inductive', 'bias', 'neural', 'component', 'example', 'recent', 'study', 'tang', 'demonstrate', 'direct', 'beneﬁts', 'choose', 'inductive', 'bias', 'align', 'well', 'iterative', 'algorithm', 'algorithm', 'also', 'use', 'high', 'light', 'importance', 'attention', 'mechanism', 'grave', 'disambiguate', 'various', 'message', 'pass', 'mecha', 'nism', 'graph', 'neural', 'network', 'wattenhofer', 'algorithm', 'require', 'repeat', 'computation', 'recursion', 'perform', 'different', 'form', 'computation', 'ditione', 'input', 'provide', 'excellent', 'testbed', 'evaluate', 'compositionality', 'ie', 'ex', 'ecutor', 'effectively', 'exploit', 'repeat', 'computation', 'control', 'amount', 'memory', 'require', 'solve', 'problem', 'instance', 'hence', 'test', 'memorization', 'ability', 'neural', 'network', 'moreover', 'build', 'curriculum', 'task', 'increase', 'memory', 'requirement', 'zaremba', 'sutskever', 'control', 'difﬁculty', 'problem', 'instance', 'also', 'allow', 'behaviour', 'train', 'model', 'test', 'ple', 'neural', 'network', 'highly', 'efﬁcient', 'solve', 'complex', 'perceptual', 'task', 'current', 'theoretical', 'understanding', 'suggest', 'power', 'rely', 'ability', 'limit', 'indistribution', 'generalisation', 'general', 'sone', 'system', 'however', 'need', 'able', 'expand', 'type', 'generalization', 'ood', 'generalization', 'paramount', 'generally', 'control', 'distribution', 'model', 'face', 'time', 'deploy', 'understand', 'algorithm', 'operate', 'corner', 'case', 'standard', 'approach', 'analyse', 'correctness', 'sim', 'ilarly', 'understand', 'behaviour', 'train', 'model', 'large', 'instance', 'problem', 'instance', 'expose', 'corner', 'case', 'cover', 'training', 'set', 'elucidate', 'degree', 'model', 'truly', 'learn', 'oppose', 'overﬁtte', 'speciﬁc', 'statistic', 'training', 'datum', 'particularly', 'control', 'far', 'training', 'distribution', 'test', 'instance', 'potentially', 'allow', 'e', 'understand', 'extent', 'model', 'generalize', 'ood', 'circumstance', 'turn', 'offer', 'insight', 'effectiveness', 'different', 'inductive', 'bias', 'highlight', 'kind', 'inductive', 'bias', 'useful', 'mimic', 'reasoning', 'process', 'b', 'b', 'c', 'b', 'b', 'b', 'b', 'c', 'b', 'b', 'b', 'b', 'c', 'clrs', 'algorithmic', 'reasoning', 'benchmark', 'also', 'expect', 'general', 'reasoning', 'system', 'able', 'reuse', 'part', 'learn', 'computation', 'learn', 'new', 'task', 'compose', 'learnt', 'computational', 'subrou', 'tine', 'lake', 'grifﬁth', 'alet', 'form', 'generalization', 'aim', 'several', 'learn', 'paradigm', 'transfer', 'learn', 'metalearning', 'continual', 'learning', 'domain', 'adaptation', 'however', 'many', 'paradigm', 'rely', 'concept', 'task', 'measuring', 'understand', 'ability', 'learned', 'sy', 'reuse', 'compose', 'require', 'ability', 'decompose', 'task', 'subtask', 'able', 'relate', 'task', 'many', 'scenario', 'decomposition', 'biguous', 'clear', 'segmentation', 'subtask', 'clearly', 'deﬁne', 'distance', 'metric', 'task', 'du', 'conversely', 'algorithm', 'build', 'base', 'subroutine', 'tend', 'extensively', 'share', 'provide', 'good', 'playground', 'formalizing', 'measure', 'reuse', 'composition', 'make', 'algorithmic', 'reasoning', 'benchmark', 'potentially', 'attractive', 'metalearning', 'practitioner', 'lastly', 'fundamentally', 'computer', 'scientist', 'rely', 'rela', 'tively', 'small2', 'number', 'algorithm', 'address', 'extremely', 'vast', 'set', 'problem', 'see', 'powerful', 'basis', 'span', 'form', 'reasoning', 'process', 'hand', 'mean', 'generic', 'reasoning', 'system', 'likely', 'able', 'reproduce', 'kind', 'procedure', 'hence', 'build', 'system', 'properly', 'learn', 'major', 'stepping', 'stone', 'generic', 'reasoning', 'hand', 'mean', 'use', 'discover', 'inductive', 'bias', 'enable', 'tackle', 'complex', 'problem', 'complex', 'problem', 'see', 'combination', 'several', 'algorithm', 'learn', 'certain', 'algorithm', 'provide', 'reliable', 'way', 'model', 'learn', 'access', 'memory', 'attend', 'input', 'internal', 'mechanism', 'ﬁrst', 'training', 'algorithm', 'potentially', 'control', 'difﬁculty', 'training', 'instance', 'pretrain', 'task', 'full', 'trajectory', 'available', 'example', 'discover', 'novel', 'polynomial', 'time', 'heuristic', 'combinatorial', 'optimisation', 'bengio', 'khalil', 'reinforce', 'ment', 'learn', 'deac', 'note', 'focus', 'benchmark', 'lie', 'learn', 'basic', 'algorithm', 'selve', 'prove', 'sufﬁciently', 'challenge', 'neural', 'network', 'useful', 'outcome', 'rea', 'son', 'highlight', 'however', 'speculate', 'neural', 'network', 'learn', 'individual', 'algorithm', 'novel', 'combination', 'multiple', 'algorithm', 'even', 'discover', 'new', 'algorithm', 'network', 'useful', 'wide', 'variety', 'problem', 'scientiﬁc', 'problem', 'pro', 'tein', 'folding', 'genomic', 'simulate', 'environment', 'use', 'reinforcement', 'learning', 'control', 'much', 'entire', 'introduction', 'algorithm', 'textbook', 'corman', 'propose', 'discuss', '∼100', 'algorithm', 'total', 'classic', 'cs', 'algorithm', 'already', 'make', 'inroad', 'domain', 'lack', 'ability', 'learn', 'datum', 'guide', 'observation', 'regard', 'ﬁrst', 'step', 'pragmatic', 'setting', 'test', 'many', 'dif', 'ferent', 'aspect', 'current', 'architecture', 'directly', 'target', 'scenario', 'outline', 'bench', 'mark', 'build', 'ease', 'expansion', 'mind', 'enable', 'extensive', 'tweaking', 'trainingteste', 'setup', 'kind', 'information', 'capture', 'trajectory', 'well', 'include', 'additional', 'algorithm', 'aim', 'consis', 'tently', 'time', 'clrs', 'algorithmic', 'reasoning', 'benchmark', 'owe', 'name', 'clrs30', 'consist', 'algorithm', 'encounter', 'clrs', 'textbook', 'corman', 'far', 'algorithm', 'trajectory', 'relevant', 'variable', 'design', 'match', 'pseudocode', 'textbook', 'closely', 'possible', 'begin', 'describe', 'selection', 'criterion', 'apply', 'determine', 'algorithm', 'include', 'initial', 'survey', 'textbook', 'yield', 'algorithm', 'datum', 'structure', 'interest', 'point', 'set', 'ﬁlter', 'set', 'algorithm', 'suitable', 'inclusion', 'initial', 'version', 'benchmark', 'criterion', 'apply', 'justiﬁcation', 'remark', 'follow', 'want', 'able', 'reliably', 'generate', 'groundtruth', 'output', 'large', 'input', 'nphard', 'task', 'approximation', 'algorithm', 'thereof', 'exclude', 'decision', 'back', 'theoretical', 'work', 'suggest', 'impossibility', 'accurately', 'model', 'nphard', 'problem', 'use', 'polynomial', 'time', 'sampler', 'npconp', 'yehuda', 'task', 'require', 'numerical', 'output', 'exclude', 'eval', 'uate', 'performance', 'ambiguous', 'depen', 'dent', 'way', 'architecture', 'choose', 'represent', 'number', 'example', 'yan', 'represent', 'number', 'binary', 'represent', 'ﬂoatingpoint', 'report', 'different', 'metric', 'predict', 'shortestpath', 'length', 'exclude', 'numbertheoretic', 'algorithm', 'linear', 'programming', 'maxﬂow3', 'exclude', 'shortestpath', 'algorithm', 'treat', 'task', 'ﬁnde', 'edge', 'belong', 'short', 'path', 'numeri', 'cal', 'value', 'path', 'length', 'treat', 'intermediate', 'part', 'trajectory', 'directly', 'evaluate', 'standalone', 'datum', 'structure', 'directly', 'represent', 'task4', '3it', 'note', 'fulkerson', 'maxﬂow', 'problem', 'cast', 'ﬁnde', 'minimum', 'cut', 'contain', 'source', 'vertex', 'discrete', 'decision', 'problem', 'input', 'vertex', 'hence', 'violate', 'constraint', 'include', 'future', 'iteration', 'programming', 'language', 'term', 'algorithm', 'tend', 'clrs', 'algorithmic', 'reasoning', 'benchmark', 'rather', 'target', 'appropriately', 'update', 'internal', 'state', 'data', 'structure', 'hence', 'include', 'operation', 'appear', 'component', 'algorithm', 'course', 'look', 'forward', 'include', 'subsequent', 'version', 'dataset', 'provide', 'useful', 'building', 'block', 'learn', 'complex', 'algorithm', 'lastly', 'representational', 'issue', 'associate', 'dy', 'namically', 'allocate', 'memory', 'unclear', 'good', 'way', 'represent', 'internal', 'memory', 'storage', 'usage', 'trajectorie', 'example', 'ambi', 'guity', 'ask', 'executor', 'start', 'scratch', 'space', 'deﬁne', 'space', 'complexity', 'problem', 'ﬁlle', 'dynamically', 'generate', 'space5', 'strathmann', 'exclude', 'algorithm', 'require', 'allocate', 'memory', 'directly', 'attach', 'set', 'object', 'provide', 'input', 'time', 'exclude', 'algorithm', 'merge', 'hierholzer', 'ﬁnde', 'euler', 'tour', 'hierholzer', 'wiener', 'string', 'matching', 'use', 'ﬁnite', 'automata', 'apply', 'arrive', 'algorithm', 'select', 'categorize', 'follow', 'sort', 'insertion', 'sort', 'bubble', 'hoare', 'search', 'minimum', 'binary', 'search', 'quickselect', 'hoare', 'divide', 'conquer', 'variant', 'bentley', 'greedy', 'activity', 'selection', 'gavril', 'task', 'scheduling', 'lawler', 'dynamic', 'programming', 'matrix', 'chain', 'multiplication', 'long', 'common', 'subsequence', 'optimal', 'binary', 'search', 'tree', 'aho', 'graph', 'depthﬁrst', 'breadthﬁrst', 'search', 'moore', 'topological', 'sort', 'knuth', 'articulation', 'point', 'bridge', 'stronglyconnecte', 'component', 'algo', 'aho', '’s', 'algorithm', 'minimum', 'span', 'tree', 'bellmanford', 'dijkstra', '’s', 'algorithm', 'singlesource', 'short', 'path', 'bellman', 'dijkstra', 'recte', 'acyclic', 'version', 'floydwarshall', 'allpair', 'short', 'path', 'floyd', 'string', 'string', 'match', 'string', 'matcher', 'knuth', 'geometry', 'segment', 'intersection', 'hull', 'algorithm', 'graham', 'choose', 'algorithm', 'span', 'wide', 'variety', 'reasoning', 'void', 'type', 'malloclike', 'call', 'c', 'procedure', 'hence', 'serve', 'good', 'basis', 'algorith', 'mic', 'reasoning', 'evaluation', 'well', 'extrapolation', 'challenging', 'problem', 'implementation', 'probe', 'representation', 'implement', 'select', 'algorithm', 'iomatic', 'way', 'align', 'closely', 'possible', 'origi', 'nal', 'pseudocode', 'corman', 'allow', 'automatically', 'generate', 'inputoutput', 'pair', 'enable', 'full', 'control', 'input', 'datum', 'distribution', 'long', 'conform', 'precondition', 'far', 'capture', 'intermediate', 'trajectory', 'form', 'hint', 'detail', 'section', 'allow', 'insight', 'inner', 'working', 'trajectory', 'already', 'extensively', 'use', 'relate', 'work', 'georgiev', 'deac', 'typically', 'crucial', 'ood', 'generalisation', 'generic', 'sense', 'algorithm', 'see', 'nipulating', 'set', 'object', 'relation', 'decompose', 'binary', 'relation', 'set', 'partially', 'order', 'array', 'rooted', 'tree', 'impose', 'include', 'predecessor', 'link', 'therefore', 'algorithm', 'generally', 'operate', 'graph', 'motivate', 'exist', 'theoretical', 'result', 'show', 'graph', 'neural', 'network', 'align', 'well', 'dynamic', 'programmingstyle', 'computation', 'propose', 'graphoriente', 'way', 'encode', 'data', 'generally', 'datum', 'represent', 'set', 'vertices6', 'hyperparameter', 'provide', 'part', 'dataset', 'generation', 'process', 'semantic', 'node', 'immediately', 'clear', 'task', 'graph', 'algorithm', 'naturally', 'operate', 'graph', 'node', 'make', 'appropriate', 'modiﬁcation', 'derive', 'node', 'example', 'sort', 'algorithm', 'treat', 'input', 'list', 'element', 'separate', 'node', 'string', 'matching', 'treat', 'character', 'input', 'string', 'separate', 'node', 'information', 'graph', 'fall', 'follow', 'categorisation', 'stage', 'feature', 'observation', 'trajectory', 'part', 'input', 'output', 'hint', 'cover', 'algorithm', 'perform', 'online', 'querying', 'algorithm', 'exactly', 'snapshot', 'input', 'output', 'value', 'hint', 'timeserie', 'intermediate', 'state', 'location', 'feature', 'present', 'node', 'edge', 'pair', 'node', 'graph7', '6edge', 'present', 'represent', 'predecessor', 'vertex', 'input', 'partially', 'order', 'also', 'determine', 'shape', 'feature', 'node', 'feature', 'clrs', 'algorithmic', 'reasoning', 'benchmark', 'type', 'feature', 'ﬁve', 'possible', 'type', 'determine', 'appropriate', 'method', 'encodingdecode', 'appropriate', 'loss', 'function', 'use', 'learn', 'predict', 'scalar', 'floatingpoint', 'scalar8', 'feature', 'typically', 'use', 'meansquare', 'error', 'categorical', 'categorical', 'feature', 'ble', 'class', 'type', 'correspond', 'typically', 'cross', 'loss', 'class', 'mask', 'categorical', 'feature', 'class', 'use', 'binary', 'crossentropy', 'mask', 'categorical', 'feature', 'class', 'exactly', 'node', 'active', 'onehot', 'generally', 'optimise', 'argmax', 'operation', 'use', 'categorical', 'crossentropy', 'pointer', 'categorical', 'feature', 'n', 'node', 'predict', 'similarity', 'score', 'node', 'typically', 'optimise', 'use', 'categorical', 'cross', 'tropy', 'introduce', 'pointer', 'graph', 'network', 'pgn', 'specify', 'feature', '’s', 'stage', 'location', 'type', 'fully', 'deter', 'mine', 'role', 'dataﬂow', 'tuple', 'stage', 'loc', 'type', 'value', 'refer', 'probe', 'algorithm', 'static', 'wrt', 'stage', 'location', 'type', 'set', 'probe', 'consider', 'spec', 'algorithm', 'later', 'describe', 'spec', 'use', 'construct', 'baseline', 'architecture', 'benchmark', 'node', 'always', 'endow', 'position', 'scalar', 'input', 'probe', 'uniquely', 'index', 'value', 'linearly', 'space', 'index', 'allow', 'represent', 'datum', 'sequentially', 'appropriate', 'also', 'serve', 'useful', 'tiebreaker', 'algorithm', 'make', 'arbitrary', 'choice', 'node', 'explore', 'next', 'force', 'algorithm', 'favour', 'node', 'small', 'position', 'value', 'illustrate', 'concept', 'far', 'end', 'section', 'describe', 'probe', 'detail', 'popular', 'algorithm', 'insertion', 'sort', 'note', 'format', 'datum', 'way', 'clearly', 'favour', 'graph', 'neural', 'network', 'executor', 'easily', 'adapt', 'different', 'type', 'neural', 'architecture', 'exam', 'ple', 'sequence', 'sequence', 'model', 'sutskever', 'shape', '×', 'edge', 'feature', 'shape', '×', '×', 'graph', 'feature', 'shape', 'dimension', 'feature', 'exclude', 'batch', 'axis', 'current', 'restriction', 'numerical', 'prediction', 'scalar', 'type', 'never', 'give', 'output', 'stage', 'overall', 'clrs30', 'require', 'generate', 'occupy', 'gb', 'uncompressed', 'task', '∼', '∼', 'hint', 'hint', 'important', 'component', 'benchmark', 'ﬁnd', 'fundamental', 'order', 'make', 'progress', 'algorith', 'mic', 'reasoning', 'previously', 'argue', 'advantage', 'algorithm', 'task', 'understanding', 'behaviour', 'ability', 'decompose', 'useful', 'subroutine', 'share', 'repeatedly', 'apply', 'implicitly', 'hope', 'decomposition', 'happen', 'learned', 'system', 'even', 'train', 'use', 'input', 'output', 'study', 'degree', 'measure', 'encourage', 'limit', 'typical', 'endtoend', 'learning', 'process', 'often', 'generalisation', 'happen', 'indistribution', 'observe', 'underlie', 'statistically', 'identiﬁable', 'small', 'set', 'inputoutput', 'pair', 'conversely', 'perfect', 'decomposition', 'task', 'small', 'subtask', 'generate', 'algorithmic', 'problem', 'individual', 'model', 'subtask', 'train', 'compose', 'solution', 'approach', 'con', 'struction', 'provide', 'strong', 'decompositional', 'beneﬁts', 'ie', 'perfect', 'ood', 'generalisation', 'observe', 'model', 'even', 'gener', 'test', 'algorithm', 'reuse', 'module', 'however', 'downstream', 'applicability', 'potentially', 'limited', 'face', 'novel', 'task', 'easily', 'decompose', 'subtask', 'hard', 'decide', 'reuse', 'learnt', 'module', 'believe', 'hint', 'lie', 'inbetween', 'approach', 'hand', 'represent', 'intermediate', 'target', 'net', 'work', 'able', 'predict', 'perform', 'reasoning', 'simi', 'lar9', 'ground', 'truth', 'suppose', 'mimic', 'indeed', 'several', 'line', 'recent', 'work', 'georgiev', 'veliˇckovi´c', 'make', 'favourable', 'conclusion', 'use', 'come', 'achieve', 'strong', 'ood', 'generalisation', 'far', 'model', 'leverage', 'hint', 'still', 'endtoend', 'model', 'face', 'novel', 'task', 'testtime', 'need', 'explicit', 'knowledge', 'task', 'hint', 'order', 'reuse', 'weight', 'learn', 'task', 'algorithm', 'specify', 'way', 'attack', 'problem', 'explicitly', 'detailed', 'hint', 'sense', 'insertion', 'sort', 'present', 'shortly', 'way', 'implement', 'architecture', 'supervise', 'way', 'usually', 'model', 'hint', 'perfectly', 'deviate', 'target', 'subtle', 'way', 'veliˇckovi´c', 'perform', 'qualitative', 'study', 'show', 'gpuspecialise', 'datum', 'structure', 'emerge', 'result', 'setup', 'clrs', 'algorithmic', 'reasoning', 'benchmark', 'figure', 'sequence', 'hint', 'insertion', 'sort', 'list', 'green', 'pointer', 'correspond', 'predecessor', 'pointer', 'specify', 'list', 'state', '’s', 'execution', 'note', 'head', 'list', 'always', 'point', 'convention', 'far', 'note', 'step', 'list', 'rewire', 'node', 'select', 'blue', 'pointer', 'slot', 'point', 'current', 'iterator', 'point', 'sort', 'function', 'sort', 'algorithm', 'model', 'sorting', 'function', 'hence', 'identical', 'output', 'identical', 'input', 'aspect', 'set', 'different', 'sort', 'algorithm', 'apart', 'expose', 'hint', 'mindful', 'fact', 'neural', 'network', 'commonly', 'run', 'parallelisable', 'architecture', 'make', 'effort', 'compress', 'hint', 'much', 'possible', 'example', 'single', 'loop', 'use', 'sweep', 'datum', 'detect', 'node', 'optimise', 'certain', 'quantity', 'ordersensitive', 'computation', 'loop', 'typi', 'cally', 'entirely', 'skip', 'record', 'hint', 'parallel', 'architecture', 'typically', 'examine', 'node', 'far', 'make', 'effort', 'possible', 'hint', 'step', 'predictable', 'hint', 'step', 'use', 'single', 'step', 'message', 'pass', 'work', 'example', 'insertion', 'sort', 'illustrate', 'concept', 'outline', 'observe', 'trajectory', 'extract', 'datum', 'collection', 'procedure', 'example', 'insertion', 'sort', 'array', 'insertion', 'sort', 'use', 'pointer', 'scan', 'array', 'pointer', 'slot', 'jth', 'item', 'correct', 'place', '0j', 'ascertain', 'invariant', 'step', 'subarray', 'element', 'com', 'pletely', 'sort', 'hence', 'trajectory', 'mark', '2i', '5ij', 'step', 'scan', 'array', 'indicate', 'correct', 'place', 'element', 'start', 'iteration', 'convert', 'trajectory', 'graph', 'representation', 'quire', 'consideration', 'require', 'model', 'perform', 'explicit', 'swapping', 'node', 'value', 'ultimately', 'require', 'numerical', 'prediction', 'avoid', 'ask', 'model', 'predict', 'predecessor', 'pointer', 'node', 'conven', 'tion', 'head', 'array', 'point', 'hence', 'actual', 'recorded', 'trajectory', 'realise', 'depict', 'figure', 'ﬁgure', 'green', 'pointer', 'correspond', 'predecessor', 'pointer', 'red', 'one', 'point', 'blue', 'one', 'point', 'realise', 'type', 'mask', 'predecessor', 'type', 'pointer', 'store', 'node', 'red', 'blue', 'pointer', 'represent', 'hint', 'task', 'finally', 'note', 'original', 'insertion', 'sort', 'pseudocode', 'mandate', 'iteration', 'start', 'position', 'shift', 'backward', 'right', 'position', 'find', 'however', 'procedure', 'perform', 'step', 'gnn', 'locate', 'correct', 'position', 'examine', 'relevant', 'position', 'omit', 'intermediate', 'step', 'order', 'far', 'illustrate', 'hint', 'collect', 'also', 'provide', 'informal', 'pseudocode', 'collect', 'hint', 'insertion', 'sort', 'algorithm', 'hint', 'update', 'insertion', 'sort', 'input', 'input', 'array', 'val', 'position', 'pos', 'hint', 'predecessor', 'pre', 'iterator', 'iter', 'swap', 'slot', 'slot', 'initialise', 'list', 'predi', 'iter', 'iter', 'iter', 'iter', 'node', 'valiter', 'slot', 'predi', 'iter', 'otherwise', 'predi', 'slot', 'argmin', 'iter', 'iter', 'predslot', 'max', 'slot', 'iiter∧predslotslot', 'iiter∧predslotcid54slot', 'predi', 'iter', 'otherwise', 'end', 'end', 'return', 'pred', 'return', 'final', 'list', 'interest', 'illustrate', 'hint', 'structure', 'far', 'provide', 'work', 'example', 'trajectory', 'clrs', 'algorithmic', 'reasoning', 'benchmark', 'gorithm', 'dynamic', 'programming', 'pathﬁnding', 'string', 'matching', 'b', 'remark', 'directly', 'expose', 'hint', 'collection', 'routine', 'code', 'clrs', 'library', 'allow', 'direct', 'inspection', 'empirical', 'evaluation', 'survey', 'speciﬁcs', 'clrs30', 'present', 'experimental', 'result', 'several', 'propose', 'algorithmic', 'reasoning', 'model', 'primarily', 'investigate', 'natu', 'ral', 'ladder', 'model', 'performance', 'emerge', 'extrapo', 'lating', 'large', 'input', 'believe', 'bench', 'mark', 'useful', 'empirically', 'examine', 'many', 'property', 'algorithmic', 'model', 'evaluate', 'alisation', 'different', 'graph', 'type', 'task', 'type', 'various', 'multitask', 'continual', 'learning', 'setup', 'make', 'available', 'complete', 'implementation', 'datum', 'generating', 'probe', 'model', 'training', 'subroutine', 'make', 'evaluate', 'setting', 'simple', 'deploy10', 'survey', 'several', 'key', 'way', 'interact', 'benchmark', 'eg', 'implement', 'baseline', 'modify', 'dataset', 'add', 'new', 'algorithm', 'baseline', 'model', 'encodeprocessdecode', 'experimental', 'validation', 'adopt', 'encodeprocessdecode', 'paradigm', 'hamrick', 'common', 'direction', 'several', 'hint', 'base', 'architecture', 'georgiev', 'veliˇckovi´c', 'namely', 'consider', 'setup', 'edge', 'g', 'graph', 'ﬁrst', 'encode', 'use', 'linear', 'layer', 'fg', 'obtain', 'encoding', 'fnxi', 'hij', 'hg', 'fgg', 'feed', 'latent', 'processor', 'network', 'perform', 'step', 'computation', 'focus', 'graph', 'representation', 'learn', 'current', 'data', 'format', 'processor', 'realise', 'graph', 'neural', 'net', 'work', 'gilmer', 'generally', 'edge', 'message', 'node', 'com', 'put', 'use', 'message', 'function', 'fm', 'message', 'aggregate', 'neighbouring', 'node', 'use', 'permutationinvariant', 'aggregation', 'function', 'finally', 'readout', 'network', 'transform', 'aggregated', 'message', 'node', 'encoding', 'process', 'node', 'encoding', 'fmhi', 'hij', 'mji', 'hcid48', 'frhi', 'node', 'encoding', 'update', 'decode', 'make', 'various', 'prediction', 'step', 'reasoning', 'depend', 'e', 'type', 'prediction', 'require', 'use', 'relevant', 'decoder', 'function', 'prescribe', 'section', 'far', 'keep', 'track', 'previousstep', 'encoding', 'explicitly', 'use', 'recurrent', 'cell', 'update', 'exactly', 'opt', 'provide', 'recurrent', 'update', 'order', 'provide', 'longrange', 'capacity', 'model', 'lastly', 'need', 'decide', 'capacity', 'hint', 'use', 'provide', 'result', 'option', 'hint', 'decode', 'use', 'compute', 'loss', 'function', 'encode', 'consider', 'part', 'testing', 'time', 'encode', 'hint', 'equal', 'hint', 'decode', 'previous', 'step', 'stabilise', 'trajectory', 'training', 'time', 'perform', 'noisy', 'teacher', 'force', 'inspire', 'noisy', 'node', 'godwin', 'step', 'feed', 'back', 'groundtruth', 'hint', 'probability', 'quantity', 'hint', 'still', 'use', 'determine', 'number', 'processor', 'step', 'perform', 'evaluation', 'time', 'requirement', 'know', 'hintsize', 'lift', 'use', 'termina', 'tion', 'network', 'banino', 'align', 'iterative', 'algorithm', 'tang', 'processor', 'network', 'remain', 'component', 'specify', 'processor', 'network', 'use', 'model', 'component', 'carry', 'computational', 'load', 'also', 'obvious', 'module', 'sweep', 'provide', 'implementation', 'hyperparameter', 'codebase', 'otherwise', 'assume', 'fullyconnected', 'graph', 'hence', 'node', 'necte', 'node', 'consider', 'follow', 'baseline', 'processor', 'network', 'n', 'deep', 'set', 'zaheer', 'et', 'node', 'ie', 'choice', 'connect', 'vant', 'model', 'popular', 'summary', 'statistic', 'task', 'n', 'graph', 'attention', 'network', 'aggregation', 'function', 'selfattention', 'vaswani', 'message', 'function', 'merely', 'extract', 'sender', 'feature', 'fmhi', 'hij', 'whi', 'report', 'good', 'performance', 'gat', 'attention', 'mechanism', 'messagepasse', 'neural', 'network', 'gilmer', 'correspond', 'exactly', 'formulation', 'equation', 'max', 'prescribe', 'previous', 'work', 'sanity', 'check', 'also', 'attempt', 'ﬁnde', 'underperformed', 'task', 'compare', 'max', 'pointer', 'graph', 'network', 'use', 'graph', 'neighbourhood', 'speciﬁe', 'union', 'pointer', 'edge', 'mask', 'hint', 'max', 'restrict', 'model', 'reason', 'edge', 'deem', 'important', 'input', 'hint', 'memory', 'network', 'sukhbaatar', 'clrs', 'algorithmic', 'reasoning', 'benchmark', 'figure', 'validation', 'result', 'representative', 'algorithm', 'selector', 'binary', 'search', 'subarray', 'graham', 'scan', 'insertion', 'sort', 'matrix', 'chain', 'order', 'na¨ıve', 'string', 'matcher', 'average', 'seed', 'case', 'yaxis', 'legend', 'mpnn', 'red', 'purple', 'deep', 'set', 'blue', 'gat', 'orange', 'memory', 'network', 'green', 'validation', 'result', 'individual', 'algorithm', 'find', 'appendix', 'use', 'past', 'baseline', 'investigate', 'reasoning', 'neural', 'network', 'eg', 'banino', 'provide', 'alternative', 'way', 'use', 'structural', 'dependency', 'graph', 'treat', 'edge', 'memory', 'node', 'query', 'use', 'latent', 'represent', 'node', 'feature', 'query', 'latent', 'represent', 'edge', 'feature', 'hij', 'connect', 'edge', 'otherwise', 'memory', 'input', 'dataset', 'statistic', 'provide', 'canonical', 'set', 'training', 'validation', 'test', 'trajectory', 'benchmarke', 'outofdistribution', 'generalisation', 'obtain', 'trajectory', 'run', 'algorithm', 'randomly', 'sample', 'input', 'conform', 'input', 'speciﬁcation', 'imply', 'eg', 'input', 'graph', 'algorithm', 'r´enyi', 'graph', 'r´enyi', 'certain', 'edge', 'scalar', 'input', 'sample', 'validation', 'aim', 'measure', 'indistribution', 'gener', 'alisation', 'hence', 'sample', 'input', 'node', 'generate', 'trajectory', 'training', 'validation', 'testing', 'measure', 'outofdistribution', 'generalisation', 'sample', 'trajectory', 'input', 'node', 'algo', 'rithm', 'output', 'graph', 'stage', 'rather', 'nodeedge', 'generate', 'trajectory', 'order', 'equalise', 'number', 'target', 'task', '×', 'optimise', 'model', 'training', 'trajectory', 'teacherforced', 'fashion', 'batch', 'size', 'use', 'initial', 'learn', 'ing', 'rate', 'train', 'step', 'early', 'stop', 'pe', 'validation', 'performance', 'model', 'train', 'volta', 'gpu', 'require', 'roughly', '30h', 'train', 'depend', 'time', 'complexity', 'example', 'lineartime', 'algorithm', 'signiﬁcantly', 'hint', 'hence', 'message', 'pass', 'step', 'cubictime', 'one', 'validation', 'indistribution', 'performance', 'provide', 'indistribution', 'performance', 'train', 'ing', 'figure', 'representative', 'task', 'type', 'see', 'full', 'result', 'algorithm', 'regime', 'mpnn', 'appear', 'dominate', 'task', 'achieve', 'f1', 'score', 'seem', 'strong', 'evidence', 'favour', 'fullyconnected', 'mpnn', 'add', 'degree', 'free', 'dom', 'also', 'make', 'mpnn', 'prone', 'overﬁtte', 'speciﬁcs', 'input', 'eg', 'input', 'graph', 'size', 'rather', 'truly', 'learn', 'underlying', 'reasoning', 'rule', 'present', 'outofdistribution', 'result', 'next', 'order', 'make', 'distinction', 'clear', 'clrs', 'algorithmic', 'reasoning', 'benchmark', 'table', 'average', 'test', 'microf1', 'score', 'model', 'class', 'full', 'test', 'result', 'algorithm', 'breakdown', 'wintieloss', 'metric', 'give', 'deep', 'set', 'gat', 'memnet', 'conquer', 'dynamic', 'prog', 'geometry', 'graph', 'greedy', 'search', 'sort', 'string', 'overall', 'average', 'wintieloss', 'count', 'test', 'outofdistribution', 'performance', 'conclusion', 'average', 'outofdistribution', 'performance', 'use', 'earlystopped', 'model', 'validation', 'type', 'provide', 'table', 'see', 'full', 'result', 'algorithm', 'mpnn', 'unable', 'transfer', 'impressive', 'gain', 'graph', 'time', 'large', 'fact', 'pgn', 'take', 'performant', 'model', 'average', 'task', 'type', 'align', 'well', 'prior', 'research', 'outperfor', 'mance', 'also', 'observe', 'count', 'frequently', 'model', 'bestperforming', 'model', 'give', 'wintieloss', 'metric', 'ex', 'plain', 'model', 'additionally', 'outperform', 'model', 'deep', 'set', 'memory', 'net', 'reinforce', 'gnn', 'useful', 'primitive', 'algorithmic', 'reasoning', 'aside', 'note', 'ood', 'version', 'benchmark', 'highly', 'challenging', 'far', 'solve', 'task', 'make', 'meaningful', 'informant', 'future', 'progress', 'area', 'particular', 'pgn', 'struggle', 'task', 'require', 'longrange', 'rollout', 'dfs', 'recursive', 'reasoning', 'quicksort', 'quickselect', 'invite', 'research', 'algorithmic', 'reasoner', 'support', 'computation', 'far', 'reveal', 'specialised', 'inductive', 'bias', 'training', 'regime', 'require', 'deal', 'string', 'matching', 'algorithm', 'processor', 'study', 'tend', 'perform', 'good', 'task', 'favourable', 'sublinear', 'com', 'plexity', 'term', 'hint', 'count', 'bfs', 'bellmanford', 'task', 'scheduling', 'speciﬁc', 'result', 'obtain', 'baseline', 'validate', 'several', 'bit', 'prior', 'research', 'area', 'also', 'strate', 'still', 'long', 'way', 'go', 'even', 'simple', 'ood', 'scenario', 'ﬁt', 'microf1', 'performance', 'introduce', 'dataset', 'contain', 'trajectory', 'classical', 'algorithm', 'benchmark', 'constitute', 'effective', 'way', 'test', 'outofdistribution', 'generalization', 'transfer', 'bring', 'mean', 'evaluate', 'algorithmic', 'reasoning', 'learn', 'neural', 'network', 'model', 'dataset', 'provide', 'inputoutput', 'pair', 'algorithm', 'well', 'intermediate', 'trajectory', 'information', 'hint', 'hope', 'useful', 'tool', 'shepherd', 'future', 'research', 'algorithmic', 'reasoning', 'prior', 'art', 'area', 'largely', 'generate', 'dataset', 'make', 'progress', 'track', 'challenge', 'far', 'hope', 'make', 'algorithmic', 'reasoning', 'accessible', 'area', 'one', 'need', 'background', 'theoretical', 'computer', 'science', 'generate', 'dataset', 'focus', 'modelling', 'convince', 'try', 'library', 'consult', 'appendix', 'detailed', 'instruction', 'common', 'way', 'interact', 'platform', 'clrs', 'constant', 'develop', 'ment', 'welcome', 'feedback', 'acknowledgement', 'clrs30', 'develop', 'long', 'timeframe', 'many', 'useful', 'contribution', 'kindly', 'acknowledge', 'like', 'particularly', 'thank', 'merous', 'ﬁxe', 'addition', 'lay', 'foundation', 'fu', 'ture', 'iteration', 'additionally', 'warmly', 'thank', 'salah', 'andreea', 'deac', 'frederik', 'nijweide', 'brik', 'georgiev', 'support', 'identify', 'numerous', 'bug', 'development', 'finally', 'thank', 'review', 'paper', 'prior', 'submission', 'anonymous', 'reviewer', 'careful', 'feedback', 'strengthen', 'paper', 'signiﬁcantly', 'clrs', 'algorithmic', 'reasoning', 'benchmark', 'reference', 'aho', 'v', 'hopcroft', 'e', 'design', 'analysis', 'computer', 'algorithm', 'read', 'alet', 'kaelble', 'p', 'modular', 'metalearning', 'volume', 'proceeding', 'machine', 'learn', 'research', 'pmlr', 'banino', 'badia', 'p', 'k¨oster', 'r', 'chadwick', 'blundell', 'memo', 'deep', 'net', 'work', 'ﬂexible', 'combination', 'episodic', 'memory', 'international', 'conference', 'learn', 'representation', 'pondernet', 'learn', 'e', 'ponder', 'preprint', 'corman', 'h', 'leiserson', 'e', 'riv', 'r', 'l', 'stein', 'introduction', 'algorithm', 'mit', 'press', 'veliˇckovi´c', 'p', 'principal', 'neighbourhood', 'aggregation', 'graph', 'net', 'arxiv', 'preprint', 'deac', 'bacon', 'graph', 'neural', 'induc', 'tion', 'value', 'iteration', 'preprint', 'ai', 'p', 'milinkovic', 'bacon', 'neural', 'algorithmic', 'reasoner', 'implicit', 'planner', 'advance', 'neural', 'information', 'processing', 'system', 'dijkstra', 'e', 'w', 'note', 'problem', 'connex', 'ion', 'numerische', 'model', 'double', 'scent', 'weak', 'feature', 'preprint', 'adapt', 'auxiliary', 'loss', 'use', 'gradient', 'similarity', 'bellemare', 'naddaf', 'bowl', 'arcade', 'learn', 'environment', 'evaluation', 'plat', 'form', 'general', 'agent', 'journal', 'artiﬁcial', 'intelligence', 'research', '47253–279', 'bellman', 'r', 'routing', 'problem', 'quarterly', 'apply', 'mathematic', 'bengio', 'lodi', 'prouvost', 'machine', 'learning', 'combinatorial', 'optimization', 'methodological', 'tour', 'd’horizon', 'european', 'operational', 'research', 'bentley', 'program', 'pearl', 'design', 'tech', 'nique', 'communication', 'acm', 'bevilacqua', 'sizeinvariant', 'graph', 'representation', 'graph', 'classiﬁcation', 'extrapola', 'tion', 'international', 'conference', 'machine', 'learning', 'pmlr', 'brody', 'alon', 'yahav', 'e', 'attentive', 'graph', 'attention', 'network', 'arxiv', 'preprint', 'cappart', 'q', 'ch´etelat', 'khalil', 'e', 'lodi', 'c', 'p', 'combinatorial', 'optimization', 'reasoning', 'graph', 'neural', 'network', 'preprint', 'veliˇckovi´c', 'p', 'graph', 'neural', 'network', 'dynamic', 'programmer', 'preprint', 'dwivedi', 'p', 'bengio', 'bresson', 'benchmarke', 'graph', 'neural', 'network', 'preprint', 'erd¨os', 'p', 'evolution', 'random', 'graph', 'structure', 'dynamic', 'network', 'press', 'floyd', 'r', 'short', 'path', 'communication', 'acm', 'r', 'fulkerson', 'r', 'flow', 'network', 'press', 'freivald', 'ˇsostak', 'neural', 'shufﬂe', 'exchange', 'networkssequence', 'process', 'log', 'advance', 'neural', 'information', 'processing', 'time', 'system', 'gavril', 'f', 'algorithm', 'minimum', 'coloring', 'maximum', 'clique', 'minimum', 'covering', 'clique', 'maximum', 'independent', 'set', 'chordal', 'graph', 'compute', '12180–187', 'georgiev', 'p', 'neural', 'bipartite', 'match', 'preprint', 'graph', 'preprint', 'neural', 'network', 'count', 'substructure', 'gilmer', 'vinyal', 'dahl', 'g', 'neural', 'message', 'pass', 'quantum', 'preprint', 'clrs', 'algorithmic', 'reasoning', 'benchmark', 'godwin', 'gaunt', 'l', 'sanchez', 'gonzalez', 'rubanova', 'p', 'kirkpatrick', 'p', 'simple', 'regularisation', 'molecular', 'property', 'prediction', 'interna', 'tional', 'conference', 'learn', 'representation', 'graham', 'r', 'efﬁcient', 'algorithm', 'determine', 'info', 'hull', 'ﬁnite', 'planar', 'set', 'grave', 'wayne', 'g', 'neural', 'ture', 'machine', 'preprint', 'grifﬁth', 'grant', 'e', 'krueger', 'lieder', 'less', 'metareasoning', 'metalearning', 'human', 'machine', 'current', 'opinion', 'behavioral', 'science', 'hamrick', 'tenenbaum', 'relational', 'inductive', 'bias', 'physical', 'construction', 'human', 'machine', 'preprint', 'kingma', 'p', 'method', 'stochastic', 'optimization', 'arxiv', 'preprint', 'knuth', 'fundamental', 'algorithm', 'knuth', 'r', 'fast', 'pattern', 'match', 'string', 'journal', 'compute', 'h', 'well', 'attention', 'preprint', 'learn', 'solve', 'routing', 'problem', 'shortest', 'span', 'subtree', 'graph', 'travel', 'salesman', 'problem', 'proceeding', 'lake', 'b', 'compositional', 'generalization', 'meta', 'sequencetosequence', 'learning', 'advance', 'neural', 'formation', 'processing', 'system', 'pp', 'lawler', 'l', 'travel', 'salesman', 'problem', 'guide', 'tour', 'combinatorial', 'optimization', 'wileyinterscience', 'series', 'discrete', 'cai', 'babuschkin', 'haiku', 'sonnet', 'vinyal', 'strong', 'general', 'ization', 'efﬁciency', 'neural', 'preprint', 'hierholzer', '¨uber', 'die', 'linienzug', 'ohne', 'wiederholung', 'und', 'ohne', 'unterbrechung', 'hoare', 'c', 'ﬁnd', 'communication', 'acm', 'hoare', 'quicksort', 'computer', 'jacot', 'kernel', 'convergence', 'generalization', 'neural', 'network', 'advance', 'neural', 'information', 'processing', 'system', 'jarvis', 'r', 'identiﬁcation', 'convex', 'hull', 'ﬁnite', 'set', 'point', 'plane', 'information', 'processing', 'letter', 'laurent', 'bresson', 'learn', 'tsp', 'require', 'rethink', 'generaliza', 'tion', 'preprint', 'kaiser', 'sutskever', 'neural', 'learn', 'preprint', 'khalil', 'e', 'learn', 'combinatorial', 'optimization', 'algorithm', 'graph', 'advance', 'neural', 'information', 'processing', 'system', 'l', 'belkin', 'theory', 'optimization', 'overparameterized', 'system', 'non', 'linear', 'equation', 'lesson', 'deep', 'learning', 'abs200300307', 'bradbury', 'socher', 'r', 'preprint', 'pointer', 'mixture', 'model', 'mikolov', 'kombrink', 'burget', 'l', 'empirical', 'evaluation', 'combination', 'inter', 'advanced', 'language', 'modeling', 'technique', 'speech', 'moore', 'short', 'path', 'maze', 'int', 'symp', 'switch', 'theory', 'prim', 'r', 'c', 'short', 'connection', 'network', 'eralization', 'richter', 'wattenhofer', 'r', 'normalize', 'attention', 'probability', 'cage', 'preprint', 'krause', 'karpathy', 'khosla', 'bernstein', 'imagenet', 'large', 'scale', 'visual', 'recognition', 'challenge', 'international', 'journal', 'computer', 'vision', 'ijcv', 'doi', 'clrs', 'algorithmic', 'reasoning', 'benchmark', 'strathmann', 'p', 'persistent', 'message', 'pass', 'preprint', 'neural', 'network', 'reason', 'preprint', 'fergus', 'r', 'end', 'memory', 'network', 'preprint', 'sutskever', 'vinyal', 'q', 'v', 'sequence', 'se', 'quence', 'learn', 'neural', 'network', 'advance', 'neural', 'information', 'processing', 'system', 'scaleinvariant', 'graphrelate', 'problem', 'solve', 'itera', 'tive', 'homogeneous', 'gnn', '34th', 'annual', 'conference', 'neural', 'information', 'processing', 'system', 'neurip', 'neural', 'network', 'extrapolate', 'feedforward', 'graph', 'neural', 'network', 'preprint', 'heshemi', 'neural', 'execution', 'engine', 'learn', 'execute', 'subroutine', 'preprint', 'yehuda', 'schuster', '’', 'machine', 'learn', '’', 'teach', 'preprint', 'trask', 'hill', 'dyer', 'som', 'p', 'neural', 'arithmetic', 'logic', 'unit', 'advance', 'neural', 'information', 'processing', 'system', 'zaheer', 'salakhutdinov', 'r', 'r', 'smola', 'deep', 'set', 'advance', 'neural', 'information', 'processing', 'system', 'zamir', 'r', 'sax', 'shen', 'savarese', 'taskonomy', 'disentangle', 'task', 'learning', 'proceeding', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'zaremba', 'w', 'sutskever', 'learn', 'execute', 'arxiv', 'preprint', 'vaswani', 'shazeer', 'n', 'kaiser', 'ł', 'atten', 'tion', 'need', 'advance', 'neural', 'information', 'processing', 'system', '5998–6008', 'veliˇckovi´c', 'p', 'blundell', 'neural', 'algorithmic', 'reasoning', 'arxiv', 'preprint', 'veliˇckovi´c', 'p', 'cucurull', 'g', 'casanova', 'romero', 'lio', 'p', 'bengio', 'graph', 'attention', 'network', 'arxiv', 'preprint', 'veliˇckovi´c', 'p', 'ying', 'r', 'hadsell', 'r', 'blundell', 'neural', 'execution', 'graph', 'arxiv', 'preprint', 'veliˇckovi´c', 'p', 'buese', 'l', 'overlan', 'pascanu', 'r', 'vinyal', 'blundell', 'pointer', 'graph', 'network', 'arxiv', 'preprint', 'veliˇckovi´c', 'p', 'boˇsnjak', 'hadsell', 'r', 'pascanu', 'r', 'blundell', 'c', 'reasoningmodulate', 'representation', 'arxiv', 'preprint', 'vinyal', 'jaitly', 'n', 'pointer', 'network', 'advance', 'neural', 'information', 'processing', 'system', 'heapsort', 'commun', 'acm', 'transfer', 'algorithmic', 'reasoning', 'knowledge', 'learn', 'new', 'algorithm', 'advance', 'neural', 'information', 'pro', 'cesse', 'system', 'interfacing', 'clrs', 'benchmark', 'clrs', 'algorithmic', 'reasoning', 'benchmark', 'clrs', 'benchmark', 'publicly', 'host', 'code', 'artifact', 'release', 'apache', 'license', 'highly', 'permissive', 'clrsexamplesrunpy', 'demonstrate', 'extensively', 'conﬁgurable', 'example', 'script', 'evaluate', 'speciﬁc', 'baseline', 'baseline', 'provide', 'dataset', 'generate', 'use', 'numpy', 'make', 'possible', 'create', 'learn', 'pipeline', 'virtually', 'framework', 'include', 'pytorch', 'tensorflow', 'highlight', 'key', 'way', 'researcher', 'interface', 'library', 'evaluate', 'new', 'baseline', 'clrs30', 'support', 'new', 'baseline', 'recommend', 'path', 'depend', 'fundamentally', 'different', 'baseline', 'encode', 'processdecode', 'gnn', 'case', 'anticipate', 'processor', 'network', 'need', 'change', 'remainder', 'architec', 'ture', 'match', 'baseline', 'case', 'necessary', 'implement', 'new', 'processor', 'network', 'clrs', 'srcprocessorspy', 'appropriately', 'set', 'selfmpnn', 'construct', 'processor', 'method', 'clrs', 'srcbaselinespy', 'fundamentally', 'different', 'baseline', 'necessary', 'create', 'new', 'class', 'extend', 'model', 'api', 'find', 'clrs', 'srcmodelpy', 'clrs', 'srcbaselinespy', 'provide', 'example', 'efﬁciently', 'case', 'baseline', 'modify', 'datum', 'distribution', 'clrs30', 'user', 'want', 'train', 'evaluate', 'model', 'different', 'version', 'task', 'give', 'key', 'routine', 'modify', 'locate', 'clrs', 'srcsamplerspy', 'easy', 'modiﬁcation', 'concern', 'graph', 'size', 'andor', 'number', 'trajectory', 'directly', 'change', 'modify', 'clrs30', 'dictionary', 'top', 'elaborate', 'modiﬁcation', 'eg', 'speciﬁc', 'datum', 'sample', 'distribution', 'user', 'need', 'modify', 'andor', 'extend', 'relevant', 'sampler', 'class', 'guide', 'example', 'provide', 'sortingsampler', 'class', 'convenient', 'generate', 'input', 'sort', 'algorithm', 'speciﬁc', 'sampler', 'use', 'task', 'provide', 'sampler', 'dictionary', 'end', 'a3', 'add', 'new', 'algorithm', 'clrs', 'elaborate', 'workﬂow', 'add', 'new', 'algorithm', 'task', 'suite', 'require', 'follow', 'several', 'step', 'potentially', 'comprehensive', 'depend', 'complexity', 'however', 'clrs', 'benchmark', 'code', 'still', 'provide', 'helper', 'routine', 'probe', 'batch', 'facilitate', 'inclusion', 'novel', 'algorithm', 'step', 'follow', 'first', 'determine', 'inputhintoutput', 'speciﬁcation', 'algorithm', 'include', 'spec', 'dictionary', 'clrs', 'srcspecspy', 'implement', 'desire', 'abstractiﬁed', 'form', 'example', 'find', 'clrs', 'srcalgorithm', 'folder', 'next', 'choose', 'appropriate', 'moment', '’s', 'execution', 'create', 'probe', 'capture', 'input', 'output', 'intermediate', 'state', 'use', 'probingpush', 'function', 'generate', 'probe', 'prepare', 'use', 'probingfinalize', 'method', 'return', 'together', 'output', 'lastly', 'implement', 'appropriate', 'input', 'datum', 'sampler', 'algorithm', 'include', 'sampler', 'dictionary', 'clrs', 'srcsamplerspy', 'b', 'additional', 'work', 'example', 'trajectorie', 'clrs', 'algorithmic', 'reasoning', 'benchmark', 'matrix', 'chain', 'order', 'representative', 'dynamic', 'programming', 'visualise', 'step', 'procedure', 'optimise', 'order', 'multiplication', 'chain', 'matrix', 'multiply', 'matrix', 'size', 'assume', 'multiplication', '×', '×', '×', 'proceed', 'ﬁlle', 'uppertriangular', 'part', 'dynamic', 'programming', 'matrix', 'cell', 'correspond', 'optimal', 'number', 'operation', 'matrix', 'also', 'represent', 'pyramidal', 'form', 'additionally', 'maintain', 'return', 'optimal', 'way', 'recursively', 'divide', 'subsequence', 'yield', 'store', 'optimal', 'dividing', 'point', 'green', 'optimal', 'operation', 'multiply', 'remane', 'matrix', '×', 'yield', 'operation', 'total', '×', '×', '×', 'note', 'pointer', 'point', 'original', 'input', 'node', 'low', 'level', 'cell', 'pyramid', 'correspond', 'pair', 'input', 'node', 'specify', 'corresponding', 'range', 'therefore', 'rather', 'create', 'on2', 'auxiliary', 'node', 'instead', 'record', 'relevant', 'value', 'edge', 'scalar', 'edge', 'pointer', 'store', 'node', 'low', 'level', 'pyramid', 'far', 'particular', 'edge', 'populate', 'yet', 'indicator', 'store', 'additional', 'binary', 'bellmanford', 'representative', 'graph', 'algorithm', 'visualise', 'step', 'ﬁnde', 'singlesource', 'short', 'path', 'give', 'graph', 'initially', 'source', 'node', 'label', 'distance', 'node', 'distance', 'represent', 'binary', 'node', 'hint', 'iteratively', 'relax', 'edge', 'follow', 'convergence', 'achieve', 'update', 'distance', 'value', 'also', 'maintain', 'return', 'predict', 'short', 'path', 'tree', 'node', 'pointer', 'predecessor', 'optimal', 'path', 'source', 'convention', 'source', 'node', 'point', 'pointer', 'visualise', 'string', 'matcher', 'representative', 'string', 'visualise', 'step', 'string', 'matcher', 'detect', 'string', 'string', 'clrs', 'algorithmic', 'reasoning', 'benchmark', 'case', 'character', 'string', 'give', 'separate', 'node', 'set', 'index', 'maintain', 'indicate', 'start', 'current', 'candidate', 'match', 'current', 'position', 'check', 'haystack', 'red', 'needle', 'purple', 'scan', 'candidate', 'position', 'lefttoright', 'full', 'match', 'detect', 'ﬁrst', 'time', 'additionally', 'character', 'tag', 'predecessor', 'string', 'green', 'binary', 'ﬂag', 'indicating', 'string', 'belong', 'show', 'test', 'result', 'algorithm', 'test', 'performance', 'algorithm', 'find', 'table', 'addition', 'provide', 'wintieloss', 'metric', 'way', 'differentiate', 'model', 'performance', 'less', 'sensitive', 'outlier', 'result', 'count', 'provide', 'table', 'compute', 'follow', 'let', 'µa', 'table', 'mean', 'standard', 'deviation', 'model', 'test', 'performance', 'say', 'model', 'otherwise', 'outperform', 'model', 'denote', 'b', '−', 'model', 'win', 'x', 'σa', 'b', 'model', 'lose', '∃x', 'otherwise', 'model', 'tie', 'wintieloss', 'count', 'aggregate', 'algorithm', 'obtain', 'metric', 'model', 'already', 'mention', 'detail', 'peralgorithm', 'level', 'give', 'table', 'validation', 'result', 'individual', 'plot', 'validation', 'performance', 'algorithm', 'find', 'figure', 'convenience', 'also', 'report', 'earlystoppe', 'validation', 'performance', 'table', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'clrs', 'algorithmic', 'reasoning', 'benchmark', 'deep', 'set', 'gat', 'memnet', 'mpnn', 'table', 'test', 'performance', 'model', 'algorithm', 'activity', 'selector', 'articulation', 'point', 'bellmanford', 'binary', 'search', 'bridge', 'bubble', 'sort', 'dag', 'short', 'path', 'find', 'max', 'subarray', 'graham', 'scan', 'heapsort', 'insertion', 'matcher', 'lcs', 'length', 'matrix', 'chain', 'order', 'mstkruskal', 'mstprim', 'string', 'match', 'optimal', 'bst', 'quickselect', 'quicksort', 'segment', 'intersect', 'scc', 'task', 'scheduling', 'topological', 'sort', 'overall', 'average', 'clrs', 'algorithmic', 'reasoning', 'benchmark', 'figure', 'validation', 'result', 'algorithm', 'average', 'seed', 'clrs', 'algorithmic', 'reasoning', 'benchmark', 'table', 'wintieloss', 'count', 'model', 'algorithm', 'legend', 'tie', 'l', 'loss', 'deep', 'set', 'gat', 'memnet', 'mpnn', 'activity', 'selector', 'articulation', 'point', 'bellmanford', 'binary', 'search', 'bridge', 'bubble', 'sort', 'dag', 'short', 'path', 'find', 'max', 'subarray', 'graham', 'scan', 'heapsort', 'insertion', 'matcher', 'lcs', 'length', 'matrix', 'chain', 'order', 'mstkruskal', 'mstprim', 'string', 'match', 'optimal', 'bst', 'quickselect', 'quicksort', 'segment', 'intersect', 'scc', 'task', 'scheduling', 'topological', 'sort', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'overall', 'count', 'clrs', 'algorithmic', 'reasoning', 'benchmark', 'deep', 'set', 'gat', 'memnet', 'mpnn', 'table', 'earlystoppe', 'validation', 'result', 'model', 'algorithm', 'activity', 'selector', 'articulation', 'point', 'bellmanford', 'binary', 'search', 'bridge', 'bubble', 'sort', 'dag', 'short', 'path', 'find', 'max', 'subarray', 'graham', 'scan', 'heapsort', 'insertion', 'matcher', 'lcs', 'length', 'matrix', 'chain', 'order', 'mstkruskal', 'mstprim', 'string', 'match', 'optimal', 'bst', 'quickselect', 'quicksort', 'segment', 'intersect', 'scc', 'task', 'scheduling', 'topological', 'sort', 'overall', 'average']"
"Learning Task-relevant Representations for Generalization via
  Characteristic Functions of Reward Sequence Distributions","[{'title': 'doi', 'href': 'http://dx.doi.org/10.1145/3534678.3539391', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2205.10218v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2205.10218v2', 'rel': 'related', 'type': 'application/pdf'}]",2022-05-20 14:52:03,"2
2
0
2

y
a
M
6
2

]
I

A
.
s
c
[

1
v
1
2
5
3
1
.
5
0
2
2
:
v
i
X
r
a

Discovering Policies with DOMiNO:
Diversity Optimization Maintaining Near Optimality

Tom Zahavy
DeepMind, London
tomzahavy@deepmind.com

Yannick Schroecker
DeepMind, London
yschroecker@deepmind.com

Feryal Behbahani
DeepMind, London
feryal@deepmind.com

Kate Baumli
DeepMind, London
baumli@deepmind.com

Sebastian Flennerhag
DeepMind, London
ﬂennerhag@deepmind.com

Shaobo Hou
DeepMind, London
shaobohou@deepmind.com

Satinder Singh
DeepMind, London
baveja@deepmind.com

Abstract

Finding different solutions to the same problem is a key aspect of intelligence
associated with creativity and adaptation to novel situations. In reinforcement
learning, a set of diverse policies can be useful for exploration, transfer, hierarchy,
and robustness. We propose DOMiNO, a method for Diversity Optimization
Maintaining Near Optimality. We formalize the problem as a Constrained Markov
Decision Process where the objective is to ﬁnd diverse policies, measured by the
distance between the state occupancies of the policies in the set, while remaining
near-optimal with respect to the extrinsic reward. We demonstrate that the method
can discover diverse and meaningful behaviors in various domains, such as different
locomotion patterns in the DeepMind Control Suite. We perform extensive analysis
of our approach, compare it with other multi-objective baselines, demonstrate
that we can control both the quality and the diversity of the set via interpretable
hyperparameters, and show that the discovered set is robust to perturbations.

1

Introduction

Creative problem solving is the mental process of searching for an original and previously unknown
solution to a problem [38]. The relationship between creativity and intelligence is widely recognized
across many ﬁelds; for example, in the ﬁeld of Mathematics, ﬁnding different proofs to the same
theorem is considered elegant and often leads to new insights.

Closer to Artiﬁcial Intelligence (AI), consider the ﬁeld of game playing and, speciﬁcally, the game
of Chess in which a move is considered creative when it goes beyond known patterns [17]. In
some cases, such moves can only be detected by human players while remaining invisible to current
state-of-the-art Chess engines. A famous example thereof is the winning move in game eight of the
Classical World Chess Championship 2004 between Leko and Kramnik [8, 3]. Humans and, indeed,
many animals employ similarly creative behavior on a daily basis; faced with a challenging problem,
we often consider qualitatively different alternative solutions.

Yet, the majority of AI research is focused on ﬁnding a single best solution to a given problem. For
example, in the ﬁeld of Reinforcement Learning (RL), most algorithms are designed to ﬁnd a single
reward-maximizing policy. However, for many problems of interest there may be many qualitatively

Preprint. Under review.

 
 
 
 
 
 
different optimal or near-optimal policies; ﬁnding such diverse set of policies may help an RL agent
become more robust to changes in the task and/or environment and to generalize better to future tasks.

The majority of the literature on this problem has been done in the ﬁeld of Quality-Diversity (QD),
which comprises of two main families of algorithms: MAP-Elites [36, 16] and novelty search with
local competition [32]. QD algorithms typically maintain a collection of policies and adapt it using
evolutionary algorithms to balance the QD trade-off [41, 51, 15]. Further references can be found on
the QD webpage.

In contrast to this line of work, we propose a differentiable optimization framework for maximizing the
diversity of a set of RL policies. We do so by formulating diversity maximization as an optimization
problem in state occupancies, and then showing that we can solve this problem by maximizing an
intrinsic reward that corresponds to the gradient of the diversity objective.

In related work, intrinsic rewards have been used for learning diversity in terms of the discriminability
of different trajectory-speciﬁc quantities [24, 20, 46, 7]. Other works implicitly induce diversity to
learn policies that maximize the set robustness to the worst-possible reward [31, 57], or use diversity
as a regularizer when maximizing the extrinsic reward [29, 34, 40, 49, 59, 46].

Our work makes the following contributions. First, we propose DOMiNO, a method for Diversity
Optimization that Maintains Nearly Optimal policies. DOMiNO trains a set of policies using a
policy-speciﬁc, weighted combination of the extrinsic reward and an intrinsic diversity reward. The
weights are adapted as Lagrange multipliers to guarantee that each policy is near-optimal. Second,
we propose to measure diversity via expected features; i.e., the features that a policy observes in its
state occupancy. Under this measure of diversity, we introduce two novel objectives for diversity
optimization: a repulsive force that motivates policies to have distinct expected features and a Van
Der Waals force, which combines the repulsive force with an attractive one and allows us to specify
the degree of diversity in the set. Third, we perform experiments in the DeepMind Control Suite [52]
and the BiPedal walker environment [13] and show that DOMiNO discovers qualitatively diverse
locomotion behaviors (Fig. 1b). We analyze our approach and compare it to other multi-objective
strategies for handling the QD trade-off. Lastly, we demonstrate that the discovered set is robust to
perturbations of the environment and the morphology of the avatar.

(a)

(b)

Figure 1: (a) DOMiNO’s architecture: The agent learns a set of QD policies via a single latent-
conditioned actor-critic network with intrinsic and extrinsic value heads. Dashed arrows signify
training objectives. (b) DOMiNO’s π: Near optimal diverse policies in walker.stand corresponding
to standing on both legs, standing on either leg, lifting the other leg forward and backward, spreading
the legs and stamping. Not only are the policies different from each other, they also achieve high
extrinsic reward in standing (see values on top of each policy).

2 Preliminaries and Notation

In this work, we will express objectives in terms of the state occupancy measure dπ. Intuitively
speaking, dπ measures how often a policy π visits each state-action pair. As we will soon see, the
classic RL objective of reward maximization can be expressed as a linear product between the reward
vector and the state occupancy. In addition, in this work we will formulate diversity maximization via
an objective that is a nonlinear function of the state occupancy. While it might seem unclear which

2

i

R
u
n
n
n
g
A
v
e
r
a
g
e

U
p
d
a
t
e
c
o
n
s
t
r
a
i
n
t

&
L
a
g
r
a
n
g
e

MLP

Torso

MLP

i

R
u
n
n
n
g
A
v
e
r
a
g
e

D
i
v
e
r
s
i
t
y
R
e
w
a
r
d

 
 
 
 
 
 
 
 
 
reward should be maximized to solve such an objective, we take inspiration from Convex MDPs [58]
where one such reward is the gradient of the objective with respect to dπ.

We begin with some formal deﬁnitions. In RL an agent interacts with an environment and seeks to
maximize its cumulative reward. We consider two cases, the average reward case and the discounted
case. The Markov decision process [42, MDP] is deﬁned by the tuple (S, A, P, R) for the average
reward case and by the tuple (S, A, P, R, γ, D0) for the discounted case. We assume an inﬁnite
horizon, ﬁnite state-action problem. Initially, the state of the agent is sampled according to s0 ∼ D0.
At time t, given state st, the agent selects action at according to its policy π(st, ·), receives a reward
rt ∼ R(st, at) and transitions to a new state st+1 ∼ P (·, st, at). We consider two performance
t=1γtrt, for the average reward
metrics, given by vavg
case and discounted case respectively. The goal of the agent is to ﬁnd a policy that maximizes vavg
or vγ
π. Let Pπ(st = ·) be the probability measure over states at time t under policy π, then the
state occupancy measure dπ is given as davg
t=1 Pπ(st = s)π(s, a), and
π(s, a) = (1 − γ)E (cid:80)∞
t=1 γtPπ(st = s)π(s, a), for the average reward case and the discounted
dγ
case respectively. With these, we can rewrite the RL objective in as a linear function of the occupancy
measure maxdπ∈K
s,a r(s, a)dπ(s, a), where K is the set of admissible distributions [58]. Next,
consider an objective of the form:

π (s, a) = limT →∞

π = (1 − γ)E(cid:80)∞

π = limT →∞

T E (cid:80)T

t=1rt, vγ

T E(cid:80)T

(cid:80)

π

1

1

min
dπ∈K

f (dπ),

(1)

where f : K → R is a nonlinear function. Sequential decision making problems that take this form
include Apprenticeship Learning (AL) and pure exploration, among others [1, 55, 26, 59, 23, 58, 45,
9, 37]. In the remainder of this section, we brieﬂy explain how to solve Eq. (1) using RL methods
when the function f is convex. We begin with rewriting Eq. (1) using Fenchel duality as

min
dπ∈K

f (dπ) = max
λ∈Λ

min
dπ∈K

(λ · dπ − f ∗(λ))

(2)

where Λ is the closure of (sub-)gradient space {∂f (dπ)|dπ ∈ K}, which is compact and convex [2],
and f ∗ is the Fenchel conjugate of the function f .

Eq. (2) presents a zero-sum max-min game between two players, the policy player Algπ and the
cost player Algλ. We can see that from the perspective of the policy player, the objective is a linear
minimization problem in dπ. Thus, intuitively speaking, the goal of the policy player is to maximize
the negative cost as a reward r = −λ. To solve Eq. (2), we employ the meta algorithm from [2],
which uses two online learning algorithms. The policy player Algπ generates a sequence of policies
{πk}k∈N by maximizing a sequence of negative costs {−λk}k∈N as rewards that are produced by
the cost player Algλ. In this paper, the policy player uses an online RL algorithm and the cost player
uses the Follow the Leader (FTL) algorithm. This implies that the cost at time k is given as

λk = ∇f ( ¯dk−1

π

).

(3)

π

In other words, to solve an RL problem with a convex objective function (Eq. (1)), the policy
player maximizes a non stationary reward that at time k corresponds to the negative gradient of the
objective function f w.r.t ¯dk−1
. When the function f is convex, it is guaranteed that the average
state occupancy of these polices, ¯dK
π, converges to an optimal solution to Eq. (1), i.e.,
¯dK
π → d(cid:63)
Features and expected features. We focus on the case where each state-action pair is associated
with some observable features φ(s, a) ∈ Rd. For example, in the DM control suite [52], these features
correspond to the positions and velocities of the body joints being controlled by the agent. In other
cases, we can learn φ with a neural network.

π ∈ arg mindπ∈K f (dπ) [58].

π = 1
K

k=1 dk

(cid:80)K

Similar to value functions, which represent the expectation of the reward under the state occupancy,
we deﬁne expected features as ψπ(s, a) = Es(cid:48),a(cid:48)∼dπ(s,a) φ(s(cid:48), a(cid:48)) ∈ Rd. Note that in the special case
of one-hot feature vectors, the expected features coincide with the state occupancy.
The deﬁnition of ψπ depends on the state occupancy we consider. In the discounted case, ψγ
π ∈ Rd
is also known as Successor Features (SFs) as deﬁned in [5, 6]. In the average case, ψavg
π ∈ Rd
represents the expected features under the policy’s stationary distribution and therefore it has the
same value for all the state action pairs. Similar deﬁnitions were suggested in [35, 56].

3

3 Discovering diverse near-optimal policies

We now introduce Diversity Optimization Maintaining Near Optimality, or, DOMiNO, which
discovers a set of n policies Πn = {πi}n

i=1 by solving the optimization problem:

max
Πn

Diversity(Πn) s.t dπ · re ≥ αv∗

e , ∀π ∈ Πn,

(4)

where v∗
e is the value of the optimal policy. In other words, we are looking for a set of policies
that are as diverse from each other as possible, deﬁned as Diversity : {R|S||A|}n → R. In addition,
we constrain the policies in the set to be nearly optimal. To deﬁne near-optimality we introduce a
hyperparameter α ∈ [0, 1], such that a policy is said to be near optimal if it achieves a value that is at
e . In practice, we “ﬁx” the Lagrange multiplier for the ﬁrst policy µ1 = 1, so this policy only
least αv∗
receives extrinsic reward, and use the value of this policy to estimate v∗
e ). Notice that this
estimate is changing through training.

e = v1

e (v∗

Before we dive into the details, we brieﬂy explain the main components of DOMiNO. Building
on Section 2 and, in particular, Eq. (3), we ﬁnd policies that maximize the diversity objective by
maximizing its gradient as a reward signal, i.e., ri
π). We discuss two
candidates for this objective and derive an analytical formula for the associated rewards in Section 3.1.

Diversity(d1

π, . . . , dn

d = ∇di

π

Then, in Section 3.2 we explain how to combine the two rewards via the coefﬁcients ce, cd. Thus,
each of the policies, π1, . . . , πn, will be maximizing a reward signal ri that is a linear combination
of the extrinsic reward re and ri
d : i.e., ri(s, a) = cere(s, a) + cdri
d(s, a). To this end, we focus on
the method of Lagrange multipliers, which adapts the coefﬁcients online in order to solve Eq. (4) and
compare it with other multi-objective baselines.

3.1 Diversity

Next, we present an objective that motivates policies to visit different states on average. It does so by
leveraging the information about the policies’ long-term behavior available in their expected features,
and motivating the state occupancies to be different from each other. For that reason, we refer to this
objective as a repulsive force (Eq. (5)). We then extend this objective and combine it with a second,
attractive force (Eq. (7)), taking inspiration from the Van Der Waals (VDW) force. The manner in
which we combine these two forces allows us to control the degree of diversity in the set.

A repulsive force. How do we compute a set of policies with maximal distances between their
expected features? To answer this question, we ﬁrst consider the simpler scenario where there
are only two policies in the set and consider the following objective maxπ1,π2 ||ψ1 − ψ2||2
2. This
objective is related to the objective of Apprenticeship Learning [AL; 1], i.e., solving the problem
minψ ||ψ − ψE||2
2, where ψE are the feature expectations of an expert. Both problems use the
euclidean norm in the feature expectation space to measure distances between policies. Since we are
interested in diversity, we are maximizing this objective, while AL aims to minimize it. In a similar
fashion, the mutual information between policies and states, which is equivalent to the KL divergence
between state occupancies [58, 21] is minimized for AL [28] and maximized for diversity [20].
Next, we investigate how to measure the distance of a policy from the set of multiple policies, Πn.
First we introduce the Hausdorff distance [43] that measures how far two subsets D, C of a metric
space are from each other: Dist(D, C) = minc∈C,d∈D ||c − d||2
2. In other words, two sets are far
from each other in the Hausdorff distance if every point of either set is far from all the points of the
other set. Building on this deﬁnition, we can deﬁne the distance from an expected features vector ψi
to the set of the other expected features vectors as minj(cid:54)=i ||ψi − ψj||2
2. This equation gives us the
distance between each individual policy and the other policies in the set. Maximizing it across the
policies in the set, gives us our ﬁrst diversity objective:

max
d1
π,...,dn
π

0.5

(cid:88)n

i=1

min
j(cid:54)=i

||ψi − ψj||2
2.

(5)

π

Diversity(d1

to compute the associated diversity reward, we compute the gradient ri

d =
In order
∇di
π). To do so, we begin with a simpler case where there are only two policies,
2 = φ · (ψ1 − ψ2),
i.e., r = ∇d1
2 = ∇d1
such that r(s, a) = φ(s, a) · (ψ1 − ψ2). This reward was ﬁrst derived by Abbeel & Ng [1], but here it
is with an opposite sign since we care about maximizing it. Lastly, for a given policy πi, we deﬁne by

π(s,a)φ(s, a) − Es(cid:48),a(cid:48)∼d2

π, . . . , dn
||ψ1 − ψ2||2

π(s,a)φ(s, a)||2

||Es(cid:48),a(cid:48)∼d1

π

π

4

π

i , we get1 that ∇di

i the index of the policy with the closest expected features to πi, i.e., j∗
j∗
Using the deﬁnition of j∗
2 = ∇di

minj(cid:54)=i ||ψi − ψj||2
d(s, a) = φ(s, a) · (ψi − ψj∗
ri
The Van Der Waals force. Next, we propose a second diversity objective that allows us to control
the degree of diversity in the set via a hyperparameter. The objective is inspired from molecular
physics, and speciﬁcally, by how atoms in a crystal lattice self-organize themselves at equal distances
from each other. This phenomenon is typically explained as an equilibrium between two distance
dependent forces operating between the atoms known as the Van Der Waals (VDW) forces; one force
that is attractive and another that is repulsive.

i = arg minj(cid:54)=i ||ψi − ψj||2
2.
i ||2

||ψi − ψj∗

2, and that

i ).

(6)

π

The VDW force is typically characterized by a distance in which the combined force becomes
repulsive rather than attractive (see, for example, [47]). This distance is called the VDW contact
distance, and we denote it by (cid:96)0. In addition, we denote by (cid:96)i = ||ψi − ψj∗
i ||2 the Hausdorff distance
for policy i. With this notation, we deﬁne our second diversity objective as2

max
π,...,dn
d1
π

(cid:88)n

i=1

0.5(cid:96)2
i
(cid:124) (cid:123)(cid:122) (cid:125)
Repulsive

i /(cid:96)3
0

−0.2(cid:0)(cid:96)5
(cid:123)(cid:122)
(cid:124)
Attractive

(cid:1)

(cid:125)

.

(7)

We can see that Eq. (7) is a polynomial in (cid:96)i, composed of two forces with opposite signs and different
powers. The different powers determine when each force dominates the other. For example, when
the expected features are close to each other ((cid:96)i << (cid:96)0), the repulsive force dominates, and when
((cid:96)i >> (cid:96)0) the attractive force dominates. The gradient (and hence, the associated reward) is given by
d(s, a) = (1 − ((cid:96)i/(cid:96)0)3)φ(s, a) · (ψi − ψj∗
ri
Inspecting Eq. (8) we can see that when the expected features are organized at the VDW contact
distance (cid:96)0, the objective is maximized and the gradient is zero. In a related line of work, Vassiliades
et al. [54] suggested to use voronoi tessellation to partition the feature space of the MAP-Elite
algorithm to regions of equal size and Liu et al. [33] proposed a Stein Variational Policy Gradient
with repulsive and attractive components. However, using a VDW force to control diversity is novel
to the best of our knowledge.

i ).

(8)

3.2 Balancing Quality with Diversity

Constrained MDPs. At the core of our approach is a solution to the CMDP in Eq. (4). There exist
different methods for solving CMDPs and we refer the reader to [4] and [50] for treatments of the
subject at different levels of abstraction. In this work we will focus on a reduction of CMDPs to
MDPs via gradient updates, known as Lagrangian methods [11, 10, 53, 14].

Most of the literature on CMDPs has focused on linear objectives and linear constraints. In Section 2,
we discussed how to solve an unconstrained convex RL problem of the form of Eq. (1) as a saddle point
problem. We now extend these results to the case where the objective is convex and the constraint is
linear, i.e. mindπ∈K f (dπ), subject to g(dπ) ≤ 0, where f denotes the diversity objective and g is a
linear function of the form g(dπ) = αv∗
e − dπ · re deﬁning the constraint. Solving this problem is
equivalent to solving the following problem:

min
dπ∈K

max
µ≥0

f (dπ) + µg(dπ) = min
dπ∈K

max
µ≥0,λ

λ · dπ − f ∗(λ) + µ(αv∗

e − dπ · re),

(9)

where the equality follows from Fenchel duality as before. Similar to Section 2, we use the FTL
algorithm for the λ player (Eq. (3)). This implies that the cost at iteration k, λk, is equivalent to the
gradient of the diversity objective, which we denote by rd. Eq. (9) involves a vector, λ − µre, linearly
interacting with dπ. Thus, intuitively speaking, minimizing Eq. (9) from the perspective of the policy
player is equivalent to maximizing a reward rd + µre.
The objective for the Lagrange multiplier µ is to maximize Eq. (9), or equivalently µ(αv∗
e − dπ · re).
Intuitively speaking, when the policy achieves an extrinsic value that satisﬁes the constraint, the

1In the rare case that the arg min has more than one solution, the gradient is not deﬁned, but we can still use

Eq. (6) as a reward.

2The coefﬁcients in Eq. (7) are chosen to simplify the reward in Eq. (8). I.e., since the reward is the gradient

of the objective, after differentiation the coefﬁcients equal 1 in Eq. (8).

5

Lagrange multiplier µ decreases (putting a smaller weight on the extrinsic component of the reward)
and it increases otherwise. More formally, we can solve the problem in Eq. (9) as a three-player
game. In this case the policy player controls dπ as before, the cost player chooses λ using Eq. (3),
and the Lagrange player chooses µ with gradient descent. Proving this statement is out of the scope
of this work, but we shall investigate it empirically.

3.3 Multi-objective alternatives

We conclude this section by discussing two alternative approaches for balancing the QD trade-off,
which we later compare empirically with the CMDP approach. First, consider a multi-objective
MDP that combines the diversity objective with the extrinsic reward as
π, . . . , dn
cedi

π · re + cdDiversity(d1

(10)

π),

max
Πn

where ce, cd are ﬁxed weights that balance the diversity objective and the extrinsic reward. We note
that the solution of such a multi-objective MDP cannot be a solution to a CMDP in general. I.e., it
is not possible to ﬁnd the optimal dual variables µ∗, plug them into Eq. (9) and simply solve the
resulting (unconstrained) MDP. Such an approach ignores the fact that the dual variables must be a
‘best-response’ to the policy and is referred to as the ”scalarization fallacy” in [50, Section 4].

While multi-objective MDPs have been used in prior QD-RL papers [29, 34, 39, 22, 40, 60], we now
outline a few potential advantages for using CMDPs. First, the CMDP formulation guarantees that the
policies that we ﬁnd are near optimal (satisfy the constraint). Secondly, the weighting coefﬁcient in
multi-objective MDPs has to be tuned, where in CMDPs it is adapted. This is particularly important
in the context of maximizing diversity while satisfying reward.

Next, consider a hybrid approach that combines a multi objective MDP with a CMDP as

max
Πn

max(0, αv∗

e − di

π · re) + cdDiversity(d1

π, . . . , dn

π).

We denote by I i an indicator function for the event in which the constraint is not satisﬁed for policy
πi, i.e., I i = 1 if di

e , and 0, otherwise. With this notation the reward is given by

π · re < αv∗

ri(s, a) = I ire(s, a) + cdri

d(s, a)

(11)

In other words, the agent maximizes a weighted combination of the extrinsic reward and the diversity
reward when the constraint is violated and only the diversity reward when the constraint is satisﬁed.
Kumar et al. [31] proposed a similar approach where the agent maximizes a weighted combination of
the rewards when the constraint is satisﬁed, and only the extrinsic reward otherwise:

ri(s, a) = re(s, a) + cd(1 − I i)ri

d(s, a)

(12)

We refer to Eq. (12) as SMERL, as was coined in [31], and to Eq. (11) as Reverse SMERL. Note that
these methods come with an additional hyperparameter cd which balances the two objectives as a
multi-objective MDP, in addition to the optimality ratio α.

4 Experiments

Our experiments address the following questions: (a) Can we learn diverse policies that are also
near optimal? (see Section 4.1) (b) How critical are various components of our algorithm and how
does it compare to other multi-objective baselines in terms of QD trade-off? (see Section 4.1) (c)
Does our method scale to settings where the feature space is high-dimensional and unstructured?
(see Section 4.2) (d) Finally, can the diverse policies discovered by DOMiNO enable robustness and
adaptation to novel perturbations in the environment and agent? (see Section 4.3)

Environment. We conducted most of our experiments on domains from the DM Control Suite [52],
standard continuous control locomotion tasks where diverse near-optimal policies should naturally
correspond to different gaits. Due to space considerations, we present Control Suite results on the
walker.stand task. In the supplementary, however, we present similar results for walker.walk and
BiPedal walker from OpenAI Gym [13] suggesting that the method generalizes across different
reward functions and domains. We also include the challenging dog domain with 38 actions and a
223−dimensional observation space, which is one of the more challenging domains in control suite.

6

Agent. Fig. 1a shows an overview of DOMiNO’s components and their interactions, instantiated in
an actor-critic agent. While acting, the agent samples a new latent variable z ∈ [1, n] uniformly at
random at the start of each episode. We train all the policies simultaneously and provide this latent
variable as an input. For the average reward state occupancy, the agent keeps an empirical running
average for each latent policy of the rewards ˜vavg
πi and features (either from the environment φobs or
torso embedding φembedding) ˜ψavg
πi encountered, where the average is taken as ˜xi = αd ˜xi−1 + (1 −
αd) 1
t=1 xi(st, at) with decay factor αd. Varying αd can make the estimation more online (small
T
αd, as used for the constraint), or less online (large αd, as needed for Eq. (4)).
The agent uses ˜ψavg
to compute the diversity reward as described in Eq. (6). ˜vavg
is used to optimize
πi
πi
the Lagrange multiplier µ for each policy as in Eq. (9) which is then used to weight the quality and
diversity advantages for the policy gradient update. Pseudo code and further implementation details,
as well as treatment of the discounted state occupancy, can be found in Appendix A.2.

(cid:80)T

4.1 Quality and diversity

To measure diversity qualitatively, we present ”motion ﬁgures” by discretizing the videos (details in
the Appendix) that give a fair impression of the behaviors. The videos, associated with these ﬁgures,
can be found in the supplementary as well. Fig. 1b presents ten polices discovered by DOMiNO with
the repulsive objective and the optimality ratio set to 0.9. The policies are ordered from top-left to
bottom right, so policy 1, which only maximizes extrinsic reward and sets the constraint, is always at
the top left. The policies exhibit different types of standing: standing on both legs, standing on either
leg, lifting the other leg forward and backward, spreading the legs and stamping. Not only are the
policies different from each other, they also achieve high extrinsic reward in standing (see values on
top of each policy visualization). Similar ﬁgures for other domains can be found in Appendix B.1.

To further study the QD trade-off, we use scatter plots, showing the episode return on the y-axis, and
the diversity score, corresponding to the Hausdorff distance (Eq. (5)), on the x-axis. The top-right
corner of the diagram, therefore, represents the most diverse and highest quality policies. Each ﬁgure
presents a sweep over one or two hyperparameters and we use the color and a marker to indicate
the values. In all of our experiments, we report 95% conﬁdence intervals. In the scatter plots, they
correspond to 5 seeds and are indicated by the crosses surrounding each point.

Fig. 2 (left) presents the results for DOMiNO with the repulsive reward in the walker.stand domain.
We can observe that regardless of the set size, DOMiNO achieves roughly the same extrinsic reward
across different optimality ratios (points with the same color obtain the same y-value). This implies
that the constraint mechanism is working as expected across different set sizes and optimality ratios.
In addition, we can inspect how the QD trade-off is affected when changing the optimality ratio α for
sets of the same size (indicated in the ﬁgures with light lines). This observation can be explained
by the fact that for lower values of α, the volume of the constrained set is larger, and therefore, it is
possible to ﬁnd more diversity within it. This behavior is consistent across different set sizes, though
it is naturally more difﬁcult to ﬁnd a set of diverse policies as the set size gets larger (remember that
we measure the distance to the closest policy in the set).

Figure 2: DOMiNO QD results in walker.stand. Left: Set size vs. optimality ratio (α) with the
repulsive reward. Center: Set size vs. α with the Van der Waals (VDW) reward. Right: Target
diversity ((cid:96)0) vs. α with the VDW reward.
We present the same investigation for the VDW reward in Fig. 2 (center). Similar to the repulsive
reward, we can observe that the constraint is satisﬁed, and that reducing the optimality ratio allows
for more diversity. Fig. 2 (right) shows how different values of (cid:96)0 affect the QD trade-off for a set of
size 10. We can observe that the different combinations of (cid:96)0 and α are organized as a grid in the

7

QD scatter, suggesting that we can control both the level of optimality and the degree of diversity by
setting these two interpretable hyperparameters.

Fig. 3 compares the QD balance yielded by DOMiNO to the alternative strategies described in
Section 3.2. Speciﬁcally, we look at DOMiNO’s Lagrangian method, the linear multi-objective
combination of the objectives (Eq. (10)), and the two hybrid strategies, SMERL (Eq. (12)) and
Reverse SMERL (Eq. (11)) for a set of ten policies in walker.stand. Note that in all cases we are
using DOMiNO’s repulsive diversity objective, and the comparison is strictly about strategies for
combining the quality and diversity objectives. The plot for each strategy shows how the solution
to the QD tradeoff varies according to the relevant hyperparameters for that strategy, namely, the
optimality ratio α for DOMiNO, the ﬁxed constant ce for the multi-objective strategy (we implicitly
set cd = 1 − ce), and both α and constant cd for the hybrid approaches (in the hybrid plots, cd value
is labeled directly next to the marker, while α is indicated by color).

Figure 3: DOMiNO’s Lagrangian method ﬁnds only solutions that push the upper right boundary
of quality and diversity, and varies in a smooth, interpretable way with its only hyperparameter, α,
contrasted with the jumpy nature of the multi-objective hyperparameter ce, and the redundancy of the
hyperparameters in the hybrid methods (SMERL and Reverse SMERL).

For the multi-objective approach, shown on the right, the ce parameter proves ill-behaved and choppy,
quickly jumping from the extreme of all diversity no quality to the opposite, without a smooth interim.

In contrast, the DOMiNO approach of solving the CMDP directly for the Lagrange multiplier yields
solutions that push along the upper right diagonal boundary, ﬁnding the highest diversity (farthest
right) set of policies for a given optimality ratio (color), varying smoothly along this line as α varies.
Another advantage of DOMiNO’s approach is that it only ﬁnds such QD-optimal solutions, where, in
contrast, SMERL (left), when appropriately tuned, can also yield some solutions along the upper-right
QD border, but often ﬁnds sub-optimal solutions, and therefore must be tuned further with cd to ﬁnd
the best QD solutions. We further explore the difﬁculty tuning SMERL in the supplementary (Fig. 10)
and ﬁnd that the best cd for 10 policies provides solutions with no diversity for other set sizes.

4.2 Feature analysis

The choice of feature space used for optimizing diversity can have a huge impact on the kind of
diverse behavior learned. In environments where the observation space is high dimensional and
less structured (e.g. pixel observations), computing diversity using the raw features may not lead to
meaningful behavior. As speciﬁed in Section 3.1 the feature space used to compute diversity in our
Control Suite experiments throughout the paper corresponds to the positions and velocities of the
body joints returned as observations by the environment. We show that it is feasible to use a learned
embedding space instead. As a proof of principle we use the output of the torso network as a learned
embedding described in Section 4 for computing our diversity metric.

Table 1 compares the diversity mea-
sured in raw observation features
(Diversityobs) and embedding features
(Diversityembedding) in the walker.stand
domain. Columns indicate the feature
space that was used to compute the di-
versity objective during training averaged across 20 seeds. Inspecting the table, we can see that
agents trained to optimize diversity in the learned embedding space and agents that directly optimize
diversity in the observation space achieve comparable diversity if measured in either space, indicating
that learned embeddings can feasibly be used to achieve meaningful diversity.

Diversityobs
Diversityembedding

φembedding
1.01 ± 0.05
2.35 ± 0.09

φobs
1.21 ± 0.05
2.14 ± 0.09

Table 1

8

Figure 4: K-shot adaptation in Control Suite. We report mean episode return (95% CI) on held-out
test tasks relative to the performance of a single policy trained on extrinsic rewards. While not
invariant to sudden changes in the environment, DOMiNO is more robust to a variety of perturbations.

4.3 Closing the loop: k-shot adaptation

We motivated qualitative diversity by saying that diverse solutions can be robust and allow for rapid
adaptation to new tasks and changes in the environment. Here we validate this claim in a k-shot
adaptation experiment: we train a set of QD policies on a canonical benchmark task, then test their
ability to adapt to environment and agent perturbations.

These include four kinematics and dynamics perturbations from the Real World RL suite [18], and
a ﬁfth perturbation inspired by a ”motor failure” condition [31] which, every 50 steps and starting
at T = 10, disables action-inputs for the ﬁrst two joints for a ﬁxed amount of time3. In Fig. 4, we
present the results in the walker.walk and walker.stand domains (rows). Columns correspond to
perturbation types and the x-axis corresponds to the magnitude of the perturbation.

K-shot adaptation is measured in the following manner. For each perturbed environment, and each
method, we ﬁrst execute k = 10 environment trajectories with each policy. Then, for each method we
select the policy that performs the best in the set. We then evaluate this policy for 40 more trajectories
and measure the average reward of the selected policy rmethod. The y-axis in each ﬁgure measures
rmethod/rbaseline, where rbaseline measures the reward in the perturbed environment of an RL baseline agent
that was trained with a single policy to maximize the extrinsic reward in the original task. We note
that the baseline was trained with the same RL algorithm, but without diversity, and it matches the
state-of-the-art in each training domain (it is almost optimal). The raw rewards rmethod, rbaseline can be
found in the supplementary (Fig. 12). Lastly, we repeat this process across 20 training seeds, and
report the average with a 95% Conﬁdence Interval (CI) 4.

We compare the following methods: DOMiNO, SMERL, Multi-Objective, and No diversity, where all
the diversity methods use the diversity reward from Eq. (6) and all the methods are with 10 policies.
Since we are treating the perturbed environments as hold out tasks, we selected the hyper parameters
for each method based on the results in Fig. 3, i.e.we chose the conﬁguration that was the most
qualitatively diverse (in the upper-right most corner of Fig. 3). Concretely, for DOMiNO and SMERL
α = 0.9, for SMERL cd = 0.5 and for Multi-Objective ce = 0.7. More K-shot adaptation curves
with other hyper parameter values can be found in Appendix D. The No diversity method is similar
to rbaseline, but uses 10 policies that all maximize the extrinsic reward (instead of a single policy).

Inspecting Fig. 4 we can see that for small perturbations, DOMiNO retains the performance of the
baseline. However, as the magnitude of the perturbation increases, the performance of DOMiNO is
much higher than the baseline (by a factor of 1.5 − 2.0). This observation highlights that a diverse
set of policies as found by DOMiNO is much more capable at handling changes to the environment
and can serve as a strong starting point for recovering optimal behavior. As we have shown in 3.2,
other approaches to managing the trade-off between quality and diversity such as SMERL are much
more sensitive to the choice of hyper-parameters and require signiﬁcant tuning. While SMERL is
able to ﬁnd a useful, diverse set of policies with some effort, it is difﬁcult to match DOMiNO’s
performance across all pertubations and tasks. See the supplementary material for further comparison
of DOMiNO with SMERL and Multi-objective over more hyper parameters. We also include a video
that illustrates how the individual policies adapt to the environment perturbations.

3While we tried to recreate a similar condition to [31], the tasks are not directly comparable due to signiﬁcant

differences in the simulators that have been used as well as the termination conditions in the base task.

4We use boosted CI with nested sampling as implemented in the bootstrap function here, which reﬂects the

amount of training seeds and the amount of evaluation seeds per training seed

9

s
n
r
u
t
e
r

e
v
i
t
a
e
R

l

s
n
r
u
t
e
r

e
v
i
t
a
e
R

l

d
n
a
t
S

1.50

1.25

1.00

1.35

l

k
a
W

1.20

1.05

1.80

1.50

1.20

0.90

1.80

1.50

1.20

0.90

0

0

1

0

2

0

3

0

4

5

2

0 . 2

Motor failure (duration)

5

7

0 . 3

0 . 3
0 . 2
Thigh length

2.00

1.60

1.20

0.80
1.75

1.50

1.25

1.00

0 . 4

0 . 3

1.40

1.20

1.00

2.40

2.00

1.60

1.20

1.20

1.05

0.90

1.35

1.20

1.05

0.90

0 . 5

0 . 1

1 . 0

2 . 0

3 . 0

4 . 0

0 . 7

Joint damping

5

5

0 . 4

0 . 4
0 . 3
Torso length

DOMiNO
SMERL
No diversity
Multi-Objective

1

0

0 . 0

2

0 . 1

0 . 0
Contact friction

0 . 0

0

5

 
 
 
 
5 Conclusions

In this work we proposed DOMiNO, an algorithm for discovering diverse behaviors that maintain
optimality. We framed the problem as a CMDP in the state occupancies of the policies in the set
and developed an end-to-end differentiable solution to it based on reward maximization. In our
experiments we demonstrated that the policies discovered by DOMiNO, or, DOMiNO’s π, are diverse
and maintain optimality. We then explored how DOMiNO balances the QD trade-off and compared it
with multi-objective baselines. Our results suggest that DOMiNO can control the degree of quality
and diversity via two interpretable hyperparameters, while other baselines struggle to capture both.

In our K-shot experiments we demonstrated that DOMiNO’s π can adapt to changes in the environ-
ment. An exciting direction for future work is to use DOMiNO in a never ending RL setting, where
the environment changes smoothly over time, and see if maintaing a set of QD diverse policies will
make it more resilient to such changes.

10

References

[1] Abbeel, P. and Ng, A. Y. Apprenticeship learning via inverse reinforcement learning.

In
Proceedings of the twenty-ﬁrst international conference on Machine learning, pp. 1. ACM,
2004.

[2] Abernethy, J. D. and Wang, J.-K. On frank-wolfe and equilibrium computation.

In
Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and
Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Cur-
ran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
7371364b3d72ac9a3ed8638e6f0be2c9-Paper.pdf.

[3] Agadmator. Invisible to engines — one of the greatest moves ever played. agadmator’s Chess

Channel, 2018. URL https://www.youtube.com/watch?v=yGnpewUKP88&t=1s.

[4] Altman, E. Constrained Markov decision processes, volume 7. CRC Press, 1999.

[5] Barreto, A., Dabney, W., Munos, R., Hunt, J. J., Schaul, T., van Hasselt, H. P., and Silver, D.
Successor features for transfer in reinforcement learning. In Advances in neural information
processing systems, pp. 4055–4065, 2017.

[6] Barreto, A., Hou, S., Borsa, D., Silver, D., and Precup, D. Fast reinforcement learning with

generalized policy updates. Proceedings of the National Academy of Sciences, 2020.

[7] Baumli, K., Warde-Farley, D., Hansen, S., and Mnih, V. Relative variational intrinsic control.
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 35:6732–6740, May 2021. URL
https://ojs.aaai.org/index.php/AAAI/article/view/16832.

[8] Behovits, R. Game 8: Leko wins to take the lead. Chess news, 2004. URL https://en.

chessbase.com/post/game-8-leko-wins-to-take-the-lead.

[9] Belogolovsky, S., Korsunsky, P., Mannor, S., Tessler, C., and Zahavy, T. Inverse reinforcement

learning in contextual mdps. Machine Learning, pp. 1–40, 2021.

[10] Bhatnagar, S. and Lakshmanan, K. An online actor–critic algorithm with function approximation
for constrained markov decision processes. Journal of Optimization Theory and Applications,
153(3):688–708, 2012.

[11] Borkar, V. S. An actor-critic algorithm for constrained markov decision processes. Systems &

control letters, 54(3):207–213, 2005.

[12] Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., and Wanderman-
Milne, S. JAX: composable transformations of Python+NumPy programs, 2018. URL http:
//github.com/google/jax.

[13] Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba,

W. Openai gym. arXiv preprint arXiv:1606.01540, 2016.

[14] Calian, D. A., Mankowitz, D. J., Zahavy, T., Xu, Z., Oh, J., Levine, N., and Mann, T. Balancing
constraints and rewards with meta-gradient d4pg. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=TQt98Ya7UMP.

[15] Cully, A. Autonomous skill discovery with quality-diversity and unsupervised descriptors. In
Proceedings of the Genetic and Evolutionary Computation Conference, pp. 81–89, 2019.

[16] Cully, A., Clune, J., Tarapore, D., and Mouret, J.-B. Robots that can adapt like animals. Nature,

521(7553):503–507, 2015.

[17] da Fonseca-Wollheim, C. Swapping songs with chess grandmaster garry kasparov. The
New York Times, 2020. URL https://www.nytimes.com/2020/12/18/arts/music/
garry-kasparov-classical-music.html.

[18] Dulac-Arnold, G., Mankowitz, D., and Hester, T. Challenges of real-world reinforcement

learning. arXiv preprint arXiv:1904.12901, 2019.

11

[19] Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron, Y., Firoiu, V.,
Harley, T., Dunning, I., et al. Impala: Scalable distributed deep-rl with importance weighted
actor-learner architectures. In International Conference on Machine Learning, pp. 1407–1416.
PMLR, 2018.

[20] Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S. Diversity is all you need: Learning skills
without a reward function. In International Conference on Learning Representations, 2019.
URL https://openreview.net/forum?id=SJx63jRqFm.

[21] Eysenbach, B., Salakhutdinov, R., and Levine, S. The information geometry of unsupervised

reinforcement learning. arXiv preprint arXiv:2110.02719, 2021.

[22] Gangwani, T., Peng, J., and Zhou, Y. Harnessing distribution ratio estimators for learning agents

with quality and diversity. arXiv preprint arXiv:2011.02614, 2020.

[23] Geist, M., P´erolat, J., Lauri`ere, M., Elie, R., Perrin, S., Bachem, O., Munos, R., and Pietquin,
O. Concave utility reinforcement learning: the mean-ﬁeld game viewpoint. arXiv preprint
arXiv:2106.03787, 2021.

[24] Gregor, K., Rezende, D. J., and Wierstra, D. Variational intrinsic control.

International
Conference on Learning Representations, Workshop Track, 2017. URL https://openreview.
net/forum?id=Skc-Fo4Yg.

[25] Ha, D. Reinforcement learning for improving agent design. arXiv preprint arXiv:1810.03779,

2018.

[26] Hazan, E., Kakade, S., Singh, K., and Van Soest, A. Provably efﬁcient maximum entropy
exploration. In International Conference on Machine Learning, pp. 2681–2691. PMLR, 2019.

[27] Hessel, M., Kroiss, M., Clark, A., Kemaev, I., Quan, J., Keck, T., Viola, F., and van Hasselt, H.
Podracer architectures for scalable reinforcement learning. arXiv preprint arXiv:2104.06272,
2021.

[28] Ho, J. and Ermon, S. Generative adversarial imitation learning. arXiv preprint arXiv:1606.03476,

2016.

[29] Hong, Z.-W., Shann, T.-Y., Su, S.-Y., Chang, Y.-H., Fu, T.-J., and Lee, C.-Y. Diversity-driven
exploration strategy for deep reinforcement learning. In Proceedings of the 32nd International
Conference on Neural Information Processing Systems, pp. 10510–10521, 2018.

[30] Jouppi, N. P., Young, C., Patil, N., Patterson, D. A., Agrawal, G., Bajwa, R., Bates, S., Bhatia,
S., Boden, N., Borchers, A., Boyle, R., Cantin, P., Chao, C., Clark, C., Coriell, J., Daley, M.,
Dau, M., Dean, J., Gelb, B., Ghaemmaghami, T. V., Gottipati, R., Gulland, W., Hagmann, R.,
Ho, R. C., Hogberg, D., Hu, J., Hundt, R., Hurt, D., Ibarz, J., Jaffey, A., Jaworski, A., Kaplan,
A., Khaitan, H., Koch, A., Kumar, N., Lacy, S., Laudon, J., Law, J., Le, D., Leary, C., Liu,
Z., Lucke, K., Lundin, A., MacKean, G., Maggiore, A., Mahony, M., Miller, K., Nagarajan,
R., Narayanaswami, R., Ni, R., Nix, K., Norrie, T., Omernick, M., Penukonda, N., Phelps, A.,
Ross, J., Salek, A., Samadiani, E., Severn, C., Sizikov, G., Snelham, M., Souter, J., Steinberg,
D., Swing, A., Tan, M., Thorson, G., Tian, B., Toma, H., Tuttle, E., Vasudevan, V., Walter,
R., Wang, W., Wilcox, E., and Yoon, D. H. In-datacenter performance analysis of a tensor
processing unit. CoRR, abs/1704.04760, 2017. URL http://arxiv.org/abs/1704.04760.

[31] Kumar, S., Kumar, A., Levine, S., and Finn, C. One solution is not all you need: Few-shot
extrapolation via structured maxent rl. Advances in Neural Information Processing Systems, 33,
2020.

[32] Lehman, J. and Stanley, K. O. Evolving a diversity of virtual creatures through novelty
search and local competition. In Proceedings of the 13th annual conference on Genetic and
evolutionary computation, pp. 211–218, 2011.

[33] Liu, Y., Ramachandran, P., Liu, Q., and Peng, J. Stein variational policy gradient. In 33rd

Conference on Uncertainty in Artiﬁcial Intelligence, UAI 2017, 2017.

12

[34] Masood, M. A. and Doshi-Velez, F. Diversity-inducing policy gradient: Using maximum mean

discrepancy to ﬁnd a set of diverse policies. arXiv preprint arXiv:1906.00088, 2019.

[35] Mehta, N., Natarajan, S., Tadepalli, P., and Fern, A. Transfer in variable-reward hierarchical

reinforcement learning. Machine Learning, 73(3):289, 2008.

[36] Mouret, J.-B. and Clune, J. Illuminating search spaces by mapping elites. arXiv preprint

arXiv:1504.04909, 2015.

[37] Mutti, M., De Santi, R., De Bartolomeis, P., and Restelli, M. Challenging common assumptions

in convex reinforcement learning. arXiv preprint arXiv:2202.01511, 2022.

[38] Osborn, A. F. Applied imagination. Scribner’s, 1953.

[39] Parker-Holder, J., Pacchiano, A., Choromanski, K. M., and Roberts, S. J. Effective diversity in
population based reinforcement learning. Advances in Neural Information Processing Systems,
33, 2020.

[40] Peng, Z., Sun, H., and Zhou, B. Non-local policy optimization via diversity-regularized

collaborative exploration. arXiv preprint arXiv:2006.07781, 2020.

[41] Pugh, J. K., Soros, L. B., and Stanley, K. O. Quality diversity: A new frontier for evolutionary

computation. Frontiers in Robotics and AI, 3:40, 2016.

[42] Puterman, M. L. Markov decision processes: discrete stochastic dynamic programming. John

Wiley & Sons, 1984.

[43] Rockafellar, R. T. Convex analysis. Princeton university press, 1970.

[44] Schmitt, S., Hessel, M., and Simonyan, K. Off-policy actor-critic with shared experience replay.

In International Conference on Machine Learning, pp. 8545–8554. PMLR, 2020.

[45] Shani, L., Zahavy, T., and Mannor, S. Online apprenticeship learning. arXiv preprint

arXiv:2102.06924, 2021.

[46] Sharma, A., Gu, S., Levine, S., Kumar, V., and Hausman, K. Dynamics-aware unsupervised
discovery of skills. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=HJgLZR4KvH.

[47] Singh, A. K. Chapter 2 - structure, synthesis, and application of nanoparticles. In Singh,
A. K. (ed.), Engineered Nanoparticles, pp. 19–76. Academic Press, Boston, 2016.
ISBN
978-0-12-801406-6. doi: https://doi.org/10.1016/B978-0-12-801406-6.00002-9. URL https:
//www.sciencedirect.com/science/article/pii/B9780128014066000029.

[48] Stooke, A., Achiam, J., and Abbeel, P. Responsive safety in reinforcement learning by pid
lagrangian methods. In International Conference on Machine Learning, pp. 9133–9143. PMLR,
2020.

[49] Sun, H., Peng, Z., Dai, B., Guo, J., Lin, D., and Zhou, B. Novel policy seeking with constrained

optimization. arXiv preprint arXiv:2005.10696, 2020.

[50] Szepesv´ari, C. Constrained mdps and the reward hypothesis. Musings about machine learn-
ing and other things (blog), 2020. URL https://readingsml.blogspot.com/2020/03/
constrained-mdps-and-reward-hypothesis.html.

[51] Tarapore, D., Clune, J., Cully, A., and Mouret, J.-B. How do different encodings inﬂuence
the performance of the map-elites algorithm? In Proceedings of the Genetic and Evolutionary
Computation Conference 2016, pp. 173–180, 2016.

[52] Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. d. L., Budden, D., Abdolmaleki, A.,
Merel, J., Lefrancq, A., et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.

[53] Tessler, C., Mankowitz, D. J., and Mannor, S. Reward constrained policy optimization. In
International Conference on Learning Representations, 2019. URL https://openreview.
net/forum?id=SkfrvsA9FX.

13

[54] Vassiliades, V., Chatzilygeroudis, K., and Mouret, J.-B. Scaling up map-elites using centroidal

voronoi tessellations. arXiv preprint arXiv:1610.05729, 2016.

[55] Zahavy, T., Cohen, A., Kaplan, H., and Mansour, Y. Apprenticeship learning via frank-wolfe.

AAAI, 2020, 2020.

[56] Zahavy, T., Cohen, A., Kaplan, H., and Mansour, Y. Average reward reinforcement learning
with unknown mixing times. The Conference on Uncertainty in Artiﬁcial Intelligence (UAI),
2020.

[57] Zahavy, T., Barreto, A., Mankowitz, D. J., Hou, S., O’Donoghue, B., Kemaev, I., and Singh, S.
Discovering a set of policies for the worst case reward. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=PUkhWz65dy5.

[58] Zahavy, T., O’Donoghue, B., Desjardins, G., and Singh, S. Reward is enough for convex
MDPs. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in
Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=
ELndVeVA-TR.

[59] Zhang, J., Koppel, A., Bedi, A. S., Szepesvari, C., and Wang, M. Variational policy gradient
method for reinforcement learning with general utilities. arXiv preprint arXiv:2007.02151,
2020.

[60] Zhang, Y., Yu, W., and Turk, G. Learning novel policies for tasks. In Chaudhuri, K. and
Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learn-
ing, volume 97 of Proceedings of Machine Learning Research, pp. 7483–7492. PMLR, 09–15
Jun 2019. URL http://proceedings.mlr.press/v97/zhang19q.html.

14

Checklist

1. For all authors...

(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s

contributions and scope? [Yes]

(b) Did you describe the limitations of your work? [Yes] See the discussion in Appendix C

.

(c) Did you discuss any potential negative societal impacts of your work? [No] We could

identify any potential negative societal impacts for this work

(d) Have you read the ethics review guidelines and ensured that your paper conforms to

them? [Yes]

2. If you are including theoretical results...

(a) Did you state the full set of assumptions of all theoretical results? [N/A]
(b) Did you include complete proofs of all theoretical results? [N/A]

3. If you ran experiments...

(a) Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [No] While the DM
control domain is open sourced, our code is proprietary and we are not able to share it.
That said, we shared the source code for the diversity reward function in Appendix A.3
and provided pseudo code and hyper parameter details in Appendix A.2.

(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they

were chosen)? [Yes] See above.

(c) Did you report error bars (e.g., with respect to the random seed after running experi-

ments multiple times)? [Yes]

(d) Did you include the total amount of compute and the type of resources used (e.g., type

of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix E

4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...

(a) If your work uses existing assets, did you cite the creators? [N/A]
(b) Did you mention the license of the assets? [N/A]
(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]

(d) Did you discuss whether and how consent was obtained from people whose data you’re

using/curating? [N/A]

(e) Did you discuss whether the data you are using/curating contains personally identiﬁable

information or offensive content? [N/A]

5. If you used crowdsourcing or conducted research with human subjects...

(a) Did you include the full text of instructions given to participants and screenshots, if

applicable? [N/A]

(b) Did you describe any potential participant risks, with links to Institutional Review

Board (IRB) approvals, if applicable? [N/A]

(c) Did you include the estimated hourly wage paid to participants and the total amount

spent on participant compensation? [N/A]

15

A Additional Experiment Details

A.1 Environment

We evaluate our method on a number of tasks. These tasks have different complexity in terms of
control, dynamics and reward structure.

• walker.walk (Control Suite): a 8-dimensional control task with a 23-dimensional observa-
tion space, where the goal is to control the joints of a humanoid character to make it walk
forward in a 2D plane.

• walker.stand (Control Suite): a 8-dimensional control task with a 23-dimensional observa-
tion space, where the goal is to control the joints of a humanoid character to make it stand
up.

• dog.walk (Control Suite): a 38-dimensional control task with a 223-dimensional observation
space, where the goal is to control the joints of a dog character to make it walk forward in a
2D plane.

• dog.stand (Control Suite): a 38-dimensional control task with a 223-dimensional obser-
vation space, where the goal is to control the joints of a dog character to make it stand
up.

• BiPedal Walker: a 4-dimensional control task with a 24-dimensional observation space,
where the goal is to control the joints of a bipedal-walker with a large payload in order to
traverse a terrain from left to right as fast as possible.

16

A.2

Implementation details

Distributed agent Acting and learning are decoupled, with multiple actors gathering data in parallel
from a batched stream of environments, and storing their trajectories, including the latent variable z,
in a replay buffer and queue from which the learner can sample a mixed batch of online and replay
trajectories [44, 27]. The latent variable z is sampled uniformly at random in [1, n] during acting
at the beginning of each new episode. The learner differentiates the loss function as described in
Algorithm 1, and uses the optimizer (speciﬁed in Table 2) to update the network parameters and the
Lagrange multipliers (speciﬁed in Table 3). Lastly, the learner also updates the and moving averages
as described in Algorithm 1.

Initialization When training begins we initialize the network parameters as well as the Lagrange
multipliers: µi = σ−1(0.5), ∀i ∈ [2, n], where σ−1 is the inverse of the Sigmoid function µ1 = 1;
and the moving averages: ˜vavg
πi = ¯1/d, ∀i ∈ [1, n]. Here n is the number of
policies and d is the dimension of the features φ.

πi = 0., ∀i ∈ [1, n], ˜ψavg

Bounded Lagrange multiplier To ensure the Lagrange multiplier does not get too large so as to
increase the magnitude of the extrinsic reward and destabilize learning, we use a bounded Lagrange
multiplier [48] by applying Sigmoid activation on µ so the effective reward is a convex combination
of the diversity and the extrinsic rewards: r(s, a) = σ(µi)re(s, a) + (1 − σ(µi))ri
d(s, a), and the
objective for µ is σ(µ)(αv∗

e − dπ · re).

Average state occupancy The empirical feature averages used for experiments in the main text are
good, though imperfect due to the bias from samples before the policy mixes. In our experiments,
however, since the mixing time for the DM Control Suite is much shorter than the episode length T ,
the bias is small (∼ 5%).

Discounted state occupancy For a more scalable solution, as mentioned in Section 2, we can instead
predict successor features using an additional network head as shown in Fig. 11a. Similar to value
learning, we use V-trace [19] targets for training successor features. In discounted state occupancy
case we also use the extrinsic value function of each policy vi
e (Fig. 1a) to estimate dπ · re, instead of
the running average ˜vavg

πi . We show experimental results for this setup in Fig. 11b.

Loss functions. Instead of learning a single value head for the combined reward, our network has
two value heads, one for diversity reward and one for extrinsic reward.

We use V-trace [19] to compute td-errors and advantages for each of the value heads using the
”vtrace td error and advantage” function implemented here https://github.com/deepmind/
rlax/blob/master/rlax/_src/vtrace.py. The value loss for each head is the squared (cid:96)2 loss
d + td2
of the td-errors, and the combined value loss for the network is the sum of these two losses: td2
e.
In addition to that, our network has a policy head that is trained with a policy gradient loss as
implemented in https://github.com/deepmind/rlax/blob/master/rlax/_src/policy_
gradients.py). When training the policy, we combine the intrinsic and extrinsic advantages
δ = σ(µi)δe + (1 − σ(µi))δd (see the Weight cumulants function in Appendix A.3) which has the
same effect as combining the reward. However, we found that having two value heads is more stable
as each value can have a different scale.

The ﬁnal loss of the agent is a weighted sum of the value loss the policy loss and the entropy
regularization loss, and the weights can be found in Table 2.

Algorithm 1 also returns a Lagrange loss function, designed to force the policies to achieve a value
that is at least α times the value of the ﬁrst policy (which only maximizes extrinsic reward), where α
is the optimally ratio (Table 3). We update the Lagrange multipliers µ using the optimizer speciﬁed
in Table 3 but keep the multiplier of the ﬁrst policy ﬁxed µ1 = 1.

Lastly, Algorithm 1 also updates the moving averages.

17

Algorithm 1: Loss function

.

πi

i=1

(cid:111)n

s|xj

(cid:9)n
i=1,

(cid:110) ˜ψavg

s) is the probability assigned to aj
s), vd(xj

j=1 of size T, τj = (cid:8)zj, xj
s in state xi
s)} ← Network({τj}m
s), δe(xj

Parameters: Network parameters θ, Lagrange multipliers µ, moving averages
(cid:8)˜vavg
πi
Data: m trajectories {τj}m
µ(aj
Forward pass: {π(aj
s), ve(xj
Compute extrinsic td-errors and advantages: tde(xj
extrinsic reward rj
Compute intrinsic reward: ri
Compute intrinsic td-errors and advantages: tdd(xj
intrinsic reward rj
Combine advantages: δ(xj
Weighted loss:

s and intrinsic critic vd(xj
s)
s) + (1 − σ(µi))δd(xj
s) = σ(µi)δe(xj
s)

s and extrinsic critic ve(xj
s)
s) from ˜ψavg
πz , z, φj
d(xj

s with Eq. (6) or (8)

j=1)
s) ← V-trace with

s) ← V-trace with

s), δd(xj

s, µ(aj

s)(cid:9)T

s, φj

s, rj

s|xj

s|xj
s, aj
s=1 , where
s by the behaviour policy µ(a|x).

bv(tde(xj

s)2 + tdd(xj

s)2) + bπ log(π(aj

s|xj

s))δ(xj

s) + bEntEntropy(π(aj

s|xj

s))

(cid:88)

s,j

Lagrange loss:

Update moving averages:

n
(cid:88)

i=1

σ(µi)(˜vavg

πi − α˜vavg
π1 )

πi = α˜vavg
˜vavg

d

πi + (1 − α˜vavg
˜vavg

d

)rt,

˜ψavg

˜ψavg
πi = α
d

˜ψavg

πi + (1 − α

˜ψavg
d

)φt

return Weighted loss, Lagrange loss, (cid:8)˜vavg
πi

(cid:9)n
i=1,

(cid:110) ˜ψavg

πi

(cid:111)n

i=1

A.3 Functions

18

1 def intrinsic_reward ( phi , sfs , latents , attractive_power =3. ,

repulsive_power =0. , attractive_coeff =0. , target_d =1.) :
"""""" Computes a diversity reward using successor features .

Args :

phi : features [ tbf ].
sfs : avg successor features [ lf ] or
predicted , discounted successor features [ tbfl ].
latents : [ tbl ].
attractive_power : the power of the attractive force .
repulsive_power : the power of the repulsive force .
attractive_coeff : convex mixing of attractive & repulsive forces
target_d (\ ell_0 ) : desired target distance between the sfs .
When attractive_coeff =0.5 , target_d is the minimizer of the
objective , i . e . , the gradient ( the reward ) is zero .

Returns :

intrinsic_reward .

""""""
# If sfs are predicted we have 2 extra leading dims .
if jnp . ndim ( sfs ) == 4:

sfs = jnp . swapaxes ( sfs , 2 , 3)
of avg sf )
compute_dist_fn = jax . vmap ( jax . vmap ( compute_distances ) )
matmul_fn = lambda x , y : jnp . einsum ( ’tbl , tblf - > tbf ’ , x , y )

# tbfl -> tblf ( to match lf shape

elif jnp . ndim ( sfs ) == 2:

compute_dist_fn = compute_dist ances
matmul_fn = jnp . matmul

else :

raise ValueError ( ’ Invalid shape for argument ‘sfs ‘. ’)

l , f = sfs . shape [ -2:]
# Computes an tb lxl matrix where each row , corresponding to a

latent , is a 1 hot vector indicating the index of the latent with
the closest sfs

dists = compute_dist_fn ( sfs , sfs )
dists += jnp . eye ( l ) * jnp . max ( dists )
ne a r e s t_ l a t en ts_ ma tr ix = jax . nn . one_hot (

jnp . argmin ( dists , axis = -2) , num_classes = l )

# Computes a [ tbl ] vector with the nearest latent to each latent in

latents

nearest_latents = matmul_fn ( latents , ne ar es t_l at en ts_ ma tr ix )
# Compute psi_i - psi_j
psi_diff = matmul_fn ( latents - nearest_latents , sfs )
norm_diff = jnp . sqrt ( jnp . sum ( jnp . square ( psi_diff ) , axis = -1) ) /

# tbf

target_d

c = (1. - attractive_coeff ) * norm_diff ** repulsive_power
c -= attractive_coeff * norm_diff ** attractive_power
reward = c * jnp . sum ( phi * psi_diff , axis = -1) / f
return reward

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43
44 def l2dist (x , y ) :
45

"""""" Returns the L2 distance between a pair of inputs . """"""
return jnp . sqrt ( jnp . sum ( jnp . square ( x - y ) ) )

46

47
48 def c ompute_distances (x , y , dist_fn = l2dist ) :
49

"""""" Returns the distance between each pair of the two collections of

inputs . """"""

50

return jax . vmap ( jax . vmap ( dist_fn , ( None , 0) ) , (0 , None ) ) (x , y )

Listing 1: Intrinsic Reward

19

1 def weight_cumulants ( lagrange , latents , extrinsic_cumulants ,

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

i ntr i nsi c_cum ula n ts ) :
"""""" Weights cumulants using the Lagrange multiplier .

Args :

lagrange : lagrange [ l ].
latents : latents [ tbl ].
e xtr in si c_c u mul ants : [ tb ].
i ntr in si c_c u mul ants : [ tb ].

Returns :

extrinsic reward r_e and intrinsic_reward r_d .

""""""
sig_lagrange = jax . nn . sigmoid ( lagrange )
l ate n t_s ig_la gra n ge = jnp . matmul ( latents , sig_lagrange )
# No diversity rewards for latent 0 , only maximize extrinsic
reward
i ntr i nsi c_cum ula n ts *= (1 - latents [: , : , 0])
return (1 - la tent_s ig_ lagrang e ) * intrins ic_cumulants +
l ate n t_s ig_la gra n ge * extr insic_c um ulants
Listing 2: Weight cumulants

# l

# tb

1 def lagrangian ( lagrange , r , optimality_ratio ) :
2

"""""" Loss function for the Lagrange multiplier .

3

4

5

6

7

8

9

Args :

lagrange : lagrange [ l ].
r : moving averages of reward [ l ].
optimality_ratio : [1]. """"""
l_ = jax . nn . sigmoid ( lagrange )
return jnp . sum ( l_ * ( r -

r [0] * optimality_ratio ) )

Listing 3: lagrange loss function

A.4 Motion ﬁgures

Our ”motion ﬁgures” were created in the following manner. Given a trajectory of frames that
composes a video f1, . . . , fT , we ﬁrst trim and sub sample the trajectory into a point of interest
in time: fn, . . . , fn+m. We always use the same trimming across the same set of policies (the
sub ﬁgures in a ﬁgure). We then sub sample frames from the trimmed sequence at frequency 1/p:
fn, fn+p, fn+2p . . . ,. After that, we take the maximum over the sequence and present this ”max”
image. In Python for example, this simply corresponds to

n=400, m=30, p=3
indices = range(n, n+m, p)
im = np.max(f[indices])

This creates the effect of motion in single ﬁgure since the object has higher values than the background.

A.5 Hyperparameters

The hyperparameters in Table 2 are shared across all environments except in the BiPedal Domain
the learning rate is set to 10−5 and the learner frames are 5 × 107. We report the DOMiNO speciﬁc
hyperparameters in Table 3.

20

Hyperparameter

Replay capacity
Learning rate
Learner frames
Discount factor
bEnt Entropy regularization weight
bπ Policy loss weight
bv Value loss weight
Replay batch size
Online batch size
Sequence length
Optimizer

Value
5 × 105
10−4
2 × 107
0.99
0.01
1.0
1.0
600
6
40
RMSprop

Table 2: General hyperparameters

Hyperparameter

Control Suite BiPedal Walker

α Optimality ratio
Lagrange initialization
Lagrange learning rate
Lagrange optimizer
πi decay factor α˜vavg
˜vavg
d
˜ψavg
˜ψavg
πi decay factor α
d

0.9
0.5
10−3
Adam
0.9
0.99

0.7
0.5
10−3
Adam
0.999
0.9999

Table 3: DOMiNO hyperparameters

B Additional Experiment Results

B.1 Motion ﬁgures

We now present motion ﬁgures, similar to Fig. 1b, but in other domains (see Fig. 6-9). The videos,
associated with these ﬁgures can be found in a separate .zip ﬁle. Each ﬁgure presents ten polices
discovered by DOMiNO and their associated rewards (in white text) with the repulsive objective and
the optimality ratio set to 0.9. As we can see, the policies exhibit different gaits.

Next to each ﬁgure, we also present the distances between the expected features of the discovered
policies measured by the (cid:96)2 norm. In addition, in each row i we use a dark black frame to indicate
the the index of the policy with the closest expected features to πi , i.e., in the i-th row we highlight
the j-th column such that j = j∗

i = arg minj(cid:54)=i ||ψi − ψj||2
2.

21

Figure 5: QD in walker.stand. Left: 10 policies and corresponding rewards. Right: distances in (cid:96)2
norm between the Successor features of the policies.

Figure 6: QD in walker.walk. Left: 10 policies and corresponding rewards. Right: distances in (cid:96)2
norm between the Successor features of the policies.

Figure 7: QD in dog.stand. Left: 10 policies and corresponding rewards. Right: distances in (cid:96)2
norm between the Successor features of the policies.

Figure 8: QD in dog.walk. Left: 10 policies and corresponding rewards. Right: distances in (cid:96)2 norm
between the Successor features of the policies.

22

B.2 Additional Quality Diversity Results

We now present additional experimental results evaluating the trade-off between quality and diversity
using the scatter plots introduced in Section 4.1. y-axis shows the episode return while the diversity
score, corresponding to the Hausdorff distance (Eq. (5)), is on the x-axis. The top-right corner of the
diagram represents the most diverse and highest quality policies. Each ﬁgure presents a sweep over
one or two hyperparameters and we use the color and a marker to indicate the values. In all of our
scatter plots, we report 95% conﬁdence intervals, corresponding to 5 seeds, which are indicated by
the crosses surrounding each point.

Quality Diversity: walker.walk In Fig. 9 we show experimental results for DOMiNO in the
walker.walk domain. Consistent with Fig. 2 which shows similar results on walker.stand, we show
that our constraint mechanism is working as expected across different set sizes and optimality ratios
across different tasks.

Figure 9: QD Scaling results on walker.walk task. Left: Number of policies vs. optimality ratio in
walker.walk with the repulsive reward and (center) with the VDW reward. Right: Optimality ratio vs.
VDW target distance (cid:96)0.

Quality Diversity: SMERL vs DOMiNO In Fig. 10 we show further experimental results in
walker.stand for SMERL in comparison to DOMiNO. When SMERL is appropriately tuned (here for
the 10 policies conﬁguration), it can ﬁnd some solutions along the upper-right QD border; however
we ﬁnd that the best cd does not transfer to other conﬁgurations. The choice of cd that enables the
agent to ﬁnd a set of 10 diverse policies produces sets without diversity for any other set size.

Figure 10: Scaling SMERL (left) vs. DOMiNO (right) on Walker.Stand. Set size is indicated with
marker, color corresponds to optimality ratio α. The cd for SMERL is set to 0.5, which was tuned
using a set size of 10 policies (see 3, left). This choice does not scale well to any other set size,
where regardless of optimality ratios, all policies only optimize for extrinsic reward, at the expense of
diversity.

Discounted State Occupancy We run the same experiments reported in Fig. 2 with DOMiNO’s
Lagrangian method and report the results in Fig. 11b. As can be observed, using predicted discounted
features does not make any signiﬁcant difference in performance. Since the mixing time for the DM
Control Suite is much shorter than the episode length T , the bias in the empirical feature averages is
small.

23

(a)

(b)

Figure 11: (a) DOMiNO with a discounted state occupancy. An additional network head is trained
to predict successor features ψγ, which are used instead of the average features ψavg to compute
the diversity reward. The discounted, extrinsic value is used as a constraint instead of the averaged
rewards. Dashed lines signify training objectives. (b) Number of policies vs. optimality ratio in
walker.stand with DOMiNO, consistent with Fig. 2.

C Limitations

Diversity increasing by decreasing α Inspecting Fig. 9, Fig. 11b and Fig. 2. we can observe that
the diversity score increases for lower optimality ratios. Recall that the optimality ratio α speciﬁes a
feasibility region in the state-occupancy space (the set of all α-optimal policies). Thus, the size of
this space increases as α decreases, and we observe more diverse sets for smaller values of α. This
intuition was correct in most of our experiments, but not always (e.g., Fig. 9).

One possible explanation is that the Lagrange multipliers solution is seeking for the lowest value of
λ that satisﬁes the constraint (so that we can get more diversity), i.e., it ﬁnds solutions that satisfy
the constraint almost with equality: vi
e ). The size of the level sets
e ≥ αv∗
e ) do not necessarily increase with lower values of α (while the feasibility sets vi
(vi
e
do). Another explanation is that in walker.walk (Fig. 9) it might be easier to ﬁnd diverse walking
(e.g., α = 0.9) than diverse “half walking” (e.g., α = 0.5). This might be explained by “half walking”
being less stable (it is harder to ﬁnd diverse modes for it).

e (instead of vi

e > αv∗

e ∼ αv∗

e = αv∗

Features Another possible limitation of our approach is that diversity is deﬁned via the environment
features. We partially addressed this concern in Section 4.2 where we showed that it is possible to
learn QD policies with our approach using the embedding of a NN as features. In future work we
plan to scale our approach to higher dimensional domains and study which auxiliary losses should be
added to learn good representations for diversity.

D Additional K-shot experiments

D.1 Control suite

Next, we report additional results for K-shot adaptation in the control suite. In Fig. 12 we report the
absolute values achieved by each method obtained (in the exact same setup as in Fig. 4). That is,
we report rmethod for each method (instead of rmethod/rbaseline as in Fig. 4). Additionally, we report
rbaseline, which is the ”Single policy baseline” (blue) in Fig. 12. Inspecting Fig. 12, we can see that all
the methods deteriorate in performance as the magnitude of the perturbation increases. However, the
performance of DOMiNO (orange) deteriorates slower than that of the other methods. We can also
see that the performance of the no diversity baseline is similar when it learns 10 policies (red) and a

24

U
p
d
a
t
e
c
o
n
s
t
r
a
i
n
t

&
L
a
g
r
a
n
g
e

MLP

Torso

MLP

D
i
v
e
r
s
i
t
y
R
e
w
a
r
d

 
 
 
 
 
 
 
single policy (blue), which indicates that when the algorithm maximize only the extrinsic reward, it
ﬁnds the same policy again and again with each of the 10 policies.

Figure 12: K-shot adaptation in Control Suite, similar to Figure 4, but reporting absolute rather than
relative returns.

Next, we inspect a wider range of hyper parameters for the SMERL and Multi-Objective methods.
Concretely, for DOMiNO and SMERL α = 0.9, for SMERL cd ∈ [0.5, 1, 2, 5] and for Multi-
Objective ce ∈ [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.9] and all methods are trained with 10 policies.
These values correspond to the values that we considered in Fig. 3.

Inspecting Fig. 13 we can see that the best methods overall are DOMiNO and SMERL (with cd = 1).
We can also see that DOMiNO and SMERL consistently outperform the multi-objective baseline
for many hyper parameter values. This is consistent with our results in Fig. 3 which suggest that
the Multi-Objective method tends to be either diverse or high performing and fails to capture a
good balance in between. Lastly, it is reasonable that SMERL and DOMiNO perform similar since
they are both valid solutions to the same CMDP. However, SMERL comes an with additional hyper
parameter cd that may be tricky to tune in some situations. For example, trying to tune cd based
on the results in the vanilla domain (picking the upper-right most point in Fig. 3) led us to choose
cd = 0.5 for SMERL, instead of 1. The Lagrange multipliers formulation in DOMiNO does not have
this challenge as it does not have an extra hyper parameter.

Figure 13: K-shot in Control Suite, similar to Figure 4, but reporting a wider range of hyper parameters
for SMERL and Multi-Objective.

D.2 BipedalWalker

2, h1

For the BipedalWalker environment, we either perturb the morphology or the terrain. To perturb the
morphology, we follow [25] and specify a set of re-scaling factors. Speciﬁcally, each leg is made
up of two rectangles, with pre-deﬁned width and height parameters: leg1 = ((w1
1)),
leg2 = ((w1
2)). To generate a perturbed morphology, we deﬁne a scaling range [0, η]
withing which we uniformly sample scaling factors (cid:96)j
i ∼ [−η, η], for i = 1, 2 and j = 1, 2. A
perturbed environment is deﬁned by re-scaling the default parameters: (cid:102)leg1 = (((1 + (cid:96)1
1, (1 +
ν1
2 )h2
1 )h2
1 )h1
1))).
The values for this perturbations can be found in Table 4.

1))), and (cid:102)leg2 = (((1+(cid:96)1

1)w1
1, (1+ν2

1), ((1+(cid:96)2

1), ((1+(cid:96)2

1, (1+ν2

1, (1+ν1

1), (w2

2), (w2

1)w2

2)w2

2)w1

i , νj

1, h2

1, h1

2, h2

2 )h1

25

s
n
r
u
t
e
r

n
a
e
M

s
n
r
u
t
e
r

n
a
e
M

1000

d
n
a
t
S

400

1000

l

k
a
W

200

200

200

0

0

1

0

2

0

3

200

0

4

0 . 2

5

2

Motor failure (duration)

200

0 . 4

0 . 3

5

7

0 . 3
0 . 3
0 . 2
Thigh length

600

400

600

200

5

0 . 4
0 . 4
0 . 3
Torso length

5

0 . 5

0 . 1

3 . 0
2 . 0
1 . 0
Joint damping

4 . 0

0 . 7

2

0 . 1

0 . 0
Contact friction

0 . 0

0

Single policy baseline
DOMiNO
SMERL
No diversity
Multi-Objective

5

1

0

0 . 0

 
 
1000

s
n
r
u
t
e
r

n
a
e
M

d
n
a
t
S

200

1000

s
n
r
u
t
e
r

n
a
e
M

l

k
a
W

200

200

200

400

600

200

200

200

200

0

0

1

0

2

0

3

0

4

0 . 2

5

2

7

0 . 3

0 . 2

5

0 . 3

0 . 4

0 . 3

5

0 . 3

0 . 4

5

0 . 4

0 . 5

0 . 1

1 . 0

2 . 0

3 . 0

4 . 0

0 . 7

0 . 1

2

0 . 0

5

0

0 . 0

1

0

0 . 0

Motor failure (duration)

Thigh length

Torso length

Joint damping

Contact friction

Single policy baseline
DOMiNO
Multi Objective (0.1)
Multi Objective (0.2)
Multi Objective (0.3)
Multi Objective (0.4)
Multi Objective (0.5)
Multi Objective (0.6)
Multi Objective (0.9)
Multi Objective (0.7)
SMERL ( =1.0)

SMERL ( =2.0)

SMERL ( =5.0)

SMERL ( =0.5)

 
 
Perturbation type

Perturbation scale parameter values (η)

Morphology
Stumps (height, width)
Pits (width)
Stairs (height, width)

0., 0.10, 0.15, 0.20, 0.25, 0.30, 0.35
0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.
0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.
0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.

Table 4: Bipedal perturbation scale values

For terrain changes, we selectively enable one of three available obstacles available in the OpenAI
Gym implementation: stumps, pits, or stairs. For each obstacle, we specify a perturbation interval
[0, η]. This interval determines the upper bounds on the obstacles height and width when the
environment generates terrain for an episode. For details see the “Hardcore” implementation of the
BiPedal environment. Note that for stairs, we ﬁxed the upper bound on the number of steps the
environment can generate in one go to 5.

To evaluate adaptation, we ﬁrst train 10 agents independently on the “BiPedalwalker-v3” environment,
which only uses a ﬂat terrain. To evaluate the trained agents we sample random perturbations of the
environment. Speciﬁcally, for each type of perturbation (morphology, pits, stumps, stairs) and for
each value of the scale parameter η, we randomly sample 30 perturbations. We then run each option
for 40 episodes; adaptation takes the form of using the ﬁrst 10 episodes to estimate the option with
highest episode return, which is then used for evaluation on the remaining 30 episodes.

Figure 14: K-shot adaptation in BiPedal walker

Fig. 14 shows that, while performance degrades as the morphology is deformed, DOMiNO exhibits
greater adaptability as evidenced by less severe degradation of performance. In terms of morphology,
we ﬁnd a gradual decline in performance as we increase the degree of deformation. Similar to
the Control Suite, diversity is beneﬁcial and helps the agent adapt while not being impervious to
these changes. In terms of terrain perturbations, these have a more abrupt impact on the agent’s
performance. While diversity does not prevent a signiﬁcant drop in performance, it is still beneﬁcial
when adapting to stumps and pits and does not negatively impact performance in the case of stairs.

E Computing Infrastructure

We run our experiments using a distributed infrastructure implemented in JAX [12]. Each run took
approximately 10 hours to complete. The computing infrastructure is based on an actor-learner
decomposition [19], where multiple actors generate experience in parallel, and this experience is
channelled into a learner.

It allows us to run experiments in two modalities. In the ﬁrst modality, following [19], the actors
programs are distributed across multiple CPU machines, and both the stepping of environments and
the network inference happens on CPU. The data generated by the actor programs is processed in

26

400

r
e
k
a
W

l

l

No diversity
With diversity

a
d
e
P
B

i

-100

0.0 0.1 0.2 0.3
Morphology scale

0.0 0.3 0.6 0.9
Stump size

400

r
e
k
a
W

l

l

a
d
e
P
B

i

-100

0.0 0.3 0.6 0.9
Pit size

0.0 0.3 0.6 0.9
Stairs step size

 
 
batches by a single learner using a GPU. Alternatively, both the actors and learners are co-located
on a single machine, where the host is equipped with 56 CPU cores and connected to 8 TPU cores
[30]. To minimize the effect of Python’s Global Interpreter Lock, each actor-thread interacts with a
batched environment; this is exposed to Python as a single special environment that takes a batch of
actions and returns a batch of observations, but that behind the scenes steps each environment in the
batch in C++. The actor threads share 2 of the 8 TPU cores (to perform inference on the network),
and send batches of ﬁxed size trajectories of length T to a queue. The learner threads takes these
batches of trajectories and splits them across the remaining 6 TPU cores for computing the parameter
update (these are averaged with an all reduce across the participating cores). Updated parameters are
sent to the actor’s TPU cores via a fast device to device channel, as soon as the new parameters are
available. This minimal unit can be replicates across multiple hosts, each connected to its own 56
CPU cores and 8 TPU cores, in which case the learner updates are synced and averaged across all
cores (again via fast device to device communication).

27

","2 2 0 2 y a M 6 2 ] I A . s c [ 1 v 1 2 5 3 1 . 5 0 2 2 : v i X r a Discovering Policies with DOMiNO: Diversity Optimization Maintaining Near Optimality Tom Zahavy DeepMind, London tomzahavy@deepmind.com Yannick Schroecker DeepMind, London yschroecker@deepmind.com Feryal Behbahani DeepMind, London feryal@deepmind.com Kate Baumli DeepMind, London baumli@deepmind.com Sebastian Flennerhag DeepMind, London ﬂennerhag@deepmind.com Shaobo Hou DeepMind, London shaobohou@deepmind.com Satinder Singh DeepMind, London baveja@deepmind.com Abstract Finding different solutions to the same problem is a key aspect of intelligence associated with creativity and adaptation to novel situations. In reinforcement learning, a set of diverse policies can be useful for exploration, transfer, hierarchy, and robustness. We propose DOMiNO, a method for Diversity Optimization Maintaining Near Optimality. We formalize the problem as a Constrained Markov Decision Process where the objective is to ﬁnd diverse policies, measured by the distance between the state occupancies of the policies in the set, while remaining near-optimal with respect to the extrinsic reward. We demonstrate that the method can discover diverse and meaningful behaviors in various domains, such as different locomotion patterns in the DeepMind Control Suite. We perform extensive analysis of our approach, compare it with other multi-objective baselines, demonstrate that we can control both the quality and the diversity of the set via interpretable hyperparameters, and show that the discovered set is robust to perturbations. 1 Introduction Creative problem solving is the mental process of searching for an original and previously unknown solution to a problem [38]. The relationship between creativity and intelligence is widely recognized across many ﬁelds; for example, in the ﬁeld of Mathematics, ﬁnding different proofs to the same theorem is considered elegant and often leads to new insights. Closer to Artiﬁcial Intelligence (AI), consider the ﬁeld of game playing and, speciﬁcally, the game of Chess in which a move is considered creative when it goes beyond known patterns [17]. In some cases, such moves can only be detected by human players while remaining invisible to current state-of-the-art Chess engines. A famous example thereof is the winning move in game eight of the Classical World Chess Championship 2004 between Leko and Kramnik [8, 3]. Humans and, indeed, many animals employ similarly creative behavior on a daily basis; faced with a challenging problem, we often consider qualitatively different alternative solutions. Yet, the majority of AI research is focused on ﬁnding a single best solution to a given problem. For example, in the ﬁeld of Reinforcement Learning (RL), most algorithms are designed to ﬁnd a single reward-maximizing policy. However, for many problems of interest there may be many qualitatively Preprint. Under review. different optimal or near-optimal policies; ﬁnding such diverse set of policies may help an RL agent become more robust to changes in the task and/or environment and to generalize better to future tasks. The majority of the literature on this problem has been done in the ﬁeld of Quality-Diversity (QD), which comprises of two main families of algorithms: MAP-Elites [36, 16] and novelty search with local competition [32]. QD algorithms typically maintain a collection of policies and adapt it using evolutionary algorithms to balance the QD trade-off [41, 51, 15]. Further references can be found on the QD webpage. In contrast to this line of work, we propose a differentiable optimization framework for maximizing the diversity of a set of RL policies. We do so by formulating diversity maximization as an optimization problem in state occupancies, and then showing that we can solve this problem by maximizing an intrinsic reward that corresponds to the gradient of the diversity objective. In related work, intrinsic rewards have been used for learning diversity in terms of the discriminability of different trajectory-speciﬁc quantities [24, 20, 46, 7]. Other works implicitly induce diversity to learn policies that maximize the set robustness to the worst-possible reward [31, 57], or use diversity as a regularizer when maximizing the extrinsic reward [29, 34, 40, 49, 59, 46]. Our work makes the following contributions. First, we propose DOMiNO, a method for Diversity Optimization that Maintains Nearly Optimal policies. DOMiNO trains a set of policies using a policy-speciﬁc, weighted combination of the extrinsic reward and an intrinsic diversity reward. The weights are adapted as Lagrange multipliers to guarantee that each policy is near-optimal. Second, we propose to measure diversity via expected features; i.e., the features that a policy observes in its state occupancy. Under this measure of diversity, we introduce two novel objectives for diversity optimization: a repulsive force that motivates policies to have distinct expected features and a Van Der Waals force, which combines the repulsive force with an attractive one and allows us to specify the degree of diversity in the set. Third, we perform experiments in the DeepMind Control Suite [52] and the BiPedal walker environment [13] and show that DOMiNO discovers qualitatively diverse locomotion behaviors (Fig. 1b). We analyze our approach and compare it to other multi-objective strategies for handling the QD trade-off. Lastly, we demonstrate that the discovered set is robust to perturbations of the environment and the morphology of the avatar. (a) (b) Figure 1: (a) DOMiNO’s architecture: The agent learns a set of QD policies via a single latent- conditioned actor-critic network with intrinsic and extrinsic value heads. Dashed arrows signify training objectives. (b) DOMiNO’s π: Near optimal diverse policies in walker.stand corresponding to standing on both legs, standing on either leg, lifting the other leg forward and backward, spreading the legs and stamping. Not only are the policies different from each other, they also achieve high extrinsic reward in standing (see values on top of each policy). 2 Preliminaries and Notation In this work, we will express objectives in terms of the state occupancy measure dπ. Intuitively speaking, dπ measures how often a policy π visits each state-action pair. As we will soon see, the classic RL objective of reward maximization can be expressed as a linear product between the reward vector and the state occupancy. In addition, in this work we will formulate diversity maximization via an objective that is a nonlinear function of the state occupancy. While it might seem unclear which 2 i R u n n n g A v e r a g e U p d a t e c o n s t r a i n t & L a g r a n g e MLP Torso MLP i R u n n n g A v e r a g e D i v e r s i t y R e w a r d reward should be maximized to solve such an objective, we take inspiration from Convex MDPs [58] where one such reward is the gradient of the objective with respect to dπ. We begin with some formal deﬁnitions. In RL an agent interacts with an environment and seeks to maximize its cumulative reward. We consider two cases, the average reward case and the discounted case. The Markov decision process [42, MDP] is deﬁned by the tuple (S, A, P, R) for the average reward case and by the tuple (S, A, P, R, γ, D0) for the discounted case. We assume an inﬁnite horizon, ﬁnite state-action problem. Initially, the state of the agent is sampled according to s0 ∼ D0. At time t, given state st, the agent selects action at according to its policy π(st, ·), receives a reward rt ∼ R(st, at) and transitions to a new state st+1 ∼ P (·, st, at). We consider two performance t=1γtrt, for the average reward metrics, given by vavg case and discounted case respectively. The goal of the agent is to ﬁnd a policy that maximizes vavg or vγ π. Let Pπ(st = ·) be the probability measure over states at time t under policy π, then the state occupancy measure dπ is given as davg t=1 Pπ(st = s)π(s, a), and π(s, a) = (1 − γ)E (cid:80)∞ t=1 γtPπ(st = s)π(s, a), for the average reward case and the discounted dγ case respectively. With these, we can rewrite the RL objective in as a linear function of the occupancy measure maxdπ∈K s,a r(s, a)dπ(s, a), where K is the set of admissible distributions [58]. Next, consider an objective of the form: π (s, a) = limT →∞ π = (1 − γ)E(cid:80)∞ π = limT →∞ T E (cid:80)T t=1rt, vγ T E(cid:80)T (cid:80) π 1 1 min dπ∈K f (dπ), (1) where f : K → R is a nonlinear function. Sequential decision making problems that take this form include Apprenticeship Learning (AL) and pure exploration, among others [1, 55, 26, 59, 23, 58, 45, 9, 37]. In the remainder of this section, we brieﬂy explain how to solve Eq. (1) using RL methods when the function f is convex. We begin with rewriting Eq. (1) using Fenchel duality as min dπ∈K f (dπ) = max λ∈Λ min dπ∈K (λ · dπ − f ∗(λ)) (2) where Λ is the closure of (sub-)gradient space {∂f (dπ)|dπ ∈ K}, which is compact and convex [2], and f ∗ is the Fenchel conjugate of the function f . Eq. (2) presents a zero-sum max-min game between two players, the policy player Algπ and the cost player Algλ. We can see that from the perspective of the policy player, the objective is a linear minimization problem in dπ. Thus, intuitively speaking, the goal of the policy player is to maximize the negative cost as a reward r = −λ. To solve Eq. (2), we employ the meta algorithm from [2], which uses two online learning algorithms. The policy player Algπ generates a sequence of policies {πk}k∈N by maximizing a sequence of negative costs {−λk}k∈N as rewards that are produced by the cost player Algλ. In this paper, the policy player uses an online RL algorithm and the cost player uses the Follow the Leader (FTL) algorithm. This implies that the cost at time k is given as λk = ∇f ( ¯dk−1 π ). (3) π In other words, to solve an RL problem with a convex objective function (Eq. (1)), the policy player maximizes a non stationary reward that at time k corresponds to the negative gradient of the objective function f w.r.t ¯dk−1 . When the function f is convex, it is guaranteed that the average state occupancy of these polices, ¯dK π, converges to an optimal solution to Eq. (1), i.e., ¯dK π → d(cid:63) Features and expected features. We focus on the case where each state-action pair is associated with some observable features φ(s, a) ∈ Rd. For example, in the DM control suite [52], these features correspond to the positions and velocities of the body joints being controlled by the agent. In other cases, we can learn φ with a neural network. π ∈ arg mindπ∈K f (dπ) [58]. π = 1 K k=1 dk (cid:80)K Similar to value functions, which represent the expectation of the reward under the state occupancy, we deﬁne expected features as ψπ(s, a) = Es(cid:48),a(cid:48)∼dπ(s,a) φ(s(cid:48), a(cid:48)) ∈ Rd. Note that in the special case of one-hot feature vectors, the expected features coincide with the state occupancy. The deﬁnition of ψπ depends on the state occupancy we consider. In the discounted case, ψγ π ∈ Rd is also known as Successor Features (SFs) as deﬁned in [5, 6]. In the average case, ψavg π ∈ Rd represents the expected features under the policy’s stationary distribution and therefore it has the same value for all the state action pairs. Similar deﬁnitions were suggested in [35, 56]. 3 3 Discovering diverse near-optimal policies We now introduce Diversity Optimization Maintaining Near Optimality, or, DOMiNO, which discovers a set of n policies Πn = {πi}n i=1 by solving the optimization problem: max Πn Diversity(Πn) s.t dπ · re ≥ αv∗ e , ∀π ∈ Πn, (4) where v∗ e is the value of the optimal policy. In other words, we are looking for a set of policies that are as diverse from each other as possible, deﬁned as Diversity : {R|S||A|}n → R. In addition, we constrain the policies in the set to be nearly optimal. To deﬁne near-optimality we introduce a hyperparameter α ∈ [0, 1], such that a policy is said to be near optimal if it achieves a value that is at e . In practice, we “ﬁx” the Lagrange multiplier for the ﬁrst policy µ1 = 1, so this policy only least αv∗ receives extrinsic reward, and use the value of this policy to estimate v∗ e ). Notice that this estimate is changing through training. e = v1 e (v∗ Before we dive into the details, we brieﬂy explain the main components of DOMiNO. Building on Section 2 and, in particular, Eq. (3), we ﬁnd policies that maximize the diversity objective by maximizing its gradient as a reward signal, i.e., ri π). We discuss two candidates for this objective and derive an analytical formula for the associated rewards in Section 3.1. Diversity(d1 π, . . . , dn d = ∇di π Then, in Section 3.2 we explain how to combine the two rewards via the coefﬁcients ce, cd. Thus, each of the policies, π1, . . . , πn, will be maximizing a reward signal ri that is a linear combination of the extrinsic reward re and ri d : i.e., ri(s, a) = cere(s, a) + cdri d(s, a). To this end, we focus on the method of Lagrange multipliers, which adapts the coefﬁcients online in order to solve Eq. (4) and compare it with other multi-objective baselines. 3.1 Diversity Next, we present an objective that motivates policies to visit different states on average. It does so by leveraging the information about the policies’ long-term behavior available in their expected features, and motivating the state occupancies to be different from each other. For that reason, we refer to this objective as a repulsive force (Eq. (5)). We then extend this objective and combine it with a second, attractive force (Eq. (7)), taking inspiration from the Van Der Waals (VDW) force. The manner in which we combine these two forces allows us to control the degree of diversity in the set. A repulsive force. How do we compute a set of policies with maximal distances between their expected features? To answer this question, we ﬁrst consider the simpler scenario where there are only two policies in the set and consider the following objective maxπ1,π2 ||ψ1 − ψ2||2 2. This objective is related to the objective of Apprenticeship Learning [AL; 1], i.e., solving the problem minψ ||ψ − ψE||2 2, where ψE are the feature expectations of an expert. Both problems use the euclidean norm in the feature expectation space to measure distances between policies. Since we are interested in diversity, we are maximizing this objective, while AL aims to minimize it. In a similar fashion, the mutual information between policies and states, which is equivalent to the KL divergence between state occupancies [58, 21] is minimized for AL [28] and maximized for diversity [20]. Next, we investigate how to measure the distance of a policy from the set of multiple policies, Πn. First we introduce the Hausdorff distance [43] that measures how far two subsets D, C of a metric space are from each other: Dist(D, C) = minc∈C,d∈D ||c − d||2 2. In other words, two sets are far from each other in the Hausdorff distance if every point of either set is far from all the points of the other set. Building on this deﬁnition, we can deﬁne the distance from an expected features vector ψi to the set of the other expected features vectors as minj(cid:54)=i ||ψi − ψj||2 2. This equation gives us the distance between each individual policy and the other policies in the set. Maximizing it across the policies in the set, gives us our ﬁrst diversity objective: max d1 π,...,dn π 0.5 (cid:88)n i=1 min j(cid:54)=i ||ψi − ψj||2 2. (5) π Diversity(d1 to compute the associated diversity reward, we compute the gradient ri d = In order ∇di π). To do so, we begin with a simpler case where there are only two policies, 2 = φ · (ψ1 − ψ2), i.e., r = ∇d1 2 = ∇d1 such that r(s, a) = φ(s, a) · (ψ1 − ψ2). This reward was ﬁrst derived by Abbeel & Ng [1], but here it is with an opposite sign since we care about maximizing it. Lastly, for a given policy πi, we deﬁne by π(s,a)φ(s, a) − Es(cid:48),a(cid:48)∼d2 π, . . . , dn ||ψ1 − ψ2||2 π(s,a)φ(s, a)||2 ||Es(cid:48),a(cid:48)∼d1 π π 4 π i , we get1 that ∇di i the index of the policy with the closest expected features to πi, i.e., j∗ j∗ Using the deﬁnition of j∗ 2 = ∇di minj(cid:54)=i ||ψi − ψj||2 d(s, a) = φ(s, a) · (ψi − ψj∗ ri The Van Der Waals force. Next, we propose a second diversity objective that allows us to control the degree of diversity in the set via a hyperparameter. The objective is inspired from molecular physics, and speciﬁcally, by how atoms in a crystal lattice self-organize themselves at equal distances from each other. This phenomenon is typically explained as an equilibrium between two distance dependent forces operating between the atoms known as the Van Der Waals (VDW) forces; one force that is attractive and another that is repulsive. i = arg minj(cid:54)=i ||ψi − ψj||2 2. i ||2 ||ψi − ψj∗ 2, and that i ). (6) π The VDW force is typically characterized by a distance in which the combined force becomes repulsive rather than attractive (see, for example, [47]). This distance is called the VDW contact distance, and we denote it by (cid:96)0. In addition, we denote by (cid:96)i = ||ψi − ψj∗ i ||2 the Hausdorff distance for policy i. With this notation, we deﬁne our second diversity objective as2 max π,...,dn d1 π (cid:88)n i=1 0.5(cid:96)2 i (cid:124) (cid:123)(cid:122) (cid:125) Repulsive i /(cid:96)3 0 −0.2(cid:0)(cid:96)5 (cid:123)(cid:122) (cid:124) Attractive (cid:1) (cid:125) . (7) We can see that Eq. (7) is a polynomial in (cid:96)i, composed of two forces with opposite signs and different powers. The different powers determine when each force dominates the other. For example, when the expected features are close to each other ((cid:96)i << (cid:96)0), the repulsive force dominates, and when ((cid:96)i >> (cid:96)0) the attractive force dominates. The gradient (and hence, the associated reward) is given by d(s, a) = (1 − ((cid:96)i/(cid:96)0)3)φ(s, a) · (ψi − ψj∗ ri Inspecting Eq. (8) we can see that when the expected features are organized at the VDW contact distance (cid:96)0, the objective is maximized and the gradient is zero. In a related line of work, Vassiliades et al. [54] suggested to use voronoi tessellation to partition the feature space of the MAP-Elite algorithm to regions of equal size and Liu et al. [33] proposed a Stein Variational Policy Gradient with repulsive and attractive components. However, using a VDW force to control diversity is novel to the best of our knowledge. i ). (8) 3.2 Balancing Quality with Diversity Constrained MDPs. At the core of our approach is a solution to the CMDP in Eq. (4). There exist different methods for solving CMDPs and we refer the reader to [4] and [50] for treatments of the subject at different levels of abstraction. In this work we will focus on a reduction of CMDPs to MDPs via gradient updates, known as Lagrangian methods [11, 10, 53, 14]. Most of the literature on CMDPs has focused on linear objectives and linear constraints. In Section 2, we discussed how to solve an unconstrained convex RL problem of the form of Eq. (1) as a saddle point problem. We now extend these results to the case where the objective is convex and the constraint is linear, i.e. mindπ∈K f (dπ), subject to g(dπ) ≤ 0, where f denotes the diversity objective and g is a linear function of the form g(dπ) = αv∗ e − dπ · re deﬁning the constraint. Solving this problem is equivalent to solving the following problem: min dπ∈K max µ≥0 f (dπ) + µg(dπ) = min dπ∈K max µ≥0,λ λ · dπ − f ∗(λ) + µ(αv∗ e − dπ · re), (9) where the equality follows from Fenchel duality as before. Similar to Section 2, we use the FTL algorithm for the λ player (Eq. (3)). This implies that the cost at iteration k, λk, is equivalent to the gradient of the diversity objective, which we denote by rd. Eq. (9) involves a vector, λ − µre, linearly interacting with dπ. Thus, intuitively speaking, minimizing Eq. (9) from the perspective of the policy player is equivalent to maximizing a reward rd + µre. The objective for the Lagrange multiplier µ is to maximize Eq. (9), or equivalently µ(αv∗ e − dπ · re). Intuitively speaking, when the policy achieves an extrinsic value that satisﬁes the constraint, the 1In the rare case that the arg min has more than one solution, the gradient is not deﬁned, but we can still use Eq. (6) as a reward. 2The coefﬁcients in Eq. (7) are chosen to simplify the reward in Eq. (8). I.e., since the reward is the gradient of the objective, after differentiation the coefﬁcients equal 1 in Eq. (8). 5 Lagrange multiplier µ decreases (putting a smaller weight on the extrinsic component of the reward) and it increases otherwise. More formally, we can solve the problem in Eq. (9) as a three-player game. In this case the policy player controls dπ as before, the cost player chooses λ using Eq. (3), and the Lagrange player chooses µ with gradient descent. Proving this statement is out of the scope of this work, but we shall investigate it empirically. 3.3 Multi-objective alternatives We conclude this section by discussing two alternative approaches for balancing the QD trade-off, which we later compare empirically with the CMDP approach. First, consider a multi-objective MDP that combines the diversity objective with the extrinsic reward as π, . . . , dn cedi π · re + cdDiversity(d1 (10) π), max Πn where ce, cd are ﬁxed weights that balance the diversity objective and the extrinsic reward. We note that the solution of such a multi-objective MDP cannot be a solution to a CMDP in general. I.e., it is not possible to ﬁnd the optimal dual variables µ∗, plug them into Eq. (9) and simply solve the resulting (unconstrained) MDP. Such an approach ignores the fact that the dual variables must be a ‘best-response’ to the policy and is referred to as the ”scalarization fallacy” in [50, Section 4]. While multi-objective MDPs have been used in prior QD-RL papers [29, 34, 39, 22, 40, 60], we now outline a few potential advantages for using CMDPs. First, the CMDP formulation guarantees that the policies that we ﬁnd are near optimal (satisfy the constraint). Secondly, the weighting coefﬁcient in multi-objective MDPs has to be tuned, where in CMDPs it is adapted. This is particularly important in the context of maximizing diversity while satisfying reward. Next, consider a hybrid approach that combines a multi objective MDP with a CMDP as max Πn max(0, αv∗ e − di π · re) + cdDiversity(d1 π, . . . , dn π). We denote by I i an indicator function for the event in which the constraint is not satisﬁed for policy πi, i.e., I i = 1 if di e , and 0, otherwise. With this notation the reward is given by π · re < αv∗ ri(s, a) = I ire(s, a) + cdri d(s, a) (11) In other words, the agent maximizes a weighted combination of the extrinsic reward and the diversity reward when the constraint is violated and only the diversity reward when the constraint is satisﬁed. Kumar et al. [31] proposed a similar approach where the agent maximizes a weighted combination of the rewards when the constraint is satisﬁed, and only the extrinsic reward otherwise: ri(s, a) = re(s, a) + cd(1 − I i)ri d(s, a) (12) We refer to Eq. (12) as SMERL, as was coined in [31], and to Eq. (11) as Reverse SMERL. Note that these methods come with an additional hyperparameter cd which balances the two objectives as a multi-objective MDP, in addition to the optimality ratio α. 4 Experiments Our experiments address the following questions: (a) Can we learn diverse policies that are also near optimal? (see Section 4.1) (b) How critical are various components of our algorithm and how does it compare to other multi-objective baselines in terms of QD trade-off? (see Section 4.1) (c) Does our method scale to settings where the feature space is high-dimensional and unstructured? (see Section 4.2) (d) Finally, can the diverse policies discovered by DOMiNO enable robustness and adaptation to novel perturbations in the environment and agent? (see Section 4.3) Environment. We conducted most of our experiments on domains from the DM Control Suite [52], standard continuous control locomotion tasks where diverse near-optimal policies should naturally correspond to different gaits. Due to space considerations, we present Control Suite results on the walker.stand task. In the supplementary, however, we present similar results for walker.walk and BiPedal walker from OpenAI Gym [13] suggesting that the method generalizes across different reward functions and domains. We also include the challenging dog domain with 38 actions and a 223−dimensional observation space, which is one of the more challenging domains in control suite. 6 Agent. Fig. 1a shows an overview of DOMiNO’s components and their interactions, instantiated in an actor-critic agent. While acting, the agent samples a new latent variable z ∈ [1, n] uniformly at random at the start of each episode. We train all the policies simultaneously and provide this latent variable as an input. For the average reward state occupancy, the agent keeps an empirical running average for each latent policy of the rewards ˜vavg πi and features (either from the environment φobs or torso embedding φembedding) ˜ψavg πi encountered, where the average is taken as ˜xi = αd ˜xi−1 + (1 − αd) 1 t=1 xi(st, at) with decay factor αd. Varying αd can make the estimation more online (small T αd, as used for the constraint), or less online (large αd, as needed for Eq. (4)). The agent uses ˜ψavg to compute the diversity reward as described in Eq. (6). ˜vavg is used to optimize πi πi the Lagrange multiplier µ for each policy as in Eq. (9) which is then used to weight the quality and diversity advantages for the policy gradient update. Pseudo code and further implementation details, as well as treatment of the discounted state occupancy, can be found in Appendix A.2. (cid:80)T 4.1 Quality and diversity To measure diversity qualitatively, we present ”motion ﬁgures” by discretizing the videos (details in the Appendix) that give a fair impression of the behaviors. The videos, associated with these ﬁgures, can be found in the supplementary as well. Fig. 1b presents ten polices discovered by DOMiNO with the repulsive objective and the optimality ratio set to 0.9. The policies are ordered from top-left to bottom right, so policy 1, which only maximizes extrinsic reward and sets the constraint, is always at the top left. The policies exhibit different types of standing: standing on both legs, standing on either leg, lifting the other leg forward and backward, spreading the legs and stamping. Not only are the policies different from each other, they also achieve high extrinsic reward in standing (see values on top of each policy visualization). Similar ﬁgures for other domains can be found in Appendix B.1. To further study the QD trade-off, we use scatter plots, showing the episode return on the y-axis, and the diversity score, corresponding to the Hausdorff distance (Eq. (5)), on the x-axis. The top-right corner of the diagram, therefore, represents the most diverse and highest quality policies. Each ﬁgure presents a sweep over one or two hyperparameters and we use the color and a marker to indicate the values. In all of our experiments, we report 95% conﬁdence intervals. In the scatter plots, they correspond to 5 seeds and are indicated by the crosses surrounding each point. Fig. 2 (left) presents the results for DOMiNO with the repulsive reward in the walker.stand domain. We can observe that regardless of the set size, DOMiNO achieves roughly the same extrinsic reward across different optimality ratios (points with the same color obtain the same y-value). This implies that the constraint mechanism is working as expected across different set sizes and optimality ratios. In addition, we can inspect how the QD trade-off is affected when changing the optimality ratio α for sets of the same size (indicated in the ﬁgures with light lines). This observation can be explained by the fact that for lower values of α, the volume of the constrained set is larger, and therefore, it is possible to ﬁnd more diversity within it. This behavior is consistent across different set sizes, though it is naturally more difﬁcult to ﬁnd a set of diverse policies as the set size gets larger (remember that we measure the distance to the closest policy in the set). Figure 2: DOMiNO QD results in walker.stand. Left: Set size vs. optimality ratio (α) with the repulsive reward. Center: Set size vs. α with the Van der Waals (VDW) reward. Right: Target diversity ((cid:96)0) vs. α with the VDW reward. We present the same investigation for the VDW reward in Fig. 2 (center). Similar to the repulsive reward, we can observe that the constraint is satisﬁed, and that reducing the optimality ratio allows for more diversity. Fig. 2 (right) shows how different values of (cid:96)0 affect the QD trade-off for a set of size 10. We can observe that the different combinations of (cid:96)0 and α are organized as a grid in the 7 QD scatter, suggesting that we can control both the level of optimality and the degree of diversity by setting these two interpretable hyperparameters. Fig. 3 compares the QD balance yielded by DOMiNO to the alternative strategies described in Section 3.2. Speciﬁcally, we look at DOMiNO’s Lagrangian method, the linear multi-objective combination of the objectives (Eq. (10)), and the two hybrid strategies, SMERL (Eq. (12)) and Reverse SMERL (Eq. (11)) for a set of ten policies in walker.stand. Note that in all cases we are using DOMiNO’s repulsive diversity objective, and the comparison is strictly about strategies for combining the quality and diversity objectives. The plot for each strategy shows how the solution to the QD tradeoff varies according to the relevant hyperparameters for that strategy, namely, the optimality ratio α for DOMiNO, the ﬁxed constant ce for the multi-objective strategy (we implicitly set cd = 1 − ce), and both α and constant cd for the hybrid approaches (in the hybrid plots, cd value is labeled directly next to the marker, while α is indicated by color). Figure 3: DOMiNO’s Lagrangian method ﬁnds only solutions that push the upper right boundary of quality and diversity, and varies in a smooth, interpretable way with its only hyperparameter, α, contrasted with the jumpy nature of the multi-objective hyperparameter ce, and the redundancy of the hyperparameters in the hybrid methods (SMERL and Reverse SMERL). For the multi-objective approach, shown on the right, the ce parameter proves ill-behaved and choppy, quickly jumping from the extreme of all diversity no quality to the opposite, without a smooth interim. In contrast, the DOMiNO approach of solving the CMDP directly for the Lagrange multiplier yields solutions that push along the upper right diagonal boundary, ﬁnding the highest diversity (farthest right) set of policies for a given optimality ratio (color), varying smoothly along this line as α varies. Another advantage of DOMiNO’s approach is that it only ﬁnds such QD-optimal solutions, where, in contrast, SMERL (left), when appropriately tuned, can also yield some solutions along the upper-right QD border, but often ﬁnds sub-optimal solutions, and therefore must be tuned further with cd to ﬁnd the best QD solutions. We further explore the difﬁculty tuning SMERL in the supplementary (Fig. 10) and ﬁnd that the best cd for 10 policies provides solutions with no diversity for other set sizes. 4.2 Feature analysis The choice of feature space used for optimizing diversity can have a huge impact on the kind of diverse behavior learned. In environments where the observation space is high dimensional and less structured (e.g. pixel observations), computing diversity using the raw features may not lead to meaningful behavior. As speciﬁed in Section 3.1 the feature space used to compute diversity in our Control Suite experiments throughout the paper corresponds to the positions and velocities of the body joints returned as observations by the environment. We show that it is feasible to use a learned embedding space instead. As a proof of principle we use the output of the torso network as a learned embedding described in Section 4 for computing our diversity metric. Table 1 compares the diversity mea- sured in raw observation features (Diversityobs) and embedding features (Diversityembedding) in the walker.stand domain. Columns indicate the feature space that was used to compute the di- versity objective during training averaged across 20 seeds. Inspecting the table, we can see that agents trained to optimize diversity in the learned embedding space and agents that directly optimize diversity in the observation space achieve comparable diversity if measured in either space, indicating that learned embeddings can feasibly be used to achieve meaningful diversity. Diversityobs Diversityembedding φembedding 1.01 ± 0.05 2.35 ± 0.09 φobs 1.21 ± 0.05 2.14 ± 0.09 Table 1 8 Figure 4: K-shot adaptation in Control Suite. We report mean episode return (95% CI) on held-out test tasks relative to the performance of a single policy trained on extrinsic rewards. While not invariant to sudden changes in the environment, DOMiNO is more robust to a variety of perturbations. 4.3 Closing the loop: k-shot adaptation We motivated qualitative diversity by saying that diverse solutions can be robust and allow for rapid adaptation to new tasks and changes in the environment. Here we validate this claim in a k-shot adaptation experiment: we train a set of QD policies on a canonical benchmark task, then test their ability to adapt to environment and agent perturbations. These include four kinematics and dynamics perturbations from the Real World RL suite [18], and a ﬁfth perturbation inspired by a ”motor failure” condition [31] which, every 50 steps and starting at T = 10, disables action-inputs for the ﬁrst two joints for a ﬁxed amount of time3. In Fig. 4, we present the results in the walker.walk and walker.stand domains (rows). Columns correspond to perturbation types and the x-axis corresponds to the magnitude of the perturbation. K-shot adaptation is measured in the following manner. For each perturbed environment, and each method, we ﬁrst execute k = 10 environment trajectories with each policy. Then, for each method we select the policy that performs the best in the set. We then evaluate this policy for 40 more trajectories and measure the average reward of the selected policy rmethod. The y-axis in each ﬁgure measures rmethod/rbaseline, where rbaseline measures the reward in the perturbed environment of an RL baseline agent that was trained with a single policy to maximize the extrinsic reward in the original task. We note that the baseline was trained with the same RL algorithm, but without diversity, and it matches the state-of-the-art in each training domain (it is almost optimal). The raw rewards rmethod, rbaseline can be found in the supplementary (Fig. 12). Lastly, we repeat this process across 20 training seeds, and report the average with a 95% Conﬁdence Interval (CI) 4. We compare the following methods: DOMiNO, SMERL, Multi-Objective, and No diversity, where all the diversity methods use the diversity reward from Eq. (6) and all the methods are with 10 policies. Since we are treating the perturbed environments as hold out tasks, we selected the hyper parameters for each method based on the results in Fig. 3, i.e.we chose the conﬁguration that was the most qualitatively diverse (in the upper-right most corner of Fig. 3). Concretely, for DOMiNO and SMERL α = 0.9, for SMERL cd = 0.5 and for Multi-Objective ce = 0.7. More K-shot adaptation curves with other hyper parameter values can be found in Appendix D. The No diversity method is similar to rbaseline, but uses 10 policies that all maximize the extrinsic reward (instead of a single policy). Inspecting Fig. 4 we can see that for small perturbations, DOMiNO retains the performance of the baseline. However, as the magnitude of the perturbation increases, the performance of DOMiNO is much higher than the baseline (by a factor of 1.5 − 2.0). This observation highlights that a diverse set of policies as found by DOMiNO is much more capable at handling changes to the environment and can serve as a strong starting point for recovering optimal behavior. As we have shown in 3.2, other approaches to managing the trade-off between quality and diversity such as SMERL are much more sensitive to the choice of hyper-parameters and require signiﬁcant tuning. While SMERL is able to ﬁnd a useful, diverse set of policies with some effort, it is difﬁcult to match DOMiNO’s performance across all pertubations and tasks. See the supplementary material for further comparison of DOMiNO with SMERL and Multi-objective over more hyper parameters. We also include a video that illustrates how the individual policies adapt to the environment perturbations. 3While we tried to recreate a similar condition to [31], the tasks are not directly comparable due to signiﬁcant differences in the simulators that have been used as well as the termination conditions in the base task. 4We use boosted CI with nested sampling as implemented in the bootstrap function here, which reﬂects the amount of training seeds and the amount of evaluation seeds per training seed 9 s n r u t e r e v i t a e R l s n r u t e r e v i t a e R l d n a t S 1.50 1.25 1.00 1.35 l k a W 1.20 1.05 1.80 1.50 1.20 0.90 1.80 1.50 1.20 0.90 0 0 1 0 2 0 3 0 4 5 2 0 . 2 Motor failure (duration) 5 7 0 . 3 0 . 3 0 . 2 Thigh length 2.00 1.60 1.20 0.80 1.75 1.50 1.25 1.00 0 . 4 0 . 3 1.40 1.20 1.00 2.40 2.00 1.60 1.20 1.20 1.05 0.90 1.35 1.20 1.05 0.90 0 . 5 0 . 1 1 . 0 2 . 0 3 . 0 4 . 0 0 . 7 Joint damping 5 5 0 . 4 0 . 4 0 . 3 Torso length DOMiNO SMERL No diversity Multi-Objective 1 0 0 . 0 2 0 . 1 0 . 0 Contact friction 0 . 0 0 5 5 Conclusions In this work we proposed DOMiNO, an algorithm for discovering diverse behaviors that maintain optimality. We framed the problem as a CMDP in the state occupancies of the policies in the set and developed an end-to-end differentiable solution to it based on reward maximization. In our experiments we demonstrated that the policies discovered by DOMiNO, or, DOMiNO’s π, are diverse and maintain optimality. We then explored how DOMiNO balances the QD trade-off and compared it with multi-objective baselines. Our results suggest that DOMiNO can control the degree of quality and diversity via two interpretable hyperparameters, while other baselines struggle to capture both. In our K-shot experiments we demonstrated that DOMiNO’s π can adapt to changes in the environ- ment. An exciting direction for future work is to use DOMiNO in a never ending RL setting, where the environment changes smoothly over time, and see if maintaing a set of QD diverse policies will make it more resilient to such changes. 10 References [1] Abbeel, P. and Ng, A. Y. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-ﬁrst international conference on Machine learning, pp. 1. ACM, 2004. [2] Abernethy, J. D. and Wang, J.-K. On frank-wolfe and equilibrium computation. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Cur- ran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 7371364b3d72ac9a3ed8638e6f0be2c9-Paper.pdf. [3] Agadmator. Invisible to engines — one of the greatest moves ever played. agadmator’s Chess Channel, 2018. URL https://www.youtube.com/watch?v=yGnpewUKP88&t=1s. [4] Altman, E. Constrained Markov decision processes, volume 7. CRC Press, 1999. [5] Barreto, A., Dabney, W., Munos, R., Hunt, J. J., Schaul, T., van Hasselt, H. P., and Silver, D. Successor features for transfer in reinforcement learning. In Advances in neural information processing systems, pp. 4055–4065, 2017. [6] Barreto, A., Hou, S., Borsa, D., Silver, D., and Precup, D. Fast reinforcement learning with generalized policy updates. Proceedings of the National Academy of Sciences, 2020. [7] Baumli, K., Warde-Farley, D., Hansen, S., and Mnih, V. Relative variational intrinsic control. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 35:6732–6740, May 2021. URL https://ojs.aaai.org/index.php/AAAI/article/view/16832. [8] Behovits, R. Game 8: Leko wins to take the lead. Chess news, 2004. URL https://en. chessbase.com/post/game-8-leko-wins-to-take-the-lead. [9] Belogolovsky, S., Korsunsky, P., Mannor, S., Tessler, C., and Zahavy, T. Inverse reinforcement learning in contextual mdps. Machine Learning, pp. 1–40, 2021. [10] Bhatnagar, S. and Lakshmanan, K. An online actor–critic algorithm with function approximation for constrained markov decision processes. Journal of Optimization Theory and Applications, 153(3):688–708, 2012. [11] Borkar, V. S. An actor-critic algorithm for constrained markov decision processes. Systems & control letters, 54(3):207–213, 2005. [12] Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., and Wanderman- Milne, S. JAX: composable transformations of Python+NumPy programs, 2018. URL http: //github.com/google/jax. [13] Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. Openai gym. arXiv preprint arXiv:1606.01540, 2016. [14] Calian, D. A., Mankowitz, D. J., Zahavy, T., Xu, Z., Oh, J., Levine, N., and Mann, T. Balancing constraints and rewards with meta-gradient d4pg. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=TQt98Ya7UMP. [15] Cully, A. Autonomous skill discovery with quality-diversity and unsupervised descriptors. In Proceedings of the Genetic and Evolutionary Computation Conference, pp. 81–89, 2019. [16] Cully, A., Clune, J., Tarapore, D., and Mouret, J.-B. Robots that can adapt like animals. Nature, 521(7553):503–507, 2015. [17] da Fonseca-Wollheim, C. Swapping songs with chess grandmaster garry kasparov. The New York Times, 2020. URL https://www.nytimes.com/2020/12/18/arts/music/ garry-kasparov-classical-music.html. [18] Dulac-Arnold, G., Mankowitz, D., and Hester, T. Challenges of real-world reinforcement learning. arXiv preprint arXiv:1904.12901, 2019. 11 [19] Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron, Y., Firoiu, V., Harley, T., Dunning, I., et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International Conference on Machine Learning, pp. 1407–1416. PMLR, 2018. [20] Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S. Diversity is all you need: Learning skills without a reward function. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=SJx63jRqFm. [21] Eysenbach, B., Salakhutdinov, R., and Levine, S. The information geometry of unsupervised reinforcement learning. arXiv preprint arXiv:2110.02719, 2021. [22] Gangwani, T., Peng, J., and Zhou, Y. Harnessing distribution ratio estimators for learning agents with quality and diversity. arXiv preprint arXiv:2011.02614, 2020. [23] Geist, M., P´erolat, J., Lauri`ere, M., Elie, R., Perrin, S., Bachem, O., Munos, R., and Pietquin, O. Concave utility reinforcement learning: the mean-ﬁeld game viewpoint. arXiv preprint arXiv:2106.03787, 2021. [24] Gregor, K., Rezende, D. J., and Wierstra, D. Variational intrinsic control. International Conference on Learning Representations, Workshop Track, 2017. URL https://openreview. net/forum?id=Skc-Fo4Yg. [25] Ha, D. Reinforcement learning for improving agent design. arXiv preprint arXiv:1810.03779, 2018. [26] Hazan, E., Kakade, S., Singh, K., and Van Soest, A. Provably efﬁcient maximum entropy exploration. In International Conference on Machine Learning, pp. 2681–2691. PMLR, 2019. [27] Hessel, M., Kroiss, M., Clark, A., Kemaev, I., Quan, J., Keck, T., Viola, F., and van Hasselt, H. Podracer architectures for scalable reinforcement learning. arXiv preprint arXiv:2104.06272, 2021. [28] Ho, J. and Ermon, S. Generative adversarial imitation learning. arXiv preprint arXiv:1606.03476, 2016. [29] Hong, Z.-W., Shann, T.-Y., Su, S.-Y., Chang, Y.-H., Fu, T.-J., and Lee, C.-Y. Diversity-driven exploration strategy for deep reinforcement learning. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp. 10510–10521, 2018. [30] Jouppi, N. P., Young, C., Patil, N., Patterson, D. A., Agrawal, G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers, A., Boyle, R., Cantin, P., Chao, C., Clark, C., Coriell, J., Daley, M., Dau, M., Dean, J., Gelb, B., Ghaemmaghami, T. V., Gottipati, R., Gulland, W., Hagmann, R., Ho, R. C., Hogberg, D., Hu, J., Hundt, R., Hurt, D., Ibarz, J., Jaffey, A., Jaworski, A., Kaplan, A., Khaitan, H., Koch, A., Kumar, N., Lacy, S., Laudon, J., Law, J., Le, D., Leary, C., Liu, Z., Lucke, K., Lundin, A., MacKean, G., Maggiore, A., Mahony, M., Miller, K., Nagarajan, R., Narayanaswami, R., Ni, R., Nix, K., Norrie, T., Omernick, M., Penukonda, N., Phelps, A., Ross, J., Salek, A., Samadiani, E., Severn, C., Sizikov, G., Snelham, M., Souter, J., Steinberg, D., Swing, A., Tan, M., Thorson, G., Tian, B., Toma, H., Tuttle, E., Vasudevan, V., Walter, R., Wang, W., Wilcox, E., and Yoon, D. H. In-datacenter performance analysis of a tensor processing unit. CoRR, abs/1704.04760, 2017. URL http://arxiv.org/abs/1704.04760. [31] Kumar, S., Kumar, A., Levine, S., and Finn, C. One solution is not all you need: Few-shot extrapolation via structured maxent rl. Advances in Neural Information Processing Systems, 33, 2020. [32] Lehman, J. and Stanley, K. O. Evolving a diversity of virtual creatures through novelty search and local competition. In Proceedings of the 13th annual conference on Genetic and evolutionary computation, pp. 211–218, 2011. [33] Liu, Y., Ramachandran, P., Liu, Q., and Peng, J. Stein variational policy gradient. In 33rd Conference on Uncertainty in Artiﬁcial Intelligence, UAI 2017, 2017. 12 [34] Masood, M. A. and Doshi-Velez, F. Diversity-inducing policy gradient: Using maximum mean discrepancy to ﬁnd a set of diverse policies. arXiv preprint arXiv:1906.00088, 2019. [35] Mehta, N., Natarajan, S., Tadepalli, P., and Fern, A. Transfer in variable-reward hierarchical reinforcement learning. Machine Learning, 73(3):289, 2008. [36] Mouret, J.-B. and Clune, J. Illuminating search spaces by mapping elites. arXiv preprint arXiv:1504.04909, 2015. [37] Mutti, M., De Santi, R., De Bartolomeis, P., and Restelli, M. Challenging common assumptions in convex reinforcement learning. arXiv preprint arXiv:2202.01511, 2022. [38] Osborn, A. F. Applied imagination. Scribner’s, 1953. [39] Parker-Holder, J., Pacchiano, A., Choromanski, K. M., and Roberts, S. J. Effective diversity in population based reinforcement learning. Advances in Neural Information Processing Systems, 33, 2020. [40] Peng, Z., Sun, H., and Zhou, B. Non-local policy optimization via diversity-regularized collaborative exploration. arXiv preprint arXiv:2006.07781, 2020. [41] Pugh, J. K., Soros, L. B., and Stanley, K. O. Quality diversity: A new frontier for evolutionary computation. Frontiers in Robotics and AI, 3:40, 2016. [42] Puterman, M. L. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 1984. [43] Rockafellar, R. T. Convex analysis. Princeton university press, 1970. [44] Schmitt, S., Hessel, M., and Simonyan, K. Off-policy actor-critic with shared experience replay. In International Conference on Machine Learning, pp. 8545–8554. PMLR, 2020. [45] Shani, L., Zahavy, T., and Mannor, S. Online apprenticeship learning. arXiv preprint arXiv:2102.06924, 2021. [46] Sharma, A., Gu, S., Levine, S., Kumar, V., and Hausman, K. Dynamics-aware unsupervised discovery of skills. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HJgLZR4KvH. [47] Singh, A. K. Chapter 2 - structure, synthesis, and application of nanoparticles. In Singh, A. K. (ed.), Engineered Nanoparticles, pp. 19–76. Academic Press, Boston, 2016. ISBN 978-0-12-801406-6. doi: https://doi.org/10.1016/B978-0-12-801406-6.00002-9. URL https: //www.sciencedirect.com/science/article/pii/B9780128014066000029. [48] Stooke, A., Achiam, J., and Abbeel, P. Responsive safety in reinforcement learning by pid lagrangian methods. In International Conference on Machine Learning, pp. 9133–9143. PMLR, 2020. [49] Sun, H., Peng, Z., Dai, B., Guo, J., Lin, D., and Zhou, B. Novel policy seeking with constrained optimization. arXiv preprint arXiv:2005.10696, 2020. [50] Szepesv´ari, C. Constrained mdps and the reward hypothesis. Musings about machine learn- ing and other things (blog), 2020. URL https://readingsml.blogspot.com/2020/03/ constrained-mdps-and-reward-hypothesis.html. [51] Tarapore, D., Clune, J., Cully, A., and Mouret, J.-B. How do different encodings inﬂuence the performance of the map-elites algorithm? In Proceedings of the Genetic and Evolutionary Computation Conference 2016, pp. 173–180, 2016. [52] Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. d. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, A., et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018. [53] Tessler, C., Mankowitz, D. J., and Mannor, S. Reward constrained policy optimization. In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=SkfrvsA9FX. 13 [54] Vassiliades, V., Chatzilygeroudis, K., and Mouret, J.-B. Scaling up map-elites using centroidal voronoi tessellations. arXiv preprint arXiv:1610.05729, 2016. [55] Zahavy, T., Cohen, A., Kaplan, H., and Mansour, Y. Apprenticeship learning via frank-wolfe. AAAI, 2020, 2020. [56] Zahavy, T., Cohen, A., Kaplan, H., and Mansour, Y. Average reward reinforcement learning with unknown mixing times. The Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2020. [57] Zahavy, T., Barreto, A., Mankowitz, D. J., Hou, S., O’Donoghue, B., Kemaev, I., and Singh, S. Discovering a set of policies for the worst case reward. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=PUkhWz65dy5. [58] Zahavy, T., O’Donoghue, B., Desjardins, G., and Singh, S. Reward is enough for convex MDPs. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id= ELndVeVA-TR. [59] Zhang, J., Koppel, A., Bedi, A. S., Szepesvari, C., and Wang, M. Variational policy gradient method for reinforcement learning with general utilities. arXiv preprint arXiv:2007.02151, 2020. [60] Zhang, Y., Yu, W., and Turk, G. Learning novel policies for tasks. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learn- ing, volume 97 of Proceedings of Machine Learning Research, pp. 7483–7492. PMLR, 09–15 Jun 2019. URL http://proceedings.mlr.press/v97/zhang19q.html. 14 Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See the discussion in Appendix C . (c) Did you discuss any potential negative societal impacts of your work? [No] We could identify any potential negative societal impacts for this work (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experi- mental results (either in the supplemental material or as a URL)? [No] While the DM control domain is open sourced, our code is proprietary and we are not able to share it. That said, we shared the source code for the diversity reward function in Appendix A.3 and provided pseudo code and hyper parameter details in Appendix A.2. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See above. (c) Did you report error bars (e.g., with respect to the random seed after running experi- ments multiple times)? [Yes] (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix E 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [N/A] (b) Did you mention the license of the assets? [N/A] (c) Did you include any new assets either in the supplemental material or as a URL? [N/A] (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identiﬁable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] 15 A Additional Experiment Details A.1 Environment We evaluate our method on a number of tasks. These tasks have different complexity in terms of control, dynamics and reward structure. • walker.walk (Control Suite): a 8-dimensional control task with a 23-dimensional observa- tion space, where the goal is to control the joints of a humanoid character to make it walk forward in a 2D plane. • walker.stand (Control Suite): a 8-dimensional control task with a 23-dimensional observa- tion space, where the goal is to control the joints of a humanoid character to make it stand up. • dog.walk (Control Suite): a 38-dimensional control task with a 223-dimensional observation space, where the goal is to control the joints of a dog character to make it walk forward in a 2D plane. • dog.stand (Control Suite): a 38-dimensional control task with a 223-dimensional obser- vation space, where the goal is to control the joints of a dog character to make it stand up. • BiPedal Walker: a 4-dimensional control task with a 24-dimensional observation space, where the goal is to control the joints of a bipedal-walker with a large payload in order to traverse a terrain from left to right as fast as possible. 16 A.2 Implementation details Distributed agent Acting and learning are decoupled, with multiple actors gathering data in parallel from a batched stream of environments, and storing their trajectories, including the latent variable z, in a replay buffer and queue from which the learner can sample a mixed batch of online and replay trajectories [44, 27]. The latent variable z is sampled uniformly at random in [1, n] during acting at the beginning of each new episode. The learner differentiates the loss function as described in Algorithm 1, and uses the optimizer (speciﬁed in Table 2) to update the network parameters and the Lagrange multipliers (speciﬁed in Table 3). Lastly, the learner also updates the and moving averages as described in Algorithm 1. Initialization When training begins we initialize the network parameters as well as the Lagrange multipliers: µi = σ−1(0.5), ∀i ∈ [2, n], where σ−1 is the inverse of the Sigmoid function µ1 = 1; and the moving averages: ˜vavg πi = ¯1/d, ∀i ∈ [1, n]. Here n is the number of policies and d is the dimension of the features φ. πi = 0., ∀i ∈ [1, n], ˜ψavg Bounded Lagrange multiplier To ensure the Lagrange multiplier does not get too large so as to increase the magnitude of the extrinsic reward and destabilize learning, we use a bounded Lagrange multiplier [48] by applying Sigmoid activation on µ so the effective reward is a convex combination of the diversity and the extrinsic rewards: r(s, a) = σ(µi)re(s, a) + (1 − σ(µi))ri d(s, a), and the objective for µ is σ(µ)(αv∗ e − dπ · re). Average state occupancy The empirical feature averages used for experiments in the main text are good, though imperfect due to the bias from samples before the policy mixes. In our experiments, however, since the mixing time for the DM Control Suite is much shorter than the episode length T , the bias is small (∼ 5%). Discounted state occupancy For a more scalable solution, as mentioned in Section 2, we can instead predict successor features using an additional network head as shown in Fig. 11a. Similar to value learning, we use V-trace [19] targets for training successor features. In discounted state occupancy case we also use the extrinsic value function of each policy vi e (Fig. 1a) to estimate dπ · re, instead of the running average ˜vavg πi . We show experimental results for this setup in Fig. 11b. Loss functions. Instead of learning a single value head for the combined reward, our network has two value heads, one for diversity reward and one for extrinsic reward. We use V-trace [19] to compute td-errors and advantages for each of the value heads using the ”vtrace td error and advantage” function implemented here https://github.com/deepmind/ rlax/blob/master/rlax/_src/vtrace.py. The value loss for each head is the squared (cid:96)2 loss d + td2 of the td-errors, and the combined value loss for the network is the sum of these two losses: td2 e. In addition to that, our network has a policy head that is trained with a policy gradient loss as implemented in https://github.com/deepmind/rlax/blob/master/rlax/_src/policy_ gradients.py). When training the policy, we combine the intrinsic and extrinsic advantages δ = σ(µi)δe + (1 − σ(µi))δd (see the Weight cumulants function in Appendix A.3) which has the same effect as combining the reward. However, we found that having two value heads is more stable as each value can have a different scale. The ﬁnal loss of the agent is a weighted sum of the value loss the policy loss and the entropy regularization loss, and the weights can be found in Table 2. Algorithm 1 also returns a Lagrange loss function, designed to force the policies to achieve a value that is at least α times the value of the ﬁrst policy (which only maximizes extrinsic reward), where α is the optimally ratio (Table 3). We update the Lagrange multipliers µ using the optimizer speciﬁed in Table 3 but keep the multiplier of the ﬁrst policy ﬁxed µ1 = 1. Lastly, Algorithm 1 also updates the moving averages. 17 Algorithm 1: Loss function . πi i=1 (cid:111)n s|xj (cid:9)n i=1, (cid:110) ˜ψavg s) is the probability assigned to aj s), vd(xj j=1 of size T, τj = (cid:8)zj, xj s in state xi s)} ← Network({τj}m s), δe(xj Parameters: Network parameters θ, Lagrange multipliers µ, moving averages (cid:8)˜vavg πi Data: m trajectories {τj}m µ(aj Forward pass: {π(aj s), ve(xj Compute extrinsic td-errors and advantages: tde(xj extrinsic reward rj Compute intrinsic reward: ri Compute intrinsic td-errors and advantages: tdd(xj intrinsic reward rj Combine advantages: δ(xj Weighted loss: s and intrinsic critic vd(xj s) s) + (1 − σ(µi))δd(xj s) = σ(µi)δe(xj s) s and extrinsic critic ve(xj s) s) from ˜ψavg πz , z, φj d(xj s with Eq. (6) or (8) j=1) s) ← V-trace with s) ← V-trace with s), δd(xj s, µ(aj s)(cid:9)T s, φj s, rj s|xj s|xj s, aj s=1 , where s by the behaviour policy µ(a|x). bv(tde(xj s)2 + tdd(xj s)2) + bπ log(π(aj s|xj s))δ(xj s) + bEntEntropy(π(aj s|xj s)) (cid:88) s,j Lagrange loss: Update moving averages: n (cid:88) i=1 σ(µi)(˜vavg πi − α˜vavg π1 ) πi = α˜vavg ˜vavg d πi + (1 − α˜vavg ˜vavg d )rt, ˜ψavg ˜ψavg πi = α d ˜ψavg πi + (1 − α ˜ψavg d )φt return Weighted loss, Lagrange loss, (cid:8)˜vavg πi (cid:9)n i=1, (cid:110) ˜ψavg πi (cid:111)n i=1 A.3 Functions 18 1 def intrinsic_reward ( phi , sfs , latents , attractive_power =3. , repulsive_power =0. , attractive_coeff =0. , target_d =1.) : """""" Computes a diversity reward using successor features . Args : phi : features [ tbf ]. sfs : avg successor features [ lf ] or predicted , discounted successor features [ tbfl ]. latents : [ tbl ]. attractive_power : the power of the attractive force . repulsive_power : the power of the repulsive force . attractive_coeff : convex mixing of attractive & repulsive forces target_d (\ ell_0 ) : desired target distance between the sfs . When attractive_coeff =0.5 , target_d is the minimizer of the objective , i . e . , the gradient ( the reward ) is zero . Returns : intrinsic_reward . """""" # If sfs are predicted we have 2 extra leading dims . if jnp . ndim ( sfs ) == 4: sfs = jnp . swapaxes ( sfs , 2 , 3) of avg sf ) compute_dist_fn = jax . vmap ( jax . vmap ( compute_distances ) ) matmul_fn = lambda x , y : jnp . einsum ( ’tbl , tblf - > tbf ’ , x , y ) # tbfl -> tblf ( to match lf shape elif jnp . ndim ( sfs ) == 2: compute_dist_fn = compute_dist ances matmul_fn = jnp . matmul else : raise ValueError ( ’ Invalid shape for argument ‘sfs ‘. ’) l , f = sfs . shape [ -2:] # Computes an tb lxl matrix where each row , corresponding to a latent , is a 1 hot vector indicating the index of the latent with the closest sfs dists = compute_dist_fn ( sfs , sfs ) dists += jnp . eye ( l ) * jnp . max ( dists ) ne a r e s t_ l a t en ts_ ma tr ix = jax . nn . one_hot ( jnp . argmin ( dists , axis = -2) , num_classes = l ) # Computes a [ tbl ] vector with the nearest latent to each latent in latents nearest_latents = matmul_fn ( latents , ne ar es t_l at en ts_ ma tr ix ) # Compute psi_i - psi_j psi_diff = matmul_fn ( latents - nearest_latents , sfs ) norm_diff = jnp . sqrt ( jnp . sum ( jnp . square ( psi_diff ) , axis = -1) ) / # tbf target_d c = (1. - attractive_coeff ) * norm_diff ** repulsive_power c -= attractive_coeff * norm_diff ** attractive_power reward = c * jnp . sum ( phi * psi_diff , axis = -1) / f return reward 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def l2dist (x , y ) : 45 """""" Returns the L2 distance between a pair of inputs . """""" return jnp . sqrt ( jnp . sum ( jnp . square ( x - y ) ) ) 46 47 48 def c ompute_distances (x , y , dist_fn = l2dist ) : 49 """""" Returns the distance between each pair of the two collections of inputs . """""" 50 return jax . vmap ( jax . vmap ( dist_fn , ( None , 0) ) , (0 , None ) ) (x , y ) Listing 1: Intrinsic Reward 19 1 def weight_cumulants ( lagrange , latents , extrinsic_cumulants , 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 i ntr i nsi c_cum ula n ts ) : """""" Weights cumulants using the Lagrange multiplier . Args : lagrange : lagrange [ l ]. latents : latents [ tbl ]. e xtr in si c_c u mul ants : [ tb ]. i ntr in si c_c u mul ants : [ tb ]. Returns : extrinsic reward r_e and intrinsic_reward r_d . """""" sig_lagrange = jax . nn . sigmoid ( lagrange ) l ate n t_s ig_la gra n ge = jnp . matmul ( latents , sig_lagrange ) # No diversity rewards for latent 0 , only maximize extrinsic reward i ntr i nsi c_cum ula n ts *= (1 - latents [: , : , 0]) return (1 - la tent_s ig_ lagrang e ) * intrins ic_cumulants + l ate n t_s ig_la gra n ge * extr insic_c um ulants Listing 2: Weight cumulants # l # tb 1 def lagrangian ( lagrange , r , optimality_ratio ) : 2 """""" Loss function for the Lagrange multiplier . 3 4 5 6 7 8 9 Args : lagrange : lagrange [ l ]. r : moving averages of reward [ l ]. optimality_ratio : [1]. """""" l_ = jax . nn . sigmoid ( lagrange ) return jnp . sum ( l_ * ( r - r [0] * optimality_ratio ) ) Listing 3: lagrange loss function A.4 Motion ﬁgures Our ”motion ﬁgures” were created in the following manner. Given a trajectory of frames that composes a video f1, . . . , fT , we ﬁrst trim and sub sample the trajectory into a point of interest in time: fn, . . . , fn+m. We always use the same trimming across the same set of policies (the sub ﬁgures in a ﬁgure). We then sub sample frames from the trimmed sequence at frequency 1/p: fn, fn+p, fn+2p . . . ,. After that, we take the maximum over the sequence and present this ”max” image. In Python for example, this simply corresponds to n=400, m=30, p=3 indices = range(n, n+m, p) im = np.max(f[indices]) This creates the effect of motion in single ﬁgure since the object has higher values than the background. A.5 Hyperparameters The hyperparameters in Table 2 are shared across all environments except in the BiPedal Domain the learning rate is set to 10−5 and the learner frames are 5 × 107. We report the DOMiNO speciﬁc hyperparameters in Table 3. 20 Hyperparameter Replay capacity Learning rate Learner frames Discount factor bEnt Entropy regularization weight bπ Policy loss weight bv Value loss weight Replay batch size Online batch size Sequence length Optimizer Value 5 × 105 10−4 2 × 107 0.99 0.01 1.0 1.0 600 6 40 RMSprop Table 2: General hyperparameters Hyperparameter Control Suite BiPedal Walker α Optimality ratio Lagrange initialization Lagrange learning rate Lagrange optimizer πi decay factor α˜vavg ˜vavg d ˜ψavg ˜ψavg πi decay factor α d 0.9 0.5 10−3 Adam 0.9 0.99 0.7 0.5 10−3 Adam 0.999 0.9999 Table 3: DOMiNO hyperparameters B Additional Experiment Results B.1 Motion ﬁgures We now present motion ﬁgures, similar to Fig. 1b, but in other domains (see Fig. 6-9). The videos, associated with these ﬁgures can be found in a separate .zip ﬁle. Each ﬁgure presents ten polices discovered by DOMiNO and their associated rewards (in white text) with the repulsive objective and the optimality ratio set to 0.9. As we can see, the policies exhibit different gaits. Next to each ﬁgure, we also present the distances between the expected features of the discovered policies measured by the (cid:96)2 norm. In addition, in each row i we use a dark black frame to indicate the the index of the policy with the closest expected features to πi , i.e., in the i-th row we highlight the j-th column such that j = j∗ i = arg minj(cid:54)=i ||ψi − ψj||2 2. 21 Figure 5: QD in walker.stand. Left: 10 policies and corresponding rewards. Right: distances in (cid:96)2 norm between the Successor features of the policies. Figure 6: QD in walker.walk. Left: 10 policies and corresponding rewards. Right: distances in (cid:96)2 norm between the Successor features of the policies. Figure 7: QD in dog.stand. Left: 10 policies and corresponding rewards. Right: distances in (cid:96)2 norm between the Successor features of the policies. Figure 8: QD in dog.walk. Left: 10 policies and corresponding rewards. Right: distances in (cid:96)2 norm between the Successor features of the policies. 22 B.2 Additional Quality Diversity Results We now present additional experimental results evaluating the trade-off between quality and diversity using the scatter plots introduced in Section 4.1. y-axis shows the episode return while the diversity score, corresponding to the Hausdorff distance (Eq. (5)), is on the x-axis. The top-right corner of the diagram represents the most diverse and highest quality policies. Each ﬁgure presents a sweep over one or two hyperparameters and we use the color and a marker to indicate the values. In all of our scatter plots, we report 95% conﬁdence intervals, corresponding to 5 seeds, which are indicated by the crosses surrounding each point. Quality Diversity: walker.walk In Fig. 9 we show experimental results for DOMiNO in the walker.walk domain. Consistent with Fig. 2 which shows similar results on walker.stand, we show that our constraint mechanism is working as expected across different set sizes and optimality ratios across different tasks. Figure 9: QD Scaling results on walker.walk task. Left: Number of policies vs. optimality ratio in walker.walk with the repulsive reward and (center) with the VDW reward. Right: Optimality ratio vs. VDW target distance (cid:96)0. Quality Diversity: SMERL vs DOMiNO In Fig. 10 we show further experimental results in walker.stand for SMERL in comparison to DOMiNO. When SMERL is appropriately tuned (here for the 10 policies conﬁguration), it can ﬁnd some solutions along the upper-right QD border; however we ﬁnd that the best cd does not transfer to other conﬁgurations. The choice of cd that enables the agent to ﬁnd a set of 10 diverse policies produces sets without diversity for any other set size. Figure 10: Scaling SMERL (left) vs. DOMiNO (right) on Walker.Stand. Set size is indicated with marker, color corresponds to optimality ratio α. The cd for SMERL is set to 0.5, which was tuned using a set size of 10 policies (see 3, left). This choice does not scale well to any other set size, where regardless of optimality ratios, all policies only optimize for extrinsic reward, at the expense of diversity. Discounted State Occupancy We run the same experiments reported in Fig. 2 with DOMiNO’s Lagrangian method and report the results in Fig. 11b. As can be observed, using predicted discounted features does not make any signiﬁcant difference in performance. Since the mixing time for the DM Control Suite is much shorter than the episode length T , the bias in the empirical feature averages is small. 23 (a) (b) Figure 11: (a) DOMiNO with a discounted state occupancy. An additional network head is trained to predict successor features ψγ, which are used instead of the average features ψavg to compute the diversity reward. The discounted, extrinsic value is used as a constraint instead of the averaged rewards. Dashed lines signify training objectives. (b) Number of policies vs. optimality ratio in walker.stand with DOMiNO, consistent with Fig. 2. C Limitations Diversity increasing by decreasing α Inspecting Fig. 9, Fig. 11b and Fig. 2. we can observe that the diversity score increases for lower optimality ratios. Recall that the optimality ratio α speciﬁes a feasibility region in the state-occupancy space (the set of all α-optimal policies). Thus, the size of this space increases as α decreases, and we observe more diverse sets for smaller values of α. This intuition was correct in most of our experiments, but not always (e.g., Fig. 9). One possible explanation is that the Lagrange multipliers solution is seeking for the lowest value of λ that satisﬁes the constraint (so that we can get more diversity), i.e., it ﬁnds solutions that satisfy the constraint almost with equality: vi e ). The size of the level sets e ≥ αv∗ e ) do not necessarily increase with lower values of α (while the feasibility sets vi (vi e do). Another explanation is that in walker.walk (Fig. 9) it might be easier to ﬁnd diverse walking (e.g., α = 0.9) than diverse “half walking” (e.g., α = 0.5). This might be explained by “half walking” being less stable (it is harder to ﬁnd diverse modes for it). e (instead of vi e > αv∗ e ∼ αv∗ e = αv∗ Features Another possible limitation of our approach is that diversity is deﬁned via the environment features. We partially addressed this concern in Section 4.2 where we showed that it is possible to learn QD policies with our approach using the embedding of a NN as features. In future work we plan to scale our approach to higher dimensional domains and study which auxiliary losses should be added to learn good representations for diversity. D Additional K-shot experiments D.1 Control suite Next, we report additional results for K-shot adaptation in the control suite. In Fig. 12 we report the absolute values achieved by each method obtained (in the exact same setup as in Fig. 4). That is, we report rmethod for each method (instead of rmethod/rbaseline as in Fig. 4). Additionally, we report rbaseline, which is the ”Single policy baseline” (blue) in Fig. 12. Inspecting Fig. 12, we can see that all the methods deteriorate in performance as the magnitude of the perturbation increases. However, the performance of DOMiNO (orange) deteriorates slower than that of the other methods. We can also see that the performance of the no diversity baseline is similar when it learns 10 policies (red) and a 24 U p d a t e c o n s t r a i n t & L a g r a n g e MLP Torso MLP D i v e r s i t y R e w a r d single policy (blue), which indicates that when the algorithm maximize only the extrinsic reward, it ﬁnds the same policy again and again with each of the 10 policies. Figure 12: K-shot adaptation in Control Suite, similar to Figure 4, but reporting absolute rather than relative returns. Next, we inspect a wider range of hyper parameters for the SMERL and Multi-Objective methods. Concretely, for DOMiNO and SMERL α = 0.9, for SMERL cd ∈ [0.5, 1, 2, 5] and for Multi- Objective ce ∈ [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.9] and all methods are trained with 10 policies. These values correspond to the values that we considered in Fig. 3. Inspecting Fig. 13 we can see that the best methods overall are DOMiNO and SMERL (with cd = 1). We can also see that DOMiNO and SMERL consistently outperform the multi-objective baseline for many hyper parameter values. This is consistent with our results in Fig. 3 which suggest that the Multi-Objective method tends to be either diverse or high performing and fails to capture a good balance in between. Lastly, it is reasonable that SMERL and DOMiNO perform similar since they are both valid solutions to the same CMDP. However, SMERL comes an with additional hyper parameter cd that may be tricky to tune in some situations. For example, trying to tune cd based on the results in the vanilla domain (picking the upper-right most point in Fig. 3) led us to choose cd = 0.5 for SMERL, instead of 1. The Lagrange multipliers formulation in DOMiNO does not have this challenge as it does not have an extra hyper parameter. Figure 13: K-shot in Control Suite, similar to Figure 4, but reporting a wider range of hyper parameters for SMERL and Multi-Objective. D.2 BipedalWalker 2, h1 For the BipedalWalker environment, we either perturb the morphology or the terrain. To perturb the morphology, we follow [25] and specify a set of re-scaling factors. Speciﬁcally, each leg is made up of two rectangles, with pre-deﬁned width and height parameters: leg1 = ((w1 1)), leg2 = ((w1 2)). To generate a perturbed morphology, we deﬁne a scaling range [0, η] withing which we uniformly sample scaling factors (cid:96)j i ∼ [−η, η], for i = 1, 2 and j = 1, 2. A perturbed environment is deﬁned by re-scaling the default parameters: (cid:102)leg1 = (((1 + (cid:96)1 1, (1 + ν1 2 )h2 1 )h2 1 )h1 1))). The values for this perturbations can be found in Table 4. 1))), and (cid:102)leg2 = (((1+(cid:96)1 1)w1 1, (1+ν2 1), ((1+(cid:96)2 1), ((1+(cid:96)2 1, (1+ν2 1, (1+ν1 1), (w2 2), (w2 1)w2 2)w2 2)w1 i , νj 1, h2 1, h1 2, h2 2 )h1 25 s n r u t e r n a e M s n r u t e r n a e M 1000 d n a t S 400 1000 l k a W 200 200 200 0 0 1 0 2 0 3 200 0 4 0 . 2 5 2 Motor failure (duration) 200 0 . 4 0 . 3 5 7 0 . 3 0 . 3 0 . 2 Thigh length 600 400 600 200 5 0 . 4 0 . 4 0 . 3 Torso length 5 0 . 5 0 . 1 3 . 0 2 . 0 1 . 0 Joint damping 4 . 0 0 . 7 2 0 . 1 0 . 0 Contact friction 0 . 0 0 Single policy baseline DOMiNO SMERL No diversity Multi-Objective 5 1 0 0 . 0 1000 s n r u t e r n a e M d n a t S 200 1000 s n r u t e r n a e M l k a W 200 200 200 400 600 200 200 200 200 0 0 1 0 2 0 3 0 4 0 . 2 5 2 7 0 . 3 0 . 2 5 0 . 3 0 . 4 0 . 3 5 0 . 3 0 . 4 5 0 . 4 0 . 5 0 . 1 1 . 0 2 . 0 3 . 0 4 . 0 0 . 7 0 . 1 2 0 . 0 5 0 0 . 0 1 0 0 . 0 Motor failure (duration) Thigh length Torso length Joint damping Contact friction Single policy baseline DOMiNO Multi Objective (0.1) Multi Objective (0.2) Multi Objective (0.3) Multi Objective (0.4) Multi Objective (0.5) Multi Objective (0.6) Multi Objective (0.9) Multi Objective (0.7) SMERL ( =1.0) SMERL ( =2.0) SMERL ( =5.0) SMERL ( =0.5) Perturbation type Perturbation scale parameter values (η) Morphology Stumps (height, width) Pits (width) Stairs (height, width) 0., 0.10, 0.15, 0.20, 0.25, 0.30, 0.35 0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. 0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. 0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. Table 4: Bipedal perturbation scale values For terrain changes, we selectively enable one of three available obstacles available in the OpenAI Gym implementation: stumps, pits, or stairs. For each obstacle, we specify a perturbation interval [0, η]. This interval determines the upper bounds on the obstacles height and width when the environment generates terrain for an episode. For details see the “Hardcore” implementation of the BiPedal environment. Note that for stairs, we ﬁxed the upper bound on the number of steps the environment can generate in one go to 5. To evaluate adaptation, we ﬁrst train 10 agents independently on the “BiPedalwalker-v3” environment, which only uses a ﬂat terrain. To evaluate the trained agents we sample random perturbations of the environment. Speciﬁcally, for each type of perturbation (morphology, pits, stumps, stairs) and for each value of the scale parameter η, we randomly sample 30 perturbations. We then run each option for 40 episodes; adaptation takes the form of using the ﬁrst 10 episodes to estimate the option with highest episode return, which is then used for evaluation on the remaining 30 episodes. Figure 14: K-shot adaptation in BiPedal walker Fig. 14 shows that, while performance degrades as the morphology is deformed, DOMiNO exhibits greater adaptability as evidenced by less severe degradation of performance. In terms of morphology, we ﬁnd a gradual decline in performance as we increase the degree of deformation. Similar to the Control Suite, diversity is beneﬁcial and helps the agent adapt while not being impervious to these changes. In terms of terrain perturbations, these have a more abrupt impact on the agent’s performance. While diversity does not prevent a signiﬁcant drop in performance, it is still beneﬁcial when adapting to stumps and pits and does not negatively impact performance in the case of stairs. E Computing Infrastructure We run our experiments using a distributed infrastructure implemented in JAX [12]. Each run took approximately 10 hours to complete. The computing infrastructure is based on an actor-learner decomposition [19], where multiple actors generate experience in parallel, and this experience is channelled into a learner. It allows us to run experiments in two modalities. In the ﬁrst modality, following [19], the actors programs are distributed across multiple CPU machines, and both the stepping of environments and the network inference happens on CPU. The data generated by the actor programs is processed in 26 400 r e k a W l l No diversity With diversity a d e P B i -100 0.0 0.1 0.2 0.3 Morphology scale 0.0 0.3 0.6 0.9 Stump size 400 r e k a W l l a d e P B i -100 0.0 0.3 0.6 0.9 Pit size 0.0 0.3 0.6 0.9 Stairs step size batches by a single learner using a GPU. Alternatively, both the actors and learners are co-located on a single machine, where the host is equipped with 56 CPU cores and connected to 8 TPU cores [30]. To minimize the effect of Python’s Global Interpreter Lock, each actor-thread interacts with a batched environment; this is exposed to Python as a single special environment that takes a batch of actions and returns a batch of observations, but that behind the scenes steps each environment in the batch in C++. The actor threads share 2 of the 8 TPU cores (to perform inference on the network), and send batches of ﬁxed size trajectories of length T to a queue. The learner threads takes these batches of trajectories and splits them across the remaining 6 TPU cores for computing the parameter update (these are averaged with an all reduce across the participating cores). Updated parameters are sent to the actor’s TPU cores via a fast device to device channel, as soon as the new parameters are available. This minimal unit can be replicates across multiple hosts, each connected to its own 56 CPU cores and 8 TPU cores, in which case the learner updates are synced and averaged across all cores (again via fast device to device communication). 27","['x', 'discover', 'policy', 'domino', 'diversity', 'optimization', 'maintain', 'optimality', 'yschroeckerdeepmindcom', 'deepmind', 'ﬂennerhagdeepmindcom', 'shaobo', 'shaobohoudeepmindcom', 'abstract', 'find', 'different', 'solution', 'problem', 'key', 'aspect', 'intelligence', 'associate', 'creativity', 'adaptation', 'novel', 'situation', 'reinforcement', 'learn', 'set', 'diverse', 'policy', 'useful', 'exploration', 'transfer', 'hierarchy', 'robustness', 'propose', 'method', 'diversity', 'optimization', 'maintain', 'optimality', 'formalize', 'problem', 'constrain', 'markov', 'decision', 'process', 'objective', 'ﬁnd', 'diverse', 'policy', 'measure', 'distance', 'state', 'occupancy', 'policy', 'set', 'remain', 'nearoptimal', 'respect', 'extrinsic', 'reward', 'demonstrate', 'method', 'discover', 'diverse', 'meaningful', 'behavior', 'various', 'domain', 'different', 'locomotion', 'pattern', 'deepmind', 'control', 'suite', 'perform', 'extensive', 'analysis', 'approach', 'compare', 'multiobjective', 'baseline', 'demonstrate', 'control', 'quality', 'diversity', 'set', 'interpretable', 'hyperparameter', 'show', 'discover', 'set', 'robust', 'perturbation', 'introduction', 'creative', 'problem', 'solve', 'mental', 'process', 'search', 'original', 'previously', 'unknown', 'solution', 'problem', 'relationship', 'creativity', 'intelligence', 'widely', 'recognize', 'many', 'ﬁeld', 'example', 'ﬁeld', 'mathematic', 'ﬁnde', 'different', 'proof', 'theorem', 'consider', 'elegant', 'often', 'lead', 'new', 'insight', 'close', 'artiﬁcial', 'intelligence', 'consider', 'ﬁeld', 'game', 'playing', 'speciﬁcally', 'game', 'chess', 'move', 'consider', 'creative', 'go', 'know', 'pattern', 'case', 'move', 'detect', 'human', 'player', 'remain', 'invisible', 'current', 'stateoftheart', 'chess', 'engine', 'famous', 'example', 'thereof', 'win', 'move', 'game', 'classical', 'world', 'chess', 'championship', 'leko', 'human', 'indeed', 'many', 'animal', 'employ', 'similarly', 'creative', 'behavior', 'daily', 'basis', 'face', 'challenging', 'problem', 'often', 'consider', 'qualitatively', 'different', 'alternative', 'solution', 'majority', 'research', 'focus', 'ﬁnde', 'single', 'good', 'solution', 'give', 'problem', 'example', 'ﬁeld', 'reinforcement', 'learn', 'algorithm', 'design', 'single', 'rewardmaximize', 'policy', 'however', 'many', 'problem', 'interest', 'many', 'qualitatively', 'preprint', 'review', 'different', 'optimal', 'nearoptimal', 'policy', 'ﬁnde', 'diverse', 'set', 'policy', 'help', 'rl', 'agent', 'become', 'robust', 'change', 'task', 'andor', 'environment', 'generalize', 'well', 'future', 'task', 'majority', 'literature', 'problem', 'ﬁeld', 'qualitydiversity', 'comprise', 'main', 'family', 'algorithm', 'mapelite', 'novelty', 'search', 'local', 'competition', 'qd', 'algorithm', 'typically', 'maintain', 'collection', 'policy', 'adapt', 'use', 'evolutionary', 'algorithm', 'balance', 'qd', 'tradeoff', 'reference', 'find', 'qd', 'webpage', 'contrast', 'line', 'work', 'propose', 'differentiable', 'optimization', 'framework', 'maximize', 'diversity', 'set', 'rl', 'policy', 'formulate', 'diversity', 'maximization', 'optimization', 'problem', 'state', 'occupancy', 'show', 'solve', 'problem', 'maximize', 'intrinsic', 'reward', 'correspond', 'gradient', 'diversity', 'objective', 'relate', 'work', 'intrinsic', 'reward', 'use', 'learn', 'diversity', 'term', 'discriminability', 'different', 'trajectoryspeciﬁc', 'quantity', 'work', 'implicitly', 'induce', 'diversity', 'learn', 'policy', 'maximize', 'set', 'robustness', 'worstpossible', 'reward', 'use', 'diversity', 'regularizer', 'maximize', 'extrinsic', 'reward', 'work', 'make', 'follow', 'contribution', 'first', 'propose', 'method', 'diversity', 'optimization', 'maintain', 'nearly', 'optimal', 'policy', 'domino', 'train', 'set', 'policy', 'use', 'policyspeciﬁc', 'weight', 'combination', 'extrinsic', 'reward', 'intrinsic', 'diversity', 'reward', 'weight', 'adapt', 'lagrange', 'multiplier', 'guarantee', 'policy', 'nearoptimal', 'second', 'propose', 'measure', 'diversity', 'expect', 'feature', 'feature', 'policy', 'observe', 'state', 'occupancy', 'measure', 'diversity', 'introduce', 'novel', 'objective', 'diversity', 'optimization', 'repulsive', 'force', 'motivate', 'policy', 'distinct', 'expect', 'feature', 'der', 'waal', 'force', 'combine', 'repulsive', 'force', 'attractive', 'one', 'allow', 'specify', 'degree', 'diversity', 'set', 'third', 'perform', 'experiment', 'deepmind', 'control', 'bipedal', 'walker', 'environment', 'show', 'domino', 'discover', 'qualitatively', 'diverse', 'locomotion', 'behavior', 'fig', 'analyze', 'approach', 'compare', 'multiobjective', 'strategy', 'handle', 'qd', 'tradeoff', 'lastly', 'demonstrate', 'discover', 'set', 'robust', 'perturbation', 'environment', 'morphology', 'avatar', 'b', 'figure', 'domino', 'architecture', 'agent', 'learn', 'set', 'qd', 'policy', 'single', 'latent', 'condition', 'actorcritic', 'network', 'intrinsic', 'extrinsic', 'value', 'head', 'dash', 'arrow', 'signify', 'training', 'objective', 'domino', 'π', 'optimal', 'diverse', 'policy', 'walkerstand', 'correspond', 'stand', 'leg', 'stand', 'leg', 'lift', 'leg', 'forward', 'backward', 'spread', 'leg', 'stamp', 'policy', 'different', 'also', 'achieve', 'high', 'extrinsic', 'reward', 'stand', 'see', 'value', 'top', 'policy', 'preliminary', 'notation', 'work', 'express', 'objective', 'term', 'state', 'occupancy', 'measure', 'intuitively', 'speak', 'measure', 'often', 'policy', 'π', 'visit', 'stateaction', 'pair', 'soon', 'see', 'classic', 'objective', 'reward', 'maximization', 'express', 'linear', 'product', 'reward', 'vector', 'state', 'occupancy', 'addition', 'work', 'formulate', 'diversity', 'maximization', 'objective', 'nonlinear', 'function', 'state', 'occupancy', 'seem', 'unclear', 'r', 'u', 'g', 'v', 'e', 'r', 'g', 'e', 'u', 'p', 'e', 'c', 'n', 'g', 'r', 'n', 'g', 'e', 'r', 'g', 'v', 'e', 'r', 'g', 'e', 'v', 'e', 'r', 'r', 'e', 'r', 'reward', 'maximize', 'solve', 'objective', 'take', 'inspiration', 'reward', 'gradient', 'objective', 'respect', 'begin', 'formal', 'deﬁnition', 'agent', 'interact', 'environment', 'seek', 'maximize', 'cumulative', 'reward', 'consider', 'case', 'average', 'reward', 'case', 'discount', 'case', 'markov', 'decision', 'process', 'mdp', 'deﬁne', 'p', 'r', 'average', 'reward', 'case', 'p', 'r', 'γ', 'd0', 'discount', 'case', 'assume', 'inﬁnite', 'horizon', 'ﬁnite', 'stateaction', 'problem', 'initially', 'state', 'agent', 'sample', 'accord', 's0', '∼', 'time', 'give', 'state', 'agent', 'select', 'action', 'accord', 'policy', 'πst', 'receive', 'reward', 'rt', '∼', 'transition', 'new', 'state', 'st1', 'p', 'consider', 'performance', 't1γtrt', 'average', 'reward', 'metric', 'give', 'case', 'discount', 'case', 'respectively', 'goal', 'agent', 'policy', 'maximize', 'vavg', 'vγ', 'let', 'pπst', 'probability', 'measure', 'state', 'time', 'policy', 'state', 'occupancy', 'measure', 'dπ', 'give', 't1', '−', 'γe', 'cid80∞', 't1', 'γtpπst', 'average', 'reward', 'case', 'discount', 'case', 'respectively', 'rewrite', 'rl', 'objective', 'linear', 'function', 'occupancy', 'measure', 'adπ', 'set', 'admissible', 'distribution', 'next', 'consider', 'objective', 'form', 'limt', '−', 'γecid80∞', 'vγ', 'ecid80', 'dπ', 'r', 'nonlinear', 'function', 'sequential', 'decision', 'make', 'problem', 'take', 'form', 'include', 'apprenticeship', 'learn', 'pure', 'exploration', 'remainder', 'section', 'brieﬂy', 'explain', 'solve', 'eq', 'use', 'rl', 'method', 'function', 'convex', 'begin', 'rewrite', 'eq', 'use', 'duality', '∗λ', 'closure', 'subgradient', 'space', 'compact', 'convex', '∗', 'conjugate', 'function', 'eq', 'present', 'zerosum', 'maxmin', 'game', 'player', 'policy', 'player', 'algπ', 'cost', 'player', 'algλ', 'see', 'perspective', 'policy', 'player', 'objective', 'linear', 'minimization', 'problem', 'dπ', 'thus', 'intuitively', 'speak', 'goal', 'policy', 'player', 'maximize', 'negative', 'cost', 'reward', 'r', 'solve', 'eq', 'employ', 'meta', 'use', 'online', 'learning', 'algorithm', 'policy', 'player', 'algπ', 'generate', 'sequence', 'policy', 'πkk∈n', 'maximize', 'sequence', 'negative', 'cost', '−λkk∈n', 'reward', 'produce', 'cost', 'player', 'algλ', 'paper', 'policy', 'player', 'use', 'online', 'cost', 'player', 'use', 'follow', 'leader', 'imply', 'cost', 'time', 'give', 'π', 'word', 'solve', 'rl', 'problem', 'convex', 'objective', 'function', 'eq', 'policy', 'player', 'maximize', 'non', 'stationary', 'reward', 'time', 'correspond', 'negative', 'gradient', 'objective', 'function', 'function', 'convex', 'guarantee', 'average', 'state', 'occupancy', 'police', 'π', 'converge', 'optimal', 'solution', 'eq', 'π', 'dcid63', 'feature', 'expect', 'feature', 'focus', 'case', 'stateaction', 'pair', 'associate', 'observable', 'feature', 'φs', '∈', 'rd', 'example', 'dm', 'control', 'suite', 'feature', 'correspond', 'position', 'velocity', 'body', 'joint', 'control', 'agent', 'case', 'learn', 'φ', 'neural', 'network', 'cid80k', 'similar', 'value', 'function', 'represent', 'expectation', 'reward', 'state', 'occupancy', 'deﬁne', 'expect', 'feature', 'φscid48', 'acid48', 'note', 'special', 'case', 'onehot', 'feature', 'vector', 'expect', 'feature', 'coincide', 'state', 'occupancy', 'deﬁnition', 'ψπ', 'depend', 'state', 'occupancy', 'consider', 'discount', 'case', 'ψγ', 'also', 'know', 'successor', 'feature', 'sfs', 'deﬁne', 'average', 'case', 'represent', 'expect', 'feature', 'policy', 'stationary', 'distribution', 'therefore', 'value', 'state', 'action', 'pair', 'similar', 'deﬁnition', 'suggest', 'discover', 'diverse', 'nearoptimal', 'policy', 'introduce', 'diversity', 'optimization', 'maintain', 'optimality', 'domino', 'discover', 'set', 'policy', 'πin', 'i1', 'solve', 'optimization', 'problem', 'max', '≥', 'αv∗', 'value', 'optimal', 'policy', 'word', 'look', 'set', 'policy', 'diverse', 'possible', 'deﬁne', 'diversity', 'r', 'addition', 'constrain', 'policy', 'set', 'nearly', 'optimal', 'deﬁne', 'nearoptimality', 'introduce', 'hyperparameter', 'policy', 'say', 'optimal', 'achieve', 'value', 'practice', 'ﬁx', 'lagrange', 'multiplier', 'ﬁrst', 'policy', 'policy', 'least', 'αv∗', 'receive', 'extrinsic', 'reward', 'use', 'value', 'policy', 'estimate', 'notice', 'estimate', 'change', 'training', 'e', 'e', 'v∗', 'dive', 'detail', 'brieﬂy', 'explain', 'main', 'component', 'domino', 'building', 'section', 'particular', 'eq', 'ﬁnd', 'policy', 'maximize', 'diversity', 'objective', 'maximize', 'gradient', 'reward', 'discuss', 'candidate', 'objective', 'derive', 'analytical', 'formula', 'associate', 'reward', 'section', 'diversityd1', 'π', '∇di', 'π', 'section', 'explain', 'combine', 'reward', 'coefﬁcient', 'ce', 'thus', 'policy', 'maximize', 'reward', 'signal', 'linear', 'combination', 'extrinsic', 'reward', 'ri', 'ris', 'ceres', 'cdri', 'end', 'focus', 'method', 'lagrange', 'multiplier', 'adapt', 'coefﬁcient', 'online', 'order', 'solve', 'eq', 'compare', 'multiobjective', 'baseline', 'diversity', 'next', 'present', 'objective', 'motivate', 'policy', 'visit', 'different', 'state', 'average', 'leverage', 'information', 'policy', 'longterm', 'behavior', 'available', 'expect', 'feature', 'motivate', 'state', 'occupancy', 'different', 'reason', 'refer', 'objective', 'repulsive', 'force', 'eq', 'extend', 'objective', 'combine', 'second', 'attractive', 'force', 'eq', 'take', 'inspiration', 'waal', 'vdw', 'force', 'manner', 'combine', 'force', 'allow', 'control', 'degree', 'diversity', 'set', 'repulsive', 'force', 'compute', 'set', 'policy', 'maximal', 'distance', 'expect', 'feature', 'answer', 'question', 'ﬁrst', 'consider', 'simple', 'scenario', 'policy', 'set', 'consider', 'follow', 'objective', '−', 'ψ22', 'objective', 'relate', 'objective', 'apprenticeship', 'learn', 'solve', 'problem', 'feature', 'expectation', 'expert', 'problem', 'use', 'euclidean', 'norm', 'feature', 'expectation', 'space', 'measure', 'distance', 'policy', 'interested', 'diversity', 'maximize', 'objective', 'aim', 'minimize', 'similar', 'fashion', 'mutual', 'information', 'policy', 'state', 'equivalent', 'divergence', 'state', 'occupancy', 'minimize', 'maximize', 'diversity', 'next', 'investigate', 'measure', 'distance', 'policy', 'set', 'multiple', 'policy', 'first', 'introduce', 'hausdorff', 'distance', 'measure', 'far', 'subset', 'c', 'metric', 'space', 'distd', 'minc∈cd∈d', 'c', '−', 'word', 'set', 'far', 'hausdorff', 'distance', 'point', 'set', 'far', 'point', 'set', 'building', 'deﬁnition', 'deﬁne', 'distance', 'expect', 'feature', 'vector', 'ψi', 'set', 'expect', 'feature', 'vector', 'ψj2', 'equation', 'give', 'distance', 'individual', 'policy', 'policy', 'set', 'maximize', 'policy', 'set', 'give', 'ﬁrst', 'diversity', 'objective', 'max', 'd1', 'i1', 'jcid54i', 'ψj2', 'diversityd1', 'compute', 'associate', 'diversity', 'reward', 'compute', 'gradient', 'order', '∇di', 'π', 'begin', 'simple', 'case', 'policy', 'φ', '−', 'ψ2', 'r', 'φs', '−', 'ψ2', 'reward', 'ﬁrst', 'derive', 'opposite', 'sign', 'care', 'maximize', 'lastly', 'give', 'policy', 'πi', 'deﬁne', 'escid48acid48∼d2', 'dn', 'ψ22', 'get1', '∇di', 'index', 'policy', 'close', 'expect', 'feature', 'πi', 'j∗', 'use', 'deﬁnition', 'j∗', '∇di', 'ψj2', 'φs', 'ψj∗', 'ri', 'waal', 'force', 'next', 'propose', 'second', 'diversity', 'objective', 'allow', 'control', 'degree', 'diversity', 'set', 'hyperparameter', 'objective', 'inspire', 'molecular', 'physics', 'speciﬁcally', 'atom', 'crystal', 'lattice', 'selforganize', 'equal', 'distance', 'phenomenon', 'typically', 'explain', 'equilibrium', 'distance', 'dependent', 'force', 'operate', 'atom', 'know', 'waal', 'force', 'force', 'attractive', 'repulsive', 'ψj2', 'ψj∗', 'π', 'vdw', 'force', 'typically', 'characterize', 'distance', 'combined', 'force', 'become', 'repulsive', 'rather', 'attractive', 'see', 'example', 'distance', 'call', 'vdw', 'contact', 'distance', 'denote', 'addition', 'denote', 'ψj∗', 'hausdorff', 'distance', 'policy', 'notation', 'deﬁne', 'second', 'diversity', 'objective', 'as2', 'max', 'πdn', 'd1', 'i1', '05cid962', 'repulsive', 'attractive', 'cid1', 'see', 'eq', 'polynomial', 'compose', 'force', 'opposite', 'sign', 'different', 'power', 'different', 'power', 'determine', 'force', 'dominate', 'example', 'expect', 'feature', 'close', 'repulsive', 'force', 'dominate', 'attractive', 'force', 'dominate', 'gradient', 'hence', 'associated', 'reward', 'give', '−', 'ψj∗', 'inspect', 'eq', 'see', 'expect', 'feature', 'organize', 'vdw', 'contact', 'distance', 'objective', 'maximized', 'gradient', 'related', 'line', 'work', 'vassiliade', 'suggest', 'use', 'voronoi', 'tessellation', 'partition', 'feature', 'space', 'mapelite', 'region', 'equal', 'size', 'propose', 'stein', 'variational', 'policy', 'gradient', 'repulsive', 'attractive', 'component', 'however', 'use', 'vdw', 'force', 'control', 'diversity', 'novel', 'good', 'knowledge', 'balance', 'quality', 'diversity', 'constrain', 'mdps', 'core', 'approach', 'solution', 'cmdp', 'eq', 'exist', 'different', 'method', 'solve', 'cmdps', 'refer', 'reader', 'treatment', 'subject', 'different', 'level', 'abstraction', 'work', 'focus', 'reduction', 'cmdps', 'mdps', 'gradient', 'update', 'know', 'lagrangian', 'method', 'literature', 'focus', 'linear', 'objective', 'linear', 'constraint', 'section', 'discuss', 'solve', 'unconstrained', 'convex', 'problem', 'form', 'eq', 'saddle', 'point', 'problem', 'extend', 'result', 'case', 'objective', 'convex', 'constraint', 'linear', 'subject', 'denote', 'diversity', 'objective', 'g', 'linear', 'function', 'form', 'gdπ', 'αv∗', 'e', '−', 'deﬁne', 'constraint', 'solve', 'problem', 'equivalent', 'solve', 'follow', 'problem', 'µαv∗', '−', 'equality', 'follow', 'duality', 'similar', 'section', 'use', 'ftl', 'player', 'eq', 'imply', 'cost', 'iteration', 'equivalent', 'gradient', 'diversity', 'objective', 'denote', 'rd', 'eq', 'involve', 'vector', 'µre', 'linearly', 'interact', 'dπ', 'thus', 'intuitively', 'speak', 'minimize', 'eq', 'perspective', 'policy', 'player', 'equivalent', 'maximize', 'reward', 'objective', 'lagrange', 'multipli', 'µ', 'maximize', 'eq', 'equivalently', 'µαv∗', 'e', '−', 'intuitively', 'speak', 'policy', 'achieve', 'extrinsic', 'value', 'satisﬁes', 'constraint', 'rare', 'case', 'arg', 'solution', 'gradient', 'deﬁne', 'still', 'use', 'eq', 'reward', 'coefﬁcient', 'eq', 'choose', 'simplify', 'reward', 'eq', 'reward', 'gradient', 'objective', 'differentiation', 'coefﬁcient', 'equal', 'eq', 'lagrange', 'multiplier', 'decrease', 'put', 'small', 'weight', 'extrinsic', 'component', 'reward', 'increase', 'otherwise', 'formally', 'solve', 'problem', 'eq', 'threeplayer', 'game', 'case', 'policy', 'player', 'control', 'dπ', 'cost', 'player', 'choose', 'use', 'eq', 'lagrange', 'player', 'choose', 'µ', 'gradient', 'descent', 'prove', 'statement', 'scope', 'work', 'investigate', 'empirically', 'multiobjective', 'alternative', 'conclude', 'section', 'discuss', 'alternative', 'approach', 'balance', 'qd', 'tradeoff', 'later', 'compare', 'empirically', 'cmdp', 'approach', 'first', 'consider', 'multiobjective', 'mdp', 'combine', 'diversity', 'objective', 'extrinsic', 'reward', 'dn', 'cedi', 'π', 'cddiversityd1', 'max', 'ﬁxe', 'weight', 'balance', 'diversity', 'objective', 'extrinsic', 'reward', 'note', 'solution', 'multiobjective', 'mdp', 'solution', 'cmdp', 'general', 'possible', 'optimal', 'dual', 'variable', 'µ∗', 'plug', 'eq', 'simply', 'solve', 'result', 'unconstrained', 'mdp', 'approach', 'ignore', 'fact', 'dual', 'variable', 'bestresponse', 'policy', 'refer', 'scalarization', 'fallacy', 'section', 'multiobjective', 'mdps', 'use', 'prior', 'qdrl', 'paper', 'outline', 'potential', 'advantage', 'use', 'first', 'cmdp', 'formulation', 'guarantee', 'policy', 'optimal', 'satisfy', 'constraint', 'secondly', 'weighting', 'coefﬁcient', 'multiobjective', 'mdps', 'tune', 'cmdps', 'adapt', 'particularly', 'important', 'context', 'maximize', 'diversity', 'satisfy', 'reward', 'next', 'consider', 'hybrid', 'approach', 'combine', 'multi', 'objective', 'mdp', 'cmdp', 'αv∗', 'e', '−', 'di', 'π', 'cddiversityd1', 'π', 'dn', 'denote', 'indicator', 'function', 'event', 'constraint', 'satisﬁed', 'policy', 'ie', 'otherwise', 'notation', 'reward', 'give', 'π', 'αv∗', 'ris', 'ire', 'cdri', 'word', 'agent', 'maximize', 'weight', 'combination', 'extrinsic', 'reward', 'diversity', 'reward', 'constraint', 'violate', 'diversity', 'reward', 'constraint', 'propose', 'similar', 'approach', 'agent', 'maximize', 'weight', 'combination', 'reward', 'constraint', 'satisﬁed', 'extrinsic', 'reward', 'otherwise', 'ris', 'cd1', '−', 'iri', 'refer', 'eq', 'smerl', 'coin', 'eq', 'reverse', 'smerl', 'note', 'method', 'come', 'additional', 'hyperparameter', 'cd', 'balance', 'objective', 'multiobjective', 'mdp', 'addition', 'optimality', 'ratio', 'experiment', 'experiment', 'address', 'follow', 'question', 'learn', 'diverse', 'policy', 'also', 'optimal', 'see', 'section', 'b', 'critical', 'various', 'component', 'compare', 'multiobjective', 'baseline', 'term', 'tradeoff', 'section', 'c', 'method', 'scale', 'setting', 'feature', 'space', 'highdimensional', 'unstructured', 'see', 'section', 'finally', 'diverse', 'policy', 'discover', 'domino', 'enable', 'robustness', 'adaptation', 'novel', 'perturbation', 'environment', 'agent', 'see', 'section', 'environment', 'conduct', 'experiment', 'domain', 'dm', 'control', 'suite', 'standard', 'continuous', 'control', 'locomotion', 'task', 'diverse', 'nearoptimal', 'policy', 'naturally', 'correspond', 'different', 'gait', 'space', 'consideration', 'present', 'control', 'suite', 'result', 'walkerstand', 'task', 'supplementary', 'however', 'present', 'similar', 'result', 'walkerwalk', 'bipedal', 'walker', 'openai', 'gym', 'suggest', 'method', 'generalize', 'different', 'reward', 'function', 'domain', 'also', 'include', 'challenging', 'dog', 'domain', 'action', '223−dimensional', 'observation', 'space', 'challenging', 'domain', 'control', 'suite', 'agent', 'fig', 'show', 'overview', 'domino', 'component', 'interaction', 'instantiate', 'actorcritic', 'agent', 'act', 'agent', 'sample', 'new', 'latent', 'variable', 'uniformly', 'random', 'start', 'episode', 'train', 'policy', 'simultaneously', 'provide', 'latent', 'variable', 'input', 'average', 'reward', 'state', 'occupancy', 'agent', 'keep', 'empirical', 'running', 'average', 'latent', 'policy', 'reward', 'feature', 'environment', 'φob', 'torso', 'embed', 'φembedde', '˜ψavg', 'πi', 'encounter', 'average', 'take', '−', 'αd', 't1', 'xist', 'decay', 'factor', 'vary', 'αd', 'make', 'estimation', 'online', 'small', 'αd', 'use', 'constraint', 'less', 'online', 'large', 'αd', 'need', 'eq', 'agent', 'use', '˜ψavg', 'compute', 'diversity', 'reward', 'describe', 'eq', '˜vavg', 'use', 'optimize', 'πi', 'lagrange', 'multipli', 'µ', 'policy', 'eq', 'use', 'weight', 'quality', 'diversity', 'advantage', 'policy', 'gradient', 'update', 'pseudo', 'code', 'implementation', 'detail', 'well', 'treatment', 'discount', 'state', 'occupancy', 'find', 'quality', 'diversity', 'measure', 'diversity', 'qualitatively', 'present', 'motion', 'ﬁgure', 'discretize', 'video', 'detail', 'appendix', 'give', 'fair', 'impression', 'behavior', 'video', 'associate', 'ﬁgure', 'find', 'supplementary', 'well', 'fig', 'present', 'police', 'discover', 'domino', 'repulsive', 'objective', 'optimality', 'ratio', 'set', 'policy', 'order', 'topleft', 'bottom', 'right', 'policy', 'maximize', 'extrinsic', 'reward', 'set', 'constraint', 'always', 'top', 'leave', 'policy', 'exhibit', 'different', 'type', 'stand', 'stand', 'leg', 'stand', 'leg', 'lift', 'leg', 'forward', 'backward', 'spread', 'leg', 'stamp', 'policy', 'different', 'also', 'achieve', 'high', 'extrinsic', 'reward', 'stand', 'see', 'value', 'top', 'policy', 'visualization', 'similar', 'ﬁgure', 'domain', 'find', 'b1', 'far', 'study', 'qd', 'tradeoff', 'use', 'scatter', 'plot', 'show', 'episode', 'return', 'yaxis', 'diversity', 'score', 'correspond', 'hausdorff', 'distance', 'eq', 'xaxis', 'topright', 'corner', 'diagram', 'therefore', 'represent', 'diverse', 'high', 'quality', 'policy', 'ﬁgure', 'present', 'sweep', 'hyperparameter', 'use', 'color', 'marker', 'indicate', 'value', 'experiment', 'report', 'conﬁdence', 'interval', 'scatter', 'plot', 'correspond', 'seed', 'indicate', 'crosse', 'surround', 'point', 'fig', 'leave', 'present', 'result', 'domino', 'repulsive', 'reward', 'walkerstand', 'domain', 'observe', 'regardless', 'set', 'size', 'domino', 'achieve', 'roughly', 'extrinsic', 'reward', 'different', 'optimality', 'ratio', 'point', 'color', 'obtain', 'yvalue', 'imply', 'constraint', 'mechanism', 'work', 'expect', 'different', 'set', 'size', 'optimality', 'ratio', 'addition', 'inspect', 'qd', 'tradeoff', 'affect', 'change', 'optimality', 'ratio', 'α', 'set', 'size', 'indicate', 'ﬁgure', 'light', 'line', 'observation', 'explain', 'fact', 'low', 'value', 'volume', 'constrain', 'set', 'large', 'therefore', 'possible', 'ﬁnd', 'diversity', 'behavior', 'consistent', 'different', 'set', 'size', 'naturally', 'difﬁcult', 'set', 'diverse', 'policy', 'set', 'size', 'get', 'large', 'remember', 'measure', 'distance', 'close', 'policy', 'set', 'figure', 'domino', 'qd', 'result', 'walkerstand', 'leave', 'set', 'size', 'optimality', 'ratio', 'α', 'repulsive', 'center', 'set', 'size', 'α', 'der', 'waal', 'vdw', 'reward', 'right', 'target', 'diversity', 'α', 'vdw', 'reward', 'present', 'investigation', 'vdw', 'reward', 'fig', 'center', 'similar', 'repulsive', 'reward', 'observe', 'constraint', 'satisﬁed', 'reduce', 'optimality', 'ratio', 'allow', 'diversity', 'fig', 'right', 'show', 'different', 'value', 'affect', 'qd', 'tradeoff', 'set', 'size', 'observe', 'different', 'combination', 'α', 'organize', 'grid', 'qd', 'scatter', 'suggest', 'control', 'level', 'optimality', 'degree', 'diversity', 'set', 'interpretable', 'hyperparameter', 'fig', 'compare', 'qd', 'balance', 'yield', 'domino', 'alternative', 'strategy', 'describe', 'section', 'speciﬁcally', 'look', 'lagrangian', 'method', 'linear', 'multiobjective', 'combination', 'objective', 'eq', 'hybrid', 'strategy', 'smerl', 'eq', 'reverse', 'smerl', 'eq', 'set', 'policy', 'walkerstand', 'note', 'case', 'use', 'repulsive', 'diversity', 'objective', 'comparison', 'strictly', 'strategy', 'combine', 'quality', 'diversity', 'objective', 'plot', 'strategy', 'show', 'solution', 'qd', 'tradeoff', 'vary', 'accord', 'relevant', 'hyperparameter', 'strategy', 'namely', 'optimality', 'ratio', 'α', 'domino', 'ﬁxed', 'constant', 'ce', 'multiobjective', 'strategy', 'implicitly', 'set', 'ce', 'constant', 'cd', 'hybrid', 'approach', 'hybrid', 'plot', 'cd', 'value', 'label', 'directly', 'next', 'marker', 'α', 'indicate', 'color', 'figure', 'domino', 'lagrangian', 'method', 'ﬁnd', 'solution', 'push', 'upper', 'right', 'boundary', 'quality', 'diversity', 'vary', 'smooth', 'interpretable', 'way', 'hyperparameter', 'contrast', 'jumpy', 'nature', 'multiobjective', 'hyperparameter', 'ce', 'redundancy', 'hyperparameter', 'hybrid', 'method', 'smerl', 'reverse', 'smerl', 'multiobjective', 'approach', 'show', 'right', 'parameter', 'prove', 'illbehave', 'choppy', 'quickly', 'jump', 'extreme', 'diversity', 'quality', 'opposite', 'smooth', 'interim', 'contrast', 'domino', 'approach', 'solve', 'cmdp', 'directly', 'lagrange', 'multipli', 'yield', 'solution', 'push', 'upper', 'right', 'diagonal', 'boundary', 'ﬁnde', 'high', 'diversity', 'farth', 'right', 'set', 'policy', 'give', 'optimality', 'ratio', 'color', 'vary', 'smoothly', 'line', 'vary', 'advantage', 'approach', 'ﬁnd', 'qdoptimal', 'solution', 'contrast', 'smerl', 'leave', 'appropriately', 'tune', 'also', 'yield', 'solution', 'upperright', 'qd', 'border', 'often', 'ﬁnd', 'suboptimal', 'solution', 'therefore', 'tune', 'far', 'good', 'qd', 'solution', 'far', 'explore', 'difﬁculty', 'tune', 'smerl', 'supplementary', 'fig', 'good', 'cd', 'policy', 'provide', 'solution', 'diversity', 'set', 'size', 'feature', 'analysis', 'choice', 'feature', 'space', 'use', 'optimize', 'diversity', 'huge', 'impact', 'kind', 'diverse', 'behavior', 'learn', 'environment', 'observation', 'space', 'high', 'dimensional', 'less', 'structure', 'pixel', 'observation', 'compute', 'diversity', 'use', 'raw', 'feature', 'lead', 'meaningful', 'behavior', 'section', 'feature', 'space', 'use', 'compute', 'diversity', 'control', 'suite', 'experiment', 'paper', 'correspond', 'position', 'velocity', 'body', 'joint', 'return', 'observation', 'environment', 'show', 'feasible', 'use', 'learned', 'embed', 'space', 'instead', 'proof', 'principle', 'use', 'output', 'torso', 'network', 'learn', 'embed', 'describe', 'section', 'compute', 'diversity', 'metric', 'table', 'compare', 'diversity', 'mea', 'sure', 'raw', 'observation', 'feature', 'diversityob', 'embed', 'feature', 'diversityembedde', 'walkerstand', 'domain', 'column', 'indicate', 'feature', 'space', 'use', 'compute', 'di', 'versity', 'objective', 'training', 'average', 'seed', 'inspect', 'table', 'see', 'agent', 'train', 'optimize', 'diversity', 'learn', 'embed', 'space', 'agent', 'directly', 'optimize', 'diversity', 'observation', 'space', 'achieve', 'comparable', 'diversity', 'measure', 'space', 'indicate', 'learn', 'embedding', 'feasibly', 'use', 'achieve', 'meaningful', 'diversity', 'diversityob', 'diversityembedde', 'φembedde', 'φob', 'table', 'figure', 'kshot', 'adaptation', 'control', 'suite', 'report', 'mean', 'episode', 'return', 'ci', 'heldout', 'test', 'task', 'relative', 'performance', 'single', 'policy', 'train', 'extrinsic', 'reward', 'invariant', 'sudden', 'change', 'environment', 'domino', 'robust', 'variety', 'perturbation', 'close', 'loop', 'kshot', 'adaptation', 'motivate', 'qualitative', 'diversity', 'say', 'diverse', 'solution', 'robust', 'allow', 'rapid', 'adaptation', 'new', 'task', 'change', 'environment', 'validate', 'claim', 'kshot', 'adaptation', 'experiment', 'train', 'set', 'qd', 'policy', 'canonical', 'benchmark', 'task', 'test', 'ability', 'adapt', 'environment', 'agent', 'perturbation', 'include', 'kinematic', 'dynamic', 'perturbation', 'real', 'world', 'suite', 'ﬁfth', 'perturbation', 'inspire', 'motor', 'failure', 'condition', 'step', 'start', 'disable', 'actioninput', 'ﬁrst', 'joint', 'ﬁxed', 'amount', 'time3', 'fig', 'present', 'result', 'walkerwalk', 'walkerstand', 'domain', 'row', 'column', 'correspond', 'perturbation', 'type', 'xaxis', 'correspond', 'magnitude', 'perturbation', 'kshot', 'adaptation', 'measure', 'following', 'manner', 'perturb', 'environment', 'method', 'ﬁrst', 'execute', 'environment', 'trajectory', 'policy', 'method', 'select', 'policy', 'perform', 'good', 'set', 'evaluate', 'policy', 'trajectory', 'measure', 'average', 'reward', 'select', 'policy', 'rmethod', 'yaxis', 'ﬁgure', 'measure', 'rmethodrbaseline', 'rbaseline', 'measure', 'reward', 'perturb', 'environment', 'rl', 'baseline', 'agent', 'train', 'single', 'policy', 'maximize', 'extrinsic', 'reward', 'original', 'task', 'note', 'baseline', 'train', 'diversity', 'match', 'stateoftheart', 'training', 'domain', 'almost', 'optimal', 'raw', 'reward', 'find', 'supplementary', 'fig', 'lastly', 'repeat', 'process', 'training', 'seed', 'report', 'average', 'conﬁdence', 'interval', 'ci', 'compare', 'follow', 'method', 'domino', 'smerl', 'multiobjective', 'diversity', 'diversity', 'method', 'use', 'diversity', 'reward', 'eq', 'method', 'policy', 'treat', 'perturb', 'environment', 'hold', 'task', 'select', 'hyper', 'parameter', 'method', 'base', 'result', 'fig', 'iewe', 'choose', 'conﬁguration', 'qualitatively', 'diverse', 'upperright', 'corner', 'fig', 'concretely', 'domino', 'smerl', 'smerl', 'multiobjective', 'ce', 'kshot', 'adaptation', 'curve', 'hyper', 'parameter', 'value', 'find', 'diversity', 'method', 'similar', 'rbaseline', 'use', 'policy', 'maximize', 'extrinsic', 'reward', 'instead', 'single', 'policy', 'inspect', 'fig', 'see', 'small', 'perturbation', 'domino', 'retain', 'performance', 'baseline', 'however', 'magnitude', 'perturbation', 'increase', 'performance', 'domino', 'much', 'high', 'baseline', 'factor', '−', 'observation', 'highlight', 'diverse', 'set', 'policy', 'find', 'domino', 'much', 'capable', 'handle', 'change', 'environment', 'serve', 'strong', 'starting', 'point', 'recover', 'optimal', 'behavior', 'show', 'approach', 'manage', 'tradeoff', 'quality', 'diversity', 'smerl', 'much', 'sensitive', 'choice', 'hyperparameter', 'require', 'signiﬁcant', 'tuning', 'smerl', 'able', 'ﬁnd', 'useful', 'diverse', 'set', 'policy', 'effort', 'difﬁcult', 'match', 'performance', 'pertubation', 'task', 'see', 'supplementary', 'material', 'comparison', 'domino', 'smerl', 'multiobjective', 'hyper', 'parameter', 'also', 'include', 'video', 'illustrate', 'individual', 'policy', 'adapt', 'environment', 'perturbation', 'try', 'recreate', 'similar', 'condition', 'task', 'directly', 'comparable', 'due', 'signiﬁcant', 'difference', 'simulator', 'use', 'well', 'termination', 'condition', 'base', 'task', '4we', 'use', 'boost', 'ci', 'nested', 'sampling', 'implement', 'bootstrap', 'function', 'reﬂect', 'amount', 'training', 'seed', 'amount', 'evaluation', 'seed', 'training', 'seed', 'r', 'u', 'e', 'r', 'e', 'e', 'r', 'r', 'u', 'e', 'r', 'e', 'e', 'r', 'l', 'motor', 'failure', 'duration', 'thigh', 'length', 'joint', 'damp', 'length', 'domino', 'smerl', 'diversity', 'multiobjective', 'contact', 'friction', 'conclusion', 'work', 'propose', 'domino', 'algorithm', 'discover', 'diverse', 'behavior', 'maintain', 'optimality', 'frame', 'problem', 'cmdp', 'state', 'occupancy', 'policy', 'set', 'develop', 'endtoend', 'differentiable', 'solution', 'base', 'reward', 'maximization', 'experiment', 'demonstrate', 'policy', 'discover', 'domino', 'domino', 'diverse', 'maintain', 'optimality', 'explore', 'domino', 'balance', 'qd', 'tradeoff', 'compare', 'multiobjective', 'baseline', 'result', 'suggest', 'domino', 'control', 'degree', 'quality', 'diversity', 'interpretable', 'hyperparameter', 'baseline', 'struggle', 'capture', 'kshot', 'experiment', 'demonstrate', 'domino', 'adapt', 'change', 'environ', 'ment', 'exciting', 'direction', 'future', 'work', 'use', 'domino', 'never', 'end', 'set', 'environment', 'change', 'smoothly', 'time', 'see', 'maintae', 'set', 'qd', 'diverse', 'policy', 'make', 'resilient', 'change', 'reference', 'abbeel', 'p', 'apprenticeship', 'learn', 'inverse', 'reinforcement', 'learning', 'proceeding', 'twentyﬁrst', 'international', 'conference', 'machine', 'learning', 'acm', 'frankwolfe', 'equilibrium', 'computation', 'luxburg', 'u', 'bengio', 'wallach', 'fergus', 'r', 'ed', 'advance', 'neural', 'information', 'processing', 'system', 'volume', 'cur', 'run', 'url', 'agadmator', 'invisible', 'engine', 'great', 'move', 'ever', 'play', 'chess', 'channel', 'constrain', 'markov', 'decision', 'process', 'volume', 'crc', 'press', 'barreto', 'dabney', 'muno', 'h', 'p', 'silver', 'successor', 'feature', 'transfer', 'reinforcement', 'learning', 'advance', 'neural', 'information', 'processing', 'system', 'barreto', 'silver', 'fast', 'reinforcement', 'learning', 'generalize', 'policy', 'update', 'proceeding', 'science', 'mnih', 'v', 'relative', 'variational', 'intrinsic', 'control', 'proceeding', 'conference', 'artiﬁcial', 'intelligence', 'behovit', 'r', 'game', 'leko', 'win', 'take', 'lead', 'chess', 'news', 'url', 'chessbasecompostgame8lekowinstotakethelead', 'belogolovsky', 'zahavy', 'inverse', 'reinforcement', 'learning', 'contextual', 'mdps', 'machine', 'learn', 'online', 'actor', 'critic', 'function', 'approximation', 'constrain', 'markov', 'decision', 'process', 'journal', 'optimization', 'theory', 'application', 'borkar', 'actorcritic', 'algorithm', 'constrain', 'markov', 'decision', 'process', 'system', 'control', 'letter', 'bradbury', 'r', 'j', 'maclaurin', 'wanderman', 'milne', 'jax', 'composable', 'transformation', 'pythonnumpy', 'program', 'brockman', 'l', 'schneider', 'openai', 'gym', 'arxiv', 'preprint', 'calian', 'levine', 'n', 'mann', 'balancing', 'constraint', 'reward', 'metagradient', 'international', 'conference', 'learn', 'representation', 'cully', 'autonomous', 'skill', 'discovery', 'qualitydiversity', 'unsupervised', 'descriptor', 'proceeding', 'genetic', 'evolutionary', 'computation', 'conference', 'cully', 'clune', 'tarapore', 'mouret', 'robot', 'adapt', 'animal', 'nature', '5217553503–507', 'da', 'fonsecawollheim', 'c', 'swap', 'song', 'chess', 'grandmaster', 'hester', 'challenge', 'realworld', 'reinforcement', 'learn', 'preprint', 'espeholt', 'l', 'soyer', 'muno', 'r', 'ward', 'doron', 'dun', 'et', 'scalable', 'distribute', 'deeprl', 'importance', 'weight', 'actorlearner', 'architecture', 'international', 'conference', 'machine', 'learning', 'pmlr', 'eysenbach', 'b', 'diversity', 'need', 'learning', 'skill', 'reward', 'function', 'international', 'conference', 'learn', 'representation', 'eysenbach', 'b', 'salakhutdinov', 'r', 'information', 'geometry', 'unsupervised', 'reinforcement', 'learn', 'preprint', 'gangwani', 'harness', 'distribution', 'ratio', 'estimator', 'learn', 'agent', 'quality', 'diversity', 'preprint', 'geist', 'p´erolat', 'lauriere', 'bachem', 'muno', 'r', 'pietquin', 'concave', 'utility', 'reinforcement', 'learn', 'meanﬁeld', 'game', 'viewpoint', 'arxiv', 'preprint', 'variational', 'intrinsic', 'control', 'international', 'conference', 'learn', 'representation', 'workshop', 'track', 'url', 'reinforcement', 'learn', 'improve', 'agent', 'design', 'arxiv', 'preprint', 'hazan', 'e', 'provably', 'efﬁcient', 'maximum', 'exploration', 'international', 'conference', 'machine', 'learning', 'pmlr', 'hessel', 'clark', 'kemaev', 'h', 'podracer', 'architecture', 'scalable', 'reinforcement', 'learn', 'arxiv', 'preprint', 'generative', 'adversarial', 'imitation', 'learn', 'preprint', 'shann', 'tj', 'exploration', 'strategy', 'deep', 'reinforcement', 'learning', 'proceeding', '32nd', 'international', 'conference', 'neural', 'information', 'processing', 'system', 'young', 'c', 'patterson', 'agrawal', 'g', 'bajwa', 'r', 'bate', 'n', 'borcher', 'boyle', 'r', 'r', 'r', 'jaworski', 'kaplan', 'lucke', 'mackean', 'g', 'maggiore', 'mahony', 'phelp', 'ross', 'sizikov', 'swing', 'toma', 'h', 'tuttle', 'wilcox', 'e', 'yoon', 'h', 'indatacenter', 'performance', 'analysis', 'tensor', 'processing', 'unit', 'abs170404760', 'solution', 'need', 'fewshot', 'extrapolation', 'structured', 'maxent', 'advance', 'neural', 'information', 'processing', 'system', 'lehman', 'evolve', 'diversity', 'virtual', 'creature', 'novelty', 'search', 'local', 'competition', 'proceeding', '13th', 'annual', 'conference', 'genetic', 'evolutionary', 'computation', 'variational', 'policy', 'gradient', '33rd', 'conference', 'uncertainty', 'artiﬁcial', 'intelligence', 'masood', 'diversityinduce', 'policy', 'gradient', 'use', 'maximum', 'mean', 'discrepancy', 'set', 'diverse', 'policy', 'arxiv', 'preprint', 'mehta', 'fern', 'transfer', 'variablereward', 'hierarchical', 'reinforcement', 'learn', 'machine', 'learning', 'mouret', 'illuminate', 'search', 'space', 'mapping', 'elite', 'preprint', 'mutti', 'r', 'p', 'challenge', 'common', 'assumption', 'reinforcement', 'learn', 'preprint', 'osborn', 'apply', 'imagination', 'scribner', 'parkerholder', 'choromanski', 'robert', 'j', 'effective', 'diversity', 'population', 'base', 'reinforcement', 'learning', 'advance', 'neural', 'information', 'processing', 'system', 'sun', 'nonlocal', 'policy', 'optimization', 'diversityregularized', 'collaborative', 'exploration', 'preprint', 'pugh', 'quality', 'diversity', 'new', 'frontier', 'evolutionary', 'computation', 'frontier', 'robotic', 'ai', 'puterman', 'decision', 'process', 'discrete', 'stochastic', 'dynamic', 'programming', 'son', 'rockafellar', 'r', 'press', 'schmitt', 'simonyan', 'offpolicy', 'actorcritic', 'shared', 'experience', 'replay', 'international', 'conference', 'machine', 'learning', 'pmlr', 'shani', 'apprenticeship', 'learn', 'preprint', 'unsupervised', 'discovery', 'skill', 'international', 'conference', 'learn', 'representation', 'singh', 'chapter', 'structure', 'synthesis', 'application', 'nanoparticle', 'engineer', 'nanoparticle', 'academic', 'isbn', 'stooke', 'abbeel', 'p', 'responsive', 'safety', 'reinforcement', 'learning', 'pid', 'lagrangian', 'method', 'international', 'conference', 'machine', 'learning', 'pmlr', 'sun', 'novel', 'policy', 'seek', 'constrain', 'optimization', 'preprint', 'c', 'constrain', 'mdps', 'reward', 'hypothesis', 'musing', 'machine', 'learn', 'ing', 'thing', 'blog', 'tarapore', 'clune', 'j', 'cully', 'mouret', 'different', 'encoding', 'inﬂuence', 'performance', 'mapelite', 'algorithm', 'proceeding', 'genetic', 'evolutionary', 'computation', 'conference', 'tassa', 'muldal', 'erez', 'merel', 'preprint', 'constrain', 'policy', 'optimization', 'international', 'conference', 'learn', 'representation', 'vassiliade', 'mouret', 'scale', 'mapelite', 'use', 'centroidal', 'tessellation', 'preprint', 'cohen', 'kaplan', 'h', 'mansour', 'apprenticeship', 'learn', 'zahavy', 'cohen', 'kaplan', 'h', 'mansour', 'average', 'reward', 'reinforcement', 'learn', 'unknown', 'mixing', 'time', 'conference', 'uncertainty', 'artiﬁcial', 'intelligence', 'uai', 'zahavy', 'barreto', 'discover', 'set', 'policy', 'bad', 'case', 'reward', 'international', 'conference', 'learn', 'representation', 'o’donoghue', 'desjardin', 'g', 'reward', 'enough', 'mdps', 'beygelzimer', 'advance', 'neural', 'information', 'processing', 'system', 'elndvevatr', 'bedi', 'variational', 'policy', 'gradient', 'method', 'reinforcement', 'learning', 'general', 'utility', 'arxiv', 'preprint', 'g', 'learn', 'novel', 'policy', 'task', 'salakhutdinov', 'r', 'ed', 'proceeding', '36th', 'international', 'conference', 'machine', 'learn', 'ing', 'volume', 'proceeding', 'machine', 'learning', 'research', 'pmlr', 'checklist', 'author', 'main', 'claim', 'make', 'abstract', 'introduction', 'accurately', 'reﬂect', 'paper', 'contribution', 'scope', 'describe', 'limitation', 'work', 'see', 'discussion', 'discuss', 'potential', 'negative', 'societal', 'impact', 'work', 'identify', 'potential', 'negative', 'societal', 'impact', 'work', 'read', 'ethic', 'review', 'guideline', 'ensure', 'paper', 'conform', 'include', 'theoretical', 'result', 'state', 'full', 'set', 'assumption', 'theoretical', 'result', 'b', 'include', 'complete', 'proof', 'theoretical', 'result', 'run', 'experiment', 'include', 'code', 'datum', 'instruction', 'need', 'reproduce', 'main', 'experi', 'mental', 'result', 'supplemental', 'material', 'url', 'dm', 'control', 'domain', 'open', 'source', 'code', 'proprietary', 'able', 'share', 'say', 'share', 'source', 'code', 'diversity', 'reward', 'function', 'a3', 'provide', 'pseudo', 'code', 'hyper', 'parameter', 'detail', 'training', 'detail', 'eg', 'datum', 'split', 'hyperparameter', 'choose', 'see', 'c', 'report', 'error', 'bar', 'eg', 'respect', 'random', 'seed', 'run', 'experi', 'ment', 'multiple', 'time', 'include', 'total', 'amount', 'compute', 'type', 'resource', 'use', 'type', 'internal', 'cluster', 'cloud', 'provider', 'see', 'e', 'use', 'exist', 'asset', 'eg', 'code', 'datum', 'model', 'curatingrelease', 'new', 'asset', 'work', 'use', 'exist', 'asset', 'cite', 'creator', 'b', 'mention', 'license', 'asset', 'c', 'include', 'new', 'asset', 'supplemental', 'material', 'url', 'discuss', 'consent', 'obtain', 'people', 'datum', 'usingcurating', 'e', 'discuss', 'datum', 'usingcurating', 'contain', 'personally', 'identiﬁable', 'information', 'offensive', 'content', 'use', 'crowdsource', 'conduct', 'research', 'human', 'subject', 'include', 'full', 'text', 'instruction', 'give', 'participant', 'screenshot', 'applicable', 'b', 'describe', 'potential', 'participant', 'risk', 'link', 'board', 'approval', 'applicable', 'c', 'include', 'estimate', 'hourly', 'wage', 'pay', 'participant', 'total', 'amount', 'spend', 'participant', 'compensation', 'additional', 'experiment', 'detail', 'environment', 'evaluate', 'method', 'number', 'task', 'task', 'different', 'complexity', 'term', 'control', 'dynamic', 'reward', 'structure', '8dimensional', 'control', 'task', '23dimensional', 'observa', 'tion', 'space', 'goal', 'control', 'joint', 'humanoid', 'character', 'make', 'walk', 'forward', 'plane', '8dimensional', 'control', 'task', '23dimensional', 'observa', 'tion', 'space', 'goal', 'control', 'joint', 'humanoid', 'character', 'make', 'stand', 'control', 'suite', '38dimensional', 'control', 'task', 'observation', 'space', 'goal', 'control', 'joint', 'dog', 'character', 'make', 'walk', 'forward', 'plane', 'control', 'suite', '38dimensional', 'control', 'task', 'obser', 'vation', 'space', 'goal', 'control', 'joint', 'dog', 'character', 'make', 'stand', 'bipedal', 'walker', '4dimensional', 'control', 'task', '24dimensional', 'observation', 'space', 'goal', 'control', 'joint', 'bipedalwalker', 'large', 'payload', 'order', 'traverse', 'terrain', 'left', 'right', 'fast', 'possible', 'implementation', 'detail', 'distribute', 'agent', 'act', 'learning', 'decouple', 'multiple', 'actor', 'gather', 'datum', 'parallel', 'batched', 'stream', 'environment', 'store', 'trajectory', 'include', 'latent', 'variable', 'z', 'replay', 'buffer', 'queue', 'learner', 'sample', 'mixed', 'batch', 'online', 'replay', 'trajectory', 'latent', 'variable', 'z', 'sample', 'uniformly', 'random', 'act', 'beginning', 'new', 'episode', 'learner', 'differentiate', 'loss', 'function', 'describe', 'use', 'optimizer', 'speciﬁed', 'table', 'update', 'network', 'parameter', 'lagrange', 'multiplier', 'speciﬁed', 'table', 'lastly', 'learner', 'also', 'update', 'move', 'average', 'describe', 'initialization', 'training', 'begin', 'initialize', 'network', 'parameter', 'well', 'lagrange', 'multiplier', 'σ−105', '∀i', 'inverse', 'move', 'average', '¯1d', '∀i', '∈', 'number', 'policy', 'dimension', 'feature', '∀i', 'n', 'bound', 'lagrange', 'multiplier', 'ensure', 'lagrange', 'multiplier', 'get', 'large', 'increase', 'magnitude', 'extrinsic', 'reward', 'destabilize', 'learn', 'use', 'bound', 'lagrange', 'multipli', 'apply', 'sigmoid', 'activation', 'effective', 'reward', 'convex', 'combination', 'diversity', 'extrinsic', 'reward', 'rs', 'σµire', '−', 'σµiri', 'objective', 'average', 'state', 'occupancy', 'empirical', 'feature', 'average', 'use', 'experiment', 'main', 'text', 'good', 'imperfect', 'bias', 'sample', 'policy', 'mix', 'experiment', 'however', 'mix', 'time', 'dm', 'control', 'suite', 'much', 'short', 'episode', 'length', 'bias', 'small', '∼', 'discount', 'state', 'occupancy', 'scalable', 'solution', 'mention', 'section', 'instead', 'predict', 'successor', 'feature', 'use', 'additional', 'network', 'head', 'show', 'fig', '11a', 'similar', 'value', 'learn', 'use', 'vtrace', 'target', 'training', 'successor', 'feature', 'discount', 'state', 'occupancy', 'case', 'also', 'use', 'extrinsic', 'value', 'function', 'policy', 'e', 'fig', 'estimate', 'instead', 'run', 'average', '˜vavg', 'show', 'experimental', 'result', 'setup', 'fig', 'loss', 'function', 'instead', 'learn', 'single', 'value', 'head', 'combined', 'reward', 'network', 'value', 'head', 'diversity', 'reward', 'extrinsic', 'reward', 'use', 'vtrace', 'compute', 'tderror', 'advantage', 'value', 'head', 'use', 'vtrace', 'error', 'advantage', 'function', 'implement', 'rlaxblobmasterrlaxsrcvtracepy', 'value', 'loss', 'head', 'squared', 'loss', 'td2', 'tderror', 'combine', 'value', 'loss', 'network', 'sum', 'loss', 'e', 'addition', 'network', 'policy', 'head', 'train', 'policy', 'gradient', 'loss', 'implement', 'train', 'policy', 'combine', 'intrinsic', 'extrinsic', 'advantage', '−', 'σµiδd', 'see', 'weight', 'cumulant', 'function', 'a3', 'effect', 'combine', 'reward', 'however', 'find', 'value', 'head', 'stable', 'value', 'different', 'scale', 'ﬁnal', 'loss', 'agent', 'weighted', 'sum', 'value', 'loss', 'policy', 'loss', 'regularization', 'loss', 'weight', 'find', 'table', 'also', 'return', 'lagrange', 'loss', 'function', 'design', 'force', 'policy', 'achieve', 'value', 'least', 'α', 'time', 'value', 'ﬁrst', 'policy', 'maximize', 'extrinsic', 'reward', 'optimally', 'ratio', 'table', 'update', 'lagrange', 'multiplier', 'use', 'optimizer', 'speciﬁed', 'table', 'keep', 'multipli', 'ﬁrst', 'policy', 'ﬁxe', 'lastly', 'also', 'update', 'move', 'average', 'algorithm', 'loss', 'function', 'i1', 'probability', 'assign', 'size', 'parameter', 'network', 'parameter', 'θ', 'lagrange', 'multiplier', 'move', 'average', 'datum', 'trajectory', 'τjm', 'compute', 'extrinsic', 'tderror', 'advantage', 'extrinsic', 'reward', 'intrinsic', 'reward', 'compute', 'intrinsic', 'tderror', 'advantage', 'tddxj', 'intrinsic', 'reward', 'combine', 'advantage', 'δxj', 'weighted', 'loss', 'intrinsic', 'critic', 'vdxj', '−', 'σµiδdxj', 'extrinsic', 'eq', 'j1', 'δdxj', 'behaviour', 'policy', 'bententropyπaj', 'loss', 'update', 'move', 'average', 'i1', 'πi', 'return', 'weight', 'loss', 'lagrange', 'loss', 'πi', 'i1', 'a3', 'function', 'def', 'intrinsicreward', 'phi', 'sfs', 'latent', 'attractivepower', 'repulsivepower', 'targetd', 'compute', 'diversity', 'reward', 'use', 'successor', 'feature', 'phi', 'feature', 'sfs', 'successor', 'feature', 'predict', 'discount', 'successor', 'feature', 'tbfl', 'latent', 'attractivepower', 'power', 'attractive', 'force', 'repulsivepower', 'power', 'repulsive', 'force', 'mix', 'attractive', 'repulsive', 'force', 'ell0', 'desire', 'target', 'distance', 'targetd', 'minimizer', 'objective', 'e', 'gradient', 'reward', 'return', 'intrinsicreward', 'sfs', 'predict', 'extra', 'lead', 'dim', 'sfs', 'swapaxe', 'sfs', 'computedistfn', 'vmap', 'vmap', 'computedistance', 'einsum', 'tblf', 'tblf', 'match', 'shape', 'elif', 'sfs', 'computedistfn', 'computedist', 'ance', 'else', 'raise', 'valueerror', 'invalid', 'shape', 'argument', 'sfs', 'shape', 'compute', 'lxl', 'matrix', 'row', 'correspond', 'latent', 'hot', 'vector', 'indicate', 'index', 'latent', 'close', 'sfs', 'dist', 'computedistfn', 'sfs', 'dist', 'eye', 'dist', 'r', 'e', 'l', 'tr', 'argmin', 'dist', 'axis', 'compute', 'vector', 'near', 'latent', 'latent', 'latent', 'nearestlatent', 'latent', 'tr', 'compute', 'psii', 'latent', 'nearestlatent', 'sfs', 'sum', 'psidiff', 'repulsivepower', 'attractivepow', 'sum', 'phi', 'psidiff', 'axis', 'return', 'reward', 'def', 'l2dist', 'return', 'l2', 'distance', 'pair', 'input', 'return', 'sum', 'def', 'omputedistance', 'l2dist', 'return', 'distance', 'pair', 'collection', 'input', 'return', 'vmap', 'vmap', 'distfn', 'none', 'none', 'list', 'intrinsic', 'reward', 'def', 'weightcumulant', 'lagrange', 'latent', 'extrinsiccumulant', 'ntr', 'weight', 'cumulant', 'use', 'lagrange', 'multiplier', 'lagrange', 'lagrange', 'latent', 'latent', 'xtr', 'ant', 'ntr', 'ant', 'return', 'extrinsic', 'reward', 'intrinsicreward', 'sigmoid', 'lagrange', 'l', 'eat', 'ts', 'latent', 'siglagrange', 'diversity', 'reward', 'latent', 'maximize', 'extrinsic', 'reward', 'ntr', 'ts', 'latent', 'return', 'tent', 'lagrang', 'e', 'intrin', 'iccumulant', 'l', 'eat', 'ts', 'insicc', 'ulant', 'list', 'weight', 'cumulant', 'tb', 'def', 'lagrange', 'optimalityratio', 'loss', 'function', 'lagrange', 'multiplier', 'args', 'lagrange', 'lagrange', 'r', 'move', 'average', 'optimalityratio', 'sigmoid', 'lagrange', 'return', 'sum', 'r', 'optimalityratio', 'list', 'lagrange', 'loss', 'function', 'motion', 'ﬁgure', 'motion', 'ﬁgure', 'create', 'following', 'manner', 'give', 'trajectory', 'frame', 'compose', 'video', 'f1', 'ﬁrst', 'trim', 'sample', 'trajectory', 'point', 'interest', 'time', 'fnm', 'always', 'use', 'trimming', 'set', 'policy', 'sub', 'ﬁgure', 'ﬁgure', 'sub', 'sample', 'frame', 'trimmed', 'sequence', 'frequency', 'fnp', 'fn2p', 'take', 'maximum', 'sequence', 'present', 'max', 'image', 'example', 'simply', 'correspond', 'm30', 'rangen', 'p', 'npmaxfindice', 'create', 'effect', 'motion', 'single', 'ﬁgure', 'object', 'high', 'value', 'background', 'hyperparameter', 'hyperparameter', 'table', 'share', 'environment', 'bipedal', 'domain', 'learning', 'rate', 'set', 'learner', 'frame', '×', 'report', 'domino', 'speciﬁc', 'hyperparameter', 'table', 'hyperparameter', 'replay', 'capacity', 'learning', 'rate', 'learner', 'frame', 'discount', 'factor', 'regularization', 'weight', 'bπ', 'policy', 'loss', 'weight', 'value', 'loss', 'weight', 'replay', 'batch', 'size', 'online', 'batch', 'size', 'sequence', 'length', 'optimizer', 'value', '×', '×', 'rmsprop', 'table', 'general', 'hyperparameter', 'control', 'suite', 'optimality', 'ratio', 'lagrange', 'initialization', 'lagrange', 'learning', 'rate', 'lagrange', 'optimizer', 'decay', 'factor', 'table', 'domino', 'hyperparameter', 'additional', 'experiment', 'result', 'b1', 'motion', 'ﬁgure', 'present', 'motion', 'ﬁgure', 'similar', 'fig', 'domain', 'see', 'fig', 'video', 'associate', 'ﬁgure', 'find', 'separate', 'zip', 'ﬁle', 'ﬁgure', 'present', 'police', 'discover', 'domino', 'associate', 'reward', 'white', 'text', 'repulsive', 'objective', 'optimality', 'ratio', 'set', 'see', 'policy', 'exhibit', 'different', 'gait', 'ﬁgure', 'also', 'present', 'distance', 'expect', 'feature', 'discover', 'policy', 'measure', 'norm', 'addition', 'row', 'use', 'dark', 'black', 'frame', 'indicate', 'index', 'policy', 'close', 'expect', 'feature', 'row', 'highlight', 'column', 'ψj2', 'figure', 'qd', 'walkerstand', 'leave', 'policy', 'correspond', 'reward', 'distance', 'norm', 'successor', 'feature', 'policy', 'figure', 'qd', 'walkerwalk', 'leave', 'policy', 'correspond', 'reward', 'distance', 'norm', 'successor', 'feature', 'policy', 'figure', 'qd', 'dogstand', 'leave', 'policy', 'correspond', 'reward', 'distance', 'norm', 'successor', 'feature', 'policy', 'figure', 'qd', 'dogwalk', 'leave', 'policy', 'correspond', 'reward', 'distance', 'norm', 'successor', 'feature', 'policy', 'additional', 'quality', 'diversity', 'result', 'present', 'additional', 'experimental', 'result', 'evaluate', 'tradeoff', 'quality', 'diversity', 'use', 'scatter', 'plot', 'introduce', 'section', 'yaxis', 'show', 'episode', 'return', 'diversity', 'score', 'correspond', 'hausdorff', 'distance', 'eq', 'xaxis', 'topright', 'corner', 'diagram', 'represent', 'diverse', 'high', 'quality', 'policy', 'ﬁgure', 'present', 'sweep', 'hyperparameter', 'use', 'color', 'marker', 'indicate', 'value', 'scatter', 'plot', 'report', 'conﬁdence', 'interval', 'correspond', 'seed', 'indicate', 'crosse', 'surround', 'point', 'quality', 'diversity', 'walkerwalk', 'fig', 'show', 'experimental', 'result', 'domino', 'walkerwalk', 'domain', 'consistent', 'fig', 'show', 'similar', 'result', 'walkerstand', 'show', 'constraint', 'mechanism', 'work', 'expect', 'different', 'set', 'size', 'optimality', 'ratio', 'different', 'task', 'figure', 'qd', 'scale', 'result', 'walkerwalk', 'task', 'leave', 'number', 'policy', 'optimality', 'ratio', 'walkerwalk', 'repulsive', 'reward', 'center', 'vdw', 'reward', 'right', 'optimality', 'ratio', 'vdw', 'target', 'distance', 'quality', 'diversity', 'smerl', 'domino', 'fig', 'show', 'experimental', 'result', 'walkerstand', 'smerl', 'comparison', 'domino', 'smerl', 'appropriately', 'tune', 'policy', 'conﬁguration', 'ﬁnd', 'solution', 'upperright', 'qd', 'border', 'however', 'ﬁnd', 'good', 'cd', 'transfer', 'conﬁguration', 'choice', 'enable', 'agent', 'set', 'diverse', 'policy', 'produce', 'set', 'diversity', 'set', 'size', 'figure', 'scale', 'smerl', 'leave', 'domino', 'right', 'walkerstand', 'set', 'size', 'indicate', 'marker', 'color', 'correspond', 'optimality', 'ratio', 'smerl', 'set', 'tune', 'use', 'set', 'size', 'policy', 'see', 'leave', 'choice', 'scale', 'well', 'set', 'size', 'regardless', 'optimality', 'ratio', 'policy', 'optimize', 'extrinsic', 'reward', 'expense', 'diversity', 'discount', 'state', 'occupancy', 'run', 'experiment', 'report', 'fig', 'lagrangian', 'method', 'report', 'result', 'fig', '11b', 'observe', 'use', 'predict', 'discount', 'feature', 'make', 'signiﬁcant', 'difference', 'performance', 'mix', 'time', 'dm', 'control', 'suite', 'much', 'short', 'episode', 'length', 'bias', 'empirical', 'feature', 'average', 'small', 'b', 'figure', 'domino', 'discount', 'state', 'occupancy', 'additional', 'network', 'head', 'train', 'predict', 'successor', 'feature', 'use', 'instead', 'average', 'feature', 'ψavg', 'compute', 'diversity', 'reward', 'discount', 'extrinsic', 'value', 'use', 'constraint', 'instead', 'average', 'reward', 'dash', 'line', 'signify', 'training', 'objective', 'number', 'policy', 'optimality', 'ratio', 'walkerstand', 'domino', 'consistent', 'fig', 'c', 'limitation', 'diversity', 'increase', 'decrease', 'inspect', 'fig', 'fig', '11b', 'fig', 'observe', 'diversity', 'score', 'increase', 'low', 'optimality', 'ratio', 'recall', 'optimality', 'ratio', 'α', 'speciﬁes', 'feasibility', 'region', 'stateoccupancy', 'space', 'set', 'αoptimal', 'policy', 'thus', 'size', 'space', 'increase', 'observe', 'diverse', 'set', 'small', 'value', 'intuition', 'correct', 'experiment', 'always', 'eg', 'fig', 'possible', 'explanation', 'lagrange', 'multiplier', 'solution', 'seek', 'low', 'value', 'satisﬁes', 'constraint', 'get', 'diversity', 'ﬁnd', 'solution', 'satisfy', 'constraint', 'almost', 'equality', 'e', 'size', 'level', 'set', '≥', 'αv∗', 'e', 'necessarily', 'increase', 'low', 'value', 'feasibility', 'set', 'vi', 'vi', 'e', 'explanation', 'walkerwalk', 'fig', 'easy', 'ﬁnd', 'diverse', 'walk', 'diverse', 'half', 'walk', 'explain', 'half', 'walk', 'less', 'stable', 'hard', 'ﬁnd', 'diverse', 'mode', 'e', 'instead', 'vi', 'e', 'αv∗', 'e', '∼', 'αv∗', 'e', 'αv∗', 'feature', 'possible', 'limitation', 'approach', 'diversity', 'deﬁne', 'environment', 'feature', 'partially', 'address', 'concern', 'section', 'show', 'possible', 'learn', 'policy', 'approach', 'use', 'embedding', 'nn', 'feature', 'future', 'work', 'plan', 'scale', 'approach', 'high', 'dimensional', 'domain', 'study', 'auxiliary', 'loss', 'add', 'learn', 'good', 'representation', 'diversity', 'additional', 'kshot', 'experiment', 'd1', 'control', 'suite', 'next', 'report', 'additional', 'result', 'kshot', 'adaptation', 'control', 'suite', 'fig', 'report', 'absolute', 'value', 'achieve', 'method', 'obtain', 'exact', 'setup', 'fig', 'report', 'rmethod', 'method', 'instead', 'rmethodrbaseline', 'fig', 'additionally', 'report', 'rbaseline', 'single', 'policy', 'baseline', 'blue', 'fig', 'inspect', 'fig', 'see', 'method', 'deteriorate', 'performance', 'magnitude', 'perturbation', 'increase', 'however', 'performance', 'deteriorate', 'slow', 'method', 'also', 'see', 'performance', 'diversity', 'baseline', 'similar', 'learn', 'policy', 'red', 'u', 'p', 'e', 'c', 'n', 'g', 'r', 'n', 'g', 'e', 'v', 'e', 'r', 'r', 'e', 'r', 'single', 'policy', 'blue', 'indicate', 'maximize', 'extrinsic', 'reward', 'ﬁnd', 'policy', 'policy', 'figure', 'kshot', 'adaptation', 'control', 'suite', 'similar', 'figure', 'report', 'absolute', 'rather', 'relative', 'return', 'next', 'inspect', 'wide', 'range', 'hyper', 'parameter', 'smerl', 'multiobjective', 'method', 'concretely', 'domino', 'smerl', 'smerl', 'multi', 'objective', 'method', 'train', 'policy', 'value', 'correspond', 'value', 'consider', 'fig', 'inspect', 'fig', 'see', 'good', 'method', 'overall', 'domino', 'smerl', 'also', 'see', 'domino', 'smerl', 'consistently', 'outperform', 'multiobjective', 'baseline', 'many', 'hyper', 'parameter', 'value', 'consistent', 'result', 'fig', 'suggest', 'multiobjective', 'method', 'tend', 'diverse', 'high', 'performing', 'fail', 'capture', 'good', 'balance', 'lastly', 'reasonable', 'smerl', 'domino', 'perform', 'similar', 'valid', 'solution', 'cmdp', 'however', 'smerl', 'come', 'additional', 'hyper', 'parameter', 'tricky', 'tune', 'situation', 'example', 'try', 'tune', 'base', 'result', 'vanilla', 'domain', 'pick', 'upperright', 'point', 'fig', 'lead', 'choose', 'smerl', 'instead', 'lagrange', 'multiplier', 'formulation', 'domino', 'challenge', 'extra', 'hyper', 'parameter', 'figure', 'kshot', 'suite', 'similar', 'figure', 'report', 'wide', 'range', 'hyper', 'parameter', 'smerl', 'multiobjective', 'd2', 'bipedalwalker', 'h1', 'bipedalwalker', 'environment', 'perturb', 'morphology', 'terrain', 'perturb', 'morphology', 'follow', 'specify', 'set', 'rescale', 'factor', 'speciﬁcally', 'leg', 'make', 'rectangle', 'predeﬁne', 'width', 'height', 'parameter', 'leg1', 'leg2', 'w1', 'generate', 'perturb', 'morphology', 'deﬁne', 'scaling', 'range', 'η', 'withing', 'uniformly', 'sample', 'scaling', 'factor', 'η', 'perturb', 'environment', 'deﬁne', 'rescale', 'default', 'parameter', 'cid102leg1', 'h2', 'value', 'perturbation', 'find', 'table', '1w1', '1ν2', '1ν2', 'νj', 'h1', 'r', 'u', 'e', 'r', 'e', 'r', 'u', 'e', 'r', 'e', 'l', 'w', 'motor', 'failure', 'duration', 'thigh', 'length', 'length', 'joint', 'damp', 'contact', 'friction', 'single', 'policy', 'baseline', 'domino', 'smerl', 'diversity', 'multiobjective', 'r', 'u', 'e', 'r', 'e', 'r', 'u', 'e', 'r', 'e', 'l', 'w', 'motor', 'failure', 'duration', 'thigh', 'length', 'torso', 'length', 'joint', 'damp', 'contact', 'friction', 'single', 'policy', 'baseline', 'domino', 'objective', 'multi', 'objective', 'multi', 'objective', 'multi', 'objective', 'multi', 'objective', 'multi', 'objective', 'multi', 'objective', 'multi', 'objective', 'smerl', 'smerl', 'smerl', 'smerl', 'perturbation', 'type', 'perturbation', 'scale', 'parameter', 'value', 'morphology', 'stump', 'width', 'pit', 'width', 'stair', 'width', 'table', 'bipedal', 'perturbation', 'scale', 'value', 'terrain', 'change', 'selectively', 'enable', 'available', 'obstacle', 'available', 'openai', 'gym', 'implementation', 'stump', 'pit', 'stair', 'obstacle', 'specify', 'perturbation', 'interval', 'η', 'interval', 'determine', 'upper', 'bound', 'obstacle', 'height', 'width', 'environment', 'generate', 'terrain', 'episode', 'detail', 'see', 'hardcore', 'implementation', 'bipedal', 'environment', 'note', 'stair', 'ﬁxe', 'upper', 'bind', 'number', 'step', 'environment', 'generate', 'go', 'evaluate', 'adaptation', 'ﬁrst', 'train', 'agent', 'independently', 'bipedalwalkerv3', 'environment', 'use', 'ﬂat', 'terrain', 'evaluate', 'train', 'agent', 'sample', 'random', 'perturbation', 'environment', 'speciﬁcally', 'type', 'perturbation', 'morphology', 'pit', 'stump', 'stair', 'value', 'scale', 'parameter', 'randomly', 'sample', 'perturbation', 'run', 'option', 'episode', 'adaptation', 'take', 'form', 'use', 'ﬁrst', 'episode', 'estimate', 'option', 'high', 'episode', 'return', 'use', 'evaluation', 'remain', 'episode', 'figure', 'kshot', 'adaptation', 'show', 'performance', 'degrade', 'morphology', 'deform', 'domino', 'exhibit', 'great', 'adaptability', 'evidence', 'less', 'severe', 'degradation', 'performance', 'term', 'morphology', 'ﬁnd', 'gradual', 'decline', 'performance', 'increase', 'degree', 'deformation', 'similar', 'control', 'suite', 'diversity', 'beneﬁcial', 'help', 'agent', 'adapt', 'impervious', 'change', 'term', 'terrain', 'perturbation', 'abrupt', 'impact', 'agent', 'performance', 'diversity', 'prevent', 'signiﬁcant', 'drop', 'performance', 'still', 'beneﬁcial', 'adapt', 'stump', 'pit', 'negatively', 'impact', 'performance', 'case', 'stair', 'e', 'compute', 'infrastructure', 'run', 'experiment', 'use', 'distribute', 'infrastructure', 'implement', 'run', 'take', 'approximately', 'hour', 'complete', 'compute', 'infrastructure', 'base', 'actorlearner', 'decomposition', 'multiple', 'actor', 'generate', 'experience', 'parallel', 'experience', 'channel', 'learner', 'allow', 'run', 'experiment', 'modality', 'ﬁrst', 'modality', 'follow', 'actor', 'program', 'distribute', 'multiple', 'cpu', 'machine', 'stepping', 'environment', 'network', 'inference', 'happen', 'cpu', 'datum', 'generate', 'actor', 'program', 'process', 'r', 'l', 'l', 'diversity', 'diversity', 'e', 'p', 'b', 'morphology', 'scale', 'size', 'r', 'e', 'l', 'l', 'e', 'p', 'b', 'pit', 'size', 'stair', 'step', 'size', 'batch', 'single', 'learner', 'use', 'gpu', 'alternatively', 'actor', 'learner', 'colocate', 'single', 'machine', 'host', 'equip', 'cpu', 'core', 'connect', 'tpu', 'core', 'minimize', 'effect', 'global', 'interpreter', 'lock', 'actorthread', 'interact', 'batched', 'environment', 'expose', 'single', 'special', 'environment', 'take', 'batch', 'action', 'return', 'batch', 'observation', 'scene', 'step', 'environment', 'batch', 'actor', 'thread', 'share', 'tpu', 'core', 'perform', 'inference', 'network', 'send', 'batch', 'ﬁxed', 'size', 'trajectory', 'length', 'queue', 'learner', 'thread', 'take', 'batch', 'trajectory', 'split', 'remain', 'tpu', 'core', 'compute', 'parameter', 'update', 'average', 'reduce', 'participate', 'core', 'update', 'parameter', 'send', 'actor', 'tpu', 'core', 'fast', 'device', 'device', 'channel', 'soon', 'new', 'parameter', 'available', 'minimal', 'unit', 'replicate', 'multiple', 'host', 'connect', 'cpu', 'core', 'tpu', 'core', 'case', 'learner', 'update', 'sync', 'average', 'core', 'fast', 'device', 'device', 'communication']"
"Avoid Overfitting User Specific Information in Federated Keyword
  Spotting","[{'href': 'http://arxiv.org/abs/2206.08864v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2206.08864v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-06-17 16:05:35,"Avoid Overﬁtting User Speciﬁc Information in Federated Keyword Spotting

Xin-Chun Li1, Jin-Lin Tang1, Shaoming Song2, Bingshuai Li2, Yinchuan Li2, Yunfeng Shao2,
Le Gan1, De-Chuan Zhan1

1State Key Laboratory for Novel Software Technology, Nanjing University
2Huawei Noah’s Ark Lab
{lixc, tangjl}@lamda.nju.edu.cn, ganle@nju.edu.cn, zhandc@nju.edu.cn,
{shaoming.song, libingshuai, liyinchuan, shaoyunfeng}@huawei.com

2
2
0
2

n
u
J

7
1

]

G
L
.
s
c
[

1
v
4
6
8
8
0
.
6
0
2
2
:
v
i
X
r
a

Abstract

Keyword spotting (KWS) aims to discriminate a speciﬁc wake-
up word from other signals precisely and efﬁciently for different
users. Recent works utilize various deep networks to train KWS
models with all users’ speech data centralized without consid-
ering data privacy. Federated KWS (FedKWS) could serve as
a solution without directly sharing users’ data. However, the
small amount of data, different user habits, and various ac-
cents could lead to fatal problems, e.g., overﬁtting or weight
divergence. Hence, we propose several strategies to encourage
the model not to overﬁt user-speciﬁc information in FedKWS.
Speciﬁcally, we ﬁrst propose an adversarial learning strategy,
which updates the downloaded global model against an overﬁt-
ted local model and explicitly encourages the global model to
capture user-invariant information. Furthermore, we propose an
adaptive local training strategy, letting clients with more train-
ing data and more uniform class distributions undertake more
local update steps. Equivalently, this strategy could weaken
the negative impacts of those users whose data is less quali-
ﬁed. Our proposed FedKWS-UI could explicitly and implicitly
learn user-invariant information in FedKWS. Abundant exper-
imental results on federated Google Speech Commands verify
the effectiveness of FedKWS-UI.
Index Terms: keyword spotting, federated learning, data het-
erogeneity, user-invariant

1. Introduction

Deep learning has been successfully applied to automatic
speech recognition (ASR) [1, 2], facilitating the emergence of
intelligent voice assistants (e.g., Amazon Alexa). To wake up
the smart assistant, some predeﬁned keywords (e.g., “Alexa”)
need to be identiﬁed precisely from users’ speech recordings,
i.e., keyword spotting (KWS) [3, 4]. This identiﬁcation process
must be efﬁcient to complete, and the utilized models should
have minimal memory footprint. Furthermore, the KWS pro-
cess should be robust to users with various accents or preferred
spoken words.

Recent works utilize various deep networks for KWS [4,
5, 6, 7, 8]. These methods take a data centralized training
style based on the publicly available benchmark such as Google
Speech Commands [9]. However, there may be signiﬁcant pri-
vacy implications in sharing users’ audio recordings, which re-
quires a data decentralized training style for privacy protec-

Supported by National Natural Science Foundation of China (Grant
No.
41901270), NSFC-NRF Joint Research Project under Grant
61861146001, and Natural Science Foundation of Jiangsu Province
(Grant No. BK20190296). Thanks to Huawei Noah’s Ark Lab Net-
MIND Research Team for funding this research. De-Chuan Zhan is the
corresponding author. Email: zhandc@nju.edu.cn

tion. Federated learning (FL) [10, 11] has been effectively ap-
plied for communication efﬁcient decentralized training with
basic privacy protection. Although FL could be directly ap-
plied to decentralized KWS training, the non-independent and
identically distributed data (non-i.i.d. data) poses many chal-
lenges [12, 13]. Non-i.i.d. in KWS refers to the fact that some
users only own a small amount of data (i.e., quantity skew),
users tend to use different spoken words (i.e., label distribution
skew), and users usually have accents (i.e., feature distribution
skew).

This paper investigates FedKWS on Google Speech Com-
mands [9] with several popular network architectures. Com-
pared with centralized training, we observe a signiﬁcant per-
formance degradation in FedKWS due to non-i.i.d. data.
In
fact, the small amount of data and the distribution skew problem
make the downloaded global model easily overﬁt user-speciﬁc
information. For example, the feature extractor mistakenly
takes a user’s accent as an important factor, or the classiﬁca-
tion layer is biased towards a user’s commonly spoken words.
To solve these challenges and enhance the generalization per-
formance of the federated model, we propose several strategies
to avoid the local model overﬁtting user-speciﬁc information.

2. Related Works

Our work is closely related to keyword spotting (KWS) [4, 3]
and federated learning (FL) [10, 11, 14]. Current works for-
mulate KWS as a classiﬁcation problem, aiming to identify
whether a short speech recording is a speciﬁc word, silence, or
unknown. Considering the success of deep learning, CNN has
been applied to KWS [4]. Depth-separable CNN (DSCNN) [5]
is applied to obtain the goal of small footprint memory, and
residual network (ResNet) [7] is utilized to enhance perfor-
mances. Recurrent neural networks with multi-head atten-
tion (MHAttRNN) [6, 8] and varieties of transformers (Trans-
former) [8, 15] have also been applied to KWS and obtain SOTA
results. Some other advanced techniques in deep learning have
also been veriﬁed helpful in KWS [16]. FL has also been ap-
plied to KWS for decentralized training [17, 18]. [17] conducts
extensive experiments of FedAvg [10] on “Hey Snips” dataset
and uses an adaptive averaging strategy for global model aggre-
gation as done in [19]. The work [18] investigates data aug-
mentation and distillation in FedKWS for overcoming resource
constraints and example labeling. FL studies have also been
presented in ASR [20, 21, 22]. Compared with these studies, we
primarily focus on the non-i.i.d. data challenge in FedKWS and
propose a novel method to focus on extracting user-invariant in-
formation. We investigate our methods with various network
architectures and show that our approach is universal.

 
 
 
 
 
 
3. Background of Federated Learning

(cid:80)

k∈St

ˆψk

(cid:80)K

FedAvg [10]: Suppose we have K clients and each client owns
a data distribution Dk = P k(x, y), k ∈ [K]. FL aims to op-
k=1 pkL(Dk; ψ), where ψ denotes the global
timize minψ
parameters, pk denotes the weight of each client. FedAvg [10]
solves this problem via multiple communication rounds of lo-
cal and global procedures. During local procedure, a partial
set of clients St download the global model ψt and update it
on their local data for multiple steps. During global proce-
dure, the server collects these updated local models (denoted as
ˆψk
t , k ∈ St) and aggregates them via parameter averaging, i.e.,
ψt+1 ← 1
t . t denotes the communication round.
|St|
These two procedures will iterate T rounds until convergence.
Non-I.I.D. Data: The users’ data in FL are often naturally
heterogeneous, e.g., the speech data in Google Speech Com-
mands [9] are collected from users with various accents. As de-
clared in [12], the local update direction will diverge a lot from
the global one due to non-i.i.d. data, making the model aggre-
gation inaccurate. FedOpt [19] utilizes an adaptive optimization
strategy on the server instead of a simple parameter averaging.
FedRS [23] speciﬁes the challenge of label shift across clients
and proposes restricted softmax as the solution. FedProx [13]
and FedMMD [24] add regularization to prevent local models
from being updated too away, which could decrease the weight
divergence for better aggregation. Although some FL methods
(e.g., FedProx [13], FedDyn [25], MOON [26]) could also elab-
orate a regularization effect during local procedures, they only
stay on the parameter or the intermediate feature levels. By
contrast, we adversarially update the global model against an
overﬁtted local model and regularize the local procedure on the
functional level. Furthermore, we design an adaptive local train-
ing procedure from the system scheduling level.

4. Proposed Methods

This section proposes two strategies to prevent the global model
from overﬁtting user-speciﬁc information (e.g., accents or fa-
vorite spoken words) in FedKWS.
Adversarial Learning against Overﬁtted models (ALO):
Clients update the downloaded global model on their data dur-
ing the local procedure, which could overﬁt some user-speciﬁc
information. Speciﬁcally, the local data distribution P k(x, y)
may diverge signiﬁcantly from the global data distribution. Ac-
cording to some previous works [27, 28, 29], the lower/higher
layers of a neural network tend to be inﬂuenced signiﬁcantly by
feature/label distribution skew, i.e., various P k(x) or P k(y).
FedKWS simultaneously faces these two kinds of distribution
skew (e.g., accents and favorite spoken words), making the
complete model biased towards a speciﬁc user during the lo-
cal procedure. Hence, we must regularize the local training
from the functional perspective instead of focusing on speciﬁc
neural network layers. We resort to private-shared models and
adversarially update the global model (shared among users)
against overﬁtted local models (private for each user). Private-
shared models are utilized in some recent FL solutions [30, 31].
Speciﬁcally, we build private models ψk
p , k ∈ [K] for each
client. We ﬁrst train private models with the cross-entropy loss
L(Dk; ψk
c=1 I{yi = c} log[fp(xi)]c],
where I{·} is the indicator function and fp(·) is the prediction
function based on private model ψk
p that outputs a probability
distribution. After abundant training steps, we expect this pri-
vate model to overﬁt user-speciﬁc data information. Then, we

p ) = Exi,yi∼Dk [− (cid:80)C

Figure 1: Left: data heterogeneity in federated Google Speech
Commands. We only plot 20 clients (users) in task 35. Right:
number of samples and class distribution entropy of each client
(user) in task 12 and 35 (each point shows a client).

train the global model with the following loss:

(cid:34)

Lls = Exi,yi

−

C
(cid:88)

[(1 − µ)I{yi = c} + µ/C] log[f (xi)]c

(cid:35)

,

c=1

Ladv = −
(cid:124)(cid:123)(cid:122)(cid:125)
negative

Exi,yi

(cid:34)

−

C
(cid:88)

[fp(xi)]c log[f (xi)]c

(cid:35)

,

c=1

L(Dk; ψk) = Lls(Dk; ψk) + λLadv(Dk; ψk),

(1)

(2)

(3)

where we omit the communication round index t and some
other symbols for simpliﬁcation. fp(·) represents the func-
tion of the overﬁtted private model while f (·) for the down-
loaded global model. Eq. (2) could be seen as “negative distil-
lation”, which could push the global model’s prediction f (xi)
away from overﬁtted areas. Eq. (2) follows the formula of dis-
tillation [32, 33, 34] but works signiﬁcantly different. Label
smoothing in Eq. (1) could also regularize the global model not
be too over-conﬁdent on a speciﬁc user’s data. We investigate
the hyperparameters of µ and λ in ablation studies.
Adaptive Local Training (ALT): Due to data heterogeneity,
both amount imbalance and class imbalance could occur in
clients’ data. The former implies that different clients may own
various numbers of training samples. The second one refers
to that label distributions may diverge across clients. These
two types of imbalance on Google Speech Commands [9] are
shown in Figure 1.
Intuitively, few training samples could
lead to overﬁtting, and imbalanced data could bias the model
towards identifying a user’s favorite words. Hence, we en-
courage clients who own more training data and more uni-
form class distributions to undertake more local updates. For-
mally, in FedAvg [10], every selected client uniformly takes
E local training steps without considering the data quality.
Assume the kth client owns nk training samples and the
class distribution is qk ∈ RC with (cid:80)C
c=1 qk,c = 1 and
qk,c ≥ 0, ∀c. C is the total number of classes. We cal-
culate the normalized amount of training samples as nk =
nk/ maxK
j=1 nj ∈ [0, 1], and the normalized class entropy as

Statistics (C=12)
2
0
8
1

1

0

0

6

0

0

4

0

1

4

1

Number of Samples

Statistics (C=35)
0
0
5
5

0

0

0

0

2

2

1

1

Client-Class Distribution (C=35)
0

5

1

5

1

0

2

0

0

2

y
p
o
r
t
n
E

n
o
i
t
u
b
i
r
t
s
i
D
s
s
a
C

l

y
p
o
r
t
n
E

n
o
i
t
u
b
i
r
t
s
i
D
s
s
a
C

l

2.49

2.0

1.5

1.0

0

0

5

3.56

3.0

2.5

2.0

1.5

1.0

1

0

5

10

15

20

25

30

34

x
e
d
n
I

s
s
a
C

l

Sampled 20 Clients

Number of Samples

3.49

2.67

1.85

1.03

0.21

4.70

3.56

2.43

1.29

0.15

6

1

3

Table 1: Detail information of federated Speech Commands.

Table 3: Comparisons on FA and FR rate. The lower the better.

C
12
35

K
2,234
2,434

N
45.6k
105.5k

Avg.nk Max.nk

20.4
43.3

141
316

M
4.9k
11.0k

FedAvg [10, 18]
0.27
5.03

FedOpt [19, 17]
0.32
3.19

FedKWS-UI
0.23
2.78

FA
FR

Table 2: Detail of networks and centralized training results.

Num.of.Params

Centralized Acc.
C = 12 C = 35 C = 12 C = 35
96.95
169K
97.05
228K
97.31
238K
97.14
232K

97.19
97.31
97.89
96.21

173K
232K
239K
234K

DSCNN [5]
MHAttRNN [6]
ResNet [7]
Transformer [8]

ek = (− (cid:80)C
c=1 qk,c log qk,c)/ log C ∈ [0, 1]. Then we calcu-
late the harmonic mean of nk and ek, i.e., rk = 2nkek/(nk +
ek). We use rk ∈ [0, 1] to measure clients’ utility in FL, and
we heuristically let clients with larger rk contribute more to FL.
That is, we reallocate the computation resources among clients
via allowing the kth client take on r0 ∗ rk ∗ E gradient steps,
where the determination of r0 should satisfy (cid:80)K
k=1 r0∗rk∗E ≈
K ∗ E for conservation. Easily, r0 = K/ (cid:80)K
k=1 rk. Although
the computation ability of clients should also be considered, we
focus on non-i.i.d. data in this work and leave it as future work.

5. Experiments

Datasets: We name the proposed method as “Federated KWS
with User-Invariant information” (FedKWS-UI), and investi-
gate it on Google Speech Commands [9]1 (recommended by
FedScale [35]) to identify whether a 1s-long speech recording is
a word, silence, or unknown. The benchmark contains two tasks
with 12 classes (10 words, silence, unknown) and 35 classes (35
words). The two tasks contain 2,234 and 2,434 users. We split
the data into corresponding clients with each user as a client.
The number of total training samples (N ), the training samples
of each client on average (Avg.nk), the number of test samples
(M ) are listed in Table 1. The class distributions of randomly
selected 20 clients in task C = 35 are shown in left of Fig-
ure 1. Larger circles correspond to more samples. The train
and test data is split via the provided lists in Google Speech
Commands. We extract 40 MFCC features for each 30ms win-
dow frame with a stride of 10ms. We also follow the settings
in Google Speech Commands: performing random time-shift
of Y ∼ [−100, 100] milliseconds and adding 0.1 volume back-
ground noise with a probability of 0.8.
Networks and Centralized Training: We investigate vari-
ous network architectures and moderately modify them to keep
nearly the same number of parameters. We use DSCNN [5]
with 172 channels, MHAttRNN [6, 8] with 4 heads and 80 hid-
den neurons, ResNet [7] with 15 layers and 45 channels in each
basic block, Transformer [8] with 4 layers and a model dimen-
sion of 96. We ﬁrst use these networks for centralized train-
ing. For DSCNN, MHAttRNN and ResNet, we utilize SGD
optimizer with momentum 0.9, and we vary the learning rate
in {0.1, 0.05, 0.03, 0.01} and select the best result. For Trans-
former, we utilize AdamW optimizer and vary learning rate in

1https://pytorch.org/audio/stable/datasets.html

{0.005, 0.002, 0.0008}. We set batch size as 128. The number
of network parameters and the accuracies on test data are shown
in Table 2. We do not obtain SOTA results via Transformer be-
cause we only use 4 layers with 0.23M parameters while [8]
uses a network with up to 5.4M parameters.
FedKWS: For FedKWS, we split the training data in Google
Speech Commands via the provided user IDs. The test data
is still used to evaluate the generalization ability of the aggre-
gated model. We plot the statistics of clients’ number of sam-
ples and class distribution entropy at the right of Figure 1. We
compare FedKWS-UI with FedAvg [10] (used in [18]), Fed-
Prox [13], FedMMD [24], FedOpt [19] (used in [17]). For
all methods, we use a batch size of 32, local training steps
E = 50 and run 300 rounds. We also vary the learning rate
as aforementioned and take the best one for comparison. Addi-
tionally, for FedProx and FedMMD, we vary the regularization
coefﬁcient in {0.0001, 0.001, 0.01}. For FedOpt, we vary the
global optimizer in {SGD, Adam} and the global learning rate
in {1.0, 0.1} and {0.001, 0.0001}, respectively. For FedKWS-
UI, we use r0 = 3.5, 5.0 for C = 12, 35, and show r0 ∗ rk
values of all clients via the shades of color at the right of Fig-
ure 1. The max and min values are shown at the color bar, and
the top-right points (clients) tend to have larger r0 ∗ rk. We uti-
lize µ = 0.2 and λ = 0.001 in FedKWS-UI (Eq. (1), Eq. (3)).
We record the accuracy on the global test set every 3 rounds
and plot the convergence curves in Figure 2. First, we can
clearly observe that the decentralized performances drop a lot
compared with centralized training. For example, all of the
compared methods could only obtain accuracy as high as 86.39
on task 12, far away from the centralized training (97.89). Then,
comparing the network architectures, we could ﬁnd that MHAt-
tRNN tends to obtain higher performances while Transformer
performs worst. We guess that MHAttRNN could be more ro-
bust to the random time-shift because it directly computes the
sequential information, and the attention mechanism could pre-
cisely capture the important signals. Furthermore, FedProx and
FedMMD add regularization during local training procedures
on the parameter and intermediate features, which perform not
so well. Overall, FedKWS-UI could lead to better results on
all of these architectures, particularly on ResNet and Trans-
former, verifying the versatility of our methods. Signiﬁcantly,
FedKWS-UI surpasses all compared methods by a large margin
on task 12 with DSCNN, ResNet, and Transformer. For exam-
ple, FedKWS-UI could boost the Transformer performances on
task 12 from 72.71 to 79.06. We also evaluate the false accept
(FA) and false reject (FR) rate as done in [17, 18]. In task 12, we
take the 10 words as positive classes and average their FA rates,
while the silence and unknown as negative classes. We do not
adjust the prediction conﬁdence threshold to control the FA and
directly report FA and FR with the predictions. We use DSCNN
and calculate the FA and FR rate of the ﬁnal aggregated model.
We show the results in Table 3. We ﬁnd that FedKWS-UI could
obtain fewer false alarms/rejections.
Ablation Studies: We investigate the effects of components
in FedKWS-UI. First, we set µ = 0 and λ = 0 to omit the
part of adversarial learning against overﬁtted models (ALO)

Figure 2: Comparison results on federated Google Speech Commands. Rows show the results on task 12 and 35, and columns show
results of four utilized networks. The legends also show the average accuracy of the ﬁnal 5 communication rounds.

sults on task 35 to further show the plausibility and advantage
of the proposed strategies. Speciﬁcally, we ﬁrst train a well-
performed KWS model (θ0) on the centralized training set (test
accuracy up to 93.0%). Then, we respectively update θ0 on
an inferior and qualiﬁed user’s data for 20 epochs. The infe-
rior user owns only 100 samples and the classes are imbalanced
(i.e., the bottom-left user shown in the right part of Figure 1),
while the qualiﬁed user owns about 250 samples and the classes
are more balanced (i.e., the top-right user shown in the right
part of Figure 1). The updated models are denoted as θ1 and θ2.
Then, we plot the performance landscape of the interpolation
θ0 + γ1(θ1 − θ0) + γ2(θ2 − θ0) within the grid space where
γ1 ∈ [−0.1, 1.1], γ2 ∈ [−0.1, 1.1]. The left part of Figure 4
shows that updating θ0 on the qualiﬁed user’s data keeps the
generalization ability of the global model while the result on the
inferior user’s data becomes worse. Hence, it is rational that our
proposed ALT encourages qualiﬁed users to contribute more to
FedKWS. Additionally, for the inferior user, we utilize the pro-
posed ALO to train another model ˆθ1 against the overﬁtted θ1,
and ˆθ1 performs better as shown on the right of Figure 4. The
interpolation landscape along the inferior user’s data becomes
smoother with ALO, beneﬁting the model aggregation proce-
dure in FL. This veriﬁes the advantage of the proposed ALO.
Overall, FedKWS-UI could enhance the generalization ability
of the federated model even with few or skewed samples.

6. Conclusion

We investigate popular networks for FedKWS, where the
data heterogeneity leads to signiﬁcant performance degradation
compared with centralized training. We propose to learn user-
invariant information via adversarial learning against overﬁtted
local models and a computation re-allocation strategy named
adaptive local training. These two strategies could avoid overﬁt-
ting user-speciﬁc information during local training and facilitate
model aggregation. Experimental results verify the superiori-
ties of our proposed FedKWS-UI. Future works will extend this
work to streaming KWS [6] and utilize differential privacy [36]
to satisfy stricter privacy requirements.

Figure 3: Ablation studies of only using ALT (left) and the
hyper-parameters in ALO (µ (middle), and λ (right)).

and only use adaptive local training (ALT). We record the re-
sults using four networks on task 35 at the left of Figure 3.
We ﬁnd that only using adaptive local training could still per-
form well on MHAttRNN and Transformer, while it works
worse on DSCNN and especially on ResNet. Hence, it is still
necessary to utilize the adversarial learning to improve perfor-
mances further. Then, we vary µ ∈ {0.0, 0.1, 0.2, 0.3, 0.5} and
λ ∈ {0.0, 0.0001, 0.001, 0.01, 0.1} correspondingly, studying
the effects of label smoothing and adversarial loss in ALO. We
investigate the task C = 12 with DSCNN and ResNet. The
results are shown at the middle and right of Figure 3. Utilizing
label smoothing could almost lead to better performances, and
setting µ around 0.2 is a better choice. Similarly, λ = 0.001 is
recommended for the proposed adversarial loss, and a larger λ
(e.g., 0.1) could be harmful.

Figure 4: Visualization of the plausibility and advantage of the
proposed strategies.

Visualization Analysis: We then present some visualization re-

y
c
a
r
u
c
c
A

l

a
b
o
G

l

0.8

0.6

0.4

0.2

0.8

0.6

0.4

y
c
a
r
u
c
c
A

l

a
b
o
G

l

C=12, DSCNN

C=12, MHAttRNN

C=12, ResNet

C=12, Transformer

0.8

0.7

0.6

0.5

FedAvg:82.85
FedProx:85.51
FedMMD:83.74
FedOpt:85.50
FedKWS-UI:87.29

0.8

0.6

0.4

FedAvg:86.20
FedProx:86.39
FedMMD:85.92
FedOpt:86.38
FedKWS-UI:86.57

0.8

0.6

0.4

0.2

FedAvg:83.48
FedProx:83.93
FedMMD:83.90
FedOpt:81.30
FedKWS-UI:86.23

FedAvg:71.17
FedProx:72.15
FedMMD:72.71
FedOpt:71.12
FedKWS-UI:79.06

3

60

120

180

240

300

3

60

120

180

240

300

3

60

120

180

240

300

3

60

120

180

240

300

C=35, DSCNN

C=35, MHAttRNN

C=35, ResNet

C=35, Transformer

0.8

0.6

0.4

FedAvg:82.59
FedProx:81.44
FedMMD:82.79
FedOpt:84.23
FedKWS-UI:84.51

0.8

0.6

0.4

0.2

FedAvg:79.80
FedProx:83.10
FedMMD:83.64
FedOpt:82.83
FedKWS-UI:84.60

0.6

0.4

0.2

FedAvg:79.05
FedProx:75.78
FedMMD:80.07
FedOpt:79.33
FedKWS-UI:80.50

FedAvg:70.24
FedProx:70.07
FedMMD:66.78
FedOpt:54.56
FedKWS-UI:74.75

3

120

60
Communication Round

180

240

300

3

120

60
Communication Round

180

240

300

3

120

60
Communication Round

180

240

300

3

120

60
Communication Round

180

240

300

0.8

0.6

0.4

0.2

y
c
a
r
u
c
c
A

l

a
b
o
G

l

Ablation of µ=0.0, λ = 0.0 (C=35)

FedKWS-UI: DSCNN:82.27
FedKWS-UI: MHAttRNN:84.73
FedKWS-UI: ResNet:74.10
FedKWS-UI: Transformer:70.57

0.9

0.8

0.7

0.6

0.5

0.4

0.3

Ablation of µ (C=12, DSCNN)

Ablation of λ (C=12, ResNet)

0.8

0.6

0.4

0.2

FedKWS-UI: µ=0.0:86.55
FedKWS-UI: µ=0.1:87.24
FedKWS-UI: µ=0.2:87.29
FedKWS-UI: µ=0.3:87.21
FedKWS-UI: µ=0.5:86.96

FedKWS-UI: λ=0.0:85.29
FedKWS-UI: λ=1e-4:86.07
FedKWS-UI: λ=1e-3:86.23
FedKWS-UI: λ=1e-2:82.79
FedKWS-UI: λ=1e-1:76.99

0

60

120

180

240

300

0

60

120

180

240

300

0

60

120

180

240

300

Communication Round

Communication Round

Communication Round

1.1

1.0

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

)
2
γ
(

r
e
s
U
d
e
ﬁ

i
l

a
u
Q
g
n
o
A

l

-0.1

- 0 . 1

0 . 0

Without ALO

θ2

θ0

0 . 4

0 . 3

0 . 2

0 . 1
0 . 9
0 . 5
Along Inferior User (γ1)

0 . 6

0 . 8

0 . 7

0.90

0.75

0.60

0.45

0.30

0.15

1.1

1.0

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

-0.1

- 0 . 1

0 . 0

θ1

1 . 0

1 . 1

With ALO

θ2

θ0

0 . 4

0 . 3

0 . 2

0 . 1
0 . 9
0 . 5
Along Inferior User (γ1)

0 . 6

0 . 8

0 . 7

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

)
y
c
a
r
u
c
c
A

t
s
e
T
(

n
o
i
t
a
z
i
l

a
r
e
n
e
G

ˆθ1

1 . 0

1 . 1

[23] X. Li and D. Zhan, “FedRS: Federated learning with restricted
softmax for label distribution non-iid data,” in KDD, 2021, pp.
995–1005.

[24] X. Yao, C. Huang, and L. Sun, “Two-stream federated learning:
Reduce the communication costs,” in VCIP, 2018, pp. 1–4.

[25] D. A. E. Acar, Y. Zhao, R. M. Navarro, M. Mattina, P. N. What-
mough, and V. Saligrama, “Federated learning based on dynamic
regularization,” in ICLR, 2021.

[26] Q. Li, B. He, and D. Song, “Model-contrastive federated learn-

ing,” in CVPR, 2021, pp. 10 713–10 722.

[27] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, “How transfer-
able are features in deep neural networks?” in NeurIPS, 2014, pp.
3320–3328.

[28] L. Collins, H. Hassani, A. Mokhtari, and S. Shakkottai, “Exploit-
ing shared representations for personalized federated learning,” in
ICML, 2021, pp. 2089–2099.

[29] P. P. Liang, T. Liu, Z. Liu, R. Salakhutdinov, and L. Morency,
“Think locally, act globally: Federated learning with local and
global representations,” CoRR, vol. abs/2001.01523, 2020.

[30] X. Li, D. Zhan, Y. Shao, B. Li, and S. Song, “FedPHP: Federated
personalization with inherited private models,” in ECML/PKDD,
2021, pp. 587–602.

[31] X. Li, L. Gan, D. Zhan, Y. Shao, B. Li, and S. Song, “Aggregate or
not? exploring where to privatize in DNN based federated learn-
ing under different non-iid scenes,” CoRR, vol. abs/2107.11954,
2021.

[32] G. E. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge

in a neural network,” CoRR, vol. abs/1503.02531, 2015.

[33] T. Shen, J. Zhang, X. Jia, F. Zhang, G. Huang, P. Zhou, F. Wu, and
C. Wu, “Federated mutual learning,” CoRR, vol. abs/2006.16765,
2020.

[34] C. He, M. Annavaram, and S. Avestimehr, “Group knowledge
transfer: Federated learning of large cnns at the edge,” in NeurIPS,
2020.

[35] F. Lai, Y. Dai, X. Zhu, H. V. Madhyastha, and M. Chowdhury,
“Fedscale: Benchmarking model and system performance of fed-
erated learning,” in ResilientFL, 2021, pp. 1–3.

[36] M. Abadi, A. Chu, I. J. Goodfellow, H. B. McMahan, I. Mironov,
K. Talwar, and L. Zhang, “Deep learning with differential pri-
vacy,” in CCS, 2016, pp. 308–318.

7. References
[1] W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals, “Listen, attend
and spell: A neural network for large vocabulary conversational
speech recognition,” in ICASSP, 2016, pp. 4960–4964.

[2] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke,
D. Yu, and G. Zweig, “The microsoft 2016 conversational speech
recognition system,” in ICASSP, 2017, pp. 5255–5259.

[3] G. Chen, C. Parada, and G. Heigold, “Small-footprint keyword
spotting using deep neural networks,” in ICASSP, 2014, pp. 4087–
4091.

[4] T. N. Sainath and C. Parada, “Convolutional neural networks for
small-footprint keyword spotting,” in INTERSPEECH, 2015, pp.
1478–1482.

[5] Y. Zhang, N. Suda, L. Lai, and V. Chandra, “Hello edge: Keyword
spotting on microcontrollers,” CoRR, vol. abs/1711.07128, 2017.

[6] O. Rybakov, N. Kononenko, N. Subrahmanya, M. Visontai, and
S. Laurenzo, “Streaming keyword spotting on mobile devices,” in
INTERSPEECH, 2020, pp. 2277–2281.

[7] R. Tang and J. Lin, “Deep residual learning for small-footprint

keyword spotting,” in ICASSP, 2018, pp. 5484–5488.

[8] A. Berg, M. O’Connor, and M. T. Cruz, “Keyword trans-
former: A self-attention model for keyword spotting,” CoRR, vol.
abs/2104.00769, 2021.

[9] P. Warden, “Speech commands: A dataset for limited-vocabulary

speech recognition,” CoRR, vol. abs/1804.03209, 2018.

[10] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Ar-
cas, “Communication-efﬁcient learning of deep networks from
decentralized data,” in AISTATS, 2017, pp. 1273–1282.

[11] Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine learn-
ing: Concept and applications,” ACM TIST, vol. 10, no. 2, pp.
12:1–12:19, 2019.

[12] Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, “Fed-
erated learning with non-iid data,” CoRR, vol. abs/1806.00582,
2018.

[13] T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and
V. Smith, “Federated optimization in heterogeneous networks,”
in MLSys, 2020.

[14] X. Li, Y. Xu, S. Song, B. Li, Y. Li, Y. Shao, and D. Zhan,
“Federated learning with position-aware neurons,” CoRR, vol.
abs/2203.14666, 2022.

[15] Y. Gong, Y. Chung, and J. R. Glass, “AST: audio spectrogram

transformer,” CoRR, vol. abs/2104.01778, 2021.

[16] S. Chang, H. Park, J. Cho, H. Park, S. Yun, and K. Hwang,
“Subspectral normalization for neural audio data processing,” in
ICASSP, 2021, pp. 850–854.

[17] D. Leroy, A. Coucke, T. Lavril, T. Gisselbrecht, and J. Dureau,
“Federated learning for keyword spotting,” in ICASSP, 2019, pp.
6341–6345.

[18] A. Hard, K. Partridge, C. Nguyen, N. Subrahmanya, A. Shah,
P. Zhu, I. Lopez-Moreno, and R. Mathews, “Training keyword
spotting models on non-iid data with federated learning,” in IN-
TERSPEECH, 2020, pp. 4343–4347.

[19] S. J. Reddi, Z. Charles, M. Zaheer, Z. Garrett, K. Rush,
J. Koneˇcn´y, S. Kumar, and H. B. McMahan, “Adaptive federated
optimization,” in ICLR, 2021.

[20] X. Cui, S. Lu, and B. Kingsbury, “Federated acoustic modeling for
automatic speech recognition,” in ICASSP, 2021, pp. 6748–6752.

[21] K. Nandury, A. Mohan, and F. Weber, “Cross-silo federated train-
ing in the cloud with diversity scaling and semi-supervised learn-
ing,” in ICASSP, 2021, pp. 3085–3089.

[22] D. Guliani, F. Beaufays, and G. Motta, “Training speech recogni-
tion models with federated learning: A quality/cost framework,”
in ICASSP, 2021, pp. 3080–3084.

","Avoid Overﬁtting User Speciﬁc Information in Federated Keyword Spotting Xin-Chun Li1, Jin-Lin Tang1, Shaoming Song2, Bingshuai Li2, Yinchuan Li2, Yunfeng Shao2, Le Gan1, De-Chuan Zhan1 1State Key Laboratory for Novel Software Technology, Nanjing University 2Huawei Noah’s Ark Lab {lixc, tangjl}@lamda.nju.edu.cn, ganle@nju.edu.cn, zhandc@nju.edu.cn, {shaoming.song, libingshuai, liyinchuan, shaoyunfeng}@huawei.com 2 2 0 2 n u J 7 1 ] G L . s c [ 1 v 4 6 8 8 0 . 6 0 2 2 : v i X r a Abstract Keyword spotting (KWS) aims to discriminate a speciﬁc wake- up word from other signals precisely and efﬁciently for different users. Recent works utilize various deep networks to train KWS models with all users’ speech data centralized without consid- ering data privacy. Federated KWS (FedKWS) could serve as a solution without directly sharing users’ data. However, the small amount of data, different user habits, and various ac- cents could lead to fatal problems, e.g., overﬁtting or weight divergence. Hence, we propose several strategies to encourage the model not to overﬁt user-speciﬁc information in FedKWS. Speciﬁcally, we ﬁrst propose an adversarial learning strategy, which updates the downloaded global model against an overﬁt- ted local model and explicitly encourages the global model to capture user-invariant information. Furthermore, we propose an adaptive local training strategy, letting clients with more train- ing data and more uniform class distributions undertake more local update steps. Equivalently, this strategy could weaken the negative impacts of those users whose data is less quali- ﬁed. Our proposed FedKWS-UI could explicitly and implicitly learn user-invariant information in FedKWS. Abundant exper- imental results on federated Google Speech Commands verify the effectiveness of FedKWS-UI. Index Terms: keyword spotting, federated learning, data het- erogeneity, user-invariant 1. Introduction Deep learning has been successfully applied to automatic speech recognition (ASR) [1, 2], facilitating the emergence of intelligent voice assistants (e.g., Amazon Alexa). To wake up the smart assistant, some predeﬁned keywords (e.g., “Alexa”) need to be identiﬁed precisely from users’ speech recordings, i.e., keyword spotting (KWS) [3, 4]. This identiﬁcation process must be efﬁcient to complete, and the utilized models should have minimal memory footprint. Furthermore, the KWS pro- cess should be robust to users with various accents or preferred spoken words. Recent works utilize various deep networks for KWS [4, 5, 6, 7, 8]. These methods take a data centralized training style based on the publicly available benchmark such as Google Speech Commands [9]. However, there may be signiﬁcant pri- vacy implications in sharing users’ audio recordings, which re- quires a data decentralized training style for privacy protec- Supported by National Natural Science Foundation of China (Grant No. 41901270), NSFC-NRF Joint Research Project under Grant 61861146001, and Natural Science Foundation of Jiangsu Province (Grant No. BK20190296). Thanks to Huawei Noah’s Ark Lab Net- MIND Research Team for funding this research. De-Chuan Zhan is the corresponding author. Email: zhandc@nju.edu.cn tion. Federated learning (FL) [10, 11] has been effectively ap- plied for communication efﬁcient decentralized training with basic privacy protection. Although FL could be directly ap- plied to decentralized KWS training, the non-independent and identically distributed data (non-i.i.d. data) poses many chal- lenges [12, 13]. Non-i.i.d. in KWS refers to the fact that some users only own a small amount of data (i.e., quantity skew), users tend to use different spoken words (i.e., label distribution skew), and users usually have accents (i.e., feature distribution skew). This paper investigates FedKWS on Google Speech Com- mands [9] with several popular network architectures. Com- pared with centralized training, we observe a signiﬁcant per- formance degradation in FedKWS due to non-i.i.d. data. In fact, the small amount of data and the distribution skew problem make the downloaded global model easily overﬁt user-speciﬁc information. For example, the feature extractor mistakenly takes a user’s accent as an important factor, or the classiﬁca- tion layer is biased towards a user’s commonly spoken words. To solve these challenges and enhance the generalization per- formance of the federated model, we propose several strategies to avoid the local model overﬁtting user-speciﬁc information. 2. Related Works Our work is closely related to keyword spotting (KWS) [4, 3] and federated learning (FL) [10, 11, 14]. Current works for- mulate KWS as a classiﬁcation problem, aiming to identify whether a short speech recording is a speciﬁc word, silence, or unknown. Considering the success of deep learning, CNN has been applied to KWS [4]. Depth-separable CNN (DSCNN) [5] is applied to obtain the goal of small footprint memory, and residual network (ResNet) [7] is utilized to enhance perfor- mances. Recurrent neural networks with multi-head atten- tion (MHAttRNN) [6, 8] and varieties of transformers (Trans- former) [8, 15] have also been applied to KWS and obtain SOTA results. Some other advanced techniques in deep learning have also been veriﬁed helpful in KWS [16]. FL has also been ap- plied to KWS for decentralized training [17, 18]. [17] conducts extensive experiments of FedAvg [10] on “Hey Snips” dataset and uses an adaptive averaging strategy for global model aggre- gation as done in [19]. The work [18] investigates data aug- mentation and distillation in FedKWS for overcoming resource constraints and example labeling. FL studies have also been presented in ASR [20, 21, 22]. Compared with these studies, we primarily focus on the non-i.i.d. data challenge in FedKWS and propose a novel method to focus on extracting user-invariant in- formation. We investigate our methods with various network architectures and show that our approach is universal. 3. Background of Federated Learning (cid:80) k∈St ˆψk (cid:80)K FedAvg [10]: Suppose we have K clients and each client owns a data distribution Dk = P k(x, y), k ∈ [K]. FL aims to op- k=1 pkL(Dk; ψ), where ψ denotes the global timize minψ parameters, pk denotes the weight of each client. FedAvg [10] solves this problem via multiple communication rounds of lo- cal and global procedures. During local procedure, a partial set of clients St download the global model ψt and update it on their local data for multiple steps. During global proce- dure, the server collects these updated local models (denoted as ˆψk t , k ∈ St) and aggregates them via parameter averaging, i.e., ψt+1 ← 1 t . t denotes the communication round. |St| These two procedures will iterate T rounds until convergence. Non-I.I.D. Data: The users’ data in FL are often naturally heterogeneous, e.g., the speech data in Google Speech Com- mands [9] are collected from users with various accents. As de- clared in [12], the local update direction will diverge a lot from the global one due to non-i.i.d. data, making the model aggre- gation inaccurate. FedOpt [19] utilizes an adaptive optimization strategy on the server instead of a simple parameter averaging. FedRS [23] speciﬁes the challenge of label shift across clients and proposes restricted softmax as the solution. FedProx [13] and FedMMD [24] add regularization to prevent local models from being updated too away, which could decrease the weight divergence for better aggregation. Although some FL methods (e.g., FedProx [13], FedDyn [25], MOON [26]) could also elab- orate a regularization effect during local procedures, they only stay on the parameter or the intermediate feature levels. By contrast, we adversarially update the global model against an overﬁtted local model and regularize the local procedure on the functional level. Furthermore, we design an adaptive local train- ing procedure from the system scheduling level. 4. Proposed Methods This section proposes two strategies to prevent the global model from overﬁtting user-speciﬁc information (e.g., accents or fa- vorite spoken words) in FedKWS. Adversarial Learning against Overﬁtted models (ALO): Clients update the downloaded global model on their data dur- ing the local procedure, which could overﬁt some user-speciﬁc information. Speciﬁcally, the local data distribution P k(x, y) may diverge signiﬁcantly from the global data distribution. Ac- cording to some previous works [27, 28, 29], the lower/higher layers of a neural network tend to be inﬂuenced signiﬁcantly by feature/label distribution skew, i.e., various P k(x) or P k(y). FedKWS simultaneously faces these two kinds of distribution skew (e.g., accents and favorite spoken words), making the complete model biased towards a speciﬁc user during the lo- cal procedure. Hence, we must regularize the local training from the functional perspective instead of focusing on speciﬁc neural network layers. We resort to private-shared models and adversarially update the global model (shared among users) against overﬁtted local models (private for each user). Private- shared models are utilized in some recent FL solutions [30, 31]. Speciﬁcally, we build private models ψk p , k ∈ [K] for each client. We ﬁrst train private models with the cross-entropy loss L(Dk; ψk c=1 I{yi = c} log[fp(xi)]c], where I{·} is the indicator function and fp(·) is the prediction function based on private model ψk p that outputs a probability distribution. After abundant training steps, we expect this pri- vate model to overﬁt user-speciﬁc data information. Then, we p ) = Exi,yi∼Dk [− (cid:80)C Figure 1: Left: data heterogeneity in federated Google Speech Commands. We only plot 20 clients (users) in task 35. Right: number of samples and class distribution entropy of each client (user) in task 12 and 35 (each point shows a client). train the global model with the following loss: (cid:34) Lls = Exi,yi − C (cid:88) [(1 − µ)I{yi = c} + µ/C] log[f (xi)]c (cid:35) , c=1 Ladv = − (cid:124)(cid:123)(cid:122)(cid:125) negative Exi,yi (cid:34) − C (cid:88) [fp(xi)]c log[f (xi)]c (cid:35) , c=1 L(Dk; ψk) = Lls(Dk; ψk) + λLadv(Dk; ψk), (1) (2) (3) where we omit the communication round index t and some other symbols for simpliﬁcation. fp(·) represents the func- tion of the overﬁtted private model while f (·) for the down- loaded global model. Eq. (2) could be seen as “negative distil- lation”, which could push the global model’s prediction f (xi) away from overﬁtted areas. Eq. (2) follows the formula of dis- tillation [32, 33, 34] but works signiﬁcantly different. Label smoothing in Eq. (1) could also regularize the global model not be too over-conﬁdent on a speciﬁc user’s data. We investigate the hyperparameters of µ and λ in ablation studies. Adaptive Local Training (ALT): Due to data heterogeneity, both amount imbalance and class imbalance could occur in clients’ data. The former implies that different clients may own various numbers of training samples. The second one refers to that label distributions may diverge across clients. These two types of imbalance on Google Speech Commands [9] are shown in Figure 1. Intuitively, few training samples could lead to overﬁtting, and imbalanced data could bias the model towards identifying a user’s favorite words. Hence, we en- courage clients who own more training data and more uni- form class distributions to undertake more local updates. For- mally, in FedAvg [10], every selected client uniformly takes E local training steps without considering the data quality. Assume the kth client owns nk training samples and the class distribution is qk ∈ RC with (cid:80)C c=1 qk,c = 1 and qk,c ≥ 0, ∀c. C is the total number of classes. We cal- culate the normalized amount of training samples as nk = nk/ maxK j=1 nj ∈ [0, 1], and the normalized class entropy as Statistics (C=12) 2 0 8 1 1 0 0 6 0 0 4 0 1 4 1 Number of Samples Statistics (C=35) 0 0 5 5 0 0 0 0 2 2 1 1 Client-Class Distribution (C=35) 0 5 1 5 1 0 2 0 0 2 y p o r t n E n o i t u b i r t s i D s s a C l y p o r t n E n o i t u b i r t s i D s s a C l 2.49 2.0 1.5 1.0 0 0 5 3.56 3.0 2.5 2.0 1.5 1.0 1 0 5 10 15 20 25 30 34 x e d n I s s a C l Sampled 20 Clients Number of Samples 3.49 2.67 1.85 1.03 0.21 4.70 3.56 2.43 1.29 0.15 6 1 3 Table 1: Detail information of federated Speech Commands. Table 3: Comparisons on FA and FR rate. The lower the better. C 12 35 K 2,234 2,434 N 45.6k 105.5k Avg.nk Max.nk 20.4 43.3 141 316 M 4.9k 11.0k FedAvg [10, 18] 0.27 5.03 FedOpt [19, 17] 0.32 3.19 FedKWS-UI 0.23 2.78 FA FR Table 2: Detail of networks and centralized training results. Num.of.Params Centralized Acc. C = 12 C = 35 C = 12 C = 35 96.95 169K 97.05 228K 97.31 238K 97.14 232K 97.19 97.31 97.89 96.21 173K 232K 239K 234K DSCNN [5] MHAttRNN [6] ResNet [7] Transformer [8] ek = (− (cid:80)C c=1 qk,c log qk,c)/ log C ∈ [0, 1]. Then we calcu- late the harmonic mean of nk and ek, i.e., rk = 2nkek/(nk + ek). We use rk ∈ [0, 1] to measure clients’ utility in FL, and we heuristically let clients with larger rk contribute more to FL. That is, we reallocate the computation resources among clients via allowing the kth client take on r0 ∗ rk ∗ E gradient steps, where the determination of r0 should satisfy (cid:80)K k=1 r0∗rk∗E ≈ K ∗ E for conservation. Easily, r0 = K/ (cid:80)K k=1 rk. Although the computation ability of clients should also be considered, we focus on non-i.i.d. data in this work and leave it as future work. 5. Experiments Datasets: We name the proposed method as “Federated KWS with User-Invariant information” (FedKWS-UI), and investi- gate it on Google Speech Commands [9]1 (recommended by FedScale [35]) to identify whether a 1s-long speech recording is a word, silence, or unknown. The benchmark contains two tasks with 12 classes (10 words, silence, unknown) and 35 classes (35 words). The two tasks contain 2,234 and 2,434 users. We split the data into corresponding clients with each user as a client. The number of total training samples (N ), the training samples of each client on average (Avg.nk), the number of test samples (M ) are listed in Table 1. The class distributions of randomly selected 20 clients in task C = 35 are shown in left of Fig- ure 1. Larger circles correspond to more samples. The train and test data is split via the provided lists in Google Speech Commands. We extract 40 MFCC features for each 30ms win- dow frame with a stride of 10ms. We also follow the settings in Google Speech Commands: performing random time-shift of Y ∼ [−100, 100] milliseconds and adding 0.1 volume back- ground noise with a probability of 0.8. Networks and Centralized Training: We investigate vari- ous network architectures and moderately modify them to keep nearly the same number of parameters. We use DSCNN [5] with 172 channels, MHAttRNN [6, 8] with 4 heads and 80 hid- den neurons, ResNet [7] with 15 layers and 45 channels in each basic block, Transformer [8] with 4 layers and a model dimen- sion of 96. We ﬁrst use these networks for centralized train- ing. For DSCNN, MHAttRNN and ResNet, we utilize SGD optimizer with momentum 0.9, and we vary the learning rate in {0.1, 0.05, 0.03, 0.01} and select the best result. For Trans- former, we utilize AdamW optimizer and vary learning rate in 1https://pytorch.org/audio/stable/datasets.html {0.005, 0.002, 0.0008}. We set batch size as 128. The number of network parameters and the accuracies on test data are shown in Table 2. We do not obtain SOTA results via Transformer be- cause we only use 4 layers with 0.23M parameters while [8] uses a network with up to 5.4M parameters. FedKWS: For FedKWS, we split the training data in Google Speech Commands via the provided user IDs. The test data is still used to evaluate the generalization ability of the aggre- gated model. We plot the statistics of clients’ number of sam- ples and class distribution entropy at the right of Figure 1. We compare FedKWS-UI with FedAvg [10] (used in [18]), Fed- Prox [13], FedMMD [24], FedOpt [19] (used in [17]). For all methods, we use a batch size of 32, local training steps E = 50 and run 300 rounds. We also vary the learning rate as aforementioned and take the best one for comparison. Addi- tionally, for FedProx and FedMMD, we vary the regularization coefﬁcient in {0.0001, 0.001, 0.01}. For FedOpt, we vary the global optimizer in {SGD, Adam} and the global learning rate in {1.0, 0.1} and {0.001, 0.0001}, respectively. For FedKWS- UI, we use r0 = 3.5, 5.0 for C = 12, 35, and show r0 ∗ rk values of all clients via the shades of color at the right of Fig- ure 1. The max and min values are shown at the color bar, and the top-right points (clients) tend to have larger r0 ∗ rk. We uti- lize µ = 0.2 and λ = 0.001 in FedKWS-UI (Eq. (1), Eq. (3)). We record the accuracy on the global test set every 3 rounds and plot the convergence curves in Figure 2. First, we can clearly observe that the decentralized performances drop a lot compared with centralized training. For example, all of the compared methods could only obtain accuracy as high as 86.39 on task 12, far away from the centralized training (97.89). Then, comparing the network architectures, we could ﬁnd that MHAt- tRNN tends to obtain higher performances while Transformer performs worst. We guess that MHAttRNN could be more ro- bust to the random time-shift because it directly computes the sequential information, and the attention mechanism could pre- cisely capture the important signals. Furthermore, FedProx and FedMMD add regularization during local training procedures on the parameter and intermediate features, which perform not so well. Overall, FedKWS-UI could lead to better results on all of these architectures, particularly on ResNet and Trans- former, verifying the versatility of our methods. Signiﬁcantly, FedKWS-UI surpasses all compared methods by a large margin on task 12 with DSCNN, ResNet, and Transformer. For exam- ple, FedKWS-UI could boost the Transformer performances on task 12 from 72.71 to 79.06. We also evaluate the false accept (FA) and false reject (FR) rate as done in [17, 18]. In task 12, we take the 10 words as positive classes and average their FA rates, while the silence and unknown as negative classes. We do not adjust the prediction conﬁdence threshold to control the FA and directly report FA and FR with the predictions. We use DSCNN and calculate the FA and FR rate of the ﬁnal aggregated model. We show the results in Table 3. We ﬁnd that FedKWS-UI could obtain fewer false alarms/rejections. Ablation Studies: We investigate the effects of components in FedKWS-UI. First, we set µ = 0 and λ = 0 to omit the part of adversarial learning against overﬁtted models (ALO) Figure 2: Comparison results on federated Google Speech Commands. Rows show the results on task 12 and 35, and columns show results of four utilized networks. The legends also show the average accuracy of the ﬁnal 5 communication rounds. sults on task 35 to further show the plausibility and advantage of the proposed strategies. Speciﬁcally, we ﬁrst train a well- performed KWS model (θ0) on the centralized training set (test accuracy up to 93.0%). Then, we respectively update θ0 on an inferior and qualiﬁed user’s data for 20 epochs. The infe- rior user owns only 100 samples and the classes are imbalanced (i.e., the bottom-left user shown in the right part of Figure 1), while the qualiﬁed user owns about 250 samples and the classes are more balanced (i.e., the top-right user shown in the right part of Figure 1). The updated models are denoted as θ1 and θ2. Then, we plot the performance landscape of the interpolation θ0 + γ1(θ1 − θ0) + γ2(θ2 − θ0) within the grid space where γ1 ∈ [−0.1, 1.1], γ2 ∈ [−0.1, 1.1]. The left part of Figure 4 shows that updating θ0 on the qualiﬁed user’s data keeps the generalization ability of the global model while the result on the inferior user’s data becomes worse. Hence, it is rational that our proposed ALT encourages qualiﬁed users to contribute more to FedKWS. Additionally, for the inferior user, we utilize the pro- posed ALO to train another model ˆθ1 against the overﬁtted θ1, and ˆθ1 performs better as shown on the right of Figure 4. The interpolation landscape along the inferior user’s data becomes smoother with ALO, beneﬁting the model aggregation proce- dure in FL. This veriﬁes the advantage of the proposed ALO. Overall, FedKWS-UI could enhance the generalization ability of the federated model even with few or skewed samples. 6. Conclusion We investigate popular networks for FedKWS, where the data heterogeneity leads to signiﬁcant performance degradation compared with centralized training. We propose to learn user- invariant information via adversarial learning against overﬁtted local models and a computation re-allocation strategy named adaptive local training. These two strategies could avoid overﬁt- ting user-speciﬁc information during local training and facilitate model aggregation. Experimental results verify the superiori- ties of our proposed FedKWS-UI. Future works will extend this work to streaming KWS [6] and utilize differential privacy [36] to satisfy stricter privacy requirements. Figure 3: Ablation studies of only using ALT (left) and the hyper-parameters in ALO (µ (middle), and λ (right)). and only use adaptive local training (ALT). We record the re- sults using four networks on task 35 at the left of Figure 3. We ﬁnd that only using adaptive local training could still per- form well on MHAttRNN and Transformer, while it works worse on DSCNN and especially on ResNet. Hence, it is still necessary to utilize the adversarial learning to improve perfor- mances further. Then, we vary µ ∈ {0.0, 0.1, 0.2, 0.3, 0.5} and λ ∈ {0.0, 0.0001, 0.001, 0.01, 0.1} correspondingly, studying the effects of label smoothing and adversarial loss in ALO. We investigate the task C = 12 with DSCNN and ResNet. The results are shown at the middle and right of Figure 3. Utilizing label smoothing could almost lead to better performances, and setting µ around 0.2 is a better choice. Similarly, λ = 0.001 is recommended for the proposed adversarial loss, and a larger λ (e.g., 0.1) could be harmful. Figure 4: Visualization of the plausibility and advantage of the proposed strategies. Visualization Analysis: We then present some visualization re- y c a r u c c A l a b o G l 0.8 0.6 0.4 0.2 0.8 0.6 0.4 y c a r u c c A l a b o G l C=12, DSCNN C=12, MHAttRNN C=12, ResNet C=12, Transformer 0.8 0.7 0.6 0.5 FedAvg:82.85 FedProx:85.51 FedMMD:83.74 FedOpt:85.50 FedKWS-UI:87.29 0.8 0.6 0.4 FedAvg:86.20 FedProx:86.39 FedMMD:85.92 FedOpt:86.38 FedKWS-UI:86.57 0.8 0.6 0.4 0.2 FedAvg:83.48 FedProx:83.93 FedMMD:83.90 FedOpt:81.30 FedKWS-UI:86.23 FedAvg:71.17 FedProx:72.15 FedMMD:72.71 FedOpt:71.12 FedKWS-UI:79.06 3 60 120 180 240 300 3 60 120 180 240 300 3 60 120 180 240 300 3 60 120 180 240 300 C=35, DSCNN C=35, MHAttRNN C=35, ResNet C=35, Transformer 0.8 0.6 0.4 FedAvg:82.59 FedProx:81.44 FedMMD:82.79 FedOpt:84.23 FedKWS-UI:84.51 0.8 0.6 0.4 0.2 FedAvg:79.80 FedProx:83.10 FedMMD:83.64 FedOpt:82.83 FedKWS-UI:84.60 0.6 0.4 0.2 FedAvg:79.05 FedProx:75.78 FedMMD:80.07 FedOpt:79.33 FedKWS-UI:80.50 FedAvg:70.24 FedProx:70.07 FedMMD:66.78 FedOpt:54.56 FedKWS-UI:74.75 3 120 60 Communication Round 180 240 300 3 120 60 Communication Round 180 240 300 3 120 60 Communication Round 180 240 300 3 120 60 Communication Round 180 240 300 0.8 0.6 0.4 0.2 y c a r u c c A l a b o G l Ablation of µ=0.0, λ = 0.0 (C=35) FedKWS-UI: DSCNN:82.27 FedKWS-UI: MHAttRNN:84.73 FedKWS-UI: ResNet:74.10 FedKWS-UI: Transformer:70.57 0.9 0.8 0.7 0.6 0.5 0.4 0.3 Ablation of µ (C=12, DSCNN) Ablation of λ (C=12, ResNet) 0.8 0.6 0.4 0.2 FedKWS-UI: µ=0.0:86.55 FedKWS-UI: µ=0.1:87.24 FedKWS-UI: µ=0.2:87.29 FedKWS-UI: µ=0.3:87.21 FedKWS-UI: µ=0.5:86.96 FedKWS-UI: λ=0.0:85.29 FedKWS-UI: λ=1e-4:86.07 FedKWS-UI: λ=1e-3:86.23 FedKWS-UI: λ=1e-2:82.79 FedKWS-UI: λ=1e-1:76.99 0 60 120 180 240 300 0 60 120 180 240 300 0 60 120 180 240 300 Communication Round Communication Round Communication Round 1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 ) 2 γ ( r e s U d e ﬁ i l a u Q g n o A l -0.1 - 0 . 1 0 . 0 Without ALO θ2 θ0 0 . 4 0 . 3 0 . 2 0 . 1 0 . 9 0 . 5 Along Inferior User (γ1) 0 . 6 0 . 8 0 . 7 0.90 0.75 0.60 0.45 0.30 0.15 1.1 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 - 0 . 1 0 . 0 θ1 1 . 0 1 . 1 With ALO θ2 θ0 0 . 4 0 . 3 0 . 2 0 . 1 0 . 9 0 . 5 Along Inferior User (γ1) 0 . 6 0 . 8 0 . 7 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 ) y c a r u c c A t s e T ( n o i t a z i l a r e n e G ˆθ1 1 . 0 1 . 1 [23] X. Li and D. Zhan, “FedRS: Federated learning with restricted softmax for label distribution non-iid data,” in KDD, 2021, pp. 995–1005. [24] X. Yao, C. Huang, and L. Sun, “Two-stream federated learning: Reduce the communication costs,” in VCIP, 2018, pp. 1–4. [25] D. A. E. Acar, Y. Zhao, R. M. Navarro, M. Mattina, P. N. What- mough, and V. Saligrama, “Federated learning based on dynamic regularization,” in ICLR, 2021. [26] Q. Li, B. He, and D. Song, “Model-contrastive federated learn- ing,” in CVPR, 2021, pp. 10 713–10 722. [27] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, “How transfer- able are features in deep neural networks?” in NeurIPS, 2014, pp. 3320–3328. [28] L. Collins, H. Hassani, A. Mokhtari, and S. Shakkottai, “Exploit- ing shared representations for personalized federated learning,” in ICML, 2021, pp. 2089–2099. [29] P. P. Liang, T. Liu, Z. Liu, R. Salakhutdinov, and L. Morency, “Think locally, act globally: Federated learning with local and global representations,” CoRR, vol. abs/2001.01523, 2020. [30] X. Li, D. Zhan, Y. Shao, B. Li, and S. Song, “FedPHP: Federated personalization with inherited private models,” in ECML/PKDD, 2021, pp. 587–602. [31] X. Li, L. Gan, D. Zhan, Y. Shao, B. Li, and S. Song, “Aggregate or not? exploring where to privatize in DNN based federated learn- ing under different non-iid scenes,” CoRR, vol. abs/2107.11954, 2021. [32] G. E. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” CoRR, vol. abs/1503.02531, 2015. [33] T. Shen, J. Zhang, X. Jia, F. Zhang, G. Huang, P. Zhou, F. Wu, and C. Wu, “Federated mutual learning,” CoRR, vol. abs/2006.16765, 2020. [34] C. He, M. Annavaram, and S. Avestimehr, “Group knowledge transfer: Federated learning of large cnns at the edge,” in NeurIPS, 2020. [35] F. Lai, Y. Dai, X. Zhu, H. V. Madhyastha, and M. Chowdhury, “Fedscale: Benchmarking model and system performance of fed- erated learning,” in ResilientFL, 2021, pp. 1–3. [36] M. Abadi, A. Chu, I. J. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang, “Deep learning with differential pri- vacy,” in CCS, 2016, pp. 308–318. 7. References [1] W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals, “Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,” in ICASSP, 2016, pp. 4960–4964. [2] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, and G. Zweig, “The microsoft 2016 conversational speech recognition system,” in ICASSP, 2017, pp. 5255–5259. [3] G. Chen, C. Parada, and G. Heigold, “Small-footprint keyword spotting using deep neural networks,” in ICASSP, 2014, pp. 4087– 4091. [4] T. N. Sainath and C. Parada, “Convolutional neural networks for small-footprint keyword spotting,” in INTERSPEECH, 2015, pp. 1478–1482. [5] Y. Zhang, N. Suda, L. Lai, and V. Chandra, “Hello edge: Keyword spotting on microcontrollers,” CoRR, vol. abs/1711.07128, 2017. [6] O. Rybakov, N. Kononenko, N. Subrahmanya, M. Visontai, and S. Laurenzo, “Streaming keyword spotting on mobile devices,” in INTERSPEECH, 2020, pp. 2277–2281. [7] R. Tang and J. Lin, “Deep residual learning for small-footprint keyword spotting,” in ICASSP, 2018, pp. 5484–5488. [8] A. Berg, M. O’Connor, and M. T. Cruz, “Keyword trans- former: A self-attention model for keyword spotting,” CoRR, vol. abs/2104.00769, 2021. [9] P. Warden, “Speech commands: A dataset for limited-vocabulary speech recognition,” CoRR, vol. abs/1804.03209, 2018. [10] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Ar- cas, “Communication-efﬁcient learning of deep networks from decentralized data,” in AISTATS, 2017, pp. 1273–1282. [11] Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine learn- ing: Concept and applications,” ACM TIST, vol. 10, no. 2, pp. 12:1–12:19, 2019. [12] Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, “Fed- erated learning with non-iid data,” CoRR, vol. abs/1806.00582, 2018. [13] T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith, “Federated optimization in heterogeneous networks,” in MLSys, 2020. [14] X. Li, Y. Xu, S. Song, B. Li, Y. Li, Y. Shao, and D. Zhan, “Federated learning with position-aware neurons,” CoRR, vol. abs/2203.14666, 2022. [15] Y. Gong, Y. Chung, and J. R. Glass, “AST: audio spectrogram transformer,” CoRR, vol. abs/2104.01778, 2021. [16] S. Chang, H. Park, J. Cho, H. Park, S. Yun, and K. Hwang, “Subspectral normalization for neural audio data processing,” in ICASSP, 2021, pp. 850–854. [17] D. Leroy, A. Coucke, T. Lavril, T. Gisselbrecht, and J. Dureau, “Federated learning for keyword spotting,” in ICASSP, 2019, pp. 6341–6345. [18] A. Hard, K. Partridge, C. Nguyen, N. Subrahmanya, A. Shah, P. Zhu, I. Lopez-Moreno, and R. Mathews, “Training keyword spotting models on non-iid data with federated learning,” in IN- TERSPEECH, 2020, pp. 4343–4347. [19] S. J. Reddi, Z. Charles, M. Zaheer, Z. Garrett, K. Rush, J. Koneˇcn´y, S. Kumar, and H. B. McMahan, “Adaptive federated optimization,” in ICLR, 2021. [20] X. Cui, S. Lu, and B. Kingsbury, “Federated acoustic modeling for automatic speech recognition,” in ICASSP, 2021, pp. 6748–6752. [21] K. Nandury, A. Mohan, and F. Weber, “Cross-silo federated train- ing in the cloud with diversity scaling and semi-supervised learn- ing,” in ICASSP, 2021, pp. 3085–3089. [22] D. Guliani, F. Beaufays, and G. Motta, “Training speech recogni- tion models with federated learning: A quality/cost framework,” in ICASSP, 2021, pp. 3080–3084.","['avoid', 'overﬁtte', 'user', 'speciﬁc', 'information', 'federated', 'spot', 'li1', 'shaome', 'key', 'laboratory', 'novel', 'software', 'technology', 'lixc', 'tangjllamdanjueducn', 'ganlenjueducn', 'x', 'r', 'abstract', 'keyword', 'spot', 'aim', 'discriminate', 'speciﬁc', 'wake', 'word', 'signal', 'precisely', 'efﬁciently', 'different', 'user', 'recent', 'work', 'utilize', 'various', 'deep', 'network', 'train', 'kws', 'model', 'user', 'speech', 'datum', 'centralize', 'consid', 'ere', 'datum', 'privacy', 'federate', 'kws', 'fedkw', 'serve', 'solution', 'directly', 'share', 'user', 'datum', 'however', 'small', 'amount', 'datum', 'different', 'user', 'habit', 'various', 'cent', 'lead', 'fatal', 'problem', 'eg', 'overﬁtte', 'weight', 'divergence', 'hence', 'propose', 'several', 'strategy', 'encourage', 'model', 'information', 'speciﬁcally', 'ﬁrst', 'propose', 'adversarial', 'learning', 'strategy', 'update', 'download', 'global', 'model', 'local', 'model', 'explicitly', 'encourage', 'global', 'model', 'capture', 'userinvariant', 'information', 'furthermore', 'propose', 'adaptive', 'local', 'training', 'strategy', 'let', 'client', 'train', 'e', 'datum', 'uniform', 'class', 'distribution', 'undertake', 'local', 'update', 'step', 'equivalently', 'strategy', 'weaken', 'negative', 'impact', 'user', 'datum', 'less', 'propose', 'fedkwsui', 'explicitly', 'implicitly', 'learn', 'userinvariant', 'information', 'abundant', 'exper', 'imental', 'result', 'federated', 'speech', 'command', 'verify', 'effectiveness', 'fedkwsui', 'index', 'term', 'spot', 'federated', 'learn', 'datum', 'het', 'erogeneity', 'userinvariant', 'introduction', 'deep', 'learning', 'successfully', 'apply', 'automatic', 'speech', 'recognition', 'asr', 'facilitate', 'emergence', 'intelligent', 'voice', 'assistant', 'eg', 'amazon', 'alexa', 'wake', 'smart', 'assistant', 'predeﬁne', 'keyword', 'eg', 'need', 'identiﬁe', 'precisely', 'user', 'speech', 'recording', 'spot', 'identiﬁcation', 'process', 'efﬁcient', 'complete', 'utilize', 'model', 'minimal', 'memory', 'footprint', 'furthermore', 'kws', 'pro', 'cess', 'robust', 'user', 'various', 'accent', 'preferred', 'spoken', 'word', 'recent', 'work', 'utilize', 'various', 'deep', 'network', 'method', 'take', 'data', 'centralized', 'training', 'style', 'base', 'publicly', 'available', 'benchmark', 'speech', 'command', 'however', 'signiﬁcant', 'vacy', 'implication', 'share', 'user', 'audio', 'recording', 'quire', 'data', 'decentralize', 'training', 'style', 'privacy', 'protec', 'support', 'national', 'natural', 'science', 'foundation', 'grant', 'nsfcnrf', 'joint', 'research', 'project', 'grant', 'natural', 'science', 'foundation', 'grant', 'thank', 'net', 'research', 'team', 'fund', 'research', 'dechuan', 'correspond', 'author', 'email', 'tion', 'federate', 'learn', 'fl', 'effectively', 'ap', 'plied', 'communication', 'efﬁcient', 'decentralize', 'training', 'basic', 'privacy', 'protection', 'fl', 'directly', 'ply', 'decentralized', 'kws', 'train', 'nonindependent', 'identically', 'distribute', 'data', 'noniid', 'datum', 'pose', 'many', 'chal', 'lenge', 'noniid', 'refer', 'fact', 'user', 'small', 'amount', 'datum', 'quantity', 'skew', 'user', 'tend', 'use', 'different', 'spoken', 'word', 'label', 'distribution', 'skew', 'user', 'usually', 'accent', 'feature', 'distribution', 'skew', 'paper', 'investigate', 'fedkws', 'speech', 'com', 'mand', 'several', 'popular', 'network', 'architecture', 'com', 'pare', 'centralized', 'training', 'observe', 'signiﬁcant', 'formance', 'degradation', 'noniid', 'datum', 'fact', 'small', 'amount', 'datum', 'distribution', 'skew', 'problem', 'make', 'download', 'global', 'model', 'easily', 'userspeciﬁc', 'information', 'example', 'feature', 'extractor', 'mistakenly', 'take', 'user', 'accent', 'important', 'factor', 'classiﬁca', 'tion', 'layer', 'bias', 'user', 'commonly', 'speak', 'word', 'solve', 'challenge', 'enhance', 'generalization', 'formance', 'federated', 'model', 'propose', 'several', 'strategy', 'avoid', 'local', 'model', 'overﬁtte', 'userspeciﬁc', 'information', 'relate', 'work', 'work', 'closely', 'relate', 'spot', 'federate', 'learning', 'fl', 'current', 'work', 'mulate', 'classiﬁcation', 'problem', 'aim', 'identify', 'short', 'speech', 'recording', 'speciﬁc', 'word', 'silence', 'unknown', 'consider', 'success', 'deep', 'learning', 'apply', 'depthseparable', 'apply', 'obtain', 'goal', 'small', 'footprint', 'memory', 'residual', 'network', 'resnet', 'utilize', 'enhance', 'perfor', 'mance', 'recurrent', 'neural', 'network', 'multihead', 'atten', 'tion', 'variety', 'transformer', 'former', 'also', 'apply', 'kws', 'obtain', 'sota', 'result', 'advanced', 'technique', 'deep', 'learning', 'also', 'veriﬁe', 'helpful', 'fl', 'also', 'ap', 'plied', 'kws', 'decentralized', 'training', 'conduct', 'extensive', 'experiment', 'fedavg', 'snip', 'dataset', 'use', 'adaptive', 'averaging', 'strategy', 'global', 'model', 'aggre', 'gation', 'work', 'investigate', 'data', 'aug', 'mentation', 'distillation', 'fedkw', 'overcome', 'resource', 'constraint', 'example', 'labeling', 'fl', 'study', 'also', 'present', 'compare', 'study', 'primarily', 'focus', 'datum', 'challenge', 'propose', 'novel', 'method', 'focus', 'extract', 'userinvariant', 'formation', 'investigate', 'method', 'various', 'network', 'architecture', 'show', 'approach', 'universal', 'background', 'federated', 'suppose', 'client', 'client', 'data', 'distribution', 'p', 'aim', 'op', 'ψ', 'denote', 'global', 'timize', 'minψ', 'parameter', 'pk', 'denote', 'weight', 'client', 'fedavg', 'solve', 'problem', 'multiple', 'communication', 'round', 'lo', 'cal', 'global', 'procedure', 'local', 'procedure', 'partial', 'set', 'client', 'download', 'global', 'model', 'ψt', 'update', 'local', 'datum', 'multiple', 'step', 'global', 'proce', 'dure', 'server', 'collect', 'update', 'local', 'model', 'denote', 'aggregate', 'parameter', 'denote', 'communication', 'procedure', 'iterate', 'round', 'convergence', 'noniid', 'data', 'user', 'datum', 'fl', 'often', 'naturally', 'heterogeneous', 'eg', 'speech', 'datum', 'speech', 'com', 'mand', 'collect', 'user', 'various', 'accent', 'clare', 'local', 'update', 'direction', 'diverge', 'lot', 'global', 'due', 'noniid', 'datum', 'make', 'model', 'aggre', 'gation', 'inaccurate', 'fedopt', 'utilize', 'adaptive', 'optimization', 'strategy', 'server', 'instead', 'simple', 'parameter', 'average', 'speciﬁes', 'challenge', 'label', 'shift', 'client', 'propose', 'restrict', 'softmax', 'solution', 'fedprox', 'add', 'regularization', 'prevent', 'local', 'model', 'update', 'away', 'decrease', 'weight', 'divergence', 'well', 'aggregation', 'fl', 'method', 'eg', 'fedprox', 'feddyn', 'moon', 'also', 'elab', 'orate', 'regularization', 'effect', 'local', 'procedure', 'stay', 'parameter', 'intermediate', 'feature', 'level', 'contrast', 'adversarially', 'update', 'global', 'model', 'overﬁtted', 'local', 'model', 'regularize', 'local', 'procedure', 'functional', 'level', 'furthermore', 'design', 'adaptive', 'local', 'train', 'ing', 'procedure', 'system', 'scheduling', 'level', 'propose', 'method', 'section', 'propose', 'strategy', 'prevent', 'global', 'model', 'overﬁtte', 'userspeciﬁc', 'information', 'eg', 'accent', 'vorite', 'spoken', 'word', 'adversarial', 'learn', 'overﬁtted', 'model', 'client', 'update', 'download', 'global', 'model', 'datum', 'dur', 'e', 'local', 'procedure', 'overﬁt', 'userspeciﬁc', 'information', 'speciﬁcally', 'local', 'data', 'distribution', 'p', 'diverge', 'signiﬁcantly', 'global', 'data', 'distribution', 'cord', 'previous', 'work', 'lowerhigher', 'layer', 'neural', 'network', 'tend', 'inﬂuence', 'signiﬁcantly', 'featurelabel', 'distribution', 'skew', 'various', 'p', 'p', 'fedkw', 'simultaneously', 'face', 'kind', 'distribution', 'skew', 'eg', 'accent', 'favorite', 'spoken', 'word', 'make', 'complete', 'model', 'bias', 'speciﬁc', 'user', 'lo', 'cal', 'procedure', 'hence', 'regularize', 'local', 'training', 'functional', 'perspective', 'instead', 'focus', 'speciﬁc', 'network', 'layer', 'resort', 'privateshare', 'model', 'adversarially', 'update', 'global', 'model', 'share', 'user', 'overﬁtted', 'local', 'model', 'private', 'user', 'private', 'share', 'model', 'utilize', 'recent', 'fl', 'solution', 'speciﬁcally', 'build', 'private', 'model', 'ψk', 'client', 'ﬁrst', 'train', 'private', 'model', 'crossentropy', 'loss', 'ldk', 'ψk', 'logfpxic', 'indicator', 'function', 'fp', 'prediction', 'function', 'base', 'private', 'model', 'ψk', 'p', 'output', 'probability', 'distribution', 'abundant', 'training', 'step', 'expect', 'vate', 'datum', 'information', 'p', 'figure', 'leave', 'datum', 'heterogeneity', 'federated', 'speech', 'command', 'plot', 'client', 'user', 'task', 'right', 'number', 'sample', 'class', 'distribution', 'entropy', 'client', 'user', 'task', 'point', 'show', 'client', 'train', 'global', 'model', 'following', 'loss', 'c', '−', 'µiyi', 'µc', 'logf', '−', 'cid124cid123cid122cid125', 'negative', '−', 'c', 'logf', 'λladvdk', 'omit', 'communication', 'round', 'index', 'symbol', 'simpliﬁcation', 'represent', 'func', 'tion', 'overﬁtted', 'private', 'model', 'loaded', 'global', 'model', 'eq', 'see', 'negative', 'lation', 'push', 'global', 'model', 'prediction', 'away', 'overﬁtted', 'area', 'eq', 'follow', 'formula', 'work', 'signiﬁcantly', 'different', 'label', 'smooth', 'eq', 'also', 'regularize', 'global', 'model', 'overconﬁdent', 'speciﬁc', 'user', 'datum', 'investigate', 'hyperparameter', 'ablation', 'study', 'adaptive', 'local', 'training', 'alt', 'datum', 'heterogeneity', 'amount', 'imbalance', 'class', 'imbalance', 'occur', 'client', 'datum', 'former', 'imply', 'different', 'client', 'various', 'number', 'training', 'sample', 'second', 'one', 'refer', 'label', 'distribution', 'diverge', 'client', 'type', 'imbalance', 'speech', 'command', 'show', 'figure', 'intuitively', 'training', 'sample', 'lead', 'overﬁtte', 'imbalance', 'datum', 'bias', 'model', 'identify', 'user', 'favorite', 'word', 'hence', 'courage', 'client', 'training', 'datum', 'uni', 'form', 'class', 'distribution', 'undertake', 'local', 'update', 'mally', 'select', 'client', 'uniformly', 'take', 'e', 'local', 'training', 'step', 'consider', 'datum', 'quality', 'assume', 'client', 'training', 'sample', 'class', 'distribution', 'c', 'total', 'number', 'class', 'cal', 'culate', 'normalize', 'amount', 'training', 'sample', 'nk', 'j1', 'normalize', 'class', 'entropy', 'statistic', 'number', 'sample', 'statistic', 'clientclass', 'distribution', 'c35', 'p', 'r', 'e', 'u', 'r', 'c', 'l', 'p', 'r', 'e', 'u', 'r', 'c', 'l', 'x', 'e', 'c', 'l', 'sample', 'client', 'number', 'sample', 'table', 'detail', 'information', 'federate', 'speech', 'command', 'table', 'comparison', 'fa', 'fr', 'rate', 'low', 'well', 'k', '456k', 'avgnk', 'maxnk', '49k', 'fedkwsui', 'fa', 'fr', 'table', 'detail', 'network', 'centralized', 'training', 'result', 'numofparams', 'centralize', 'c', 'c', 'c', '169k', '228k', '238k', 'mhattrnn', 'resnet', 'transformer', 'calcu', 'late', 'harmonic', 'mean', 'nk', 'use', 'measure', 'client', 'utility', 'fl', 'heuristically', 'let', 'client', 'large', 'contribute', 'fl', 'reallocate', 'computation', 'resource', 'client', 'allow', 'client', 'take', '∗', 'rk', '∗', 'e', 'gradient', 'step', 'determination', 'r0', 'satisfy', 'cid80k', 'k1', 'r0∗rk∗e', 'k', '∗', 'e', 'conservation', 'easily', 'r0', 'k1', 'rk', 'computation', 'ability', 'client', 'also', 'consider', 'focus', 'noniid', 'datum', 'work', 'leave', 'future', 'work', 'experiment', 'dataset', 'name', 'propose', 'method', 'federate', 'kws', 'userinvariant', 'information', 'fedkwsui', 'gate', 'speech', 'command', 'recommend', 'fedscale', 'identify', 'speech', 'recording', 'word', 'silence', 'unknown', 'benchmark', 'contain', 'task', 'class', 'word', 'silence', 'unknown', 'class', 'word', 'task', 'contain', 'user', 'split', 'datum', 'correspond', 'client', 'user', 'client', 'number', 'total', 'training', 'sample', 'training', 'sample', 'client', 'average', 'avgnk', 'number', 'test', 'sample', 'list', 'table', 'class', 'distribution', 'randomly', 'select', 'client', 'task', 'show', 'leave', 'fig', 'ure', 'large', 'circle', 'correspond', 'sample', 'train', 'test', 'datum', 'split', 'provide', 'list', 'speech', 'command', 'extract', 'mfcc', 'feature', '30ms', 'win', 'stride', '10ms', 'also', 'follow', 'setting', 'speech', 'command', 'perform', 'random', 'timeshift', 'millisecond', 'add', 'volume', 'back', 'ground', 'noise', 'probability', 'network', 'centralized', 'training', 'investigate', 'ous', 'network', 'architecture', 'moderately', 'modify', 'keep', 'nearly', 'number', 'parameter', 'use', 'channel', 'mhattrnn', 'head', 'hide', 'den', 'neuron', 'resnet', 'layer', 'channel', 'basic', 'block', 'transformer', 'layer', 'model', 'diman', 'sion', 'ﬁrst', 'use', 'network', 'centralized', 'train', 'ing', 'resnet', 'utilize', 'optimizer', 'momentum', 'vary', 'learning', 'rate', 'select', 'good', 'result', 'former', 'utilize', 'optimizer', 'vary', 'learning', 'rate', 'set', 'batch', 'size', 'number', 'network', 'parameter', 'accuracy', 'test', 'datum', 'show', 'table', 'obtain', 'sota', 'result', 'transformer', 'use', 'layer', 'parameter', 'use', 'network', 'parameter', 'fedkw', 'fedkw', 'split', 'training', 'datum', 'speech', 'command', 'provide', 'user', 'id', 'test', 'datum', 'still', 'use', 'evaluate', 'generalization', 'ability', 'aggre', 'gate', 'model', 'plot', 'statistic', 'client', 'number', 'ple', 'class', 'distribution', 'entropy', 'right', 'figure', 'compare', 'fedkwsui', 'use', 'prox', 'fedmmd', 'fedopt', 'use', 'method', 'use', 'batch', 'size', 'local', 'training', 'step', 'run', 'round', 'also', 'vary', 'learning', 'rate', 'aforementione', 'take', 'good', 'comparison', 'tionally', 'fedprox', 'fedmmd', 'vary', 'regularization', 'coefﬁcient', 'fedopt', 'vary', 'global', 'optimizer', 'global', 'learning', 'rate', 'respectively', 'fedkw', 'use', 'show', '∗', 'rk', 'value', 'client', 'shade', 'color', 'right', 'fig', 'ure', 'max', 'min', 'value', 'show', 'color', 'bar', 'topright', 'point', 'client', 'tend', 'large', '∗', 'rk', 'lize', 'fedkwsui', 'eq', 'eq', 'record', 'accuracy', 'global', 'test', 'set', 'round', 'plot', 'convergence', 'curve', 'figure', 'first', 'clearly', 'observe', 'decentralize', 'performance', 'drop', 'lot', 'compare', 'centralized', 'training', 'example', 'compare', 'method', 'obtain', 'accuracy', 'high', 'task', 'far', 'away', 'centralized', 'training', 'compare', 'network', 'architecture', 'ﬁnd', 'trnn', 'tend', 'obtain', 'high', 'performance', 'transformer', 'perform', 'worst', 'guess', 'mhattrnn', 'ro', 'bust', 'random', 'timeshift', 'directly', 'compute', 'sequential', 'information', 'attention', 'mechanism', 'cisely', 'capture', 'important', 'signal', 'furthermore', 'fedprox', 'fedmmd', 'add', 'regularization', 'local', 'training', 'procedure', 'parameter', 'feature', 'perform', 'well', 'overall', 'fedkwsui', 'lead', 'well', 'result', 'architecture', 'particularly', 'resnet', 'former', 'verify', 'versatility', 'method', 'signiﬁcantly', 'fedkwsui', 'surpasse', 'compare', 'method', 'large', 'margin', 'task', 'resnet', 'transformer', 'exam', 'ple', 'fedkwsui', 'boost', 'transformer', 'performance', 'task', 'also', 'evaluate', 'false', 'accept', 'fa', 'false', 'reject', 'rate', 'task', 'take', 'word', 'positive', 'class', 'average', 'fa', 'rate', 'silence', 'unknown', 'negative', 'class', 'adjust', 'prediction', 'conﬁdence', 'threshold', 'control', 'fa', 'directly', 'report', 'fa', 'fr', 'prediction', 'use', 'dscnn', 'calculate', 'fa', 'fr', 'rate', 'ﬁnal', 'aggregated', 'model', 'show', 'result', 'table', 'fedkwsui', 'obtain', 'false', 'alarmsrejection', 'ablation', 'study', 'investigate', 'effect', 'component', 'fedkwsui', 'first', 'set', 'omit', 'part', 'adversarial', 'learning', 'overﬁtted', 'model', 'figure', 'comparison', 'result', 'federated', 'speech', 'command', 'row', 'show', 'result', 'task', 'column', 'show', 'result', 'utilize', 'network', 'legend', 'also', 'show', 'average', 'accuracy', 'ﬁnal', 'communication', 'round', 'sult', 'task', 'far', 'show', 'plausibility', 'advantage', 'propose', 'strategy', 'speciﬁcally', 'train', 'well', 'perform', 'kws', 'model', 'θ0', 'centralized', 'training', 'set', 'test', 'accuracy', 'respectively', 'update', 'θ0', 'inferior', 'datum', 'epoch', 'infe', 'user', 'sample', 'class', 'imbalance', 'bottomleft', 'user', 'show', 'right', 'part', 'figure', 'user', 'sample', 'class', 'balanced', 'topright', 'user', 'show', 'right', 'part', 'figure', 'update', 'model', 'denote', 'θ2', 'plot', 'performance', 'landscape', 'interpolation', 'γ2θ2', '−', 'θ0', 'grid', 'space', 'γ2', 'left', 'part', 'figure', 'show', 'update', 'θ0', 'datum', 'keep', 'generalization', 'ability', 'global', 'model', 'result', 'inferior', 'user', 'datum', 'become', 'bad', 'hence', 'rational', 'propose', 'alt', 'encourage', 'user', 'contribute', 'fedkws', 'additionally', 'inferior', 'user', 'utilize', 'pro', 'pose', 'alo', 'train', 'model', 'overﬁtted', 'perform', 'well', 'show', 'right', 'figure', 'interpolation', 'landscape', 'inferior', 'user', 'datum', 'become', 'smooth', 'alo', 'beneﬁte', 'model', 'aggregation', 'proce', 'dure', 'fl', 'veriﬁes', 'advantage', 'propose', 'alo', 'overall', 'fedkwsui', 'enhance', 'generalization', 'ability', 'federated', 'model', 'even', 'skewed', 'sample', 'conclusion', 'investigate', 'popular', 'network', 'fedkw', 'datum', 'heterogeneity', 'lead', 'signiﬁcant', 'performance', 'degradation', 'compare', 'centralized', 'training', 'propose', 'learn', 'user', 'invariant', 'information', 'adversarial', 'learning', 'overﬁtted', 'local', 'model', 'computation', 'reallocation', 'strategy', 'name', 'adaptive', 'local', 'training', 'strategy', 'avoid', 'information', 'local', 'training', 'facilitate', 'model', 'aggregation', 'experimental', 'result', 'verify', 'superiori', 'tie', 'propose', 'fedkwsui', 'future', 'work', 'extend', 'work', 'stream', 'utilize', 'differential', 'privacy', 'satisfy', 'strict', 'privacy', 'requirement', 'figure', 'ablation', 'study', 'use', 'alt', 'leave', 'hyperparameter', 'middle', 'λ', 'right', 'use', 'adaptive', 'local', 'training', 'alt', 'record', 'sult', 'use', 'network', 'task', 'left', 'figure', 'ﬁnd', 'use', 'adaptive', 'local', 'training', 'still', 'form', 'well', 'mhattrnn', 'transformer', 'work', 'bad', 'dscnn', 'especially', 'resnet', 'hence', 'still', 'necessary', 'utilize', 'adversarial', 'learning', 'improve', 'perfor', 'mance', 'far', 'vary', 'correspondingly', 'study', 'effect', 'label', 'smoothing', 'adversarial', 'loss', 'investigate', 'task', 'dscnn', 'resnet', 'result', 'show', 'middle', 'right', 'figure', 'utilize', 'label', 'smoothing', 'almost', 'lead', 'well', 'performance', 'set', 'around', 'well', 'choice', 'similarly', 'recommend', 'propose', 'adversarial', 'loss', 'large', 'harmful', 'figure', 'visualization', 'plausibility', 'advantage', 'propose', 'strategy', 'visualization', 'analysis', 'present', 'visualization', 'r', 'u', 'c', 'l', 'b', 'r', 'u', 'c', 'l', 'b', 'resnet', 'transformer', 'c35', 'dscnn', 'c35', 'mhattrnn', 'c35', 'resnet', 'c35', 'transformer', 'communication', 'round', 'communication', 'round', 'communication', 'round', 'communication', 'round', 'r', 'u', 'c', 'l', 'b', 'g', 'l', 'ablation', 'c35', 'fedkwsui', 'fedkwsui', 'fedkwsui', 'fedkwsui', 'ablation', 'ablation', 'fedkwsui', 'fedkwsui', 'µ018724', 'fedkwsui', 'µ028729', 'fedkwsui', 'fedkwsui', 'fedkwsui', 'fedkwsui', 'λ1e48607', 'fedkwsui', 'λ1e38623', 'fedkwsui', 'λ1e28279', 'fedkwsui', 'λ1e17699', 'communication', 'round', 'communication', 'round', 'communication', 'round', 'r', 'e', 'l', 'u', 'q', 'g', 'n', 'inferior', 'user', 'inferior', 'user', 'r', 'u', 'c', 'n', 'z', 'l', 'r', 'e', 'e', 'g', 'fedr', 'federate', 'learning', 'restrict', 'softmax', 'label', 'distribution', 'noniid', 'datum', 'pp', 'x', 'sun', 'twostream', 'federated', 'learning', 'reduce', 'communication', 'cost', 'vcip', 'e', 'acar', 'r', 'mough', 'saligrama', 'federated', 'learning', 'base', 'dynamic', 'regularization', 'q', 'modelcontrastive', 'federated', 'learn', 'e', 'cvpr', 'j', 'yosinski', 'clune', 'bengio', 'lipson', 'transfer', 'able', 'feature', 'deep', 'neural', 'network', 'neurip', 'l', 'collin', 'hassani', 'mokhtari', 'shakkottai', 'exploit', 'e', 'share', 'representation', 'personalized', 'federated', 'learning', 'icml', 'p', 'p', 'l', 'think', 'locally', 'act', 'globally', 'federate', 'learning', 'local', 'global', 'representation', 'fedphp', 'federate', 'personalization', 'inherit', 'private', 'model', 'explore', 'privatize', 'dnn', 'base', 'federate', 'learn', 'e', 'different', 'noniid', 'scene', 'vol', 'abs210711954', 'g', 'vinyal', 'dean', 'distil', 'knowledge', 'neural', 'network', 'federate', 'mutual', 'learning', 'c', 'avestimehr', 'group', 'knowledge', 'transfer', 'federate', 'learning', 'large', 'cnn', 'edge', 'neurip', 'madhyastha', 'chowdhury', 'fedscale', 'benchmarking', 'model', 'system', 'performance', 'erated', 'abadi', 'chu', 'goodfellow', 'b', 'deep', 'learning', 'differential', 'vacy', 'reference', 'jaitly', 'v', 'vinyal', 'listen', 'attend', 'spell', 'neural', 'network', 'large', 'vocabulary', 'conversational', 'speech', 'recognition', 'seltzer', 'stolcke', 'conversational', 'speech', 'recognition', 'system', 'g', 'heigold', 'smallfootprint', 'keyword', 'spotting', 'use', 'deep', 'neural', 'network', 'pp', 'sainath', 'parada', 'convolutional', 'neural', 'network', 'smallfootprint', 'keyword', 'spot', 'interspeech', 'chandra', 'edge', 'spot', 'microcontroller', 'vol', 'abs171107128', 'rybakov', 'visontai', 'laurenzo', 'stream', 'spot', 'mobile', 'device', 'interspeech', 'r', 'tang', 'deep', 'residual', 'learning', 'smallfootprint', 'spotting', 'berg', 'o’connor', 'former', 'selfattention', 'model', 'spot', 'corr', 'vol', 'abs210400769', 'p', 'warden', 'speech', 'command', 'dataset', 'limitedvocabulary', 'speech', 'recognition', 'vol', 'abs180403209', 'b', 'e', 'moore', 'ramage', 'hampson', 'cas', 'communicationefﬁcient', 'learning', 'deep', 'network', 'decentralized', 'datum', 'aistat', 'q', 'machine', 'learn', 'e', 'concept', 'application', 'acm', 'tist', 'vol', 'pp', 'suda', 'civin', 'feed', 'erated', 'learning', 'vol', 'sahu', 'sanjabi', 'talwalkar', 'optimization', 'heterogeneous', 'network', 'mlsy', 'federate', 'learning', 'positionaware', 'neuron', 'r', 'glass', 'ast', 'audio', 'spectrogram', 'transformer', 'vol', 'abs210401778', 'park', 'normalization', 'neural', 'audio', 'datum', 'processing', 'leroy', 'coucke', 'lavril', 'gisselbrecht', 'federate', 'learn', 'spotting', 'hard', 'partridge', 'subrahmanya', 'shah', 'p', 'lopezmoreno', 'r', 'mathew', 'train', 'keyword', 'spot', 'model', 'noniid', 'datum', 'federated', 'learning', 'terspeech', 'adaptive', 'federated', 'optimization', 'kingsbury', 'federate', 'acoustic', 'modeling', 'automatic', 'speech', 'recognition', 'mohan', 'weber', 'crosssilo', 'federate', 'train', 'ing', 'cloud', 'diversity', 'scaling', 'semisupervise', 'learn', 'e', '3085–3089', 'guliani', 'beaufay', 'g', 'motta', 'training', 'speech', 'recogni', 'tion', 'model', 'federated', 'learn', 'qualitycost', 'framework']"
Fast Finite Width Neural Tangent Kernel,"[{'href': 'http://arxiv.org/abs/2206.08720v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2206.08720v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-06-17 12:18:22,"2
2
0
2

n
u
J

9

]

G
L
.
s
c
[

2
v
8
1
2
0
1
.
5
0
2
2
:
v
i
X
r
a

Learning Task-relevant Representations for Generalization via
Characteristic Functions of Reward Sequence Distributions

Rui Yang
yr0013@mail.ustc.edu.cn
University of Science and Technology
of China

Jie Wang∗
jiewangx@ustc.edu.cn
Institute of Artificial Intelligence
Hefei Comprehensive National
Science Center
University of Science and Technology
of China

Zijie Geng
ustcgzj@mail.ustc.edu.cn
University of Science and Technology
of China

Mingxuan Ye
mingxuanye@miralab.ai
University of Science and Technology
of China

Shuiwang Ji
sji@tamu.edu
Texas A&M University
College Station, TX

Bin Li
binli@ustc.edu.cn
University of Science and Technology
of China

Feng Wu
fengwu@ustc.edu.cn
University of Science and Technology
of China

ABSTRACT
Generalization across different environments with the same tasks is
critical for successful applications of visual reinforcement learning
(RL) in real scenarios. However, visual distractions—which are com-
mon in real scenes—from high-dimensional observations can be
hurtful to the learned representations in visual RL, thus degrading
the performance of generalization. To tackle this problem, we pro-
pose a novel approach, namely Characteristic Reward Sequence
Prediction (CRESP), to extract the task-relevant information by
learning reward sequence distributions (RSDs), as the reward sig-
nals are task-relevant in RL and invariant to visual distractions.
Specifically, to effectively capture the task-relevant information via
RSDs, CRESP introduces an auxiliary task—that is, predicting the
characteristic functions of RSDs—to learn task-relevant represen-
tations, because we can well approximate the high-dimensional
distributions by leveraging the corresponding characteristic func-
tions. Experiments demonstrate that CRESP significantly improves
the performance of generalization on unseen environments, out-
performing several state-of-the-arts on DeepMind Control tasks
with different visual distractions.

∗ Corresponding Author.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
KDD ’22, August 14–18, 2022, Washington, DC, USA
© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9385-0/22/08. . . $15.00
https://doi.org/10.1145/3534678.3539391

CCS CONCEPTS
• Computing methodologies → Sequential decision making;
Image representations; Markov decision processes.

KEYWORDS
Task-relevant representation learning, reward sequence, character-
istic function, generalization, visual reinforcement learning

ACM Reference Format:
Rui Yang, Jie Wang∗, Zijie Geng, Mingxuan Ye, Shuiwang Ji, Bin Li, and Feng
Wu. 2022. Learning Task-relevant Representations for Generalization via
Characteristic Functions of Reward Sequence Distributions. In Proceedings of
the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD ’22), August 14–18, 2022, Washington, DC, USA. ACM, New York, NY,
USA, 11 pages. https://doi.org/10.1145/3534678.3539391

1 INTRODUCTION
Visual reinforcement learning (RL) algorithms aim to solve com-
plex control tasks from high-dimensional visual observations. No-
table successes include DrQ for locomotion control [30], IMPALA
for multi-task learning [5], and QT-Opt for robot grasping [13].
Although these methods perform well on training environments,
they can hardly generalize to new environments, even these environ-
ments are semantically similar to the training environments. This
is because image observations often involve many task-irrelevant
visual factors, such as dynamic backgrounds and colors of the object
under control. Minor changes in such visual factors may cause large
distributional shifts of the environments, which prevent the agent
from extracting underlying task-relevant information when we put
it into a new environment. This indicates that many existing RL
agents memorize the trajectories on specific environments [22, 25],
rather than learning transferable skills.

To learn a policy with transferable skills for generalization,
many prior works focus on learning representations that encode

 
 
 
 
 
 
KDD ’22, August 14–18, 2022, Washington, DC, USA

Yang et al.

functions on top of the reward sequence representations. Experi-
ments on DeepMind Control Suite [27] with visual distractors [26]
demonstrate that CRESP significantly improves several state-of-
the-arts on unseen environments.

Our main contributions in this paper are as follows:

• We introduce the reward sequence distributions (RSDs) to
discard the task-irrelevant features and preserve the task-
relevant features.

• We propose CRESP, a novel approach that extracts the task-
relevant information by learning the characteristic functions
of RSDs for representation learning.

• Experiments demonstrate that the representations learned
by CRESP preserve more task-relevant features than prior
methods, outperforming several state-of-the-arts on the ma-
jority of tasks by substantial margins.

2 RELATED WORK

Generalization in visual RL. The study of generalization in deep
RL focuses on the capability of RL methods to generalize to un-
seen environments under a limited set of training environments.
Several works propose to apply regularization techniques origi-
nally developed for supervised learning, including dropout [12]
and batch normalization [7, 12]. Although practical and easy to im-
plement, these methods do not exploit any properties of sequential
decision-making problems. Other approaches for preventing over-
fitting focus on data augmentation [17, 19, 21, 31], which enlarge
the available data space and implicitly provide the prior knowledge
to the agent. Although these methods show promising results in
well-designed experimental settings, strong assumptions such as
prior knowledge of the testing environments may limit their real ap-
plications. In contrast to these methods, we consider a more realistic
setting without assuming this prior knowledge of environments.

Representation Learning in visual RL. Many prior works focus on
representation learning for generalization in visual RL. Some of the
works [14, 15] use a two-step learning process, which first trains an
auto-encoder by using a reconstruction loss for low-dimensional
representations, and then uses this representation for policy opti-
mization. However, such representations encode all elements from
observations, whether they are relevant to the task or not. Other
works use bisimulation metrics to learn a representation that is
invariant to irrelevant visual features [34]. However, such methods
use the transition dynamics, which vary with the environments,
leading to the learned representation involving task-irrelevant fea-
tures of the visual distractions. A recent study [20] leverages the
reward prediction for representation learning. However, the rep-
resentation learning method only considers finite MDPs, which
cannot extend to visual RL tasks.

Characteristic Functions of Random Variables. Characteristic func-
tions are the Fourier transforms of probability density functions.
They are well studied in probability theory and can be used to
specify high-dimensional distributions. This is because two random
variables have the same distribution if and only if they have the
same characteristic function. Some prior works [2, 32] use character-
istic functions to solve some statistical problems. We leverage this
tool for a simple and tractable approximation of high-dimensional

Figure 1: The agent-environment interactions in Block
MDPs with visual distractions. Each environment 𝑒 provides
a state 𝑠𝑡 and a background 𝑥𝑡 , which generate an observa-
tion 𝑜𝑡 = 𝑔(𝑠𝑡 , 𝑥𝑡 ) through a nonlinear function 𝑔. The agent
receives 𝑜𝑡 and takes an action 𝑎𝑡 in 𝑒, leading to the transi-
tions of states (from 𝑠𝑡 to 𝑠𝑡 +1), backgrounds (from 𝑥𝑡 to 𝑥𝑡 +1),
and thus the observation transitions (from 𝑜𝑡 to 𝑜𝑡 +1). Notice
that the red arrows represent the transitions that vary with
different environments, while the blue arrow represents the
transition invariant to environments.

only the task-relevant information while discarding task-irrelevant
visual factors. Some of them propose similarity metrics [3, 16] to
find semantically equivalent observations for representation learn-
ing [1, 34]. Others design objectives by integrating MDP proper-
ties to learn a causal representation that is invariant to irrelevant
features [23, 33]. These aforementioned methods leverage rewards
and transition dynamics to capture task-relevant features. However,
the observation transition dynamics (see Figure 1) may induce the
task-irrelevant information relating to visual distractions into the
representations, thus hindering generalization [24, 33]. Detailed
discussions are in Section 4.1.

In contrast to the above methods, we propose a novel approach,
namely Characteristic Reward Sequence Prediction (CRESP), which
only uses reward signals but observation transition dynamics to
learn task-relevant representations, as the reward signals are task-
relevant in RL and invariant to visual factors. To preserve infor-
mation that is relevant to the task, CRESP introduces the reward
sequence distributions (RSDs), which are the conditional distribu-
tions of reward sequences given a starting observation and various
subsequent actions. CRESP leverages RSDs to learn a task-relevant
representation that only encodes the information of RSDs, which
we call reward sequence representation. Specifically, considering that
the characteristic function can specify high-dimensional distribu-
tions [2], we propose to learn such task-relevant representation by
an auxiliary task that predicts the characteristic functions of RSDs.
Moreover, we provide a theoretical analysis of the value bounds
between the true optimal value functions and the optimal value

...

Env Env Env 

Agent

Env 
Env Env 

...

Agent

observation transition dynamics

Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions KDD ’22, August 14–18, 2022, Washington, DC, USA

distributions. Our experiments demonstrate that the characteris-
tic functions perform well to specify the distributions of reward
sequences in our method.

3 PRELIMINARIES
In visual RL tasks, we deal with high-dimensional image observa-
tions, instead of the states as the inputs. We consider a family of
environments with the same high-level task but different visual dis-
tractions. Denote E as the set of these environments. We model each
environment 𝑒 ∈ E as a Block Markov Decision Process (BMDP) [4,
33], which is described by a tuple M𝑒 = (S, O, A, R, 𝑝, 𝑝𝑒, 𝛾). Here
S is the state space, O is the observation space, A is the action
space, R is the reward space, which we assume to be bounded,
𝑝 (𝑠 ′, 𝑟 |𝑠, 𝑎) is the state transition probability, 𝑝𝑒 (𝑜 ′, 𝑟 |𝑜, 𝑎) is the
observation transition probability, which varies with environments
𝑒 ∈ E, and 𝛾 ∈ [0, 1) is the discount factor.

At each time step 𝑡, we suppose that the environment is in a state
𝑆𝑡 .1 The agent, instead of directly achieving 𝑆𝑡 , obtains an observa-
tion 𝑂𝑡 on environment 𝑒 ∈ E. It is reasonable to assume that the
observation is determined by the state and some task-irrelevant
visual factors that vary with environments, such as backgrounds
or agent colors in DeepMind Control tasks. Symbolically, let X
be the set of such visual factors. We suppose that there exists an
observation function 𝑔 : S × X → O [4, 25] such that 𝑂𝑡 = 𝑔(𝑆𝑡 , 𝑋𝑡 ),
where 𝑋𝑡 is a random variable in X, independent with 𝑆𝑡 and 𝐴𝑡 ,
with a transition probability 𝑞𝑒 (𝑥 ′|𝑥). See Figure 1 for an illustra-
tion. We aim to find a policy 𝜋 (·|𝑜𝑡 ) that maximizes the expected
accumulated reward E𝑒 (cid:2)(cid:205)∞
𝛾𝑡 𝑅𝑡 (cid:3) simultaneously in all environ-
𝑡 =0
ments 𝑒 ∈ E, where E𝑒 [·] means that the expectation is taken in
the environment 𝑒.

Moreover, we assume that the environments follow a general-
ized Block structure [4, 33]. That is, an observation 𝑜 ∈ O uniquely
determines its generating state 𝑠, and the visual factor 𝑥. This
assumption implies that the observation function 𝑔(𝑠, 𝑥) is invert-
ible with respect to both 𝑠 and 𝑥. For simplicity, we denote 𝑠 = [𝑜]𝑠
and 𝑥 = [𝑜]𝑥 as the generating state and visual factor, respectively.
Furthermore, we have 𝑝𝑒 (𝑜 ′, 𝑟 |𝑜, 𝑎) = 𝑝 (𝑠 ′, 𝑟 |𝑠, 𝑎)𝑞𝑒 (𝑥 ′|𝑥), where
𝑠 = [𝑜]𝑠, 𝑠 ′ = [𝑜 ′]𝑠 , 𝑥 = [𝑜]𝑥 and 𝑥 ′ = [𝑜 ′]𝑥 .

4 REPRESENTATION LEARNING VIA

REWARD SEQUENCE DISTRIBUTIONS

An encoder, or a representation, refers to an embedding function
Φ : O → Z, which maps the observational inputs onto a latent
state representation space Z. Our goal is to find a suitable represen-
tation that encodes only task-relevant information and is invariant
to visual distractions. In Section 4.1, we discuss the notion of task
relevance in visual RL, introduce reward sequence distributions
(RSDs), and formulate the reward sequence representations for gen-
eralization. In Section 4.2, we provide a theoretical analysis that
reformulates the reward sequence representation via the charac-
teristic functions of RSDs. In Section 4.3, we present a practical
method, based on the prediction of characteristic functions of RSDs,
to learn such a reward sequence representation.

1Throughout this paper, we use uppercase letters such as 𝑆𝑡 and 𝑂𝑡 to denote random
variables, and use lowercase letters such as 𝑠𝑡 and 𝑜𝑡 to denote the corresponding
values that the random variables take.

Figure 2: The relationship between observations and RSD
mappings. We can divide the observation space into differ-
ent equivalence classes, where the equivalent observations
are generated from the same state. Each equivalence class
corresponds to a same mapping from action sequences a ∈
A𝑇 to reward sequence distributions 𝑝 (·|𝑜, a) ∈ Δ(R𝑇 ).

4.1 Task-relevant Invariance in Visual RL
The key idea of our approach is to capture the task-relevant infor-
mation across different environments from observations, and lever-
age such information for representation learning to improve the
performance of generalization.

Reward signals and transition dynamics are major properties
of MDP, which are commonly used for representation learning in
visual RL. We start with a discussion on the distractions induced
by observation transition dynamics. In visual RL, we can hardly
learn about the state transition dynamics, as the state space is un-
available in practice. Instead, many methods learn the observation
transition dynamics by a probabilistic dynamics model [28, 29, 34].
However, the observation transition dynamics are relevant to the
visual factors because they comprise the transition dynamics of
both states and task-irrelevant visual factors. Formally, we have
the reward and observation transition dynamics 𝑝𝑒 (𝑜 ′, 𝑟 |𝑜, 𝑎) =
𝑝 (𝑠 ′, 𝑟 |𝑠, 𝑎)𝑞𝑒 (𝑥 ′|𝑥). This formula shows that the observation tran-
sition probability varies with the environment 𝑒 ∈ E. We present a
case in Figure 1 to illustrate the observation transition dynamics.
Therefore, representations that encode information about observa-
tion transition dynamics are subject to visual distractions and have
difficulty learning transferable skills.

In contrast to observation transition dynamics, the distributions
of reward signals are relevant to the RL tasks and are invariant
to visual distractions. Formally, if two observations 𝑜 and 𝑜 ′ are
generated by the same state 𝑠, i.e., [𝑜]𝑠 = [𝑜 ′]𝑠 , then we have
𝑝𝑒 (𝑟 |𝑜, 𝑎) = 𝑝𝑒 (𝑟 |𝑜 ′, 𝑎) for any 𝑎 ∈ A and 𝑒 ∈ E. This motivates us
to use the reward signals instead of observation transition dynamics
for representation learning. As our goal is to maximize the expected
accumulative rewards, what we need is not only the current reward
but also the sequences of future rewards. Therefore, we propose to
utilize the reward sequences for representation learning.

For a mathematical formulation, we introduce some new nota-
tions. We denote A𝑇 = {a = (𝑎1, · · · , 𝑎𝑇 ) : 𝑎𝑖 ∈ A} and R𝑇 =
{r = (𝑟1, · · · , 𝑟𝑇 ) : 𝑟𝑖 ∈ R} as the spaces of action sequences
and reward sequences with length 𝑇 , respectively. Let Δ(R𝑇 ) be
the set of probability distributions over R𝑇 . At each time step 𝑡,

Observation Space

Action sequence space

RSD space

Action sequence space

RSD space

𝐚

𝑝(∙ |𝑜, 𝐚)

𝐴𝑇

∆(𝑅𝑇)

𝐴𝑇

∆(𝑅𝑇)

Set of RSD Mappings

KDD ’22, August 14–18, 2022, Washington, DC, USA

Yang et al.

the sequence of the subsequent actions A𝑇
𝑡 = (𝐴𝑡 , · · · , 𝐴𝑡 +𝑇 −1)
is a 𝑇 -dimensional random vector over A𝑇 . The sequence of the
subsequent rewards R𝑇
𝑡 +1 = (𝑅𝑡 +1, · · · , 𝑅𝑡 +𝑇 ) is a 𝑇 -dimensional
random vector over R𝑇 . 2

𝑡 +1

𝑡 +1

To clarify our idea, we first consider a deterministic environ-
ment. Starting from an observation 𝑜𝑡 ∈ O, with the corresponding
state 𝑠𝑡 = [𝑜𝑡 ]𝑠 ∈ S, suppose that we perform a given action se-
quence a𝑇
𝑡 = (𝑎𝑡 , · · · , 𝑎𝑡 +𝑇 −1) ∈ A𝑇 and receive a reward sequence
𝑡 +1 = (𝑟𝑡 +1, · · · , 𝑟𝑡 +𝑇 ) ∈ R𝑇 from the environment. This reward
r𝑇
sequence r𝑇
is uniquely determined by the starting state 𝑠𝑡 and
the given action sequence a𝑇
𝑡 . Therefore, we can find that the rela-
tionship between the given action sequence a𝑇
𝑡 and the received
reward sequence r𝑇
is invariant to visual distractions. We can use
such a relationship to identify the task-relevant information from
observations. To formulate this relationship, we consider the map-
pings from action sequences a ∈ A𝑇 to the corresponding reward
sequences r ∈ R𝑇 —that the agent receives from an observation 𝑜
by following the action sequence a. We consider two observations
𝑜 and 𝑜 ′ that have same mappings from a ∈ A𝑇 to r ∈ R𝑇 for any
dimension 𝑇 . In other words, we suppose that the agent receives the
equal reward sequence r ∈ R𝑇 from 𝑜 and 𝑜 ′, when it follows any
action sequence a ∈ A𝑇 for any 𝑇 . Then the two observations have
similar task properties, in the sense that the agent will receive the
equal accumulative rewards from 𝑜 and 𝑜 ′ no matter what actions
the agent takes. Therefore, the mappings from action sequences
a ∈ A𝑇 to the corresponding reward sequences r ∈ R𝑇 can be used
to identify the task-relevant information from the observations.

We then consider the stochastic environment, the case of which
is similar to the deterministic environment. In the stochastic en-
vironment, the reward sequence R𝑇
is random even for fixed
𝑡 +1
observation 𝑜𝑡 and action sequence A𝑇
𝑡 . Therefore, we cannot sim-
ply consider the mappings from A𝑇 to R𝑇 . Instead, we apply the
mappings from A𝑇 to Δ(R𝑇 ), which map the action sequences to
the distributions of the sequences of reward random variables.

Formally, let 𝑝 (r|𝑜, a) be the probability density function of the
at the point r ∈ R𝑇 , conditioned on the starting
𝑡 = a ∈ A𝑇 . For any

random vector R𝑇
observation 𝑂𝑡 = 𝑜 and the action sequence A𝑇
𝑜 ∈ O, a = (𝑎1, · · · , 𝑎𝑇 ), and r = (𝑟2, · · · , 𝑟𝑇 +1), we have

𝑡 +1

𝑝 (r|𝑜, a) = 𝑝 (𝑟2|𝑠, 𝑎1)𝑝 (𝑟3|𝑠, 𝑎1, 𝑎2) · · · 𝑝 (𝑟𝑇 +1|𝑠, 𝑎1, · · · , 𝑎𝑇 ),
where 𝑠 = [𝑜]𝑠 , and 𝑝 (𝑟 |𝑠, 𝑎1, · · · , 𝑎𝑡 ) denotes the probability den-
sity function of the reward 𝑟 that the agent receives, after fol-
lowing an action sequence (𝑎1, · · · , 𝑎𝑡 ), starting from the state 𝑠.
Furthermore, for any 𝑜, 𝑜 ′ ∈ O such that [𝑜]𝑠 = [𝑜 ′]𝑠 , we have
𝑝 (r|𝑜, a) = 𝑝 (r|𝑜 ′, a). The formulas imply that the conditional dis-
tributions 𝑝 (·|𝑜, a) of reward sequences are determined by the gen-
erating states of the observations as well as the action sequences.
Therefore, the mappings from the action sequences a ∈ A𝑇 to
the corresponding RSDs 𝑝 (·|𝑜, a) are task-relevant and invariant
to visual distractions. Thus we can use the mappings to determine
task relevance. See Figure 2 for an illustration.

The analysis above motivates our method that leverages the RSDs
to learn representations. Specifically, we learn a representation

2We use bold uppercase letters such as A and R to denote random vectors in high-
dimensional spaces and use bold lowercase letters such as a and r to denote determin-
istic vectors in such spaces.

that can derive a function, which maps the action sequences to
the corresponding RSDs. Formally, we define the 𝑇 -level reward
sequence representation as follows.

Definition 4.1. A representation Φ : O → Z is a 𝑇 -level reward
sequence representation if it can derive the distribution of any reward
sequence received from any observation by following any action
sequence with length 𝑇 , i.e., there exists 𝑓 such that

𝑓 (r; Φ(𝑜), a) = 𝑝 (r|𝑜, a), ∀ r ∈ R

𝑇 , 𝑜 ∈ O, a ∈ A𝑇 .

Intuitively, the 𝑇 -level reward sequence representation encodes
the task-relevant information about the relation between the action
sequences a and the RSDs 𝑝 (r|𝑜, a) in the next 𝑇 steps. Notice that
a 𝑇 -level reward sequence representation is also a 𝑇 ′-level reward
sequence representation, where 𝑇 ,𝑇 ′ ∈ N∗ and 𝑇 > 𝑇 ′. If 𝑇 tends to
infinity, the representation will encode all task-relevant information
from the objective of RL tasks. This derives the following definition.

Definition 4.2. A representation Φ : O → Z is a reward sequence
representation if it is a 𝑇 -level reward sequence representation for
all 𝑇 ∈ N∗.

The reward sequence representation is equivalent to a ∞-level
reward sequence representation. In practice, we learn a finite 𝑇 -
level reward sequence representation as an approximation of the
reward sequence representation. To provide a theoretical guarantee
for the approximation, the following theorem gives a value bound
between the true optimal value function and the value function on
top of the 𝑇 -level reward sequence representation.

Theorem 4.3. Let Φ : O → Z be a 𝑇 -level representation, 𝑉 𝑒
∗ :
O → R be the optimal value function in the environment 𝑒 ∈ E, ¯𝑉 𝑒
∗ :
Z → R be the optimal value function on the latent representation
space, built on top of the representation Φ. Let ¯𝑟 be a bound of the
reward space, i.e., |𝑟 | < ¯𝑟 for any 𝑟 ∈ R. Then we have

0 ≤ 𝑉 𝑒

∗ (𝑜) − ¯𝑉 𝑒

∗ ◦ Φ(𝑜) ≤

2𝛾𝑇
1 − 𝛾

¯𝑟,

for any 𝑜 ∈ O and 𝑒 ∈ E.

Proof. See Appendix A.1

□

4.2 Characteristic Functions for
Representation Learning

In Section 4.1, we formulate the 𝑇 -level reward sequence repre-
sentation that can derive the probability density function 𝑝 (r|𝑜, a),
where r ∈ R𝑇 is a reward sequence, 𝑜 ∈ O is an observation, and
a ∈ A𝑇 is an action sequence. However, learning the probability
density functions is usually technically impractical [2]. Leveraging
the characteristic function of random vectors, we propose an al-
ternative approach, which is simple to implement and effective to
learn the distributions.

Consider a random vector R defined on the space R𝑇 , with
a probability density function 𝑝R (·). The characteristic function
𝜑R : R𝑇 → C of R is defined as

𝜑R (𝝎) = ER∼𝑝R ( ·)

𝑒𝑖 ⟨𝝎,R⟩ (cid:105)
(cid:104)

=

∫

𝑒𝑖 ⟨𝝎,r⟩𝑝R (r)dr,
√

where 𝝎 ∈ R𝑇 denotes the input of 𝑝R (·), and 𝑖 =
−1 is the
imaginary unit. Since we consider discounted cumulative rewards

Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions KDD ’22, August 14–18, 2022, Washington, DC, USA

Figure 3: The overall architecture of CRESP. CRESP minimizes the prediction loss to train an encoder Φ, and simultaneously
uses Φ to learn a policy in an actor-critic setting. In the prediction task, CRESP predicts the characteristic functions of reward
sequence distributions through the encoder Φ and the predictor Ψ (in the purple box). The prediction loss L N
D (Φ, Ψ) provides
the gradients (red lines) to update both the predictor Ψ and the encoder Φ. Here r𝑇
𝑡 are the sequences drawn from a
replay buffer D. The inputs 𝝎 of characteristic functions are sampled from a Gaussian distribution N .

𝑡 +1 and a𝑇

𝑡 =1

in RL tasks, we use ⟨·, ·⟩ to denote the weighted inner product in
R𝑇 , i.e., ⟨𝝎, r⟩ = (cid:205)𝑇

𝛾𝑡 𝜔𝑡𝑟𝑡 , where 𝛾 is the discounted factor.

Characteristic functions are useful tools well studied in probabil-
ity theory. In contrast to the probability density function, the char-
acteristic function has some good basic properties. 1) |𝜑R (𝝎)| ≤
𝑒𝑖 ⟨𝝎,R⟩(cid:12)
(cid:12)
ER∼𝑝R ( ·)
(cid:12) = 1, which indicates that the characteristic func-
(cid:12)
(cid:12)
(cid:12)
tion always exists and is uniformly bounded. 2) The characteris-
tic function 𝜑R is uniformly continuous on R𝑇 , which makes it
tractable for learning.

The following lemma states a fact that the distribution of a ran-

dom vector can be specified by its characteristic function.

Lemma 4.4.

[8] Two random vectors X and Y have the same
characteristic function if and only if they have the same probability
distribution function.

This lemma implies that we can recapture the information about
the distributions of random vectors via their characteristic functions.
Therefore, instead of learning the conditional density functions
of reward sequences that are intractable, we propose to leverage
characteristic functions of the RSDs for representation learning.
Specifically, we have the following theorem.

Theorem 4.5. A representation Φ : O → Z is a 𝑇 -level reward
sequence representation if and only if there exits a predictor Ψ such
that for all 𝒘 ∈ R𝑇 , 𝑜 ∈ O and a ∈ A𝑇 ,

Ψ(𝝎; Φ(𝑜), a) = 𝜑R|𝑜,a (𝝎) = ER∼𝑝 ( · |𝑜,a)

𝑒𝑖 ⟨𝝎,R⟩ (cid:105)
(cid:104)

.

Proof. See Appendix A.2.

□

Theorem 4.5 provides an equivalent definition of 𝑇 -level reward
sequence representation and inspires our novel approach to predict
the characteristic functions of RSDs for representation learning.

Algorithm 1 Characteristic Reward Sequence Prediction

Initialize a replay buffer D, a policy 𝜋, a representation Φ, and a
function approximator Ψ
for each iteration do
for 𝑒 in E do

for each environment step 𝑡 do

Execute action 𝑎𝑡 ∼ 𝜋 (·|Φ(𝑜𝑡 ))
Receive a transition 𝑜𝑡 +1, 𝑟𝑡 +1 ∼ 𝑝𝑒 (·|𝑜𝑡 , 𝑎𝑡 )
Record partial trajectories {(𝑜𝑡 −𝑖, 𝑎𝑡 −𝑖, 𝑟𝑡 +1−𝑖 )}𝑇 −1
𝑖=0

in D

end for

end for
for each gradient step do

Sample partial trajectories from D
Update the representation: L N
Update the policy: LRL (𝜋)

D (Φ, Ψ)

end for

end for

4.3 Characteristic Reward Sequence Prediction
To improve the generalization of a learned policy on unseen en-
vironments with visual distractions, we propose Characteristic
Reward Sequence Prediction (CRESP), a novel approach to learn
representations for task relevance from high-dimensional observa-
tions. As discussed above, CRESP learns RSDs by predicting the
characteristic functions 𝜑R|𝑜,a (𝝎). In this section, we focus on the
detailed learning procedure for the prediction.

For an observation 𝑜 ∈ O and an action sequence a ∈ A𝑇 , the
true characteristic function of the corresponding reward sequence
is 𝜑R|𝑜,a (𝝎) = ER∼𝑝 ( · |𝑜,a) [𝑒𝑖 ⟨𝝎,R⟩]. We estimate the characteristic
function by a predictor Ψ(𝝎; Φ(𝑜), a). We use the weighted squared
distance between the true and predicted characteristic functions as

Characteristic Reward Sequence Prediction

𝑠𝑡

𝑥𝑡

𝑇 = (𝑟𝑡+1, … , 𝑟𝑇)
𝒓𝑡+1

𝝎 = 𝜔1, … , 𝜔𝑇−𝑡

𝑇 = 𝑎𝑡, … , 𝑎𝑇−1
𝐚𝑡

. ..

. . .

𝑔

𝑜𝑡

Encoder
𝚽

Characteristic
Function

signals

Prediction Loss
𝒩(𝚽, 𝚿)

ℒ𝒟

𝝍cos

𝝍sin

grad

Predictor 𝚿

𝚿 𝝎; 𝚽 𝑜𝑡 , 𝐚𝑡

Critic

𝑄 𝑜𝑡, 𝑎𝑡

Actor

𝑎𝑡

RL Loss

KDD ’22, August 14–18, 2022, Washington, DC, USA

Yang et al.

Figure 4: Learning curves of six methods on six tasks with dynamic background distractions for 500K environment steps.
The solid curves denote the means and the shaded regions denote the minimum and maximum returns over 6 trials. Each
checkpoint is evaluated by 10 episodes on unseen environments. Curves are smoothed for visual clarity.

the prediction loss:

𝐿W (Φ, Ψ|𝑜, a) = E𝛀∼W

(cid:104)(cid:13)
(cid:13)Ψ (𝛀; Φ(𝑜), a) − 𝜑R|𝑜,a (𝛀)(cid:13)
2
(cid:13)
2
(cid:12)Ψ (𝝎; Φ(𝑜), a) − 𝜑R|𝑜,a (𝝎)(cid:12)
(cid:12)
2 W (𝝎)d𝝎,
(cid:12)

(cid:105)

=

∫

R𝑇

where W is any probability density function on R𝑇 . We optimize
the expected loss for observations and action sequences taken from
the replay buffer D:

𝐿W
D (Φ, Ψ) = E(𝑂,A)∼D

(cid:104)

𝐿W (Φ, Ψ|𝑂, A)

(cid:105)

= E(𝑂,A)∼D,𝛀∼W

(cid:104)(cid:13)
(cid:13)Ψ (𝛀; Φ(𝑂), A) − 𝜑R|𝑂,A (𝛀)(cid:13)
(cid:13)

(cid:105)

2
2

.

In practice, Since we have no access to the true characteristic

functions, we propose to optimize an upper bound on 𝐿W
D

:

L W

D (Φ, Ψ)

= E(𝑂,A,R)∼D,𝛀∼W
(cid:20)(cid:13)
(cid:13)
(cid:13)

≥ E(𝑂,A)∼D,𝛀∼W

(cid:20)(cid:13)
(cid:13)
(cid:13)

Ψ (𝛀; Φ(𝑂), A) − 𝑒𝑖 ⟨𝛀,R⟩(cid:13)
2
(cid:13)
(cid:13)
2

(cid:21)

Ψ (𝛀; Φ(𝑂), A) − ER∼𝑝 ( · |𝑂,A)

(cid:21)

𝑒𝑖 ⟨𝛀,R⟩ (cid:105)(cid:13)
(cid:104)
2
(cid:13)
(cid:13)
2

= 𝐿W

D (Φ, Ψ).
Due to the complex form of characteristic functions, we divide
the predictor Ψ into two parts Ψ = (𝜓cos,𝜓sin), where 𝜓cos es-
timates the real parts, and 𝜓sin estimates the imaginary parts of
characteristic functions, respectively. Moreover, we draw Ω from
a Gaussian distribution W = N (𝝁, 𝝈 2) in practice. We then pa-
rameterize this distribution N (𝝁, 𝝈 2) and perform ablation on it
in Appendix B.1. Based on the experimental results, we leverage

D (Φ, Ψ) = E(𝑂,A,R)∼D,𝛀∼N

the standard Gaussian distribution N . Then the loss function is:
L N

(cid:2)∥𝜓cos (𝛀; Φ(𝑂), A) − cos (⟨𝛀, R⟩)∥2
2
(cid:3) .
+ ∥𝜓sin (𝛀; Φ(𝑂), A) − sin (⟨𝛀, R⟩)∥2
2
In the training process, we update the encoder Φ and the pre-
dictor Ψ due to the auxiliary loss L N
D (Φ, Ψ), and use the trained
encoder Φ for the RL tasks. The whole architecture of CRESP and
training procedure are illustrated in Figure 3 and Algorithm 1.

5 EXPERIMENTS
In this paper, we improve the performance of generalization on
unseen environments with visual distractions. We focus on training
agents in multi-environments under traditional off-policy settings
without any prior environmental knowledge, such as strong aug-
mentations designed for visual factors [6, 19, 35], fine-tuning in test
environments [11], or environmental labels for invariance [1, 24].
We then investigate the performances of agents trained by different
algorithms on various unseen test environments.

For each environment, we benchmark CRESP extensively against
prior state-of-the-art methods: 1) CURL [18]: a RL method with
an auxiliary contrastive task; 2) DrQ [30]: an effective method
with state-of-the-art performance on DeepMind Control (DMCon-
trol) [27]; 3) MISA [33]: a recent approach from causal inference to
learn invariant representations by approximating one-step rewards
and dynamics; 4) DBC [34]: a research for generalization in RL
to learn representations via the bisimulation metric; 5) SAC [9]: a
traditional off-policy deep RL algorithm.

Network Details. Our method builds upon SAC and follows the
network architecture of DrQ and CURL. We use a 4-layer feed-
forward ConvNet with no residual connection as the encoder. Then,

","2 2 0 2 n u J 9 ] G L . s c [ 2 v 8 1 2 0 1 . 5 0 2 2 : v i X r a Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions Rui Yang yr0013@mail.ustc.edu.cn University of Science and Technology of China Jie Wang∗ jiewangx@ustc.edu.cn Institute of Artificial Intelligence Hefei Comprehensive National Science Center University of Science and Technology of China Zijie Geng ustcgzj@mail.ustc.edu.cn University of Science and Technology of China Mingxuan Ye mingxuanye@miralab.ai University of Science and Technology of China Shuiwang Ji sji@tamu.edu Texas A&M University College Station, TX Bin Li binli@ustc.edu.cn University of Science and Technology of China Feng Wu fengwu@ustc.edu.cn University of Science and Technology of China ABSTRACT Generalization across different environments with the same tasks is critical for successful applications of visual reinforcement learning (RL) in real scenarios. However, visual distractions—which are com- mon in real scenes—from high-dimensional observations can be hurtful to the learned representations in visual RL, thus degrading the performance of generalization. To tackle this problem, we pro- pose a novel approach, namely Characteristic Reward Sequence Prediction (CRESP), to extract the task-relevant information by learning reward sequence distributions (RSDs), as the reward sig- nals are task-relevant in RL and invariant to visual distractions. Specifically, to effectively capture the task-relevant information via RSDs, CRESP introduces an auxiliary task—that is, predicting the characteristic functions of RSDs—to learn task-relevant represen- tations, because we can well approximate the high-dimensional distributions by leveraging the corresponding characteristic func- tions. Experiments demonstrate that CRESP significantly improves the performance of generalization on unseen environments, out- performing several state-of-the-arts on DeepMind Control tasks with different visual distractions. ∗ Corresponding Author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD ’22, August 14–18, 2022, Washington, DC, USA © 2022 Association for Computing Machinery. ACM ISBN 978-1-4503-9385-0/22/08. . . $15.00 https://doi.org/10.1145/3534678.3539391 CCS CONCEPTS • Computing methodologies → Sequential decision making; Image representations; Markov decision processes. KEYWORDS Task-relevant representation learning, reward sequence, character- istic function, generalization, visual reinforcement learning ACM Reference Format: Rui Yang, Jie Wang∗, Zijie Geng, Mingxuan Ye, Shuiwang Ji, Bin Li, and Feng Wu. 2022. Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’22), August 14–18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3534678.3539391 1 INTRODUCTION Visual reinforcement learning (RL) algorithms aim to solve com- plex control tasks from high-dimensional visual observations. No- table successes include DrQ for locomotion control [30], IMPALA for multi-task learning [5], and QT-Opt for robot grasping [13]. Although these methods perform well on training environments, they can hardly generalize to new environments, even these environ- ments are semantically similar to the training environments. This is because image observations often involve many task-irrelevant visual factors, such as dynamic backgrounds and colors of the object under control. Minor changes in such visual factors may cause large distributional shifts of the environments, which prevent the agent from extracting underlying task-relevant information when we put it into a new environment. This indicates that many existing RL agents memorize the trajectories on specific environments [22, 25], rather than learning transferable skills. To learn a policy with transferable skills for generalization, many prior works focus on learning representations that encode KDD ’22, August 14–18, 2022, Washington, DC, USA Yang et al. functions on top of the reward sequence representations. Experi- ments on DeepMind Control Suite [27] with visual distractors [26] demonstrate that CRESP significantly improves several state-of- the-arts on unseen environments. Our main contributions in this paper are as follows: • We introduce the reward sequence distributions (RSDs) to discard the task-irrelevant features and preserve the task- relevant features. • We propose CRESP, a novel approach that extracts the task- relevant information by learning the characteristic functions of RSDs for representation learning. • Experiments demonstrate that the representations learned by CRESP preserve more task-relevant features than prior methods, outperforming several state-of-the-arts on the ma- jority of tasks by substantial margins. 2 RELATED WORK Generalization in visual RL. The study of generalization in deep RL focuses on the capability of RL methods to generalize to un- seen environments under a limited set of training environments. Several works propose to apply regularization techniques origi- nally developed for supervised learning, including dropout [12] and batch normalization [7, 12]. Although practical and easy to im- plement, these methods do not exploit any properties of sequential decision-making problems. Other approaches for preventing over- fitting focus on data augmentation [17, 19, 21, 31], which enlarge the available data space and implicitly provide the prior knowledge to the agent. Although these methods show promising results in well-designed experimental settings, strong assumptions such as prior knowledge of the testing environments may limit their real ap- plications. In contrast to these methods, we consider a more realistic setting without assuming this prior knowledge of environments. Representation Learning in visual RL. Many prior works focus on representation learning for generalization in visual RL. Some of the works [14, 15] use a two-step learning process, which first trains an auto-encoder by using a reconstruction loss for low-dimensional representations, and then uses this representation for policy opti- mization. However, such representations encode all elements from observations, whether they are relevant to the task or not. Other works use bisimulation metrics to learn a representation that is invariant to irrelevant visual features [34]. However, such methods use the transition dynamics, which vary with the environments, leading to the learned representation involving task-irrelevant fea- tures of the visual distractions. A recent study [20] leverages the reward prediction for representation learning. However, the rep- resentation learning method only considers finite MDPs, which cannot extend to visual RL tasks. Characteristic Functions of Random Variables. Characteristic func- tions are the Fourier transforms of probability density functions. They are well studied in probability theory and can be used to specify high-dimensional distributions. This is because two random variables have the same distribution if and only if they have the same characteristic function. Some prior works [2, 32] use character- istic functions to solve some statistical problems. We leverage this tool for a simple and tractable approximation of high-dimensional Figure 1: The agent-environment interactions in Block MDPs with visual distractions. Each environment 𝑒 provides a state 𝑠𝑡 and a background 𝑥𝑡 , which generate an observa- tion 𝑜𝑡 = 𝑔(𝑠𝑡 , 𝑥𝑡 ) through a nonlinear function 𝑔. The agent receives 𝑜𝑡 and takes an action 𝑎𝑡 in 𝑒, leading to the transi- tions of states (from 𝑠𝑡 to 𝑠𝑡 +1), backgrounds (from 𝑥𝑡 to 𝑥𝑡 +1), and thus the observation transitions (from 𝑜𝑡 to 𝑜𝑡 +1). Notice that the red arrows represent the transitions that vary with different environments, while the blue arrow represents the transition invariant to environments. only the task-relevant information while discarding task-irrelevant visual factors. Some of them propose similarity metrics [3, 16] to find semantically equivalent observations for representation learn- ing [1, 34]. Others design objectives by integrating MDP proper- ties to learn a causal representation that is invariant to irrelevant features [23, 33]. These aforementioned methods leverage rewards and transition dynamics to capture task-relevant features. However, the observation transition dynamics (see Figure 1) may induce the task-irrelevant information relating to visual distractions into the representations, thus hindering generalization [24, 33]. Detailed discussions are in Section 4.1. In contrast to the above methods, we propose a novel approach, namely Characteristic Reward Sequence Prediction (CRESP), which only uses reward signals but observation transition dynamics to learn task-relevant representations, as the reward signals are task- relevant in RL and invariant to visual factors. To preserve infor- mation that is relevant to the task, CRESP introduces the reward sequence distributions (RSDs), which are the conditional distribu- tions of reward sequences given a starting observation and various subsequent actions. CRESP leverages RSDs to learn a task-relevant representation that only encodes the information of RSDs, which we call reward sequence representation. Specifically, considering that the characteristic function can specify high-dimensional distribu- tions [2], we propose to learn such task-relevant representation by an auxiliary task that predicts the characteristic functions of RSDs. Moreover, we provide a theoretical analysis of the value bounds between the true optimal value functions and the optimal value ... Env Env Env Agent Env Env Env ... Agent observation transition dynamics Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions KDD ’22, August 14–18, 2022, Washington, DC, USA distributions. Our experiments demonstrate that the characteris- tic functions perform well to specify the distributions of reward sequences in our method. 3 PRELIMINARIES In visual RL tasks, we deal with high-dimensional image observa- tions, instead of the states as the inputs. We consider a family of environments with the same high-level task but different visual dis- tractions. Denote E as the set of these environments. We model each environment 𝑒 ∈ E as a Block Markov Decision Process (BMDP) [4, 33], which is described by a tuple M𝑒 = (S, O, A, R, 𝑝, 𝑝𝑒, 𝛾). Here S is the state space, O is the observation space, A is the action space, R is the reward space, which we assume to be bounded, 𝑝 (𝑠 ′, 𝑟 |𝑠, 𝑎) is the state transition probability, 𝑝𝑒 (𝑜 ′, 𝑟 |𝑜, 𝑎) is the observation transition probability, which varies with environments 𝑒 ∈ E, and 𝛾 ∈ [0, 1) is the discount factor. At each time step 𝑡, we suppose that the environment is in a state 𝑆𝑡 .1 The agent, instead of directly achieving 𝑆𝑡 , obtains an observa- tion 𝑂𝑡 on environment 𝑒 ∈ E. It is reasonable to assume that the observation is determined by the state and some task-irrelevant visual factors that vary with environments, such as backgrounds or agent colors in DeepMind Control tasks. Symbolically, let X be the set of such visual factors. We suppose that there exists an observation function 𝑔 : S × X → O [4, 25] such that 𝑂𝑡 = 𝑔(𝑆𝑡 , 𝑋𝑡 ), where 𝑋𝑡 is a random variable in X, independent with 𝑆𝑡 and 𝐴𝑡 , with a transition probability 𝑞𝑒 (𝑥 ′|𝑥). See Figure 1 for an illustra- tion. We aim to find a policy 𝜋 (·|𝑜𝑡 ) that maximizes the expected accumulated reward E𝑒 (cid:2)(cid:205)∞ 𝛾𝑡 𝑅𝑡 (cid:3) simultaneously in all environ- 𝑡 =0 ments 𝑒 ∈ E, where E𝑒 [·] means that the expectation is taken in the environment 𝑒. Moreover, we assume that the environments follow a general- ized Block structure [4, 33]. That is, an observation 𝑜 ∈ O uniquely determines its generating state 𝑠, and the visual factor 𝑥. This assumption implies that the observation function 𝑔(𝑠, 𝑥) is invert- ible with respect to both 𝑠 and 𝑥. For simplicity, we denote 𝑠 = [𝑜]𝑠 and 𝑥 = [𝑜]𝑥 as the generating state and visual factor, respectively. Furthermore, we have 𝑝𝑒 (𝑜 ′, 𝑟 |𝑜, 𝑎) = 𝑝 (𝑠 ′, 𝑟 |𝑠, 𝑎)𝑞𝑒 (𝑥 ′|𝑥), where 𝑠 = [𝑜]𝑠, 𝑠 ′ = [𝑜 ′]𝑠 , 𝑥 = [𝑜]𝑥 and 𝑥 ′ = [𝑜 ′]𝑥 . 4 REPRESENTATION LEARNING VIA REWARD SEQUENCE DISTRIBUTIONS An encoder, or a representation, refers to an embedding function Φ : O → Z, which maps the observational inputs onto a latent state representation space Z. Our goal is to find a suitable represen- tation that encodes only task-relevant information and is invariant to visual distractions. In Section 4.1, we discuss the notion of task relevance in visual RL, introduce reward sequence distributions (RSDs), and formulate the reward sequence representations for gen- eralization. In Section 4.2, we provide a theoretical analysis that reformulates the reward sequence representation via the charac- teristic functions of RSDs. In Section 4.3, we present a practical method, based on the prediction of characteristic functions of RSDs, to learn such a reward sequence representation. 1Throughout this paper, we use uppercase letters such as 𝑆𝑡 and 𝑂𝑡 to denote random variables, and use lowercase letters such as 𝑠𝑡 and 𝑜𝑡 to denote the corresponding values that the random variables take. Figure 2: The relationship between observations and RSD mappings. We can divide the observation space into differ- ent equivalence classes, where the equivalent observations are generated from the same state. Each equivalence class corresponds to a same mapping from action sequences a ∈ A𝑇 to reward sequence distributions 𝑝 (·|𝑜, a) ∈ Δ(R𝑇 ). 4.1 Task-relevant Invariance in Visual RL The key idea of our approach is to capture the task-relevant infor- mation across different environments from observations, and lever- age such information for representation learning to improve the performance of generalization. Reward signals and transition dynamics are major properties of MDP, which are commonly used for representation learning in visual RL. We start with a discussion on the distractions induced by observation transition dynamics. In visual RL, we can hardly learn about the state transition dynamics, as the state space is un- available in practice. Instead, many methods learn the observation transition dynamics by a probabilistic dynamics model [28, 29, 34]. However, the observation transition dynamics are relevant to the visual factors because they comprise the transition dynamics of both states and task-irrelevant visual factors. Formally, we have the reward and observation transition dynamics 𝑝𝑒 (𝑜 ′, 𝑟 |𝑜, 𝑎) = 𝑝 (𝑠 ′, 𝑟 |𝑠, 𝑎)𝑞𝑒 (𝑥 ′|𝑥). This formula shows that the observation tran- sition probability varies with the environment 𝑒 ∈ E. We present a case in Figure 1 to illustrate the observation transition dynamics. Therefore, representations that encode information about observa- tion transition dynamics are subject to visual distractions and have difficulty learning transferable skills. In contrast to observation transition dynamics, the distributions of reward signals are relevant to the RL tasks and are invariant to visual distractions. Formally, if two observations 𝑜 and 𝑜 ′ are generated by the same state 𝑠, i.e., [𝑜]𝑠 = [𝑜 ′]𝑠 , then we have 𝑝𝑒 (𝑟 |𝑜, 𝑎) = 𝑝𝑒 (𝑟 |𝑜 ′, 𝑎) for any 𝑎 ∈ A and 𝑒 ∈ E. This motivates us to use the reward signals instead of observation transition dynamics for representation learning. As our goal is to maximize the expected accumulative rewards, what we need is not only the current reward but also the sequences of future rewards. Therefore, we propose to utilize the reward sequences for representation learning. For a mathematical formulation, we introduce some new nota- tions. We denote A𝑇 = {a = (𝑎1, · · · , 𝑎𝑇 ) : 𝑎𝑖 ∈ A} and R𝑇 = {r = (𝑟1, · · · , 𝑟𝑇 ) : 𝑟𝑖 ∈ R} as the spaces of action sequences and reward sequences with length 𝑇 , respectively. Let Δ(R𝑇 ) be the set of probability distributions over R𝑇 . At each time step 𝑡, Observation Space Action sequence space RSD space Action sequence space RSD space 𝐚 𝑝(∙ |𝑜, 𝐚) 𝐴𝑇 ∆(𝑅𝑇) 𝐴𝑇 ∆(𝑅𝑇) Set of RSD Mappings KDD ’22, August 14–18, 2022, Washington, DC, USA Yang et al. the sequence of the subsequent actions A𝑇 𝑡 = (𝐴𝑡 , · · · , 𝐴𝑡 +𝑇 −1) is a 𝑇 -dimensional random vector over A𝑇 . The sequence of the subsequent rewards R𝑇 𝑡 +1 = (𝑅𝑡 +1, · · · , 𝑅𝑡 +𝑇 ) is a 𝑇 -dimensional random vector over R𝑇 . 2 𝑡 +1 𝑡 +1 To clarify our idea, we first consider a deterministic environ- ment. Starting from an observation 𝑜𝑡 ∈ O, with the corresponding state 𝑠𝑡 = [𝑜𝑡 ]𝑠 ∈ S, suppose that we perform a given action se- quence a𝑇 𝑡 = (𝑎𝑡 , · · · , 𝑎𝑡 +𝑇 −1) ∈ A𝑇 and receive a reward sequence 𝑡 +1 = (𝑟𝑡 +1, · · · , 𝑟𝑡 +𝑇 ) ∈ R𝑇 from the environment. This reward r𝑇 sequence r𝑇 is uniquely determined by the starting state 𝑠𝑡 and the given action sequence a𝑇 𝑡 . Therefore, we can find that the rela- tionship between the given action sequence a𝑇 𝑡 and the received reward sequence r𝑇 is invariant to visual distractions. We can use such a relationship to identify the task-relevant information from observations. To formulate this relationship, we consider the map- pings from action sequences a ∈ A𝑇 to the corresponding reward sequences r ∈ R𝑇 —that the agent receives from an observation 𝑜 by following the action sequence a. We consider two observations 𝑜 and 𝑜 ′ that have same mappings from a ∈ A𝑇 to r ∈ R𝑇 for any dimension 𝑇 . In other words, we suppose that the agent receives the equal reward sequence r ∈ R𝑇 from 𝑜 and 𝑜 ′, when it follows any action sequence a ∈ A𝑇 for any 𝑇 . Then the two observations have similar task properties, in the sense that the agent will receive the equal accumulative rewards from 𝑜 and 𝑜 ′ no matter what actions the agent takes. Therefore, the mappings from action sequences a ∈ A𝑇 to the corresponding reward sequences r ∈ R𝑇 can be used to identify the task-relevant information from the observations. We then consider the stochastic environment, the case of which is similar to the deterministic environment. In the stochastic en- vironment, the reward sequence R𝑇 is random even for fixed 𝑡 +1 observation 𝑜𝑡 and action sequence A𝑇 𝑡 . Therefore, we cannot sim- ply consider the mappings from A𝑇 to R𝑇 . Instead, we apply the mappings from A𝑇 to Δ(R𝑇 ), which map the action sequences to the distributions of the sequences of reward random variables. Formally, let 𝑝 (r|𝑜, a) be the probability density function of the at the point r ∈ R𝑇 , conditioned on the starting 𝑡 = a ∈ A𝑇 . For any random vector R𝑇 observation 𝑂𝑡 = 𝑜 and the action sequence A𝑇 𝑜 ∈ O, a = (𝑎1, · · · , 𝑎𝑇 ), and r = (𝑟2, · · · , 𝑟𝑇 +1), we have 𝑡 +1 𝑝 (r|𝑜, a) = 𝑝 (𝑟2|𝑠, 𝑎1)𝑝 (𝑟3|𝑠, 𝑎1, 𝑎2) · · · 𝑝 (𝑟𝑇 +1|𝑠, 𝑎1, · · · , 𝑎𝑇 ), where 𝑠 = [𝑜]𝑠 , and 𝑝 (𝑟 |𝑠, 𝑎1, · · · , 𝑎𝑡 ) denotes the probability den- sity function of the reward 𝑟 that the agent receives, after fol- lowing an action sequence (𝑎1, · · · , 𝑎𝑡 ), starting from the state 𝑠. Furthermore, for any 𝑜, 𝑜 ′ ∈ O such that [𝑜]𝑠 = [𝑜 ′]𝑠 , we have 𝑝 (r|𝑜, a) = 𝑝 (r|𝑜 ′, a). The formulas imply that the conditional dis- tributions 𝑝 (·|𝑜, a) of reward sequences are determined by the gen- erating states of the observations as well as the action sequences. Therefore, the mappings from the action sequences a ∈ A𝑇 to the corresponding RSDs 𝑝 (·|𝑜, a) are task-relevant and invariant to visual distractions. Thus we can use the mappings to determine task relevance. See Figure 2 for an illustration. The analysis above motivates our method that leverages the RSDs to learn representations. Specifically, we learn a representation 2We use bold uppercase letters such as A and R to denote random vectors in high- dimensional spaces and use bold lowercase letters such as a and r to denote determin- istic vectors in such spaces. that can derive a function, which maps the action sequences to the corresponding RSDs. Formally, we define the 𝑇 -level reward sequence representation as follows. Definition 4.1. A representation Φ : O → Z is a 𝑇 -level reward sequence representation if it can derive the distribution of any reward sequence received from any observation by following any action sequence with length 𝑇 , i.e., there exists 𝑓 such that 𝑓 (r; Φ(𝑜), a) = 𝑝 (r|𝑜, a), ∀ r ∈ R 𝑇 , 𝑜 ∈ O, a ∈ A𝑇 . Intuitively, the 𝑇 -level reward sequence representation encodes the task-relevant information about the relation between the action sequences a and the RSDs 𝑝 (r|𝑜, a) in the next 𝑇 steps. Notice that a 𝑇 -level reward sequence representation is also a 𝑇 ′-level reward sequence representation, where 𝑇 ,𝑇 ′ ∈ N∗ and 𝑇 > 𝑇 ′. If 𝑇 tends to infinity, the representation will encode all task-relevant information from the objective of RL tasks. This derives the following definition. Definition 4.2. A representation Φ : O → Z is a reward sequence representation if it is a 𝑇 -level reward sequence representation for all 𝑇 ∈ N∗. The reward sequence representation is equivalent to a ∞-level reward sequence representation. In practice, we learn a finite 𝑇 - level reward sequence representation as an approximation of the reward sequence representation. To provide a theoretical guarantee for the approximation, the following theorem gives a value bound between the true optimal value function and the value function on top of the 𝑇 -level reward sequence representation. Theorem 4.3. Let Φ : O → Z be a 𝑇 -level representation, 𝑉 𝑒 ∗ : O → R be the optimal value function in the environment 𝑒 ∈ E, ¯𝑉 𝑒 ∗ : Z → R be the optimal value function on the latent representation space, built on top of the representation Φ. Let ¯𝑟 be a bound of the reward space, i.e., |𝑟 | < ¯𝑟 for any 𝑟 ∈ R. Then we have 0 ≤ 𝑉 𝑒 ∗ (𝑜) − ¯𝑉 𝑒 ∗ ◦ Φ(𝑜) ≤ 2𝛾𝑇 1 − 𝛾 ¯𝑟, for any 𝑜 ∈ O and 𝑒 ∈ E. Proof. See Appendix A.1 □ 4.2 Characteristic Functions for Representation Learning In Section 4.1, we formulate the 𝑇 -level reward sequence repre- sentation that can derive the probability density function 𝑝 (r|𝑜, a), where r ∈ R𝑇 is a reward sequence, 𝑜 ∈ O is an observation, and a ∈ A𝑇 is an action sequence. However, learning the probability density functions is usually technically impractical [2]. Leveraging the characteristic function of random vectors, we propose an al- ternative approach, which is simple to implement and effective to learn the distributions. Consider a random vector R defined on the space R𝑇 , with a probability density function 𝑝R (·). The characteristic function 𝜑R : R𝑇 → C of R is defined as 𝜑R (𝝎) = ER∼𝑝R ( ·) 𝑒𝑖 ⟨𝝎,R⟩ (cid:105) (cid:104) = ∫ 𝑒𝑖 ⟨𝝎,r⟩𝑝R (r)dr, √ where 𝝎 ∈ R𝑇 denotes the input of 𝑝R (·), and 𝑖 = −1 is the imaginary unit. Since we consider discounted cumulative rewards Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions KDD ’22, August 14–18, 2022, Washington, DC, USA Figure 3: The overall architecture of CRESP. CRESP minimizes the prediction loss to train an encoder Φ, and simultaneously uses Φ to learn a policy in an actor-critic setting. In the prediction task, CRESP predicts the characteristic functions of reward sequence distributions through the encoder Φ and the predictor Ψ (in the purple box). The prediction loss L N D (Φ, Ψ) provides the gradients (red lines) to update both the predictor Ψ and the encoder Φ. Here r𝑇 𝑡 are the sequences drawn from a replay buffer D. The inputs 𝝎 of characteristic functions are sampled from a Gaussian distribution N . 𝑡 +1 and a𝑇 𝑡 =1 in RL tasks, we use ⟨·, ·⟩ to denote the weighted inner product in R𝑇 , i.e., ⟨𝝎, r⟩ = (cid:205)𝑇 𝛾𝑡 𝜔𝑡𝑟𝑡 , where 𝛾 is the discounted factor. Characteristic functions are useful tools well studied in probabil- ity theory. In contrast to the probability density function, the char- acteristic function has some good basic properties. 1) |𝜑R (𝝎)| ≤ 𝑒𝑖 ⟨𝝎,R⟩(cid:12) (cid:12) ER∼𝑝R ( ·) (cid:12) = 1, which indicates that the characteristic func- (cid:12) (cid:12) (cid:12) tion always exists and is uniformly bounded. 2) The characteris- tic function 𝜑R is uniformly continuous on R𝑇 , which makes it tractable for learning. The following lemma states a fact that the distribution of a ran- dom vector can be specified by its characteristic function. Lemma 4.4. [8] Two random vectors X and Y have the same characteristic function if and only if they have the same probability distribution function. This lemma implies that we can recapture the information about the distributions of random vectors via their characteristic functions. Therefore, instead of learning the conditional density functions of reward sequences that are intractable, we propose to leverage characteristic functions of the RSDs for representation learning. Specifically, we have the following theorem. Theorem 4.5. A representation Φ : O → Z is a 𝑇 -level reward sequence representation if and only if there exits a predictor Ψ such that for all 𝒘 ∈ R𝑇 , 𝑜 ∈ O and a ∈ A𝑇 , Ψ(𝝎; Φ(𝑜), a) = 𝜑R|𝑜,a (𝝎) = ER∼𝑝 ( · |𝑜,a) 𝑒𝑖 ⟨𝝎,R⟩ (cid:105) (cid:104) . Proof. See Appendix A.2. □ Theorem 4.5 provides an equivalent definition of 𝑇 -level reward sequence representation and inspires our novel approach to predict the characteristic functions of RSDs for representation learning. Algorithm 1 Characteristic Reward Sequence Prediction Initialize a replay buffer D, a policy 𝜋, a representation Φ, and a function approximator Ψ for each iteration do for 𝑒 in E do for each environment step 𝑡 do Execute action 𝑎𝑡 ∼ 𝜋 (·|Φ(𝑜𝑡 )) Receive a transition 𝑜𝑡 +1, 𝑟𝑡 +1 ∼ 𝑝𝑒 (·|𝑜𝑡 , 𝑎𝑡 ) Record partial trajectories {(𝑜𝑡 −𝑖, 𝑎𝑡 −𝑖, 𝑟𝑡 +1−𝑖 )}𝑇 −1 𝑖=0 in D end for end for for each gradient step do Sample partial trajectories from D Update the representation: L N Update the policy: LRL (𝜋) D (Φ, Ψ) end for end for 4.3 Characteristic Reward Sequence Prediction To improve the generalization of a learned policy on unseen en- vironments with visual distractions, we propose Characteristic Reward Sequence Prediction (CRESP), a novel approach to learn representations for task relevance from high-dimensional observa- tions. As discussed above, CRESP learns RSDs by predicting the characteristic functions 𝜑R|𝑜,a (𝝎). In this section, we focus on the detailed learning procedure for the prediction. For an observation 𝑜 ∈ O and an action sequence a ∈ A𝑇 , the true characteristic function of the corresponding reward sequence is 𝜑R|𝑜,a (𝝎) = ER∼𝑝 ( · |𝑜,a) [𝑒𝑖 ⟨𝝎,R⟩]. We estimate the characteristic function by a predictor Ψ(𝝎; Φ(𝑜), a). We use the weighted squared distance between the true and predicted characteristic functions as Characteristic Reward Sequence Prediction 𝑠𝑡 𝑥𝑡 𝑇 = (𝑟𝑡+1, … , 𝑟𝑇) 𝒓𝑡+1 𝝎 = 𝜔1, … , 𝜔𝑇−𝑡 𝑇 = 𝑎𝑡, … , 𝑎𝑇−1 𝐚𝑡 . .. . . . 𝑔 𝑜𝑡 Encoder 𝚽 Characteristic Function signals Prediction Loss 𝒩(𝚽, 𝚿) ℒ𝒟 𝝍cos 𝝍sin grad Predictor 𝚿 𝚿 𝝎; 𝚽 𝑜𝑡 , 𝐚𝑡 Critic 𝑄 𝑜𝑡, 𝑎𝑡 Actor 𝑎𝑡 RL Loss KDD ’22, August 14–18, 2022, Washington, DC, USA Yang et al. Figure 4: Learning curves of six methods on six tasks with dynamic background distractions for 500K environment steps. The solid curves denote the means and the shaded regions denote the minimum and maximum returns over 6 trials. Each checkpoint is evaluated by 10 episodes on unseen environments. Curves are smoothed for visual clarity. the prediction loss: 𝐿W (Φ, Ψ|𝑜, a) = E𝛀∼W (cid:104)(cid:13) (cid:13)Ψ (𝛀; Φ(𝑜), a) − 𝜑R|𝑜,a (𝛀)(cid:13) 2 (cid:13) 2 (cid:12)Ψ (𝝎; Φ(𝑜), a) − 𝜑R|𝑜,a (𝝎)(cid:12) (cid:12) 2 W (𝝎)d𝝎, (cid:12) (cid:105) = ∫ R𝑇 where W is any probability density function on R𝑇 . We optimize the expected loss for observations and action sequences taken from the replay buffer D: 𝐿W D (Φ, Ψ) = E(𝑂,A)∼D (cid:104) 𝐿W (Φ, Ψ|𝑂, A) (cid:105) = E(𝑂,A)∼D,𝛀∼W (cid:104)(cid:13) (cid:13)Ψ (𝛀; Φ(𝑂), A) − 𝜑R|𝑂,A (𝛀)(cid:13) (cid:13) (cid:105) 2 2 . In practice, Since we have no access to the true characteristic functions, we propose to optimize an upper bound on 𝐿W D : L W D (Φ, Ψ) = E(𝑂,A,R)∼D,𝛀∼W (cid:20)(cid:13) (cid:13) (cid:13) ≥ E(𝑂,A)∼D,𝛀∼W (cid:20)(cid:13) (cid:13) (cid:13) Ψ (𝛀; Φ(𝑂), A) − 𝑒𝑖 ⟨𝛀,R⟩(cid:13) 2 (cid:13) (cid:13) 2 (cid:21) Ψ (𝛀; Φ(𝑂), A) − ER∼𝑝 ( · |𝑂,A) (cid:21) 𝑒𝑖 ⟨𝛀,R⟩ (cid:105)(cid:13) (cid:104) 2 (cid:13) (cid:13) 2 = 𝐿W D (Φ, Ψ). Due to the complex form of characteristic functions, we divide the predictor Ψ into two parts Ψ = (𝜓cos,𝜓sin), where 𝜓cos es- timates the real parts, and 𝜓sin estimates the imaginary parts of characteristic functions, respectively. Moreover, we draw Ω from a Gaussian distribution W = N (𝝁, 𝝈 2) in practice. We then pa- rameterize this distribution N (𝝁, 𝝈 2) and perform ablation on it in Appendix B.1. Based on the experimental results, we leverage D (Φ, Ψ) = E(𝑂,A,R)∼D,𝛀∼N the standard Gaussian distribution N . Then the loss function is: L N (cid:2)∥𝜓cos (𝛀; Φ(𝑂), A) − cos (⟨𝛀, R⟩)∥2 2 (cid:3) . + ∥𝜓sin (𝛀; Φ(𝑂), A) − sin (⟨𝛀, R⟩)∥2 2 In the training process, we update the encoder Φ and the pre- dictor Ψ due to the auxiliary loss L N D (Φ, Ψ), and use the trained encoder Φ for the RL tasks. The whole architecture of CRESP and training procedure are illustrated in Figure 3 and Algorithm 1. 5 EXPERIMENTS In this paper, we improve the performance of generalization on unseen environments with visual distractions. We focus on training agents in multi-environments under traditional off-policy settings without any prior environmental knowledge, such as strong aug- mentations designed for visual factors [6, 19, 35], fine-tuning in test environments [11], or environmental labels for invariance [1, 24]. We then investigate the performances of agents trained by different algorithms on various unseen test environments. For each environment, we benchmark CRESP extensively against prior state-of-the-art methods: 1) CURL [18]: a RL method with an auxiliary contrastive task; 2) DrQ [30]: an effective method with state-of-the-art performance on DeepMind Control (DMCon- trol) [27]; 3) MISA [33]: a recent approach from causal inference to learn invariant representations by approximating one-step rewards and dynamics; 4) DBC [34]: a research for generalization in RL to learn representations via the bisimulation metric; 5) SAC [9]: a traditional off-policy deep RL algorithm. Network Details. Our method builds upon SAC and follows the network architecture of DrQ and CURL. We use a 4-layer feed- forward ConvNet with no residual connection as the encoder. Then,","['x', 'learn', 'taskrelevant', 'representation', 'generalization', 'characteristic', 'function', 'reward', 'sequence', 'distribution', 'science', 'technology', 'artificial', 'intelligence', 'comprehensive', 'national', 'science', 'technology', 'ustcgzjmailustceducn', 'science', 'technology', 'science', 'technology', 'college', 'station', 'science', 'technology', 'science', 'technology', 'abstract', 'generalization', 'different', 'environment', 'task', 'critical', 'successful', 'application', 'visual', 'reinforcement', 'learn', 'rl', 'real', 'scenario', 'however', 'visual', 'distraction', 'com', 'mon', 'real', 'scene', 'highdimensional', 'observation', 'hurtful', 'learn', 'representation', 'visual', 'rl', 'thus', 'degrade', 'performance', 'generalization', 'tackle', 'problem', 'pro', 'pose', 'novel', 'approach', 'namely', 'characteristic', 'reward', 'sequence', 'prediction', 'cresp', 'extract', 'taskrelevant', 'information', 'learn', 'reward', 'sequence', 'distribution', 'rsds', 'reward', 'sig', 'nal', 'taskrelevant', 'rl', 'invariant', 'visual', 'distraction', 'specifically', 'effectively', 'capture', 'taskrelevant', 'information', 'cresp', 'introduce', 'auxiliary', 'task', 'predict', 'characteristic', 'function', 'rsds', 'learn', 'taskrelevant', 'represen', 'tation', 'well', 'approximate', 'highdimensional', 'distribution', 'leverage', 'correspond', 'characteristic', 'func', 'tion', 'experiment', 'demonstrate', 'cresp', 'significantly', 'improve', 'performance', 'generalization', 'unseen', 'environment', 'perform', 'several', 'stateoftheart', 'deepmind', 'control', 'task', 'different', 'visual', 'distraction', '∗', 'correspond', 'author', 'permission', 'make', 'digital', 'hard', 'copy', 'part', 'work', 'personal', 'classroom', 'use', 'grant', 'fee', 'provide', 'copy', 'make', 'distribute', 'profit', 'commercial', 'advantage', 'copy', 'bear', 'notice', 'full', 'citation', 'first', 'page', 'copyright', 'component', 'work', 'acm', 'honor', 'abstract', 'credit', 'permit', 'copy', 'otherwise', 'republish', 'post', 'server', 'redistribute', 'list', 'require', 'prior', 'specific', 'permission', 'andor', 'fee', 'request', 'permission', 'association', 'compute', 'machinery', 'acm', 'isbn', 'concept', 'compute', 'methodology', 'sequential', 'decision', 'make', 'image', 'representation', 'markov', 'decision', 'process', 'keyword', 'taskrelevant', 'representation', 'learn', 'reward', 'sequence', 'character', 'istic', 'function', 'generalization', 'visual', 'reinforcement', 'learn', 'acm', 'reference', 'format', 'learn', 'taskrelevant', 'representation', 'generalization', 'characteristic', 'function', 'reward', 'sequence', 'distribution', 'proceeding', '28th', 'acm', 'sigkdd', 'conference', 'knowledge', 'discovery', 'datum', 'mining', 'page', 'introduction', 'visual', 'reinforcement', 'learn', 'algorithm', 'aim', 'solve', 'com', 'plex', 'control', 'task', 'highdimensional', 'visual', 'observation', 'table', 'success', 'include', 'drq', 'locomotion', 'control', 'impala', 'multitask', 'learning', 'qtopt', 'robot', 'grasp', 'method', 'perform', 'well', 'training', 'environment', 'hardly', 'generalize', 'new', 'environment', 'even', 'environ', 'ment', 'semantically', 'similar', 'training', 'environment', 'image', 'observation', 'often', 'involve', 'many', 'taskirrelevant', 'visual', 'factor', 'dynamic', 'background', 'color', 'object', 'control', 'minor', 'change', 'visual', 'factor', 'cause', 'large', 'distributional', 'shift', 'environment', 'prevent', 'agent', 'extract', 'underlie', 'taskrelevant', 'information', 'put', 'new', 'environment', 'indicate', 'many', 'exist', 'agent', 'memorize', 'trajectory', 'specific', 'environment', 'rather', 'learn', 'transferable', 'skill', 'learn', 'policy', 'transferable', 'skill', 'generalization', 'many', 'prior', 'work', 'focus', 'learn', 'representation', 'function', 'top', 'reward', 'sequence', 'representation', 'experi', 'ment', 'control', 'visual', 'distractor', 'demonstrate', 'cresp', 'significantly', 'improve', 'several', 'stateof', 'theart', 'unseen', 'environment', 'main', 'contribution', 'paper', 'follow', 'introduce', 'reward', 'sequence', 'distribution', 'rsds', 'discard', 'taskirrelevant', 'feature', 'preserve', 'task', 'relevant', 'feature', 'propose', 'novel', 'approach', 'extract', 'task', 'relevant', 'information', 'learn', 'characteristic', 'function', 'rsds', 'representation', 'learn', 'experiment', 'demonstrate', 'representation', 'learn', 'cresp', 'preserve', 'taskrelevant', 'feature', 'prior', 'method', 'outperform', 'several', 'stateoftheart', 'jority', 'task', 'substantial', 'margin', 'relate', 'work', 'generalization', 'visual', 'study', 'generalization', 'deep', 'rl', 'focus', 'capability', 'method', 'generalize', 'see', 'environment', 'limited', 'set', 'training', 'environment', 'several', 'work', 'propose', 'apply', 'regularization', 'technique', 'origi', 'nally', 'develop', 'supervised', 'learning', 'include', 'dropout', 'batch', 'normalization', 'practical', 'easy', 'plement', 'method', 'exploit', 'property', 'sequential', 'decisionmake', 'problem', 'approach', 'prevent', 'fitting', 'focus', 'datum', 'augmentation', 'enlarge', 'available', 'data', 'space', 'implicitly', 'provide', 'prior', 'knowledge', 'agent', 'method', 'show', 'promise', 'result', 'welldesigne', 'experimental', 'setting', 'strong', 'assumption', 'prior', 'knowledge', 'testing', 'environment', 'limit', 'real', 'ap', 'plication', 'contrast', 'method', 'consider', 'realistic', 'setting', 'assume', 'prior', 'knowledge', 'environment', 'representation', 'learn', 'visual', 'many', 'prior', 'work', 'focus', 'representation', 'learn', 'generalization', 'visual', 'work', 'use', 'twostep', 'learning', 'process', 'first', 'train', 'autoencoder', 'use', 'reconstruction', 'loss', 'lowdimensional', 'representation', 'use', 'representation', 'policy', 'mization', 'however', 'representation', 'encode', 'element', 'observation', 'relevant', 'task', 'work', 'use', 'bisimulation', 'metric', 'learn', 'representation', 'invariant', 'irrelevant', 'visual', 'feature', 'however', 'method', 'use', 'transition', 'dynamic', 'vary', 'environment', 'lead', 'learn', 'representation', 'involve', 'taskirrelevant', 'fea', 'ture', 'visual', 'distraction', 'recent', 'study', 'leverage', 'reward', 'prediction', 'representation', 'learn', 'however', 'learning', 'method', 'consider', 'extend', 'visual', 'rl', 'task', 'characteristic', 'function', 'random', 'variable', 'characteristic', 'func', 'tion', 'fouri', 'transform', 'probability', 'density', 'function', 'well', 'study', 'probability', 'theory', 'use', 'specify', 'highdimensional', 'distribution', 'random', 'variable', 'distribution', 'characteristic', 'function', 'prior', 'work', 'use', 'character', 'istic', 'function', 'solve', 'statistical', 'problem', 'leverage', 'tool', 'simple', 'tractable', 'approximation', 'highdimensional', 'figure', 'agentenvironment', 'interaction', 'block', 'mdps', 'visual', 'distraction', 'environment', '𝑒', 'provide', 'state', '𝑠𝑡', 'background', '𝑥𝑡', 'generate', 'observa', 'tion', 'nonlinear', 'function', 'agent', 'receive', '𝑜𝑡', 'take', 'action', '𝑎𝑡', '𝑒', 'lead', 'transi', 'tion', 'state', 'background', '𝑥𝑡', 'thus', 'observation', 'transition', '𝑜𝑡', 'notice', 'red', 'arrow', 'represent', 'transition', 'vary', 'different', 'environment', 'blue', 'arrow', 'represent', 'transition', 'invariant', 'environment', 'taskrelevant', 'information', 'discard', 'taskirrelevant', 'visual', 'factor', 'propose', 'similarity', 'metric', 'find', 'semantically', 'equivalent', 'observation', 'representation', 'learn', 'e', 'design', 'objective', 'integrate', 'mdp', 'proper', 'tie', 'learn', 'causal', 'representation', 'invariant', 'irrelevant', 'feature', 'aforementioned', 'method', 'leverage', 'reward', 'transition', 'dynamic', 'capture', 'taskrelevant', 'feature', 'however', 'observation', 'transition', 'dynamic', 'see', 'figure', 'induce', 'taskirrelevant', 'information', 'relate', 'visual', 'distraction', 'representation', 'thus', 'hinder', 'generalization', 'detailed', 'discussion', 'section', 'contrast', 'method', 'propose', 'novel', 'approach', 'namely', 'characteristic', 'reward', 'sequence', 'prediction', 'cresp', 'use', 'reward', 'signal', 'observation', 'transition', 'dynamic', 'learn', 'taskrelevant', 'representation', 'reward', 'signal', 'task', 'relevant', 'rl', 'invariant', 'visual', 'factor', 'preserve', 'infor', 'mation', 'relevant', 'task', 'cresp', 'introduce', 'reward', 'sequence', 'distribution', 'rsds', 'conditional', 'distribu', 'tion', 'reward', 'sequence', 'give', 'starting', 'observation', 'various', 'subsequent', 'action', 'leverage', 'rsds', 'learn', 'taskrelevant', 'representation', 'encode', 'information', 'rsds', 'call', 'reward', 'sequence', 'representation', 'specifically', 'consider', 'characteristic', 'function', 'specify', 'highdimensional', 'distribu', 'tion', 'propose', 'learn', 'taskrelevant', 'representation', 'auxiliary', 'task', 'predict', 'characteristic', 'function', 'rsds', 'moreover', 'provide', 'theoretical', 'analysis', 'value', 'bound', 'true', 'optimal', 'value', 'function', 'optimal', 'value', 'env', 'env', 'env', 'agent', 'env', 'env', 'env', 'agent', 'observation', 'transition', 'dynamic', 'learn', 'taskrelevant', 'representation', 'generalization', 'characteristic', 'function', 'reward', 'sequence', 'distribution', 'distribution', 'experiment', 'demonstrate', 'function', 'perform', 'well', 'specify', 'distribution', 'reward', 'sequence', 'method', 'preliminary', 'visual', 'rl', 'task', 'deal', 'highdimensional', 'image', 'observa', 'tion', 'instead', 'state', 'input', 'consider', 'family', 'environment', 'highlevel', 'task', 'different', 'visual', 'dis', 'traction', 'denote', 'e', 'set', 'environment', 'model', 'environment', '𝑒', '∈', 'e', 'block', 'markov', 'decision', 'process', 'bmdp', 'describe', 'tuple', 'r', 'state', 'space', 'observation', 'space', 'action', 'space', 'r', 'reward', 'space', 'assume', 'bound', '′', '𝑟', '𝑎', 'state', 'transition', 'probability', '𝑟', '𝑜', '𝑎', 'observation', 'transition', 'probability', 'vary', 'environment', '𝑒', '∈', 'e', 'discount', 'factor', 'time', 'step', '𝑡', 'suppose', 'environment', 'state', 'agent', 'instead', 'directly', 'achieve', 'obtain', 'observa', 'tion', 'environment', '𝑒', '∈', 'e', 'reasonable', 'assume', 'observation', 'determine', 'state', 'taskirrelevant', 'visual', 'factor', 'vary', 'environment', 'background', 'agent', 'color', 'deepmind', 'control', 'task', 'symbolically', 'let', 'set', 'visual', 'factor', 'suppose', 'exist', 'observation', 'function', '×', 'random', 'variable', 'independent', 'transition', 'probability', 'see', 'figure', 'tion', 'aim', 'find', 'policy', '𝜋', 'maximize', 'expect', 'accumulate', 'reward', 'cid2cid205∞', 'cid3', 'simultaneously', 'environ', 'ment', '∈', 'e', 'mean', 'expectation', 'take', 'environment', '𝑒', 'moreover', 'assume', 'environment', 'follow', 'general', 'ized', 'block', 'structure', 'observation', '∈', 'uniquely', 'determine', 'generate', 'state', '𝑠', 'visual', 'factor', 'assumption', 'imply', 'observation', 'function', 'invert', 'ible', 'respect', 'simplicity', 'denote', '𝑜𝑠', 'generate', 'state', 'visual', 'factor', 'respectively', 'furthermore', '𝑟', '𝑜', '′', '𝑟', '𝑎𝑞𝑒', 'representation', 'learn', 'sequence', 'distribution', 'encoder', 'representation', 'refer', 'embed', 'function', 'map', 'observational', 'input', 'latent', 'state', 'representation', 'space', 'z', 'goal', 'find', 'suitable', 'represen', 'tation', 'encode', 'taskrelevant', 'information', 'invariant', 'visual', 'distraction', 'section', 'discuss', 'notion', 'task', 'relevance', 'visual', 'rl', 'introduce', 'reward', 'sequence', 'distribution', 'rsds', 'formulate', 'reward', 'sequence', 'representation', 'eralization', 'section', 'provide', 'theoretical', 'analysis', 'reformulate', 'reward', 'sequence', 'representation', 'charac', 'teristic', 'function', 'rsds', 'section', 'present', 'practical', 'method', 'base', 'prediction', 'characteristic', 'function', 'rsds', 'learn', 'reward', 'sequence', 'representation', 'paper', 'use', 'uppercase', 'letter', 'denote', 'random', 'variable', 'use', 'lowercase', 'letter', '𝑜𝑡', 'denote', 'correspond', 'value', 'random', 'variable', 'take', 'figure', 'relationship', 'observation', 'rsd', 'mapping', 'divide', 'observation', 'space', 'differ', 'ent', 'equivalence', 'class', 'equivalent', 'observation', 'generate', 'state', 'equivalence', 'class', 'correspond', 'mapping', 'action', 'sequence', '∈', 'a𝑇', 'reward', 'sequence', 'distribution', '𝑜', '∈', 'taskrelevant', 'invariance', 'visual', 'key', 'idea', 'approach', 'capture', 'taskrelevant', 'infor', 'mation', 'different', 'environment', 'observation', 'lever', 'age', 'information', 'representation', 'learn', 'improve', 'performance', 'generalization', 'reward', 'signal', 'transition', 'dynamic', 'major', 'property', 'mdp', 'commonly', 'use', 'representation', 'learn', 'visual', 'start', 'discussion', 'distraction', 'induce', 'observation', 'transition', 'dynamic', 'visual', 'hardly', 'learn', 'state', 'transition', 'dynamic', 'state', 'space', 'available', 'practice', 'instead', 'many', 'method', 'learn', 'observation', 'transition', 'dynamic', 'probabilistic', 'dynamic', 'model', 'however', 'observation', 'transition', 'dynamic', 'relevant', 'visual', 'factor', 'comprise', 'transition', 'dynamic', 'state', 'taskirrelevant', 'visual', 'factor', 'formally', 'reward', 'observation', 'transition', 'dynamic', '𝑝𝑒', '𝑟', '𝑜', '′', '𝑟', '𝑎𝑞𝑒', 'formula', 'show', 'observation', 'tran', 'sition', 'probability', 'vary', 'environment', '𝑒', '∈', 'e', 'present', 'case', 'figure', 'illustrate', 'observation', 'transition', 'dynamic', 'therefore', 'representation', 'encode', 'information', 'tion', 'transition', 'dynamic', 'subject', 'visual', 'distraction', 'difficulty', 'learn', 'transferable', 'skill', 'contrast', 'observation', 'transition', 'dynamic', 'distribution', 'reward', 'signal', 'relevant', 'rl', 'task', 'invariant', 'visual', 'distraction', 'formally', 'observation', '𝑜', '′', 'generate', 'state', '𝑟', '𝑜', '𝑜', '′', '𝑎', '∈', '𝑒', '∈', 'e', 'motivate', 'use', 'reward', 'signal', 'instead', 'observation', 'transition', 'dynamic', 'representation', 'learn', 'goal', 'maximize', 'expect', 'accumulative', 'reward', 'need', 'current', 'reward', 'also', 'sequence', 'future', 'reward', 'therefore', 'propose', 'utilize', 'reward', 'sequence', 'representation', 'learn', 'mathematical', 'formulation', 'introduce', 'new', 'nota', 'tion', 'denote', 'a𝑇', 'r', '∈', 'r', 'space', 'action', 'sequence', 'reward', 'sequence', 'length', 'respectively', 'let', 'set', 'probability', 'distribution', 'time', 'step', 'observation', 'space', 'action', 'space', 'space', '𝐚', '𝑝∙', '𝑜', '𝐚', '∆𝑅𝑇', 'set', 'sequence', 'subsequent', 'action', 'a𝑇', 'dimensional', 'random', 'vector', 'a𝑇', 'sequence', 'subsequent', 'reward', '𝑅𝑡', 'dimensional', 'random', 'vector', 'clarify', 'idea', 'first', 'consider', 'deterministic', 'environ', 'ment', 'start', 'observation', 'correspond', 'state', 'suppose', 'perform', 'give', 'action', 'quence', 'a𝑇', 'a𝑇', 'receive', 'reward', 'sequence', '𝑟𝑡', '∈', 'environment', 'reward', 'r𝑇', 'sequence', 'uniquely', 'determine', 'start', 'state', 'give', 'action', 'sequence', 'a𝑇', 'therefore', 'find', 'rela', 'tionship', 'give', 'action', 'sequence', 'a𝑇', 'received', 'reward', 'sequence', 'invariant', 'visual', 'distraction', 'use', 'relationship', 'identify', 'taskrelevant', 'information', 'observation', 'formulate', 'relationship', 'consider', 'map', 'ping', 'action', 'sequence', '∈', 'a𝑇', 'correspond', 'reward', 'sequence', 'agent', 'receive', 'observation', '𝑜', 'follow', 'action', 'sequence', 'consider', 'observation', '𝑜', '′', 'mapping', '∈', 'a𝑇', 'r', '∈', 'dimension', 'word', 'suppose', 'agent', 'receive', 'equal', 'reward', 'sequence', 'r', '∈', '𝑜', '′', 'follow', 'action', 'sequence', '∈', 'a𝑇', 'observation', 'similar', 'task', 'property', 'sense', 'agent', 'receive', 'equal', 'accumulative', 'reward', '𝑜', '𝑜', '′', 'matter', 'action', 'agent', 'take', 'therefore', 'mapping', 'action', 'sequence', '∈', 'a𝑇', 'correspond', 'reward', 'sequence', 'use', 'identify', 'taskrelevant', 'information', 'observation', 'consider', 'stochastic', 'environment', 'case', 'similar', 'deterministic', 'environment', 'stochastic', 'vironment', 'reward', 'sequence', 'random', 'even', 'fix', 'observation', '𝑜𝑡', 'action', 'sequence', 'a𝑇', 'therefore', 'sim', 'ply', 'consider', 'mapping', 'a𝑇', 'instead', 'apply', 'mapping', 'a𝑇', 'map', 'action', 'sequence', 'distribution', 'sequence', 'reward', 'random', 'variable', 'formally', 'let', 'r𝑜', 'probability', 'density', 'function', 'point', 'r', '∈', 'condition', 'starting', '∈', 'a𝑇', 'random', 'vector', 'observation', '𝑜', 'action', 'sequence', 'a𝑇', '𝑜', '∈', 'r', '𝑟𝑇', 'denote', 'probability', 'den', 'sity', 'function', 'reward', '𝑟', 'agent', 'receive', 'fol', 'low', 'action', 'sequence', 'start', 'state', 'furthermore', '𝑜', '𝑜', '′', 'formula', 'imply', 'conditional', 'tribution', '𝑜', 'reward', 'sequence', 'determine', 'erate', 'state', 'observation', 'well', 'action', 'sequence', 'therefore', 'mapping', 'action', 'sequence', '∈', 'a𝑇', 'corresponding', 'rsds', '𝑜', 'taskrelevant', 'invariant', 'visual', 'distraction', 'thus', 'use', 'mapping', 'determine', 'task', 'relevance', 'see', 'figure', 'illustration', 'analysis', 'motivate', 'method', 'leverage', 'rsds', 'learn', 'representation', 'specifically', 'learn', 'representation', '2we', 'use', 'bold', 'uppercase', 'letter', 'r', 'denote', 'random', 'vector', 'high', 'dimensional', 'space', 'use', 'bold', 'lowercase', 'letter', 'r', 'denote', 'determin', 'istic', 'vector', 'space', 'derive', 'function', 'map', 'action', 'sequence', 'correspond', 'rsds', 'formally', 'define', 'level', 'reward', 'sequence', 'representation', 'follow', 'definition', 'representation', 'z', 'level', 'reward', 'sequence', 'representation', 'derive', 'distribution', 'reward', 'sequence', 'receive', 'observation', 'follow', 'action', 'sequence', 'length', 'exist', 'r', 'r', '∈', 'r', '𝑜', '∈', '∈', 'a𝑇', 'intuitively', 'level', 'reward', 'sequence', 'representation', 'encode', 'taskrelevant', 'information', 'relation', 'action', 'sequence', 'rsds', 'next', 'step', 'notice', 'level', 'reward', 'sequence', 'representation', 'also', 'reward', 'sequence', 'representation', '′', 'n∗', '′', 'tend', 'infinity', 'representation', 'encode', 'taskrelevant', 'information', 'objective', 'task', 'derive', 'follow', 'definition', 'definition', 'representation', 'z', 'reward', 'sequence', 'representation', 'level', 'reward', 'sequence', 'representation', '∈', 'n∗', 'reward', 'sequence', 'representation', 'equivalent', '∞level', 'reward', 'sequence', 'representation', 'practice', 'learn', 'finite', 'level', 'reward', 'sequence', 'representation', 'approximation', 'reward', 'sequence', 'representation', 'provide', 'theoretical', 'guarantee', 'approximation', 'follow', 'theorem', 'give', 'value', 'bind', 'true', 'optimal', 'value', 'function', 'value', 'function', 'top', 'level', 'reward', 'sequence', 'representation', 'theorem', 'let', 'z', 'level', 'representation', '𝑉', '∗', 'r', 'optimal', 'value', 'function', 'environment', '𝑒', '∈', '𝑒', '∗', 'r', 'optimal', 'value', 'function', 'latent', 'representation', 'space', 'build', 'top', 'representation', 'let', '¯𝑟', 'bound', 'reward', 'space', '𝑟', '¯𝑟', '∈', 'r', '∗', '𝑒', '∗', '◦', '𝛾', '¯𝑟', '𝑜', '∈', '𝑒', '∈', 'e', 'proof', 'see', 'appendix', 'characteristic', 'function', 'representation', 'learn', 'section', 'formulate', 'level', 'reward', 'sequence', 'repre', 'sentation', 'derive', 'probability', 'density', 'function', 'r', '∈', 'reward', 'sequence', 'observation', '∈', 'a𝑇', 'action', 'sequence', 'however', 'learn', 'probability', 'density', 'function', 'usually', 'technically', 'impractical', 'leverage', 'characteristic', 'function', 'random', 'vector', 'propose', 'ternative', 'approach', 'simple', 'implement', 'effective', 'learn', 'distribution', 'consider', 'random', 'vector', 'r', 'define', 'space', 'probability', 'density', 'function', 'characteristic', 'function', 'c', 'r', 'define', '𝑒𝑖', '⟨𝝎r⟩𝑝r', '∈', 'denote', 'input', 'imaginary', 'unit', 'consider', 'discount', 'cumulative', 'reward', 'learn', 'taskrelevant', 'representation', 'generalization', 'characteristic', 'function', 'reward', 'sequence', 'distribution', 'overall', 'architecture', 'cresp', 'cresp', 'minimize', 'prediction', 'loss', 'train', 'encoder', 'φ', 'simultaneously', 'use', 'learn', 'policy', 'actorcritic', 'setting', 'prediction', 'task', 'cresp', 'predict', 'characteristic', 'function', 'reward', 'sequence', 'distribution', 'encoder', 'predictor', 'purple', 'box', 'prediction', 'loss', 'l', 'provide', 'gradient', 'red', 'line', 'update', 'predictor', 'encoder', 'sequence', 'draw', 'replay', 'buffer', 'input', 'characteristic', 'function', 'sample', 'gaussian', 'distribution', 'a𝑇', 'task', 'use', '⟩', 'denote', 'weighted', 'inner', 'product', '𝛾𝑡', 'discount', 'factor', 'characteristic', 'function', 'useful', 'tool', 'well', 'study', 'probabil', 'ity', 'theory', 'contrast', 'probability', 'density', 'function', 'acteristic', 'function', 'good', 'basic', 'property', '⟨𝝎r⟩cid12', 'indicate', 'characteristic', 'func', 'tion', 'always', 'exist', 'uniformly', 'bound', 'uniformly', 'continuous', 'make', 'tractable', 'learn', 'follow', 'state', 'fact', 'distribution', 'ran', 'dom', 'vector', 'specify', 'characteristic', 'function', 'lemma', 'random', 'vector', 'characteristic', 'function', 'probability', 'distribution', 'function', 'lemma', 'imply', 'recapture', 'information', 'distribution', 'random', 'vector', 'characteristic', 'function', 'therefore', 'instead', 'learn', 'conditional', 'density', 'function', 'sequence', 'intractable', 'propose', 'leverage', 'characteristic', 'function', 'rsds', 'representation', 'learn', 'specifically', 'follow', 'theorem', 'representation', 'z', 'level', 'reward', 'sequence', 'representation', 'exit', 'predictor', 'ψ', '𝒘', '∈', '𝑜', '∈', '∈', 'a𝑇', 'ψ𝝎', '𝜑r𝑜a', 'proof', 'see', 'a2', 'theorem', 'provide', 'equivalent', 'definition', 'level', 'reward', 'sequence', 'representation', 'inspire', 'novel', 'approach', 'predict', 'characteristic', 'function', 'rsds', 'representation', 'learn', 'characteristic', 'reward', 'sequence', 'prediction', 'initialize', 'replay', 'buffer', 'policy', 'representation', 'function', 'approximator', 'ψ', 'iteration', '𝑒', 'e', 'environment', 'step', '𝑡', 'execute', 'action', '∼', 'φ𝑜𝑡', 'receive', 'transition', '𝑜𝑡', '𝑟𝑡', '∼', '𝑝𝑒', 'record', 'partial', 'trajectory', '1−𝑖', 'end', 'end', 'gradient', 'step', 'sample', 'partial', 'trajectory', 'update', 'representation', 'l', 'n', 'update', 'policy', 'lrl', 'end', 'end', 'characteristic', 'reward', 'sequence', 'prediction', 'improve', 'generalization', 'learn', 'policy', 'unseen', 'vironment', 'visual', 'distraction', 'propose', 'characteristic', 'reward', 'sequence', 'prediction', 'cresp', 'novel', 'approach', 'learn', 'representation', 'task', 'relevance', 'highdimensional', 'observa', 'tion', 'discuss', 'cresp', 'learn', 'rsds', 'predict', 'characteristic', 'function', '𝜑r𝑜a', '𝝎', 'section', 'focus', 'detailed', 'learning', 'procedure', 'prediction', 'observation', '𝑜', '∈', 'action', 'sequence', '∈', 'a𝑇', 'true', 'characteristic', 'function', 'corresponding', 'reward', 'sequence', '𝜑r𝑜a', 'estimate', 'characteristic', 'function', 'predictor', 'ψ𝝎', 'use', 'weighted', 'square', 'distance', 'true', 'predict', 'characteristic', 'function', 'characteristic', 'reward', 'sequence', 'prediction', '𝑠𝑡', '𝑎𝑇−1', 'encoder', '𝚽', 'characteristic', 'function', 'signal', 'prediction', 'loss', '𝝍co', '𝝍sin', 'predictor', 'critic', '𝑜𝑡', 'actor', 'loss', 'figure', 'learn', 'curve', 'method', 'task', 'dynamic', 'background', 'distraction', 'environment', 'step', 'solid', 'curve', 'denote', 'mean', 'shaded', 'region', 'denote', 'minimum', 'maximum', 'return', 'trial', 'checkpoint', 'evaluate', 'episode', 'unseen', 'environment', 'curve', 'smooth', 'visual', 'clarity', 'prediction', 'loss', '𝐿w', 'φ', 'e𝛀∼w', '−', '𝜑r𝑜a', '−', '𝜑r𝑜a', '𝝎cid12', '𝝎d𝝎', '∫', 'probability', 'density', 'function', 'optimize', 'expect', 'loss', 'observation', 'action', 'sequence', 'take', 'replay', 'buffer', 'e𝑂a∼d', 'cid104', '−', '𝜑r𝑂a', 'practice', 'access', 'true', 'characteristic', 'function', 'propose', 'optimize', 'upper', 'bound', 'l', '≥', 'e𝑂a∼d𝛀∼w', 'ψ', '𝛀', 'φ𝑂', '−', '𝛀', 'φ𝑂', '−', 'er∼𝑝', '𝐿w', 'complex', 'form', 'characteristic', 'function', 'divide', 'predictor', 'part', '𝜓cos𝜓sin', 'timate', 'real', 'part', '𝜓sin', 'estimate', 'imaginary', 'part', 'characteristic', 'function', 'respectively', 'moreover', 'draw', 'ω', 'gaussian', 'distribution', 'n', '𝝁', 'practice', 'rameterize', 'distribution', '𝝁', 'perform', 'ablation', 'b1', 'base', 'experimental', 'result', 'leverage', 'e𝑂ar∼d𝛀∼n', 'standard', 'gaussian', 'distribution', 'loss', 'function', 'l', 'n', 'cid2∥𝜓co', '𝛀', '−', '⟨𝛀', 'cid3', '𝛀', '−', 'sin', '⟨𝛀', 'training', 'process', 'update', 'encoder', 'dictor', 'due', 'auxiliary', 'loss', 'l', 'ψ', 'use', 'train', 'encoder', 'rl', 'task', 'whole', 'architecture', 'cresp', 'training', 'procedure', 'illustrate', 'figure', 'experiment', 'paper', 'improve', 'performance', 'generalization', 'unseen', 'environment', 'visual', 'distraction', 'focus', 'train', 'agent', 'multienvironment', 'traditional', 'offpolicy', 'setting', 'prior', 'environmental', 'knowledge', 'strong', 'mentation', 'design', 'visual', 'factor', 'finetune', 'test', 'environment', 'environmental', 'label', 'invariance', 'investigate', 'performance', 'agent', 'train', 'different', 'algorithm', 'various', 'unseen', 'test', 'environment', 'environment', 'benchmark', 'cresp', 'extensively', 'prior', 'stateoftheart', 'method', 'curl', 'rl', 'method', 'auxiliary', 'contrastive', 'task', 'drq', 'effective', 'method', 'stateoftheart', 'performance', 'misa', 'recent', 'approach', 'causal', 'inference', 'learn', 'invariant', 'representation', 'approximate', 'onestep', 'reward', 'dynamic', 'dbc', 'research', 'generalization', 'learn', 'representation', 'bisimulation', 'metric', 'sac', 'traditional', 'offpolicy', 'deep', 'network', 'detail', 'method', 'build', 'sac', 'follow', 'network', 'architecture', 'drq', 'curl', 'use', 'feed', 'forward', 'convnet', 'residual', 'connection', 'encoder']"
"Dynamical Modeling for non-Gaussian Data with High-dimensional Sparse
  Ordinary Differential Equations","[{'href': 'http://arxiv.org/abs/2206.08616v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2206.08616v1', 'rel': 'related', 'type': 'application/pdf'}]",2022-06-17 08:17:29,"Fast Finite Width Neural Tangent Kernel

Roman Novak 1 Jascha Sohl-Dickstein 1 Samuel S. Schoenholz 1

2
2
0
2

n
u
J

7
1

]

G
L
.
s
c
[

1
v
0
2
7
8
0
.
6
0
2
2
:
v
i
X
r
a

Abstract

The Neural Tangent Kernel (NTK), deﬁned as
θ (x1, x2) = (cid:2)∂f (θ, x1)(cid:14)∂θ(cid:3) (cid:2)∂f (θ, x2)(cid:14)∂θ(cid:3)T
Θf
where (cid:2)∂f (θ,
)(cid:14)∂θ(cid:3) is a neural network (NN) Ja-
·
cobian, has emerged as a central object of study
in deep learning. In the inﬁnite width limit, the
NTK can sometimes be computed analytically
and is useful for understanding training and gener-
alization of NN architectures. At ﬁnite widths, the
NTK is also used to better initialize NNs, compare
the conditioning across models, perform architec-
ture search, and do meta-learning. Unfortunately,
the ﬁnite width NTK is notoriously expensive
to compute, which severely limits its practical
utility. We perform the ﬁrst in-depth analysis of
the compute and memory requirements for NTK
computation in ﬁnite width networks. Leverag-
ing the structure of neural networks, we further
propose two novel algorithms that change the ex-
ponent of the compute and memory requirements
of the ﬁnite width NTK, dramatically improving
efﬁciency. Our algorithms can be applied in a
black box fashion to any differentiable function,
including those implementing neural networks.
We open-source our implementations within the
Neural Tangents package (Novak et al., 2020) at
github.com/google/neural-tangents.

1. Introduction

The past few years have seen signiﬁcant progress towards
a theoretical foundation for deep learning. Much of this
work has focused on understanding the properties of ran-
dom functions in high dimensions. One signiﬁcant line of
work (Neal, 1994; Lee et al., 2018; Matthews et al., 2018;
Borovykh, 2018; Garriga-Alonso et al., 2019; Novak et al.,
2019; Yang, 2019; Hron et al., 2020b;a; Hu et al., 2020)

1Google Brain, Mountain View, California, United States.

Correspondence to: Roman Novak <romann@google.com>.

Proceedings of the 39 th International Conference on Machine
Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-
right 2022 by the author(s).

established that in the limit of inﬁnite width, randomly ini-
tialized Neural Networks (NNs) are Gaussian Processes
(called NNGPs). Building on this development, Jacot et al.
(2018) showed that in function space the dynamics under
gradient descent could be computed analytically using the
so-called Neural Tangent Kernel (NTK) and Lee et al. (2019)
showed that wide neural networks reduce to their lineariza-
tions in weight space throughout training. A related set of
results (Belkin et al., 2019; Spigler et al., 2019) showed that
the ubiquitous bias-variance decomposition breaks down
as high-dimensional models enter the so-called interpolat-
ing regime. Together these results describe learning in the
inﬁnite width limit and help explain the impressive general-
ization capabilities of NNs.

Insights from the wide network limit have had signiﬁcant
practical impact. The conditioning of the NTK has been
shown to signiﬁcantly impact trainability and generaliza-
tion in NNs (Schoenholz et al., 2017; Xiao et al., 2018;
2020). This notion inspired initialization schemes like
Fixup (Zhang et al., 2019), MetaInit (Dauphin & Schoen-
holz, 2019), and Normalizer Free networks (Brock et al.,
2021a;b), and has enabled efﬁcient neural architecture
search (Park et al., 2020; Chen et al., 2021b). The NTK
has additionally given insight into a wide range of phe-
nomena such as: behavior of Generative Adversarial Net-
works (Franceschi et al., 2021), neural scaling laws (Bahri
et al., 2021), and neural irradiance ﬁelds (Tancik et al.,
2020). Kernel regression using the NTK has further enabled
strong performance on small datasets (Arora et al., 2020),
and applications such as approximate inference (Khan et al.,
2019), dataset distillation (Nguyen et al., 2020; 2021), and
uncertainty prediction (He et al., 2020; Adlam et al., 2020).

Despite the signiﬁcant promise of theory based on the NTK,
computing the NTK in practice is challenging. In the inﬁnite
width limit, the NTK can sometimes be computed analyti-
cally. However, the inﬁnite-width kernel remains intractable
for many architectures and ﬁnite width corrections are often
important to describe actual NNs used in practice (see §I for
detailed discussion). The NTK matrix can be computed for
ﬁnite width networks as the outer product of Jacobians using
forward or reverse mode automatic differentiation (AD),

Θf
(cid:124)

θ (x1, x2)
(cid:123)(cid:122)
(cid:125)
O×O

:= (cid:2)∂f (θ, x1)(cid:14)∂θ(cid:3)
(cid:123)(cid:122)
(cid:125)
O×P

(cid:124)

(cid:2)∂f (θ, x2)(cid:14)∂θ(cid:3)T
,
(cid:125)
(cid:123)(cid:122)
(cid:124)
P×O

(1)

 
 
 
 
 
 
Fast Finite Width Neural Tangent Kernel

where f is the forward pass NN function producing outputs
in RO, θ
RP are all trainable parameters, and x1 and x2
are two inputs to the network. If inputs are batches of sizes
N1 and N2, the NTK is an N1O

N2O matrix.

∈

×

Unfortunately, evaluating Eq. (1) is often infeasible due to
time and memory requirements. For modern machine learn-
ing tasks O is often greater (sometimes much greater) than
1000 (e.g. for ImageNet (Deng et al., 2009)), while even
modestly sized models feature tens of millions of parame-
107. This makes both storing ([N1 + N2] OP
ters, or P
memory) and contracting ([N1N2] O2P time) the Jacobians
in Eq. (1) very costly. The theoretical importance of the
NTK together with its prohibitive computational costs im-
plies that performance improvements will unlock impactful
novel research.

∼

We perform the ﬁrst in-depth analysis of the compute and
memory requirements for the NTK as in Eq. (1). Noting
that forward and reverse mode AD are two extremes of a
wide range of AD strategies (Naumann, 2004; 2008), we
explore other methods for computing the NTK leveraging
the structure of NNs used in practice. We propose two
novel methods for computing the NTK that exploit different
orderings of the computation. We describe the compute and
memory requirements of our techniques in fully-connected
(FCN) and convolutional (CNN) settings, and show that
one is asymptotically more efﬁcient in both settings. We
compute the NTK over a wide range of NN architectures and
demonstrate that these improvements are robust in practice.
We open-source our implementations as general-purpose
JAX1 (Bradbury et al., 2018) function transformations.

2. Related Work

The ﬁnite width NTK (denoted simply NTK throughout this
work2) has been used extensively in many recent works, but
to our knowledge implementation details and compute costs
were rarely made public. Below we draw comparison to
some of these works, but we stress that it only serves as
a sanity check to make sure our contribution is valuable
relative to the scale of problems that have been attempted.
None of these works had efﬁcient NTK computation as their
central goal.

In order to compare performance of models based on the
NTK and the inﬁnite width NTK, Arora et al. (2019a, Table
2) compute the NTK of up to 20-layer, 128-channel CNN
in a binary CIFAR-2 classiﬁcation setting. In an equivalent

1Our algorithms are framework-agnostic, but implementation
in JAX is easier, as described in §M. We also provide instructions
for implementation in other frameworks like Tensorﬂow (Abadi
et al., 2016) and PyTorch (Paszke et al., 2019) in §M.

2See §I for a comparison between the ﬁnite and inﬁnite width

settings.

setting with the same hardware (NVIDIA V100), we are
able to compute the NTK of a 2048-channel CNN, i.e. a
network with at least 256 times more parameters.

To demonstrate the stability of the NTK during training for
wide networks, Lee et al. (2019, Figure S6) compute the
NTK of up to 3-layer 212-wide or 1-layer 214-wide FCNs.
In the same setting with the same hardware (NVIDIA V100),
we can reach widths of at least 214 and 218 respectively, i.e.
handle networks with 4 to 16 times more parameters.

To investigate convergence of a WideResNet WRN-28-k
(Zagoruyko & Komodakis, 2016) to its inﬁnite width limit,
Novak et al. (2020, Figure 2) evaluate the NTK of this model
with widening factor k up to 32. In matching setting and
hardware, we are able to reach a widening factor of at least
64, i.e. work with models at least 4 times larger.

To meta-learn NN parameters for transfer learning in a
MAML-like (Finn et al., 2017) setting, Zhou et al. (2021,
Table 7) replace the inner training loop with NTK-based
inference. They use up to 5-layer, 200-channel CNNs on
MiniImageNet (Oreshkin et al., 2018) with scalar outputs
and batch size 25. In same setting we achieve at least 512
channels, i.e. support models at least 6 times larger.

Park et al. (2020, §4.1) use the NTK to predict the gen-
eralization performance of architectures in the context of
Neural Architecture Search (Zoph & Le, 2016, NAS); how-
ever, the authors comment on its high computational burden
and ultimately use a different proxy objective. In another
NAS setting, Chen et al. (2021a, §3.1.1) use the condition
number of NTK to predict a model’s trainability. Chen et al.
(2021b, Table 1) use the NTK to evaluate the trainability
of several ImageNet models such as ResNet 50/152 (He
et al., 2016), Vision Transformer (Dosovitskiy et al., 2021)
and MLP-Mixer (Tolstikhin et al., 2021). However, due
to the prohibitive computational cost, in all of these cases
the authors only evaluate a pseudo-NTK, i.e. the NTK of
a scalar-valued function,3 which impacts the quality of the
respective trainability/generalization proxy.

By contrast, in this work we can compute the full 1000
×
1000 (1000 classes) NTK for the same models, i.e. perform
a task 1000 times more costly.

Finally, we remark that in all of the above settings, scaling
up by increasing width or by working with the true NTK
(vs the pseudo-NTK) should lead to improved downstream
task performance due to a better inﬁnite width/linearization
approximation or a higher-quality trainability/generalization
proxy respectively, which makes our work especially rele-
vant to modern research.

3Precisely, computing the Jacobian only for a single logit or
the sum of all 1000 class logits. The result is not the full NTK,
but rather a single diagonal block or the sum of its 1000 diagonal
blocks (the ﬁnite width NTK is a dense matrix, not block-diagonal).

Fast Finite Width Neural Tangent Kernel

3. Algorithms for Efﬁcient NTK Computation

We now describe our algorithms for fast NTK computation.

In §3.1 we cover preliminaries. We begin by introduc-
ing notation used throughout the paper (§3.1.1). We then
(§3.1.2) describe primitive building blocks of AD includ-
ing the Jacobian-vector products (JVP) and vector-Jacobian
products (VJP) that correspond to forward and reverse mode
AD respectively before discussing the Jacobian (§3.1.3).

In §3.2 we apply the above tools to describe the computa-
tional complexity of the baseline approach to computing the
NTK that is used in most (likely all) prior works.

In §3.3 and §3.4 we present our two algorithms that each
enable accelerating the computation by orders of magnitude
in different ways.

3.1. Preliminaries

3.1.1. NOTATION

∈

RO with O outputs (e.g. class
Consider a NN f (θ, x)
logits) per input x and a total number P of trainable pa-
rameters θ = vec (cid:2)θ0, . . . , θL(cid:3), with each θl of size Pl,
P = (cid:80)L
l=0 Pl. Also assume the network has K intermediate
primitive outputs yk of size Yk each (for example, activa-
tions or pre-activations), and let Y = (cid:80)K
k=1 Yk be the total
size of the outputs (see Fig. 5 and Fig. 6). The NTK is

Θf
θ
(cid:124)(cid:123)(cid:122)(cid:125)
O×O

:= (cid:2)∂f (θ, x1)(cid:14)∂θ(cid:3)
(cid:123)(cid:122)
(cid:125)
O×P

(cid:124)

(cid:2)∂f (θ, x2) (cid:14)∂θ(cid:3)T
(cid:123)(cid:122)
(cid:125)
(cid:124)
P×O

L
(cid:88)

l=0

(cid:2)∂f (θ, x1)(cid:14)∂θl(cid:3)
(cid:124)
(cid:123)(cid:122)
(cid:125)
O×Pl

(cid:2)∂f (θ, x2)(cid:14)∂θl(cid:3)T
(cid:123)(cid:122)
(cid:125)
(cid:124)
Pl×O

.

(3)

We denote FP to be the (time or memory, depending on con-
text) cost of a single forward pass f (θ, x). For memory, we
exclude the cost of storing all P weights, but rather deﬁne it
to be the cost of evaluating f one primitive yk at a time. This
(cid:0)maxk Yk + maxl Pl(cid:1),
gives a memory cost of at most
which we denote as simply Yk + Pl.4 Finally, we will con-
sider x1 and x2 to be batches of N inputs each, in which
case the NTK will be an NO
NO matrix, obtained by com-
×
puting Eq. (2) for each pair of inputs. See §B for glossary.

O

3.1.2. JACOBIAN-VECTOR PRODUCTS (JVP) AND
VECTOR-JACOBIAN PRODUCTS (VJP)

Following Maclaurin et al. (2015) we deﬁne

JVPf

VJPf

(θ,x) : θt ∈
(θ,x) : fc ∈

P

R

O

R

(cid:2)∂f (θ, x)(cid:14)∂θ(cid:3) θt ∈
R
(cid:2)∂f (θ, x)(cid:14)∂θ(cid:3)T
fc ∈

(cid:55)→

(cid:55)→

O;

P.

R

(4)

(5)

The JVP can be understood as pushing forward a tangent
vector θt in weight space to a tangent vector in the space of
outputs; by contrast the VJP pulls back a cotangent vector fc
in the space of outputs to a cotangent vector in weight space.
These elementary operations enable forward and reverse
mode AD respectively and serve as a basis for typical AD
computations such as gradients, Jacobians, Hessians, etc.

The time cost of both is comparable to FP (see §D and
Griewank & Walther (2008)). The memory cost of a JVP is
FP as well (i.e. Yk + Pl), while the memory cost of a VJP is
generally Y + P, since it requires storing all K intermediate
primitive outputs for efﬁcient backprop and all L output
cotangents. However, for the purpose of computing the
NTK, we never need to store the whole Jacobian ∂f /∂θ,
but only individual cotangents like ∂f /∂θl to compute the
sum in Eq. (2) layer-by-layer. Hence we consider VJP to
cost Y + Pl memory. To summarize, for a batch of N inputs,

• JVP costs N [FP] time; N (cid:2)Yk(cid:3) + P memory.
• VJP costs N [FP] time; N (cid:2)Y + Pl(cid:3) + P memory.

3.1.3. JACOBIAN

The reverse mode Jacobian ∂f /∂θ is computed via O VJP
calls on rows of the identity matrix IO

RO×O, i.e.

= (2)

(cid:2)∂f (θ, x) (cid:14)∂θ(cid:3)T

∈
= (cid:2)∂f (θ, x) (cid:14)∂θ(cid:3)T

P×O,

R

(6)

IO

∈

and therefore costs O [VJP] time and memory apart from
parameters and primitive outputs that can be reused across
VJPs. Therefore, for a batch of N inputs,

Jacobian costs NO [FP] time; NO (cid:2)Yk + Pl(cid:3)+NY+
P memory.

3.2. Jacobian contraction – the Baseline

This baseline method of evaluating the NTK consists in
computing the Jacobians ∂f /∂θ and contracting them
as in Eq. (2). The contraction costs N2O2P time and
N2O2 + NOPl memory, to store the result Θf
θ and individ-
ual layer-by-layer cotangents ∂f /∂θl. Including the cost of
computing the cotangents via the batch Jacobian ∂f /∂θ =
(cid:2)∂f /∂θ0, . . . , ∂f /∂θL(cid:3) from §3.1.3 we arrive at

Jacobian contraction costs NO [FP] + N2O2P time;
N2O2 + NO (cid:2)Yk + Pl(cid:3) + NY + P memory.

4To declutter notation throughout this work, in time and mem-
ory complexity expressions, we (1) omit the O symbol, and (2)
imply taking the maximum over any free index.

In summary, Jacobian contraction performs NO forward
passes followed by an expensive N2O2P contraction. Next
we demonstrate how to reduce the contraction cost.

Fast Finite Width Neural Tangent Kernel

3.3. NTK-vector products

Consider the NTK-vector product function (for N = 1):

Θf

θ VP : v

O

R

∈

(cid:55)→

Θf
θ v

∈

O.

R

Taking the NTK-vector product with O columns of the iden-
tity matrix IO yields the full NTK, i.e. Θf
θ . Ex-
panding Θf

θ IO = Θf

θ VP(v) as

θ v = (cid:2)∂f (θ, x1)(cid:14)∂θ(cid:3) (cid:2)∂f (θ, x2)(cid:14)∂θ(cid:3)T
Θf
(θ,x2) (v) =

= (cid:2)∂f (θ, x1)(cid:14)∂θ(cid:3) VJPf
(cid:104)
= JVPf

(cid:105)
(θ,x2) (v)

VJPf

(θ,x1)

,

v =

(7)

(8)

(9)

where we have observed that the NTK-vector product can
be expressed as a composition of a JVP and a VJP. The cost
of computing Θf
θ is then asymptotically the cost of the Ja-
cobian, since it consists of O VJPs followed by O (cheaper)
JVPs, therefore O [FP] time and O (cid:2)Yk + Pl(cid:3)+Y+P mem-
ory. In the batched setting Eq. (7) is repeated for each pair
of inputs, and therefore time increases by a factor of N2
to become N2O [FP]. However, the memory cost grows
only linearly in N (except for the cost of storing the NTK
of size N2O2), since intermediate primitive outputs and tan-
gents/cotangents can be computed for each batch x1 and x2
separately and then reused for every pairwise combination.
Therefore the memory cost is asymptotically the cost to
store the NTK and compute the Jacobian. Altogether,

NTK-vector products cost N2O [FP] time; N2O2 +
NO (cid:2)Yk + Pl(cid:3) + NY + P memory.

In summary, NTK-vector products eliminate the costly
N2O2P contraction of Jacobian contraction, but perform
N2O forward passes (as opposed to NO), and the memory
requirement is identical. As a result, this method is bene-
ﬁcial for small N, and for networks with a cheap forward
pass FP relative to OP, which is always the case for FCNs
(§4.1), but not necessarily for CNNs (§4.2).

3.4. Structured derivatives

Rewriting Θf
the primitive outputs yk, we ﬁnd:

θ from Eq. (2) using the chain rule in terms of

(cid:32)

(cid:32)

∂f1
∂yk1
1

∂f1
∂yk1
1

(cid:33) (cid:32)

∂yk1
1
∂θl

∂f2
∂yk2
2

∂yk2
2
∂θl

(cid:33)T

T (cid:33)

T

∂yk1
1
∂θl

∂yk2
2
∂θl

∂f2
∂yk2
2

Θf

θ [l, k1, k2] ,

Θf

θ =

=

=:

(cid:88)

l,k1,k2

(cid:88)

l,k1,k2
(cid:88)

l,k1,k2

(10)

(11)

(12)

i /∂θl
i . We have also

where we deﬁne fi := f (θ, xi), and only consider ∂yki
to be non-zero if θl is a direct input to yki
deﬁned Θf

θ [l, k1, k2] to be individual summands.
Both Jacobian contraction and NTK-vector products per-
form this sum of contractions via VJPs and JVPs, without
explicit instantiation of primitive Jacobians ∂yki
i /∂θl. How-
ever, while VJPs and JVPs themselves are guaranteed to
be computationally optimal (§D), higher order computa-
tions like their composition (NTK-vector products) or con-
traction (Jacobian contraction) are not. Speciﬁcally, each
Θf
θ [l, k1, k2] from Eq. (10) is a matrix-Jacobian-Jacobian-
matrix product (MJJMP), which, as we will show shortly,
can’t always be evaluated optimally with VJPs and JVPs.

The idea of Structured derivatives is to design rules for
efﬁcient computation of MJJMPs, similarly to AD rules for
JVPs and VJPs.

From Eq. (10), in the general case this requires hand-made
rules for all pairwise combinations of primitives y1 and y2,
of which there are 1362 > 10, 000 in JAX, and even more in
Tensorﬂow (Abadi et al., 2016) and PyTorch (Paszke et al.,
2019) (see §M). We dramatically reduce this number by:

(θ,·)

JVPf
θ

1. Linearization.

It follows from Eq. (4), that Θf

θ =
, i.e. the NTK of f evaluated at parameters θ is
Θ
equal to the NTK of the JVP of f given primal θ. JVP is a
linear function of input tangents θ, and therefore we only
need to implement efﬁcient MJJMPs for linear primitives,
of which JAX has only 56.5

2. MJJMPs through structured derivatives. We further
reduce the necessary MJJMP rule count from 562 down to
only 56 by decomposing an MJJMP rule into two parts:

∈

RW, θl

RW×W, and y (cid:0)θl(cid:1) = θlx

1. Structured derivative rule. Given a single primitive
y, this rule identiﬁes the smallest subarray of ∂y/∂θl
sufﬁcient to reconstruct the entire primitive Jacobian
∂y/∂θl, and the (constant and negligible in memory
size) metadata necessary for the reconstruction. For
example, if x
∈
RW (matrix-vector multiplication), then ∂y/∂θl =
, and the rule will indicate that (1)
IW
only the subarray (cid:2)∂y/∂θl(cid:3)
R1×W,6 needs to be
computed (which is equal to xT in this case), and (2)
that the entire primitive Jacobian can be reconstructed
as ∂y/∂θl = I
1,:W. In other words, this
rule annotates linear primitives y with the structure of
their Jacobians, such as block diagonal, constant-block
diagonal, or tiling along certain dimensions.

∈
RW×W2

(cid:2)∂y/∂θl(cid:3)

1,:W ∈

xT

⊗

⊗

∈

5A linear function can contain nonlinear primitives. However,
linearizing any function in JAX is guaranteed to produce only
linear primitives (Frostig et al., 2021; Radul et al., 2022).

6We deﬁne [A]i,:j := [Ai,1, . . . , Ai,j] ∈ R

1×j.

Fast Finite Width Neural Tangent Kernel

2. MJJMPs with structured Jacobians. Given input
tensors A, B, C, D, where B and C are provided
in the structured form as described above (i.e. only
small subarrays along with their metadata) this rule ef-
ﬁciently computes the 4-way contraction ABCD (i.e.
the NTK summand Θf
θ [l, k1, k2]). This amounts to
using np.einsum with the optimal contraction order
and adjusting its instructions based on provided meta-
data. For example, if B = IW
and
C = IW

bT
RW), then

⊗
(for b, c

RW×W2

RW×W2

∈

∈

⊗

cT
ABCD = A (cid:0)I
= A (cid:0)I

⊗

⊗

∈
bT (cid:1) (cid:0)I
cT (cid:1)T
D =
bT c(cid:1) D = (cid:0)bT c(cid:1) AD,

⊗

(13)

(14)

where were able to pull out bT c since it is a scalar.
As we will see in §4 and §E, this and other similar
contraction rules can enable signiﬁcant speedups.

Therefore we avoid implementing 562 MJJMP rules by in-
stead having (1) a single routine to perform 4-way tensor
contractions with structured tensors, and (2) 56 rules anno-
tating the structure in the 56 linear primitive Jacobians. We
list all these structures and associated MJJMP costs in §E.
Our approach does not guarantee optimality for the NTK
of an arbitrary function, however, as we show in §4, it is
asymptotically better than Jacobian contraction for FCNs
and CNNs, and can provide orders of magnitude speedups in
much more complex contemporary ImageNet models (§5).

3. Focusing on MJJMPs for typical operations. Many
of the 56 linear JAX primitives are trivial to implement
or rarely arise in NNs. At the time of writing we have
only annotated 21 linear primitives (Table 4), which was
sufﬁcient for the empirical speedups observed in §4 and §5.

Summary. Structured derivatives amount to evaluating the
sum of MJJMPs in Eq. (10), where (1) only small subarrays
of primitive Jacobians ∂yki
i /∂θl are instantiated, and (2)
MJJMPs leverage the structure of these primitive Jacobians
for efﬁcient contractions. Together, this incurs

1. The cost of computing primitive output cotangents
∂fi/∂yki
for Eq. (10), which is equivalent to the cost
i
of the reverse mode Jacobian (§3.1.3), less the cost of
computing (NOP) and storing (NOPl) weight-space
cotangents ∂fi/∂θl, since they aren’t used in Eq. (10),
i.e. NO [FP] time7 and NOYk + NY + P memory.

2. The cost of computing primitive Jacobian ∂yki

subarrays, denoted as Jki
(cid:80)

l with J := (cid:80)
. This amounts to NJ time and NJk

Jk2
l

l,k2

i /∂θl
Jk1
l +
l,k1
l memory.

3. The cost of evaluating Θf

θ [l, k1, k2] via the efﬁcient
MJJMP, which we denote as simply MJJMP and sub-
stitute speciﬁc values based on the primitive. Required
memory to store the result is N2O2.

We add up these costs below and in Table 1, and show in §4
and §5 how they are beneﬁcial in most practical settings.

Structured derivatives cost NO [FP] + MJJMP +
l + NY + P
N [J
memory.

OP] time; N2O2 + NOYk + NJk

−

4. Examples

The three algorithms in §3 (and our implementations) apply
to any differentiable function f , but the resulting complexi-
ties depend on variables such as FP and P, which depend
on f (Table 1). Below we compute and compare all three
complexities for a deep FCN (§4.1) and CNN (§4.2), sum-
marizing the results in Table 2 and Table 3 respectively.

4.1. FCN NTK Complexity

We apply our algorithms from §3 to FCNs with L hidden
layers of width W. For simplicity we assume no biases, and
RW, i.e. inputs of same size as the width. We deﬁne
x
yl := θlxl, and xl := φ (cid:0)yl−1(cid:1) for l > 0, with x0 := x.
Output is f (x, θ) := yL. See Fig. 5 (top) for L = 2.

∈

In this case K = 2L+1 (L+1 matrix-vector multiplications
and L nonlinearities), Pl = W2 for l < L and OW for the
top layer l = L, P = LW2 + OW, Yk = W for k < K and
O for k = K, and Y
LW + O. Finally, a single forward
pass FP

LW2 + OW time and W2 + OW memory.

∼

∼

Plugging the above into the cost of the baseline algorithm
Jacobian contraction in §3.2, we obtain

FCN Jacobian contraction costs N2O2LW2 +
N2O3W time; N2O2 + NOW2 + NO2W + NLW +
LW2 memory.

Similarly, the cost of NTK-vector products from §3.3 is

FCN NTK-vector products cost N2OLW2 +
N2O2W time; N2O2 + NOW2 + NO2W + NLW +
LW2 memory.

7Note that NOP time saving is not reﬂected in the asymptotic
cost since it is dominated by NO [FP] required to compute primi-
tive output cotangents. However, as we will see in Fig. 1, it often
provides a substantial practical beneﬁt, up to allowing to compute
the NTK faster than computing the two Jacobians themselves.

For Structured derivatives (§3.4), we additionally need to
derive values of J and MJJMP. For an arbitrary primitive,
J and MJJMP costs are derived by (1) looking up the type
of structure in the primitive Jacobian in Table 4, followed
by (2) extracting the costs for a given structure from Table 5

Fast Finite Width Neural Tangent Kernel

Method
Jacobian contraction
NTK-vector products N2O [FP]
Structured derivatives N O [FP] + MJJMP + NJ N2O2 + NOYk + NJk

Memory
N2O2 + NO (cid:2)Yk + Pl(cid:3) + NY + P
N2O2 + NO (cid:2)Yk + Pl(cid:3) + NY + P
l + NY + P

Time
N O [FP] + N2O2P

Use when
P < Y, small O
FP < OP, large O, small N
FP > OP, large O, large N

Table 1. Generic NTK computation costs. NTK-vector products trade-off contractions for more FP. Structured derivatives usually save
both time and memory. See §3.1.1 and §3.4 for notation, and §B for a glossary of symbols.

Time

Method
Jacobian contraction
NTK-vector products N2OLW2 + N2O2 W
Structured derivatives N OLW2 + N2O2LW + N2O3

N2O2LW2 + N2O3W N2O2 + NOW2 + NO2W + NLW + LW2
N2O2 + NOW2 + NO2W + NLW + LW2
N2O2 + NOW

Use when
Don’t
O > W or N = 1
+ NLW + LW2 O < W or L = 1

Memory

Table 2. FCN NTK computation cost. The costs are obtained by substituting into Table 1 speciﬁc values for FP, P, Y, J, and MJJMP
that correspond to an FCN as described in §4.1. NTK-vector products allow a reduction of the time complexity, while Structured
derivatives reduce both time and memory complexity. See Fig. 1 for empirical conﬁrmation with FLOPs and wall-clock time. See §4.1 for
discussion and notation (§B for the full glossary of symbols).

Method

Time

Jacobian contraction

N O (cid:2)LDFW2 + OW(cid:3) + N2O2 (cid:2)LFW2 + OW(cid:3)

NTK-vector products N2O (cid:2)LDFW2 + OW(cid:3)
Structured derivatives N O (cid:2)LDFW2 + OW(cid:3) + N2O2 (cid:104)

L min

(cid:16)

FW2, DW + DFW2

O , DW + D2W

O + D2FW

O2

Memory

Use when

N2O2 + NO (cid:2)DW + FW2 + OW(cid:3) + N [LDW] + (cid:2)LFW2 + OW2(cid:3) D > OW
N2O2 + NO (cid:2)DW + FW2 + OW(cid:3) + N [LDW] + (cid:2)LFW2 + OW2(cid:3) N = 1

(cid:17)

(cid:105)

+ O

N2O2 + NO [DW] + NDFW + N [LDW] + (cid:2)LFW2 + OW2(cid:3) D < OW

Table 3. CNN NTK computation cost for a CNN with D pixels and ﬁlter size F. Structured derivatives reduce time complexity, and
have lower memory cost if D < OW, which is a common setting. See Fig. 2 for experiments with ResNets, §4.2 for discussion, Table 2
for FCN, Table 1 for generic cost analysis, and §B for a glossary of symbols.

(see §E). We apply this formal approach in §E.9, but for
demonstration purposes below present the derivation that
does not require referencing the Appendix.

i = θlxl

We ﬁrst note that we only need to consider k1 = k2 = 2l +1
indices in Eq. (10), since all other summands are zero due
to absence of weight sharing between layers. For matrix-
vector multiplication yl
i our rules indicate (per ex-
(cid:2)∂yl
ample given in §3.4) that ∂yl
, and command to only compute (cid:2)∂yl
RW×W2
R1×W (which is xl
i
and J = 2 (cid:80)L
∼
Finally, the efﬁcient MJJMP for this structured ∂yl
i/∂θl can
be computed, analogously to Eq. (13), as follows for l < L:

i/∂θl(cid:3)
i/∂θl(cid:3)
T in this case). Therefore J2l+1

1,:W ∈
1,:W ∈
= W,

i/∂θl = IW

l=1 J2l+1

LW.

⊗

l

l

Θf
(cid:124)

θ [l, 2l + 1, 2l + 1]
(cid:123)(cid:122)
(cid:125)
O×O

=

∂f1
∂yl
1

∂yl
1
∂θl

∂yl
2
∂θl

T

T

∂f2
∂yl
2

= (15)



IW







IW


T



T

xl
2
(cid:124)(cid:123)(cid:122)(cid:125)
1×W

⊗

T

xl
1
(cid:124)(cid:123)(cid:122)(cid:125)
1×W

⊗

=

∂f1
∂yl
1
(cid:124)(cid:123)(cid:122)(cid:125)
O×W

= (16)

T

∂f2
∂yl
2
(cid:124) (cid:123)(cid:122) (cid:125)
W×O

=

T


xl
1
(cid:124)(cid:123)(cid:122)(cid:125)
1×W

xl
2
(cid:124)(cid:123)(cid:122)(cid:125)
W×1





∂f1
∂yl
1
(cid:124)(cid:123)(cid:122)(cid:125)
O×W

,

T

∂f2
∂yl
2
(cid:124) (cid:123)(cid:122) (cid:125)
W×O

(17)

which can be contracted in only O2W time. An analogous
derivation applied to l = L yields O3 + W time. Therefore
N2LO2W + N2O3,
the total contraction cost is MJJMP
when accounting for depth L and batch size N. Altogether,

∼

FCN Structured derivatives cost NOLW2 +
N2O2LW + N2O3 time; N2O2 + NOW + NLW +
LW2 memory.

Summary. We summarize all FCN costs in Table 2. We
conclude that Structured derivatives and NTK-vector prod-
ucts allow a reduction in the time cost of NTK computation
in different ways, while Structured derivatives also reduce
memory requirements. Structured derivatives are beneﬁcial
for wide networks, with large W, and NTK-vector prod-
ucts are beneﬁcial for networks with large outputs O.

We conﬁrm our predictions with FLOPs measurements in
Fig. 1. We further conﬁrm our methods provide orders
of magnitude speed-ups and memory savings on all major
hardware platforms in Fig. 1 (right) and Fig. 3. However,
we notice that time measurements often deviate from pre-
dictions due to unaccounted constant overheads of various
methods, hardware speciﬁcs, padding, and the behavior of
the XLA compiler. We ﬁnd Structured derivatives to almost
always outperform NTK-vector products.

Fast Finite Width Neural Tangent Kernel

FLOPs (per NTK entry)

Wall-clock time (TPUv3)

Figure 1. FLOPs (left) and wall-clock time (right) of computing the NTK for a 10-layer ReLU FCN. As predicted by Table 2, our
methods almost always outperform Jacobian contraction, allowing orders of magnitude speed-ups and memory improvements for realistic
problem sizes.

Left: FLOPs per NTK entry. We conﬁrm several speciﬁc
theoretical predictions from §4.1:

Right: Wall-clock runtime.
XLA compiler and hardware speciﬁcs, we observe that:

In real applications, given the

1. NTK-vector products are the best performing method for
N = 1, and have cost equivalent to Jacobian for any width
W or output size O (top row);

2. NTK-vector products offer an O-fold improvement over

Jacobian contraction (left to right columns);

3. NTK-vector products are equivalent to Jacobian contrac-

tion for O = 1 (leftmost column);

4. Structured derivatives outperform NTK-vector products iﬀ
O < W (O = W are plotted as pale vertical lines, which is
where Structured derivatives and NTK-vector products in-
tersect);

5. Structured derivatives approach the cost of Jacobian in the

limit of large width W (left to right);

6. All methods, as expected, scale quadratically with width W

(pale grey dashed line depicts W2 scaling).

More: see Fig. 3 for other hardware platforms, and §N for details.

1. NTK-vector products improve upon Jacobian contrac-
tion for O > 1, but the effect is not perfectly robust (see
bottom row for small W and Fig. 3, notably GPU plat-
forms);

2. Structured derivatives robustly outperform all other meth-
ods, including simply computing the Jacobian, as discussed
in §3.4;

3. Structured derivatives have lower memory footprint, and
reach up to 8x larger widths (bottom right; missing points
indicate out-of-memory), i.e. can process models up to 64x
larger than other methods, as discussed in §3.4;

4. All methods have a smaller memory footprint than Jaco-

bian (see §3.1.3).

O =  1 logits

O =  16 logits

O =  128 logits

N
=
1
b
a
t
c
h

N
=
1
6
b
a
t
c
h

N
=
1
2
8
b
a
t
c
h

s
P
O
L
F

107

104

101

s
P
O
L
F

107

104

101

s
P
O
L
F

107

104

101

20 23 26 29 212
Width W

20 23 26 29 212
Width W

20 23 26 29 212
Width W

 
 
 
 
 
 
 
 
 
101

s
d
n
o
c
e
S

10 1

10 3

101

s
d
n
o
c
e
S

10 1

10 3

101

s
d
n
o
c
e
S

10 1

10 3

O =  1 logits

O =  16 logits

O =  128 logits

N
=
1
b
a
t
c
h

N
=
1
6
b
a
t
c
h

N
=
1
2
8
b
a
t
c
h

20 23 26 29 212
Width W

20 23 26 29 212
Width W

20 23 26 29 212
Width W

 
 
 
 
 
 
 
 
 
Jacobian contraction
NTK-vector products

Structured derivatives
Jacobian

W = O
W 2

O =  1 logits

O =  16 logits

O =  128 logits

101

s

d

n

o

c

e

S

10 1

10 3

101

s

d

n

o

c

e

S

10 1

10 3

101

s

d

n

o

c

e

S

10 1

10 3

N

=

1

b

a

t

c

h

s

i

z

e

N

=

1

6

b

a

t

c

h

s

i

z

e

N

=

1

2

8

b

a

t

c

h

s

i

z

e

20 23 26 29 212

20 23 26 29 212

20 23 26 29 212

Width W

Width W

Width W

 
 
 
 
 
 
 
 
 
 
 
 
Fast Finite Width Neural Tangent Kernel

Figure 2. Wall-clock time of computing an NTK for several ResNet sizes on a pair of ImageNet images. Structured derivatives allow
the NTK to be computed faster and for larger models (see bottom row – missing points indicate out-of-memory). NTK-vector products, as
predicted in §3.3 and Table 1, are advantageous for large O (bottom row), but are suboptimal when the cost of the forward pass FP is large
relative to the output size and the number of parameters OP, e.g. when there is a lot of weight sharing (see Table 3 and Table 1), which is
the case for convolutions, notably for O = 1 (top). See Fig. 4 for more ImageNet models, §4.2 for CNN NTK complexity analysis, and
§N for experimental details.

4.2. CNN NTK Complexity

We perform analogous derivations for a CNN with W chan-
nels, D pixels, and ﬁlter size F in §F. We arrive at Table 3,
and make two observations speciﬁc to CNNs.

First, the speeds of NTK-vector products and Jacobian con-
traction are now much more similar, due to the higher cost
of the forward pass FP relative to P (i.e. weight sharing),
and how they perform will depend on the speciﬁc values
of parameters. We conﬁrm this in our experiments on Ima-
geNet models in §5, where NTK-vector products typically
underperform for O = 1, but outperform for O = 1000.

Secondly, Structured derivatives continue to perform faster
than Jacobian contraction, but the relative memory costs
depend on other hyperparameters, requiring D < OW. This
is a common case for ImageNet with O = 1000, and is
conﬁrmed in our experiments in Fig. 2 and Fig. 4 (bottom).

5. ImageNet Experiments

In §4 we have derived asymptotic time and memory beneﬁts
of NTK-vector products and Structured derivatives over the
baseline Jacobian contraction for FCNs and CNNs. How-
ever, contemporary architectures rarely resemble vanilla
feedforward networks, but instead result in much more com-
plex computational graphs comprised of many different
primitives, making complexity analysis impractical.

We therefore evaluate our methods in the wild, and con-
ﬁrm computational beneﬁts on full ImageNet models in
Fig. 2 (ResNets, He et al. (2016)) and Fig. 4 (WideRes-
Nets, Zagoruyko & Komodakis (2016); Vision Transformers
and Transformer-ResNet hybrids Dosovitskiy et al. (2021);

Steiner et al. (2021); and MLP-Mixers Tolstikhin et al.
(2021)). Computing the full O
1000 NTK is
often only possible with Structured derivatives.

O = 1000

×

×

6. Implementation

All algorithms are implemented in JAX8 (Bradbury et al.,
2018) and integrated into Neural Tangents (Novak et al.,
2020). Jacobian contraction and NTK-vector products are
built with core operations such as vjp , jvp , and vmap .
Structured derivatives are implemented as a Jaxpr inter-
preter, built on top of the JAX reverse mode AD interpreter.

Owing to the nuanced trade-offs between different methods
in the general case, we release all implementations within a
single function that allows the user to manually select imple-
mentation. We also include an automated setting which will
perform FLOPs analysis for each method at compilation
time and automatically choose the most efﬁcient one.

7. Conclusion

We have performed the ﬁrst extensive analysis of the com-
putational complexity of the NTK, and have shown how it
can be improved dramatically with mixed-order AD (NTK-
vector products), or with a custom interpreter for more efﬁ-
cient higher-order AD operations (Structured derivatives).

The NTK computation is similar to many other objects of
interest in machine learning, such as the Gauss-Newton
or the Fisher Information matrix, and we look forward to
extensions of our algorithms to more settings in future work.

8See §M for discussion about other frameworks.

10 2

s
d
n
o
c
e
S

10 3

10 4

s
d
n
o
c
e
S

1.7 × 100
1.6 × 100
1.5 × 100
1.4 × 100
1.3 × 100
1.2 × 100
1.1 × 100
100

NVIDIA V100

TPUv4

CPU

10 3

10 4

10 5

100

Jacobian contraction
NTK-vector products
Structured derivatives
Jacobian

100

10 2

103

102

O
=
1

l

o
g
i
t
s

O
=
1
0
0
0

l

o
g
i
t
s

18 34 50

101

152

200

18 34 50

101

152

200

18 34 50

101

152

200

ResNet depth

ResNet depth

ResNet depth

 
 
 
 
 
 
Fast Finite Width Neural Tangent Kernel

Acknowledgements

We thank Lechao Xiao for useful discussion, review and
comments on the initial version of this manuscript, and
Jaehoon Lee for useful discussion and code review.

We also thank Shaobo Hou for his work on and help with
TF2Jax, and the JAX team for their help and advice on JAX
and Jax2TF.

References

Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,
J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al.
Tensorﬂow: A system for large-scale machine learning. In
12th USENIX Symposium on Operating Systems Design
and Implementation (OSDI 16), 2016. Cited on page 2, 4,
26.

Adlam, B., Lee, J., Xiao, L., Pennington, J., and Snoek, J.
Exploring the uncertainty properties of neural networks’
implicit priors in the inﬁnite-width limit. In International
Conference on Learning Representations, 2020. Cited on
page 1, 23.

Arﬁan, W.

Ukraine vectors by vecteezy.

https:

//www.vecteezy.com/vector-art/7506324-stand-
with-ukraine-text-with-ukraine-flag-ribbon-
and-ukraine-map-vector-design-on-a-dark-
blue-background, 2022. Cited on page 27.

Arora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R. R.,
and Wang, R. On exact computation with an inﬁnitely
wide neural net. In Advances in Neural Information Pro-
cessing Systems, pp. 8141–8150. Curran Associates, Inc.,
2019a. Cited on page 2.

Arora, S., Du, S. S., Hu, W., Li, Z., and Wang, R. Fine-
grained analysis of optimization and generalization for
overparameterized two-layer neural networks. arXiv
preprint arXiv:1901.08584, 2019b. Cited on page 24.

Arora, S., Du, S. S., Li, Z., Salakhutdinov, R., Wang, R.,
and Yu, D. Harnessing the power of inﬁnitely wide
deep nets on small-data tasks. In International Confer-
ence on Learning Representations, 2020. URL https:
//openreview.net/forum?id=rkl8sJBYvH. Cited on
page 1.

Babuschkin, I., Baumli, K., Bell, A., Bhupatiraju, S., Bruce,
J., Buchlovsky, P., Budden, D., Cai, T., Clark, A., Dani-
helka, I., Fantacci, C., Godwin, J., Jones, C., Hennigan,
T., Hessel, M., Kapturowski, S., Keck, T., Kemaev, I.,
King, M., Martens, L., Mikulik, V., Norman, T., Quan,
J., Papamakarios, G., Ring, R., Ruiz, F., Sanchez, A.,
Schneider, R., Sezener, E., Spencer, S., Srinivasan, S.,

Stokowiec, W., and Viola, F. The DeepMind JAX Ecosys-
tem, 2020. URL http://github.com/deepmind. Cited
on page 27.

Bahri, Y., Dyer, E., Kaplan, J., Lee, J., and Sharma,
arXiv preprint

U. Explaining neural scaling laws.
arXiv:2102.06701, 2021. Cited on page 1.

Bai, J., Lu, F., Zhang, K., et al. Onnx: Open neural
network exchange. https://github.com/onnx/onnx,
2019. Cited on page 27.

Belkin, M., Hsu, D., Ma, S., and Mandal, S. Reconciling
modern machine-learning practice and the classical bias–
variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849–15854, 2019. Cited on page
1.

Borovykh, A. A gaussian process perspective on convolu-
tional neural networks. arXiv preprint arXiv:1810.10798,
2018. Cited on page 1.

Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary,
C., Maclaurin, D., and Wanderman-Milne, S. JAX: com-
posable transformations of Python+NumPy programs,
2018. URL http://github.com/google/jax. Cited on
page 2, 8, 26, 27.

Brock, A., De, S., and Smith, S. L. Characterizing signal
propagation to close the performance gap in unnormal-
ized resnets. arXiv preprint arXiv:2101.08692, 2021a.
Cited on page 1.

Brock, A., De, S., Smith, S. L., and Simonyan, K. High-
performance large-scale image recognition without nor-
malization. arXiv preprint arXiv:2102.06171, 2021b.
Cited on page 1.

Chen, W., Gong, X., and Wang, Z. Neural architecture
search on imagenet in four gpu hours: A theoretically
In International Conference on
inspired perspective.
Learning Representations, 2021a. Cited on page 2, 26.

Chen, X., Hsieh, C.-J., and Gong, B. When vision trans-
formers outperform resnets without pretraining or strong
data augmentations, 2021b. Cited on page 1, 2, 26.

Dauphin, Y. N. and Schoenholz, S. Metainit: Initializing
learning by learning to initialize. Advances in Neural
Information Processing Systems, 32, 2019. Cited on page
1, 26.

Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,
L. Imagenet: A large-scale hierarchical image database.
In 2009 IEEE conference on computer vision and pattern
recognition, pp. 248–255. Ieee, 2009. Cited on page 2.

Fast Finite Width Neural Tangent Kernel

Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,
M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby,
N. An image is worth 16x16 words: Transformers for
In International Confer-
image recognition at scale.
ence on Learning Representations, 2021. URL https:
//openreview.net/forum?id=YicbFdNTTy. Cited on
page 2, 8, 14.

Du, S. S., Hou, K., Salakhutdinov, R. R., Poczos, B., Wang,
R., and Xu, K. Graph neural tangent kernel: Fusing graph
neural networks with graph kernels. Advances in neural
information processing systems, 32, 2019. Cited on page
24.

Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-
learning for fast adaptation of deep networks. In Pre-
cup, D. and Teh, Y. W. (eds.), Proceedings of the 34th
International Conference on Machine Learning, vol-
ume 70 of Proceedings of Machine Learning Research,
pp. 1126–1135. PMLR, 06–11 Aug 2017. URL https:
//proceedings.mlr.press/v70/finn17a.html. Cited
on page 2.

Franceschi, J.-Y., de B´ezenac, E., Ayed, I., Chen, M., Lam-
prier, S., and Gallinari, P. A neural tangent kernel per-
spective of gans. arXiv preprint arXiv:2106.05566, 2021.
Cited on page 1.

Frostig, R., Johnson, M. J., Maclaurin, D., Paszke, A., and
Radul, A. Decomposing reverse-mode automatic differ-
entiation. arXiv preprint arXiv:2105.09469, 2021. Cited
on page 4, 26.

Garriga-Alonso, A., Aitchison, L., and Rasmussen, C. E.
Deep convolutional networks as shallow gaussian pro-
cesses. In International Conference on Learning Repre-
sentations, 2019. Cited on page 1.

Griewank, A. and Walther, A.

Evaluating Deriva-
tives.
Society for Industrial and Applied Math-
ematics,
10.1137/
second edition, 2008.
1.9780898717761. URL https://epubs.siam.org/
doi/abs/10.1137/1.9780898717761. Cited on page 3,
17.

doi:

Grosse, R.

Neural net

Jan-
URL https://www.cs.toronto.edu/

training dynamics,

uary 2021.
∼rgrosse/courses/csc2541 2021/readings/
L02 Taylor approximations.pdf.
25.

Cited on page

Hanin, B. and Nica, M. Finite depth and width corrections
In International Confer-
to the neural tangent kernel.
ence on Learning Representations, 2020. URL https:
//openreview.net/forum?id=SJgndT4KwB. Cited on
page 24.

He, B., Lakshminarayanan, B., and Teh, Y. W. Bayesian
In
deep ensembles via the neural tangent kernel.
Larochelle, H., Ranzato, M., Hadsell, R., Balcan,
M., and Lin, H. (eds.), Advances in Neural Infor-
mation Processing Systems 33: Annual Conference
Information Processing Systems 2020,
on Neural
NeurIPS 2020, December 6-12, 2020, virtual, 2020.
https://proceedings.neurips.cc/paper/
URL
2020/hash/0b1ec366924b26fc98fa7b71a9c249cf-
Abstract.html. Cited on page 1.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition,
pp. 770–778, 2016. Cited on page 2, 8.

Heek, J., Levskaya, A., Oliver, A., Ritter, M., Rondepierre,
B., Steiner, A., and van Zee, M. Flax: A neural network
library and ecosystem for JAX, 2020. URL http://
github.com/google/flax. Cited on page 24, 27.

Hennigan, T., Cai, T., Norman, T., and Babuschkin, I. Haiku:
Sonnet for JAX, 2020. URL http://github.com/
deepmind/dm-haiku. Cited on page 24.

Horace He, R. Z. functorch: Jax-like composable function
transforms for pytorch. https://github.com/pytorch/
functorch, 2021. Cited on page 27.

Hron, J., Bahri, Y., Novak, R., Pennington, J., and Sohl-
Dickstein, J. Exact posterior distributions of wide
bayesian neural networks, 2020a. Cited on page 1.

Hron, J., Bahri, Y., Sohl-Dickstein, J., and Novak, R. Inﬁnite
attention: NNGP and NTK for deep attention networks.
In International Conference on Machine Learning, 2020b.
Cited on page 1, 24.

Hu, J., Shen, J., Yang, B., and Shao, L. Inﬁnitely wide
graph convolutional networks: semi-supervised learning
via gaussian processes. arXiv preprint arXiv:2002.12168,
2020. Cited on page 1.

Jacot, A., Gabriel, F., and Hongler, C. Neural tangent ker-
nel: Convergence and generalization in neural networks.
In Advances in Neural Information Processing Systems,
2018. Cited on page 1, 23, 26.

Khan, M. E. E., Immer, A., Abedi, E., and Korzepa, M.
Approximate inference turns deep networks into gaussian
processes. In Advances in neural information processing
systems, 2019. Cited on page 1.

Lee, J., Bahri, Y., Novak, R., Schoenholz, S., Pennington,
J., and Sohl-dickstein, J. Deep neural networks as gaus-
sian processes. In International Conference on Learning
Representations, 2018. Cited on page 1.

Fast Finite Width Neural Tangent Kernel

Lee, J., Xiao, L., Schoenholz, S. S., Bahri, Y., Novak, R.,
Sohl-Dickstein, J., and Pennington, J. Wide neural net-
works of any depth evolve as linear models under gradient
descent. In Advances in Neural Information Processing
Systems, 2019. Cited on page 1, 2, 23, 24, 26.

Lee, J., Schoenholz, S., Pennington, J., Adlam, B., Xiao, L.,
Novak, R., and Sohl-Dickstein, J. Finite versus inﬁnite
neural networks: an empirical study. Advances in Neural
Information Processing Systems, 33:15156–15172, 2020.
Cited on page 24.

Maclaurin, D., Duvenaud, D., and Adams, R. P. Auto-
In ICML 2015
grad: Effortless gradients in numpy.
AutoML Workshop, 2015. URL https://github.com/
HIPS/autograd. Cited on page 3.

Martens, J. and Grosse, R. Optimizing neural networks with
kronecker-factored approximate curvature. In Interna-
tional conference on machine learning, pp. 2408–2417.
PMLR, 2015. Cited on page 25.

Matthews, A., Hron, J., Rowland, M., Turner, R. E., and
Ghahramani, Z. Gaussian process behaviour in wide
deep neural networks. In International Conference on
Learning Representations, 2018. Cited on page 1.

M¨untz, H. Solution directe de l’´equation s´eculaire et de
quelques probl`emes analogues transcendants. C. R. Acad.
Sci. Paris, 156:43–46, 1913. Cited on page 26.

Naumann, U. Optimal accumulation of jacobian matrices
by elimination methods on the dual computational graph.
Mathematical Programming, 99(3):399–421, 2004. Cited
on page 2.

Naumann, U. Optimal jacobian accumulation is np-
complete. Mathematical Programming, 112(2):427–441,
2008. Cited on page 2.

Neal, R. M. Priors for inﬁnite networks (tech. rep. no. crg-
tr-94-1). University of Toronto, 1994. Cited on page
1.

Nguyen, T., Chen, Z., and Lee, J.

Dataset meta-
learning from kernel ridge-regression. arXiv preprint
arXiv:2011.00050, 2020. Cited on page 1.

Nguyen, T., Novak, R., Xiao, L., and Lee, J. Dataset distil-
lation with inﬁnitely wide convolutional networks. arXiv
preprint arXiv:2107.13034, 2021. Cited on page 1.

Novak, R., Xiao, L., Lee, J., Bahri, Y., Yang, G., Hron, J.,
Abolaﬁa, D. A., Pennington, J., and Sohl-Dickstein, J.
Bayesian deep convolutional networks with many chan-
nels are gaussian processes. In International Conference
on Learning Representations, 2019. Cited on page 1, 24.

Novak, R., Xiao, L., Hron, J., Lee, J., Alemi, A. A.,
Sohl-Dickstein, J., and Schoenholz, S. S. Neural
tangents: Fast and easy inﬁnite neural networks in
python. In International Conference on Learning Repre-
sentations, 2020. URL https://github.com/google/
neural-tangents. Cited on page 1, 2, 8, 24, 27.

Oreshkin, B. N., L´opez, P. R., and Lacoste, A. Tadam:
Task dependent adaptive metric for improved few-shot
learning. In NeurIPS, 2018. Cited on page 2.

Park, D. S., Lee, J., Peng, D., Cao, Y., and Sohl-Dickstein,
J. Towards nngp-guided neural architecture search. arXiv
preprint arXiv:2011.06006, 2020. Cited on page 1, 2.

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury,
J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N.,
Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito,
Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner,
B., Fang, L., Bai, J., and Chintala, S. Pytorch: An
imperative style, high-performance deep learning
library.
In Wallach, H., Larochelle, H., Beygelzimer,
A., d'Alch´e-Buc, F., Fox, E., and Garnett, R. (eds.),
Advances in Neural Information Processing Systems 32,
pp. 8024–8035. Curran Associates, Inc., 2019. URL
http://papers.neurips.cc/paper/9015-pytorch-
an-imperative-style-high-performance-deep-
learning-library.pdf. Cited on page 2, 4, 26.

Pennington, J. and Bahri, Y. Geometry of neural net-
work loss surfaces via random matrix theory.
In
Precup, D. and Teh, Y. W. (eds.), Proceedings of
the 34th International Conference on Machine Learn-
ing, volume 70 of Proceedings of Machine Learn-
ing Research, pp. 2798–2806. PMLR, 06–11 Aug
2017. URL https://proceedings.mlr.press/v70/
pennington17a.html. Cited on page 25.

Radul, A., Paszke, A., Frostig, R., Johnson, M., and Maclau-
rin, D. You only linearize once: Tangents transpose to
gradients. arXiv preprint arXiv:2204.10923, 2022. Cited
on page 4, 17.

Schoenholz, S. S., Gilmer, J., Ganguli, S., and Sohl-
Dickstein, J. Deep information propagation. Interna-
tional Conference on Learning Representations, 2017.
Cited on page 1.

Spigler, S., Geiger, M., d’Ascoli, S., Sagun, L., Biroli, G.,
and Wyart, M. A jamming transition from under-to over-
parametrization affects generalization in deep learning.
Journal of Physics A: Mathematical and Theoretical, 52
(47):474001, 2019. Cited on page 1.

Steiner, A., Kolesnikov, A., Zhai, X., Wightman, R., Uszkor-
eit, J., and Beyer, L. How to train your vit? data, augmen-
tation, and regularization in vision transformers. arXiv
preprint arXiv:2106.10270, 2021. Cited on page 8, 14.

Fast Finite Width Neural Tangent Kernel

Tancik, M., Srinivasan, P. P., Mildenhall, B., Fridovich-Keil,
S., Raghavan, N., Singhal, U., Ramamoorthi, R., Barron,
J. T., and Ng, R. Fourier features let networks learn
high frequency functions in low dimensional domains.
NeurIPS, 2020. Cited on page 1.

Tolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai,
X., Unterthiner, T., Yung, J., Steiner, A., Keysers, D.,
Uszkoreit, J., Lucic, M., and Dosovitskiy, A. Mlp-mixer:
An all-mlp architecture for vision, 2021. Cited on page 2,
8, 14.

Xiao, L., Bahri, Y., Sohl-Dickstein, J., Schoenholz, S., and
Pennington, J. Dynamical isometry and a mean ﬁeld
theory of CNNs: How to train 10,000-layer vanilla con-
volutional neural networks. In International Conference
on Machine Learning, 2018. Cited on page 1.

Xiao, L., Pennington, J., and Schoenholz, S. S. Disentan-
gling trainability and generalization in deep learning. In
International Conference on Machine Learning, 2020.
Cited on page 1, 23.

Yaida, S. Non-Gaussian processes and neural networks at
ﬁnite widths. In Mathematical and Scientiﬁc Machine
Learning Conference, 2020. Cited on page 24.

Yang, G. Scaling limits of wide neural networks with
weight sharing: Gaussian process behavior, gradient in-
dependence, and neural tangent kernel derivation. arXiv
preprint arXiv:1902.04760, 2019. Cited on page 1, 24.

Yang, G. Tensor programs ii: Neural tangent kernel for
any architecture. arXiv preprint arXiv:2006.14548, 2020.
Cited on page 24.

Yang, G., Pennington, J., Rao, V., Sohl-Dickstein, J., and
Schoenholz, S. S. A mean ﬁeld theory of batch nor-
In International Conference on Learning
malization.
Representations, 2019. Cited on page 24.

Zagoruyko, S. and Komodakis, N. Wide residual networks.
In British Machine Vision Conference, 2016. Cited on
page 2, 8, 14.

Zhang, H., Dauphin, Y. N., and Ma, T. Fixup initialization:
Residual learning without normalization. arXiv preprint
arXiv:1901.09321, 2019. Cited on page 1.

Zhou, Y., Wang, Z., Xian, J., Chen, C., and Xu, J.
In In-
Meta-learning with neural
ternational Conference on Learning Representations,
URL https://openreview.net/forum?id=
2021.
Ti87Pv5Oc8. Cited on page 2, 26.

tangent kernels.

Zoph, B. and Le, Q. V. Neural architecture search with
reinforcement learning, 2016. URL http://arxiv.org/
abs/1611.01578. Cited on page 2.

Fast Finite Width Neural Tangent Kernel

Appendix

A. Additional Figures

CPU (Skylake)

NVIDIA V100

TPUv4

NVIDIA P100

Figure 3. Wall-clock time of computing the NTK of a 10-layer ReLU FCN on different platforms. In all settings, Structured
derivatives allow orders of magnitude improvement in wall-clock time and memory (missing points indicate out-of-memory error).
However, we remark that on GPU platforms (right), NTK-vector products deliver a robust improvement only for large O (rightmost
column), while for O = 16 the cost is comparable or even larger than Jacobian contraction. See Fig. 1 for FLOPs, TPUv3 platform, and
more discussion. See §N for details.

103

100

10 3

103

100

10 3

103

100

10 3

s
d
n
o
c
e
S

s
d
n
o
c
e
S

s
d
n
o
c
e
S

O =  1 logits

O =  16 logits

O =  128 logits

N
=
1
b
a
t
c
h

N
=
1
6
b
a
t
c
h

N
=
1
2
8
b
a
t
c
h

20 23 26 29 212
Width W

20 23 26 29 212
Width W

20 23 26 29 212
Width W

 
 
 
 
 
 
 
 
 
102

100

10 2

102

100

10 2

102

100

10 2

s
d
n
o
c
e
S

s
d
n
o
c
e
S

s
d
n
o
c
e
S

O =  1 logits

O =  16 logits

O =  128 logits

N
=
1
b
a
t
c
h

N
=
1
6
b
a
t
c
h

N
=
1
2
8
b
a
t
c
h

20 23 26 29 212
Width W

20 23 26 29 212
Width W

20 23 26 29 212
Width W

 
 
 
 
 
 
 
 
 
O =  1 logits

O =  16 logits

O =  128 logits

N
=
1
b
a
t
c
h

N
=
1
6
b
a
t
c
h

N
=
1
2
8
b
a
t
c
h

s
d
n
o
c
e
S

10 1

10 3

s
d
n
o
c
e
S

10 1

10 3

s
d
n
o
c
e
S

10 1

10 3

20 23 26 29 212
Width W

20 23 26 29 212
Width W

20 23 26 29 212
Width W

 
 
 
 
 
 
 
 
 
O =  1 logits

O =  16 logits

O =  128 logits

N
=
1
b
a
t
c
h

N
=
1
6
b
a
t
c
h

N
=
1
2
8
b
a
t
c
h

s
d
n
o
c
e
S

s
d
n
o
c
e
S

s
d
n
o
c
e
S

102

100

10 2

102

100

10 2

102

100

10 2

20 23 26 29 212
Width W

20 23 26 29 212
Width W

20 23 26 29 212
Width W

 
 
 
 
 
 
 
 
 
Jacobian contraction
NTK-vector products

Structured derivatives
Jacobian

W = O
W 2

O =  1 logits

O =  16 logits

O =  128 logits

101

s

d

n

o

c

e

S

10 1

10 3

101

s

d

n

o

c

e

S

10 1

10 3

101

s

d

n

o

c

e

S

10 1

10 3

N

=

1

b

a

t

c

h

s

i

z

e

N

=

1

6

b

a

t

c

h

s

i

z

e

N

=

1

2

8

b

a

t

c

h

s

i

z

e

20 23 26 29 212

20 23 26 29 212

20 23 26 29 212

Width W

Width W

Width W

 
 
 
 
 
 
 
 
 
 
 
 
Fast Finite Width Neural Tangent Kernel

Figure 4. Wall-clock time per input pair of computing NTK on various ImageNet models like Vision Tansformers and hybrids
(Dosovitskiy et al., 2021; Steiner et al., 2021), WideResNets (Zagoruyko & Komodakis, 2016) and MLP-Mixers (Tolstikhin et al., 2021).
Structured derivatives generally allow fastest computation, but also are able to process more models due to lower memory requirements
(lower left; missing points indicate out-of-memory error). For the case of single output logit O = 1 (top row), NTK-vector products are
generally detrimental due to a costly forward pass FP relative to the size of parameters P (i.e. a lot of weight sharing; see Table 1).
However, since NTK-vector products scale better than other methods with output size, for O = 1000 (bottom row), they perform
comparably or better than other methods.
Finally, we remark that the Jacobian not only runs out of memory faster, but can also take more time to compute. We conjecture that due
to a larger memory footprint, XLA can sometimes perform optimizations that trade off speed for memory, and therefore compute the
Jacobian in a less optimal way than if it had more memory available. Alternatively, XLA could also be performing simpliﬁcations of the
NTK expression in these cases, such that those would not be possible in Jacobian computation alone.

See Fig. 2 for ResNets, and §N for details.

Figure 5. Notation used in §4.1 (FCN, top) and §F (CNN, bottom). In both settings L = 2. For FCN, K = 5 (3 matrix multiplication
primitives and two nonlinearities φ), D = F = 1. For CNN, there is an extra global average pooling primitive as the penultimate layer,
therefore K = 6, and D = 8, F = 3.

NVIDIA V100

TPUv4

CPU

s
d
n
o
c
e
S

10 2

10 4

s
d
n
o
c
e
100S

10 3

10 5

100

10 1

Jacobian contraction
NTK-vector products
Structured derivatives
Jacobian

100

10 2

104

103

102

O
=
1

l

o
g
i
t
s

O
=
1
0
0
0

l

o
g
i
t
s

6
1
_
i
T
-
T
V
+
R

i

6
1
_
i
T
-
T
V

i

2
3
_
S
-
T
V

i

6
1
_
S
-
T
V

i

2
3
_
B
-
T
V

i

2
3
_
L
-
T
V

i

6
1
_
B
-
r
e
x
M

i

6
1
_
L
-
T
V

i

6
1
_
L
-
r
e
x
M

i

0
1
_
8
2
_
n
r
w

2
1
_
8
2
_
n
r
w

4
1
_
H
-
T
V

i

i

6
1
_
B
-
T
V
+
0
5
R

i

2
3
_
L
-
T
V
+
0
5
R

6
1
_
B
-
T
V

i

i

2
3
_
B
-
T
V
+
6
2
R

i

2
3
_
S
-
T
V
+
6
2
R

Model

6
1
_
i
T
-
T
V
+
R

i

6
1
_
i
T
-
T
V

i

2
3
_
S
-
T
V

i

6
1
_
S
-
T
V

i

2
3
_
B
-
T
V

i

2
3
_
L
-
T
V

i

6
1
_
B
-
r
e
x
M

i

6
1
_
L
-
T
V

i

6
1
_
L
-
r
e
x
M

i

0
1
_
8
2
_
n
r
w

2
1
_
8
2
_
n
r
w

4
1
_
H
-
T
V

i

i

6
1
_
B
-
T
V
+
0
5
R

i

2
3
_
L
-
T
V
+
0
5
R

6
1
_
B
-
T
V

i

i

2
3
_
B
-
T
V
+
6
2
R

i

2
3
_
S
-
T
V
+
6
2
R

Model

6
1
_
i
T
-
T
V
+
R

i

6
1
_
i
T
-
T
V

i

2
3
_
S
-
T
V

i

6
1
_
S
-
T
V

i

2
3
_
B
-
T
V

i

2
3
_
L
-
T
V

i

6
1
_
B
-
r
e
x
M

i

6
1
_
L
-
T
V

i

6
1
_
L
-
r
e
x
M

i

0
1
_
8
2
_
n
r
w

2
1
_
8
2
_
n
r
w

4
1
_
H
-
T
V

i

i

6
1
_
B
-
T
V
+
0
5
R

i

2
3
_
L
-
T
V
+
0
5
R

6
1
_
B
-
T
V

i

i

2
3
_
B
-
T
V
+
6
2
R

i

2
3
_
S
-
T
V
+
6
2
R

Model

 
 
 
 
 
 
4

=

< l a t e x i t   s h a 1 _ b a s e 6 4 = "" D R a Y i w Z Y S Z q S 2 p V k f L w Q v A u Q a + Q = "" > A A A C 9 X i c d Z L L j t M w F I b d c J k h 3 G Z g y c a i Q m J V J S M E b E Y z A h Y s K h g k O h 2 p i U a 2 4 7 T W + C b b o V R W X o E t b F i B 2 P I Y P A J C v A x O 0 w V p 4 E i W f p 3 z H Z + L j T V n 1 i X J r 0 F 0 6 f K V q z u 7 1 + L r N 2 7 e u r 2 3 f + f U q s o Q O i G K K 3 O G k a W c S T p x z H F 6 p g 1 F A n M 6 x R f P m / j 0 H T W W K f n W r T T N B Z p L V j K C X O P K 5 O G j 8 7 1 h M k r W B v s i 3 Y j h 0 Y / 4 U H / 5 G Z + c 7 w 9 + Z 4 U i l a D S E Y 6 s n a W J d r l H x j H C a R 1 n l a U a k Q s 0 p 7 M g J R L U 5 n 7 d b A 0 f B E 8 B S 2 X C k Q 6 u v X 9 n e C S s X Q k c S I H c w m 7 H G u c / Y x i L r d K u f J p 7 J n X l q C R t 5 b L i 0 C n Y r A I W z F D i + C o I R A w L z U O y Q A Y R F x b 2 n y H e t 1 P E c S b p k i g h k C x 8 J u t Z k n u f r Y N e V 0 a H N f g M l / B V X d d d t i g 6 s K O I t + i L H q p U B 8 W 8 2 t z 6 u o e O x x 1 U G S T n G 3 j c g 5 c d l q y Q b M l p Q 4 b v k G 4 / f l + c H o z S x 6 O D N 8 n w + B l o b R f c A / f B Q 5 C C J + A Y v A Q n Y A I I W I A P 4 C P 4 F C 2 j z 9 H X 6 F u L R o N N z l 3 Q s e j 7 H 8 h v + w I = < / l a t e x i t >

N

4

=

< l a t e x i t   s h a 1 _ b a s e 6 4 = "" D R a Y i w Z Y S Z q S 2 p V k f L w Q v A u Q a + Q = "" > A A A C 9 X i c d Z L L j t M w F I b d c J k h 3 G Z g y c a i Q m J V J S M E b E Y z A h Y s K h g k O h 2 p i U a 2 4 7 T W + C b b o V R W X o E t b F i B 2 P I Y P A J C v A x O 0 w V p 4 E i W f p 3 z H Z + L j T V n 1 i X J r 0 F 0 6 f K V q z u 7 1 + L r N 2 7 e u r 2 3 f + f U q s o Q O i G K K 3 O G k a W c S T p x z H F 6 p g 1 F A n M 6 x R f P m / j 0 H T W W K f n W r T T N B Z p L V j K C X O P K 5 O G j 8 7 1 h M k r W B v s i 3 Y j h 0 Y / 4 U H / 5 G Z + c 7 w 9 + Z 4 U i l a D S E Y 6 s n a W J d r l H x j H C a R 1 n l a U a k Q s 0 p 7 M g J R L U 5 n 7 d b A 0 f B E 8 B S 2 X C k Q 6 u v X 9 n e C S s X Q k c S I H c w m 7 H G u c / Y x i L r d K u f J p 7 J n X l q C R t 5 b L i 0 C n Y r A I W z F D i + C o I R A w L z U O y Q A Y R F x b 2 n y H e t 1 P E c S b p k i g h k C x 8 J u t Z k n u f r Y N e V 0 a H N f g M l / B V X d d d t i g 6 s K O I t + i L H q p U B 8 W 8 2 t z 6 u o e O x x 1 U G S T n G 3 j c g 5 c d l q y Q b M l p Q 4 b v k G 4 / f l + c H o z S x 6 O D N 8 n w + B l o b R f c A / f B Q 5 C C J + A Y v A Q n Y A I I W I A P 4 C P 4 F C 2 j z 9 H X 6 F u L R o N N z l 3 Q s e j 7 H 8 h v + w I = < / l a t e x i t >

N

4

=

< l a t e x i t   s h a 1 _ b a s e 6 4 = "" D R a Y i w Z Y S Z q S 2 p V k f L w Q v A u Q a + Q = "" > A A A C 9 X i c d Z L L j t M w F I b d c J k h 3 G Z g y c a i Q m J V J S M E b E Y z A h Y s K h g k O h 2 p i U a 2 4 7 T W + C b b o V R W X o E t b F i B 2 P I Y P A J C v A x O 0 w V p 4 E i W f p 3 z H Z + L j T V n 1 i X J r 0 F 0 6 f K V q z u 7 1 + L r N 2 7 e u r 2 3 f + f U q s o Q O i G K K 3 O G k a W c S T p x z H F 6 p g 1 F A n M 6 x R f P m / j 0 H T W W K f n W r T T N B Z p L V j K C X O P K 5 O G j 8 7 1 h M k r W B v s i 3 Y j h 0 Y / 4 U H / 5 G Z + c 7 w 9 + Z 4 U i l a D S E Y 6 s n a W J d r l H x j H C a R 1 n l a U a k Q s 0 p 7 M g J R L U 5 n 7 d b A 0 f B E 8 B S 2 X C k Q 6 u v X 9 n e C S s X Q k c S I H c w m 7 H G u c / Y x i L r d K u f J p 7 J n X l q C R t 5 b L i 0 C n Y r A I W z F D i + C o I R A w L z U O y Q A Y R F x b 2 n y H e t 1 P E c S b p k i g h k C x 8 J u t Z k n u f r Y N e V 0 a H N f g M l / B V X d d d t i g 6 s K O I t + i L H q p U B 8 W 8 2 t z 6 u o e O x x 1 U G S T n G 3 j c g 5 c d l q y Q b M l p Q 4 b v k G 4 / f l + c H o z S x 6 O D N 8 n w + B l o b R f c A / f B Q 5 C C J + A Y v A Q n Y A I I W I A P 4 C P 4 F C 2 j z 9 H X 6 F u L R o N N z l 3 Q s e j 7 H 8 h v + w I = < / l a t e x i t >

N

<latexit sha1_base64=""N7dWDDp5SaZjiomhSQBw4NYjymw="">AAACs3icdZHLTttAFIYnbrk0XArtslVlgZBYRXYkCkvUbrrIAqQGkGILnRmfJCPmppkxEFledttt2zfhVfoMfYlOEha1KUca6dd/vtG5USO480nyuxO9eLmyurb+qruxubX9emf3zYXTpWU4ZFpoe0XBoeAKh557gVfGIkgq8JLefJ7nL2/ROq7VVz8zmEuYKD7mDPzcysyUX+/sJ71kEfFTkT6K/dP3D+d/vn14OLve7fzKCs1KicozAc6N0sT4vALrORNYd7PSoQF2AxMcBalAosurRbN1fBCcIh5rG57y8cL990cF0rmZpIGU4KeunZub/81RKlul/fgkr7gypUfFlpXHpYi9jueriAtukXkxCwKY5aH5mE3BAvNhYc8Mcb+cotvNFN4xLSWoospUPUryqsoWycqU1oQ1qLpuUkXRwDyCqIs2pHUDoqLEWrehwaABaQtqgvUgYOGYaft0T8VFv5d+7PXPw1U/kWWsk3dkjxySlByTU/KFnJEhYWRKvpMf5Gd0FI0iGhVLNOo8/nlLGhHJv52w4M8=</latexit>","Fast Finite Width Neural Tangent Kernel Roman Novak 1 Jascha Sohl-Dickstein 1 Samuel S. Schoenholz 1 2 2 0 2 n u J 7 1 ] G L . s c [ 1 v 0 2 7 8 0 . 6 0 2 2 : v i X r a Abstract The Neural Tangent Kernel (NTK), deﬁned as θ (x1, x2) = (cid:2)∂f (θ, x1)(cid:14)∂θ(cid:3) (cid:2)∂f (θ, x2)(cid:14)∂θ(cid:3)T Θf where (cid:2)∂f (θ, )(cid:14)∂θ(cid:3) is a neural network (NN) Ja- · cobian, has emerged as a central object of study in deep learning. In the inﬁnite width limit, the NTK can sometimes be computed analytically and is useful for understanding training and gener- alization of NN architectures. At ﬁnite widths, the NTK is also used to better initialize NNs, compare the conditioning across models, perform architec- ture search, and do meta-learning. Unfortunately, the ﬁnite width NTK is notoriously expensive to compute, which severely limits its practical utility. We perform the ﬁrst in-depth analysis of the compute and memory requirements for NTK computation in ﬁnite width networks. Leverag- ing the structure of neural networks, we further propose two novel algorithms that change the ex- ponent of the compute and memory requirements of the ﬁnite width NTK, dramatically improving efﬁciency. Our algorithms can be applied in a black box fashion to any differentiable function, including those implementing neural networks. We open-source our implementations within the Neural Tangents package (Novak et al., 2020) at github.com/google/neural-tangents. 1. Introduction The past few years have seen signiﬁcant progress towards a theoretical foundation for deep learning. Much of this work has focused on understanding the properties of ran- dom functions in high dimensions. One signiﬁcant line of work (Neal, 1994; Lee et al., 2018; Matthews et al., 2018; Borovykh, 2018; Garriga-Alonso et al., 2019; Novak et al., 2019; Yang, 2019; Hron et al., 2020b;a; Hu et al., 2020) 1Google Brain, Mountain View, California, United States. Correspondence to: Roman Novak <romann@google.com>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s). established that in the limit of inﬁnite width, randomly ini- tialized Neural Networks (NNs) are Gaussian Processes (called NNGPs). Building on this development, Jacot et al. (2018) showed that in function space the dynamics under gradient descent could be computed analytically using the so-called Neural Tangent Kernel (NTK) and Lee et al. (2019) showed that wide neural networks reduce to their lineariza- tions in weight space throughout training. A related set of results (Belkin et al., 2019; Spigler et al., 2019) showed that the ubiquitous bias-variance decomposition breaks down as high-dimensional models enter the so-called interpolat- ing regime. Together these results describe learning in the inﬁnite width limit and help explain the impressive general- ization capabilities of NNs. Insights from the wide network limit have had signiﬁcant practical impact. The conditioning of the NTK has been shown to signiﬁcantly impact trainability and generaliza- tion in NNs (Schoenholz et al., 2017; Xiao et al., 2018; 2020). This notion inspired initialization schemes like Fixup (Zhang et al., 2019), MetaInit (Dauphin & Schoen- holz, 2019), and Normalizer Free networks (Brock et al., 2021a;b), and has enabled efﬁcient neural architecture search (Park et al., 2020; Chen et al., 2021b). The NTK has additionally given insight into a wide range of phe- nomena such as: behavior of Generative Adversarial Net- works (Franceschi et al., 2021), neural scaling laws (Bahri et al., 2021), and neural irradiance ﬁelds (Tancik et al., 2020). Kernel regression using the NTK has further enabled strong performance on small datasets (Arora et al., 2020), and applications such as approximate inference (Khan et al., 2019), dataset distillation (Nguyen et al., 2020; 2021), and uncertainty prediction (He et al., 2020; Adlam et al., 2020). Despite the signiﬁcant promise of theory based on the NTK, computing the NTK in practice is challenging. In the inﬁnite width limit, the NTK can sometimes be computed analyti- cally. However, the inﬁnite-width kernel remains intractable for many architectures and ﬁnite width corrections are often important to describe actual NNs used in practice (see §I for detailed discussion). The NTK matrix can be computed for ﬁnite width networks as the outer product of Jacobians using forward or reverse mode automatic differentiation (AD), Θf (cid:124) θ (x1, x2) (cid:123)(cid:122) (cid:125) O×O := (cid:2)∂f (θ, x1)(cid:14)∂θ(cid:3) (cid:123)(cid:122) (cid:125) O×P (cid:124) (cid:2)∂f (θ, x2)(cid:14)∂θ(cid:3)T , (cid:125) (cid:123)(cid:122) (cid:124) P×O (1) Fast Finite Width Neural Tangent Kernel where f is the forward pass NN function producing outputs in RO, θ RP are all trainable parameters, and x1 and x2 are two inputs to the network. If inputs are batches of sizes N1 and N2, the NTK is an N1O N2O matrix. ∈ × Unfortunately, evaluating Eq. (1) is often infeasible due to time and memory requirements. For modern machine learn- ing tasks O is often greater (sometimes much greater) than 1000 (e.g. for ImageNet (Deng et al., 2009)), while even modestly sized models feature tens of millions of parame- 107. This makes both storing ([N1 + N2] OP ters, or P memory) and contracting ([N1N2] O2P time) the Jacobians in Eq. (1) very costly. The theoretical importance of the NTK together with its prohibitive computational costs im- plies that performance improvements will unlock impactful novel research. ∼ We perform the ﬁrst in-depth analysis of the compute and memory requirements for the NTK as in Eq. (1). Noting that forward and reverse mode AD are two extremes of a wide range of AD strategies (Naumann, 2004; 2008), we explore other methods for computing the NTK leveraging the structure of NNs used in practice. We propose two novel methods for computing the NTK that exploit different orderings of the computation. We describe the compute and memory requirements of our techniques in fully-connected (FCN) and convolutional (CNN) settings, and show that one is asymptotically more efﬁcient in both settings. We compute the NTK over a wide range of NN architectures and demonstrate that these improvements are robust in practice. We open-source our implementations as general-purpose JAX1 (Bradbury et al., 2018) function transformations. 2. Related Work The ﬁnite width NTK (denoted simply NTK throughout this work2) has been used extensively in many recent works, but to our knowledge implementation details and compute costs were rarely made public. Below we draw comparison to some of these works, but we stress that it only serves as a sanity check to make sure our contribution is valuable relative to the scale of problems that have been attempted. None of these works had efﬁcient NTK computation as their central goal. In order to compare performance of models based on the NTK and the inﬁnite width NTK, Arora et al. (2019a, Table 2) compute the NTK of up to 20-layer, 128-channel CNN in a binary CIFAR-2 classiﬁcation setting. In an equivalent 1Our algorithms are framework-agnostic, but implementation in JAX is easier, as described in §M. We also provide instructions for implementation in other frameworks like Tensorﬂow (Abadi et al., 2016) and PyTorch (Paszke et al., 2019) in §M. 2See §I for a comparison between the ﬁnite and inﬁnite width settings. setting with the same hardware (NVIDIA V100), we are able to compute the NTK of a 2048-channel CNN, i.e. a network with at least 256 times more parameters. To demonstrate the stability of the NTK during training for wide networks, Lee et al. (2019, Figure S6) compute the NTK of up to 3-layer 212-wide or 1-layer 214-wide FCNs. In the same setting with the same hardware (NVIDIA V100), we can reach widths of at least 214 and 218 respectively, i.e. handle networks with 4 to 16 times more parameters. To investigate convergence of a WideResNet WRN-28-k (Zagoruyko & Komodakis, 2016) to its inﬁnite width limit, Novak et al. (2020, Figure 2) evaluate the NTK of this model with widening factor k up to 32. In matching setting and hardware, we are able to reach a widening factor of at least 64, i.e. work with models at least 4 times larger. To meta-learn NN parameters for transfer learning in a MAML-like (Finn et al., 2017) setting, Zhou et al. (2021, Table 7) replace the inner training loop with NTK-based inference. They use up to 5-layer, 200-channel CNNs on MiniImageNet (Oreshkin et al., 2018) with scalar outputs and batch size 25. In same setting we achieve at least 512 channels, i.e. support models at least 6 times larger. Park et al. (2020, §4.1) use the NTK to predict the gen- eralization performance of architectures in the context of Neural Architecture Search (Zoph & Le, 2016, NAS); how- ever, the authors comment on its high computational burden and ultimately use a different proxy objective. In another NAS setting, Chen et al. (2021a, §3.1.1) use the condition number of NTK to predict a model’s trainability. Chen et al. (2021b, Table 1) use the NTK to evaluate the trainability of several ImageNet models such as ResNet 50/152 (He et al., 2016), Vision Transformer (Dosovitskiy et al., 2021) and MLP-Mixer (Tolstikhin et al., 2021). However, due to the prohibitive computational cost, in all of these cases the authors only evaluate a pseudo-NTK, i.e. the NTK of a scalar-valued function,3 which impacts the quality of the respective trainability/generalization proxy. By contrast, in this work we can compute the full 1000 × 1000 (1000 classes) NTK for the same models, i.e. perform a task 1000 times more costly. Finally, we remark that in all of the above settings, scaling up by increasing width or by working with the true NTK (vs the pseudo-NTK) should lead to improved downstream task performance due to a better inﬁnite width/linearization approximation or a higher-quality trainability/generalization proxy respectively, which makes our work especially rele- vant to modern research. 3Precisely, computing the Jacobian only for a single logit or the sum of all 1000 class logits. The result is not the full NTK, but rather a single diagonal block or the sum of its 1000 diagonal blocks (the ﬁnite width NTK is a dense matrix, not block-diagonal). Fast Finite Width Neural Tangent Kernel 3. Algorithms for Efﬁcient NTK Computation We now describe our algorithms for fast NTK computation. In §3.1 we cover preliminaries. We begin by introduc- ing notation used throughout the paper (§3.1.1). We then (§3.1.2) describe primitive building blocks of AD includ- ing the Jacobian-vector products (JVP) and vector-Jacobian products (VJP) that correspond to forward and reverse mode AD respectively before discussing the Jacobian (§3.1.3). In §3.2 we apply the above tools to describe the computa- tional complexity of the baseline approach to computing the NTK that is used in most (likely all) prior works. In §3.3 and §3.4 we present our two algorithms that each enable accelerating the computation by orders of magnitude in different ways. 3.1. Preliminaries 3.1.1. NOTATION ∈ RO with O outputs (e.g. class Consider a NN f (θ, x) logits) per input x and a total number P of trainable pa- rameters θ = vec (cid:2)θ0, . . . , θL(cid:3), with each θl of size Pl, P = (cid:80)L l=0 Pl. Also assume the network has K intermediate primitive outputs yk of size Yk each (for example, activa- tions or pre-activations), and let Y = (cid:80)K k=1 Yk be the total size of the outputs (see Fig. 5 and Fig. 6). The NTK is Θf θ (cid:124)(cid:123)(cid:122)(cid:125) O×O := (cid:2)∂f (θ, x1)(cid:14)∂θ(cid:3) (cid:123)(cid:122) (cid:125) O×P (cid:124) (cid:2)∂f (θ, x2) (cid:14)∂θ(cid:3)T (cid:123)(cid:122) (cid:125) (cid:124) P×O L (cid:88) l=0 (cid:2)∂f (θ, x1)(cid:14)∂θl(cid:3) (cid:124) (cid:123)(cid:122) (cid:125) O×Pl (cid:2)∂f (θ, x2)(cid:14)∂θl(cid:3)T (cid:123)(cid:122) (cid:125) (cid:124) Pl×O . (3) We denote FP to be the (time or memory, depending on con- text) cost of a single forward pass f (θ, x). For memory, we exclude the cost of storing all P weights, but rather deﬁne it to be the cost of evaluating f one primitive yk at a time. This (cid:0)maxk Yk + maxl Pl(cid:1), gives a memory cost of at most which we denote as simply Yk + Pl.4 Finally, we will con- sider x1 and x2 to be batches of N inputs each, in which case the NTK will be an NO NO matrix, obtained by com- × puting Eq. (2) for each pair of inputs. See §B for glossary. O 3.1.2. JACOBIAN-VECTOR PRODUCTS (JVP) AND VECTOR-JACOBIAN PRODUCTS (VJP) Following Maclaurin et al. (2015) we deﬁne JVPf VJPf (θ,x) : θt ∈ (θ,x) : fc ∈ P R O R (cid:2)∂f (θ, x)(cid:14)∂θ(cid:3) θt ∈ R (cid:2)∂f (θ, x)(cid:14)∂θ(cid:3)T fc ∈ (cid:55)→ (cid:55)→ O; P. R (4) (5) The JVP can be understood as pushing forward a tangent vector θt in weight space to a tangent vector in the space of outputs; by contrast the VJP pulls back a cotangent vector fc in the space of outputs to a cotangent vector in weight space. These elementary operations enable forward and reverse mode AD respectively and serve as a basis for typical AD computations such as gradients, Jacobians, Hessians, etc. The time cost of both is comparable to FP (see §D and Griewank & Walther (2008)). The memory cost of a JVP is FP as well (i.e. Yk + Pl), while the memory cost of a VJP is generally Y + P, since it requires storing all K intermediate primitive outputs for efﬁcient backprop and all L output cotangents. However, for the purpose of computing the NTK, we never need to store the whole Jacobian ∂f /∂θ, but only individual cotangents like ∂f /∂θl to compute the sum in Eq. (2) layer-by-layer. Hence we consider VJP to cost Y + Pl memory. To summarize, for a batch of N inputs, • JVP costs N [FP] time; N (cid:2)Yk(cid:3) + P memory. • VJP costs N [FP] time; N (cid:2)Y + Pl(cid:3) + P memory. 3.1.3. JACOBIAN The reverse mode Jacobian ∂f /∂θ is computed via O VJP calls on rows of the identity matrix IO RO×O, i.e. = (2) (cid:2)∂f (θ, x) (cid:14)∂θ(cid:3)T ∈ = (cid:2)∂f (θ, x) (cid:14)∂θ(cid:3)T P×O, R (6) IO ∈ and therefore costs O [VJP] time and memory apart from parameters and primitive outputs that can be reused across VJPs. Therefore, for a batch of N inputs, Jacobian costs NO [FP] time; NO (cid:2)Yk + Pl(cid:3)+NY+ P memory. 3.2. Jacobian contraction – the Baseline This baseline method of evaluating the NTK consists in computing the Jacobians ∂f /∂θ and contracting them as in Eq. (2). The contraction costs N2O2P time and N2O2 + NOPl memory, to store the result Θf θ and individ- ual layer-by-layer cotangents ∂f /∂θl. Including the cost of computing the cotangents via the batch Jacobian ∂f /∂θ = (cid:2)∂f /∂θ0, . . . , ∂f /∂θL(cid:3) from §3.1.3 we arrive at Jacobian contraction costs NO [FP] + N2O2P time; N2O2 + NO (cid:2)Yk + Pl(cid:3) + NY + P memory. 4To declutter notation throughout this work, in time and mem- ory complexity expressions, we (1) omit the O symbol, and (2) imply taking the maximum over any free index. In summary, Jacobian contraction performs NO forward passes followed by an expensive N2O2P contraction. Next we demonstrate how to reduce the contraction cost. Fast Finite Width Neural Tangent Kernel 3.3. NTK-vector products Consider the NTK-vector product function (for N = 1): Θf θ VP : v O R ∈ (cid:55)→ Θf θ v ∈ O. R Taking the NTK-vector product with O columns of the iden- tity matrix IO yields the full NTK, i.e. Θf θ . Ex- panding Θf θ IO = Θf θ VP(v) as θ v = (cid:2)∂f (θ, x1)(cid:14)∂θ(cid:3) (cid:2)∂f (θ, x2)(cid:14)∂θ(cid:3)T Θf (θ,x2) (v) = = (cid:2)∂f (θ, x1)(cid:14)∂θ(cid:3) VJPf (cid:104) = JVPf (cid:105) (θ,x2) (v) VJPf (θ,x1) , v = (7) (8) (9) where we have observed that the NTK-vector product can be expressed as a composition of a JVP and a VJP. The cost of computing Θf θ is then asymptotically the cost of the Ja- cobian, since it consists of O VJPs followed by O (cheaper) JVPs, therefore O [FP] time and O (cid:2)Yk + Pl(cid:3)+Y+P mem- ory. In the batched setting Eq. (7) is repeated for each pair of inputs, and therefore time increases by a factor of N2 to become N2O [FP]. However, the memory cost grows only linearly in N (except for the cost of storing the NTK of size N2O2), since intermediate primitive outputs and tan- gents/cotangents can be computed for each batch x1 and x2 separately and then reused for every pairwise combination. Therefore the memory cost is asymptotically the cost to store the NTK and compute the Jacobian. Altogether, NTK-vector products cost N2O [FP] time; N2O2 + NO (cid:2)Yk + Pl(cid:3) + NY + P memory. In summary, NTK-vector products eliminate the costly N2O2P contraction of Jacobian contraction, but perform N2O forward passes (as opposed to NO), and the memory requirement is identical. As a result, this method is bene- ﬁcial for small N, and for networks with a cheap forward pass FP relative to OP, which is always the case for FCNs (§4.1), but not necessarily for CNNs (§4.2). 3.4. Structured derivatives Rewriting Θf the primitive outputs yk, we ﬁnd: θ from Eq. (2) using the chain rule in terms of (cid:32) (cid:32) ∂f1 ∂yk1 1 ∂f1 ∂yk1 1 (cid:33) (cid:32) ∂yk1 1 ∂θl ∂f2 ∂yk2 2 ∂yk2 2 ∂θl (cid:33)T T (cid:33) T ∂yk1 1 ∂θl ∂yk2 2 ∂θl ∂f2 ∂yk2 2 Θf θ [l, k1, k2] , Θf θ = = =: (cid:88) l,k1,k2 (cid:88) l,k1,k2 (cid:88) l,k1,k2 (10) (11) (12) i /∂θl i . We have also where we deﬁne fi := f (θ, xi), and only consider ∂yki to be non-zero if θl is a direct input to yki deﬁned Θf θ [l, k1, k2] to be individual summands. Both Jacobian contraction and NTK-vector products per- form this sum of contractions via VJPs and JVPs, without explicit instantiation of primitive Jacobians ∂yki i /∂θl. How- ever, while VJPs and JVPs themselves are guaranteed to be computationally optimal (§D), higher order computa- tions like their composition (NTK-vector products) or con- traction (Jacobian contraction) are not. Speciﬁcally, each Θf θ [l, k1, k2] from Eq. (10) is a matrix-Jacobian-Jacobian- matrix product (MJJMP), which, as we will show shortly, can’t always be evaluated optimally with VJPs and JVPs. The idea of Structured derivatives is to design rules for efﬁcient computation of MJJMPs, similarly to AD rules for JVPs and VJPs. From Eq. (10), in the general case this requires hand-made rules for all pairwise combinations of primitives y1 and y2, of which there are 1362 > 10, 000 in JAX, and even more in Tensorﬂow (Abadi et al., 2016) and PyTorch (Paszke et al., 2019) (see §M). We dramatically reduce this number by: (θ,·) JVPf θ 1. Linearization. It follows from Eq. (4), that Θf θ = , i.e. the NTK of f evaluated at parameters θ is Θ equal to the NTK of the JVP of f given primal θ. JVP is a linear function of input tangents θ, and therefore we only need to implement efﬁcient MJJMPs for linear primitives, of which JAX has only 56.5 2. MJJMPs through structured derivatives. We further reduce the necessary MJJMP rule count from 562 down to only 56 by decomposing an MJJMP rule into two parts: ∈ RW, θl RW×W, and y (cid:0)θl(cid:1) = θlx 1. Structured derivative rule. Given a single primitive y, this rule identiﬁes the smallest subarray of ∂y/∂θl sufﬁcient to reconstruct the entire primitive Jacobian ∂y/∂θl, and the (constant and negligible in memory size) metadata necessary for the reconstruction. For example, if x ∈ RW (matrix-vector multiplication), then ∂y/∂θl = , and the rule will indicate that (1) IW only the subarray (cid:2)∂y/∂θl(cid:3) R1×W,6 needs to be computed (which is equal to xT in this case), and (2) that the entire primitive Jacobian can be reconstructed as ∂y/∂θl = I 1,:W. In other words, this rule annotates linear primitives y with the structure of their Jacobians, such as block diagonal, constant-block diagonal, or tiling along certain dimensions. ∈ RW×W2 (cid:2)∂y/∂θl(cid:3) 1,:W ∈ xT ⊗ ⊗ ∈ 5A linear function can contain nonlinear primitives. However, linearizing any function in JAX is guaranteed to produce only linear primitives (Frostig et al., 2021; Radul et al., 2022). 6We deﬁne [A]i,:j := [Ai,1, . . . , Ai,j] ∈ R 1×j. Fast Finite Width Neural Tangent Kernel 2. MJJMPs with structured Jacobians. Given input tensors A, B, C, D, where B and C are provided in the structured form as described above (i.e. only small subarrays along with their metadata) this rule ef- ﬁciently computes the 4-way contraction ABCD (i.e. the NTK summand Θf θ [l, k1, k2]). This amounts to using np.einsum with the optimal contraction order and adjusting its instructions based on provided meta- data. For example, if B = IW and C = IW bT RW), then ⊗ (for b, c RW×W2 RW×W2 ∈ ∈ ⊗ cT ABCD = A (cid:0)I = A (cid:0)I ⊗ ⊗ ∈ bT (cid:1) (cid:0)I cT (cid:1)T D = bT c(cid:1) D = (cid:0)bT c(cid:1) AD, ⊗ (13) (14) where were able to pull out bT c since it is a scalar. As we will see in §4 and §E, this and other similar contraction rules can enable signiﬁcant speedups. Therefore we avoid implementing 562 MJJMP rules by in- stead having (1) a single routine to perform 4-way tensor contractions with structured tensors, and (2) 56 rules anno- tating the structure in the 56 linear primitive Jacobians. We list all these structures and associated MJJMP costs in §E. Our approach does not guarantee optimality for the NTK of an arbitrary function, however, as we show in §4, it is asymptotically better than Jacobian contraction for FCNs and CNNs, and can provide orders of magnitude speedups in much more complex contemporary ImageNet models (§5). 3. Focusing on MJJMPs for typical operations. Many of the 56 linear JAX primitives are trivial to implement or rarely arise in NNs. At the time of writing we have only annotated 21 linear primitives (Table 4), which was sufﬁcient for the empirical speedups observed in §4 and §5. Summary. Structured derivatives amount to evaluating the sum of MJJMPs in Eq. (10), where (1) only small subarrays of primitive Jacobians ∂yki i /∂θl are instantiated, and (2) MJJMPs leverage the structure of these primitive Jacobians for efﬁcient contractions. Together, this incurs 1. The cost of computing primitive output cotangents ∂fi/∂yki for Eq. (10), which is equivalent to the cost i of the reverse mode Jacobian (§3.1.3), less the cost of computing (NOP) and storing (NOPl) weight-space cotangents ∂fi/∂θl, since they aren’t used in Eq. (10), i.e. NO [FP] time7 and NOYk + NY + P memory. 2. The cost of computing primitive Jacobian ∂yki subarrays, denoted as Jki (cid:80) l with J := (cid:80) . This amounts to NJ time and NJk Jk2 l l,k2 i /∂θl Jk1 l + l,k1 l memory. 3. The cost of evaluating Θf θ [l, k1, k2] via the efﬁcient MJJMP, which we denote as simply MJJMP and sub- stitute speciﬁc values based on the primitive. Required memory to store the result is N2O2. We add up these costs below and in Table 1, and show in §4 and §5 how they are beneﬁcial in most practical settings. Structured derivatives cost NO [FP] + MJJMP + l + NY + P N [J memory. OP] time; N2O2 + NOYk + NJk − 4. Examples The three algorithms in §3 (and our implementations) apply to any differentiable function f , but the resulting complexi- ties depend on variables such as FP and P, which depend on f (Table 1). Below we compute and compare all three complexities for a deep FCN (§4.1) and CNN (§4.2), sum- marizing the results in Table 2 and Table 3 respectively. 4.1. FCN NTK Complexity We apply our algorithms from §3 to FCNs with L hidden layers of width W. For simplicity we assume no biases, and RW, i.e. inputs of same size as the width. We deﬁne x yl := θlxl, and xl := φ (cid:0)yl−1(cid:1) for l > 0, with x0 := x. Output is f (x, θ) := yL. See Fig. 5 (top) for L = 2. ∈ In this case K = 2L+1 (L+1 matrix-vector multiplications and L nonlinearities), Pl = W2 for l < L and OW for the top layer l = L, P = LW2 + OW, Yk = W for k < K and O for k = K, and Y LW + O. Finally, a single forward pass FP LW2 + OW time and W2 + OW memory. ∼ ∼ Plugging the above into the cost of the baseline algorithm Jacobian contraction in §3.2, we obtain FCN Jacobian contraction costs N2O2LW2 + N2O3W time; N2O2 + NOW2 + NO2W + NLW + LW2 memory. Similarly, the cost of NTK-vector products from §3.3 is FCN NTK-vector products cost N2OLW2 + N2O2W time; N2O2 + NOW2 + NO2W + NLW + LW2 memory. 7Note that NOP time saving is not reﬂected in the asymptotic cost since it is dominated by NO [FP] required to compute primi- tive output cotangents. However, as we will see in Fig. 1, it often provides a substantial practical beneﬁt, up to allowing to compute the NTK faster than computing the two Jacobians themselves. For Structured derivatives (§3.4), we additionally need to derive values of J and MJJMP. For an arbitrary primitive, J and MJJMP costs are derived by (1) looking up the type of structure in the primitive Jacobian in Table 4, followed by (2) extracting the costs for a given structure from Table 5 Fast Finite Width Neural Tangent Kernel Method Jacobian contraction NTK-vector products N2O [FP] Structured derivatives N O [FP] + MJJMP + NJ N2O2 + NOYk + NJk Memory N2O2 + NO (cid:2)Yk + Pl(cid:3) + NY + P N2O2 + NO (cid:2)Yk + Pl(cid:3) + NY + P l + NY + P Time N O [FP] + N2O2P Use when P < Y, small O FP < OP, large O, small N FP > OP, large O, large N Table 1. Generic NTK computation costs. NTK-vector products trade-off contractions for more FP. Structured derivatives usually save both time and memory. See §3.1.1 and §3.4 for notation, and §B for a glossary of symbols. Time Method Jacobian contraction NTK-vector products N2OLW2 + N2O2 W Structured derivatives N OLW2 + N2O2LW + N2O3 N2O2LW2 + N2O3W N2O2 + NOW2 + NO2W + NLW + LW2 N2O2 + NOW2 + NO2W + NLW + LW2 N2O2 + NOW Use when Don’t O > W or N = 1 + NLW + LW2 O < W or L = 1 Memory Table 2. FCN NTK computation cost. The costs are obtained by substituting into Table 1 speciﬁc values for FP, P, Y, J, and MJJMP that correspond to an FCN as described in §4.1. NTK-vector products allow a reduction of the time complexity, while Structured derivatives reduce both time and memory complexity. See Fig. 1 for empirical conﬁrmation with FLOPs and wall-clock time. See §4.1 for discussion and notation (§B for the full glossary of symbols). Method Time Jacobian contraction N O (cid:2)LDFW2 + OW(cid:3) + N2O2 (cid:2)LFW2 + OW(cid:3) NTK-vector products N2O (cid:2)LDFW2 + OW(cid:3) Structured derivatives N O (cid:2)LDFW2 + OW(cid:3) + N2O2 (cid:104) L min (cid:16) FW2, DW + DFW2 O , DW + D2W O + D2FW O2 Memory Use when N2O2 + NO (cid:2)DW + FW2 + OW(cid:3) + N [LDW] + (cid:2)LFW2 + OW2(cid:3) D > OW N2O2 + NO (cid:2)DW + FW2 + OW(cid:3) + N [LDW] + (cid:2)LFW2 + OW2(cid:3) N = 1 (cid:17) (cid:105) + O N2O2 + NO [DW] + NDFW + N [LDW] + (cid:2)LFW2 + OW2(cid:3) D < OW Table 3. CNN NTK computation cost for a CNN with D pixels and ﬁlter size F. Structured derivatives reduce time complexity, and have lower memory cost if D < OW, which is a common setting. See Fig. 2 for experiments with ResNets, §4.2 for discussion, Table 2 for FCN, Table 1 for generic cost analysis, and §B for a glossary of symbols. (see §E). We apply this formal approach in §E.9, but for demonstration purposes below present the derivation that does not require referencing the Appendix. i = θlxl We ﬁrst note that we only need to consider k1 = k2 = 2l +1 indices in Eq. (10), since all other summands are zero due to absence of weight sharing between layers. For matrix- vector multiplication yl i our rules indicate (per ex- (cid:2)∂yl ample given in §3.4) that ∂yl , and command to only compute (cid:2)∂yl RW×W2 R1×W (which is xl i and J = 2 (cid:80)L ∼ Finally, the efﬁcient MJJMP for this structured ∂yl i/∂θl can be computed, analogously to Eq. (13), as follows for l < L: i/∂θl(cid:3) i/∂θl(cid:3) T in this case). Therefore J2l+1 1,:W ∈ 1,:W ∈ = W, i/∂θl = IW l=1 J2l+1 LW. ⊗ l l Θf (cid:124) θ [l, 2l + 1, 2l + 1] (cid:123)(cid:122) (cid:125) O×O = ∂f1 ∂yl 1 ∂yl 1 ∂θl ∂yl 2 ∂θl T T ∂f2 ∂yl 2 = (15)  IW    IW  T  T xl 2 (cid:124)(cid:123)(cid:122)(cid:125) 1×W ⊗ T xl 1 (cid:124)(cid:123)(cid:122)(cid:125) 1×W ⊗ = ∂f1 ∂yl 1 (cid:124)(cid:123)(cid:122)(cid:125) O×W = (16) T ∂f2 ∂yl 2 (cid:124) (cid:123)(cid:122) (cid:125) W×O = T  xl 1 (cid:124)(cid:123)(cid:122)(cid:125) 1×W xl 2 (cid:124)(cid:123)(cid:122)(cid:125) W×1   ∂f1 ∂yl 1 (cid:124)(cid:123)(cid:122)(cid:125) O×W , T ∂f2 ∂yl 2 (cid:124) (cid:123)(cid:122) (cid:125) W×O (17) which can be contracted in only O2W time. An analogous derivation applied to l = L yields O3 + W time. Therefore N2LO2W + N2O3, the total contraction cost is MJJMP when accounting for depth L and batch size N. Altogether, ∼ FCN Structured derivatives cost NOLW2 + N2O2LW + N2O3 time; N2O2 + NOW + NLW + LW2 memory. Summary. We summarize all FCN costs in Table 2. We conclude that Structured derivatives and NTK-vector prod- ucts allow a reduction in the time cost of NTK computation in different ways, while Structured derivatives also reduce memory requirements. Structured derivatives are beneﬁcial for wide networks, with large W, and NTK-vector prod- ucts are beneﬁcial for networks with large outputs O. We conﬁrm our predictions with FLOPs measurements in Fig. 1. We further conﬁrm our methods provide orders of magnitude speed-ups and memory savings on all major hardware platforms in Fig. 1 (right) and Fig. 3. However, we notice that time measurements often deviate from pre- dictions due to unaccounted constant overheads of various methods, hardware speciﬁcs, padding, and the behavior of the XLA compiler. We ﬁnd Structured derivatives to almost always outperform NTK-vector products. Fast Finite Width Neural Tangent Kernel FLOPs (per NTK entry) Wall-clock time (TPUv3) Figure 1. FLOPs (left) and wall-clock time (right) of computing the NTK for a 10-layer ReLU FCN. As predicted by Table 2, our methods almost always outperform Jacobian contraction, allowing orders of magnitude speed-ups and memory improvements for realistic problem sizes. Left: FLOPs per NTK entry. We conﬁrm several speciﬁc theoretical predictions from §4.1: Right: Wall-clock runtime. XLA compiler and hardware speciﬁcs, we observe that: In real applications, given the 1. NTK-vector products are the best performing method for N = 1, and have cost equivalent to Jacobian for any width W or output size O (top row); 2. NTK-vector products offer an O-fold improvement over Jacobian contraction (left to right columns); 3. NTK-vector products are equivalent to Jacobian contrac- tion for O = 1 (leftmost column); 4. Structured derivatives outperform NTK-vector products iﬀ O < W (O = W are plotted as pale vertical lines, which is where Structured derivatives and NTK-vector products in- tersect); 5. Structured derivatives approach the cost of Jacobian in the limit of large width W (left to right); 6. All methods, as expected, scale quadratically with width W (pale grey dashed line depicts W2 scaling). More: see Fig. 3 for other hardware platforms, and §N for details. 1. NTK-vector products improve upon Jacobian contrac- tion for O > 1, but the effect is not perfectly robust (see bottom row for small W and Fig. 3, notably GPU plat- forms); 2. Structured derivatives robustly outperform all other meth- ods, including simply computing the Jacobian, as discussed in §3.4; 3. Structured derivatives have lower memory footprint, and reach up to 8x larger widths (bottom right; missing points indicate out-of-memory), i.e. can process models up to 64x larger than other methods, as discussed in §3.4; 4. All methods have a smaller memory footprint than Jaco- bian (see §3.1.3). O = 1 logits O = 16 logits O = 128 logits N = 1 b a t c h N = 1 6 b a t c h N = 1 2 8 b a t c h s P O L F 107 104 101 s P O L F 107 104 101 s P O L F 107 104 101 20 23 26 29 212 Width W 20 23 26 29 212 Width W 20 23 26 29 212 Width W 101 s d n o c e S 10 1 10 3 101 s d n o c e S 10 1 10 3 101 s d n o c e S 10 1 10 3 O = 1 logits O = 16 logits O = 128 logits N = 1 b a t c h N = 1 6 b a t c h N = 1 2 8 b a t c h 20 23 26 29 212 Width W 20 23 26 29 212 Width W 20 23 26 29 212 Width W Jacobian contraction NTK-vector products Structured derivatives Jacobian W = O W 2 O = 1 logits O = 16 logits O = 128 logits 101 s d n o c e S 10 1 10 3 101 s d n o c e S 10 1 10 3 101 s d n o c e S 10 1 10 3 N = 1 b a t c h s i z e N = 1 6 b a t c h s i z e N = 1 2 8 b a t c h s i z e 20 23 26 29 212 20 23 26 29 212 20 23 26 29 212 Width W Width W Width W Fast Finite Width Neural Tangent Kernel Figure 2. Wall-clock time of computing an NTK for several ResNet sizes on a pair of ImageNet images. Structured derivatives allow the NTK to be computed faster and for larger models (see bottom row – missing points indicate out-of-memory). NTK-vector products, as predicted in §3.3 and Table 1, are advantageous for large O (bottom row), but are suboptimal when the cost of the forward pass FP is large relative to the output size and the number of parameters OP, e.g. when there is a lot of weight sharing (see Table 3 and Table 1), which is the case for convolutions, notably for O = 1 (top). See Fig. 4 for more ImageNet models, §4.2 for CNN NTK complexity analysis, and §N for experimental details. 4.2. CNN NTK Complexity We perform analogous derivations for a CNN with W chan- nels, D pixels, and ﬁlter size F in §F. We arrive at Table 3, and make two observations speciﬁc to CNNs. First, the speeds of NTK-vector products and Jacobian con- traction are now much more similar, due to the higher cost of the forward pass FP relative to P (i.e. weight sharing), and how they perform will depend on the speciﬁc values of parameters. We conﬁrm this in our experiments on Ima- geNet models in §5, where NTK-vector products typically underperform for O = 1, but outperform for O = 1000. Secondly, Structured derivatives continue to perform faster than Jacobian contraction, but the relative memory costs depend on other hyperparameters, requiring D < OW. This is a common case for ImageNet with O = 1000, and is conﬁrmed in our experiments in Fig. 2 and Fig. 4 (bottom). 5. ImageNet Experiments In §4 we have derived asymptotic time and memory beneﬁts of NTK-vector products and Structured derivatives over the baseline Jacobian contraction for FCNs and CNNs. How- ever, contemporary architectures rarely resemble vanilla feedforward networks, but instead result in much more com- plex computational graphs comprised of many different primitives, making complexity analysis impractical. We therefore evaluate our methods in the wild, and con- ﬁrm computational beneﬁts on full ImageNet models in Fig. 2 (ResNets, He et al. (2016)) and Fig. 4 (WideRes- Nets, Zagoruyko & Komodakis (2016); Vision Transformers and Transformer-ResNet hybrids Dosovitskiy et al. (2021); Steiner et al. (2021); and MLP-Mixers Tolstikhin et al. (2021)). Computing the full O 1000 NTK is often only possible with Structured derivatives. O = 1000 × × 6. Implementation All algorithms are implemented in JAX8 (Bradbury et al., 2018) and integrated into Neural Tangents (Novak et al., 2020). Jacobian contraction and NTK-vector products are built with core operations such as vjp , jvp , and vmap . Structured derivatives are implemented as a Jaxpr inter- preter, built on top of the JAX reverse mode AD interpreter. Owing to the nuanced trade-offs between different methods in the general case, we release all implementations within a single function that allows the user to manually select imple- mentation. We also include an automated setting which will perform FLOPs analysis for each method at compilation time and automatically choose the most efﬁcient one. 7. Conclusion We have performed the ﬁrst extensive analysis of the com- putational complexity of the NTK, and have shown how it can be improved dramatically with mixed-order AD (NTK- vector products), or with a custom interpreter for more efﬁ- cient higher-order AD operations (Structured derivatives). The NTK computation is similar to many other objects of interest in machine learning, such as the Gauss-Newton or the Fisher Information matrix, and we look forward to extensions of our algorithms to more settings in future work. 8See §M for discussion about other frameworks. 10 2 s d n o c e S 10 3 10 4 s d n o c e S 1.7 × 100 1.6 × 100 1.5 × 100 1.4 × 100 1.3 × 100 1.2 × 100 1.1 × 100 100 NVIDIA V100 TPUv4 CPU 10 3 10 4 10 5 100 Jacobian contraction NTK-vector products Structured derivatives Jacobian 100 10 2 103 102 O = 1 l o g i t s O = 1 0 0 0 l o g i t s 18 34 50 101 152 200 18 34 50 101 152 200 18 34 50 101 152 200 ResNet depth ResNet depth ResNet depth Fast Finite Width Neural Tangent Kernel Acknowledgements We thank Lechao Xiao for useful discussion, review and comments on the initial version of this manuscript, and Jaehoon Lee for useful discussion and code review. We also thank Shaobo Hou for his work on and help with TF2Jax, and the JAX team for their help and advice on JAX and Jax2TF. References Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al. Tensorﬂow: A system for large-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), 2016. Cited on page 2, 4, 26. Adlam, B., Lee, J., Xiao, L., Pennington, J., and Snoek, J. Exploring the uncertainty properties of neural networks’ implicit priors in the inﬁnite-width limit. In International Conference on Learning Representations, 2020. Cited on page 1, 23. Arﬁan, W. Ukraine vectors by vecteezy. https: //www.vecteezy.com/vector-art/7506324-stand- with-ukraine-text-with-ukraine-flag-ribbon- and-ukraine-map-vector-design-on-a-dark- blue-background, 2022. Cited on page 27. Arora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R. R., and Wang, R. On exact computation with an inﬁnitely wide neural net. In Advances in Neural Information Pro- cessing Systems, pp. 8141–8150. Curran Associates, Inc., 2019a. Cited on page 2. Arora, S., Du, S. S., Hu, W., Li, Z., and Wang, R. Fine- grained analysis of optimization and generalization for overparameterized two-layer neural networks. arXiv preprint arXiv:1901.08584, 2019b. Cited on page 24. Arora, S., Du, S. S., Li, Z., Salakhutdinov, R., Wang, R., and Yu, D. Harnessing the power of inﬁnitely wide deep nets on small-data tasks. In International Confer- ence on Learning Representations, 2020. URL https: //openreview.net/forum?id=rkl8sJBYvH. Cited on page 1. Babuschkin, I., Baumli, K., Bell, A., Bhupatiraju, S., Bruce, J., Buchlovsky, P., Budden, D., Cai, T., Clark, A., Dani- helka, I., Fantacci, C., Godwin, J., Jones, C., Hennigan, T., Hessel, M., Kapturowski, S., Keck, T., Kemaev, I., King, M., Martens, L., Mikulik, V., Norman, T., Quan, J., Papamakarios, G., Ring, R., Ruiz, F., Sanchez, A., Schneider, R., Sezener, E., Spencer, S., Srinivasan, S., Stokowiec, W., and Viola, F. The DeepMind JAX Ecosys- tem, 2020. URL http://github.com/deepmind. Cited on page 27. Bahri, Y., Dyer, E., Kaplan, J., Lee, J., and Sharma, arXiv preprint U. Explaining neural scaling laws. arXiv:2102.06701, 2021. Cited on page 1. Bai, J., Lu, F., Zhang, K., et al. Onnx: Open neural network exchange. https://github.com/onnx/onnx, 2019. Cited on page 27. Belkin, M., Hsu, D., Ma, S., and Mandal, S. Reconciling modern machine-learning practice and the classical bias– variance trade-off. Proceedings of the National Academy of Sciences, 116(32):15849–15854, 2019. Cited on page 1. Borovykh, A. A gaussian process perspective on convolu- tional neural networks. arXiv preprint arXiv:1810.10798, 2018. Cited on page 1. Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., and Wanderman-Milne, S. JAX: com- posable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax. Cited on page 2, 8, 26, 27. Brock, A., De, S., and Smith, S. L. Characterizing signal propagation to close the performance gap in unnormal- ized resnets. arXiv preprint arXiv:2101.08692, 2021a. Cited on page 1. Brock, A., De, S., Smith, S. L., and Simonyan, K. High- performance large-scale image recognition without nor- malization. arXiv preprint arXiv:2102.06171, 2021b. Cited on page 1. Chen, W., Gong, X., and Wang, Z. Neural architecture search on imagenet in four gpu hours: A theoretically In International Conference on inspired perspective. Learning Representations, 2021a. Cited on page 2, 26. Chen, X., Hsieh, C.-J., and Gong, B. When vision trans- formers outperform resnets without pretraining or strong data augmentations, 2021b. Cited on page 1, 2, 26. Dauphin, Y. N. and Schoenholz, S. Metainit: Initializing learning by learning to initialize. Advances in Neural Information Processing Systems, 32, 2019. Cited on page 1, 26. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Cited on page 2. Fast Finite Width Neural Tangent Kernel Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for In International Confer- image recognition at scale. ence on Learning Representations, 2021. URL https: //openreview.net/forum?id=YicbFdNTTy. Cited on page 2, 8, 14. Du, S. S., Hou, K., Salakhutdinov, R. R., Poczos, B., Wang, R., and Xu, K. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. Advances in neural information processing systems, 32, 2019. Cited on page 24. Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta- learning for fast adaptation of deep networks. In Pre- cup, D. and Teh, Y. W. (eds.), Proceedings of the 34th International Conference on Machine Learning, vol- ume 70 of Proceedings of Machine Learning Research, pp. 1126–1135. PMLR, 06–11 Aug 2017. URL https: //proceedings.mlr.press/v70/finn17a.html. Cited on page 2. Franceschi, J.-Y., de B´ezenac, E., Ayed, I., Chen, M., Lam- prier, S., and Gallinari, P. A neural tangent kernel per- spective of gans. arXiv preprint arXiv:2106.05566, 2021. Cited on page 1. Frostig, R., Johnson, M. J., Maclaurin, D., Paszke, A., and Radul, A. Decomposing reverse-mode automatic differ- entiation. arXiv preprint arXiv:2105.09469, 2021. Cited on page 4, 26. Garriga-Alonso, A., Aitchison, L., and Rasmussen, C. E. Deep convolutional networks as shallow gaussian pro- cesses. In International Conference on Learning Repre- sentations, 2019. Cited on page 1. Griewank, A. and Walther, A. Evaluating Deriva- tives. Society for Industrial and Applied Math- ematics, 10.1137/ second edition, 2008. 1.9780898717761. URL https://epubs.siam.org/ doi/abs/10.1137/1.9780898717761. Cited on page 3, 17. doi: Grosse, R. Neural net Jan- URL https://www.cs.toronto.edu/ training dynamics, uary 2021. ∼rgrosse/courses/csc2541 2021/readings/ L02 Taylor approximations.pdf. 25. Cited on page Hanin, B. and Nica, M. Finite depth and width corrections In International Confer- to the neural tangent kernel. ence on Learning Representations, 2020. URL https: //openreview.net/forum?id=SJgndT4KwB. Cited on page 24. He, B., Lakshminarayanan, B., and Teh, Y. W. Bayesian In deep ensembles via the neural tangent kernel. Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Infor- mation Processing Systems 33: Annual Conference Information Processing Systems 2020, on Neural NeurIPS 2020, December 6-12, 2020, virtual, 2020. https://proceedings.neurips.cc/paper/ URL 2020/hash/0b1ec366924b26fc98fa7b71a9c249cf- Abstract.html. Cited on page 1. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Cited on page 2, 8. Heek, J., Levskaya, A., Oliver, A., Ritter, M., Rondepierre, B., Steiner, A., and van Zee, M. Flax: A neural network library and ecosystem for JAX, 2020. URL http:// github.com/google/flax. Cited on page 24, 27. Hennigan, T., Cai, T., Norman, T., and Babuschkin, I. Haiku: Sonnet for JAX, 2020. URL http://github.com/ deepmind/dm-haiku. Cited on page 24. Horace He, R. Z. functorch: Jax-like composable function transforms for pytorch. https://github.com/pytorch/ functorch, 2021. Cited on page 27. Hron, J., Bahri, Y., Novak, R., Pennington, J., and Sohl- Dickstein, J. Exact posterior distributions of wide bayesian neural networks, 2020a. Cited on page 1. Hron, J., Bahri, Y., Sohl-Dickstein, J., and Novak, R. Inﬁnite attention: NNGP and NTK for deep attention networks. In International Conference on Machine Learning, 2020b. Cited on page 1, 24. Hu, J., Shen, J., Yang, B., and Shao, L. Inﬁnitely wide graph convolutional networks: semi-supervised learning via gaussian processes. arXiv preprint arXiv:2002.12168, 2020. Cited on page 1. Jacot, A., Gabriel, F., and Hongler, C. Neural tangent ker- nel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems, 2018. Cited on page 1, 23, 26. Khan, M. E. E., Immer, A., Abedi, E., and Korzepa, M. Approximate inference turns deep networks into gaussian processes. In Advances in neural information processing systems, 2019. Cited on page 1. Lee, J., Bahri, Y., Novak, R., Schoenholz, S., Pennington, J., and Sohl-dickstein, J. Deep neural networks as gaus- sian processes. In International Conference on Learning Representations, 2018. Cited on page 1. Fast Finite Width Neural Tangent Kernel Lee, J., Xiao, L., Schoenholz, S. S., Bahri, Y., Novak, R., Sohl-Dickstein, J., and Pennington, J. Wide neural net- works of any depth evolve as linear models under gradient descent. In Advances in Neural Information Processing Systems, 2019. Cited on page 1, 2, 23, 24, 26. Lee, J., Schoenholz, S., Pennington, J., Adlam, B., Xiao, L., Novak, R., and Sohl-Dickstein, J. Finite versus inﬁnite neural networks: an empirical study. Advances in Neural Information Processing Systems, 33:15156–15172, 2020. Cited on page 24. Maclaurin, D., Duvenaud, D., and Adams, R. P. Auto- In ICML 2015 grad: Effortless gradients in numpy. AutoML Workshop, 2015. URL https://github.com/ HIPS/autograd. Cited on page 3. Martens, J. and Grosse, R. Optimizing neural networks with kronecker-factored approximate curvature. In Interna- tional conference on machine learning, pp. 2408–2417. PMLR, 2015. Cited on page 25. Matthews, A., Hron, J., Rowland, M., Turner, R. E., and Ghahramani, Z. Gaussian process behaviour in wide deep neural networks. In International Conference on Learning Representations, 2018. Cited on page 1. M¨untz, H. Solution directe de l’´equation s´eculaire et de quelques probl`emes analogues transcendants. C. R. Acad. Sci. Paris, 156:43–46, 1913. Cited on page 26. Naumann, U. Optimal accumulation of jacobian matrices by elimination methods on the dual computational graph. Mathematical Programming, 99(3):399–421, 2004. Cited on page 2. Naumann, U. Optimal jacobian accumulation is np- complete. Mathematical Programming, 112(2):427–441, 2008. Cited on page 2. Neal, R. M. Priors for inﬁnite networks (tech. rep. no. crg- tr-94-1). University of Toronto, 1994. Cited on page 1. Nguyen, T., Chen, Z., and Lee, J. Dataset meta- learning from kernel ridge-regression. arXiv preprint arXiv:2011.00050, 2020. Cited on page 1. Nguyen, T., Novak, R., Xiao, L., and Lee, J. Dataset distil- lation with inﬁnitely wide convolutional networks. arXiv preprint arXiv:2107.13034, 2021. Cited on page 1. Novak, R., Xiao, L., Lee, J., Bahri, Y., Yang, G., Hron, J., Abolaﬁa, D. A., Pennington, J., and Sohl-Dickstein, J. Bayesian deep convolutional networks with many chan- nels are gaussian processes. In International Conference on Learning Representations, 2019. Cited on page 1, 24. Novak, R., Xiao, L., Hron, J., Lee, J., Alemi, A. A., Sohl-Dickstein, J., and Schoenholz, S. S. Neural tangents: Fast and easy inﬁnite neural networks in python. In International Conference on Learning Repre- sentations, 2020. URL https://github.com/google/ neural-tangents. Cited on page 1, 2, 8, 24, 27. Oreshkin, B. N., L´opez, P. R., and Lacoste, A. Tadam: Task dependent adaptive metric for improved few-shot learning. In NeurIPS, 2018. Cited on page 2. Park, D. S., Lee, J., Peng, D., Cao, Y., and Sohl-Dickstein, J. Towards nngp-guided neural architecture search. arXiv preprint arXiv:2011.06006, 2020. Cited on page 1, 2. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch- an-imperative-style-high-performance-deep- learning-library.pdf. Cited on page 2, 4, 26. Pennington, J. and Bahri, Y. Geometry of neural net- work loss surfaces via random matrix theory. In Precup, D. and Teh, Y. W. (eds.), Proceedings of the 34th International Conference on Machine Learn- ing, volume 70 of Proceedings of Machine Learn- ing Research, pp. 2798–2806. PMLR, 06–11 Aug 2017. URL https://proceedings.mlr.press/v70/ pennington17a.html. Cited on page 25. Radul, A., Paszke, A., Frostig, R., Johnson, M., and Maclau- rin, D. You only linearize once: Tangents transpose to gradients. arXiv preprint arXiv:2204.10923, 2022. Cited on page 4, 17. Schoenholz, S. S., Gilmer, J., Ganguli, S., and Sohl- Dickstein, J. Deep information propagation. Interna- tional Conference on Learning Representations, 2017. Cited on page 1. Spigler, S., Geiger, M., d’Ascoli, S., Sagun, L., Biroli, G., and Wyart, M. A jamming transition from under-to over- parametrization affects generalization in deep learning. Journal of Physics A: Mathematical and Theoretical, 52 (47):474001, 2019. Cited on page 1. Steiner, A., Kolesnikov, A., Zhai, X., Wightman, R., Uszkor- eit, J., and Beyer, L. How to train your vit? data, augmen- tation, and regularization in vision transformers. arXiv preprint arXiv:2106.10270, 2021. Cited on page 8, 14. Fast Finite Width Neural Tangent Kernel Tancik, M., Srinivasan, P. P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., Ramamoorthi, R., Barron, J. T., and Ng, R. Fourier features let networks learn high frequency functions in low dimensional domains. NeurIPS, 2020. Cited on page 1. Tolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., Yung, J., Steiner, A., Keysers, D., Uszkoreit, J., Lucic, M., and Dosovitskiy, A. Mlp-mixer: An all-mlp architecture for vision, 2021. Cited on page 2, 8, 14. Xiao, L., Bahri, Y., Sohl-Dickstein, J., Schoenholz, S., and Pennington, J. Dynamical isometry and a mean ﬁeld theory of CNNs: How to train 10,000-layer vanilla con- volutional neural networks. In International Conference on Machine Learning, 2018. Cited on page 1. Xiao, L., Pennington, J., and Schoenholz, S. S. Disentan- gling trainability and generalization in deep learning. In International Conference on Machine Learning, 2020. Cited on page 1, 23. Yaida, S. Non-Gaussian processes and neural networks at ﬁnite widths. In Mathematical and Scientiﬁc Machine Learning Conference, 2020. Cited on page 24. Yang, G. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient in- dependence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760, 2019. Cited on page 1, 24. Yang, G. Tensor programs ii: Neural tangent kernel for any architecture. arXiv preprint arXiv:2006.14548, 2020. Cited on page 24. Yang, G., Pennington, J., Rao, V., Sohl-Dickstein, J., and Schoenholz, S. S. A mean ﬁeld theory of batch nor- In International Conference on Learning malization. Representations, 2019. Cited on page 24. Zagoruyko, S. and Komodakis, N. Wide residual networks. In British Machine Vision Conference, 2016. Cited on page 2, 8, 14. Zhang, H., Dauphin, Y. N., and Ma, T. Fixup initialization: Residual learning without normalization. arXiv preprint arXiv:1901.09321, 2019. Cited on page 1. Zhou, Y., Wang, Z., Xian, J., Chen, C., and Xu, J. In In- Meta-learning with neural ternational Conference on Learning Representations, URL https://openreview.net/forum?id= 2021. Ti87Pv5Oc8. Cited on page 2, 26. tangent kernels. Zoph, B. and Le, Q. V. Neural architecture search with reinforcement learning, 2016. URL http://arxiv.org/ abs/1611.01578. Cited on page 2. Fast Finite Width Neural Tangent Kernel Appendix A. Additional Figures CPU (Skylake) NVIDIA V100 TPUv4 NVIDIA P100 Figure 3. Wall-clock time of computing the NTK of a 10-layer ReLU FCN on different platforms. In all settings, Structured derivatives allow orders of magnitude improvement in wall-clock time and memory (missing points indicate out-of-memory error). However, we remark that on GPU platforms (right), NTK-vector products deliver a robust improvement only for large O (rightmost column), while for O = 16 the cost is comparable or even larger than Jacobian contraction. See Fig. 1 for FLOPs, TPUv3 platform, and more discussion. See §N for details. 103 100 10 3 103 100 10 3 103 100 10 3 s d n o c e S s d n o c e S s d n o c e S O = 1 logits O = 16 logits O = 128 logits N = 1 b a t c h N = 1 6 b a t c h N = 1 2 8 b a t c h 20 23 26 29 212 Width W 20 23 26 29 212 Width W 20 23 26 29 212 Width W 102 100 10 2 102 100 10 2 102 100 10 2 s d n o c e S s d n o c e S s d n o c e S O = 1 logits O = 16 logits O = 128 logits N = 1 b a t c h N = 1 6 b a t c h N = 1 2 8 b a t c h 20 23 26 29 212 Width W 20 23 26 29 212 Width W 20 23 26 29 212 Width W O = 1 logits O = 16 logits O = 128 logits N = 1 b a t c h N = 1 6 b a t c h N = 1 2 8 b a t c h s d n o c e S 10 1 10 3 s d n o c e S 10 1 10 3 s d n o c e S 10 1 10 3 20 23 26 29 212 Width W 20 23 26 29 212 Width W 20 23 26 29 212 Width W O = 1 logits O = 16 logits O = 128 logits N = 1 b a t c h N = 1 6 b a t c h N = 1 2 8 b a t c h s d n o c e S s d n o c e S s d n o c e S 102 100 10 2 102 100 10 2 102 100 10 2 20 23 26 29 212 Width W 20 23 26 29 212 Width W 20 23 26 29 212 Width W Jacobian contraction NTK-vector products Structured derivatives Jacobian W = O W 2 O = 1 logits O = 16 logits O = 128 logits 101 s d n o c e S 10 1 10 3 101 s d n o c e S 10 1 10 3 101 s d n o c e S 10 1 10 3 N = 1 b a t c h s i z e N = 1 6 b a t c h s i z e N = 1 2 8 b a t c h s i z e 20 23 26 29 212 20 23 26 29 212 20 23 26 29 212 Width W Width W Width W Fast Finite Width Neural Tangent Kernel Figure 4. Wall-clock time per input pair of computing NTK on various ImageNet models like Vision Tansformers and hybrids (Dosovitskiy et al., 2021; Steiner et al., 2021), WideResNets (Zagoruyko & Komodakis, 2016) and MLP-Mixers (Tolstikhin et al., 2021). Structured derivatives generally allow fastest computation, but also are able to process more models due to lower memory requirements (lower left; missing points indicate out-of-memory error). For the case of single output logit O = 1 (top row), NTK-vector products are generally detrimental due to a costly forward pass FP relative to the size of parameters P (i.e. a lot of weight sharing; see Table 1). However, since NTK-vector products scale better than other methods with output size, for O = 1000 (bottom row), they perform comparably or better than other methods. Finally, we remark that the Jacobian not only runs out of memory faster, but can also take more time to compute. We conjecture that due to a larger memory footprint, XLA can sometimes perform optimizations that trade off speed for memory, and therefore compute the Jacobian in a less optimal way than if it had more memory available. Alternatively, XLA could also be performing simpliﬁcations of the NTK expression in these cases, such that those would not be possible in Jacobian computation alone. See Fig. 2 for ResNets, and §N for details. Figure 5. Notation used in §4.1 (FCN, top) and §F (CNN, bottom). In both settings L = 2. For FCN, K = 5 (3 matrix multiplication primitives and two nonlinearities φ), D = F = 1. For CNN, there is an extra global average pooling primitive as the penultimate layer, therefore K = 6, and D = 8, F = 3. NVIDIA V100 TPUv4 CPU s d n o c e S 10 2 10 4 s d n o c e 100S 10 3 10 5 100 10 1 Jacobian contraction NTK-vector products Structured derivatives Jacobian 100 10 2 104 103 102 O = 1 l o g i t s O = 1 0 0 0 l o g i t s 6 1 _ i T - T V + R i 6 1 _ i T - T V i 2 3 _ S - T V i 6 1 _ S - T V i 2 3 _ B - T V i 2 3 _ L - T V i 6 1 _ B - r e x M i 6 1 _ L - T V i 6 1 _ L - r e x M i 0 1 _ 8 2 _ n r w 2 1 _ 8 2 _ n r w 4 1 _ H - T V i i 6 1 _ B - T V + 0 5 R i 2 3 _ L - T V + 0 5 R 6 1 _ B - T V i i 2 3 _ B - T V + 6 2 R i 2 3 _ S - T V + 6 2 R Model 6 1 _ i T - T V + R i 6 1 _ i T - T V i 2 3 _ S - T V i 6 1 _ S - T V i 2 3 _ B - T V i 2 3 _ L - T V i 6 1 _ B - r e x M i 6 1 _ L - T V i 6 1 _ L - r e x M i 0 1 _ 8 2 _ n r w 2 1 _ 8 2 _ n r w 4 1 _ H - T V i i 6 1 _ B - T V + 0 5 R i 2 3 _ L - T V + 0 5 R 6 1 _ B - T V i i 2 3 _ B - T V + 6 2 R i 2 3 _ S - T V + 6 2 R Model 6 1 _ i T - T V + R i 6 1 _ i T - T V i 2 3 _ S - T V i 6 1 _ S - T V i 2 3 _ B - T V i 2 3 _ L - T V i 6 1 _ B - r e x M i 6 1 _ L - T V i 6 1 _ L - r e x M i 0 1 _ 8 2 _ n r w 2 1 _ 8 2 _ n r w 4 1 _ H - T V i i 6 1 _ B - T V + 0 5 R i 2 3 _ L - T V + 0 5 R 6 1 _ B - T V i i 2 3 _ B - T V + 6 2 R i 2 3 _ S - T V + 6 2 R Model 4 = < l a t e x i t s h a 1 _ b a s e 6 4 = "" D R a Y i w Z Y S Z q S 2 p V k f L w Q v A u Q a + Q = "" > A A A C 9 X i c d Z L L j t M w F I b d c J k h 3 G Z g y c a i Q m J V J S M E b E Y z A h Y s K h g k O h 2 p i U a 2 4 7 T W + C b b o V R W X o E t b F i B 2 P I Y P A J C v A x O 0 w V p 4 E i W f p 3 z H Z + L j T V n 1 i X J r 0 F 0 6 f K V q z u 7 1 + L r N 2 7 e u r 2 3 f + f U q s o Q O i G K K 3 O G k a W c S T p x z H F 6 p g 1 F A n M 6 x R f P m / j 0 H T W W K f n W r T T N B Z p L V j K C X O P K 5 O G j 8 7 1 h M k r W B v s i 3 Y j h 0 Y / 4 U H / 5 G Z + c 7 w 9 + Z 4 U i l a D S E Y 6 s n a W J d r l H x j H C a R 1 n l a U a k Q s 0 p 7 M g J R L U 5 n 7 d b A 0 f B E 8 B S 2 X C k Q 6 u v X 9 n e C S s X Q k c S I H c w m 7 H G u c / Y x i L r d K u f J p 7 J n X l q C R t 5 b L i 0 C n Y r A I W z F D i + C o I R A w L z U O y Q A Y R F x b 2 n y H e t 1 P E c S b p k i g h k C x 8 J u t Z k n u f r Y N e V 0 a H N f g M l / B V X d d d t i g 6 s K O I t + i L H q p U B 8 W 8 2 t z 6 u o e O x x 1 U G S T n G 3 j c g 5 c d l q y Q b M l p Q 4 b v k G 4 / f l + c H o z S x 6 O D N 8 n w + B l o b R f c A / f B Q 5 C C J + A Y v A Q n Y A I I W I A P 4 C P 4 F C 2 j z 9 H X 6 F u L R o N N z l 3 Q s e j 7 H 8 h v + w I = < / l a t e x i t > N 4 = < l a t e x i t s h a 1 _ b a s e 6 4 = "" D R a Y i w Z Y S Z q S 2 p V k f L w Q v A u Q a + Q = "" > A A A C 9 X i c d Z L L j t M w F I b d c J k h 3 G Z g y c a i Q m J V J S M E b E Y z A h Y s K h g k O h 2 p i U a 2 4 7 T W + C b b o V R W X o E t b F i B 2 P I Y P A J C v A x O 0 w V p 4 E i W f p 3 z H Z + L j T V n 1 i X J r 0 F 0 6 f K V q z u 7 1 + L r N 2 7 e u r 2 3 f + f U q s o Q O i G K K 3 O G k a W c S T p x z H F 6 p g 1 F A n M 6 x R f P m / j 0 H T W W K f n W r T T N B Z p L V j K C X O P K 5 O G j 8 7 1 h M k r W B v s i 3 Y j h 0 Y / 4 U H / 5 G Z + c 7 w 9 + Z 4 U i l a D S E Y 6 s n a W J d r l H x j H C a R 1 n l a U a k Q s 0 p 7 M g J R L U 5 n 7 d b A 0 f B E 8 B S 2 X C k Q 6 u v X 9 n e C S s X Q k c S I H c w m 7 H G u c / Y x i L r d K u f J p 7 J n X l q C R t 5 b L i 0 C n Y r A I W z F D i + C o I R A w L z U O y Q A Y R F x b 2 n y H e t 1 P E c S b p k i g h k C x 8 J u t Z k n u f r Y N e V 0 a H N f g M l / B V X d d d t i g 6 s K O I t + i L H q p U B 8 W 8 2 t z 6 u o e O x x 1 U G S T n G 3 j c g 5 c d l q y Q b M l p Q 4 b v k G 4 / f l + c H o z S x 6 O D N 8 n w + B l o b R f c A / f B Q 5 C C J + A Y v A Q n Y A I I W I A P 4 C P 4 F C 2 j z 9 H X 6 F u L R o N N z l 3 Q s e j 7 H 8 h v + w I = < / l a t e x i t > N 4 = < l a t e x i t s h a 1 _ b a s e 6 4 = "" D R a Y i w Z Y S Z q S 2 p V k f L w Q v A u Q a + Q = "" > A A A C 9 X i c d Z L L j t M w F I b d c J k h 3 G Z g y c a i Q m J V J S M E b E Y z A h Y s K h g k O h 2 p i U a 2 4 7 T W + C b b o V R W X o E t b F i B 2 P I Y P A J C v A x O 0 w V p 4 E i W f p 3 z H Z + L j T V n 1 i X J r 0 F 0 6 f K V q z u 7 1 + L r N 2 7 e u r 2 3 f + f U q s o Q O i G K K 3 O G k a W c S T p x z H F 6 p g 1 F A n M 6 x R f P m / j 0 H T W W K f n W r T T N B Z p L V j K C X O P K 5 O G j 8 7 1 h M k r W B v s i 3 Y j h 0 Y / 4 U H / 5 G Z + c 7 w 9 + Z 4 U i l a D S E Y 6 s n a W J d r l H x j H C a R 1 n l a U a k Q s 0 p 7 M g J R L U 5 n 7 d b A 0 f B E 8 B S 2 X C k Q 6 u v X 9 n e C S s X Q k c S I H c w m 7 H G u c / Y x i L r d K u f J p 7 J n X l q C R t 5 b L i 0 C n Y r A I W z F D i + C o I R A w L z U O y Q A Y R F x b 2 n y H e t 1 P E c S b p k i g h k C x 8 J u t Z k n u f r Y N e V 0 a H N f g M l / B V X d d d t i g 6 s K O I t + i L H q p U B 8 W 8 2 t z 6 u o e O x x 1 U G S T n G 3 j c g 5 c d l q y Q b M l p Q 4 b v k G 4 / f l + c H o z S x 6 O D N 8 n w + B l o b R f c A / f B Q 5 C C J + A Y v A Q n Y A I I W I A P 4 C P 4 F C 2 j z 9 H X 6 F u L R o N N z l 3 Q s e j 7 H 8 h v + w I = < / l a t e x i t > N <latexit sha1_base64=""N7dWDDp5SaZjiomhSQBw4NYjymw="">AAACs3icdZHLTttAFIYnbrk0XArtslVlgZBYRXYkCkvUbrrIAqQGkGILnRmfJCPmppkxEFledttt2zfhVfoMfYlOEha1KUca6dd/vtG5USO480nyuxO9eLmyurb+qruxubX9emf3zYXTpWU4ZFpoe0XBoeAKh557gVfGIkgq8JLefJ7nL2/ROq7VVz8zmEuYKD7mDPzcysyUX+/sJ71kEfFTkT6K/dP3D+d/vn14OLve7fzKCs1KicozAc6N0sT4vALrORNYd7PSoQF2AxMcBalAosurRbN1fBCcIh5rG57y8cL990cF0rmZpIGU4KeunZub/81RKlul/fgkr7gypUfFlpXHpYi9jueriAtukXkxCwKY5aH5mE3BAvNhYc8Mcb+cotvNFN4xLSWoospUPUryqsoWycqU1oQ1qLpuUkXRwDyCqIs2pHUDoqLEWrehwaABaQtqgvUgYOGYaft0T8VFv5d+7PXPw1U/kWWsk3dkjxySlByTU/KFnJEhYWRKvpMf5Gd0FI0iGhVLNOo8/nlLGhHJv52w4M8=</latexit>","['fast', 'finite', 'novak', 'x', 'r', 'abstract', 'deﬁne', 'x2', 'θ', 'x1cid14∂θcid3', 'x2cid14∂θcid3', 'cid14∂θcid3', 'neural', 'network', 'emerge', 'central', 'object', 'study', 'deep', 'learning', 'inﬁnite', 'width', 'limit', 'ntk', 'sometimes', 'compute', 'analytically', 'useful', 'understand', 'training', 'gener', 'alization', 'architecture', 'ﬁnite', 'width', 'ntk', 'also', 'use', 'well', 'initialize', 'nn', 'compare', 'conditioning', 'model', 'perform', 'architec', 'ture', 'search', 'metalearne', 'unfortunately', 'ﬁnite', 'width', 'notoriously', 'expensive', 'compute', 'severely', 'limit', 'practical', 'utility', 'perform', 'ﬁrst', 'indepth', 'analysis', 'compute', 'memory', 'requirement', 'computation', 'ﬁnite', 'width', 'network', 'leverag', 'e', 'structure', 'neural', 'network', 'far', 'propose', 'novel', 'algorithm', 'change', 'ex', 'ponent', 'compute', 'memory', 'requirement', 'ﬁnite', 'width', 'dramatically', 'improve', 'efﬁciency', 'algorithm', 'apply', 'black', 'box', 'fashion', 'differentiable', 'function', 'include', 'implement', 'neural', 'network', 'opensource', 'implementation', 'neural', 'tangent', 'package', 'novak', 'githubcomgoogleneuraltangent', 'introduction', 'past', 'year', 'see', 'signiﬁcant', 'progress', 'theoretical', 'foundation', 'deep', 'learning', 'much', 'work', 'focus', 'understand', 'property', 'run', 'dom', 'function', 'high', 'dimension', 'signiﬁcant', 'line', 'matthew', 'garrigaalonso', 'novak', 'brain', 'mountain', 'view', 'correspondence', 'proceeding', 'international', 'conference', 'machine', 'learn', 'copy', 'right', 'author', 'establish', 'limit', 'inﬁnite', 'tialize', 'neural', 'network', 'nns', 'gaussian', 'process', 'call', 'nngp', 'build', 'development', 'show', 'function', 'space', 'dynamic', 'gradient', 'descent', 'compute', 'analytically', 'use', 'socalle', 'kernel', 'show', 'wide', 'neural', 'network', 'reduce', 'lineariza', 'tion', 'weight', 'space', 'train', 'relate', 'set', 'result', 'belkin', 'spigler', 'show', 'ubiquitous', 'biasvariance', 'decomposition', 'break', 'highdimensional', 'model', 'enter', 'socalled', 'interpolat', 'e', 'regime', 'together', 'result', 'describe', 'learn', 'inﬁnite', 'width', 'limit', 'help', 'explain', 'impressive', 'general', 'ization', 'capability', 'nn', 'insight', 'wide', 'network', 'limit', 'signiﬁcant', 'practical', 'impact', 'conditioning', 'ntk', 'show', 'signiﬁcantly', 'impact', 'trainability', 'generaliza', 'tion', 'schoenholz', 'notion', 'inspire', 'initialization', 'scheme', 'metainit', 'schoen', 'normalizer', 'free', 'network', 'brock', 'enable', 'efﬁcient', 'neural', 'architecture', 'search', 'park', 'additionally', 'give', 'insight', 'wide', 'range', 'nomena', 'behavior', 'generative', 'adversarial', 'net', 'work', 'neural', 'scale', 'law', 'neural', 'irradiance', 'ﬁeld', 'tancik', 'kernel', 'regression', 'use', 'ntk', 'far', 'enable', 'strong', 'performance', 'small', 'dataset', 'application', 'approximate', 'inference', 'distillation', 'nguyen', 'uncertainty', 'prediction', 'adlam', 'signiﬁcant', 'promise', 'theory', 'base', 'ntk', 'compute', 'ntk', 'practice', 'challenge', 'inﬁnite', 'width', 'limit', 'ntk', 'sometimes', 'compute', 'analyti', 'cally', 'however', 'inﬁnitewidth', 'kernel', 'remain', 'intractable', 'many', 'architecture', 'ﬁnite', 'width', 'correction', 'often', 'important', 'describe', 'actual', 'nn', 'use', 'practice', 'see', 'detailed', 'discussion', 'ntk', 'matrix', 'compute', 'ﬁnite', 'width', 'network', 'outer', 'product', 'jacobian', 'use', 'forward', 'reverse', 'mode', 'automatic', 'differentiation', 'ad', 'x1cid14∂θcid3', 'θ', 'x2cid14∂θcid3', 'cid125', 'cid123cid122', 'p×o', 'fast', 'finite', 'kernel', 'forward', 'pass', 'function', 'produce', 'output', 'rp', 'trainable', 'parameter', 'x1', 'x2', 'input', 'network', 'input', 'batch', 'size', 'n1', 'ntk', 'n2o', 'matrix', '×', 'unfortunately', 'evaluate', 'eq', 'often', 'infeasible', 'time', 'memory', 'requirement', 'modern', 'machine', 'learn', 'ing', 'task', 'often', 'great', 'sometimes', 'much', 'great', 'eg', 'even', 'modestly', 'sized', 'model', 'feature', 'ten', 'million', 'parame', 'make', 'store', 'n1', 'n2', 'op', 'ter', 'p', 'memory', 'contracting', 'o2p', 'time', 'jacobian', 'eq', 'costly', 'theoretical', 'importance', 'ntk', 'together', 'prohibitive', 'computational', 'cost', 'ply', 'performance', 'improvement', 'unlock', 'impactful', 'novel', 'research', '∼', 'perform', 'ﬁrst', 'indepth', 'analysis', 'compute', 'memory', 'requirement', 'ntk', 'eq', 'note', 'forward', 'reverse', 'mode', 'ad', 'extreme', 'wide', 'range', 'ad', 'strategy', 'naumann', 'explore', 'method', 'compute', 'ntk', 'leverage', 'structure', 'nn', 'use', 'practice', 'propose', 'novel', 'method', 'compute', 'ntk', 'exploit', 'different', 'ordering', 'computation', 'describe', 'compute', 'memory', 'requirement', 'technique', 'fullyconnected', 'fcn', 'convolutional', 'setting', 'show', 'one', 'asymptotically', 'efﬁcient', 'setting', 'compute', 'ntk', 'wide', 'range', 'architecture', 'demonstrate', 'improvement', 'robust', 'practice', 'opensource', 'implementation', 'generalpurpose', 'bradbury', 'function', 'transformation', 'relate', 'work', 'ﬁnite', 'width', 'ntk', 'denote', 'simply', 'ntk', 'work2', 'use', 'extensively', 'many', 'recent', 'work', 'knowledge', 'implementation', 'detail', 'compute', 'cost', 'rarely', 'make', 'public', 'draw', 'comparison', 'work', 'stress', 'serve', 'sanity', 'check', 'make', 'sure', 'contribution', 'valuable', 'relative', 'scale', 'problem', 'attempt', 'none', 'work', 'efﬁcient', 'ntk', 'computation', 'central', 'goal', 'order', 'compare', 'performance', 'model', 'base', 'ntk', 'inﬁnite', 'width', 'ntk', 'table', 'compute', 'ntk', 'binary', 'cifar2', 'classiﬁcation', 'set', 'equivalent', 'algorithm', 'frameworkagnostic', 'implementation', 'easy', 'describe', '§', 'also', 'provide', 'instruction', 'implementation', 'framework', 'abadi', 'pytorch', 'paszke', 'comparison', 'ﬁnite', 'inﬁnite', 'width', 'setting', 'set', 'hardware', 'able', 'compute', 'ntk', 'network', 'least', 'time', 'parameter', 'demonstrate', 'stability', 'ntk', 'training', 'wide', 'network', 'lee', 'figure', 'compute', 'ntk', 'fcns', 'setting', 'hardware', 'reach', 'width', 'least', 'respectively', 'handle', 'network', 'time', 'parameter', 'investigate', 'convergence', 'wideresnet', 'zagoruyko', 'komodakis', 'inﬁnite', 'width', 'limit', 'novak', 'et', 'figure', 'evaluate', 'ntk', 'model', 'widen', 'factor', 'matching', 'setting', 'hardware', 'able', 'reach', 'widen', 'factor', 'least', 'work', 'model', 'least', 'time', 'large', 'metalearn', 'parameter', 'transfer', 'learn', 'mamllike', 'finn', 'set', 'et', 'table', 'replace', 'inner', 'training', 'loop', 'ntkbase', 'inference', 'use', 'cnn', 'miniimagenet', 'oreshkin', 'scalar', 'output', 'batch', 'size', 'setting', 'achieve', 'least', 'channel', 'support', 'model', 'least', 'time', 'large', 'park', '§', 'use', 'ntk', 'predict', 'eralization', 'performance', 'architecture', 'context', 'neural', 'architecture', 'search', 'zoph', 'na', 'ever', 'author', 'comment', 'high', 'computational', 'burden', 'ultimately', 'use', 'different', 'proxy', 'objective', 'nas', 'set', '§', 'use', 'condition', 'number', 'ntk', 'predict', 'model', 'trainability', 'table', 'use', 'ntk', 'evaluate', 'trainability', 'several', 'imagenet', 'model', 'resnet', 'vision', 'transformer', 'dosovitskiy', 'mlpmixer', 'however', 'prohibitive', 'computational', 'cost', 'case', 'author', 'evaluate', 'pseudontk', 'ntk', 'scalarvalue', 'function3', 'impact', 'quality', 'respective', 'trainabilitygeneralization', 'proxy', 'contrast', 'work', 'compute', 'full', '×', 'class', 'ntk', 'model', 'ie', 'perform', 'task', 'time', 'costly', 'finally', 'remark', 'setting', 'scale', 'increase', 'width', 'work', 'true', 'ntk', 'pseudontk', 'lead', 'improved', 'downstream', 'task', 'performance', 'well', 'inﬁnite', 'widthlinearization', 'approximation', 'higherquality', 'trainabilitygeneralization', 'proxy', 'respectively', 'make', 'work', 'especially', 'rele', 'vant', 'modern', 'research', '3precisely', 'compute', 'jacobian', 'single', 'logit', 'sum', 'class', 'logit', 'result', 'full', 'ntk', 'rather', 'single', 'diagonal', 'block', 'sum', 'diagonal', 'block', 'ﬁnite', 'width', 'dense', 'matrix', 'blockdiagonal', 'fast', 'finite', 'kernel', 'algorithm', 'efﬁcient', 'ntk', 'computation', 'describe', 'algorithm', 'fast', 'ntk', 'computation', 'cover', 'preliminary', 'begin', 'ing', 'notation', 'use', 'paper', '§', 'describe', 'primitive', 'building', 'block', 'ad', 'includ', 'e', 'jacobianvector', 'product', 'jvp', 'vectorjacobian', 'product', 'correspond', 'forward', 'reverse', 'mode', 'ad', 'respectively', 'discuss', 'apply', 'tool', 'describe', 'computa', 'tional', 'complexity', 'baseline', 'approach', 'compute', 'ntk', 'use', 'prior', 'work', '§', 'present', 'algorithm', 'enable', 'accelerate', 'computation', 'order', 'magnitude', 'different', 'way', 'preliminary', 'notation', 'ro', 'output', 'class', 'consider', 'θ', 'x', 'logit', 'input', 'total', 'number', 'p', 'trainable', 'pa', 'rameter', 'vec', 'cid2θ0', 'θlcid3', 'size', 'also', 'assume', 'network', 'intermediate', 'primitive', 'output', 'size', 'example', 'activa', 'tion', 'preactivation', 'let', 'yk', 'total', 'size', 'output', 'see', 'fig', 'fig', 'ntk', 'cid124cid123cid122cid125', 'x1cid14∂θcid3', 'θ', 'x2', 'cid14∂θcid3', 'cid125', 'l', 'θ', 'x2cid14∂θlcid3', 'cid125', 'denote', 'time', 'memory', 'depend', 'con', 'text', 'cost', 'single', 'forward', 'pass', 'θ', 'memory', 'exclude', 'cost', 'store', 'p', 'weight', 'rather', 'deﬁne', 'cost', 'evaluate', 'primitive', 'yk', 'time', 'plcid1', 'give', 'memory', 'cost', 'denote', 'simply', 'finally', 'con', 'sider', 'x2', 'batch', 'input', 'case', 'ntk', 'matrix', 'obtain', 'com', '×', 'put', 'eq', 'pair', 'input', 'see', '§', 'b', 'glossary', 'jacobianvector', 'product', 'jvp', 'vectorjacobian', 'product', 'follow', 'maclaurin', 'deﬁne', 'vjpf', 'p', 'r', 'r', 'θ', 'r', 'xcid14∂θcid3', 'p', 'r', 'jvp', 'understand', 'push', 'forward', 'tangent', 'vector', 'θt', 'weight', 'space', 'tangent', 'vector', 'space', 'output', 'contrast', 'pull', 'back', 'cotangent', 'vector', 'fc', 'space', 'output', 'cotangent', 'vector', 'weight', 'space', 'elementary', 'operation', 'enable', 'forward', 'reverse', 'mode', 'ad', 'respectively', 'serve', 'basis', 'typical', 'ad', 'computation', 'gradient', 'jacobian', 'hessian', 'time', 'cost', 'comparable', 'see', '§', 'walther', 'memory', 'cost', 'jvp', 'fp', 'well', 'pl', 'memory', 'cost', 'vjp', 'generally', 'p', 'require', 'store', 'intermediate', 'primitive', 'output', 'efﬁcient', 'backprop', 'l', 'output', 'cotangent', 'however', 'purpose', 'compute', 'ntk', 'never', 'need', 'store', 'whole', 'jacobian', 'individual', 'cotangent', 'compute', 'sum', 'eq', 'layerbylayer', 'hence', 'consider', 'cost', 'pl', 'memory', 'summarize', 'batch', 'input', 'jvp', 'cost', 'time', 'cid2ykcid3', 'p', 'memory', 'cost', 'time', 'plcid3', 'p', 'memory', 'jacobian', 'reverse', 'mode', 'jacobian', 'compute', 'call', 'row', 'identity', 'matrix', 'io', 'cid14∂θcid3', '∈', 'cid14∂θcid3', 'p×o', 'r', 'io', 'therefore', 'cost', 'vjp', 'time', 'memory', 'apart', 'parameter', 'primitive', 'output', 'reuse', 'vjps', 'therefore', 'batch', 'input', 'jacobian', 'cost', 'fp', 'time', 'cid2yk', 'plcid3ny', 'p', 'memory', 'jacobian', 'contraction', 'baseline', 'baseline', 'method', 'evaluate', 'ntk', 'consist', 'compute', 'jacobian', 'contract', 'eq', 'contraction', 'cost', 'time', 'nopl', 'memory', 'store', 'result', 'layerbylayer', 'cotangent', 'include', 'cost', 'compute', 'cotangent', 'batch', 'jacobian', '∂θ0', 'arrive', 'jacobian', 'contraction', 'cost', 'fp', 'time', 'cid2yk', 'plcid3', 'p', 'memory', '4to', 'declutter', 'notation', 'work', 'time', 'ory', 'complexity', 'expression', 'omit', 'symbol', 'imply', 'take', 'maximum', 'free', 'index', 'summary', 'jacobian', 'contraction', 'perform', 'forward', 'pass', 'follow', 'expensive', 'contraction', 'next', 'demonstrate', 'reduce', 'contraction', 'cost', 'fast', 'finite', 'ntkvector', 'product', 'consider', 'ntkvector', 'product', 'function', 'v', 'r', 'v', 'r', 'take', 'ntkvector', 'product', 'column', 'tity', 'matrix', 'io', 'yield', 'full', 'pande', 'vpv', 'θ', 'θ', 'x1cid14∂θcid3', 'x2cid14∂θcid3', 'θ', 'x1cid14∂θcid3', 'vjpf', 'observe', 'ntkvector', 'product', 'express', 'composition', 'jvp', 'vjp', 'cost', 'compute', 'asymptotically', 'cost', 'consist', 'vjps', 'follow', 'cheap', 'jvps', 'therefore', 'fp', 'time', 'cid2yk', 'batched', 'setting', 'eq', 'repeat', 'pair', 'input', 'therefore', 'time', 'increase', 'factor', 'become', 'n2o', 'however', 'memory', 'cost', 'grow', 'linearly', 'cost', 'store', 'size', 'intermediate', 'primitive', 'output', 'gentscotangent', 'compute', 'batch', 'x1', 'x2', 'separately', 'reuse', 'pairwise', 'combination', 'therefore', 'memory', 'cost', 'asymptotically', 'cost', 'store', 'ntk', 'compute', 'altogether', 'ntkvector', 'product', 'cost', 'n2o', 'time', 'cid2yk', 'plcid3', 'ny', 'p', 'memory', 'summary', 'ntkvector', 'product', 'eliminate', 'costly', 'contraction', 'jacobian', 'contraction', 'perform', 'n2o', 'forward', 'pass', 'oppose', 'memory', 'requirement', 'identical', 'result', 'method', 'bene', 'ﬁcial', 'small', 'n', 'network', 'cheap', 'forward', 'pass', 'fp', 'relative', 'always', 'case', 'necessarily', 'cnn', 'structure', 'derivative', 'rewrite', 'primitive', 'output', 'ﬁnd', 'θ', 'eq', 'use', 'chain', 'rule', 'term', '∂yk2', '∂yk2', 'cid33', '∂yk2', '∂yk2', 'also', 'deﬁne', 'consider', 'nonzero', 'direct', 'input', 'deﬁne', 'k1', 'k2', 'individual', 'summand', 'jacobian', 'contraction', 'ntkvector', 'product', 'form', 'sum', 'contraction', 'vjps', 'explicit', 'instantiation', 'primitive', 'jacobian', 'ever', 'vjps', 'jvps', 'guarantee', 'computationally', 'optimal', '§', 'high', 'order', 'computa', 'tion', 'composition', 'ntkvector', 'product', 'con', 'traction', 'jacobian', 'contraction', 'speciﬁcally', 'k1', 'matrixjacobianjacobian', 'matrix', 'product', 'mjjmp', 'show', 'shortly', 'always', 'evaluate', 'optimally', 'vjps', 'jvps', 'idea', 'structured', 'derivative', 'design', 'rule', 'efﬁcient', 'computation', 'mjjmp', 'similarly', 'ad', 'rule', 'vjps', 'eq', 'general', 'case', 'require', 'handmade', 'rule', 'pairwise', 'combination', 'primitive', 'y1', 'y2', 'even', 'abadi', 'pytorch', 'paszke', 'see', '§', 'dramatically', 'reduce', 'number', 'θ', 'linearization', 'follow', 'eq', 'θf', 'ntk', 'evaluate', 'parameter', 'θ', 'θ', 'equal', 'ntk', 'jvp', 'give', 'primal', 'linear', 'function', 'input', 'tangent', 'θ', 'therefore', 'need', 'implement', 'efﬁcient', 'mjjmp', 'linear', 'primitive', 'mjjmp', 'structured', 'derivative', 'far', 'reduce', 'necessary', 'mjjmp', 'rule', 'count', 'decompose', 'mjjmp', 'rule', 'part', 'cid0θlcid1', 'θlx', 'structure', 'derivative', 'rule', 'give', 'single', 'primitive', 'rule', 'identiﬁes', 'small', 'subarray', '∂y∂θl', 'sufﬁcient', 'reconstruct', 'entire', 'primitive', 'constant', 'negligible', 'memory', 'size', 'metadata', 'necessary', 'reconstruction', 'example', 'matrixvector', 'multiplication', '∂y∂θl', 'rule', 'indicate', 'subarray', 'cid2∂y∂θlcid3', 'r1×w6', 'need', 'compute', 'equal', 'case', 'entire', 'primitive', 'jacobian', 'reconstruct', 'word', 'rule', 'annotate', 'linear', 'primitive', 'structure', 'jacobian', 'block', 'diagonal', 'constantblock', 'diagonal', 'tile', 'certain', 'dimension', 'cid2∂y∂θlcid3', '⊗', 'linear', 'function', 'contain', 'nonlinear', 'primitive', 'however', 'linearize', 'function', 'guarantee', 'produce', 'linear', 'primitive', 'frostig', 'radul', 'et', '6we', 'deﬁne', 'aij', 'r', '1×j', 'fast', 'finite', 'kernel', 'mjjmp', 'structure', 'jacobian', 'give', 'input', 'tensor', 'b', 'c', 'c', 'provide', 'structured', 'form', 'describe', 'small', 'subarray', 'metadata', 'rule', 'ﬁciently', 'compute', 'k2', 'amount', 'use', 'npeinsum', 'optimal', 'contraction', 'order', 'adjust', 'instruction', 'base', 'provide', 'meta', 'datum', 'example', '⊗', '⊗', '⊗', '⊗', 'cid1', 'ct', 'cid1', 'ccid1', 'cid0bt', 'ccid1', '⊗', 'able', 'pull', 'c', 'scalar', 'see', '§', 'e', 'similar', 'contraction', 'rule', 'enable', 'signiﬁcant', 'speedup', 'therefore', 'avoid', 'implement', 'mjjmp', 'rule', 'stead', 'single', 'routine', 'perform', 'tensor', 'contraction', 'structure', 'tensor', 'rule', 'anno', 'tat', 'structure', 'linear', 'primitive', 'jacobian', 'list', 'structure', 'associated', 'mjjmp', 'cost', '§', 'e', 'approach', 'guarantee', 'optimality', 'ntk', 'arbitrary', 'function', 'however', 'show', 'asymptotically', 'well', 'jacobian', 'contraction', 'fcns', 'cnn', 'provide', 'order', 'magnitude', 'speedup', 'much', 'complex', 'contemporary', 'imagenet', 'model', '§', 'focus', 'mjjmp', 'typical', 'operation', 'many', 'linear', 'jax', 'primitive', 'trivial', 'implement', 'rarely', 'arise', 'nn', 'time', 'writing', 'annotate', 'linear', 'primitive', 'table', 'sufﬁcient', 'empirical', 'speedup', 'observe', '§', 'summary', 'structure', 'derivative', 'amount', 'evaluate', 'sum', 'mjjmp', 'eq', 'small', 'subarray', 'primitive', 'jacobian', 'instantiate', 'mjjmp', 'leverage', 'structure', 'primitive', 'jacobian', 'efﬁcient', 'contraction', 'together', 'incur', 'cost', 'compute', 'primitive', 'output', 'cotangent', 'eq', 'equivalent', 'cost', 'reverse', 'mode', 'less', 'cost', 'compute', 'nop', 'store', 'nopl', 'weightspace', 'cotangent', 'use', 'fp', 'time7', 'noyk', 'ny', 'p', 'memory', 'cost', 'compute', 'primitive', 'jacobian', 'subarray', 'denote', 'jki', 'l', 'amount', 'time', 'l', 'memory', 'cost', 'evaluate', 'k1', 'efﬁcient', 'mjjmp', 'denote', 'simply', 'mjjmp', 'sub', 'stitute', 'speciﬁc', 'value', 'base', 'primitive', 'required', 'memory', 'store', 'result', 'add', 'cost', 'table', 'show', 'beneﬁcial', 'practical', 'setting', 'structure', 'derivative', 'cost', 'fp', 'mjjmp', 'p', 'memory', 'op', 'time', 'njk', 'example', 'algorithm', '§', 'implementation', 'apply', 'differentiable', 'function', 'result', 'complexi', 'tie', 'depend', 'variable', 'fp', 'p', 'depend', 'table', 'compute', 'compare', 'complexity', 'deep', 'fcn', 'sum', 'marize', 'result', 'table', 'table', 'respectively', 'fcn', 'ntk', 'complexity', 'apply', 'algorithm', 'fcns', 'l', 'hide', 'layer', 'simplicity', 'assume', 'bias', 'input', 'size', 'width', 'deﬁne', 'θlxl', 'output', 'see', 'fig', 'top', 'case', '2l1', 'matrixvector', 'multiplication', 'l', 'nonlinearitie', 'l', 'top', 'layer', 'p', 'finally', 'single', 'forward', 'pass', 'fp', 'ow', 'time', 'memory', '∼', '∼', 'plug', 'cost', 'baseline', 'jacobian', 'contraction', '§', 'obtain', 'jacobian', 'contraction', 'cost', 'n2o2lw2', 'time', 'no2w', 'lw2', 'memory', 'similarly', 'cost', 'ntkvector', 'product', 'ntkvector', 'product', 'cost', 'n2olw2', 'time', 'no2w', 'memory', 'nop', 'time', 'saving', 'reﬂecte', 'asymptotic', 'cost', 'dominate', 'fp', 'require', 'compute', 'primi', 'tive', 'output', 'cotangent', 'however', 'see', 'fig', 'often', 'provide', 'substantial', 'practical', 'beneﬁt', 'allow', 'compute', 'ntk', 'fast', 'compute', 'jacobian', 'structure', 'derivative', '§', 'additionally', 'need', 'derive', 'value', 'mjjmp', 'arbitrary', 'primitive', 'mjjmp', 'cost', 'derive', 'look', 'type', 'structure', 'primitive', 'jacobian', 'table', 'follow', 'extract', 'cost', 'give', 'structure', 'table', 'fast', 'finite', 'kernel', 'method', 'jacobian', 'contraction', 'ntkvector', 'product', 'structured', 'derivative', 'mjjmp', 'noyk', 'njk', 'memory', 'cid2yk', 'plcid3', 'p', 'cid2yk', 'plcid3', 'p', 'time', 'use', 'small', 'op', 'large', 'small', 'op', 'large', 'large', 'table', 'generic', 'ntk', 'computation', 'cost', 'ntkvector', 'product', 'tradeoff', 'contraction', 'fp', 'structured', 'derivative', 'usually', 'save', 'time', 'memory', 'see', '§', 'notation', '§', 'b', 'glossary', 'symbol', 'time', 'method', 'jacobian', 'contraction', 'ntkvector', 'product', 'structure', 'derivative', 'n2o2lw2', 'no2w', 'no2w', 'use', 'memory', 'table', 'fcn', 'ntk', 'computation', 'cost', 'cost', 'obtain', 'substitute', 'table', 'speciﬁc', 'value', 'fp', 'mjjmp', 'correspond', 'fcn', 'describe', 'ntkvector', 'product', 'allow', 'reduction', 'time', 'complexity', 'structured', 'derivative', 'reduce', 'time', 'memory', 'complexity', 'see', 'fig', 'empirical', 'conﬁrmation', 'flop', 'wallclock', 'time', 'see', 'discussion', 'notation', '§', 'b', 'full', 'glossary', 'symbol', 'method', 'time', 'jacobian', 'contraction', 'owcid3', 'owcid3', 'ntkvector', 'product', 'owcid3', 'structured', 'derivative', 'owcid3', 'l', 'min', 'dw', 'dfw2', 'dw', 'memory', 'use', 'cid2dw', 'owcid3', 'cid2dw', 'owcid3', 'n2o2', 'dw', 'ow', 'table', 'ntk', 'computation', 'cost', 'cnn', 'pixel', 'ﬁlter', 'size', 'structure', 'derivative', 'reduce', 'time', 'complexity', 'low', 'memory', 'cost', 'ow', 'common', 'setting', 'see', 'fig', 'experiment', 'resnet', 'discussion', 'table', 'fcn', 'table', 'generic', 'cost', 'analysis', '§', 'b', 'glossary', 'symbol', 'see', 'e', 'apply', 'formal', 'approach', '§', 'demonstration', 'purpose', 'present', 'derivation', 'require', 'reference', 'appendix', 'θlxl', 'ﬁrst', 'note', 'need', 'consider', 'index', 'eq', 'summand', 'absence', 'weight', 'sharing', 'layer', 'matrix', 'vector', 'multiplication', 'rule', 'indicate', 'ex', 'ample', 'give', '§', 'command', 'compute', 'r1×w', '∼', 'finally', 'efﬁcient', 'mjjmp', 'structured', 'i∂θl', 'compute', 'analogously', 'follow', 'l', 'case', 'therefore', '⊗', '\uf8f6', '\uf8eb', '\uf8f6', '\uf8f8', 'cid124cid123cid122cid125', '⊗', 'cid124cid123cid122cid125', '⊗', 'cid124cid123cid122cid125', 'o×w', 'cid125', 'w×o', 'cid124cid123cid122cid125', 'cid124cid123cid122cid125', 'cid124cid123cid122cid125', 'o×w', 'cid125', 'w×o', 'contract', 'o2w', 'time', 'analogous', 'derivation', 'apply', 'l', 'yield', 'time', 'therefore', 'total', 'contraction', 'cost', 'mjjmp', 'account', 'depth', 'l', 'batch', 'size', 'altogether', '∼', 'fcn', 'structure', 'derivative', 'cost', 'nolw2', 'n2o3', 'time', 'memory', 'summary', 'summarize', 'fcn', 'cost', 'table', 'conclude', 'structured', 'derivative', 'ntkvector', 'prod', 'uct', 'allow', 'reduction', 'time', 'cost', 'computation', 'different', 'way', 'structured', 'derivative', 'also', 'reduce', 'memory', 'requirement', 'structure', 'derivative', 'beneﬁcial', 'wide', 'network', 'large', 'ntkvector', 'prod', 'uct', 'beneﬁcial', 'network', 'large', 'output', 'conﬁrm', 'prediction', 'flop', 'measurement', 'fig', 'far', 'conﬁrm', 'method', 'provide', 'order', 'magnitude', 'speedup', 'memory', 'saving', 'major', 'hardware', 'platform', 'fig', 'right', 'fig', 'however', 'notice', 'time', 'measurement', 'often', 'deviate', 'pre', 'diction', 'unaccounted', 'constant', 'overhead', 'various', 'method', 'hardware', 'speciﬁcs', 'padding', 'behavior', 'compiler', 'structure', 'derivative', 'almost', 'always', 'outperform', 'ntkvector', 'product', 'fast', 'finite', 'kernel', 'flop', 'ntk', 'entry', 'wallclock', 'time', 'tpuv3', 'figure', 'flop', 'leave', 'wallclock', 'time', 'right', 'compute', 'ntk', 'relu', 'fcn', 'predict', 'table', 'method', 'almost', 'always', 'outperform', 'jacobian', 'contraction', 'allow', 'order', 'magnitude', 'speedup', 'memory', 'improvement', 'realistic', 'problem', 'size', 'leave', 'flop', 'ntk', 'entry', 'conﬁrm', 'several', 'speciﬁc', 'theoretical', 'prediction', 'right', 'wallclock', 'runtime', 'compiler', 'hardware', 'speciﬁcs', 'observe', 'real', 'application', 'give', 'ntkvector', 'product', 'good', 'perform', 'method', 'cost', 'equivalent', 'width', 'w', 'output', 'size', 'top', 'row', 'ntkvector', 'product', 'offer', 'ofold', 'improvement', 'jacobian', 'contraction', 'leave', 'right', 'column', 'ntkvector', 'product', 'equivalent', 'jacobian', 'contrac', 'tion', 'leftmost', 'column', 'structure', 'derivative', 'outperform', 'ntkvector', 'product', 'w', 'plot', 'pale', 'vertical', 'line', 'structure', 'derivative', 'ntkvector', 'product', 'tersect', 'structure', 'derivative', 'approach', 'cost', 'limit', 'large', 'width', 'leave', 'right', 'method', 'expect', 'scale', 'quadratically', 'pale', 'grey', 'dash', 'line', 'depict', 'w2', 'scale', 'see', 'fig', 'hardware', 'platform', '§', 'n', 'detail', 'ntkvector', 'product', 'improve', 'jacobian', 'contrac', 'tion', 'effect', 'perfectly', 'robust', 'see', 'bottom', 'row', 'small', 'fig', 'notably', 'plat', 'form', 'structured', 'derivative', 'robustly', 'outperform', 'meth', 'od', 'include', 'simply', 'compute', 'discuss', '§', 'structure', 'derivative', 'low', 'memory', 'footprint', 'reach', '8x', 'large', 'width', 'bottom', 'right', 'miss', 'point', 'indicate', 'process', 'model', '64x', 'large', 'method', 'discuss', '§', 'method', 'small', 'memory', 'footprint', 'see', 'logit', 'logit', 'logit', 'b', 'c', 'h', 'b', 'c', 'h', 'b', 'c', 'h', 'p', 'l', 'p', 'l', 'p', 'l', 'width', 'width', 'width', 'c', 'e', 'c', 'e', 'c', 'e', 'logit', 'logit', 'logit', 'b', 'c', 'h', 'b', 'c', 'h', 'b', 'c', 'h', 'width', 'width', 'width', 'jacobian', 'contraction', 'ntkvector', 'product', 'structure', 'derivative', 'jacobian', 'w', 'logit', 'logit', 'logit', 'c', 'e', 'c', 'e', 'c', 'e', 'b', 'c', 'h', 'e', 'b', 'c', 'h', 'e', 'b', 'c', 'h', 'e', 'width', 'width', 'fast', 'finite', 'kernel', 'figure', 'wallclock', 'time', 'compute', 'ntk', 'several', 'resnet', 'size', 'pair', 'imagenet', 'image', 'structure', 'derivative', 'allow', 'ntk', 'compute', 'fast', 'large', 'model', 'see', 'bottom', 'row', 'miss', 'point', 'indicate', 'outofmemory', 'ntkvector', 'product', 'predict', '§', 'table', 'advantageous', 'large', 'bottom', 'row', 'suboptimal', 'cost', 'forward', 'pass', 'fp', 'large', 'relative', 'output', 'size', 'number', 'parameter', 'eg', 'lot', 'weight', 'sharing', 'see', 'table', 'table', 'case', 'convolution', 'notably', 'top', 'see', 'fig', 'imagenet', 'model', '§', 'complexity', 'analysis', '§', 'n', 'experimental', 'detail', 'ntk', 'complexity', 'perform', 'analogous', 'derivation', 'cnn', 'nel', 'pixel', 'ﬁlter', 'size', 'arrive', 'table', 'make', 'observation', 'speciﬁc', 'cnn', 'first', 'speed', 'ntkvector', 'product', 'jacobian', 'con', 'traction', 'much', 'similar', 'high', 'cost', 'forward', 'pass', 'fp', 'relative', 'p', 'weight', 'sharing', 'perform', 'depend', 'speciﬁc', 'value', 'parameter', 'conﬁrm', 'experiment', 'genet', 'model', 'ntkvector', 'product', 'typically', 'underperform', 'outperform', 'secondly', 'structure', 'derivative', 'continue', 'perform', 'fast', 'jacobian', 'contraction', 'relative', 'memory', 'cost', 'depend', 'hyperparameter', 'require', 'common', 'case', 'imagenet', 'conﬁrme', 'experiment', 'fig', 'fig', 'bottom', 'imagenet', 'experiment', '§', 'derive', 'asymptotic', 'time', 'memory', 'beneﬁts', 'ntkvector', 'product', 'structure', 'derivative', 'baseline', 'jacobian', 'contraction', 'fcns', 'cnn', 'ever', 'contemporary', 'architecture', 'rarely', 'resemble', 'vanilla', 'feedforward', 'network', 'instead', 'result', 'much', 'com', 'plex', 'computational', 'graph', 'comprise', 'many', 'different', 'primitive', 'make', 'complexity', 'analysis', 'impractical', 'therefore', 'evaluate', 'method', 'wild', 'ﬁrm', 'computational', 'beneﬁts', 'full', 'imagenet', 'model', 'fig', 'resnet', 'fig', 'widere', 'net', 'zagoruyko', 'komodakis', 'vision', 'transformer', 'transformerresnet', 'hybrid', 'dosovitskiy', 'steiner', 'mlpmixer', 'tolstikhin', 'compute', 'full', 'ntk', 'often', 'possible', 'structured', 'derivative', '×', '×', 'implementation', 'algorithm', 'implement', 'jax8', 'bradbury', 'integrate', 'neural', 'tangent', 'novak', 'jacobian', 'contraction', 'ntkvector', 'product', 'build', 'core', 'operation', 'vmap', 'structured', 'derivative', 'implement', 'jaxpr', 'inter', 'preter', 'build', 'top', 'reverse', 'mode', 'ad', 'interpreter', 'owe', 'nuanced', 'tradeoff', 'different', 'method', 'general', 'case', 'release', 'implementation', 'single', 'function', 'allow', 'user', 'manually', 'select', 'imple', 'mentation', 'also', 'include', 'automate', 'setting', 'perform', 'flop', 'analysis', 'method', 'compilation', 'time', 'automatically', 'choose', 'efﬁcient', 'conclusion', 'perform', 'ﬁrst', 'extensive', 'analysis', 'com', 'putational', 'complexity', 'ntk', 'show', 'improve', 'dramatically', 'mixedorder', 'ad', 'ntk', 'vector', 'product', 'custom', 'interpreter', 'efﬁ', 'cient', 'higherorder', 'ad', 'operation', 'structure', 'derivative', 'ntk', 'computation', 'similar', 'many', 'object', 'interest', 'machine', 'learning', 'gaussnewton', 'fisher', 'information', 'matrix', 'look', 'forward', 'extension', 'algorithm', 'setting', 'future', 'work', 'discussion', 'framework', 'c', 'e', 'c', 'e', '×', '×', '×', '×', '×', '×', '×', 'nvidia', 'tpuv4', 'jacobian', 'contraction', 'ntkvector', 'product', 'structure', 'derivative', 'jacobian', 'l', 'g', 'l', 'g', 'resnet', 'depth', 'resnet', 'depth', 'resnet', 'depth', 'fast', 'finite', 'kernel', 'acknowledgement', 'thank', 'useful', 'discussion', 'review', 'comment', 'initial', 'version', 'manuscript', 'useful', 'discussion', 'code', 'review', 'also', 'thank', 'shaobo', 'hou', 'work', 'help', 'team', 'help', 'advice', 'reference', 'abadi', 'isard', 'system', 'largescale', 'machine', 'learning', '12th', 'usenix', 'symposium', 'operating', 'system', 'design', 'implementation', 'cite', 'page', 'adlam', 'snoek', 'explore', 'uncertainty', 'property', 'neural', 'network', 'implicit', 'prior', 'inﬁnitewidth', 'limit', 'international', 'conference', 'learn', 'representation', 'cite', 'page', 'arﬁan', 'ukraine', 'vector', 'cite', 'page', 'arora', 'salakhutdinov', 'r', 'r', 'r', 'exact', 'computation', 'inﬁnitely', 'wide', 'neural', 'net', 'advance', 'neural', 'information', 'pro', 'cesse', 'system', 'cite', 'page', 'arora', 'grain', 'analysis', 'optimization', 'generalization', 'overparameterize', 'twolayer', 'neural', 'network', 'preprint', '2019b', 'cite', 'page', 'arora', 'harness', 'power', 'inﬁnitely', 'wide', 'deep', 'net', 'smalldata', 'task', 'international', 'confer', 'ence', 'learn', 'representation', 'openreviewnetforumidrkl8sjbyvh', 'cite', 'page', 'babuschkin', 'baumli', 'bhupatiraju', 'cai', 'clark', 'dani', 'helka', 'fantacci', 'king', 'g', 'ring', 'r', 'ruiz', 'sanchez', 'schneider', 'r', 'sezener', 'e', 'stokowiec', 'viola', 'deepmind', 'url', 'cite', 'page', 'dyer', 'preprint', 'u', 'explain', 'neural', 'scaling', 'law', 'cite', 'page', 'network', 'exchange', 'cite', 'page', 'belkin', 'mandal', 'reconcile', 'modern', 'machinelearning', 'practice', 'classical', 'bias', 'tradeoff', 'proceeding', 'science', 'cite', 'page', 'borovykh', 'gaussian', 'process', 'perspective', 'convolu', 'tional', 'network', 'preprint', 'cite', 'page', 'bradbury', 'r', 'j', 'maclaurin', 'com', 'posable', 'transformation', 'pythonnumpy', 'program', 'cite', 'page', 'brock', 'characterize', 'signal', 'propagation', 'close', 'performance', 'gap', 'unnormal', 'ized', 'resnet', 'preprint', 'cite', 'page', 'brock', 'simonyan', 'high', 'performance', 'largescale', 'image', 'recognition', 'malization', 'arxiv', 'preprint', 'cite', 'page', 'neural', 'architecture', 'search', 'imagenet', 'hour', 'theoretically', 'international', 'conference', 'inspire', 'perspective', 'learning', 'representation', 'cite', 'page', 'gong', 'b', 'vision', 'former', 'resnet', 'pretraine', 'strong', 'datum', 'augmentation', 'cite', 'page', 'dauphin', 'schoenholz', 'metainit', 'initialize', 'learning', 'learn', 'initialize', 'advance', 'neural', 'information', 'processing', 'system', 'cite', 'page', 'r', 'imagenet', 'largescale', 'hierarchical', 'image', 'database', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'ieee', 'cite', 'page', 'fast', 'finite', 'kernel', 'dosovitskiy', 'beyer', 'weissenborn', 'heigold', 'g', 'gelly', 'houlsby', 'n', 'image', 'worth', 'word', 'transformer', 'international', 'confer', 'image', 'recognition', 'scale', 'ence', 'learn', 'representation', 'cite', 'page', 'r', 'r', 'graph', 'kernel', 'fuse', 'graph', 'neural', 'network', 'graph', 'kernel', 'advance', 'neural', 'information', 'processing', 'system', 'cite', 'page', 'abbeel', 'modelagnostic', 'meta', 'learning', 'fast', 'adaptation', 'deep', 'network', 'teh', 'proceeding', '34th', 'international', 'conference', 'machine', 'learning', 'vol', 'ume', 'proceeding', 'machine', 'learn', 'research', 'pmlr', 'cite', 'page', 'e', 'aye', 'p', 'neural', 'tangent', 'kernel', 'spective', 'preprint', 'cite', 'page', 'frostig', 'r', 'paszke', 'radul', 'decomposing', 'reversemode', 'automatic', 'differ', 'entiation', 'arxiv', 'preprint', 'cite', 'page', 'garrigaalonso', 'aitchison', 'l', 'e', 'deep', 'convolutional', 'network', 'shallow', 'gaussian', 'pro', 'cesse', 'international', 'conference', 'learn', 'repre', 'sentation', 'cite', 'page', 'griewank', 'walther', 'evaluate', 'deriva', 'tive', 'society', 'industrial', 'apply', 'math', 'ematics', 'edition', 'url', 'cite', 'page', 'grosse', 'r', 'neural', 'net', 'training', 'dynamic', 'uary', '∼rgrossecoursescsc2541', '2021reading', 'l02', 'cite', 'page', 'depth', 'width', 'correction', 'international', 'confer', 'kernel', 'ence', 'learn', 'representation', 'cite', 'page', 'teh', 'deep', 'ensemble', 'r', 'ed', 'advance', 'mation', 'processing', 'system', 'annual', 'conference', 'information', 'processing', 'system', 'neural', 'neurip', 'virtual', 'cite', 'page', 'sun', 'deep', 'residual', 'learn', 'e', 'image', 'recognition', 'proceeding', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'cite', 'page', 'heek', 'oliver', 'ritter', 'rondepierre', 'steiner', 'flax', 'neural', 'network', 'library', 'ecosystem', 'url', 'githubcomgoogleflax', 'cite', 'page', 'cai', 'babuschkin', 'haiku', 'sonnet', 'deepminddmhaiku', 'cite', 'page', 'horace', 'r', 'functorch', 'jaxlike', 'composable', 'function', 'transform', 'pytorch', 'functorch', 'cite', 'page', 'sohl', 'dickstein', 'exact', 'posterior', 'distribution', 'wide', 'neural', 'network', '2020a', 'cite', 'page', 'novak', 'r', 'inﬁnite', 'attention', 'nngp', 'ntk', 'deep', 'attention', 'network', 'international', 'conference', 'machine', 'learning', '2020b', 'cite', 'page', 'inﬁnitely', 'wide', 'graph', 'convolutional', 'network', 'semisupervise', 'learning', 'gaussian', 'process', 'preprint', 'cite', 'page', 'neural', 'generalization', 'neural', 'network', 'advance', 'neural', 'information', 'processing', 'system', 'cite', 'page', 'immer', 'abedi', 'e', 'inference', 'turn', 'deep', 'network', 'gaussian', 'process', 'advance', 'neural', 'information', 'processing', 'system', 'cite', 'page', 'r', 'sohldickstein', 'deep', 'neural', 'network', 'process', 'international', 'conference', 'learn', 'representation', 'cite', 'page', 'fast', 'finite', 'wide', 'neural', 'net', 'work', 'depth', 'evolve', 'linear', 'model', 'gradient', 'descent', 'advance', 'neural', 'information', 'processing', 'system', 'cite', 'page', 'r', 'sohldickstein', 'finite', 'inﬁnite', 'neural', 'network', 'empirical', 'study', 'advance', 'neural', 'information', 'processing', 'system', 'cite', 'page', 'maclaurin', 'duvenaud', 'adam', 'r', 'p', 'auto', 'icml', 'grad', 'effortless', 'gradient', 'numpy', 'workshop', 'url', 'hipsautograd', 'cite', 'page', 'grosse', 'r', 'optimize', 'neural', 'network', 'kroneckerfactore', 'approximate', 'curvature', 'interna', 'tional', 'conference', 'machine', 'learning', 'pmlr', 'cite', 'page', 'matthew', 'r', 'ghahramani', 'z', 'gaussian', 'process', 'behaviour', 'wide', 'deep', 'neural', 'network', 'international', 'conference', 'learn', 'representation', 'cite', 'page', 'h', 'solution', 'directe', 's´eculaire', 'quelque', 'probleme', 'analogue', 'transcendant', 'r', 'cite', 'page', 'naumann', 'u', 'optimal', 'accumulation', 'jacobian', 'matrix', 'elimination', 'method', 'dual', 'computational', 'graph', 'mathematical', 'programming', 'cite', 'page', 'naumann', 'u', 'optimal', 'jacobian', 'accumulation', 'complete', 'mathematical', 'programming', '1122427–441', 'cite', 'page', 'neal', 'r', 'prior', 'inﬁnite', 'network', 'tech', 'rep', 'cite', 'page', 'nguyen', 'meta', 'learning', 'kernel', 'ridgeregression', 'preprint', 'cite', 'page', 'nguyen', 'lation', 'inﬁnitely', 'wide', 'convolutional', 'network', 'preprint', 'cite', 'page', 'novak', 'r', 'pennington', 'bayesian', 'deep', 'convolutional', 'network', 'many', 'nel', 'gaussian', 'process', 'international', 'conference', 'learn', 'representation', 'cite', 'page', 'novak', 'r', 'sohldickstein', 'neural', 'tangent', 'fast', 'easy', 'inﬁnite', 'neural', 'network', 'international', 'conference', 'learn', 'repre', 'sentation', 'neuraltangent', 'cite', 'page', 'oreshkin', 'b', 'l´opez', 'p', 'r', 'lacoste', 'task', 'dependent', 'adaptive', 'metric', 'improve', 'fewshot', 'learning', 'cite', 'page', 'park', 'nngpguide', 'neural', 'architecture', 'search', 'arxiv', 'preprint', 'cite', 'page', 'paszke', 'gross', 'massa', 'lerer', 'bradbury', 'antiga', 'l', 'desmaison', 'raison', 'tejani', 'chilamkurthy', 'steiner', 'l', 'pytorch', 'imperative', 'style', 'highperformance', 'deep', 'learn', 'library', 'beygelzimer', 'dalch´ebuc', 'fox', 'e', 'garnett', 'r', 'ed', 'advance', 'neural', 'information', 'processing', 'system', 'animperativestylehighperformancedeep', 'learninglibrarypdf', 'cite', 'page', 'pennington', 'geometry', 'neural', 'net', 'work', 'loss', 'surface', 'random', 'matrix', 'theory', 'teh', 'proceeding', '34th', 'international', 'conference', 'machine', 'learn', 'ing', 'volume', 'proceeding', 'machine', 'learn', 'e', 'research', 'pmlr', 'pennington17ahtml', 'cite', 'page', 'radul', 'paszke', 'frostig', 'r', 'linearize', 'tangent', 'transpose', 'gradient', 'preprint', 'cite', 'page', 'schoenholz', 'sohl', 'dickstein', 'deep', 'information', 'propagation', 'interna', 'tional', 'conference', 'learn', 'representation', 'cite', 'page', 'spigler', 'sagun', 'l', 'biroli', 'g', 'wyart', 'jamming', 'transition', 'underto', 'parametrization', 'affect', 'generalization', 'deep', 'learning', 'journal', 'physics', 'mathematical', 'theoretical', 'cite', 'page', 'steiner', 'beyer', 'l', 'train', 'vit', 'data', 'augman', 'tation', 'regularization', 'vision', 'transformer', 'preprint', 'cite', 'page', 'fast', 'finite', 'srinivasan', 'p', 'p', 'mildenhall', 'b', 'singhal', 'r', 'fouri', 'feature', 'let', 'network', 'learn', 'high', 'frequency', 'function', 'low', 'dimensional', 'domain', 'neurip', 'cite', 'page', 'tolstikhin', 'houlsby', 'beyer', 'keyser', 'uszkoreit', 'dosovitskiy', 'mlpmixer', 'allmlp', 'architecture', 'vision', 'cite', 'page', 'dynamical', 'isometry', 'mean', 'ﬁeld', 'theory', 'cnn', 'train', '10000layer', 'vanilla', 'volutional', 'neural', 'network', 'international', 'conference', 'machine', 'learning', 'cite', 'page', 'disentan', 'gle', 'trainability', 'generalization', 'deep', 'learning', 'international', 'conference', 'machine', 'learning', 'cite', 'page', 'nongaussian', 'process', 'neural', 'network', 'ﬁnite', 'width', 'mathematical', 'scientiﬁc', 'machine', 'learning', 'conference', 'cite', 'page', 'g', 'scale', 'limit', 'wide', 'neural', 'network', 'weight', 'share', 'gaussian', 'process', 'behavior', 'gradient', 'dependence', 'neural', 'kernel', 'derivation', 'arxiv', 'preprint', 'cite', 'page', 'neural', 'tangent', 'kernel', 'architecture', 'arxiv', 'preprint', 'cite', 'page', 'mean', 'ﬁeld', 'theory', 'batch', 'international', 'conference', 'learn', 'malization', 'representation', 'cite', 'page', 'komodaki', 'wide', 'residual', 'network', 'british', 'machine', 'vision', 'conference', 'cite', 'page', 'h', 'initialization', 'residual', 'learning', 'normalization', 'preprint', 'cite', 'page', 'metalearning', 'neural', 'ternational', 'conference', 'learn', 'representation', 'ti87pv5oc8', 'cite', 'page', 'tangent', 'kernel', 'zoph', 'b', 'q', 'v', 'neural', 'architecture', 'search', 'reinforcement', 'learn', 'abs161101578', 'cite', 'page', 'fast', 'finite', 'kernel', 'appendix', 'additional', 'figure', 'cpu', 'skylake', 'tpuv4', 'figure', 'wallclock', 'time', 'compute', 'ntk', 'relu', 'fcn', 'different', 'platform', 'setting', 'structure', 'derivative', 'allow', 'order', 'magnitude', 'improvement', 'wallclock', 'time', 'memory', 'miss', 'point', 'indicate', 'outofmemory', 'error', 'however', 'remark', 'platform', 'right', 'ntkvector', 'product', 'deliver', 'robust', 'improvement', 'large', 'rightmost', 'column', 'cost', 'comparable', 'even', 'large', 'jacobian', 'contraction', 'see', 'fig', 'flop', 'tpuv3', 'platform', 'discussion', 'see', 'n', 'detail', 'e', 'e', 'c', 'e', 'logit', 'logit', 'logit', 'b', 'c', 'h', 'b', 'c', 'h', 'b', 'c', 'h', 'width', 'width', 'width', 'e', 'e', 'c', 'e', 'logit', 'logit', 'logit', 'b', 'c', 'h', 'b', 'c', 'h', 'b', 'c', 'h', 'width', 'width', 'width', 'logit', 'logit', 'logit', 'b', 'c', 'h', 'b', 'c', 'h', 'b', 'c', 'h', 'c', 'e', 'c', 'e', 'c', 'e', 'width', 'width', 'width', 'logit', 'logit', 'logit', 'b', 'c', 'h', 'b', 'c', 'h', 'b', 'c', 'h', 'e', 'e', 'c', 'e', 'width', 'width', 'width', 'jacobian', 'contraction', 'ntkvector', 'product', 'structure', 'derivative', 'jacobian', 'w', 'logit', 'logit', 'logit', 'c', 'e', 'c', 'e', 'c', 'e', 'b', 'c', 'h', 'e', 'b', 'c', 'h', 'e', 'b', 'c', 'h', 'e', 'width', 'width', 'fast', 'finite', 'kernel', 'figure', 'wallclock', 'time', 'input', 'pair', 'compute', 'ntk', 'various', 'imagenet', 'model', 'vision', 'tansformer', 'hybrid', 'dosovitskiy', 'steiner', 'wideresnet', 'komodakis', 'mlpmixer', 'tolstikhin', 'structure', 'derivative', 'generally', 'allow', 'fast', 'computation', 'also', 'able', 'process', 'model', 'low', 'memory', 'requirement', 'low', 'leave', 'miss', 'point', 'indicate', 'outofmemory', 'error', 'case', 'single', 'output', 'logit', 'top', 'row', 'ntkvector', 'product', 'generally', 'detrimental', 'costly', 'forward', 'pass', 'fp', 'relative', 'size', 'parameter', 'p', 'lot', 'weight', 'sharing', 'see', 'table', 'however', 'ntkvector', 'product', 'scale', 'well', 'method', 'output', 'size', 'bottom', 'row', 'perform', 'comparably', 'well', 'method', 'finally', 'remark', 'jacobian', 'run', 'memory', 'fast', 'also', 'take', 'time', 'compute', 'conjecture', 'large', 'memory', 'footprint', 'sometimes', 'perform', 'optimization', 'trade', 'speed', 'memory', 'therefore', 'compute', 'less', 'optimal', 'way', 'memory', 'available', 'alternatively', 'also', 'perform', 'simpliﬁcation', 'expression', 'case', 'possible', 'jacobian', 'computation', 'alone', 'see', 'fig', 'resnet', '§', 'n', 'detail', 'figure', 'notation', 'use', '§', 'fcn', 'top', '§', 'bottom', 'setting', 'matrix', 'multiplication', 'primitive', 'nonlinearitie', 'extra', 'global', 'average', 'pooling', 'primitive', 'penultimate', 'layer', 'therefore', 'nvidia', 'tpuv4', 'c', 'e', 'e', 'jacobian', 'contraction', 'ntkvector', 'product', 'structure', 'derivative', 'jacobian', 'l', 'g', 'l', 'g', 'r', 'b', 'b', 'r', 'r', 'n', 'r', 'n', 'r', 'b', 'r', 'b', 'b', 'r', 'r', 'model', 'r', 'b', 'b', 'r', 'r', 'n', 'r', 'n', 'r', 'b', 'r', 'b', 'b', 'r', 'r', 'model', 'r', 'b', 'b', 'r', 'r', 'n', 'r', 'n', 'r', 'b', 'r', 'b', 'b', 'r', 'r', 'model', 'e', 'h', 'b', 'e', 'p', 'l', 'q', 'u', 'q', 'q', 'c', 'x', 'l', 'g', 'q', 'e', 'h', 'g', 'h', 'p', 'u', 'b', 'b', 'r', 'w', 'x', 'e', 'p', 'c', 'x', 'w', 'p', 'e', 'p', 'h', 'r', 'z', 'u', 'r', 'e', 'u', 'r', 'q', 'g', 'p', 'h', 'p', 'g', 'n', 'r', 'p', 'h', 'r', 'z', 'p', 'l', 'p', 'h', 'v', 'u', 'c', 'z', 'u', 'l', 'l', 'h', 'c', 'r', 'l', 'u', 'p', 'g', 'b', 'b', 'b', 'c', 'u', 'n', 'e', 'h', 'h', 'g', 'l', 'r', 'l', 'q', 'c', 'r', 'b', 'l', 'r', 'w', 'c', 'r', 'l', 'z', 'u', 'r', 'n', 'h', 'e', 'p', 'e', 'c', 'p', 'g', 'j', 'r', 'h', 'g', 'b', 'v', 'g', 'l', 'h', 'q', 'p', 'u', 'e', 'x', 'u', 'l', 'q', 'l', 'p', 'q', 'b', 'c', 'h', 'l', 'b', 'r', 'c', 'c', 'c', 'q', 'n', 'w', 'p', 'c', 'p', 'c', 'h', 'u', 'l', 'r', 'n', 'l', 'q', 'h', 'h', 'l', 'e', 'e', 'h', 'b', 'e', 'p', 'l', 'q', 'u', 'q', 'q', 'c', 'x', 'l', 'g', 'q', 'e', 'h', 'g', 'h', 'p', 'u', 'b', 'b', 'r', 'w', 'x', 'e', 'p', 'c', 'x', 'w', 'p', 'e', 'p', 'h', 'r', 'z', 'u', 'r', 'e', 'u', 'r', 'q', 'g', 'p', 'h', 'p', 'g', 'n', 'r', 'p', 'h', 'r', 'z', 'p', 'l', 'p', 'h', 'v', 'u', 'c', 'z', 'u', 'l', 'l', 'h', 'c', 'r', 'l', 'u', 'p', 'g', 'b', 'b', 'b', 'c', 'u', 'n', 'e', 'h', 'h', 'g', 'l', 'r', 'l', 'q', 'c', 'r', 'b', 'l', 'r', 'w', 'c', 'r', 'l', 'z', 'u', 'r', 'n', 'h', 'e', 'p', 'e', 'c', 'p', 'g', 'j', 'r', 'h', 'g', 'b', 'v', 'g', 'l', 'h', 'q', 'p', 'u', 'e', 'x', 'u', 'l', 'q', 'l', 'p', 'q', 'b', 'c', 'h', 'l', 'b', 'r', 'c', 'c', 'c', 'q', 'n', 'w', 'p', 'c', 'p', 'c', 'h', 'u', 'l', 'r', 'n', 'l', 'q', 'h', 'h', 'l', 'e', 'e', 'h', 'b', 'e', 'p', 'l', 'q', 'u', 'q', 'q', 'c', 'x', 'l', 'g', 'q', 'e', 'h', 'g', 'h', 'p', 'u', 'b', 'b', 'r', 'w', 'x', 'e', 'p', 'c', 'x', 'w', 'p', 'e', 'p', 'h', 'r', 'z', 'u', 'r', 'e', 'u', 'r', 'q', 'g', 'p', 'h', 'p', 'g', 'n', 'r', 'p', 'h', 'r', 'z', 'p', 'l', 'p', 'h', 'v', 'u', 'c', 'z', 'u', 'l', 'l', 'h', 'c', 'r', 'l', 'u', 'p', 'g', 'b', 'b', 'b', 'c', 'u', 'n', 'e', 'h', 'h', 'g', 'l', 'r', 'l', 'q', 'c', 'r', 'b', 'l', 'r', 'w', 'c', 'r', 'l', 'z', 'u', 'r', 'n', 'h', 'e', 'p', 'e', 'c', 'p', 'g', 'j', 'r', 'h', 'g', 'b', 'v', 'g', 'l', 'h', 'q', 'p', 'u', 'e', 'x', 'u', 'l', 'q', 'l', 'p', 'q', 'b', 'c', 'h', 'l', 'b', 'r', 'c', 'c', 'c', 'q', 'n', 'w', 'p', 'c', 'p', 'c', 'h', 'u', 'l', 'r', 'n', 'l', 'q', 'h', 'h', 'l', 'e', 'n', 'latexit']"
