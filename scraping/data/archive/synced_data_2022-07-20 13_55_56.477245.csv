title,url,date,summary,text,cleaning,tokens
Google & MIT’s  Confident Adaptive Language Modeling Uses Dynamic Compute Allocation to Achieve 3x Speedups,https://syncedreview.com/2022/07/19/google-mits-confident-adaptive-language-modeling-uses-dynamic-compute-allocation-to-achieve-3x-speedups/,2022-07-19,"
In the new paper Confident Adaptive Language Modeling, a research team from Google and MIT presents Confident Adaptive Language Modeling (CALM), a framework that dynamically allocates different amounts of compute to each input and generation timestep, achieving up to 3x speedups while maintaining high performance.

 ","There was a nineteenth-century saying that mocked the use of “a sledgehammer to crack a peanut.” Google AI researcher Tal Schuster echoes this concept in introducing the new paper Confident Adaptive Language Modeling. While acknowledging the tremendous power of transformer-based large language models (LLMs), Schuster notes that many of the predictions they work on “require only minimal effort.” It could be said that using the entire LLM in such cases amounts to a sledgehammer-like overkill. LLMs’ ever-increasing computation costs and associated inference slowdowns are the main bottlenecks impeding their practical application. Developed by a Google and MIT team, the proposed Confident Adaptive Language Modeling (CALM) framework addresses these issues by dynamically allocating different compute amounts to each input and generation timestep. CALM achieves up to 3x speedups on natural language processing (NLP) tasks while maintaining high model performance. The team summarizes their main contributions as: The proposed framework is based on a saturation theory: that the top-ranked prediction in LLMs remains unchanged after some layer and is propagated upward. The number of layers used by the model can thus be dynamically decided with regard to each input. Following this idea, the team develops an adaptive compute approach to dynamically allocate computational resources per input to reduce model complexity while maintaining good performance. This method is also referred to as “early-exiting.” Building on their analysis of the early-exiting paradigm, the team developed CALM as a principled method for increasing model efficiency. CALM leverages a distribution-free risk control technique for calibrating local, per-token exit decisions, such that model performance is provably maintained with arbitrarily high probability. CALM can dynamically allocate different amounts of compute per generated token, following explicitly defined tolerance levels based on the full generation output. In their empirical study, the team implemented CALM on top of the T5 encoder-decoder model and evaluated text-generation task performance on three datasets — CNN/DM, WMT EN-FR, and SQUAD. The results show that CALM can reduce model compute burdens and gain speedups of up to 3x while maintaining high performance.The paper Confident Adaptive Language Modeling is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","There was a nineteenth-century saying that mocked the use of “a sledgehammer to crack a peanut.” Google AI researcher Tal Schuster echoes this concept in introducing the new paper Confident Adaptive Language Modeling. While acknowledging the tremendous power of transformer-based large language models (LLMs), Schuster notes that many of the predictions they work on “require only minimal effort.” It could be said that using the entire LLM in such cases amounts to a sledgehammer-like overkill. LLMs’ ever-increasing computation costs and associated inference slowdowns are the main bottlenecks impeding their practical application. Developed by a Google and MIT team, the proposed Confident Adaptive Language Modeling (CALM) framework addresses these issues by dynamically allocating different compute amounts to each input and generation timestep. CALM achieves up to 3x speedups on natural language processing (NLP) tasks while maintaining high model performance. The team summarizes their main contributions as: The proposed framework is based on a saturation theory: that the top-ranked prediction in LLMs remains unchanged after some layer and is propagated upward. The number of layers used by the model can thus be dynamically decided with regard to each input. Following this idea, the team develops an adaptive compute approach to dynamically allocate computational resources per input to reduce model complexity while maintaining good performance. This method is also referred to as “early-exiting.” Building on their analysis of the early-exiting paradigm, the team developed CALM as a principled method for increasing model efficiency. CALM leverages a distribution-free risk control technique for calibrating local, per-token exit decisions, such that model performance is provably maintained with arbitrarily high probability. CALM can dynamically allocate different amounts of compute per generated token, following explicitly defined tolerance levels based on the full generation output. In their empirical study, the team implemented CALM on top of the T5 encoder-decoder model and evaluated text-generation task performance on three datasets — CNN/DM, WMT EN-FR, and SQUAD. The results show that CALM can reduce model compute burdens and gain speedups of up to 3x while maintaining high performance.The paper Confident Adaptive Language Modeling is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['nineteenthcentury', 'say', 'mock', 'use', 'sledgehammer', 'crack', 'peanut', 'researcher', 'tal', 'schuster', 'echo', 'concept', 'introduce', 'new', 'paper', 'confident', 'adaptive', 'language', 'modeling', 'acknowledge', 'tremendous', 'power', 'transformerbase', 'large', 'language', 'model', 'llm', 'schuster', 'note', 'many', 'prediction', 'work', 'require', 'minimal', 'effort', 'say', 'use', 'entire', 'llm', 'case', 'amount', 'sledgehammerlike', 'overkill', 'llm', 'everincrease', 'computation', 'cost', 'associated', 'inference', 'slowdown', 'main', 'bottleneck', 'impede', 'practical', 'application', 'develop', 'mit', 'team', 'propose', 'confident', 'adaptive', 'language', 'model', 'calm', 'framework', 'address', 'issue', 'dynamically', 'allocate', 'different', 'compute', 'amount', 'input', 'generation', 'timestep', 'calm', 'achieve', 'speedup', 'natural', 'language', 'processing', 'task', 'maintain', 'high', 'model', 'performance', 'team', 'summarize', 'main', 'contribution', 'propose', 'framework', 'base', 'saturation', 'theory', 'topranke', 'prediction', 'llm', 'remain', 'unchanged', 'layer', 'propagate', 'upward', 'number', 'layer', 'use', 'model', 'thus', 'dynamically', 'decide', 'regard', 'input', 'follow', 'idea', 'team', 'develop', 'adaptive', 'compute', 'approach', 'dynamically', 'allocate', 'computational', 'resource', 'input', 'reduce', 'model', 'complexity', 'maintain', 'good', 'performance', 'method', 'also', 'refer', 'earlyexiting', 'building', 'analysis', 'earlyexiting', 'paradigm', 'team', 'develop', 'calm', 'principled', 'method', 'increase', 'model', 'efficiency', 'calm', 'leverage', 'distributionfree', 'risk', 'control', 'technique', 'calibrate', 'local', 'pertoken', 'exit', 'decision', 'model', 'performance', 'provably', 'maintain', 'arbitrarily', 'high', 'probability', 'calm', 'dynamically', 'allocate', 'different', 'amount', 'compute', 'generate', 'token', 'follow', 'explicitly', 'define', 'tolerance', 'level', 'base', 'full', 'generation', 'output', 'empirical', 'study', 'team', 'implement', 'calm', 'top', 't5', 'encoderdecod', 'model', 'evaluate', 'textgeneration', 'task', 'performance', 'dataset', 'cnndm', 'wmt', 'squad', 'result', 'show', 'calm', 'reduce', 'model', 'compute', 'burden', 'gain', 'speedup', 'maintain', 'high', 'performancethe', 'paper', 'confident', 'adaptive', 'language', 'modeling', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
Amazon’s Sockeye 3: Neural Machine Translation With PyTorch That Is 126% Faster on GPUs,https://syncedreview.com/2022/07/18/amazons-sockeye-3-neural-machine-translation-with-pytorch-that-is-126-faster-on-gpus/,2022-07-18,"
Amazon has introduced the latest version of their Sockeye toolkit for the efficient training of stronger and faster neural machine translation (NMT) models. Sockeye 3 achieves speeds up to 126 percent faster than other PyTorch implementations on GPUs and up to 292 percent faster on CPUs.
","Anyone who regularly uses machine translation systems will have noticed huge performance improvements over the last few years, attributable to neural network-based models that have largely replaced the previous generation of phrase-based systems. Introduced in 2018, Sockeye is an open-source framework that offers fast and reliable PyTorch implementation for neural machine translation (NMT) and has been powering Amazon Translate and other NMT applications. Sockeye 2 was released in 2020. In the new paper Sockeye 3: Fast Neural Machine Translation with PyTorch, an Amazon team presents the latest version of the Sockeye toolkit for efficient training of stronger and faster models. Sockeye 3 achieves speeds up to 126 percent faster than other PyTorch implementations on GPUs and up to 292 percent faster on CPUs. Sockeye 3 optimizes a distributed mixed precision training strategy to yield faster calculations and speedups by fitting larger batches into memory. Moreover, it can scale to any number of GPUs and any size of training data by launching separate training processes that use PyTorch’s distributed data parallelism to synchronize updates. For inference design, Sockeye 3 uses static computation graphs to minimize the impacts of dynamic shapes and data-dependent control flow, enabling it to trace various model components via PyTorch’s JIT compiler. The developers also maintain backward compatibility with Sockeye 2 MXNet models — all models that were trained with Sockeye 2 can be converted to models running on Sockeye 3 with PyTorch. Sockeye 3 also introduces many new advanced features: It supports replacing the decoder’s self-attention layers with Simpler Simple Recurrent Units (SSRUs) and fine-tuning with parameter freezing, and enables users to specify arbitrary prefixes (sequences of tokens) on both the source and target sides for any input. In their empirical studies, the team compared Sockeye with benchmark NMT toolkits that included Fairseq (Ott et al., 2019) and OpenNMT (Klein et al., 2017). In the evaluations, Sockeye 3 achieved comparable or better performance on GPUs and CPUs: delivering a 15 percent improvement for batched GPU inference, +126 percent for non-batched GPU inference, and +292 percent for CPU inference. Overall, Sockeye 3 provides much faster model implementations and more advanced features for NMT. As with previous versions, It has been open-sourced under an Apache 2.0 license, and the Amazon team welcomes pull requests from community members. The code is available on the project’s GitHub. The paper Sockeye 3: Fast Neural Machine Translation with PyTorch is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Anyone who regularly uses machine translation systems will have noticed huge performance improvements over the last few years, attributable to neural network-based models that have largely replaced the previous generation of phrase-based systems. Introduced in 2018, Sockeye is an open-source framework that offers fast and reliable PyTorch implementation for neural machine translation (NMT) and has been powering Amazon Translate and other NMT applications. Sockeye 2 was released in 2020. In the new paper Sockeye 3: Fast Neural Machine Translation with PyTorch, an Amazon team presents the latest version of the Sockeye toolkit for efficient training of stronger and faster models. Sockeye 3 achieves speeds up to 126 percent faster than other PyTorch implementations on GPUs and up to 292 percent faster on CPUs. Sockeye 3 optimizes a distributed mixed precision training strategy to yield faster calculations and speedups by fitting larger batches into memory. Moreover, it can scale to any number of GPUs and any size of training data by launching separate training processes that use PyTorch’s distributed data parallelism to synchronize updates. For inference design, Sockeye 3 uses static computation graphs to minimize the impacts of dynamic shapes and data-dependent control flow, enabling it to trace various model components via PyTorch’s JIT compiler. The developers also maintain backward compatibility with Sockeye 2 MXNet models — all models that were trained with Sockeye 2 can be converted to models running on Sockeye 3 with PyTorch. Sockeye 3 also introduces many new advanced features: It supports replacing the decoder’s self-attention layers with Simpler Simple Recurrent Units (SSRUs) and fine-tuning with parameter freezing, and enables users to specify arbitrary prefixes (sequences of tokens) on both the source and target sides for any input. In their empirical studies, the team compared Sockeye with benchmark NMT toolkits that included Fairseq (Ott et al., 2019) and OpenNMT (Klein et al., 2017). In the evaluations, Sockeye 3 achieved comparable or better performance on GPUs and CPUs: delivering a 15 percent improvement for batched GPU inference, +126 percent for non-batched GPU inference, and +292 percent for CPU inference. Overall, Sockeye 3 provides much faster model implementations and more advanced features for NMT. As with previous versions, It has been open-sourced under an Apache 2.0 license, and the Amazon team welcomes pull requests from community members. The code is available on the project’s GitHub. The paper Sockeye 3: Fast Neural Machine Translation with PyTorch is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['regularly', 'use', 'machine', 'translation', 'system', 'notice', 'huge', 'performance', 'improvement', 'last', 'year', 'attributable', 'neural', 'networkbased', 'model', 'largely', 'replace', 'previous', 'generation', 'phrasebase', 'system', 'introduce', 'sockeye', 'opensource', 'framework', 'offer', 'fast', 'reliable', 'pytorch', 'implementation', 'neural', 'machine', 'translation', 'nmt', 'power', 'amazon', 'translate', 'nmt', 'application', 'sockeye', 'release', 'new', 'paper', 'sockeye', 'fast', 'neural', 'machine', 'translation', 'pytorch', 'amazon', 'team', 'present', 'late', 'version', 'sockeye', 'toolkit', 'efficient', 'training', 'strong', 'fast', 'model', 'sockeye', 'achieve', 'speed', 'percent', 'fast', 'pytorch', 'implementation', 'gpus', 'percent', 'fast', 'sockeye', 'optimize', 'distribute', 'mixed', 'precision', 'training', 'strategy', 'yield', 'fast', 'calculation', 'speedup', 'fit', 'large', 'batch', 'memory', 'moreover', 'scale', 'number', 'gpus', 'size', 'training', 'datum', 'launch', 'separate', 'training', 'process', 'use', 'pytorch', 'distribute', 'datum', 'parallelism', 'synchronize', 'update', 'inference', 'design', 'sockeye', 'use', 'static', 'computation', 'graph', 'minimize', 'impact', 'dynamic', 'shape', 'datadependent', 'control', 'flow', 'enable', 'trace', 'various', 'model', 'component', 'pytorch', 'developer', 'also', 'maintain', 'backward', 'compatibility', 'sockeye', 'mxnet', 'model', 'model', 'train', 'sockeye', 'convert', 'model', 'run', 'sockeye', 'pytorch', 'sockeye', 'also', 'introduce', 'many', 'new', 'advanced', 'feature', 'support', 'replace', 'decoder', 'layer', 'simple', 'simple', 'recurrent', 'unit', 'ssrus', 'finetune', 'parameter', 'freeze', 'enable', 'user', 'specify', 'arbitrary', 'prefix', 'sequence', 'token', 'source', 'target', 'side', 'input', 'empirical', 'study', 'team', 'compare', 'sockeye', 'benchmark', 'nmt', 'toolkit', 'include', 'fairseq', 'ott', 'klein', 'evaluation', 'sockeye', 'achieve', 'comparable', 'well', 'performance', 'cpus', 'deliver', 'percent', 'improvement', 'batched', 'inference', 'percent', 'nonbatched', 'inference', 'percent', 'cpu', 'inference', 'overall', 'sockeye', 'provide', 'much', 'fast', 'model', 'implementation', 'advanced', 'feature', 'nmt', 'previous', 'version', 'opensource', 'apache', 'license', 'amazon', 'team', 'pull', 'request', 'community', 'member', 'code', 'available', 'project', 'paper', 'sockeye', 'fast', 'neural', 'machine', 'translation', 'pytorch', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
Google Open-Sources Its TensorFlow GNN  Framework to Encourage Graph Neural Network Productization and Experimentation,https://syncedreview.com/2022/07/14/google-open-sources-its-tensorflow-gnn-framework-to-encourage-graph-neural-network-productization-and-experimentation/,2022-07-14,"
In the new paper TF-GNN: Graph Neural Networks in TensorFlow, a research team from Google Core ML, Google Research, and DeepMind open-sources the TensorFlow GNN (TF-GNN) scalable library, which leverages heterogeneous relational data to create graph neural network models.
","Graph Neural Networks (GNNs) that operate on graph-based data bring multimodal capabilities to machine learning models and have practical applications in areas as diverse as the modelling of physics systems, learning molecular fingerprints, predicting protein interfaces, classifying social networks and more. A key component for pushing GNN development is better software frameworks for learning from graph-structured data. In the new paper TF-GNN: Graph Neural Networks in TensorFlow, a research team from Google Core ML, Google Research, and DeepMind open-sources the TensorFlow GNN (TF-GNN) scalable library, which leverages heterogeneous relational data to create GNN models and enable GNN training and inference on arbitrary graph-structured data. The research team summarizes their main contributions as follows: TF-GNN includes four API components of varying abstraction levels to assist developers with varying machine learning expertise in creating graph models: 1) a data level for representing heterogeneous graphs and loading them into TensorFlow, which will appeal to proficient users; 2) a data exchange level for sending information between its nodes, edges, and the graph context, aimed at intermediate users; 3) a model level that offers trainable transformations of the data exchanged across the graphs; and 4) a minimal-code experience level for beginners, where an “Orchestrator” toolkit that includes popular graph learning objectives, distributed training capabilities and accelerator support — and can handle some of the vexing TensorFlow idiosyncrasies — enables simpler data input, feature processing, graph objectives, training and validation. TF-GNN models are currently being used by many Google teams, and the company hopes the library’s open-sourcing will facilitate the creation of GNNs for developers of all levels and push the industrial adaptation of these promising models at more organizations. The paper TF-GNN: Graph Neural Networks in TensorFlow is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Graph Neural Networks (GNNs) that operate on graph-based data bring multimodal capabilities to machine learning models and have practical applications in areas as diverse as the modelling of physics systems, learning molecular fingerprints, predicting protein interfaces, classifying social networks and more. A key component for pushing GNN development is better software frameworks for learning from graph-structured data. In the new paper TF-GNN: Graph Neural Networks in TensorFlow, a research team from Google Core ML, Google Research, and DeepMind open-sources the TensorFlow GNN (TF-GNN) scalable library, which leverages heterogeneous relational data to create GNN models and enable GNN training and inference on arbitrary graph-structured data. The research team summarizes their main contributions as follows: TF-GNN includes four API components of varying abstraction levels to assist developers with varying machine learning expertise in creating graph models: 1) a data level for representing heterogeneous graphs and loading them into TensorFlow, which will appeal to proficient users; 2) a data exchange level for sending information between its nodes, edges, and the graph context, aimed at intermediate users; 3) a model level that offers trainable transformations of the data exchanged across the graphs; and 4) a minimal-code experience level for beginners, where an “Orchestrator” toolkit that includes popular graph learning objectives, distributed training capabilities and accelerator support — and can handle some of the vexing TensorFlow idiosyncrasies — enables simpler data input, feature processing, graph objectives, training and validation. TF-GNN models are currently being used by many Google teams, and the company hopes the library’s open-sourcing will facilitate the creation of GNNs for developers of all levels and push the industrial adaptation of these promising models at more organizations. The paper TF-GNN: Graph Neural Networks in TensorFlow is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['graph', 'neural', 'network', 'gnn', 'operate', 'graphbased', 'datum', 'bring', 'multimodal', 'capability', 'machine', 'learning', 'model', 'practical', 'application', 'area', 'diverse', 'modelling', 'physics', 'system', 'learn', 'molecular', 'fingerprint', 'predict', 'protein', 'interface', 'classify', 'social', 'network', 'key', 'component', 'push', 'development', 'well', 'software', 'framework', 'learn', 'graphstructure', 'datum', 'new', 'paper', 'tfgnn', 'graph', 'neural', 'network', 'tensorflow', 'research', 'team', 'deepmind', 'opensource', 'tensorflow', 'tfgnn', 'scalable', 'library', 'leverage', 'heterogeneous', 'relational', 'datum', 'create', 'gnn', 'model', 'enable', 'gnn', 'training', 'inference', 'arbitrary', 'graphstructure', 'datum', 'research', 'team', 'summarize', 'main', 'contribution', 'follow', 'tfgnn', 'include', 'api', 'component', 'vary', 'abstraction', 'level', 'assist', 'developer', 'vary', 'machine', 'learning', 'expertise', 'create', 'graph', 'model', 'data', 'level', 'represent', 'heterogeneous', 'graph', 'load', 'tensorflow', 'appeal', 'proficient', 'user', 'datum', 'exchange', 'level', 'send', 'information', 'node', 'edge', 'graph', 'context', 'aim', 'intermediate', 'user', 'model', 'level', 'offer', 'trainable', 'transformation', 'datum', 'exchange', 'graph', 'minimalcode', 'experience', 'level', 'beginner', 'orchestrator', 'toolkit', 'include', 'popular', 'graph', 'learning', 'objective', 'distribute', 'training', 'capability', 'accelerator', 'support', 'handle', 'vexing', 'tensorflow', 'idiosyncrasy', 'enable', 'simple', 'datum', 'input', 'feature', 'processing', 'graph', 'objective', 'training', 'validation', 'tfgnn', 'model', 'currently', 'use', 'many', 'google', 'team', 'company', 'hope', 'opensourcing', 'facilitate', 'creation', 'gnn', 'developer', 'level', 'push', 'industrial', 'adaptation', 'promising', 'model', 'organization', 'paper', 'tfgnn', 'graph', 'neural', 'network', 'tensorflow', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
Colossal-AI Seamlessly Accelerates Large Models at Low Costs with Hugging Face,https://syncedreview.com/2022/07/13/colossal-ai-seamlessly-accelerates-large-models-at-low-costs-with-hugging-face/,2022-07-13,"
HPC-AI Tech’s flagship open-source and large-scale AI system, Colossal-AI, now allows Hugging Face users to seamlessly develop their ML models in a distributed and easy manner. 
","Forbes News, the world’s leading voice, recently declared large AI models as one of six AI trends to watch for in 2022. As large-scale AI models continue their superior performances across different domains, trends emerge, leading to distinguished and efficient AI applications that have never been seen in the industry. For example, Microsoft-owned GitHub and OpenAI partnered to launch Copilot recently. Copilot plays the role of an AI pair programmer, offering suggestions for code and entire functions in real-time. Such developments continue to make coding easier than before. Another example released by OpenAI, DALL-E 2, is a powerful tool that creates original and realistic images as well as art from only simple text. One month later, Google announced its own robust text-to-image diffusion model called Imagen. Imagen delivers exceptional results and accelerates the race of large AI models to a climax. Image Generated by Imagen (left 2 col.) vs DALLE-2 (right 2 col.)“Greek statue of a man tripping over a cat” In recent years, the outstanding performance of model scaling has led to an escalation in the size of pre-trained models. Unfortunately, training and even simply fine-tuning large AI models are usually unaffordable, requiring tens or hundreds of GPUs. Existing deep learning frameworks like PyTorch and Tensorflow may not offer a satisfactory solution for very large AI models. Furthermore, advanced knowledge of AI systems is typically required for sophisticated configurations and optimization of specific models. Therefore, many AI users, such as engineers from small and medium-sized enterprises, can’t help but feel overwhelmed by the emergence of large AI models. In fact, the core reasons for the increased cost of large AI models are GPU memory restrictions and the inability to accommodate sizeable models. In response to all of this, Colossal-AI developed the Gemini module, which efficiently manages and utilizes the heterogeneous memory of GPU and CPU and is expected to help solve the mentioned bottlenecks. Best of all, it is completely open-source and requires only minimal modifications to allow existing deep learning projects to be trained with much larger models on a single consumer-grade graphics card. In particular, it makes downstream tasks and application deployments such as large AI model fine-tuning and inference much easier. It even grants the convenience of training AI models at home! Hugging Face is a popular AI community that strives to advance and democratize AI through open source and open science. Hugging Face has had success collating large-scale models into their own model hub with over 50,000 models, including trendy large AI models like GPT and OPT. HPC-AI Tech’s flagship open-source and large-scale AI system, Colossal-AI, now allows Hugging Face users to seamlessly develop their ML models in a distributed and easy manner. In the following paragraphs, we will take one of the most popular AI models in Hugging Face Hub, OPT from Meta, to demonstrate how to train and fine-tune your large AI models at a low cost with minimal modifications to your code.Open source code: https://github.com/hpcaitech/ColossalAI Meta recently released Open Pretrained Transformer (OPT), a 175-Billion parameter AI language model. To encourage AI democratization in the community, Meta has released both the code and trained model weights, which stimulates AI programmers to perform various downstream tasks and application deployments. We will now demonstrate fine-tuning Casual Language Modelling with pre-training weights of the OPT model provided by Hugging Face Hub. It is very simple to use the powerful features of Colossal-AI. Users only need a simple configuration file, and are not required to alter their training logic to equip models with their desired features (e.g. mixed-precision training, gradient accumulation, multi-dimensional parallel training, and memory redundancy elimination).Suppose we intend to develop the OPT on one GPU. We can accomplish this by leveraging heterogeneous training from Colossal-AI, which only requires users to add relevant items to the configuration files. Among the items added, tensor_placement_policy, which can be configured as cuda, cpu, or auto, determines our heterogeneous training strategy. Each training strategy has its distinct advantages: For typical users, they can just select the auto strategy, which maximizes training efficiency by dynamically adapting its heterogeneous strategy with respect to its current memory state. With the configuration file ready, only a few lines of code are needed for the newly declared functions.Firstly, awaken Colossal-AI through a single line of code in the configuration file. Colossal-AI will automatically initialize the distributed environment, read in configuration settings, and integrate the configuration settings into its components (i.e. models and optimizers). After that, users may define their own datasets, models, optimizers, and loss functions per usual, or by using raw PyTorch code. Only their models need to be initialized under ZeroInitContext. In the given example, we adopt the OPTForCausalLM model along with its pretrained weights by Hugging Face and make adjustments to the Wikitext dataset.  Next, use colossalai.initialize to integrate heterogeneous memory functions defined in the configuration file, into the training engine to enable the feature. On a single GPU, Colossal-AI’s automatic strategy provides remarkable performance gains from the ZeRO Offloading strategy by Microsoft DeepSpeed. Users can experience up to a 40% speedup, at a variety of model scales. However, when using a traditional deep learning training framework like PyTorch, a single GPU can no longer support the training of models at such a scale. Adopting the distributed training strategy with 8 GPUs is as simple as adding a -nprocs 8 to the training command of Colossal-AI! Such remarkable improvements come from Colossal-AI’s efficient heterogeneous memory management system, Gemini. To put it simply, Gemini uses a few warmup steps during model training to collect memory usage information from PyTorch computational graphs. After warm-up, and before performing each operation, Gemini pre-allocates memory for the operator equivalent to its peak usage based on the collected memory usage records. At the same time, it re-allocates some model tensors from GPU memory to CPU memory. The inbuilt memory manager by Gemini attaches a state to each tensor, including HOLD, COMPUTE, FREE, etc. Based on the queried memory usage, the manager constantly converts the tensor states, and adjusts tensor positions. Compared to the static memory classification by DeepSpeed’s ZeRO Offload, Colossal-AI Gemini employs a more efficient use of GPU and CPU memory, maximizes model capacities, and balances training speeds, all with small amounts of hardware equipment. For the representative of large models, GPT, Colossal-AI is capable of training up to 1.5 billion parameters on a gaming laptop with RTX 2060 6GB. For a PC with RTX3090 24GB, Colossal-AI can train GPT with 18 billion parameters. Colossal-AI can also bring significant improvements to high performance graphics cards such as a Tesla V100. Parallel and distributed technologies are vital methods to further accelerate model training. To train the world’s largest and most advanced AI models within the shortest time, efficient distributed parallelization is still a necessity. Issues found in existing solutions include limited parallel dimension, low efficiency, poor versatility, difficult deployment, and lack of maintenance. With this in mind, Colossal-AI uses technologies such as efficient multi-dimensional parallelism and heterogeneous parallelism to allow users to deploy large AI models efficiently and rapidly with minimal modifications to their code. To counter complications arising from data, pipeline, and 2.5D parallelism simultaneously, a simple line of code declaration suffices with Colossal-AI. The typical system/framework method of hacking into underlined code logic is no longer necessary. For a super-large AI model such as GPT-3, Colossal-AI only needs half the computing resources compared to the NVIDIA solution to start training. If the same computing resources were used, the speed could be further increased by 11%, which could reduce the training cost of GPT-3 by over a million dollars. In theory, this sounds fantastic, but what about in practice? Colossal-AI has proven its capabilities in application to real-world issues across a variety of industries, including autonomous driving, cloud computing, retail, medicine, and chip production. For AlphaFold, which is used for protein structure prediction, our team has introduced FastFold, based on the Colossal-AI acceleration scheme. FastFold has successfully surpassed other schemes including those proposed by Google and Columbia University. It successfully reduces the training time of AlphaFold from 11 days to 67 hours, simultaneously lowering the overall cost. Moreover, the process of long sequence inference is accelerated by about 9.3 to 11.6 times. Colossal-AI values open-source community construction. We offer detailed tutorials and support the latest cutting-edge applications such as PaLM and AlphaFold. Colossal-AI will regularly produce new and innovative features. We always welcome suggestions and discussions and would be more than willing to help if you encounter any issues. You can raise an issue here or create a discussion topic in our forum. Your suggestions are highly appreciated. Recently, Colossal-AI reached No. 1 in trending projects on Github and Papers With Code, together with projects that have as many as 10K stars. The open-source code is on Project’s GitHub. Referencehttps://medium.com/@yangyou_berkeley/colossal-ai-seamlessly-accelerates-large-models-at-low-costs-with-hugging-face-4d1a887e500d  We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Forbes News, the world’s leading voice, recently declared large AI models as one of six AI trends to watch for in 2022. As large-scale AI models continue their superior performances across different domains, trends emerge, leading to distinguished and efficient AI applications that have never been seen in the industry. For example, Microsoft-owned GitHub and OpenAI partnered to launch Copilot recently. Copilot plays the role of an AI pair programmer, offering suggestions for code and entire functions in real-time. Such developments continue to make coding easier than before. Another example released by OpenAI, DALL-E 2, is a powerful tool that creates original and realistic images as well as art from only simple text. One month later, Google announced its own robust text-to-image diffusion model called Imagen. Imagen delivers exceptional results and accelerates the race of large AI models to a climax. Image Generated by Imagen (left 2 col.) vs DALLE-2 (right 2 col.)“Greek statue of a man tripping over a cat” In recent years, the outstanding performance of model scaling has led to an escalation in the size of pre-trained models. Unfortunately, training and even simply fine-tuning large AI models are usually unaffordable, requiring tens or hundreds of GPUs. Existing deep learning frameworks like PyTorch and Tensorflow may not offer a satisfactory solution for very large AI models. Furthermore, advanced knowledge of AI systems is typically required for sophisticated configurations and optimization of specific models. Therefore, many AI users, such as engineers from small and medium-sized enterprises, can’t help but feel overwhelmed by the emergence of large AI models. In fact, the core reasons for the increased cost of large AI models are GPU memory restrictions and the inability to accommodate sizeable models. In response to all of this, Colossal-AI developed the Gemini module, which efficiently manages and utilizes the heterogeneous memory of GPU and CPU and is expected to help solve the mentioned bottlenecks. Best of all, it is completely open-source and requires only minimal modifications to allow existing deep learning projects to be trained with much larger models on a single consumer-grade graphics card. In particular, it makes downstream tasks and application deployments such as large AI model fine-tuning and inference much easier. It even grants the convenience of training AI models at home! Hugging Face is a popular AI community that strives to advance and democratize AI through open source and open science. Hugging Face has had success collating large-scale models into their own model hub with over 50,000 models, including trendy large AI models like GPT and OPT. HPC-AI Tech’s flagship open-source and large-scale AI system, Colossal-AI, now allows Hugging Face users to seamlessly develop their ML models in a distributed and easy manner. In the following paragraphs, we will take one of the most popular AI models in Hugging Face Hub, OPT from Meta, to demonstrate how to train and fine-tune your large AI models at a low cost with minimal modifications to your code.Open source code: https://github.com/hpcaitech/ColossalAI Meta recently released Open Pretrained Transformer (OPT), a 175-Billion parameter AI language model. To encourage AI democratization in the community, Meta has released both the code and trained model weights, which stimulates AI programmers to perform various downstream tasks and application deployments. We will now demonstrate fine-tuning Casual Language Modelling with pre-training weights of the OPT model provided by Hugging Face Hub. It is very simple to use the powerful features of Colossal-AI. Users only need a simple configuration file, and are not required to alter their training logic to equip models with their desired features (e.g. mixed-precision training, gradient accumulation, multi-dimensional parallel training, and memory redundancy elimination).Suppose we intend to develop the OPT on one GPU. We can accomplish this by leveraging heterogeneous training from Colossal-AI, which only requires users to add relevant items to the configuration files. Among the items added, tensor_placement_policy, which can be configured as cuda, cpu, or auto, determines our heterogeneous training strategy. Each training strategy has its distinct advantages: For typical users, they can just select the auto strategy, which maximizes training efficiency by dynamically adapting its heterogeneous strategy with respect to its current memory state. With the configuration file ready, only a few lines of code are needed for the newly declared functions.Firstly, awaken Colossal-AI through a single line of code in the configuration file. Colossal-AI will automatically initialize the distributed environment, read in configuration settings, and integrate the configuration settings into its components (i.e. models and optimizers). After that, users may define their own datasets, models, optimizers, and loss functions per usual, or by using raw PyTorch code. Only their models need to be initialized under ZeroInitContext. In the given example, we adopt the OPTForCausalLM model along with its pretrained weights by Hugging Face and make adjustments to the Wikitext dataset. Next, use colossalai.initialize to integrate heterogeneous memory functions defined in the configuration file, into the training engine to enable the feature. On a single GPU, Colossal-AI’s automatic strategy provides remarkable performance gains from the ZeRO Offloading strategy by Microsoft DeepSpeed. Users can experience up to a 40% speedup, at a variety of model scales. However, when using a traditional deep learning training framework like PyTorch, a single GPU can no longer support the training of models at such a scale. Adopting the distributed training strategy with 8 GPUs is as simple as adding a -nprocs 8 to the training command of Colossal-AI! Such remarkable improvements come from Colossal-AI’s efficient heterogeneous memory management system, Gemini. To put it simply, Gemini uses a few warmup steps during model training to collect memory usage information from PyTorch computational graphs. After warm-up, and before performing each operation, Gemini pre-allocates memory for the operator equivalent to its peak usage based on the collected memory usage records. At the same time, it re-allocates some model tensors from GPU memory to CPU memory. The inbuilt memory manager by Gemini attaches a state to each tensor, including HOLD, COMPUTE, FREE, etc. Based on the queried memory usage, the manager constantly converts the tensor states, and adjusts tensor positions. Compared to the static memory classification by DeepSpeed’s ZeRO Offload, Colossal-AI Gemini employs a more efficient use of GPU and CPU memory, maximizes model capacities, and balances training speeds, all with small amounts of hardware equipment. For the representative of large models, GPT, Colossal-AI is capable of training up to 1.5 billion parameters on a gaming laptop with RTX 2060 6GB. For a PC with RTX3090 24GB, Colossal-AI can train GPT with 18 billion parameters. Colossal-AI can also bring significant improvements to high performance graphics cards such as a Tesla V100. Parallel and distributed technologies are vital methods to further accelerate model training. To train the world’s largest and most advanced AI models within the shortest time, efficient distributed parallelization is still a necessity. Issues found in existing solutions include limited parallel dimension, low efficiency, poor versatility, difficult deployment, and lack of maintenance. With this in mind, Colossal-AI uses technologies such as efficient multi-dimensional parallelism and heterogeneous parallelism to allow users to deploy large AI models efficiently and rapidly with minimal modifications to their code. To counter complications arising from data, pipeline, and 2.5D parallelism simultaneously, a simple line of code declaration suffices with Colossal-AI. The typical system/framework method of hacking into underlined code logic is no longer necessary. For a super-large AI model such as GPT-3, Colossal-AI only needs half the computing resources compared to the NVIDIA solution to start training. If the same computing resources were used, the speed could be further increased by 11%, which could reduce the training cost of GPT-3 by over a million dollars. In theory, this sounds fantastic, but what about in practice? Colossal-AI has proven its capabilities in application to real-world issues across a variety of industries, including autonomous driving, cloud computing, retail, medicine, and chip production. For AlphaFold, which is used for protein structure prediction, our team has introduced FastFold, based on the Colossal-AI acceleration scheme. FastFold has successfully surpassed other schemes including those proposed by Google and Columbia University. It successfully reduces the training time of AlphaFold from 11 days to 67 hours, simultaneously lowering the overall cost. Moreover, the process of long sequence inference is accelerated by about 9.3 to 11.6 times. Colossal-AI values open-source community construction. We offer detailed tutorials and support the latest cutting-edge applications such as PaLM and AlphaFold. Colossal-AI will regularly produce new and innovative features. We always welcome suggestions and discussions and would be more than willing to help if you encounter any issues. You can raise an issue here or create a discussion topic in our forum. Your suggestions are highly appreciated. Recently, Colossal-AI reached No. 1 in trending projects on Github and Papers With Code, together with projects that have as many as 10K stars. The open-source code is on Project’s GitHub. Referencehttps://medium.com/@yangyou_berkeley/colossal-ai-seamlessly-accelerates-large-models-at-low-costs-with-hugging-face-4d1a887e500d We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['forbe', 'world', 'lead', 'voice', 'recently', 'declare', 'large', 'ai', 'model', 'ai', 'trend', 'watch', 'ai', 'model', 'continue', 'superior', 'performance', 'different', 'domain', 'trend', 'emerge', 'lead', 'distinguished', 'efficient', 'ai', 'application', 'never', 'see', 'industry', 'example', 'microsoftowne', 'github', 'openai', 'partner', 'launch', 'copilot', 'recently', 'copilot', 'play', 'role', 'ai', 'pair', 'programmer', 'offer', 'suggestion', 'code', 'entire', 'function', 'realtime', 'development', 'continue', 'make', 'code', 'easy', 'example', 'release', 'openai', 'dalle', 'powerful', 'tool', 'create', 'original', 'realistic', 'image', 'well', 'art', 'simple', 'text', 'month', 'later', 'announce', 'robust', 'texttoimage', 'diffusion', 'model', 'call', 'imagen', 'imagen', 'deliver', 'exceptional', 'result', 'accelerate', 'race', 'large', 'ai', 'model', 'climax', 'image', 'generate', 'imagen', 'leave', 'col', 'dalle2', 'right', 'colgreek', 'statue', 'man', 'trip', 'cat', 'recent', 'year', 'outstanding', 'performance', 'model', 'scaling', 'lead', 'escalation', 'size', 'pretraine', 'model', 'unfortunately', 'train', 'even', 'simply', 'finetune', 'large', 'ai', 'model', 'usually', 'unaffordable', 'require', 'ten', 'hundred', 'exist', 'deep', 'learning', 'framework', 'pytorch', 'tensorflow', 'offer', 'satisfactory', 'solution', 'large', 'ai', 'model', 'furthermore', 'advanced', 'knowledge', 'system', 'typically', 'require', 'sophisticated', 'configuration', 'optimization', 'specific', 'model', 'therefore', 'many', 'ai', 'user', 'engineer', 'small', 'mediumsize', 'enterprise', 'help', 'feel', 'overwhelmed', 'emergence', 'large', 'ai', 'model', 'fact', 'core', 'reason', 'increase', 'cost', 'large', 'ai', 'model', 'memory', 'restriction', 'inability', 'accommodate', 'sizeable', 'model', 'response', 'colossalai', 'develop', 'gemini', 'module', 'efficiently', 'manage', 'utilize', 'heterogeneous', 'memory', 'cpu', 'expect', 'help', 'solve', 'mention', 'bottleneck', 'well', 'completely', 'opensource', 'require', 'minimal', 'modification', 'allow', 'exist', 'deep', 'learning', 'project', 'train', 'much', 'large', 'model', 'single', 'consumergrade', 'graphic', 'card', 'particular', 'make', 'downstream', 'task', 'application', 'deployment', 'large', 'model', 'finetune', 'inference', 'much', 'easy', 'even', 'grant', 'convenience', 'training', 'ai', 'model', 'home', 'hugging', 'face', 'popular', 'ai', 'community', 'strive', 'advance', 'democratize', 'ai', 'open', 'source', 'open', 'science', 'hugging', 'face', 'success', 'collate', 'largescale', 'model', 'model', 'hub', 'model', 'include', 'trendy', 'large', 'ai', 'model', 'flagship', 'opensource', 'largescale', 'ai', 'system', 'allow', 'hugging', 'face', 'user', 'seamlessly', 'develop', 'ml', 'model', 'distribute', 'easy', 'manner', 'following', 'paragraph', 'take', 'popular', 'ai', 'model', 'hug', 'face', 'opt', 'demonstrate', 'train', 'finetune', 'large', 'ai', 'model', 'low', 'cost', 'minimal', 'modification', 'codeopen', 'source', 'code', 'meta', 'recently', 'release', 'open', 'pretraine', 'transformer', 'opt', '175billion', 'parameter', 'ai', 'language', 'model', 'encourage', 'democratization', 'community', 'meta', 'release', 'code', 'train', 'model', 'weight', 'stimulate', 'ai', 'programmer', 'perform', 'various', 'downstream', 'task', 'application', 'deployment', 'demonstrate', 'finetune', 'casual', 'language', 'modelling', 'pretraine', 'weight', 'opt', 'model', 'provide', 'hug', 'face', 'hub', 'simple', 'use', 'powerful', 'feature', 'colossalai', 'user', 'need', 'simple', 'configuration', 'file', 'require', 'alter', 'training', 'logic', 'model', 'desire', 'feature', 'eg', 'mixedprecision', 'training', 'gradient', 'accumulation', 'multidimensional', 'parallel', 'training', 'memory', 'redundancy', 'eliminationsuppose', 'intend', 'develop', 'opt', 'gpu', 'accomplish', 'leverage', 'heterogeneous', 'training', 'colossalai', 'require', 'user', 'add', 'relevant', 'item', 'configuration', 'file', 'item', 'add', 'tensorplacementpolicy', 'configure', 'cuda', 'cpu', 'auto', 'determine', 'heterogeneous', 'training', 'strategy', 'training', 'strategy', 'distinct', 'advantage', 'typical', 'user', 'select', 'auto', 'strategy', 'maximize', 'training', 'efficiency', 'dynamically', 'adapt', 'heterogeneous', 'strategy', 'respect', 'current', 'memory', 'state', 'configuration', 'file', 'ready', 'line', 'code', 'need', 'newly', 'declare', 'functionsfirstly', 'awaken', 'colossalai', 'single', 'line', 'code', 'configuration', 'file', 'colossalai', 'automatically', 'initialize', 'distribute', 'environment', 'read', 'configuration', 'setting', 'integrate', 'configuration', 'setting', 'component', 'model', 'optimizer', 'user', 'define', 'dataset', 'model', 'optimizer', 'loss', 'function', 'usual', 'use', 'raw', 'pytorch', 'code', 'model', 'need', 'initialize', 'zeroinitcontext', 'give', 'example', 'adopt', 'optforcausallm', 'model', 'pretraine', 'weight', 'hug', 'face', 'make', 'adjustment', 'wikitext', 'dataset', 'next', 'use', 'colossalaiinitialize', 'integrate', 'heterogeneous', 'memory', 'function', 'define', 'configuration', 'file', 'training', 'engine', 'enable', 'feature', 'single', 'automatic', 'strategy', 'provide', 'remarkable', 'performance', 'gain', 'offload', 'strategy', 'deepspee', 'user', 'experience', 'speedup', 'variety', 'model', 'scale', 'however', 'use', 'traditional', 'deep', 'learning', 'training', 'framework', 'pytorch', 'single', 'gpu', 'long', 'support', 'training', 'model', 'scale', 'adopt', 'distribute', 'training', 'strategy', 'gpus', 'simple', 'add', 'nprocs', 'training', 'command', 'remarkable', 'improvement', 'come', 'efficient', 'heterogeneous', 'memory', 'management', 'system', 'gemini', 'put', 'simply', 'gemini', 'use', 'warmup', 'step', 'model', 'training', 'collect', 'memory', 'usage', 'information', 'pytorch', 'computational', 'graph', 'perform', 'operation', 'gemini', 'preallocate', 'memory', 'operator', 'equivalent', 'peak', 'usage', 'base', 'collected', 'memory', 'usage', 'record', 'time', 'reallocate', 'model', 'tensor', 'memory', 'cpu', 'memory', 'inbuilt', 'memory', 'manager', 'state', 'tensor', 'include', 'hold', 'compute', 'free', 'etc', 'base', 'query', 'memory', 'usage', 'manager', 'constantly', 'convert', 'tensor', 'state', 'adjust', 'tensor', 'position', 'compare', 'static', 'memory', 'classification', 'offload', 'employ', 'efficient', 'use', 'cpu', 'memory', 'maximize', 'model', 'capacity', 'balance', 'training', 'speed', 'small', 'amount', 'hardware', 'equipment', 'representative', 'large', 'model', 'capable', 'training', 'parameter', 'gaming', 'laptop', 'rtx', 'gb', 'pc', 'rtx3090', 'colossalai', 'train', 'parameter', 'colossalai', 'also', 'bring', 'significant', 'improvement', 'high', 'performance', 'graphic', 'card', 'tesla', 'parallel', 'distribute', 'technology', 'vital', 'method', 'far', 'accelerate', 'model', 'training', 'train', 'world', 'large', 'advanced', 'ai', 'model', 'short', 'time', 'efficient', 'distribute', 'parallelization', 'still', 'necessity', 'issue', 'find', 'exist', 'solution', 'include', 'limited', 'parallel', 'dimension', 'low', 'efficiency', 'poor', 'versatility', 'difficult', 'deployment', 'lack', 'maintenance', 'mind', 'colossalai', 'use', 'technology', 'efficient', 'multidimensional', 'parallelism', 'heterogeneous', 'parallelism', 'allow', 'user', 'deploy', 'large', 'ai', 'model', 'efficiently', 'rapidly', 'minimal', 'modification', 'code', 'counter', 'complication', 'arise', 'datum', 'pipeline', '25d', 'parallelism', 'simultaneously', 'simple', 'line', 'code', 'declaration', 'suffice', 'colossalai', 'typical', 'systemframework', 'method', 'hack', 'underlined', 'code', 'logic', 'long', 'necessary', 'superlarge', 'ai', 'model', 'need', 'compute', 'resource', 'compare', 'solution', 'start', 'train', 'computing', 'resource', 'use', 'speed', 'far', 'increase', 'reduce', 'training', 'cost', 'gpt3', 'dollar', 'theory', 'sound', 'fantastic', 'practice', 'prove', 'capability', 'application', 'realworld', 'issue', 'variety', 'industry', 'include', 'autonomous', 'drive', 'cloud', 'compute', 'retail', 'medicine', 'chip', 'production', 'alphafold', 'use', 'protein', 'structure', 'prediction', 'team', 'introduce', 'fastfold', 'base', 'acceleration', 'scheme', 'fastfold', 'successfully', 'surpass', 'scheme', 'include', 'propose', 'successfully', 'reduce', 'training', 'time', 'alphafold', 'day', 'hour', 'simultaneously', 'lower', 'overall', 'cost', 'moreover', 'process', 'long', 'sequence', 'inference', 'accelerate', 'time', 'colossalai', 'value', 'opensource', 'community', 'construction', 'offer', 'detailed', 'tutorial', 'support', 'late', 'cuttingedge', 'application', 'palm', 'alphafold', 'regularly', 'produce', 'new', 'innovative', 'feature', 'always', 'welcome', 'suggestion', 'discussion', 'willing', 'help', 'encounter', 'issue', 'raise', 'issue', 'create', 'discussion', 'topic', 'forum', 'suggestion', 'highly', 'appreciate', 'recently', 'reach', 'trend', 'project', 'github', 'paper', 'code', 'together', 'project', 'many', 'star', 'opensource', 'code', 'project', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
"Academia Sinica’s YOLOv7 Outperforms All Object Detectors, Reduces Costs by 50%",https://syncedreview.com/2022/07/12/academia-sinicas-yolov7-outperforms-all-object-detectors-reduces-costs-by-50/,2022-07-12,"
 In the new paper YOLOv7: Trainable Bag-Of-Freebies Sets New State-Of-The-Art for Real-Time Object Detectors, an Academia Sinica research team releases YOLOv7. This latest YOLO version introduces novel “extend” and “compound scaling” methods that effectively utilize parameters and computation; and surpasses all known real-time object detectors in speed and accuracy.
","The 2016 release at CVPR of the YOLO (You Only Look Once) real-time object detector revolutionized the field of computer vision. YOLO delivered unprecedented speed and accuracy on a fundamental task with applications in autonomous driving, robotics, security, medical image analysis and more. Various techniques and tricks (multi-scale predictions, a better backbone classifier, etc.) have since been implemented to improve YOLO training and boost performance. A research team from Taiwan’s Institute of Information Science, Academia Sinica furthers YOLO development in their new paper YOLOv7: Trainable Bag-Of-Freebies Sets New State-Of-The-Art for Real-Time Object Detectors. This latest YOLO version introduces novel “extend” and “compound scaling” methods that effectively utilize parameters and computation; and surpasses all known real-time object detectors in speed and accuracy. The team summarizes their main contributions as: The team starts by building an efficient architecture. Their extended efficient layer aggregation network (Extended-ELAN, or E-ELAN) uses expand, shuffle, and merge cardinality to continuously enhance the network’s learning ability without changing the original gradient path, i.e. it changes only the computational block and leaves the transition layer untouched. For model scaling, the researchers propose a compound scaling method that can be applied to concatenation-based architectures and calculate changes in the output channel of a computational block to enable depth factor scaling. This proposed compound scaling method can thus maintain the properties of the original model design and the optimal structure. In their empirical study, the researchers compared the proposed YOLOv7 with state-of-the-art object detectors. YOLOv7 achieved 1.5 percent higher AP than YOLOv4 despite having 75 percent fewer parameters and using 36 percent less computation. When trained only on the MS COCO dataset and without any pretrained weights, YOLOv7 beat all other popular detectors (YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B) in the evaluations. The YOLOv7 source code has been released on the project’s GitHub. The paper YOLOv7: Trainable Bag-Of-Freebies Sets New State-Of-The-Art for Real-Time Object Detectors is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","The 2016 release at CVPR of the YOLO (You Only Look Once) real-time object detector revolutionized the field of computer vision. YOLO delivered unprecedented speed and accuracy on a fundamental task with applications in autonomous driving, robotics, security, medical image analysis and more. Various techniques and tricks (multi-scale predictions, a better backbone classifier, etc.) have since been implemented to improve YOLO training and boost performance. A research team from Taiwan’s Institute of Information Science, Academia Sinica furthers YOLO development in their new paper YOLOv7: Trainable Bag-Of-Freebies Sets New State-Of-The-Art for Real-Time Object Detectors. This latest YOLO version introduces novel “extend” and “compound scaling” methods that effectively utilize parameters and computation; and surpasses all known real-time object detectors in speed and accuracy. The team summarizes their main contributions as: The team starts by building an efficient architecture. Their extended efficient layer aggregation network (Extended-ELAN, or E-ELAN) uses expand, shuffle, and merge cardinality to continuously enhance the network’s learning ability without changing the original gradient path, i.e. it changes only the computational block and leaves the transition layer untouched. For model scaling, the researchers propose a compound scaling method that can be applied to concatenation-based architectures and calculate changes in the output channel of a computational block to enable depth factor scaling. This proposed compound scaling method can thus maintain the properties of the original model design and the optimal structure. In their empirical study, the researchers compared the proposed YOLOv7 with state-of-the-art object detectors. YOLOv7 achieved 1.5 percent higher AP than YOLOv4 despite having 75 percent fewer parameters and using 36 percent less computation. When trained only on the MS COCO dataset and without any pretrained weights, YOLOv7 beat all other popular detectors (YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B) in the evaluations. The YOLOv7 source code has been released on the project’s GitHub. The paper YOLOv7: Trainable Bag-Of-Freebies Sets New State-Of-The-Art for Real-Time Object Detectors is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['release', 'cvpr', 'yolo', 'look', 'realtime', 'object', 'detector', 'revolutionize', 'field', 'computer', 'vision', 'deliver', 'unprecedented', 'speed', 'accuracy', 'fundamental', 'task', 'application', 'autonomous', 'drive', 'robotic', 'security', 'medical', 'image', 'analysis', 'various', 'technique', 'trick', 'multiscale', 'prediction', 'well', 'backbone', 'classifier', 'implement', 'improve', 'yolo', 'training', 'boost', 'performance', 'research', 'team', 'information', 'development', 'new', 'paper', 'yolov7', 'trainable', 'bagoffreebie', 'set', 'new', 'stateoftheart', 'realtime', 'object', 'detector', 'late', 'yolo', 'version', 'introduce', 'novel', 'extend', 'compound', 'scaling', 'method', 'effectively', 'utilize', 'parameter', 'computation', 'surpass', 'know', 'realtime', 'object', 'detector', 'speed', 'accuracy', 'team', 'summarize', 'main', 'contribution', 'team', 'start', 'build', 'efficient', 'architecture', 'extended', 'efficient', 'layer', 'aggregation', 'network', 'extendedelan', 'eelan', 'use', 'expand', 'shuffle', 'merge', 'cardinality', 'continuously', 'enhance', 'network', 'learn', 'ability', 'change', 'original', 'gradient', 'path', 'change', 'computational', 'block', 'leave', 'transition', 'layer', 'untouche', 'model', 'scale', 'researcher', 'propose', 'compound', 'scaling', 'method', 'apply', 'concatenationbase', 'architecture', 'calculate', 'change', 'output', 'channel', 'computational', 'block', 'enable', 'depth', 'factor', 'scale', 'propose', 'compound', 'scaling', 'method', 'thus', 'maintain', 'property', 'original', 'model', 'design', 'optimal', 'structure', 'empirical', 'study', 'researcher', 'compare', 'propose', 'yolov7', 'stateoftheart', 'object', 'detector', 'yolov7', 'achieve', 'percent', 'high', 'ap', 'yolov4', 'percent', 'parameter', 'use', 'percent', 'less', 'computation', 'train', 'dataset', 'pretraine', 'weight', 'yolov7', 'beat', 'popular', 'detector', 'yolor', 'deformable', 'detr', 'vitadapterb', 'evaluation', 'yolov7', 'source', 'code', 'release', 'project', 'paper', 'yolov7', 'trainable', 'bagoffreebie', 'set', 'new', 'stateoftheart', 'realtime', 'object', 'detector', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
A New Network Design Direction? DeepMind Examines How Networks Generalize and Climb the Chomsky Hierarchy,https://syncedreview.com/2022/07/11/a-new-network-design-direction-deepmind-examines-how-networks-generalize-and-climb-the-chomsky-hierarchy/,2022-07-11,"
In the new paper Neural Networks and the Chomsky Hierarchy, DeepMind researchers examine generalization in neural network architectures and whether insights from the theory of computation and the Chomsky hierarchy can predict the practical limits of network generalization.
","Reliable generalization to out-of-distribution inputs is a crucial feature for developing strong machine learning models. But determining how and why neural networks are able to generalize on algorithmic sequence prediction tasks remains an open question. In the new paper Neural Networks and the Chomsky Hierarchy, a DeepMind research team conducts an extensive generalization study on neural network architectures that explores whether insights from the theory of computation and the Chomsky hierarchy can predict the practical limits of neural network generalization. The team summarizes their main contributions as: Many previous works have investigated whether conventional neural network architectures are able to learn a formal language. While these studies have typically focused on single architectures and a limited set of tasks, the DeepMind paper presents an extensive empirical study on a wide range of models with regard to the Chomsky hierarchy. Named after the influential American linguist and philosopher who developed it, the Chomsky hierarchy is basically a containment hierarchy of formal grammar (unrestricted grammar, context-sensitive grammar, context-free grammar, and regular grammar) that classifies languages based on the type of automaton able to recognize them. By relating different models to the Chomsky hierarchy, it is possible to determine whether they can recognize certain regular languages. The researchers note that lower-level automata have restrictive memory models and can only solve lower-level problem sets, while atop the hierarchy, Turing machines with infinite memory and unrestricted memory access can solve all computable problems, i.e. are Turing complete. The paper examines a wide range of neural network architectures and memory-augmented neural networks — transformer, RNN, LSTM, Stack-RNN, NDStack-RNN and Tape-RNN — covering a total of 2200 models applied to 16 sequence-prediction tasks. The results show that LSTMs and transformers are not Turing complete as they cannot solve simple sequence tasks such as duplicating a string when the sequences are significantly longer than those seen during training. Models interacting with an external memory structures meanwhile can climb the Chomsky hierarchy, indicating this setup as a promising research direction for improving architecture design.The code is publicly available on the project’s GitHub. The paper Neural Networks and the Chomsky Hierarchy is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Reliable generalization to out-of-distribution inputs is a crucial feature for developing strong machine learning models. But determining how and why neural networks are able to generalize on algorithmic sequence prediction tasks remains an open question. In the new paper Neural Networks and the Chomsky Hierarchy, a DeepMind research team conducts an extensive generalization study on neural network architectures that explores whether insights from the theory of computation and the Chomsky hierarchy can predict the practical limits of neural network generalization. The team summarizes their main contributions as: Many previous works have investigated whether conventional neural network architectures are able to learn a formal language. While these studies have typically focused on single architectures and a limited set of tasks, the DeepMind paper presents an extensive empirical study on a wide range of models with regard to the Chomsky hierarchy. Named after the influential American linguist and philosopher who developed it, the Chomsky hierarchy is basically a containment hierarchy of formal grammar (unrestricted grammar, context-sensitive grammar, context-free grammar, and regular grammar) that classifies languages based on the type of automaton able to recognize them. By relating different models to the Chomsky hierarchy, it is possible to determine whether they can recognize certain regular languages. The researchers note that lower-level automata have restrictive memory models and can only solve lower-level problem sets, while atop the hierarchy, Turing machines with infinite memory and unrestricted memory access can solve all computable problems, i.e. are Turing complete. The paper examines a wide range of neural network architectures and memory-augmented neural networks — transformer, RNN, LSTM, Stack-RNN, NDStack-RNN and Tape-RNN — covering a total of 2200 models applied to 16 sequence-prediction tasks. The results show that LSTMs and transformers are not Turing complete as they cannot solve simple sequence tasks such as duplicating a string when the sequences are significantly longer than those seen during training. Models interacting with an external memory structures meanwhile can climb the Chomsky hierarchy, indicating this setup as a promising research direction for improving architecture design.The code is publicly available on the project’s GitHub. The paper Neural Networks and the Chomsky Hierarchy is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['reliable', 'generalization', 'outofdistribution', 'input', 'crucial', 'feature', 'develop', 'strong', 'machine', 'learning', 'model', 'determine', 'neural', 'network', 'able', 'generalize', 'algorithmic', 'sequence', 'prediction', 'task', 'remain', 'open', 'question', 'new', 'paper', 'neural', 'network', 'chomsky', 'hierarchy', 'deepmind', 'research', 'team', 'conduct', 'extensive', 'generalization', 'study', 'neural', 'network', 'architecture', 'explore', 'insight', 'theory', 'computation', 'chomsky', 'hierarchy', 'predict', 'practical', 'limit', 'neural', 'network', 'generalization', 'team', 'summarize', 'main', 'contribution', 'many', 'previous', 'work', 'investigate', 'conventional', 'neural', 'network', 'architecture', 'able', 'learn', 'formal', 'language', 'study', 'typically', 'focus', 'single', 'architecture', 'limited', 'set', 'task', 'deepmind', 'paper', 'present', 'extensive', 'empirical', 'study', 'wide', 'range', 'model', 'regard', 'chomsky', 'hierarchy', 'name', 'influential', 'american', 'linguist', 'philosopher', 'develop', 'chomsky', 'hierarchy', 'basically', 'containment', 'hierarchy', 'formal', 'grammar', 'unrestricted', 'grammar', 'contextsensitive', 'grammar', 'contextfree', 'grammar', 'regular', 'grammar', 'classify', 'language', 'base', 'type', 'able', 'recognize', 'relate', 'different', 'model', 'chomsky', 'hierarchy', 'possible', 'determine', 'recognize', 'certain', 'regular', 'language', 'researcher', 'note', 'lowerlevel', 'automata', 'restrictive', 'memory', 'model', 'solve', 'lowerlevel', 'problem', 'set', 'hierarchy', 'ture', 'machine', 'infinite', 'memory', 'unrestricted', 'memory', 'access', 'solve', 'computable', 'problem', 'ture', 'complete', 'paper', 'examine', 'wide', 'range', 'neural', 'network', 'architecture', 'memoryaugmented', 'neural', 'network', 'transformer', 'rnn', 'lstm', 'stackrnn', 'ndstackrnn', 'tapernn', 'cover', 'total', 'model', 'apply', 'sequenceprediction', 'task', 'result', 'show', 'lstms', 'transformer', 'ture', 'complete', 'solve', 'simple', 'sequence', 'task', 'duplicate', 'string', 'sequence', 'significantly', 'long', 'see', 'training', 'model', 'interact', 'external', 'memory', 'structure', 'meanwhile', 'climb', 'chomsky', 'hierarchy', 'indicate', 'setup', 'promising', 'research', 'direction', 'improve', 'architecture', 'designthe', 'code', 'publicly', 'available', 'project', 'paper', 'neural', 'network', 'chomsky', 'hierarchy', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
Salesforce’s CodeRL Achieves SOTA Code Generation Results With Strong Zero-Shot Transfer Capabilities,https://syncedreview.com/2022/07/07/salesforces-coderl-achieves-sota-code-generation-results-with-strong-zero-shot-transfer-capabilities/,2022-07-07,"
In the new paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning, a Salesforce Research team presents CodeRL, a novel framework for program synthesis tasks that employs pretrained language models (LMs) and deep reinforcement learning (RL) and achieves state-of-the-art performance on the challenging APPS benchmark while also demonstrating impressive zero-shot transfer capabilities. 
","Large-scale pretrained language models (LMs) have shown promising results on simple code generation tasks, but they have several limitations: training models with only next-token prediction objectives leads to accumulating errors, and neglecting potentially meaningful signals from unit tests results in poor generalization capability when facing complex unseen coding tasks. A Salesforce Research team addresses these issues in the new paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning, proposing CodeRL, a novel framework for program synthesis tasks that employs pretrained LMs and deep reinforcement learning (RL) and achieves state-of-the-art performance on the challenging APPS benchmark while demonstrating impressive zero-shot transfer capabilities. The team extends the Salesforce CodeT5 (Wang et al., 2021) unified pretrained encoder-decoder transformer architecture as CodeRL’s backbone. Although CodeT5 pretraining tasks such as masked span prediction (MSP) can benefit code understanding tasks, they do not necessarily align with program synthesis objectives. To mitigate this, a next-token prediction (NTP) pretraining task is integrated into CodeT5 to uniformly sample a pivot location for each code sample, then pass the content preceding the pivot to the encoder and the remaining content to the decoder. The researchers formulate CodeRL’s program synthesis as an RL problem and introduce an actor-critic approach to improve model performance by utilizing the unit test signals in both the model optimization and generation processes. The team conducted experiments on the challenging APPS (Automated Programming Progress Standard) code generation benchmark (Hendrycks et al., 2021) to evaluate the performance of the proposed CodeRL; and used the MBPP (Mostly Basic Programming Problems) benchmark (Austin et al., 2021) to evaluate its zero-shot ability. On APPS, researchers compared their models with strong conventional baselines that included GPT-2, GPT-Neo, GPT3, Codex, and AlphaCode, where CodeRL with CodeT5 achieved new SOTA results of 2.69 percent pass@1, 6.81 percent pass@5, and 20.98 percent pass@1000. On MBPP, CodeRL with CodeT5 obtained surprisingly good zero-shot performance, achieving a new SOTA of 63.0 percent pass@80 over GPT-137B’s 61.4 percent pass@80. This work shows that the CodeRL method can effectively leverage unit test signals to push code generation performance to new SOTA performance and achieve strong zero-shot transfer capabilities. The code is available on the project’s GitHub. The paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Large-scale pretrained language models (LMs) have shown promising results on simple code generation tasks, but they have several limitations: training models with only next-token prediction objectives leads to accumulating errors, and neglecting potentially meaningful signals from unit tests results in poor generalization capability when facing complex unseen coding tasks. A Salesforce Research team addresses these issues in the new paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning, proposing CodeRL, a novel framework for program synthesis tasks that employs pretrained LMs and deep reinforcement learning (RL) and achieves state-of-the-art performance on the challenging APPS benchmark while demonstrating impressive zero-shot transfer capabilities. The team extends the Salesforce CodeT5 (Wang et al., 2021) unified pretrained encoder-decoder transformer architecture as CodeRL’s backbone. Although CodeT5 pretraining tasks such as masked span prediction (MSP) can benefit code understanding tasks, they do not necessarily align with program synthesis objectives. To mitigate this, a next-token prediction (NTP) pretraining task is integrated into CodeT5 to uniformly sample a pivot location for each code sample, then pass the content preceding the pivot to the encoder and the remaining content to the decoder. The researchers formulate CodeRL’s program synthesis as an RL problem and introduce an actor-critic approach to improve model performance by utilizing the unit test signals in both the model optimization and generation processes. The team conducted experiments on the challenging APPS (Automated Programming Progress Standard) code generation benchmark (Hendrycks et al., 2021) to evaluate the performance of the proposed CodeRL; and used the MBPP (Mostly Basic Programming Problems) benchmark (Austin et al., 2021) to evaluate its zero-shot ability. On APPS, researchers compared their models with strong conventional baselines that included GPT-2, GPT-Neo, GPT3, Codex, and AlphaCode, where CodeRL with CodeT5 achieved new SOTA results of 2.69 percent pass@1, 6.81 percent pass@5, and 20.98 percent pass@1000. On MBPP, CodeRL with CodeT5 obtained surprisingly good zero-shot performance, achieving a new SOTA of 63.0 percent pass@80 over GPT-137B’s 61.4 percent pass@80. This work shows that the CodeRL method can effectively leverage unit test signals to push code generation performance to new SOTA performance and achieve strong zero-shot transfer capabilities. The code is available on the project’s GitHub. The paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['pretraine', 'language', 'model', 'lm', 'show', 'promise', 'result', 'simple', 'code', 'generation', 'task', 'several', 'limitation', 'training', 'model', 'nexttoken', 'prediction', 'objective', 'lead', 'accumulate', 'error', 'neglect', 'potentially', 'meaningful', 'signal', 'unit', 'test', 'result', 'poor', 'generalization', 'capability', 'face', 'complex', 'unseen', 'code', 'task', 'salesforce', 'research', 'team', 'address', 'issue', 'new', 'paper', 'coderl', 'master', 'code', 'generation', 'pretraine', 'model', 'deep', 'reinforcement', 'learn', 'propose', 'coderl', 'novel', 'framework', 'program', 'synthesis', 'task', 'employ', 'pretraine', 'lm', 'deep', 'reinforcement', 'learn', 'rl', 'achieve', 'stateoftheart', 'performance', 'challenge', 'app', 'benchmark', 'demonstrate', 'impressive', 'zeroshot', 'transfer', 'capabilitie', 'team', 'extend', 'salesforce', 'unify', 'pretraine', 'encoderdecod', 'transformer', 'architecture', '’s', 'backbone', 'codet5', 'pretraine', 'task', 'mask', 'span', 'prediction', 'msp', 'benefit', 'code', 'understanding', 'task', 'necessarily', 'align', 'program', 'synthesis', 'objective', 'mitigate', 'nexttoken', 'prediction', 'ntp', 'pretraine', 'task', 'integrate', 'codet5', 'uniformly', 'sample', 'pivot', 'location', 'code', 'sample', 'pass', 'content', 'precede', 'pivot', 'encoder', 'remain', 'content', 'decoder', 'researcher', 'formulate', 'program', 'synthesis', 'rl', 'problem', 'introduce', 'actorcritic', 'approach', 'improve', 'model', 'performance', 'utilize', 'unit', 'test', 'signal', 'model', 'optimization', 'generation', 'process', 'team', 'conduct', 'experiment', 'challenging', 'app', 'automate', 'programming', 'progress', 'standard', 'code', 'generation', 'benchmark', 'hendryck', 'evaluate', 'performance', 'propose', 'coderl', 'use', 'mbpp', 'mostly', 'basic', 'programming', 'problem', 'benchmark', 'evaluate', 'zeroshot', 'ability', 'app', 'researcher', 'compare', 'model', 'strong', 'conventional', 'baseline', 'include', 'gpt2', 'gptneo', 'gpt3', 'codex', 'alphacode', 'coderl', 'codet5', 'achieve', 'new', 'sota', 'result', 'percent', 'pass1', 'percent', 'pass5', 'percent', 'mbpp', 'coderl', 'codet5', 'obtain', 'surprisingly', 'good', 'zeroshot', 'performance', 'achieve', 'new', 'sota', 'percent', 'pass80', 'percent', 'pass80', 'work', 'show', 'coderl', 'method', 'effectively', 'leverage', 'unit', 'test', 'signal', 'push', 'code', 'generation', 'performance', 'new', 'sota', 'performance', 'achieve', 'strong', 'zeroshot', 'transfer', 'capabilitie', 'code', 'available', 'project', 'paper', 'coderl', 'master', 'code', 'generation', 'pretraine', 'model', 'deep', 'reinforcement', 'learning', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
NYU Explores the  Principles for Modelling Neural Collapse and Its Role in Generalization,https://syncedreview.com/2022/07/06/nyu-explores-the-principles-for-modelling-neural-collapse-and-its-role-in-generalization/,2022-07-06,"
 In the new paper Neural Collapse: A Review on Modelling Principles and Generalization, researchers from New York University analyze Neural Collapse (NC) and present a thought model to explain the effects of variance collapse, aiming at a better understanding of the generalization capabilities of neural networks. 
","Deep neural networks (DNNs) have advanced the state-of-the-art on tasks ranging from image classification to language processing and gameplay. But as models have become deeper and more complex, understanding their behaviours has become more challenging. A case in point is an intriguing empirical phenomenon called Neural Collapse, first identified by Papyan et al. in 2020. In the new paper Neural Collapse: A Review on Modelling Principles and Generalization, researchers from New York University analyze the principles of Neural Collapse (NC) and present a thought model designed to explain the effect of variance collapse, aiming at insights on and a better understanding of the generalization capabilities of DNNs. The team summarizes their main contributions as: The modern training paradigm for DNNs involves training well beyond the zero error threshold and toward zero loss. This post-zero-error phase is called the Terminal Phase of Training (TPT), which begins at the epoch where training error first vanishes. During TPT, the training error stays effectively zero while the training loss is pushed to zero. The TPT however is exposed to a pervasive inductive bias, NC, which involves four deeply interconnected phenomena: The team uses a principled approach to review the NC phenomena, first confirming that the final layer classifiers in DNNs tend to fall into a simple symmetric structure that helps the models obtain their high performance and state-of-the-art results. In an effort to capture the essence of the NC phenomena, the researchers then analyze such models from the ground up and unify them under a common set of principles. Overall, the paper provides a solid overview of current efforts to explain NC, It also probes the implications of NC on generalization and transfer learning via a thought model that explains the effects of variance collapse on transfer learning based on the inverse-square law to provide additional insights on the generalization capabilities of DNNs. The team hopes their work’s analytical results will be of interest to the deep learning community and encourage future research in this area.The paper Neural Collapse: A Review on Modelling Principles and Generalization is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Deep neural networks (DNNs) have advanced the state-of-the-art on tasks ranging from image classification to language processing and gameplay. But as models have become deeper and more complex, understanding their behaviours has become more challenging. A case in point is an intriguing empirical phenomenon called Neural Collapse, first identified by Papyan et al. in 2020. In the new paper Neural Collapse: A Review on Modelling Principles and Generalization, researchers from New York University analyze the principles of Neural Collapse (NC) and present a thought model designed to explain the effect of variance collapse, aiming at insights on and a better understanding of the generalization capabilities of DNNs. The team summarizes their main contributions as: The modern training paradigm for DNNs involves training well beyond the zero error threshold and toward zero loss. This post-zero-error phase is called the Terminal Phase of Training (TPT), which begins at the epoch where training error first vanishes. During TPT, the training error stays effectively zero while the training loss is pushed to zero. The TPT however is exposed to a pervasive inductive bias, NC, which involves four deeply interconnected phenomena: The team uses a principled approach to review the NC phenomena, first confirming that the final layer classifiers in DNNs tend to fall into a simple symmetric structure that helps the models obtain their high performance and state-of-the-art results. In an effort to capture the essence of the NC phenomena, the researchers then analyze such models from the ground up and unify them under a common set of principles. Overall, the paper provides a solid overview of current efforts to explain NC, It also probes the implications of NC on generalization and transfer learning via a thought model that explains the effects of variance collapse on transfer learning based on the inverse-square law to provide additional insights on the generalization capabilities of DNNs. The team hopes their work’s analytical results will be of interest to the deep learning community and encourage future research in this area.The paper Neural Collapse: A Review on Modelling Principles and Generalization is on arXiv. Author: Hecate He | Editor: Michael Sarazen We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","['deep', 'neural', 'network', 'dnn', 'advance', 'stateoftheart', 'task', 'range', 'image', 'classification', 'language', 'processing', 'gameplay', 'model', 'become', 'deep', 'complex', 'understanding', 'behaviour', 'become', 'challenging', 'case', 'point', 'intriguing', 'empirical', 'phenomenon', 'call', 'neural', 'collapse', 'first', 'identify', 'papyan', 'new', 'paper', 'neural', 'collapse', 'review', 'modelling', 'principle', 'generalization', 'researcher', 'analyze', 'principle', 'neural', 'collapse', 'present', 'thought', 'model', 'design', 'explain', 'effect', 'variance', 'collapse', 'aim', 'insight', 'well', 'understanding', 'generalization', 'capability', 'dnn', 'team', 'summarize', 'main', 'contribution', 'modern', 'training', 'paradigm', 'dnn', 'involve', 'train', 'well', 'error', 'threshold', 'loss', 'postzeroerror', 'phase', 'call', 'terminal', 'phase', 'training', 'tpt', 'begin', 'epoch', 'training', 'error', 'first', 'vanishe', 'tpt', 'training', 'error', 'stay', 'effectively', 'training', 'loss', 'push', 'tpt', 'however', 'expose', 'pervasive', 'inductive', 'bias', 'involve', 'deeply', 'interconnect', 'phenomena', 'team', 'use', 'principled', 'approach', 'review', 'first', 'confirm', 'final', 'layer', 'classifier', 'dnn', 'tend', 'fall', 'simple', 'symmetric', 'structure', 'help', 'model', 'obtain', 'high', 'performance', 'stateoftheart', 'result', 'effort', 'capture', 'essence', 'researcher', 'analyze', 'model', 'ground', 'unify', 'common', 'set', 'principle', 'overall', 'paper', 'provide', 'solid', 'overview', 'current', 'effort', 'explain', 'also', 'probe', 'implication', 'generalization', 'transfer', 'learning', 'thought', 'model', 'explain', 'effect', 'variance', 'collapse', 'transfer', 'learning', 'base', 'inversesquare', 'law', 'provide', 'additional', 'insight', 'generalization', 'capability', 'dnn', 'team', 'hope', 'work', '’s', 'analytical', 'result', 'interest', 'deep', 'learning', 'community', 'encourage', 'future', 'research', 'paper', 'neural', 'collapse', 'review', 'modelling', 'principle', 'generalization', 'author', 'hecate', 'editor', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'ai', 'update']"
