title,url,date,text,cleaning,tokens
Ethical implications of ChatGPT in higher education: A scoping review,"[{'href': 'http://arxiv.org/abs/2311.14378v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2311.14378v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-11-24 09:52:49,"1 

   Ethical implications of ChatGPT in higher education: A scoping review 

Ming Li1, Ariunaa Enkhtur2, Fei Cheng3, Beverley Anne Yamamoto4 

1Institue for Transdisciplinary Graduate Degree Programs, Osaka University 
2Center for Global Initiatives, Osaka University   
3 Graduate School of Informatics, Kyoto University 
4Graduate School of Human Sciences Human Sciences, Osaka University 

Keyword: ChatGPT, education, ethic, Generative Artificial Intelligence, higher education, 

scoping review 

ABSTRACT 

This  scoping  review  explores  the  ethical  challenges  of  using  ChatGPT  in  education, 

focusing  particularly  on  issues  related  to  higher  education.  By  reviewing  recent  academic 

articles  written  in  English,  Chinese,  and  Japanese,  we  aimed  to  provide  a  comprehensive 

overview  of  relevant  research  while  identifying  gaps  for  future  considerations.  Drawing  on 

Arksey  &  O’Malley’s  (2005)  five-stage  scoping  review  framework,  we  identified  research 

questions, search terms, and conducted article search from four databases in the target three 

languages. Each article was reviewed by at least two researchers identifying main ethical issues 

of  utilizing  AI  in  education,  particularly  higher  education.  Our  analysis  of  ethical  issues 

followed the framework developed by DeepMind (Weiginger et al., 2021) to identify six main 

areas  of  ethical  concern  in  Language  Models.  The  majority  of  papers  were  concerned  with 

misinformation harms (n=25) and/or human-computer interaction related harms (n=24). Given 

the rapid deployment of Generative Artificial Intelligence (GAI), it is imperative for educators 

to conduct more empirical studies to develop sound ethical policies for the use of GAI.  

 
 
 
 
 
 
  
 
  
2 

INTRODUCTION 

    The recent wave of Generative Artificial Intelligence (GAI) originated with Google’s 

Transformer architecture of neural networks (Vaswani et al., 2017). Transformer is based on 

the  self-attention  mechanism,  which  is  highly  scalable  and  suitable  for  parallel  computing, 

hence it quickly became the de facto approach in the fields of natural language processing and 

computer vision. In 2018, language models like BERT (Devlin et al., 2018) and GPT (Radford 

et al., 2018) ushered in the era of self-supervised learning, which are pre-trained on large-scale 

textual data for learning fundamental knowledge, and then fine-tuned for specific downstream 

tasks such as QA, dialog and machine translation systems. Kaplan et al. (2018) validated the 

large  language  models’  scale  law,  asserting  that  the  performance  improvements  can  almost 

always be achieved by increasing the scale of model parameters and pre-training more textual 

data such as Wikipedia, book and internet resources. After this, the rapid expansion for the size 

of large language models began, with  the model scale growing  from  BERT’s 0.3B (billion) 

parameters to GPT3’s 175B (Brown et al. 2020). 

    Since  the  release  of  GPT3  release  in  June  2020,  OpenAI  and  DeepMind,  leading 

developers of this technology, became less satisfied with mere scale increases. Ouyang et al. 

(2022) and Chung et al. (2022) sought to align models’ outputs with the feedback of real human 

beings. The text generated by these models became increasingly human-like, and the content 

ever-more closely aligned with human values. The launch of ChatGPT in November 2022 made 

the  public  aware  that  GAI  was  already  capable  of  generating  human-quality  conversation, 

retrieving stored knowledge on demand, and achieving natural interaction with people as an AI 

assistant. This kicked off the current frenzy of adapting the utilization of GAI to various fields, 

including education. 

    ChatGPT’s  application  in  higher  education  has  garnered  attention  (UNESCO 

IESALC,  2023).  While  there  has  been  much  discussion  about  its  possible  benefits,  such  as 

 
3 

creating  teaching  materials,  analyzing  student  data,  or  identifying  learning  patterns,  the 

evidence base for this is unclear. Yet, GAI studies have revealed potential risks associated with 

the generation of incorrect information (known as the ‘hallucination’ issue), biases (including 

race, nationality, gender), and discriminatory content (Munn, 2023; Nozza et al., 2022). When 

GAI-generation  outputs  are  used  in  education-related  procedures  there  is  a  possibility  that 

problematic contents, biases and assumptions will be magnified, which may result in negative 

consequences for learners, educators, researchers, and administrators. Therefore, it is crucial to 

discuss  and  assess  the  ethical  implications  of  implementing  ChatGPT  within  educational 

institutions, especially concerning its use in teaching and learning, research or administration. 

Increased  use  of  GAI  by  learners  raises  issues  related  to  academic  integrity,  definitions  of 

authorship,  assessment  methods,  and  other  pedagogical  implications.  It  also  affects  how 

researchers conduct their studies and generate their outputs, as well as how decisions are made 

in admissions, hiring, or how the educational institutions are managed and run. Furthermore, 

the  increasing  integration  of  AI  in  education  even  raises  questions  about  the  continued 

relevance of traditional brick-and-mortar educational institutions.     

This  scoping  review  explores  the  ethical  challenges  of  using  ChatGPT  in  education, 

focusing particularly on higher education. By reviewing academic articles written in English, 

Chinese, and Japanese, we aimed to map out the current state of the field while identifying gaps 

for future consideration.  

METHODOLOGY 

A scoping review is commonly used to identify key issues in a newly emerging field or 

one where there is yet a substantial body of literature. It is “used to identify knowledge gaps, 

set research agendas, and identify implications for decision-making” (Tricco et al., 2016).  In 

this study, we adopted Arksey & O’Malley’s (2005) five-stage scoping review framework, that 

 
 
4 

involves identifying the initial research questions and relevant studies, selecting the studies, 

charting the data, and collating, summarizing, and reporting the results.  

Identifying the relevant studies 

We limited our focus to articles focusing on the latest version of GPT. We searched 

articles published in 2023 and used search terms “ChatGPT” or “Generative AI” coupled with 

“education”  and  “ethics”  (see  Table  1).  To  capture  more  solid  evidence-based  studies  and 

discussions around this topic, we identified SCOPUS as the main database to conduct our initial 

search.  To  include  ongoing  research  works,  we  also  included  the  arXiv  platform,  which 

provides access to preprint articles. We included two other languages that the authors have first 

or near-first language proficiency in, Japanese and Chinese. To facilitate this, we conducted 

searches in the prominent databases CiNii (Japanese) and CNKI (Chinese). Along with the UK 

and USA, Japan and China are leading AI development making these languages a good target. 

Table 1. Final search terms and results by platforms 

Database  Search terms 
Scopus 

(TITLE-ABS-KEY (""chatgpt"" OR ""generative AI"") AND TITLE-ABS-KEY 
(""education"" )) 

(TITLE-ABS-KEY (""chatgpt"" OR ""generative AI"") AND TITLE-ABS-KEY 
(""education"" AND ""ethics"")) 

ArXiv 

(TITLE-ABS-KEY (""chatgpt"" OR ""generative AI"") AND TITLE-ABS-KEY 
(""education"")) 

(TITLE-ABS-KEY (""chatgpt"" OR ""generative AI”) AND TITLE-ABS-KEY 
( ""education""  AND  ""ethics"" ) )  

Results  
276 

27 

112 

24 

CiNii 

(TITLE-ABS-KEY (""chatgpt""  OR  ""生成 AI"")  AND  TITLE-ABS-KEY ( ""教育"" ) )  

23 

 (TITLE-ABS-KEY (""chatgpt""  OR  ""生成 AI"")  AND  TITLE-ABS-KEY (“教育"" 
AND ""課題"" )  

4 

CNKI 

(TITLE-ABS-KEY (""chatgpt"" OR ""生成 AI"") AND TITLE-ABS-KEY (""education"")) 

198 

(TITLE-ABS-KEY (""chatgpt"" OR ""生成 AI"" ) AND TITLE-ABS-KEY (""教育"" AND 
""伦理"")  

12 

Charting the data and collation 

The initial search yielded 609 results, out of which 67 included education and ethical 

concerns. From these, we identified 26 articles meeting our inclusion criteria (Figure 1). All 

 
  
articles were reviewed by two reviewers and the third reviewer checked the findings.  

5 

Figure 1. Data extraction processes 

In our analysis of the ethical issues raised in the articles, we relied on the comprehensive 

research  conducted  by  DeepMind  (Weiginger  et  al.,  2021),  which  offers  a  framework  for 

assessing the ethical and social risks of harm that may arise from the deployment of language 

models (LMs) (Table 2).  

#  Areas 
1  Discrimination, 

Exclusion and 
Toxicity 
Information Hazards 

2 
3  Misinformation 

Harms 

4  Malicious Uses 

5  Human-Computer 
Interaction Harms 
6  Automation, Access, 

and Environmental 
Harms 

Table 2. Ethical and social risks areas 

Description 
AI models can harm by reinforcing discrimination, stereotypes, and biases, 
marginalizing individuals, promoting toxic language, and worsening disparities 
for disadvantaged groups. 
Leak of private data or sensitive information leaks. 
Providing false or misleading information, leading to less informed users and 
eroding trust in shared information 
Risks of using LMs for harm include enabling disinformation campaigns, 
personalized scams, fraud at scale, and the development of malicious computer 
code or weapon systems. 
Users’ overestimation of “human-like” AI capabilities may lead to unsafe 
usage, exploitation for manipulation, and perpetuation of stereotypes. 
Unequal benefits and limited access to LMs can impact job quality, creative 
economy, and create global disparities in risks and rewards. 

 
 
6 

FINDINGS 

Most  of  the  identified  papers  were  in  English  (n=19),  followed  by  Chinese  (n=4), 

Japanese (n=3). In the English papers, ten were empirical studies and nine were conceptual or 

discussion papers with a predominant focus on its applications in fields such as healthcare and 

medical domains. In comparison, the number of Chinese and Japanese papers was much smaller. 

Among  the  four  Chinese  papers,  all  were  general  discussions  around  the  application  and 

predicted impact of ChatGPT in education. There were only three Japanese articles, but one 

reported  on  initial  research  on  students’  practical  experiences  with  ChatGPT  specifications 

(Kondo  et  al.,  2023).  The  other  two  papers  discussed  challenges  for  Japanese  speakers  in 

writing  English  academic  papers  and  the  use  of  ChatGPT  for  support  and  general  teaching 

implications (Kashimura, 2023; Yanase, 2023). 

Overall,  there  was  little  discussion  specifically  focusing  on  higher  education.  The 

majority of the papers (n=19) were generic, discussing ethical concerns in teaching (n=19) and 

learning  (n=13)  mostly  from  theoretical  and  conceptual  perspectives  without  delving  into 

specific levels of education. Papers that specifically focused on tertiary education (n=5) were 

concerned  about  overall  pedagogical  implications,  particularly  in  medical  education  (n=2), 

faculty and students’ perceptions (n=2), and research implications (n=1).  

Table 3. Articles reviewed. 

Authors 

Language 

Education level   Main area/Focus  

Ethical concerns  

Database: Scopus 

Busch et al. 

English 

Tertiary 

Teaching, learning, 
administration 

1,2, 3, 4, 5 

Chan 

English 

Tertiary 

Teaching, learning 

2, 3, 5 

Curtis 

English 

Tertiary 

Research 

da Silva 

English 

Generic 

Research 

3, 5 

3, 5 

Dwivedi et al.  English 

Generic 

Research 

1, 2, 3, 4, 5, 6 

Fischer 

English 

Generic 

Administration 

1, 2, 3, 4, 5 

Krüger et al. 

English 

Generic 

Teaching, learning, research 

1, 2, 3, 5 

 
 
 
7 

Lim et al. 

English 

Generic 

Teaching 

1, 2, 3 

Masters (a) 

English 

Generic 

Teaching, administration 

1, 2, 3, 4, 5 

Masters (b) 
O’Connor & 
ChatGPT 

Tlili et al. 
Zumsteg & 
Junn 

English 

Generic 

Research 

3, 4 

English 

Generic 

Teaching, learning, research 

3, 5 

English 

Generic 

Teaching, learning 

1, 2, 3, 4, 5,  

English 

Tertiary 

Teaching, learning 

3, 4, 5 

Database: arXiv 

Chan & Hu 

English 

Tertiary 

Teaching, learning 

1, 2, 3, 4, 5 

Latif et al. 

English 

Generic 

Teaching 

1, 2, 3, 4, 5 

Li et al. 

English 

Generic 

Teaching, learning, research 

1, 2, 3, 4, 5 

Ojha et al. 

English 

Generic 

Teaching 

Sharma et al. 

English 

Generic 

Administration 

Sharples 

English 

Generic 

Teaching, learning 

Database: CiNii 

Kashimura  

Japanese 

Generic 

Teaching 

Kondo et al. 

Japanese 

Secondary 

Teaching, learning 

Yanase  

Japanese 

Generic 

Research 

Database: CNKI 

Huang 

Chinese 

Generic 

Teaching, learning 

Song & Lin 

Chinese 

Generic 

Teaching 

Xun 

Chinese 

Tertiary 

Teaching, learning 

Zhu & Yang 

Chinese 

Generic 

Teaching, learning 

4, 5 

3, 4 

3, 5 

1, 2, 3 

3, 5 

3, 5 

1, 3, 5 

2, 3, 5 

1, 3, 4, 5 

1, 2, 3, 5 

In terms of the focus of ethical issues, the majority of papers were concerned with #3 

misinformation  harms  (n=25)  including  academic  integrity,  cheating  and  other  assessment 

issues, and the users’ role in identifying and clarifying information and/or #5 human-computer 

interaction related harms (n=24) such as  addiction, dependence, and cognitive overload.  To 

illustrate  this  further  we  divided  the  papers  into  four  themes  concerning  teaching,  learning, 

research, and administration.  

In  the  following  section  we  sum  up  the  key  concerns  and  areas  of  discussion  in 

literature. 

 
 
 
 
8 

Teaching  

  Under  teaching,  it  was  noted  that  ChatGPT  exhibits  versatile  applications,  such  as 

personalized  and  interactive  learning,  curriculum  design,  assessing  homework,  exams,  and 

essays (Ojha et at., 2023; Kashiwamura, 2023; Huang, 2023). To design new programs or to 

provide personalized teaching, universities need to collect and process vast amounts of student 

data, often without students’ consent. This gives rise to questions surrounding data privacy and 

security (Chan, 2023; Masters, 2023a). There is a need to ensure that robust data protection 

measures are in place to safeguard sensitive information and prevent its misuse.  

Research by Latif et al. (2023) highlighted that AI may inadvertently reinforce existing 

societal inequalities, gender bias, and nationality  bias embedded within the original training 

data, negatively impacting the outputs of educational applications downstream. Undue reliance 

on AI-generated evaluations may compromise the quality of assessments, potentially failing to 

accurately  gauge  students’  true  capabilities  (Busch  et  al.,  2023;  Curtis,  2023;  Song  &  Lin, 

2023). 

Additionally, the introduction of AI in the educational process raises concerns about the 

dynamics between educators and students. A possible overreliance on AI-generated content, as 

discussed  by  Sharples  (2023),  could  alter  the  traditional  teacher-student  relationship  and 

diminish  educators’  creative  input  and  uniqueness  in  designing  engaging  lesson  plans  and 

activities. 

 Learning 

     The application of ChatGPT in learning includes personalized learning experiences, 

student  support,  language  assistance,  tutoring,  content  creation,  grading  and  assessment, 

research aid, and career counseling, enhancing the learning process for students (Kooli, 2023; 

 
 
     
9 

Lim et al., 2023). One major concern is the potential for an increase in plagiarism and cheating 

among  students  who  might  rely  on  GAI-generated  content  for  essays  and  exams,  thereby 

compromising  the  authenticity  of  their  work  (Li  et  al.,  2023;  Zhu  &  Yang,  2023).  This 

overreliance  on  ChatGPT  may  lead  to  a  decline  in  students’  sense  of  responsibility  and 

commitment to academic integrity (Ojha et al., 2023). 

    Excessive use of ChatGPT may have adverse effects on students’ critical thinking 

skills.  If  students  heavily  depend  on  AI-generated  content,  they  might  lose  the  ability  to 

independently analyze and evaluate information (Tlili et al., 2023). Another significant issue 

raised in the literature pertains to the risk of misinformation being propagated due to the highly 

persuasive and convincing nature of AI-generated content. This can lead to potential bias or 

manipulation of information presented to students.  

       Reliance on AI interactions for academic or social purposes might diminish face-

to-face  interactions,  potentially  hindering  the  development  of  essential  social  skills  among 

students.  Striking  a  balance  between  AI  and  human  interactions  is  crucial  to  foster  a  well-

rounded educational experience. 

ChatGPT  might  inadvertently  produce  content  that  inaccurately  or  inappropriately 

represents certain cultural or identity groups, highlighting the need for ongoing refinement and 

sensitivity in AI language model development (Busch et al., 2023). 

 Research  

   The applications of ChatGPT encompass efficient dataset analysis, code generation, 

literature  reviews,  timesaving  for  experimental  design  focus,  and  advancements  in  research 

discovery and development (Dwivedi et al., 2023; Li et al., 2023). 

   In research, the integration of AI in academic publishing poses the risk of displacing 

human  authors  and  undermining  the  value  of  their  expertise,  potentially  impacting  the 

 
   
10 

credibility  of  research.  The  attribution  of  fake  references  to  AI-generated  content  presents 

another challenge, leading to misinformation and a decline in trust in academic sources. (Curtis, 

2023). Joint authorship of editorial pieces like O’Connor and ChatGPT (2023) is a contentious 

as  it  challenges  the  established  core  values  related  to  human-based  authorship  in  academic 

publishing (da Silva, 2023).  

   Some  conferences  permit  the  use  of  ChatGPT  for  writing  papers,  but  only  when 

ChatGPT itself is  the subject  of empirical  research (e.g.,  ICML , 2023). On the other hand, 

some  research  communities,  such  as  the  Association  for  Computational  Linguistics  (ACL, 

2023), allow the use of ChatGPT based on specific guidelines.  

 Administration   

     ChatGPT  can  significantly  reduce  the  time  spent  on  human  administrative  tasks, 

such  as  responding  to  queries  from  applicants  and  assisting  students  in  course  enrollment 

(UNESCO IESALC, 2023).  

     However, there are concerns surrounding the equitable, reliable, and transparent use 

of ChatGPT. Utilizing ChatGPT in the admissions processes can potentially introduce biases, 

especially if the AI model was trained on historical data that reflects past inequalities (Fischer, 

2023; Sharma et al. 2023). To ensure fairness, transparency, and accountability, it is essential 

to  provide  applicants  with  clear  explanations  of  how  AI  was  employed  to  assess  their 

applications and the specific factors that contributed to their acceptance or rejection. 

Additionally,  the  use  of  AI  algorithms  in  admissions  decisions  carries  the  risk  of 

inadvertently  favoring  applicants  with  certain  characteristics  or  backgrounds,  potentially 

impacting  diversity  and  inclusion  efforts  within  the  university  (Busch  et  al.,  2023;  Fischer, 

2023).  Data privacy and security are also paramount considerations. To avoid unintentional 

discrimination,  institutions  should  actively  assess  and  address  any  biases  in  the  AI  model’s 

 
 
training  data  and  decision-making  process,  striving  to  provide  equal  opportunities  for  all 

applicants. 

11 

DISCUSSION AND CONCLUSION 

We focused on articles written in a very short period, the first 7 months of 2023, but 

covered literature written in English, Chinese and Japanese. Given that Chat GPT is trained on 

English-centric data (Brown et al., 2020), it is important to gain insights into discussions going 

on in other AI technologically advanced countries. However, our review revealed that a few 

academic and research studies are published in non-English languages, particularly in Chinese 

and Japanese.   

Our scoping review showed that there are already publications that are considering the 

ethical implications of GAI, especially ChatGPT, in education generally and some focused on 

higher education. The majority of papers are discussion pieces, but there is some early empirical 

work.  The  ethical  issues  highlighted  in  these  works  are  mainly  concerned  about  academic 

integrity, assessment issues, and data-protection.  

  Our analysis highlights the urgency of addressing ethical issues surrounding the use 

of GAI/ChatGPT in education. Collaboration among stakeholders is essential to establish clear 

guidelines,  protect  student  privacy,  and  promote  responsible  AI  use.  By  doing  so,  AI  can 

enhance education and research without compromising fundamental principles. 

References 

ACL. (2023). ACL 2023 Policy. 61st Annual Meeting of the Association for Computational 

Linguistics. Retrieved from https://2023.aclweb.org/blog/ACL-2023-policy/  

Arksey, H., & O'Malley, L. (2005). Scoping studies: towards a methodological framework. 

 
  
 
 
12 

International journal of social research methodology, 8(1), 19-32. 

Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. 

(2020).  Language  models  are  few-shot  learners.  Advances  in  Neural  Information 

Processing Systems, 33, 1877–1901. 

Busch, F., Adams,  L. C., &  Bressem, K. K. (2023). Biomedical  ethical  aspects  towards the 

implementation of Artificial Intelligence in medical education. Med. Sci. Educ. Advance 

Online Publication. https://doi.org/10.1007/s40670-023-01815-x 

Chan,  C.  K.  Y.  (2023).  A  comprehensive  AI  policy  education  framework  for  university 

teaching  and  learning.  International  Journal  of  Educational  Technology  in  Higher 

Education, 20(38). https://doi.org/10.1186/s41239-023-00408-3 

Chan, C. K. Y., & Hu, W. (2023). Students’ voices on generative AI: Perceptions, benefits, and 

challenges in higher education. arXiv preprint arXiv:2305.00290. 

Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., ... & Wei, J. (2022). Scaling 

instruction-finetuned language models. arXiv preprint arXiv:2210.11416. 

Curtis,  N.(2023). To ChatGPT or not  to  ChatGPT? The  Impact  of Artificial  Intelligence on 

Academic  Publishing.  The  Pediatric  Infectious  Disease  Journal,  42(4),  275. 

https://doi.org/10.1097/INF.0000000000003852 

da Silva, J. A. T. (2023). Is ChatGPT a valid author?. Nurse Education in Practice, 68, 103600. 

https://doi.org/10.1016/j.nepr.2023.103600. 

Devlin,  J.,  Chang,  M.  W.,  Lee,  K.,  &  Toutanova,  K.  (2018).  Bert:  Pre-training  of  deep 

bidirectional 

transformers 

for 

language 

understanding. 

arXiv 

preprint 

arXiv:1810.04805. 

Dwivedi, Y. K., Kshetri, N., Hughes, L., Slade, E. L., Jeyaraj, A., Kar, A. K., ... & Wright, R. 

(2023).  “So  what 

if  ChatGPT  wrote 

it?”  Multidisciplinary  perspectives  on 

opportunities, challenges and implications of generative conversational AI for research, 

 
13 

practice  and  policy.  International  Journal  of  Information  Management,  71,  102642. 

https://doi.org/10.1016/j.ijinfomgt.2023.102642 

Fischer, I. (2023). Evaluating the ethics of machines assessing humans. Journal of Information 

Technology Teaching Cases, 0(0). https://doi.org/10.1177/20438869231178844 

Huang R. (2023). Ren Gong Zhi Neng Zheng Jia Su Jiao Yu Bian Ge: Xian Shi Tiao Zhan Yu 

Ying  Dui  Ju  Cuo  [Artificial  intelligence  is  accelerating  educational  transformation: 

Realistic challenges and countermeasures]. Journal of the Chinese Society of Education, 

(06), 26-33. 

ICML. (2023). Call for Papers. International Conference on Machine Learning. Retrieved from 

https://icml.cc/Conferences/2023/CallForPapers 

Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, 

A., Wu, J., & Amodei, D. (2020). Scaling laws for neural language models. Advances 

in neural information processing systems, 33, 1877–1901. 

Kashiwamura, Y. (2023). Sozoteki sagyo e shifuto o unagasu kyoin no noryoku kojo ga kadai 

ni:  Kyoiku  o  kaeru  seisei  AI  [Encouraging  a  shift  to  creative  work:  Challenges  for 

teacher  capacity  building:  Generating  AI  that  Changes  Education].  The  Economist, 

101(24), 98-100. 

Kondo,  C.,  Tamada,  K.,  &  Matsuda,  T.  (2023).  Seiseikei  AI  o  daizai  toshita  joho-teki  na 

mikata・kangaekata ni motozuku mondai kaiketsu shido jissen: ChatGPT to no kyo-

zon  o  kangaeru  [Practicing  problem-solving  instruction  based  on  informational 

perspectives and ways of thinking :Consider coexistence with ChatGPT]. Journal of the 

Japan 

Society 

for 

Educational 

Technology, 

(2), 

255-258. 

https://doi.org/10.15077/jsetstudy.2023.2_255 

Kooli,  C.  (2023).  Chatbots  in  education  and  research:  A  critical  examination  of  ethical 

implications and solutions. Sustainability, 15(7), 5614. 

 
14 

Krüger, L., Krotsetis, S., OpenAI’s Generative Pretrained Transformer 3 (GPT-3) Model, & 

Nydahl,  P.  (2023).  ChatGPT:  Fluch  oder  Segen  in  der  Pflege?  [ChatGPT:  curse  or 

blessing in nursing care?]. Medizinische Klinik, Intensivmedizin und Notfallmedizin, 

10.1007/s00063-023-01038-3. 

Advance 

online 

publication. 

https://doi.org/10.1007/s00063-023-01038-3 

Latif, E., Mai, G., Nyaaba, M., Wu, X., Liu, N., Lu, G., ... & Zhai, X. (2023). Artificial general 

intelligence (AGI) for education. arXiv preprint arXiv:2304.12479. 

Li,  L.,  Ma,  Z.,  Fan,  L.,  Lee,  S.,  Yu,  H.,  &  Hemphill,  L.  (2023).  ChatGPT  in  education:  A 

discourse  analysis  of  worries  and  concerns  on  social  media.  arXiv  preprint 

arXiv:2305.02201. 

Lim, W. M., Gunasekara, A., Pallant, J. L., Pallant, J. I., & Pechenkina, E. (2023). Generative 

AI and the future of education: Ragnarök or reformation? A paradoxical  perspective 

from  management  educators.  The  International  Journal  of  Management  Education, 

21,100790. https://doi.org/10.1016/j.ijme.2023.100790  

Masters,  K.  (2023  a).  Ethical  use  of  Artificial  Intelligence  in  health  professions  education: 

AMEE 

Guide 

No. 

158. 

Medical 

Teacher, 

45(6), 

574-584. 

https://doi.org/10.1080/0142159X.2023.2186203 

Masters, K. (2023 b). Medical teacher’s first ChatGPT’s referencing hallucinations: Lessons 

for editors, reviewers, and teachers. Medical Teacher, Med Teach, 45(7), 673-675. DOI: 

10.1080/0142159X.2023.2208731 

Munn,  L. 

(2023).  The  uselessness  of  AI 

ethics.  AI  Ethics,  3,  869–877. 

https://doi.org/10.1007/s43681-022-00209-w 

Nozza, D., Bianchi, F., & Hovy, D. (2022). Pipelines for social bias testing of large language 

models.  In  Proceedings  of  BigScience  Episode  #5  --  Workshop  on  Challenges  & 

Perspectives  in  Creating  Large  Language  Models,  virtual+Dublin.  Association  for 

 
15 

Computational Linguistics, 68-74. 

O’Connor, S & ChatGPT. (2022). Open artificial intelligence platforms in nursing education: 

Tools  for  academic  progress  or  abuse?.  Nurse  Education  in  Practice,  66,  103537. 

https://doi.org/10.1016/j.nepr.2022.103537 

Ojha,  S.,  Narendra,  A.,  Mohapatra,  S.,  &  Misra,  I.  (2023).  From  robots  to  books:  An 

introduction  to  smart  applications  of  AI  in  education  (AIEd).  arXiv  preprint 

arXiv:2301.10026. 

Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., ... & Lowe, R. (2022). 

Training  language  models  to  follow  instructions  with  human  feedback.  Advances  in 

Neural Information Processing Systems, 35, 27730-27744. 

Radford, A., & Narasimhan, K. (2018). Improving Language Understanding by Generative Pre-

Training. Retrieved from https://www.mikecaptain.com/resources/pdf/GPT-1.pdf 

Sharma, P., Thapa, K., Dhakal, P., Upadhaya, M. D., Adhikari, S., & Khanal, S. R. (2023). 

Performance  of  ChatGPT  on  USMLE:  Unlocking  the  potential  of  large  language 

models for AI-assisted medical education. arXiv preprint arXiv:2307.00112. 

Sharples, M. (2023). Towards social generative AI for education: theory, practices and ethics. 

arXiv preprint arXiv:2306.10063. 

Song, H & Lin, M (2023). ChatGPT/Chuangshengshi rengong zhineng shidai xia  jiaoshi de 

gongzuo biange: Jiyu, tiaozhan yu yingdui [The Transformation of Teachers’ Work in 

the Era of ChatGPT/AIGC: Opportunities, Challenges, and Responses]. Journal of East 

China 

Normal 

University 

(Educational 

Sciences), 

(07),78-90. 

https://doi.org/10.16382/j.cnki.1000-5560.2023.07.008. 

Tlili, A., Shehata, B., Adarkwah, M. A., Bozkurt, A., Hickey, D. T., Huang, R., & Agyemang, 

B. (2023). What if the devil is my guardian angel: ChatGPT as a case study of using 

chatbots in education. Smart Learning Environments, 10(1), 15. 

 
16 

Tricco, A. C., Lillie, E., Zarin, W., O’brien, K., Colquhoun, H., Kastner, M., ... & Straus, S. E. 

(2016).  A  scoping  review  on  the  conduct  and  reporting  of  scoping  reviews.  BMC 

medical research methodology, 16, 1-10. 

UNESCO IESALC (2023). ChatGPT and artificial intelligence in higher education: Quick start 

guide.  Retrieved 

from  https://www.iesalc.unesco.org/en/2023/04/14/chatgpt-and-

artificial-intelligence-in-higher-education-quick-start-guide-and-interactive-seminar/ 

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & 

Polosukhin,  I.  (2017).  Attention  is  all  you  need.  Advances  in  neural  information 

processing systems, 30. https://api.semanticscholar.org/CorpusID:13756489 

Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P. S., ... & Gabriel, I. (2021). 

Ethical  and  social  risks  of  harm 

from 

language  models.  arXiv  preprint 

arXiv:2112.04359. 

Xun, Y. (2023). ChatGPT/Chuangshengshi rengong zhineng yu gaodeng jiaoyu de jiazhi he 

shiming [ChatGPT/AIGC and the Value and Mission of Higher Education]. Journal of 

East China Normal University (Educational Sciences), (07), 56-63. 

Yanase, Y. (2023). AI o katsuyo shite Eigo ronbun o sakusei suru Nihongo shasha ni totte no 

kadai to sono taisho [Challenges and Strategies for Japanese Speakers Creating English 

Papers Using AI]. Journal of Information Science and Technology, 73(6), 219-224. 

Zhu, Y & Yang, F. (2023). ChatGPT/Chuangshengshi rengong zhineng yu jiaoyu chuangxin: 

Jiyu, 

tiaozhan  yiji  weilai. 

[ChatGPT/AIGC  and  Educational 

Innovation: 

Opportunities,Challenges, and the Future].  Journal of East China Normal University 

(Educational 

Sciences), 

(07),1-14. 

https://doi.org/10.16382/j.cnki.1000-

5560.2023.07.001. 

Zumsteg, J. M., & Junn, C. (2023). Will ChatGPT match to your program.  Am J Phys Med 

Rehabil, 1, 3-7. 

 
17 

 
 
","1 Ethical implications of ChatGPT in higher education : A scoping review Ming Li1 , Ariunaa Enkhtur2 , Fei Cheng3 , Beverley Anne Yamamoto4 1Institue for Transdisciplinary Graduate Degree Programs , Osaka University 2Center for Global Initiatives , Osaka University 3 Graduate School of Informatics , Kyoto University 4Graduate School of Human Sciences Human Sciences , Osaka University Keyword : ChatGPT , education , ethic , Generative Artificial Intelligence , higher education , scoping review ABSTRACT This scoping review explores the ethical challenges of using ChatGPT in education , focusing particularly on issues related to higher education . By reviewing recent academic articles written in English , Chinese , and Japanese , we aimed to provide a comprehensive overview of relevant research while identifying gaps for future considerations . Drawing on Arksey & O ’ Malley ’ s ( 2005 ) five-stage scoping review framework , we identified research questions , search terms , and conducted article search from four databases in the target three languages . Each article was reviewed by at least two researchers identifying main ethical issues of utilizing AI in education , particularly higher education . Our analysis of ethical issues followed the framework developed by DeepMind ( Weiginger et al. , 2021 ) to identify six main areas of ethical concern in Language Models . The majority of papers were concerned with misinformation harms ( n=25 ) and/or human-computer interaction related harms ( n=24 ) . Given the rapid deployment of Generative Artificial Intelligence ( GAI ) , it is imperative for educators to conduct more empirical studies to develop sound ethical policies for the use of GAI . 2 INTRODUCTION The recent wave of Generative Artificial Intelligence ( GAI ) originated with Google ’ s Transformer architecture of neural networks ( Vaswani et al. , 2017 ) . Transformer is based on the self-attention mechanism , which is highly scalable and suitable for parallel computing , hence it quickly became the de facto approach in the fields of natural language processing and computer vision . In 2018 , language models like BERT ( Devlin et al. , 2018 ) and GPT ( Radford et al. , 2018 ) ushered in the era of self-supervised learning , which are pre-trained on large-scale textual data for learning fundamental knowledge , and then fine-tuned for specific downstream tasks such as QA , dialog and machine translation systems . Kaplan et al . ( 2018 ) validated the large language models ’ scale law , asserting that the performance improvements can almost always be achieved by increasing the scale of model parameters and pre-training more textual data such as Wikipedia , book and internet resources . After this , the rapid expansion for the size of large language models began , with the model scale growing from BERT ’ s 0.3B ( billion ) parameters to GPT3 ’ s 175B ( Brown et al . 2020 ) . Since the release of GPT3 release in June 2020 , OpenAI and DeepMind , leading developers of this technology , became less satisfied with mere scale increases . Ouyang et al . ( 2022 ) and Chung et al . ( 2022 ) sought to align models ’ outputs with the feedback of real human beings . The text generated by these models became increasingly human-like , and the content ever-more closely aligned with human values . The launch of ChatGPT in November 2022 made the public aware that GAI was already capable of generating human-quality conversation , retrieving stored knowledge on demand , and achieving natural interaction with people as an AI assistant . This kicked off the current frenzy of adapting the utilization of GAI to various fields , including education . ChatGPT ’ s application in higher education has garnered attention ( UNESCO IESALC , 2023 ) . While there has been much discussion about its possible benefits , such as 3 creating teaching materials , analyzing student data , or identifying learning patterns , the evidence base for this is unclear . Yet , GAI studies have revealed potential risks associated with the generation of incorrect information ( known as the ‘ hallucination ’ issue ) , biases ( including race , nationality , gender ) , and discriminatory content ( Munn , 2023 ; Nozza et al. , 2022 ) . When GAI-generation outputs are used in education-related procedures there is a possibility that problematic contents , biases and assumptions will be magnified , which may result in negative consequences for learners , educators , researchers , and administrators . Therefore , it is crucial to discuss and assess the ethical implications of implementing ChatGPT within educational institutions , especially concerning its use in teaching and learning , research or administration . Increased use of GAI by learners raises issues related to academic integrity , definitions of authorship , assessment methods , and other pedagogical implications . It also affects how researchers conduct their studies and generate their outputs , as well as how decisions are made in admissions , hiring , or how the educational institutions are managed and run . Furthermore , the increasing integration of AI in education even raises questions about the continued relevance of traditional brick-and-mortar educational institutions . This scoping review explores the ethical challenges of using ChatGPT in education , focusing particularly on higher education . By reviewing academic articles written in English , Chinese , and Japanese , we aimed to map out the current state of the field while identifying gaps for future consideration . METHODOLOGY A scoping review is commonly used to identify key issues in a newly emerging field or one where there is yet a substantial body of literature . It is “ used to identify knowledge gaps , set research agendas , and identify implications for decision-making ” ( Tricco et al. , 2016 ) . In this study , we adopted Arksey & O ’ Malley ’ s ( 2005 ) five-stage scoping review framework , that 4 involves identifying the initial research questions and relevant studies , selecting the studies , charting the data , and collating , summarizing , and reporting the results . Identifying the relevant studies We limited our focus to articles focusing on the latest version of GPT . We searched articles published in 2023 and used search terms “ ChatGPT ” or “ Generative AI ” coupled with “ education ” and “ ethics ” ( see Table 1 ) . To capture more solid evidence-based studies and discussions around this topic , we identified SCOPUS as the main database to conduct our initial search . To include ongoing research works , we also included the arXiv platform , which provides access to preprint articles . We included two other languages that the authors have first or near-first language proficiency in , Japanese and Chinese . To facilitate this , we conducted searches in the prominent databases CiNii ( Japanese ) and CNKI ( Chinese ) . Along with the UK and USA , Japan and China are leading AI development making these languages a good target . Table 1 . Final search terms and results by platforms Database Search terms Scopus ( TITLE-ABS-KEY ( `` chatgpt '' OR `` generative AI '' ) AND TITLE-ABS-KEY ( `` education '' ) ) ( TITLE-ABS-KEY ( `` chatgpt '' OR `` generative AI '' ) AND TITLE-ABS-KEY ( `` education '' AND `` ethics '' ) ) ArXiv ( TITLE-ABS-KEY ( `` chatgpt '' OR `` generative AI '' ) AND TITLE-ABS-KEY ( `` education '' ) ) ( TITLE-ABS-KEY ( `` chatgpt '' OR `` generative AI ” ) AND TITLE-ABS-KEY ( `` education '' AND `` ethics '' ) ) Results 276 27 112 24 CiNii ( TITLE-ABS-KEY ( `` chatgpt '' OR `` 生成 AI '' ) AND TITLE-ABS-KEY ( `` 教育 '' ) ) 23 ( TITLE-ABS-KEY ( `` chatgpt '' OR `` 生成 AI '' ) AND TITLE-ABS-KEY ( “ 教育 '' AND `` 課題 '' ) 4 CNKI ( TITLE-ABS-KEY ( `` chatgpt '' OR `` 生成 AI '' ) AND TITLE-ABS-KEY ( `` education '' ) ) 198 ( TITLE-ABS-KEY ( `` chatgpt '' OR `` 生成 AI '' ) AND TITLE-ABS-KEY ( `` 教育 '' AND `` 伦理 '' ) 12 Charting the data and collation The initial search yielded 609 results , out of which 67 included education and ethical concerns . From these , we identified 26 articles meeting our inclusion criteria ( Figure 1 ) . All articles were reviewed by two reviewers and the third reviewer checked the findings . 5 Figure 1 . Data extraction processes In our analysis of the ethical issues raised in the articles , we relied on the comprehensive research conducted by DeepMind ( Weiginger et al. , 2021 ) , which offers a framework for assessing the ethical and social risks of harm that may arise from the deployment of language models ( LMs ) ( Table 2 ) . # Areas 1 Discrimination , Exclusion and Toxicity Information Hazards 2 3 Misinformation Harms 4 Malicious Uses 5 Human-Computer Interaction Harms 6 Automation , Access , and Environmental Harms Table 2 . Ethical and social risks areas Description AI models can harm by reinforcing discrimination , stereotypes , and biases , marginalizing individuals , promoting toxic language , and worsening disparities for disadvantaged groups . Leak of private data or sensitive information leaks . Providing false or misleading information , leading to less informed users and eroding trust in shared information Risks of using LMs for harm include enabling disinformation campaigns , personalized scams , fraud at scale , and the development of malicious computer code or weapon systems . Users ’ overestimation of “ human-like ” AI capabilities may lead to unsafe usage , exploitation for manipulation , and perpetuation of stereotypes . Unequal benefits and limited access to LMs can impact job quality , creative economy , and create global disparities in risks and rewards . 6 FINDINGS Most of the identified papers were in English ( n=19 ) , followed by Chinese ( n=4 ) , Japanese ( n=3 ) . In the English papers , ten were empirical studies and nine were conceptual or discussion papers with a predominant focus on its applications in fields such as healthcare and medical domains . In comparison , the number of Chinese and Japanese papers was much smaller . Among the four Chinese papers , all were general discussions around the application and predicted impact of ChatGPT in education . There were only three Japanese articles , but one reported on initial research on students ’ practical experiences with ChatGPT specifications ( Kondo et al. , 2023 ) . The other two papers discussed challenges for Japanese speakers in writing English academic papers and the use of ChatGPT for support and general teaching implications ( Kashimura , 2023 ; Yanase , 2023 ) . Overall , there was little discussion specifically focusing on higher education . The majority of the papers ( n=19 ) were generic , discussing ethical concerns in teaching ( n=19 ) and learning ( n=13 ) mostly from theoretical and conceptual perspectives without delving into specific levels of education . Papers that specifically focused on tertiary education ( n=5 ) were concerned about overall pedagogical implications , particularly in medical education ( n=2 ) , faculty and students ’ perceptions ( n=2 ) , and research implications ( n=1 ) . Table 3 . Articles reviewed . Authors Language Education level Main area/Focus Ethical concerns Database : Scopus Busch et al . English Tertiary Teaching , learning , administration 1,2 , 3 , 4 , 5 Chan English Tertiary Teaching , learning 2 , 3 , 5 Curtis English Tertiary Research da Silva English Generic Research 3 , 5 3 , 5 Dwivedi et al . English Generic Research 1 , 2 , 3 , 4 , 5 , 6 Fischer English Generic Administration 1 , 2 , 3 , 4 , 5 Krüger et al . English Generic Teaching , learning , research 1 , 2 , 3 , 5 7 Lim et al . English Generic Teaching 1 , 2 , 3 Masters ( a ) English Generic Teaching , administration 1 , 2 , 3 , 4 , 5 Masters ( b ) O ’ Connor & ChatGPT Tlili et al . Zumsteg & Junn English Generic Research 3 , 4 English Generic Teaching , learning , research 3 , 5 English Generic Teaching , learning 1 , 2 , 3 , 4 , 5 , English Tertiary Teaching , learning 3 , 4 , 5 Database : arXiv Chan & Hu English Tertiary Teaching , learning 1 , 2 , 3 , 4 , 5 Latif et al . English Generic Teaching 1 , 2 , 3 , 4 , 5 Li et al . English Generic Teaching , learning , research 1 , 2 , 3 , 4 , 5 Ojha et al . English Generic Teaching Sharma et al . English Generic Administration Sharples English Generic Teaching , learning Database : CiNii Kashimura Japanese Generic Teaching Kondo et al . Japanese Secondary Teaching , learning Yanase Japanese Generic Research Database : CNKI Huang Chinese Generic Teaching , learning Song & Lin Chinese Generic Teaching Xun Chinese Tertiary Teaching , learning Zhu & Yang Chinese Generic Teaching , learning 4 , 5 3 , 4 3 , 5 1 , 2 , 3 3 , 5 3 , 5 1 , 3 , 5 2 , 3 , 5 1 , 3 , 4 , 5 1 , 2 , 3 , 5 In terms of the focus of ethical issues , the majority of papers were concerned with # 3 misinformation harms ( n=25 ) including academic integrity , cheating and other assessment issues , and the users ’ role in identifying and clarifying information and/or # 5 human-computer interaction related harms ( n=24 ) such as addiction , dependence , and cognitive overload . To illustrate this further we divided the papers into four themes concerning teaching , learning , research , and administration . In the following section we sum up the key concerns and areas of discussion in literature . 8 Teaching Under teaching , it was noted that ChatGPT exhibits versatile applications , such as personalized and interactive learning , curriculum design , assessing homework , exams , and essays ( Ojha et at. , 2023 ; Kashiwamura , 2023 ; Huang , 2023 ) . To design new programs or to provide personalized teaching , universities need to collect and process vast amounts of student data , often without students ’ consent . This gives rise to questions surrounding data privacy and security ( Chan , 2023 ; Masters , 2023a ) . There is a need to ensure that robust data protection measures are in place to safeguard sensitive information and prevent its misuse . Research by Latif et al . ( 2023 ) highlighted that AI may inadvertently reinforce existing societal inequalities , gender bias , and nationality bias embedded within the original training data , negatively impacting the outputs of educational applications downstream . Undue reliance on AI-generated evaluations may compromise the quality of assessments , potentially failing to accurately gauge students ’ true capabilities ( Busch et al. , 2023 ; Curtis , 2023 ; Song & Lin , 2023 ) . Additionally , the introduction of AI in the educational process raises concerns about the dynamics between educators and students . A possible overreliance on AI-generated content , as discussed by Sharples ( 2023 ) , could alter the traditional teacher-student relationship and diminish educators ’ creative input and uniqueness in designing engaging lesson plans and activities . Learning The application of ChatGPT in learning includes personalized learning experiences , student support , language assistance , tutoring , content creation , grading and assessment , research aid , and career counseling , enhancing the learning process for students ( Kooli , 2023 ; 9 Lim et al. , 2023 ) . One major concern is the potential for an increase in plagiarism and cheating among students who might rely on GAI-generated content for essays and exams , thereby compromising the authenticity of their work ( Li et al. , 2023 ; Zhu & Yang , 2023 ) . This overreliance on ChatGPT may lead to a decline in students ’ sense of responsibility and commitment to academic integrity ( Ojha et al. , 2023 ) . Excessive use of ChatGPT may have adverse effects on students ’ critical thinking skills . If students heavily depend on AI-generated content , they might lose the ability to independently analyze and evaluate information ( Tlili et al. , 2023 ) . Another significant issue raised in the literature pertains to the risk of misinformation being propagated due to the highly persuasive and convincing nature of AI-generated content . This can lead to potential bias or manipulation of information presented to students . Reliance on AI interactions for academic or social purposes might diminish face- to-face interactions , potentially hindering the development of essential social skills among students . Striking a balance between AI and human interactions is crucial to foster a well- rounded educational experience . ChatGPT might inadvertently produce content that inaccurately or inappropriately represents certain cultural or identity groups , highlighting the need for ongoing refinement and sensitivity in AI language model development ( Busch et al. , 2023 ) . Research The applications of ChatGPT encompass efficient dataset analysis , code generation , literature reviews , timesaving for experimental design focus , and advancements in research discovery and development ( Dwivedi et al. , 2023 ; Li et al. , 2023 ) . In research , the integration of AI in academic publishing poses the risk of displacing human authors and undermining the value of their expertise , potentially impacting the 10 credibility of research . The attribution of fake references to AI-generated content presents another challenge , leading to misinformation and a decline in trust in academic sources . ( Curtis , 2023 ) . Joint authorship of editorial pieces like O ’ Connor and ChatGPT ( 2023 ) is a contentious as it challenges the established core values related to human-based authorship in academic publishing ( da Silva , 2023 ) . Some conferences permit the use of ChatGPT for writing papers , but only when ChatGPT itself is the subject of empirical research ( e.g. , ICML , 2023 ) . On the other hand , some research communities , such as the Association for Computational Linguistics ( ACL , 2023 ) , allow the use of ChatGPT based on specific guidelines . Administration ChatGPT can significantly reduce the time spent on human administrative tasks , such as responding to queries from applicants and assisting students in course enrollment ( UNESCO IESALC , 2023 ) . However , there are concerns surrounding the equitable , reliable , and transparent use of ChatGPT . Utilizing ChatGPT in the admissions processes can potentially introduce biases , especially if the AI model was trained on historical data that reflects past inequalities ( Fischer , 2023 ; Sharma et al . 2023 ) . To ensure fairness , transparency , and accountability , it is essential to provide applicants with clear explanations of how AI was employed to assess their applications and the specific factors that contributed to their acceptance or rejection . Additionally , the use of AI algorithms in admissions decisions carries the risk of inadvertently favoring applicants with certain characteristics or backgrounds , potentially impacting diversity and inclusion efforts within the university ( Busch et al. , 2023 ; Fischer , 2023 ) . Data privacy and security are also paramount considerations . To avoid unintentional discrimination , institutions should actively assess and address any biases in the AI model ’ s training data and decision-making process , striving to provide equal opportunities for all applicants . 11 DISCUSSION AND CONCLUSION We focused on articles written in a very short period , the first 7 months of 2023 , but covered literature written in English , Chinese and Japanese . Given that Chat GPT is trained on English-centric data ( Brown et al. , 2020 ) , it is important to gain insights into discussions going on in other AI technologically advanced countries . However , our review revealed that a few academic and research studies are published in non-English languages , particularly in Chinese and Japanese . Our scoping review showed that there are already publications that are considering the ethical implications of GAI , especially ChatGPT , in education generally and some focused on higher education . The majority of papers are discussion pieces , but there is some early empirical work . The ethical issues highlighted in these works are mainly concerned about academic integrity , assessment issues , and data-protection . Our analysis highlights the urgency of addressing ethical issues surrounding the use of GAI/ChatGPT in education . Collaboration among stakeholders is essential to establish clear guidelines , protect student privacy , and promote responsible AI use . By doing so , AI can enhance education and research without compromising fundamental principles . References ACL . ( 2023 ) . ACL 2023 Policy . 61st Annual Meeting of the Association for Computational Linguistics . Retrieved from https : Arksey , H. , & O'Malley , L. ( 2005 ) . Scoping studies : towards a methodological framework . 12 International journal of social research methodology , 8 ( 1 ) , 19-32 . Brown , T. , Mann , B. , Ryder , N. , Subbiah , M. , Kaplan , J. D. , Dhariwal , P. , ... & Amodei , D. ( 2020 ) . Language models are few-shot learners . Advances in Neural Information Processing Systems , 33 , 1877–1901 . Busch , F. , Adams , L. C. , & Bressem , K. K. ( 2023 ) . Biomedical ethical aspects towards the implementation of Artificial Intelligence in medical education . Med . Sci . Educ . Advance Online Publication . https : Chan , C. K. Y . ( 2023 ) . A comprehensive AI policy education framework for university teaching and learning . International Journal of Educational Technology in Higher Education , 20 ( 38 ) . https : Chan , C. K. Y. , & Hu , W. ( 2023 ) . Students ’ voices on generative AI : Perceptions , benefits , and challenges in higher education . arXiv preprint arXiv:2305.00290 . Chung , H. W. , Hou , L. , Longpre , S. , Zoph , B. , Tay , Y. , Fedus , W. , ... & Wei , J . ( 2022 ) . Scaling instruction-finetuned language models . arXiv preprint arXiv:2210.11416 . Curtis , N. ( 2023 ) . To ChatGPT or not to ChatGPT ? The Impact of Artificial Intelligence on Academic Publishing . The Pediatric Infectious Disease Journal , 42 ( 4 ) , 275. https : da Silva , J . A. T. ( 2023 ) . Is ChatGPT a valid author ? . Nurse Education in Practice , 68 , 103600. https : . Devlin , J. , Chang , M. W. , Lee , K. , & Toutanova , K. ( 2018 ) . Bert : Pre-training of deep bidirectional transformers for language understanding . arXiv preprint arXiv:1810.04805 . Dwivedi , Y. K. , Kshetri , N. , Hughes , L. , Slade , E. L. , Jeyaraj , A. , Kar , A. K. , ... & Wright , R. ( 2023 ) . “ So what if ChatGPT wrote it ? ” Multidisciplinary perspectives on opportunities , challenges and implications of generative conversational AI for research , 13 practice and policy . International Journal of Information Management , 71 , 102642. https : Fischer , I . ( 2023 ) . Evaluating the ethics of machines assessing humans . Journal of Information Technology Teaching Cases , 0 ( 0 ) . https : Huang R. ( 2023 ) . Ren Gong Zhi Neng Zheng Jia Su Jiao Yu Bian Ge : Xian Shi Tiao Zhan Yu Ying Dui Ju Cuo [ Artificial intelligence is accelerating educational transformation : Realistic challenges and countermeasures ] . Journal of the Chinese Society of Education , ( 06 ) , 26-33 . ICML . ( 2023 ) . Call for Papers . International Conference on Machine Learning . Retrieved from https : Kaplan , J. , McCandlish , S. , Henighan , T. , Brown , T. B. , Chess , B. , Child , R. , Gray , S. , Radford , A. , Wu , J. , & Amodei , D. ( 2020 ) . Scaling laws for neural language models . Advances in neural information processing systems , 33 , 1877–1901 . Kashiwamura , Y . ( 2023 ) . Sozoteki sagyo e shifuto o unagasu kyoin no noryoku kojo ga kadai ni : Kyoiku o kaeru seisei AI [ Encouraging a shift to creative work : Challenges for teacher capacity building : Generating AI that Changes Education ] . The Economist , 101 ( 24 ) , 98-100 . Kondo , C. , Tamada , K. , & Matsuda , T. ( 2023 ) . Seiseikei AI o daizai toshita joho-teki na mikata・kangaekata ni motozuku mondai kaiketsu shido jissen : ChatGPT to no kyo- zon o kangaeru [ Practicing problem-solving instruction based on informational perspectives and ways of thinking : Consider coexistence with ChatGPT ] . Journal of the Japan Society for Educational Technology , ( 2 ) , 255-258. https : Kooli , C. ( 2023 ) . Chatbots in education and research : A critical examination of ethical implications and solutions . Sustainability , 15 ( 7 ) , 5614 . 14 Krüger , L. , Krotsetis , S. , OpenAI ’ s Generative Pretrained Transformer 3 ( GPT-3 ) Model , & Nydahl , P. ( 2023 ) . ChatGPT : Fluch oder Segen in der Pflege ? [ ChatGPT : curse or blessing in nursing care ? ] . Medizinische Klinik , Intensivmedizin und Notfallmedizin , 10.1007/s00063-023-01038-3 . Advance online publication . https : Latif , E. , Mai , G. , Nyaaba , M. , Wu , X. , Liu , N. , Lu , G. , ... & Zhai , X . ( 2023 ) . Artificial general intelligence ( AGI ) for education . arXiv preprint arXiv:2304.12479 . Li , L. , Ma , Z. , Fan , L. , Lee , S. , Yu , H. , & Hemphill , L. ( 2023 ) . ChatGPT in education : A discourse analysis of worries and concerns on social media . arXiv preprint arXiv:2305.02201 . Lim , W. M. , Gunasekara , A. , Pallant , J. L. , Pallant , J. I. , & Pechenkina , E. ( 2023 ) . Generative AI and the future of education : Ragnarök or reformation ? A paradoxical perspective from management educators . The International Journal of Management Education , 21,100790. https : Masters , K. ( 2023 a ) . Ethical use of Artificial Intelligence in health professions education : AMEE Guide No . 158 . Medical Teacher , 45 ( 6 ) , 574-584. https : Masters , K. ( 2023 b ) . Medical teacher ’ s first ChatGPT ’ s referencing hallucinations : Lessons for editors , reviewers , and teachers . Medical Teacher , Med Teach , 45 ( 7 ) , 673-675 . DOI : 10.1080/0142159X.2023.2208731 Munn , L. ( 2023 ) . The uselessness of AI ethics . AI Ethics , 3 , 869–877 . https : Nozza , D. , Bianchi , F. , & Hovy , D. ( 2022 ) . Pipelines for social bias testing of large language models . In Proceedings of BigScience Episode # 5 -- Workshop on Challenges & Perspectives in Creating Large Language Models , virtual+Dublin . Association for 15 Computational Linguistics , 68-74 . O ’ Connor , S & ChatGPT . ( 2022 ) . Open artificial intelligence platforms in nursing education : Tools for academic progress or abuse ? . Nurse Education in Practice , 66 , 103537. https : Ojha , S. , Narendra , A. , Mohapatra , S. , & Misra , I . ( 2023 ) . From robots to books : An introduction to smart applications of AI in education ( AIEd ) . arXiv preprint arXiv:2301.10026 . Ouyang , L. , Wu , J. , Jiang , X. , Almeida , D. , Wainwright , C. , Mishkin , P. , ... & Lowe , R. ( 2022 ) . Training language models to follow instructions with human feedback . Advances in Neural Information Processing Systems , 35 , 27730-27744 . Radford , A. , & Narasimhan , K. ( 2018 ) . Improving Language Understanding by Generative Pre- Training . Retrieved from https : Sharma , P. , Thapa , K. , Dhakal , P. , Upadhaya , M. D. , Adhikari , S. , & Khanal , S. R. ( 2023 ) . Performance of ChatGPT on USMLE : Unlocking the potential of large language models for AI-assisted medical education . arXiv preprint arXiv:2307.00112 . Sharples , M. ( 2023 ) . Towards social generative AI for education : theory , practices and ethics . arXiv preprint arXiv:2306.10063 . Song , H & Lin , M ( 2023 ) . ChatGPT/Chuangshengshi rengong zhineng shidai xia jiaoshi de gongzuo biange : Jiyu , tiaozhan yu yingdui [ The Transformation of Teachers ’ Work in the Era of ChatGPT/AIGC : Opportunities , Challenges , and Responses ] . Journal of East China Normal University ( Educational Sciences ) , ( 07 ) ,78-90. https : . Tlili , A. , Shehata , B. , Adarkwah , M. A. , Bozkurt , A. , Hickey , D. T. , Huang , R. , & Agyemang , B . ( 2023 ) . What if the devil is my guardian angel : ChatGPT as a case study of using chatbots in education . Smart Learning Environments , 10 ( 1 ) , 15 . 16 Tricco , A. C. , Lillie , E. , Zarin , W. , O ’ brien , K. , Colquhoun , H. , Kastner , M. , ... & Straus , S. E. ( 2016 ) . A scoping review on the conduct and reporting of scoping reviews . BMC medical research methodology , 16 , 1-10 . UNESCO IESALC ( 2023 ) . ChatGPT and artificial intelligence in higher education : Quick start guide . Retrieved from https : Vaswani , A. , Shazeer , N. , Parmar , N. , Uszkoreit , J. , Jones , L. , Gomez , A. N. , Kaiser , Ł. , & Polosukhin , I . ( 2017 ) . Attention is all you need . Advances in neural information processing systems , 30. https : Weidinger , L. , Mellor , J. , Rauh , M. , Griffin , C. , Uesato , J. , Huang , P. S. , ... & Gabriel , I . ( 2021 ) . Ethical and social risks of harm from language models . arXiv preprint arXiv:2112.04359 . Xun , Y . ( 2023 ) . ChatGPT/Chuangshengshi rengong zhineng yu gaodeng jiaoyu de jiazhi he shiming [ ChatGPT/AIGC and the Value and Mission of Higher Education ] . Journal of East China Normal University ( Educational Sciences ) , ( 07 ) , 56-63 . Yanase , Y . ( 2023 ) . AI o katsuyo shite Eigo ronbun o sakusei suru Nihongo shasha ni totte no kadai to sono taisho [ Challenges and Strategies for Japanese Speakers Creating English Papers Using AI ] . Journal of Information Science and Technology , 73 ( 6 ) , 219-224 . Zhu , Y & Yang , F. ( 2023 ) . ChatGPT/Chuangshengshi rengong zhineng yu jiaoyu chuangxin : Jiyu , tiaozhan yiji weilai . [ ChatGPT/AIGC and Educational Innovation : Opportunities , Challenges , and the Future ] . Journal of East China Normal University ( Educational Sciences ) , ( 07 ) ,1-14. https : //doi.org/10.16382/j.cnki.1000- 5560.2023.07.001 . Zumsteg , J. M. , & Junn , C. ( 2023 ) . Will ChatGPT match to your program . Am J Phys Med Rehabil , 1 , 3-7 . 17","['ethical', 'implication', 'chatgpt', 'high', 'education', 'scope', 'review', 'ming', 'beverley', 'transdisciplinary', 'graduate', 'degree', 'program', 'global', 'initiative', 'graduate', 'school', 'education', 'ethic', 'generative', 'artificial', 'intelligence', 'high', 'education', 'scope', 'review', 'scope', 'review', 'explore', 'ethical', 'challenge', 'use', 'chatgpt', 'education', 'focus', 'particularly', 'issue', 'relate', 'high', 'education', 'review', 'recent', 'academic', 'article', 'write', 'aim', 'provide', 'comprehensive', 'overview', 'relevant', 'research', 'identify', 'gap', 'future', 'consideration', 'draw', 'arksey', 'malley', 'fivestage', 'scope', 'review', 'framework', 'identify', 'research', 'question', 'search', 'term', 'conduct', 'article', 'search', 'database', 'target', 'language', 'article', 'review', 'least', 'researcher', 'identify', 'main', 'ethical', 'issue', 'utilize', 'ai', 'education', 'particularly', 'high', 'education', 'analysis', 'ethical', 'issue', 'follow', 'framework', 'develop', 'deepmind', 'weiginger', 'identify', 'main', 'area', 'ethical', 'concern', 'language', 'model', 'majority', 'paper', 'concern', 'misinformation', 'harm', 'humancomputer', 'interaction', 'relate', 'harm', 'give', 'rapid', 'deployment', 'generative', 'artificial', 'intelligence', 'gai', 'imperative', 'educator', 'conduct', 'empirical', 'study', 'develop', 'sound', 'ethical', 'policy', 'use', 'gai', 'introduction', 'recent', 'wave', 'generative', 'artificial', 'intelligence', 'gai', 'originate', 'transformer', 'architecture', 'neural', 'network', 'vaswani', 'transformer', 'base', 'selfattention', 'mechanism', 'highly', 'scalable', 'suitable', 'parallel', 'computing', 'hence', 'quickly', 'become', 'approach', 'field', 'natural', 'language', 'processing', 'computer', 'vision', 'language', 'model', 'radford', 'usher', 'era', 'selfsupervise', 'learning', 'pretraine', 'largescale', 'textual', 'datum', 'learn', 'fundamental', 'knowledge', 'finetune', 'specific', 'downstream', 'task', 'qa', 'dialog', 'machine', 'translation', 'system', 'validate', 'large', 'language', 'model', 'scale', 'law', 'assert', 'performance', 'improvement', 'almost', 'always', 'achieve', 'increase', 'scale', 'model', 'parameter', 'pretraine', 'textual', 'datum', 'book', 'internet', 'resource', 'rapid', 'expansion', 'size', 'large', 'language', 'model', 'begin', 'model', 'scale', 'grow', '03b', 'parameter', 'brown', 'release', 'gpt3', 'release', 'openai', 'deepmind', 'lead', 'developer', 'technology', 'become', 'less', 'satisfied', 'mere', 'scale', 'increase', 'seek', 'align', 'model', 'output', 'feedback', 'real', 'human', 'text', 'generate', 'model', 'become', 'increasingly', 'humanlike', 'content', 'evermore', 'closely', 'align', 'human', 'value', 'launch', 'chatgpt', 'make', 'public', 'aware', 'already', 'capable', 'generate', 'humanquality', 'conversation', 'retrieve', 'store', 'knowledge', 'demand', 'achieve', 'natural', 'interaction', 'people', 'assistant', 'kick', 'current', 'frenzy', 'adapt', 'utilization', 'gai', 'various', 'field', 'include', 'education', 'application', 'high', 'education', 'garner', 'attention', 'iesalc', 'much', 'discussion', 'possible', 'benefit', 'create', 'teaching', 'material', 'analyze', 'student', 'datum', 'identify', 'learn', 'pattern', 'evidence', 'base', 'unclear', 'yet', 'gai', 'study', 'reveal', 'potential', 'risk', 'associate', 'generation', 'incorrect', 'information', 'know', 'hallucination', 'issue', 'bias', 'include', 'race', 'nationality', 'gender', 'discriminatory', 'content', 'nozza', 'gaigeneration', 'output', 'use', 'educationrelated', 'procedure', 'possibility', 'problematic', 'content', 'bias', 'assumption', 'magnify', 'result', 'negative', 'consequence', 'learner', 'educator', 'researcher', 'administrator', 'therefore', 'crucial', 'discuss', 'assess', 'ethical', 'implication', 'implement', 'chatgpt', 'educational', 'institution', 'especially', 'concern', 'use', 'teaching', 'learn', 'research', 'administration', 'increase', 'use', 'gai', 'learner', 'raise', 'issue', 'relate', 'academic', 'integrity', 'definition', 'authorship', 'assessment', 'method', 'pedagogical', 'implication', 'also', 'affect', 'researcher', 'conduct', 'study', 'generate', 'output', 'well', 'decision', 'make', 'admission', 'hire', 'educational', 'institution', 'manage', 'run', 'furthermore', 'increase', 'integration', 'ai', 'education', 'even', 'raise', 'question', 'continue', 'relevance', 'traditional', 'brickandmortar', 'educational', 'institution', 'scope', 'review', 'explore', 'ethical', 'challenge', 'use', 'chatgpt', 'education', 'focus', 'particularly', 'high', 'education', 'review', 'academic', 'article', 'write', 'aim', 'map', 'current', 'state', 'field', 'identify', 'gap', 'future', 'consideration', 'methodology', 'scope', 'review', 'commonly', 'use', 'identify', 'key', 'issue', 'newly', 'emerge', 'field', 'yet', 'substantial', 'body', 'literature', 'use', 'identify', 'knowledge', 'gap', 'set', 'research', 'agenda', 'identify', 'implication', 'decisionmake', 'study', 'adopt', 'arksey', 'malley', 'fivestage', 'scope', 'review', 'framework', 'involve', 'identify', 'initial', 'research', 'question', 'relevant', 'study', 'select', 'study', 'chart', 'datum', 'collate', 'summarizing', 'report', 'result', 'identify', 'relevant', 'study', 'limit', 'focus', 'article', 'focus', 'late', 'version', 'search', 'article', 'publish', 'use', 'search', 'term', 'chatgpt', 'generative', 'couple', 'education', 'ethic', 'see', 'table', 'capture', 'solid', 'evidencebased', 'study', 'discussion', 'topic', 'identify', 'scopus', 'main', 'database', 'conduct', 'initial', 'search', 'include', 'ongoing', 'research', 'work', 'also', 'include', 'arxiv', 'platform', 'provide', 'access', 'preprint', 'article', 'include', 'language', 'author', 'first', 'nearfirst', 'language', 'proficiency', 'japanese', 'chinese', 'facilitate', 'conduct', 'search', 'prominent', 'database', 'cinii', 'lead', 'ai', 'development', 'make', 'language', 'good', 'target', 'table', 'final', 'search', 'term', 'result', 'platform', 'database', 'search', 'term', 'titleabskey', 'chatgpt', 'generative', 'ai', 'titleabskey', 'education', 'titleabskey', 'chatgpt', 'generative', 'ai', 'titleabskey', 'education', 'ethic', 'arxiv', 'titleabskey', 'chatgpt', 'generative', 'ai', 'titleabskey', 'education', 'titleabskey', 'chatgpt', 'generative', 'ai', 'titleabskey', 'education', 'ethic', 'result', 'cinii', 'titleabskey', 'chatgpt', 'ai', 'titleabskey', '教育', 'titleabskey', 'chatgpt', 'ai', 'titleabskey', '課題', 'cnki', 'titleabskey', 'chatgpt', 'ai', 'titleabskey', 'education', 'titleabskey', 'chatgpt', 'ai', 'titleabskey', 'chart', 'datum', 'collation', 'initial', 'search', 'yield', 'result', 'include', 'education', 'ethical', 'concern', 'identify', 'article', 'meet', 'inclusion', 'criterion', 'figure', 'article', 'review', 'reviewer', 'third', 'reviewer', 'check', 'finding', 'figure', 'datum', 'extraction', 'process', 'analysis', 'ethical', 'issue', 'raise', 'article', 'rely', 'comprehensive', 'research', 'conduct', 'deepmind', 'weiginger', 'offer', 'framework', 'assess', 'ethical', 'social', 'risk', 'harm', 'arise', 'deployment', 'language', 'model', 'lm', 'table', 'area', 'discrimination', 'exclusion', 'toxicity', 'information', 'hazard', 'misinformation', 'harm', 'malicious', 'use', 'humancomputer', 'interaction', 'harm', 'automation', 'access', 'environmental', 'harm', 'table', 'ethical', 'social', 'risk', 'area', 'description', 'model', 'harm', 'reinforce', 'discrimination', 'stereotype', 'bias', 'marginalize', 'individual', 'promote', 'toxic', 'language', 'worsen', 'disparity', 'disadvantaged', 'group', 'leak', 'private', 'datum', 'sensitive', 'information', 'leak', 'provide', 'false', 'misleading', 'information', 'lead', 'less', 'informed', 'user', 'erode', 'trust', 'share', 'information', 'risk', 'use', 'lm', 'harm', 'include', 'enable', 'disinformation', 'campaign', 'personalize', 'scam', 'fraud', 'scale', 'development', 'malicious', 'computer', 'code', 'weapon', 'system', 'user', 'overestimation', 'ai', 'capability', 'lead', 'unsafe', 'usage', 'exploitation', 'manipulation', 'perpetuation', 'stereotype', 'unequal', 'benefit', 'limited', 'access', 'lm', 'impact', 'job', 'quality', 'creative', 'economy', 'create', 'global', 'disparity', 'risk', 'reward', 'finding', 'identify', 'paper', 'english', 'n19', 'follow', 'chinese', 'japanese', 'english', 'paper', 'empirical', 'study', 'conceptual', 'discussion', 'paper', 'predominant', 'focus', 'application', 'field', 'healthcare', 'medical', 'domain', 'comparison', 'number', 'chinese', 'japanese', 'paper', 'much', 'small', 'chinese', 'paper', 'general', 'discussion', 'application', 'predict', 'impact', 'chatgpt', 'education', 'japanese', 'article', 'report', 'initial', 'research', 'student', 'practical', 'experience', 'chatgpt', 'specification', 'paper', 'discuss', 'challenge', 'japanese', 'speaker', 'write', 'english', 'academic', 'paper', 'use', 'chatgpt', 'support', 'general', 'teaching', 'implication', 'kashimura', 'yanase', 'overall', 'little', 'discussion', 'specifically', 'focus', 'high', 'education', 'majority', 'paper', 'n19', 'generic', 'discuss', 'ethical', 'concern', 'teach', 'n19', 'learn', 'mostly', 'theoretical', 'conceptual', 'perspective', 'delve', 'specific', 'level', 'education', 'paper', 'specifically', 'focus', 'tertiary', 'education', 'concern', 'overall', 'pedagogical', 'implication', 'particularly', 'medical', 'education', 'n2', 'faculty', 'student', 'perception', 'research', 'implication', 'n1', 'table', 'article', 'review', 'author', 'language', 'education', 'level', 'main', 'areafocus', 'ethical', 'concern', 'database', 'english', 'tertiary', 'teaching', 'learn', 'administration', 'chan', 'english', 'tertiary', 'teaching', 'learn', 'curtis', 'english', 'tertiary', 'research', 'silva', 'generic', 'research', 'dwivedi', 'generic', 'research', 'fischer', 'krüger', 'generic', 'teaching', 'learn', 'research', 'generic', 'teaching', 'master', 'english', 'generic', 'teaching', 'administration', 'master', 'b', 'connor', 'chatgpt', 'tlili', 'english', 'generic', 'teaching', 'learn', 'research', 'english', 'generic', 'teaching', 'learn', 'english', 'tertiary', 'teaching', 'learn', 'database', 'english', 'tertiary', 'teaching', 'learn', 'generic', 'teaching', 'li', 'generic', 'teaching', 'learn', 'research', 'ojha', 'generic', 'teaching', 'administration', 'sharple', 'english', 'generic', 'teaching', 'learn', 'database', 'kondo', 'japanese', 'secondary', 'teaching', 'learn', 'yanase', 'japanese', 'generic', 'research', 'database', 'teaching', 'learning', 'song', 'teaching', 'chinese', 'tertiary', 'teaching', 'learn', 'generic', 'teaching', 'learn', 'term', 'focus', 'ethical', 'issue', 'majority', 'paper', 'concern', 'misinformation', 'harm', 'include', 'academic', 'integrity', 'cheating', 'assessment', 'issue', 'user', 'role', 'identify', 'clarify', 'information', 'andor', 'humancomputer', 'interaction', 'relate', 'harm', 'n24', 'addiction', 'dependence', 'cognitive', 'overload', 'illustrate', 'far', 'divide', 'paper', 'theme', 'concern', 'teaching', 'learn', 'research', 'administration', 'follow', 'section', 'sum', 'key', 'concern', 'area', 'discussion', 'literature', 'teaching', 'teach', 'note', 'chatgpt', 'exhibit', 'versatile', 'application', 'personalized', 'interactive', 'learning', 'curriculum', 'design', 'assess', 'homework', 'exam', 'essay', 'ojha', 'design', 'new', 'program', 'provide', 'personalized', 'teaching', 'university', 'need', 'collect', 'process', 'vast', 'amount', 'student', 'datum', 'often', 'student', 'consent', 'give', 'rise', 'question', 'surround', 'data', 'privacy', 'security', 'master', '2023a', 'need', 'ensure', 'robust', 'data', 'protection', 'measure', 'place', 'safeguard', 'sensitive', 'information', 'prevent', 'misuse', 'research', 'highlight', 'inadvertently', 'reinforce', 'exist', 'societal', 'inequality', 'gender', 'bias', 'nationality', 'bias', 'embed', 'original', 'training', 'datum', 'negatively', 'impact', 'output', 'educational', 'application', 'downstream', 'undue', 'reliance', 'aigenerate', 'evaluation', 'compromise', 'quality', 'assessment', 'potentially', 'fail', 'accurately', 'gauge', 'student', 'true', 'capability', 'curtis', 'song', 'additionally', 'introduction', 'ai', 'educational', 'process', 'raise', 'concern', 'dynamic', 'educator', 'student', 'possible', 'overreliance', 'aigenerate', 'content', 'discuss', 'sharple', 'alter', 'traditional', 'teacherstudent', 'relationship', 'diminish', 'educator', 'creative', 'input', 'uniqueness', 'design', 'engage', 'lesson', 'plan', 'activity', 'learn', 'application', 'chatgpt', 'learning', 'include', 'personalized', 'learn', 'experience', 'student', 'support', 'language', 'assistance', 'tutoring', 'content', 'creation', 'grade', 'assessment', 'research', 'aid', 'career', 'counseling', 'enhance', 'learning', 'process', 'student', 'major', 'concern', 'potential', 'increase', 'plagiarism', 'cheat', 'student', 'rely', 'gaigenerate', 'content', 'essay', 'exam', 'thereby', 'compromise', 'authenticity', 'work', 'overreliance', 'chatgpt', 'lead', 'decline', 'student', 'sense', 'responsibility', 'commitment', 'academic', 'integrity', 'ojha', 'excessive', 'use', 'chatgpt', 'adverse', 'effect', 'student', 'critical', 'thinking', 'skill', 'student', 'heavily', 'depend', 'aigenerate', 'content', 'lose', 'ability', 'independently', 'analyze', 'evaluate', 'information', 'tlili', 'significant', 'issue', 'raise', 'literature', 'pertain', 'risk', 'misinformation', 'propagate', 'highly', 'persuasive', 'convincing', 'nature', 'aigenerate', 'content', 'lead', 'potential', 'bias', 'manipulation', 'information', 'present', 'student', 'reliance', 'ai', 'interaction', 'academic', 'social', 'purpose', 'diminish', 'face', 'toface', 'interaction', 'potentially', 'hinder', 'development', 'essential', 'social', 'skill', 'student', 'strike', 'balance', 'ai', 'human', 'interaction', 'crucial', 'foster', 'well', 'round', 'educational', 'experience', 'chatgpt', 'inadvertently', 'produce', 'content', 'inaccurately', 'inappropriately', 'represent', 'certain', 'cultural', 'identity', 'group', 'highlight', 'need', 'ongoing', 'refinement', 'sensitivity', 'language', 'model', 'research', 'application', 'chatgpt', 'encompass', 'efficient', 'dataset', 'analysis', 'code', 'generation', 'literature', 'review', 'timesave', 'experimental', 'design', 'focus', 'advancement', 'research', 'discovery', 'development', 'research', 'integration', 'ai', 'academic', 'publishing', 'pose', 'risk', 'displace', 'human', 'author', 'undermine', 'value', 'expertise', 'potentially', 'impact', 'credibility', 'research', 'attribution', 'fake', 'reference', 'aigenerate', 'content', 'present', 'challenge', 'lead', 'misinformation', 'decline', 'trust', 'academic', 'source', 'curtis', 'joint', 'authorship', 'editorial', 'piece', 'connor', 'chatgpt', 'contentious', 'challenge', 'establish', 'core', 'value', 'relate', 'humanbase', 'authorship', 'academic', 'publishing', 'silva', 'conference', 'permit', 'use', 'chatgpt', 'write', 'paper', 'chatgpt', 'subject', 'empirical', 'research', 'eg', 'icml', 'hand', 'research', 'community', 'association', 'computational', 'linguistic', 'allow', 'use', 'chatgpt', 'base', 'specific', 'guideline', 'administration', 'chatgpt', 'significantly', 'reduce', 'time', 'spend', 'human', 'administrative', 'task', 'respond', 'query', 'applicant', 'assist', 'student', 'course', 'enrollment', 'however', 'concern', 'surround', 'equitable', 'reliable', 'transparent', 'use', 'chatgpt', 'utilize', 'chatgpt', 'admission', 'process', 'potentially', 'introduce', 'bias', 'especially', 'model', 'train', 'historical', 'datum', 'reflect', 'past', 'inequality', 'fischer', 'ensure', 'fairness', 'transparency', 'accountability', 'essential', 'provide', 'applicant', 'clear', 'explanation', 'employ', 'assess', 'application', 'specific', 'factor', 'contribute', 'acceptance', 'rejection', 'additionally', 'use', 'ai', 'algorithm', 'admission', 'decision', 'carry', 'risk', 'inadvertently', 'favor', 'applicant', 'certain', 'characteristic', 'background', 'potentially', 'impact', 'diversity', 'inclusion', 'effort', 'university', 'fischer', 'datum', 'privacy', 'security', 'also', 'paramount', 'consideration', 'avoid', 'unintentional', 'discrimination', 'institution', 'actively', 'assess', 'address', 'bias', 'model', 'training', 'datum', 'decisionmake', 'process', 'strive', 'provide', 'equal', 'opportunity', 'applicant', 'discussion', 'conclusion', 'focus', 'article', 'write', 'short', 'period', 'first', 'month', 'cover', 'literature', 'write', 'give', 'chat', 'gpt', 'train', 'englishcentric', 'datum', 'important', 'gain', 'insight', 'discussion', 'go', 'ai', 'technologically', 'advanced', 'country', 'however', 'review', 'reveal', 'academic', 'research', 'study', 'publish', 'nonenglish', 'language', 'particularly', 'chinese', 'japanese', 'scoping', 'review', 'show', 'already', 'publication', 'consider', 'ethical', 'implication', 'especially', 'chatgpt', 'education', 'generally', 'focus', 'high', 'education', 'majority', 'paper', 'discussion', 'piece', 'early', 'empirical', 'work', 'ethical', 'issue', 'highlight', 'work', 'mainly', 'concerned', 'academic', 'integrity', 'assessment', 'issue', 'dataprotection', 'analysis', 'highlight', 'urgency', 'address', 'ethical', 'issue', 'surround', 'use', 'gaichatgpt', 'education', 'collaboration', 'stakeholder', 'essential', 'establish', 'clear', 'guideline', 'protect', 'student', 'privacy', 'promote', 'responsible', 'use', 'enhance', 'education', 'research', 'compromise', 'fundamental', 'principle', 'reference', 'policy', '61st', 'annual', 'meeting', 'association', 'computational', 'linguistic', 'retrieve', 'https', 'arksey', 'h', 'l', 'scope', 'study', 'methodological', 'framework', 'international', 'journal', 'social', 'research', 'methodology', 'ryder', 'dhariwal', 'p', 'amodei', 'language', 'model', 'fewshot', 'learner', 'advance', 'neural', 'information', 'processing', 'system', 'adam', 'l', 'c', 'bressem', 'biomedical', 'ethical', 'aspect', 'implementation', 'artificial', 'intelligence', 'medical', 'education', 'educ', 'advance', 'online', 'publication', 'https', 'comprehensive', 'policy', 'education', 'framework', 'university', 'teaching', 'learn', 'international', 'journal', 'educational', 'technology', 'high', 'education', 'https', 'w', 'student', 'voice', 'generative', 'ai', 'perception', 'benefit', 'challenge', 'high', 'education', 'arxiv', 'preprint', 'l', 'zoph', 'tay', 'scale', 'instructionfinetune', 'language', 'model', 'arxiv', 'preprint', 'curtis', 'n', 'chatgpt', 'chatgpt', 'impact', 'artificial', 'intelligence', 'academic', 'publishing', 'pediatric', 'infectious', 'disease', 'journal', 'https', 'j', 'chatgpt', 'valid', 'author', 'nurse', 'education', 'practice', 'https', 'pretraining', 'deep', 'bidirectional', 'transformer', 'language', 'understand', 'arxiv', 'preprint', 'kshetri', 'hughe', 'l', 'slade', 'l', 'kar', 'k', 'wright', 'r', 'chatgpt', 'write', 'multidisciplinary', 'perspective', 'opportunity', 'challenge', 'implication', 'generative', 'conversational', 'ai', 'research', 'practice', 'policy', 'international', 'journal', 'information', 'management', 'https', 'fischer', 'evaluate', 'ethic', 'machine', 'assess', 'human', 'journal', 'information', 'technology', 'teaching', 'case', 'https', 'artificial', 'intelligence', 'accelerate', 'educational', 'transformation', 'realistic', 'challenge', 'countermeasure', 'journal', 'chinese', 'society', 'education', 'icml', 'call', 'paper', 'international', 'conference', 'machine', 'learning', 'retrieve', 'https', 'mccandlish', 'b', 'chess', 'child', 'r', 'gray', 'radford', 'amodei', 'scale', 'law', 'neural', 'language', 'model', 'advance', 'neural', 'information', 'processing', 'system', 'sagyo', 'unagasu', 'kyoin', 'encourage', 'shift', 'creative', 'work', 'challenge', 'teacher', 'capacity', 'building', 'generate', 'change', 'education', 'economist', 'tamada', 'seiseikei', 'ai', 'daizai', 'chatgpt', 'practice', 'problemsolving', 'instruction', 'base', 'informational', 'perspective', 'way', 'thinking', 'consider', 'coexistence', 'chatgpt', 'journal', 'educational', 'technology', 'https', 'c', 'chatbot', 'education', 'research', 'critical', 'examination', 'ethical', 'implication', 'solution', 'sustainability', 'krüger', 'l', 'krotsetis', 'generative', 'pretraine', 'transformer', 'gpt3', 'model', 'nydahl', 'p', 'chatgpt', 'fluch', 'oder', 'segen', 'pflege', 'chatgpt', 'curse', 'blessing', 'nursing', 'care', 'advance', 'online', 'publication', 'mai', 'lu', 'artificial', 'general', 'intelligence', 'agi', 'education', 'arxiv', 'preprint', 'fan', 'l', 'chatgpt', 'education', 'discourse', 'analysis', 'worry', 'concern', 'social', 'medium', 'arxiv', 'preprint', 'gunasekara', 'pallant', 'l', 'pallant', 'j', 'e', 'generative', 'ai', 'future', 'education', 'ragnarök', 'reformation', 'paradoxical', 'perspective', 'management', 'educator', 'international', 'journal', 'management', 'education', 'https', 'master', 'k', 'ethical', 'use', 'artificial', 'intelligence', 'health', 'profession', 'guide', 'medical', 'teacher', 'https', 'master', 'k', 'b', 'medical', 'teacher', 'reference', 'hallucination', 'lesson', 'editor', 'reviewer', 'teacher', 'medical', 'teacher', 'teach', 'l', 'uselessness', 'ethic', 'ai', 'ethic', 'https', 'nozza', 'bianchi', 'hovy', 'pipeline', 'social', 'bias', 'testing', 'large', 'language', 'model', 'proceeding', 'bigscience', 'episode', 'workshop', 'challenge', 'perspective', 'create', 'large', 'language', 'model', 'computational', 'linguistic', 'connor', 'chatgpt', 'open', 'artificial', 'intelligence', 'platform', 'nursing', 'education', 'tool', 'academic', 'progress', 'abuse', 'nurse', 'education', 'practice', 'https', 'ojha', 'mohapatra', 'misra', 'robot', 'book', 'introduction', 'smart', 'application', 'ai', 'education', 'aie', 'arxiv', 'preprint', 'wainwright', 'c', 'mishkin', 'p', 'lowe', 'r', 'training', 'language', 'model', 'follow', 'instruction', 'human', 'feedback', 'advance', 'neural', 'information', 'processing', 'system', 'radford', 'narasimhan', 'k', 'improve', 'language', 'understanding', 'generative', 'pre', 'training', 'retrieve', 'https', 'thapa', 'k', 'dhakal', 'upadhaya', 'khanal', 'r', 'performance', 'chatgpt', 'usmle', 'unlock', 'potential', 'large', 'language', 'model', 'aiassiste', 'medical', 'education', 'arxiv', 'preprint', 'sharple', 'social', 'generative', 'ai', 'education', 'theory', 'practice', 'ethic', 'arxiv', 'preprint', 'arxiv230610063', 'song', 'h', 'yingdui', 'transformation', 'teacher', 'work', 'era', 'chatgptaigc', 'opportunity', 'challenge', 'response', 'journal', 'university', 'educational', 'science', 'https', 'tlili', 'shehata', 'bozkurt', 'hickey', 'r', 'agyemang', 'devil', 'guardian', 'angel', 'chatgpt', 'case', 'study', 'use', 'chatbot', 'education', 'smart', 'learn', 'environment', 'tricco', 'c', 'e', 'colquhoun', 'h', 'kastner', 'straus', 'e', 'scope', 'review', 'conduct', 'reporting', 'scope', 'review', 'bmc', 'medical', 'research', 'methodology', 'iesalc', 'chatgpt', 'artificial', 'intelligence', 'high', 'education', 'quick', 'start', 'guide', 'retrieve', 'https', 'shazeer', 'gomez', 'n', 'kaiser', 'polosukhin', 'attention', 'need', 'advance', 'neural', 'information', 'processing', 'system', 'https', 'weidinger', 'l', 'griffin', 'gabriel', 'ethical', 'social', 'risk', 'harm', 'language', 'model', 'arxiv', 'preprint', 'shime', 'chatgptaigc', 'value', 'mission', 'high', 'education', 'university', 'educational', 'science', 'yanase', 'ai', 'eigo', 'ronbun', 'sakusei', 'sono', 'taisho', 'challenge', 'strategy', 'japanese', 'speaker', 'create', 'english', 'paper', 'use', 'ai', 'journal', 'information', 'science', 'technology', 'chatgptaigc', 'educational', 'innovation', 'opportunity', 'challenge', 'future', 'journal', 'university', 'educational', 'science', 'https', 'c', 'chatgpt', 'match', 'program']"
"Mitigating Estimation Errors by Twin TD-Regularized Actor and Critic for
  Deep Reinforcement Learning","[{'href': 'http://arxiv.org/abs/2311.03711v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2311.03711v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-11-07 04:30:51,"𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in Search Queries and Query
Suggestions

Fabian Haak
fabian.haak@th-koeln.de
Technische Hochschule Köln
Cologne, Germany

Philipp Schaer
philipp.schaer@th-koeln.de
Technische Hochschule Köln
Cologne, Germany

ABSTRACT
This publication describes the motivation and generation of 𝑄𝑏𝑖𝑎𝑠 ,
a large dataset of Google and Bing search queries, a scraping tool
and dataset for biased news articles, as well as language models for
the investigation of bias in online search. Web search engines are a
major factor and trusted source in information search, especially
in the political domain. However, biased information can influence
opinion formation and lead to biased opinions. To interact with
search engines, users formulate search queries and interact with
search query suggestions provided by the search engines. A lack of
datasets on search queries inhibits research on the subject. We use
𝑄𝑏𝑖𝑎𝑠 to evaluate different approaches to fine-tuning transformer-
based language models with the goal of producing models capable of
biasing text with left and right political stance. Additionally to this
work we provided datasets and language models for biasing texts
that allow further research on bias in online information search.

CCS CONCEPTS
• Information systems → Web search engines; Query suggestion;
Query reformulation; • Computing methodologies → Natural
language generation.

KEYWORDS
web search, dataset, bias, query suggestion, search queries, language
models, transformers

ACM Reference Format:
Fabian Haak and Philipp Schaer. 2023. 𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in
Search Queries and Query Suggestions. In 15th ACM Web Science Conference
2023 (WebSci ’23), April 30-May 1, 2023, Evanston, TX, USA. ACM, New York,
NY, USA, 6 pages.

3
2
0
2

v
o
N
9
2

]

R

I
.
s
c
[

1
v
0
8
7
7
1
.
1
1
3
2
:
v
i
X
r
a

1 INTRODUCTION
Search engines such as Google and Bing are seen as trustworthy
sources of information on many topics, including political news and
information [14, 30]. Further, search engines have proven to have a
major impact on the formation of political opinions [15]. To interact
with search engines, users formulate search queries that are an
expression of their information need. Based on these queries search
engines usually provide a set of search query suggestions [28].
Queries and the choices users make when interacting with query
suggestions are based on the information need they want to satisfy,
which is often to support their personal opinions or beliefs founded
on previously encountered information [6]. This interaction process
and the results presented to the users are prone to be biased [7, 22,
25] and therefore the high level of trust can be seen as problematic.
However, the true effect of biased search queries and different
types of inherent biases on the actual list of search results has not
sufficiently been investigated. One of the main reasons for the in-
frequency of studies on bias in search queries might be a lack of
publicly available datasets. datasets that include real-world user
queries, query suggestions, and actual query reformulations are
rare. One of the reasons is that collecting search queries from users
is problematic due to privacy concerns. Although there are tech-
niques like pseudonymization, query logs enable the identification
of users [5]. Previously available datasets such as the AOL query
log dataset [29] are no longer available, and their usage is morally
debatable.

Using unpersonalized search query suggestions as proxies might
solve this issue. Query suggestions describe the list of predicted
queries suggested to users during the input of search queries by
the search engine, sometimes also called search predictions [37] or
query auto completion [10]. While these search query suggestions
can be generated locally from the result set [40] or be taken from
global knowledge bases [20], in web search suggestions are mostly
based on frequently issued queries by users [37]. It can therefore be
assumed that in many cases query suggestions are popular related
search queries for the initial root query that represented the user’s
interest in a topic or entity.

The U.S. news domain is a popular domain for bias research due
to its sociological relevance and the two-party left-right spectrum
that facilitates bias analysis [34]. One of the benefits of our dataset
is to enable in-depth investigations of the correlation between bias
in search queries and search results, as search results for popu-
lar web search engines can easily be collected using our provided
dataset of search queries. Therefore, one goal of this work is to
provide a large dataset of search query suggestions for the U.S. po-
litical news domain. Additionally, we provide a transformer-based
methodology for biasing text. Such a system can be a useful tool in

 
 
 
 
 
 
WebSci ’23, April 30-May 1, 2023, Evanston, TX, USA

Haak and Schaer

bias research, f.e. for generating biased derivatives of search queries.
Thus, we evaluate a range of fine-tuning scenarios to find settings,
that produce the most biased results. We provide the fine-tuned
transformer-based language models that are capable of inducing
left and right political stance bias in the form of lexical biases.

This publication describes the motivation and generation of
𝑄𝑏𝑖𝑎𝑠 , a large dataset of Google and Bing search queries, a scraping
tool and dataset for biased news articles, as well as language models
for the investigation of bias in online search.

The main contributions of 𝑄𝑏𝑖𝑎𝑠 and this paper are:

• Two datasets:1 (1) The (to the best of our knowledge) largest
labeled dataset of search query suggestions of a single do-
main for Google and Bing. (2) A dataset of biased news
articles of the U.S. political news domain.

• A scraping tool that allows researchers to easily retrieve an

up-to-date version of the biased news dataset.2

• A new approach for producing biased search queries, us-
ing intentionally biased transformer-based language models
capable of producing biased texts.

• An evaluation of different fine-tuning settings to find the
most capable setup for producing bias-inducing language
models.

2 BACKGROUND AND RELATED WORK

Definition of Bias is the News Domain. Due to the subjective na-
ture of bias, few publications attempt to explicitly define bias in
news [1, 11, 13, 32]. ’Biased news’ generally describes non-neutral
and opinionated news, but both are fuzzy attributes [32]. In the polit-
ical domain, the term opinionated describes having a fixed political
opinion and agenda, aspect of a cognitive (author- or reader-sided)
bias often described as partisan bias [17] or (political) stance [24].
Partisanship manifests at different granularities. At publishing or
reporting level, partisanship is expressed by the selection of certain
topics called selection bias, and coverage bias, the selection of dif-
ferent views on a topic [13]. At text level, statement bias describes
“members of the media interjecting their own opinions into the
text” [13], manifesting in overall opinionated texts. This can take
various forms, ranging from phrasing bias, the use of non-neutral
language [21] to moral framing and ideological bias [26]. Spin bias
describes bias introduced either by omitting necessary (neutral)
information or adding unnecessary information [11]. At word and
n-gram level, biases manifest as linguistic or lexical biases [11].
Most linguistic biases are highly domain- and context-specific. For
example, framing bias describes subjective words, while epistemo-
logical bias can be attributed to words targeting the credibility of a
statement [32]. Since these biases can be identified more objectively,
most approaches to detecting biases rely on the identification of
linguistic biases [1].

Research on Bias in Online Search and Search Queries. Few pub-
lications investigate bias in online search in aspects other than
search results: Robertson et al. [34] investigate partisan bias and
filter bubble effects in political searches by auditing SERPS for a
set of queries. They did not find significant evidence, that unbiased

search queries in real search sessions performed by real users lead
to filter bubble effects. The unanswered question is, whether biased
queries in general lead to biased search results.

Few studies investigate bias in search queries. Due to the difficult
accessibility of biased search queries, most studies focus on bias in
search query suggestions: in most of those, the authors investigate
topical group biases in search query suggestions in the political
domain [8, 18, 19]. Overall, they observe minor topical gender biases
for search queries consisting of names of politicians. Research on
bias in online search has shown, that “factors such as the topic of
the query, the phrasing of query and the time at which a query is
issued also impact the bias seen by the users”[23].

Datasets of Biased News and Search Queries. Baly et al. [4] predict
media bias using news articles collected from AllSides balanced
news.3 AllSides news is a popular source for balanced news [4, 11,
26], since AllSides has a high standard for assigning bias labels [1].
Similarly, Chen et al. [11] use AllSides-labeled news articles and
adfontes labels to analyze bias at different granularities. Mokhbe-
rian et al. [26] develop a framing bias detection and quantification
approach, using a collection of news articles that they label accord-
ing to news outlet bias labels provided by AllSides.4 datasets for
search queries and query suggestions are sparse: most are either not
available anymore [29] or focus on a narrow topic [8]. Robertson
et al. [35] introduce recursive algorithm interrogation, a technique
for recursively retrieving query suggestions of a root query and
their consecutive suggestions. This technique was employed by
Haak and Schaer [19] to investigate bias in the German political
domain. However, to the best of our knowledge, there currently is
no large-scale dataset on search queries.

3 ALLSIDES SCRAPER AND DATASETS
We present two novel datasets, that promote the investigation of
bias in online news search. Both datasets can be found on Zenodo as
mentioned in section 1. Further, we provide a web scraping tool for
retrieving an up-to-date version of the AllSides dataset we provide.
This section describes the content of the datasets as well as their
creation process. Figure 1 shows how we assembled the datasets
and use them in our approach for creating biasing language models.

3.1 AllSides Scraper
As described in section 2, the AllSides platform and the provides
news is a frequently used source for high quality labeled biased
news. We want to provide an easy means of retrieving the news
and all corresponding information. Similar datasets have previously
been produced, as mentioned in section 2. However, compared to the
currently most recent available version by Baly et al. [4], our version
includes more than 20 percent of additional articles. Furthermore,
for many tasks, especially in the news domain, it is relevant to have
the most recent documents available. We provide a Python-based
scraper, that scrapes all available AllSides news articles and gathers
available information as described in section 3.2. By providing the
scraper we facilitate access to a recent version of the dataset for
other researchers.

1The datasets can be found at Zenodo (https://doi.org/10.5281/zenodo.7682914)
2https://github.com/irgroup/Qbias

3https://www.allsides.com/unbiased-balanced-news
4https://www.kaggle.com/datasets/snapcrack/all-the-news

𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in Search Queries and Query Suggestions

WebSci ’23, April 30-May 1, 2023, Evanston, TX, USA

3.3 Search Queries in U.S. News Domain Dataset
The second dataset we provide consists of 671,669 search query
suggestions for root queries based on tags of the AllSides biased
news dataset. We collected search query suggestions from Google
and Bing for the 1,431 topic tags, that have been used for tagging
AllSides news at least five times, approximately half of the total
amount of topics. The topic tags include names, a wide range of
political terms, agendas, and topics (e.g., ""communism"", ""libertarian
party"", ""same-sex marriage""), cultural and religious terms (e.g., ""Ra-
madan"", ""pope Francis""), locations and other news-relevant terms.
On average, the dataset contains 469 search queries for each topic.
In total, 318,185 suggestions have been retrieved from Google and
353,484 from Bing.

Using a python implementation loosely adopting the framework
provided by Robertson et al. [35], we scraped query suggestions for
the topics as root queries. Using Google Colab to run our scraper,
we retrieved ten search query suggestions provided by the Google
and Bing search autocomplete systems for the input of each of these
root queries. Furthermore, we extended the root queries by the let-
ters a to z (e.g., ""democrats"" (root term) → ""democrats a"" (query
input) → ""democrats and recession"" (query suggestion)). The goal
of this procedure is to simulate a user’s input during information
search. Retrieving the suggestions for the root query and the ex-
tended queries generates a total of up to 270 query suggestions per
topic and search engine. The dataset we provide contains columns
for root term, query input, and query suggestion for each suggested
query. The location from which the search is performed is the loca-
tion of the Google servers running Colab, in our case Iowa in the
United States of America, which is added to the dataset. Since we
perform the scrape on a blank browser for each of the searches,
personalization effects other than the location did not effect the
suggested search queries. Our scraping setup thus eliminates per-
sonalization effects that could, in theory, cause echo chamber ef-
fects [34]. Search engine providers describe that query suggestions
are based on other users’ searches [37]. Successful attempts to in-
fluence query suggestions have confirmed this claim [38]. Thus, we
deduce that query suggestions reflect real, frequently used search
queries and can be used as proxies for search queries. Despite the
lack of information on the frequency of the collected search queries,
our dataset contains the ranks of suggestions, as well as the sugges-
tions to the root term for approximating the most frequent search
queries. Assuming that the topical tags in the AllSides news reflect
popular topics, the produced dataset consists of real search queries
for news-relevant topics.

4 DEVELOPING BIASING LANGUAGE MODELS
Another contribution 𝑄𝑏𝑖𝑎𝑠 is to produce a pair of transformer-
based language models that are capable of inducing left or right
partisanship as linguistic biases. This can be done by leveraging
the masking function on the words of the target document.

4.1 Domain-Adopting DistilBERT
This section describes the methodological approach for develop-
ing transformer-based language models capable of biasing texts.
Usually, systems are developed with the goal of producing and
reproducing as little bias as possible or to debias biased texts [31].

Figure 1: Pipeline used for generating bias-inducing language
models. Black boxes represent datasets, language models
(only available upon request), and tools provided via Zenodo,
and GitHub.

3.2 AllSides Biased News Dataset
The dataset contains 21,747 news articles collected from AllSides
balanced news headline roundups [2] in November 2022. The All-
Sides balanced news feature three expert-selected U.S. news articles
from sources of different political views (left, right, center), often
featuring spin bias, slant other forms of non-neutral reporting on
political news [1]. All articles are tagged with a bias label by four
expert annotators based on the expressed political partisanship,
left, right, or neutral [1]. The AllSides balanced news aims to offer
multiple political perspectives on important news stories, educate
users on biases, and provide multiple viewpoints [1]. Collected data
includes the headline, news text, publishing date, topic tags (e.g.,
""Republican party"", ""coronavirus"", ""federal jobs""), links to the article,
and the publishing news outlet. We also include AllSides’ neutral
description of the topic of the articles. Overall, the produced dataset
contains 10,273 articled tagged as left, 7,222 articles tagged as right,
and 4,252 articles tagged as center. The collected articles have been
published between June 2012, and the date of the data collection at
the end of November 2022. We use the AllSides dataset for develop-
ing our biasing language model, as described in section 4. To allow
for easier access to the most recent and complete version of the
dataset for future research, we provide the scraping tool described
in section 3.1.

WebSci ’23, April 30-May 1, 2023, Evanston, TX, USA

Haak and Schaer

We try to achieve the opposite, biasing texts by reproducing biases
inherent in biased fine-tuning datasets. This would, for example,
allow for simulating a user that searches for biased terms poten-
tially caused by exposure to biased news. With the goal of cap-
turing biased language in pretrained language models and using
the HuggingFace Transformers module[39], we fine-tune the base
DistilBERT model, which was trained in Wikipedia articles and a
large book dataset.5 DistilBERT–[36] was chosen as a base model
since it has proven to perform well with comparably small datasets,
performs better than BERT despite its smaller size [36], and for its
aptness for adopting domain-specificity [3, 9]. We are aware that
there are more effective models, but to show the suitability of our
approach, we chose DistilBERT due to its efficiency and sufficient
effectiveness.

We fine-tuned a range of pairs of models, each with one left
model fine-tuned on the part of the AllSides corpus tagged as left-
biased news and one right model fine-tuned with documents of
the dataset labeled as right-biased news. We produced 24 models,
a left and a right model for 12 combinations of parameters. Our
goal is to find the ideal approach for capturing and reproducing
as much bias as possible while producing meaningful results. The
central aspect we varied in fine-tuning the models is the data used.
The models are developed using three different data configurations:
(a) the headline and news text of each of the left and right news
articles of the AllSides dataset, (b) only the news text, and (c) only
the headline. Another factor we evaluate is the use of padding or
concatenation to generate a consistent chunk size for fine-tuning.
The chosen chunk size is the max length of fine-tuning documents
for the padding approach and 128 for the concatenation approach.
Lastly, we compare the effects of intentional overfitting by rais-
ing the number of epochs to 20, compared to a more reasonable 6
epochs, which proved to produce a good combination of training
loss and validation loss for all dataset configurations. In theory,
overfitting could be another possibility for reproducing biased for-
mulations from the texts used in fine-tuning. This is why we tried
to intentionally raise the number of epochs as a parameter [27].

4.2 Evaluating the Biasing Language Models
All 24 models are available upon request. We decided not to make
the models publicly available due to concerns about potential harms
that could be caused by misuse of the systems that we further elab-
orated in section 5. For evaluating the models’ effectiveness, we
choose a combination of manual assessment and quantitative mea-
sures. The task of the models is to generate biased output, however,
biases are diverse and not easily measurable. Since we have no way
of identifying biased tokens in the dataset, perplexity is not a useful
measure for evaluating the model. Further, we cannot effectively
measure bias as a criterion for the effectiveness of the ability of the
systems to produce biased versions of queries. However, we can
assume that when masking words in search queries for topics of
the political news domain and letting the pairs of language models
predict the words, the output of the left and right models should
differ from each other. We choose to evaluate their performance on
search queries since we want to use the models in future research
on bias in search results for biased and unbiased search queries.

5https://huggingface.co/distilbert-base-uncased

Further, they represent texts of a different type but the same domain
as the texts used in training the models.

For each pair of models, we measure the difference between the
two models’ 10 most probable non-punctuation predictions for 100
randomly selected queries from the dataset by measuring the Rank
Biased Overlap (RBO) with 𝑝 = 0.9. We mask a random token of
each query that is neither part of the original news topic nor a stop-
word since we want to let the models generate meaning-carrying
tokens while keeping the query’s topic intact. Although nonsensical
random predictions or neologisms would lead to great RBO scores,
we want to generate queries, that humans could have formulated.
To assure that the models generate output that makes sense, we let
the models generate next word predictions for ten bias-provoking
sentences (e.g., ""Hilary Clinton is a"", ""Covid Vaccines should be"").
Two domain experts (a professor and a postdoctoral researcher)
then label nonsensical and biased predictions. We measure the inter-
annotator agreement on the individual statements (independent of
the models, that produce the statements) with Cohen’s Kappa [12].
For nonsensical predictions, we obtained a value of 0.18 (slight
agreement), for biased predictions the value is 0.84 (almost perfect).
The bias labels are assigned if the suggested word induces political
stance bias.

Table 1 shows the results of the model evaluation process. The
lower ℎ,𝑡, or ℎ𝑡 describes if headline, text, or both have been used.
𝑅𝐵𝑂 is the rank bias overlap between the left and the right model
of each configuration. 𝑃𝑛𝑜𝑛𝑠 describes the average percentage of
nonsensical predictions of both models and 𝑃𝑏𝑖𝑎𝑠 the percentage
of biased predictions. Overall, the models fine-tuned with only
headlines show the best (lowest) RBO scores, as well as the lowest
𝑃𝑛𝑜𝑛𝑠 scores. Headlines and text and only text perform more or
less equally in terms of their RBO. Concatenating texts produces
on average better results than padding, although for fine-tuning
with only headlines, the effect is minimal. Intentional overfitting
by raising the amount of training epochs seems to worsen the RBO
scores. The percentage of nonsensical predictions does not differ
much between different fine-tuning setups, fine-tuning on texts
only seems to be the only scenario that increases the percentage
of nonsensical predictions. Many of the models generate a high
percentage of biased suggestions, with the headline models having
the highest percentage of biased predictions.

5 DISCUSSION
Our language models show, that by using small, high-quality datasets,
it is possible to fine-tune transformer-based language models to bias
texts. In our fine-tuning setup, models fine-tuned with headlines
produce the overall best results. As the main reason for that, we
assume that the high amount of quotes in news texts, which often
are statements the authors of the articles disagree with, might have
induced noise in fine-tuning the models. Further, the condensed na-
ture of headlines, which aims to catch attention, might also reflect
in resulting language models. The output of left and right models
fine-tuned with headlines produce texts containing linguistic biases.
For example, for ""Donald Trump is a"", our model produces ""hero""
as right-biased and ""fraud"" as left-biased next word predictions.
The overall low percentage of nonsensical predictions supports
RBO as a suited evaluation metric. Despite the overall good results,

𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in Search Queries and Query Suggestions

WebSci ’23, April 30-May 1, 2023, Evanston, TX, USA

Table 1: Evaluation results of the fine-tuned language models.
The highlighted values are the best results for each metric.

Model configuration
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡𝑡
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ𝑡
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔𝑡
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ𝑡
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔

𝑅𝐵𝑂 𝑃𝑛𝑜𝑛𝑠
0.54
0.04
0.10
0.72
0.08
0.68
0.03
0.56
0.11
0.73
0.07
0.68
0.54
0.04
0.05
0.74
0.07
0.77
0.03
0.58
0.06
0.77
0.07
0.77

𝑃𝑏𝑖𝑎𝑠
0.33
0.14
0.15
0.5
0.14
0.23
0.38
0.28
0.1
0.43
0.24
0.12

future research should investigate the findings with other language
models and compare the performance to our results.

Our scraping tool and the datasets not only allow us to effectively
generate the language models but enables the investigation of other
research topics in the political news information search domain
such as bias classification or sentiment analysis.

Remark on Moral Issues. We are aware, that building bias-inducing
systems and providing datasets that enable the reproduction of
developing similar systems is problematic. We also did consider
that our work can inspire and help to develop transformer-based
language models capable of producing biased, toxic, or otherwise
harmful texts. The severity of openly published biased systems
has been shown by other such models, e.g. the gpt-4chan model.6
The publication of the model incited a debate, that shows how
problematic biased language made available to a wide public au-
dience are, despite explicitly stating the intended use for research
applications [16, 33].

Our models are biased primarily in terms of linguistic biases
that reflect political stances and views on political topics. Since this
can in part include objectively wrong and opinionated statements,
hate speech, racism, and other forms of despicable language and
toxicity, our models can and will reproduce text, that conveys these
phenomena. To minimize the harmful effects of our publication
that could be caused by the misuse of the models, we decided
to not make our biased models publicly accessible. Despite that,
we provide access to the models upon request for research, if a
reasonable application use case is provided. However, we do so
in the interest of investigating biases and correlations of bias in
online information search, with the goal of increasing transparency
and fairness. Raising awareness for how easily, intentionally or not,
biases can be reproduced and induced with AI systems is part of
this endeavor.

Although we have shown how the data we provide can be used
to create models that produce biased language we chose to publish
the datasets and scraper. This is mainly because of three reasons:
(a) as stated in section 3 similar datasets are available publicly, (b)
we believe that the benefits of raising awareness outweigh making

6https://huggingface.co/ykilcher/gpt-4chan

already public biased data more accessible, and (c) datasets on biased
language are required for developing systems to detect and inhibit
bias. With our work, we hope to highlight the need to assure, that
data used for developing models is unbiased and raise awareness for
how easily transformer-based language models can be fine-tuned
to produce biased language.

6 OUTLOOK
This publication represents the first and foundational milestone of
a larger-scale investigation of bias in online information search. As
a major next contribution, we plan to use the presented datasets
and models to investigate the effects of biased and unbiased search
queries on the search results of different search engines for popular
topics of the U.S. political news domain. To accomplish this, we need
to overcome the issue of subjectivity and lack of effective method-
ological approaches to bias identification other than by employing
human annotation. We plan to introduce an ensemble of methods,
including bias-agnostic analysis of linguistic differences, lexical
features, and transformer-based approaches for bias classification.
Further, we plan to conduct a study using a simulation approach. By
simulation different user behaviors in terms of information need,
formulating queries, and interacting with query suggestions in an
interactive information search simulation, we plan to gain insights
into echo chamber effects and bias formation in information search.
Additionally, we hope to identify which user properties and be-
haviors increase and which lower the risk of encountering bias in
information search.

7 CONCLUSION
This work presents 𝑄𝑏𝑖𝑎𝑠 , a first milestone of our ongoing research
on bias in online search. We present two datasets: a biased news
dataset and a large dataset of biased and unbiased search queries
for topics of the U.S. political news domain. Further, we provide a
scraping tool, that allows for collecting bias-labeled news texts from
AllSides. Lastly, we evaluate approaches to fine-tuning DistilBERT
transformer-based language models for biasing texts and publish
our models capable of inducing left and right political stance bias
in the form of lexical biases.

REFERENCES
[1] AllSides. 2021. How AllSides Creates Balanced News: A Step-by-Step Guide.
Retrieved Nov 30, 2022 from https://www.allsides.com/blog/how-does-allsides-
create-balanced-news

[2] AllSides. 2022. Balanced News Headlines Roundup. Retrieved Nov 30, 2022

from https://www.allsides.com/unbiased-balanced-news

[3] Jing Bai, Rui Cao, Wen Ma, and Hiroyuki Shinnou. 2020. Construction of Domain-

Specific DistilBERT Model by Using Fine-Tuning. In TAAI. 237–241.

[4] Ramy Baly, Giovanni Da San Martino, James Glass, and Preslav Nakov. 2020.
We Can Detect Your Bias: Predicting the Political Ideology of News Articles. In
EMNLP. 4982–4991.

[5] Michael Barbaro and Tom Zeller. 2006. A Face is exposed for AOL searcher no.

4417749. New York Times (01 2006).

[6] Nicholas Belkin, Colleen Cool, Diane Kelly, S.-J Lin, S.Y Park, Jose Perez-carballo,
and Cynthia Sikora. 2001. Iterative exploration, design and evaluation of support
for query reformulation in interactive information retrieval. IPM 37 (05 2001),
403–434.

[7] Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam
Kalai. 2016. Man is to Computer Programmer as Woman is to Homemaker?
Debiasing Word Embeddings. In NIPS. 4356–4364.

[8] Malte Bonart, Anastasiia Samokhina, Gernot Heisenberg, and Philipp Schaer.
2019. An investigation of biases in web search engine query suggestions. OIR 44,
2 (2019), 365–381.

WebSci ’23, April 30-May 1, 2023, Evanston, TX, USA

Haak and Schaer

[39] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe
Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu,
Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
and Alexander Rush. 2020. Transformers: State-of-the-Art Natural Language
Processing. In EMNLP. 38–45.

[40] Jinxi Xu and W. Bruce Croft. 2000. Improving the Effectiveness of Information
Retrieval with Local Context Analysis. ACM Trans. Inf. Syst. 18, 1 (Jan. 2000),
79–112. https://doi.org/10.1145/333135.333138

[9] Berfu Büyüköz, Ali Hürriyetoˇglu, and Arzucan Özgür. 2020. Analyzing ELMo

and DistilBERT on Socio-political News Classification. In AESPEN.

[10] Fei Cai and Maarten de Rijke. 2016. A Survey of Query Auto Completion in

Information Retrieval. FNTIR 10, 4 (2016), 273–363.

[11] Wei-Fan Chen, Khalid Al Khatib, Henning Wachsmuth, and Benno Stein. 2020.
Analyzing Political Bias and Unfairness in News Articles at Different Levels of
Granularity. In NLPCSS. 149–154. https://doi.org/10.18653/v1/2020.nlpcss-1.16
[12] Jacob Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educ Psychol

Meas 20, 1 (1960), 37–46.

[13] Dave D’Alessio and Mike Allen. 2000. Media Bias in Presidential Elections: A

Meta-Analysis. J. Commun. 50, 4 (2000), 133–156.

[14] Edelman. 2022. 2022 Edelman Trust Barometer. Retrieved Nov 30, 2022 from

https://www.edelman.com/trust/2022-trust-barometer

[15] Robert Epstein and Ronald E. Robertson. 2015. The search engine manipulation
effect (SEME) and its possible impact on the outcomes of elections. PNAS 112,
33, E4512–E4521. Publisher: National Academy of Sciences Section: PNAS Plus.
[16] Matthew Gault. 2022. AI Trained on 4Chan Becomes ‘Hate Speech Machine’.
Retrieved Feb 28, 2023 from https://www.vice.com/en/article/7k8zwx/ai-trained-
on-4chan-becomes-hate-speech-machine

[17] Bertram Gawronski. 2021. Partisan bias in the identification of fake news. TiCS

25, 9 (2021), 723–724.

[18] Fabian Haak and Philipp Schaer. 2021. Perception-Aware Bias Detection for

Query Suggestions. In BIAS. 130–142.

[19] Fabian Haak and Philipp Schaer. 2022. Auditing Search Query Suggestion Bias

Through Recursive Algorithm Interrogation. In WebSci. 219–227.

[20] Daniel Hienert, Philipp Schaer, Johann Schaible, and Philipp Mayr. 2011. A Novel
Combined Term Suggestion Service for Domain-Specific Digital Libraries.. In
TPDL (Lecture Notes in Computer Science, Vol. 6966), Stefan Gradmann, Francesca
Borri, Carlo Meghini, and Heiko Schuldt (Eds.). Springer, 192–203. http://dblp.
uni-trier.de/db/conf/ercimdl/tpdl2011.html#HienertSSM11

[21] Christoph Hube and Besnik Fetahu. 2019. Neural Based Statement Classification

for Biased Language. In WSDM. ACM.

[22] L. Introna and H. Nissenbaum. 2000. Defining the Web: the politics of search

engines. Computer 33, 1, 54–62.

[23] Juhi Kulshrestha, Motahhare Eslami, Johnnatan Messias, Muhammad Bilal Zafar,
Saptarshi Ghosh, Krishna P. Gummadi, and Karrie Karahalios. 2019. Search bias
quantification: investigating political bias in social media and web search. Inf.
Retr. J. 22, 1, 188–227.

[24] Ruibo Liu, Chenyan Jia, and Soroush Vosoughi. 2021. A Transformer-based
Framework for Neutralizing and Reversing the Political Polarity of News Articles.
Proc. ACM Hum.-Comput. Interact. 5, 1–26.

[25] Bhaskar Mitra, Milad Shokouhi, Filip Radlinski, and Katja Hofmann. 2014. On

user interactions with query auto-completion. In SIGIR. 1055–1058.

[26] Negar Mokhberian, André s Abeliuk, Patrick Cummings, and Kristina Lerman.

2020. Moral Framing and Ideological Bias of News. In SocInfo. 206–219.
[27] Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. 2021. On
the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong
Baselines. In ICLR. https://openreview.net/forum?id=nzpLWnVAyah

[28] Xi Niu and Diane Kelly. 2014. The use of query suggestions during information

search. IPM 50, 1, 218–234.

[29] Greg Pass, Abdur Chowdhury, and Cayley Torgeson. 2006. A Picture of Search.

In InfoScale. 1–es.

[30] Lily Ray. 2020. 2020 Google Search Survey: How Much Do Users Trust Their
Search Results? Retrieved Nov 30, 2022 from https://moz.com/blog/2020-google-
search-survey

[31] Shaina Raza, Deepak John Reji, and Chen Ding. 2022. Dbias: Detecting biases

and ensuring Fairness in news articles. Int J Data Sci Anal (2022).

[33] Reddit. 2022.

[32] Marta Recasens, Cristian Danescu-Niculescu-Mizil, and Dan Jurafsky. 2013. Lin-
guistic Models for Analyzing and Detecting Biased Language. In ACL. 1650–1659.
Retrieved Feb 28, 2023
from https://www.reddit.com/r/MachineLearning/comments/v42pej/p_this_is_
the_worst_ai_ever_gpt4chan_model/

This is the worst AI ever.

[34] Ronald E. Robertson, Shan Jiang, Kenneth Joseph, Lisa Friedland, David Lazer,
and Christo Wilson. 2018. Auditing Partisan Audience Bias within Google Search.
Proc. ACM Hum.-Comput. Interact. 2, CSCW, Article 148 (2018).

[35] Ronald E. Robertson, Shan Jiang, David Lazer, and Christo Wilson. 2019. Auditing
Autocomplete: Suggestion Networks and Recursive Algorithm Interrogation. In
WebSci. 235–244.

[36] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Dis-
tilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. CoRR
abs/1910.01108 (2019).

[37] Danny Sullivan. 2018. How Google autocomplete works in Search. Retrieved Nov
30, 2022 from https://blog.google/products/search/how-google-autocomplete-
works-search/

[38] Peng Wang, Xianghang mi, Xiaojing Liao, Xiaofeng Wang, Kan Yuan, Feng Qian,
and Raheem Beyah. 2018. Game of Missuggestions: Semantic Analysis of Search-
Autocomplete Manipulations. In NDSS.

","𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in Search Queries and Query Suggestions Fabian Haak fabian.haak @ th-koeln.de Technische Hochschule Köln Cologne , Germany Philipp Schaer philipp.schaer @ th-koeln.de Technische Hochschule Köln Cologne , Germany ABSTRACT This publication describes the motivation and generation of 𝑄𝑏𝑖𝑎𝑠 , a large dataset of Google and Bing search queries , a scraping tool and dataset for biased news articles , as well as language models for the investigation of bias in online search . Web search engines are a major factor and trusted source in information search , especially in the political domain . However , biased information can influence opinion formation and lead to biased opinions . To interact with search engines , users formulate search queries and interact with search query suggestions provided by the search engines . A lack of datasets on search queries inhibits research on the subject . We use 𝑄𝑏𝑖𝑎𝑠 to evaluate different approaches to fine-tuning transformer- based language models with the goal of producing models capable of biasing text with left and right political stance . Additionally to this work we provided datasets and language models for biasing texts that allow further research on bias in online information search . CCS CONCEPTS • Information systems → Web search engines ; Query suggestion ; Query reformulation ; • Computing methodologies → Natural language generation . KEYWORDS web search , dataset , bias , query suggestion , search queries , language models , transformers ACM Reference Format : Fabian Haak and Philipp Schaer . 2023 . 𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in Search Queries and Query Suggestions . In 15th ACM Web Science Conference 2023 ( WebSci ’ 23 ) , April 30-May 1 , 2023 , Evanston , TX , USA . ACM , New York , NY , USA , 6 pages . 3 2 0 2 v o N 9 2 ] R I . s c [ 1 v 0 8 7 7 1 . 1 1 3 2 : v i X r a 1 INTRODUCTION Search engines such as Google and Bing are seen as trustworthy sources of information on many topics , including political news and information [ 14 , 30 ] . Further , search engines have proven to have a major impact on the formation of political opinions [ 15 ] . To interact with search engines , users formulate search queries that are an expression of their information need . Based on these queries search engines usually provide a set of search query suggestions [ 28 ] . Queries and the choices users make when interacting with query suggestions are based on the information need they want to satisfy , which is often to support their personal opinions or beliefs founded on previously encountered information [ 6 ] . This interaction process and the results presented to the users are prone to be biased [ 7 , 22 , 25 ] and therefore the high level of trust can be seen as problematic . However , the true effect of biased search queries and different types of inherent biases on the actual list of search results has not sufficiently been investigated . One of the main reasons for the in- frequency of studies on bias in search queries might be a lack of publicly available datasets . datasets that include real-world user queries , query suggestions , and actual query reformulations are rare . One of the reasons is that collecting search queries from users is problematic due to privacy concerns . Although there are tech- niques like pseudonymization , query logs enable the identification of users [ 5 ] . Previously available datasets such as the AOL query log dataset [ 29 ] are no longer available , and their usage is morally debatable . Using unpersonalized search query suggestions as proxies might solve this issue . Query suggestions describe the list of predicted queries suggested to users during the input of search queries by the search engine , sometimes also called search predictions [ 37 ] or query auto completion [ 10 ] . While these search query suggestions can be generated locally from the result set [ 40 ] or be taken from global knowledge bases [ 20 ] , in web search suggestions are mostly based on frequently issued queries by users [ 37 ] . It can therefore be assumed that in many cases query suggestions are popular related search queries for the initial root query that represented the user ’ s interest in a topic or entity . The U.S. news domain is a popular domain for bias research due to its sociological relevance and the two-party left-right spectrum that facilitates bias analysis [ 34 ] . One of the benefits of our dataset is to enable in-depth investigations of the correlation between bias in search queries and search results , as search results for popu- lar web search engines can easily be collected using our provided dataset of search queries . Therefore , one goal of this work is to provide a large dataset of search query suggestions for the U.S. po- litical news domain . Additionally , we provide a transformer-based methodology for biasing text . Such a system can be a useful tool in WebSci ’ 23 , April 30-May 1 , 2023 , Evanston , TX , USA Haak and Schaer bias research , f.e . for generating biased derivatives of search queries . Thus , we evaluate a range of fine-tuning scenarios to find settings , that produce the most biased results . We provide the fine-tuned transformer-based language models that are capable of inducing left and right political stance bias in the form of lexical biases . This publication describes the motivation and generation of 𝑄𝑏𝑖𝑎𝑠 , a large dataset of Google and Bing search queries , a scraping tool and dataset for biased news articles , as well as language models for the investigation of bias in online search . The main contributions of 𝑄𝑏𝑖𝑎𝑠 and this paper are : • Two datasets:1 ( 1 ) The ( to the best of our knowledge ) largest labeled dataset of search query suggestions of a single do- main for Google and Bing . ( 2 ) A dataset of biased news articles of the U.S. political news domain . • A scraping tool that allows researchers to easily retrieve an up-to-date version of the biased news dataset.2 • A new approach for producing biased search queries , us- ing intentionally biased transformer-based language models capable of producing biased texts . • An evaluation of different fine-tuning settings to find the most capable setup for producing bias-inducing language models . 2 BACKGROUND AND RELATED WORK Definition of Bias is the News Domain . Due to the subjective na- ture of bias , few publications attempt to explicitly define bias in news [ 1 , 11 , 13 , 32 ] . ’ Biased news ’ generally describes non-neutral and opinionated news , but both are fuzzy attributes [ 32 ] . In the polit- ical domain , the term opinionated describes having a fixed political opinion and agenda , aspect of a cognitive ( author- or reader-sided ) bias often described as partisan bias [ 17 ] or ( political ) stance [ 24 ] . Partisanship manifests at different granularities . At publishing or reporting level , partisanship is expressed by the selection of certain topics called selection bias , and coverage bias , the selection of dif- ferent views on a topic [ 13 ] . At text level , statement bias describes “ members of the media interjecting their own opinions into the text ” [ 13 ] , manifesting in overall opinionated texts . This can take various forms , ranging from phrasing bias , the use of non-neutral language [ 21 ] to moral framing and ideological bias [ 26 ] . Spin bias describes bias introduced either by omitting necessary ( neutral ) information or adding unnecessary information [ 11 ] . At word and n-gram level , biases manifest as linguistic or lexical biases [ 11 ] . Most linguistic biases are highly domain- and context-specific . For example , framing bias describes subjective words , while epistemo- logical bias can be attributed to words targeting the credibility of a statement [ 32 ] . Since these biases can be identified more objectively , most approaches to detecting biases rely on the identification of linguistic biases [ 1 ] . Research on Bias in Online Search and Search Queries . Few pub- lications investigate bias in online search in aspects other than search results : Robertson et al . [ 34 ] investigate partisan bias and filter bubble effects in political searches by auditing SERPS for a set of queries . They did not find significant evidence , that unbiased search queries in real search sessions performed by real users lead to filter bubble effects . The unanswered question is , whether biased queries in general lead to biased search results . Few studies investigate bias in search queries . Due to the difficult accessibility of biased search queries , most studies focus on bias in search query suggestions : in most of those , the authors investigate topical group biases in search query suggestions in the political domain [ 8 , 18 , 19 ] . Overall , they observe minor topical gender biases for search queries consisting of names of politicians . Research on bias in online search has shown , that “ factors such as the topic of the query , the phrasing of query and the time at which a query is issued also impact the bias seen by the users ” [ 23 ] . Datasets of Biased News and Search Queries . Baly et al . [ 4 ] predict media bias using news articles collected from AllSides balanced news.3 AllSides news is a popular source for balanced news [ 4 , 11 , 26 ] , since AllSides has a high standard for assigning bias labels [ 1 ] . Similarly , Chen et al . [ 11 ] use AllSides-labeled news articles and adfontes labels to analyze bias at different granularities . Mokhbe- rian et al . [ 26 ] develop a framing bias detection and quantification approach , using a collection of news articles that they label accord- ing to news outlet bias labels provided by AllSides.4 datasets for search queries and query suggestions are sparse : most are either not available anymore [ 29 ] or focus on a narrow topic [ 8 ] . Robertson et al . [ 35 ] introduce recursive algorithm interrogation , a technique for recursively retrieving query suggestions of a root query and their consecutive suggestions . This technique was employed by Haak and Schaer [ 19 ] to investigate bias in the German political domain . However , to the best of our knowledge , there currently is no large-scale dataset on search queries . 3 ALLSIDES SCRAPER AND DATASETS We present two novel datasets , that promote the investigation of bias in online news search . Both datasets can be found on Zenodo as mentioned in section 1 . Further , we provide a web scraping tool for retrieving an up-to-date version of the AllSides dataset we provide . This section describes the content of the datasets as well as their creation process . Figure 1 shows how we assembled the datasets and use them in our approach for creating biasing language models . 3.1 AllSides Scraper As described in section 2 , the AllSides platform and the provides news is a frequently used source for high quality labeled biased news . We want to provide an easy means of retrieving the news and all corresponding information . Similar datasets have previously been produced , as mentioned in section 2 . However , compared to the currently most recent available version by Baly et al . [ 4 ] , our version includes more than 20 percent of additional articles . Furthermore , for many tasks , especially in the news domain , it is relevant to have the most recent documents available . We provide a Python-based scraper , that scrapes all available AllSides news articles and gathers available information as described in section 3.2 . By providing the scraper we facilitate access to a recent version of the dataset for other researchers . 1The datasets can be found at Zenodo ( https : //doi.org/10.5281/zenodo.7682914 ) 2https : //github.com/irgroup/Qbias 3https : 4https : 𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in Search Queries and Query Suggestions WebSci ’ 23 , April 30-May 1 , 2023 , Evanston , TX , USA 3.3 Search Queries in U.S. News Domain Dataset The second dataset we provide consists of 671,669 search query suggestions for root queries based on tags of the AllSides biased news dataset . We collected search query suggestions from Google and Bing for the 1,431 topic tags , that have been used for tagging AllSides news at least five times , approximately half of the total amount of topics . The topic tags include names , a wide range of political terms , agendas , and topics ( e.g. , `` communism '' , `` libertarian party '' , `` same-sex marriage '' ) , cultural and religious terms ( e.g. , `` Ra- madan '' , `` pope Francis '' ) , locations and other news-relevant terms . On average , the dataset contains 469 search queries for each topic . In total , 318,185 suggestions have been retrieved from Google and 353,484 from Bing . Using a python implementation loosely adopting the framework provided by Robertson et al . [ 35 ] , we scraped query suggestions for the topics as root queries . Using Google Colab to run our scraper , we retrieved ten search query suggestions provided by the Google and Bing search autocomplete systems for the input of each of these root queries . Furthermore , we extended the root queries by the let- ters a to z ( e.g. , `` democrats '' ( root term ) → `` democrats a '' ( query input ) → `` democrats and recession '' ( query suggestion ) ) . The goal of this procedure is to simulate a user ’ s input during information search . Retrieving the suggestions for the root query and the ex- tended queries generates a total of up to 270 query suggestions per topic and search engine . The dataset we provide contains columns for root term , query input , and query suggestion for each suggested query . The location from which the search is performed is the loca- tion of the Google servers running Colab , in our case Iowa in the United States of America , which is added to the dataset . Since we perform the scrape on a blank browser for each of the searches , personalization effects other than the location did not effect the suggested search queries . Our scraping setup thus eliminates per- sonalization effects that could , in theory , cause echo chamber ef- fects [ 34 ] . Search engine providers describe that query suggestions are based on other users ’ searches [ 37 ] . Successful attempts to in- fluence query suggestions have confirmed this claim [ 38 ] . Thus , we deduce that query suggestions reflect real , frequently used search queries and can be used as proxies for search queries . Despite the lack of information on the frequency of the collected search queries , our dataset contains the ranks of suggestions , as well as the sugges- tions to the root term for approximating the most frequent search queries . Assuming that the topical tags in the AllSides news reflect popular topics , the produced dataset consists of real search queries for news-relevant topics . 4 DEVELOPING BIASING LANGUAGE MODELS Another contribution 𝑄𝑏𝑖𝑎𝑠 is to produce a pair of transformer- based language models that are capable of inducing left or right partisanship as linguistic biases . This can be done by leveraging the masking function on the words of the target document . 4.1 Domain-Adopting DistilBERT This section describes the methodological approach for develop- ing transformer-based language models capable of biasing texts . Usually , systems are developed with the goal of producing and reproducing as little bias as possible or to debias biased texts [ 31 ] . Figure 1 : Pipeline used for generating bias-inducing language models . Black boxes represent datasets , language models ( only available upon request ) , and tools provided via Zenodo , and GitHub . 3.2 AllSides Biased News Dataset The dataset contains 21,747 news articles collected from AllSides balanced news headline roundups [ 2 ] in November 2022 . The All- Sides balanced news feature three expert-selected U.S. news articles from sources of different political views ( left , right , center ) , often featuring spin bias , slant other forms of non-neutral reporting on political news [ 1 ] . All articles are tagged with a bias label by four expert annotators based on the expressed political partisanship , left , right , or neutral [ 1 ] . The AllSides balanced news aims to offer multiple political perspectives on important news stories , educate users on biases , and provide multiple viewpoints [ 1 ] . Collected data includes the headline , news text , publishing date , topic tags ( e.g. , `` Republican party '' , `` coronavirus '' , `` federal jobs '' ) , links to the article , and the publishing news outlet . We also include AllSides ’ neutral description of the topic of the articles . Overall , the produced dataset contains 10,273 articled tagged as left , 7,222 articles tagged as right , and 4,252 articles tagged as center . The collected articles have been published between June 2012 , and the date of the data collection at the end of November 2022 . We use the AllSides dataset for develop- ing our biasing language model , as described in section 4 . To allow for easier access to the most recent and complete version of the dataset for future research , we provide the scraping tool described in section 3.1 . WebSci ’ 23 , April 30-May 1 , 2023 , Evanston , TX , USA Haak and Schaer We try to achieve the opposite , biasing texts by reproducing biases inherent in biased fine-tuning datasets . This would , for example , allow for simulating a user that searches for biased terms poten- tially caused by exposure to biased news . With the goal of cap- turing biased language in pretrained language models and using the HuggingFace Transformers module [ 39 ] , we fine-tune the base DistilBERT model , which was trained in Wikipedia articles and a large book dataset.5 DistilBERT– [ 36 ] was chosen as a base model since it has proven to perform well with comparably small datasets , performs better than BERT despite its smaller size [ 36 ] , and for its aptness for adopting domain-specificity [ 3 , 9 ] . We are aware that there are more effective models , but to show the suitability of our approach , we chose DistilBERT due to its efficiency and sufficient effectiveness . We fine-tuned a range of pairs of models , each with one left model fine-tuned on the part of the AllSides corpus tagged as left- biased news and one right model fine-tuned with documents of the dataset labeled as right-biased news . We produced 24 models , a left and a right model for 12 combinations of parameters . Our goal is to find the ideal approach for capturing and reproducing as much bias as possible while producing meaningful results . The central aspect we varied in fine-tuning the models is the data used . The models are developed using three different data configurations : ( a ) the headline and news text of each of the left and right news articles of the AllSides dataset , ( b ) only the news text , and ( c ) only the headline . Another factor we evaluate is the use of padding or concatenation to generate a consistent chunk size for fine-tuning . The chosen chunk size is the max length of fine-tuning documents for the padding approach and 128 for the concatenation approach . Lastly , we compare the effects of intentional overfitting by rais- ing the number of epochs to 20 , compared to a more reasonable 6 epochs , which proved to produce a good combination of training loss and validation loss for all dataset configurations . In theory , overfitting could be another possibility for reproducing biased for- mulations from the texts used in fine-tuning . This is why we tried to intentionally raise the number of epochs as a parameter [ 27 ] . 4.2 Evaluating the Biasing Language Models All 24 models are available upon request . We decided not to make the models publicly available due to concerns about potential harms that could be caused by misuse of the systems that we further elab- orated in section 5 . For evaluating the models ’ effectiveness , we choose a combination of manual assessment and quantitative mea- sures . The task of the models is to generate biased output , however , biases are diverse and not easily measurable . Since we have no way of identifying biased tokens in the dataset , perplexity is not a useful measure for evaluating the model . Further , we can not effectively measure bias as a criterion for the effectiveness of the ability of the systems to produce biased versions of queries . However , we can assume that when masking words in search queries for topics of the political news domain and letting the pairs of language models predict the words , the output of the left and right models should differ from each other . We choose to evaluate their performance on search queries since we want to use the models in future research on bias in search results for biased and unbiased search queries . 5https : Further , they represent texts of a different type but the same domain as the texts used in training the models . For each pair of models , we measure the difference between the two models ’ 10 most probable non-punctuation predictions for 100 randomly selected queries from the dataset by measuring the Rank Biased Overlap ( RBO ) with 𝑝 = 0.9 . We mask a random token of each query that is neither part of the original news topic nor a stop- word since we want to let the models generate meaning-carrying tokens while keeping the query ’ s topic intact . Although nonsensical random predictions or neologisms would lead to great RBO scores , we want to generate queries , that humans could have formulated . To assure that the models generate output that makes sense , we let the models generate next word predictions for ten bias-provoking sentences ( e.g. , `` Hilary Clinton is a '' , `` Covid Vaccines should be '' ) . Two domain experts ( a professor and a postdoctoral researcher ) then label nonsensical and biased predictions . We measure the inter- annotator agreement on the individual statements ( independent of the models , that produce the statements ) with Cohen ’ s Kappa [ 12 ] . For nonsensical predictions , we obtained a value of 0.18 ( slight agreement ) , for biased predictions the value is 0.84 ( almost perfect ) . The bias labels are assigned if the suggested word induces political stance bias . Table 1 shows the results of the model evaluation process . The lower ℎ , 𝑡 , or ℎ𝑡 describes if headline , text , or both have been used . 𝑅𝐵𝑂 is the rank bias overlap between the left and the right model of each configuration . 𝑃𝑛𝑜𝑛𝑠 describes the average percentage of nonsensical predictions of both models and 𝑃𝑏𝑖𝑎𝑠 the percentage of biased predictions . Overall , the models fine-tuned with only headlines show the best ( lowest ) RBO scores , as well as the lowest 𝑃𝑛𝑜𝑛𝑠 scores . Headlines and text and only text perform more or less equally in terms of their RBO . Concatenating texts produces on average better results than padding , although for fine-tuning with only headlines , the effect is minimal . Intentional overfitting by raising the amount of training epochs seems to worsen the RBO scores . The percentage of nonsensical predictions does not differ much between different fine-tuning setups , fine-tuning on texts only seems to be the only scenario that increases the percentage of nonsensical predictions . Many of the models generate a high percentage of biased suggestions , with the headline models having the highest percentage of biased predictions . 5 DISCUSSION Our language models show , that by using small , high-quality datasets , it is possible to fine-tune transformer-based language models to bias texts . In our fine-tuning setup , models fine-tuned with headlines produce the overall best results . As the main reason for that , we assume that the high amount of quotes in news texts , which often are statements the authors of the articles disagree with , might have induced noise in fine-tuning the models . Further , the condensed na- ture of headlines , which aims to catch attention , might also reflect in resulting language models . The output of left and right models fine-tuned with headlines produce texts containing linguistic biases . For example , for `` Donald Trump is a '' , our model produces `` hero '' as right-biased and `` fraud '' as left-biased next word predictions . The overall low percentage of nonsensical predictions supports RBO as a suited evaluation metric . Despite the overall good results , 𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in Search Queries and Query Suggestions WebSci ’ 23 , April 30-May 1 , 2023 , Evanston , TX , USA Table 1 : Evaluation results of the fine-tuned language models . The highlighted values are the best results for each metric . Model configuration 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡𝑡 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ𝑡 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔𝑡 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ𝑡 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔 𝑅𝐵𝑂 𝑃𝑛𝑜𝑛𝑠 0.54 0.04 0.10 0.72 0.08 0.68 0.03 0.56 0.11 0.73 0.07 0.68 0.54 0.04 0.05 0.74 0.07 0.77 0.03 0.58 0.06 0.77 0.07 0.77 𝑃𝑏𝑖𝑎𝑠 0.33 0.14 0.15 0.5 0.14 0.23 0.38 0.28 0.1 0.43 0.24 0.12 future research should investigate the findings with other language models and compare the performance to our results . Our scraping tool and the datasets not only allow us to effectively generate the language models but enables the investigation of other research topics in the political news information search domain such as bias classification or sentiment analysis . Remark on Moral Issues . We are aware , that building bias-inducing systems and providing datasets that enable the reproduction of developing similar systems is problematic . We also did consider that our work can inspire and help to develop transformer-based language models capable of producing biased , toxic , or otherwise harmful texts . The severity of openly published biased systems has been shown by other such models , e.g . the gpt-4chan model.6 The publication of the model incited a debate , that shows how problematic biased language made available to a wide public au- dience are , despite explicitly stating the intended use for research applications [ 16 , 33 ] . Our models are biased primarily in terms of linguistic biases that reflect political stances and views on political topics . Since this can in part include objectively wrong and opinionated statements , hate speech , racism , and other forms of despicable language and toxicity , our models can and will reproduce text , that conveys these phenomena . To minimize the harmful effects of our publication that could be caused by the misuse of the models , we decided to not make our biased models publicly accessible . Despite that , we provide access to the models upon request for research , if a reasonable application use case is provided . However , we do so in the interest of investigating biases and correlations of bias in online information search , with the goal of increasing transparency and fairness . Raising awareness for how easily , intentionally or not , biases can be reproduced and induced with AI systems is part of this endeavor . Although we have shown how the data we provide can be used to create models that produce biased language we chose to publish the datasets and scraper . This is mainly because of three reasons : ( a ) as stated in section 3 similar datasets are available publicly , ( b ) we believe that the benefits of raising awareness outweigh making 6https : already public biased data more accessible , and ( c ) datasets on biased language are required for developing systems to detect and inhibit bias . With our work , we hope to highlight the need to assure , that data used for developing models is unbiased and raise awareness for how easily transformer-based language models can be fine-tuned to produce biased language . 6 OUTLOOK This publication represents the first and foundational milestone of a larger-scale investigation of bias in online information search . As a major next contribution , we plan to use the presented datasets and models to investigate the effects of biased and unbiased search queries on the search results of different search engines for popular topics of the U.S. political news domain . To accomplish this , we need to overcome the issue of subjectivity and lack of effective method- ological approaches to bias identification other than by employing human annotation . We plan to introduce an ensemble of methods , including bias-agnostic analysis of linguistic differences , lexical features , and transformer-based approaches for bias classification . Further , we plan to conduct a study using a simulation approach . By simulation different user behaviors in terms of information need , formulating queries , and interacting with query suggestions in an interactive information search simulation , we plan to gain insights into echo chamber effects and bias formation in information search . Additionally , we hope to identify which user properties and be- haviors increase and which lower the risk of encountering bias in information search . 7 CONCLUSION This work presents 𝑄𝑏𝑖𝑎𝑠 , a first milestone of our ongoing research on bias in online search . We present two datasets : a biased news dataset and a large dataset of biased and unbiased search queries for topics of the U.S. political news domain . Further , we provide a scraping tool , that allows for collecting bias-labeled news texts from AllSides . Lastly , we evaluate approaches to fine-tuning DistilBERT transformer-based language models for biasing texts and publish our models capable of inducing left and right political stance bias in the form of lexical biases . REFERENCES [ 1 ] AllSides . 2021 . How AllSides Creates Balanced News : A Step-by-Step Guide . Retrieved Nov 30 , 2022 from https : create-balanced-news [ 2 ] AllSides . 2022 . Balanced News Headlines Roundup . Retrieved Nov 30 , 2022 from https : [ 3 ] Jing Bai , Rui Cao , Wen Ma , and Hiroyuki Shinnou . 2020 . Construction of Domain- Specific DistilBERT Model by Using Fine-Tuning . In TAAI . 237–241 . [ 4 ] Ramy Baly , Giovanni Da San Martino , James Glass , and Preslav Nakov . 2020 . We Can Detect Your Bias : Predicting the Political Ideology of News Articles . In EMNLP . 4982–4991 . [ 5 ] Michael Barbaro and Tom Zeller . 2006 . A Face is exposed for AOL searcher no . 4417749 . New York Times ( 01 2006 ) . [ 6 ] Nicholas Belkin , Colleen Cool , Diane Kelly , S.-J Lin , S.Y Park , Jose Perez-carballo , and Cynthia Sikora . 2001 . Iterative exploration , design and evaluation of support for query reformulation in interactive information retrieval . IPM 37 ( 05 2001 ) , 403–434 . [ 7 ] Tolga Bolukbasi , Kai-Wei Chang , James Zou , Venkatesh Saligrama , and Adam Kalai . 2016 . Man is to Computer Programmer as Woman is to Homemaker ? Debiasing Word Embeddings . In NIPS . 4356–4364 . [ 8 ] Malte Bonart , Anastasiia Samokhina , Gernot Heisenberg , and Philipp Schaer . 2019 . An investigation of biases in web search engine query suggestions . OIR 44 , 2 ( 2019 ) , 365–381 . WebSci ’ 23 , April 30-May 1 , 2023 , Evanston , TX , USA Haak and Schaer [ 39 ] Thomas Wolf , Lysandre Debut , Victor Sanh , Julien Chaumond , Clement Delangue , Anthony Moi , Pierric Cistac , Tim Rault , Remi Louf , Morgan Funtowicz , Joe Davison , Sam Shleifer , Patrick von Platen , Clara Ma , Yacine Jernite , Julien Plu , Canwen Xu , Teven Le Scao , Sylvain Gugger , Mariama Drame , Quentin Lhoest , and Alexander Rush . 2020 . Transformers : State-of-the-Art Natural Language Processing . In EMNLP . 38–45 . [ 40 ] Jinxi Xu and W. Bruce Croft . 2000 . Improving the Effectiveness of Information Retrieval with Local Context Analysis . ACM Trans . Inf . Syst . 18 , 1 ( Jan. 2000 ) , 79–112 . https : //doi.org/10.1145/333135.333138 [ 9 ] Berfu Büyüköz , Ali Hürriyetoˇglu , and Arzucan Özgür . 2020 . Analyzing ELMo and DistilBERT on Socio-political News Classification . In AESPEN . [ 10 ] Fei Cai and Maarten de Rijke . 2016 . A Survey of Query Auto Completion in Information Retrieval . FNTIR 10 , 4 ( 2016 ) , 273–363 . [ 11 ] Wei-Fan Chen , Khalid Al Khatib , Henning Wachsmuth , and Benno Stein . 2020 . Analyzing Political Bias and Unfairness in News Articles at Different Levels of Granularity . In NLPCSS . 149–154 . https : [ 12 ] Jacob Cohen . 1960 . A Coefficient of Agreement for Nominal Scales . Educ Psychol Meas 20 , 1 ( 1960 ) , 37–46 . [ 13 ] Dave D ’ Alessio and Mike Allen . 2000 . Media Bias in Presidential Elections : A Meta-Analysis . J. Commun . 50 , 4 ( 2000 ) , 133–156 . [ 14 ] Edelman . 2022 . 2022 Edelman Trust Barometer . Retrieved Nov 30 , 2022 from https : [ 15 ] Robert Epstein and Ronald E. Robertson . 2015 . The search engine manipulation effect ( SEME ) and its possible impact on the outcomes of elections . PNAS 112 , 33 , E4512–E4521 . Publisher : National Academy of Sciences Section : PNAS Plus . [ 16 ] Matthew Gault . 2022 . AI Trained on 4Chan Becomes ‘ Hate Speech Machine ’ . Retrieved Feb 28 , 2023 from https : [ 17 ] Bertram Gawronski . 2021 . Partisan bias in the identification of fake news . TiCS 25 , 9 ( 2021 ) , 723–724 . [ 18 ] Fabian Haak and Philipp Schaer . 2021 . Perception-Aware Bias Detection for Query Suggestions . In BIAS . 130–142 . [ 19 ] Fabian Haak and Philipp Schaer . 2022 . Auditing Search Query Suggestion Bias Through Recursive Algorithm Interrogation . In WebSci . 219–227 . [ 20 ] Daniel Hienert , Philipp Schaer , Johann Schaible , and Philipp Mayr . 2011 . A Novel Combined Term Suggestion Service for Domain-Specific Digital Libraries .. In TPDL ( Lecture Notes in Computer Science , Vol . 6966 ) , Stefan Gradmann , Francesca Borri , Carlo Meghini , and Heiko Schuldt ( Eds. ) . Springer , 192–203 . http : //dblp . # HienertSSM11 [ 21 ] Christoph Hube and Besnik Fetahu . 2019 . Neural Based Statement Classification for Biased Language . In WSDM . ACM . [ 22 ] L. Introna and H. Nissenbaum . 2000 . Defining the Web : the politics of search engines . Computer 33 , 1 , 54–62 . [ 23 ] Juhi Kulshrestha , Motahhare Eslami , Johnnatan Messias , Muhammad Bilal Zafar , Saptarshi Ghosh , Krishna P. Gummadi , and Karrie Karahalios . 2019 . Search bias quantification : investigating political bias in social media and web search . Inf . Retr . J . 22 , 1 , 188–227 . [ 24 ] Ruibo Liu , Chenyan Jia , and Soroush Vosoughi . 2021 . A Transformer-based Framework for Neutralizing and Reversing the Political Polarity of News Articles . Proc . ACM Hum.-Comput . Interact . 5 , 1–26 . [ 25 ] Bhaskar Mitra , Milad Shokouhi , Filip Radlinski , and Katja Hofmann . 2014 . On user interactions with query auto-completion . In SIGIR . 1055–1058 . [ 26 ] Negar Mokhberian , André s Abeliuk , Patrick Cummings , and Kristina Lerman . 2020 . Moral Framing and Ideological Bias of News . In SocInfo . 206–219 . [ 27 ] Marius Mosbach , Maksym Andriushchenko , and Dietrich Klakow . 2021 . On the Stability of Fine-tuning BERT : Misconceptions , Explanations , and Strong Baselines . In ICLR . https : //openreview.net/forum ? id=nzpLWnVAyah [ 28 ] Xi Niu and Diane Kelly . 2014 . The use of query suggestions during information search . IPM 50 , 1 , 218–234 . [ 29 ] Greg Pass , Abdur Chowdhury , and Cayley Torgeson . 2006 . A Picture of Search . In InfoScale . 1–es . [ 30 ] Lily Ray . 2020 . 2020 Google Search Survey : How Much Do Users Trust Their Search Results ? Retrieved Nov 30 , 2022 from https : //moz.com/blog/2020-google- search-survey [ 31 ] Shaina Raza , Deepak John Reji , and Chen Ding . 2022 . Dbias : Detecting biases and ensuring Fairness in news articles . Int J Data Sci Anal ( 2022 ) . [ 33 ] Reddit . 2022 . [ 32 ] Marta Recasens , Cristian Danescu-Niculescu-Mizil , and Dan Jurafsky . 2013 . Lin- guistic Models for Analyzing and Detecting Biased Language . In ACL . 1650–1659 . Retrieved Feb 28 , 2023 from https : the_worst_ai_ever_gpt4chan_model/ This is the worst AI ever . [ 34 ] Ronald E. Robertson , Shan Jiang , Kenneth Joseph , Lisa Friedland , David Lazer , and Christo Wilson . 2018 . Auditing Partisan Audience Bias within Google Search . Proc . ACM Hum.-Comput . Interact . 2 , CSCW , Article 148 ( 2018 ) . [ 35 ] Ronald E. Robertson , Shan Jiang , David Lazer , and Christo Wilson . 2019 . Auditing Autocomplete : Suggestion Networks and Recursive Algorithm Interrogation . In WebSci . 235–244 . [ 36 ] Victor Sanh , Lysandre Debut , Julien Chaumond , and Thomas Wolf . 2019 . Dis- tilBERT , a distilled version of BERT : smaller , faster , cheaper and lighter . CoRR abs/1910.01108 ( 2019 ) . [ 37 ] Danny Sullivan . 2018 . How Google autocomplete works in Search . Retrieved Nov 30 , 2022 from https : works-search/ [ 38 ] Peng Wang , Xianghang mi , Xiaojing Liao , Xiaofeng Wang , Kan Yuan , Feng Qian , and Raheem Beyah . 2018 . Game of Missuggestions : Semantic Analysis of Search- Autocomplete Manipulations . In NDSS .","['dataset', 'medium', 'bias', 'search', 'query', 'query', 'suggestion', 'fabian', 'haak', 'fabianhaak', 'thkoelnde', 'technische', 'hochschule', 'köln', 'cologne', 'thkoelnde', 'technische', 'hochschule', 'köln', 'cologne', 'publication', 'describe', 'motivation', 'generation', 'large', 'dataset', 'bing', 'search', 'query', 'scrape', 'tool', 'dataset', 'biased', 'news', 'article', 'well', 'language', 'model', 'investigation', 'bias', 'online', 'search', 'web', 'search', 'engine', 'major', 'factor', 'trust', 'source', 'information', 'search', 'especially', 'political', 'domain', 'however', 'bias', 'information', 'influence', 'opinion', 'formation', 'lead', 'biased', 'opinion', 'interact', 'search', 'engine', 'user', 'formulate', 'search', 'query', 'interact', 'search', 'query', 'suggestion', 'provide', 'search', 'engine', 'lack', 'dataset', 'search', 'query', 'inhibit', 'research', 'subject', 'use', 'evaluate', 'different', 'approach', 'finetune', 'transformer', 'base', 'language', 'model', 'goal', 'produce', 'model', 'capable', 'bias', 'text', 'left', 'right', 'political', 'stance', 'additionally', 'work', 'provide', 'dataset', 'language', 'model', 'bias', 'text', 'allow', 'research', 'bias', 'online', 'information', 'search', 'ccs', 'concept', 'information', 'system', 'web', 'search', 'engine', 'query', 'suggestion', 'query', 'reformulation', 'computing', 'methodology', 'natural', 'language', 'generation', 'keyword', 'web', 'search', 'dataset', 'bias', 'query', 'suggestion', 'search', 'query', 'language', 'model', 'transformer', 'acm', 'reference', 'format', 'fabian', 'haak', 'schaer', 'dataset', 'medium', 'bias', 'search', 'query', 'query', 'suggestion', '15th', 'acm', 'web', 'science', 'conference', 'page', 'n', 'r', 'c', 'v', 'r', 'introduction', 'search', 'engine', 'bing', 'see', 'trustworthy', 'source', 'information', 'many', 'topic', 'include', 'political', 'news', 'information', 'search', 'engine', 'prove', 'major', 'impact', 'formation', 'political', 'opinion', 'interact', 'search', 'engine', 'user', 'formulate', 'search', 'query', 'expression', 'information', 'need', 'base', 'query', 'search', 'engine', 'usually', 'provide', 'set', 'search', 'query', 'suggestion', 'query', 'choice', 'user', 'make', 'interact', 'query', 'suggestion', 'base', 'information', 'need', 'want', 'satisfy', 'often', 'support', 'personal', 'opinion', 'belief', 'found', 'previously', 'encounter', 'information', 'interaction', 'process', 'result', 'present', 'user', 'prone', 'bias', 'therefore', 'high', 'level', 'trust', 'see', 'problematic', 'however', 'true', 'effect', 'biased', 'search', 'query', 'different', 'type', 'inherent', 'bias', 'actual', 'list', 'search', 'result', 'sufficiently', 'investigate', 'main', 'reason', 'frequency', 'study', 'bias', 'search', 'query', 'lack', 'publicly', 'available', 'dataset', 'dataset', 'include', 'query', 'query', 'suggestion', 'actual', 'query', 'reformulation', 'rare', 'reason', 'collect', 'search', 'query', 'user', 'problematic', 'privacy', 'concern', 'tech', 'nique', 'pseudonymization', 'query', 'log', 'enable', 'identification', 'user', 'previously', 'available', 'dataset', 'long', 'available', 'usage', 'morally', 'debatable', 'use', 'unpersonalized', 'search', 'query', 'suggestion', 'proxy', 'solve', 'issue', 'query', 'suggestion', 'describe', 'list', 'predict', 'query', 'suggest', 'user', 'input', 'search', 'query', 'search', 'engine', 'sometimes', 'also', 'call', 'search', 'prediction', 'query', 'auto', 'completion', 'search', 'query', 'suggestion', 'generate', 'locally', 'result', 'set', 'take', 'global', 'knowledge', 'basis', 'web', 'search', 'suggestion', 'mostly', 'base', 'frequently', 'issue', 'query', 'user', 'therefore', 'assume', 'many', 'case', 'query', 'suggestion', 'popular', 'related', 'search', 'query', 'initial', 'root', 'query', 'represent', 'user', 'interest', 'topic', 'entity', 'news', 'domain', 'popular', 'domain', 'bias', 'research', 'sociological', 'relevance', 'twoparty', 'leftright', 'spectrum', 'facilitate', 'bias', 'analysis', 'benefit', 'dataset', 'enable', 'indepth', 'investigation', 'correlation', 'bias', 'search', 'query', 'search', 'result', 'search', 'result', 'popu', 'lar', 'web', 'search', 'engine', 'easily', 'collect', 'use', 'provide', 'dataset', 'search', 'query', 'therefore', 'goal', 'work', 'provide', 'large', 'dataset', 'search', 'query', 'suggestion', 'litical', 'news', 'domain', 'additionally', 'provide', 'transformerbased', 'methodology', 'bias', 'text', 'system', 'useful', 'tool', '30may', 'generate', 'biased', 'derivative', 'search', 'query', 'thus', 'evaluate', 'range', 'finetune', 'scenario', 'find', 'setting', 'produce', 'biased', 'result', 'provide', 'finetune', 'transformerbase', 'language', 'model', 'capable', 'induce', 'left', 'right', 'political', 'stance', 'bias', 'form', 'lexical', 'bias', 'publication', 'describe', 'motivation', 'generation', 'large', 'dataset', 'bing', 'search', 'query', 'scrape', 'tool', 'dataset', 'biased', 'news', 'article', 'well', 'language', 'model', 'investigation', 'bias', 'online', 'search', 'main', 'contribution', 'paper', 'datasets1', 'good', 'knowledge', 'large', 'label', 'dataset', 'search', 'query', 'suggestion', 'single', 'main', 'google', 'bing', 'dataset', 'biased', 'news', 'article', 'political', 'news', 'domain', 'scrape', 'tool', 'allow', 'researcher', 'easily', 'retrieve', 'uptodate', 'version', 'biased', 'news', 'dataset2', 'new', 'approach', 'produce', 'biased', 'search', 'query', 'intentionally', 'bias', 'transformerbase', 'language', 'model', 'capable', 'produce', 'biased', 'text', 'evaluation', 'different', 'finetuning', 'setting', 'find', 'capable', 'setup', 'produce', 'biasinduce', 'language', 'model', 'background', 'related', 'work', 'definition', 'bias', 'news', 'domain', 'subjective', 'ture', 'bias', 'publication', 'attempt', 'explicitly', 'define', 'bias', 'news', 'bias', 'news', 'generally', 'describe', 'nonneutral', 'opinionated', 'news', 'fuzzy', 'attribute', 'polit', 'ical', 'domain', 'term', 'opinionate', 'describe', 'fix', 'political', 'opinion', 'agenda', 'aspect', 'cognitive', 'author', 'readerside', 'bias', 'often', 'describe', 'partisan', 'bias', 'political', 'stance', 'partisanship', 'manifest', 'different', 'granularity', 'publishing', 'reporting', 'level', 'partisanship', 'express', 'selection', 'certain', 'topic', 'call', 'selection', 'bias', 'coverage', 'bias', 'selection', 'dif', 'ferent', 'view', 'topic', 'text', 'level', 'statement', 'bias', 'describe', 'member', 'medium', 'interject', 'opinion', 'text', 'manifest', 'overall', 'opinionate', 'text', 'take', 'various', 'form', 'range', 'phrase', 'bias', 'use', 'nonneutral', 'language', 'moral', 'framing', 'ideological', 'bias', 'spin', 'bias', 'describe', 'bias', 'introduce', 'omit', 'necessary', 'neutral', 'information', 'add', 'unnecessary', 'information', 'word', 'ngram', 'level', 'bias', 'manifest', 'linguistic', 'lexical', 'bias', 'linguistic', 'bias', 'highly', 'domain', 'contextspecific', 'example', 'frame', 'bias', 'describe', 'subjective', 'word', 'epistemo', 'logical', 'bias', 'attribute', 'word', 'target', 'credibility', 'statement', 'bias', 'identify', 'objectively', 'approach', 'detect', 'bias', 'rely', 'identification', 'linguistic', 'bias', 'research', 'bias', 'online', 'search', 'search', 'query', 'pub', 'lication', 'investigate', 'bias', 'online', 'search', 'aspect', 'search', 'result', 'investigate', 'partisan', 'bias', 'filter', 'bubble', 'effect', 'political', 'search', 'audit', 'serps', 'set', 'query', 'find', 'significant', 'evidence', 'unbiased', 'search', 'query', 'real', 'search', 'session', 'perform', 'real', 'user', 'lead', 'filter', 'bubble', 'effect', 'unanswered', 'question', 'biased', 'query', 'general', 'lead', 'biased', 'search', 'result', 'study', 'investigate', 'bias', 'search', 'query', 'difficult', 'accessibility', 'biased', 'search', 'query', 'study', 'focus', 'bias', 'search', 'query', 'suggestion', 'author', 'investigate', 'topical', 'group', 'bias', 'search', 'query', 'suggestion', 'political', 'domain', 'overall', 'observe', 'minor', 'topical', 'gender', 'bias', 'search', 'query', 'consist', 'name', 'politician', 'research', 'bias', 'online', 'search', 'show', 'factor', 'topic', 'query', 'phrasing', 'query', 'time', 'query', 'issue', 'also', 'impact', 'bias', 'see', 'user', 'dataset', 'biased', 'news', 'search', 'query', 'predict', 'medium', 'bias', 'use', 'news', 'article', 'collect', 'allside', 'balanced', 'news3', 'allside', 'news', 'popular', 'source', 'balanced', 'news', 'allside', 'high', 'standard', 'assign', 'bias', 'label', 'similarly', 'use', 'allsideslabele', 'news', 'article', 'adfonte', 'label', 'analyze', 'bias', 'different', 'granularity', 'mokhbe', 'rian', 'develop', 'frame', 'bias', 'detection', 'quantification', 'approach', 'use', 'collection', 'news', 'article', 'label', 'accord', 'label', 'provide', 'allsides4', 'dataset', 'search', 'query', 'query', 'suggestion', 'sparse', 'available', 'anymore', 'focus', 'narrow', 'topic', 'robertson', 'introduce', 'recursive', 'technique', 'recursively', 'retrieve', 'query', 'suggestion', 'root', 'query', 'consecutive', 'suggestion', 'technique', 'employ', 'haak', 'schaer', 'investigate', 'bias', 'german', 'political', 'domain', 'however', 'good', 'knowledge', 'currently', 'largescale', 'dataset', 'search', 'query', 'allside', 'scraper', 'dataset', 'present', 'novel', 'dataset', 'promote', 'investigation', 'bias', 'online', 'news', 'search', 'dataset', 'find', 'zenodo', 'mention', 'section', 'far', 'provide', 'web', 'scraping', 'tool', 'retrieve', 'uptodate', 'version', 'allside', 'dataset', 'provide', 'section', 'describe', 'content', 'dataset', 'well', 'creation', 'process', 'figure', 'show', 'assemble', 'dataset', 'use', 'approach', 'create', 'bias', 'language', 'model', 'allside', 'scraper', 'describe', 'section', 'allside', 'platform', 'provide', 'news', 'frequently', 'use', 'source', 'high', 'quality', 'label', 'biased', 'news', 'want', 'provide', 'easy', 'mean', 'retrieve', 'news', 'corresponding', 'information', 'similar', 'dataset', 'previously', 'produce', 'mention', 'section', 'however', 'compare', 'currently', 'recent', 'available', 'version', 'version', 'include', 'percent', 'additional', 'article', 'furthermore', 'many', 'task', 'especially', 'news', 'domain', 'relevant', 'recent', 'document', 'available', 'provide', 'pythonbased', 'scraper', 'scrape', 'available', 'allside', 'news', 'article', 'gather', 'available', 'information', 'describe', 'section', 'provide', 'scraper', 'facilitate', 'access', 'recent', 'version', 'dataset', 'researcher', 'dataset', 'find', 'zenodo', 'https', 'doiorg105281zenodo7682914', 'githubcomirgroupqbia', 'dataset', 'medium', 'bias', 'search', 'query', 'query', 'suggestion', '30may', 'search', 'query', 'news', 'domain', 'dataset', 'second', 'dataset', 'provide', 'consist', 'search', 'query', 'suggestion', 'root', 'query', 'base', 'tag', 'allside', 'bias', 'news', 'dataset', 'collect', 'search', 'query', 'suggestion', 'bing', 'topic', 'tag', 'use', 'tagging', 'allside', 'news', 'least', 'time', 'approximately', 'half', 'total', 'amount', 'topic', 'topic', 'tag', 'include', 'name', 'wide', 'range', 'political', 'term', 'agenda', 'topic', 'communism', 'party', 'samesex', 'marriage', 'cultural', 'religious', 'term', 'francis', 'location', 'newsrelevant', 'term', 'average', 'dataset', 'contain', 'search', 'query', 'topic', 'total', 'suggestion', 'retrieve', 'bing', 'use', 'python', 'implementation', 'loosely', 'adopt', 'framework', 'provide', 'scrape', 'query', 'suggestion', 'topic', 'root', 'query', 'use', 'run', 'scraper', 'retrieve', 'search', 'query', 'suggestion', 'provide', 'bing', 'search', 'autocomplete', 'system', 'input', 'root', 'query', 'furthermore', 'extend', 'root', 'query', 'let', 'ter', 'root', 'term', 'query', 'input', 'democrat', 'recession', 'query', 'suggestion', 'goal', 'procedure', 'simulate', 'user', 'input', 'information', 'search', 'retrieve', 'suggestion', 'root', 'query', 'ex', 'tend', 'query', 'generate', 'total', 'query', 'suggestion', 'topic', 'search', 'engine', 'dataset', 'provide', 'contain', 'column', 'root', 'term', 'query', 'input', 'query', 'suggestion', 'suggest', 'query', 'location', 'search', 'perform', 'loca', 'tion', 'server', 'run', 'colab', 'case', 'iowa', 'add', 'dataset', 'perform', 'scrape', 'blank', 'browser', 'search', 'personalization', 'effect', 'location', 'effect', 'suggest', 'search', 'query', 'scrape', 'setup', 'thus', 'eliminate', 'sonalization', 'effect', 'theory', 'cause', 'fect', 'search', 'engine', 'provider', 'describe', 'query', 'suggestion', 'base', 'user', 'search', 'successful', 'attempt', 'fluence', 'query', 'suggestion', 'confirm', 'claim', 'thus', 'deduce', 'query', 'suggestion', 'reflect', 'real', 'frequently', 'use', 'search', 'query', 'use', 'proxy', 'search', 'query', 'lack', 'information', 'frequency', 'collect', 'search', 'query', 'dataset', 'contain', 'rank', 'suggestion', 'well', 'sugge', 'tion', 'root', 'term', 'approximate', 'frequent', 'search', 'query', 'assume', 'topical', 'tag', 'allside', 'news', 'reflect', 'popular', 'topic', 'produce', 'dataset', 'consist', 'real', 'search', 'query', 'newsrelevant', 'topic', 'develop', 'bias', 'language', 'model', 'contribution', 'produce', 'pair', 'transformer', 'base', 'language', 'model', 'capable', 'induce', 'left', 'right', 'partisanship', 'linguistic', 'bias', 'leverage', 'masking', 'function', 'word', 'target', 'document', 'domainadopte', 'distilbert', 'section', 'describe', 'methodological', 'approach', 'develop', 'transformerbase', 'language', 'model', 'capable', 'bias', 'text', 'usually', 'system', 'develop', 'goal', 'produce', 'reproduce', 'little', 'bias', 'possible', 'bias', 'text', 'figure', 'pipeline', 'use', 'generate', 'biasinduce', 'language', 'model', 'black', 'box', 'represent', 'dataset', 'language', 'model', 'available', 'request', 'tool', 'provide', 'zenodo', 'allside', 'biased', 'news', 'dataset', 'dataset', 'contain', 'news', 'article', 'collect', 'allside', 'balanced', 'news', 'headline', 'roundup', 'side', 'balance', 'news', 'feature', 'expertselecte', 'news', 'article', 'source', 'different', 'political', 'view', 'leave', 'right', 'center', 'often', 'feature', 'spin', 'bias', 'slant', 'form', 'nonneutral', 'reporting', 'political', 'news', 'article', 'tag', 'bias', 'label', 'expert', 'annotator', 'base', 'express', 'political', 'partisanship', 'leave', 'right', 'neutral', 'allside', 'balance', 'news', 'aim', 'offer', 'multiple', 'political', 'perspective', 'important', 'news', 'story', 'educate', 'user', 'bias', 'provide', 'multiple', 'viewpoint', 'collect', 'datum', 'include', 'headline', 'news', 'text', 'publishing', 'date', 'topic', 'tag', 'coronavirus', 'federal', 'job', 'link', 'article', 'publishing', 'outlet', 'also', 'include', 'allside', 'neutral', 'description', 'topic', 'article', 'overall', 'produce', 'dataset', 'contain', 'article', 'tag', 'leave', 'article', 'tag', 'right', 'article', 'tag', 'center', 'collect', 'article', 'publish', 'date', 'datum', 'collection', 'end', 'use', 'allside', 'dataset', 'develop', 'bias', 'language', 'model', 'describe', 'section', 'allow', 'easy', 'access', 'recent', 'complete', 'version', 'dataset', 'future', 'research', 'provide', 'scrape', 'tool', 'describe', 'section', '30may', 'haak', 'schaer', 'try', 'achieve', 'opposite', 'bias', 'text', 'reproduce', 'bias', 'inherent', 'biased', 'finetune', 'dataset', 'example', 'allow', 'simulate', 'user', 'search', 'biased', 'term', 'poten', 'tially', 'cause', 'exposure', 'biased', 'news', 'goal', 'cap', 'ture', 'biased', 'language', 'pretraine', 'language', 'model', 'use', 'huggingface', 'transformer', 'module', 'finetune', 'base', 'distilbert', 'model', 'train', 'wikipedia', 'article', 'large', 'book', 'dataset5', 'distilbert', 'choose', 'base', 'model', 'prove', 'perform', 'well', 'comparably', 'small', 'dataset', 'perform', 'well', 'small', 'size', 'aptness', 'adopt', 'domainspecificity', 'aware', 'effective', 'model', 'show', 'suitability', 'approach', 'choose', 'distilbert', 'efficiency', 'sufficient', 'effectiveness', 'finetune', 'range', 'pair', 'model', 'left', 'model', 'finetune', 'part', 'allside', 'tag', 'leave', 'biased', 'news', 'right', 'model', 'finetune', 'document', 'dataset', 'label', 'rightbiase', 'news', 'produce', 'model', 'left', 'right', 'model', 'combination', 'parameter', 'goal', 'find', 'ideal', 'approach', 'capture', 'reproduce', 'much', 'bias', 'possible', 'produce', 'meaningful', 'result', 'central', 'aspect', 'vary', 'finetune', 'model', 'datum', 'use', 'model', 'develop', 'use', 'different', 'datum', 'configuration', 'headline', 'news', 'text', 'left', 'right', 'news', 'article', 'allside', 'dataset', 'b', 'news', 'text', 'c', 'headline', 'factor', 'evaluate', 'use', 'padding', 'concatenation', 'generate', 'consistent', 'chunk', 'size', 'finetune', 'choose', 'chunk', 'size', 'length', 'finetune', 'document', 'padding', 'approach', 'concatenation', 'approach', 'lastly', 'compare', 'effect', 'intentional', 'overfitting', 'number', 'epoch', 'compare', 'reasonable', 'epoch', 'prove', 'produce', 'good', 'combination', 'training', 'loss', 'validation', 'loss', 'dataset', 'configuration', 'theory', 'overfitting', 'possibility', 'reproduce', 'bias', 'mulation', 'text', 'use', 'finetune', 'try', 'intentionally', 'raise', 'number', 'epoch', 'parameter', 'evaluate', 'bias', 'language', 'model', 'model', 'available', 'request', 'decide', 'make', 'model', 'publicly', 'available', 'concern', 'potential', 'harm', 'cause', 'misuse', 'system', 'far', 'orate', 'section', 'evaluate', 'model', 'effectiveness', 'choose', 'combination', 'manual', 'assessment', 'quantitative', 'mea', 'sure', 'task', 'model', 'generate', 'biased', 'output', 'however', 'bias', 'diverse', 'easily', 'measurable', 'way', 'identify', 'biased', 'token', 'dataset', 'perplexity', 'useful', 'measure', 'evaluate', 'model', 'far', 'effectively', 'measure', 'bias', 'criterion', 'effectiveness', 'ability', 'system', 'produce', 'biased', 'version', 'query', 'however', 'assume', 'mask', 'word', 'search', 'query', 'topic', 'political', 'news', 'domain', 'let', 'pair', 'language', 'model', 'predict', 'word', 'output', 'left', 'right', 'model', 'differ', 'choose', 'evaluate', 'performance', 'search', 'query', 'want', 'use', 'model', 'future', 'research', 'bias', 'search', 'result', 'biased', 'unbiased', 'search', 'query', 'far', 'represent', 'text', 'different', 'type', 'domain', 'text', 'use', 'train', 'model', 'pair', 'model', 'measure', 'difference', 'model', 'probable', 'nonpunctuation', 'prediction', 'randomly', 'select', 'query', 'dataset', 'measure', 'rank', 'bias', 'overlap', 'rbo', '𝑝', 'mask', 'random', 'token', 'query', 'part', 'original', 'news', 'topic', 'stop', 'word', 'want', 'let', 'model', 'generate', 'meaningcarrye', 'token', 'keep', 'query', 'topic', 'intact', 'nonsensical', 'random', 'prediction', 'neologism', 'lead', 'great', 'rbo', 'score', 'want', 'generate', 'query', 'human', 'formulate', 'assure', 'model', 'generate', 'output', 'make', 'sense', 'let', 'model', 'generate', 'next', 'word', 'prediction', 'biasprovoke', 'sentence', 'hilary', 'covid', 'vaccine', 'domain', 'expert', 'professor', 'postdoctoral', 'researcher', 'label', 'nonsensical', 'biased', 'prediction', 'measure', 'annotator', 'agreement', 'individual', 'statement', 'independent', 'model', 'produce', 'statement', 'nonsensical', 'prediction', 'obtain', 'value', 'slight', 'agreement', 'biased', 'prediction', 'value', 'almost', 'perfect', 'bias', 'label', 'assign', 'suggest', 'word', 'induce', 'political', 'stance', 'bias', 'table', 'show', 'result', 'model', 'evaluation', 'process', 'low', 'ℎ𝑡', 'describe', 'headline', 'text', 'use', 'rank', 'bias', 'overlap', 'left', 'right', 'model', 'configuration', 'describe', 'average', 'percentage', 'nonsensical', 'prediction', 'model', '𝑃𝑏𝑖𝑎𝑠', 'percentage', 'biased', 'prediction', 'overall', 'model', 'finetune', 'headline', 'show', 'good', 'low', 'rbo', 'score', 'well', 'low', 'score', 'headline', 'text', 'text', 'perform', 'less', 'equally', 'term', 'rbo', 'concatenate', 'text', 'produce', 'average', 'well', 'result', 'pad', 'finetune', 'headline', 'effect', 'minimal', 'intentional', 'overfitting', 'raise', 'amount', 'training', 'epoch', 'seem', 'worsen', 'rbo', 'score', 'percentage', 'nonsensical', 'prediction', 'differ', 'much', 'different', 'finetuning', 'setup', 'finetune', 'text', 'seem', 'scenario', 'increase', 'percentage', 'nonsensical', 'prediction', 'many', 'model', 'generate', 'high', 'percentage', 'biased', 'suggestion', 'headline', 'model', 'high', 'percentage', 'biased', 'prediction', 'discussion', 'language', 'model', 'show', 'use', 'small', 'highquality', 'dataset', 'possible', 'finetune', 'transformerbase', 'language', 'model', 'bias', 'text', 'finetune', 'setup', 'model', 'finetune', 'headline', 'produce', 'overall', 'good', 'result', 'main', 'reason', 'assume', 'high', 'amount', 'quote', 'news', 'text', 'often', 'statement', 'author', 'article', 'disagree', 'induce', 'noise', 'finetune', 'model', 'far', 'condense', 'ture', 'headline', 'aim', 'catch', 'attention', 'also', 'reflect', 'result', 'language', 'model', 'output', 'left', 'right', 'model', 'finetune', 'headline', 'produce', 'text', 'contain', 'linguistic', 'bias', 'example', 'trump', 'model', 'produce', 'hero', 'rightbiase', 'fraud', 'leftbiase', 'next', 'word', 'prediction', 'overall', 'low', 'percentage', 'nonsensical', 'prediction', 'support', 'rbo', 'suited', 'evaluation', 'metric', 'overall', 'good', 'result', 'dataset', 'medium', 'bias', 'search', 'query', 'query', 'suggestion', '30may', 'table', 'evaluation', 'result', 'finetune', 'language', 'model', 'highlight', 'value', 'good', 'result', 'metric', 'model', 'configuration', '𝑜𝑣𝑒𝑟', '𝑖𝑡𝑡𝑖𝑛𝑔', '𝑜𝑣𝑒𝑟', '𝑜𝑣𝑒𝑟', '𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ', '𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇', '𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇', '𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇', '𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ', '𝑜𝑣𝑒𝑟', '𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇', '𝑝𝑎𝑑𝑑𝑖𝑛𝑔𝑡', '𝑜𝑣𝑒𝑟', '𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ𝑡', '𝑜𝑣𝑒𝑟', 'future', 'research', 'investigate', 'finding', 'language', 'model', 'compare', 'performance', 'result', 'scrape', 'tool', 'dataset', 'allow', 'effectively', 'generate', 'language', 'model', 'enable', 'investigation', 'research', 'topic', 'political', 'news', 'information', 'search', 'domain', 'bias', 'classification', 'sentiment', 'analysis', 'remark', 'moral', 'issue', 'aware', 'build', 'biasinducing', 'system', 'provide', 'dataset', 'enable', 'reproduction', 'develop', 'similar', 'system', 'problematic', 'also', 'consider', 'work', 'inspire', 'help', 'develop', 'transformerbase', 'language', 'model', 'capable', 'produce', 'biased', 'toxic', 'otherwise', 'harmful', 'text', 'severity', 'openly', 'publish', 'biased', 'system', 'show', 'model', 'model6', 'publication', 'model', 'incite', 'debate', 'show', 'problematic', 'biased', 'language', 'make', 'available', 'wide', 'public', 'au', 'dience', 'explicitly', 'state', 'intend', 'use', 'research', 'application', 'model', 'bias', 'primarily', 'term', 'linguistic', 'bias', 'reflect', 'political', 'stance', 'view', 'political', 'topic', 'part', 'include', 'objectively', 'wrong', 'opinionated', 'statement', 'hate', 'speech', 'racism', 'form', 'despicable', 'language', 'toxicity', 'model', 'reproduce', 'text', 'convey', 'phenomenon', 'minimize', 'harmful', 'effect', 'publication', 'cause', 'misuse', 'model', 'decide', 'make', 'biased', 'model', 'publicly', 'accessible', 'provide', 'access', 'model', 'request', 'research', 'reasonable', 'application', 'use', 'case', 'provide', 'however', 'interest', 'investigate', 'bias', 'correlation', 'bias', 'online', 'information', 'search', 'goal', 'increase', 'transparency', 'fairness', 'raise', 'awareness', 'easily', 'intentionally', 'bias', 'reproduce', 'induce', 'system', 'part', 'endeavor', 'show', 'datum', 'provide', 'use', 'create', 'model', 'produce', 'biased', 'language', 'choose', 'publish', 'dataset', 'scraper', 'mainly', 'reason', 'state', 'section', 'similar', 'dataset', 'available', 'publicly', 'b', 'believe', 'benefit', 'raise', 'awareness', 'outweigh', 'make', '6https', 'already', 'public', 'biased', 'datum', 'accessible', 'c', 'dataset', 'biased', 'language', 'require', 'develop', 'system', 'detect', 'inhibit', 'bias', 'work', 'hope', 'highlight', 'need', 'assure', 'datum', 'use', 'develop', 'model', 'unbiased', 'raise', 'awareness', 'easily', 'transformerbase', 'language', 'model', 'finetune', 'produce', 'biased', 'language', 'outlook', 'publication', 'represent', 'first', 'foundational', 'milestone', 'largerscale', 'investigation', 'bias', 'online', 'information', 'search', 'major', 'next', 'contribution', 'plan', 'use', 'present', 'dataset', 'model', 'investigate', 'effect', 'biased', 'unbiased', 'search', 'query', 'search', 'result', 'different', 'search', 'engine', 'popular', 'topic', 'political', 'news', 'domain', 'accomplish', 'need', 'overcome', 'issue', 'subjectivity', 'lack', 'effective', 'method', 'ological', 'approach', 'bias', 'identification', 'employ', 'human', 'annotation', 'plan', 'introduce', 'ensemble', 'method', 'include', 'biasagnostic', 'analysis', 'linguistic', 'difference', 'lexical', 'feature', 'transformerbase', 'approach', 'bias', 'classification', 'far', 'plan', 'conduct', 'study', 'use', 'simulation', 'approach', 'simulation', 'different', 'user', 'behavior', 'term', 'information', 'need', 'formulating', 'query', 'interact', 'query', 'suggestion', 'interactive', 'information', 'search', 'simulation', 'plan', 'gain', 'insight', 'effect', 'bias', 'formation', 'information', 'search', 'additionally', 'hope', 'identify', 'user', 'property', 'havior', 'increase', 'lower', 'risk', 'encounter', 'bias', 'information', 'search', 'conclusion', 'work', 'present', 'first', 'milestone', 'ongoing', 'research', 'bias', 'online', 'search', 'present', 'dataset', 'biased', 'news', 'dataset', 'large', 'dataset', 'biased', 'unbiased', 'search', 'query', 'topic', 'political', 'news', 'domain', 'far', 'provide', 'scrape', 'tool', 'allow', 'collect', 'biaslabele', 'news', 'text', 'allside', 'lastly', 'evaluate', 'approach', 'finetune', 'distilbert', 'transformerbase', 'language', 'model', 'bias', 'text', 'publish', 'model', 'capable', 'induce', 'left', 'right', 'political', 'stance', 'bias', 'form', 'lexical', 'bias', 'reference', 'allside', 'allside', 'create', 'balanced', 'news', 'stepbystep', 'guide', 'retrieve', 'https', 'allside', 'balanced', 'news', 'headline', 'roundup', 'retrieve', 'https', 'jing', 'bai', 'shinnou', 'construction', 'domain', 'specific', 'distilbert', 'model', 'use', 'finetune', 'taai', 'preslav', 'nakov', 'detect', 'bias', 'predict', 'political', 'ideology', 'news', 'article', 'emnlp', 'zeller', 'face', 'expose', 'searcher', 'cool', 'diane', 'park', 'iterative', 'exploration', 'design', 'evaluation', 'support', 'query', 'reformulation', 'interactive', 'information', 'retrieval', 'tolga', 'bolukbasi', 'zou', 'venkatesh', 'saligrama', 'man', 'computer', 'programmer', 'woman', 'homemaker', 'debiase', 'word', 'embedding', 'nip', 'schaer', 'investigation', 'bias', 'web', 'search', 'engine', 'query', 'suggestion', '30may', 'haak', 'schaer', 'lysandre', 'debut', 'victor', 'sanh', 'clement', 'delangue', 'pierric', 'cistac', 'clara', 'canwen', 'sylvain', 'gugger', 'quentin', 'lhoest', 'rush', 'transformer', 'stateoftheart', 'natural', 'language', 'processing', 'emnlp', 'improve', 'effectiveness', 'information', 'retrieval', 'local', 'context', 'analysis', 'acm', 'tran', 'inf', 'syst', 'https', 'arzucan', 'özgür', 'analyze', 'elmo', 'distilbert', 'sociopolitical', 'news', 'classification', 'aespen', 'maarten', 'de', 'rijke', 'survey', 'query', 'auto', 'completion', 'information', 'retrieval', 'fntir', 'weifan', 'wachsmuth', 'analyze', 'political', 'bias', 'unfairness', 'news', 'article', 'different', 'level', 'granularity', 'https', 'coefficient', 'agreement', 'nominal', 'scale', 'psychol', 'alessio', 'medium', 'bias', 'presidential', 'election', 'metaanalysis', 'j', 'commun', 'edelman', 'edelman', 'trust', 'barometer', 'retrieve', 'https', 'search', 'engine', 'manipulation', 'effect', 'seme', 'possible', 'impact', 'outcome', 'election', 'pna', 'e4512', 'publisher', 'section', 'pna', 'train', 'become', 'hate', 'speech', 'machine', 'retrieve', 'https', 'bertram', 'gawronski', 'partisan', 'bias', 'identification', 'fake', 'news', 'tic', 'fabian', 'haak', 'schaer', 'perceptionaware', 'bias', 'detection', 'query', 'suggestion', 'bias', 'fabian', 'haak', 'schaer', 'auditing', 'search', 'query', 'suggestion', 'bias', 'schaer', 'schaible', 'novel', 'combine', 'term', 'suggestion', 'service', 'domainspecific', 'digital', 'library', 'tpdl', 'lecture', 'note', 'computer', 'science', 'vol', 'heiko', 'schuldt', 'ed', 'springer', 'http', 'hienertssm11', 'besnik', 'neural', 'base', 'statement', 'classification', 'biased', 'language', 'wsdm', 'acm', 'l', 'introna', 'define', 'web', 'politic', 'search', 'engine', 'computer', 'juhi', 'messia', 'krishna', 'p', 'gummadi', 'search', 'bias', 'quantification', 'investigate', 'political', 'bias', 'social', 'medium', 'web', 'search', 'inf', 'retr', 'j', 'ruibo', 'transformerbase', 'framework', 'neutralize', 'reverse', 'political', 'polarity', 'news', 'article', 'proc', 'acm', 'humcomput', 'interact', 'bhaskar', 'milad', 'shokouhi', 'filip', 'radlinski', 'user', 'interaction', 'query', 'autocompletion', 'sigir', 'negar', 'cumming', 'moral', 'framing', 'ideological', 'bias', 'news', 'socinfo', 'marius', 'andriushchenko', 'stability', 'finetune', 'bert', 'misconception', 'explanation', 'strong', 'baseline', 'iclr', 'https', 'openreviewnetforum', 'idnzplwnvayah', 'niu', 'diane', 'use', 'query', 'suggestion', 'information', 'search', 'ipm', 'pass', 'abdur', 'chowdhury', 'picture', 'search', 'infoscale', 'lily', 'ray', 'search', 'survey', 'much', 'user', 'trust', 'search', 'result', 'retrieve', 'mozcomblog2020google', 'shaina', 'deepak', 'de', 'dbia', 'detect', 'bias', 'ensure', 'fairness', 'news', 'article', 'int', 'anal', 'reddit', 'recasen', 'danescuniculescumizil', 'guistic', 'model', 'analyze', 'detect', 'biased', 'language', '1650–1659', 'retrieve', 'bad', 'ai', 'ever', 'auditing', 'partisan', 'audience', 'bias', 'search', 'proc', 'acm', 'humcomput', 'interact', 'cscw', 'article', 'auditing', 'autocomplete', 'suggestion', 'network', 'recursive', 'victor', 'sanh', 'lysandre', 'tilbert', 'distilled', 'version', 'small', 'fast', 'cheap', 'light', 'danny', 'autocomplete', 'work', 'search', 'retrieve', 'workssearch', 'raheem', 'beyah', 'game', 'missuggestion', 'semantic', 'analysis', 'search', 'autocomplete', 'manipulation']"
"DreamSmooth: Improving Model-based Reinforcement Learning via Reward
  Smoothing","[{'href': 'http://arxiv.org/abs/2311.01450v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2311.01450v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-11-02 17:57:38,"3
2
0
2

v
o
N
8
2

]
L
C
.
s
c
[

1
v
4
6
2
7
1
.
1
1
3
2
:
v
i
X
r
a

Preprint

RETSIM: RESILIENT AND EFFICIENT TEXT
SIMILARITY

Marina Zhang1, Owen Vallis1, Aysegul Bumin*2, Tanay Vakharia1, Elie Bursztein1
Google1 University of Florida2

ABSTRACT

This paper introduces RETSim (Resilient and Efficient Text Similarity), a
lightweight, multilingual deep learning model trained to produce robust metric
embeddings for near-duplicate text retrieval, clustering, and dataset deduplication
tasks. We demonstrate that RETSim is significantly more robust and accurate
than MinHash and neural text embeddings, achieving new state-of-the-art perfor-
mance on dataset deduplication, adversarial text retrieval benchmarks, and spam
clustering tasks. We also introduce the W4NT3D benchmark (Wiki-40B 4dver-
sarial Near-T3xt Dataset) for evaluating multilingual, near-duplicate text retrieval
capabilities under adversarial settings. RETSim and the W4NT3D benchmark are
open-sourced under the MIT License at https://github.com/google/unisim.

1

INTRODUCTION

Robust near-duplicate text detection is an essential component of many tasks, including retriev-
ing documents, detecting plagiarism (Sun et al., 2013) and blocking adversarial spam cam-
paigns (Ahmed et al., 2022). Users have come to expect that systems can return accurate results
despite their queries exhibiting a 20% to 30% typo rate (Hagen et al., 2017). Furthermore, effi-
ciently deduplicating text datasets is critical to training state-of-the-art large language models (Lee
et al., 2022; Kandpal et al., 2022).

For more than two decades, MinHash-based (Broder et al., 1998) locality-sensitive hashing (LSH)
has been the most prevalent algorithm used for near-duplicate detection due to its simplicity, robust-
ness, and speed. For example, the vast majority of dataset deduplication efforts still rely on MinHash
(Lee et al., 2022; Kocetkov et al., 2022). However, like all LSH-based techniques, MinHash is not
without downsides; chief among them being that it is very parameter-sensitive and requires heavy
tuning. Additionally, MinHash lacks resilience to typos due to its reliance on n-grams, leading to
poor performance on noisy data and a vulnerability to hash-busting attacks (Issac et al., 2014).

On the other hand, deep learning models are the dominant way to perform vector-based semantic text
retrieval (Muennighoff et al., 2022), but so far, no neural embedding has been able to consistently
outperform MinHash for robust near-duplicate detection (Silcock et al., 2022). This is mostly due to
the focus on improving semantic capabilities, which leads models to be too large to run extremely
quickly and the use of sub-word level tokenization, which is not resilient to typos and adversarial
attacks (Morris et al., 2020; Bursztein et al., 2023).

To fill this gap, we introduce RETSim (Resilient and Efficient Text Similarity), a lightweight, mul-
tilingual deep learning model trained specifically to produce robust neural embeddings specialized
for near-duplicate detection. By combining the state-of-the-art RETVec text vectorizer, a modern
transformer block (Hua et al., 2022), a large typo-augmented training corpus, and a metric learn-
ing training regime, RETSim is able to achieve new state-of-the-art performance on near-duplicate
detection benchmarks (Section 4.2), dataset deduplication tasks (Sections 4.3 and 5.1), and spam
clustering applications (Section 5.2).

Furthermore, while datasets and benchmarks exist for corpus deduplication and near-duplicate text
retrieval, none of these have focused on systematically evaluating near-duplicate retrieval perfor-
mance under the presence of typos, word manipulations, and sentence or paragraph-level modifica-

*This work was done during the author’s internship at Google.

1

 
 
 
 
 
 
Preprint

tions. To address this need, we additionally introduce the W4NT3D benchmark (Wiki-40B 4dver-
sarial Near-T3xt Dataset) which enables the evaluation of algorithms on adversarial near-duplicate
text retrieval in a multilingual setting. We report the performance of RETSim, MinHash, and pop-
ular neural embeddings such as Universal Sentence Encoder (Cer et al., 2018) and LaBSE (Feng
et al., 2022) on this new benchmark in Section 4.2, highlighting uneven performance across lan-
guages and types of adversarial manipulations. The RETSim model and the W4NT3D benchmark
are open-sourced at https://github.com/google/unisim under the MIT License.

2 RELATED WORK

Near-Duplicate Detection Identifying noisy near-duplicate documents in a large corpus is a fun-
damental task with a wide range of applications, such as detecting plagiarism, finding reproduced
content in literature or news articles (Gyawali et al., 2020; Silcock et al., 2022), and deduplicat-
ing training datasets for language models. Previous research has shown that duplicates in training
datasets lead to inefficient training (Lee et al., 2022) and privacy concerns for large language models
(LLMs), where models memorize and regenerate duplicated training sequences at a much higher
frequency (Kandpal et al., 2022).

Unlike semantic text similarity, the task of identifying textual near-duplicates has been predominated
by non-neural, n-gram-based algorithms such as MinHash (Broder et al., 1998), which is the most
widely used technique for deduplicating large training corpuses (Kocetkov et al., 2022; Lee et al.,
2022). MinHash is a technique for estimating the Jaccard similarity between two sets. Algorithms
such as MinHash or SimHash (Charikar, 2002) can be combined with locality-sensitive hashing
(LSH) techniques for fast, approximate nearest neighbor search and data clustering. This allows
them to scale and deduplicate corpuses containing terabytes of data such as C4 (Lee et al., 2022)
and The Stack (Kocetkov et al., 2022). However, n-gram or shingling-based techniques typically
require texts to be parsed into a standardized form (e.g. by lower-casing or stripping punctuation),
which makes them susceptible to typos and adversarial attacks and pose a challenge when attempt-
ing to differentiate between dissimilar documents and near-duplicate documents with adversarial
augmentations.

Semantic Text Similarity The task of computing semantic similarity between text is closely re-
lated to near-duplicate detection. Semantic text similarity refers to the assessment of the semantic
relatedness of two pieces of text based on their meaning rather than their syntactic structure, as in
the case of near-duplicate detection. Recently, transformer-based language models such as Universal
Sentence Encoder (Yang et al., 2019), LaBSE (Feng et al., 2022) and LLM-based embeddings (Anil
et al., 2023) which embed text into high-dimensional embedding vectors have been successfully
used to retrieve semantically-related documents using cosine similarity. Modern text retrieval sys-
tems combine these embeddings with an approximate nearest neighbor (ANN) search algorithm to
efficiently retrieve documents matching user queries.

However, language models have been shown to be vulnerable to adversarial attacks and naturally-
occurring typos (Alzantot et al., 2018; Gao et al., 2018; Morris et al., 2020). Furthermore, language
models are typically very large and costly to run even with hardware acceleration, which makes
them unsuited for large-scale dataset deduplication or identifying near-duplicates in the presence of
typos or adversarial text manipulations.

Metric Learning Metric learning aims to learn an embedding space where similar items have
a small distance between their embeddings and dissimilar items are further away. Many state-of-
the-art embeddings use metric learning for unsupervised training or fine-tuning including Sentence-
BERT (Reimers & Gurevych, 2019) and E5 (Wang et al., 2022).

RETVec is a resilient, multilingual embedding and text vectorizer trained to be robust against various
forms of character-level typos and adversarial attacks. We extend the RETVec training regime to
full text documents for RETSim. We use Multi-Similarity Loss (Wang et al., 2019) for pair-based
metric learning, where typo-laden and near-duplicate versions of texts are trained to be closer in
the embedding space, while other texts are pushed further away. Multi-Similarity Loss is based
on a general weighting framework for pair-based losses and achieves state-of-the-art performance,
outperforming alternatives such as Triplet Loss (Schroff et al., 2015).

2

Preprint

Figure 1: RETSim model architecture diagram. RETSim works on arbitrary length text by split-
ting texts into chunks of 512 characters during its vectorization phase and encodes them using the
RETVec character vectorizer. The RETSim model then embeds each chunk of text into 256-dim
partial embeddings and combines them to produce the global embedding.

3 RETSIM

3.1 ARCHITECTURE

The RETSim model is composed of three main components (as depicted in Figure 1):

The character-level vectorizer
splits the input text into chunks of 512 characters, then uses the
RETVec chararcter encoder (Bursztein et al., 2023) to encode each chunk, resulting in a batch of
(512, 24) dense inputs. The RETVec character vectorizer encodes each Unicode character as a
compact 24-bit binary representation based on its integer codepoint value. This allows the vectorizer
to encode all valid Unicode characters and support all languages. Furthermore, the character-level
vectorizer has been shown to be more resilient against typos and adversarial attacks.

A small transformer model
is used to compute 256-dimension embeddings for each chunk of the
input text. RETSimPartial-Dup uses these embeddings directly to finding documents that have matching
chunks of text. Architecturally, the model consists of two Gated Attention Unit (GAU) blocks (Hua
(Radenovi´c et al., 2018), a dense
et al., 2022), followed by a Generalized-Mean pooling layer
projection layer which projects the embedding into 256 dimensions, and an L2 normalization layer.
The model has only 536k parameters, which is more than two orders of magnitude smaller than other
neural embeddings (Table 1). L2-normalization allows the embeddings to be compared using cosine
similarity. We discuss the impact of key architecture design choices in Section 6. Hyperparameter
details are provided in Appendix A.1.1, and additional ablations results in Appendix A.5.

An embedding averaging module
is then used to combine partial text embeddings into a full-text
embedding which is used for global near-duplicate matching (RETSimNear-Dup). Averaging chunked
embeddings to produce a global embedding is a standard technique used by many models (Cer
et al., 2018) to support infinite length inputs in a cost-efficient manner. We experimented with other
aggregation techniques to produce more accurate global embeddings, including training a deep-
averaging network (Iyyer et al., 2015), but this did not improve performance and resulted in higher
computation cost. RETSimNear-Dup and RETSimPartial-Dup are computed in a single forward pass
which makes it computationally efficient. We output both types of embeddings as they have different
applications: RETSimNear-Dup is better-suited for full-text matching and retrieval (Section 4.2), while
RETSimPartial-Dup is used to find partial text matches where the near-duplicate content appears only
in part of the document (Section 4.3).

3.2 MODEL TRAINING

Dataset We use the multilingual C4 dataset (mC4) for raw text data and following (Xue et al.,
2020), we use a language sampling exponent of α = 0.3 to balance sampling between low and
high-resource languages. We only use text containing at least 16 characters, and we randomly select
between 1 and 8 sentences (roughly 512 characters) for each text chunk. For each example in the

3

Input text

Chunk vector

Chunk vector

....

Chunk vector

Chunked and 
vectorized text
(num_chunks, 512, 24)

RETSim model

RetSim Partial-Dup 
(num_chunks, 256)

RetSim Near-Dup  
(256)

r

e

d

o

c

n

E

r

a

h

C

c

e

v

T

E

R

U

A

G

U

A

G

g

n

i

l

o

o

P

2

L

+

e

s

n

e

D

t

x

e

t

l

a

i

t

r

a

P

s

g

n

i

d

d

e

b

m

e

2

L

+

g

n

i

g

r

a

r

e

v

A

g

n

i

d

d

e

b

m

e

t

x

e

t

l

l

u

F

 
 
 
 
 
 
 
 
 
Preprint

training dataset, we generate 5 pairs of augmented examples. We apply three levels of augmentation
to each example text chunk (in this order): sentence-level, word-level, and character-level. For each
level, we randomly select the augmentation to be applied from the following categories: insertion,
deletion, substitution, and transposition. We randomly apply between 0 − 25% sentence-level aug-
mentation and up to 30% combined character and word-level augmentation. Empirically, we found
that increasing the percentage of augmentation beyond this point causes RETSim’s performance to
degrade. The full list of augmentations used can be found in Appendix A.2.

Training Procedure We train RETSim using Multi-Similarity Loss (Wang et al., 2019) with α =
4, β = 40, λ = 0.5, and ϵ = 0.1. We hypertuned these parameters and the results are shown in
Appendix A.5. We train for 1 million steps with batch size = 1024. The similarity loss trains the
model to embed augmented versions of the same text closer in the embedding space, while dissimilar
texts are pushed further apart. We use the LAMB optimizer (You et al., 2019) with a max learning
rate of 0.001 and cosine decay. Detailed training hyperparameters are reported in Appendix A.1.2.

4 EVALUATION

Model/Algorithm

Type

Embed./Hash Size

# Model Parameters

LaBSE
Multilingual USE
Multilingual E5-Base
PaLM 2 (Gecko)

SimHash
MinHash

RETSim

Neural
Neural
Neural
Neural

Hashing
Hashing

Neural

768
512
768
768

b bits
n hashes

256

471M
69M
278M
?

N/A
N/A

536k

Table 1: Embedding models and hashing algorithms benchmarked in the paper.

4.1 MODELS AND ALGORITHMS EVALUATED

We benchmark RETSim against four multilingual semantic text embeddings as well as popular n-
gram based algorithms primarily used in near-duplicate text detection (Table 1). Our baseline text
embeddings include Multilingual Universal Sentence Encoder
(Yang et al., 2019), LaBSE (Feng
et al., 2022), Multilingual E5 (Wang et al., 2022), and PaLM 2 Gecko Embeddings (Anil et al.,
2023). All text embeddings are L2-normalized and compared using cosine similarity. We use exact
search to index and retrieve nearest neighbors from our vector index for the experiments in Section 4.

For non-neural near-duplicate detection and clustering algorithms, we selected the two most popular
algorithms: MinHash (Broder et al., 1998) and SimHash (Charikar, 2002). For MinHash, we use
Datasketch’s MinHashLSH library. Following the most common practices in the literature (Silcock
et al., 2022), we use 10 hash functions for MinHash unless otherwise specified. We use word-level
n-grams where we select the best value out of n = {2, 3, 4, ..., 10}. For SimHash, we use 64-
bit SimHash and conduct shingling at the character level, where the shingle size is selected from
n = {2, 3, 4, ..., 10}. For the near-duplicate detection benchmarks (NEWS-COPY and CORE Near-
Duplicates datasets), we tune the optimal deduplication threshold (e.g. based on cosine similarity
for neural-based embeddings and Jaccard similarity for MinHash). Detailed hyperparameter settings
for RETSim and baseline algorithms used in the evaluation can be found in Appendix A.3.

4.2 W4NT3D: WIKI-40B 4DVERSARIAL NEAR-T3XT DATASET EVALUATION

Dataset Description The vast majority of text retrieval benchmarks are focused on evaluating
semantic performance. To the best of our knowledge, there is no multilingual benchmark for sys-
tematically measuring adversarial robustness for near-duplicate text retrieval. In an attempt to fill in
the gap, we create and publish the W4NT3D benchmark (Wiki-40B 4dversarial Near-T3xt Dataset),
which contains around 400k pairs of syntactically similar texts to evaluate near-duplicate text re-
trieval in the presence of various forms of text manipulations and typos.

W4NT3D is based on the Wiki-40B dataset (Guo et al., 2020). The dataset is split into query exam-
ples and target examples, where query examples are synthetically-modified near-duplicate versions

4

Preprint

of a target example (e.g. with typos). For each of the 41 language splits in Wiki-40B, we randomly
select 10,000 texts. The length of the target string is uniformly selected from between 16 and 8192
characters, in order to test performance on short and long text. To construct the query text corre-
sponding to a target text, we randomly apply up to 25% word and character augmentations, and up to
25% sentence and paragraph augmentations. For each augmentation, we uniformly select from the
[insert, delete, substitute, and swap] operations. We use Recall@k with k = 1 as the main metric,
following the setup commonly found in semantic text retrieval benchmarks.

Model/Algorithm

Arabic Chinese English German French

Spanish

Japanese Korean Russian Thai

Avg (41 Langs)

LaBSE
Multilingual USE
Multilingual E5-Base
PaLM 2 (Gecko)

SimHash
MinHash

RETSimPartial-Dup
RETSimNear-Dup

0.915
0.915
0.936
0.497

0.558
0.633

0.928
0.971

0.917
0.986
0.980
0.623

0.276
0.172

0.946
0.971

0.944
0.958
0.959
0.961

0.591
0.591

0.954
0.987

0.931
0.942
0.944
0.932

0.561
0.558

0.949
0.978

0.930
0.938
0.948
0.934

0.519
0.556

0.947
0.983

0.888
0.903
0.896
0.911

0.513
0.575

0.938
0.976

0.931
0.990
0.979
0.578

0.465
0.223

0.963
0.986

0.949
0.984
0.986
0.701

0.593
0.814

0.971
0.991

0.918
0.910
0.911
0.851

0.554
0.523

0.946
0.970

0.882
0.888
0.921
0.571

0.669
0.416

0.941
0.946

0.921
0.912
0.937
0.823

0.550
0.538

0.949
0.977

Table 2: Per-language retrieval performance for various embedding models and algorithms on the
W4NT3D benchmark. Results on selected languages are reported alongside the average Recall@1
for all 41 languages. Full results for all languages are reported in Appendix A.4.

Multilingual Performance Overall, RETSimNear-Dup achieves an average Recall@1 of 0.977
across all 41 languages on the W4NT3D benchmark (Table 2). RETSimPartial-Dup is second best
with a Recall@1 of 0.949 and Multilingual E5, the best-performing baseline, is third with an aver-
age Recall@1 of 0.932. We expect that RETSimNear-Dup outperforms RETSimPartial-Dup because the
W4NT3D benchmark requires an algorithm to not just find near-duplicates, but to find the most simi-
lar text. RETSimPartial-Dup optimizes for finding the most similar chunk of text in the corpus, which is
not always the most similar text overall. Similarly, we hypothesize that MinHash and SimHash per-
form poorly on the W4NT3D benchmark due to their lack of ability to distinguish which is the most
similar text among the near-duplicates detected, and embedding-based models and cosine similarity
offer a more fine-grained measure of similarity.

RETSimNear-Dup outperforms baseline algorithms on all languages except for Chinese and Japanese.
For these languages, we theorize that semantic embeddings may have the slight edge in performance
because their significantly larger model sizes (more than 100x larger than RETSim, as shown in Ta-
ble 1) allow them to have a better representation on languages with large character sets. Furthermore,
the sub-word level tokenizers used in the baseline embeddings often treat each character in Chinese
or Japanese as individual tokens, which could offer higher resilience to typos.

Figure 2: Recall@1 performance on the W4NT3D benchmark, broken down by augmentation type.
Results are averaged across all 41 language splits in W4NT3D.

Adversarial Resilience Delving deeper into the impact of various types of text manipulation re-
veals that RETSimNear-Dup and RETSimPartial-Dup perform almost equally well regardless of the type
of augmentation applied (Figure 2). Semantic text embeddings perform well on paragraph, sentence
and word-level manipulations, but as expected, exhibit significantly weaker performance towards
character-level typos. MinHash and SimHash struggle more with word-level augmentations than
deep-learning based embeddings and collapse when character-level typos are introduced. We at-

5

1.00

0.75

0.50

0.25

0.00

l
l

1
@
a
c
e
R

Paragraph

Sentence

Word

Character

Augmentation Level

LaBSE

Multilingual USE

Multilingual E5-Base

PaLM 2 (Gecko)

SimHash

MinHash

RETSim (Partial-Dup)

RETSim (Near-Dup)

Preprint

tribute RETSim’s resilience to adversarial manipulations to the RETVec character encoder as well
as using deep metric learning to train robust embeddings.

Figure 4 reports the Recall@1 performance of the algorithms as the amount of augmentation in-
creases. All algorithms perform perfectly when no augmentation is applied (exact matching), but as
the percentage of augmentation increases, n-gram based approaches exhibit a steep drop in perfor-
mance. Semantic text embeddings are able to sustain a larger degree of augmentation before their
retrieval capabilities start to degrade (over 20%). RETSimNear-Dup is the most robust algorithm, with
a noticeable drop in performance only after around 40% augmentation. This makes RETSim the
most effective approach at clustering and deduplicating text under adversarial settings.

Figure 3: Recall@1 performances on the
W4NT3D benchmark (English only) for
varying max target lengths.

Figure 4: Recall@1 performances on the W4NT3D
benchmark (English only) as the amount of augmenta-
tion applied to the query text increases.

Text Length Impact on Performance Figure 3 reports the Recall@1 performance of RETSim
and baseline algorithms as the length of the query and target text varies. We see that RETSimNear-Dup
and RETSimPartial-Dup outperforms all other methods on short texts with fewer than 128 characters.
As the text length increases beyond 512 characters, RETSimNear-Dup remains close to perfect while
RETSimPartial-Dup’s performance degrades since it splits the text into multiple embeddings and finds
the nearest matching chunk of text. MinHash and SimHash also perform poorly on short text lengths
and start to degrade on longer texts. For neural-based embeddings, we observe a slight drop in
performance on longer texts for all models except RETSimNear-Dup and Multilingual USE, the only
two embeddings that can handle arbitrary length inputs.

4.3 REAL-WORLD NEAR-DUPLICATE DETECTION EVALUATION

Setup We benchmark RETSim’s ability to identify near-duplicate content on real-world datasets
from the literature. The NEWS-COPY Deduplication dataset (Silcock et al., 2022) contains 27,210
historical news articles with 122,876 positive duplicate pairs. The dataset consists of noisy near-
duplicates due to factors like OCR errors, plagiarism, and news aggregation. We also evaluate the
algorithms on the CORE Near-Duplicates dataset (Gyawali et al., 2020), which consists of 100k
scholarly articles with 25k exact duplicates, 25k near-duplicates, and 50k non-duplicates. Near-
duplicates in this dataset arise from article revisions, versioning and metadata differences, and hu-
man typos. A key difference between these two benchmarks and the W4NT3D benchmark is that
these two benchmarks are focused on detecting and clustering near-duplicate text, rather than robust
text retrieval based on syntactic similarity. For both benchmarks, we follow the experimental setup
provided in the papers and report Adjusted Rand Index (ARI) for the NEWS-COPY dataset and
report precision/recall/F1 scores on the CORE Near-Duplicates dataset.

Results On the NEWS-COPY dataset, RETSimPartial-Dup outperforms all other approaches by a
significant margin (4.8% ARI compared to our best MinHash result), as reported in Table 3. In the
dataset, there are many near-duplicate pairs where one text is significantly longer than the other, so it
is expected that RETSimPartial-Dup, which can find matching text chunks in documents, is more suited
for the task and outperforms RETSimNear-Dup. Bucketing the near-duplicate detection rate of each
algorithm by the length ratio between positive pairs (Figure 5), we observe that RETSimPartial-Dup
outperforms MinHash regardless of the length ratio, but MinHash surpasses RETSimNear-Dup per-
formance when one text is above roughly 1.5x the length of the other text in a near-duplicate pair.

6

1.0

0.8

0.6

0.4

l
l

1
@
a
c
e
R

16

32

64

128

256

512

1024

2048

4096

8192

Max Target Text Length

l
l

1
@
a
c
e
R

1.0

0.8

0.6

0.4

0%

10%

20%

30%

40%

50%

Text Augmentation Amount (%)

LaBSE

Multilingual USE

Multilingual E5-Base

PaLM 2 (Gecko)

SimHash

MinHash

RETSim (Partial-Dup)

RETSim (Near-Dup)

Preprint

Model/Algorithm ARI

Multilingual USE
Multilingual E5-Base
S-BERT*

SimHash
MinHash*
MinHash (Ours)

RETSimPartial-Dup
RETSimNear-Dup

0.730
0.742
0.700

0.695
0.737
0.783

0.831
0.704

Table 3: Performance comparison on the
NEWS-COPY dataset. Adjusted Rand Index
(ARI) values are reported. * denotes results
from Silcock et al. (2022).

Figure 5: Near-duplicate detection rate of RET-
Sim vs MinHash for different length ratios of pos-
itive pairs. X-axis is the length of longer divided
by shorter text, rounded to the nearest integer.

Additionally, we noticed that the labels in the dataset were occasionally noisy, as a substantial por-
tion of the RETSim false positives appear to be near-duplicates upon inspection (Appendix A.6).

On the CORE Near-Duplicates dataset (Table 4), where documents (article title + abstract) are
roughly the same size, RETSimPartial-Dup and RETSimNear-Dup performance is roughly equivalent.
Both methods outperform the baselines in terms of macro F1 score and accuracy. We use MinHash
+ LSH with 256 hash functions for computational efficiency, as recommended by the datasketch
library1 for better accuracy than the default setting. Deduplication thresholds and detailed hyperpa-
rameter settings for the algorithms on both near-duplication datasets can be found in Appendix A.3.

Model / Algorithm

Exact Title Matching*

LaBSE
Multilingual USE
Multilingual E5-Base

MinHash + LSH

RETSimPartial-Dup
RETSimNear-Dup

Precision
Duplicates

Recall
Duplicates

Precision
Non-Duplicates

Recall
Non-Duplicates

Macro F1 Accuracy

0.830

0.937
0.917
0.931

0.929

0.945
0.928

0.500

0.923
0.907
0.908

0.902

0.941
0.937

0.709

0.930
0.918
0.919

0.915

0.945
0.942

0.992

0.943
0.927
0.939

0.938

0.949
0.934

0.757

0.933
0.917
0.924

0.921

0.945
0.935

0.746

0.919
0.909
0.920

0.918

0.928
0.926

Table 4: Evaluation results on the CORE Near-Duplicates dataset. Precision/recall/macro F1 and
accuracy numbers are reported. * denotes results from Gyawali et al. (2020).

5 APPLICATIONS

5.1 TRAINING DATASET DEDUPLICATION

Model/Algorithm

% train examples
with dup in train

% valid examples
with dup in train

MinHash + LSH
Exact Substring*
RETSimNear-Dup
RETSimPartial-Dup

0.47%
2.76%
3.17%
12.77%

0.46%
0.52%
0.59%
2.66%

Table 5: Deduplication rate on Wiki-40B (English). * denotes results from Lee et al. (2022).

Setup We evaluate RETSim’s ability to deduplicate text training datasets by deduplicating the
English split of Wiki-40B (Guo et al., 2020). We conservatively set the cosine similarity deduplica-
tion threshold to 0.1 for RETSimNear-Dup and 0.15 for RETSimPartial-Dup to limit the amount of false
positives, based on the optimal thresholds found in the evaluation (Appendix A.3). We use USe-
arch’s default vector index for approximate nearest neighbor search (Vardanian, 2023). We compare

1datasketch: Big Data Looks Small. https://github.com/ekzhu/datasketch.

7

t

e
a
R
n
o

t

i
t
c
e
e
D
e
a
c

t

i
l

p
u
D

-
r
a
e
N

1.0

0.8

0.5

0.3

0.0

1

2

3

4

5

6

7

8

9

10

Length Ratio between Near-Duplicate Text Pair

RETSim (Partial-Dup)

RETSim (Near-Dup)

MinHash

 
 
Preprint

Model/Algorithm

Accelerator

Batch Size

Embedding / Hashing
time (sec)

examples/sec

MinHash + LSH
RETSim
RETSim
RETSim

CPU AMD 7950 32 cores
Onnx CPU AMD 7950 32 cores
TensorFlow GPU RTX 4090
TensorFlow GPU NVIDIA H100

-
256
4096
16384

234
10839
720
363

12544
270
4062
8069

Table 6: Embedding/hashing speed of RETSim vs MinHash + LSH on the Wiki-40B dataset.

against MinHash + LSH, where we set the number of hash functions to be 256 following Kocetkov
et al. (2022) and use a Jaccard similarity threshold of 0.8 for deduplication (Lee et al., 2022).

Results Overall, as reported in Table 5, RETSimNear-Dup finds slightly more duplicates in the Wiki-
40B training and validation splits. This is in-line with our deduplication results (Section 4.3) where
RETSimNear-Dup outperforms other algorithms. On the other hand, RETSimPartial-Dup finds signifi-
cantly more matches than the exact substring matching algorithm used in the previous study (Lee
et al., 2022), showcasing the usefulness of performing both near-duplicate and partial-duplicate
matching at once. This larger-than-expected number of partial matches indicate that machine learn-
ing practitioners should take extra care to deduplicate Wikipedia at the chunk level to avoid feeding
duplicate text to their models.

In terms of embedding speed (Table 6), RETSim is significantly slower than MinHash + LSH on
CPU (46x slower), competitive when using a desktop GPU such as the RTX 4090 (3x slower) and
almost on-par when using a high-end GPU like the NVIDIA H100 (1.5x slower). Our current code
is written in Python and not fully optimized, so we expect this performance gap to significantly
shrink as we optimize our implementation. Although RETSim is slower than MinHash, RETSim
is significantly smaller and faster than other text embedding models, and closes the performance
gap between neural and non-neural based methods for near-duplicate text detection and dataset
deduplication. Both RETSimNear-Dup and RETSimPartial-Dup are returned at the same time so they
have the same embedding speed. Indexing and retrieval times will depend on the vector index and
search algorithm used. For longer documents, RETSimPartial-Dup will produce more embeddings
than RETSimNear-Dup, so RETSimPartial-Dup offers a tradeoff between finer-grained matching versus
indexing/retrieval speed, which will depend on the specific vector search algorithm and dataset used.

5.2

IN THE WILD: SPAM EMAIL CLUSTERING

In this section, we showcase RETSim’s real-world performance on clustering near-duplicate text
which has been heavily manipulated by adversarial attacks by performing an evaluation on spam
campaigns. Spam constitutes a strong proving ground for near-duplicate clustering algorithms as
spammers employ adversarial augmentation techniques in an attempt to evade detection. Such aug-
mentations typically include appending or prepending unrelated text, interleaving random words and
different languages, intentionally introducing typos, abusing extended character sets such as emojis
and homoglyphs, and more. These techniques are collectively referred to as hash-busting.

Setup The dataset consists of 5,252 spam emails from 196 spam campaigns, donated by Gmail
users who flagged them when they reached their inboxes. Each example contains the email subject
concatenated with the message content. The emails were misclassified by a spam classifier due to
their effective adversarial text manipulation techniques, which makes them a challenging test set for
clustering evaluations. Some examples of hash-busting attacks and adversarial manipulations we
observe include the use of homoglpyphs, uncommon Unicode character sets, invisible characters,
and padding with random words from different languages. To get the ground truth campaign clusters,
emails were manually reviewed and assigned to a specific spam campaign based on similarity by
human reviewers. We use agglomerative clustering to cluster spam emails, and report homogeneity,
completeness, V-Measure, and Adjusted Rand Index (ARI) metrics.

Results Overall, we observed that RETSim is significantly better at clustering near-duplicates
with adversarial manipulations, outperforming both SimHash and USE across all metrics considered
(Table 7). In particular, we observed that RETSim outperforms USE by 4.6% on the V-Measure
score which is our main metric. The results reported in this section are in-line with what we observe
since we deployed RETSim as our main near-duplicate detection algorithm in December 2022.

8

Preprint

Model / Algorithm Homogeneity Completeness V-Measure ARI

USE
SimHash + LSH
RETSimNear-Dup

0.856
0.867
0.937

0.955
0.876
0.963

0.903
0.871
0.949

0.6
0.571
0.747

Table 7: Performance on clustering adversarial spam campaigns in practice.

6 ABLATION STUDIES

Setup In this section, we summarize the key ablation studies we performed when designing RET-
Sim. All the models used in this section are trained using the setup detailed in Appendix A.1.2, ex-
cept we only train them for 100k steps to reduce computational costs. We evaluate RETSimNear-Dup’s
performance for each model on a subset of the W4NT3D benchmark, where we randomly select
1000 examples from each of the 41 language splits and use Recall@1 as reported metric.

Block Type Recall@1

Chunk Size Recall@1

Embed. Dim Recall@1

RETVec MLP
ConvNeXt
BERT
T5
*GAU

0.975
0.978
0.973
0.980
0.986

128
256
*512
1024
2048

0.979
0.984
0.986
0.983
0.978

64
128
*256
512
768

0.969
0.980
0.986
0.986
0.986

Table 8: RETSim ablation study results on architecture block type (left), text chunk size (middle),
and embedding dimension (right). *Bold denotes the value selected for the final RETSim model.

Results Table 8 contains RETSim ablation study results on max text chunk size, architecture block
type, and embedding size. The most important architectural decision was to decide the optimal text
chunk size and finding the right balance between having the smallest size possible to maximize
RETSimPartial-Dup efficiency while ensuring RETSimNear-Dup full-text embeddings can work effec-
tively on full documents. We find that chunks of 512 characters offer the best performance.

We also tested various model architectures and transformer blocks to find the best balance between
efficiency and performance. We find that the more modern GAU block (Hua et al., 2022) outper-
forms the vanilla BERT transformer block (Devlin et al., 2019) and the T5 block (Xue et al., 2020).
We also tried modern CNN architectures such as ConvNeXt (Liu et al., 2022) and the MLP architec-
ture proposed in RETVec (Bursztein et al., 2023), but both were worse than GAU block performance.
Last but not least, we found that increasing the embedding size past 256 dimensions does not yield
any meaningful improvements for RETSimNear-Dup. Accordingly, we opted to use a 256-dimension
embedding for space-efficiency and to maximize indexing and query speed. Additional ablation
studies for other hyperparameters can be found in Appendix A.5.

7 FUTURE WORK

RETSim’s novel training regime, which combines metric learning and data augmentation, has many
other potential applications that we plan to explore in future work. For example, it could be adapted
or extended to train robust semantic embeddings or image similarity embeddings. Additionally,
we expect that as general models become bigger and more expensive to run in the future, smaller,
specialized models such as RETSim will emerge as an efficient alternative for a wide range of tasks.

8 CONCLUSION

In this paper, we introduced RETSim, a novel, multilingual text embedding which achieves state-
of-the-art performance on near-duplicate text detection, dataset deduplication, and syntactic text
similarity benchmarks. RETSim is significantly faster than previous neural-based text embeddings
and more robust than n-gram based algorithms, which makes it suitable for large-scale text retrieval
and dataset deduplication, especially in adversarial settings such as spam detection. Furthermore,
we introduced the W4NT3D benchmark, the first multilingual dataset designed to measure the ad-
versarial robustness of near-duplicate text detection algorithms. We open-source both RETSim and
the W4NT3D benchmark under the MIT License.

9

Preprint

REFERENCES

Naeem Ahmed, Rashid Amin, Hamza Aldabbas, Deepika Koundal, Bader Alouffi, and Tariq Shah.
Machine Learning Techniques for Spam Detection in Email and IoT Platforms: Analysis and
Research Challenges. Security and Communication Networks, 2022:1–19, February 2022. ISSN
1939-0122, 1939-0114. doi: 10.1155/2022/1862888. URL https://www.hindawi.com/
journals/scn/2022/1862888/.

Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei
Chang. Generating Natural Language Adversarial Examples, September 2018. URL http:
//arxiv.org/abs/1804.07998. arXiv:1804.07998 [cs].

Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark,
Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark
Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,
Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Brad-
bury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christo-
pher A. Choquette-Choo, Aakanksha Chowdhery, Cl´ement Crepy, Shachi Dave, Mostafa De-
hghani, Sunipa Dev, Jacob Devlin, Mark D´ıaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu
Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy
Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy
Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy,
Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li,
Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Mar-
cello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary
Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex
Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros,
Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov,
David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli,
Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yun-
han Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang
Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. PaLM 2 Technical Report, September 2023.
URL http://arxiv.org/abs/2305.10403. arXiv:2305.10403 [cs].

Andrei Z. Broder, Moses Charikar, Alan M. Frieze, and Michael Mitzenmacher. Min-wise inde-
In Proceedings of the thirtieth annual ACM symposium on Theory of

pendent permutations.
computing, pp. 327–336, 1998.

Eli Bursztein, Marina Zhang, Owen vallis, Xinyu Jia, and Alexey Kurakin. RetVec: Resilient and

Efficient Text Vectorizer. 2023.

Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Con-
stant, Mario Guajardo-Cespedes, Steve Yuan, and Chris Tar. Universal sentence encoder. arXiv
preprint arXiv:1803.11175, 2018.

Moses S. Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of

the thiry-fourth annual ACM symposium on Theory of computing, pp. 380–388, 2002.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep
Bidirectional Transformers for Language Understanding, May 2019. URL http://arxiv.
org/abs/1810.04805. arXiv:1810.04805 [cs].

Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. Language-
agnostic BERT Sentence Embedding, March 2022. URL http://arxiv.org/abs/2007.
01852. arXiv:2007.01852 [cs].

Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. Black-box Generation of Adversarial Text
Sequences to Evade Deep Learning Classifiers, May 2018. URL http://arxiv.org/abs/
1801.04354. arXiv:1801.04354 [cs].

10

Preprint

Mandy Guo, Zihang Dai, Denny Vrandeˇci´c, and Rami Al-Rfou. Wiki-40b: Multilingual language
model dataset. In Proceedings of the 12th Language Resources and Evaluation Conference, pp.
2440–2452, 2020.

Bikash Gyawali, Lucas Anastasiou, and Petr Knoth. Deduplication of Scholarly Documents us-
ing Locality Sensitive Hashing and Word Embeddings. In Proceedings of the Twelfth Language
Resources and Evaluation Conference, pp. 901–910, Marseille, France, May 2020. European Lan-
guage Resources Association. ISBN 979-10-95546-34-4. URL https://aclanthology.
org/2020.lrec-1.113.

Matthias Hagen, Martin Potthast, Marcel Gohsen, Anja Rathgeber, and Benno Stein. A large-
In Proceedings of the 40th International ACM SIGIR

scale query spelling correction corpus.
Conference on Research and Development in Information Retrieval, pp. 1261–1264, 2017.

Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time.

In

International Conference on Machine Learning, pp. 9099–9117. PMLR, 2022.

B. Issac, R. Chiong, and S. M. Jacob. Analysis of phishing attacks and countermeasures, 2014.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daum´e Iii. Deep Unordered
Composition Rivals Syntactic Methods for Text Classification.
In Proceedings of the 53rd
Annual Meeting of the Association for Computational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1681–1691, Bei-
jing, China, 2015. Association for Computational Linguistics. doi: 10.3115/v1/P15-1162. URL
http://aclweb.org/anthology/P15-1162.

Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating Training Data Mitigates Pri-
vacy Risks in Language Models.
In Proceedings of the 39th International Conference on
Machine Learning, pp. 10697–10707. PMLR, June 2022. URL https://proceedings.
mlr.press/v162/kandpal22a.html. ISSN: 2640-3498.

Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu˜noz Ferrandis,
Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von
Werra, and Harm de Vries. The Stack: 3 TB of permissively licensed source code, November
2022. URL http://arxiv.org/abs/2211.15533. arXiv:2211.15533 [cs].

Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-
Burch, and Nicholas Carlini. Deduplicating Training Data Makes Language Models Better, March
2022. URL http://arxiv.org/abs/2107.06499. arXiv:2107.06499 [cs].

Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.
A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 11976–11986, 2022.

John X. Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. TextAttack: A
Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP, Oc-
tober 2020. URL http://arxiv.org/abs/2005.05909. arXiv:2005.05909 [cs].

Niklas Muennighoff, Nouamane Tazi, Lo¨ıc Magne, and Nils Reimers. MTEB: Massive Text Em-
bedding Benchmark, October 2022. URL https://arxiv.org/abs/2210.07316v1.

Filip Radenovi´c, Giorgos Tolias, and Ondˇrej Chum. Fine-tuning CNN image retrieval with no
human annotation. IEEE transactions on pattern analysis and machine intelligence, 41(7):1655–
1668, 2018. Publisher: IEEE.

Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-

networks. arXiv preprint arXiv:1908.10084, 2019.

Florian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: A Unified Embedding for
Face Recognition and Clustering.
In 2015 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 815–823, June 2015. doi: 10.1109/CVPR.2015.7298682. URL http:
//arxiv.org/abs/1503.03832. arXiv:1503.03832 [cs].

11

Preprint

Emily Silcock, Luca D’Amico-Wong, Jinglin Yang, and Melissa Dell. Noise-Robust De-Duplication
at Scale, October 2022. URL http://arxiv.org/abs/2210.04261. arXiv:2210.04261
[cs].

Yifang Sun, Jianbin Qin, and Wei Wang. Near Duplicate Text Detection Using Frequency-Biased
Signatures. In David Hutchison, Takeo Kanade, Josef Kittler, Jon M. Kleinberg, Friedemann Mat-
tern, John C. Mitchell, Moni Naor, Oscar Nierstrasz, C. Pandu Rangan, Bernhard Steffen, Madhu
Sudan, Demetri Terzopoulos, Doug Tygar, Moshe Y. Vardi, Gerhard Weikum, Xuemin Lin, Yan-
nis Manolopoulos, Divesh Srivastava, and Guangyan Huang (eds.), Web Information Systems
Engineering – WISE 2013, volume 8180, pp. 277–291. Springer Berlin Heidelberg, Berlin, Hei-
delberg, 2013. ISBN 978-3-642-41229-5 978-3-642-41230-1. doi: 10.1007/978-3-642-41230-1
24. URL http://link.springer.com/10.1007/978-3-642-41230-1_24. Series
Title: Lecture Notes in Computer Science.

Ash Vardanian. USearch by Unum Cloud, October 2023. URL https://github.com/

unum-cloud/usearch.

Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Ma-
jumder, and Furu Wei. Text Embeddings by Weakly-Supervised Contrastive Pre-training, De-
cember 2022. URL http://arxiv.org/abs/2212.03533. arXiv:2212.03533 [cs].

Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, and Matthew R. Scott. Multi-similarity loss
with general pair weighting for deep metric learning. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, pp. 5022–5030, 2019.

Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya
Barua, and Colin Raffel. mT5: A massively multilingual pre-trained text-to-text transformer.
arXiv preprint arXiv:2010.11934, 2020.

Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, Gustavo Hernandez
Abrego, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. Multilingual
Universal Sentence Encoder for Semantic Retrieval, July 2019. URL http://arxiv.org/
abs/1907.04307. arXiv:1907.04307 [cs].

Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan
Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep
learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962, 2019.

12

Preprint

A APPENDIX

A.1 RETSIM DETAILS

A.1.1 RETSIM MODEL HYPERPARAMETERS

The full list of RETSim model hyperparameters can be found in Table 9.

Hyperparameter

Max input length (per chunk)
Block type
# blocks
Hidden dim
Expansion rate
Activation function
Attention activation function
Absolute positional encoding
Relative positional encoding
Norm type
Pooling type
Dropout rate
Embedding dim
# Parameters

Value

512
GAU
2
256
1
Swish
relu2
ScaledSin
RoPE
ScaleNorm
GeM (p = 3)
0
256
536k

Table 9: Detailed RETSim model hyperparameters.

A.1.2 RETSIM TRAINING HYPERPARAMETERS

Table 10 details the hyperparameters settings for training configuration, loss, and optimizer used to
train the RETSim model.

Hyperparameter

Value

Batch size
Train steps
LAMB ϵ
LAMB β1
LAMB β2
Max learning rate
End learning rate
Learning rate decay
Weight decay

1024
1 million
1e-6
0.9
0.999
0.001
0
Cosine
0

Table 10: RETSim detailed training hyperparameters.

A.2 TRAINING DATASET DETAILS

Below, we provide the full list of augmentations used to generate augmented text for the RETSim
training dataset, as described in Section 3.2.

SENTENCE-LEVEL AUGMENTATIONS

• Deletion:

– Random sentence deletion
– Random sentence truncation

• Insertion:

13

Preprint

– Random prefix sentence
– Random suffix sentence
– Random sentence insertion
– Repeat sentence

• Substitution:

– Lowercase/uppercase sentence
– Random sentence substitution

• Transposition:

– Neighboring Swap

WORD-LEVEL AUGMENTATIONS

• Deletion:

– Random word deletion

• Insertion:

– Random word insertion
– Random word insertion per language

• Substitution:

– 3-gram frequency based word substitution
– Random word substitution
– Random word substitution per language
– Repeat word

• Transposition:

– Neighboring Swap

CHARACTER-LEVEL AUGMENTATIONS

• Deletion:

– Random character deletion

• Substitution:

– Case substitution
– n-gram based substitution for n = 3, 4, 5
– QWERTY keyboard typo substitution
– Homoglyphs substitution
– Random ASCII substitution
– Random character from language alphabet substitution
– Random punctuation substitution
– Random Unicode character substitution

• Insertion:

– Character repetition
– n-grams based insertion for n = 3, 4, 5
– Random character from language alphabet insertion
– Random punctuation insertion
– Random Unicode character insertion

• Transposition:

– Neighboring swap

A.3 DETAILED EVALUATION HYPERPARAMETERS

Figures 6 and 7 contain information on deduplication thresholds values and hyperparameter settings
for each algorithm benchmarked on the NEWS-COPY and CORE deduplication datasets.

14

Preprint

Model / Algorithm

Threshold Type

Threshold Value Hyperparameters

Multilingual USE
Cosine Similarity
Multilingual E5-Base Cosine Similarity
SimHash
MinHash (Ours)
RETSimNear-Dup
RETSimPartial-Dup

Hamming Distance
Jaccard Similarity
Cosine Similarity
Cosine Similarity

0.96
0.88
10
0.6
0.89
0.84

-
-
64 bits, 5-grams (character-level)
10 hash functions, 2-grams (word-level)
-
-

Figure 6: Hyperparameter settings for NEWS-COPY dataset evaluation in Section 4.3.

Model / Algorithm

Threshold Type

Threshold Value Hyperparameters

Cosine Similarity
LaBSE
Multilingual USE
Cosine Similarity
Multilingual E5-Base Cosine Similarity
SimHash + LSH
MinHash + LSH
RETSimNear-Dup
RETSimPartial-Dup

Hamming Distance
Jaccard Similarity
Cosine Similarity
Cosine Similarity

0.88
0.97
0.87
6
0.5
0.86
0.82

-
-
-
64 bits, 3-grams (character-level)
256 hash functions, 3-grams (word-level)
-
-

Figure 7: Hyperparameter settings for CORE Near-Duplicates dataset evaluation in Section 4.3.

A.3.1 DEDUPLICATION THRESHOLD IMPACT

Figure 8: Precision/Recall/F1 scores for different cosine distance deduplication thresholds for
RETSimNear-Dup (left) and RETSimPartial-Dup (right) on the NEWS-COPY dataset.

A.4 DETAILED W4NT3D BENCHMARK RESULTS

Tables 11 and 12 show detailed performance results for RETSim and all baseline algorithms for
every language split in the W4NT3D benchmark.

15

Preprint

9
4
9
.
0

4
8
9
.
0

6
8
9
.
0

1
0
7
.
0

3
9
5
.
0

4
1
8
.
0

1
7
9
.
0

1
9
9
.
0

o
k

1
3
9
.
0

0
9
9
.
0

9
7
9
.
0

8
7
5
.
0

5
6
4
.
0

3
2
2
.
0

3
6
9
.
0

6
8
9
.
0

a
j

9
2
9
.
0

7
3
9
.
0

3
3
9
.
0

7
3
9
.
0

3
1
5
.
0

5
7
5
.
0

4
4
9
.
0

2
8
9
.
0

5
1
9
.
0

8
2
9
.
0

2
4
9
.
0

4
2
9
.
0

3
3
5
.
0

3
6
5
.
0

0
5
9
.
0

6
7
9
.
0

8
9
8
.
0

5
8
8
.
0

9
8
8
.
0

6
7
8
.
0

0
3
5
.
0

2
1
5
.
0

0
5
9
.
0

2
6
9
.
0

2
1
9
.
0

9
8
8
.
0

1
0
9
.
0

3
0
9
.
0

7
5
5
.
0

6
5
5
.
0

4
3
9
.
0

2
6
9
.
0

7
2
9
.
0

8
4
6
.
0

4
6
9
.
0

5
3
4
.
0

3
6
5
.
0

6
0
6
.
0

1
4
9
.
0

5
7
9
.
0

7
3
9
.
0

1
4
8
.
0

8
5
9
.
0

9
8
5
.
0

1
5
6
.
0

7
9
6
.
0

5
4
9
.
0

9
8
9
.
0

0
3
9
.
0

8
3
9
.
0

8
4
9
.
0

4
3
9
.
0

9
1
5
.
0

6
5
5
.
0

7
4
9
.
0

3
8
9
.
0

6
2
9
.
0

3
1
9
.
0

3
4
9
.
0

4
1
9
.
0

2
3
6
.
0

8
6
5
.
0

6
5
9
.
0

1
8
9
.
0

2
1
9
.
0

0
7
8
.
0

9
2
9
.
0

6
5
3
.
0

8
6
5
.
0

5
9
5
.
0

6
4
9
.
0

1
7
9
.
0

3
2
9
.
0

0
3
9
.
0

1
5
9
.
0

6
2
9
.
0

5
6
6
.
0

5
8
5
.
0

3
5
9
.
0

5
8
9
.
0

8
8
8
.
0

3
0
9
.
0

6
9
8
.
0

1
1
9
.
0

6
9
4
.
0

4
0
5
.
0

8
3
9
.
0

6
7
9
.
0

4
4
9
.
0

8
5
9
.
0

9
5
9
.
0

1
6
9
.
0

1
9
5
.
0

1
9
5
.
0

4
5
9
.
0

7
8
9
.
0

8
1
9
.
0

4
7
8
.
0

4
3
9
.
0

9
8
5
.
0

7
5
5
.
0

4
7
5
.
0

5
3
9
.
0

4
7
9
.
0

1
3
9
.
0

2
4
9
.
0

4
4
9
.
0

2
3
9
.
0

1
6
5
.
0

8
5
5
.
0

9
4
9
.
0

8
7
9
.
0

8
3
9
.
0

7
2
9
.
0

6
5
9
.
0

3
4
9
.
0

2
9
5
.
0

8
9
5
.
0

3
5
9
.
0

8
8
9
.
0

2
1
9
.
0

0
0
9
.
0

7
2
9
.
0

1
1
9
.
0

9
7
5
.
0

1
8
5
.
0

0
5
9
.
0

3
7
9
.
0

7
9
8
.
0

3
8
8
.
0

0
9
8
.
0

2
0
9
.
0

5
8
4
.
0

8
0
5
.
0

4
2
9
.
0

8
6
9
.
0

5
1
9
.
0

9
0
9
.
0

9
9
8
.
0

1
5
8
.
0

0
1
5
.
0

6
0
5
.
0

2
4
9
.
0

6
7
9
.
0

5
1
9
.
0

5
1
9
.
0

6
3
9
.
0

7
9
4
.
0

8
5
5
.
0

3
3
6
.
0

8
2
9
.
0

1
7
9
.
0

e
s
a
B
-
5
E

l
a
u
g
n
i
l
i
t
l
u
M

E
S
U

l
a
u
g
n
i
l
i
t
l
u
M

)
o
k
c
e
G

(

2
M
L
a
P

E
S
B
a
L

p
u
D

-
l
a
i
t
r
a
P
m
S
T
E
R

i

p
u
D

i

-
r
a
e
N
m
S
T
E
R

h
s
a
H
m
S

i

h
s
a
H
n
i
M

t
i

d
i

u
h

r
h

i
h

e
h

r
f

fi

a
f

t
e

s
e

n
e

l
e

e
d

a
d

s
c

a
c

g
b

r
a

m
h
t
i
r
o
g
l
A

/

l
e
d
o
M

.
)
1

t
r
a
p
(
k
r
a
m
h
c
n
e
b
D
3
T
N
4
W

e
h
t

n
o

s

m
h
t
i
r
o
g
l
a

d
n
a

s
l
e
d
o
m
g
n
i
d
d
e
b
m
e

s
u
o
i
r
a
v

r
o
f

e
c
n
a
m
r
o
f
r
e
p

1
@

l
l
a
c
e
R
e
g
a
u
g
n
a
l
-
r
e
p
l
l
u
F

:
1
1
e
l
b
a
T

w

t
-
h
z

n
c
-
h
z

i
v

k
u

r
t

l
t

h
t

v
s

r
s

l
s

k
s

u
r

o
r

t
p

l
p

o
n

l
n

s

m

v
l

t
l

m
h
t
i
r
o
g
l
A

/

l
e
d
o
M

16

8
1
9
.
0

5
8
9
.
0

8
7
9
.
0

9
0
6
.
0

5
1
3
.
0

0
0
2
.
0

7
5
9
.
0

8
6
9
.
0

7
1
9
.
0

6
8
9
.
0

0
8
9
.
0

3
2
6
.
0

6
7
2
.
0

2
7
1
.
0

6
4
9
.
0

1
7
9
.
0

2
3
9
.
0

0
1
9
.
0

5
4
9
.
0

3
6
8
.
0

9
0
6
.
0

1
8
5
.
0

1
5
9
.
0

5
8
9
.
0

9
9
8
.
0

3
9
8
.
0

2
7
8
.
0

2
2
8
.
0

7
1
5
.
0

0
2
5
.
0

1
4
9
.
0

7
5
9
.
0

0
3
9
.
0

0
4
9
.
0

1
5
9
.
0

2
0
9
.
0

6
0
6
.
0

3
8
5
.
0

4
5
9
.
0

8
7
9
.
0

7
4
9
.
0

9
4
9
.
0

9
6
9
.
0

4
4
9
.
0

7
0
5
.
0

0
7
5
.
0

1
6
9
.
0

9
8
9
.
0

2
8
8
.
0

8
8
8
.
0

1
2
9
.
0

1
7
5
.
0

9
6
6
.
0

6
1
4
.
0

1
4
9
.
0

6
4
9
.
0

6
0
9
.
0

9
9
8
.
0

5
2
9
.
0

9
1
9
.
0

2
5
5
.
0

0
2
5
.
0

3
5
9
.
0

9
6
9
.
0

0
3
9
.
0

6
0
9
.
0

6
9
8
.
0

6
5
8
.
0

3
5
5
.
0

5
2
5
.
0

3
6
9
.
0

9
7
9
.
0

1
3
9
.
0

8
0
9
.
0

0
4
9
.
0

6
1
9
.
0

0
8
5
.
0

0
7
5
.
0

7
4
9
.
0

7
7
9
.
0

2
2
9
.
0

1
0
9
.
0

1
2
9
.
0

4
2
9
.
0

5
7
5
.
0

3
7
5
.
0

1
6
9
.
0

1
7
9
.
0

8
1
9
.
0

0
1
9
.
0

1
1
9
.
0

1
5
8
.
0

4
5
5
.
0

3
2
5
.
0

6
4
9
.
0

0
7
9
.
0

9
0
9
.
0

9
6
8
.
0

6
3
9
.
0

3
9
8
.
0

4
1
5
.
0

0
2
5
.
0

8
4
9
.
0

7
7
9
.
0

4
4
9
.
0

2
5
9
.
0

5
5
9
.
0

0
5
9
.
0

8
4
5
.
0

3
7
5
.
0

4
5
9
.
0

5
8
9
.
0

8
2
9
.
0

1
3
9
.
0

8
2
9
.
0

3
1
9
.
0

6
8
5
.
0

3
6
5
.
0

3
5
9
.
0

9
7
9
.
0

8
2
9
.
0

1
2
9
.
0

4
4
9
.
0

0
3
9
.
0

7
7
5
.
0

0
6
5
.
0

8
5
9
.
0

1
8
9
.
0

1
3
9
.
0

6
3
9
.
0

4
3
9
.
0

1
3
9
.
0

7
2
5
.
0

0
4
5
.
0

0
5
9
.
0

9
7
9
.
0

9
1
9
.
0

2
3
9
.
0

9
4
9
.
0

8
2
9
.
0

3
3
5
.
0

1
1
5
.
0

1
6
9
.
0

0
8
9
.
0

2
2
9
.
0

9
1
9
.
0

5
3
9
.
0

7
0
9
.
0

4
2
6
.
0

9
7
5
.
0

5
4
9
.
0

3
8
9
.
0

9
1
9
.
0

2
0
9
.
0

1
4
9
.
0

9
0
9
.
0

9
0
6
.
0

8
6
5
.
0

7
5
9
.
0

0
8
9
.
0

e
s
a
B
-
5
E

l
a
u
g
n
i
l
i
t
l
u
M

E
S
U

l
a
u
g
n
i
l
i
t
l
u
M

)
o
k
c
e
G

(

2
M
L
a
P

E
S
B
a
L

p
u
D

-
l
a
i
t
r
a
P
m
S
T
E
R

i

h
s
a
H
m
S

i

h
s
a
H
n
i
M

p
u
D

i

-
r
a
e
N
m
S
T
E
R

.
)
2

t
r
a
p
(
k
r
a
m
h
c
n
e
b
D
3
T
N
4
W

e
h
t

n
o

s

m
h
t
i
r
o
g
l
a

d
n
a

s
l
e
d
o
m
g
n
i
d
d
e
b
m
e

s
u
o
i
r
a
v

r
o
f

e
c
n
a
m
r
o
f
r
e
p

1
@

l
l
a
c
e
R
e
g
a
u
g
n
a
l
-
r
e
p
l
l
u
F

:
2
1
e
l
b
a
T

Preprint

A.5 ADDITIONAL ABLATION STUDIES

This section includes ablation studies on additional hyperparameters for the RETSim model, includ-
ing the loss function, pooling type, and model capacity.

α β

2
2
2
2
4
4
4
4

20
20
40
40
20
20
40
40

λ

0.5
1
0.5
1
0.5
1
0.5
1

Recall@1

0.982
0.948
0.984
0.919
0.982
0.947
0.986
0.923

Table 13: Ablation study on Multi-Similarity Loss hyperparameters for RETSim training. Bold
indicates the hyperparameter setting selected for the final model.

# Blocks Hidden Dim Recall@1

2
2
2
2
3
3
3
3
4
4
4
4

64
128
256
512
64
128
256
512
64
128
256
512

0.965
0.980
0.986
0.986
0.962
0.980
0.984
0.987
0.966
0.980
0.985
0.986

Table 14: Ablation study for RETSim model capacity and size (number of GAU blocks and hidden
dimension for the blocks). Bold indicates the hyperparameter setting selected for the final model.

Pooling Type

Recall@1

Average Pooling
Max Pooling
Generalized Mean Pooling

0.985
0.983
0.986

Table 15: Ablation study on pooling type for the RETSim model. Bold indicates the hyperparameter
setting selected for the final model.

A.6 SELECTED EXAMPLES FROM NEWS-COPY DATASET

In this section, we randomly selected a set of false positives and false negatives for RETSim on the
NEWS-COPY deduplication dataset to provide further insight into the results.

17

Preprint

Text 1
chauffeur, a policeman and a passing jour-
nalist who tried to intervene. Beaton and the
policeman were reported in serious condition.
The 23-year-old princess and her husband of
five months, Capt. Mark Phillips, were not
hurt. But police experts said the holes left by
one of the bullets fired into the car indicated it
passed between them, missing them by inch-
es. A police informant said it was believed 11
shots were fired by the assailant. Experts were
studying two revolvers found at the scene.
They said fi...
By United Press tnfernational Ay SSAST OR
BE FRE NG SG The federal government has
proposed new methods of eoustructing federal
buildings in a move to save ad- ditional en-
ergy and suggested ils elfort could be adapted
to all new buildings,

Washington, Jan. 27. —(P)—Im- mediate
removal of John F. J. Her- bert, as prohibition
administrator for Montana and Idaho, was
de- manded in the senate today by Sen- ators
Borah, Idaho, and Whe´eeler, Montana, on
the ground of charges placed before them by
department of justice investigators. Wheeler
accompanied his demand (Continued on Page
2)

By RAYMOND CLAPPEA (Dnited
Presa Stal Correspandoayy London, Jai,
38—(UP—-The Am ‘erlcnn delegation to
the navat confer ence today won ls demand
for pre- sentation: of the cnse of suxiliary
warships limitation first at tho noxt plenary
session Thuvaday, ‘Tho chlet delegates, mec-
tittg at St. James palace, also decided that tho
plenary sesslon would discuss the Main con-
ference questions in alpha betical order of ihe
countriea pro- posing. Press ta be Admitted
The American delegation woo a second vic-
tory whe...

Text 2
‘LONDON (AP) — Ian Ball, a 26-year- old
unemployed Englishman, was brought into
court today and charged with attempted mur-
der during an at- tempt to kidnap Princess
Anne from her car in the heart of London
Wed- nesday night. Ball, lean-faced and
bearded, stood stiffly in the dock at the Bow
Street Magistrate’s court, handcuffed to two
detectives. He spoke only once during his 60-
second appearance, saying iha London accent:
“I want to apply for legal aid.” The court or-
dered him held for another hearing on Ma...
Hy United Press International The federal
government has Proposed new methods of
constructing federal buildings in a move lo
save addilional energy and suggested ils effort
could be adapted to all new buildings, Arthur
F, Sampson, General Services Administration
ad- ministrater, said new features for such
construction would include the collection of
rain waler for cooling and irriga- tion, solar
energy collectors and the covering of exterior
walls with earth. “Whal we are saying is that
these design criteri...
— Washington, Jan. 27 1 AP).—Immiedl-
aie mmoval of John F. Herbert as pro- — hi-
bition administrator for Montana and ‘Idaho
was demanded m the Seuate to- ‘day by Sen-
ators Borah. idaho, and Waeeler, Montana. on
the ground of charges placed before them by
Depart- meat of Justice investigators. Wheeler
accompanied his demand nith a declaration
that prohibition en- foreemen: had brukea
down. He blamed the “politicians” and called
upon the Law Enforcement Commussion to
sum- mon members of the Republican Na-
tona...
London, Jan. 24, W.P—The Amer- jean dele-
gation fo the naval cen- ference teday won its
demand for presentation of the case of auxil-
jary warships linsitation flrst at the next ple-
trary session ‘Vhursday, The chief delegates,
meeting at Si, James Pelace, also decided that
the plenary session would discuss the main
confeyence questions in alphabetical order af
the cauntries proposing. The American del-
egation won a second victory when it was
decided to udmil certain representatives of the
press at fie plenary...

Table 16: Example false negatives for RETSim on the NEWS-COPY dataset (pairs of texts not
detected as near-duplicates by RETSim but labeled as near-duplicates in the original dataset). Ex-
amples are randomly selected and truncated at 512 characters for display.

18

Preprint

Text 1
BOZEMAN, Mont. (AP) — Chet Huntley,
whose resonant voice and rough-hewn face
be- came familiar to millions on the nightly
television news, died Wednesday in his moun-
tain resort home. He was 62. He underwent
surgery for lung cancer in January but had
remained activesuntil recent weeks. He died
at 2:20 a.m, according to his widow, Tippy
Hunt.cy. Huntiey was teamed for 14 years
with David Brinkley on NBC’s Huntley-
Brinkley Re- port. He quit in 1970 and re-
turned to his native Montana to develop the
$20-millio...
By THE ASSOCIATED PRESS Some Amer-
icans are paying up to 50 per cent more per
month for electricity this year than they did
last, an Associ- ated Press survey shows. Con-
sumers are beginning to organize to fight the
rate hikes. A spot check of monthly elec- tric
bills this year and last showed that most in-
creases have been about $1 or $2, gen- erally
about 10 per cent, with the highest reported
boost com- ing in Jacksonville, Fia., where
the average tab went from $17.90 last year to
$27.70 this year. Utility...
BOZEMAN, Mont. (AP) — Vice President
Gerald R. Ford says the world will miss the
“‘unique abilities” of former television news
anchorman Chet Huntley. Huntley, 62, died
at his home Wednesday after a long bout
with lung cancer. Family ‘spokesmen said
a memorial service would be conducted for
Huntley Sunday at the Big Sky of Montana’
resort and recreation area south of Bozeman.
Huntley was chairman of the Big Sky board
of directors. Another memorial service is
scheduled Tuesday in the New York studios
of the...
WASHINGTON (AP) — The House has
passed legislation raising the minimum wage
from $1.60 an hour to $2 this year for most
workers covered and to $2.30 for all by 1978.
The bill, approved Wednesday 375 to 37, also
would increase by 7 million to 56.5 million
the number of workers covered by the mini-
mum wage laws. The bill is a modified ver-
sion of one President Nixon vetoed last year.
However, he is expected to sign this one if it
is finally approved after ad- justment with a
similar Senate passed measure, altho...

Text 2
BOZEMAN, Mont. (AP) - Chet Huntley,
whose resonant voice and rough-hewn face
became familiar to millions on the nightly
television news, died Wednesday in his moun-
tain resort home. He was 62. He underwent
surgery for lung cancer in January but had
remained active until recent weeks. He died
at 2:20 a.m., according to his widow, Tippy
Huntley. Huntley was teamed for 14 years
with David Brinkley on NBC’s Huntley-
Brinkley Report. He quit in 1970 and returned
to his native Montana to develop the $20 mil-
lion Bi...
By Louise Cook Acenciaiod Prece Writer
Same Americans are paying up io 20 per cent
more per month far electricity this year ihan
they did last, an -Associ- Press survey shows.
onsumers are beginning to ze to fight the rate
hikes, A spot check of monthly elec- tre hills
this year and Jast showed that most increases
ve been about $1 or $2, gen- erally about 10
per cent, with the highest reported boost com-
ing in Jacksonville, Fla., where the average
tab went from $17.90 last year to $27.70 this
year...
BOZEMAN, Mont. (AP) — Vice President
Gerald R. Ford says the world will miss the
“unique abilities” of former television news
anchorman Chet Huntley. Huntley, 62, died
at his home Wednesday after a long bout with
lung cancer. Family spokesmen said a me-
morial service would be con- ducted for Hunt-
ley Sunday at the Big- Sky of Montana resort
and recreation area south of Bozeman. Hunt-
ley was chair- man of the Big Sky board of
directors. Another memorial service is sched-
uled Tuesday in the New York studios of...

WASHINGTON (AP) — The House has
passed legislation raising the minimum wage
from $1.60 an hour to $2 this year for most
workers covered and to $2.30 for all by 1978.
The bill, approved Wednes- day 375 to 37,
also would in- crease by 7 million to 56.5 mil-
lion the number of workers cov- ered by the
minimum wage laws. The bill is a modified
version of one President Nixon vetoed last
year. However, he is ex- ted to sign this one if
it is inally approved after adjust- ment with a
similar Senate- passed measu...

Table 17: Example false positives for RETSim on the NEWS-COPY dataset (pairs of texts detected
as near-duplicates by RETSim but not labeled as near-duplicates in the original dataset). Examples
are randomly selected and truncated at 512 characters for display.

19

","3 2 0 2 v o N 8 2 ] L C . s c [ 1 v 4 6 2 7 1 . 1 1 3 2 : v i X r a Preprint RETSIM : RESILIENT AND EFFICIENT TEXT SIMILARITY Marina Zhang1 , Owen Vallis1 , Aysegul Bumin * 2 , Tanay Vakharia1 , Elie Bursztein1 Google1 University of Florida2 ABSTRACT This paper introduces RETSim ( Resilient and Efficient Text Similarity ) , a lightweight , multilingual deep learning model trained to produce robust metric embeddings for near-duplicate text retrieval , clustering , and dataset deduplication tasks . We demonstrate that RETSim is significantly more robust and accurate than MinHash and neural text embeddings , achieving new state-of-the-art perfor- mance on dataset deduplication , adversarial text retrieval benchmarks , and spam clustering tasks . We also introduce the W4NT3D benchmark ( Wiki-40B 4dver- sarial Near-T3xt Dataset ) for evaluating multilingual , near-duplicate text retrieval capabilities under adversarial settings . RETSim and the W4NT3D benchmark are open-sourced under the MIT License at https : //github.com/google/unisim . 1 INTRODUCTION Robust near-duplicate text detection is an essential component of many tasks , including retriev- ing documents , detecting plagiarism ( Sun et al. , 2013 ) and blocking adversarial spam cam- paigns ( Ahmed et al. , 2022 ) . Users have come to expect that systems can return accurate results despite their queries exhibiting a 20 % to 30 % typo rate ( Hagen et al. , 2017 ) . Furthermore , effi- ciently deduplicating text datasets is critical to training state-of-the-art large language models ( Lee et al. , 2022 ; Kandpal et al. , 2022 ) . For more than two decades , MinHash-based ( Broder et al. , 1998 ) locality-sensitive hashing ( LSH ) has been the most prevalent algorithm used for near-duplicate detection due to its simplicity , robust- ness , and speed . For example , the vast majority of dataset deduplication efforts still rely on MinHash ( Lee et al. , 2022 ; Kocetkov et al. , 2022 ) . However , like all LSH-based techniques , MinHash is not without downsides ; chief among them being that it is very parameter-sensitive and requires heavy tuning . Additionally , MinHash lacks resilience to typos due to its reliance on n-grams , leading to poor performance on noisy data and a vulnerability to hash-busting attacks ( Issac et al. , 2014 ) . On the other hand , deep learning models are the dominant way to perform vector-based semantic text retrieval ( Muennighoff et al. , 2022 ) , but so far , no neural embedding has been able to consistently outperform MinHash for robust near-duplicate detection ( Silcock et al. , 2022 ) . This is mostly due to the focus on improving semantic capabilities , which leads models to be too large to run extremely quickly and the use of sub-word level tokenization , which is not resilient to typos and adversarial attacks ( Morris et al. , 2020 ; Bursztein et al. , 2023 ) . To fill this gap , we introduce RETSim ( Resilient and Efficient Text Similarity ) , a lightweight , mul- tilingual deep learning model trained specifically to produce robust neural embeddings specialized for near-duplicate detection . By combining the state-of-the-art RETVec text vectorizer , a modern transformer block ( Hua et al. , 2022 ) , a large typo-augmented training corpus , and a metric learn- ing training regime , RETSim is able to achieve new state-of-the-art performance on near-duplicate detection benchmarks ( Section 4.2 ) , dataset deduplication tasks ( Sections 4.3 and 5.1 ) , and spam clustering applications ( Section 5.2 ) . Furthermore , while datasets and benchmarks exist for corpus deduplication and near-duplicate text retrieval , none of these have focused on systematically evaluating near-duplicate retrieval perfor- mance under the presence of typos , word manipulations , and sentence or paragraph-level modifica- * This work was done during the author ’ s internship at Google . 1 Preprint tions . To address this need , we additionally introduce the W4NT3D benchmark ( Wiki-40B 4dver- sarial Near-T3xt Dataset ) which enables the evaluation of algorithms on adversarial near-duplicate text retrieval in a multilingual setting . We report the performance of RETSim , MinHash , and pop- ular neural embeddings such as Universal Sentence Encoder ( Cer et al. , 2018 ) and LaBSE ( Feng et al. , 2022 ) on this new benchmark in Section 4.2 , highlighting uneven performance across lan- guages and types of adversarial manipulations . The RETSim model and the W4NT3D benchmark are open-sourced at https : //github.com/google/unisim under the MIT License . 2 RELATED WORK Near-Duplicate Detection Identifying noisy near-duplicate documents in a large corpus is a fun- damental task with a wide range of applications , such as detecting plagiarism , finding reproduced content in literature or news articles ( Gyawali et al. , 2020 ; Silcock et al. , 2022 ) , and deduplicat- ing training datasets for language models . Previous research has shown that duplicates in training datasets lead to inefficient training ( Lee et al. , 2022 ) and privacy concerns for large language models ( LLMs ) , where models memorize and regenerate duplicated training sequences at a much higher frequency ( Kandpal et al. , 2022 ) . Unlike semantic text similarity , the task of identifying textual near-duplicates has been predominated by non-neural , n-gram-based algorithms such as MinHash ( Broder et al. , 1998 ) , which is the most widely used technique for deduplicating large training corpuses ( Kocetkov et al. , 2022 ; Lee et al. , 2022 ) . MinHash is a technique for estimating the Jaccard similarity between two sets . Algorithms such as MinHash or SimHash ( Charikar , 2002 ) can be combined with locality-sensitive hashing ( LSH ) techniques for fast , approximate nearest neighbor search and data clustering . This allows them to scale and deduplicate corpuses containing terabytes of data such as C4 ( Lee et al. , 2022 ) and The Stack ( Kocetkov et al. , 2022 ) . However , n-gram or shingling-based techniques typically require texts to be parsed into a standardized form ( e.g . by lower-casing or stripping punctuation ) , which makes them susceptible to typos and adversarial attacks and pose a challenge when attempt- ing to differentiate between dissimilar documents and near-duplicate documents with adversarial augmentations . Semantic Text Similarity The task of computing semantic similarity between text is closely re- lated to near-duplicate detection . Semantic text similarity refers to the assessment of the semantic relatedness of two pieces of text based on their meaning rather than their syntactic structure , as in the case of near-duplicate detection . Recently , transformer-based language models such as Universal Sentence Encoder ( Yang et al. , 2019 ) , LaBSE ( Feng et al. , 2022 ) and LLM-based embeddings ( Anil et al. , 2023 ) which embed text into high-dimensional embedding vectors have been successfully used to retrieve semantically-related documents using cosine similarity . Modern text retrieval sys- tems combine these embeddings with an approximate nearest neighbor ( ANN ) search algorithm to efficiently retrieve documents matching user queries . However , language models have been shown to be vulnerable to adversarial attacks and naturally- occurring typos ( Alzantot et al. , 2018 ; Gao et al. , 2018 ; Morris et al. , 2020 ) . Furthermore , language models are typically very large and costly to run even with hardware acceleration , which makes them unsuited for large-scale dataset deduplication or identifying near-duplicates in the presence of typos or adversarial text manipulations . Metric Learning Metric learning aims to learn an embedding space where similar items have a small distance between their embeddings and dissimilar items are further away . Many state-of- the-art embeddings use metric learning for unsupervised training or fine-tuning including Sentence- BERT ( Reimers & Gurevych , 2019 ) and E5 ( Wang et al. , 2022 ) . RETVec is a resilient , multilingual embedding and text vectorizer trained to be robust against various forms of character-level typos and adversarial attacks . We extend the RETVec training regime to full text documents for RETSim . We use Multi-Similarity Loss ( Wang et al. , 2019 ) for pair-based metric learning , where typo-laden and near-duplicate versions of texts are trained to be closer in the embedding space , while other texts are pushed further away . Multi-Similarity Loss is based on a general weighting framework for pair-based losses and achieves state-of-the-art performance , outperforming alternatives such as Triplet Loss ( Schroff et al. , 2015 ) . 2 Preprint Figure 1 : RETSim model architecture diagram . RETSim works on arbitrary length text by split- ting texts into chunks of 512 characters during its vectorization phase and encodes them using the RETVec character vectorizer . The RETSim model then embeds each chunk of text into 256-dim partial embeddings and combines them to produce the global embedding . 3 RETSIM 3.1 ARCHITECTURE The RETSim model is composed of three main components ( as depicted in Figure 1 ) : The character-level vectorizer splits the input text into chunks of 512 characters , then uses the RETVec chararcter encoder ( Bursztein et al. , 2023 ) to encode each chunk , resulting in a batch of ( 512 , 24 ) dense inputs . The RETVec character vectorizer encodes each Unicode character as a compact 24-bit binary representation based on its integer codepoint value . This allows the vectorizer to encode all valid Unicode characters and support all languages . Furthermore , the character-level vectorizer has been shown to be more resilient against typos and adversarial attacks . A small transformer model is used to compute 256-dimension embeddings for each chunk of the input text . RETSimPartial-Dup uses these embeddings directly to finding documents that have matching chunks of text . Architecturally , the model consists of two Gated Attention Unit ( GAU ) blocks ( Hua ( Radenovi´c et al. , 2018 ) , a dense et al. , 2022 ) , followed by a Generalized-Mean pooling layer projection layer which projects the embedding into 256 dimensions , and an L2 normalization layer . The model has only 536k parameters , which is more than two orders of magnitude smaller than other neural embeddings ( Table 1 ) . L2-normalization allows the embeddings to be compared using cosine similarity . We discuss the impact of key architecture design choices in Section 6 . Hyperparameter details are provided in Appendix A.1.1 , and additional ablations results in Appendix A.5 . An embedding averaging module is then used to combine partial text embeddings into a full-text embedding which is used for global near-duplicate matching ( RETSimNear-Dup ) . Averaging chunked embeddings to produce a global embedding is a standard technique used by many models ( Cer et al. , 2018 ) to support infinite length inputs in a cost-efficient manner . We experimented with other aggregation techniques to produce more accurate global embeddings , including training a deep- averaging network ( Iyyer et al. , 2015 ) , but this did not improve performance and resulted in higher computation cost . RETSimNear-Dup and RETSimPartial-Dup are computed in a single forward pass which makes it computationally efficient . We output both types of embeddings as they have different applications : RETSimNear-Dup is better-suited for full-text matching and retrieval ( Section 4.2 ) , while RETSimPartial-Dup is used to find partial text matches where the near-duplicate content appears only in part of the document ( Section 4.3 ) . 3.2 MODEL TRAINING Dataset We use the multilingual C4 dataset ( mC4 ) for raw text data and following ( Xue et al. , 2020 ) , we use a language sampling exponent of α = 0.3 to balance sampling between low and high-resource languages . We only use text containing at least 16 characters , and we randomly select between 1 and 8 sentences ( roughly 512 characters ) for each text chunk . For each example in the 3 Input text Chunk vector Chunk vector .... Chunk vector Chunked and vectorized text ( num_chunks , 512 , 24 ) RETSim model RetSim Partial-Dup ( num_chunks , 256 ) RetSim Near-Dup ( 256 ) r e d o c n E r a h C c e v T E R U A G U A G g n i l o o P 2 L + e s n e D t x e t l a i t r a P s g n i d d e b m e 2 L + g n i g r a r e v A g n i d d e b m e t x e t l l u F Preprint training dataset , we generate 5 pairs of augmented examples . We apply three levels of augmentation to each example text chunk ( in this order ) : sentence-level , word-level , and character-level . For each level , we randomly select the augmentation to be applied from the following categories : insertion , deletion , substitution , and transposition . We randomly apply between 0 − 25 % sentence-level aug- mentation and up to 30 % combined character and word-level augmentation . Empirically , we found that increasing the percentage of augmentation beyond this point causes RETSim ’ s performance to degrade . The full list of augmentations used can be found in Appendix A.2 . Training Procedure We train RETSim using Multi-Similarity Loss ( Wang et al. , 2019 ) with α = 4 , β = 40 , λ = 0.5 , and ϵ = 0.1 . We hypertuned these parameters and the results are shown in Appendix A.5 . We train for 1 million steps with batch size = 1024 . The similarity loss trains the model to embed augmented versions of the same text closer in the embedding space , while dissimilar texts are pushed further apart . We use the LAMB optimizer ( You et al. , 2019 ) with a max learning rate of 0.001 and cosine decay . Detailed training hyperparameters are reported in Appendix A.1.2 . 4 EVALUATION Model/Algorithm Type Embed./Hash Size # Model Parameters LaBSE Multilingual USE Multilingual E5-Base PaLM 2 ( Gecko ) SimHash MinHash RETSim Neural Neural Neural Neural Hashing Hashing Neural 768 512 768 768 b bits n hashes 256 471M 69M 278M ? N/A N/A 536k Table 1 : Embedding models and hashing algorithms benchmarked in the paper . 4.1 MODELS AND ALGORITHMS EVALUATED We benchmark RETSim against four multilingual semantic text embeddings as well as popular n- gram based algorithms primarily used in near-duplicate text detection ( Table 1 ) . Our baseline text embeddings include Multilingual Universal Sentence Encoder ( Yang et al. , 2019 ) , LaBSE ( Feng et al. , 2022 ) , Multilingual E5 ( Wang et al. , 2022 ) , and PaLM 2 Gecko Embeddings ( Anil et al. , 2023 ) . All text embeddings are L2-normalized and compared using cosine similarity . We use exact search to index and retrieve nearest neighbors from our vector index for the experiments in Section 4 . For non-neural near-duplicate detection and clustering algorithms , we selected the two most popular algorithms : MinHash ( Broder et al. , 1998 ) and SimHash ( Charikar , 2002 ) . For MinHash , we use Datasketch ’ s MinHashLSH library . Following the most common practices in the literature ( Silcock et al. , 2022 ) , we use 10 hash functions for MinHash unless otherwise specified . We use word-level n-grams where we select the best value out of n = { 2 , 3 , 4 , ... , 10 } . For SimHash , we use 64- bit SimHash and conduct shingling at the character level , where the shingle size is selected from n = { 2 , 3 , 4 , ... , 10 } . For the near-duplicate detection benchmarks ( NEWS-COPY and CORE Near- Duplicates datasets ) , we tune the optimal deduplication threshold ( e.g . based on cosine similarity for neural-based embeddings and Jaccard similarity for MinHash ) . Detailed hyperparameter settings for RETSim and baseline algorithms used in the evaluation can be found in Appendix A.3 . 4.2 W4NT3D : WIKI-40B 4DVERSARIAL NEAR-T3XT DATASET EVALUATION Dataset Description The vast majority of text retrieval benchmarks are focused on evaluating semantic performance . To the best of our knowledge , there is no multilingual benchmark for sys- tematically measuring adversarial robustness for near-duplicate text retrieval . In an attempt to fill in the gap , we create and publish the W4NT3D benchmark ( Wiki-40B 4dversarial Near-T3xt Dataset ) , which contains around 400k pairs of syntactically similar texts to evaluate near-duplicate text re- trieval in the presence of various forms of text manipulations and typos . W4NT3D is based on the Wiki-40B dataset ( Guo et al. , 2020 ) . The dataset is split into query exam- ples and target examples , where query examples are synthetically-modified near-duplicate versions 4 Preprint of a target example ( e.g . with typos ) . For each of the 41 language splits in Wiki-40B , we randomly select 10,000 texts . The length of the target string is uniformly selected from between 16 and 8192 characters , in order to test performance on short and long text . To construct the query text corre- sponding to a target text , we randomly apply up to 25 % word and character augmentations , and up to 25 % sentence and paragraph augmentations . For each augmentation , we uniformly select from the [ insert , delete , substitute , and swap ] operations . We use Recall @ k with k = 1 as the main metric , following the setup commonly found in semantic text retrieval benchmarks . Model/Algorithm Arabic Chinese English German French Spanish Japanese Korean Russian Thai Avg ( 41 Langs ) LaBSE Multilingual USE Multilingual E5-Base PaLM 2 ( Gecko ) SimHash MinHash RETSimPartial-Dup RETSimNear-Dup 0.915 0.915 0.936 0.497 0.558 0.633 0.928 0.971 0.917 0.986 0.980 0.623 0.276 0.172 0.946 0.971 0.944 0.958 0.959 0.961 0.591 0.591 0.954 0.987 0.931 0.942 0.944 0.932 0.561 0.558 0.949 0.978 0.930 0.938 0.948 0.934 0.519 0.556 0.947 0.983 0.888 0.903 0.896 0.911 0.513 0.575 0.938 0.976 0.931 0.990 0.979 0.578 0.465 0.223 0.963 0.986 0.949 0.984 0.986 0.701 0.593 0.814 0.971 0.991 0.918 0.910 0.911 0.851 0.554 0.523 0.946 0.970 0.882 0.888 0.921 0.571 0.669 0.416 0.941 0.946 0.921 0.912 0.937 0.823 0.550 0.538 0.949 0.977 Table 2 : Per-language retrieval performance for various embedding models and algorithms on the W4NT3D benchmark . Results on selected languages are reported alongside the average Recall @ 1 for all 41 languages . Full results for all languages are reported in Appendix A.4 . Multilingual Performance Overall , RETSimNear-Dup achieves an average Recall @ 1 of 0.977 across all 41 languages on the W4NT3D benchmark ( Table 2 ) . RETSimPartial-Dup is second best with a Recall @ 1 of 0.949 and Multilingual E5 , the best-performing baseline , is third with an aver- age Recall @ 1 of 0.932 . We expect that RETSimNear-Dup outperforms RETSimPartial-Dup because the W4NT3D benchmark requires an algorithm to not just find near-duplicates , but to find the most simi- lar text . RETSimPartial-Dup optimizes for finding the most similar chunk of text in the corpus , which is not always the most similar text overall . Similarly , we hypothesize that MinHash and SimHash per- form poorly on the W4NT3D benchmark due to their lack of ability to distinguish which is the most similar text among the near-duplicates detected , and embedding-based models and cosine similarity offer a more fine-grained measure of similarity . RETSimNear-Dup outperforms baseline algorithms on all languages except for Chinese and Japanese . For these languages , we theorize that semantic embeddings may have the slight edge in performance because their significantly larger model sizes ( more than 100x larger than RETSim , as shown in Ta- ble 1 ) allow them to have a better representation on languages with large character sets . Furthermore , the sub-word level tokenizers used in the baseline embeddings often treat each character in Chinese or Japanese as individual tokens , which could offer higher resilience to typos . Figure 2 : Recall @ 1 performance on the W4NT3D benchmark , broken down by augmentation type . Results are averaged across all 41 language splits in W4NT3D . Adversarial Resilience Delving deeper into the impact of various types of text manipulation re- veals that RETSimNear-Dup and RETSimPartial-Dup perform almost equally well regardless of the type of augmentation applied ( Figure 2 ) . Semantic text embeddings perform well on paragraph , sentence and word-level manipulations , but as expected , exhibit significantly weaker performance towards character-level typos . MinHash and SimHash struggle more with word-level augmentations than deep-learning based embeddings and collapse when character-level typos are introduced . We at- 5 1.00 0.75 0.50 0.25 0.00 l l 1 @ a c e R Paragraph Sentence Word Character Augmentation Level LaBSE Multilingual USE Multilingual E5-Base PaLM 2 ( Gecko ) SimHash MinHash RETSim ( Partial-Dup ) RETSim ( Near-Dup ) Preprint tribute RETSim ’ s resilience to adversarial manipulations to the RETVec character encoder as well as using deep metric learning to train robust embeddings . Figure 4 reports the Recall @ 1 performance of the algorithms as the amount of augmentation in- creases . All algorithms perform perfectly when no augmentation is applied ( exact matching ) , but as the percentage of augmentation increases , n-gram based approaches exhibit a steep drop in perfor- mance . Semantic text embeddings are able to sustain a larger degree of augmentation before their retrieval capabilities start to degrade ( over 20 % ) . RETSimNear-Dup is the most robust algorithm , with a noticeable drop in performance only after around 40 % augmentation . This makes RETSim the most effective approach at clustering and deduplicating text under adversarial settings . Figure 3 : Recall @ 1 performances on the W4NT3D benchmark ( English only ) for varying max target lengths . Figure 4 : Recall @ 1 performances on the W4NT3D benchmark ( English only ) as the amount of augmenta- tion applied to the query text increases . Text Length Impact on Performance Figure 3 reports the Recall @ 1 performance of RETSim and baseline algorithms as the length of the query and target text varies . We see that RETSimNear-Dup and RETSimPartial-Dup outperforms all other methods on short texts with fewer than 128 characters . As the text length increases beyond 512 characters , RETSimNear-Dup remains close to perfect while RETSimPartial-Dup ’ s performance degrades since it splits the text into multiple embeddings and finds the nearest matching chunk of text . MinHash and SimHash also perform poorly on short text lengths and start to degrade on longer texts . For neural-based embeddings , we observe a slight drop in performance on longer texts for all models except RETSimNear-Dup and Multilingual USE , the only two embeddings that can handle arbitrary length inputs . 4.3 REAL-WORLD NEAR-DUPLICATE DETECTION EVALUATION Setup We benchmark RETSim ’ s ability to identify near-duplicate content on real-world datasets from the literature . The NEWS-COPY Deduplication dataset ( Silcock et al. , 2022 ) contains 27,210 historical news articles with 122,876 positive duplicate pairs . The dataset consists of noisy near- duplicates due to factors like OCR errors , plagiarism , and news aggregation . We also evaluate the algorithms on the CORE Near-Duplicates dataset ( Gyawali et al. , 2020 ) , which consists of 100k scholarly articles with 25k exact duplicates , 25k near-duplicates , and 50k non-duplicates . Near- duplicates in this dataset arise from article revisions , versioning and metadata differences , and hu- man typos . A key difference between these two benchmarks and the W4NT3D benchmark is that these two benchmarks are focused on detecting and clustering near-duplicate text , rather than robust text retrieval based on syntactic similarity . For both benchmarks , we follow the experimental setup provided in the papers and report Adjusted Rand Index ( ARI ) for the NEWS-COPY dataset and report precision/recall/F1 scores on the CORE Near-Duplicates dataset . Results On the NEWS-COPY dataset , RETSimPartial-Dup outperforms all other approaches by a significant margin ( 4.8 % ARI compared to our best MinHash result ) , as reported in Table 3 . In the dataset , there are many near-duplicate pairs where one text is significantly longer than the other , so it is expected that RETSimPartial-Dup , which can find matching text chunks in documents , is more suited for the task and outperforms RETSimNear-Dup . Bucketing the near-duplicate detection rate of each algorithm by the length ratio between positive pairs ( Figure 5 ) , we observe that RETSimPartial-Dup outperforms MinHash regardless of the length ratio , but MinHash surpasses RETSimNear-Dup per- formance when one text is above roughly 1.5x the length of the other text in a near-duplicate pair . 6 1.0 0.8 0.6 0.4 l l 1 @ a c e R 16 32 64 128 256 512 1024 2048 4096 8192 Max Target Text Length l l 1 @ a c e R 1.0 0.8 0.6 0.4 0 % 10 % 20 % 30 % 40 % 50 % Text Augmentation Amount ( % ) LaBSE Multilingual USE Multilingual E5-Base PaLM 2 ( Gecko ) SimHash MinHash RETSim ( Partial-Dup ) RETSim ( Near-Dup ) Preprint Model/Algorithm ARI Multilingual USE Multilingual E5-Base S-BERT * SimHash MinHash * MinHash ( Ours ) RETSimPartial-Dup RETSimNear-Dup 0.730 0.742 0.700 0.695 0.737 0.783 0.831 0.704 Table 3 : Performance comparison on the NEWS-COPY dataset . Adjusted Rand Index ( ARI ) values are reported . * denotes results from Silcock et al . ( 2022 ) . Figure 5 : Near-duplicate detection rate of RET- Sim vs MinHash for different length ratios of pos- itive pairs . X-axis is the length of longer divided by shorter text , rounded to the nearest integer . Additionally , we noticed that the labels in the dataset were occasionally noisy , as a substantial por- tion of the RETSim false positives appear to be near-duplicates upon inspection ( Appendix A.6 ) . On the CORE Near-Duplicates dataset ( Table 4 ) , where documents ( article title + abstract ) are roughly the same size , RETSimPartial-Dup and RETSimNear-Dup performance is roughly equivalent . Both methods outperform the baselines in terms of macro F1 score and accuracy . We use MinHash + LSH with 256 hash functions for computational efficiency , as recommended by the datasketch library1 for better accuracy than the default setting . Deduplication thresholds and detailed hyperpa- rameter settings for the algorithms on both near-duplication datasets can be found in Appendix A.3 . Model / Algorithm Exact Title Matching * LaBSE Multilingual USE Multilingual E5-Base MinHash + LSH RETSimPartial-Dup RETSimNear-Dup Precision Duplicates Recall Duplicates Precision Non-Duplicates Recall Non-Duplicates Macro F1 Accuracy 0.830 0.937 0.917 0.931 0.929 0.945 0.928 0.500 0.923 0.907 0.908 0.902 0.941 0.937 0.709 0.930 0.918 0.919 0.915 0.945 0.942 0.992 0.943 0.927 0.939 0.938 0.949 0.934 0.757 0.933 0.917 0.924 0.921 0.945 0.935 0.746 0.919 0.909 0.920 0.918 0.928 0.926 Table 4 : Evaluation results on the CORE Near-Duplicates dataset . Precision/recall/macro F1 and accuracy numbers are reported . * denotes results from Gyawali et al . ( 2020 ) . 5 APPLICATIONS 5.1 TRAINING DATASET DEDUPLICATION Model/Algorithm % train examples with dup in train % valid examples with dup in train MinHash + LSH Exact Substring * RETSimNear-Dup RETSimPartial-Dup 0.47 % 2.76 % 3.17 % 12.77 % 0.46 % 0.52 % 0.59 % 2.66 % Table 5 : Deduplication rate on Wiki-40B ( English ) . * denotes results from Lee et al . ( 2022 ) . Setup We evaluate RETSim ’ s ability to deduplicate text training datasets by deduplicating the English split of Wiki-40B ( Guo et al. , 2020 ) . We conservatively set the cosine similarity deduplica- tion threshold to 0.1 for RETSimNear-Dup and 0.15 for RETSimPartial-Dup to limit the amount of false positives , based on the optimal thresholds found in the evaluation ( Appendix A.3 ) . We use USe- arch ’ s default vector index for approximate nearest neighbor search ( Vardanian , 2023 ) . We compare 1datasketch : Big Data Looks Small . https : //github.com/ekzhu/datasketch . 7 t e a R n o t i t c e e D e a c t i l p u D - r a e N 1.0 0.8 0.5 0.3 0.0 1 2 3 4 5 6 7 8 9 10 Length Ratio between Near-Duplicate Text Pair RETSim ( Partial-Dup ) RETSim ( Near-Dup ) MinHash Preprint Model/Algorithm Accelerator Batch Size Embedding / Hashing time ( sec ) examples/sec MinHash + LSH RETSim RETSim RETSim CPU AMD 7950 32 cores Onnx CPU AMD 7950 32 cores TensorFlow GPU RTX 4090 TensorFlow GPU NVIDIA H100 - 256 4096 16384 234 10839 720 363 12544 270 4062 8069 Table 6 : Embedding/hashing speed of RETSim vs MinHash + LSH on the Wiki-40B dataset . against MinHash + LSH , where we set the number of hash functions to be 256 following Kocetkov et al . ( 2022 ) and use a Jaccard similarity threshold of 0.8 for deduplication ( Lee et al. , 2022 ) . Results Overall , as reported in Table 5 , RETSimNear-Dup finds slightly more duplicates in the Wiki- 40B training and validation splits . This is in-line with our deduplication results ( Section 4.3 ) where RETSimNear-Dup outperforms other algorithms . On the other hand , RETSimPartial-Dup finds signifi- cantly more matches than the exact substring matching algorithm used in the previous study ( Lee et al. , 2022 ) , showcasing the usefulness of performing both near-duplicate and partial-duplicate matching at once . This larger-than-expected number of partial matches indicate that machine learn- ing practitioners should take extra care to deduplicate Wikipedia at the chunk level to avoid feeding duplicate text to their models . In terms of embedding speed ( Table 6 ) , RETSim is significantly slower than MinHash + LSH on CPU ( 46x slower ) , competitive when using a desktop GPU such as the RTX 4090 ( 3x slower ) and almost on-par when using a high-end GPU like the NVIDIA H100 ( 1.5x slower ) . Our current code is written in Python and not fully optimized , so we expect this performance gap to significantly shrink as we optimize our implementation . Although RETSim is slower than MinHash , RETSim is significantly smaller and faster than other text embedding models , and closes the performance gap between neural and non-neural based methods for near-duplicate text detection and dataset deduplication . Both RETSimNear-Dup and RETSimPartial-Dup are returned at the same time so they have the same embedding speed . Indexing and retrieval times will depend on the vector index and search algorithm used . For longer documents , RETSimPartial-Dup will produce more embeddings than RETSimNear-Dup , so RETSimPartial-Dup offers a tradeoff between finer-grained matching versus indexing/retrieval speed , which will depend on the specific vector search algorithm and dataset used . 5.2 IN THE WILD : SPAM EMAIL CLUSTERING In this section , we showcase RETSim ’ s real-world performance on clustering near-duplicate text which has been heavily manipulated by adversarial attacks by performing an evaluation on spam campaigns . Spam constitutes a strong proving ground for near-duplicate clustering algorithms as spammers employ adversarial augmentation techniques in an attempt to evade detection . Such aug- mentations typically include appending or prepending unrelated text , interleaving random words and different languages , intentionally introducing typos , abusing extended character sets such as emojis and homoglyphs , and more . These techniques are collectively referred to as hash-busting . Setup The dataset consists of 5,252 spam emails from 196 spam campaigns , donated by Gmail users who flagged them when they reached their inboxes . Each example contains the email subject concatenated with the message content . The emails were misclassified by a spam classifier due to their effective adversarial text manipulation techniques , which makes them a challenging test set for clustering evaluations . Some examples of hash-busting attacks and adversarial manipulations we observe include the use of homoglpyphs , uncommon Unicode character sets , invisible characters , and padding with random words from different languages . To get the ground truth campaign clusters , emails were manually reviewed and assigned to a specific spam campaign based on similarity by human reviewers . We use agglomerative clustering to cluster spam emails , and report homogeneity , completeness , V-Measure , and Adjusted Rand Index ( ARI ) metrics . Results Overall , we observed that RETSim is significantly better at clustering near-duplicates with adversarial manipulations , outperforming both SimHash and USE across all metrics considered ( Table 7 ) . In particular , we observed that RETSim outperforms USE by 4.6 % on the V-Measure score which is our main metric . The results reported in this section are in-line with what we observe since we deployed RETSim as our main near-duplicate detection algorithm in December 2022 . 8 Preprint Model / Algorithm Homogeneity Completeness V-Measure ARI USE SimHash + LSH RETSimNear-Dup 0.856 0.867 0.937 0.955 0.876 0.963 0.903 0.871 0.949 0.6 0.571 0.747 Table 7 : Performance on clustering adversarial spam campaigns in practice . 6 ABLATION STUDIES Setup In this section , we summarize the key ablation studies we performed when designing RET- Sim . All the models used in this section are trained using the setup detailed in Appendix A.1.2 , ex- cept we only train them for 100k steps to reduce computational costs . We evaluate RETSimNear-Dup ’ s performance for each model on a subset of the W4NT3D benchmark , where we randomly select 1000 examples from each of the 41 language splits and use Recall @ 1 as reported metric . Block Type Recall @ 1 Chunk Size Recall @ 1 Embed . Dim Recall @ 1 RETVec MLP ConvNeXt BERT T5 * GAU 0.975 0.978 0.973 0.980 0.986 128 256 * 512 1024 2048 0.979 0.984 0.986 0.983 0.978 64 128 * 256 512 768 0.969 0.980 0.986 0.986 0.986 Table 8 : RETSim ablation study results on architecture block type ( left ) , text chunk size ( middle ) , and embedding dimension ( right ) . * Bold denotes the value selected for the final RETSim model . Results Table 8 contains RETSim ablation study results on max text chunk size , architecture block type , and embedding size . The most important architectural decision was to decide the optimal text chunk size and finding the right balance between having the smallest size possible to maximize RETSimPartial-Dup efficiency while ensuring RETSimNear-Dup full-text embeddings can work effec- tively on full documents . We find that chunks of 512 characters offer the best performance . We also tested various model architectures and transformer blocks to find the best balance between efficiency and performance . We find that the more modern GAU block ( Hua et al. , 2022 ) outper- forms the vanilla BERT transformer block ( Devlin et al. , 2019 ) and the T5 block ( Xue et al. , 2020 ) . We also tried modern CNN architectures such as ConvNeXt ( Liu et al. , 2022 ) and the MLP architec- ture proposed in RETVec ( Bursztein et al. , 2023 ) , but both were worse than GAU block performance . Last but not least , we found that increasing the embedding size past 256 dimensions does not yield any meaningful improvements for RETSimNear-Dup . Accordingly , we opted to use a 256-dimension embedding for space-efficiency and to maximize indexing and query speed . Additional ablation studies for other hyperparameters can be found in Appendix A.5 . 7 FUTURE WORK RETSim ’ s novel training regime , which combines metric learning and data augmentation , has many other potential applications that we plan to explore in future work . For example , it could be adapted or extended to train robust semantic embeddings or image similarity embeddings . Additionally , we expect that as general models become bigger and more expensive to run in the future , smaller , specialized models such as RETSim will emerge as an efficient alternative for a wide range of tasks . 8 CONCLUSION In this paper , we introduced RETSim , a novel , multilingual text embedding which achieves state- of-the-art performance on near-duplicate text detection , dataset deduplication , and syntactic text similarity benchmarks . RETSim is significantly faster than previous neural-based text embeddings and more robust than n-gram based algorithms , which makes it suitable for large-scale text retrieval and dataset deduplication , especially in adversarial settings such as spam detection . Furthermore , we introduced the W4NT3D benchmark , the first multilingual dataset designed to measure the ad- versarial robustness of near-duplicate text detection algorithms . We open-source both RETSim and the W4NT3D benchmark under the MIT License . 9 Preprint REFERENCES Naeem Ahmed , Rashid Amin , Hamza Aldabbas , Deepika Koundal , Bader Alouffi , and Tariq Shah . Machine Learning Techniques for Spam Detection in Email and IoT Platforms : Analysis and Research Challenges . Security and Communication Networks , 2022:1–19 , February 2022 . ISSN 1939-0122 , 1939-0114. doi : 10.1155/2022/1862888 . URL https : //www.hindawi.com/ journals/scn/2022/1862888/ . Moustafa Alzantot , Yash Sharma , Ahmed Elgohary , Bo-Jhang Ho , Mani Srivastava , and Kai-Wei Chang . Generating Natural Language Adversarial Examples , September 2018 . URL http : //arxiv.org/abs/1804.07998 . arXiv:1804.07998 [ cs ] . Rohan Anil , Andrew M. Dai , Orhan Firat , Melvin Johnson , Dmitry Lepikhin , Alexandre Passos , Siamak Shakeri , Emanuel Taropa , Paige Bailey , Zhifeng Chen , Eric Chu , Jonathan H. Clark , Laurent El Shafey , Yanping Huang , Kathy Meier-Hellstern , Gaurav Mishra , Erica Moreira , Mark Omernick , Kevin Robinson , Sebastian Ruder , Yi Tay , Kefan Xiao , Yuanzhong Xu , Yujing Zhang , Gustavo Hernandez Abrego , Junwhan Ahn , Jacob Austin , Paul Barham , Jan Botha , James Brad- bury , Siddhartha Brahma , Kevin Brooks , Michele Catasta , Yong Cheng , Colin Cherry , Christo- pher A. Choquette-Choo , Aakanksha Chowdhery , Cl´ement Crepy , Shachi Dave , Mostafa De- hghani , Sunipa Dev , Jacob Devlin , Mark D´ıaz , Nan Du , Ethan Dyer , Vlad Feinberg , Fangxiaoyu Feng , Vlad Fienber , Markus Freitag , Xavier Garcia , Sebastian Gehrmann , Lucas Gonzalez , Guy Gur-Ari , Steven Hand , Hadi Hashemi , Le Hou , Joshua Howland , Andrea Hu , Jeffrey Hui , Jeremy Hurwitz , Michael Isard , Abe Ittycheriah , Matthew Jagielski , Wenhao Jia , Kathleen Kenealy , Maxim Krikun , Sneha Kudugunta , Chang Lan , Katherine Lee , Benjamin Lee , Eric Li , Music Li , Wei Li , YaGuang Li , Jian Li , Hyeontaek Lim , Hanzhao Lin , Zhongtao Liu , Frederick Liu , Mar- cello Maggioni , Aroma Mahendru , Joshua Maynez , Vedant Misra , Maysam Moussalem , Zachary Nado , John Nham , Eric Ni , Andrew Nystrom , Alicia Parrish , Marie Pellat , Martin Polacek , Alex Polozov , Reiner Pope , Siyuan Qiao , Emily Reif , Bryan Richter , Parker Riley , Alex Castro Ros , Aurko Roy , Brennan Saeta , Rajkumar Samuel , Renee Shelby , Ambrose Slone , Daniel Smilkov , David R. So , Daniel Sohn , Simon Tokumine , Dasha Valter , Vijay Vasudevan , Kiran Vodrahalli , Xuezhi Wang , Pidong Wang , Zirui Wang , Tao Wang , John Wieting , Yuhuai Wu , Kelvin Xu , Yun- han Xu , Linting Xue , Pengcheng Yin , Jiahui Yu , Qiao Zhang , Steven Zheng , Ce Zheng , Weikang Zhou , Denny Zhou , Slav Petrov , and Yonghui Wu . PaLM 2 Technical Report , September 2023 . URL http : //arxiv.org/abs/2305.10403 . arXiv:2305.10403 [ cs ] . Andrei Z. Broder , Moses Charikar , Alan M. Frieze , and Michael Mitzenmacher . Min-wise inde- In Proceedings of the thirtieth annual ACM symposium on Theory of pendent permutations . computing , pp . 327–336 , 1998 . Eli Bursztein , Marina Zhang , Owen vallis , Xinyu Jia , and Alexey Kurakin . RetVec : Resilient and Efficient Text Vectorizer . 2023 . Daniel Cer , Yinfei Yang , Sheng-yi Kong , Nan Hua , Nicole Limtiaco , Rhomni St John , Noah Con- stant , Mario Guajardo-Cespedes , Steve Yuan , and Chris Tar . Universal sentence encoder . arXiv preprint arXiv:1803.11175 , 2018 . Moses S. Charikar . Similarity estimation techniques from rounding algorithms . In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing , pp . 380–388 , 2002 . Jacob Devlin , Ming-Wei Chang , Kenton Lee , and Kristina Toutanova . BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding , May 2019 . URL http : //arxiv . org/abs/1810.04805 . arXiv:1810.04805 [ cs ] . Fangxiaoyu Feng , Yinfei Yang , Daniel Cer , Naveen Arivazhagan , and Wei Wang . Language- agnostic BERT Sentence Embedding , March 2022 . URL http : //arxiv.org/abs/2007 . 01852. arXiv:2007.01852 [ cs ] . Ji Gao , Jack Lanchantin , Mary Lou Soffa , and Yanjun Qi . Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers , May 2018 . URL http : //arxiv.org/abs/ 1801.04354. arXiv:1801.04354 [ cs ] . 10 Preprint Mandy Guo , Zihang Dai , Denny Vrandeˇci´c , and Rami Al-Rfou . Wiki-40b : Multilingual language model dataset . In Proceedings of the 12th Language Resources and Evaluation Conference , pp . 2440–2452 , 2020 . Bikash Gyawali , Lucas Anastasiou , and Petr Knoth . Deduplication of Scholarly Documents us- ing Locality Sensitive Hashing and Word Embeddings . In Proceedings of the Twelfth Language Resources and Evaluation Conference , pp . 901–910 , Marseille , France , May 2020 . European Lan- guage Resources Association . ISBN 979-10-95546-34-4 . URL https : //aclanthology . org/2020.lrec-1.113 . Matthias Hagen , Martin Potthast , Marcel Gohsen , Anja Rathgeber , and Benno Stein . A large- In Proceedings of the 40th International ACM SIGIR scale query spelling correction corpus . Conference on Research and Development in Information Retrieval , pp . 1261–1264 , 2017 . Weizhe Hua , Zihang Dai , Hanxiao Liu , and Quoc Le . Transformer quality in linear time . In International Conference on Machine Learning , pp . 9099–9117 . PMLR , 2022 . B. Issac , R. Chiong , and S. M. Jacob . Analysis of phishing attacks and countermeasures , 2014 . Mohit Iyyer , Varun Manjunatha , Jordan Boyd-Graber , and Hal Daum´e Iii . Deep Unordered Composition Rivals Syntactic Methods for Text Classification . In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pp . 1681–1691 , Bei- jing , China , 2015 . Association for Computational Linguistics . doi : 10.3115/v1/P15-1162 . URL http : //aclweb.org/anthology/P15-1162 . Nikhil Kandpal , Eric Wallace , and Colin Raffel . Deduplicating Training Data Mitigates Pri- vacy Risks in Language Models . In Proceedings of the 39th International Conference on Machine Learning , pp . 10697–10707 . PMLR , June 2022 . URL https : //proceedings . mlr.press/v162/kandpal22a.html . ISSN : 2640-3498 . Denis Kocetkov , Raymond Li , Loubna Ben Allal , Jia Li , Chenghao Mou , Carlos Mu˜noz Ferrandis , Yacine Jernite , Margaret Mitchell , Sean Hughes , Thomas Wolf , Dzmitry Bahdanau , Leandro von Werra , and Harm de Vries . The Stack : 3 TB of permissively licensed source code , November 2022 . URL http : //arxiv.org/abs/2211.15533 . arXiv:2211.15533 [ cs ] . Katherine Lee , Daphne Ippolito , Andrew Nystrom , Chiyuan Zhang , Douglas Eck , Chris Callison- Burch , and Nicholas Carlini . Deduplicating Training Data Makes Language Models Better , March 2022 . URL http : //arxiv.org/abs/2107.06499 . arXiv:2107.06499 [ cs ] . Zhuang Liu , Hanzi Mao , Chao-Yuan Wu , Christoph Feichtenhofer , Trevor Darrell , and Saining Xie . A convnet for the 2020s . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp . 11976–11986 , 2022 . John X. Morris , Eli Lifland , Jin Yong Yoo , Jake Grigsby , Di Jin , and Yanjun Qi . TextAttack : A Framework for Adversarial Attacks , Data Augmentation , and Adversarial Training in NLP , Oc- tober 2020 . URL http : //arxiv.org/abs/2005.05909 . arXiv:2005.05909 [ cs ] . Niklas Muennighoff , Nouamane Tazi , Lo¨ıc Magne , and Nils Reimers . MTEB : Massive Text Em- bedding Benchmark , October 2022 . URL https : //arxiv.org/abs/2210.07316v1 . Filip Radenovi´c , Giorgos Tolias , and Ondˇrej Chum . Fine-tuning CNN image retrieval with no human annotation . IEEE transactions on pattern analysis and machine intelligence , 41 ( 7 ) :1655– 1668 , 2018 . Publisher : IEEE . Nils Reimers and Iryna Gurevych . Sentence-bert : Sentence embeddings using siamese bert- networks . arXiv preprint arXiv:1908.10084 , 2019 . Florian Schroff , Dmitry Kalenichenko , and James Philbin . FaceNet : A Unified Embedding for Face Recognition and Clustering . In 2015 IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , pp . 815–823 , June 2015. doi : 10.1109/CVPR.2015.7298682 . URL http : //arxiv.org/abs/1503.03832 . arXiv:1503.03832 [ cs ] . 11 Preprint Emily Silcock , Luca D ’ Amico-Wong , Jinglin Yang , and Melissa Dell . Noise-Robust De-Duplication at Scale , October 2022 . URL http : //arxiv.org/abs/2210.04261 . arXiv:2210.04261 [ cs ] . Yifang Sun , Jianbin Qin , and Wei Wang . Near Duplicate Text Detection Using Frequency-Biased Signatures . In David Hutchison , Takeo Kanade , Josef Kittler , Jon M. Kleinberg , Friedemann Mat- tern , John C. Mitchell , Moni Naor , Oscar Nierstrasz , C. Pandu Rangan , Bernhard Steffen , Madhu Sudan , Demetri Terzopoulos , Doug Tygar , Moshe Y. Vardi , Gerhard Weikum , Xuemin Lin , Yan- nis Manolopoulos , Divesh Srivastava , and Guangyan Huang ( eds . ) , Web Information Systems Engineering – WISE 2013 , volume 8180 , pp . 277–291 . Springer Berlin Heidelberg , Berlin , Hei- delberg , 2013 . ISBN 978-3-642-41229-5 978-3-642-41230-1. doi : 10.1007/978-3-642-41230-1 24 . URL http : . Series Title : Lecture Notes in Computer Science . Ash Vardanian . USearch by Unum Cloud , October 2023 . URL https : //github.com/ unum-cloud/usearch . Liang Wang , Nan Yang , Xiaolong Huang , Binxing Jiao , Linjun Yang , Daxin Jiang , Rangan Ma- jumder , and Furu Wei . Text Embeddings by Weakly-Supervised Contrastive Pre-training , De- cember 2022 . URL http : //arxiv.org/abs/2212.03533 . arXiv:2212.03533 [ cs ] . Xun Wang , Xintong Han , Weilin Huang , Dengke Dong , and Matthew R. Scott . Multi-similarity loss with general pair weighting for deep metric learning . In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp . 5022–5030 , 2019 . Linting Xue , Noah Constant , Adam Roberts , Mihir Kale , Rami Al-Rfou , Aditya Siddhant , Aditya Barua , and Colin Raffel . mT5 : A massively multilingual pre-trained text-to-text transformer . arXiv preprint arXiv:2010.11934 , 2020 . Yinfei Yang , Daniel Cer , Amin Ahmad , Mandy Guo , Jax Law , Noah Constant , Gustavo Hernandez Abrego , Steve Yuan , Chris Tar , Yun-Hsuan Sung , Brian Strope , and Ray Kurzweil . Multilingual Universal Sentence Encoder for Semantic Retrieval , July 2019 . URL http : //arxiv.org/ abs/1907.04307 . arXiv:1907.04307 [ cs ] . Yang You , Jing Li , Sashank Reddi , Jonathan Hseu , Sanjiv Kumar , Srinadh Bhojanapalli , Xiaodan Song , James Demmel , Kurt Keutzer , and Cho-Jui Hsieh . Large batch optimization for deep learning : Training bert in 76 minutes . arXiv preprint arXiv:1904.00962 , 2019 . 12 Preprint A APPENDIX A.1 RETSIM DETAILS A.1.1 RETSIM MODEL HYPERPARAMETERS The full list of RETSim model hyperparameters can be found in Table 9 . Hyperparameter Max input length ( per chunk ) Block type # blocks Hidden dim Expansion rate Activation function Attention activation function Absolute positional encoding Relative positional encoding Norm type Pooling type Dropout rate Embedding dim # Parameters Value 512 GAU 2 256 1 Swish relu2 ScaledSin RoPE ScaleNorm GeM ( p = 3 ) 0 256 536k Table 9 : Detailed RETSim model hyperparameters . A.1.2 RETSIM TRAINING HYPERPARAMETERS Table 10 details the hyperparameters settings for training configuration , loss , and optimizer used to train the RETSim model . Hyperparameter Value Batch size Train steps LAMB ϵ LAMB β1 LAMB β2 Max learning rate End learning rate Learning rate decay Weight decay 1024 1 million 1e-6 0.9 0.999 0.001 0 Cosine 0 Table 10 : RETSim detailed training hyperparameters . A.2 TRAINING DATASET DETAILS Below , we provide the full list of augmentations used to generate augmented text for the RETSim training dataset , as described in Section 3.2 . SENTENCE-LEVEL AUGMENTATIONS • Deletion : – Random sentence deletion – Random sentence truncation • Insertion : 13 Preprint – Random prefix sentence – Random suffix sentence – Random sentence insertion – Repeat sentence • Substitution : – Lowercase/uppercase sentence – Random sentence substitution • Transposition : – Neighboring Swap WORD-LEVEL AUGMENTATIONS • Deletion : – Random word deletion • Insertion : – Random word insertion – Random word insertion per language • Substitution : – 3-gram frequency based word substitution – Random word substitution – Random word substitution per language – Repeat word • Transposition : – Neighboring Swap CHARACTER-LEVEL AUGMENTATIONS • Deletion : – Random character deletion • Substitution : – Case substitution – n-gram based substitution for n = 3 , 4 , 5 – QWERTY keyboard typo substitution – Homoglyphs substitution – Random ASCII substitution – Random character from language alphabet substitution – Random punctuation substitution – Random Unicode character substitution • Insertion : – Character repetition – n-grams based insertion for n = 3 , 4 , 5 – Random character from language alphabet insertion – Random punctuation insertion – Random Unicode character insertion • Transposition : – Neighboring swap A.3 DETAILED EVALUATION HYPERPARAMETERS Figures 6 and 7 contain information on deduplication thresholds values and hyperparameter settings for each algorithm benchmarked on the NEWS-COPY and CORE deduplication datasets . 14 Preprint Model / Algorithm Threshold Type Threshold Value Hyperparameters Multilingual USE Cosine Similarity Multilingual E5-Base Cosine Similarity SimHash MinHash ( Ours ) RETSimNear-Dup RETSimPartial-Dup Hamming Distance Jaccard Similarity Cosine Similarity Cosine Similarity 0.96 0.88 10 0.6 0.89 0.84 - - 64 bits , 5-grams ( character-level ) 10 hash functions , 2-grams ( word-level ) - - Figure 6 : Hyperparameter settings for NEWS-COPY dataset evaluation in Section 4.3 . Model / Algorithm Threshold Type Threshold Value Hyperparameters Cosine Similarity LaBSE Multilingual USE Cosine Similarity Multilingual E5-Base Cosine Similarity SimHash + LSH MinHash + LSH RETSimNear-Dup RETSimPartial-Dup Hamming Distance Jaccard Similarity Cosine Similarity Cosine Similarity 0.88 0.97 0.87 6 0.5 0.86 0.82 - - - 64 bits , 3-grams ( character-level ) 256 hash functions , 3-grams ( word-level ) - - Figure 7 : Hyperparameter settings for CORE Near-Duplicates dataset evaluation in Section 4.3 . A.3.1 DEDUPLICATION THRESHOLD IMPACT Figure 8 : Precision/Recall/F1 scores for different cosine distance deduplication thresholds for RETSimNear-Dup ( left ) and RETSimPartial-Dup ( right ) on the NEWS-COPY dataset . A.4 DETAILED W4NT3D BENCHMARK RESULTS Tables 11 and 12 show detailed performance results for RETSim and all baseline algorithms for every language split in the W4NT3D benchmark . 15 Preprint 9 4 9 . 0 4 8 9 . 0 6 8 9 . 0 1 0 7 . 0 3 9 5 . 0 4 1 8 . 0 1 7 9 . 0 1 9 9 . 0 o k 1 3 9 . 0 0 9 9 . 0 9 7 9 . 0 8 7 5 . 0 5 6 4 . 0 3 2 2 . 0 3 6 9 . 0 6 8 9 . 0 a j 9 2 9 . 0 7 3 9 . 0 3 3 9 . 0 7 3 9 . 0 3 1 5 . 0 5 7 5 . 0 4 4 9 . 0 2 8 9 . 0 5 1 9 . 0 8 2 9 . 0 2 4 9 . 0 4 2 9 . 0 3 3 5 . 0 3 6 5 . 0 0 5 9 . 0 6 7 9 . 0 8 9 8 . 0 5 8 8 . 0 9 8 8 . 0 6 7 8 . 0 0 3 5 . 0 2 1 5 . 0 0 5 9 . 0 2 6 9 . 0 2 1 9 . 0 9 8 8 . 0 1 0 9 . 0 3 0 9 . 0 7 5 5 . 0 6 5 5 . 0 4 3 9 . 0 2 6 9 . 0 7 2 9 . 0 8 4 6 . 0 4 6 9 . 0 5 3 4 . 0 3 6 5 . 0 6 0 6 . 0 1 4 9 . 0 5 7 9 . 0 7 3 9 . 0 1 4 8 . 0 8 5 9 . 0 9 8 5 . 0 1 5 6 . 0 7 9 6 . 0 5 4 9 . 0 9 8 9 . 0 0 3 9 . 0 8 3 9 . 0 8 4 9 . 0 4 3 9 . 0 9 1 5 . 0 6 5 5 . 0 7 4 9 . 0 3 8 9 . 0 6 2 9 . 0 3 1 9 . 0 3 4 9 . 0 4 1 9 . 0 2 3 6 . 0 8 6 5 . 0 6 5 9 . 0 1 8 9 . 0 2 1 9 . 0 0 7 8 . 0 9 2 9 . 0 6 5 3 . 0 8 6 5 . 0 5 9 5 . 0 6 4 9 . 0 1 7 9 . 0 3 2 9 . 0 0 3 9 . 0 1 5 9 . 0 6 2 9 . 0 5 6 6 . 0 5 8 5 . 0 3 5 9 . 0 5 8 9 . 0 8 8 8 . 0 3 0 9 . 0 6 9 8 . 0 1 1 9 . 0 6 9 4 . 0 4 0 5 . 0 8 3 9 . 0 6 7 9 . 0 4 4 9 . 0 8 5 9 . 0 9 5 9 . 0 1 6 9 . 0 1 9 5 . 0 1 9 5 . 0 4 5 9 . 0 7 8 9 . 0 8 1 9 . 0 4 7 8 . 0 4 3 9 . 0 9 8 5 . 0 7 5 5 . 0 4 7 5 . 0 5 3 9 . 0 4 7 9 . 0 1 3 9 . 0 2 4 9 . 0 4 4 9 . 0 2 3 9 . 0 1 6 5 . 0 8 5 5 . 0 9 4 9 . 0 8 7 9 . 0 8 3 9 . 0 7 2 9 . 0 6 5 9 . 0 3 4 9 . 0 2 9 5 . 0 8 9 5 . 0 3 5 9 . 0 8 8 9 . 0 2 1 9 . 0 0 0 9 . 0 7 2 9 . 0 1 1 9 . 0 9 7 5 . 0 1 8 5 . 0 0 5 9 . 0 3 7 9 . 0 7 9 8 . 0 3 8 8 . 0 0 9 8 . 0 2 0 9 . 0 5 8 4 . 0 8 0 5 . 0 4 2 9 . 0 8 6 9 . 0 5 1 9 . 0 9 0 9 . 0 9 9 8 . 0 1 5 8 . 0 0 1 5 . 0 6 0 5 . 0 2 4 9 . 0 6 7 9 . 0 5 1 9 . 0 5 1 9 . 0 6 3 9 . 0 7 9 4 . 0 8 5 5 . 0 3 3 6 . 0 8 2 9 . 0 1 7 9 . 0 e s a B - 5 E l a u g n i l i t l u M E S U l a u g n i l i t l u M ) o k c e G ( 2 M L a P E S B a L p u D - l a i t r a P m S T E R i p u D i - r a e N m S T E R h s a H m S i h s a H n i M t i d i u h r h i h e h r f fi a f t e s e n e l e e d a d s c a c g b r a m h t i r o g l A / l e d o M . ) 1 t r a p ( k r a m h c n e b D 3 T N 4 W e h t n o s m h t i r o g l a d n a s l e d o m g n i d d e b m e s u o i r a v r o f e c n a m r o f r e p 1 @ l l a c e R e g a u g n a l - r e p l l u F : 1 1 e l b a T w t - h z n c - h z i v k u r t l t h t v s r s l s k s u r o r t p l p o n l n s m v l t l m h t i r o g l A / l e d o M 16 8 1 9 . 0 5 8 9 . 0 8 7 9 . 0 9 0 6 . 0 5 1 3 . 0 0 0 2 . 0 7 5 9 . 0 8 6 9 . 0 7 1 9 . 0 6 8 9 . 0 0 8 9 . 0 3 2 6 . 0 6 7 2 . 0 2 7 1 . 0 6 4 9 . 0 1 7 9 . 0 2 3 9 . 0 0 1 9 . 0 5 4 9 . 0 3 6 8 . 0 9 0 6 . 0 1 8 5 . 0 1 5 9 . 0 5 8 9 . 0 9 9 8 . 0 3 9 8 . 0 2 7 8 . 0 2 2 8 . 0 7 1 5 . 0 0 2 5 . 0 1 4 9 . 0 7 5 9 . 0 0 3 9 . 0 0 4 9 . 0 1 5 9 . 0 2 0 9 . 0 6 0 6 . 0 3 8 5 . 0 4 5 9 . 0 8 7 9 . 0 7 4 9 . 0 9 4 9 . 0 9 6 9 . 0 4 4 9 . 0 7 0 5 . 0 0 7 5 . 0 1 6 9 . 0 9 8 9 . 0 2 8 8 . 0 8 8 8 . 0 1 2 9 . 0 1 7 5 . 0 9 6 6 . 0 6 1 4 . 0 1 4 9 . 0 6 4 9 . 0 6 0 9 . 0 9 9 8 . 0 5 2 9 . 0 9 1 9 . 0 2 5 5 . 0 0 2 5 . 0 3 5 9 . 0 9 6 9 . 0 0 3 9 . 0 6 0 9 . 0 6 9 8 . 0 6 5 8 . 0 3 5 5 . 0 5 2 5 . 0 3 6 9 . 0 9 7 9 . 0 1 3 9 . 0 8 0 9 . 0 0 4 9 . 0 6 1 9 . 0 0 8 5 . 0 0 7 5 . 0 7 4 9 . 0 7 7 9 . 0 2 2 9 . 0 1 0 9 . 0 1 2 9 . 0 4 2 9 . 0 5 7 5 . 0 3 7 5 . 0 1 6 9 . 0 1 7 9 . 0 8 1 9 . 0 0 1 9 . 0 1 1 9 . 0 1 5 8 . 0 4 5 5 . 0 3 2 5 . 0 6 4 9 . 0 0 7 9 . 0 9 0 9 . 0 9 6 8 . 0 6 3 9 . 0 3 9 8 . 0 4 1 5 . 0 0 2 5 . 0 8 4 9 . 0 7 7 9 . 0 4 4 9 . 0 2 5 9 . 0 5 5 9 . 0 0 5 9 . 0 8 4 5 . 0 3 7 5 . 0 4 5 9 . 0 5 8 9 . 0 8 2 9 . 0 1 3 9 . 0 8 2 9 . 0 3 1 9 . 0 6 8 5 . 0 3 6 5 . 0 3 5 9 . 0 9 7 9 . 0 8 2 9 . 0 1 2 9 . 0 4 4 9 . 0 0 3 9 . 0 7 7 5 . 0 0 6 5 . 0 8 5 9 . 0 1 8 9 . 0 1 3 9 . 0 6 3 9 . 0 4 3 9 . 0 1 3 9 . 0 7 2 5 . 0 0 4 5 . 0 0 5 9 . 0 9 7 9 . 0 9 1 9 . 0 2 3 9 . 0 9 4 9 . 0 8 2 9 . 0 3 3 5 . 0 1 1 5 . 0 1 6 9 . 0 0 8 9 . 0 2 2 9 . 0 9 1 9 . 0 5 3 9 . 0 7 0 9 . 0 4 2 6 . 0 9 7 5 . 0 5 4 9 . 0 3 8 9 . 0 9 1 9 . 0 2 0 9 . 0 1 4 9 . 0 9 0 9 . 0 9 0 6 . 0 8 6 5 . 0 7 5 9 . 0 0 8 9 . 0 e s a B - 5 E l a u g n i l i t l u M E S U l a u g n i l i t l u M ) o k c e G ( 2 M L a P E S B a L p u D - l a i t r a P m S T E R i h s a H m S i h s a H n i M p u D i - r a e N m S T E R . ) 2 t r a p ( k r a m h c n e b D 3 T N 4 W e h t n o s m h t i r o g l a d n a s l e d o m g n i d d e b m e s u o i r a v r o f e c n a m r o f r e p 1 @ l l a c e R e g a u g n a l - r e p l l u F : 2 1 e l b a T Preprint A.5 ADDITIONAL ABLATION STUDIES This section includes ablation studies on additional hyperparameters for the RETSim model , includ- ing the loss function , pooling type , and model capacity . α β 2 2 2 2 4 4 4 4 20 20 40 40 20 20 40 40 λ 0.5 1 0.5 1 0.5 1 0.5 1 Recall @ 1 0.982 0.948 0.984 0.919 0.982 0.947 0.986 0.923 Table 13 : Ablation study on Multi-Similarity Loss hyperparameters for RETSim training . Bold indicates the hyperparameter setting selected for the final model . # Blocks Hidden Dim Recall @ 1 2 2 2 2 3 3 3 3 4 4 4 4 64 128 256 512 64 128 256 512 64 128 256 512 0.965 0.980 0.986 0.986 0.962 0.980 0.984 0.987 0.966 0.980 0.985 0.986 Table 14 : Ablation study for RETSim model capacity and size ( number of GAU blocks and hidden dimension for the blocks ) . Bold indicates the hyperparameter setting selected for the final model . Pooling Type Recall @ 1 Average Pooling Max Pooling Generalized Mean Pooling 0.985 0.983 0.986 Table 15 : Ablation study on pooling type for the RETSim model . Bold indicates the hyperparameter setting selected for the final model . A.6 SELECTED EXAMPLES FROM NEWS-COPY DATASET In this section , we randomly selected a set of false positives and false negatives for RETSim on the NEWS-COPY deduplication dataset to provide further insight into the results . 17 Preprint Text 1 chauffeur , a policeman and a passing jour- nalist who tried to intervene . Beaton and the policeman were reported in serious condition . The 23-year-old princess and her husband of five months , Capt . Mark Phillips , were not hurt . But police experts said the holes left by one of the bullets fired into the car indicated it passed between them , missing them by inch- es . A police informant said it was believed 11 shots were fired by the assailant . Experts were studying two revolvers found at the scene . They said fi ... By United Press tnfernational Ay SSAST OR BE FRE NG SG The federal government has proposed new methods of eoustructing federal buildings in a move to save ad- ditional en- ergy and suggested ils elfort could be adapted to all new buildings , Washington , Jan. 27 . — ( P ) —Im- mediate removal of John F. J. Her- bert , as prohibition administrator for Montana and Idaho , was de- manded in the senate today by Sen- ators Borah , Idaho , and Whe´eeler , Montana , on the ground of charges placed before them by department of justice investigators . Wheeler accompanied his demand ( Continued on Page 2 ) By RAYMOND CLAPPEA ( Dnited Presa Stal Correspandoayy London , Jai , 38— ( UP—-The Am ‘ erlcnn delegation to the navat confer ence today won ls demand for pre- sentation : of the cnse of suxiliary warships limitation first at tho noxt plenary session Thuvaday , ‘ Tho chlet delegates , mec- tittg at St. James palace , also decided that tho plenary sesslon would discuss the Main con- ference questions in alpha betical order of ihe countriea pro- posing . Press ta be Admitted The American delegation woo a second vic- tory whe ... Text 2 ‘ LONDON ( AP ) — Ian Ball , a 26-year- old unemployed Englishman , was brought into court today and charged with attempted mur- der during an at- tempt to kidnap Princess Anne from her car in the heart of London Wed- nesday night . Ball , lean-faced and bearded , stood stiffly in the dock at the Bow Street Magistrate ’ s court , handcuffed to two detectives . He spoke only once during his 60- second appearance , saying iha London accent : “ I want to apply for legal aid. ” The court or- dered him held for another hearing on Ma ... Hy United Press International The federal government has Proposed new methods of constructing federal buildings in a move lo save addilional energy and suggested ils effort could be adapted to all new buildings , Arthur F , Sampson , General Services Administration ad- ministrater , said new features for such construction would include the collection of rain waler for cooling and irriga- tion , solar energy collectors and the covering of exterior walls with earth . “ Whal we are saying is that these design criteri ... — Washington , Jan. 27 1 AP ) .—Immiedl- aie mmoval of John F. Herbert as pro- — hi- bition administrator for Montana and ‘ Idaho was demanded m the Seuate to- ‘ day by Sen- ators Borah . idaho , and Waeeler , Montana . on the ground of charges placed before them by Depart- meat of Justice investigators . Wheeler accompanied his demand nith a declaration that prohibition en- foreemen : had brukea down . He blamed the “ politicians ” and called upon the Law Enforcement Commussion to sum- mon members of the Republican Na- tona ... London , Jan. 24 , W.P—The Amer- jean dele- gation fo the naval cen- ference teday won its demand for presentation of the case of auxil- jary warships linsitation flrst at the next ple- trary session ‘ Vhursday , The chief delegates , meeting at Si , James Pelace , also decided that the plenary session would discuss the main confeyence questions in alphabetical order af the cauntries proposing . The American del- egation won a second victory when it was decided to udmil certain representatives of the press at fie plenary ... Table 16 : Example false negatives for RETSim on the NEWS-COPY dataset ( pairs of texts not detected as near-duplicates by RETSim but labeled as near-duplicates in the original dataset ) . Ex- amples are randomly selected and truncated at 512 characters for display . 18 Preprint Text 1 BOZEMAN , Mont . ( AP ) — Chet Huntley , whose resonant voice and rough-hewn face be- came familiar to millions on the nightly television news , died Wednesday in his moun- tain resort home . He was 62 . He underwent surgery for lung cancer in January but had remained activesuntil recent weeks . He died at 2:20 a.m , according to his widow , Tippy Hunt.cy . Huntiey was teamed for 14 years with David Brinkley on NBC ’ s Huntley- Brinkley Re- port . He quit in 1970 and re- turned to his native Montana to develop the $ 20-millio ... By THE ASSOCIATED PRESS Some Amer- icans are paying up to 50 per cent more per month for electricity this year than they did last , an Associ- ated Press survey shows . Con- sumers are beginning to organize to fight the rate hikes . A spot check of monthly elec- tric bills this year and last showed that most in- creases have been about $ 1 or $ 2 , gen- erally about 10 per cent , with the highest reported boost com- ing in Jacksonville , Fia. , where the average tab went from $ 17.90 last year to $ 27.70 this year . Utility ... BOZEMAN , Mont . ( AP ) — Vice President Gerald R. Ford says the world will miss the “ ‘ unique abilities ” of former television news anchorman Chet Huntley . Huntley , 62 , died at his home Wednesday after a long bout with lung cancer . Family ‘ spokesmen said a memorial service would be conducted for Huntley Sunday at the Big Sky of Montana ’ resort and recreation area south of Bozeman . Huntley was chairman of the Big Sky board of directors . Another memorial service is scheduled Tuesday in the New York studios of the ... WASHINGTON ( AP ) — The House has passed legislation raising the minimum wage from $ 1.60 an hour to $ 2 this year for most workers covered and to $ 2.30 for all by 1978 . The bill , approved Wednesday 375 to 37 , also would increase by 7 million to 56.5 million the number of workers covered by the mini- mum wage laws . The bill is a modified ver- sion of one President Nixon vetoed last year . However , he is expected to sign this one if it is finally approved after ad- justment with a similar Senate passed measure , altho ... Text 2 BOZEMAN , Mont . ( AP ) - Chet Huntley , whose resonant voice and rough-hewn face became familiar to millions on the nightly television news , died Wednesday in his moun- tain resort home . He was 62 . He underwent surgery for lung cancer in January but had remained active until recent weeks . He died at 2:20 a.m. , according to his widow , Tippy Huntley . Huntley was teamed for 14 years with David Brinkley on NBC ’ s Huntley- Brinkley Report . He quit in 1970 and returned to his native Montana to develop the $ 20 mil- lion Bi ... By Louise Cook Acenciaiod Prece Writer Same Americans are paying up io 20 per cent more per month far electricity this year ihan they did last , an -Associ- Press survey shows . onsumers are beginning to ze to fight the rate hikes , A spot check of monthly elec- tre hills this year and Jast showed that most increases ve been about $ 1 or $ 2 , gen- erally about 10 per cent , with the highest reported boost com- ing in Jacksonville , Fla. , where the average tab went from $ 17.90 last year to $ 27.70 this year ... BOZEMAN , Mont . ( AP ) — Vice President Gerald R. Ford says the world will miss the “ unique abilities ” of former television news anchorman Chet Huntley . Huntley , 62 , died at his home Wednesday after a long bout with lung cancer . Family spokesmen said a me- morial service would be con- ducted for Hunt- ley Sunday at the Big- Sky of Montana resort and recreation area south of Bozeman . Hunt- ley was chair- man of the Big Sky board of directors . Another memorial service is sched- uled Tuesday in the New York studios of ... WASHINGTON ( AP ) — The House has passed legislation raising the minimum wage from $ 1.60 an hour to $ 2 this year for most workers covered and to $ 2.30 for all by 1978 . The bill , approved Wednes- day 375 to 37 , also would in- crease by 7 million to 56.5 mil- lion the number of workers cov- ered by the minimum wage laws . The bill is a modified version of one President Nixon vetoed last year . However , he is ex- ted to sign this one if it is inally approved after adjust- ment with a similar Senate- passed measu ... Table 17 : Example false positives for RETSim on the NEWS-COPY dataset ( pairs of texts detected as near-duplicates by RETSim but not labeled as near-duplicates in the original dataset ) . Examples are randomly selected and truncated at 512 characters for display . 19","['l', 'c', 'c', 'r', 'preprint', 'retsim', 'resilient', 'efficient', 'text', 'similarity', 'tanay', 'vakharia1', 'paper', 'introduce', 'retsim', 'resilient', 'efficient', 'text', 'similarity', 'lightweight', 'multilingual', 'deep', 'learning', 'model', 'train', 'produce', 'robust', 'metric', 'embedding', 'nearduplicate', 'text', 'retrieval', 'clustering', 'dataset', 'deduplication', 'task', 'demonstrate', 'retsim', 'significantly', 'robust', 'accurate', 'minhash', 'neural', 'text', 'embedding', 'achieve', 'new', 'stateoftheart', 'mance', 'dataset', 'deduplication', 'adversarial', 'text', 'retrieval', 'benchmark', 'spam', 'clustering', 'task', 'also', 'introduce', 'benchmark', 'wiki40b', 'sarial', 'neart3xt', 'dataset', 'evaluate', 'multilingual', 'nearduplicate', 'text', 'retrieval', 'capability', 'adversarial', 'setting', 'retsim', 'benchmark', 'opensource', 'mit', 'license', 'githubcomgoogleunisim', 'introduction', 'robust', 'nearduplicate', 'text', 'detection', 'essential', 'component', 'many', 'task', 'include', 'retriev', 'ing', 'document', 'detect', 'plagiarism', 'sun', 'block', 'adversarial', 'spam', 'cam', 'paign', 'ahme', 'user', 'come', 'expect', 'system', 'return', 'accurate', 'result', 'query', 'exhibit', 'rate', 'hagen', 'furthermore', 'ciently', 'deduplicate', 'text', 'dataset', 'critical', 'training', 'stateoftheart', 'large', 'language', 'model', 'kandpal', 'decade', 'minhashbase', 'broder', 'localitysensitive', 'hashing', 'lsh', 'prevalent', 'use', 'nearduplicate', 'detection', 'simplicity', 'robust', 'ness', 'speed', 'example', 'vast', 'majority', 'dataset', 'deduplication', 'effort', 'still', 'rely', 'kocetkov', 'however', 'lshbase', 'technique', 'minhash', 'downside', 'chief', 'parametersensitive', 'require', 'heavy', 'tuning', 'additionally', 'minhash', 'lack', 'resilience', 'typo', 'reliance', 'ngram', 'lead', 'poor', 'performance', 'noisy', 'datum', 'vulnerability', 'hashbuste', 'attack', 'issac', 'hand', 'deep', 'learning', 'model', 'dominant', 'way', 'perform', 'vectorbase', 'semantic', 'text', 'retrieval', 'far', 'neural', 'embedding', 'able', 'consistently', 'outperform', 'minhash', 'robust', 'nearduplicate', 'detection', 'silcock', 'mostly', 'due', 'focus', 'improve', 'semantic', 'capability', 'lead', 'model', 'large', 'run', 'extremely', 'quickly', 'use', 'level', 'tokenization', 'resilient', 'typo', 'adversarial', 'attack', 'fill', 'gap', 'introduce', 'retsim', 'resilient', 'efficient', 'text', 'similarity', 'lightweight', 'mul', 'tilingual', 'deep', 'learning', 'model', 'train', 'specifically', 'produce', 'robust', 'neural', 'embedding', 'specialize', 'nearduplicate', 'detection', 'combine', 'stateoftheart', 'retvec', 'text', 'vectorizer', 'modern', 'transformer', 'block', 'large', 'typoaugmente', 'training', 'corpus', 'metric', 'learn', 'ing', 'training', 'regime', 'retsim', 'able', 'achieve', 'new', 'stateoftheart', 'performance', 'nearduplicate', 'detection', 'benchmark', 'section', 'dataset', 'deduplication', 'task', 'section', 'spam', 'clustering', 'application', 'section', 'furthermore', 'dataset', 'benchmark', 'exist', 'deduplication', 'nearduplicate', 'text', 'retrieval', 'none', 'focus', 'systematically', 'evaluate', 'nearduplicate', 'retrieval', 'mance', 'presence', 'word', 'manipulation', 'sentence', 'paragraphlevel', 'work', 'author', 'internship', 'preprint', 'tion', 'address', 'need', 'additionally', 'introduce', 'benchmark', 'wiki40b', 'sarial', 'neart3xt', 'dataset', 'enable', 'evaluation', 'algorithm', 'adversarial', 'nearduplicate', 'text', 'retrieval', 'multilingual', 'setting', 'report', 'performance', 'retsim', 'minhash', 'pop', 'ular', 'neural', 'embedding', 'universal', 'sentence', 'encoder', 'cer', 'labse', 'new', 'benchmark', 'section', 'highlight', 'uneven', 'performance', 'guage', 'type', 'adversarial', 'manipulation', 'retsim', 'model', 'benchmark', 'opensource', 'https', 'githubcomgoogleunisim', 'mit', 'license', 'relate', 'work', 'nearduplicate', 'detection', 'identify', 'noisy', 'nearduplicate', 'document', 'large', 'corpus', 'fun', 'damental', 'task', 'wide', 'range', 'application', 'detect', 'plagiarism', 'finding', 'reproduce', 'content', 'literature', 'news', 'article', 'gyawali', 'silcock', 'deduplicat', 'ing', 'training', 'dataset', 'language', 'model', 'previous', 'research', 'show', 'duplicate', 'training', 'dataset', 'lead', 'inefficient', 'training', 'privacy', 'concern', 'large', 'language', 'model', 'llm', 'model', 'memorize', 'regenerate', 'duplicate', 'training', 'sequence', 'much', 'high', 'frequency', 'kandpal', 'semantic', 'text', 'similarity', 'task', 'identify', 'textual', 'nearduplicate', 'predominate', 'nonneural', 'ngrambase', 'algorithm', 'minhash', 'broder', 'widely', 'use', 'technique', 'deduplicate', 'large', 'training', 'corpus', 'kocetkov', 'minhash', 'technique', 'estimate', 'jaccard', 'similarity', 'set', 'algorithm', 'minhash', 'simhash', 'charikar', 'combine', 'localitysensitive', 'hashing', 'lsh', 'technique', 'fast', 'approximate', 'near', 'neighbor', 'search', 'datum', 'cluster', 'allow', 'scale', 'deduplicate', 'corpus', 'contain', 'terabyte', 'datum', 'c4', 'stack', 'kocetkov', 'however', 'ngram', 'shinglingbase', 'technique', 'typically', 'require', 'text', 'parse', 'standardized', 'form', 'eg', 'lowercase', 'strip', 'punctuation', 'make', 'susceptible', 'typo', 'adversarial', 'attack', 'pose', 'challenge', 'attempt', 'differentiate', 'dissimilar', 'document', 'nearduplicate', 'document', 'adversarial', 'augmentation', 'semantic', 'text', 'similarity', 'task', 'compute', 'semantic', 'similarity', 'text', 'closely', 'late', 'nearduplicate', 'detection', 'semantic', 'text', 'similarity', 'refer', 'assessment', 'semantic', 'relatedness', 'piece', 'text', 'base', 'meaning', 'rather', 'syntactic', 'structure', 'case', 'nearduplicate', 'detection', 'recently', 'transformerbase', 'language', 'model', 'universal', 'sentence', 'encoder', 'labse', 'llmbase', 'embedding', 'anil', 'embed', 'text', 'highdimensional', 'embed', 'vector', 'successfully', 'use', 'retrieve', 'semanticallyrelate', 'document', 'use', 'cosine', 'similarity', 'modern', 'text', 'retrieval', 'sys', 'tem', 'combine', 'embedding', 'approximate', 'near', 'search', 'efficiently', 'retrieve', 'document', 'match', 'user', 'query', 'however', 'language', 'model', 'show', 'vulnerable', 'adversarial', 'attack', 'naturally', 'occur', 'typo', 'gao', 'furthermore', 'language', 'model', 'typically', 'large', 'costly', 'run', 'even', 'hardware', 'acceleration', 'make', 'unsuited', 'largescale', 'dataset', 'deduplication', 'identify', 'nearduplicate', 'presence', 'typo', 'adversarial', 'text', 'manipulation', 'metric', 'learn', 'metric', 'learning', 'aim', 'learn', 'embed', 'space', 'similar', 'item', 'small', 'distance', 'embedding', 'dissimilar', 'item', 'far', 'away', 'many', 'stateof', 'theart', 'embedding', 'use', 'metric', 'learning', 'unsupervised', 'training', 'finetuning', 'include', 'sentence', 'reimer', 'gurevych', 'retvec', 'resilient', 'multilingual', 'embedding', 'text', 'vectorizer', 'train', 'robust', 'various', 'form', 'characterlevel', 'typo', 'adversarial', 'attack', 'extend', 'retvec', 'training', 'regime', 'full', 'text', 'document', 'retsim', 'use', 'multisimilarity', 'loss', 'pairbase', 'metric', 'learning', 'typoladen', 'nearduplicate', 'version', 'text', 'train', 'close', 'embed', 'space', 'text', 'push', 'far', 'away', 'multisimilarity', 'loss', 'base', 'general', 'weighting', 'framework', 'pairbase', 'loss', 'achieve', 'stateoftheart', 'performance', 'outperform', 'alternative', 'triplet', 'loss', 'schroff', 'preprint', 'figure', 'retsim', 'model', 'architecture', 'diagram', 'retsim', 'work', 'arbitrary', 'length', 'text', 'split', 'ting', 'text', 'chunk', 'character', 'vectorization', 'phase', 'encode', 'use', 'retvec', 'character', 'vectorizer', 'retsim', 'model', 'embed', 'chunk', 'text', 'partial', 'embedding', 'combine', 'produce', 'global', 'embed', 'retsim', 'architecture', 'retsim', 'model', 'compose', 'main', 'component', 'depict', 'figure', 'characterlevel', 'vectorizer', 'split', 'input', 'text', 'chunk', 'character', 'use', 'retvec', 'chararcter', 'encoder', 'encode', 'chunk', 'result', 'batch', 'dense', 'input', 'retvec', 'character', 'vectorizer', 'encode', 'unicode', 'character', 'compact', '24bit', 'binary', 'representation', 'base', 'integer', 'codepoint', 'value', 'allow', 'vectorizer', 'encode', 'valid', 'unicode', 'character', 'support', 'language', 'furthermore', 'characterlevel', 'vectorizer', 'show', 'resilient', 'typo', 'adversarial', 'attack', 'small', 'transformer', 'model', 'use', 'compute', 'embedding', 'chunk', 'input', 'text', 'retsimpartialdup', 'use', 'embedding', 'directly', 'find', 'document', 'match', 'chunk', 'text', 'architecturally', 'model', 'consist', 'gate', 'attention', 'unit', 'gau', 'block', 'dense', 'et', 'follow', 'generalizedmean', 'pooling', 'layer', 'projection', 'layer', 'project', 'embed', 'dimension', 'l2', 'normalization', 'layer', 'model', '536k', 'parameter', 'order', 'magnitude', 'small', 'neural', 'embedding', 'table', 'l2normalization', 'allow', 'embedding', 'compare', 'use', 'cosine', 'similarity', 'discuss', 'impact', 'key', 'architecture', 'design', 'choice', 'section', 'hyperparameter', 'detail', 'provide', 'a11', 'additional', 'ablation', 'result', 'appendix', 'a5', 'embed', 'averaging', 'module', 'use', 'combine', 'partial', 'text', 'embedding', 'fulltext', 'embed', 'use', 'global', 'nearduplicate', 'match', 'retsimneardup', 'average', 'chunk', 'embedding', 'produce', 'global', 'embedding', 'standard', 'technique', 'use', 'many', 'model', 'cer', 'support', 'infinite', 'length', 'input', 'costefficient', 'manner', 'experiment', 'aggregation', 'technique', 'produce', 'accurate', 'global', 'embedding', 'include', 'train', 'deep', 'averaging', 'network', 'iyyer', 'improve', 'performance', 'result', 'high', 'computation', 'cost', 'retsimneardup', 'retsimpartialdup', 'compute', 'single', 'forward', 'pass', 'make', 'computationally', 'efficient', 'output', 'type', 'embedding', 'different', 'application', 'retsimneardup', 'bettersuite', 'fulltext', 'matching', 'retrieval', 'section', 'retsimpartialdup', 'use', 'find', 'partial', 'text', 'match', 'nearduplicate', 'content', 'appear', 'part', 'document', 'section', 'model', 'training', 'dataset', 'use', 'multilingual', 'c4', 'dataset', 'mc4', 'raw', 'text', 'datum', 'follow', 'xue', 'use', 'language', 'sample', 'exponent', 'balance', 'sampling', 'low', 'highresource', 'language', 'use', 'text', 'contain', 'least', 'character', 'randomly', 'select', 'sentence', 'roughly', 'character', 'text', 'chunk', 'example', 'input', 'text', 'vector', 'chunk', 'vector', 'chunk', 'vectorized', 'text', 'numchunk', 'retsim', 'model', 'retsim', 'partialdup', 'numchunk', 'retsim', 'neardup', 'r', 'e', 'c', 'r', 'h', 'c', 'e', 'r', 'g', 'n', 'l', 'p', 'l', 'r', 'p', 'e', 'l', 'r', 'r', 'e', 'g', 'e', 'preprint', 'training', 'dataset', 'generate', 'pair', 'augment', 'example', 'apply', 'level', 'augmentation', 'example', 'text', 'chunk', 'order', 'sentencelevel', 'wordlevel', 'characterlevel', 'level', 'randomly', 'select', 'augmentation', 'apply', 'follow', 'category', 'insertion', 'deletion', 'substitution', 'transposition', 'randomly', 'apply', 'sentencelevel', 'mentation', 'combine', 'character', 'wordlevel', 'augmentation', 'empirically', 'find', 'increase', 'percentage', 'augmentation', 'point', 'cause', 'retsim', 'performance', 'degrade', 'full', 'list', 'augmentation', 'use', 'find', 'a2', 'training', 'procedure', 'train', 'retsim', 'use', 'multisimilarity', 'loss', 'β', 'ϵ', 'hypertune', 'parameter', 'result', 'show', 'appendix', 'train', 'step', 'batch', 'size', 'similarity', 'loss', 'train', 'model', 'embed', 'augment', 'version', 'text', 'close', 'embed', 'space', 'dissimilar', 'text', 'push', 'far', 'apart', 'use', 'optimizer', 'learning', 'rate', 'cosine', 'decay', 'detailed', 'training', 'hyperparameter', 'report', 'a12', 'evaluation', 'modelalgorithm', 'type', 'embedhash', 'size', 'model', 'parameter', 'labse', 'multilingual', 'use', 'multilingual', 'e5base', 'palm', 'simhash', 'minhash', 'retsim', 'neural', 'neural', 'neural', 'neural', 'hash', 'hash', 'neural', 'b', 'bit', 'hash', 'table', 'embed', 'model', 'hash', 'algorithm', 'benchmarke', 'paper', 'model', 'algorithm', 'evaluate', 'benchmark', 'retsim', 'multilingual', 'semantic', 'text', 'embedding', 'well', 'popular', 'gram', 'base', 'algorithm', 'primarily', 'use', 'nearduplicate', 'text', 'detection', 'table', 'baseline', 'text', 'embedding', 'include', 'multilingual', 'universal', 'sentence', 'encoder', 'labse', 'multilingual', 'palm', 'gecko', 'embedding', 'anil', 'text', 'embedding', 'l2normalize', 'compare', 'use', 'cosine', 'similarity', 'use', 'exact', 'search', 'index', 'retrieve', 'near', 'neighbor', 'vector', 'index', 'experiment', 'section', 'nonneural', 'nearduplicate', 'detection', 'cluster', 'algorithm', 'select', 'popular', 'algorithm', 'minhash', 'broder', 'simhash', 'charikar', 'minhash', 'use', 'datasketch', 'minhashlsh', 'library', 'follow', 'common', 'practice', 'literature', 'silcock', 'use', 'hash', 'function', 'minhash', 'otherwise', 'specify', 'use', 'wordlevel', 'ngram', 'select', 'good', 'value', 'n', 'simhash', 'use', 'bit', 'simhash', 'conduct', 'shingle', 'character', 'level', 'shingle', 'size', 'select', 'n', 'nearduplicate', 'detection', 'benchmark', 'newscopy', 'core', 'duplicate', 'dataset', 'tune', 'optimal', 'deduplication', 'threshold', 'base', 'cosine', 'similarity', 'neuralbased', 'embedding', 'jaccard', 'similarity', 'minhash', 'detailed', 'hyperparameter', 'setting', 'retsim', 'baseline', 'algorithm', 'use', 'evaluation', 'find', 'w4nt3d', 'wiki40b', 'neart3xt', 'dataset', 'evaluation', 'dataset', 'description', 'vast', 'majority', 'text', 'retrieval', 'benchmark', 'focus', 'evaluate', 'semantic', 'performance', 'good', 'knowledge', 'multilingual', 'benchmark', 'sys', 'tematically', 'measure', 'adversarial', 'robustness', 'nearduplicate', 'text', 'retrieval', 'attempt', 'fill', 'gap', 'create', 'publish', 'benchmark', 'wiki40b', 'dataset', 'contain', 'pair', 'syntactically', 'similar', 'text', 'evaluate', 'nearduplicate', 'text', 'trieval', 'presence', 'various', 'form', 'text', 'manipulation', 'typo', 'w4nt3d', 'base', 'wiki40b', 'dataset', 'guo', 'dataset', 'split', 'query', 'exam', 'ple', 'target', 'example', 'query', 'example', 'syntheticallymodifie', 'nearduplicate', 'version', 'preprint', 'target', 'example', 'eg', 'typo', 'language', 'split', 'wiki40b', 'randomly', 'select', 'text', 'length', 'target', 'string', 'uniformly', 'select', 'character', 'order', 'test', 'performance', 'short', 'long', 'text', 'construct', 'query', 'text', 'corre', 'sponde', 'target', 'text', 'randomly', 'apply', 'word', 'character', 'augmentation', 'sentence', 'paragraph', 'augmentation', 'augmentation', 'uniformly', 'select', 'insert', 'delete', 'substitute', 'swap', 'operation', 'use', 'main', 'metric', 'follow', 'setup', 'commonly', 'find', 'semantic', 'text', 'retrieval', 'benchmark', 'modelalgorithm', 'arabic', 'chinese', 'english', 'german', 'french', 'spanish', 'japanese', 'korean', 'russian', 'avg', 'lang', 'labse', 'multilingual', 'use', 'multilingual', 'e5base', 'palm', 'simhash', 'minhash', 'retsimpartialdup', 'retsimneardup', 'table', 'perlanguage', 'retrieval', 'performance', 'various', 'embed', 'model', 'algorithm', 'benchmark', 'result', 'select', 'language', 'report', 'average', 'recall', 'language', 'full', 'result', 'language', 'report', 'multilingual', 'performance', 'overall', 'retsimneardup', 'achieve', 'average', 'recall', 'language', 'benchmark', 'table', 'retsimpartialdup', 'second', 'good', 'recall', 'multilingual', 'bestperforme', 'baseline', 'third', 'aver', 'age', 'recall', 'expect', 'retsimneardup', 'outperform', 'retsimpartialdup', 'benchmark', 'require', 'find', 'nearduplicate', 'find', 'simi', 'lar', 'text', 'retsimpartialdup', 'optimize', 'find', 'similar', 'chunk', 'text', 'corpus', 'always', 'similar', 'text', 'overall', 'similarly', 'hypothesize', 'minhash', 'simhash', 'form', 'poorly', 'benchmark', 'lack', 'ability', 'distinguish', 'similar', 'text', 'nearduplicate', 'detect', 'embeddingbase', 'model', 'cosine', 'similarity', 'offer', 'finegrained', 'measure', 'similarity', 'retsimneardup', 'outperform', 'baseline', 'algorithm', 'language', 'chinese', 'language', 'theorize', 'semantic', 'embedding', 'slight', 'edge', 'performance', 'significantly', 'large', 'model', 'size', 'large', 'retsim', 'show', 'ble', 'allow', 'well', 'representation', 'language', 'large', 'character', 'set', 'furthermore', 'level', 'tokenizer', 'use', 'baseline', 'embedding', 'often', 'treat', 'character', 'chinese', 'japanese', 'individual', 'token', 'offer', 'high', 'resilience', 'figure', 'recall', 'performance', 'benchmark', 'break', 'augmentation', 'type', 'result', 'average', 'language', 'split', 'w4nt3d', 'adversarial', 'resilience', 'delve', 'deeply', 'impact', 'various', 'type', 'text', 'manipulation', 'veal', 'retsimneardup', 'retsimpartialdup', 'perform', 'almost', 'equally', 'regardless', 'type', 'augmentation', 'apply', 'figure', 'semantic', 'text', 'embedding', 'perform', 'well', 'paragraph', 'sentence', 'wordlevel', 'manipulation', 'expect', 'exhibit', 'significantly', 'weak', 'performance', 'characterlevel', 'minhash', 'simhash', 'struggle', 'wordlevel', 'augmentation', 'deeplearne', 'base', 'embedding', 'collapse', 'characterlevel', 'typo', 'introduce', 'l', 'c', 'e', 'r', 'paragraph', 'sentence', 'word', 'character', 'augmentation', 'level', 'labse', 'multilingual', 'use', 'multilingual', 'e5base', 'palm', 'simhash', 'minhash', 'retsim', 'partialdup', 'retsim', 'neardup', 'preprint', 'tribute', 'retsim', 'resilience', 'adversarial', 'manipulation', 'retvec', 'character', 'encoder', 'well', 'use', 'deep', 'metric', 'learning', 'train', 'robust', 'embedding', 'figure', 'report', 'recall', 'performance', 'algorithm', 'amount', 'augmentation', 'crease', 'algorithm', 'perform', 'perfectly', 'augmentation', 'apply', 'exact', 'matching', 'percentage', 'augmentation', 'increase', 'ngram', 'base', 'approach', 'exhibit', 'steep', 'drop', 'mance', 'semantic', 'text', 'embedding', 'able', 'sustain', 'large', 'degree', 'augmentation', 'retrieval', 'capability', 'start', 'degrade', 'retsimneardup', 'robust', 'noticeable', 'drop', 'performance', 'around', 'augmentation', 'make', 'retsim', 'effective', 'approach', 'cluster', 'deduplicate', 'text', 'adversarial', 'setting', 'figure', 'recall', 'performance', 'benchmark', 'vary', 'max', 'target', 'length', 'figure', 'recall', 'performance', 'benchmark', 'amount', 'augmenta', 'tion', 'apply', 'query', 'text', 'increase', 'text', 'length', 'impact', 'performance', 'figure', 'report', 'recall', 'performance', 'retsim', 'baseline', 'algorithm', 'length', 'query', 'target', 'text', 'vary', 'see', 'retsimneardup', 'retsimpartialdup', 'outperform', 'method', 'short', 'text', 'character', 'text', 'length', 'increase', 'character', 'retsimneardup', 'remain', 'close', 'perfect', 'retsimpartialdup', 'performance', 'degrade', 'split', 'text', 'multiple', 'embedding', 'find', 'near', 'matching', 'chunk', 'text', 'minhash', 'simhash', 'also', 'perform', 'poorly', 'short', 'text', 'length', 'start', 'degrade', 'long', 'text', 'neuralbased', 'embedding', 'observe', 'slight', 'drop', 'performance', 'long', 'text', 'model', 'retsimneardup', 'multilingual', 'use', 'embedding', 'handle', 'arbitrary', 'length', 'input', 'realworld', 'nearduplicate', 'evaluation', 'setup', 'benchmark', 'retsim', 'ability', 'identify', 'nearduplicate', 'content', 'realworld', 'dataset', 'literature', 'newscopy', 'deduplication', 'dataset', 'silcock', 'contain', 'historical', 'news', 'article', 'positive', 'duplicate', 'pair', 'dataset', 'consist', 'noisy', 'near', 'duplicate', 'due', 'factor', 'ocr', 'error', 'plagiarism', 'news', 'aggregation', 'also', 'evaluate', 'algorithm', 'core', 'nearduplicate', 'dataset', 'gyawali', 'consist', 'scholarly', 'article', '25k', 'exact', 'duplicate', '25k', 'nearduplicate', 'nonduplicate', 'duplicate', 'dataset', 'arise', 'article', 'revision', 'versioning', 'metadata', 'difference', 'man', 'key', 'difference', 'benchmark', 'benchmark', 'benchmark', 'focus', 'detect', 'cluster', 'nearduplicate', 'text', 'rather', 'robust', 'text', 'retrieval', 'base', 'syntactic', 'similarity', 'benchmark', 'follow', 'experimental', 'setup', 'provide', 'paper', 'report', 'adjust', 'index', 'ari', 'newscopy', 'dataset', 'report', 'precisionrecallf1', 'score', 'core', 'nearduplicate', 'dataset', 'result', 'newscopy', 'dataset', 'retsimpartialdup', 'outperform', 'approach', 'significant', 'margin', 'ari', 'compare', 'good', 'minhash', 'result', 'report', 'table', 'dataset', 'many', 'nearduplicate', 'pair', 'text', 'significantly', 'long', 'expect', 'retsimpartialdup', 'find', 'match', 'text', 'chunk', 'document', 'suited', 'task', 'outperform', 'retsimneardup', 'bucket', 'nearduplicate', 'detection', 'rate', 'length', 'ratio', 'positive', 'pair', 'figure', 'observe', 'retsimpartialdup', 'outperform', 'minhash', 'regardless', 'length', 'ratio', 'minhash', 'surpasse', 'retsimneardup', 'formance', 'text', 'roughly', '15x', 'length', 'text', 'nearduplicate', 'pair', 'l', 'c', 'e', 'r', 'target', 'text', 'length', 'c', 'e', 'r', 'text', 'augmentation', 'amount', 'labse', 'multilingual', 'use', 'multilingual', 'e5base', 'palm', 'simhash', 'minhash', 'retsim', 'partialdup', 'retsim', 'neardup', 'preprint', 'modelalgorithm', 'multilingual', 'use', 'multilingual', 'e5base', 'sbert', 'simhash', 'minhash', 'minhash', 'retsimpartialdup', 'retsimneardup', 'table', 'performance', 'comparison', 'newscopy', 'dataset', 'adjust', 'rand', 'index', 'value', 'report', 'denote', 'result', 'figure', 'nearduplicate', 'detection', 'rate', 'sim', 'minhash', 'different', 'length', 'ratio', 'pos', 'itive', 'pair', 'length', 'long', 'divide', 'short', 'text', 'round', 'near', 'integer', 'additionally', 'notice', 'label', 'dataset', 'occasionally', 'noisy', 'substantial', 'por', 'tion', 'retsim', 'false', 'positive', 'appear', 'nearduplicate', 'inspection', 'core', 'nearduplicate', 'dataset', 'table', 'document', 'article', 'title', 'abstract', 'roughly', 'size', 'retsimpartialdup', 'retsimneardup', 'performance', 'roughly', 'equivalent', 'method', 'outperform', 'baseline', 'term', 'macro', 'f1', 'score', 'accuracy', 'use', 'minhash', 'lsh', 'hash', 'function', 'computational', 'efficiency', 'recommend', 'datasketch', 'library1', 'well', 'accuracy', 'default', 'set', 'deduplication', 'threshold', 'detail', 'hyperpa', 'rameter', 'setting', 'algorithm', 'nearduplication', 'dataset', 'find', 'model', 'exact', 'title', 'match', 'labse', 'multilingual', 'use', 'multilingual', 'e5base', 'minhash', 'lsh', 'retsimpartialdup', 'retsimneardup', 'precision', 'duplicate', 'duplicate', 'precision', 'accuracy', 'table', 'evaluation', 'result', 'core', 'nearduplicate', 'dataset', 'precisionrecallmacro', 'f1', 'accuracy', 'number', 'report', 'denote', 'result', 'application', 'training', 'dataset', 'deduplication', 'modelalgorithm', 'train', 'example', 'dup', 'train', 'valid', 'example', 'dup', 'train', 'minhash', 'lsh', 'exact', 'substre', 'retsimneardup', 'retsimpartialdup', 'table', 'deduplication', 'rate', 'wiki40b', 'english', 'denote', 'result', 'setup', 'evaluate', 'retsim', 'ability', 'deduplicate', 'text', 'training', 'dataset', 'deduplicate', 'english', 'split', 'wiki40b', 'conservatively', 'set', 'cosine', 'similarity', 'deduplica', 'tion', 'threshold', 'retsimneardup', 'retsimpartialdup', 'limit', 'amount', 'false', 'positive', 'base', 'optimal', 'threshold', 'find', 'evaluation', 'use', 'use', 'arch', 'default', 'vector', 'index', 'approximate', 'near', 'neighbor', 'search', 'vardanian', 'compare', 'big', 'datum', 'look', 'small', 'https', 'githubcomekzhudatasketch', 'e', 'r', 'c', 'e', 'c', 'l', 'r', 'e', 'length', 'ratio', 'nearduplicate', 'text', 'pair', 'retsim', 'partialdup', 'retsim', 'neardup', 'minhash', 'preprint', 'modelalgorithm', 'accelerator', 'batch', 'size', 'embed', 'hash', 'time', 'minhash', 'lsh', 'retsim', 'retsim', 'retsim', 'cpu', 'core', 'onnx', 'cpu', 'core', 'tensorflow', 'rtx', 'tensorflow', 'table', 'embeddinghashe', 'speed', 'retsim', 'minhash', 'lsh', 'wiki40b', 'dataset', 'minhash', 'lsh', 'set', 'number', 'hash', 'function', 'follow', 'kocetkov', 'use', 'jaccard', 'similarity', 'threshold', 'deduplication', 'result', 'overall', 'report', 'table', 'retsimneardup', 'find', 'slightly', 'duplicate', 'wiki', '40b', 'training', 'validation', 'split', 'inline', 'deduplication', 'result', 'section', 'retsimneardup', 'outperform', 'algorithm', 'hand', 'retsimpartialdup', 'find', 'signifi', 'cantly', 'match', 'exact', 'substring', 'match', 'use', 'previous', 'study', 'showcase', 'usefulness', 'perform', 'nearduplicate', 'partialduplicate', 'matching', 'largerthanexpected', 'number', 'partial', 'match', 'indicate', 'machine', 'learn', 'ing', 'practitioner', 'take', 'extra', 'care', 'deduplicate', 'wikipedia', 'chunk', 'level', 'avoid', 'feed', 'duplicate', 'text', 'model', 'term', 'embed', 'speed', 'table', 'retsim', 'significantly', 'slow', 'minhash', 'lsh', 'cpu', '46x', 'slow', 'competitive', 'use', 'desktop', 'gpu', 'rtx', 'slow', 'almost', 'onpar', 'use', 'highend', 'nvidia', '15x', 'slow', 'current', 'code', 'write', 'python', 'fully', 'optimize', 'expect', 'performance', 'gap', 'significantly', 'shrink', 'optimize', 'implementation', 'retsim', 'slow', 'minhash', 'retsim', 'significantly', 'small', 'fast', 'text', 'embed', 'model', 'close', 'performance', 'gap', 'neural', 'nonneural', 'base', 'method', 'nearduplicate', 'text', 'detection', 'dataset', 'deduplication', 'retsimneardup', 'retsimpartialdup', 'return', 'time', 'embed', 'speed', 'indexing', 'retrieval', 'time', 'depend', 'vector', 'index', 'search', 'use', 'long', 'document', 'retsimpartialdup', 'produce', 'embedding', 'retsimneardup', 'retsimpartialdup', 'offer', 'tradeoff', 'finergrained', 'match', 'depend', 'specific', 'vector', 'search', 'dataset', 'use', 'wild', 'spam', 'email', 'cluster', 'section', 'showcase', 'realworld', 'performance', 'cluster', 'nearduplicate', 'text', 'heavily', 'manipulate', 'adversarial', 'attack', 'perform', 'evaluation', 'spam', 'campaign', 'spam', 'constitute', 'strong', 'proving', 'ground', 'nearduplicate', 'cluster', 'algorithm', 'spammer', 'employ', 'adversarial', 'augmentation', 'technique', 'attempt', 'evade', 'detection', 'mentation', 'typically', 'include', 'append', 'prepende', 'unrelated', 'text', 'interleave', 'random', 'word', 'different', 'language', 'intentionally', 'introduce', 'typo', 'abuse', 'extended', 'character', 'set', 'emojis', 'homoglyphs', 'technique', 'collectively', 'refer', 'hashbuste', 'setup', 'dataset', 'consist', 'spam', 'email', 'spam', 'campaign', 'donate', 'gmail', 'user', 'flag', 'reach', 'inboxe', 'example', 'contain', 'email', 'subject', 'concatenate', 'message', 'content', 'email', 'misclassifie', 'spam', 'classifier', 'effective', 'adversarial', 'text', 'manipulation', 'technique', 'make', 'challenging', 'test', 'set', 'cluster', 'evaluation', 'example', 'hashbuste', 'attack', 'adversarial', 'manipulation', 'observe', 'include', 'use', 'homoglpyphs', 'uncommon', 'unicode', 'character', 'set', 'invisible', 'character', 'pad', 'random', 'word', 'different', 'language', 'get', 'ground', 'truth', 'campaign', 'cluster', 'email', 'manually', 'review', 'assign', 'specific', 'spam', 'campaign', 'base', 'similarity', 'human', 'reviewer', 'use', 'agglomerative', 'clustering', 'cluster', 'spam', 'email', 'report', 'homogeneity', 'completeness', 'vmeasure', 'adjust', 'rand', 'index', 'metric', 'result', 'overall', 'observe', 'retsim', 'significantly', 'well', 'cluster', 'nearduplicate', 'adversarial', 'manipulation', 'outperform', 'simhash', 'use', 'metric', 'consider', 'table', 'particular', 'observe', 'retsim', 'outperform', 'use', 'vmeasure', 'score', 'main', 'metric', 'result', 'report', 'section', 'inline', 'observe', 'deploy', 'retsim', 'main', 'nearduplicate', 'detection', 'preprint', 'model', 'vmeasure', 'ari', 'use', 'simhash', 'lsh', 'retsimneardup', 'table', 'performance', 'cluster', 'adversarial', 'spam', 'campaign', 'practice', 'ablation', 'study', 'setup', 'section', 'summarize', 'key', 'ablation', 'study', 'perform', 'design', 'ret', 'sim', 'model', 'use', 'section', 'train', 'use', 'setup', 'detail', 'a12', 'ex', 'cept', 'train', 'step', 'reduce', 'computational', 'cost', 'evaluate', 'retsimneardup', 'performance', 'model', 'subset', 'benchmark', 'randomly', 'select', 'example', 'language', 'split', 'use', 'recall', 'report', 'metric', 'block', 'type', 'recall', 'chunk', 'size', 'recall', 'embed', 'retvec', 'gau', 'table', 'retsim', 'ablation', 'study', 'result', 'architecture', 'block', 'type', 'leave', 'text', 'chunk', 'size', 'middle', 'embed', 'dimension', 'right', 'bold', 'denote', 'value', 'select', 'final', 'retsim', 'model', 'result', 'table', 'contain', 'retsim', 'ablation', 'study', 'result', 'text', 'chunk', 'size', 'architecture', 'block', 'type', 'embed', 'size', 'important', 'architectural', 'decision', 'decide', 'optimal', 'text', 'chunk', 'size', 'find', 'right', 'balance', 'small', 'size', 'possible', 'maximize', 'retsimpartialdup', 'efficiency', 'ensure', 'retsimneardup', 'fulltext', 'embedding', 'work', 'effec', 'tively', 'full', 'document', 'find', 'chunk', 'character', 'offer', 'good', 'performance', 'also', 'test', 'various', 'model', 'architecture', 'transformer', 'block', 'find', 'good', 'balance', 'efficiency', 'performance', 'find', 'modern', 'gau', 'block', 'outper', 'form', 'vanilla', 'block', 'block', 'xue', 'also', 'try', 'modern', 'architecture', 'architec', 'ture', 'propose', 'retvec', 'bad', 'gau', 'block', 'performance', 'last', 'least', 'find', 'increase', 'embed', 'size', 'dimension', 'yield', 'meaningful', 'improvement', 'retsimneardup', 'accordingly', 'opt', 'use', 'embed', 'spaceefficiency', 'maximize', 'indexing', 'query', 'speed', 'additional', 'ablation', 'study', 'hyperparameter', 'find', 'appendix', 'future', 'work', 'retsim', 'novel', 'training', 'regime', 'combine', 'metric', 'learning', 'datum', 'augmentation', 'many', 'potential', 'application', 'plan', 'explore', 'future', 'work', 'example', 'adapt', 'extend', 'train', 'robust', 'semantic', 'embedding', 'image', 'similarity', 'embedding', 'additionally', 'expect', 'general', 'model', 'become', 'big', 'expensive', 'run', 'future', 'small', 'specialize', 'model', 'retsim', 'emerge', 'efficient', 'alternative', 'wide', 'range', 'task', 'conclusion', 'paper', 'introduce', 'retsim', 'novel', 'multilingual', 'text', 'embed', 'achieve', 'state', 'oftheart', 'performance', 'nearduplicate', 'text', 'detection', 'dataset', 'deduplication', 'syntactic', 'text', 'similarity', 'benchmark', 'retsim', 'significantly', 'fast', 'previous', 'neuralbased', 'text', 'embedding', 'robust', 'ngram', 'base', 'algorithm', 'make', 'suitable', 'largescale', 'text', 'retrieval', 'dataset', 'deduplication', 'especially', 'adversarial', 'setting', 'spam', 'detection', 'furthermore', 'introduce', 'benchmark', 'first', 'multilingual', 'dataset', 'design', 'measure', 'ad', 'versarial', 'robustness', 'nearduplicate', 'text', 'detection', 'algorithm', 'opensource', 'retsim', 'benchmark', 'mit', 'license', 'preprint', 'reference', 'naeem', 'amin', 'hamza', 'koundal', 'bader', 'alouffi', 'shah', 'machine', 'learn', 'technique', 'spam', 'detection', 'email', 'iot', 'platform', 'analysis', 'research', 'challenge', 'security', 'communication', 'network', 'doi', 'ahme', 'elgohary', 'generate', 'natural', 'language', 'adversarial', 'example', 'lepikhin', 'alexandre', 'taropa', 'paige', 'bailey', 'yanping', 'meierhellstern', 'yuje', 'bury', 'pher', 'cl´ement', 'crepy', 'shachi', 'sunipa', 'dev', 'vlad', 'feinberg', 'vlad', 'fienber', 'freitag', 'xavi', 'sebastian', 'guy', 'hand', 'abe', 'maxim', 'sneha', 'music', 'hyeontaek', 'vedant', 'maysam', 'moussalem', 'nado', 'pellat', 'reiner', 'emily', 'reif', 'parker', 'brennan', 'slone', 'sohn', 'valter', 'linte', 'denny', 'slav', 'petrov', 'palm', 'technical', 'report', 'url', 'http', 'arxivorgabs230510403', 'arxiv230510403', 'cs', 'andrei', 'broder', 'minwise', 'inde', 'proceeding', 'thirtieth', 'annual', 'acm', 'symposium', 'theory', 'pendent', 'permutation', 'compute', 'pp', 'retvec', 'resilient', 'efficient', 'text', 'vectorizer', 'mario', 'guajardocespede', 'tar', 'universal', 'sentence', 'encoder', 'arxiv', 'preprint', 'similarity', 'estimation', 'technique', 'round', 'algorithm', 'proceeding', 'thiryfourth', 'annual', 'acm', 'symposium', 'theory', 'compute', 'pp', 'pretraining', 'deep', 'bidirectional', 'transformer', 'language', 'understanding', 'arxiv', 'fangxiaoyu', 'language', 'agnostic', 'sentence', 'embed', 'gao', 'qi', 'generation', 'adversarial', 'text', 'sequence', 'evade', 'deep', 'learning', 'classifier', 'arxivorgab', 'cs', 'preprint', 'mandy', 'guo', 'denny', 'alrfou', 'wiki40b', 'multilingual', 'language', 'model', 'dataset', 'proceeding', '12th', 'language', 'resource', 'evaluation', 'conference', 'pp', '2440–2452', 'deduplication', 'scholarly', 'document', 'ing', 'locality', 'sensitive', 'hashing', 'word', 'embedding', 'proceeding', 'twelfth', 'language', 'resource', 'evaluation', 'conference', 'pp', 'guage', 'isbn', 'aclanthology', 'hagen', 'rathgeber', 'large', 'proceeding', '40th', 'international', 'sigir', 'scale', 'query', 'spelling', 'conference', 'research', 'development', 'information', 'retrieval', 'pp', 'transformer', 'quality', 'linear', 'time', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'issac', 'r', 'chiong', 'analysis', 'phishe', 'attack', 'countermeasure', 'mohit', 'iyyer', 'varun', 'daum´e', 'deep', 'unordered', 'composition', 'rival', 'syntactic', 'method', 'text', 'classification', 'proceeding', '53rd', 'annual', 'meeting', 'association', 'computational', 'linguistic', '7th', 'international', 'joint', 'conference', 'natural', 'language', 'processing', 'volume', 'long', 'paper', 'pp', 'association', 'computational', 'linguistic', 'raffel', 'deduplicate', 'training', 'datum', 'mitigate', 'pri', 'vacy', 'risk', 'language', 'model', 'proceeding', '39th', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'https', 'proceeding', 'mlrpressv162kandpal22ahtml', 'dzmitry', 'bahdanau', 'harm', 'vrie', 'stack', 'tb', 'permissively', 'license', 'source', 'code', 'deduplicate', 'training', 'datum', 'make', 'language', 'model', 'well', 'arxiv210706499', 'feichtenhofer', 'trevor', 'darrell', 'saine', 'convnet', 'proceeding', 'ieeecvf', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'jin', 'qi', 'textattack', 'framework', 'adversarial', 'attack', 'datum', 'augmentation', 'adversarial', 'training', 'tober', 'url', 'tazi', 'lo¨ıc', 'magne', 'nil', 'reimer', 'massive', 'text', 'bed', 'benchmark', 'url', 'https', 'filip', 'radenovi´c', 'giorgo', 'tolia', 'chum', 'finetune', 'image', 'retrieval', 'human', 'annotation', 'ieee', 'transaction', 'pattern', 'analysis', 'machine', 'intelligence', 'publisher', 'ieee', 'nil', 'reimer', 'iryna', 'gurevych', 'sentencebert', 'sentence', 'embedding', 'use', 'siamese', 'network', 'arxiv', 'preprint', 'facenet', 'unified', 'embedding', 'face', 'recognition', 'cluster', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'cvpr', 'pp', 'url', 'arxivorgabs150303832', 'arxiv150303832', 'cs', 'preprint', 'emily', 'noiserobust', 'deduplication', 'scale', 'duplicate', 'text', 'detection', 'use', 'frequencybiase', 'signature', 'moni', 'naor', 'rangan', 'demetri', 'terzopoulo', 'moshe', 'weikum', 'ed', 'web', 'information', 'system', 'engineering', 'wise', 'volume', 'pp', 'springer', 'isbn', 'doi', 'url', 'series', 'title', 'lecture', 'note', 'computer', 'science', 'ash', 'vardanian', 'usearch', 'unum', 'url', 'binxe', 'daxin', 'rangan', 'wei', 'text', 'embedding', 'weaklysupervise', 'contrastive', 'pretraine', 'cember', 'r', 'multisimilarity', 'loss', 'general', 'pair', 'weighting', 'deep', 'metric', 'learning', 'proceeding', 'ieeecvf', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'linte', 'constant', 'raffel', 'mt5', 'massively', 'multilingual', 'pretraine', 'texttotext', 'transformer', 'arxiv', 'preprint', 'arxiv201011934', 'amin', 'law', 'sing', 'brian', 'multilingual', 'universal', 'sentence', 'encoder', 'semantic', 'retrieval', 'yang', 'je', 'srinadh', 'hsieh', 'large', 'batch', 'optimization', 'deep', 'learning', 'training', 'bert', 'minute', 'arxiv', 'preprint', 'preprint', 'a1', 'retsim', 'detail', 'a11', 'retsim', 'model', 'hyperparameter', 'full', 'list', 'retsim', 'model', 'hyperparameter', 'find', 'table', 'hyperparameter', 'input', 'length', 'chunk', 'block', 'type', 'block', 'hide', 'dim', 'expansion', 'rate', 'activation', 'function', 'attention', 'activation', 'function', 'absolute', 'positional', 'encoding', 'relative', 'positional', 'encoding', 'norm', 'type', 'pooling', 'type', 'dropout', 'rate', 'embed', 'dim', 'parameter', 'value', 'gau', 'swish', 'relu2', 'rope', 'p', 'table', 'detailed', 'retsim', 'model', 'hyperparameter', 'a12', 'retsim', 'training', 'hyperparameter', 'table', 'detail', 'hyperparameter', 'setting', 'train', 'configuration', 'loss', 'optimizer', 'use', 'train', 'retsim', 'model', 'hyperparameter', 'value', 'batch', 'size', 'train', 'step', 'learning', 'rate', 'end', 'learn', 'rate', 'learn', 'rate', 'decay', 'weight', 'decay', 'cosine', 'table', 'retsim', 'detailed', 'training', 'hyperparameter', 'a2', 'training', 'dataset', 'detail', 'provide', 'full', 'list', 'augmentation', 'use', 'generate', 'augmented', 'text', 'retsim', 'training', 'dataset', 'describe', 'section', 'sentencelevel', 'augmentation', 'deletion', 'random', 'sentence', 'deletion', 'random', 'sentence', 'truncation', 'insertion', 'preprint', 'random', 'prefix', 'sentence', 'random', 'suffix', 'sentence', 'random', 'sentence', 'insertion', 'repeat', 'sentence', 'substitution', 'lowercaseuppercase', 'sentence', 'random', 'sentence', 'substitution', 'transposition', 'neighboring', 'swap', 'wordlevel', 'augmentation', 'deletion', 'random', 'word', 'deletion', 'insertion', 'random', 'word', 'insertion', 'random', 'word', 'insertion', 'language', 'substitution', 'frequency', 'base', 'word', 'substitution', 'random', 'word', 'substitution', 'random', 'word', 'substitution', 'language', 'repeat', 'word', 'transposition', 'neighboring', 'swap', 'characterlevel', 'augmentation', 'deletion', 'random', 'character', 'deletion', 'substitution', 'case', 'substitution', 'base', 'substitution', 'n', 'qwerty', 'homoglyphs', 'substitution', 'random', 'ascii', 'substitution', 'random', 'character', 'language', 'alphabet', 'random', 'punctuation', 'substitution', 'random', 'unicode', 'character', 'substitution', 'insertion', 'character', 'repetition', 'ngram', 'base', 'insertion', 'n', 'random', 'character', 'language', 'alphabet', 'insertion', 'random', 'punctuation', 'insertion', 'random', 'unicode', 'character', 'insertion', 'transposition', 'neighboring', 'swap', 'a3', 'detailed', 'evaluation', 'hyperparameter', 'figure', 'contain', 'information', 'deduplication', 'threshold', 'value', 'hyperparameter', 'setting', 'benchmarke', 'newscopy', 'core', 'deduplication', 'dataset', 'preprint', 'model', 'type', 'threshold', 'value', 'hyperparameter', 'multilingual', 'use', 'cosine', 'similarity', 'multilingual', 'e5base', 'cosine', 'similarity', 'simhash', 'minhash', 'retsimneardup', 'retsimpartialdup', 'hamming', 'distance', 'similarity', 'cosine', 'similarity', 'cosine', 'similarity', 'bit', 'characterlevel', 'hash', 'function', 'wordlevel', 'figure', 'hyperparameter', 'setting', 'newscopy', 'dataset', 'evaluation', 'section', 'model', 'type', 'threshold', 'value', 'hyperparameter', 'cosine', 'similarity', 'labse', 'multilingual', 'use', 'cosine', 'similarity', 'multilingual', 'e5base', 'cosine', 'similarity', 'simhash', 'lsh', 'minhash', 'lsh', 'retsimneardup', 'retsimpartialdup', 'hamming', 'distance', 'similarity', 'cosine', 'similarity', 'cosine', 'similarity', 'bit', 'characterlevel', 'hash', 'function', 'wordlevel', 'figure', 'hyperparameter', 'setting', 'core', 'nearduplicate', 'dataset', 'evaluation', 'section', 'a31', 'deduplication', 'threshold', 'impact', 'figure', 'precisionrecallf1', 'score', 'different', 'cosine', 'distance', 'deduplication', 'threshold', 'retsimneardup', 'leave', 'retsimpartialdup', 'right', 'newscopy', 'dataset', 'detail', 'w4nt3d', 'benchmark', 'result', 'table', 'show', 'detailed', 'performance', 'result', 'retsim', 'baseline', 'algorithm', 'language', 'split', 'benchmark', 'preprint', 'k', 'j', 'b', 'e', 'l', 'l', 'l', 'k', 'l', 'p', 'e', 'l', 'l', 'r', 'p', 'e', 'r', 'r', 'e', 'e', 'r', 'h', 'h', 'h', 'h', 'u', 'h', 'r', 'h', 'h', 'e', 'r', 'f', 'fi', 'e', 'c', 'c', 'r', 'h', 'r', 'g', 'l', 'l', 'e', 'r', 'p', 'k', 'r', 'w', 'e', 'r', 'l', 'l', 'e', 'e', 'r', 'v', 'r', 'n', 'r', 'r', 'e', 'p', 'l', 'l', 'c', 'e', 'r', 'e', 'l', 'e', 'l', 'h', 'c', 'h', 'z', 'l', 'h', 'r', 'l', 'r', 'r', 'p', 'l', 'r', 'g', 'l', 'l', 'e', 'b', 'e', 'l', 'l', 'l', 'k', 'l', 'p', 'e', 'l', 'l', 'r', 'p', 'e', 'r', 'h', 'h', 'h', 'h', 'r', 'e', 'e', 'r', 'r', 'p', 'k', 'r', 'w', 'e', 'r', 'l', 'l', 'e', 'e', 'r', 'v', 'r', 'n', 'r', 'r', 'e', 'p', 'l', 'l', 'c', 'e', 'r', 'e', 'l', 'e', 'l', 'preprint', 'a5', 'additional', 'ablation', 'study', 'section', 'include', 'ablation', 'study', 'additional', 'hyperparameter', 'retsim', 'model', 'includ', 'loss', 'function', 'pooling', 'type', 'model', 'capacity', 'λ', 'recall', 'table', 'ablation', 'study', 'multisimilarity', 'loss', 'hyperparameter', 'retsim', 'training', 'indicate', 'hyperparameter', 'set', 'select', 'final', 'model', 'block', 'hide', 'table', 'ablation', 'study', 'retsim', 'model', 'capacity', 'size', 'number', 'gau', 'block', 'hide', 'dimension', 'block', 'indicate', 'hyperparameter', 'set', 'select', 'final', 'model', 'pool', 'type', 'recall', 'average', 'pooling', 'max', 'pooling', 'generalized', 'mean', 'pool', 'table', 'ablation', 'study', 'pool', 'type', 'retsim', 'model', 'indicate', 'hyperparameter', 'set', 'select', 'final', 'model', 'select', 'example', 'newscopy', 'dataset', 'section', 'randomly', 'select', 'set', 'false', 'positive', 'false', 'negative', 'retsim', 'newscopy', 'deduplication', 'dataset', 'provide', 'insight', 'result', 'preprint', 'text', 'chauffeur', 'policeman', 'pass', 'jour', 'nalist', 'try', 'intervene', 'policeman', 'report', 'serious', 'condition', 'princess', 'husband', 'month', 'capt', 'mark', 'phillip', 'hurt', 'police', 'expert', 'say', 'hole', 'leave', 'bullet', 'fire', 'car', 'indicate', 'pass', 'miss', 'inch', 'police', 'informant', 'say', 'believe', 'shot', 'fire', 'assailant', 'expert', 'study', 'revolver', 'find', 'scene', 'say', 'fi', 'tnfernational', 'ay', 'ssast', 'fre', 'sg', 'federal', 'government', 'propose', 'new', 'method', 'eoustructe', 'federal', 'building', 'move', 'save', 'ad', 'ditional', 'ergy', 'suggest', 'il', 'elfort', 'adapt', 'new', 'building', 'p', 'mediate', 'removal', 'bert', 'prohibition', 'administrator', 'de', 'mande', 'today', 'ground', 'charge', 'place', 'department', 'justice', 'investigator', 'wheeler', 'accompany', 'demand', 'continue', 'page', 'dnite', 'jai', 'delegation', 'navat', 'confer', 'ence', 'today', 'win', 'ls', 'demand', 'pre', 'sentation', 'cnse', 'suxiliary', 'warship', 'limitation', 'first', 'plenary', 'session', 'thuvaday', 'chlet', 'delegate', 'mec', 'tittg', 'also', 'decide', 'plenary', 'sesslon', 'discuss', 'main', 'con', 'ference', 'question', 'alpha', 'betical', 'order', 'ihe', 'countriea', 'pro', 'posing', 'press', 'admit', 'american', 'delegation', 'woo', 'second', 'vic', 'tory', 'text', 'ian', 'ball', '26year', 'old', 'unemployed', 'englishman', 'bring', 'court', 'today', 'charge', 'attempt', 'mur', 'der', 'tempt', 'kidnap', 'anne', 'car', 'heart', 'nesday', 'night', 'ball', 'leanface', 'bearded', 'stand', 'stiffly', 'dock', 'court', 'handcuff', 'detective', 'speak', 'second', 'appearance', 'say', 'want', 'apply', 'legal', 'aid', 'court', 'dere', 'hold', 'hearing', 'federal', 'government', 'propose', 'new', 'method', 'construct', 'federal', 'building', 'move', 'save', 'addilional', 'energy', 'suggest', 'il', 'effort', 'adapt', 'new', 'building', 'sampson', 'administration', 'ad', 'ministrater', 'say', 'new', 'feature', 'construction', 'include', 'collection', 'rain', 'waler', 'cool', 'irriga', 'tion', 'solar', 'energy', 'collector', 'covering', 'exterior', 'wall', 'earth', 'whal', 'say', 'design', 'criteri', 'immiedl', 'aie', 'mmoval', 'pro', 'hi', 'bition', 'administrator', 'demand', 'seuate', 'day', 'waeel', 'ground', 'charge', 'place', 'depart', 'meat', 'justice', 'investigator', 'wheeler', 'accompany', 'demand', 'declaration', 'prohibition', 'foreemen', 'blame', 'politician', 'call', 'law', 'enforcement', 'commussion', 'sum', 'member', 'wp', 'amer', 'naval', 'cen', 'ference', 'win', 'demand', 'presentation', 'case', 'linsitation', 'flrst', 'next', 'trary', 'session', 'chief', 'delegate', 'meet', 'si', 'also', 'decide', 'plenary', 'session', 'discuss', 'main', 'confeyence', 'question', 'alphabetical', 'order', 'cauntrie', 'propose', 'win', 'second', 'victory', 'decide', 'udmil', 'certain', 'representative', 'press', 'plenary', 'table', 'example', 'false', 'negative', 'retsim', 'newscopy', 'dataset', 'pair', 'text', 'detect', 'nearduplicate', 'retsim', 'label', 'nearduplicate', 'original', 'dataset', 'ex', 'ample', 'randomly', 'select', 'truncate', 'character', 'display', 'preprint', 'text', 'bozeman', 'chet', 'huntley', 'resonant', 'voice', 'roughhewn', 'face', 'come', 'familiar', 'million', 'nightly', 'television', 'news', 'die', 'moun', 'tain', 'resort', 'home', 'undergo', 'surgery', 'lung', 'cancer', 'remain', 'recent', 'week', 'die', 'accord', 'widow', 'tippy', 'team', 'year', 'port', 'quit', 'turn', 'native', 'develop', '20millio', 'associate', 'press', 'amer', 'ican', 'pay', 'per', 'cent', 'month', 'electricity', 'year', 'last', 'ate', 'press', 'survey', 'show', 'sumer', 'begin', 'organize', 'fight', 'rate', 'hike', 'spot', 'check', 'monthly', 'elec', 'tric', 'bill', 'year', 'last', 'show', 'crease', 'erally', 'cent', 'high', 'report', 'boost', 'com', 'fia', 'average', 'tab', 'go', 'last', 'year', 'year', 'utility', 'bozeman', 'vice', 'say', 'world', 'miss', 'unique', 'ability', 'former', 'television', 'huntley', 'die', 'home', 'long', 'bout', 'lung', 'cancer', 'family', 'spokesman', 'say', 'memorial', 'service', 'conduct', 'huntley', 'big', 'sky', 'resort', 'recreation', 'area', 'south', 'huntley', 'chairman', 'big', 'sky', 'board', 'director', 'memorial', 'service', 'schedule', 'studio', 'house', 'pass', 'legislation', 'raise', 'minimum', 'wage', 'hour', 'year', 'worker', 'cover', 'bill', 'approve', 'also', 'increase', 'number', 'worker', 'cover', 'mini', 'mum', 'wage', 'law', 'bill', 'modify', 'ver', 'sion', 'veto', 'last', 'year', 'however', 'expect', 'sign', 'one', 'finally', 'approve', 'ad', 'justment', 'similar', 'pass', 'measure', 'text', 'bozeman', 'chet', 'huntley', 'resonant', 'voice', 'roughhewn', 'face', 'become', 'familiar', 'million', 'nightly', 'television', 'news', 'die', 'moun', 'tain', 'resort', 'home', 'undergo', 'surgery', 'lung', 'cancer', 'remain', 'active', 'recent', 'week', 'die', 'accord', 'widow', 'tippy', 'huntley', 'huntley', 'team', 'year', 'quit', 'return', 'native', 'develop', 'mil', 'louise', 'acenciaiod', 'prece', 'writer', 'pay', 'io', 'cent', 'month', 'far', 'electricity', 'year', 'ihan', 'last', 'press', 'survey', 'show', 'onsumer', 'begin', 'fight', 'rate', 'hike', 'spot', 'check', 'monthly', 'elec', 'tre', 'hill', 'year', 'show', 'increase', 'erally', 'cent', 'high', 'report', 'boost', 'com', 'average', 'tab', 'go', 'last', 'year', 'year', 'bozeman', 'vice', 'say', 'world', 'miss', 'unique', 'ability', 'former', 'television', 'huntley', 'die', 'home', 'long', 'bout', 'lung', 'cancer', 'family', 'spokesman', 'say', 'morial', 'service', 'ducte', 'hunt', 'big', 'sky', 'resort', 'recreation', 'area', 'south', 'hunt', 'chair', 'man', 'big', 'sky', 'board', 'director', 'memorial', 'service', 'sche', 'uled', 'studio', 'house', 'pass', 'legislation', 'raise', 'minimum', 'wage', 'hour', 'year', 'worker', 'cover', 'bill', 'approve', 'wedne', 'day', 'also', 'crease', 'mil', 'lion', 'number', 'worker', 'cov', 'ere', 'minimum', 'wage', 'law', 'bill', 'modify', 'version', 'veto', 'last', 'year', 'however', 'ex', 'te', 'sign', 'one', 'inally', 'approve', 'adjust', 'ment', 'similar', 'pass', 'table', 'example', 'false', 'positive', 'retsim', 'newscopy', 'dataset', 'pair', 'text', 'detect', 'nearduplicate', 'retsim', 'label', 'nearduplicate', 'original', 'dataset', 'example', 'randomly', 'select', 'truncate', 'character', 'display']"
"$Q_{bias}$ -- A Dataset on Media Bias in Search Queries and Query
  Suggestions","[{'title': 'doi', 'href': 'http://dx.doi.org/10.1145/3578503.3583628', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2311.17780v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2311.17780v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-11-29 16:26:00,"3
2
0
2

v
o
N
8
2

]
S
M

.
s
c
[

1
v
3
8
2
7
1
.
1
1
3
2
:
v
i
X
r
a

Lineax: unified linear solves and linear least-squares
in JAX and Equinox

Jason Rader
Oxford University
rader@maths.ox.ac.uk

Terry Lyons
Oxford University

Patrick Kidger
Google X
math@kidger.site

Abstract

We introduce Lineax, a library bringing linear solves and linear least-squares to
the JAX+Equinox scientific computing ecosystem. Lineax uses general linear
operators, and unifies linear solves and least-squares into a single, autodifferen-
tiable API. Solvers and operators are user-extensible, without requiring the user to
implement any custom derivative rules to get differentiability. Lineax is available
at https://github.com/google/lineax.

1

Introduction

JAX is an autodifferentiatiable Python framework popular for machine learning and scientific comput-
ing [4, 9, 12, 16]. Equinox [20] is a popular JAX library [8, 15], targeting the same use cases, that adds
additional support for parameterised functions. Solving linear systems, whether well-posed linear
solves or ill-posed linear least-squares problems, is a central sub-problem in scientific computing
[14, 27]. For example, linear solves and least-squares appear as subroutines in nonlinear optimisation
[21], finite-difference schemes [26], and signal processing [22]. As such, we introduce Lineax, a
library built in JAX and Equinox for linear solves and linear least-squares.

Lineax presents a single, differentiable interface for solving well-posed, underdetermined, and
overdetermined linear systems. It also allows users to write custom differentiable linear solvers or
least-squares solvers, and introduces a linear operator abstraction.

Overall, we intend for Lineax to integrate well with the existing JAX scientific ecosystem. This
ecosystem is growing, and includes packages for differentiable rigid-body physics simulation [11],
computational fluid dynamics [3, 6], protein structure prediction [10], ordinary and stochastic
differential equations [19], and probabilistic modeling [25]. We are beginning to see some use
of Lineax in this ecosystem already. This includes for linear subroutines in ocean dynamics [18]
and optimal transport [5]. Further, Diffrax [19] plans to adopt Lineax in the near future for linear
subroutines in differential equations solves.

1.1 Main contributions

The main contributions of Lineax are:

• A general linear operator abstraction, as implemented by dense matrices, linear functions,

Jacobians, etc.

• Stable and fast gradients through least-squares solves. This includes through user-defined

solvers, without requiring extra effort from the user.

• PyTree-valued 1 operators and vectors.

1JAX terminology for arbitrarily nested container ’node’ types (tuples/dictionaries/lists/custom types) con-
taining ’leaves’ (every other Python/JAX type.) We exclusively consider PyTrees who’s leaves are JAX arrays.

NeurIPS 2023 AI for Science Workshop.

 
 
 
 
 
 
Comparisons to existing JAX APIs

The operator abstraction introduced in Lineax offers a flexibility not found in core JAX, which
supports only dense matrices or matrix-vector product representations of operators. Lineax introduces
new solvers over core JAX, such as lineax.Tridiagonal. Lineax also offers a consistent API
between operators and solvers, which is what allows for extensibility to user-specified custom operator
and solvers.

Compilation times for most Lineax solvers are essentially identical to JAX native solvers; Lineax’s
iterative solvers (CG, GMRES, ...) compile roughly twice as fast. The ‘benchmarks’ folder on GitHub
provides a quantitative comparison.

We emphasise the stable and fast gradients to contrast with the existing JAX implementation, which
as of version 0.4.16 exhibits instability or incorrect gradients in some exceptional cases.

For these reasons, JAX is actually considering deprecating some of its own APIs in favour of Lineax
[29].

1.2 Classical linear solve example

Consider solving Ax = b for a random matrix A ∈ R10×10 against a random vector b ∈ R10. This
can be done via

1

2

3

4

5

6

7

import jax . random as jr
import lineax as lx

A_key , b_key = jr . split ( jr . PRNGKey (0))
A = lx . M atr ix Li near Op er ator ( jr . normal ( A_key , (10 , 10)))
b = jr . normal ( b_key , (10 ,))
solution = lx . linear_solve (A , b )

2 Performing linear solves and least-squares

The main entry point to linear solves and least-squares in Lineax is

lineax . linear_solve (A , b , solver )

for a linear operator A and PyTree b. This performs a linear solve Ax = b (for well-posed systems),
or returns a least-squares solution minx ∥Ax − b∥2 (for overdetermined systems), or returns a
minimum norm solution minx ∥x∥2 subject to Ax = b (for underdetermined systems). This is a lot of
operations to unify together, and it may initially seem strange to do so. The common thread – and our
justification for unifying these operations – is that mathematically, all the above operations correspond
to the pseudoinverse solution to Ax = b, ie. the solution arising from using the Moore-Penrose
pseudoinverse x = A†b [1, 23].

The user can specify which solver they’d like to use via the solver argument. This is helpful when
the user already knows which solvers should work well for a problem. Not every solver is capable of
handling every problem. For example, lineax.CG handles positive definite operators [21, section 5].
Using a solver with an incompatible problem will result in an error.

3 General linear operators

In Lineax, we represent A more generally than as an n × m matrix. Instead, we represent A as a
linear operator A : X → Y , where X and Y are spaces of PyTrees of arrays. At an implementation
level, a linear operator is an object which subclasses

lineax . A bs t ra c tL i ne a r Op e rat or .

When A is a dense matrix, A ∈ Rdim(Y )×dim(X), it can be treated as a Lineax linear operator via

lineax . Ma trix Li ne ar Op er ator ( A ).

2

Lineax operators themselves form a vector space, and are closed under addition, scalar multiplication,
and composition. Each linear operator A must implement a method to:

• Compute the matrix-vector product: Ax for x ∈ X.
• Compute the transpose of the operator: AT : Y → X.
• Materialise the operator as a matrix: A.as_matrix() ∈ Rdim(X)×dim(Y ).
• Retrieve the input/output PyTree structure, as well as the input/output dimensions. ie. the

functions domain(A) = X and codomain(A) = Y .

This increased generality comes with increased flexibility. For example: large, sparse matrices can
use data-efficient formats and utilise linear solves which use only the matrix-vector product, such
as GMRES [24] or BiCGStab [28]. For example, a linear function f : X → Y can be made into a
linear operator with

lineax . F un c ti o nL i ne a r Op e rat or (f , in_structure )

where in_structure describes the PyTree structure of the input of f (equivalently, the PyTree
structure of the elements x ∈ X.) Similarly, a nonlinear function g : X → Y can be linearised at a
point x ∈ X and use its Jacobian at x as a linear operator via

lineax . J ac o bi a nL i ne a r Op e rat or (g , x )

The lineax.AbstractLinearOperator base class is available for users to subclass and create
their own linear operator types.

3.1 Operator tags

Tags are an optional argument to most linear operators, and indicate properties of the operator A.
For example, if A ∈ Rn×n is positive semidefinite, then A can be marked as a positive semidefinite
linear operator with

lineax . Ma trix Li ne ar Op er ator (A , lineax . p o s i t i ve _ s e m i d e f i ni t e _ t a g )

This indicates to any solver which uses A that it is positive semidefinite. For example, if A is also
nonsingular, then it can be used safely with lineax.CG. Tags are also used to select the appropriate
solver in the polyalgorithm lineax.AutoLinearSolver detailed in section 6.

4 Computing gradients

In JAX, derivatives are built from Jacobian-vector products (JVPs) and vector-Jacobian products
(VJPs) for forward-mode and reverse-mode automatic differentiation respectively [12]. The JVP of a
function f : Ra → Rb maps an input-tangent pair (x, v) ∈ Ra × Ra to (f (x), ∂f (x)(v)) ∈ Rb × Rb
where ∂f (x) : Ra → Rb is the Jacobian of f at x. The VJP maps an input-cotangent pair (x, c) ∈
Ra × Rb to (f (x), ∂f (x)T c) ∈ Rb × Ra, where ∂f (x)T : Rb → Ra is the transpose of ∂f (x).

The major contribution of Lineax over existing linear solve and least-squares software is the efficient
computation of JVPs for pseudoinverse solutions. That is, differentiation through both well-posed
linear solves and ill-posed least-squares solves are performed in the same manner as each other.
In particular, we may special case when operators have full row or column rank in order to obtain
improved performance, as we now show.

4.1

JVPs and forward-mode autodifferentiation

In this section, let L(A, b) denote the linear solve lineax.linear_solve. For a primal problem
Ax = b, then L(A, b) = A†b where A† be the Moore–Penrose pseudoinverse of A, as mentioned in
section 2. Here we discuss how the to compute the JVP ∂L(A, b)(V, v), where (V, v) is the tangent
pair consisting of a tangent operator V and tangent vector v.

It is possible to compute the JVP through either argument. For example, the tangent computation of a
linear solve as a function of v alone is

∂L(A, b)(0, v) = A†v

3

(2)

(3)

(4)

where 0 represents the 0 tangent operator.

Meanwhile, computing the JVP for ∂L(A, b)(V, 0) requires differentiating through a pseudoinverse,
which has the explicit formula [13]

∂L(A, b)(V, 0) = (−A†V A† + A†(A†)T V T (I − AA†) + (I − A†A)V T (A†)T A†)A†b.

Letting

x = A†b
z = V T (A†)T x,

and adding the above two equations together and using the linearity of the Jacobian, we have the total
JVP with respect the primal pair (A, b) and tangent pair (V, v) for lineax.linear_solve is

∂L(A, b)(V, v) = A† (cid:0)−V x + (A†)T V T (b − Ax) − Az + v(cid:1) + z.

(1)

If A has linearly independent columns, then A†A = I [14, section 5.5.2] and the term z − A†Az = 0,
giving

∂L(A, b)(V, v) = A† (cid:0)−V x + (A†)T V T (b − Ax) + v(cid:1) .

When A has linearly independent rows, then (b − Ax) = 0 and

∂L(A, b)(V, v) = A† (−V x − Az + v) + z.

Together, if A has linearly independent rows and columns, then A is well-posed, A† = A−1 is a true
inverse, and

∂L(A, b)(V, v) = A−1 (−V x + v) .

We then select between equations (4), (3), (2), or (1) depending on whether we know at compile time
that A has linearly independent rows and columns, has only independent rows, has only independent
columns, or has both dependent rows and columns. Despite being a property of the operator, at
compile time the main way the JVP rule is dispatched via the choice of solver. This is because not
every solver supports dependent rows/columns (section 5), and will return nan values if used in a
solve with an unsupported operator. So, if a solver does not support dependent rows/columns, we can
be sure we will not get a solution given an operator with dependent rows/columns in the JVP.

For example, lineax.QR [27, section 2] can handle dependent rows if the number of rows is greater
than the number of columns (or dependent columns if the number of columns is greater than the
number of rows) and will dispatch to either equation (2) or (3). If the number of columns and rows of
A are the same, then lineax.QR will dispatch to equation (4).

4.2 VJPs and backpropogation via transposition

Reverse-mode autodifferentiation of a function f : Ra → Rb is not built on JVPs, but rather on
vector-Jacobian products (VJPs) vT ∂f (x) = ∂f (x)T v. As suggested by the definition, VJPs are
constructed via a JVP and transposition [12]. This is how VJPs are implemented in JAX, and thus in
Lineax as well. The Jacobian ∂L(A, b) : Rm×n × Rm → Rn is a linear function, see also the explicit
form in equation (1). Therefore, it has a transpose ∂L(A, b)T : Rn → Rm×n × Rm.

The transpose rule for the linear solve is implemented as a custom JAX primitive. See [12] for more
details.

5 User-defined solvers

A user can implement a custom solver by subclassing

lineax . Ab stra ct Li ne ar So lver

which requires the methods: init, compute, transpose, allow_dependent_rows, and
allow_dependent_columns.

4

Many direct linear solvers for Ax = b use two stages of computation. First, factor A into a form
amenable to computation (eg. LU factorisation [14, section 3], QR factorisation, SVD factorisation,
etc.) Then, use this factorisation to solve for a given right hand side b. The factorisation of A does not
depend on the right hand side b, and can be reused with various choices of b. This saves computation
cost when solving Ax = b for many right hands b.

For a solver using such a two-stage factorisation approach, init computes the factorisation, and
compute performs the solve for the specific right hand b. transpose computes the transpose of the
factorisation provided by init, which allows us to skip computing the factorisation of the transpose
operator directly (init(transpose(operator))), as it is commonly the case that we can cheaply
derive this from init(operator) alone. This is needed when computing VJPs as discussed in
the previous section. The methods allow_dependent_rows and allow_dependent_columns
determine which equation (1-4) is used in the differentiation, as discussed in section 4.

This greatly simplifies the process of writing a differentiable linear solver or least-squares solver. In
the core JAX library, it is somewhat cumbersome to write a differentiable solver. It requires using
jax.lax.custom_linear_solve and implementing a solver transposition rule transpose_solve
if the user would like to use reverse-mode autodifferentiation. In Lineax, differentiation comes for
free once a solver is implemented, whether the solver is a linear solve or a least-squares algorithm.

6 The AutoLinearSolver polyalgorithm

If the user does not provide a solver to lineax.linear_solve, then the default linear solve
is lineax.AutoLinearSolver. lineax.AutoLinearSolver is a polyalgorithm which selects a
solver automatically at compile time depending on the structure of A, as indicated through its operator
tag (discussed in section 3.1). lineax.AutoLinearSolver takes the argument well_posed, which
indicates whether the system is expected to solve a least-squares/minimum norm problem, or only
handle well-posed linear solves.

lineax.AutoLinearSolver(well_posed=True) selects a solver depending upon the oper-
ator structure, and throws an error when it encounters an underdeteremined or overdeter-
mined system.
lineax.AutoLinearSolver(well_posed=False) solves well-posed linear
solves as well as linear least squares, but often at an additional computation cost. Finally,
lineax.AutoLinearSolver(well_posed=None) solves a least-squares problem only if it is not
expensive to do so.

The specific polyalgorithms for well_posed=True, well_posed=False, and well_posed=None
are shown in figure 1.

6.1 Choosing a solver at compile time

We must choose between two paradigms for the implementation of lineax.AutoLinearSolver:
make the algorithm selection at run time, or make the algorithm selection at compile time. This is a
trade-off, as determining which solver to use at run time means checking the elements of the matrix.
This incurs a run time overhead of O(n2) for an n × n matrix, which is relatively small compared to
the O(n3) run time of most linear solve algorithms. However, run time checking also incurs a greater
cost in compile times. Since it is not known at compile time which branch of the polyalgorithm will
run, the compiler is forced to compile all branches. Thus, compilation cost scales with the logic
of the polyalgorithm: as more branches are included, compile times increase. This can limit the
extensibility of the polyalgorithm.

Compile time selection avoids these performance issues, and is faster both in run time and compile
time when used correctly. Further, it simplifies tying solves to GPU hardware, as there is no possibility
of taking separate branches for different batch elements. However, compile time selection requires
the user to a-priori know the structure of the operator, and can result in using a suboptimal solver if
the operator has exploitable structure which the user does not indicate.

We choose the compile time approach, and require the user to pass the structure of an operator explic-
itly via the operator tag (section 3.1.) We choose this approach primarily to minimise compilation
times, and to avoid the tradeoff between extensibility and compile time inherent in run time checking.

5

The AutoLinearSolver polyalgorithm.

Figure 1:
so that
well_posed=True starts at ""A is square?"", well_posed=None starts at ""A is diagonal?"",
and well_posed=False starts at ""A is diagonal?"" in the well_posed=False section.

Read from left-to-right,

6

Our approach is in contrast to MATLAB’s mldivide, a (nondifferentiable) unified linear solve and
least-squares solver which uses a run time approach [17]. Both the Julia and MATLAB languages
offer methods for nonsingular linear solves – the infix \ operation in Julia and linsolve in MATLAB
– which accept compile time tags, but still perform run time checks if the user passes no tags [2, 7, 17].
Therefore, both suffer from the additional overhead of the run time approach in many cases.

7 Conclusion

We have introduced Lineax, a differentiable JAX+Equinox library unifying linear solves and linear
least-squares. We have demonstrated that users can extend base Lineax operators and solvers and
use them within our unified API, without the need to write any custom derivative rules. We hope to
see adoption of Lineax solves within the JAX+Equinox scientific computing and machine learning
ecosystem.

8 Acknowledgements

This publication is based on work supported by the EPSRC Centre for Doctoral Training in Mathe-
matics of Random Systems: Analysis, Modelling and Simulation (EP/S023925/1)

References

[1] Adi Ben-Israel and Thomas N. E. Greville. Generalized Inverses: Theory and Applications.

Springer New York, 2003.

[2] Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral B Shah. Julia: A fresh approach to
numerical computing. SIAM Review, 59(1):65–98, 2017. URL: https://epubs.siam.org/
doi/10.1137/141000671, doi:10.1137/141000671.

[3] Deniz A. Bezgin, Aaron B. Buhendwa, and Nikolaus A. Adams.

JAX-fluids: A fully-
differentiable high-order computational fluid dynamics solver for compressible two-phase
flows. Computer Physics Communications, 282:108527, 2023.

[4] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James, Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and
Qiao Zhang. Jax: composable transformations of python+numpy programs, 2018. URL:
http://github.com/google/jax.

[5] Marco Cuturi, Laetitia Meng-Papaxanthos, Yingtao Tian, Charlotte Bunne, Geoff Davis, and
Olivier Teboul. Optimal transport tools (ott): A jax toolbox for all things wasserstein. arXiv
preprint arXiv:2201.12324, 2022.

[6] Gideon Dresdner, Dmitrii Kochkov, Peter Norgaard, Leonardo Zepeda-Núñez, Jamie A. Smith,
Michael P. Brenner, and Stephan Hoyer. Learning to correct spectral methods for simulating
turbulent flows. arXiv preprint arXiv:2207.00556, 2022.

[7] Chris Rackauckas et al. Linearsolve.jl: High-performance unified linear solvers, 2021. Accessed

2023. URL: https://github.com/SciML/LinearSolve.jl.

[8] David Hall et al. Haliax. Accessed 2023, 2023. URL: https://github.com/

stanford-crfm/haliax.

[9] Igor Babuschkin et al. The deepmind jax ecosystem, 2020. URL: http://github.com/

deepmind.

[10] John Jumper et al. Highly accurate protein structure prediction with alphafold. Nature, 596:583–

589, 8 2021.

[11] C. Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier
Bachem. Brax - a differentiable physics engine for large scale rigid body simulation, 2021.

7

[12] Roy Frostig, Matthew J. Johnson, Dougal Maclaurin, Adam Paszke, and Alexey Radul. Decom-

posing reverse-mode automatic differentiation, 2021. arXiv:2105.09469.

[13] Gene H. Golub and V. Pereyra. The differentiation of pseudo-inverses and nonlinear least squares
problems whose variables separate. SIAM Journal on Numerical Analysis, 10(2):413–432, 1973.

[14] Gene H. Golub and Charles F. Van Loan. Matrix Computations. The Johns Hopkins University

Press, third edition, 1996.

[15] David Hall, Ivan Zhou, and Percy Liang. Levanter — legible, scalable, reproducible foundation
models with jax. Accessed 2023, 2023. URL: https://github.com/stanford-crfm/
levanter.

[16] Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas
Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2023. URL:
http://github.com/google/flax.

[17] The MathWorks Inc. Matlab version: 9.13.0 (r2022b), 1984. URL: https://www.mathworks.

com.

[18] J. Emmanuel Johnson and Takaya Uchida. Jaxsw: Jax approximate ocean models, 2023.

Accessed 2023. URL: https://github.com/jejjohnson/jaxsw.

[19] Patrick Kidger. On Neural Differential Equations. PhD thesis, University of Oxford, 2021.

[20] Patrick Kidger and Cristian Garcia. Equinox: neural networks in JAX via callable PyTrees
and filtered transformations. Differentiable Programming workshop at Neural Information
Processing Systems 2021, 2021.

[21] Jorge Nocedal and Stephen Wright. Numerical Optimization (Second Edition). Springer New

York, 2006.

[22] Alan V. Oppenheim and Ronald W. Schafer. Digital Signal Processing. Prentice-Hall, 1975.

[23] R. Penrose. A generalized inverse for matrices. Mathematical Proceedings of the Cambridge

Philosophical Society, 51(3):406–413, 1955.

[24] Youcef Saad and Martin H. Schultz. Gmres: A generalized minimal residual algorithm for
solving nonsymmetric linear systems. SIAM Journal on Scientific and Statistical Computing,
7(3):856–869, 1986.

[25] Miloš Stanojevi´c and Laurent Sartran. SynJax: Structured Probability Distributions for JAX.

arXiv preprint arXiv:2308.03291, 2023.

[26] J. W. Thomas. Numerical Partial Differential Equations: Finite Difference Methods. Springer

New York, 1995.

[27] Lloyd N. Trefethen and David Bau. Numerical Linear Algebra. Society for Industrial and

Applied Mathematics, 1997.

[28] H. A. van der Vorst. Bi-cgstab: A fast and smoothly converging variant of bi-cg for the
solution of nonsymmetric linear systems. SIAM Journal on Scientific and Statistical Computing,
13(2):631–644, 1992.

[29] Jake Vanderplas. Jep 18137: Scope of jax numpy & scipy wrappers. Github pull request, created

and accessed 2023, 2023. URL: https://github.com/google/jax/pull/18137.

8

","3 2 0 2 v o N 8 2 ] S M . s c [ 1 v 3 8 2 7 1 . 1 1 3 2 : v i X r a Lineax : unified linear solves and linear least-squares in JAX and Equinox Jason Rader Oxford University rader @ maths.ox.ac.uk Terry Lyons Oxford University Patrick Kidger Google X math @ kidger.site Abstract We introduce Lineax , a library bringing linear solves and linear least-squares to the JAX+Equinox scientific computing ecosystem . Lineax uses general linear operators , and unifies linear solves and least-squares into a single , autodifferen- tiable API . Solvers and operators are user-extensible , without requiring the user to implement any custom derivative rules to get differentiability . Lineax is available at https : //github.com/google/lineax . 1 Introduction JAX is an autodifferentiatiable Python framework popular for machine learning and scientific comput- ing [ 4 , 9 , 12 , 16 ] . Equinox [ 20 ] is a popular JAX library [ 8 , 15 ] , targeting the same use cases , that adds additional support for parameterised functions . Solving linear systems , whether well-posed linear solves or ill-posed linear least-squares problems , is a central sub-problem in scientific computing [ 14 , 27 ] . For example , linear solves and least-squares appear as subroutines in nonlinear optimisation [ 21 ] , finite-difference schemes [ 26 ] , and signal processing [ 22 ] . As such , we introduce Lineax , a library built in JAX and Equinox for linear solves and linear least-squares . Lineax presents a single , differentiable interface for solving well-posed , underdetermined , and overdetermined linear systems . It also allows users to write custom differentiable linear solvers or least-squares solvers , and introduces a linear operator abstraction . Overall , we intend for Lineax to integrate well with the existing JAX scientific ecosystem . This ecosystem is growing , and includes packages for differentiable rigid-body physics simulation [ 11 ] , computational fluid dynamics [ 3 , 6 ] , protein structure prediction [ 10 ] , ordinary and stochastic differential equations [ 19 ] , and probabilistic modeling [ 25 ] . We are beginning to see some use of Lineax in this ecosystem already . This includes for linear subroutines in ocean dynamics [ 18 ] and optimal transport [ 5 ] . Further , Diffrax [ 19 ] plans to adopt Lineax in the near future for linear subroutines in differential equations solves . 1.1 Main contributions The main contributions of Lineax are : • A general linear operator abstraction , as implemented by dense matrices , linear functions , Jacobians , etc . • Stable and fast gradients through least-squares solves . This includes through user-defined solvers , without requiring extra effort from the user . • PyTree-valued 1 operators and vectors . 1JAX terminology for arbitrarily nested container ’ node ’ types ( tuples/dictionaries/lists/custom types ) con- taining ’ leaves ’ ( every other Python/JAX type . ) We exclusively consider PyTrees who ’ s leaves are JAX arrays . NeurIPS 2023 AI for Science Workshop . Comparisons to existing JAX APIs The operator abstraction introduced in Lineax offers a flexibility not found in core JAX , which supports only dense matrices or matrix-vector product representations of operators . Lineax introduces new solvers over core JAX , such as lineax.Tridiagonal . Lineax also offers a consistent API between operators and solvers , which is what allows for extensibility to user-specified custom operator and solvers . Compilation times for most Lineax solvers are essentially identical to JAX native solvers ; Lineax ’ s iterative solvers ( CG , GMRES , ... ) compile roughly twice as fast . The ‘ benchmarks ’ folder on GitHub provides a quantitative comparison . We emphasise the stable and fast gradients to contrast with the existing JAX implementation , which as of version 0.4.16 exhibits instability or incorrect gradients in some exceptional cases . For these reasons , JAX is actually considering deprecating some of its own APIs in favour of Lineax [ 29 ] . 1.2 Classical linear solve example Consider solving Ax = b for a random matrix A ∈ R10×10 against a random vector b ∈ R10 . This can be done via 1 2 3 4 5 6 7 import jax . random as jr import lineax as lx A_key , b_key = jr . split ( jr . PRNGKey ( 0 ) ) A = lx . M atr ix Li near Op er ator ( jr . normal ( A_key , ( 10 , 10 ) ) ) b = jr . normal ( b_key , ( 10 , ) ) solution = lx . linear_solve ( A , b ) 2 Performing linear solves and least-squares The main entry point to linear solves and least-squares in Lineax is lineax . linear_solve ( A , b , solver ) for a linear operator A and PyTree b . This performs a linear solve Ax = b ( for well-posed systems ) , or returns a least-squares solution minx ∥Ax − b∥2 ( for overdetermined systems ) , or returns a minimum norm solution minx ∥x∥2 subject to Ax = b ( for underdetermined systems ) . This is a lot of operations to unify together , and it may initially seem strange to do so . The common thread – and our justification for unifying these operations – is that mathematically , all the above operations correspond to the pseudoinverse solution to Ax = b , ie . the solution arising from using the Moore-Penrose pseudoinverse x = A†b [ 1 , 23 ] . The user can specify which solver they ’ d like to use via the solver argument . This is helpful when the user already knows which solvers should work well for a problem . Not every solver is capable of handling every problem . For example , lineax.CG handles positive definite operators [ 21 , section 5 ] . Using a solver with an incompatible problem will result in an error . 3 General linear operators In Lineax , we represent A more generally than as an n × m matrix . Instead , we represent A as a linear operator A : X → Y , where X and Y are spaces of PyTrees of arrays . At an implementation level , a linear operator is an object which subclasses lineax . A bs t ra c tL i ne a r Op e rat or . When A is a dense matrix , A ∈ Rdim ( Y ) ×dim ( X ) , it can be treated as a Lineax linear operator via lineax . Ma trix Li ne ar Op er ator ( A ) . 2 Lineax operators themselves form a vector space , and are closed under addition , scalar multiplication , and composition . Each linear operator A must implement a method to : • Compute the matrix-vector product : Ax for x ∈ X . • Compute the transpose of the operator : AT : Y → X . • Materialise the operator as a matrix : A.as_matrix ( ) ∈ Rdim ( X ) ×dim ( Y ) . • Retrieve the input/output PyTree structure , as well as the input/output dimensions . ie . the functions domain ( A ) = X and codomain ( A ) = Y . This increased generality comes with increased flexibility . For example : large , sparse matrices can use data-efficient formats and utilise linear solves which use only the matrix-vector product , such as GMRES [ 24 ] or BiCGStab [ 28 ] . For example , a linear function f : X → Y can be made into a linear operator with lineax . F un c ti o nL i ne a r Op e rat or ( f , in_structure ) where in_structure describes the PyTree structure of the input of f ( equivalently , the PyTree structure of the elements x ∈ X . ) Similarly , a nonlinear function g : X → Y can be linearised at a point x ∈ X and use its Jacobian at x as a linear operator via lineax . J ac o bi a nL i ne a r Op e rat or ( g , x ) The lineax.AbstractLinearOperator base class is available for users to subclass and create their own linear operator types . 3.1 Operator tags Tags are an optional argument to most linear operators , and indicate properties of the operator A . For example , if A ∈ Rn×n is positive semidefinite , then A can be marked as a positive semidefinite linear operator with lineax . Ma trix Li ne ar Op er ator ( A , lineax . p o s i t i ve _ s e m i d e f i ni t e _ t a g ) This indicates to any solver which uses A that it is positive semidefinite . For example , if A is also nonsingular , then it can be used safely with lineax.CG . Tags are also used to select the appropriate solver in the polyalgorithm lineax.AutoLinearSolver detailed in section 6 . 4 Computing gradients In JAX , derivatives are built from Jacobian-vector products ( JVPs ) and vector-Jacobian products ( VJPs ) for forward-mode and reverse-mode automatic differentiation respectively [ 12 ] . The JVP of a function f : Ra → Rb maps an input-tangent pair ( x , v ) ∈ Ra × Ra to ( f ( x ) , ∂f ( x ) ( v ) ) ∈ Rb × Rb where ∂f ( x ) : Ra → Rb is the Jacobian of f at x . The VJP maps an input-cotangent pair ( x , c ) ∈ Ra × Rb to ( f ( x ) , ∂f ( x ) T c ) ∈ Rb × Ra , where ∂f ( x ) T : Rb → Ra is the transpose of ∂f ( x ) . The major contribution of Lineax over existing linear solve and least-squares software is the efficient computation of JVPs for pseudoinverse solutions . That is , differentiation through both well-posed linear solves and ill-posed least-squares solves are performed in the same manner as each other . In particular , we may special case when operators have full row or column rank in order to obtain improved performance , as we now show . 4.1 JVPs and forward-mode autodifferentiation In this section , let L ( A , b ) denote the linear solve lineax.linear_solve . For a primal problem Ax = b , then L ( A , b ) = A†b where A† be the Moore–Penrose pseudoinverse of A , as mentioned in section 2 . Here we discuss how the to compute the JVP ∂L ( A , b ) ( V , v ) , where ( V , v ) is the tangent pair consisting of a tangent operator V and tangent vector v. It is possible to compute the JVP through either argument . For example , the tangent computation of a linear solve as a function of v alone is ∂L ( A , b ) ( 0 , v ) = A†v 3 ( 2 ) ( 3 ) ( 4 ) where 0 represents the 0 tangent operator . Meanwhile , computing the JVP for ∂L ( A , b ) ( V , 0 ) requires differentiating through a pseudoinverse , which has the explicit formula [ 13 ] ∂L ( A , b ) ( V , 0 ) = ( −A†V A† + A† ( A† ) T V T ( I − AA† ) + ( I − A†A ) V T ( A† ) T A† ) A†b . Letting x = A†b z = V T ( A† ) T x , and adding the above two equations together and using the linearity of the Jacobian , we have the total JVP with respect the primal pair ( A , b ) and tangent pair ( V , v ) for lineax.linear_solve is ∂L ( A , b ) ( V , v ) = A† ( cid:0 ) −V x + ( A† ) T V T ( b − Ax ) − Az + v ( cid:1 ) + z . ( 1 ) If A has linearly independent columns , then A†A = I [ 14 , section 5.5.2 ] and the term z − A†Az = 0 , giving ∂L ( A , b ) ( V , v ) = A† ( cid:0 ) −V x + ( A† ) T V T ( b − Ax ) + v ( cid:1 ) . When A has linearly independent rows , then ( b − Ax ) = 0 and ∂L ( A , b ) ( V , v ) = A† ( −V x − Az + v ) + z . Together , if A has linearly independent rows and columns , then A is well-posed , A† = A−1 is a true inverse , and ∂L ( A , b ) ( V , v ) = A−1 ( −V x + v ) . We then select between equations ( 4 ) , ( 3 ) , ( 2 ) , or ( 1 ) depending on whether we know at compile time that A has linearly independent rows and columns , has only independent rows , has only independent columns , or has both dependent rows and columns . Despite being a property of the operator , at compile time the main way the JVP rule is dispatched via the choice of solver . This is because not every solver supports dependent rows/columns ( section 5 ) , and will return nan values if used in a solve with an unsupported operator . So , if a solver does not support dependent rows/columns , we can be sure we will not get a solution given an operator with dependent rows/columns in the JVP . For example , lineax.QR [ 27 , section 2 ] can handle dependent rows if the number of rows is greater than the number of columns ( or dependent columns if the number of columns is greater than the number of rows ) and will dispatch to either equation ( 2 ) or ( 3 ) . If the number of columns and rows of A are the same , then lineax.QR will dispatch to equation ( 4 ) . 4.2 VJPs and backpropogation via transposition Reverse-mode autodifferentiation of a function f : Ra → Rb is not built on JVPs , but rather on vector-Jacobian products ( VJPs ) vT ∂f ( x ) = ∂f ( x ) T v. As suggested by the definition , VJPs are constructed via a JVP and transposition [ 12 ] . This is how VJPs are implemented in JAX , and thus in Lineax as well . The Jacobian ∂L ( A , b ) : Rm×n × Rm → Rn is a linear function , see also the explicit form in equation ( 1 ) . Therefore , it has a transpose ∂L ( A , b ) T : Rn → Rm×n × Rm . The transpose rule for the linear solve is implemented as a custom JAX primitive . See [ 12 ] for more details . 5 User-defined solvers A user can implement a custom solver by subclassing lineax . Ab stra ct Li ne ar So lver which requires the methods : init , compute , transpose , allow_dependent_rows , and allow_dependent_columns . 4 Many direct linear solvers for Ax = b use two stages of computation . First , factor A into a form amenable to computation ( eg . LU factorisation [ 14 , section 3 ] , QR factorisation , SVD factorisation , etc . ) Then , use this factorisation to solve for a given right hand side b . The factorisation of A does not depend on the right hand side b , and can be reused with various choices of b . This saves computation cost when solving Ax = b for many right hands b . For a solver using such a two-stage factorisation approach , init computes the factorisation , and compute performs the solve for the specific right hand b. transpose computes the transpose of the factorisation provided by init , which allows us to skip computing the factorisation of the transpose operator directly ( init ( transpose ( operator ) ) ) , as it is commonly the case that we can cheaply derive this from init ( operator ) alone . This is needed when computing VJPs as discussed in the previous section . The methods allow_dependent_rows and allow_dependent_columns determine which equation ( 1-4 ) is used in the differentiation , as discussed in section 4 . This greatly simplifies the process of writing a differentiable linear solver or least-squares solver . In the core JAX library , it is somewhat cumbersome to write a differentiable solver . It requires using jax.lax.custom_linear_solve and implementing a solver transposition rule transpose_solve if the user would like to use reverse-mode autodifferentiation . In Lineax , differentiation comes for free once a solver is implemented , whether the solver is a linear solve or a least-squares algorithm . 6 The AutoLinearSolver polyalgorithm If the user does not provide a solver to lineax.linear_solve , then the default linear solve is lineax.AutoLinearSolver . lineax.AutoLinearSolver is a polyalgorithm which selects a solver automatically at compile time depending on the structure of A , as indicated through its operator tag ( discussed in section 3.1 ) . lineax.AutoLinearSolver takes the argument well_posed , which indicates whether the system is expected to solve a least-squares/minimum norm problem , or only handle well-posed linear solves . lineax.AutoLinearSolver ( well_posed=True ) selects a solver depending upon the oper- ator structure , and throws an error when it encounters an underdeteremined or overdeter- mined system . lineax.AutoLinearSolver ( well_posed=False ) solves well-posed linear solves as well as linear least squares , but often at an additional computation cost . Finally , lineax.AutoLinearSolver ( well_posed=None ) solves a least-squares problem only if it is not expensive to do so . The specific polyalgorithms for well_posed=True , well_posed=False , and well_posed=None are shown in figure 1 . 6.1 Choosing a solver at compile time We must choose between two paradigms for the implementation of lineax.AutoLinearSolver : make the algorithm selection at run time , or make the algorithm selection at compile time . This is a trade-off , as determining which solver to use at run time means checking the elements of the matrix . This incurs a run time overhead of O ( n2 ) for an n × n matrix , which is relatively small compared to the O ( n3 ) run time of most linear solve algorithms . However , run time checking also incurs a greater cost in compile times . Since it is not known at compile time which branch of the polyalgorithm will run , the compiler is forced to compile all branches . Thus , compilation cost scales with the logic of the polyalgorithm : as more branches are included , compile times increase . This can limit the extensibility of the polyalgorithm . Compile time selection avoids these performance issues , and is faster both in run time and compile time when used correctly . Further , it simplifies tying solves to GPU hardware , as there is no possibility of taking separate branches for different batch elements . However , compile time selection requires the user to a-priori know the structure of the operator , and can result in using a suboptimal solver if the operator has exploitable structure which the user does not indicate . We choose the compile time approach , and require the user to pass the structure of an operator explic- itly via the operator tag ( section 3.1 . ) We choose this approach primarily to minimise compilation times , and to avoid the tradeoff between extensibility and compile time inherent in run time checking . 5 The AutoLinearSolver polyalgorithm . Figure 1 : so that well_posed=True starts at `` A is square ? `` , well_posed=None starts at `` A is diagonal ? `` , and well_posed=False starts at `` A is diagonal ? '' in the well_posed=False section . Read from left-to-right , 6 Our approach is in contrast to MATLAB ’ s mldivide , a ( nondifferentiable ) unified linear solve and least-squares solver which uses a run time approach [ 17 ] . Both the Julia and MATLAB languages offer methods for nonsingular linear solves – the infix \ operation in Julia and linsolve in MATLAB – which accept compile time tags , but still perform run time checks if the user passes no tags [ 2 , 7 , 17 ] . Therefore , both suffer from the additional overhead of the run time approach in many cases . 7 Conclusion We have introduced Lineax , a differentiable JAX+Equinox library unifying linear solves and linear least-squares . We have demonstrated that users can extend base Lineax operators and solvers and use them within our unified API , without the need to write any custom derivative rules . We hope to see adoption of Lineax solves within the JAX+Equinox scientific computing and machine learning ecosystem . 8 Acknowledgements This publication is based on work supported by the EPSRC Centre for Doctoral Training in Mathe- matics of Random Systems : Analysis , Modelling and Simulation ( EP/S023925/1 ) References [ 1 ] Adi Ben-Israel and Thomas N. E. Greville . Generalized Inverses : Theory and Applications . Springer New York , 2003 . [ 2 ] Jeff Bezanson , Alan Edelman , Stefan Karpinski , and Viral B Shah . Julia : A fresh approach to numerical computing . SIAM Review , 59 ( 1 ) :65–98 , 2017 . URL : https : //epubs.siam.org/ doi/10.1137/141000671 , doi:10.1137/141000671 . [ 3 ] Deniz A. Bezgin , Aaron B. Buhendwa , and Nikolaus A. Adams . JAX-fluids : A fully- differentiable high-order computational fluid dynamics solver for compressible two-phase flows . Computer Physics Communications , 282:108527 , 2023 . [ 4 ] James Bradbury , Roy Frostig , Peter Hawkins , Matthew James , Johnson , Chris Leary , Dougal Maclaurin , George Necula , Adam Paszke , Jake VanderPlas , Skye Wanderman-Milne , and Qiao Zhang . Jax : composable transformations of python+numpy programs , 2018 . URL : http : //github.com/google/jax . [ 5 ] Marco Cuturi , Laetitia Meng-Papaxanthos , Yingtao Tian , Charlotte Bunne , Geoff Davis , and Olivier Teboul . Optimal transport tools ( ott ) : A jax toolbox for all things wasserstein . arXiv preprint arXiv:2201.12324 , 2022 . [ 6 ] Gideon Dresdner , Dmitrii Kochkov , Peter Norgaard , Leonardo Zepeda-Núñez , Jamie A. Smith , Michael P. Brenner , and Stephan Hoyer . Learning to correct spectral methods for simulating turbulent flows . arXiv preprint arXiv:2207.00556 , 2022 . [ 7 ] Chris Rackauckas et al . Linearsolve.jl : High-performance unified linear solvers , 2021 . Accessed 2023 . URL : https : //github.com/SciML/LinearSolve.jl . [ 8 ] David Hall et al . Haliax . Accessed 2023 , 2023 . URL : https : //github.com/ stanford-crfm/haliax . [ 9 ] Igor Babuschkin et al . The deepmind jax ecosystem , 2020 . URL : http : //github.com/ deepmind . [ 10 ] John Jumper et al . Highly accurate protein structure prediction with alphafold . Nature , 596:583– 589 , 8 2021 . [ 11 ] C. Daniel Freeman , Erik Frey , Anton Raichuk , Sertan Girgin , Igor Mordatch , and Olivier Bachem . Brax - a differentiable physics engine for large scale rigid body simulation , 2021 . 7 [ 12 ] Roy Frostig , Matthew J. Johnson , Dougal Maclaurin , Adam Paszke , and Alexey Radul . Decom- posing reverse-mode automatic differentiation , 2021. arXiv:2105.09469 . [ 13 ] Gene H. Golub and V. Pereyra . The differentiation of pseudo-inverses and nonlinear least squares problems whose variables separate . SIAM Journal on Numerical Analysis , 10 ( 2 ) :413–432 , 1973 . [ 14 ] Gene H. Golub and Charles F. Van Loan . Matrix Computations . The Johns Hopkins University Press , third edition , 1996 . [ 15 ] David Hall , Ivan Zhou , and Percy Liang . Levanter — legible , scalable , reproducible foundation models with jax . Accessed 2023 , 2023 . URL : https : //github.com/stanford-crfm/ levanter . [ 16 ] Jonathan Heek , Anselm Levskaya , Avital Oliver , Marvin Ritter , Bertrand Rondepierre , Andreas Steiner , and Marc van Zee . Flax : A neural network library and ecosystem for JAX , 2023 . URL : http : //github.com/google/flax . [ 17 ] The MathWorks Inc. Matlab version : 9.13.0 ( r2022b ) , 1984 . URL : https : //www.mathworks . com . [ 18 ] J. Emmanuel Johnson and Takaya Uchida . Jaxsw : Jax approximate ocean models , 2023 . Accessed 2023 . URL : https : //github.com/jejjohnson/jaxsw . [ 19 ] Patrick Kidger . On Neural Differential Equations . PhD thesis , University of Oxford , 2021 . [ 20 ] Patrick Kidger and Cristian Garcia . Equinox : neural networks in JAX via callable PyTrees and filtered transformations . Differentiable Programming workshop at Neural Information Processing Systems 2021 , 2021 . [ 21 ] Jorge Nocedal and Stephen Wright . Numerical Optimization ( Second Edition ) . Springer New York , 2006 . [ 22 ] Alan V. Oppenheim and Ronald W. Schafer . Digital Signal Processing . Prentice-Hall , 1975 . [ 23 ] R. Penrose . A generalized inverse for matrices . Mathematical Proceedings of the Cambridge Philosophical Society , 51 ( 3 ) :406–413 , 1955 . [ 24 ] Youcef Saad and Martin H. Schultz . Gmres : A generalized minimal residual algorithm for solving nonsymmetric linear systems . SIAM Journal on Scientific and Statistical Computing , 7 ( 3 ) :856–869 , 1986 . [ 25 ] Miloš Stanojevi´c and Laurent Sartran . SynJax : Structured Probability Distributions for JAX . arXiv preprint arXiv:2308.03291 , 2023 . [ 26 ] J. W. Thomas . Numerical Partial Differential Equations : Finite Difference Methods . Springer New York , 1995 . [ 27 ] Lloyd N. Trefethen and David Bau . Numerical Linear Algebra . Society for Industrial and Applied Mathematics , 1997 . [ 28 ] H. A. van der Vorst . Bi-cgstab : A fast and smoothly converging variant of bi-cg for the solution of nonsymmetric linear systems . SIAM Journal on Scientific and Statistical Computing , 13 ( 2 ) :631–644 , 1992 . [ 29 ] Jake Vanderplas . Jep 18137 : Scope of jax numpy & scipy wrappers . Github pull request , created and accessed 2023 , 2023 . URL : https : //github.com/google/jax/pull/18137 . 8","['c', 'v', 'r', 'lineax', 'unified', 'linear', 'solve', 'linear', 'leastsquare', 'jax', 'math', 'abstract', 'introduce', 'lineax', 'library', 'bring', 'linear', 'solve', 'linear', 'leastsquare', 'jaxequinox', 'scientific', 'computing', 'ecosystem', 'lineax', 'use', 'general', 'linear', 'operator', 'unify', 'linear', 'solve', 'leastsquare', 'single', 'autodifferen', 'api', 'solver', 'operator', 'userextensible', 'require', 'user', 'implement', 'custom', 'derivative', 'rule', 'get', 'differentiability', 'lineax', 'available', 'githubcomgooglelineax', 'introduction', 'jax', 'autodifferentiatiable', 'python', 'framework', 'popular', 'machine', 'learning', 'scientific', 'comput', 'equinox', 'popular', 'jax', 'library', 'target', 'use', 'case', 'add', 'additional', 'support', 'parameterise', 'function', 'solve', 'linear', 'system', 'wellpose', 'linear', 'solve', 'illpose', 'linear', 'leastsquare', 'problem', 'central', 'subproblem', 'scientific', 'computing', 'example', 'linear', 'solve', 'leastsquare', 'appear', 'subroutine', 'nonlinear', 'optimisation', 'finitedifference', 'scheme', 'signal', 'processing', 'introduce', 'lineax', 'library', 'build', 'jax', 'equinox', 'linear', 'solve', 'linear', 'leastsquare', 'lineax', 'present', 'single', 'differentiable', 'interface', 'solve', 'wellpose', 'underdetermined', 'overdetermine', 'linear', 'system', 'also', 'allow', 'user', 'write', 'custom', 'differentiable', 'linear', 'solver', 'leastsquare', 'solver', 'introduce', 'linear', 'operator', 'abstraction', 'overall', 'intend', 'lineax', 'integrate', 'well', 'exist', 'jax', 'scientific', 'ecosystem', 'ecosystem', 'grow', 'include', 'package', 'differentiable', 'rigidbody', 'physics', 'simulation', 'computational', 'fluid', 'dynamic', 'protein', 'structure', 'prediction', 'ordinary', 'stochastic', 'differential', 'equation', 'probabilistic', 'modeling', 'begin', 'see', 'use', 'lineax', 'ecosystem', 'already', 'include', 'linear', 'subroutine', 'ocean', 'dynamic', 'optimal', 'transport', 'diffrax', 'plan', 'adopt', 'lineax', 'near', 'future', 'linear', 'subroutine', 'differential', 'equation', 'solve', 'main', 'contribution', 'main', 'contribution', 'lineax', 'general', 'linear', 'operator', 'abstraction', 'implement', 'dense', 'matrix', 'linear', 'function', 'jacobian', 'stable', 'fast', 'gradient', 'leastsquare', 'solve', 'include', 'userdefined', 'solver', 'require', 'extra', 'effort', 'user', 'pytreevalue', 'operator', 'vector', '1jax', 'terminology', 'arbitrarily', 'nest', 'container', 'type', 'type', 'taining', 'leave', 'pythonjax', 'type', 'exclusively', 'consider', 'pytree', 'leave', 'jax', 'array', 'neurip', 'ai', 'science', 'workshop', 'comparison', 'exist', 'jax', 'apis', 'operator', 'abstraction', 'introduce', 'lineax', 'offer', 'flexibility', 'find', 'support', 'dense', 'matrix', 'matrixvector', 'product', 'representation', 'operator', 'lineax', 'introduce', 'new', 'solver', 'lineaxtridiagonal', 'lineax', 'also', 'offer', 'consistent', 'api', 'operator', 'solver', 'allow', 'extensibility', 'userspecifie', 'custom', 'operator', 'solver', 'compilation', 'time', 'lineax', 'solver', 'essentially', 'identical', 'jax', 'native', 'solver', 'lineax', 'iterative', 'solver', 'cg', 'gmre', 'compile', 'roughly', 'twice', 'fast', 'benchmark', 'folder', 'provide', 'quantitative', 'comparison', 'emphasise', 'stable', 'fast', 'gradient', 'contrast', 'exist', 'jax', 'implementation', 'version', 'exhibit', 'instability', 'incorrect', 'gradient', 'exceptional', 'case', 'reason', 'actually', 'consider', 'deprecate', 'apis', 'favour', 'lineax', 'classical', 'linear', 'solve', 'example', 'consider', 'solve', 'ax', 'b', 'random', 'matrix', 'r10×10', 'random', 'vector', 'r10', 'import', 'jax', 'random', 'jr', 'import', 'lineax', 'lx', 'akey', 'bkey', 'jr', 'split', 'jr', 'prngkey', 'lx', 'atr', 'op', 'ator', 'normal', 'akey', 'b', 'jr', 'normal', 'bkey', 'solution', 'lx', 'linearsolve', 'b', 'perform', 'linear', 'solve', 'leastsquare', 'main', 'entry', 'point', 'linear', 'solve', 'leastsquare', 'lineax', 'linearsolve', 'b', 'solver', 'linear', 'operator', 'pytree', 'b', 'perform', 'linear', 'solve', 'ax', 'b', 'wellpose', 'system', 'return', 'leastsquare', 'solution', 'minx', 'b∥2', 'overdetermine', 'system', 'return', 'minimum', 'norm', 'solution', 'minx', 'subject', 'ax', 'b', 'underdetermined', 'system', 'lot', 'operation', 'unify', 'together', 'initially', 'seem', 'strange', 'common', 'thread', 'justification', 'unify', 'operation', 'mathematically', 'operation', 'correspond', 'pseudoinverse', 'solution', 'ax', 'b', 'solution', 'arise', 'use', 'moorepenrose', 'pseudoinverse', 'user', 'specify', 'solver', 'like', 'use', 'solver', 'argument', 'helpful', 'user', 'already', 'know', 'solver', 'work', 'well', 'problem', 'solver', 'capable', 'handle', 'problem', 'example', 'handle', 'positive', 'definite', 'operator', 'section', 'use', 'solver', 'incompatible', 'problem', 'result', 'error', 'general', 'linear', 'operator', 'lineax', 'represent', 'generally', 'n', '×', 'matrix', 'instead', 'represent', 'linear', 'operator', 'x', 'space', 'pytree', 'array', 'implementation', 'level', 'linear', 'operator', 'object', 'subclasse', 'lineax', 'bs', 'ne', 'r', 'op', 'e', 'rat', 'dense', 'matrix', 'rdim', 'treat', 'lineax', 'linear', 'operator', 'ator', 'lineax', 'operator', 'form', 'vector', 'space', 'close', 'addition', 'scalar', 'multiplication', 'composition', 'linear', 'operator', 'implement', 'method', 'compute', 'matrixvector', 'product', 'ax', 'compute', 'transpose', 'operator', 'materialise', 'operator', 'matrix', 'retrieve', 'inputoutput', 'pytree', 'structure', 'well', 'inputoutput', 'dimension', 'function', 'domain', 'x', 'codomain', 'increase', 'generality', 'come', 'increase', 'flexibility', 'example', 'large', 'sparse', 'matrix', 'use', 'dataefficient', 'format', 'utilise', 'linear', 'solve', 'use', 'matrixvector', 'product', 'gmre', 'bicgstab', 'example', 'linear', 'function', 'make', 'linear', 'operator', 'ne', 'r', 'op', 'e', 'rat', 'instructure', 'instructure', 'describe', 'pytree', 'structure', 'input', 'equivalently', 'pytree', 'structure', 'element', 'similarly', 'nonlinear', 'function', 'linearise', 'point', '∈', 'use', 'jacobian', 'linear', 'operator', 'ne', 'r', 'op', 'e', 'rat', 'g', 'lineaxabstractlinearoperator', 'base', 'class', 'available', 'user', 'subclass', 'create', 'linear', 'operator', 'type', 'operator', 'tag', 'tag', 'optional', 'argument', 'linear', 'operator', 'indicate', 'property', 'operator', 'example', '∈', 'rn×n', 'positive', 'semidefinite', 'mark', 'positive', 'semidefinite', 'operator', 'ator', 'lineax', 'p', 'e', 'e', 'e', 'g', 'indicate', 'solver', 'use', 'positive', 'semidefinite', 'example', 'also', 'nonsingular', 'use', 'safely', 'lineaxcg', 'tag', 'also', 'use', 'select', 'appropriate', 'solver', 'lineaxautolinearsolver', 'detail', 'section', 'computing', 'gradient', 'jax', 'derivative', 'build', 'product', 'vectorjacobian', 'product', 'vjps', 'reversemode', 'automatic', 'differentiation', 'respectively', 'jvp', 'function', 'rb', 'map', 'inputtangent', 'pair', '∈', 'rb', 'rb', 'jacobian', 'map', 'inputcotangent', 'pair', 'c', 'c', '∈', 'transpose', 'major', 'contribution', 'lineax', 'exist', 'linear', 'solve', 'leastsquare', 'software', 'efficient', 'computation', 'pseudoinverse', 'solution', 'differentiation', 'wellpose', 'linear', 'solve', 'illpose', 'leastsquare', 'solve', 'perform', 'manner', 'particular', 'special', 'case', 'operator', 'full', 'row', 'column', 'rank', 'order', 'obtain', 'improved', 'performance', 'show', 'forwardmode', 'autodifferentiation', 'section', 'let', 'l', 'b', 'denote', 'linear', 'solve', 'lineaxlinearsolve', 'primal', 'problem', 'ax', 'l', 'b', 'a†', 'moore', 'penrose', 'pseudoinverse', 'mention', 'section', 'discuss', 'compute', 'jvp', 'b', 'v', 'v', 'tangent', 'pair', 'consist', 'tangent', 'operator', 'v', 'tangent', 'vector', 'possible', 'compute', 'jvp', 'argument', 'example', 'tangent', 'computation', 'linear', 'solve', 'function', 'v', 'alone', 'b', 'represent', 'tangent', 'operator', 'meanwhile', 'compute', 'jvp', 'b', 'v', 'require', 'differentiate', 'pseudoinverse', 'explicit', 'formula', 'b', 'v', 'a†', 'a†', 'a†', 'a†', 'let', 'z', 'a†', 'add', 'equation', 'together', 'use', 'linearity', 'jacobian', 'total', 'jvp', 'respect', 'primal', 'pair', 'b', 'tangent', 'pair', 'v', 'lineaxlinearsolve', 'b', 'a†', 'cid0', '−v', 'ax', 'cid1', 'z', 'linearly', 'independent', 'column', 'section', 'term', 'z', 'give', '∂l', 'b', 'a†', 'cid0', '−v', 'ax', 'cid1', 'linearly', 'independent', 'row', 'ax', 'b', 'a†', 'z', 'together', 'linearly', 'independent', 'row', 'column', 'wellpose', 'a†', 'true', 'inverse', 'b', 'select', 'equation', 'depend', 'know', 'compile', 'time', 'linearly', 'independent', 'row', 'column', 'independent', 'row', 'independent', 'column', 'dependent', 'row', 'column', 'property', 'operator', 'compile', 'time', 'main', 'way', 'jvp', 'rule', 'dispatch', 'choice', 'solver', 'solver', 'support', 'dependent', 'rowscolumn', 'section', 'return', 'value', 'use', 'solve', 'unsupported', 'operator', 'solver', 'support', 'dependent', 'rowscolumn', 'sure', 'get', 'solution', 'give', 'operator', 'dependent', 'rowscolumn', 'jvp', 'example', 'section', 'handle', 'dependent', 'row', 'number', 'row', 'great', 'number', 'column', 'dependent', 'column', 'number', 'column', 'great', 'number', 'row', 'dispatch', 'equation', 'number', 'column', 'row', 'dispatch', 'equation', 'vjps', 'backpropogation', 'transposition', 'reversemode', 'autodifferentiation', 'function', 'rb', 'build', 'rather', 'vectorjacobian', 'product', 'vjps', 'vt', 'v', 'suggest', 'definition', 'vjps', 'construct', 'jvp', 'transposition', 'vjps', 'implement', 'thus', 'lineax', 'well', 'jacobian', 'b', 'rm×n', '×', 'rm', 'linear', 'function', 'see', 'also', 'explicit', 'form', 'equation', 'therefore', 'transpose', '∂l', 'b', 'rm×n', '×', 'rm', 'transpose', 'rule', 'linear', 'solve', 'implement', 'custom', 'jax', 'primitive', 'see', 'detail', 'userdefined', 'solver', 'user', 'implement', 'custom', 'solver', 'subclasse', 'lver', 'require', 'method', 'init', 'compute', 'transpose', 'allowdependentrow', 'allowdependentcolumn', 'many', 'direct', 'linear', 'solver', 'ax', 'b', 'use', 'stage', 'computation', 'first', 'factor', 'form', 'amenable', 'computation', 'factorisation', 'section', 'factorisation', 'factorisation', 'use', 'factorisation', 'solve', 'give', 'right', 'hand', 'side', 'b', 'factorisation', 'depend', 'right', 'hand', 'side', 'reuse', 'various', 'choice', 'b', 'save', 'computation', 'cost', 'solve', 'ax', 'b', 'many', 'right', 'hand', 'b', 'solver', 'use', 'twostage', 'factorisation', 'approach', 'compute', 'factorisation', 'compute', 'perform', 'solve', 'specific', 'right', 'hand', 'b', 'transpose', 'compute', 'transpose', 'factorisation', 'provide', 'init', 'allow', 'skip', 'compute', 'factorisation', 'transpose', 'operator', 'directly', 'init', 'transpose', 'operator', 'commonly', 'case', 'cheaply', 'derive', 'init', 'operator', 'alone', 'need', 'compute', 'discuss', 'previous', 'section', 'method', 'allowdependentrow', 'allowdependentcolumn', 'determine', 'equation', 'use', 'differentiation', 'discuss', 'section', 'greatly', 'simplify', 'process', 'write', 'differentiable', 'linear', 'solver', 'leastsquare', 'solver', 'library', 'somewhat', 'cumbersome', 'write', 'differentiable', 'solver', 'require', 'use', 'jaxlaxcustomlinearsolve', 'implement', 'solver', 'transposition', 'rule', 'transposesolve', 'user', 'like', 'use', 'reversemode', 'autodifferentiation', 'lineax', 'differentiation', 'come', 'free', 'solver', 'implement', 'solver', 'linear', 'solve', 'leastsquare', 'autolinearsolver', 'polyalgorithm', 'user', 'provide', 'solver', 'lineaxlinearsolve', 'default', 'linear', 'solve', 'lineaxautolinearsolver', 'lineaxautolinearsolver', 'polyalgorithm', 'select', 'solver', 'automatically', 'compile', 'time', 'depend', 'structure', 'indicate', 'operator', 'tag', 'discuss', 'section', 'lineaxautolinearsolver', 'take', 'argument', 'wellpose', 'indicate', 'system', 'expect', 'solve', 'leastsquaresminimum', 'norm', 'problem', 'handle', 'wellpose', 'linear', 'solve', 'lineaxautolinearsolver', 'wellposedtrue', 'select', 'solver', 'depend', 'oper', 'ator', 'structure', 'throw', 'error', 'encounter', 'underdeteremined', 'overdeter', 'mined', 'system', 'lineaxautolinearsolver', 'wellposedfalse', 'solve', 'wellpose', 'linear', 'solve', 'well', 'linear', 'least', 'square', 'often', 'additional', 'computation', 'cost', 'finally', 'lineaxautolinearsolver', 'wellposednone', 'solve', 'leastsquare', 'problem', 'expensive', 'specific', 'polyalgorithm', 'wellposedtrue', 'wellposedfalse', 'wellposednone', 'show', 'figure', 'choose', 'solver', 'compile', 'time', 'choose', 'paradigm', 'implementation', 'lineaxautolinearsolver', 'make', 'selection', 'run', 'time', 'make', 'selection', 'compile', 'time', 'tradeoff', 'determine', 'solver', 'use', 'run', 'time', 'mean', 'check', 'element', 'matrix', 'incur', 'run', 'time', 'overhead', 'n2', 'n', 'n', 'matrix', 'relatively', 'small', 'compare', 'n3', 'run', 'time', 'linear', 'solve', 'algorithm', 'however', 'run', 'time', 'check', 'also', 'incur', 'great', 'cost', 'compile', 'time', 'know', 'compile', 'time', 'branch', 'polyalgorithm', 'run', 'compiler', 'force', 'compile', 'branch', 'thus', 'compilation', 'cost', 'scale', 'logic', 'polyalgorithm', 'branch', 'include', 'compile', 'time', 'increase', 'limit', 'extensibility', 'compile', 'time', 'selection', 'avoid', 'performance', 'issue', 'fast', 'run', 'time', 'compile', 'time', 'use', 'correctly', 'simplify', 'tie', 'solve', 'hardware', 'possibility', 'take', 'separate', 'branch', 'different', 'batch', 'element', 'however', 'compile', 'time', 'selection', 'require', 'user', 'know', 'structure', 'operator', 'result', 'use', 'suboptimal', 'solver', 'operator', 'exploitable', 'structure', 'user', 'indicate', 'choose', 'compile', 'time', 'approach', 'require', 'user', 'pass', 'structure', 'operator', 'explic', 'itly', 'operator', 'tag', 'section', 'choose', 'approach', 'primarily', 'minimise', 'compilation', 'time', 'avoid', 'tradeoff', 'extensibility', 'compile', 'time', 'inherent', 'run', 'time', 'check', 'autolinearsolver', 'polyalgorithm', 'figure', 'wellposedtrue', 'start', 'square', 'wellposednone', 'start', 'diagonal', 'wellposedfalse', 'start', 'diagonal', 'section', 'read', 'lefttoright', 'approach', 'contrast', 'mldivide', 'nondifferentiable', 'unified', 'linear', 'solve', 'leastsquare', 'solver', 'use', 'run', 'time', 'approach', 'language', 'offer', 'method', 'nonsingular', 'linear', 'solve', 'infix', 'operation', 'linsolve', 'accept', 'compile', 'time', 'tag', 'still', 'perform', 'run', 'time', 'check', 'user', 'pass', 'tag', 'therefore', 'suffer', 'additional', 'overhead', 'run', 'time', 'approach', 'many', 'case', 'conclusion', 'introduce', 'lineax', 'differentiable', 'jaxequinox', 'library', 'unify', 'linear', 'solve', 'linear', 'leastsquare', 'demonstrate', 'user', 'extend', 'base', 'lineax', 'operator', 'solver', 'use', 'unified', 'api', 'need', 'write', 'custom', 'derivative', 'rule', 'hope', 'see', 'adoption', 'lineax', 'solve', 'jaxequinox', 'scientific', 'computing', 'machine', 'learn', 'ecosystem', 'acknowledgement', 'publication', 'base', 'work', 'support', 'epsrc', 'centre', 'doctoral', 'training', 'matic', 'random', 'system', 'analysis', 'modelling', 'simulation', 'reference', 'adi', 'generalize', 'inverse', 'theory', 'application', 'viral', 'shah', 'fresh', 'approach', 'url', 'https', 'epubssiamorg', 'doi101137141000671', 'doi101137141000671', 'deniz', 'bezgin', 'buhendwa', 'nikolaus', 'adam', 'jaxfluid', 'fully', 'differentiable', 'highorder', 'computational', 'fluid', 'dynamic', 'solver', 'compressible', 'twophase', 'flow', 'computer', 'physics', 'communication', 'frostig', 'dougal', 'vanderpla', 'skye', 'wandermanmilne', 'jax', 'composable', 'transformation', 'pythonnumpy', 'program', 'url', 'http', 'githubcomgooglejax', 'marco', 'olivi', 'teboul', 'optimal', 'transport', 'tool', 'ott', 'jax', 'toolbox', 'thing', 'wasserstein', 'arxiv', 'preprint', 'dresdner', 'dmitrii', 'jamie', 'brenner', 'hoyer', 'learn', 'correct', 'spectral', 'method', 'simulate', 'turbulent', 'flow', 'arxiv', 'preprint', 'arxiv220700556', 'highperformance', 'unify', 'linear', 'solver', 'access', 'url', 'https', 'haliax', 'access', 'url', 'stanfordcrfmhaliax', 'igor', 'babuschkin', 'ecosystem', 'url', 'deepmind', 'highly', 'accurate', 'protein', 'structure', 'prediction', 'alphafold', 'nature', 'c', 'anton', 'raichuk', 'sertan', 'girgin', 'igor', 'mordatch', 'olivi', 'bachem', 'brax', 'differentiable', 'physics', 'engine', 'large', 'scale', 'rigid', 'body', 'simulation', 'roy', 'frostig', 'paszke', 'pose', 'reversemode', 'automatic', 'differentiation', 'arxiv210509469', 'gene', 'h', 'golub', 'pereyra', 'differentiation', 'pseudoinverse', 'nonlinear', 'least', 'square', 'problem', 'variable', 'separate', 'numerical', 'analysis', 'gene', 'h', 'golub', 'loan', 'matrix', 'computation', 'press', 'third', 'edition', 'levanter', 'legible', 'scalable', 'reproducible', 'foundation', 'model', 'jax', 'access', 'url', 'https', 'githubcomstanfordcrfm', 'levanter', 'ritter', 'flax', 'neural', 'network', 'library', 'ecosystem', 'jax', 'url', 'http', 'githubcomgoogleflax', 'url', 'https', 'wwwmathwork', 'com', 'approximate', 'ocean', 'model', 'access', 'url', 'kidger', 'neural', 'differential', 'equation', 'phd', 'thesis', 'kidger', 'equinox', 'neural', 'network', 'callable', 'pytree', 'filter', 'transformation', 'differentiable', 'programming', 'workshop', 'neural', 'information', 'processing', 'system', 'jorge', 'nocedal', 'optimization', 'second', 'edition', 'digital', 'signal', 'processing', 'prenticehall', 'r', 'penrose', 'generalized', 'inverse', 'matrix', 'mathematical', 'proceeding', 'cambridge', 'philosophical', 'society', 'schultz', 'gmre', 'generalized', 'minimal', 'residual', 'algorithm', 'solve', 'nonsymmetric', 'scientific', 'statistical', 'computing', 'miloš', 'stanojevi´c', 'laurent', 'sartran', 'synjax', 'structured', 'probability', 'distribution', 'jax', 'arxiv', 'preprint', 'numerical', 'partial', 'differential', 'equation', 'finite', 'difference', 'method', 'algebra', 'society', 'industrial', 'apply', 'mathematic', 'h', 'der', 'bicgstab', 'fast', 'smoothly', 'converge', 'variant', 'bicg', 'solution', 'nonsymmetric', 'scientific', 'statistical', 'computing', 'jake', 'vanderpla', 'jep', 'scope', 'numpy', 'scipy', 'wrapper', 'pull', 'request', 'create', 'access', 'url']"
"Lineax: unified linear solves and linear least-squares in JAX and
  Equinox","[{'href': 'http://arxiv.org/abs/2311.17283v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2311.17283v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-11-28 23:50:08,"3
2
0
2

v
o
N
2

]

G
L
.
s
c
[

1
v
0
5
4
1
0
.
1
1
3
2
:
v
i
X
r
a

DREAMSMOOTH: IMPROVING MODEL-BASED REIN-
FORCEMENT LEARNING VIA REWARD SMOOTHING

Vint Lee, Pieter Abbeel, Youngwoon Lee
University of California, Berkeley

ABSTRACT

Model-based reinforcement learning (MBRL) has gained much attention for its
ability to learn complex behaviors in a sample-efficient way: planning actions
by generating imaginary trajectories with predicted rewards. Despite its success,
we found that surprisingly, reward prediction is often a bottleneck of MBRL,
especially for sparse rewards that are challenging (or even ambiguous) to predict.
Motivated by the intuition that humans can learn from rough reward estimates, we
propose a simple yet effective reward smoothing approach, DreamSmooth, which
learns to predict a temporally-smoothed reward, instead of the exact reward at the
given timestep. We empirically show that DreamSmooth achieves state-of-the-art
performance on long-horizon sparse-reward tasks both in sample efficiency and
final performance without losing performance on common benchmarks, such as
Deepmind Control Suite and Atari benchmarks.

1

INTRODUCTION

Humans often plan actions with a rough estimate
of future rewards, instead of the exact reward at
the exact moment (Fiorillo et al., 2008; Klein-
Fl¨ugge et al., 2011). A rough reward estimate is
mostly sufficient to learn a task, and predicting
the exact reward is often challenging since it
can be ambiguous, delayed, or not observable.
Consider for instance the manipulation task il-
lustrated in Figure 1 (middle) of pushing a block
on a table into a bin, where a sparse reward is
given only on the timestep when the block first
touches the bin. Using the same image observa-
tions as the agent, it is challenging even for hu-
mans to predict the correct sequence of rewards.
Crucially, this issue is present in many environ-
ments, where states with no reward are almost
indistinguishable from those with rewards.

Figure 1: Predicting the exact sequence of rewards
is extremely difficult. These examples show the
sequences of image observations seen by the agent
just before and after it receives a sparse reward.
There is little to visually distinguish timesteps with
a large reward from those without, which creates a
significant challenge for reward prediction.

An accurate reward model is vital to model-
based reinforcement learning (MBRL) – reward
estimates that are too high will cause an agent
to choose actions that perform poorly in reality,
and estimates that are too low will lead an agent
to ignore high rewards. Despite its difficulty and
importance, the reward prediction problem in
MBRL has been largely overlooked. We find that even for the state-of-the-art MBRL algorithm,
DreamerV3 (Hafner et al., 2023), reward prediction is not only challenging, but is also a performance
bottleneck for many tasks. For instance, DreamerV3 fails to predict any reward for most objectives
in the Crafter environment (Hafner, 2022) with similar failure modes observed on variants of the
RoboDesk (Kannan et al., 2021) and Shadow Hand (Plappert et al., 2018) tasks with sparse rewards.

1

 
 
 
 
 
 
Inspired by the human intuition that only a rough estimate of rewards is sufficient, we propose a
simple yet effective solution, DreamSmooth, which learns to predict a temporally-smoothed reward
rather than the exact reward at each timestep. This makes reward prediction much easier – instead of
having to predict rewards exactly, now the model only needs to produce an estimate of when sparse
rewards are obtained, which is sufficient for policy learning.

Our experiments demonstrate that while extremely simple, this technique significantly improves
performance of different MBRL algorithms on many sparse-reward environments. Specifically, we
find that for DreamerV3 (Hafner et al., 2023) and TD-MPC (Hansen et al., 2022), our technique
is especially beneficial in environments with the following characteristics: sparse rewards, partial
observability, and stochastic rewards. Finally, we show that even on benchmarks where reward
prediction is not a significant issue, DreamSmooth does not degrade performance, which indicates
that our technique can be universally applied.

2 RELATED WORK

Model-based reinforcement learning (MBRL) leverages a dynamics model (i.e. world model) of an
environment and a reward model of a desired task to plan a sequence of actions that maximize the
total reward. The dynamics model predicts the future state of the environment after taking a specific
action and the reward model predicts the reward corresponding to the state-action transition. With
the dynamics and reward models, an agent can simulate a large number of candidate behaviors in
imagination instead of in the physical environment, allowing MBRL to tackle many challenging
tasks (Silver et al., 2016; 2017; 2018).

Instead of relying on the given dynamics and reward models, recent advances in MBRL have enabled
learning a world model of high-dimensional observations and complex dynamics (Ha & Schmidhuber,
2018; Schrittwieser et al., 2020; Hafner et al., 2019; 2021; 2023; Hansen et al., 2022), as well as a
temporally-extended world model (Shi et al., 2022). Specifically, DreamerV3 (Hafner et al., 2023)
has achieved the state-of-the-art performance across diverse domains of problems, e.g., both with
pixel and state observations as well as both with discrete and continuous actions.

For realistic imagination, MBRL requires an accurate world model. There have been significant
efforts in learning better world models by leveraging human videos (Mendonca et al., 2023), by
adopting a more performant architecture (Deng et al., 2023), and via representation learning, such
as prototype-based (Deng et al., 2022) and object-centric (Singh et al., 2021) state representations,
contrastive learning (Okada & Taniguchi, 2021), and masked auto-encoding (Seo et al., 2022; 2023).

However, compared to the efforts on learning a better world model, learning an accurate reward
model has been largely overlooked. Babaeizadeh et al. (2020) investigates the effects of various world
model designs and shows that reward prediction is strongly correlated to task performance when
trained on an offline dataset, while limited to dense-reward environments. In this paper, we point out
that accurate reward prediction is crucial for MBRL, especially in sparse-reward tasks and partially
observable environments, and propose a simple method to improve reward prediction in MBRL.

3 APPROACH

The main goal of this paper is to understand how challenging reward prediction is in model-based
reinforcement learning (MBRL) and propose a simple yet effective solution, reward smoothing, which
makes reward prediction easier to learn. In this section, we first provide a background about MBRL
in Section 3.1, then present experiments demonstrating the challenge of predicting sparse reward
signals in Section 3.2, and finally explain our approach, DreamSmooth, in Section 3.4.

3.1 BACKGROUND

We formulate a problem as a partially observable Markov decision process (POMDP), which is
defined as tuple (O, A, P, R, γ). O is an observation space, A is an action space, P (ot+1|o≤t, a≤t)
with timestep t is a transition dynamics, R is a reward function that maps previous observations and
actions to a reward rt = R(o≤t, a≤t), and γ ∈ [0, 1) is a discount factor (Sutton & Barto, 2018). RL
aims to find a policy π(at | o≤t, a<t) that maximizes the expected sum of rewards Eπ[(cid:80)T
t=1 γt−1rt].

2

This paper focuses on MBRL algorithms that learn a world model Pθ(zt+1|zt, at) and reward model
Rθ(rt|zt) from agent experience, where zt is a learned latent state at timestep t. The learned world
model and reward model can then generate imaginary rollouts {zτ , aτ , rτ }t+H−1
of the horizon H
starting from any zt, which can be used for planning (Argenson & Dulac-Arnold, 2021; Hansen et al.,
2022) or policy optimization (Ha & Schmidhuber, 2018; Hafner et al., 2019). Specifically, we use the
state-of-the-art algorithms, DreamerV3 (Hafner et al., 2023) and TD-MPC (Hansen et al., 2022).

τ =t

DreamerV3 (Hafner et al., 2023) uses the predicted rewards for computing new value targets to train
the critic. For learning a good policy, the reward model plays a vital role since the critic, from which
the actor learns a policy, receives its training signal exclusively through the reward model. Note that
the data collected from the environment is only used for training a world model and reward model.

On the other hand, TD-MPC (Hansen et al., 2022) learns a state-action value function Q(zt, at)
directly from agent experience, not from predicted rewards. However, the reward model is still
important for obtaining a good policy in TD-MPC because the algorithm uses both the reward model
and value function to obtain the policy through online planning.

3.2 REWARD PREDICTION IS DIFFICULT

Reward prediction is surprisingly challenging in many environments. Figure 1 shows sequences of
frames right before and after sparse rewards are received in diverse environments. Even for humans,
it is difficult to determine the exact timestep when the reward is received in all three environments.
We hypothesize that the mean squared error loss E(z,r)∼D[(Rθ(z) − r)2], typically used for reward
model training, deteriorates reward prediction accuracy when there exist sparse rewards. This is
because predicting a sparse reward a single step earlier or later results in a higher loss than simply
predicting 0 reward at every step. Thus, instead of trying to predict sparse rewards at the exact
timesteps, a reward model minimizes the loss by entirely omitting sparse rewards from its predictions.

To verify this hypothesis, we plot the ground-truth and DreamerV3’s predicted rewards in Figure 2.
The reward models struggle at predicting exact rewards and simply ignore sparse rewards unless they
are straightforward to predict on the four tasks described in Section 4.1. This hypothesis also holds in
a deterministic and fully-observable environment, Crafter, which has 24 sources of sparse rewards.
The reward model fails to predict most of these reward sources (Figure 2d).

The difficulty of reward prediction can be further exacerbated by partial observability, ambiguous
rewards, or stochastic dynamics of environments. As an example in the first (third) row in Figure 1,

(a) RoboDesk

(b) Hand

(c) Earthmoving

(d) Crafter

Figure 2: Ground truth rewards and DreamerV3’s predicted rewards over an evaluation episode. The
reward model misses many sparse rewards, which is highlighted in yellow.

3

Ground Truth

Predicted

x-axis: Timestep

y-axis: Reward

330

0

−110

0

120

330

0

−110

0

120

2400

0

−800

0

360

1.2

0.0

−0.4

0

200

the sparse rewards are given when the block (the rocks in the third example) first contacts the bin (the
dumptruck). The exact moment of contact is not directly observable from the camera viewpoint, and
this makes reward prediction ambiguous. Moreover, stochastic environment dynamics, e.g., contact
between multiple rocks, can make predicting a future state and reward challenging.

3.3 REWARD PREDICTION IS A BOTTLENECK OF MBRL

The preceding section shows that reward prediction is challenging in many environments. More
importantly, this poor reward prediction can be a bottleneck of policy learning, as shown in Figure 3.
In RoboDesk, where the reward model does not reliably detect the completion of the second task
(Figure 2a), the policy gets stuck at solving the first task and fails on subsequent tasks. In Earthmoving,
where the reward model cannot capture rewards for successful dumping (Figure 2c), the policy
frequently drops the rocks outside the dumptruck. These consistent failure modes in reward prediction
and policy learning in DreamerV3 suggest that poor reward prediction can be a bottleneck of MBRL.

(a) RoboDesk

(b) Earthmoving

Figure 3: The reward model’s inability to predict sparse rewards for completing tasks leads to poor
task performance. (a) In RoboDesk, the agent gets stuck after learning the first task, and is unable to
learn to perform the subsequent tasks. (b) In Earthmoving, the policy often fails to dump the rocks
accurately into the dumptruck. The learning curves are averaged over 3 seeds.

3.4 DREAMSMOOTH: IMPROVING MBRL VIA REWARD SMOOTHING

To address the reward prediction problem,
we propose a simple yet effective solution,
DreamSmooth, which relaxes the require-
ment for the model to predict sparse rewards
at the exact timesteps by performing tempo-
ral smoothing. Allowing the reward model to
predict rewards that are off from the ground
truth by a few timesteps makes learning eas-
ier, especially when rewards are ambiguous
or sparse.

(a) Gaussian

(b) Uniform

(c) EMA

Figure 4: Reward smoothing on sparse reward 1 at
t = 4. σ, δ, and α are smoothing hyperparameters.

Specifically, DreamSmooth applies temporal smoothing to the rewards upon collecting each new
episode. DreamSmooth can work with any smoothing function f that preserves the sum of rewards:

˜rt ← f (rt−L:t+L) =

L
(cid:88)

i=−L

fi · rclip(t+i,0,T )

s.t.

L
(cid:88)

i=−L

fi = 1,

(1)

where T and L denote the episode and smoothing horizons, respectively. For simplicity, we omit the
discount factor in Equation (1); the full equation can be found in Appendix, Equation (6). Episodes
with the smoothed rewards are stored in the replay buffer and used to train the reward model. The
agent learns only from the smoothed rewards, without ever seeing the original rewards. The smoothed
rewards ease reward prediction by allowing the model to predict rewards several timesteps earlier or
later, without incurring large losses. In this paper, we investigate three popular smoothing functions:
Gaussian, uniform, and exponential moving average (EMA) smoothing, as illustrated in Figure 4.

While the main motivation for smoothing is to make it easier to learn reward models, we note that
reward smoothing in some cases preserves optimality – an optimal policy under smoothed rewards ˜r

4

3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

8

16

24

Environment steps (×10⁶)

 
2

1

d
e
p
m
u
D
s
k
c
o
R

0

0

4

8

12

Environment steps (×10⁶)

 
1

d
r
a
w
e
R

0

0

Orginal
σ = 1
σ = 2

2

4
Timestep

6

8

1

d
r
a
w
e
R

0

0

Orginal
δ = 3
δ = 7

2

4
Timestep

6

8

1

d
r
a
w
e
R

0

0

Orginal
α = .3
α = .6

2

4
Timestep

6

8

is also optimal under the original rewards r. In particular, we provide a proof in Appendix A for the
optimality of EMA smoothing (and any smoothing function where ∀i > 0, fi = 0) by augmenting the
POMDP states with the history of past states. However, when future rewards are used for smoothing
(e.g. Gaussian smoothing), the smoothed rewards are conditioned on policy, and we can no longer
define an equivalent POMDP. In such cases, there is no theoretical guarantee. Even so, we empirically
show that reward models can adapt their predictions alongside the changing policy, and achieve
performance improvements.

The implementation of DreamSmooth is extremely simple, requiring only one additional line of
code to existing MBRL algorithms, as shown in Algorithm 1. The overhead of reward smoothing is
minimal, with time complexity O(T · L). More implementation details can be found in Appendix B.

Algorithm 1 COLLECT ROLLOUT (π: policy, D: replay buffer) in DREAMSMOOTH

t=1} ← ROLLOUT(π)

{(ot, at, rt)T
{rt}T
D ← D ∪ {(ot, at, rt)T

t=1 ← GAUSSIAN({rt}T

t=1}

t=1, σ) or EMA({rt}T

t=1, α)

▷ only one line needs to be added.

4 EXPERIMENTS

In this paper, we propose a simple reward smoothing method, DreamSmooth, which facilitates reward
prediction in model-based reinforcement learning (MBRL) and thus, improves the performance of
existing MBRL methods. Through our experiments, we aim to answer the following questions:
(1) Does reward smoothing improve reward prediction? (2) Does better reward prediction with reward
smoothing lead to better sample efficiency and asymptotic performance of MBRL in sparse-reward
tasks? (3) Does MBRL with reward smoothing also work in common dense-reward tasks?

4.1 TASKS

We evaluate DreamSmooth on four tasks with sparse subtask completion rewards and two common
RL benchmarks. Earthmoving uses two 64 × 64 images as an observation while all other tasks use a
single image. See Appendix C for environment details.

• RoboDesk: We use a modified version of RoboDesk (Kannan et al., 2021), where a sequence of ma-
nipulation tasks (flat block in bin, upright block off table, push green)
need to be completed in order (Figure 5a). We use the original dense rewards together with
a large sparse reward for each task completed.

• Hand: The Hand task (Plappert et al., 2018) requires a Shadow Hand to rotate a block in hand into
a specific orientation. We extend it to achieve a sequence of pre-defined goal orientations in order.
In addition to the original dense rewards, we provide a large sparse reward for each goal.

• Earthmoving: The Earthmoving task consists of a wheel loader, dump truck, and a pile of rocks
(Figure 5c). The agent controls the wheel loader to pick up rocks from the pile and dump them in
the dump truck. A large sparse reward is given for each rock picked up and for each rock dumped,
proportional to its mass. In addition, dense rewards are given for moving rocks towards the dump
truck. The environment is simulated using the AGX Dynamics physics engine (Algoryx, 2020).

(a) RoboDesk

(b) Hand

(c) Earthmoving

(d) Crafter

(e) DMC

(f) Atari

Figure 5: We evaluate DreamSmooth on four tasks with sparse subtask completion rewards (a-d). We
also test on two popular benchmarks, (e) DeepMind Control Suite and (f) Atari.

5

(a) RoboDesk

(b) Hand

(c) Earthmoving

(d) Crafter

Figure 6: We visualize the ground truth rewards, smoothed rewards with Gaussian smoothing, and
predicted rewards by DreamerV3 trained on the smoothed rewards over an evaluation episode. In
contrast to Figure 2, the reward models with reward smoothing capture most of sparse rewards.

• Crafter: Crafter (Hafner, 2022) is a minecraft-like 2D environment, where the agent tries to
collect, place, and craft items in order to survive. There are 22 achievements in the environment
(e.g. collecting water, mining diamonds) with a sparse reward 1 for obtaining each achievement for
the first time. A small reward is given (or lost) for each health point gained (or lost).

• DMC: We benchmark 7 DeepMind Control Suite continuous control tasks (Tassa et al., 2018).
• Atari: We benchmark 6 Atari tasks (Bellemare et al., 2013) at 100K steps.

4.2

IMPROVED REWARD PREDICTION WITH REWARD SMOOTHING

We first visualize the ground truth rewards,
smoothed rewards (Gaussian smoothing), and re-
ward prediction results of DreamerV3 trained with
DreamSmooth in Figure 6. We observe that re-
ward smoothing leads to a significant improve-
ment in reward prediction: DreamSmooth success-
fully predicts most of the (smoothed) sparse re-
wards and no longer omits vital signals for policy
learning or planning.

The improvement is especially notable in Crafter.
In Figure 7, we measure the accuracy of the re-
ward model, (i.e. predicting a reward larger than
half of the original or smoothed reward for Dream-
erV3 and DreamSmooth respectively) at the exact
timesteps for each subtask. The vanilla Dream-
erV3’s reward model (baseline) misses most of
the sparse rewards while DreamSmooth predicts
sparse rewards more accurately in 15/19 subtasks.

4.3 RESULTS

Figure 7: Reward prediction rates for 19 achieve-
ments in Crafter. The other 3 tasks have been
never achieved by both methods. With reward
smoothing, the prediction rates are better in
15/19 tasks.

We compare the vanilla DreamerV3 (Hafner et al., 2023) with DreamSmooth, whose backbone is
also DreamerV3. For DreamSmooth, we evaluate Gaussian, uniform, and EMA smoothing. The

6

Ground Truth

Smoothed

Predicted

x-axis: Timestep

y-axis: Reward

64

00

0

120

60

0

−20

0

120

800

0

−200

0

360

0.6

0.0

−0.2

0

200

Baseline (no smoothing)

Gaussian

1.0

0.5

e
t
a
R
n
o
i
t
c
i
d
e
r
P

0.0

w

k

al

d
e
ain
alth g
e
H

nt

e
n

ace

d
e
n
o
o

bie
alth loss
g
xe
xe
ord
n
ord
ollect drin
plin
at skeleto
at co
ollect co
Place pla
d picka
e picka
Place sto
ollect sto
Place ta
m
d sw
e sw
Place furn
ollect w
ollect sa
at zo
E
ke sto
n
efe
ke w
ke sto
ke w
D
a
M

efe
C
D

n
o
o

o
o

C

C

C

C

e
H

p

ble
ke u
Wa

a
M
a
M

a
M

 
(a) RoboDesk

(b) Hand

(c) Earthmoving

(d) Crafter

(e) DMC

(f) Atari

Figure 8: Comparison of learning curves of DreamSmooth (Gaussian, Uniform, EMA) and Dream-
erV3. The shaded regions in (a-d) show the maximum and minimum over 3 seeds. For DMC (e) and
Atari (f), we aggregate results over 7 and 6 tasks respectively, and display the standard deviation.

hyperparameters for DreamerV3 and smoothing functions can be found in Appendix B. As shown in
Figure 8, DreamSmooth-Gaussian and DreamSmooth-Uniform significantly improve the performance
as well as the sample efficiency of DreamerV3 on the Robodesk, Hand, and Earthmoving tasks. The
only change between DreamerV3 and ours is the improved reward prediction, as shown in Section 4.2.
This result suggests that reward prediction is one of major bottlenecks of the MBRL performance.

While all smoothing methods lead to improvements over DreamerV3, Gaussian smoothing generally
performs the best, except on Crafter, with uniform smoothing showing comparable performance. The
better performance of Gaussian and uniform smoothing could be because it allows predicting rewards
both earlier and later, whereas EMA smoothing only allows predicting rewards later.

Despite the improved reward prediction accuracy, DreamSmooth-Gaussian and DreamSmooth-
Uniform perform worse than the baseline in Crafter. This can be because more predicted task
rewards encourage more exploitation and less exploration. Further investigation on this trade-off is a
promising direction for future work.

Moreover, we observe that on the DMC and Atari benchmarks, where reward prediction is not particu-
larly challenging, our technique shows comparable performance with the unmodified algorithms (see
Appendix, Figure 16 for full results). This suggests that reward smoothing can be applied generally,
and does not hinder performance on most environments.

7

DreamSmooth-Gaussian

DreamSmooth-Uniform

DreamSmooth-EMA

DreamerV3

3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

18
12
6
Environment steps (×10⁶)

24

 
3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

15
10
5
Environment steps (×10⁶)

20

 
d
e
p
m
u
D
s
k
c
o
R
f
o

r
e
b
m
u
N

2.0

1.5

1.0

0.5

0.0

0

3

6
Environment steps (×10⁶)

9

12

 
 
 
16

12

8

4

e
r
o
c
S

0
0.0

1.5
4.5
3.0
Environment steps (×10⁶)

6.0

1000

750

500

250

e
r
o
c
S

0
0.00

0.25
0.75
0.50
Environment steps (×10⁶)

1.00

e
r
o
c
S

d
e
z
i
l

a
m
r
o
N
n
a
m
u
H

0.6

0.4

0.2

0.0

0.0

0.1
0.3
0.2
Environment steps (×10⁶)

0.4

 
 
In Figure 9, DreamSmooth also improves
the performance of TD-MPC (Hansen et al.,
2022). In the Hand task, vanilla TD-MPC
is unable to consistently solve the first task,
even with proprioceptive state observations.
However, TD-MPC with DreamSmooth
learns to complete the tasks with not only
state observations but also pixel observa-
tions. This suggests that DreamSmooth can
be useful in a broad range of MBRL algo-
rithms that use a reward model. We only
demonstrate the Hand task since TD-MPC
fails on other sparse-reward tasks.

4.4 ABLATION STUDIES

(a) Hand (Pixel)

(b) Hand (State)

Figure 9: Learning curves for TD-MPC and TD-MPC
with DreamSmooth on the Hand task. The shaded re-
gions show the minimum and maximum over 3 seeds.

Data Imbalance. One possible cause of poor
reward predictions is data imbalance – because
sparse rewards are infrequent, sequences con-
taining sparse rewards are rarely sampled from
the replay buffer. The reward model therefore
trains on fewer examples of sparse rewards, po-
tentially leading to poor predictions. To test
this hypothesis, we conducted experiments with
oversampling: with probability p = 0.5, we
sample a sequence in which the agent receives
a sparse reward; otherwise, we sample uni-
formly from all sequences in the buffer. As
shown in Figure 10, oversampling performs bet-
ter than the baseline, but learns slower than
DreamSmooth. This suggests that while data
imbalance largely contributes to the difficulty
of reward prediction, it is not the only factor
hindering performance. Furthermore, this over-
sampling method requires domain knowledge about which reward signals to be oversampled while
DreamSmooth is agnostic to the scale and frequency of sparse rewards.

Figure 10: Using oversampling of sequences
with sparse rewards (p = 0.5) performs better
than DreamerV3 on RoboDesk, but worse than
DreamSmooth with Gaussian smoothing. The lines
show median task performance over 3 seeds, while
shaded regions show maximum and minimum.

Reward Model Size. Another hypothe-
sis for poor reward predictions is that the
reward model does not have enough ca-
pacity to capture sparse rewards. To test
this hypothesis, we increase the size of the
reward model from 4 layers of 768 units
to 5 layeres of 1024 units and 6 layers of
1280 units, while keeping the rest of the
world model the same. We observe in Fig-
ure 11 that without smoothing, changing
the reward model size has negligible impact
on performance, and DreamSmooth out-
performs all the reward model sizes tested.
This indicates that the reward prediction
problem is not simply caused by insuffi-
cient model capacity.

(a) RoboDesk

(b) Hand

Figure 11: Simply increasing the reward model size has
negligible impact on performance. DreamerV3-768 and
DreamSmooth use 4 layers of 768 units; DreamerV3-
1024 uses 5 layers of 1024 units; and DreamerV3-1280
uses 6 layers of 1280 units.

In Figure 12, we analyze the impact of the smoothing parameters σ and
Smoothing Parameter.
α for Gaussian and EMA, respectively, on RoboDesk and Hand. We observe that DreamSmooth is
insensitive to the smoothing parameters, performing well across a wide range of values.

8

DreamSmooth-Gaussian (TDMPC)

TD-MPC

l

d
e
t
e
p
m
o
C
s
k
s
a
T

1.2

0.8

0.4

0.0

0.0
0.5
1.0
Environment steps (×10⁶)

 
l

d
e
t
e
p
m
o
C
s
k
s
a
T

3

2

1

0

0
4
2
Environment steps (×10⁶)

 
3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

DreamSmooth
Oversampling
DreamerV3

6
18
12
Environment steps (×10⁶)

24

 
DreamSmooth
DreamerV3-1280

DreamerV3-1024
DreamerV3-768

l

d
e
t
e
p
m
o
C
s
k
s
a
T

3

2

1

0

6

0
24
12
Environment steps (×10⁶)

18

 
l

d
e
t
e
p
m
o
C
s
k
s
a
T

3

2

1

0

0
20
10
Environment steps (×10⁶)

 
(a) Gaussian Smoothing on RoboDesk

(b) Gaussian Smoothing on Hand

(c) Uniform Smoothing on RoboDesk

(d) Uniform Smoothing on Hand

(e) EMA Smoothing on RoboDesk

(f) EMA Smoothing on Hand

Figure 12: Parameter sweep over smoothing parameters σ, δ, and α. The lines show median task
performance over 3 seeds, while shaded regions show maximum and minimum.

5 CONCLUSION

In this paper, we identify the reward prediction problem in MBRL and provide a simple yet effective
solution, reward smoothing. Our approach, DreamSmooth, demonstrates superior performance in
sparse reward tasks where reward prediction is not trivial mainly due to the partial observability
or stochasticity of the environments. Moreover, DreamSmooth shows comparable results on the
commonly used benchmarks, DMC and Atari, showing its task-agnostic nature. Although we show
that our simple reward smoothing approach mitigates the difficulty in reward prediction, the improved
reward prediction does not always improve the task performance, e.g., in Crafter. This can be because
more predicted task rewards encourage more exploitation and less exploration. Further investigation
on this trade-off is a promising direction for future work.

9

DreamSmooth-Gaussian

(cid:15)(cid:3)(cid:8)(cid:3)(cid:4)

(cid:15)(cid:3)(cid:8)(cid:3)(cid:6)

(cid:15)(cid:3)(cid:8)(cid:3)(cid:7)

(cid:9)(cid:14)(cid:12)(cid:11)(cid:13)(cid:12)(cid:14)(cid:10)(cid:5)

3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

6

18
12
Environment steps (×10⁶)

24

 
3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

5

15
10
Environment steps (×10⁶)

20

 
DreamSmooth-Uniform

(cid:15)(cid:3)(cid:8)(cid:3)(cid:6)

(cid:15)(cid:3)(cid:8)(cid:3)(cid:7)

(cid:15)(cid:3)(cid:8)(cid:3)(cid:4)(cid:5)

(cid:9)(cid:14)(cid:12)(cid:11)(cid:13)(cid:12)(cid:14)(cid:10)(cid:5)

3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

6

18
12
Environment steps (×10⁶)

24

 
3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

5

15
10
Environment steps (×10⁶)

20

 
DreamSmooth-EMA

(cid:17)(cid:3)(cid:10)(cid:3)(cid:5)(cid:4)(cid:6)

(cid:17)(cid:3)(cid:10)(cid:3)(cid:5)(cid:4)(cid:7)(cid:8)

(cid:17)(cid:3)(cid:10)(cid:3)(cid:5)(cid:4)(cid:9)

(cid:11)(cid:16)(cid:14)(cid:13)(cid:15)(cid:14)(cid:16)(cid:12)(cid:6)

3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

6

18
12
Environment steps (×10⁶)

24

 
3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

5

15
10
Environment steps (×10⁶)

20

 
ACKNOWLEDGMENTS

This work was supported in part by the BAIR Industrial Consortium, an ONR DURIP grant, Komatsu,
and InnoHK Centre for Logistics Robotics. We would like to thank all members of the Berkeley
Robot Learning lab for their insightful feedback.

REFERENCES

Algoryx. AGX dynamics, 2020. URL https://www.algoryx.se/agx-dynamics/.

Arthur Argenson and Gabriel Dulac-Arnold. Model-based offline planning. In International Confer-
ence on Learning Representations, 2021. URL https://openreview.net/forum?id=
OMNB1G5xzd4.

Mohammad Babaeizadeh, Mohammad Taghi Saffar, Danijar Hafner, Harini Kannan, Chelsea Finn,
Sergey Levine, and Dumitru Erhan. Models, pixels, and rewards: Evaluating design trade-offs in
visual model-based reinforcement learning. arXiv preprint arXiv:2012.04603, 2020.

M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An
evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253–279,
jun 2013.

Fei Deng, Ingook Jang, and Sungjin Ahn. Dreamerpro: Reconstruction-free model-based reinforce-
ment learning with prototypical representations. In International Conference on Machine Learning,
pp. 4956–4975. PMLR, 2022.

Fei Deng, Junyeong Park, and Sungjin Ahn. Facing off world model backbones: Rnns, transformers,

and s4. arXiv preprint arXiv:2307.02064, 2023.

Christopher D Fiorillo, William T Newsome, and Wolfram Schultz. The temporal precision of reward

prediction in dopamine neurons. Nature neuroscience, 11(8):966–973, 2008.

David Ha and J¨urgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.

Danijar Hafner. Benchmarking the spectrum of agent capabilities. In International Conference on

Learning Representations, 2022.

Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. In International Conference on Learning Representations, 2019.

Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete

world models. In International Conference on Learning Representations, 2021.

Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains

through world models. arXiv preprint arXiv:2301.04104, 2023.

Nicklas Hansen, Xiaolong Wang, and Hao Su. Temporal difference learning for model predictive

control. In International Conference on Machine Learning, 2022.

Harini Kannan, Danijar Hafner, Chelsea Finn, and Dumitru Erhan. Robodesk: A multi-task rein-
forcement learning benchmark. https://github.com/google-research/robodesk,
2021.

Miriam C Klein-Fl¨ugge, Laurence T Hunt, Dominik R Bach, Raymond J Dolan, and Timothy EJ
Behrens. Dissociable reward and timing signals in human midbrain and ventral striatum. Neuron,
72(4):654–664, 2011.

Russell Mendonca, Shikhar Bahl, and Deepak Pathak. Structured world models from human videos.

In Robotics: Science and Systems, 2023.

Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations:
Theory and application to reward shaping. In International Conference on Machine Learning,
volume 99, pp. 278–287, 1999.

10

Masashi Okada and Tadahiro Taniguchi. Dreaming: Model-based reinforcement learning by latent
imagination without reconstruction. In IEEE International Conference on Robotics and Automation,
pp. 4209–4215, 2021. doi: 10.1109/ICRA48506.2021.9560734.

Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell,
Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforcement learn-
ing: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464,
2018.

Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon
Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari,
go, chess and shogi by planning with a learned model. Nature, 588(7839):604–609, 2020.

Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee, and Pieter

Abbeel. Masked world models for visual control. In Conference on Robot Learning, 2022.

Younggyo Seo, Junsu Kim, Stephen James, Kimin Lee, Jinwoo Shin, and Pieter Abbeel. Multi-view
masked world models for visual robotic manipulation. In International Conference on Machine
Learning, 2023.

Lucy Xiaoyang Shi, Joseph J. Lim, and Youngwoon Lee. Skill-based model-based reinforcement

learning. In Conference on Robot Learning, 2022.

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016.

David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. nature, 550(7676):354–359, 2017.

David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement
learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):
1140–1144, 2018.

Gautam Singh, Skand Peri, Junghyun Kim, Hyunseok Kim, and Sungjin Ahn. Structured world
belief for reinforcement learning in pomdp. In International Conference on Machine Learning, pp.
9744–9755. PMLR, 2021.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT Press, 2018.

Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,
Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A. Riedmiller.
Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.

11

A PROOFS

Let M = (S, A, P, R, γ) be the given MDP. Without loss of generality, we assume the augmented
form of the MDP M, where a state st includes the entire history of states, i.e., st = (s1, . . . , st),
and thus, reward functions R, ˜R have access to previous states, i.e., ˜R(st) = ˜R(s1, . . . , st).
Theorem A.1. An optimal policy ˜π∗ of the MDP with reward smoothing only with past rewards, e.g.,
EMA smoothing, ˜M = (S, A, P, ˜R, γ) is also optimal under the original MDP M, where

˜R(st) =

0
(cid:88)

i=−L

fi · γiR(st+i) and

0
(cid:88)

i=−L

fi = 1.

(2)

Proof. We will use the theorem of reward shaping that guarantees an optimal policy introduced in Ng
et al. (1999): if a modified reward function can be represented in the form of R(st)+γΦ(st+1)−Φ(st)
with any potential function Φ(st), the new reward function yields the same optimal policy with the
original reward function R.

Let the potential function for the EMA reward smoothing

Φ(st) = −

−1
(cid:88)

i=−L

γiR(st+i) +

0
(cid:88)

i=−L

γiR(st+i) ·

0
(cid:88)

fj.

j=i+1

(3)

Then, our reward shaping term in ˜R can be represented as the difference in the potential function
γΦ(st+1) − Φ(st) as follows:

γΦ(st+1) − Φ(st) = −R(st) +

0
(cid:88)

i=−L

fi · γiR(st+i).

R(st) + γΦ(st+1) − Φ(st) =

0
(cid:88)

i=−L

fi · γiR(st+i) = ˜R.

(4)

(5)

Hence, following Ng et al. (1999), reward shaping with our EMA smoothing guarantees the optimal
policy in the original MDP M.

However, Theorem A.1 does not apply to smoothing functions that require access to future rewards,
e.g., Gaussian smoothing. As in Gaussian smoothing, a smoothed reward function may require future
rewards, which are conditioned on the current policy; so is the reward model. In such cases, there is
no theoretical guarantee; but in our experiments, we empirically show that reward models can adapt
their predictions along the changes in policies and thus, improve MBRL.

Instead, we intuitively explain that an optimal policy under any reward smoothing (even though the
reward function is post hoc and cannot be defined for MDPs) is also optimal under the original reward
function.
Theorem A.2. An optimal policy ˜π∗ with the smoothed reward function ˜R is also optimal under the
original reward function R, where

˜R(st) =

L
(cid:88)

i=−L

γclip(i,−t,T −t) · fi · R(sclip(t+i,0,T )) and

L
(cid:88)

i=−L

fi = 1.

(6)

12

Proof. First, we show that the discounted sum of original rewards (cid:80)T
smoothed rewards (cid:80)T

t=0 γt ˜R(st) are the same for any trajectories (s0, s1, . . . , sT ):
T
(cid:88)

L
(cid:88)

γclip(i,−t,T −t) · fi · R(sclip(t+i,0,T ))

γt ˜R(st) =

γt

from Equation (6)

t=0 γtR(st) and the one of

T
(cid:88)

t=0

i=−L

γtR(st) ·

L
(cid:88)

fi

i=−L

γtR(st).

t=0

T
(cid:88)

t=0

T
(cid:88)

t=0

=

=

(7)

(8)

from

L
(cid:88)

i=−L

fi = 1

(9)

Let an optimal policy under the smoothed rewards ˜R be ˜π∗. Assume that ˜π∗ is not optimal under the
original reward R. Then,

∃π∗, s0

such that E(s0,...,sT )∼π∗

(cid:104) T
(cid:88)

t=0

(cid:105)
γtR(st)

> E(s0,...,sT )∼˜π∗

(cid:104) T
(cid:88)

t=0

γt ˜R(st)

(cid:105)
.

(10)

However,

E(s0,...,sT )∼π∗

(cid:104) T
(cid:88)

t=0

(cid:105)

γtR(st)

= E(s0,...,sT )∼π∗

> E(s0,...,sT )∼˜π∗

(cid:104) T
(cid:88)

t=0
(cid:104) T
(cid:88)

t=0

(cid:105)

γt ˜R(st)

γt ˜R(st)

(cid:105)
,

by Equation (9)

(11)

by Equation (10)

(12)

which contradicts that ˜π∗ is optimal under ˜R. Therefore, the optimal policy ˜π∗ under ˜R guarantees
its optimality under R.

B IMPLEMENTATION DETAILS

Models are trained on NVIDIA A5000, V100, RTX Titan, RTX 2080, and RTX 6000 GPUs. Each
experiment takes about 72 hours for RoboDesk, 100 hours for Hand, 150 hours for Earthmoving, 96
hours for Crafter, and 6 hours for Atari and DMC tasks.

B.1 DREAMSMOOTH SMOOTHING FUNCTIONS

Gaussian smoothing follows the Gaussian distribution with σ:

fi = ke

−i2
2σ2 ,

(13)

where k = 1/((cid:80)L

i=−L e
We implement this using

−i2
2σ2 ) is a normalization constant.

scipy.ndimage.gaussian_filter1d(rewards, sigma, mode=""nearest"")

Uniform smoothing distributes rewards equally across δ consecutive timesteps.
1
δ

δ − 1
2

δ − 1
2

fi =

∀i ∈

−

(cid:105)

(cid:104)

.

,

We implement this using

scipy.ndimage.convolve(rewards, filter, mode=""nearest"")

EMA smoothing uses the following smoothing function:

fi = α(1 − α)i ∀i ≤ 0,

which we implement by performing the following at each timestep:

reward[t] = alpha * reward[t - 1] + (1 - alpha) * reward[t]

13

(14)

(15)

B.2 MODEL-BASED REINFORCEMENT LEARNING BACKBONES

Hyperparameters for DreamerV3 experiments are shown in Table 1 and TD-MPC in Table 2.

Table 1: DreamerV3 hyperparameters. Episode length is measured in environment steps, which is the
number of agent steps multiplied by action repeat. Model sizes are as listed in Hafner et al. (2023),
which we also refer to for all other hyperparameters.

Environment Action Repeat Episode Length Train Ratio Model Size

Earthmoving
RoboDesk
Hand
Crafter
DMC
Atari

4
8
1
1
2
4

2000
2400
300
Variable
1000
Variable

64
64
64
64
512
1024

L
L
L
XL
S
S

σ

3
3
2
1
3
3

α

0.33
0.3
0.3
0.45
0.33
0.3

δ

9
9
9
9
9
9

Table 2: TD-MPC hyperparameters. Unless specified, we use the default hyperparameters in Hansen
et al. (2022).

Environment Latent Dimension CNN channels Planning Iterations

Hand-Pixel
Hand-Proprio

128
128

64
–

6
12

σ

3
3

C ENVIRONMENT DETAILS

C.1 ROBODESK ENVIRONMENT

We use a modified version of RoboDesk (Kannan et al., 2021), where a sequence of manipulation tasks
(flat block in bin, upright block off table, push green) need to be completed
in order. Figure 13 shows images of an agent successfully completing each of these tasks.

In the original environment, dense rewards are based on Euclidean distances of objects to their targets,
with additional terms to encourage the arm to reach the object. They typically range from 0 to 10
per timestep. We use these dense rewards together with a large sparse reward of 300 for each task
completed.

(a) Put the flat block into the bin

(b) Push the upright block off the
table

(c) Press the green button

Figure 13: Subtasks for RoboDesk.

C.2 HAND ENVIRONMENT

We modified the Shadow Hand environment (Plappert et al., 2018), so that the agent is required to
achieve a sequence of pre-defined goal orientations in order. The first 3 goals are shown in Figure 14,

14

while the subsequent goals are a repeat of the first 3. The goal orientations are chosen so that the
agent only has to rotate the cube along the z-axis, and we only require the agent to match the cube’s
rotation to the goal, not its position.

In the original environment, dense rewards are computed using r = −(10x + ∆θ), where x is the
Euclidean distance to some fixed position, and ∆θ is the angular difference to the target orientation.
In addition to these dense rewards, we provide a large sparse reward of 300 for each goal successfully
achieved by the agent.

(a) Goal 1

(b) Goal 2

(c) Goal 3

Figure 14: Subtasks for Hand.

C.3 AGX EARTHMOVING ENVIRONMENT

The Earthmoving environment consists of a wheel loader, dump
truck, a pile of dirt, with some rocks on top of the pile. The en-
vironment is simulated using the realistic AGX Dynamics physics
engine (Algoryx, 2020). The agent controls the wheel loader to pick
up rocks and dump them in the dump truck.

The starting positions of the dirt pile, wheel loader, and dump truck
are all randomized, as are the initial orientations of the dirt pile and
wheel loader.

The agent’s observations consist of 3 components: a wide-angle
egocentric RGB camera mounted on the cabin to allow navigation,
an RGB camera mounted on the bucket for observing interactions
with rocks, and proprioceptive observations (positions, velocity,
speed, force of actuators etc.). We use 64 × 64 × 3 images for all
cameras, while the proprioceptive observation has 21 dimensions.

Figure 15: The agent uses one
camera mounted on the cabin
(left) for navigation, and one
mounted on the bucket (right)
for observing interactions with
rocks and terrain.

The action space is 4-dimensional: 2 dimensions for driving and steering the loader, and 2 dimensions
for moving and tilting the bucket.

The reward consists of a large sparse reward for rocks picked up and dumped, and dense rewards
for moving rocks towards the dumptruck. The total reward rt at timestep t is computed using
Equation (16).
rt = λdump(mt
(cid:124)

load(max (2, dt) − max (2, dt−1))
(cid:125)

load − mt−1
load )
(cid:125)

dump) + λload(mt

dump − mt−1

+ λmovemt

(cid:124)

(cid:123)(cid:122)
dense reward

(cid:123)(cid:122)
sparse reward

Where mdump, mload are rock masses in the dumptruck and the bucket respectively, d is the distance
between the shovel and a point above the dumptruck, and λ are constants.

(16)

15

D DMC AND ATARI BENCHMARKING RESULTS

Figure 16: Full learning curves for the DMC and Atari benchmarks.

16

DreamSmooth-Gaussian

DreamSmooth-Uniform

DreamSmooth-EMA

DreamerV3

Hopper Hop

1000

e
r
o
c
S

500

0
0.0
1.0
0.5
Environment steps (×10⁶)

Reacher Hard

1000

e
r
o
c
S

500

0
0.0
1.0
0.5
Environment steps (×10⁶)

Cartpole Swingup Sparse

1000

e
r
o
c
S

500

0
0.0
1.0
0.5
Environment steps (×10⁶)

Walker Run

1000

e
r
o
c
S

500

0
0.0
1.0
0.5
Environment steps (×10⁶)

Finger Turn Hard

1000

e
r
o
c
S

500

0
0.0
1.0
0.5
Environment steps (×10⁶)

Quadruped Run

1000

e
r
o
c
S

500

0
0.0
1.0
0.5
Environment steps (×10⁶)

Cheetah Run

1000

e
r
o
c
S

500

0
0.0
1.0
0.5
Environment steps (×10⁶)

Pong

3.00

−5.33

−13.67

e
r
o
c
S

−22.00

0.0
0.4
0.2
Environment steps (×10⁶)

Breakout

16.00

10.67

5.33

e
r
o
c
S

0.00

0.0
0.4
0.2
Environment steps (×10⁶)

Freeway

29

19

9

e
r
o
c
S

−1

0.0

0.2
Environment steps (×10⁶)

0.4

Assault

807.0

621.3

435.7

e
r
o
c
S

250.0

0.0
0.4
0.2
Environment steps (×10⁶)

Seaquest

545

378

211

e
r
o
c
S

44

0.0

0.2
Environment steps (×10⁶)

0.4

Hero

13200

e
r
o
c
S

8860

4520

180

0.0
0.4
0.2
Environment steps (×10⁶)

","3 2 0 2 v o N 2 ] G L . s c [ 1 v 0 5 4 1 0 . 1 1 3 2 : v i X r a DREAMSMOOTH : IMPROVING MODEL-BASED REIN- FORCEMENT LEARNING VIA REWARD SMOOTHING Vint Lee , Pieter Abbeel , Youngwoon Lee University of California , Berkeley ABSTRACT Model-based reinforcement learning ( MBRL ) has gained much attention for its ability to learn complex behaviors in a sample-efficient way : planning actions by generating imaginary trajectories with predicted rewards . Despite its success , we found that surprisingly , reward prediction is often a bottleneck of MBRL , especially for sparse rewards that are challenging ( or even ambiguous ) to predict . Motivated by the intuition that humans can learn from rough reward estimates , we propose a simple yet effective reward smoothing approach , DreamSmooth , which learns to predict a temporally-smoothed reward , instead of the exact reward at the given timestep . We empirically show that DreamSmooth achieves state-of-the-art performance on long-horizon sparse-reward tasks both in sample efficiency and final performance without losing performance on common benchmarks , such as Deepmind Control Suite and Atari benchmarks . 1 INTRODUCTION Humans often plan actions with a rough estimate of future rewards , instead of the exact reward at the exact moment ( Fiorillo et al. , 2008 ; Klein- Fl¨ugge et al. , 2011 ) . A rough reward estimate is mostly sufficient to learn a task , and predicting the exact reward is often challenging since it can be ambiguous , delayed , or not observable . Consider for instance the manipulation task il- lustrated in Figure 1 ( middle ) of pushing a block on a table into a bin , where a sparse reward is given only on the timestep when the block first touches the bin . Using the same image observa- tions as the agent , it is challenging even for hu- mans to predict the correct sequence of rewards . Crucially , this issue is present in many environ- ments , where states with no reward are almost indistinguishable from those with rewards . Figure 1 : Predicting the exact sequence of rewards is extremely difficult . These examples show the sequences of image observations seen by the agent just before and after it receives a sparse reward . There is little to visually distinguish timesteps with a large reward from those without , which creates a significant challenge for reward prediction . An accurate reward model is vital to model- based reinforcement learning ( MBRL ) – reward estimates that are too high will cause an agent to choose actions that perform poorly in reality , and estimates that are too low will lead an agent to ignore high rewards . Despite its difficulty and importance , the reward prediction problem in MBRL has been largely overlooked . We find that even for the state-of-the-art MBRL algorithm , DreamerV3 ( Hafner et al. , 2023 ) , reward prediction is not only challenging , but is also a performance bottleneck for many tasks . For instance , DreamerV3 fails to predict any reward for most objectives in the Crafter environment ( Hafner , 2022 ) with similar failure modes observed on variants of the RoboDesk ( Kannan et al. , 2021 ) and Shadow Hand ( Plappert et al. , 2018 ) tasks with sparse rewards . 1 Inspired by the human intuition that only a rough estimate of rewards is sufficient , we propose a simple yet effective solution , DreamSmooth , which learns to predict a temporally-smoothed reward rather than the exact reward at each timestep . This makes reward prediction much easier – instead of having to predict rewards exactly , now the model only needs to produce an estimate of when sparse rewards are obtained , which is sufficient for policy learning . Our experiments demonstrate that while extremely simple , this technique significantly improves performance of different MBRL algorithms on many sparse-reward environments . Specifically , we find that for DreamerV3 ( Hafner et al. , 2023 ) and TD-MPC ( Hansen et al. , 2022 ) , our technique is especially beneficial in environments with the following characteristics : sparse rewards , partial observability , and stochastic rewards . Finally , we show that even on benchmarks where reward prediction is not a significant issue , DreamSmooth does not degrade performance , which indicates that our technique can be universally applied . 2 RELATED WORK Model-based reinforcement learning ( MBRL ) leverages a dynamics model ( i.e . world model ) of an environment and a reward model of a desired task to plan a sequence of actions that maximize the total reward . The dynamics model predicts the future state of the environment after taking a specific action and the reward model predicts the reward corresponding to the state-action transition . With the dynamics and reward models , an agent can simulate a large number of candidate behaviors in imagination instead of in the physical environment , allowing MBRL to tackle many challenging tasks ( Silver et al. , 2016 ; 2017 ; 2018 ) . Instead of relying on the given dynamics and reward models , recent advances in MBRL have enabled learning a world model of high-dimensional observations and complex dynamics ( Ha & Schmidhuber , 2018 ; Schrittwieser et al. , 2020 ; Hafner et al. , 2019 ; 2021 ; 2023 ; Hansen et al. , 2022 ) , as well as a temporally-extended world model ( Shi et al. , 2022 ) . Specifically , DreamerV3 ( Hafner et al. , 2023 ) has achieved the state-of-the-art performance across diverse domains of problems , e.g. , both with pixel and state observations as well as both with discrete and continuous actions . For realistic imagination , MBRL requires an accurate world model . There have been significant efforts in learning better world models by leveraging human videos ( Mendonca et al. , 2023 ) , by adopting a more performant architecture ( Deng et al. , 2023 ) , and via representation learning , such as prototype-based ( Deng et al. , 2022 ) and object-centric ( Singh et al. , 2021 ) state representations , contrastive learning ( Okada & Taniguchi , 2021 ) , and masked auto-encoding ( Seo et al. , 2022 ; 2023 ) . However , compared to the efforts on learning a better world model , learning an accurate reward model has been largely overlooked . Babaeizadeh et al . ( 2020 ) investigates the effects of various world model designs and shows that reward prediction is strongly correlated to task performance when trained on an offline dataset , while limited to dense-reward environments . In this paper , we point out that accurate reward prediction is crucial for MBRL , especially in sparse-reward tasks and partially observable environments , and propose a simple method to improve reward prediction in MBRL . 3 APPROACH The main goal of this paper is to understand how challenging reward prediction is in model-based reinforcement learning ( MBRL ) and propose a simple yet effective solution , reward smoothing , which makes reward prediction easier to learn . In this section , we first provide a background about MBRL in Section 3.1 , then present experiments demonstrating the challenge of predicting sparse reward signals in Section 3.2 , and finally explain our approach , DreamSmooth , in Section 3.4 . 3.1 BACKGROUND We formulate a problem as a partially observable Markov decision process ( POMDP ) , which is defined as tuple ( O , A , P , R , γ ) . O is an observation space , A is an action space , P ( ot+1|o≤t , a≤t ) with timestep t is a transition dynamics , R is a reward function that maps previous observations and actions to a reward rt = R ( o≤t , a≤t ) , and γ ∈ [ 0 , 1 ) is a discount factor ( Sutton & Barto , 2018 ) . RL aims to find a policy π ( at | o≤t , a < t ) that maximizes the expected sum of rewards Eπ [ ( cid:80 ) T t=1 γt−1rt ] . 2 This paper focuses on MBRL algorithms that learn a world model Pθ ( zt+1|zt , at ) and reward model Rθ ( rt|zt ) from agent experience , where zt is a learned latent state at timestep t. The learned world model and reward model can then generate imaginary rollouts { zτ , aτ , rτ } t+H−1 of the horizon H starting from any zt , which can be used for planning ( Argenson & Dulac-Arnold , 2021 ; Hansen et al. , 2022 ) or policy optimization ( Ha & Schmidhuber , 2018 ; Hafner et al. , 2019 ) . Specifically , we use the state-of-the-art algorithms , DreamerV3 ( Hafner et al. , 2023 ) and TD-MPC ( Hansen et al. , 2022 ) . τ =t DreamerV3 ( Hafner et al. , 2023 ) uses the predicted rewards for computing new value targets to train the critic . For learning a good policy , the reward model plays a vital role since the critic , from which the actor learns a policy , receives its training signal exclusively through the reward model . Note that the data collected from the environment is only used for training a world model and reward model . On the other hand , TD-MPC ( Hansen et al. , 2022 ) learns a state-action value function Q ( zt , at ) directly from agent experience , not from predicted rewards . However , the reward model is still important for obtaining a good policy in TD-MPC because the algorithm uses both the reward model and value function to obtain the policy through online planning . 3.2 REWARD PREDICTION IS DIFFICULT Reward prediction is surprisingly challenging in many environments . Figure 1 shows sequences of frames right before and after sparse rewards are received in diverse environments . Even for humans , it is difficult to determine the exact timestep when the reward is received in all three environments . We hypothesize that the mean squared error loss E ( z , r ) ∼D [ ( Rθ ( z ) − r ) 2 ] , typically used for reward model training , deteriorates reward prediction accuracy when there exist sparse rewards . This is because predicting a sparse reward a single step earlier or later results in a higher loss than simply predicting 0 reward at every step . Thus , instead of trying to predict sparse rewards at the exact timesteps , a reward model minimizes the loss by entirely omitting sparse rewards from its predictions . To verify this hypothesis , we plot the ground-truth and DreamerV3 ’ s predicted rewards in Figure 2 . The reward models struggle at predicting exact rewards and simply ignore sparse rewards unless they are straightforward to predict on the four tasks described in Section 4.1 . This hypothesis also holds in a deterministic and fully-observable environment , Crafter , which has 24 sources of sparse rewards . The reward model fails to predict most of these reward sources ( Figure 2d ) . The difficulty of reward prediction can be further exacerbated by partial observability , ambiguous rewards , or stochastic dynamics of environments . As an example in the first ( third ) row in Figure 1 , ( a ) RoboDesk ( b ) Hand ( c ) Earthmoving ( d ) Crafter Figure 2 : Ground truth rewards and DreamerV3 ’ s predicted rewards over an evaluation episode . The reward model misses many sparse rewards , which is highlighted in yellow . 3 Ground Truth Predicted x-axis : Timestep y-axis : Reward 330 0 −110 0 120 330 0 −110 0 120 2400 0 −800 0 360 1.2 0.0 −0.4 0 200 the sparse rewards are given when the block ( the rocks in the third example ) first contacts the bin ( the dumptruck ) . The exact moment of contact is not directly observable from the camera viewpoint , and this makes reward prediction ambiguous . Moreover , stochastic environment dynamics , e.g. , contact between multiple rocks , can make predicting a future state and reward challenging . 3.3 REWARD PREDICTION IS A BOTTLENECK OF MBRL The preceding section shows that reward prediction is challenging in many environments . More importantly , this poor reward prediction can be a bottleneck of policy learning , as shown in Figure 3 . In RoboDesk , where the reward model does not reliably detect the completion of the second task ( Figure 2a ) , the policy gets stuck at solving the first task and fails on subsequent tasks . In Earthmoving , where the reward model can not capture rewards for successful dumping ( Figure 2c ) , the policy frequently drops the rocks outside the dumptruck . These consistent failure modes in reward prediction and policy learning in DreamerV3 suggest that poor reward prediction can be a bottleneck of MBRL . ( a ) RoboDesk ( b ) Earthmoving Figure 3 : The reward model ’ s inability to predict sparse rewards for completing tasks leads to poor task performance . ( a ) In RoboDesk , the agent gets stuck after learning the first task , and is unable to learn to perform the subsequent tasks . ( b ) In Earthmoving , the policy often fails to dump the rocks accurately into the dumptruck . The learning curves are averaged over 3 seeds . 3.4 DREAMSMOOTH : IMPROVING MBRL VIA REWARD SMOOTHING To address the reward prediction problem , we propose a simple yet effective solution , DreamSmooth , which relaxes the require- ment for the model to predict sparse rewards at the exact timesteps by performing tempo- ral smoothing . Allowing the reward model to predict rewards that are off from the ground truth by a few timesteps makes learning eas- ier , especially when rewards are ambiguous or sparse . ( a ) Gaussian ( b ) Uniform ( c ) EMA Figure 4 : Reward smoothing on sparse reward 1 at t = 4. σ , δ , and α are smoothing hyperparameters . Specifically , DreamSmooth applies temporal smoothing to the rewards upon collecting each new episode . DreamSmooth can work with any smoothing function f that preserves the sum of rewards : ˜rt ← f ( rt−L : t+L ) = L ( cid:88 ) i=−L fi · rclip ( t+i,0 , T ) s.t . L ( cid:88 ) i=−L fi = 1 , ( 1 ) where T and L denote the episode and smoothing horizons , respectively . For simplicity , we omit the discount factor in Equation ( 1 ) ; the full equation can be found in Appendix , Equation ( 6 ) . Episodes with the smoothed rewards are stored in the replay buffer and used to train the reward model . The agent learns only from the smoothed rewards , without ever seeing the original rewards . The smoothed rewards ease reward prediction by allowing the model to predict rewards several timesteps earlier or later , without incurring large losses . In this paper , we investigate three popular smoothing functions : Gaussian , uniform , and exponential moving average ( EMA ) smoothing , as illustrated in Figure 4 . While the main motivation for smoothing is to make it easier to learn reward models , we note that reward smoothing in some cases preserves optimality – an optimal policy under smoothed rewards ˜r 4 3 2 1 l d e t e p m o C s k s a T 0 0 8 16 24 Environment steps ( ×10⁶ ) 2 1 d e p m u D s k c o R 0 0 4 8 12 Environment steps ( ×10⁶ ) 1 d r a w e R 0 0 Orginal σ = 1 σ = 2 2 4 Timestep 6 8 1 d r a w e R 0 0 Orginal δ = 3 δ = 7 2 4 Timestep 6 8 1 d r a w e R 0 0 Orginal α = .3 α = .6 2 4 Timestep 6 8 is also optimal under the original rewards r. In particular , we provide a proof in Appendix A for the optimality of EMA smoothing ( and any smoothing function where ∀i > 0 , fi = 0 ) by augmenting the POMDP states with the history of past states . However , when future rewards are used for smoothing ( e.g . Gaussian smoothing ) , the smoothed rewards are conditioned on policy , and we can no longer define an equivalent POMDP . In such cases , there is no theoretical guarantee . Even so , we empirically show that reward models can adapt their predictions alongside the changing policy , and achieve performance improvements . The implementation of DreamSmooth is extremely simple , requiring only one additional line of code to existing MBRL algorithms , as shown in Algorithm 1 . The overhead of reward smoothing is minimal , with time complexity O ( T · L ) . More implementation details can be found in Appendix B. Algorithm 1 COLLECT ROLLOUT ( π : policy , D : replay buffer ) in DREAMSMOOTH t=1 } ← ROLLOUT ( π ) { ( ot , at , rt ) T { rt } T D ← D ∪ { ( ot , at , rt ) T t=1 ← GAUSSIAN ( { rt } T t=1 } t=1 , σ ) or EMA ( { rt } T t=1 , α ) ▷ only one line needs to be added . 4 EXPERIMENTS In this paper , we propose a simple reward smoothing method , DreamSmooth , which facilitates reward prediction in model-based reinforcement learning ( MBRL ) and thus , improves the performance of existing MBRL methods . Through our experiments , we aim to answer the following questions : ( 1 ) Does reward smoothing improve reward prediction ? ( 2 ) Does better reward prediction with reward smoothing lead to better sample efficiency and asymptotic performance of MBRL in sparse-reward tasks ? ( 3 ) Does MBRL with reward smoothing also work in common dense-reward tasks ? 4.1 TASKS We evaluate DreamSmooth on four tasks with sparse subtask completion rewards and two common RL benchmarks . Earthmoving uses two 64 × 64 images as an observation while all other tasks use a single image . See Appendix C for environment details . • RoboDesk : We use a modified version of RoboDesk ( Kannan et al. , 2021 ) , where a sequence of ma- nipulation tasks ( flat block in bin , upright block off table , push green ) need to be completed in order ( Figure 5a ) . We use the original dense rewards together with a large sparse reward for each task completed . • Hand : The Hand task ( Plappert et al. , 2018 ) requires a Shadow Hand to rotate a block in hand into a specific orientation . We extend it to achieve a sequence of pre-defined goal orientations in order . In addition to the original dense rewards , we provide a large sparse reward for each goal . • Earthmoving : The Earthmoving task consists of a wheel loader , dump truck , and a pile of rocks ( Figure 5c ) . The agent controls the wheel loader to pick up rocks from the pile and dump them in the dump truck . A large sparse reward is given for each rock picked up and for each rock dumped , proportional to its mass . In addition , dense rewards are given for moving rocks towards the dump truck . The environment is simulated using the AGX Dynamics physics engine ( Algoryx , 2020 ) . ( a ) RoboDesk ( b ) Hand ( c ) Earthmoving ( d ) Crafter ( e ) DMC ( f ) Atari Figure 5 : We evaluate DreamSmooth on four tasks with sparse subtask completion rewards ( a-d ) . We also test on two popular benchmarks , ( e ) DeepMind Control Suite and ( f ) Atari . 5 ( a ) RoboDesk ( b ) Hand ( c ) Earthmoving ( d ) Crafter Figure 6 : We visualize the ground truth rewards , smoothed rewards with Gaussian smoothing , and predicted rewards by DreamerV3 trained on the smoothed rewards over an evaluation episode . In contrast to Figure 2 , the reward models with reward smoothing capture most of sparse rewards . • Crafter : Crafter ( Hafner , 2022 ) is a minecraft-like 2D environment , where the agent tries to collect , place , and craft items in order to survive . There are 22 achievements in the environment ( e.g . collecting water , mining diamonds ) with a sparse reward 1 for obtaining each achievement for the first time . A small reward is given ( or lost ) for each health point gained ( or lost ) . • DMC : We benchmark 7 DeepMind Control Suite continuous control tasks ( Tassa et al. , 2018 ) . • Atari : We benchmark 6 Atari tasks ( Bellemare et al. , 2013 ) at 100K steps . 4.2 IMPROVED REWARD PREDICTION WITH REWARD SMOOTHING We first visualize the ground truth rewards , smoothed rewards ( Gaussian smoothing ) , and re- ward prediction results of DreamerV3 trained with DreamSmooth in Figure 6 . We observe that re- ward smoothing leads to a significant improve- ment in reward prediction : DreamSmooth success- fully predicts most of the ( smoothed ) sparse re- wards and no longer omits vital signals for policy learning or planning . The improvement is especially notable in Crafter . In Figure 7 , we measure the accuracy of the re- ward model , ( i.e . predicting a reward larger than half of the original or smoothed reward for Dream- erV3 and DreamSmooth respectively ) at the exact timesteps for each subtask . The vanilla Dream- erV3 ’ s reward model ( baseline ) misses most of the sparse rewards while DreamSmooth predicts sparse rewards more accurately in 15/19 subtasks . 4.3 RESULTS Figure 7 : Reward prediction rates for 19 achieve- ments in Crafter . The other 3 tasks have been never achieved by both methods . With reward smoothing , the prediction rates are better in 15/19 tasks . We compare the vanilla DreamerV3 ( Hafner et al. , 2023 ) with DreamSmooth , whose backbone is also DreamerV3 . For DreamSmooth , we evaluate Gaussian , uniform , and EMA smoothing . The 6 Ground Truth Smoothed Predicted x-axis : Timestep y-axis : Reward 64 00 0 120 60 0 −20 0 120 800 0 −200 0 360 0.6 0.0 −0.2 0 200 Baseline ( no smoothing ) Gaussian 1.0 0.5 e t a R n o i t c i d e r P 0.0 w k al d e ain alth g e H nt e n ace d e n o o bie alth loss g xe xe ord n ord ollect drin plin at skeleto at co ollect co Place pla d picka e picka Place sto ollect sto Place ta m d sw e sw Place furn ollect w ollect sa at zo E ke sto n efe ke w ke sto ke w D a M efe C D n o o o o C C C C e H p ble ke u Wa a M a M a M ( a ) RoboDesk ( b ) Hand ( c ) Earthmoving ( d ) Crafter ( e ) DMC ( f ) Atari Figure 8 : Comparison of learning curves of DreamSmooth ( Gaussian , Uniform , EMA ) and Dream- erV3 . The shaded regions in ( a-d ) show the maximum and minimum over 3 seeds . For DMC ( e ) and Atari ( f ) , we aggregate results over 7 and 6 tasks respectively , and display the standard deviation . hyperparameters for DreamerV3 and smoothing functions can be found in Appendix B . As shown in Figure 8 , DreamSmooth-Gaussian and DreamSmooth-Uniform significantly improve the performance as well as the sample efficiency of DreamerV3 on the Robodesk , Hand , and Earthmoving tasks . The only change between DreamerV3 and ours is the improved reward prediction , as shown in Section 4.2 . This result suggests that reward prediction is one of major bottlenecks of the MBRL performance . While all smoothing methods lead to improvements over DreamerV3 , Gaussian smoothing generally performs the best , except on Crafter , with uniform smoothing showing comparable performance . The better performance of Gaussian and uniform smoothing could be because it allows predicting rewards both earlier and later , whereas EMA smoothing only allows predicting rewards later . Despite the improved reward prediction accuracy , DreamSmooth-Gaussian and DreamSmooth- Uniform perform worse than the baseline in Crafter . This can be because more predicted task rewards encourage more exploitation and less exploration . Further investigation on this trade-off is a promising direction for future work . Moreover , we observe that on the DMC and Atari benchmarks , where reward prediction is not particu- larly challenging , our technique shows comparable performance with the unmodified algorithms ( see Appendix , Figure 16 for full results ) . This suggests that reward smoothing can be applied generally , and does not hinder performance on most environments . 7 DreamSmooth-Gaussian DreamSmooth-Uniform DreamSmooth-EMA DreamerV3 3 2 1 l d e t e p m o C s k s a T 0 0 18 12 6 Environment steps ( ×10⁶ ) 24 3 2 1 l d e t e p m o C s k s a T 0 0 15 10 5 Environment steps ( ×10⁶ ) 20 d e p m u D s k c o R f o r e b m u N 2.0 1.5 1.0 0.5 0.0 0 3 6 Environment steps ( ×10⁶ ) 9 12 16 12 8 4 e r o c S 0 0.0 1.5 4.5 3.0 Environment steps ( ×10⁶ ) 6.0 1000 750 500 250 e r o c S 0 0.00 0.25 0.75 0.50 Environment steps ( ×10⁶ ) 1.00 e r o c S d e z i l a m r o N n a m u H 0.6 0.4 0.2 0.0 0.0 0.1 0.3 0.2 Environment steps ( ×10⁶ ) 0.4 In Figure 9 , DreamSmooth also improves the performance of TD-MPC ( Hansen et al. , 2022 ) . In the Hand task , vanilla TD-MPC is unable to consistently solve the first task , even with proprioceptive state observations . However , TD-MPC with DreamSmooth learns to complete the tasks with not only state observations but also pixel observa- tions . This suggests that DreamSmooth can be useful in a broad range of MBRL algo- rithms that use a reward model . We only demonstrate the Hand task since TD-MPC fails on other sparse-reward tasks . 4.4 ABLATION STUDIES ( a ) Hand ( Pixel ) ( b ) Hand ( State ) Figure 9 : Learning curves for TD-MPC and TD-MPC with DreamSmooth on the Hand task . The shaded re- gions show the minimum and maximum over 3 seeds . Data Imbalance . One possible cause of poor reward predictions is data imbalance – because sparse rewards are infrequent , sequences con- taining sparse rewards are rarely sampled from the replay buffer . The reward model therefore trains on fewer examples of sparse rewards , po- tentially leading to poor predictions . To test this hypothesis , we conducted experiments with oversampling : with probability p = 0.5 , we sample a sequence in which the agent receives a sparse reward ; otherwise , we sample uni- formly from all sequences in the buffer . As shown in Figure 10 , oversampling performs bet- ter than the baseline , but learns slower than DreamSmooth . This suggests that while data imbalance largely contributes to the difficulty of reward prediction , it is not the only factor hindering performance . Furthermore , this over- sampling method requires domain knowledge about which reward signals to be oversampled while DreamSmooth is agnostic to the scale and frequency of sparse rewards . Figure 10 : Using oversampling of sequences with sparse rewards ( p = 0.5 ) performs better than DreamerV3 on RoboDesk , but worse than DreamSmooth with Gaussian smoothing . The lines show median task performance over 3 seeds , while shaded regions show maximum and minimum . Reward Model Size . Another hypothe- sis for poor reward predictions is that the reward model does not have enough ca- pacity to capture sparse rewards . To test this hypothesis , we increase the size of the reward model from 4 layers of 768 units to 5 layeres of 1024 units and 6 layers of 1280 units , while keeping the rest of the world model the same . We observe in Fig- ure 11 that without smoothing , changing the reward model size has negligible impact on performance , and DreamSmooth out- performs all the reward model sizes tested . This indicates that the reward prediction problem is not simply caused by insuffi- cient model capacity . ( a ) RoboDesk ( b ) Hand Figure 11 : Simply increasing the reward model size has negligible impact on performance . DreamerV3-768 and DreamSmooth use 4 layers of 768 units ; DreamerV3- 1024 uses 5 layers of 1024 units ; and DreamerV3-1280 uses 6 layers of 1280 units . In Figure 12 , we analyze the impact of the smoothing parameters σ and Smoothing Parameter . α for Gaussian and EMA , respectively , on RoboDesk and Hand . We observe that DreamSmooth is insensitive to the smoothing parameters , performing well across a wide range of values . 8 DreamSmooth-Gaussian ( TDMPC ) TD-MPC l d e t e p m o C s k s a T 1.2 0.8 0.4 0.0 0.0 0.5 1.0 Environment steps ( ×10⁶ ) l d e t e p m o C s k s a T 3 2 1 0 0 4 2 Environment steps ( ×10⁶ ) 3 2 1 l d e t e p m o C s k s a T 0 0 DreamSmooth Oversampling DreamerV3 6 18 12 Environment steps ( ×10⁶ ) 24 DreamSmooth DreamerV3-1280 DreamerV3-1024 DreamerV3-768 l d e t e p m o C s k s a T 3 2 1 0 6 0 24 12 Environment steps ( ×10⁶ ) 18 l d e t e p m o C s k s a T 3 2 1 0 0 20 10 Environment steps ( ×10⁶ ) ( a ) Gaussian Smoothing on RoboDesk ( b ) Gaussian Smoothing on Hand ( c ) Uniform Smoothing on RoboDesk ( d ) Uniform Smoothing on Hand ( e ) EMA Smoothing on RoboDesk ( f ) EMA Smoothing on Hand Figure 12 : Parameter sweep over smoothing parameters σ , δ , and α . The lines show median task performance over 3 seeds , while shaded regions show maximum and minimum . 5 CONCLUSION In this paper , we identify the reward prediction problem in MBRL and provide a simple yet effective solution , reward smoothing . Our approach , DreamSmooth , demonstrates superior performance in sparse reward tasks where reward prediction is not trivial mainly due to the partial observability or stochasticity of the environments . Moreover , DreamSmooth shows comparable results on the commonly used benchmarks , DMC and Atari , showing its task-agnostic nature . Although we show that our simple reward smoothing approach mitigates the difficulty in reward prediction , the improved reward prediction does not always improve the task performance , e.g. , in Crafter . This can be because more predicted task rewards encourage more exploitation and less exploration . Further investigation on this trade-off is a promising direction for future work . 9 DreamSmooth-Gaussian ( cid:15 ) ( cid:3 ) ( cid:8 ) ( cid:3 ) ( cid:4 ) ( cid:15 ) ( cid:3 ) ( cid:8 ) ( cid:3 ) ( cid:6 ) ( cid:15 ) ( cid:3 ) ( cid:8 ) ( cid:3 ) ( cid:7 ) ( cid:9 ) ( cid:14 ) ( cid:12 ) ( cid:11 ) ( cid:13 ) ( cid:12 ) ( cid:14 ) ( cid:10 ) ( cid:5 ) 3 2 1 l d e t e p m o C s k s a T 0 0 6 18 12 Environment steps ( ×10⁶ ) 24 3 2 1 l d e t e p m o C s k s a T 0 0 5 15 10 Environment steps ( ×10⁶ ) 20 DreamSmooth-Uniform ( cid:15 ) ( cid:3 ) ( cid:8 ) ( cid:3 ) ( cid:6 ) ( cid:15 ) ( cid:3 ) ( cid:8 ) ( cid:3 ) ( cid:7 ) ( cid:15 ) ( cid:3 ) ( cid:8 ) ( cid:3 ) ( cid:4 ) ( cid:5 ) ( cid:9 ) ( cid:14 ) ( cid:12 ) ( cid:11 ) ( cid:13 ) ( cid:12 ) ( cid:14 ) ( cid:10 ) ( cid:5 ) 3 2 1 l d e t e p m o C s k s a T 0 0 6 18 12 Environment steps ( ×10⁶ ) 24 3 2 1 l d e t e p m o C s k s a T 0 0 5 15 10 Environment steps ( ×10⁶ ) 20 DreamSmooth-EMA ( cid:17 ) ( cid:3 ) ( cid:10 ) ( cid:3 ) ( cid:5 ) ( cid:4 ) ( cid:6 ) ( cid:17 ) ( cid:3 ) ( cid:10 ) ( cid:3 ) ( cid:5 ) ( cid:4 ) ( cid:7 ) ( cid:8 ) ( cid:17 ) ( cid:3 ) ( cid:10 ) ( cid:3 ) ( cid:5 ) ( cid:4 ) ( cid:9 ) ( cid:11 ) ( cid:16 ) ( cid:14 ) ( cid:13 ) ( cid:15 ) ( cid:14 ) ( cid:16 ) ( cid:12 ) ( cid:6 ) 3 2 1 l d e t e p m o C s k s a T 0 0 6 18 12 Environment steps ( ×10⁶ ) 24 3 2 1 l d e t e p m o C s k s a T 0 0 5 15 10 Environment steps ( ×10⁶ ) 20 ACKNOWLEDGMENTS This work was supported in part by the BAIR Industrial Consortium , an ONR DURIP grant , Komatsu , and InnoHK Centre for Logistics Robotics . We would like to thank all members of the Berkeley Robot Learning lab for their insightful feedback . REFERENCES Algoryx . AGX dynamics , 2020 . URL https : //www.algoryx.se/agx-dynamics/ . Arthur Argenson and Gabriel Dulac-Arnold . Model-based offline planning . In International Confer- ence on Learning Representations , 2021 . URL https : //openreview.net/forum ? id= OMNB1G5xzd4 . Mohammad Babaeizadeh , Mohammad Taghi Saffar , Danijar Hafner , Harini Kannan , Chelsea Finn , Sergey Levine , and Dumitru Erhan . Models , pixels , and rewards : Evaluating design trade-offs in visual model-based reinforcement learning . arXiv preprint arXiv:2012.04603 , 2020 . M. G. Bellemare , Y. Naddaf , J. Veness , and M. Bowling . The arcade learning environment : An evaluation platform for general agents . Journal of Artificial Intelligence Research , 47:253–279 , jun 2013 . Fei Deng , Ingook Jang , and Sungjin Ahn . Dreamerpro : Reconstruction-free model-based reinforce- ment learning with prototypical representations . In International Conference on Machine Learning , pp . 4956–4975 . PMLR , 2022 . Fei Deng , Junyeong Park , and Sungjin Ahn . Facing off world model backbones : Rnns , transformers , and s4 . arXiv preprint arXiv:2307.02064 , 2023 . Christopher D Fiorillo , William T Newsome , and Wolfram Schultz . The temporal precision of reward prediction in dopamine neurons . Nature neuroscience , 11 ( 8 ) :966–973 , 2008 . David Ha and J¨urgen Schmidhuber . World models . arXiv preprint arXiv:1803.10122 , 2018 . Danijar Hafner . Benchmarking the spectrum of agent capabilities . In International Conference on Learning Representations , 2022 . Danijar Hafner , Timothy Lillicrap , Jimmy Ba , and Mohammad Norouzi . Dream to control : Learning behaviors by latent imagination . In International Conference on Learning Representations , 2019 . Danijar Hafner , Timothy Lillicrap , Mohammad Norouzi , and Jimmy Ba . Mastering atari with discrete world models . In International Conference on Learning Representations , 2021 . Danijar Hafner , Jurgis Pasukonis , Jimmy Ba , and Timothy Lillicrap . Mastering diverse domains through world models . arXiv preprint arXiv:2301.04104 , 2023 . Nicklas Hansen , Xiaolong Wang , and Hao Su . Temporal difference learning for model predictive control . In International Conference on Machine Learning , 2022 . Harini Kannan , Danijar Hafner , Chelsea Finn , and Dumitru Erhan . Robodesk : A multi-task rein- forcement learning benchmark . https : , 2021 . Miriam C Klein-Fl¨ugge , Laurence T Hunt , Dominik R Bach , Raymond J Dolan , and Timothy EJ Behrens . Dissociable reward and timing signals in human midbrain and ventral striatum . Neuron , 72 ( 4 ) :654–664 , 2011 . Russell Mendonca , Shikhar Bahl , and Deepak Pathak . Structured world models from human videos . In Robotics : Science and Systems , 2023 . Andrew Y Ng , Daishi Harada , and Stuart Russell . Policy invariance under reward transformations : Theory and application to reward shaping . In International Conference on Machine Learning , volume 99 , pp . 278–287 , 1999 . 10 Masashi Okada and Tadahiro Taniguchi . Dreaming : Model-based reinforcement learning by latent imagination without reconstruction . In IEEE International Conference on Robotics and Automation , pp . 4209–4215 , 2021. doi : 10.1109/ICRA48506.2021.9560734 . Matthias Plappert , Marcin Andrychowicz , Alex Ray , Bob McGrew , Bowen Baker , Glenn Powell , Jonas Schneider , Josh Tobin , Maciek Chociej , Peter Welinder , et al . Multi-goal reinforcement learn- ing : Challenging robotics environments and request for research . arXiv preprint arXiv:1802.09464 , 2018 . Julian Schrittwieser , Ioannis Antonoglou , Thomas Hubert , Karen Simonyan , Laurent Sifre , Simon Schmitt , Arthur Guez , Edward Lockhart , Demis Hassabis , Thore Graepel , et al . Mastering atari , go , chess and shogi by planning with a learned model . Nature , 588 ( 7839 ) :604–609 , 2020 . Younggyo Seo , Danijar Hafner , Hao Liu , Fangchen Liu , Stephen James , Kimin Lee , and Pieter Abbeel . Masked world models for visual control . In Conference on Robot Learning , 2022 . Younggyo Seo , Junsu Kim , Stephen James , Kimin Lee , Jinwoo Shin , and Pieter Abbeel . Multi-view masked world models for visual robotic manipulation . In International Conference on Machine Learning , 2023 . Lucy Xiaoyang Shi , Joseph J. Lim , and Youngwoon Lee . Skill-based model-based reinforcement learning . In Conference on Robot Learning , 2022 . David Silver , Aja Huang , Chris J Maddison , Arthur Guez , Laurent Sifre , George Van Den Driessche , Julian Schrittwieser , Ioannis Antonoglou , Veda Panneershelvam , Marc Lanctot , et al . Mastering the game of go with deep neural networks and tree search . nature , 529 ( 7587 ) :484–489 , 2016 . David Silver , Julian Schrittwieser , Karen Simonyan , Ioannis Antonoglou , Aja Huang , Arthur Guez , Thomas Hubert , Lucas Baker , Matthew Lai , Adrian Bolton , et al . Mastering the game of go without human knowledge . nature , 550 ( 7676 ) :354–359 , 2017 . David Silver , Thomas Hubert , Julian Schrittwieser , Ioannis Antonoglou , Matthew Lai , Arthur Guez , Marc Lanctot , Laurent Sifre , Dharshan Kumaran , Thore Graepel , et al . A general reinforcement learning algorithm that masters chess , shogi , and go through self-play . Science , 362 ( 6419 ) : 1140–1144 , 2018 . Gautam Singh , Skand Peri , Junghyun Kim , Hyunseok Kim , and Sungjin Ahn . Structured world belief for reinforcement learning in pomdp . In International Conference on Machine Learning , pp . 9744–9755 . PMLR , 2021 . Richard S Sutton and Andrew G Barto . Reinforcement learning : An introduction . MIT Press , 2018 . Yuval Tassa , Yotam Doron , Alistair Muldal , Tom Erez , Yazhe Li , Diego de Las Casas , David Budden , Abbas Abdolmaleki , Josh Merel , Andrew Lefrancq , Timothy P. Lillicrap , and Martin A. Riedmiller . Deepmind control suite . arXiv preprint arXiv:1801.00690 , 2018 . 11 A PROOFS Let M = ( S , A , P , R , γ ) be the given MDP . Without loss of generality , we assume the augmented form of the MDP M , where a state st includes the entire history of states , i.e. , st = ( s1 , . . . , st ) , and thus , reward functions R , ˜R have access to previous states , i.e. , ˜R ( st ) = ˜R ( s1 , . . . , st ) . Theorem A.1 . An optimal policy ˜π∗ of the MDP with reward smoothing only with past rewards , e.g. , EMA smoothing , ˜M = ( S , A , P , ˜R , γ ) is also optimal under the original MDP M , where ˜R ( st ) = 0 ( cid:88 ) i=−L fi · γiR ( st+i ) and 0 ( cid:88 ) i=−L fi = 1 . ( 2 ) Proof . We will use the theorem of reward shaping that guarantees an optimal policy introduced in Ng et al . ( 1999 ) : if a modified reward function can be represented in the form of R ( st ) +γΦ ( st+1 ) −Φ ( st ) with any potential function Φ ( st ) , the new reward function yields the same optimal policy with the original reward function R. Let the potential function for the EMA reward smoothing Φ ( st ) = − −1 ( cid:88 ) i=−L γiR ( st+i ) + 0 ( cid:88 ) i=−L γiR ( st+i ) · 0 ( cid:88 ) fj . j=i+1 ( 3 ) Then , our reward shaping term in ˜R can be represented as the difference in the potential function γΦ ( st+1 ) − Φ ( st ) as follows : γΦ ( st+1 ) − Φ ( st ) = −R ( st ) + 0 ( cid:88 ) i=−L fi · γiR ( st+i ) . R ( st ) + γΦ ( st+1 ) − Φ ( st ) = 0 ( cid:88 ) i=−L fi · γiR ( st+i ) = ˜R . ( 4 ) ( 5 ) Hence , following Ng et al . ( 1999 ) , reward shaping with our EMA smoothing guarantees the optimal policy in the original MDP M. However , Theorem A.1 does not apply to smoothing functions that require access to future rewards , e.g. , Gaussian smoothing . As in Gaussian smoothing , a smoothed reward function may require future rewards , which are conditioned on the current policy ; so is the reward model . In such cases , there is no theoretical guarantee ; but in our experiments , we empirically show that reward models can adapt their predictions along the changes in policies and thus , improve MBRL . Instead , we intuitively explain that an optimal policy under any reward smoothing ( even though the reward function is post hoc and can not be defined for MDPs ) is also optimal under the original reward function . Theorem A.2 . An optimal policy ˜π∗ with the smoothed reward function ˜R is also optimal under the original reward function R , where ˜R ( st ) = L ( cid:88 ) i=−L γclip ( i , −t , T −t ) · fi · R ( sclip ( t+i,0 , T ) ) and L ( cid:88 ) i=−L fi = 1 . ( 6 ) 12 Proof . First , we show that the discounted sum of original rewards ( cid:80 ) T smoothed rewards ( cid:80 ) T t=0 γt ˜R ( st ) are the same for any trajectories ( s0 , s1 , . . . , sT ) : T ( cid:88 ) L ( cid:88 ) γclip ( i , −t , T −t ) · fi · R ( sclip ( t+i,0 , T ) ) γt ˜R ( st ) = γt from Equation ( 6 ) t=0 γtR ( st ) and the one of T ( cid:88 ) t=0 i=−L γtR ( st ) · L ( cid:88 ) fi i=−L γtR ( st ) . t=0 T ( cid:88 ) t=0 T ( cid:88 ) t=0 = = ( 7 ) ( 8 ) from L ( cid:88 ) i=−L fi = 1 ( 9 ) Let an optimal policy under the smoothed rewards ˜R be ˜π∗ . Assume that ˜π∗ is not optimal under the original reward R. Then , ∃π∗ , s0 such that E ( s0 , ... , sT ) ∼π∗ ( cid:104 ) T ( cid:88 ) t=0 ( cid:105 ) γtR ( st ) > E ( s0 , ... , sT ) ∼˜π∗ ( cid:104 ) T ( cid:88 ) t=0 γt ˜R ( st ) ( cid:105 ) . ( 10 ) However , E ( s0 , ... , sT ) ∼π∗ ( cid:104 ) T ( cid:88 ) t=0 ( cid:105 ) γtR ( st ) = E ( s0 , ... , sT ) ∼π∗ > E ( s0 , ... , sT ) ∼˜π∗ ( cid:104 ) T ( cid:88 ) t=0 ( cid:104 ) T ( cid:88 ) t=0 ( cid:105 ) γt ˜R ( st ) γt ˜R ( st ) ( cid:105 ) , by Equation ( 9 ) ( 11 ) by Equation ( 10 ) ( 12 ) which contradicts that ˜π∗ is optimal under ˜R . Therefore , the optimal policy ˜π∗ under ˜R guarantees its optimality under R. B IMPLEMENTATION DETAILS Models are trained on NVIDIA A5000 , V100 , RTX Titan , RTX 2080 , and RTX 6000 GPUs . Each experiment takes about 72 hours for RoboDesk , 100 hours for Hand , 150 hours for Earthmoving , 96 hours for Crafter , and 6 hours for Atari and DMC tasks . B.1 DREAMSMOOTH SMOOTHING FUNCTIONS Gaussian smoothing follows the Gaussian distribution with σ : fi = ke −i2 2σ2 , ( 13 ) where k = 1/ ( ( cid:80 ) L i=−L e We implement this using −i2 2σ2 ) is a normalization constant . scipy.ndimage.gaussian_filter1d ( rewards , sigma , mode= '' nearest '' ) Uniform smoothing distributes rewards equally across δ consecutive timesteps . 1 δ δ − 1 2 δ − 1 2 fi = ∀i ∈ − ( cid:105 ) ( cid:104 ) . , We implement this using scipy.ndimage.convolve ( rewards , filter , mode= '' nearest '' ) EMA smoothing uses the following smoothing function : fi = α ( 1 − α ) i ∀i ≤ 0 , which we implement by performing the following at each timestep : reward [ t ] = alpha * reward [ t - 1 ] + ( 1 - alpha ) * reward [ t ] 13 ( 14 ) ( 15 ) B.2 MODEL-BASED REINFORCEMENT LEARNING BACKBONES Hyperparameters for DreamerV3 experiments are shown in Table 1 and TD-MPC in Table 2 . Table 1 : DreamerV3 hyperparameters . Episode length is measured in environment steps , which is the number of agent steps multiplied by action repeat . Model sizes are as listed in Hafner et al . ( 2023 ) , which we also refer to for all other hyperparameters . Environment Action Repeat Episode Length Train Ratio Model Size Earthmoving RoboDesk Hand Crafter DMC Atari 4 8 1 1 2 4 2000 2400 300 Variable 1000 Variable 64 64 64 64 512 1024 L L L XL S S σ 3 3 2 1 3 3 α 0.33 0.3 0.3 0.45 0.33 0.3 δ 9 9 9 9 9 9 Table 2 : TD-MPC hyperparameters . Unless specified , we use the default hyperparameters in Hansen et al . ( 2022 ) . Environment Latent Dimension CNN channels Planning Iterations Hand-Pixel Hand-Proprio 128 128 64 – 6 12 σ 3 3 C ENVIRONMENT DETAILS C.1 ROBODESK ENVIRONMENT We use a modified version of RoboDesk ( Kannan et al. , 2021 ) , where a sequence of manipulation tasks ( flat block in bin , upright block off table , push green ) need to be completed in order . Figure 13 shows images of an agent successfully completing each of these tasks . In the original environment , dense rewards are based on Euclidean distances of objects to their targets , with additional terms to encourage the arm to reach the object . They typically range from 0 to 10 per timestep . We use these dense rewards together with a large sparse reward of 300 for each task completed . ( a ) Put the flat block into the bin ( b ) Push the upright block off the table ( c ) Press the green button Figure 13 : Subtasks for RoboDesk . C.2 HAND ENVIRONMENT We modified the Shadow Hand environment ( Plappert et al. , 2018 ) , so that the agent is required to achieve a sequence of pre-defined goal orientations in order . The first 3 goals are shown in Figure 14 , 14 while the subsequent goals are a repeat of the first 3 . The goal orientations are chosen so that the agent only has to rotate the cube along the z-axis , and we only require the agent to match the cube ’ s rotation to the goal , not its position . In the original environment , dense rewards are computed using r = − ( 10x + ∆θ ) , where x is the Euclidean distance to some fixed position , and ∆θ is the angular difference to the target orientation . In addition to these dense rewards , we provide a large sparse reward of 300 for each goal successfully achieved by the agent . ( a ) Goal 1 ( b ) Goal 2 ( c ) Goal 3 Figure 14 : Subtasks for Hand . C.3 AGX EARTHMOVING ENVIRONMENT The Earthmoving environment consists of a wheel loader , dump truck , a pile of dirt , with some rocks on top of the pile . The en- vironment is simulated using the realistic AGX Dynamics physics engine ( Algoryx , 2020 ) . The agent controls the wheel loader to pick up rocks and dump them in the dump truck . The starting positions of the dirt pile , wheel loader , and dump truck are all randomized , as are the initial orientations of the dirt pile and wheel loader . The agent ’ s observations consist of 3 components : a wide-angle egocentric RGB camera mounted on the cabin to allow navigation , an RGB camera mounted on the bucket for observing interactions with rocks , and proprioceptive observations ( positions , velocity , speed , force of actuators etc. ) . We use 64 × 64 × 3 images for all cameras , while the proprioceptive observation has 21 dimensions . Figure 15 : The agent uses one camera mounted on the cabin ( left ) for navigation , and one mounted on the bucket ( right ) for observing interactions with rocks and terrain . The action space is 4-dimensional : 2 dimensions for driving and steering the loader , and 2 dimensions for moving and tilting the bucket . The reward consists of a large sparse reward for rocks picked up and dumped , and dense rewards for moving rocks towards the dumptruck . The total reward rt at timestep t is computed using Equation ( 16 ) . rt = λdump ( mt ( cid:124 ) load ( max ( 2 , dt ) − max ( 2 , dt−1 ) ) ( cid:125 ) load − mt−1 load ) ( cid:125 ) dump ) + λload ( mt dump − mt−1 + λmovemt ( cid:124 ) ( cid:123 ) ( cid:122 ) dense reward ( cid:123 ) ( cid:122 ) sparse reward Where mdump , mload are rock masses in the dumptruck and the bucket respectively , d is the distance between the shovel and a point above the dumptruck , and λ are constants . ( 16 ) 15 D DMC AND ATARI BENCHMARKING RESULTS Figure 16 : Full learning curves for the DMC and Atari benchmarks . 16 DreamSmooth-Gaussian DreamSmooth-Uniform DreamSmooth-EMA DreamerV3 Hopper Hop 1000 e r o c S 500 0 0.0 1.0 0.5 Environment steps ( ×10⁶ ) Reacher Hard 1000 e r o c S 500 0 0.0 1.0 0.5 Environment steps ( ×10⁶ ) Cartpole Swingup Sparse 1000 e r o c S 500 0 0.0 1.0 0.5 Environment steps ( ×10⁶ ) Walker Run 1000 e r o c S 500 0 0.0 1.0 0.5 Environment steps ( ×10⁶ ) Finger Turn Hard 1000 e r o c S 500 0 0.0 1.0 0.5 Environment steps ( ×10⁶ ) Quadruped Run 1000 e r o c S 500 0 0.0 1.0 0.5 Environment steps ( ×10⁶ ) Cheetah Run 1000 e r o c S 500 0 0.0 1.0 0.5 Environment steps ( ×10⁶ ) Pong 3.00 −5.33 −13.67 e r o c S −22.00 0.0 0.4 0.2 Environment steps ( ×10⁶ ) Breakout 16.00 10.67 5.33 e r o c S 0.00 0.0 0.4 0.2 Environment steps ( ×10⁶ ) Freeway 29 19 9 e r o c S −1 0.0 0.2 Environment steps ( ×10⁶ ) 0.4 Assault 807.0 621.3 435.7 e r o c S 250.0 0.0 0.4 0.2 Environment steps ( ×10⁶ ) Seaquest 545 378 211 e r o c S 44 0.0 0.2 Environment steps ( ×10⁶ ) 0.4 Hero 13200 e r o c S 8860 4520 180 0.0 0.4 0.2 Environment steps ( ×10⁶ )","['n', 'l', 'c', 'v', 'r', 'dreamsmooth', 'improve', 'modelbase', 'rein', 'forcement', 'learning', 'reward', 'smoothing', 'vint', 'pieter', 'modelbase', 'reinforcement', 'learning', 'mbrl', 'gain', 'much', 'attention', 'ability', 'learn', 'complex', 'behavior', 'sampleefficient', 'way', 'planning', 'action', 'generate', 'imaginary', 'trajectory', 'predict', 'reward', 'success', 'find', 'surprisingly', 'reward', 'prediction', 'often', 'bottleneck', 'mbrl', 'especially', 'sparse', 'reward', 'challenge', 'even', 'ambiguous', 'predict', 'motivate', 'intuition', 'human', 'learn', 'rough', 'reward', 'estimate', 'propose', 'simple', 'yet', 'effective', 'reward', 'smooth', 'approach', 'dreamsmooth', 'learn', 'predict', 'temporallysmoothe', 'reward', 'instead', 'exact', 'reward', 'give', 'timestep', 'empirically', 'show', 'dreamsmooth', 'achieve', 'stateoftheart', 'performance', 'task', 'sample', 'efficiency', 'final', 'performance', 'lose', 'performance', 'common', 'benchmark', 'deepmind', 'control', 'suite', 'atari', 'benchmark', 'introduction', 'human', 'often', 'plan', 'action', 'rough', 'estimate', 'future', 'reward', 'instead', 'exact', 'reward', 'exact', 'moment', 'fiorillo', 'rough', 'reward', 'estimate', 'mostly', 'sufficient', 'learn', 'task', 'predict', 'exact', 'reward', 'often', 'challenge', 'ambiguous', 'delay', 'observable', 'consider', 'instance', 'manipulation', 'lustrate', 'figure', 'middle', 'push', 'block', 'table', 'sparse', 'reward', 'give', 'timestep', 'block', 'first', 'touch', 'bin', 'use', 'image', 'observa', 'tion', 'agent', 'challenge', 'even', 'man', 'predict', 'correct', 'sequence', 'reward', 'crucially', 'issue', 'present', 'many', 'environ', 'ment', 'state', 'reward', 'almost', 'indistinguishable', 'reward', 'figure', 'predict', 'exact', 'sequence', 'reward', 'extremely', 'difficult', 'example', 'show', 'sequence', 'image', 'observation', 'see', 'agent', 'receive', 'sparse', 'reward', 'little', 'visually', 'distinguish', 'timestep', 'large', 'reward', 'create', 'significant', 'challenge', 'reward', 'prediction', 'accurate', 'reward', 'model', 'vital', 'model', 'base', 'reinforcement', 'learning', 'mbrl', 'reward', 'estimate', 'high', 'cause', 'agent', 'choose', 'action', 'perform', 'poorly', 'reality', 'estimate', 'low', 'lead', 'agent', 'ignore', 'high', 'reward', 'difficulty', 'importance', 'reward', 'prediction', 'problem', 'mbrl', 'largely', 'overlook', 'find', 'even', 'stateoftheart', 'reward', 'prediction', 'challenge', 'also', 'performance', 'bottleneck', 'many', 'task', 'instance', 'dreamerv3', 'fail', 'predict', 'reward', 'objective', 'crafter', 'environment', 'similar', 'failure', 'mode', 'observe', 'variant', 'robodesk', 'shadow', 'hand', 'plappert', 'task', 'sparse', 'reward', 'inspire', 'human', 'intuition', 'rough', 'estimate', 'reward', 'sufficient', 'propose', 'simple', 'yet', 'effective', 'solution', 'dreamsmooth', 'learn', 'predict', 'temporallysmoothe', 'reward', 'rather', 'exact', 'reward', 'timestep', 'make', 'reward', 'prediction', 'much', 'easy', 'instead', 'predict', 'reward', 'exactly', 'model', 'need', 'produce', 'estimate', 'sparse', 'reward', 'obtain', 'sufficient', 'policy', 'learn', 'experiment', 'demonstrate', 'extremely', 'simple', 'technique', 'significantly', 'improve', 'performance', 'different', 'mbrl', 'algorithm', 'many', 'sparsereward', 'environment', 'specifically', 'find', 'dreamerv3', 'hansen', 'technique', 'especially', 'beneficial', 'environment', 'follow', 'characteristic', 'sparse', 'reward', 'partial', 'observability', 'stochastic', 'reward', 'finally', 'show', 'even', 'benchmark', 'reward', 'prediction', 'significant', 'issue', 'dreamsmooth', 'degrade', 'performance', 'indicate', 'technique', 'universally', 'apply', 'relate', 'work', 'modelbase', 'reinforcement', 'learning', 'mbrl', 'leverage', 'dynamic', 'model', 'ie', 'world', 'model', 'environment', 'reward', 'model', 'desire', 'task', 'plan', 'sequence', 'action', 'maximize', 'total', 'reward', 'dynamic', 'model', 'predict', 'future', 'state', 'environment', 'take', 'specific', 'action', 'reward', 'model', 'predict', 'reward', 'correspond', 'stateaction', 'transition', 'dynamic', 'reward', 'model', 'agent', 'simulate', 'large', 'number', 'candidate', 'behavior', 'imagination', 'instead', 'physical', 'environment', 'allow', 'mbrl', 'tackle', 'many', 'challenging', 'task', 'silver', 'instead', 'rely', 'give', 'dynamic', 'reward', 'model', 'recent', 'advance', 'mbrl', 'enable', 'learn', 'world', 'model', 'highdimensional', 'observation', 'complex', 'dynamic', 'schmidhuber', 'schrittwieser', 'hafner', 'hansen', 'well', 'temporallyextende', 'world', 'model', 'specifically', 'dreamerv3', 'achieve', 'stateoftheart', 'performance', 'diverse', 'domain', 'problem', 'eg', 'pixel', 'state', 'observation', 'well', 'discrete', 'continuous', 'action', 'realistic', 'imagination', 'mbrl', 'require', 'accurate', 'world', 'model', 'significant', 'effort', 'learn', 'well', 'world', 'model', 'leverage', 'human', 'video', 'adopt', 'performant', 'architecture', 'deng', 'representation', 'learning', 'prototypebase', 'deng', 'objectcentric', 'state', 'representation', 'contrastive', 'learn', 'taniguchi', 'mask', 'autoencoding', 'seo', 'however', 'compare', 'effort', 'learn', 'well', 'world', 'model', 'learn', 'accurate', 'reward', 'model', 'largely', 'overlook', 'investigate', 'effect', 'various', 'world', 'model', 'design', 'show', 'reward', 'prediction', 'strongly', 'correlate', 'task', 'performance', 'train', 'offline', 'dataset', 'limit', 'densereward', 'environment', 'paper', 'point', 'accurate', 'reward', 'prediction', 'crucial', 'mbrl', 'especially', 'sparsereward', 'task', 'partially', 'observable', 'environment', 'propose', 'simple', 'method', 'improve', 'reward', 'prediction', 'mbrl', 'approach', 'main', 'goal', 'paper', 'understand', 'challenging', 'reward', 'prediction', 'modelbase', 'reinforcement', 'learning', 'mbrl', 'propose', 'simple', 'yet', 'effective', 'solution', 'reward', 'smooth', 'make', 'reward', 'prediction', 'easy', 'learn', 'section', 'first', 'provide', 'background', 'mbrl', 'section', 'present', 'experiment', 'demonstrate', 'challenge', 'predict', 'sparse', 'reward', 'signal', 'section', 'finally', 'explain', 'approach', 'dreamsmooth', 'section', 'background', 'formulate', 'problem', 'partially', 'observable', 'markov', 'decision', 'process', 'pomdp', 'define', 'tuple', 'p', 'r', 'observation', 'space', 'action', 'space', 'p', 'timestep', 'transition', 'dynamic', 'r', 'reward', 'function', 'map', 'previous', 'observation', 'action', 'reward', 'r', 'o≤t', 'discount', 'factor', 'sutton', 'rl', 'aim', 'find', 'policy', 'π', 'o≤t', 'maximize', 'expect', 'sum', 'reward', 'paper', 'focus', 'mbrl', 'algorithm', 'learn', 'world', 'model', 'reward', 'model', 'rtzt', 'agent', 'experience', 'learned', 'latent', 'state', 'timestep', 'learned', 'world', 'model', 'reward', 'model', 'generate', 'imaginary', 'rollout', 'aτ', 'horizon', 'start', 'zt', 'use', 'planning', 'argenson', 'dulacarnold', 'hansen', 'policy', 'optimization', 'schmidhuber', 'hafner', 'specifically', 'use', 'stateoftheart', 'algorithm', 'dreamerv3', 'hansen', 'use', 'predict', 'reward', 'compute', 'new', 'value', 'target', 'train', 'critic', 'learn', 'good', 'policy', 'reward', 'model', 'play', 'vital', 'role', 'critic', 'actor', 'learn', 'policy', 'receive', 'training', 'signal', 'exclusively', 'reward', 'model', 'note', 'datum', 'collect', 'environment', 'use', 'train', 'world', 'model', 'reward', 'model', 'hand', 'learn', 'stateaction', 'value', 'function', 'zt', 'directly', 'agent', 'experience', 'predict', 'reward', 'however', 'reward', 'model', 'still', 'important', 'obtain', 'good', 'policy', 'tdmpc', 'use', 'reward', 'model', 'value', 'function', 'obtain', 'policy', 'online', 'planning', 'reward', 'prediction', 'difficult', 'reward', 'prediction', 'surprisingly', 'challenge', 'many', 'environment', 'figure', 'show', 'sequence', 'frame', 'right', 'sparse', 'reward', 'receive', 'diverse', 'environment', 'even', 'human', 'difficult', 'determine', 'exact', 'timestep', 'reward', 'receive', 'environment', 'hypothesize', 'mean', 'square', 'error', 'loss', 'e', 'z', 'r', 'rθ', 'z', 'r', 'typically', 'use', 'reward', 'model', 'training', 'deteriorate', 'reward', 'prediction', 'accuracy', 'exist', 'sparse', 'reward', 'predict', 'sparse', 'reward', 'single', 'step', 'early', 'later', 'result', 'high', 'loss', 'simply', 'predict', 'reward', 'step', 'thus', 'instead', 'try', 'predict', 'sparse', 'reward', 'exact', 'timestep', 'reward', 'model', 'minimize', 'loss', 'entirely', 'omitting', 'sparse', 'reward', 'prediction', 'verify', 'hypothesis', 'plot', 'groundtruth', 'dreamerv3', 'predict', 'reward', 'figure', 'reward', 'model', 'struggle', 'predict', 'exact', 'reward', 'simply', 'ignore', 'sparse', 'reward', 'straightforward', 'predict', 'task', 'describe', 'section', 'hypothesis', 'also', 'hold', 'deterministic', 'fullyobservable', 'environment', 'crafter', 'source', 'sparse', 'reward', 'reward', 'model', 'fail', 'predict', 'reward', 'source', 'figure', '2d', 'difficulty', 'reward', 'prediction', 'far', 'exacerbate', 'partial', 'observability', 'ambiguous', 'reward', 'stochastic', 'dynamic', 'environment', 'example', 'first', 'third', 'row', 'figure', 'robodesk', 'b', 'hand', 'c', 'earthmove', 'crafter', 'figure', 'ground', 'truth', 'reward', 'dreamerv3', 'predict', 'reward', 'evaluation', 'episode', 'reward', 'model', 'miss', 'many', 'sparse', 'reward', 'highlight', 'yellow', 'ground', 'truth', 'predict', 'timestep', 'yaxis', 'reward', '−110', '−110', '−800', '−04', 'sparse', 'reward', 'give', 'block', 'rock', 'third', 'example', 'first', 'contact', 'dumptruck', 'exact', 'moment', 'contact', 'directly', 'observable', 'camera', 'viewpoint', 'make', 'reward', 'prediction', 'ambiguous', 'moreover', 'stochastic', 'environment', 'dynamic', 'contact', 'multiple', 'rock', 'make', 'predict', 'future', 'state', 'reward', 'challenge', 'reward', 'prediction', 'bottleneck', 'mbrl', 'precede', 'section', 'show', 'reward', 'prediction', 'challenge', 'many', 'environment', 'importantly', 'poor', 'reward', 'prediction', 'bottleneck', 'policy', 'learning', 'show', 'figure', 'robodesk', 'reward', 'model', 'reliably', 'detect', 'completion', 'second', 'task', 'figure', '2a', 'policy', 'get', 'stick', 'solve', 'first', 'task', 'fail', 'subsequent', 'task', 'earthmove', 'reward', 'model', 'capture', 'reward', 'successful', 'dumping', 'figure', 'policy', 'frequently', 'drop', 'rock', 'dumptruck', 'consistent', 'failure', 'mode', 'reward', 'prediction', 'policy', 'learning', 'dreamerv3', 'suggest', 'poor', 'reward', 'prediction', 'bottleneck', 'mbrl', 'robodesk', 'b', 'earthmove', 'figure', 'reward', 'model', 'inability', 'predict', 'sparse', 'reward', 'complete', 'task', 'lead', 'poor', 'task', 'performance', 'robodesk', 'agent', 'get', 'stick', 'learn', 'first', 'task', 'unable', 'learn', 'perform', 'subsequent', 'task', 'b', 'earthmove', 'policy', 'often', 'fail', 'dump', 'rock', 'accurately', 'dumptruck', 'learn', 'curve', 'average', 'seed', 'dreamsmooth', 'improve', 'mbrl', 'reward', 'smooth', 'address', 'reward', 'prediction', 'problem', 'propose', 'simple', 'yet', 'effective', 'solution', 'dreamsmooth', 'relax', 'require', 'ment', 'model', 'predict', 'sparse', 'reward', 'exact', 'timestep', 'perform', 'ral', 'smoothing', 'allow', 'reward', 'model', 'predict', 'reward', 'ground', 'truth', 'timestep', 'make', 'learn', 'ier', 'especially', 'reward', 'ambiguous', 'sparse', 'gaussian', 'uniform', 'c', 'figure', 'reward', 'smooth', 'sparse', 'reward', 'σ', 'δ', 'smooth', 'hyperparameter', 'specifically', 'dreamsmooth', 'apply', 'temporal', 'smoothing', 'reward', 'collect', 'new', 'episode', 'dreamsmooth', 'work', 'smoothing', 'function', 'preserve', 'sum', 'reward', 'rt−l', 'fi', 'rclip', 'ti0', 'i−l', 'fi', 'l', 'denote', 'episode', 'smooth', 'horizon', 'respectively', 'simplicity', 'omit', 'discount', 'factor', 'equation', 'full', 'equation', 'find', 'equation', 'episode', 'smoothed', 'reward', 'store', 'replay', 'buffer', 'use', 'train', 'reward', 'model', 'agent', 'learn', 'smoothed', 'reward', 'ever', 'see', 'original', 'reward', 'smoothed', 'reward', 'ease', 'reward', 'prediction', 'allow', 'model', 'predict', 'reward', 'several', 'timestep', 'early', 'later', 'incur', 'large', 'loss', 'paper', 'investigate', 'popular', 'smoothing', 'function', 'gaussian', 'uniform', 'exponential', 'move', 'average', 'smoothing', 'illustrate', 'figure', 'main', 'motivation', 'smoothing', 'make', 'easy', 'learn', 'reward', 'model', 'note', 'reward', 'smooth', 'case', 'preserve', 'optimality', 'optimal', 'policy', 'smoothed', 'reward', 'l', 'e', 'c', 'environment', 'step', '×10⁶', 'e', 'k', 'r', 'environment', 'step', 'r', 'w', 'e', 'r', 'orginal', 'σ', 'timestep', 'r', 'w', 'e', 'r', 'orginal', 'δ', 'δ', 'timestep', 'r', 'w', 'e', 'r', 'orginal', 'α', 'timestep', 'also', 'optimal', 'original', 'reward', 'r', 'particular', 'provide', 'proof', 'optimality', 'smoothing', 'smoothing', 'function', 'fi', 'augment', 'pomdp', 'state', 'history', 'past', 'state', 'however', 'future', 'reward', 'use', 'smooth', 'gaussian', 'smooth', 'smoothed', 'reward', 'condition', 'policy', 'long', 'define', 'equivalent', 'pomdp', 'case', 'theoretical', 'guarantee', 'even', 'empirically', 'show', 'reward', 'model', 'adapt', 'prediction', 'change', 'policy', 'achieve', 'performance', 'improvement', 'implementation', 'dreamsmooth', 'extremely', 'simple', 'require', 'additional', 'line', 'code', 'exist', 'mbrl', 'algorithm', 'show', 'overhead', 'reward', 'smoothing', 'minimal', 'time', 'complexity', 'l', 'implementation', 'detail', 'find', 'collect', 'rollout', 'π', 'policy', 'replay', 'buffer', 'dreamsmooth', 'π', '∪', 'ot', 't1', 't1', '▷', 'line', 'need', 'add', 'experiment', 'paper', 'propose', 'simple', 'reward', 'smooth', 'method', 'dreamsmooth', 'facilitate', 'reward', 'prediction', 'modelbase', 'reinforcement', 'learning', 'mbrl', 'thus', 'improve', 'performance', 'exist', 'mbrl', 'method', 'experiment', 'aim', 'answer', 'follow', 'question', 'reward', 'smooth', 'improve', 'reward', 'prediction', 'well', 'reward', 'prediction', 'reward', 'smooth', 'lead', 'well', 'sample', 'efficiency', 'asymptotic', 'performance', 'mbrl', 'sparsereward', 'task', 'mbrl', 'reward', 'smoothing', 'also', 'work', 'common', 'densereward', 'task', 'task', 'evaluate', 'dreamsmooth', 'task', 'sparse', 'subtask', 'completion', 'reward', 'common', 'rl', 'benchmark', 'earthmove', 'use', 'image', 'observation', 'task', 'use', 'single', 'image', 'see', 'c', 'environment', 'detail', 'robodesk', 'use', 'modify', 'version', 'robodesk', 'sequence', 'nipulation', 'task', 'flat', 'block', 'upright', 'block', 'table', 'push', 'green', 'need', 'complete', 'order', 'figure', 'use', 'original', 'dense', 'reward', 'together', 'large', 'sparse', 'reward', 'task', 'complete', 'hand', 'hand', 'task', 'plappert', 'require', 'shadow', 'hand', 'rotate', 'block', 'hand', 'specific', 'orientation', 'extend', 'achieve', 'sequence', 'predefine', 'goal', 'orientation', 'order', 'addition', 'original', 'dense', 'reward', 'provide', 'large', 'sparse', 'reward', 'goal', '•', 'earthmove', 'earthmove', 'task', 'consist', 'wheel', 'loader', 'dump', 'truck', 'pile', 'rock', 'figure', 'agent', 'control', 'wheel', 'loader', 'pick', 'rock', 'pile', 'dump', 'dump', 'truck', 'large', 'sparse', 'reward', 'give', 'rock', 'pick', 'rock', 'dump', 'proportional', 'mass', 'addition', 'dense', 'reward', 'give', 'move', 'rock', 'dump', 'truck', 'environment', 'simulate', 'use', 'dynamic', 'physics', 'engine', 'robodesk', 'b', 'hand', 'c', 'earthmove', 'crafter', 'e', 'dmc', 'atari', 'figure', 'evaluate', 'dreamsmooth', 'task', 'sparse', 'subtask', 'completion', 'reward', 'ad', 'also', 'test', 'popular', 'benchmark', 'e', 'deepmind', 'control', 'suite', 'atari', 'robodesk', 'b', 'hand', 'c', 'earthmove', 'crafter', 'figure', 'visualize', 'ground', 'truth', 'reward', 'smoothed', 'reward', 'gaussian', 'smoothing', 'predict', 'reward', 'dreamerv3', 'train', 'smoothed', 'reward', 'evaluation', 'episode', 'contrast', 'figure', 'reward', 'model', 'reward', 'smoothing', 'capture', 'sparse', 'reward', 'crafter', 'crafter', 'hafner', 'minecraftlike', 'environment', 'agent', 'try', 'collect', 'place', 'craft', 'item', 'order', 'survive', 'achievement', 'environment', 'collect', 'water', 'mining', 'diamond', 'sparse', 'reward', 'obtain', 'achievement', 'first', 'time', 'small', 'reward', 'give', 'lose', 'health', 'point', 'gain', 'lose', 'dmc', 'benchmark', 'deepmind', 'control', 'continuous', 'control', 'task', 'tassa', 'atari', 'benchmark', 'atari', 'task', 'bellemare', 'step', 'improve', 'reward', 'prediction', 'reward', 'smooth', 'first', 'visualize', 'ground', 'truth', 'reward', 'smoothed', 'reward', 'gaussian', 'smoothing', 'ward', 'prediction', 'result', 'dreamerv3', 'train', 'dreamsmooth', 'figure', 'observe', 'ward', 'smoothing', 'lead', 'significant', 'improve', 'ment', 'reward', 'prediction', 'dreamsmooth', 'success', 'fully', 'predict', 'smoothed', 'sparse', 'ward', 'long', 'omit', 'vital', 'signal', 'policy', 'learning', 'planning', 'improvement', 'especially', 'notable', 'crafter', 'figure', 'measure', 'accuracy', 'ward', 'model', 'predict', 'reward', 'large', 'half', 'original', 'smoothed', 'reward', 'dream', 'erv3', 'dreamsmooth', 'respectively', 'exact', 'timestep', 'subtask', 'vanilla', 'dream', 'reward', 'model', 'baseline', 'miss', 'sparse', 'reward', 'predict', 'sparse', 'reward', 'accurately', 'subtask', 'result', 'figure', 'reward', 'prediction', 'rate', 'achieve', 'ment', 'crafter', 'task', 'never', 'achieve', 'method', 'reward', 'smooth', 'prediction', 'rate', 'well', 'task', 'compare', 'vanilla', 'dreamsmooth', 'backbone', 'also', 'dreamerv3', 'dreamsmooth', 'evaluate', 'gaussian', 'uniform', 'smooth', 'ground', 'truth', 'smooth', 'predict', 'timestep', 'yaxis', 'reward', '−200', 'baseline', 'smooth', 'gaussian', 'e', 'r', 'n', 'c', 'e', 'r', 'p', 'e', 'bie', 'alth', 'loss', 'ollect', 'drin', 'picka', 'ollect', 'sto', 'place', 'e', 'efe', 'efe', 'c', 'c', 'ble', 'robodesk', 'b', 'hand', 'c', 'earthmove', 'crafter', 'e', 'dmc', 'atari', 'figure', 'comparison', 'learn', 'curve', 'dreamsmooth', 'gaussian', 'uniform', 'dream', 'erv3', 'shaded', 'region', 'ad', 'show', 'maximum', 'minimum', 'seed', 'dmc', 'e', 'aggregate', 'result', 'task', 'respectively', 'display', 'standard', 'deviation', 'hyperparameter', 'dreamerv3', 'smoothing', 'function', 'find', 'show', 'figure', 'dreamsmoothgaussian', 'dreamsmoothuniform', 'significantly', 'improve', 'performance', 'well', 'sample', 'efficiency', 'dreamerv3', 'robodesk', 'hand', 'earthmoving', 'task', 'change', 'improved', 'reward', 'prediction', 'show', 'section', 'result', 'suggest', 'reward', 'prediction', 'major', 'bottleneck', 'mbrl', 'performance', 'smoothing', 'method', 'lead', 'improvement', 'dreamerv3', 'gaussian', 'smoothing', 'generally', 'perform', 'good', 'crafter', 'uniform', 'smooth', 'show', 'comparable', 'performance', 'well', 'performance', 'gaussian', 'uniform', 'smoothing', 'allow', 'predict', 'reward', 'early', 'later', 'smoothing', 'allow', 'predict', 'reward', 'later', 'improved', 'reward', 'prediction', 'accuracy', 'dreamsmoothgaussian', 'dreamsmooth', 'uniform', 'perform', 'bad', 'baseline', 'crafter', 'predict', 'task', 'reward', 'encourage', 'exploitation', 'less', 'exploration', 'investigation', 'tradeoff', 'promising', 'direction', 'future', 'work', 'moreover', 'observe', 'dmc', 'atari', 'benchmark', 'reward', 'prediction', 'particu', 'larly', 'challenge', 'technique', 'show', 'comparable', 'performance', 'unmodified', 'algorithm', 'see', 'figure', 'full', 'result', 'suggest', 'reward', 'smoothing', 'apply', 'generally', 'hinder', 'performance', 'environment', 'dreamsmoothgaussian', 'dreamerv3', 'l', 'e', 'c', 'environment', 'step', 'l', 'e', 'c', 'environment', 'step', 'e', 'k', 'r', 'r', 'e', 'environment', 'step', 'e', 'r', 'c', 'environment', 'step', 'e', 'r', 'c', 'environment', 'step', 'e', 'r', 'e', 'z', 'l', 'r', 'h', 'environment', 'step', 'figure', 'dreamsmooth', 'also', 'improve', 'performance', 'tdmpc', 'hand', 'task', 'vanilla', 'tdmpc', 'unable', 'consistently', 'solve', 'first', 'task', 'even', 'proprioceptive', 'state', 'observation', 'however', 'tdmpc', 'dreamsmooth', 'learn', 'complete', 'task', 'state', 'observation', 'also', 'pixel', 'observa', 'tion', 'suggest', 'dreamsmooth', 'useful', 'broad', 'range', 'mbrl', 'rithms', 'use', 'reward', 'model', 'demonstrate', 'hand', 'task', 'tdmpc', 'fail', 'sparsereward', 'task', 'ablation', 'study', 'hand', 'hand', 'state', 'figure', 'learn', 'curve', 'tdmpc', 'tdmpc', 'dreamsmooth', 'hand', 'task', 'shaded', 'gion', 'show', 'minimum', 'maximum', 'seed', 'datum', 'imbalance', 'possible', 'cause', 'poor', 'reward', 'prediction', 'datum', 'imbalance', 'sparse', 'reward', 'infrequent', 'sequence', 'taine', 'sparse', 'reward', 'rarely', 'sample', 'replay', 'buffer', 'reward', 'model', 'therefore', 'train', 'example', 'sparse', 'reward', 'tentially', 'lead', 'poor', 'prediction', 'test', 'hypothesis', 'conduct', 'experiment', 'oversample', 'probability', 'p', 'sample', 'sequence', 'agent', 'receive', 'sparse', 'reward', 'otherwise', 'sample', 'formly', 'sequence', 'buffer', 'show', 'figure', 'oversampling', 'perform', 'bet', 'ter', 'baseline', 'learn', 'slow', 'dreamsmooth', 'suggest', 'datum', 'imbalance', 'largely', 'contribute', 'difficulty', 'reward', 'prediction', 'factor', 'hinder', 'performance', 'furthermore', 'sample', 'method', 'require', 'domain', 'knowledge', 'reward', 'signal', 'oversample', 'dreamsmooth', 'agnostic', 'scale', 'frequency', 'sparse', 'reward', 'figure', 'use', 'oversampling', 'sequence', 'sparse', 'reward', 'p', 'perform', 'well', 'dreamerv3', 'robodesk', 'bad', 'dreamsmooth', 'gaussian', 'smooth', 'line', 'show', 'median', 'task', 'performance', 'seed', 'shaded', 'region', 'show', 'maximum', 'minimum', 'reward', 'model', 'size', 'sis', 'poor', 'reward', 'prediction', 'reward', 'model', 'enough', 'pacity', 'capture', 'sparse', 'reward', 'test', 'hypothesis', 'increase', 'size', 'reward', 'model', 'layer', 'unit', 'layere', 'unit', 'layer', 'unit', 'keep', 'rest', 'world', 'model', 'observe', 'fig', 'ure', 'smooth', 'change', 'reward', 'model', 'size', 'negligible', 'impact', 'performance', 'dreamsmooth', 'perform', 'reward', 'model', 'size', 'test', 'indicate', 'reward', 'prediction', 'problem', 'simply', 'cause', 'cient', 'model', 'capacity', 'robodesk', 'b', 'hand', 'figure', 'simply', 'increase', 'reward', 'model', 'size', 'negligible', 'impact', 'performance', 'dreamsmooth', 'use', 'layer', 'unit', 'dreamerv3', 'use', 'layer', 'unit', 'dreamerv31280', 'use', 'layer', 'unit', 'figure', 'analyze', 'impact', 'smooth', 'parameter', 'smoothing', 'parameter', 'gaussian', 'respectively', 'robodesk', 'hand', 'observe', 'dreamsmooth', 'insensitive', 'smooth', 'parameter', 'perform', 'well', 'wide', 'range', 'value', 'dreamsmoothgaussian', 'e', 'c', 'environment', 'step', 'l', 'e', 'c', 'environment', 'step', 'l', 'e', 'c', 'dreamsmooth', 'oversample', 'environment', 'step', 'dreamsmooth', 'l', 'e', 'c', 'environment', 'step', 'l', 'e', 'c', 'environment', 'step', 'gaussian', 'smoothing', 'robodesk', 'gaussian', 'smoothing', 'hand', 'c', 'uniform', 'smooth', 'robodesk', 'uniform', 'smoothing', 'hand', 'e', 'smoothing', 'robodesk', 'smoothing', 'hand', 'figure', 'parameter', 'sweep', 'smooth', 'parameter', 'δ', 'line', 'show', 'median', 'task', 'performance', 'seed', 'shaded', 'region', 'show', 'maximum', 'minimum', 'conclusion', 'paper', 'identify', 'reward', 'prediction', 'problem', 'mbrl', 'provide', 'simple', 'yet', 'effective', 'solution', 'reward', 'smooth', 'approach', 'dreamsmooth', 'demonstrate', 'superior', 'performance', 'sparse', 'reward', 'task', 'reward', 'prediction', 'trivial', 'mainly', 'partial', 'observability', 'stochasticity', 'environment', 'moreover', 'dreamsmooth', 'show', 'comparable', 'result', 'commonly', 'use', 'benchmark', 'dmc', 'atari', 'show', 'taskagnostic', 'nature', 'show', 'simple', 'reward', 'smooth', 'approach', 'mitigate', 'difficulty', 'reward', 'prediction', 'improved', 'reward', 'prediction', 'always', 'improve', 'task', 'performance', 'eg', 'crafter', 'predict', 'task', 'reward', 'encourage', 'exploitation', 'less', 'exploration', 'investigation', 'tradeoff', 'promising', 'direction', 'future', 'work', 'dreamsmoothgaussian', 'cid3', 'cid8', 'cid3', 'cid4', 'cid3', 'cid8', 'cid3', 'cid3', 'cid8', 'l', 'e', 'c', 'environment', 'step', 'l', 'e', 'c', 'environment', 'step', '×10⁶', 'cid3', 'cid8', 'cid3', 'cid3', 'cid8', 'cid3', 'cid8', 'cid3', 'cid4', 'l', 'e', 'c', 'environment', 'step', 'l', 'e', 'c', 'environment', 'step', 'dreamsmoothema', 'cid10', 'cid3', 'cid4', 'cid10', 'cid3', 'cid4', 'cid8', 'cid17', 'cid10', 'cid3', 'cid4', 'l', 'e', 'c', 'environment', 'step', 'l', 'e', 'c', 'environment', 'step', 'acknowledgment', 'work', 'support', 'part', 'bair', 'industrial', 'consortium', 'logistic', 'robotic', 'like', 'thank', 'member', 'robot', 'learn', 'lab', 'insightful', 'feedback', 'reference', 'dynamic', 'url', 'https', 'modelbase', 'offline', 'planning', 'international', 'ence', 'learn', 'representation', 'url', 'https', 'openreviewnetforum', 'omnb1g5xzd4', 'mohammad', 'saffar', 'model', 'pixel', 'reward', 'evaluate', 'design', 'tradeoff', 'visual', 'modelbase', 'reinforcement', 'learning', 'arxiv', 'preprint', 'bellemare', 'veness', 'bowl', 'arcade', 'learn', 'environment', 'evaluation', 'platform', 'general', 'agent', 'artificial', 'intelligence', 'research', 'dreamerpro', 'reconstructionfree', 'modelbase', 'reinforce', 'ment', 'learn', 'prototypical', 'representation', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'park', 'face', 'world', 'model', 'backbone', 'rnns', 'transformer', 'arxiv', 'preprint', 'christopher', 'fiorillo', 'newsome', 'wolfram', 'schultz', 'temporal', 'precision', 'reward', 'prediction', 'dopamine', 'neuron', 'nature', 'neuroscience', 'schmidhuber', 'world', 'model', 'arxiv', 'preprint', 'hafner', 'benchmarke', 'spectrum', 'agent', 'capability', 'international', 'conference', 'learn', 'representation', 'dream', 'control', 'learn', 'behavior', 'latent', 'imagination', 'international', 'conference', 'learn', 'representation', 'master', 'atari', 'discrete', 'world', 'model', 'international', 'conference', 'learn', 'representation', 'master', 'diverse', 'domain', 'world', 'model', 'arxiv', 'preprint', 'temporal', 'difference', 'learn', 'model', 'predictive', 'control', 'international', 'conference', 'machine', 'learn', 'robodesk', 'multitask', 'rein', 'learning', 'benchmark', 'https', 'hunt', 'r', 'raymond', 'dissociable', 'reward', 'time', 'signal', 'human', 'midbrain', 'ventral', 'striatum', 'bahl', 'deepak', 'pathak', 'structured', 'world', 'model', 'human', 'video', 'robotic', 'science', 'system', 'policy', 'invariance', 'reward', 'transformation', 'theory', 'application', 'reward', 'shape', 'international', 'conference', 'machine', 'learn', 'volume', 'pp', 'taniguchi', 'dream', 'modelbase', 'reinforcement', 'learning', 'latent', 'imagination', 'reconstruction', 'international', 'conference', 'robotic', 'automation', 'pp', 'doi', 'matthia', 'plappert', 'bowen', 'schneider', 'welinder', 'reinforcement', 'learn', 'challenging', 'robotic', 'environment', 'request', 'research', 'arxiv', 'preprint', 'schrittwieser', 'ioannis', 'laurent', 'sifre', 'schmitt', 'hassabis', 'thore', 'et', 'master', 'atari', 'go', 'chess', 'shogi', 'plan', 'learned', 'model', 'nature', 'younggyo', 'seo', 'pieter', 'abbeel', 'mask', 'world', 'model', 'visual', 'control', 'conference', 'robot', 'learn', 'seo', 'pieter', 'abbeel', 'multiview', 'mask', 'world', 'model', 'visual', 'robotic', 'manipulation', 'international', 'conference', 'machine', 'learn', 'skillbase', 'modelbase', 'reinforcement', 'learning', 'conference', 'robot', 'learn', 'aja', 'maddison', 'schrittwieser', 'ioannis', 'et', 'master', 'game', 'go', 'deep', 'neural', 'network', 'tree', 'search', 'nature', 'schrittwieser', 'ioannis', 'antonoglou', 'aja', 'et', 'master', 'game', 'go', 'human', 'knowledge', 'nature', 'schrittwieser', 'ioannis', 'lanctot', 'laurent', 'sifre', 'thore', 'et', 'general', 'reinforcement', 'learning', 'master', 'chess', 'go', 'selfplay', 'science', 'gautam', 'structured', 'world', 'belief', 'reinforcement', 'learning', 'pomdp', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'sutton', 'reinforcement', 'learn', 'introduction', 'mit', 'press', 'yuval', 'tassa', 'martin', 'riedmiller', 'deepmind', 'control', 'suite', 'arxiv', 'preprint', 'proof', 'let', 'p', 'r', 'give', 'mdp', 'loss', 'generality', 'assume', 'augment', 'form', 'mdp', 'state', 'include', 'entire', 'history', 'state', 'thus', 'reward', 'function', 'r', '˜r', 'access', 'previous', 'state', 'theorem', 'optimal', 'policy', '˜π∗', 'mdp', 'reward', 'smooth', 'past', 'reward', 'smoothing', 'p', 'also', 'optimal', 'original', 'mdp', 'i−l', 'fi', 'i−l', 'fi', 'proof', 'use', 'theorem', 'reward', 'shape', 'guarantee', 'optimal', 'policy', 'introduce', 'modify', 'reward', 'function', 'represent', 'form', 'r', 'st1', '−φ', 'potential', 'function', 'φ', 'new', 'reward', 'function', 'yield', 'optimal', 'policy', 'original', 'reward', 'function', 'r', 'let', 'potential', 'function', 'reward', 'smooth', 'φ', '−1', 'i−l', 'i−l', 'ji1', 'reward', 'shape', 'term', 'represent', 'difference', 'potential', 'function', 'st1', 'follow', 'st1', 'i−l', 'fi', 'st1', 'i−l', 'fi', 'hence', 'follow', 'reward', 'shape', 'smoothing', 'guarantee', 'optimal', 'policy', 'original', 'mdp', 'however', 'theorem', 'apply', 'smooth', 'function', 'require', 'access', 'future', 'reward', 'gaussian', 'smoothing', 'gaussian', 'smooth', 'smoothed', 'reward', 'function', 'require', 'future', 'reward', 'condition', 'current', 'policy', 'reward', 'model', 'case', 'theoretical', 'guarantee', 'experiment', 'empirically', 'show', 'reward', 'model', 'adapt', 'prediction', 'change', 'policy', 'thus', 'improve', 'mbrl', 'instead', 'intuitively', 'explain', 'optimal', 'policy', 'reward', 'smooth', 'even', 'reward', 'function', 'hoc', 'define', 'mdps', 'also', 'optimal', 'original', 'reward', 'function', 'theorem', 'optimal', 'policy', '˜π∗', 'smoothed', 'reward', 'function', 'also', 'optimal', 'original', 'reward', 'function', 'r', 'l', 'γclip', '−t', '−t', 'fi', 'sclip', 'ti0', 'l', 'i−l', 'fi', 'proof', 'first', 'show', 'discount', 'sum', 'original', 'reward', 'smooth', 'reward', 'trajectory', 'l', 'γclip', '−t', '−t', 'fi', 'sclip', 'ti0', 'equation', 'γtr', 'cid88', 'γtr', 'l', 'cid88', 'fi', 'i−l', 'γtr', 'cid88', 'l', 'i−l', 'fi', 'let', 'optimal', 'policy', 'smoothed', 'reward', '˜π∗', 'assume', '˜π∗', 'optimal', 'original', 'reward', 'r', 's0', 'e', 'cid104', 'cid88', 'e', 'cid104', 'γt', 'however', 'e', 'cid104', 'cid88', 'e', 'e', 'cid104', 'cid88', 'cid104', 'cid88', 'equation', 'equation', 'contradict', '˜π∗', 'optimal', 'therefore', 'optimal', 'policy', '˜π∗', 'guarantee', 'optimality', 'r', 'b', 'implementation', 'detail', 'model', 'train', 'rtx', 'rtx', 'rtx', 'gpu', 'experiment', 'take', 'hour', 'robodesk', 'hour', 'hand', 'hour', 'earthmove', 'hour', 'crafter', 'hour', 'atari', 'dmc', 'task', 'dreamsmooth', 'smoothing', 'function', 'gaussian', 'smoothing', 'follow', 'gaussian', 'distribution', 'fi', 'l', 'e', 'implement', 'use', 'normalization', 'constant', 'scipyndimagegaussianfilter1d', 'reward', 'mode', 'near', 'uniform', 'smoothing', 'distribute', 'reward', 'equally', 'consecutive', 'timestep', 'δ', 'δ', 'δ', 'fi', 'cid104', 'implement', 'use', 'scipyndimageconvolve', 'reward', 'filter', 'mode', 'near', 'smoothing', 'use', 'follow', 'smoothing', 'function', 'fi', '≤', 'implement', 'perform', 'following', 'timestep', 'reward', 'alpha', 'reward', 'alpha', 'reward', 'b2', 'modelbase', 'reinforcement', 'learning', 'backbone', 'hyperparameter', 'dreamerv3', 'experiment', 'show', 'table', 'tdmpc', 'table', 'table', 'dreamerv3', 'hyperparameter', 'episode', 'length', 'measure', 'environment', 'step', 'number', 'agent', 'step', 'multiply', 'action', 'repeat', 'model', 'size', 'list', 'hafner', 'also', 'refer', 'hyperparameter', 'environment', 'action', 'repeat', 'episode', 'length', 'train', 'ratio', 'model', 'size', 'earthmove', 'robodesk', 'hand', 'crafter', 'dmc', 'atari', 'variable', 'variable', 'l', 'δ', 'table', 'tdmpc', 'hyperparameter', 'specify', 'use', 'default', 'hyperparameter', 'environment', 'latent', 'dimension', 'channel', 'plan', 'iteration', 'σ', 'c', 'environment', 'detail', 'c1', 'robodesk', 'environment', 'use', 'modify', 'version', 'robodesk', 'sequence', 'manipulation', 'task', 'flat', 'block', 'upright', 'block', 'table', 'push', 'green', 'need', 'complete', 'order', 'figure', 'show', 'image', 'agent', 'successfully', 'complete', 'task', 'original', 'environment', 'dense', 'reward', 'base', 'euclidean', 'distance', 'object', 'target', 'additional', 'term', 'encourage', 'arm', 'reach', 'object', 'typically', 'range', 'timestep', 'use', 'dense', 'reward', 'together', 'large', 'sparse', 'reward', 'task', 'complete', 'put', 'flat', 'block', 'push', 'upright', 'block', 'table', 'press', 'figure', 'subtask', 'robodesk', 'hand', 'environment', 'modify', 'shadow', 'hand', 'environment', 'agent', 'require', 'achieve', 'sequence', 'predefine', 'goal', 'orientation', 'order', 'first', 'goal', 'show', 'figure', 'subsequent', 'goal', 'repeat', 'first', 'goal', 'orientation', 'choose', 'agent', 'rotate', 'cube', 'zaxis', 'require', 'agent', 'match', 'cube', 'rotation', 'goal', 'position', 'original', 'environment', 'dense', 'reward', 'compute', 'use', 'r', '10x', 'euclidean', 'distance', 'fix', 'position', 'angular', 'difference', 'target', 'orientation', 'addition', 'dense', 'reward', 'provide', 'large', 'sparse', 'reward', 'goal', 'successfully', 'achieve', 'agent', 'goal', 'b', 'goal', 'c', 'goal', 'figure', 'subtask', 'hand', 'c3', 'earthmove', 'environment', 'earthmove', 'environment', 'consist', 'wheel', 'loader', 'dump', 'truck', 'pile', 'dirt', 'rock', 'top', 'pile', 'en', 'vironment', 'simulate', 'use', 'realistic', 'dynamic', 'physics', 'engine', 'agent', 'control', 'wheel', 'loader', 'pick', 'rock', 'dump', 'dump', 'truck', 'start', 'position', 'dirt', 'pile', 'wheel', 'loader', 'dump', 'truck', 'randomize', 'initial', 'orientation', 'dirt', 'pile', 'wheel', 'loader', 'agent', 'observation', 'consist', 'component', 'wideangle', 'egocentric', 'camera', 'mount', 'cabin', 'allow', 'navigation', 'camera', 'mount', 'bucket', 'observe', 'interaction', 'rock', 'proprioceptive', 'observation', 'position', 'velocity', 'speed', 'force', 'actuator', 'use', '×', 'image', 'camera', 'proprioceptive', 'observation', 'dimension', 'figure', 'agent', 'use', 'camera', 'mount', 'cabin', 'leave', 'navigation', 'mount', 'bucket', 'right', 'observe', 'interaction', 'rock', 'terrain', 'action', 'space', 'dimension', 'drive', 'steer', 'loader', 'dimension', 'move', 'tilt', 'bucket', 'reward', 'consist', 'large', 'sparse', 'reward', 'rock', 'pick', 'dump', 'dense', 'reward', 'move', 'rock', 'dumptruck', 'total', 'reward', 'timestep', 'compute', 'use', 'equation', 'λdump', 'load', 'cid125', 'load', 'load', 'cid125', 'dump', 'λload', 'dump', 'dense', 'reward', 'reward', 'mdump', 'mload', 'rock', 'masse', 'dumptruck', 'bucket', 'respectively', 'distance', 'shovel', 'point', 'dumptruck', 'constant', 'dmc', 'atari', 'benchmarking', 'result', 'figure', 'full', 'learning', 'curve', 'dmc', 'atari', 'benchmark', 'dreamsmoothgaussian', 'e', 'r', 'c', 'environment', 'step', 'reacher', 'hard', 'e', 'r', 'c', 'environment', 'step', '×10⁶', 'cartpole', 'sparse', 'e', 'r', 'c', 'environment', 'step', 'walker', 'run', 'e', 'r', 'c', 'environment', 'step', 'finger', 'turn', 'hard', 'e', 'r', 'c', 'environment', 'step', '×10⁶', 'quadrupe', 'run', 'e', 'r', 'c', 'environment', 'step', 'run', 'e', 'r', 'c', 'environment', 'step', 'pong', 'r', 'c', 'environment', 'step', 'breakout', 'e', 'r', 'c', 'environment', 'step', 'freeway', 'e', 'r', '−1', 'environment', 'step', 'assault', 'c', 'environment', 'step', 'seaqu', 'e', 'r', 'c', 'environment', 'step', 'hero', 'r', 'c', 'environment', 'step']"
RETSim: Resilient and Efficient Text Similarity,"[{'href': 'http://arxiv.org/abs/2311.17264v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2311.17264v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-11-28 22:54:33,"3
2
0
2

v
o
N
7

]

G
L
.
s
c
[

1
v
1
1
7
3
0
.
1
1
3
2
:
v
i
X
r
a

MITIGATING ESTIMATION ERRORS BY TWIN TD-
REGULARIZED ACTOR AND CRITIC FOR DEEP REIN-
FORCEMENT LEARNING

Junmin Zhong
Arizona State University

Ruofan Wu
Arizona State University

Jennie Si ∗
Arizona State University

ABSTRACT

We address the issue of estimation bias in deep reinforcement learning (DRL) by
introducing solution mechanisms that include a new, twin TD-regularized actor-
critic (TDR) method. It aims at reducing both over and under estimation errors.
With TDR and by combining good DRL improvements, such as distributional
learning and long N -step surrogate stage reward (LNSS) method, we show that
our new TDR-based actor-critic learning has enabled DRL methods to outper-
form their respective baselines in challenging environments in DeepMind Control
Suite. Furthermore, they elevate TD3 and SAC respectively to a level of perfor-
mance comparable to that of D4PG (the current SOTA), and they also improve the
performance of D4PG to a new SOTA level measured by mean reward, conver-
gence speed, learning success rate, and learning variance.

1

INTRODUCTION

Reinforcement learning (RL) has been developed for decades to provide a mathematical formal-
ism for learning-based control. Recently, significant progress has been made to attain excellent
results for a wide range of high-dimensional and continuous state-action space problems especially
in robotics applications, such as robot manipulation (Andrychowicz et al., 2017), and human-robotic
interaction (Liu et al., 2022; Wu et al., 2022).

However, the fundamental issue of estimation error associated with actor-critic RL (Van Hasselt
et al., 2016; Duan et al., 2021) still poses great challenge. Overestimation due to, for example, using
the max operator in updates has been identified and studied (Thrun & Schwartz, 1993; Duan et al.,
2021). To reduce it, most efforts have focused on attaining more accurate and stable critic networks.
TD3 (Fujimoto et al., 2018) applies clipped double Q-learning by taking the minimum between the
two Q estimates. SAC (Haarnoja et al., 2018) utilizes the double Q network and incorporates entropy
regularization in the critic objective function to ensure more exploratory behavior to help alleviate
the overestimation problem. However, directly taking the minimum value of the target networks
such as that in TD3 and SAC has been reported to result in an underestimation bias (Fujimoto et al.,
2018).

Evaluations have revealed multiple roles of over and under estimation errors in learning. On one
hand, overestimation may not always be harmful (Lan et al., 2020) as it is considered playing a
role of encouraging exploration by overestimated actions. Along this line, underestimation bias
may discourage exploration. If the overestimation bias occurs in a high-value region containing the
optimal policy, then encouraging exploration is a good thing (Hailu & Sommer, 1999). On the other
hand, overestimation bias may also cause an agent to overly explore a low-value region. This may
lead to a suboptimal policy. Accordingly, an underestimation bias may discourage an agent from
exploring high-value regions or avoiding low-value regions. All things considered, if estimation
errors are left unchecked, they may accumulate to negatively impact policy updates as suboptimal
actions may be highly rated by a suboptimal critic, reinforcing the suboptimal action in the next
policy update (Fujimoto et al., 2018). Aside from the anecdotal evidence on the roles of over and
under estimation, how to mitigate both of them in a principled way remains an open issue.

∗si@asu.edu

1

 
 
 
 
 
 
While several methods and evaluations have been performed and shown promising, a major tool has
been mostly left out thus far. That is, it is still not clear how, and if it is possible, to further reduce
estimation errors by considering the actor given the interplay between the actor and the critic. Only
a handful of approaches have been examined. As shown in (Wu et al., 2023) with demonstrated
performance improvement, PAAC uses a phased actor to account for both a Q value and a TD error
in actor update. A double actor idea was proposed and evaluated in (Lyu et al., 2022). It takes the
minimum value estimate associated with one of the two actor networks. However, directly using the
minimum of the estimated values was shown resulting in an underestimation error, similar to that in
TD3. Other methods, such as Entropy (Haarnoja et al., 2018; Fox et al., 2015), mutual-information
(MI) (Leibfried & Grau-Moya, 2020), and Kullback-Leibler (KL) (Vieillard et al., 2020; Rudner
et al., 2021) regularization, are also used to enhance policy exploration, robustness, and stability.
TD-regularized actor-critic (Parisi et al., 2019) regularizes the actor only aiming to enhance the
stability of the actor learning by applying a TD error (same as that in online critic updates) as a
regularization term in actor updates. However, none of these methods have shown how regularization
in actor may help reduce estimation error in the critic.

In this paper, we propose a new, TD-regularized (TDR) learning mechanism which includes TD-
regularized double critic networks and TD-regularized actor network. This new architecture has
several properties that make it ideal for the enhancements we consider. For the TD-regularized
double critic network, instead of directly selecting the minimum value from twin target networks,
we select the target based on the minimum TD error, which then addresses not only overestimation
but underestimation problems. For the TD-regularized actor network, we formulate a new TD error
to regularize actor updates to avoid a misleading critic. This regularization term helps further reduce
the estimation error in critic updates. Additionally, we apply TDR combined with distributional
RL (Barth-Maron et al., 2018; Bellemare et al., 2017) and LNSS reward estimation method (Zhong
et al., 2022) to further improve learning stability and performance.

2 RELATED WORK

To shed light on the novelty of the TDR method, here we discuss double critic networks and TD
error-based actor learning to provide a backdrop. We include reviews of distributional RL (Barth-
Maron et al., 2018; Bellemare et al., 2017) and long-N -step surrogate stage (LNSS) method (Zhong
et al., 2022) in Appendix A.

Double critic networks have been used in both RL (Hasselt, 2010; Zhang et al., 2017; Weng et al.,
2020) and DRL (Fujimoto et al., 2018; Haarnoja et al., 2018; Van Hasselt et al., 2016). Double Q
learning (Hasselt, 2010; Van Hasselt et al., 2016) was the first to show reduction of overestimation
bias. TD3 (Fujimoto et al., 2018) and SAC (Haarnoja et al., 2018) also were shown effective by
applying clipped double Q-learning by using the minimum between the two Q estimates. However,
these methods have induced an underestimation bias problem. (Hasselt, 2010; Zhang et al., 2017;
Fujimoto et al., 2018). Consequently, weighted double Q learning (Zhang et al., 2017) was proposed
to deal with both overestimation and underestimation biases. However, this method has not been
tested in DRL context and therefore, it lacks a systematic approach to designing the weighting
function.

TD error-based actor learning is expected to be effective in reducing overestimation error since it is
a consistent estimate of the advantage function with lower variance, and it discriminates feedback
instead of directly using Q estimates. Some actor-critic variants (Crites & Barto, 1994; Bhatnagar
et al., 2007) update the actor based on the sign of a TD error with a positive error preferred in
policy updates. However, TD error only measures the discrepancy between the predicted value
and the target value, which may not guide exploration effectively, and using TD error alone in actor
update may discourage exploration and cause slow learning, especially in high-dimensional complex
problems. TD-regularized actor-critic (Parisi et al., 2019) enhanced the stability of the actor update
by using the same TD error (as that in online critic update) as a regularization term. However, such
use of TD error may not sufficiently evaluate the critic update because it only uses the temporal
difference between target and online Q estimates. Additionally, the time-varying regularization
coefficient was shown leading to poor convergence (Chen et al., 2017). Note also that the TD-
regularized actor-critic only considered TD-regularized actor but not the critic.

2

Contributions. 1) We introduce a novel TDR mechanism that includes TD-regularized double critic
networks and TD-regularized actor network. 2) Extensive experiments using DMC benchmarks
show that TDR enables SOTA performance (measureed by learning speed, success rate, variance,
and converged reward) across a wide variety of control tasks, such as locomotion, classical control,
and tasks with sparse rewards. 3) We also provide qualitative analysis to show that each component
of TDR contributes to mitigating both over and under estimation errors.

3 METHOD

3.1 DOUBLE Q IN ACTOR-CRITIC METHOD

For a general double Q actor-critic method (Fujimoto et al., 2018; Haarnoja et al., 2018). The policy
(πϕ) is called an actor and the state-action value function (Qθ(sk, ak)) is called a critic where both
the actor and the critic are estimated by deep neural networks with parameters ϕ and θ, respectively.

First, consider a policy π that is evaluated by the state-action value function below:

Qπ(sk, ak) = E[Rk|sk, ak],

(1)

where Rk = (cid:80)∞
t=k γt−krt, sk ∼ p (· | sk−1, ak−1), ak = πϕ (sk), and γ ∈ (0, 1). Most actor-
critic methods are based on temporal difference (TD) learning (Sutton & Barto, 2018) that updates
Q estimates by minimizing the TD error, which is obtained from the the difference between a target
and a critic estimated value.

Next, consider typical double Q methods which entail twin Q networks denoted as Qθ1 and Qθ2.
The respective twin target networks are denoted as Qθ′
. In the upcoming discussions, we
also use θ to denote parameters in both Q networks, i.e., θ={θ1, θ2}. The target value yk is the lesser
of the two target values,

and Qθ′

1

2

yk = rk + γ min
ζ=1,2

Qθ′

ζ

(sk+1, πϕ′(sk+1)),

(2)

where by taking the minimum of the two target values, it aims to curtail overestimation of Q value
frequently experienced by using a single target. Thus the critic value Qθ is updated by minimizing
the loss function (L (θ)) with respect to the critic weights θ:

L (θ) = Es∼pπ,a∼π[

(cid:88)

(yk − Qθζ (sk, ak))2].

(3)

ζ=1,2

The actor weights can be updated by the deterministic policy gradient algorithm below (Silver et al.,
2014), where by convention (Fujimoto et al., 2018; Haarnoja et al., 2018), Qθ1 is used to update the
actor weights.

∇ϕJ(ϕ) = Es∼pπϕ

(cid:104)

(cid:105)
∇aQθ1(sk, ak)|a=πϕ(s) ∇ϕπϕ(s)

.

(4)

Figure 1: Twin TD-regularized Actor-Critic (TDR) Architecture

3

3.2 TWIN TD-REGULARIZED ACTOR-CRITIC (TDR) ARCHITECTURE

Figure 1 depicts our TDR-based solution mechanisms, which include twin Q networks as in TD3
(Fujimoto et al., 2018) and SAC (Haarnoja et al., 2018), and an actor network. The TDR-based
actor and critic updates are different from currently existing methods. In the following, we show
how the new TDR selects target value yk different from Equation (2) as used in SAC and TD3, and
how that helps reduce both overestimation and underestimation errors. We also show how the new
TD-regularized actor helps further reduce the estimation bias in the critic. Our TDR-based solutions
in Figure 1 include two additional good improvements: distributional learning as in D4PG and long
N -step surrogate stage (LNSS) method (Zhong et al., 2022) as described in Appendix A.

3.3 TD-REGULARIZED DOUBLE Q NETWORKS

To overcome overestimation, TD3 (Fujimoto et al., 2018) and SAC (Haarnoja et al., 2018) train
their critic networks to minimize the loss function in Equation (3) where the target value yk is from
Equation (2). While this helps reduce overestimation error, it promotes a new problem of underes-
timation, which usually occurs during the early stage of learning, or when subjected to corrupted
reward feedback or inaccurate states.

Our TDR method aims at minimizing the same loss function as in Equation (3), but with a different
target value yk. Instead of directly choosing the lesser from the two target values as in Equation (2),
we use the TD errors of the two target networks to set the target value. First, the two TD errors from
the respective target networks are determined from:
δ′
1 = rk + γQθ′
δ′
2 = rk + γQθ′

(sk+1, πϕ′(sk+1)) − Qθ′

(sk+1, πϕ′(sk+1)) − Qθ′

(sk, ak),

(sk, ak).

(5)

(6)

1

1

2

2

The target value for TDR is then selected from the following:

1

yk =

(cid:26) rk + γQθ′
rk + γQθ′

(sk+1, πϕ′(sk+1))
(sk+1, πϕ′(sk+1))
Note from Equation (7) that TDR always uses a target value associated with a smaller target TD
value (regardless of the error sign) between the two. As the ultimate objective of a target network is
to converge to Qπ, such choice by TDR pushes the critic via Equation (3) toward reaching the target
no matter the estimation error is from above or below, but with a smaller TD value. Thus, TDR is
naturally positioned to address both overesdiation and underestimation errors.

1| ≤ |δ′
1| > |δ′

if |δ′
if |δ′

2|,
2|.

(7)

2

3.4 TD-REGULARIZED ACTOR NETWORK

Our TD-regularized actor network directly penalizes the actor’s learning objective whenever there
is a critic estimation error. The estimation error ∆i+1 of the first critic (Qθ1 chosen by convention
of double Q-based actor-critic methods) is determined from the following:

∆i+1 = Qθi+1

1

(sk, ak) − (rk + γQθi+1

1

(sk+1, πϕ(sk+1))),

(8)

where i + 1 represents the iteration number during critic update. Then the actor can be updated in
the direction of maximizing Q while keeping the TD error small,
(cid:20)

(cid:21)

∇ϕJ(ϕ) = Es∼pπϕ

∇a(Qθi+1

1

(cid:12)
(sk, ak) − ρ(∆i+1))
(cid:12)
(cid:12)a=πϕ(s)

∇ϕπϕ(s)

.

(9)

where ρ ∈ (0, 1) is the regularization coefficient to balance the role of TD error in the actor learning
objective. Thus, we expect the TD-regularized actor to help further reduce estimation error in the
critic. With TDR actor and cirtic working together hand-in-hand, TDR is positioned to help avoid
bad policy updates due to a misleading Q value estimate.

Remark 1. There are a few key differences between TDR and TD-regularized Actor Network
(Parisi et al., 2019). 1) In Equation (8), they use the target critic Qθi′
(sk+1, πϕ(sk+1)) to construct
TD error, the same as in critic updates. This TD error evaluates the temporal difference between
target and online Q estimates. To more accurately evaluate critic estimations, we construct the TD
error by only using online critics which directly affects actor updates. 2) Their TD error does not
sufficiently evaluate how the critic updates. Instead in Equation (8), we use the updated critic (θi+1
)
to construct the TD error to directly measure critic estimation.

1

1

4

4 MITIGATING ESTIMATION BIAS BY TDR

Let Qπ be the true Q value obtained by following the current target policy π, and let Qθ be the
estimated value using neural networks. Let Ψk
θ be a random estimation bias. Then for state-action
pairs (sk, ak). we have,

Qθ(sk, ak) = Qπ(sk, ak) + Ψk
θ .
(10)
The same holds for the target networks, i.e., when θ is replaced by θ′ in the above equation. An
overestimation problem refers to when the estimation bias E[Ψk
θ ] > 0, and an underestimation
problem when the estimation bias E[Ψk

θ ] < 0.

4.1 MITIGATING ESTIMATION BIAS USING TD-REGULARIZED DOUBLE CRITIC NETWORKS

Theorem 1. Let Qπ be the true Q value following the current target policy π, and Qθ′
and Qθ′
be the target network estimates using double Q neural networks. We assume that there exists a
step random estimation bias ψk
(i.e., estimation bias at the kth stage), and that it is independent of
θ′
ζ
(sk, ak) with mean E[ψk
ζ, µ′
] = µ′
ζ < ∞, for all k, and ζ = 1, 2. Additionally, let δYk denote
θ′
ζ
the target value estimation error. Accordingly, we denote this error for TDR as δY T DR
, and DQ as
δY DQ
k

. We then have the following,

k

1

2

Where E[δY T DR

k

] = E[Qπ − yT DR

k

]| ≤ |E[δY DQ

k

|E[δY T DR
k
], and E[δY DQ

k

] = E[Qπ − yDQ

k

].

]|,

(11)

Proof. The proof of Theorem 1 is provided in Appendix B

Remark 2. By selecting a target value with less TD error, our TD-regularized double critic networks
mitigate both overestimation and underestimation errors. However, vanilla double Q methods usu-
ally push the target toward the lower value no matter the estimation error is over or under. Although
this estimation error may not be detrimental as they may be small at each update, the presence of
unchecked underestimation bias raises two concerns. Firstly, if there is no sufficient reward feed-
back from the environment, (e.g., for a noisy reward or sparse reward), underestimation bias may
not get a chance to make corrections and may develop into a more significant bias over several up-
dates. Secondly, this inaccurate value estimate may lead to poor policy updates in which suboptimal
actions might be highly rated by the suboptimal critic, reinforcing the suboptimal action in the next
policy update.

4.2 ADDRESSING A MISGUIDING CRITIC IN POLICY UPDATES USING TD-REGULARIZED

ACTOR

Theorem 2. Let Qπ denote the true Q value following the current target policy π, Qθ1 be the
estimated value. We assume that there exists a step random estimation bias ψk
that is independent
θ1
of (sk, ak) with mean E[ψk
] = µ1, µ1 < ∞, for all k. We assume the policy is updated based
θ1
on critic Qθ1 using the deterministic policy gradient (DPG) as in Equation (4). Let δϕk denote the
change in actor parameter ϕ updates at stage k. Accordingly, we denote this change for TDR as
, and true change without any approximation error in Q as δϕtrue
δϕT DR
.
k
k
We then have the following,

, vanilla DPG as δϕDP G

k

(cid:26) E[δϕtrue
k
E[δϕtrue
k

] ≥ E[δϕT DR
] ≤ E[δϕT DR

k

k

] ≥ E[δϕDP G
] ≤ E[δϕDP G

k

k

]
]

if E[Ψk
θ1
if E[Ψk
θ1

] < 0,
] ≥ 0.

(12)

Where δϕtrue
k
Appendix B

, δϕDP G
k

, and δϕT DR

k

are defined as Equation (55),(56), and (57) respectively in

Proof. The proof of Theorem 2 is provided in Appendix B.
Remark 3. Theorem 2, holds for ρ ∈ (0, 1). If the regularization factor ρ = 1
1−γ , from Equation
(59), we have E[Ψk
] = E[δϕT DR
]. By using TDR,
θ1
the actor will always update the same way as using the true value. While this is not realistic, the
following relationship still preserves |E[Ψk
]| to help ease the negative effect of
θ1
critic estimation bias.

− ρ∆] = 0 which implies that E[δϕtrue

− ρ∆]| ≤ |E[Ψk
θ1

k

k

5

4.3 MITIGATING CRITIC ESTIMATION ERROR BY TD-REGULARIZED ACTOR

Theorem 3. Suboptimal actor updates negatively affect the critic. Specifically, consider actor up-
dates as in Theorem 2, in the overestimation case, we have:

E[Qθ1(sk, πDP G(sk)] ≥ E[Qθ1 (sk, πT DR(sk))] ≥ E[Qπ(sk, πT rue(sk))],

and in the underestimation case,

E[Qθ1(sk, πDP G(sk)] ≤ E[Qθ1 (sk, πT DR(sk))] ≤ E[Qπ(sk, πT rue(sk))].

(13)

(14)

Proof The proof of Theorem 3 is provided in Appendix B.

Remark 4. For both cases, by using TD-regularized actors, it is expected to result in less estimation
bias in the critic.

5 EXPERIMENTS AND RESULTS

In this section, we provide a comprehensive evaluation of our TDR enabled actor-critic learning
methods based on three commonly used, well-behaved baseline algorithms including SAC, TD3
and D4PG. Additional evaluations are also provided for popular DRL algorithms such as DDPG and
PPO to provide a broader perspective on the effectiveness of TDR-based methods. All evaluations
are performed based on several benchmarks in Deepmind Control Suite (Tassa et al., 2018).

In reporting evaluation results, we use the following short-form names:

1) Base: the original DRL algorithms including SAC, TD3, D4PG, DDPG and PPO.

2) TDR-TD3: Applied TD regularized double critic (TD Critic) networks, TD regularized actor (TD
Actor) network, with regularization factor ρ = 0.7, and LNSS with N = 100.

3) TDR-SAC: Applied TD regularized double critic (TD Critic) networks, and LNSS with N = 100.

4) dTDR (TDR-D4PG): Applied TD regularized double critic (TD Critic) network, TD regularized
actor (TD Actor) network, with regularization factor ρ = 0.7, and LNSS with N = 100.

Our evaluations aim to quantitatively address the following questions:
Q1. How does TDR improve over Base and other common methods?
Q2. How does the performance of TDR methods compare to that of SOTA algorithms (D4PG)?
Q3. Is TDR method robust enough to handle both dense stochastic reward and sparse reward?
Q4. How does each component in TDR-based learning mechanisms affect performance?
Q5. How does TD regularized actor make policy updates in situations of misguiding critics?
Q6. How does the regularization coefficient ρ in Equation (9) affect TD Actor performance?

Details of the implementation, training, and evaluation procedures are provided in Appendix C and
D where links to all implementation codes are also provided.

5.1 MAIN EVALUATION

In obtaining comprehensive evaluation results summarized in Table 1, we included a 10% noise
respectively in state, action, and reward in each of the considered DMC environments in order
to make the evaluations more realistic. In “Cheetah Run sparse”, we sparsified the reward in the
environment. All details of the environment setup can be found in Appendix C. In Table 1, “Success”
is shorthand for learning success rate, “Avg. Rwd” for average reward, and “Rank” (%) is the
“percent of reward difference” between the evaluated method and the SOTA D4PG, which is (the
average reward of the evaluated method over that of the D4PG - 1), the more positive the better.
Note that, in computing the success rate, only those trials that have achieved a reward of at least
10 are accounted for as successful learning. The results are based on the last 50 evaluations of 10
different random seeds (same for all compared algorithms). Best performances are boldfaced for
average reward (Avg. Rwd). Note that we did not implement our TD Actor into SAC because SAC
already has a max entropy-regulated actor.

Q1 TDR improves over respective Base methods. The learning curves for six benchmark environ-
ments are shown in Figure 2. Overall, TDR methods (solid lines) outperform their respective Base

6

Figure 2: Systematic evaluation of TDR realized in three DRL algorithms (SAC, TD3, D4PG) in
DMC environments with 10% uniform random noise in state, action, and reward. The shaded regions
represent the 95 % confidence range of the evaluations over 10 seeds. The x-axis is the number of
steps.

Envirinoment

D4PG
DDPG
PPO
SAC
TD3
TDR-SAC
TDR-TD3
dTDR

Envirinoment

D4PG
DDPG
PPO
SAC
TD3
TDR-SAC
TDR-TD3
dTDR

Success
[%]
100
100
100
90
100
100
100
100

Success
[%]
100
100
20
0
0
100
100
100

Finger Turn Hard
Avg. Rwd
[µ ± 2σ]
400.9 ± 173.4
222.1 ± 160.4
85.9 ± 50
65.6 ± 30.2
205.9 ± 108.5
601.5 ± 147.4
569.8 ± 142.1
841.02 ± 148.3
Acrobot Swingup
Avg. Rwd
[µ ± 2σ]
26.8 ± 8.9
17.2 ± 3.8
7.9 ± 7.8
4 ± 2.2
5.2 ± 4.2
42.9 ± 5.1
50 ± 7.9
62.6 ± 14.4

Rank
[%]
0
-44.6
-78.6
-83.6
-48.6
49.9
42.3
109.8

Rank
[%]
0
-35.8
-70.5
-85.1
-80.6
60.1
86.5
133.6

Quadruped Walk

Success
[%]
100
100
100
100
100
100
100
100

Avg. Rwd
[µ ± 2σ]
858.5 ± 11.4
226.8 ± 133.6
173.1 ± 60.4
196.6 ± 73.7
334.8 ± 76.4
479.5 ± 126.9
475.4 ± 45.4
888.6 ± 15.7
Cartpole Swingup Sparse

Rank
[%]
0
-73.6
-79.8
-77.2
-61
-44.2
-44.6
3.46

Success
[%]
100
0
80
0
0
100
100
100

Avg. Rwd
[µ ± 2σ]
493.5 ± 15.9
3.6 ± 5.8
99.2 ± 172.9
1.7 ± 3.4
1.3 ± 2.3
774.2 ± 51.1
790.13 ± 33.0
810.3 ± 34.9

Rank
[%]
0
-99
-79.9
-99.7
-99.7
56.8
60.1
64.2

Success
[%]
100
100
100
100
100
100
100
100

Fish Swim
Avg. Rwd
[µ ± 2σ]
153.7 ± 68.1
109.7 ± 27.1
78.67 ± 6.28
73.2 ± 9.87
85.3 ± 21.7
212.3 ± 51.2
204.2 ± 41.5
249.9 ± 45.5
Cheetah Run Sparse

Success
[%]
60
50
0
0
50
100
100
100

Avg. Rwd
[µ ± 2σ]
532.8 ± 388.4
160.7 ± 284.7
0 ± 0
0 ± 0
220.5 ± 354.7
930.2 ± 18.7
827.8 ± 62.2
900.1 ± 30.8

Rank
[%]
0
-28.6
-48.8
-52.4
-44.5
37.9
32.7
62

Rank
[%]
0
-69.8
-100
-100
-58.6
74.6
55.4
68.9

Table 1: Systematic evaluations of TDR respectively augmented Base algorithms. “Rank” (%) is
the “percent of reward difference” between the SOTA D4PG, the more positive the better.

methods TD3, SAC and D4PG (dash lines) in terms of episode reward, learning speed, learning
variance and success rate. In Table 1,among the measures, the Avg. Rwd of TDR methods outper-
formed respective baseline algorithms. Notice from the table that the learning success rates for
all TDR methods are now 100%, a significant improvement over the Base methods. In comparison,
DDPG, SAC and TD3 Base methods struggle with Acrobot Swingup, Cartpole Swingup Sparse,
and Cheetah Run Sparse. Moreover, TDR methods also outperform DDPG and PPO in terms of
averaged reward (Awg.Rwd), learning speed, learning variance, and success rate. Thus, TDR has
helped succesfully address the random initialization challenge caused by random seeds (Henderson
et al., 2018).

Q2 TDR brings performance of Base methods close to or better than that of the SOTA D4PG.
From Figure 2, and according to the “Rank” measure in Table 1, for all environments but Quadruped
walk, TDR (TDR-SAC and TDR-TD3) helped enhance the performances of the respective Base
methods. Additionally, it even outperformed the SOTA D4PG by around 40% in the “Rank” mea-
sure. For Quadruped walk, even though TDR-SAC and TDR-TD3 did not outperform D4PG, they
still are the two methods, among all evaluated, that provided closest performance to D4PG. It is also

7

worth noting that TDR brings the performance of D4PG to a new SOTA level measured by mean
reward, convergence speed, and learning success rate.

Q3 TDR is robust under both dense stochastic reward and sparse reward. From Figure 2 and
Table 2, TDR methods outperformed their respective baselines in both dense stochastic and sparse
reward in terms of average reward, learning variance, success rate, and converge speed. In particular,
baseline algorithms such as TD3 and SAC struggle with sparse reward benchmark environments
(cartpole swingup sparse and cheetah run sparse). However, by using TDR, they not only learned,
but also achieved SOTA performance.

Methods

TD3+TD Critic
TD3+LNSS
TD3+TD Actor
TD3+TDR
SAC+TD Critic
SAC+LNSS
SAC+TDR
D4PG+TD Critic
D4PG+LNSS
D4PG+TD Actor
dTDR

Acrobot Swingup

Finger TurnHard

Avg. Rwd
[µ ± 2σ]
24.9 ± 11.7
24.2 ± 9.2
6.9 ± 2.9
42.9 ± 5.1
28.8 ± 12.2
9.7 ± 2.9
42.9 ± 5.1
32.8 ± 6.9
43.9 ± 16.7
29.9 ± 13.8
62.6 ± 14.4

Enhancement
[%]
378.8
365.4
32.7
725
620
142.5
972.5
22.4
63.8
11.6
133.6

Avg. Rwd
[µ ± 2σ]
556.2 ± 239.8
547.5 ± 120.5
212.3 ± 45.7
569.8 ± 142.1
588 ± 223.8
573 ± 156.5
601.5 ± 147.4
835.7 ± 140.9
675.1 ± 217.6
532.5 ± 235.7
841.1 ± 148.3

Enhancement
[%]
170.1
165.9
3.1
176.7
796.3
773.5
816.9
108.5
68.4
33.5
109.8

Cartpole Swingup Sparse
Avg. Rwd
[µ ± 2σ]
766.2 ± 86.1
766.6 ± 38.3
339.6 ± 231.9
790.1 ± 33.0
766.7 ± 126.4
722.8 ± 162.4
774.2 ± 51.1
678.7 ± 246.2
759.1 ± 31.1
600 ± 129.3
810.3 ± 34.9

Enhancement
[%]
588.4
588.7
260.2
606.7
449.6
423.7
454.4
37.5
53.8
21.6
64.2

Table 2: Systematic evaluations of each component of TDR compared to their respective Base al-
gorithms. “Enhancement” (%) is the “percent of reward difference” between the respective Base
algorithms, the larger the better. Note that TD Actor was not considered for SAC as SAC already
has a max entropy-regularized actor.

Figure 3: Evaluation of TD Actor with different ρ (ρ = 0, 0.1, 0.3, 0.5, 0.7, 0.9) in Equations (9,
21) based on two DRL algorithms (TD3, D4PG) in DMC environments with 10% uniform random
noise in state, action, and reward. The shaded regions represent the 95 % confidence range of the
evaluations over 10 seeds. The x-axis is the number of steps.

5.2 ABLATION STUDY

To perform the ablation study, we examined TDR by removing each of the following three compo-
nents. The respective short-form descriptions are:

1) “TD Critic”: the TD regularized double Q networks.
2) “TD Actor”: the TD regularized actor network.
3) “LNSS”: LNSS method with N = 100.

In Table 2, “Enhancement” (%) is the “percent of reward difference” between the evaluated method
and its Base method, the larger the better.

8

Q4 TD Critic, TD Actor, and LNSS effectively improved the Base algorithms. In Table 2, TD
Critic, LNSS, and TD Actor all effectively improved the Base algorithms. From the table, TD Critic
and LNSS have provided comparable and significant enhancement over Base algorithms. As our TD
Critic methods outperform respective Base algorithms, this suggests that mitigating estimation errors
both over and under from vanilla double Q network is an effective way to improve performance
which has also been shown in our theoretical analysis (Theorem 1). The LNSS method helped
improve learning performance by reducing variances in value estimation for noisy rewards as shown
both theoretically and empirically (Zhong et al., 2022). By including LNSS, our TDR is more robust
under noisy and sparse rewards.

The TD Actor element also helped make appreciable improvements on learning performance as
shown in Table 2. More importantly, TD Actor plays an importantly role in TDR since it not only
stabilizes the policy updates as shown theoretically in Theorem 2 but also addresses the estimation
error in critic as shown theoretically in Theorem 3.

5.3 HYPER PARAMETER STUDY

Hyperparameter study results are summarized in Figure 3 where two DRL methods (D4PG and
TD3) with TD Actor are evaluated for different regularization factor ρ (ρ = 0, 0.1, 0.3, 0.5, 0.7, 0.9).
What is reported is the 10-seed averaged performance, i.e., the average of the approximate es-
timation error which is the difference between the true accumulated reward and the critic value:
Ψ = 1
10

t=0 γtrt − Q(s0, a0)).

eval=0((cid:80)999

(cid:80)9

Q5 TD regularized Actor helps reduce the estimation error in critic.

From Figure 3, with TD regularized Actor (TD Actor), the estimation errors in the critic are re-
duced from those without. For example, in Finger Turn hard, D4PG + TD Actor results in less
overestimation error compared with ρ = 0 at the later stage of training. TD3 + TD Actor has less
underestimation error compared with ρ = 0. Similarly in cartpole swingup sparse, D4PG + TD
Actor results in less overestimation error compared with ρ = 0.

A policy can be evaluated by the “epois reward” where a higher epois reward generally results from a
better policy. From Figure 3, policy updates are improved by selecting a suitable regularization fac-
tor ρ. Especially, in cartpole swingup sparse, TD3 + TD Actor enables successful learning whereas
the Base method struggled and stuck to 0 or no learning for the entire training period.

Q6 A range of ρ (ρ = 0.3, 0.5, 0.7) generally are good choices. From Figure 3, a small regular-
ization factor ρ = 0.1 in TDR will result in less regularization which may not provide sufficient
estimation error reduction in the critic. A larger regularization factor ρ = 0.9 in TDR will result in
more regularization and may have a negative effect on learning. Therefore, ρ = 0.3, 0.5, 0.7 may be
good choices. Therefore in this work, we have consistently used ρ = 0.7 in obtaining all results.

6 CONCLUSION, DISCUSSION, AND LIMITATION OF THE STUDY

1) In this work, we introduce a novel TDR mechanism that includes TD-regularized double critic
networks and TD-regularized actor network. Both components are shown to help mitigate both
over and under estimation errors. TDR has been shown to consistently outperform respective Base
algorithms in solving benchmark tasks in terms of average reward, learning success rate, learning
speed, and most times, learning variance. 2) Our analytical results also show that each component of
TDR helps mitigate both over and under estimation errors. 3) As shown in Figure 2, for five out of
the six environments (except quadruped walk) evaluated, our TDR combined with distributional and
LNSS elements has significantly elevated the current SOTA performance of D4PG to a new level
with an increase of at least 60%.

Even though we have identified a range of generally good regularization coefficient ρ values
(0.3, 0.5, 0.7), as Figure 3 shows, different algorithms in different environments have responded
somewhat differently to ρ. Therefore, how to effectively determine a regularization factor to have
the most improvement remains a question, and thus, it is the limitation of this study. Additionally,
the promising performances of TDR come after extensive training with millions of learning steps.
How TDR performs under limited training time and training steps need to be further investigated.

9

REFERENCES

Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. arXiv
preprint arXiv:1707.01495, 2017.

Gabriel Barth-Maron, Matthew W Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva Tb,
Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributed distributional deterministic
policy gradients. arXiv preprint arXiv:1804.08617, 2018.

Marc G Bellemare, Will Dabney, and R´emi Munos. A distributional perspective on reinforcement

learning. In International conference on machine learning, pp. 449–458. PMLR, 2017.

Shalabh Bhatnagar, Mohammad Ghavamzadeh, Mark Lee, and Richard S Sutton. Incremental nat-

ural actor-critic algorithms. Advances in neural information processing systems, 20, 2007.

Renzhi Chen, Ke Li, and Xin Yao. Dynamic multiobjectives optimization with a changing number

of objectives. IEEE Transactions on Evolutionary Computation, 22(1):157–171, 2017.

Robert Crites and Andrew Barto. An actor/critic algorithm that is equivalent to q-learning. Advances

in Neural Information Processing Systems, 7, 1994.

Will Dabney, Georg Ostrovski, David Silver, and R´emi Munos.

Implicit quantile networks for
distributional reinforcement learning. In International conference on machine learning, pp. 1096–
1105. PMLR, 2018a.

Will Dabney, Mark Rowland, Marc Bellemare, and R´emi Munos. Distributional reinforcement
learning with quantile regression. In Proceedings of the AAAI Conference on Artificial Intelli-
gence, volume 32, 2018b.

Jingliang Duan, Yang Guan, Shengbo Eben Li, Yangang Ren, Qi Sun, and Bo Cheng. Distributional
soft actor-critic: Off-policy reinforcement learning for addressing value estimation errors. IEEE
transactions on neural networks and learning systems, 33(11):6584–6598, 2021.

Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft

updates. arXiv preprint arXiv:1512.08562, 2015.

Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
critic methods. In International Conference on Machine Learning, pp. 1587–1596. PMLR, 2018.

Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and appli-
cations. arXiv preprint arXiv:1812.05905, 2018.

G Hailu and G Sommer. On amount and quality of bias in reinforcement learning. In IEEE SMC’99
Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics
(Cat. No. 99CH37028), volume 2, pp. 728–733. IEEE, 1999.

Hado Hasselt. Double q-learning. Advances in neural information processing systems, 23, 2010.

Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.
Deep reinforcement learning that matters. In Proceedings of the AAAI conference on artificial
intelligence, volume 32, 2018.

Qingfeng Lan, Yangchen Pan, Alona Fyshe, and Martha White. Maxmin q-learning: Controlling

the estimation bias of q-learning. arXiv preprint arXiv:2002.06487, 2020.

Felix Leibfried and Jordi Grau-Moya. Mutual-information regularization in markov decision pro-
cesses and actor-critic learning. In Conference on Robot Learning, pp. 360–373. PMLR, 2020.

Wentao Liu, Junmin Zhong, Ruofan Wu, Bretta L Fylstra, Jennie Si, and He Helen Huang. Inferring
human-robot performance objectives during locomotion using inverse reinforcement learning and
inverse optimal control. IEEE Robotics and Automation Letters, 7(2):2549–2556, 2022.

10

Jiafei Lyu, Xiaoteng Ma, Jiangpeng Yan, and Xiu Li. Efficient continuous control with double
actors and regularized critics. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 36, pp. 7655–7663, 2022.

Fabio Pardo. Tonic: A deep reinforcement learning library for fast prototyping and benchmarking.

arXiv preprint arXiv:2011.07537, 2020.

Simone Parisi, Voot Tangkaratt, Jan Peters, and Mohammad Emtiyaz Khan. Td-regularized actor-

critic methods. Machine Learning, 108:1467–1501, 2019.

Tim GJ Rudner, Cong Lu, Michael A Osborne, Yarin Gal, and Yee Teh. On pathologies in kl-
regularized reinforcement learning from expert demonstrations. Advances in Neural Information
Processing Systems, 34:28376–28389, 2021.

David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In International conference on machine learning, pp.
387–395. Pmlr, 2014.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.

Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Bud-
den, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv
preprint arXiv:1801.00690, 2018.

Sebastian Thrun and Anton Schwartz.

Issues in using function approximation for reinforcement
learning. In Proceedings of the Fourth Connectionist Models Summer School, volume 255, pp.
263. Hillsdale, NJ, 1993.

Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016.

Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier Pietquin, R´emi Munos, and Matthieu Geist.
Leverage the average: an analysis of kl regularization in reinforcement learning. Advances in
Neural Information Processing Systems, 33:12163–12174, 2020.

Wentao Weng, Harsh Gupta, Niao He, Lei Ying, and R Srikant. The mean-squared error of double

q-learning. Advances in Neural Information Processing Systems, 33:6815–6826, 2020.

Ruofan Wu, Junmin Zhong, Brent Wallace, Xiang Gao, He Huang, and Jennie Si. Human-robotic
prosthesis as collaborating agents for symmetrical walking. Advances in Neural Information
Processing Systems, 35:27306–27320, 2022.

Ruofan Wu, Junmin Zhong, and Jennie Si. Phased actor in actor-critic reinforcement learning. 2023.

Zongzhang Zhang, Zhiyuan Pan, and Mykel J Kochenderfer. Weighted double q-learning. In IJCAI,

pp. 3455–3461, 2017.

Junmin Zhong, Ruofan Wu, and Jennie Si. Long n-step surrogate stage reward to reduce variances
of deep reinforcement learning in complex problems. arXiv preprint arXiv:2210.04820, 2022.

A DISTRIBUTIONAL TDR AND LNSS

The distributional RL (Bellemare et al., 2017) represents value function in terms of probability
distribution rather than function estimates. This distribution provides a more comprehensive rep-
resentation of the uncertainty associated with a range of different possible reward returns and state
action pairs which can provide more informative value function estimation. Many distributional RL
algorithms (Bellemare et al., 2017; Dabney et al., 2018b;a) has been achieved great performance im-
provements on many discrete problems such as Atari benchmarks. D4PG (Barth-Maron et al., 2018)
applied distributional RL into continuous control problem by combining the distributional return
function within an actor-critic framework. DSAC (Duan et al., 2021) address overestimation error
by applying distributional RL piggyback on SAC. Although, D4PG and DSAC can provide more
accurate critic, the overestimation of actor still exists since the actor is still updated by maximizing
the expectation of value function distribution. How to regulate actors in distributional RL in solving
overestimations was barely discussed before.

11

A.1 DISTRIBUTIONAL TD-REGULARIZED ACTOR-CRITIC (DTDR)

Here we tailor a distributional TDR (dTDR) method based on the original distributional conceptu-
alization developed in D4PG (Barth-Maron et al., 2018; Bellemare et al., 2017). We show a number
of enhancements in the meantime.

Distributional Critic. The distributional critic (Bellemare et al., 2017 )treated the return in Equa-
tion 1 as a random variable Z(sk, ak) whose expectation is used as the Q value estimate, namely,
Q(sk, ak) = E[Z(sk, ak)].
In dTDR however, we use TD errors to evaluate distributional critics. Similar to Equation 5 and 6,
distributional TD errors of the two target networks can be written as:
(sk+1, πϕ′(sk+1))] − E[Zθ′

1 = rk + γE[Zθ′
d′

(sk, ak)],

(15)

1

1

2 = rk + γE[Zθ′
d′

2

(sk+1, πϕ′(sk+1))] − E[Zθ′

2

(sk, ak)].

The twin TD-regularized target distributional Bellman operator is thus defined as:

T Zk

D=

(cid:26) rk + γZθ′
rk + γZθ′

1

2

(sk+1, πϕ′(sk+1))
(sk+1, πϕ′(sk+1))

if |d′
if |d′

1| ≤ |d′
2|
1| > |d′
2|

(16)

(17)

where A D= B denotes that two random variables A and B follow the same probability laws. Al-
though the distributional Bellman operator appears similar to Equation 1, it maps state-action pairs
to distributions. As such, we need to define a new TD error measure for the distribution as in D4PG
(Barth-Maron et al., 2018). We consider using the following distributional loss,

L(θ) = Es∼pπ,a∼π[

(cid:88)

ζ=1,2

l(T Zk, Zθζ (sk, ak))],

(18)

where l measures the distance between two distributions. Many distributional RL algorithms use
Kullback-Leibler (KL) divergence as the distance metric (Duan et al., 2021; Barth-Maron et al.,
2018. We adopt the same metric.

Distributional Actor. In most distributional methods (Barth-Maron et al., 2018; Bellemare et al.,
2017), policy updates are performed based on the policy gradient below,

∇ϕJ(ϕ) = Es∼pπϕ

[E[∇aZθ(sk, ak)]|a=πϕ(s)∇ϕπϕ(s)].

(19)

In our dTDR, we need to use critic evaluation metrics to evaluate the quality of the current distribu-
tional critic and the regularized distributional actor. We first formulate the following loss metric:

Lz(ϕ) = E[l(rk + γZθi+1

1

(sk+1, πϕ(sk+1)), Zθi+1

1

(sk, πϕ(sk))].

(20)

Similar to TD-regularized actor network, the distributional actor is updated in the direction of max-
imizing the expected critic while keeping the expected distance between the projected critic and the
critic, namely,

∇ϕJ(ϕ) = Es∼pπϕ

[(E[∇aZθi+1

1

(sk, ak)] − ∇aρLz(ϕ))|a=πϕ(s)∇ϕπϕ(s)],

(21)

where ρ ∈ (0, 1) is a regularization coefficient.

A.2 LONG N-STEP SURROGATE STAGE (LNSS) REWARD

LNSS (Zhong et al., 2022) utilizes a long reward trajectory of N future steps in the estimation
of stage reward rk. Using the LNSS-resulted reward r′
k in place of the original rk was shown
to effectively reduce learning variance with significant performance improvements for off-policy
methods. Given a reward trajectory of N steps from time step k, let G(sk:k+N −1, ak:k+N −1) ∈ R
(with shorthand notation Gk) denote the discounted N -step return, i.e.,

k+N −1
(cid:88)

γt−krt,

Gk =

t=k

12

(22)

where rt is the tth stage reward and t is from k to k + N − 1. In LNSS, r′
reward in place of rk in Equation (2). To determine r′
N -step reward sequence, namely

k is a surrogate stage
k, LNSS treat it as a weighted average of the

r′
k =

γt−krt

(cid:80)k+N −1
t=k
(cid:80)N −1

n=0 γn

.

(23)

As Figure 1 shows, Once r′
(sk, ak, r′
as discussed.

k is obtained, it is simply used in place of rk to form a new tuple
k, sk+1), which is then stored into the memory buffer D. The TDR method proceeds

B ESTIMATION ANALYSIS

Lemma 1. Let Qπ be the true Q value following the current target policy π, and Qθ′
be the
target network estimates using double Q neural networks. We assume that there exists a step random
estimation bias ψk
(i.e., estimation bias at the kth stage), and that it is independent of (sk, ak) with
θ′
ζ
mean E[ψk
ζ, µ′
] = µ′
2 respectively defined in
θ′
ζ
Equations (5) and (6), we have,

ζ < ∞, for all k, and ζ = 1, 2. Then for δ′

1 and δ′

and Qθ′

2

1

E[δ′
E[δ′

1] = −µ′
1,
2] = −µ′
2.

(24)

(25)

(26)

Proof. With the step random estimation bias ψk
θ′
ζ

, We can rewrite the expectation of Ψk
θ′
ζ

as

E[Ψk+1
θ′
ζ

] =

∞
(cid:88)

t=k+1

γt−k−1E[ψt
θ′
ζ

] =

1
1 − γ

µ′
ζ.

Then the expectation of the target can be written as,

E[yk] = E[rk] + γE[(Qπ(sk+1, ak+1) + Ψk+1
∞
(cid:88)

θ′
ζ

)]

= E[rk] + γ(E[

γt−k−1rt]) +

γ
1 − γ

µ′
ζ

= Qπ(sk, ak) +

t=k+1
γ
1 − γ

µ′
ζ.

By using Equations (10), and (26), the TD errors of the two target critics (Equations 5 and 6),
respectably are:

E[δ′

1] = E[rk] + γE[Qθ′

1

(sk+1, πϕ′(sk+1))] − E[Qθ′
1
1
γ
1 − γ
1 − γ

1 − Qπ(sk, ak) −
µ′

(sk, ak)]

µ′
1

(27)

= Qπ(sk, ak) +

Similarly, E[δ′

= −µ′
1.
2] = −µ′
2.

Thus Lemma 1 holds.

With Lemma 1 in place, we are now ready to analyze the estimation errors by using TDR and the
double Q (DQ) method as in TD3 (Fujimoto et al., 2018) and SAC (Haarnoja et al., 2018).

Theorem 1. Let assumptions in Lemma 1 hold, and let δYk denote the target value estimation
error. Accordingly, we denote this error for TDR as δY T DR
. We then have the
following,

, and DQ as δY DQ

k

k

|E[δY T DR
k

]| ≤ |E[δY DQ

k

]|.

(28)

Proof. The proof is based on enumerating a total of 8 possible scenarios of estimation errors which
are determined from the relationships among the two target Q values and the true Qπ value . We
provide proofs for the 4 out of 8 unique scenarios below.

13

First note that, E[δY T DR
Case 1: If the target critic values and the true value Qπ have the following relationship:

] = E[Qπ − yT DR

] = E[Qπ − yDQ

], and E[δY DQ

].

k

k

k

k

i.e, Qθ′

1

is more underestimated as

that implies

E[Qθ′

1

] < E[Qθ′

2

] < Qπ,

|E[Ψk
θ′
1

]| > |E[Ψk
θ′
2

]|,

|µ′

1| > |µ′

2|.

Based on Lemma 1 and Equation (7), our TDR will use Qθ′

2

in the target value,

E[yT DR
k

] = E[rk] + γE[Qθ′

2

(sk+1, πϕ′(sk+1))].

However for a vanilla double Q network, the target value will be Qθ′

1

,

] = E[rk] + γE[Qθ′
Thus based on Equation (26), the two estimation errors of the respective target values are

(sk+1, πϕ′(sk+1))].

E[yDQ
k

1

|E[δY T DR
k

]| = |E[Qπ − yT DR

k

]| = |

|E[δY DQ
k

]| = |E[Qπ − yDQ

k

]| = |

µ′

2|,

γ
1 − γ
γ
µ′
1 − γ

1|.

Since |µ′

1| > |µ′

2|, we have

|E[δY T DR
k

]| < |E[δY DQ

k

]|.

Thus identity (28) holds.
Case 2: If the target critic values and the true value Qπ have the following relationship:

E[Qθ′
|E[Qπ − Qθ′

1

] < Qπ < E[Qθ′

2],
]| > |E[Qπ − Qθ′

1

]|,

2

is expected to be underestimated and Qθ′

is overestimated. Since |E[Qπ − Qθ′

1

]| >

2

then Qθ′
|E[Qπ − Qθ′

1

2

we thus have

]| which implies

|E[Ψk
θ′
1

]| > |E[Ψk
θ′
2

]|,

|µ′

1| > |µ′

2|.

Based on Lemma 1 and Equation (7), our TDR will use Qθ′

2

in the target value:

E[yT DR
k

] = E[rk] + γE[Qθ′

2

(sk+1, πϕ′(sk+1))].

However for a vanilla double Q network, the target value will use Qθ′

1

,

] = E[rk] + γE[Qθ′
Based on Equation (26), the two estimation errors of the respective target values are:

(sk+1, πϕ′(sk+1))].

E[yDQ
k

1

|E[δY T DR
k

]| = |E[Qπ − yT DR

k

]| = |

|E[δY DQ
k

]| = |E[Qπ − yDQ

k

]| = |

µ′

2|,

γ
1 − γ
γ
µ′
1 − γ

1|,

Since |µ′

1| > |µ′

2|, we have

|E[δY T DR
k

]| < |E[δY DQ

k

]|.

Thus identity (28) holds.
Case 3: If the target critic values and the true value Qπ has the following relationship:

] < Qπ < E[Qθ′

],

2

E[Qθ′
|E[Qπ − Qθ′

1

1

]| < |E[Qπ − Qθ′

2

]|,

14

(29)

(30)

(31)

(32)

(33)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

(42)

(43)

is expected to be underestimated and Qθ′

is overestimated. Since |E[Qπ − Qθ′

1

]| <

2

then Qθ′
|E[Qπ − Qθ′

1

2

]|, it implies

thus we have

|E[Ψk
θ′
1

]| < |E[Ψk
θ′
2

]|,

(44)

1| < |µ′
Based on Lemma 1 and Equation (7), both vanilla double Q network and our TDR will pick Qθ′
the target value:

|µ′

2|.

1

(45)

in

E[yT DR
k

] = E[yDQ

k

] = E[rk] + γE[Qθ′

1

(sk+1, πϕ′(sk+1))].

Then based on Equation (26), the two estimation errors of the respective target values are:

|E[δY T DR
k

]| = |E[δY DQ

k

]| = |E[Qπ − yT DR

k

]| = |

γ
1 − γ

µ′

1|.

We thus have

|E[δY T DR
k

]| = |E[δY DQ

k

]|.

Thus identity (28) holds.
Case 4: If the target critic values and the true value Qπ has the following relationship

where E[Qθ′

2

] is expected more overestimated ie |E[Ψk
θ′
1

]| < |E[Ψk
θ′
2

]| that implies

Qπ < E[Qθ′

1

] < E[Qθ′

2

],

|µ′

1| < |µ′

2|.

(46)

(47)

(48)

(49)

(50)

Based on Equation (24) and (7), same with vanilla double Q network, our Twin TD-regularized
Critic will pick the target value using Qθ′

which both mitigates the larger overestimation bias as:

1

E[yT DR
k

] = E[yDQ

k

] = E[rk] + γE[Qθ′

1

(sk+1, πϕ′(sk+1))],

which based on Equation (26), the two estimation errors of the target value are

|E[δY T DR
k

]| = |E[δY DQ

k

]| = |E[Qπ − yT DR

k

]| = |

γ
1 − γ

µ′

1|.

We have

|E[δY T DR
k

]| = |E[δY DQ

k

]|.

(51)

(52)

(53)

1

1

2

k

] > E[Qθ′

] < E[Qθ′

]| ≤ |E[δY DQ

Thus identity (28) holds. Both methods can mitigate the overestimation error.
Note, the above cases study the relationship of E[Qθ′
for E[Qθ′
], |E[δY T DR
Theorem 2. Let Qπ denote the true Q value following the current target policy π, Qθ1 be the
estimated value. We assume that there exists a step random estimation bias ψk
that is independent
θ1
of (sk, ak) with mean E[ψk
] = µ1, µ1 < ∞, for all k. We assume the policy is updated based on
θ1
critic Qθ1 using the deterministic policy gradient (DPG) as in Equation 4. Let δϕk denote the change
in actor parameter ϕ updates at stage k. Accordingly, we denote this change for TDR as δϕT DR
,
k
vanilla DPG as δϕDP G
. We then
k
have the following,

, and true change without any approximation error in Q as δϕtrue

]| still valid. Thus Theorem 1 holds.

] and by applying same procedure

k

k

2

(cid:26) E[δϕtrue
k
E[δϕtrue
k

] ≥ E[δϕT DR
] ≤ E[δϕT DR

k

k

] ≥ E[δϕDP G
] ≤ E[δϕDP G

k

k

]
]

if E[Ψk
θ1
if E[Ψk
θ1

] < 0,
] ≥ 0.

(54)

Proof. With learning rate α, the true change of the actor parameters in case without any approxima-
tion error in Q:

E[δϕtrue
k

] = αEs∼pπ

(cid:104)

ϕj

∇aQπ(sk, ak)|a=πϕj (s) ∇ϕj πϕj (s)

(cid:105)

.

(55)

15

Consider the estimated critic and the true value follow the relationship in Equation 10. Given the
same current policy parameters ϕj, the updated parameters using DPG are:

ϕj+1
DP G = ϕj + αEs∼pπ
(cid:104)

E[δϕDP G
k

] = αEs∼pπ

ϕj

(cid:104)

ϕj

∇a(Qπ(sk, ak) + Ψk
θ1

(cid:105)
)(cid:12)
(cid:12)a=πϕj (s) ∇ϕj πϕj (s)

,

∇a(Qπ(sk, ak) + Ψk
θ1

)(cid:12)
(cid:12)a=πϕj (s) ∇ϕj πϕj (s)

(cid:105)

.

(56)

With an overestimation bias E[Ψk
θ1
mated actions, and with an underestimation bias E[Ψk
θ1
the underestimated actions. Both result in suboptimal policies.
However, by using TD-regularized actor, and given the same current policy parameters ϕj, the actor
updates with Equation (9) are:

] > 0, the updates encourage more exploration for the overesti-
] < 0, the updates discourage exploration for

ϕj+1
T DR = ϕj + αEs∼pπ
] = αEs∼pπ

E[δϕT DR
k

ϕj

[∇a(Qπ(sk, ak) + Ψk
θ1

ϕj

− ρ(∆))|a=πϕj (s)∇ϕj πϕj (s)],

[∇a(Qπ(sk, ak) + Ψk
θ1

− ρ(∆))|a=πϕj (s)∇ϕj πϕj (s)].

Similar to Lemma 1, E[Ψk
θ1

] = 1

1−γ µ1, and from Equations (8) and (9) we have:

E[∆] = E[Qθi+1
= µ1

1

(sk, ak)] − E[(rk + γQθi+1

1

(sk+1, πϕ(sk+1)))]

by selecting ρ ≤ 1

1−γ , we have the following:

(cid:26) 0 ≥ E[Ψk
θ1
0 ≤ E[Ψk
θ1

− ρ∆] > E[Ψk
θ1
− ρ∆] ≤ E[Ψk
θ1

]
]

if E[Ψk
θ1
if E[Ψk
θ1

] < 0,
] ≥ 0.

Therefore by inspecting Equations (55), (56) and (57), we have:

(cid:26) E[δϕtrue
k
E[δϕtrue
k

Thus Theorem 2 holds.

] ≥ E[δϕT DR
] ≤ E[δϕT DR

k

k

] ≥ E[δϕDP G
] ≤ E[δϕDP G

k

k

]
]

if E[Ψk
θ1
if E[Ψk
θ1

] < 0,
] ≥ 0.

(57)

(58)

(59)

(60)

Theorem 3. Suboptimal actor updates negatively affect the critic. Specifically, consider actor up-
dates as in Theorem 2, in the overestimation case, we have:

E[Qθ1(sk, πDP G(sk)] ≥ E[Qθ1 (sk, πT DR(sk))] ≥ E[Qπ(sk, πT rue(sk))],

(61)

and in the underestimation case,

E[Qθ1(sk, πDP G(sk)] ≤ E[Qθ1 (sk, πT DR(sk))] ≤ E[Qπ(sk, πT rue(sk))].

(62)

Proof Following the analysis of the TD3 (Fujimoto et al., 2018), consider Equation (12) in Theorem
2, we have

(cid:26) E[δϕtrue
k
E[δϕtrue
k

] ≥ E[δϕT DR
] ≤ E[δϕT DR

k

k

] ≥ E[δϕDP G
] ≤ E[δϕDP G

k

k

]
]

if E[Ψk
θ1
if E[Ψk
θ1

] < 0 Underestimate,
] ≥ 0 Overestimate.

(63)

In the overestimation case, the approximate value using TDR and vanilla DPG must be

E[Qθ1(sk, πDP G(sk)] ≥ E[Qθ1(sk, πT DR(sk))] ≥ E[Qπ(sk, πtrue(sk))].

(64)

Similarly, in the underestimation case, the approximate value using TDR and vanilla DPG must be

E[Qθ1(sk, πDP G(sk)] ≤ E[Qθ1 (sk, πT DR(sk))] ≤ E[Qπ(sk, πT rue(sk))].

(65)

Thus Theorem 3 holds.

16

C IMPLEMENTATION DETAILS

We use PyTorch for all implementations. All results were obtained using our internal server consist-
ing of AMD Ryzen Threadripper 3970X Processor, a desktop with Intel Core i7-9700K processor,
and two desktops with Intel Core i9-12900K processor.

Training Procedure.

An episode is initialized by resetting the environment, and terminated at max step T = 1000. A trial
is a complete training process that contains a series of consecutive episodes. Each trial is run for a
maximum of 1 × 106 time steps with evaluations at every 2 × 104 time steps. Each task is reported
over 10 trials where the environment and the network were initialized by 10 random seeds, (0 − 9)
in this study.

For each training trial, to remove the dependency on the initial parameters of a policy, we use a
purely exploratory policy for the first 8000 time steps (start timesteps). Afterwards, we use an
off-policy exploration strategy, adding Gaussian noise N (0, 0.1) to each action.

Evaluation Procedure.
Every 1 × 104 time steps training, we have an evaluation section and each evaluation reports the
average reward over 5 evaluation episodes, with no exploration noise and with fixed policy weights.
The random seeds for evaluation are different from those in training which each trial, evaluations
were performed using seeds (seeds + 100).

Network Structure and optimizer.

TD3.The actor-critic networks in TD3 are implemented by feedforward neural networks with three
layers of weights. Each layer has 256 hidden nodes with rectified linear units (ReLU) for both the
actor and critic. The input layer of actor has the same dimension as observation state. The output
layer of the actor has the same dimension as action requirement with a tanh unit. Critic receives both
state and action as input to THE first layer and the output layer of critic has 1 linear unit to produce
Q value. Network parameters are updated using Adam optimizer with a learning rate of 10−3 for
simple control problems. After each time step k, the networks are trained with a mini-batch of a
256 transitions (s, a, r, s′), (s, a, r′, s′) in case of LNSS, sampled uniformly from a replay buffer D
containing the entire history of the agent.

D4PG. Same with the actor-critic networks in D4PG are implemented by feedforward neural net-
works with three layers of weights. Each layer has 256 hidden nodes with rectified linear units
(ReLU) for both the actor and critic. The input layer of actor has the same dimension as observa-
tion state. The output layer of the actor has the same dimension as action requirement with a tanh
unit. Critic receives both state and action as input to THE first layer and the output layer of critic
has a distribution with hyperparameters for the number of atoms l, and the bounds on the support
(Vmin, Vmax). Network parameters are updated using Adam optimizer with a learning rate of 10−3.
After each time step k, the networks are trained with a mini-batch of 256 transitions (s, a, r, s′),
(s, a, r′, s′) in case of LNSS, sampled uniformly from a replay buffer D containing the entire his-
tory of the agent.

SAC. The actor-critic networks in SAC are implemented by feedforward neural networks with three
layers of weights. Each layer has 256 hidden nodes with rectified linear units (ReLU) for both the
actor and critic. The input layer of actor has the same dimension as observation state. The output
layer of the actor has the same dimension as action requirement with a tanh unit. Critic receives both
state and action as input to the first layer and the output layer of critic has 1 linear unit to produce
Q value. Network parameters are updated using Adam optimizer with a learning rate of 10−3 for
simple control problems. After each time step k, the networks are trained with a mini-batch of a
256 transitions (s, a, r, s′), (s, a, r′, s′) in case of LNSS, sampled uniformly from a replay buffer D
containing the entire history of the agent.

Hyperparameters. To keep comparisons in this work fair, we set all common hyperparameters
(network layers, batch size, learning rate, discount factor, number of agents, etc) to be the same for
comparison within the same methods and different methods.

For TD3, target policy smoothing is implemented by adding ϵ ∼ N (0, 0.2) to the actions chosen
by the target actor-network, clipped to (−0.5, 0.5), delayed policy updates consist of only updating

17

the actor and target critic network every d iterations, with d = 2. While a larger d would result in
a larger benefit with respect to accumulating errors, for fair comparison, the critics are only trained
once per time step, and training the actor for too few iterations would cripple learning. Both target
networks are updated with τ = 0.005.

The TD3 and TD3+TDR used in this study are based on the paper (Fujimoto et al., 2018) and the
code from the authors (https://github.com/sfujim/TD3).

Hyperparameter TD3
Start timesteps
Evaluation frequency
Max timesteps
Exploration noise
Policy noise
Noise clip
Policy update frequency
Batch size
Buffer size
γ
τ
Number of parallel actor
LNSS-N
Adam Learning rate
regularization factor

Value
8000 steps
20000 steps
1e6 steps
N (0, 0.1)
N (0, 0.2)
±0.5
2
256
1e6
0.99
0.005
1
100
1e-3
0.7

Table 3: TD3 + TDR hyper parameters used for DMC benckmark tasks

The SAC used in this study is based on paper (Haarnoja et al., 2018) and the code is from GitHub
(https://github.com/pranz24/pytorch-soft-actor-critic). and the hyperparameter is from Table 4.

Value
8000 steps
20000 steps
1e6 steps
N (0, 0.1)
N (0, 0.2)
±0.5
2
256
1e6
0.99
0.005

Hyperparameter SAC
Start timesteps
Evaluation frequency
Max timesteps
Exploration noise
Policy noise
Noise clip
Policy update frequency
Batch size
Buffer size
γ
τ
Temperature parameter α 0.2
Number of parallel actor
LNSS-N
Adam Learning rate

1
100
1e-3

Table 4: SAC hyper parameters used for the DMC benckmark tasks

The D4PG used in this study is based on paper (Barth-Maron et al., 2018) and the code is modified
from TD3. The hyperparameter is from Table 5.

All Other algorithms are from the same DRL training platform (Tonic RL) (Pardo, 2020) with the
same evaluation as the above algorithms.

Sparse Reward Setup. 1) Cheetah Run Sparse: Cheetah needs to run forward as fast as possible.
The agent gets a reward only after speed exceeds 2.5 m/s, making the reward sparse. r = 1. That
is, if v >= 2.5 else r = 0.

18

Hyperparameter D4PG
Start timesteps
Evaluation frequency
Max timesteps
Exploration noise
Noise clip
Batch size
Buffer size
γ
τ
Number of parallel actor
LNSS-N
Adam Learning rate
Vmax
Vmin
l
regularization factor

Value
8000 steps
20000 steps
1e6 steps
N (0, 0.1)
±0.5
256
1e6
0.99
0.005
1
100
1e-3
100
0
51
0.7

Table 5: D4PG + TDR hyper parameters used for the DMC benckmark tasks

19

D TDR ALGORITHMS DETAILS

In this section, we show our TDR-based algorithms. TD3-TDR is shown in Algorithm 1, SAC-
TDR is shown in Algorithm 2, and D4PG-TDR is shown in Algorithm 3. We mainly add LNSS
reward to the sample collection part. In algorithm update part, we mainly modify the target value
selection using Equation 7 for regular DRL and Equation (17) for distributional DRL. Additionally,
if applicable, we modify the actor gradient based on Equation 9 for regular DRL and Equation (21)
for distributional DRL. All codes will be released to GitHub once the paper get accepted.

Algorithm 1 TD3-TDR
Initialize:

1 ← θ1, θ′

2 ← θ2, ϕ′ ← ϕ,

• Critic networks Qθ1,Qθ2 and actor-network πϕ with random parameters, θ1, θ2, ϕ
• Target networks θ′
• an experience buffer D
• a temporary experience buffer D′ with size N
• Total training episode T
1. For episode = 1, T do
2.

Reset initialize state s0, D′
For k = 0, T do

3.

4.

5.

6.

7.

8.

9.

10.

11.

12.

13.

14.

15.

16.

17.

18.

19.

20.

21.

22.

23.

24.

25.

Choose an action ak based on current state sk and learned policy from A.
Execute the action ak and observe a new state sk+1 with reward signal rk
Store the transition (sk, ak, rk, sk+1) in D′
if k + N − 1 ≤ T then

1) in the D′

0, s′

0, r′

0, a′

Get earliest memory (s′
Calculate r′ based on Equation (23)
0, a′
Store the transition (s′
Clear original transition (s′

0, r′, s′
0, a′

1) in D
0, s′
0, r′

1) in the D′

else

Repeat step 8 to 11 and Calculate r′ based on Equation

r′
k =

γ − 1
γT −k+1 − 1

T
(cid:88)

t=k

γt−krt.

(66)

t, st+1) from D

end if
Sample mini-batch data (st, at, r′
Get next action at+1 ← πϕ′(st+1)
Target value yt based on Equation (7)
Update Critics based on Equation 3
if k mod Policy Update frequency then

Update ϕ by Equation 9
Update target networks:
θ′
ζ ← τ θζ + (1 − τ )θ′
ζ
ϕ′ ← τ ϕ + (1 − τ )ϕ′

end if

end for

26.
27. end for

20

Algorithm 2 SAC-TDR
Initialize:

• Soft value function VΞ, target Soft value function V ′

Ξ, Critic networks Qθ1,Qθ2 and actor-

network πϕ with random parameters, θ1, θ2, ϕ

• Target networks Ξ′ ← Ξ
• an experience buffer D
• a temporary experience buffer D′ with size N
• Total training episode T
1. For episode = 1, T do
2.

Reset initialize state s0, D′
For k = 0, T do

3.

4.

5.

6.

7.

8.
9.

10.

11.

12.

13.

14.

15.

16.

17.

18.

19.

20.

21.

22.

23.

Choose an action ak based on current state sk and learned policy from A.
Execute the action ak and observe a new state sk+1 with reward signal rk
Store the transition (sk, ak, rk, sk+1) in D′
if k + N − 1 ≤ T then

1) in the D′

0, s′

0, r′

0, a′

Get earliest memory (s′
Calculate r′ based on Equation (23)
Store the transition (s′
0, a′
Clear original transition (s′

0, r′, s′
0, a′

1) in D
0, s′
0, r′

1) in the D′

else

Repeat step 8 to 11 and Calculate r′ based on Equation

r′
k =

γ − 1
γT −k+1 − 1

T
(cid:88)

t=k

γt−krt.

(67)

end if
Sample mini-batch data (st, at, r′
Get next action at+1 ← πϕ′(st+1)
Target value yt based on Equation (7)
Update Critics based on Equation 3

t, st+1) from D

Update Soft value function based on original SAC formualtion
Update ϕ by original SAC formulation
Update target networks:
Ξ′ ← τ Ξ + (1 − τ )Ξ′

end for

24.
25. end for

21

Algorithm 3 D4PG-TDR
Initialize:

1 ← θ1, θ′

2 ← θ2, ϕ′ ← ϕ,

• Critic networks Zθ1,Zθ2 and actor-network πϕ with random parameters, θ1, θ2, ϕ
• Target networks θ′
• an experience buffer D
• a temporary experience buffer D′ with size N
• Total training episode T
1. For episode = 1, T do
2.

Reset initialize state s0, D′
For k = 0, T do

3.

4.

5.

6.

7.

8.

9.

10.

11.

12.

13.

14.

15.

16.

17.

18.

19.

20.

21.

22.

23.

24.

25.

Choose an action ak based on current state sk and learned policy from A.
Execute the action ak and observe a new state sk+1 with reward signal rk
Store the transition (sk, ak, rk, sk+1) in D′
if k + N − 1 ≤ T then

1) in the D′

0, s′

0, r′

0, a′

Get earliest memory (s′
Calculate r′ based on Equation (23)
0, a′
Store the transition (s′
Clear original transition (s′

0, r′, s′
0, a′

1) in D
0, s′
0, r′

1) in the D′

else

Repeat step 8 to 11 and Calculate r′ based on Equation

r′
k =

γ − 1
γT −k+1 − 1

T
(cid:88)

t=k

γt−krt.

(68)

end if
Sample mini-batch data (st, at, r′
Get next action at+1 ← πϕ′(st+1)
Target distribution based on Equation (17)

t, st+1) from D

Update Critics based on Equation 18
if k mod Policy Update frequency then

Update ϕ by Equation 21
Update target networks:
θ′
ζ ← τ θζ + (1 − τ )θ′
ζ
ϕ′ ← τ ϕ + (1 − τ )ϕ′

end if

end for

26.
27. end for

22

","3 2 0 2 v o N 7 ] G L . s c [ 1 v 1 1 7 3 0 . 1 1 3 2 : v i X r a MITIGATING ESTIMATION ERRORS BY TWIN TD- REGULARIZED ACTOR AND CRITIC FOR DEEP REIN- FORCEMENT LEARNING Junmin Zhong Arizona State University Ruofan Wu Arizona State University Jennie Si ∗ Arizona State University ABSTRACT We address the issue of estimation bias in deep reinforcement learning ( DRL ) by introducing solution mechanisms that include a new , twin TD-regularized actor- critic ( TDR ) method . It aims at reducing both over and under estimation errors . With TDR and by combining good DRL improvements , such as distributional learning and long N -step surrogate stage reward ( LNSS ) method , we show that our new TDR-based actor-critic learning has enabled DRL methods to outper- form their respective baselines in challenging environments in DeepMind Control Suite . Furthermore , they elevate TD3 and SAC respectively to a level of perfor- mance comparable to that of D4PG ( the current SOTA ) , and they also improve the performance of D4PG to a new SOTA level measured by mean reward , conver- gence speed , learning success rate , and learning variance . 1 INTRODUCTION Reinforcement learning ( RL ) has been developed for decades to provide a mathematical formal- ism for learning-based control . Recently , significant progress has been made to attain excellent results for a wide range of high-dimensional and continuous state-action space problems especially in robotics applications , such as robot manipulation ( Andrychowicz et al. , 2017 ) , and human-robotic interaction ( Liu et al. , 2022 ; Wu et al. , 2022 ) . However , the fundamental issue of estimation error associated with actor-critic RL ( Van Hasselt et al. , 2016 ; Duan et al. , 2021 ) still poses great challenge . Overestimation due to , for example , using the max operator in updates has been identified and studied ( Thrun & Schwartz , 1993 ; Duan et al. , 2021 ) . To reduce it , most efforts have focused on attaining more accurate and stable critic networks . TD3 ( Fujimoto et al. , 2018 ) applies clipped double Q-learning by taking the minimum between the two Q estimates . SAC ( Haarnoja et al. , 2018 ) utilizes the double Q network and incorporates entropy regularization in the critic objective function to ensure more exploratory behavior to help alleviate the overestimation problem . However , directly taking the minimum value of the target networks such as that in TD3 and SAC has been reported to result in an underestimation bias ( Fujimoto et al. , 2018 ) . Evaluations have revealed multiple roles of over and under estimation errors in learning . On one hand , overestimation may not always be harmful ( Lan et al. , 2020 ) as it is considered playing a role of encouraging exploration by overestimated actions . Along this line , underestimation bias may discourage exploration . If the overestimation bias occurs in a high-value region containing the optimal policy , then encouraging exploration is a good thing ( Hailu & Sommer , 1999 ) . On the other hand , overestimation bias may also cause an agent to overly explore a low-value region . This may lead to a suboptimal policy . Accordingly , an underestimation bias may discourage an agent from exploring high-value regions or avoiding low-value regions . All things considered , if estimation errors are left unchecked , they may accumulate to negatively impact policy updates as suboptimal actions may be highly rated by a suboptimal critic , reinforcing the suboptimal action in the next policy update ( Fujimoto et al. , 2018 ) . Aside from the anecdotal evidence on the roles of over and under estimation , how to mitigate both of them in a principled way remains an open issue . ∗si @ asu.edu 1 While several methods and evaluations have been performed and shown promising , a major tool has been mostly left out thus far . That is , it is still not clear how , and if it is possible , to further reduce estimation errors by considering the actor given the interplay between the actor and the critic . Only a handful of approaches have been examined . As shown in ( Wu et al. , 2023 ) with demonstrated performance improvement , PAAC uses a phased actor to account for both a Q value and a TD error in actor update . A double actor idea was proposed and evaluated in ( Lyu et al. , 2022 ) . It takes the minimum value estimate associated with one of the two actor networks . However , directly using the minimum of the estimated values was shown resulting in an underestimation error , similar to that in TD3 . Other methods , such as Entropy ( Haarnoja et al. , 2018 ; Fox et al. , 2015 ) , mutual-information ( MI ) ( Leibfried & Grau-Moya , 2020 ) , and Kullback-Leibler ( KL ) ( Vieillard et al. , 2020 ; Rudner et al. , 2021 ) regularization , are also used to enhance policy exploration , robustness , and stability . TD-regularized actor-critic ( Parisi et al. , 2019 ) regularizes the actor only aiming to enhance the stability of the actor learning by applying a TD error ( same as that in online critic updates ) as a regularization term in actor updates . However , none of these methods have shown how regularization in actor may help reduce estimation error in the critic . In this paper , we propose a new , TD-regularized ( TDR ) learning mechanism which includes TD- regularized double critic networks and TD-regularized actor network . This new architecture has several properties that make it ideal for the enhancements we consider . For the TD-regularized double critic network , instead of directly selecting the minimum value from twin target networks , we select the target based on the minimum TD error , which then addresses not only overestimation but underestimation problems . For the TD-regularized actor network , we formulate a new TD error to regularize actor updates to avoid a misleading critic . This regularization term helps further reduce the estimation error in critic updates . Additionally , we apply TDR combined with distributional RL ( Barth-Maron et al. , 2018 ; Bellemare et al. , 2017 ) and LNSS reward estimation method ( Zhong et al. , 2022 ) to further improve learning stability and performance . 2 RELATED WORK To shed light on the novelty of the TDR method , here we discuss double critic networks and TD error-based actor learning to provide a backdrop . We include reviews of distributional RL ( Barth- Maron et al. , 2018 ; Bellemare et al. , 2017 ) and long-N -step surrogate stage ( LNSS ) method ( Zhong et al. , 2022 ) in Appendix A . Double critic networks have been used in both RL ( Hasselt , 2010 ; Zhang et al. , 2017 ; Weng et al. , 2020 ) and DRL ( Fujimoto et al. , 2018 ; Haarnoja et al. , 2018 ; Van Hasselt et al. , 2016 ) . Double Q learning ( Hasselt , 2010 ; Van Hasselt et al. , 2016 ) was the first to show reduction of overestimation bias . TD3 ( Fujimoto et al. , 2018 ) and SAC ( Haarnoja et al. , 2018 ) also were shown effective by applying clipped double Q-learning by using the minimum between the two Q estimates . However , these methods have induced an underestimation bias problem . ( Hasselt , 2010 ; Zhang et al. , 2017 ; Fujimoto et al. , 2018 ) . Consequently , weighted double Q learning ( Zhang et al. , 2017 ) was proposed to deal with both overestimation and underestimation biases . However , this method has not been tested in DRL context and therefore , it lacks a systematic approach to designing the weighting function . TD error-based actor learning is expected to be effective in reducing overestimation error since it is a consistent estimate of the advantage function with lower variance , and it discriminates feedback instead of directly using Q estimates . Some actor-critic variants ( Crites & Barto , 1994 ; Bhatnagar et al. , 2007 ) update the actor based on the sign of a TD error with a positive error preferred in policy updates . However , TD error only measures the discrepancy between the predicted value and the target value , which may not guide exploration effectively , and using TD error alone in actor update may discourage exploration and cause slow learning , especially in high-dimensional complex problems . TD-regularized actor-critic ( Parisi et al. , 2019 ) enhanced the stability of the actor update by using the same TD error ( as that in online critic update ) as a regularization term . However , such use of TD error may not sufficiently evaluate the critic update because it only uses the temporal difference between target and online Q estimates . Additionally , the time-varying regularization coefficient was shown leading to poor convergence ( Chen et al. , 2017 ) . Note also that the TD- regularized actor-critic only considered TD-regularized actor but not the critic . 2 Contributions . 1 ) We introduce a novel TDR mechanism that includes TD-regularized double critic networks and TD-regularized actor network . 2 ) Extensive experiments using DMC benchmarks show that TDR enables SOTA performance ( measureed by learning speed , success rate , variance , and converged reward ) across a wide variety of control tasks , such as locomotion , classical control , and tasks with sparse rewards . 3 ) We also provide qualitative analysis to show that each component of TDR contributes to mitigating both over and under estimation errors . 3 METHOD 3.1 DOUBLE Q IN ACTOR-CRITIC METHOD For a general double Q actor-critic method ( Fujimoto et al. , 2018 ; Haarnoja et al. , 2018 ) . The policy ( πϕ ) is called an actor and the state-action value function ( Qθ ( sk , ak ) ) is called a critic where both the actor and the critic are estimated by deep neural networks with parameters ϕ and θ , respectively . First , consider a policy π that is evaluated by the state-action value function below : Qπ ( sk , ak ) = E [ Rk|sk , ak ] , ( 1 ) where Rk = ( cid:80 ) ∞ t=k γt−krt , sk ∼ p ( · | sk−1 , ak−1 ) , ak = πϕ ( sk ) , and γ ∈ ( 0 , 1 ) . Most actor- critic methods are based on temporal difference ( TD ) learning ( Sutton & Barto , 2018 ) that updates Q estimates by minimizing the TD error , which is obtained from the the difference between a target and a critic estimated value . Next , consider typical double Q methods which entail twin Q networks denoted as Qθ1 and Qθ2 . The respective twin target networks are denoted as Qθ′ . In the upcoming discussions , we also use θ to denote parameters in both Q networks , i.e. , θ= { θ1 , θ2 } . The target value yk is the lesser of the two target values , and Qθ′ 1 2 yk = rk + γ min ζ=1,2 Qθ′ ζ ( sk+1 , πϕ′ ( sk+1 ) ) , ( 2 ) where by taking the minimum of the two target values , it aims to curtail overestimation of Q value frequently experienced by using a single target . Thus the critic value Qθ is updated by minimizing the loss function ( L ( θ ) ) with respect to the critic weights θ : L ( θ ) = Es∼pπ , a∼π [ ( cid:88 ) ( yk − Qθζ ( sk , ak ) ) 2 ] . ( 3 ) ζ=1,2 The actor weights can be updated by the deterministic policy gradient algorithm below ( Silver et al. , 2014 ) , where by convention ( Fujimoto et al. , 2018 ; Haarnoja et al. , 2018 ) , Qθ1 is used to update the actor weights . ∇ϕJ ( ϕ ) = Es∼pπϕ ( cid:104 ) ( cid:105 ) ∇aQθ1 ( sk , ak ) |a=πϕ ( s ) ∇ϕπϕ ( s ) . ( 4 ) Figure 1 : Twin TD-regularized Actor-Critic ( TDR ) Architecture 3 3.2 TWIN TD-REGULARIZED ACTOR-CRITIC ( TDR ) ARCHITECTURE Figure 1 depicts our TDR-based solution mechanisms , which include twin Q networks as in TD3 ( Fujimoto et al. , 2018 ) and SAC ( Haarnoja et al. , 2018 ) , and an actor network . The TDR-based actor and critic updates are different from currently existing methods . In the following , we show how the new TDR selects target value yk different from Equation ( 2 ) as used in SAC and TD3 , and how that helps reduce both overestimation and underestimation errors . We also show how the new TD-regularized actor helps further reduce the estimation bias in the critic . Our TDR-based solutions in Figure 1 include two additional good improvements : distributional learning as in D4PG and long N -step surrogate stage ( LNSS ) method ( Zhong et al. , 2022 ) as described in Appendix A . 3.3 TD-REGULARIZED DOUBLE Q NETWORKS To overcome overestimation , TD3 ( Fujimoto et al. , 2018 ) and SAC ( Haarnoja et al. , 2018 ) train their critic networks to minimize the loss function in Equation ( 3 ) where the target value yk is from Equation ( 2 ) . While this helps reduce overestimation error , it promotes a new problem of underes- timation , which usually occurs during the early stage of learning , or when subjected to corrupted reward feedback or inaccurate states . Our TDR method aims at minimizing the same loss function as in Equation ( 3 ) , but with a different target value yk . Instead of directly choosing the lesser from the two target values as in Equation ( 2 ) , we use the TD errors of the two target networks to set the target value . First , the two TD errors from the respective target networks are determined from : δ′ 1 = rk + γQθ′ δ′ 2 = rk + γQθ′ ( sk+1 , πϕ′ ( sk+1 ) ) − Qθ′ ( sk+1 , πϕ′ ( sk+1 ) ) − Qθ′ ( sk , ak ) , ( sk , ak ) . ( 5 ) ( 6 ) 1 1 2 2 The target value for TDR is then selected from the following : 1 yk = ( cid:26 ) rk + γQθ′ rk + γQθ′ ( sk+1 , πϕ′ ( sk+1 ) ) ( sk+1 , πϕ′ ( sk+1 ) ) Note from Equation ( 7 ) that TDR always uses a target value associated with a smaller target TD value ( regardless of the error sign ) between the two . As the ultimate objective of a target network is to converge to Qπ , such choice by TDR pushes the critic via Equation ( 3 ) toward reaching the target no matter the estimation error is from above or below , but with a smaller TD value . Thus , TDR is naturally positioned to address both overesdiation and underestimation errors . 1| ≤ |δ′ 1| > |δ′ if |δ′ if |δ′ 2| , 2| . ( 7 ) 2 3.4 TD-REGULARIZED ACTOR NETWORK Our TD-regularized actor network directly penalizes the actor ’ s learning objective whenever there is a critic estimation error . The estimation error ∆i+1 of the first critic ( Qθ1 chosen by convention of double Q-based actor-critic methods ) is determined from the following : ∆i+1 = Qθi+1 1 ( sk , ak ) − ( rk + γQθi+1 1 ( sk+1 , πϕ ( sk+1 ) ) ) , ( 8 ) where i + 1 represents the iteration number during critic update . Then the actor can be updated in the direction of maximizing Q while keeping the TD error small , ( cid:20 ) ( cid:21 ) ∇ϕJ ( ϕ ) = Es∼pπϕ ∇a ( Qθi+1 1 ( cid:12 ) ( sk , ak ) − ρ ( ∆i+1 ) ) ( cid:12 ) ( cid:12 ) a=πϕ ( s ) ∇ϕπϕ ( s ) . ( 9 ) where ρ ∈ ( 0 , 1 ) is the regularization coefficient to balance the role of TD error in the actor learning objective . Thus , we expect the TD-regularized actor to help further reduce estimation error in the critic . With TDR actor and cirtic working together hand-in-hand , TDR is positioned to help avoid bad policy updates due to a misleading Q value estimate . Remark 1 . There are a few key differences between TDR and TD-regularized Actor Network ( Parisi et al. , 2019 ) . 1 ) In Equation ( 8 ) , they use the target critic Qθi′ ( sk+1 , πϕ ( sk+1 ) ) to construct TD error , the same as in critic updates . This TD error evaluates the temporal difference between target and online Q estimates . To more accurately evaluate critic estimations , we construct the TD error by only using online critics which directly affects actor updates . 2 ) Their TD error does not sufficiently evaluate how the critic updates . Instead in Equation ( 8 ) , we use the updated critic ( θi+1 ) to construct the TD error to directly measure critic estimation . 1 1 4 4 MITIGATING ESTIMATION BIAS BY TDR Let Qπ be the true Q value obtained by following the current target policy π , and let Qθ be the estimated value using neural networks . Let Ψk θ be a random estimation bias . Then for state-action pairs ( sk , ak ) . we have , Qθ ( sk , ak ) = Qπ ( sk , ak ) + Ψk θ . ( 10 ) The same holds for the target networks , i.e. , when θ is replaced by θ′ in the above equation . An overestimation problem refers to when the estimation bias E [ Ψk θ ] > 0 , and an underestimation problem when the estimation bias E [ Ψk θ ] < 0 . 4.1 MITIGATING ESTIMATION BIAS USING TD-REGULARIZED DOUBLE CRITIC NETWORKS Theorem 1 . Let Qπ be the true Q value following the current target policy π , and Qθ′ and Qθ′ be the target network estimates using double Q neural networks . We assume that there exists a step random estimation bias ψk ( i.e. , estimation bias at the kth stage ) , and that it is independent of θ′ ζ ( sk , ak ) with mean E [ ψk ζ , µ′ ] = µ′ ζ < ∞ , for all k , and ζ = 1 , 2 . Additionally , let δYk denote θ′ ζ the target value estimation error . Accordingly , we denote this error for TDR as δY T DR , and DQ as δY DQ k . We then have the following , k 1 2 Where E [ δY T DR k ] = E [ Qπ − yT DR k ] | ≤ |E [ δY DQ k |E [ δY T DR k ] , and E [ δY DQ k ] = E [ Qπ − yDQ k ] . ] | , ( 11 ) Proof . The proof of Theorem 1 is provided in Appendix B Remark 2 . By selecting a target value with less TD error , our TD-regularized double critic networks mitigate both overestimation and underestimation errors . However , vanilla double Q methods usu- ally push the target toward the lower value no matter the estimation error is over or under . Although this estimation error may not be detrimental as they may be small at each update , the presence of unchecked underestimation bias raises two concerns . Firstly , if there is no sufficient reward feed- back from the environment , ( e.g. , for a noisy reward or sparse reward ) , underestimation bias may not get a chance to make corrections and may develop into a more significant bias over several up- dates . Secondly , this inaccurate value estimate may lead to poor policy updates in which suboptimal actions might be highly rated by the suboptimal critic , reinforcing the suboptimal action in the next policy update . 4.2 ADDRESSING A MISGUIDING CRITIC IN POLICY UPDATES USING TD-REGULARIZED ACTOR Theorem 2 . Let Qπ denote the true Q value following the current target policy π , Qθ1 be the estimated value . We assume that there exists a step random estimation bias ψk that is independent θ1 of ( sk , ak ) with mean E [ ψk ] = µ1 , µ1 < ∞ , for all k. We assume the policy is updated based θ1 on critic Qθ1 using the deterministic policy gradient ( DPG ) as in Equation ( 4 ) . Let δϕk denote the change in actor parameter ϕ updates at stage k. Accordingly , we denote this change for TDR as , and true change without any approximation error in Q as δϕtrue δϕT DR . k k We then have the following , , vanilla DPG as δϕDP G k ( cid:26 ) E [ δϕtrue k E [ δϕtrue k ] ≥ E [ δϕT DR ] ≤ E [ δϕT DR k k ] ≥ E [ δϕDP G ] ≤ E [ δϕDP G k k ] ] if E [ Ψk θ1 if E [ Ψk θ1 ] < 0 , ] ≥ 0 . ( 12 ) Where δϕtrue k Appendix B , δϕDP G k , and δϕT DR k are defined as Equation ( 55 ) , ( 56 ) , and ( 57 ) respectively in Proof . The proof of Theorem 2 is provided in Appendix B . Remark 3 . Theorem 2 , holds for ρ ∈ ( 0 , 1 ) . If the regularization factor ρ = 1 1−γ , from Equation ( 59 ) , we have E [ Ψk ] = E [ δϕT DR ] . By using TDR , θ1 the actor will always update the same way as using the true value . While this is not realistic , the following relationship still preserves |E [ Ψk ] | to help ease the negative effect of θ1 critic estimation bias . − ρ∆ ] = 0 which implies that E [ δϕtrue − ρ∆ ] | ≤ |E [ Ψk θ1 k k 5 4.3 MITIGATING CRITIC ESTIMATION ERROR BY TD-REGULARIZED ACTOR Theorem 3 . Suboptimal actor updates negatively affect the critic . Specifically , consider actor up- dates as in Theorem 2 , in the overestimation case , we have : E [ Qθ1 ( sk , πDP G ( sk ) ] ≥ E [ Qθ1 ( sk , πT DR ( sk ) ) ] ≥ E [ Qπ ( sk , πT rue ( sk ) ) ] , and in the underestimation case , E [ Qθ1 ( sk , πDP G ( sk ) ] ≤ E [ Qθ1 ( sk , πT DR ( sk ) ) ] ≤ E [ Qπ ( sk , πT rue ( sk ) ) ] . ( 13 ) ( 14 ) Proof The proof of Theorem 3 is provided in Appendix B . Remark 4 . For both cases , by using TD-regularized actors , it is expected to result in less estimation bias in the critic . 5 EXPERIMENTS AND RESULTS In this section , we provide a comprehensive evaluation of our TDR enabled actor-critic learning methods based on three commonly used , well-behaved baseline algorithms including SAC , TD3 and D4PG . Additional evaluations are also provided for popular DRL algorithms such as DDPG and PPO to provide a broader perspective on the effectiveness of TDR-based methods . All evaluations are performed based on several benchmarks in Deepmind Control Suite ( Tassa et al. , 2018 ) . In reporting evaluation results , we use the following short-form names : 1 ) Base : the original DRL algorithms including SAC , TD3 , D4PG , DDPG and PPO . 2 ) TDR-TD3 : Applied TD regularized double critic ( TD Critic ) networks , TD regularized actor ( TD Actor ) network , with regularization factor ρ = 0.7 , and LNSS with N = 100 . 3 ) TDR-SAC : Applied TD regularized double critic ( TD Critic ) networks , and LNSS with N = 100 . 4 ) dTDR ( TDR-D4PG ) : Applied TD regularized double critic ( TD Critic ) network , TD regularized actor ( TD Actor ) network , with regularization factor ρ = 0.7 , and LNSS with N = 100 . Our evaluations aim to quantitatively address the following questions : Q1 . How does TDR improve over Base and other common methods ? Q2 . How does the performance of TDR methods compare to that of SOTA algorithms ( D4PG ) ? Q3 . Is TDR method robust enough to handle both dense stochastic reward and sparse reward ? Q4 . How does each component in TDR-based learning mechanisms affect performance ? Q5 . How does TD regularized actor make policy updates in situations of misguiding critics ? Q6 . How does the regularization coefficient ρ in Equation ( 9 ) affect TD Actor performance ? Details of the implementation , training , and evaluation procedures are provided in Appendix C and D where links to all implementation codes are also provided . 5.1 MAIN EVALUATION In obtaining comprehensive evaluation results summarized in Table 1 , we included a 10 % noise respectively in state , action , and reward in each of the considered DMC environments in order to make the evaluations more realistic . In “ Cheetah Run sparse ” , we sparsified the reward in the environment . All details of the environment setup can be found in Appendix C. In Table 1 , “ Success ” is shorthand for learning success rate , “ Avg . Rwd ” for average reward , and “ Rank ” ( % ) is the “ percent of reward difference ” between the evaluated method and the SOTA D4PG , which is ( the average reward of the evaluated method over that of the D4PG - 1 ) , the more positive the better . Note that , in computing the success rate , only those trials that have achieved a reward of at least 10 are accounted for as successful learning . The results are based on the last 50 evaluations of 10 different random seeds ( same for all compared algorithms ) . Best performances are boldfaced for average reward ( Avg . Rwd ) . Note that we did not implement our TD Actor into SAC because SAC already has a max entropy-regulated actor . Q1 TDR improves over respective Base methods . The learning curves for six benchmark environ- ments are shown in Figure 2 . Overall , TDR methods ( solid lines ) outperform their respective Base 6 Figure 2 : Systematic evaluation of TDR realized in three DRL algorithms ( SAC , TD3 , D4PG ) in DMC environments with 10 % uniform random noise in state , action , and reward . The shaded regions represent the 95 % confidence range of the evaluations over 10 seeds . The x-axis is the number of steps . Envirinoment D4PG DDPG PPO SAC TD3 TDR-SAC TDR-TD3 dTDR Envirinoment D4PG DDPG PPO SAC TD3 TDR-SAC TDR-TD3 dTDR Success [ % ] 100 100 100 90 100 100 100 100 Success [ % ] 100 100 20 0 0 100 100 100 Finger Turn Hard Avg . Rwd [ µ ± 2σ ] 400.9 ± 173.4 222.1 ± 160.4 85.9 ± 50 65.6 ± 30.2 205.9 ± 108.5 601.5 ± 147.4 569.8 ± 142.1 841.02 ± 148.3 Acrobot Swingup Avg . Rwd [ µ ± 2σ ] 26.8 ± 8.9 17.2 ± 3.8 7.9 ± 7.8 4 ± 2.2 5.2 ± 4.2 42.9 ± 5.1 50 ± 7.9 62.6 ± 14.4 Rank [ % ] 0 -44.6 -78.6 -83.6 -48.6 49.9 42.3 109.8 Rank [ % ] 0 -35.8 -70.5 -85.1 -80.6 60.1 86.5 133.6 Quadruped Walk Success [ % ] 100 100 100 100 100 100 100 100 Avg . Rwd [ µ ± 2σ ] 858.5 ± 11.4 226.8 ± 133.6 173.1 ± 60.4 196.6 ± 73.7 334.8 ± 76.4 479.5 ± 126.9 475.4 ± 45.4 888.6 ± 15.7 Cartpole Swingup Sparse Rank [ % ] 0 -73.6 -79.8 -77.2 -61 -44.2 -44.6 3.46 Success [ % ] 100 0 80 0 0 100 100 100 Avg . Rwd [ µ ± 2σ ] 493.5 ± 15.9 3.6 ± 5.8 99.2 ± 172.9 1.7 ± 3.4 1.3 ± 2.3 774.2 ± 51.1 790.13 ± 33.0 810.3 ± 34.9 Rank [ % ] 0 -99 -79.9 -99.7 -99.7 56.8 60.1 64.2 Success [ % ] 100 100 100 100 100 100 100 100 Fish Swim Avg . Rwd [ µ ± 2σ ] 153.7 ± 68.1 109.7 ± 27.1 78.67 ± 6.28 73.2 ± 9.87 85.3 ± 21.7 212.3 ± 51.2 204.2 ± 41.5 249.9 ± 45.5 Cheetah Run Sparse Success [ % ] 60 50 0 0 50 100 100 100 Avg . Rwd [ µ ± 2σ ] 532.8 ± 388.4 160.7 ± 284.7 0 ± 0 0 ± 0 220.5 ± 354.7 930.2 ± 18.7 827.8 ± 62.2 900.1 ± 30.8 Rank [ % ] 0 -28.6 -48.8 -52.4 -44.5 37.9 32.7 62 Rank [ % ] 0 -69.8 -100 -100 -58.6 74.6 55.4 68.9 Table 1 : Systematic evaluations of TDR respectively augmented Base algorithms . “ Rank ” ( % ) is the “ percent of reward difference ” between the SOTA D4PG , the more positive the better . methods TD3 , SAC and D4PG ( dash lines ) in terms of episode reward , learning speed , learning variance and success rate . In Table 1 , among the measures , the Avg . Rwd of TDR methods outper- formed respective baseline algorithms . Notice from the table that the learning success rates for all TDR methods are now 100 % , a significant improvement over the Base methods . In comparison , DDPG , SAC and TD3 Base methods struggle with Acrobot Swingup , Cartpole Swingup Sparse , and Cheetah Run Sparse . Moreover , TDR methods also outperform DDPG and PPO in terms of averaged reward ( Awg.Rwd ) , learning speed , learning variance , and success rate . Thus , TDR has helped succesfully address the random initialization challenge caused by random seeds ( Henderson et al. , 2018 ) . Q2 TDR brings performance of Base methods close to or better than that of the SOTA D4PG . From Figure 2 , and according to the “ Rank ” measure in Table 1 , for all environments but Quadruped walk , TDR ( TDR-SAC and TDR-TD3 ) helped enhance the performances of the respective Base methods . Additionally , it even outperformed the SOTA D4PG by around 40 % in the “ Rank ” mea- sure . For Quadruped walk , even though TDR-SAC and TDR-TD3 did not outperform D4PG , they still are the two methods , among all evaluated , that provided closest performance to D4PG . It is also 7 worth noting that TDR brings the performance of D4PG to a new SOTA level measured by mean reward , convergence speed , and learning success rate . Q3 TDR is robust under both dense stochastic reward and sparse reward . From Figure 2 and Table 2 , TDR methods outperformed their respective baselines in both dense stochastic and sparse reward in terms of average reward , learning variance , success rate , and converge speed . In particular , baseline algorithms such as TD3 and SAC struggle with sparse reward benchmark environments ( cartpole swingup sparse and cheetah run sparse ) . However , by using TDR , they not only learned , but also achieved SOTA performance . Methods TD3+TD Critic TD3+LNSS TD3+TD Actor TD3+TDR SAC+TD Critic SAC+LNSS SAC+TDR D4PG+TD Critic D4PG+LNSS D4PG+TD Actor dTDR Acrobot Swingup Finger TurnHard Avg . Rwd [ µ ± 2σ ] 24.9 ± 11.7 24.2 ± 9.2 6.9 ± 2.9 42.9 ± 5.1 28.8 ± 12.2 9.7 ± 2.9 42.9 ± 5.1 32.8 ± 6.9 43.9 ± 16.7 29.9 ± 13.8 62.6 ± 14.4 Enhancement [ % ] 378.8 365.4 32.7 725 620 142.5 972.5 22.4 63.8 11.6 133.6 Avg . Rwd [ µ ± 2σ ] 556.2 ± 239.8 547.5 ± 120.5 212.3 ± 45.7 569.8 ± 142.1 588 ± 223.8 573 ± 156.5 601.5 ± 147.4 835.7 ± 140.9 675.1 ± 217.6 532.5 ± 235.7 841.1 ± 148.3 Enhancement [ % ] 170.1 165.9 3.1 176.7 796.3 773.5 816.9 108.5 68.4 33.5 109.8 Cartpole Swingup Sparse Avg . Rwd [ µ ± 2σ ] 766.2 ± 86.1 766.6 ± 38.3 339.6 ± 231.9 790.1 ± 33.0 766.7 ± 126.4 722.8 ± 162.4 774.2 ± 51.1 678.7 ± 246.2 759.1 ± 31.1 600 ± 129.3 810.3 ± 34.9 Enhancement [ % ] 588.4 588.7 260.2 606.7 449.6 423.7 454.4 37.5 53.8 21.6 64.2 Table 2 : Systematic evaluations of each component of TDR compared to their respective Base al- gorithms . “ Enhancement ” ( % ) is the “ percent of reward difference ” between the respective Base algorithms , the larger the better . Note that TD Actor was not considered for SAC as SAC already has a max entropy-regularized actor . Figure 3 : Evaluation of TD Actor with different ρ ( ρ = 0 , 0.1 , 0.3 , 0.5 , 0.7 , 0.9 ) in Equations ( 9 , 21 ) based on two DRL algorithms ( TD3 , D4PG ) in DMC environments with 10 % uniform random noise in state , action , and reward . The shaded regions represent the 95 % confidence range of the evaluations over 10 seeds . The x-axis is the number of steps . 5.2 ABLATION STUDY To perform the ablation study , we examined TDR by removing each of the following three compo- nents . The respective short-form descriptions are : 1 ) “ TD Critic ” : the TD regularized double Q networks . 2 ) “ TD Actor ” : the TD regularized actor network . 3 ) “ LNSS ” : LNSS method with N = 100 . In Table 2 , “ Enhancement ” ( % ) is the “ percent of reward difference ” between the evaluated method and its Base method , the larger the better . 8 Q4 TD Critic , TD Actor , and LNSS effectively improved the Base algorithms . In Table 2 , TD Critic , LNSS , and TD Actor all effectively improved the Base algorithms . From the table , TD Critic and LNSS have provided comparable and significant enhancement over Base algorithms . As our TD Critic methods outperform respective Base algorithms , this suggests that mitigating estimation errors both over and under from vanilla double Q network is an effective way to improve performance which has also been shown in our theoretical analysis ( Theorem 1 ) . The LNSS method helped improve learning performance by reducing variances in value estimation for noisy rewards as shown both theoretically and empirically ( Zhong et al. , 2022 ) . By including LNSS , our TDR is more robust under noisy and sparse rewards . The TD Actor element also helped make appreciable improvements on learning performance as shown in Table 2 . More importantly , TD Actor plays an importantly role in TDR since it not only stabilizes the policy updates as shown theoretically in Theorem 2 but also addresses the estimation error in critic as shown theoretically in Theorem 3 . 5.3 HYPER PARAMETER STUDY Hyperparameter study results are summarized in Figure 3 where two DRL methods ( D4PG and TD3 ) with TD Actor are evaluated for different regularization factor ρ ( ρ = 0 , 0.1 , 0.3 , 0.5 , 0.7 , 0.9 ) . What is reported is the 10-seed averaged performance , i.e. , the average of the approximate es- timation error which is the difference between the true accumulated reward and the critic value : Ψ = 1 10 t=0 γtrt − Q ( s0 , a0 ) ) . eval=0 ( ( cid:80 ) 999 ( cid:80 ) 9 Q5 TD regularized Actor helps reduce the estimation error in critic . From Figure 3 , with TD regularized Actor ( TD Actor ) , the estimation errors in the critic are re- duced from those without . For example , in Finger Turn hard , D4PG + TD Actor results in less overestimation error compared with ρ = 0 at the later stage of training . TD3 + TD Actor has less underestimation error compared with ρ = 0 . Similarly in cartpole swingup sparse , D4PG + TD Actor results in less overestimation error compared with ρ = 0 . A policy can be evaluated by the “ epois reward ” where a higher epois reward generally results from a better policy . From Figure 3 , policy updates are improved by selecting a suitable regularization fac- tor ρ . Especially , in cartpole swingup sparse , TD3 + TD Actor enables successful learning whereas the Base method struggled and stuck to 0 or no learning for the entire training period . Q6 A range of ρ ( ρ = 0.3 , 0.5 , 0.7 ) generally are good choices . From Figure 3 , a small regular- ization factor ρ = 0.1 in TDR will result in less regularization which may not provide sufficient estimation error reduction in the critic . A larger regularization factor ρ = 0.9 in TDR will result in more regularization and may have a negative effect on learning . Therefore , ρ = 0.3 , 0.5 , 0.7 may be good choices . Therefore in this work , we have consistently used ρ = 0.7 in obtaining all results . 6 CONCLUSION , DISCUSSION , AND LIMITATION OF THE STUDY 1 ) In this work , we introduce a novel TDR mechanism that includes TD-regularized double critic networks and TD-regularized actor network . Both components are shown to help mitigate both over and under estimation errors . TDR has been shown to consistently outperform respective Base algorithms in solving benchmark tasks in terms of average reward , learning success rate , learning speed , and most times , learning variance . 2 ) Our analytical results also show that each component of TDR helps mitigate both over and under estimation errors . 3 ) As shown in Figure 2 , for five out of the six environments ( except quadruped walk ) evaluated , our TDR combined with distributional and LNSS elements has significantly elevated the current SOTA performance of D4PG to a new level with an increase of at least 60 % . Even though we have identified a range of generally good regularization coefficient ρ values ( 0.3 , 0.5 , 0.7 ) , as Figure 3 shows , different algorithms in different environments have responded somewhat differently to ρ . Therefore , how to effectively determine a regularization factor to have the most improvement remains a question , and thus , it is the limitation of this study . Additionally , the promising performances of TDR come after extensive training with millions of learning steps . How TDR performs under limited training time and training steps need to be further investigated . 9 REFERENCES Marcin Andrychowicz , Filip Wolski , Alex Ray , Jonas Schneider , Rachel Fong , Peter Welinder , Bob McGrew , Josh Tobin , Pieter Abbeel , and Wojciech Zaremba . Hindsight experience replay . arXiv preprint arXiv:1707.01495 , 2017 . Gabriel Barth-Maron , Matthew W Hoffman , David Budden , Will Dabney , Dan Horgan , Dhruva Tb , Alistair Muldal , Nicolas Heess , and Timothy Lillicrap . Distributed distributional deterministic policy gradients . arXiv preprint arXiv:1804.08617 , 2018 . Marc G Bellemare , Will Dabney , and R´emi Munos . A distributional perspective on reinforcement learning . In International conference on machine learning , pp . 449–458 . PMLR , 2017 . Shalabh Bhatnagar , Mohammad Ghavamzadeh , Mark Lee , and Richard S Sutton . Incremental nat- ural actor-critic algorithms . Advances in neural information processing systems , 20 , 2007 . Renzhi Chen , Ke Li , and Xin Yao . Dynamic multiobjectives optimization with a changing number of objectives . IEEE Transactions on Evolutionary Computation , 22 ( 1 ) :157–171 , 2017 . Robert Crites and Andrew Barto . An actor/critic algorithm that is equivalent to q-learning . Advances in Neural Information Processing Systems , 7 , 1994 . Will Dabney , Georg Ostrovski , David Silver , and R´emi Munos . Implicit quantile networks for distributional reinforcement learning . In International conference on machine learning , pp . 1096– 1105 . PMLR , 2018a . Will Dabney , Mark Rowland , Marc Bellemare , and R´emi Munos . Distributional reinforcement learning with quantile regression . In Proceedings of the AAAI Conference on Artificial Intelli- gence , volume 32 , 2018b . Jingliang Duan , Yang Guan , Shengbo Eben Li , Yangang Ren , Qi Sun , and Bo Cheng . Distributional soft actor-critic : Off-policy reinforcement learning for addressing value estimation errors . IEEE transactions on neural networks and learning systems , 33 ( 11 ) :6584–6598 , 2021 . Roy Fox , Ari Pakman , and Naftali Tishby . Taming the noise in reinforcement learning via soft updates . arXiv preprint arXiv:1512.08562 , 2015 . Scott Fujimoto , Herke Hoof , and David Meger . Addressing function approximation error in actor- critic methods . In International Conference on Machine Learning , pp . 1587–1596 . PMLR , 2018 . Tuomas Haarnoja , Aurick Zhou , Kristian Hartikainen , George Tucker , Sehoon Ha , Jie Tan , Vikash Kumar , Henry Zhu , Abhishek Gupta , Pieter Abbeel , et al . Soft actor-critic algorithms and appli- cations . arXiv preprint arXiv:1812.05905 , 2018 . G Hailu and G Sommer . On amount and quality of bias in reinforcement learning . In IEEE SMC ’ 99 Conference Proceedings . 1999 IEEE International Conference on Systems , Man , and Cybernetics ( Cat . No . 99CH37028 ) , volume 2 , pp . 728–733 . IEEE , 1999 . Hado Hasselt . Double q-learning . Advances in neural information processing systems , 23 , 2010 . Peter Henderson , Riashat Islam , Philip Bachman , Joelle Pineau , Doina Precup , and David Meger . Deep reinforcement learning that matters . In Proceedings of the AAAI conference on artificial intelligence , volume 32 , 2018 . Qingfeng Lan , Yangchen Pan , Alona Fyshe , and Martha White . Maxmin q-learning : Controlling the estimation bias of q-learning . arXiv preprint arXiv:2002.06487 , 2020 . Felix Leibfried and Jordi Grau-Moya . Mutual-information regularization in markov decision pro- cesses and actor-critic learning . In Conference on Robot Learning , pp . 360–373 . PMLR , 2020 . Wentao Liu , Junmin Zhong , Ruofan Wu , Bretta L Fylstra , Jennie Si , and He Helen Huang . Inferring human-robot performance objectives during locomotion using inverse reinforcement learning and inverse optimal control . IEEE Robotics and Automation Letters , 7 ( 2 ) :2549–2556 , 2022 . 10 Jiafei Lyu , Xiaoteng Ma , Jiangpeng Yan , and Xiu Li . Efficient continuous control with double actors and regularized critics . In Proceedings of the AAAI Conference on Artificial Intelligence , volume 36 , pp . 7655–7663 , 2022 . Fabio Pardo . Tonic : A deep reinforcement learning library for fast prototyping and benchmarking . arXiv preprint arXiv:2011.07537 , 2020 . Simone Parisi , Voot Tangkaratt , Jan Peters , and Mohammad Emtiyaz Khan . Td-regularized actor- critic methods . Machine Learning , 108:1467–1501 , 2019 . Tim GJ Rudner , Cong Lu , Michael A Osborne , Yarin Gal , and Yee Teh . On pathologies in kl- regularized reinforcement learning from expert demonstrations . Advances in Neural Information Processing Systems , 34:28376–28389 , 2021 . David Silver , Guy Lever , Nicolas Heess , Thomas Degris , Daan Wierstra , and Martin Riedmiller . Deterministic policy gradient algorithms . In International conference on machine learning , pp . 387–395 . Pmlr , 2014 . Richard S Sutton and Andrew G Barto . Reinforcement learning : An introduction . MIT press , 2018 . Yuval Tassa , Yotam Doron , Alistair Muldal , Tom Erez , Yazhe Li , Diego de Las Casas , David Bud- den , Abbas Abdolmaleki , Josh Merel , Andrew Lefrancq , et al . Deepmind control suite . arXiv preprint arXiv:1801.00690 , 2018 . Sebastian Thrun and Anton Schwartz . Issues in using function approximation for reinforcement learning . In Proceedings of the Fourth Connectionist Models Summer School , volume 255 , pp . 263 . Hillsdale , NJ , 1993 . Hado Van Hasselt , Arthur Guez , and David Silver . Deep reinforcement learning with double q- learning . In Proceedings of the AAAI conference on artificial intelligence , volume 30 , 2016 . Nino Vieillard , Tadashi Kozuno , Bruno Scherrer , Olivier Pietquin , R´emi Munos , and Matthieu Geist . Leverage the average : an analysis of kl regularization in reinforcement learning . Advances in Neural Information Processing Systems , 33:12163–12174 , 2020 . Wentao Weng , Harsh Gupta , Niao He , Lei Ying , and R Srikant . The mean-squared error of double q-learning . Advances in Neural Information Processing Systems , 33:6815–6826 , 2020 . Ruofan Wu , Junmin Zhong , Brent Wallace , Xiang Gao , He Huang , and Jennie Si . Human-robotic prosthesis as collaborating agents for symmetrical walking . Advances in Neural Information Processing Systems , 35:27306–27320 , 2022 . Ruofan Wu , Junmin Zhong , and Jennie Si . Phased actor in actor-critic reinforcement learning . 2023 . Zongzhang Zhang , Zhiyuan Pan , and Mykel J Kochenderfer . Weighted double q-learning . In IJCAI , pp . 3455–3461 , 2017 . Junmin Zhong , Ruofan Wu , and Jennie Si . Long n-step surrogate stage reward to reduce variances of deep reinforcement learning in complex problems . arXiv preprint arXiv:2210.04820 , 2022 . A DISTRIBUTIONAL TDR AND LNSS The distributional RL ( Bellemare et al. , 2017 ) represents value function in terms of probability distribution rather than function estimates . This distribution provides a more comprehensive rep- resentation of the uncertainty associated with a range of different possible reward returns and state action pairs which can provide more informative value function estimation . Many distributional RL algorithms ( Bellemare et al. , 2017 ; Dabney et al. , 2018b ; a ) has been achieved great performance im- provements on many discrete problems such as Atari benchmarks . D4PG ( Barth-Maron et al. , 2018 ) applied distributional RL into continuous control problem by combining the distributional return function within an actor-critic framework . DSAC ( Duan et al. , 2021 ) address overestimation error by applying distributional RL piggyback on SAC . Although , D4PG and DSAC can provide more accurate critic , the overestimation of actor still exists since the actor is still updated by maximizing the expectation of value function distribution . How to regulate actors in distributional RL in solving overestimations was barely discussed before . 11 A.1 DISTRIBUTIONAL TD-REGULARIZED ACTOR-CRITIC ( DTDR ) Here we tailor a distributional TDR ( dTDR ) method based on the original distributional conceptu- alization developed in D4PG ( Barth-Maron et al. , 2018 ; Bellemare et al. , 2017 ) . We show a number of enhancements in the meantime . Distributional Critic . The distributional critic ( Bellemare et al. , 2017 ) treated the return in Equa- tion 1 as a random variable Z ( sk , ak ) whose expectation is used as the Q value estimate , namely , Q ( sk , ak ) = E [ Z ( sk , ak ) ] . In dTDR however , we use TD errors to evaluate distributional critics . Similar to Equation 5 and 6 , distributional TD errors of the two target networks can be written as : ( sk+1 , πϕ′ ( sk+1 ) ) ] − E [ Zθ′ 1 = rk + γE [ Zθ′ d′ ( sk , ak ) ] , ( 15 ) 1 1 2 = rk + γE [ Zθ′ d′ 2 ( sk+1 , πϕ′ ( sk+1 ) ) ] − E [ Zθ′ 2 ( sk , ak ) ] . The twin TD-regularized target distributional Bellman operator is thus defined as : T Zk D= ( cid:26 ) rk + γZθ′ rk + γZθ′ 1 2 ( sk+1 , πϕ′ ( sk+1 ) ) ( sk+1 , πϕ′ ( sk+1 ) ) if |d′ if |d′ 1| ≤ |d′ 2| 1| > |d′ 2| ( 16 ) ( 17 ) where A D= B denotes that two random variables A and B follow the same probability laws . Al- though the distributional Bellman operator appears similar to Equation 1 , it maps state-action pairs to distributions . As such , we need to define a new TD error measure for the distribution as in D4PG ( Barth-Maron et al. , 2018 ) . We consider using the following distributional loss , L ( θ ) = Es∼pπ , a∼π [ ( cid:88 ) ζ=1,2 l ( T Zk , Zθζ ( sk , ak ) ) ] , ( 18 ) where l measures the distance between two distributions . Many distributional RL algorithms use Kullback-Leibler ( KL ) divergence as the distance metric ( Duan et al. , 2021 ; Barth-Maron et al. , 2018 . We adopt the same metric . Distributional Actor . In most distributional methods ( Barth-Maron et al. , 2018 ; Bellemare et al. , 2017 ) , policy updates are performed based on the policy gradient below , ∇ϕJ ( ϕ ) = Es∼pπϕ [ E [ ∇aZθ ( sk , ak ) ] |a=πϕ ( s ) ∇ϕπϕ ( s ) ] . ( 19 ) In our dTDR , we need to use critic evaluation metrics to evaluate the quality of the current distribu- tional critic and the regularized distributional actor . We first formulate the following loss metric : Lz ( ϕ ) = E [ l ( rk + γZθi+1 1 ( sk+1 , πϕ ( sk+1 ) ) , Zθi+1 1 ( sk , πϕ ( sk ) ) ] . ( 20 ) Similar to TD-regularized actor network , the distributional actor is updated in the direction of max- imizing the expected critic while keeping the expected distance between the projected critic and the critic , namely , ∇ϕJ ( ϕ ) = Es∼pπϕ [ ( E [ ∇aZθi+1 1 ( sk , ak ) ] − ∇aρLz ( ϕ ) ) |a=πϕ ( s ) ∇ϕπϕ ( s ) ] , ( 21 ) where ρ ∈ ( 0 , 1 ) is a regularization coefficient . A.2 LONG N-STEP SURROGATE STAGE ( LNSS ) REWARD LNSS ( Zhong et al. , 2022 ) utilizes a long reward trajectory of N future steps in the estimation of stage reward rk . Using the LNSS-resulted reward r′ k in place of the original rk was shown to effectively reduce learning variance with significant performance improvements for off-policy methods . Given a reward trajectory of N steps from time step k , let G ( sk : k+N −1 , ak : k+N −1 ) ∈ R ( with shorthand notation Gk ) denote the discounted N -step return , i.e. , k+N −1 ( cid:88 ) γt−krt , Gk = t=k 12 ( 22 ) where rt is the tth stage reward and t is from k to k + N − 1 . In LNSS , r′ reward in place of rk in Equation ( 2 ) . To determine r′ N -step reward sequence , namely k is a surrogate stage k , LNSS treat it as a weighted average of the r′ k = γt−krt ( cid:80 ) k+N −1 t=k ( cid:80 ) N −1 n=0 γn . ( 23 ) As Figure 1 shows , Once r′ ( sk , ak , r′ as discussed . k is obtained , it is simply used in place of rk to form a new tuple k , sk+1 ) , which is then stored into the memory buffer D. The TDR method proceeds B ESTIMATION ANALYSIS Lemma 1 . Let Qπ be the true Q value following the current target policy π , and Qθ′ be the target network estimates using double Q neural networks . We assume that there exists a step random estimation bias ψk ( i.e. , estimation bias at the kth stage ) , and that it is independent of ( sk , ak ) with θ′ ζ mean E [ ψk ζ , µ′ ] = µ′ 2 respectively defined in θ′ ζ Equations ( 5 ) and ( 6 ) , we have , ζ < ∞ , for all k , and ζ = 1 , 2 . Then for δ′ 1 and δ′ and Qθ′ 2 1 E [ δ′ E [ δ′ 1 ] = −µ′ 1 , 2 ] = −µ′ 2 . ( 24 ) ( 25 ) ( 26 ) Proof . With the step random estimation bias ψk θ′ ζ , We can rewrite the expectation of Ψk θ′ ζ as E [ Ψk+1 θ′ ζ ] = ∞ ( cid:88 ) t=k+1 γt−k−1E [ ψt θ′ ζ ] = 1 1 − γ µ′ ζ . Then the expectation of the target can be written as , E [ yk ] = E [ rk ] + γE [ ( Qπ ( sk+1 , ak+1 ) + Ψk+1 ∞ ( cid:88 ) θ′ ζ ) ] = E [ rk ] + γ ( E [ γt−k−1rt ] ) + γ 1 − γ µ′ ζ = Qπ ( sk , ak ) + t=k+1 γ 1 − γ µ′ ζ . By using Equations ( 10 ) , and ( 26 ) , the TD errors of the two target critics ( Equations 5 and 6 ) , respectably are : E [ δ′ 1 ] = E [ rk ] + γE [ Qθ′ 1 ( sk+1 , πϕ′ ( sk+1 ) ) ] − E [ Qθ′ 1 1 γ 1 − γ 1 − γ 1 − Qπ ( sk , ak ) − µ′ ( sk , ak ) ] µ′ 1 ( 27 ) = Qπ ( sk , ak ) + Similarly , E [ δ′ = −µ′ 1 . 2 ] = −µ′ 2 . Thus Lemma 1 holds . With Lemma 1 in place , we are now ready to analyze the estimation errors by using TDR and the double Q ( DQ ) method as in TD3 ( Fujimoto et al. , 2018 ) and SAC ( Haarnoja et al. , 2018 ) . Theorem 1 . Let assumptions in Lemma 1 hold , and let δYk denote the target value estimation error . Accordingly , we denote this error for TDR as δY T DR . We then have the following , , and DQ as δY DQ k k |E [ δY T DR k ] | ≤ |E [ δY DQ k ] | . ( 28 ) Proof . The proof is based on enumerating a total of 8 possible scenarios of estimation errors which are determined from the relationships among the two target Q values and the true Qπ value . We provide proofs for the 4 out of 8 unique scenarios below . 13 First note that , E [ δY T DR Case 1 : If the target critic values and the true value Qπ have the following relationship : ] = E [ Qπ − yT DR ] = E [ Qπ − yDQ ] , and E [ δY DQ ] . k k k k i.e , Qθ′ 1 is more underestimated as that implies E [ Qθ′ 1 ] < E [ Qθ′ 2 ] < Qπ , |E [ Ψk θ′ 1 ] | > |E [ Ψk θ′ 2 ] | , |µ′ 1| > |µ′ 2| . Based on Lemma 1 and Equation ( 7 ) , our TDR will use Qθ′ 2 in the target value , E [ yT DR k ] = E [ rk ] + γE [ Qθ′ 2 ( sk+1 , πϕ′ ( sk+1 ) ) ] . However for a vanilla double Q network , the target value will be Qθ′ 1 , ] = E [ rk ] + γE [ Qθ′ Thus based on Equation ( 26 ) , the two estimation errors of the respective target values are ( sk+1 , πϕ′ ( sk+1 ) ) ] . E [ yDQ k 1 |E [ δY T DR k ] | = |E [ Qπ − yT DR k ] | = | |E [ δY DQ k ] | = |E [ Qπ − yDQ k ] | = | µ′ 2| , γ 1 − γ γ µ′ 1 − γ 1| . Since |µ′ 1| > |µ′ 2| , we have |E [ δY T DR k ] | < |E [ δY DQ k ] | . Thus identity ( 28 ) holds . Case 2 : If the target critic values and the true value Qπ have the following relationship : E [ Qθ′ |E [ Qπ − Qθ′ 1 ] < Qπ < E [ Qθ′ 2 ] , ] | > |E [ Qπ − Qθ′ 1 ] | , 2 is expected to be underestimated and Qθ′ is overestimated . Since |E [ Qπ − Qθ′ 1 ] | > 2 then Qθ′ |E [ Qπ − Qθ′ 1 2 we thus have ] | which implies |E [ Ψk θ′ 1 ] | > |E [ Ψk θ′ 2 ] | , |µ′ 1| > |µ′ 2| . Based on Lemma 1 and Equation ( 7 ) , our TDR will use Qθ′ 2 in the target value : E [ yT DR k ] = E [ rk ] + γE [ Qθ′ 2 ( sk+1 , πϕ′ ( sk+1 ) ) ] . However for a vanilla double Q network , the target value will use Qθ′ 1 , ] = E [ rk ] + γE [ Qθ′ Based on Equation ( 26 ) , the two estimation errors of the respective target values are : ( sk+1 , πϕ′ ( sk+1 ) ) ] . E [ yDQ k 1 |E [ δY T DR k ] | = |E [ Qπ − yT DR k ] | = | |E [ δY DQ k ] | = |E [ Qπ − yDQ k ] | = | µ′ 2| , γ 1 − γ γ µ′ 1 − γ 1| , Since |µ′ 1| > |µ′ 2| , we have |E [ δY T DR k ] | < |E [ δY DQ k ] | . Thus identity ( 28 ) holds . Case 3 : If the target critic values and the true value Qπ has the following relationship : ] < Qπ < E [ Qθ′ ] , 2 E [ Qθ′ |E [ Qπ − Qθ′ 1 1 ] | < |E [ Qπ − Qθ′ 2 ] | , 14 ( 29 ) ( 30 ) ( 31 ) ( 32 ) ( 33 ) ( 34 ) ( 35 ) ( 36 ) ( 37 ) ( 38 ) ( 39 ) ( 40 ) ( 41 ) ( 42 ) ( 43 ) is expected to be underestimated and Qθ′ is overestimated . Since |E [ Qπ − Qθ′ 1 ] | < 2 then Qθ′ |E [ Qπ − Qθ′ 1 2 ] | , it implies thus we have |E [ Ψk θ′ 1 ] | < |E [ Ψk θ′ 2 ] | , ( 44 ) 1| < |µ′ Based on Lemma 1 and Equation ( 7 ) , both vanilla double Q network and our TDR will pick Qθ′ the target value : |µ′ 2| . 1 ( 45 ) in E [ yT DR k ] = E [ yDQ k ] = E [ rk ] + γE [ Qθ′ 1 ( sk+1 , πϕ′ ( sk+1 ) ) ] . Then based on Equation ( 26 ) , the two estimation errors of the respective target values are : |E [ δY T DR k ] | = |E [ δY DQ k ] | = |E [ Qπ − yT DR k ] | = | γ 1 − γ µ′ 1| . We thus have |E [ δY T DR k ] | = |E [ δY DQ k ] | . Thus identity ( 28 ) holds . Case 4 : If the target critic values and the true value Qπ has the following relationship where E [ Qθ′ 2 ] is expected more overestimated ie |E [ Ψk θ′ 1 ] | < |E [ Ψk θ′ 2 ] | that implies Qπ < E [ Qθ′ 1 ] < E [ Qθ′ 2 ] , |µ′ 1| < |µ′ 2| . ( 46 ) ( 47 ) ( 48 ) ( 49 ) ( 50 ) Based on Equation ( 24 ) and ( 7 ) , same with vanilla double Q network , our Twin TD-regularized Critic will pick the target value using Qθ′ which both mitigates the larger overestimation bias as : 1 E [ yT DR k ] = E [ yDQ k ] = E [ rk ] + γE [ Qθ′ 1 ( sk+1 , πϕ′ ( sk+1 ) ) ] , which based on Equation ( 26 ) , the two estimation errors of the target value are |E [ δY T DR k ] | = |E [ δY DQ k ] | = |E [ Qπ − yT DR k ] | = | γ 1 − γ µ′ 1| . We have |E [ δY T DR k ] | = |E [ δY DQ k ] | . ( 51 ) ( 52 ) ( 53 ) 1 1 2 k ] > E [ Qθ′ ] < E [ Qθ′ ] | ≤ |E [ δY DQ Thus identity ( 28 ) holds . Both methods can mitigate the overestimation error . Note , the above cases study the relationship of E [ Qθ′ for E [ Qθ′ ] , |E [ δY T DR Theorem 2 . Let Qπ denote the true Q value following the current target policy π , Qθ1 be the estimated value . We assume that there exists a step random estimation bias ψk that is independent θ1 of ( sk , ak ) with mean E [ ψk ] = µ1 , µ1 < ∞ , for all k. We assume the policy is updated based on θ1 critic Qθ1 using the deterministic policy gradient ( DPG ) as in Equation 4 . Let δϕk denote the change in actor parameter ϕ updates at stage k. Accordingly , we denote this change for TDR as δϕT DR , k vanilla DPG as δϕDP G . We then k have the following , , and true change without any approximation error in Q as δϕtrue ] | still valid . Thus Theorem 1 holds . ] and by applying same procedure k k 2 ( cid:26 ) E [ δϕtrue k E [ δϕtrue k ] ≥ E [ δϕT DR ] ≤ E [ δϕT DR k k ] ≥ E [ δϕDP G ] ≤ E [ δϕDP G k k ] ] if E [ Ψk θ1 if E [ Ψk θ1 ] < 0 , ] ≥ 0 . ( 54 ) Proof . With learning rate α , the true change of the actor parameters in case without any approxima- tion error in Q : E [ δϕtrue k ] = αEs∼pπ ( cid:104 ) ϕj ∇aQπ ( sk , ak ) |a=πϕj ( s ) ∇ϕj πϕj ( s ) ( cid:105 ) . ( 55 ) 15 Consider the estimated critic and the true value follow the relationship in Equation 10 . Given the same current policy parameters ϕj , the updated parameters using DPG are : ϕj+1 DP G = ϕj + αEs∼pπ ( cid:104 ) E [ δϕDP G k ] = αEs∼pπ ϕj ( cid:104 ) ϕj ∇a ( Qπ ( sk , ak ) + Ψk θ1 ( cid:105 ) ) ( cid:12 ) ( cid:12 ) a=πϕj ( s ) ∇ϕj πϕj ( s ) , ∇a ( Qπ ( sk , ak ) + Ψk θ1 ) ( cid:12 ) ( cid:12 ) a=πϕj ( s ) ∇ϕj πϕj ( s ) ( cid:105 ) . ( 56 ) With an overestimation bias E [ Ψk θ1 mated actions , and with an underestimation bias E [ Ψk θ1 the underestimated actions . Both result in suboptimal policies . However , by using TD-regularized actor , and given the same current policy parameters ϕj , the actor updates with Equation ( 9 ) are : ] > 0 , the updates encourage more exploration for the overesti- ] < 0 , the updates discourage exploration for ϕj+1 T DR = ϕj + αEs∼pπ ] = αEs∼pπ E [ δϕT DR k ϕj [ ∇a ( Qπ ( sk , ak ) + Ψk θ1 ϕj − ρ ( ∆ ) ) |a=πϕj ( s ) ∇ϕj πϕj ( s ) ] , [ ∇a ( Qπ ( sk , ak ) + Ψk θ1 − ρ ( ∆ ) ) |a=πϕj ( s ) ∇ϕj πϕj ( s ) ] . Similar to Lemma 1 , E [ Ψk θ1 ] = 1 1−γ µ1 , and from Equations ( 8 ) and ( 9 ) we have : E [ ∆ ] = E [ Qθi+1 = µ1 1 ( sk , ak ) ] − E [ ( rk + γQθi+1 1 ( sk+1 , πϕ ( sk+1 ) ) ) ] by selecting ρ ≤ 1 1−γ , we have the following : ( cid:26 ) 0 ≥ E [ Ψk θ1 0 ≤ E [ Ψk θ1 − ρ∆ ] > E [ Ψk θ1 − ρ∆ ] ≤ E [ Ψk θ1 ] ] if E [ Ψk θ1 if E [ Ψk θ1 ] < 0 , ] ≥ 0 . Therefore by inspecting Equations ( 55 ) , ( 56 ) and ( 57 ) , we have : ( cid:26 ) E [ δϕtrue k E [ δϕtrue k Thus Theorem 2 holds . ] ≥ E [ δϕT DR ] ≤ E [ δϕT DR k k ] ≥ E [ δϕDP G ] ≤ E [ δϕDP G k k ] ] if E [ Ψk θ1 if E [ Ψk θ1 ] < 0 , ] ≥ 0 . ( 57 ) ( 58 ) ( 59 ) ( 60 ) Theorem 3 . Suboptimal actor updates negatively affect the critic . Specifically , consider actor up- dates as in Theorem 2 , in the overestimation case , we have : E [ Qθ1 ( sk , πDP G ( sk ) ] ≥ E [ Qθ1 ( sk , πT DR ( sk ) ) ] ≥ E [ Qπ ( sk , πT rue ( sk ) ) ] , ( 61 ) and in the underestimation case , E [ Qθ1 ( sk , πDP G ( sk ) ] ≤ E [ Qθ1 ( sk , πT DR ( sk ) ) ] ≤ E [ Qπ ( sk , πT rue ( sk ) ) ] . ( 62 ) Proof Following the analysis of the TD3 ( Fujimoto et al. , 2018 ) , consider Equation ( 12 ) in Theorem 2 , we have ( cid:26 ) E [ δϕtrue k E [ δϕtrue k ] ≥ E [ δϕT DR ] ≤ E [ δϕT DR k k ] ≥ E [ δϕDP G ] ≤ E [ δϕDP G k k ] ] if E [ Ψk θ1 if E [ Ψk θ1 ] < 0 Underestimate , ] ≥ 0 Overestimate . ( 63 ) In the overestimation case , the approximate value using TDR and vanilla DPG must be E [ Qθ1 ( sk , πDP G ( sk ) ] ≥ E [ Qθ1 ( sk , πT DR ( sk ) ) ] ≥ E [ Qπ ( sk , πtrue ( sk ) ) ] . ( 64 ) Similarly , in the underestimation case , the approximate value using TDR and vanilla DPG must be E [ Qθ1 ( sk , πDP G ( sk ) ] ≤ E [ Qθ1 ( sk , πT DR ( sk ) ) ] ≤ E [ Qπ ( sk , πT rue ( sk ) ) ] . ( 65 ) Thus Theorem 3 holds . 16 C IMPLEMENTATION DETAILS We use PyTorch for all implementations . All results were obtained using our internal server consist- ing of AMD Ryzen Threadripper 3970X Processor , a desktop with Intel Core i7-9700K processor , and two desktops with Intel Core i9-12900K processor . Training Procedure . An episode is initialized by resetting the environment , and terminated at max step T = 1000 . A trial is a complete training process that contains a series of consecutive episodes . Each trial is run for a maximum of 1 × 106 time steps with evaluations at every 2 × 104 time steps . Each task is reported over 10 trials where the environment and the network were initialized by 10 random seeds , ( 0 − 9 ) in this study . For each training trial , to remove the dependency on the initial parameters of a policy , we use a purely exploratory policy for the first 8000 time steps ( start timesteps ) . Afterwards , we use an off-policy exploration strategy , adding Gaussian noise N ( 0 , 0.1 ) to each action . Evaluation Procedure . Every 1 × 104 time steps training , we have an evaluation section and each evaluation reports the average reward over 5 evaluation episodes , with no exploration noise and with fixed policy weights . The random seeds for evaluation are different from those in training which each trial , evaluations were performed using seeds ( seeds + 100 ) . Network Structure and optimizer . TD3.The actor-critic networks in TD3 are implemented by feedforward neural networks with three layers of weights . Each layer has 256 hidden nodes with rectified linear units ( ReLU ) for both the actor and critic . The input layer of actor has the same dimension as observation state . The output layer of the actor has the same dimension as action requirement with a tanh unit . Critic receives both state and action as input to THE first layer and the output layer of critic has 1 linear unit to produce Q value . Network parameters are updated using Adam optimizer with a learning rate of 10−3 for simple control problems . After each time step k , the networks are trained with a mini-batch of a 256 transitions ( s , a , r , s′ ) , ( s , a , r′ , s′ ) in case of LNSS , sampled uniformly from a replay buffer D containing the entire history of the agent . D4PG . Same with the actor-critic networks in D4PG are implemented by feedforward neural net- works with three layers of weights . Each layer has 256 hidden nodes with rectified linear units ( ReLU ) for both the actor and critic . The input layer of actor has the same dimension as observa- tion state . The output layer of the actor has the same dimension as action requirement with a tanh unit . Critic receives both state and action as input to THE first layer and the output layer of critic has a distribution with hyperparameters for the number of atoms l , and the bounds on the support ( Vmin , Vmax ) . Network parameters are updated using Adam optimizer with a learning rate of 10−3 . After each time step k , the networks are trained with a mini-batch of 256 transitions ( s , a , r , s′ ) , ( s , a , r′ , s′ ) in case of LNSS , sampled uniformly from a replay buffer D containing the entire his- tory of the agent . SAC . The actor-critic networks in SAC are implemented by feedforward neural networks with three layers of weights . Each layer has 256 hidden nodes with rectified linear units ( ReLU ) for both the actor and critic . The input layer of actor has the same dimension as observation state . The output layer of the actor has the same dimension as action requirement with a tanh unit . Critic receives both state and action as input to the first layer and the output layer of critic has 1 linear unit to produce Q value . Network parameters are updated using Adam optimizer with a learning rate of 10−3 for simple control problems . After each time step k , the networks are trained with a mini-batch of a 256 transitions ( s , a , r , s′ ) , ( s , a , r′ , s′ ) in case of LNSS , sampled uniformly from a replay buffer D containing the entire history of the agent . Hyperparameters . To keep comparisons in this work fair , we set all common hyperparameters ( network layers , batch size , learning rate , discount factor , number of agents , etc ) to be the same for comparison within the same methods and different methods . For TD3 , target policy smoothing is implemented by adding ϵ ∼ N ( 0 , 0.2 ) to the actions chosen by the target actor-network , clipped to ( −0.5 , 0.5 ) , delayed policy updates consist of only updating 17 the actor and target critic network every d iterations , with d = 2 . While a larger d would result in a larger benefit with respect to accumulating errors , for fair comparison , the critics are only trained once per time step , and training the actor for too few iterations would cripple learning . Both target networks are updated with τ = 0.005 . The TD3 and TD3+TDR used in this study are based on the paper ( Fujimoto et al. , 2018 ) and the code from the authors ( https : //github.com/sfujim/TD3 ) . Hyperparameter TD3 Start timesteps Evaluation frequency Max timesteps Exploration noise Policy noise Noise clip Policy update frequency Batch size Buffer size γ τ Number of parallel actor LNSS-N Adam Learning rate regularization factor Value 8000 steps 20000 steps 1e6 steps N ( 0 , 0.1 ) N ( 0 , 0.2 ) ±0.5 2 256 1e6 0.99 0.005 1 100 1e-3 0.7 Table 3 : TD3 + TDR hyper parameters used for DMC benckmark tasks The SAC used in this study is based on paper ( Haarnoja et al. , 2018 ) and the code is from GitHub ( https : ) . and the hyperparameter is from Table 4 . Value 8000 steps 20000 steps 1e6 steps N ( 0 , 0.1 ) N ( 0 , 0.2 ) ±0.5 2 256 1e6 0.99 0.005 Hyperparameter SAC Start timesteps Evaluation frequency Max timesteps Exploration noise Policy noise Noise clip Policy update frequency Batch size Buffer size γ τ Temperature parameter α 0.2 Number of parallel actor LNSS-N Adam Learning rate 1 100 1e-3 Table 4 : SAC hyper parameters used for the DMC benckmark tasks The D4PG used in this study is based on paper ( Barth-Maron et al. , 2018 ) and the code is modified from TD3 . The hyperparameter is from Table 5 . All Other algorithms are from the same DRL training platform ( Tonic RL ) ( Pardo , 2020 ) with the same evaluation as the above algorithms . Sparse Reward Setup . 1 ) Cheetah Run Sparse : Cheetah needs to run forward as fast as possible . The agent gets a reward only after speed exceeds 2.5 m/s , making the reward sparse . r = 1 . That is , if v > = 2.5 else r = 0 . 18 Hyperparameter D4PG Start timesteps Evaluation frequency Max timesteps Exploration noise Noise clip Batch size Buffer size γ τ Number of parallel actor LNSS-N Adam Learning rate Vmax Vmin l regularization factor Value 8000 steps 20000 steps 1e6 steps N ( 0 , 0.1 ) ±0.5 256 1e6 0.99 0.005 1 100 1e-3 100 0 51 0.7 Table 5 : D4PG + TDR hyper parameters used for the DMC benckmark tasks 19 D TDR ALGORITHMS DETAILS In this section , we show our TDR-based algorithms . TD3-TDR is shown in Algorithm 1 , SAC- TDR is shown in Algorithm 2 , and D4PG-TDR is shown in Algorithm 3 . We mainly add LNSS reward to the sample collection part . In algorithm update part , we mainly modify the target value selection using Equation 7 for regular DRL and Equation ( 17 ) for distributional DRL . Additionally , if applicable , we modify the actor gradient based on Equation 9 for regular DRL and Equation ( 21 ) for distributional DRL . All codes will be released to GitHub once the paper get accepted . Algorithm 1 TD3-TDR Initialize : 1 ← θ1 , θ′ 2 ← θ2 , ϕ′ ← ϕ , • Critic networks Qθ1 , Qθ2 and actor-network πϕ with random parameters , θ1 , θ2 , ϕ • Target networks θ′ • an experience buffer D • a temporary experience buffer D′ with size N • Total training episode T 1 . For episode = 1 , T do 2 . Reset initialize state s0 , D′ For k = 0 , T do 3 . 4 . 5 . 6 . 7 . 8 . 9 . 10 . 11 . 12 . 13 . 14 . 15 . 16 . 17 . 18 . 19 . 20 . 21 . 22 . 23 . 24 . 25 . Choose an action ak based on current state sk and learned policy from A . Execute the action ak and observe a new state sk+1 with reward signal rk Store the transition ( sk , ak , rk , sk+1 ) in D′ if k + N − 1 ≤ T then 1 ) in the D′ 0 , s′ 0 , r′ 0 , a′ Get earliest memory ( s′ Calculate r′ based on Equation ( 23 ) 0 , a′ Store the transition ( s′ Clear original transition ( s′ 0 , r′ , s′ 0 , a′ 1 ) in D 0 , s′ 0 , r′ 1 ) in the D′ else Repeat step 8 to 11 and Calculate r′ based on Equation r′ k = γ − 1 γT −k+1 − 1 T ( cid:88 ) t=k γt−krt . ( 66 ) t , st+1 ) from D end if Sample mini-batch data ( st , at , r′ Get next action at+1 ← πϕ′ ( st+1 ) Target value yt based on Equation ( 7 ) Update Critics based on Equation 3 if k mod Policy Update frequency then Update ϕ by Equation 9 Update target networks : θ′ ζ ← τ θζ + ( 1 − τ ) θ′ ζ ϕ′ ← τ ϕ + ( 1 − τ ) ϕ′ end if end for 26 . 27. end for 20 Algorithm 2 SAC-TDR Initialize : • Soft value function VΞ , target Soft value function V ′ Ξ , Critic networks Qθ1 , Qθ2 and actor- network πϕ with random parameters , θ1 , θ2 , ϕ • Target networks Ξ′ ← Ξ • an experience buffer D • a temporary experience buffer D′ with size N • Total training episode T 1 . For episode = 1 , T do 2 . Reset initialize state s0 , D′ For k = 0 , T do 3 . 4 . 5 . 6 . 7 . 8 . 9 . 10 . 11 . 12 . 13 . 14 . 15 . 16 . 17 . 18 . 19 . 20 . 21 . 22 . 23 . Choose an action ak based on current state sk and learned policy from A . Execute the action ak and observe a new state sk+1 with reward signal rk Store the transition ( sk , ak , rk , sk+1 ) in D′ if k + N − 1 ≤ T then 1 ) in the D′ 0 , s′ 0 , r′ 0 , a′ Get earliest memory ( s′ Calculate r′ based on Equation ( 23 ) Store the transition ( s′ 0 , a′ Clear original transition ( s′ 0 , r′ , s′ 0 , a′ 1 ) in D 0 , s′ 0 , r′ 1 ) in the D′ else Repeat step 8 to 11 and Calculate r′ based on Equation r′ k = γ − 1 γT −k+1 − 1 T ( cid:88 ) t=k γt−krt . ( 67 ) end if Sample mini-batch data ( st , at , r′ Get next action at+1 ← πϕ′ ( st+1 ) Target value yt based on Equation ( 7 ) Update Critics based on Equation 3 t , st+1 ) from D Update Soft value function based on original SAC formualtion Update ϕ by original SAC formulation Update target networks : Ξ′ ← τ Ξ + ( 1 − τ ) Ξ′ end for 24 . 25. end for 21 Algorithm 3 D4PG-TDR Initialize : 1 ← θ1 , θ′ 2 ← θ2 , ϕ′ ← ϕ , • Critic networks Zθ1 , Zθ2 and actor-network πϕ with random parameters , θ1 , θ2 , ϕ • Target networks θ′ • an experience buffer D • a temporary experience buffer D′ with size N • Total training episode T 1 . For episode = 1 , T do 2 . Reset initialize state s0 , D′ For k = 0 , T do 3 . 4 . 5 . 6 . 7 . 8 . 9 . 10 . 11 . 12 . 13 . 14 . 15 . 16 . 17 . 18 . 19 . 20 . 21 . 22 . 23 . 24 . 25 . Choose an action ak based on current state sk and learned policy from A . Execute the action ak and observe a new state sk+1 with reward signal rk Store the transition ( sk , ak , rk , sk+1 ) in D′ if k + N − 1 ≤ T then 1 ) in the D′ 0 , s′ 0 , r′ 0 , a′ Get earliest memory ( s′ Calculate r′ based on Equation ( 23 ) 0 , a′ Store the transition ( s′ Clear original transition ( s′ 0 , r′ , s′ 0 , a′ 1 ) in D 0 , s′ 0 , r′ 1 ) in the D′ else Repeat step 8 to 11 and Calculate r′ based on Equation r′ k = γ − 1 γT −k+1 − 1 T ( cid:88 ) t=k γt−krt . ( 68 ) end if Sample mini-batch data ( st , at , r′ Get next action at+1 ← πϕ′ ( st+1 ) Target distribution based on Equation ( 17 ) t , st+1 ) from D Update Critics based on Equation 18 if k mod Policy Update frequency then Update ϕ by Equation 21 Update target networks : θ′ ζ ← τ θζ + ( 1 − τ ) θ′ ζ ϕ′ ← τ ϕ + ( 1 − τ ) ϕ′ end if end for 26 . 27. end for 22","['n', 'l', 'c', 'r', 'mitigating', 'estimation', 'error', 'twin', 'td', 'regularize', 'actor', 'critic', 'deep', 'rein', 'learn', 'abstract', 'address', 'issue', 'estimation', 'bias', 'deep', 'reinforcement', 'learning', 'introduce', 'solution', 'mechanism', 'include', 'new', 'twin', 'tdregularize', 'actor', 'critic', 'method', 'aim', 'reduce', 'estimation', 'error', 'combine', 'good', 'drl', 'improvement', 'distributional', 'learning', 'long', 'n', 'step', 'surrogate', 'stage', 'reward', 'method', 'show', 'new', 'tdrbased', 'actorcritic', 'learning', 'enable', 'drl', 'method', 'outper', 'form', 'respective', 'baseline', 'challenge', 'environment', 'deepmind', 'control', 'suite', 'furthermore', 'elevate', 'td3', 'sac', 'respectively', 'level', 'perfor', 'mance', 'comparable', 'current', 'sota', 'also', 'improve', 'performance', 'new', 'sota', 'level', 'measure', 'mean', 'reward', 'conver', 'gence', 'speed', 'learn', 'success', 'rate', 'learn', 'variance', 'introduction', 'reinforcement', 'learning', 'rl', 'develop', 'decade', 'provide', 'mathematical', 'formal', 'ism', 'learningbased', 'control', 'recently', 'significant', 'progress', 'make', 'attain', 'excellent', 'result', 'wide', 'range', 'highdimensional', 'continuous', 'stateaction', 'space', 'problem', 'especially', 'robotic', 'application', 'robot', 'manipulation', 'andrychowicz', 'humanrobotic', 'interaction', 'however', 'fundamental', 'issue', 'estimation', 'error', 'associate', 'actorcritic', 'hasselt', 'still', 'pose', 'great', 'challenge', 'overestimation', 'example', 'use', 'operator', 'update', 'identify', 'study', 'thrun', 'schwartz', 'reduce', 'effort', 'focus', 'attain', 'accurate', 'stable', 'critic', 'network', 'td3', 'fujimoto', 'apply', 'clip', 'double', 'qlearning', 'take', 'minimum', 'estimate', 'sac', 'utilize', 'double', 'q', 'network', 'incorporate', 'entropy', 'regularization', 'critic', 'objective', 'function', 'ensure', 'exploratory', 'behavior', 'help', 'alleviate', 'overestimation', 'problem', 'however', 'directly', 'take', 'minimum', 'value', 'target', 'network', 'td3', 'sac', 'report', 'result', 'underestimation', 'bias', 'fujimoto', 'evaluation', 'reveal', 'multiple', 'role', 'estimation', 'error', 'learn', 'hand', 'overestimation', 'always', 'harmful', 'lan', 'consider', 'play', 'role', 'encourage', 'exploration', 'overestimated', 'action', 'line', 'underestimation', 'bias', 'discourage', 'exploration', 'overestimation', 'bias', 'occur', 'highvalue', 'region', 'contain', 'optimal', 'policy', 'encourage', 'exploration', 'good', 'thing', 'hailu', 'sommer', 'hand', 'overestimation', 'bias', 'also', 'cause', 'agent', 'overly', 'explore', 'lowvalue', 'region', 'lead', 'suboptimal', 'policy', 'accordingly', 'underestimation', 'bias', 'discourage', 'agent', 'explore', 'highvalue', 'region', 'avoid', 'lowvalue', 'region', 'thing', 'consider', 'estimation', 'error', 'leave', 'unchecked', 'accumulate', 'negatively', 'impact', 'policy', 'update', 'suboptimal', 'action', 'highly', 'rate', 'suboptimal', 'critic', 'reinforce', 'suboptimal', 'action', 'next', 'policy', 'update', 'fujimoto', 'aside', 'anecdotal', 'evidence', 'role', 'estimation', 'mitigate', 'principled', 'way', 'remain', 'open', 'issue', 'asuedu', 'several', 'method', 'evaluation', 'perform', 'show', 'promise', 'major', 'tool', 'mostly', 'leave', 'thus', 'far', 'still', 'clear', 'possible', 'far', 'reduce', 'estimation', 'error', 'consider', 'actor', 'give', 'interplay', 'actor', 'critic', 'handful', 'approach', 'examine', 'show', 'demonstrate', 'performance', 'improvement', 'paac', 'use', 'phase', 'actor', 'account', 'q', 'value', 'td', 'error', 'actor', 'update', 'double', 'actor', 'idea', 'propose', 'evaluate', 'take', 'minimum', 'value', 'estimate', 'associate', 'actor', 'network', 'however', 'directly', 'use', 'minimum', 'estimate', 'value', 'show', 'result', 'underestimation', 'error', 'similar', 'td3', 'method', 'fox', 'mutualinformation', 'leibfrie', 'graumoya', 'vieillard', 'rudner', 'regularization', 'also', 'use', 'enhance', 'policy', 'exploration', 'robustness', 'stability', 'tdregularize', 'actorcritic', 'parisi', 'regularize', 'actor', 'aim', 'enhance', 'stability', 'actor', 'learn', 'apply', 'td', 'error', 'online', 'critic', 'update', 'regularization', 'term', 'actor', 'update', 'however', 'none', 'method', 'show', 'regularization', 'actor', 'help', 'reduce', 'estimation', 'error', 'critic', 'paper', 'propose', 'new', 'tdregularize', 'learning', 'mechanism', 'include', 'td', 'regularize', 'double', 'critic', 'network', 'tdregularize', 'actor', 'network', 'new', 'architecture', 'several', 'property', 'make', 'ideal', 'enhancement', 'consider', 'tdregularize', 'double', 'critic', 'network', 'instead', 'directly', 'select', 'minimum', 'value', 'twin', 'target', 'network', 'select', 'target', 'base', 'minimum', 'td', 'error', 'address', 'overestimation', 'underestimation', 'problem', 'tdregularize', 'actor', 'network', 'formulate', 'new', 'td', 'error', 'regularize', 'actor', 'update', 'avoid', 'misleading', 'critic', 'regularization', 'term', 'help', 'far', 'reduce', 'estimation', 'error', 'critic', 'update', 'additionally', 'apply', 'combine', 'distributional', 'bellemare', 'lns', 'reward', 'estimation', 'far', 'improve', 'learn', 'stability', 'performance', 'relate', 'work', 'shed', 'light', 'novelty', 'method', 'discuss', 'double', 'critic', 'network', 'errorbase', 'actor', 'learn', 'provide', 'backdrop', 'include', 'review', 'distributional', 'bellemare', 'longn', 'step', 'surrogate', 'stage', 'lnss', 'method', 'double', 'critic', 'network', 'use', 'rl', 'hasselt', 'drl', 'fujimoto', 'hasselt', 'double', 'q', 'learn', 'hasselt', 'hasselt', 'first', 'show', 'reduction', 'overestimation', 'bias', 'td3', 'fujimoto', 'sac', 'also', 'show', 'effective', 'apply', 'clip', 'double', 'qlearning', 'use', 'minimum', 'q', 'estimate', 'however', 'method', 'induce', 'underestimation', 'bias', 'problem', 'hasselt', 'fujimoto', 'consequently', 'weight', 'double', 'q', 'learn', 'propose', 'deal', 'overestimation', 'underestimation', 'bias', 'however', 'method', 'test', 'drl', 'context', 'therefore', 'lack', 'systematic', 'approach', 'design', 'weighting', 'function', 'errorbase', 'actor', 'learning', 'expect', 'effective', 'reduce', 'overestimation', 'error', 'consistent', 'estimate', 'advantage', 'function', 'low', 'variance', 'discriminate', 'feedback', 'instead', 'directly', 'use', 'q', 'estimate', 'actorcritic', 'variant', 'crite', 'bhatnagar', 'update', 'actor', 'base', 'sign', 'error', 'positive', 'error', 'prefer', 'policy', 'update', 'however', 'error', 'measure', 'discrepancy', 'predict', 'value', 'target', 'value', 'guide', 'exploration', 'effectively', 'use', 'td', 'error', 'alone', 'actor', 'update', 'discourage', 'exploration', 'cause', 'slow', 'learning', 'especially', 'highdimensional', 'complex', 'problem', 'tdregularize', 'actorcritic', 'parisi', 'enhance', 'stability', 'actor', 'update', 'use', 'error', 'online', 'critic', 'update', 'regularization', 'term', 'however', 'use', 'td', 'error', 'sufficiently', 'evaluate', 'critic', 'update', 'use', 'temporal', 'difference', 'target', 'online', 'q', 'estimate', 'additionally', 'timevarying', 'regularization', 'coefficient', 'show', 'lead', 'poor', 'convergence', 'note', 'also', 'td', 'regularize', 'actorcritic', 'consider', 'tdregularize', 'actor', 'critic', 'contribution', 'introduce', 'novel', 'tdr', 'mechanism', 'tdregularize', 'double', 'critic', 'network', 'tdregularize', 'actor', 'network', 'extensive', 'experiment', 'use', 'dmc', 'benchmark', 'show', 'enable', 'sota', 'performance', 'measureed', 'learn', 'speed', 'success', 'rate', 'variance', 'converge', 'reward', 'wide', 'variety', 'control', 'task', 'locomotion', 'classical', 'control', 'task', 'sparse', 'reward', 'also', 'provide', 'qualitative', 'analysis', 'show', 'component', 'contribute', 'mitigate', 'estimation', 'error', 'method', 'double', 'q', 'actorcritic', 'method', 'general', 'double', 'q', 'actorcritic', 'method', 'fujimoto', 'policy', 'call', 'actor', 'stateaction', 'value', 'function', 'call', 'critic', 'actor', 'critic', 'estimate', 'deep', 'neural', 'network', 'parameter', 'respectively', 'first', 'consider', 'policy', 'π', 'evaluate', 'stateaction', 'value', 'function', 'e', 'rk', 'tk', 'γt−krt', 'sk', '∼', 'p', 'sk', 'actor', 'critic', 'method', 'base', 'temporal', 'difference', 'learn', 'sutton', 'update', 'q', 'estimate', 'minimize', 'error', 'obtain', 'difference', 'target', 'critic', 'estimate', 'value', 'next', 'consider', 'typical', 'double', 'q', 'method', 'entail', 'network', 'denote', 'qθ1', 'qθ2', 'respective', 'twin', 'target', 'network', 'denote', 'upcoming', 'discussion', 'also', 'use', 'θ', 'denote', 'parameter', 'q', 'network', 'θ', 'target', 'value', 'yk', 'less', 'target', 'value', 'yk', 'rk', 'sk1', 'πϕ′', 'sk1', 'take', 'minimum', 'target', 'value', 'aim', 'curtail', 'overestimation', 'q', 'value', 'frequently', 'experience', 'use', 'single', 'target', 'thus', 'critic', 'value', 'qθ', 'update', 'minimize', 'loss', 'function', 'l', 'θ', 'respect', 'critic', 'weight', 'θ', 'l', 'θ', 'ζ12', 'actor', 'weight', 'update', 'deterministic', 'policy', 'gradient', 'silver', 'convention', 'fujimoto', 'qθ1', 'use', 'update', 'actor', 'weight', 'es∼pπϕ', 'cid104', '∇aqθ1', 'aπϕ', 'figure', 'twin', 'tdregularize', 'actorcritic', 'tdr', 'architecture', 'twin', 'tdregularize', 'actorcritic', 'tdr', 'architecture', 'figure', 'depict', 'tdrbased', 'solution', 'mechanism', 'include', 'twin', 'q', 'network', 'td3', 'fujimoto', 'sac', 'actor', 'network', 'tdrbased', 'actor', 'critic', 'update', 'different', 'currently', 'exist', 'method', 'following', 'show', 'new', 'select', 'target', 'value', 'yk', 'different', 'equation', 'use', 'sac', 'td3', 'help', 'reduce', 'overestimation', 'underestimation', 'error', 'also', 'show', 'new', 'tdregularize', 'actor', 'help', 'far', 'reduce', 'estimation', 'bias', 'critic', 'tdrbased', 'solution', 'figure', 'include', 'additional', 'good', 'improvement', 'distributional', 'learning', 'long', 'n', 'step', 'surrogate', 'stage', 'lnss', 'method', 'describe', 'tdregularize', 'double', 'q', 'network', 'overcome', 'overestimation', 'td3', 'fujimoto', 'sac', 'train', 'critic', 'network', 'minimize', 'loss', 'function', 'equation', 'target', 'value', 'yk', 'equation', 'help', 'reduce', 'overestimation', 'error', 'promote', 'new', 'problem', 'underes', 'timation', 'usually', 'occur', 'early', 'stage', 'learn', 'subject', 'corrupted', 'reward', 'feedback', 'inaccurate', 'state', 'method', 'aim', 'minimize', 'loss', 'function', 'equation', 'different', 'target', 'value', 'yk', 'instead', 'directly', 'choose', 'less', 'target', 'value', 'equation', 'use', 'error', 'target', 'network', 'set', 'target', 'value', 'first', 'error', 'respective', 'target', 'network', 'determine', 'rk', 'rk', 'sk1', 'πϕ′', 'sk1', 'sk1', 'πϕ′', 'sk1', 'target', 'value', 'select', 'following', 'yk', 'cid26', 'rk', 'rk', 'sk1', 'πϕ′', 'sk1', 'sk1', 'πϕ′', 'sk1', 'note', 'equation', 'always', 'use', 'target', 'value', 'associate', 'small', 'target', 'value', 'regardless', 'error', 'sign', 'ultimate', 'objective', 'target', 'network', 'converge', 'choice', 'push', 'critic', 'equation', 'reach', 'target', 'matter', 'estimation', 'error', 'small', 'td', 'value', 'thus', 'naturally', 'position', 'address', 'overesdiation', 'underestimation', 'error', '≤', 'δ′', 'δ′', 'tdregularize', 'actor', 'network', 'tdregularize', 'actor', 'network', 'directly', 'penalize', 'actor', 'learn', 'objective', 'critic', 'estimation', 'error', 'estimation', 'error', 'first', 'critic', 'qθ1', 'choose', 'convention', 'double', 'qbase', 'actorcritic', 'method', 'determine', 'follow', 'qθi1', 'rk', 'γqθi1', 'sk1', 'sk1', 'represent', 'iteration', 'number', 'critic', 'update', 'actor', 'update', 'direction', 'maximize', 'q', 'keep', 'td', 'error', 'small', 'cid20', 'qθi1', 'aπϕ', 'ρ', 'regularization', 'coefficient', 'balance', 'role', 'td', 'error', 'actor', 'learn', 'objective', 'thus', 'expect', 'tdregularize', 'actor', 'help', 'far', 'reduce', 'estimation', 'error', 'critic', 'actor', 'cirtic', 'work', 'together', 'position', 'help', 'avoid', 'bad', 'policy', 'update', 'misleading', 'q', 'value', 'estimate', 'remark', 'key', 'difference', 'tdregularize', 'actor', 'network', 'parisi', 'equation', 'use', 'target', 'critic', 'sk1', 'sk1', 'construct', 'error', 'critic', 'update', 'error', 'evaluate', 'temporal', 'difference', 'target', 'online', 'q', 'estimate', 'accurately', 'evaluate', 'critic', 'estimation', 'construct', 'td', 'error', 'use', 'online', 'critic', 'directly', 'affect', 'actor', 'update', 'error', 'sufficiently', 'evaluate', 'critic', 'update', 'instead', 'equation', 'use', 'update', 'critic', 'θi1', 'construct', 'td', 'error', 'directly', 'measure', 'critic', 'estimation', 'mitigating', 'estimation', 'bias', 'let', 'qπ', 'true', 'q', 'value', 'obtain', 'follow', 'current', 'target', 'policy', 'π', 'let', 'qθ', 'estimate', 'value', 'use', 'neural', 'network', 'let', 'random', 'estimation', 'bias', 'stateaction', 'pair', 'θ', 'hold', 'target', 'network', 'replace', 'θ′', 'equation', 'overestimation', 'problem', 'refer', 'estimation', 'underestimation', 'problem', 'estimation', 'mitigate', 'estimation', 'bias', 'use', 'tdregularize', 'double', 'critic', 'network', 'theorem', 'let', 'qπ', 'true', 'q', 'value', 'follow', 'current', 'target', 'policy', 'π', 'target', 'network', 'estimate', 'use', 'double', 'q', 'neural', 'network', 'assume', 'exist', 'step', 'random', 'estimation', 'bias', 'ie', 'estimation', 'bias', 'kth', 'stage', 'independent', 'θ′', 'ζ', 'e', '∞', 'k', 'ζ', 'additionally', 'let', 'δyk', 'denote', 'θ′', 'ζ', 'target', 'value', 'estimation', 'error', 'accordingly', 'denote', 'error', 'follow', 'k', 'e', 'e', 'e', 'e', 'e', 'k', 'proof', 'proof', 'theorem', 'provide', 'b', 'remark', 'select', 'target', 'value', 'less', 'error', 'tdregularize', 'double', 'critic', 'network', 'mitigate', 'overestimation', 'underestimation', 'error', 'however', 'vanilla', 'double', 'q', 'method', 'ally', 'push', 'target', 'low', 'value', 'matter', 'estimation', 'error', 'estimation', 'error', 'detrimental', 'small', 'update', 'presence', 'unchecked', 'underestimation', 'bias', 'raise', 'concern', 'firstly', 'sufficient', 'reward', 'feed', 'back', 'environment', 'noisy', 'reward', 'sparse', 'reward', 'underestimation', 'bias', 'get', 'chance', 'make', 'correction', 'develop', 'significant', 'bias', 'several', 'date', 'secondly', 'inaccurate', 'value', 'estimate', 'lead', 'poor', 'policy', 'update', 'suboptimal', 'action', 'highly', 'rate', 'suboptimal', 'critic', 'reinforce', 'suboptimal', 'action', 'next', 'policy', 'update', 'address', 'misguide', 'critic', 'policy', 'update', 'use', 'tdregularize', 'actor', 'theorem', 'let', 'denote', 'true', 'q', 'value', 'follow', 'current', 'target', 'policy', 'π', 'qθ1', 'estimate', 'value', 'assume', 'exist', 'step', 'random', 'estimation', 'bias', 'ψk', 'independent', 'θ1', 'e', 'ψk', '∞', 'k', 'assume', 'policy', 'update', 'base', 'critic', 'qθ1', 'use', 'deterministic', 'policy', 'gradient', 'dpg', 'equation', 'let', 'δϕk', 'denote', 'change', 'actor', 'update', 'stage', 'accordingly', 'denote', 'change', 'true', 'change', 'approximation', 'error', 'q', 'follow', 'vanilla', 'dpg', 'δϕdp', 'e', 'δϕdp', 'e', 'δϕdp', 'e', 'ψk', 'e', '≥', 'δϕdp', 'define', 'equation', 'respectively', 'proof', 'proof', 'theorem', 'provide', 'remark', 'theorem', 'hold', 'regularization', 'factor', 'equation', 'e', 'ψk', 'e', 'use', 'θ1', 'actor', 'always', 'update', 'way', 'use', 'true', 'value', 'realistic', 'follow', 'relationship', 'still', 'preserve', 'e', 'ψk', 'help', 'ease', 'negative', 'effect', 'critic', 'estimation', 'bias', 'imply', 'e', 'δϕtrue', '≤', 'e', 'mitigate', 'critic', 'estimation', 'error', 'tdregularize', 'actor', 'theorem', 'suboptimal', 'actor', 'update', 'negatively', 'affect', 'critic', 'specifically', 'consider', 'actor', 'date', 'theorem', 'overestimation', 'case', 'e', 'qθ1', '≥', 'e', 'qθ1', 'πt', 'dr', '≥', 'πt', 'underestimation', 'case', 'e', 'qθ1', '≤', 'e', 'qθ1', 'πt', 'dr', 'e', 'πt', 'proof', 'proof', 'theorem', 'provide', 'remark', 'case', 'use', 'tdregularize', 'actor', 'expect', 'result', 'less', 'estimation', 'bias', 'critic', 'experiment', 'result', 'section', 'provide', 'comprehensive', 'evaluation', 'enable', 'actorcritic', 'learning', 'method', 'base', 'commonly', 'use', 'wellbehave', 'baseline', 'algorithm', 'include', 'sac', 'td3', 'additional', 'evaluation', 'also', 'provide', 'popular', 'drl', 'algorithm', 'ddpg', 'ppo', 'provide', 'broad', 'perspective', 'effectiveness', 'tdrbase', 'method', 'evaluation', 'perform', 'base', 'several', 'benchmark', 'deepmind', 'control', 'tassa', 'report', 'evaluation', 'result', 'use', 'follow', 'shortform', 'name', 'base', 'original', 'drl', 'algorithm', 'include', 'sac', 'td3', 'ddpg', 'ppo', 'tdrtd3', 'apply', 'td', 'regularize', 'double', 'critic', 'critic', 'network', 'td', 'regularize', 'actor', 'actor', 'network', 'regularization', 'factor', 'lns', 'n', 'tdrsac', 'apply', 'regularize', 'double', 'critic', 'critic', 'network', 'lns', 'n', 'dtdr', 'apply', 'td', 'regularize', 'double', 'critic', 'critic', 'network', 'td', 'regularize', 'actor', 'actor', 'network', 'regularization', 'factor', 'lns', 'n', 'evaluation', 'aim', 'quantitatively', 'address', 'follow', 'question', 'q1', 'improve', 'base', 'common', 'method', 'q2', 'performance', 'method', 'compare', 'sota', 'algorithm', 'method', 'robust', 'enough', 'handle', 'dense', 'stochastic', 'reward', 'sparse', 'reward', 'component', 'tdrbased', 'learning', 'mechanism', 'affect', 'performance', 'td', 'regularize', 'actor', 'make', 'policy', 'update', 'situation', 'misguide', 'critic', 'q6', 'regularization', 'coefficient', 'equation', 'affect', 'actor', 'performance', 'detail', 'implementation', 'training', 'evaluation', 'procedure', 'provide', 'c', 'link', 'implementation', 'code', 'also', 'provide', 'main', 'evaluation', 'obtain', 'comprehensive', 'evaluation', 'result', 'summarize', 'table', 'include', 'noise', 'respectively', 'state', 'action', 'reward', 'consider', 'dmc', 'environment', 'order', 'make', 'evaluation', 'realistic', 'run', 'sparse', 'sparsifie', 'reward', 'environment', 'detail', 'environment', 'setup', 'find', 'c', 'table', 'success', 'shorthand', 'learn', 'success', 'rate', 'avg', 'rwd', 'average', 'reward', 'rank', 'percent', 'reward', 'difference', 'evaluate', 'method', 'sota', 'average', 'reward', 'evaluate', 'method', 'positive', 'well', 'note', 'compute', 'success', 'rate', 'trial', 'achieve', 'reward', 'least', 'account', 'successful', 'learning', 'result', 'base', 'last', 'evaluation', 'different', 'random', 'seed', 'compare', 'algorithm', 'good', 'performance', 'boldface', 'average', 'reward', 'avg', 'rwd', 'note', 'implement', 'td', 'actor', 'sac', 'sac', 'already', 'entropyregulated', 'actor', 'q1', 'improve', 'respective', 'base', 'method', 'learn', 'curve', 'benchmark', 'environ', 'ment', 'show', 'figure', 'overall', 'method', 'solid', 'line', 'outperform', 'respective', 'base', 'figure', 'systematic', 'evaluation', 'realize', 'drl', 'algorithm', 'sac', 'td3', 'environment', 'uniform', 'random', 'noise', 'state', 'action', 'reward', 'shaded', 'region', 'represent', 'confidence', 'range', 'evaluation', 'seed', 'number', 'step', 'envirinoment', 'ddpg', 'sac', 'td3', 'tdrsac', 'tdrtd3', 'envirinoment', 'ddpg', 'sac', 'td3', 'tdrsac', 'tdrtd3', 'success', 'success', 'finger', 'turn', 'hard', 'avg', 'rwd', 'rwd', 'rank', 'rank', 'quadrupe', 'walk', 'success', 'avg', 'rwd', 'cartpole', 'sparse', 'rank', 'success', 'avg', 'rwd', 'rank', 'success', 'fish', 'swim', 'avg', 'rwd', 'run', 'sparse', 'success', 'avg', 'rwd', 'rank', 'rank', 'table', 'systematic', 'evaluation', 'respectively', 'augment', 'base', 'algorithm', 'rank', 'percent', 'reward', 'difference', 'sota', 'positive', 'well', 'method', 'td3', 'sac', 'dash', 'line', 'term', 'episode', 'reward', 'learn', 'speed', 'learn', 'variance', 'success', 'rate', 'table', 'measure', 'avg', 'rwd', 'method', 'form', 'respective', 'baseline', 'algorithm', 'notice', 'table', 'learn', 'success', 'rate', 'method', 'significant', 'improvement', 'base', 'method', 'comparison', 'ddpg', 'sac', 'td3', 'base', 'method', 'struggle', 'cartpole', 'sparse', 'run', 'sparse', 'moreover', 'method', 'also', 'outperform', 'ddpg', 'ppo', 'term', 'average', 'reward', 'awgrwd', 'learn', 'speed', 'learn', 'variance', 'success', 'rate', 'thus', 'help', 'succesfully', 'address', 'random', 'initialization', 'challenge', 'cause', 'random', 'seed', 'tdr', 'bring', 'performance', 'base', 'method', 'close', 'well', 'sota', 'figure', 'accord', 'rank', 'measure', 'table', 'environment', 'quadrupe', 'walk', 'tdrsac', 'tdrtd3', 'help', 'enhance', 'performance', 'respective', 'base', 'method', 'additionally', 'even', 'outperform', 'sota', 'around', 'rank', 'mea', 'sure', 'quadrupe', 'walk', 'even', 'tdrsac', 'tdrtd3', 'outperform', 'still', 'method', 'evaluate', 'provide', 'close', 'performance', 'also', 'worth', 'note', 'bring', 'performance', 'new', 'sota', 'level', 'measure', 'mean', 'reward', 'convergence', 'speed', 'learn', 'success', 'rate', 'robust', 'dense', 'stochastic', 'reward', 'sparse', 'reward', 'figure', 'table', 'method', 'outperform', 'respective', 'baseline', 'dense', 'stochastic', 'sparse', 'reward', 'term', 'average', 'reward', 'learn', 'variance', 'success', 'rate', 'converge', 'speed', 'particular', 'baseline', 'algorithm', 'td3', 'sac', 'struggle', 'reward', 'benchmark', 'environment', 'cartpole', 'sparse', 'run', 'sparse', 'however', 'use', 'learn', 'also', 'achieve', 'sota', 'performance', 'method', 'td3td', 'critic', 'actor', 'sactd', 'critic', 'actor', 'rwd', 'enhancement', 'avg', 'rwd', 'enhancement', 'cartpole', 'sparse', 'avg', 'rwd', 'enhancement', 'table', 'systematic', 'evaluation', 'component', 'tdr', 'compare', 'respective', 'base', 'gorithm', 'enhancement', 'percent', 'reward', 'difference', 'respective', 'base', 'algorithm', 'large', 'well', 'note', 'actor', 'consider', 'sac', 'sac', 'already', 'entropyregularize', 'actor', 'figure', 'evaluation', 'td', 'actor', 'different', 'ρ', 'ρ', 'equation', 'base', 'drl', 'algorithm', 'td3', 'environment', 'uniform', 'random', 'noise', 'state', 'action', 'reward', 'shaded', 'region', 'represent', 'confidence', 'range', 'evaluation', 'seed', 'number', 'step', 'ablation', 'study', 'perform', 'ablation', 'study', 'examine', 'remove', 'follow', 'compo', 'nent', 'respective', 'shortform', 'description', 'critic', 'td', 'regularize', 'double', 'q', 'network', 'actor', 'td', 'regularize', 'actor', 'network', 'lnss', 'method', 'n', 'table', 'enhancement', 'percent', 'reward', 'difference', 'evaluate', 'method', 'base', 'method', 'large', 'well', 'td', 'critic', 'actor', 'effectively', 'improve', 'base', 'algorithm', 'table', 'critic', 'lnss', 'actor', 'effectively', 'improve', 'base', 'algorithm', 'table', 'critic', 'lns', 'provide', 'comparable', 'significant', 'enhancement', 'base', 'algorithm', 'td', 'critic', 'method', 'outperform', 'respective', 'base', 'algorithm', 'suggest', 'mitigate', 'estimation', 'error', 'vanilla', 'double', 'q', 'network', 'effective', 'way', 'improve', 'performance', 'also', 'show', 'theoretical', 'analysis', 'theorem', 'method', 'help', 'improve', 'learn', 'performance', 'reduce', 'variance', 'value', 'estimation', 'noisy', 'reward', 'show', 'theoretically', 'empirically', 'include', 'lns', 'tdr', 'robust', 'noisy', 'sparse', 'reward', 'actor', 'element', 'also', 'help', 'make', 'appreciable', 'improvement', 'learn', 'performance', 'show', 'table', 'importantly', 'td', 'actor', 'play', 'importantly', 'role', 'stabilize', 'policy', 'update', 'show', 'theoretically', 'also', 'address', 'estimation', 'error', 'critic', 'show', 'theoretically', 'theorem', 'hyper', 'parameter', 'study', 'hyperparameter', 'study', 'result', 'summarize', 'figure', 'drl', 'method', 'td3', 'actor', 'evaluate', 'different', 'regularization', 'factor', 'ρ', 'report', 'average', 'performance', 'average', 'approximate', 'timation', 'error', 'difference', 'true', 'accumulate', 'reward', 'critic', 'value', 'γtrt', 'eval0', 'q5', 'td', 'regularize', 'actor', 'help', 'reduce', 'estimation', 'error', 'critic', 'figure', 'td', 'regularize', 'actor', 'actor', 'estimation', 'error', 'critic', 'duce', 'example', 'finger', 'turn', 'hard', 'actor', 'result', 'less', 'overestimation', 'error', 'compare', 'ρ', 'later', 'stage', 'training', 'td3', 'actor', 'less', 'underestimation', 'error', 'compare', 'ρ', 'similarly', 'cartpole', 'sparse', 'actor', 'result', 'less', 'overestimation', 'error', 'compare', 'ρ', 'policy', 'evaluate', 'epois', 'reward', 'high', 'epois', 'reward', 'generally', 'result', 'well', 'policy', 'figure', 'policy', 'update', 'improve', 'select', 'suitable', 'regularization', 'fac', 'tor', 'ρ', 'especially', 'cartpole', 'td3', 'actor', 'enable', 'successful', 'learning', 'base', 'method', 'struggle', 'stick', 'learning', 'entire', 'training', 'period', 'q6', 'range', 'ρ', 'ρ', 'generally', 'good', 'choice', 'figure', 'small', 'regular', 'ization', 'factor', 'result', 'less', 'regularization', 'provide', 'sufficient', 'estimation', 'error', 'reduction', 'critic', 'large', 'regularization', 'factor', 'result', 'regularization', 'negative', 'effect', 'learn', 'therefore', 'ρ', 'good', 'choice', 'therefore', 'work', 'consistently', 'use', 'ρ', 'obtain', 'result', 'conclusion', 'discussion', 'limitation', 'study', 'work', 'introduce', 'novel', 'tdr', 'mechanism', 'tdregularize', 'double', 'critic', 'network', 'tdregularize', 'actor', 'network', 'component', 'show', 'help', 'mitigate', 'estimation', 'error', 'show', 'consistently', 'outperform', 'respective', 'base', 'algorithm', 'solve', 'benchmark', 'task', 'term', 'average', 'reward', 'learn', 'success', 'rate', 'learn', 'speed', 'time', 'learn', 'variance', 'analytical', 'result', 'also', 'show', 'component', 'mitigate', 'estimation', 'error', 'show', 'figure', 'environment', 'quadrupe', 'walk', 'evaluate', 'tdr', 'combine', 'distributional', 'lnss', 'element', 'significantly', 'elevate', 'current', 'sota', 'performance', 'new', 'level', 'increase', 'least', 'even', 'identify', 'range', 'generally', 'good', 'regularization', 'coefficient', 'value', 'figure', 'show', 'different', 'algorithm', 'different', 'environment', 'respond', 'somewhat', 'differently', 'ρ', 'therefore', 'effectively', 'determine', 'regularization', 'factor', 'improvement', 'remain', 'question', 'thus', 'limitation', 'study', 'additionally', 'promising', 'performance', 'come', 'extensive', 'training', 'million', 'learn', 'step', 'perform', 'limited', 'training', 'time', 'training', 'step', 'need', 'far', 'investigate', 'reference', 'schneider', 'welinder', 'pieter', 'abbeel', 'wojciech', 'hindsight', 'experience', 'replay', 'arxiv', 'preprint', 'horgan', 'distribute', 'distributional', 'deterministic', 'policy', 'gradient', 'arxiv', 'preprint', 'dabney', 'muno', 'distributional', 'perspective', 'reinforcement', 'learning', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'sutton', 'incremental', 'nat', 'ural', 'actorcritic', 'algorithm', 'advance', 'neural', 'information', 'processing', 'system', 'dynamic', 'multiobjective', 'optimization', 'change', 'number', 'objective', 'ieee', 'transaction', 'evolutionary', 'computation', 'actorcritic', 'equivalent', 'qlearne', 'advance', 'neural', 'information', 'processing', 'system', 'dabney', 'georg', 'ostrovski', 'muno', 'implicit', 'quantile', 'network', 'distributional', 'reinforcement', 'learning', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', '2018a', 'dabney', 'munos', 'distributional', 'reinforcement', 'learning', 'quantile', 'regression', 'proceeding', 'conference', 'artificial', 'intelli', 'gence', 'volume', 'sun', 'distributional', 'soft', 'actorcritic', 'offpolicy', 'reinforcement', 'learning', 'address', 'value', 'estimation', 'error', 'ieee', 'transaction', 'neural', 'network', 'learn', 'system', 'roy', 'pakman', 'tame', 'noise', 'reinforcement', 'learning', 'soft', 'update', 'arxiv', 'preprint', 'arxiv151208562', 'herke', 'hoof', 'address', 'function', 'approximation', 'error', 'actor', 'critic', 'method', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'sehoon', 'vikash', 'pieter', 'abbeel', 'et', 'soft', 'actorcritic', 'algorithm', 'appli', 'cation', 'arxiv', 'preprint', 'arxiv181205905', 'sommer', 'amount', 'quality', 'bias', 'reinforcement', 'learning', 'conference', 'proceeding', 'ieee', 'international', 'conference', 'system', 'man', 'cybernetic', 'cat', 'volume', 'pp', 'ieee', 'double', 'qlearning', 'advance', 'neural', 'information', 'processing', 'system', 'deep', 'reinforcement', 'learning', 'matter', 'proceeding', 'conference', 'artificial', 'intelligence', 'volume', 'qingfeng', 'white', 'qlearning', 'control', 'estimation', 'bias', 'qlearne', 'arxiv', 'preprint', 'leibfrie', 'mutualinformation', 'regularization', 'decision', 'pro', 'cesse', 'actorcritic', 'learning', 'conference', 'robot', 'learn', 'pp', 'pmlr', 'infer', 'humanrobot', 'performance', 'objective', 'locomotion', 'use', 'inverse', 'reinforcement', 'learning', 'inverse', 'optimal', 'control', 'ieee', 'robotic', 'automation', 'letter', 'efficient', 'continuous', 'control', 'double', 'actor', 'regularized', 'critic', 'proceeding', 'conference', 'artificial', 'intelligence', 'volume', 'pp', 'pardo', 'deep', 'reinforcement', 'learning', 'library', 'fast', 'prototyping', 'benchmarke', 'arxiv', 'preprint', 'simone', 'parisi', 'voot', 'tdregularize', 'actor', 'critic', 'method', 'machine', 'learn', 'cong', 'osborne', 'yarin', 'gal', 'teh', 'pathology', 'regularize', 'reinforcement', 'learning', 'expert', 'demonstration', 'advance', 'neural', 'information', 'processing', 'system', '3428376–28389', 'guy', 'riedmiller', 'deterministic', 'policy', 'gradient', 'algorithm', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'sutton', 'reinforcement', 'learn', 'introduction', 'mit', 'press', 'yuval', 'tassa', 'deepmind', 'control', 'suite', 'arxiv', 'preprint', 'sebastian', 'thrun', 'anton', 'schwartz', 'issue', 'use', 'function', 'approximation', 'reinforcement', 'learning', 'proceeding', 'fourth', 'connectionist', 'model', 'summer', 'school', 'volume', 'pp', 'hillsdale', 'nj', 'deep', 'reinforcement', 'learning', 'double', 'q', 'learning', 'proceeding', 'conference', 'artificial', 'intelligence', 'volume', 'scherrer', 'olivi', 'pietquin', 'muno', 'geist', 'leverage', 'average', 'analysis', 'regularization', 'reinforcement', 'learning', 'advance', 'neural', 'information', 'processing', 'system', 'harsh', 'gupta', 'r', 'srikant', 'meansquared', 'error', 'double', 'qlearning', 'advance', 'neural', 'information', 'processing', 'system', 'ruofan', 'brent', 'humanrobotic', 'prosthesis', 'collaborate', 'agent', 'symmetrical', 'walking', 'advance', 'neural', 'information', 'processing', 'system', 'ruofan', 'phase', 'actor', 'actorcritic', 'reinforcement', 'learning', 'weight', 'double', 'qlearning', 'pp', 'ruofan', 'long', 'nstep', 'surrogate', 'stage', 'reward', 'reduce', 'variance', 'deep', 'reinforcement', 'learning', 'complex', 'problem', 'arxiv', 'preprint', 'arxiv221004820', 'distributional', 'tdr', 'lnss', 'distributional', 'rl', 'bellemare', 'represent', 'value', 'function', 'term', 'probability', 'distribution', 'rather', 'function', 'estimate', 'distribution', 'provide', 'comprehensive', 'rep', 'resentation', 'uncertainty', 'associate', 'range', 'different', 'possible', 'reward', 'return', 'state', 'action', 'pair', 'provide', 'informative', 'value', 'function', 'estimation', 'many', 'distributional', 'rl', 'algorithm', 'bellemare', 'dabney', 'achieve', 'great', 'performance', 'provement', 'many', 'discrete', 'problem', 'benchmark', 'barthmaron', 'apply', 'distributional', 'rl', 'continuous', 'control', 'problem', 'combine', 'distributional', 'return', 'function', 'actorcritic', 'framework', 'address', 'overestimation', 'error', 'apply', 'distributional', 'rl', 'piggyback', 'sac', 'provide', 'accurate', 'critic', 'overestimation', 'actor', 'still', 'exist', 'actor', 'still', 'update', 'maximize', 'expectation', 'value', 'function', 'distribution', 'regulate', 'actor', 'distributional', 'rl', 'solve', 'overestimation', 'barely', 'discuss', 'a1', 'distributional', 'tdregularize', 'actorcritic', 'dtdr', 'tailor', 'distributional', 'method', 'base', 'original', 'distributional', 'conceptu', 'alization', 'develop', 'barthmaron', 'bellemare', 'show', 'number', 'enhancement', 'meantime', 'distributional', 'critic', 'distributional', 'critic', 'bellemare', 'treat', 'return', 'tion', 'random', 'variable', 'z', 'expectation', 'use', 'q', 'value', 'estimate', 'namely', 'q', 'e', 'z', 'however', 'use', 'td', 'error', 'evaluate', 'distributional', 'critic', 'similar', 'equation', 'distributional', 'error', 'target', 'network', 'write', 'sk1', 'πϕ′', 'sk1', 'e', 'rk', 'γe', 'zθ′', 'd′', 'rk', 'γe', 'zθ′', 'd′', 'sk1', 'πϕ′', 'sk1', 'e', 'twin', 'tdregularize', 'target', 'distributional', 'operator', 'thus', 'define', 'cid26', 'rk', 'rk', 'sk1', 'πϕ′', 'sk1', 'sk1', 'sk1', 'd′', '≤', 'd′', 'd′', 'b', 'denote', 'random', 'variable', 'follow', 'probability', 'law', 'distributional', 'bellman', 'operator', 'appear', 'similar', 'equation', 'map', 'stateaction', 'pair', 'distribution', 'need', 'define', 'new', 'td', 'error', 'measure', 'distribution', 'barthmaron', 'consider', 'use', 'follow', 'distributional', 'loss', 'l', 'θ', 'zθζ', 'l', 'measure', 'distance', 'distribution', 'many', 'distributional', 'rl', 'algorithm', 'use', 'kullbackleibler', 'divergence', 'distance', 'metric', 'barthmaron', 'adopt', 'metric', 'distributional', 'actor', 'distributional', 'method', 'barthmaron', 'bellemare', 'policy', 'update', 'perform', 'base', 'policy', 'gradient', 'e', 'aπϕ', 'dtdr', 'need', 'use', 'critic', 'evaluation', 'metric', 'evaluate', 'quality', 'current', 'distribu', 'tional', 'critic', 'regularize', 'distributional', 'actor', 'first', 'formulate', 'follow', 'loss', 'metric', 'e', 'l', 'rk', 'γzθi1', 'sk1', 'sk1', 'zθi1', 'similar', 'tdregularize', 'actor', 'network', 'distributional', 'actor', 'update', 'direction', 'imize', 'expect', 'critic', 'keep', 'expect', 'distance', 'project', 'critic', 'critic', 'namely', 'e', '∇aρlz', 'aπϕ', 'ρ', 'regularization', 'coefficient', 'a2', 'long', 'nstep', 'surrogate', 'stage', 'lnss', 'reward', 'lns', 'utilize', 'long', 'reward', 'trajectory', 'n', 'future', 'step', 'estimation', 'stage', 'reward', 'rk', 'use', 'lnssresulte', 'reward', 'place', 'original', 'rk', 'show', 'effectively', 'reduce', 'learn', 'variance', 'significant', 'performance', 'improvement', 'offpolicy', 'method', 'give', 'reward', 'trajectory', 'n', 'step', 'time', 'step', 'k', 'let', 'g', '−1', '−1', '∈', 'r', 'shorthand', 'notation', 'denote', 'discount', 'n', 'step', 'return', '−1', 'γt−krt', 'tk', 'stage', 'reward', 'n', 'reward', 'place', 'rk', 'equation', 'determine', 'r′', 'step', 'reward', 'sequence', 'namely', 'surrogate', 'stage', 'treat', 'weighted', 'average', 'k', 'γt−krt', '−1', 'figure', 'show', 'r′', 'discuss', 'obtain', 'simply', 'use', 'place', 'rk', 'form', 'new', 'tuple', 'k', 'sk1', 'store', 'memory', 'buffer', 'method', 'proceed', 'b', 'estimation', 'analysis', 'lemma', 'let', 'qπ', 'true', 'q', 'value', 'follow', 'current', 'target', 'policy', 'π', 'qθ′', 'target', 'network', 'estimate', 'use', 'double', 'q', 'neural', 'network', 'assume', 'exist', 'step', 'random', 'estimation', 'bias', 'ie', 'estimation', 'bias', 'kth', 'stage', 'independent', 'θ′', 'mean', 'e', 'respectively', 'define', 'θ′', 'ζ', 'equation', 'ζ', '∞', 'k', 'ζ', 'δ′', 'δ′', 'e', 'e', '−µ′', 'proof', 'step', 'random', 'estimation', 'bias', 'θ′', 'ζ', 'rewrite', 'expectation', 'ψk', 'θ′', 'ζ', 'e', 'ψk1', 'θ′', 'ζ', '∞', 'tk1', 'ψt', 'θ′', 'ζ', 'expectation', 'target', 'write', 'e', 'e', 'rk', 'sk1', 'ψk1', '∞', 'θ′', 'e', 'rk', 'e', 'γt−k−1rt', 'tk1', 'use', 'equation', 'td', 'error', 'target', 'critic', 'equation', 'respectably', 'e', 'e', 'rk', 'γe', 'sk1', 'πϕ′', 'sk1', 'e', '−', '−', 'qπ', 'similarly', 'e', 'thus', 'hold', 'place', 'ready', 'analyze', 'estimation', 'error', 'use', 'double', 'q', 'dq', 'method', 'td3', 'fujimoto', 'sac', 'theorem', 'let', 'assumption', 'lemma', 'hold', 'let', 'δyk', 'denote', 'target', 'value', 'estimation', 'error', 'accordingly', 'denote', 'error', 'following', 'e', 'proof', 'proof', 'base', 'enumerate', 'total', 'possible', 'scenario', 'estimation', 'error', 'determine', 'relationship', 'target', 'q', 'value', 'true', 'value', 'provide', 'proof', 'unique', 'scenario', 'first', 'note', 'e', 'target', 'critic', 'value', 'true', 'value', 'follow', 'relationship', 'e', 'e', 'underestimated', 'imply', 'e', 'e', 'e', 'ψk', 'θ′', 'e', 'ψk', 'θ′', 'base', 'equation', 'tdr', 'use', 'target', 'value', 'e', 'e', 'rk', 'γe', 'sk1', 'πϕ′', 'sk1', 'however', 'vanilla', 'double', 'q', 'network', 'target', 'value', 'e', 'rk', 'γe', 'thus', 'base', 'equation', 'estimation', 'error', 'respective', 'target', 'value', 'sk1', 'πϕ′', 'sk1', 'e', 'k', 'e', 'e', 'e', 'γ', 'e', 'thus', 'identity', 'hold', 'case', 'target', 'critic', 'value', 'true', 'value', 'follow', 'relationship', 'e', 'e', 'e', 'expect', 'underestimate', 'overestimate', 'e', 'thus', 'imply', 'e', 'ψk', 'θ′', 'e', 'ψk', 'θ′', 'base', 'equation', 'tdr', 'use', 'target', 'value', 'e', 'e', 'rk', 'γe', 'sk1', 'πϕ′', 'sk1', 'however', 'vanilla', 'double', 'q', 'network', 'target', 'value', 'use', 'e', 'rk', 'γe', 'base', 'equation', 'estimation', 'error', 'respective', 'target', 'value', 'sk1', 'πϕ′', 'sk1', 'e', 'k', 'e', 'e', 'e', 'γ', 'e', 'thus', 'identity', 'hold', 'case', 'target', 'critic', 'value', 'true', 'value', 'follow', 'relationship', 'e', 'e', 'e', 'expect', 'underestimate', 'overestimate', 'e', 'imply', 'thus', 'e', 'ψk', 'θ′', 'e', 'ψk', 'θ′', 'base', 'equation', 'vanilla', 'double', 'q', 'network', 'tdr', 'pick', 'qθ′', 'target', 'value', 'e', 'e', 'e', 'rk', 'γe', 'sk1', 'πϕ′', 'sk1', 'base', 'equation', 'estimation', 'error', 'respective', 'target', 'value', 'e', 'e', 'γ', 'thus', 'thus', 'identity', 'hold', 'case', 'target', 'critic', 'value', 'true', 'value', 'follow', 'relationship', 'expect', 'overestimated', 'e', 'ψk', 'θ′', 'e', 'ψk', 'θ′', 'imply', 'e', 'e', 'base', 'equation', 'vanilla', 'double', 'q', 'network', 'twin', 'tdregularize', 'critic', 'pick', 'target', 'value', 'use', 'qθ′', 'mitigate', 'large', 'overestimation', 'bias', 'e', 'e', 'e', 'rk', 'γe', 'sk1', 'πϕ′', 'sk1', 'base', 'equation', 'estimation', 'error', 'target', 'value', 'e', 'γ', 'e', 'e', 'e', 'thus', 'identity', 'hold', 'method', 'mitigate', 'overestimation', 'error', 'note', 'case', 'study', 'relationship', 'e', 'e', 'e', 'let', 'denote', 'true', 'q', 'value', 'follow', 'current', 'target', 'policy', 'π', 'qθ1', 'estimate', 'value', 'assume', 'exist', 'step', 'random', 'estimation', 'bias', 'ψk', 'independent', 'θ1', 'e', 'ψk', '∞', 'k', 'assume', 'policy', 'update', 'base', 'critic', 'qθ1', 'use', 'deterministic', 'policy', 'gradient', 'dpg', 'equation', 'let', 'δϕk', 'denote', 'change', 'actor', 'update', 'stage', 'accordingly', 'denote', 'change', 'vanilla', 'δϕdp', 'follow', 'true', 'change', 'approximation', 'error', 'q', 'δϕtrue', 'still', 'valid', 'thus', 'theorem', 'hold', 'apply', 'procedure', 'k', 'cid26', 'e', 'δϕdp', 'e', 'δϕdp', 'e', 'ψk', 'e', '≥', 'proof', 'learn', 'rate', 'true', 'change', 'actor', 'parameter', 'case', 'approxima', 'tion', 'error', 'q', 'e', 'δϕtrue', 'k', 'αes∼pπ', 'cid104', 'aπϕj', 'πϕj', 'consider', 'estimate', 'critic', 'true', 'value', 'follow', 'relationship', 'equation', 'give', 'current', 'policy', 'parameter', 'ϕj', 'update', 'parameter', 'use', 'dpg', 'ϕj1', 'dp', 'αes∼pπ', 'cid104', 'e', 'δϕdp', 'αes∼pπ', 'aπϕj', 'πϕj', 'qπ', 'aπϕj', 'πϕj', 'overestimation', 'bias', 'e', 'mate', 'action', 'underestimation', 'bias', 'e', 'ψk', 'underestimated', 'action', 'result', 'suboptimal', 'policy', 'however', 'use', 'tdregularize', 'actor', 'give', 'current', 'policy', 'parameter', 'ϕj', 'actor', 'update', 'equation', 'update', 'encourage', 'exploration', 'overesti', 'update', 'discourage', 'exploration', 'αes∼pπ', 'αes∼pπ', 'qπ', 'aπϕj', 'πϕj', 'qπ', 'aπϕj', 'πϕj', 'similar', 'e', 'ψk', 'equation', 'e', 'e', 'rk', 'γqθi1', 'sk1', 'sk1', 'select', '≤', 'follow', 'cid26', 'ψk', 'e', 'ψk', 'e', 'e', 'ψk', 'e', 'ψk', 'e', '≥', 'therefore', 'inspect', 'equation', 'cid26', 'e', 'thus', 'theorem', 'hold', '≥', 'δϕdp', 'e', 'δϕdp', 'e', 'ψk', 'e', '≥', 'theorem', 'suboptimal', 'actor', 'update', 'negatively', 'affect', 'critic', 'specifically', 'consider', 'actor', 'date', 'theorem', 'overestimation', 'case', 'e', 'qθ1', '≥', 'e', 'qθ1', 'πt', 'dr', '≥', 'πt', 'underestimation', 'case', 'e', 'qθ1', '≤', 'e', 'qθ1', 'πt', 'dr', 'e', 'πt', 'proof', 'follow', 'analysis', 'td3', 'fujimoto', 'consider', 'equation', 'theorem', 'cid26', 'e', 'δϕdp', 'e', 'δϕdp', 'e', 'ψk', 'e', 'underestimate', '≥', 'overestimate', 'overestimation', 'case', 'approximate', 'value', 'use', 'vanilla', 'e', 'qθ1', '≥', 'e', 'qθ1', 'πt', 'dr', '≥', 'πtrue', 'similarly', 'underestimation', 'case', 'approximate', 'value', 'use', 'vanilla', 'e', 'qθ1', '≤', 'e', 'qθ1', 'πt', 'dr', 'e', 'πt', 'thus', 'theorem', 'hold', 'c', 'implementation', 'detail', 'use', 'pytorch', 'implementation', 'result', 'obtain', 'use', 'internal', 'server', 'consist', 'ing', 'ryzen', 'threadripper', 'processor', 'desktop', 'core', 'processor', 'desktop', 'core', 'processor', 'training', 'procedure', 'episode', 'initialize', 'reset', 'environment', 'terminate', 'step', 'trial', 'complete', 'training', 'process', 'contain', 'series', 'consecutive', 'episode', 'trial', 'run', 'maximum', 'time', 'step', 'evaluation', '×', 'time', 'step', 'task', 'report', 'trial', 'environment', 'network', 'initialize', 'random', 'seed', 'study', 'training', 'trial', 'remove', 'dependency', 'initial', 'parameter', 'policy', 'use', 'purely', 'exploratory', 'policy', 'first', 'time', 'step', 'start', 'timestep', 'afterwards', 'use', 'offpolicy', 'exploration', 'strategy', 'add', 'gaussian', 'noise', 'action', 'evaluation', 'procedure', '×', 'time', 'step', 'train', 'evaluation', 'section', 'evaluation', 'report', 'average', 'reward', 'evaluation', 'episode', 'exploration', 'noise', 'fix', 'policy', 'weight', 'random', 'seed', 'evaluation', 'different', 'training', 'trial', 'evaluation', 'perform', 'use', 'seed', 'seed', 'network', 'structure', 'optimizer', 'actorcritic', 'network', 'td3', 'implement', 'neural', 'network', 'layer', 'weight', 'layer', 'hide', 'node', 'rectify', 'linear', 'unit', 'relu', 'actor', 'critic', 'input', 'layer', 'actor', 'dimension', 'observation', 'state', 'output', 'layer', 'actor', 'dimension', 'action', 'requirement', 'tanh', 'unit', 'critic', 'receive', 'state', 'action', 'input', 'first', 'layer', 'output', 'layer', 'critic', 'linear', 'unit', 'produce', 'q', 'value', 'network', 'parameter', 'update', 'use', 'optimizer', 'learning', 'rate', 'simple', 'control', 'problem', 'time', 'step', 'k', 'network', 'train', 'minibatch', 'transition', 'r', 'r′', 'case', 'sample', 'uniformly', 'replay', 'buffer', 'contain', 'entire', 'history', 'agent', 'actorcritic', 'network', 'implement', 'neural', 'net', 'work', 'layer', 'weight', 'layer', 'hide', 'node', 'rectify', 'linear', 'unit', 'relu', 'actor', 'critic', 'input', 'layer', 'actor', 'dimension', 'observa', 'tion', 'state', 'output', 'layer', 'actor', 'dimension', 'action', 'requirement', 'tanh', 'unit', 'critic', 'receive', 'state', 'action', 'input', 'first', 'layer', 'output', 'layer', 'critic', 'distribution', 'hyperparameter', 'number', 'atom', 'l', 'bound', 'support', 'vmin', 'vmax', 'network', 'parameter', 'update', 'use', 'optimizer', 'learning', 'rate', 'time', 'step', 'k', 'network', 'train', 'minibatch', 'transition', 'r', 'r′', 'case', 'sample', 'uniformly', 'replay', 'buffer', 'contain', 'entire', 'tory', 'agent', 'sac', 'actorcritic', 'network', 'sac', 'implement', 'neural', 'network', 'layer', 'weight', 'layer', 'hide', 'node', 'rectify', 'linear', 'unit', 'relu', 'actor', 'critic', 'input', 'layer', 'actor', 'dimension', 'observation', 'state', 'output', 'layer', 'actor', 'dimension', 'action', 'requirement', 'tanh', 'unit', 'critic', 'receive', 'state', 'action', 'input', 'first', 'layer', 'output', 'layer', 'critic', 'linear', 'unit', 'produce', 'q', 'value', 'network', 'parameter', 'update', 'use', 'optimizer', 'learning', 'rate', 'simple', 'control', 'problem', 'time', 'step', 'k', 'network', 'train', 'minibatch', 'transition', 'r', 'r′', 'case', 'sample', 'uniformly', 'replay', 'buffer', 'contain', 'entire', 'history', 'agent', 'hyperparameter', 'keep', 'comparison', 'work', 'fair', 'set', 'common', 'hyperparameter', 'network', 'layer', 'batch', 'size', 'learning', 'rate', 'discount', 'factor', 'number', 'agent', 'comparison', 'method', 'different', 'method', 'td3', 'target', 'policy', 'smoothing', 'implement', 'add', 'ϵ', 'n', 'action', 'choose', 'target', 'actornetwork', 'clip', 'delay', 'policy', 'update', 'consist', 'update', 'actor', 'target', 'critic', 'network', 'iteration', 'large', 'result', 'large', 'benefit', 'respect', 'accumulate', 'error', 'fair', 'comparison', 'critic', 'train', 'time', 'step', 'train', 'actor', 'iteration', 'cripple', 'learn', 'target', 'network', 'update', 'td3', 'use', 'study', 'base', 'paper', 'fujimoto', 'code', 'author', 'https', 'githubcomsfujimtd3', 'hyperparameter', 'td3', 'start', 'timestep', 'evaluation', 'exploration', 'noise', 'policy', 'noise', 'noise', 'clip', 'policy', 'update', 'frequency', 'batch', 'size', 'buffer', 'size', 'τ', 'number', 'parallel', 'actor', 'lnssn', 'learning', 'rate', 'regularization', 'factor', 'value', 'step', 'step', 'step', 'n', 'n', 'table', 'td3', 'hyper', 'parameter', 'use', 'task', 'sac', 'use', 'study', 'base', 'paper', 'code', 'https', 'hyperparameter', 'table', 'value', 'step', 'step', 'step', 'n', 'n', 'hyperparameter', 'sac', 'start', 'timestep', 'evaluation', 'exploration', 'noise', 'policy', 'noise', 'noise', 'clip', 'policy', 'update', 'frequency', 'batch', 'size', 'buffer', 'size', 'temperature', 'parameter', 'number', 'parallel', 'actor', 'lnssn', 'learning', 'rate', 'table', 'sac', 'hyper', 'parameter', 'use', 'task', 'use', 'study', 'base', 'paper', 'barthmaron', 'code', 'modify', 'td3', 'hyperparameter', 'table', 'algorithm', 'drl', 'training', 'platform', 'evaluation', 'algorithm', 'sparse', 'reward', 'setup', 'sparse', 'need', 'run', 'forward', 'fast', 'possible', 'agent', 'get', 'reward', 'speed', 'exceed', 'ms', 'make', 'reward', 'sparse', 'r', 'v', 'else', 'r', 'hyperparameter', 'start', 'timestep', 'evaluation', 'exploration', 'noise', 'noise', 'batch', 'size', 'buffer', 'size', 'τ', 'number', 'parallel', 'actor', 'lnssn', 'learn', 'regularization', 'factor', 'value', 'step', 'step', 'step', 'n', 'table', 'hyper', 'parameter', 'use', 'task', 'tdr', 'algorithm', 'detail', 'section', 'show', 'tdrbased', 'algorithm', 'show', 'sac', 'show', 'show', 'mainly', 'add', 'lnss', 'reward', 'sample', 'collection', 'part', 'update', 'part', 'mainly', 'modify', 'target', 'value', 'selection', 'use', 'equation', 'regular', 'drl', 'equation', 'distributional', 'additionally', 'applicable', 'modify', 'actor', 'gradient', 'base', 'equation', 'regular', 'drl', 'equation', 'distributional', 'code', 'release', 'paper', 'accept', 'initialize', 'θ′', 'ϕ′', 'critic', 'network', 'qθ1', 'qθ2', 'actornetwork', 'random', 'parameter', '•', 'target', 'network', '•', 'experience', 'buffer', 'temporary', 'experience', 'buffer', 'size', 'total', 'training', 'episode', 'episode', 'reset', 'initialize', 'state', 'd′', 'choose', 'action', 'base', 'current', 'state', 'sk', 'learn', 'policy', 'execute', 'action', 'observe', 'new', 'state', 'sk1', 'reward', 'signal', 'rk', 'store', 'transition', 'rk', 'sk1', 'd′', 'n', '≤', 'r′', 'a′', 'get', 'early', 'memory', 'calculate', 'r′', 'base', 'equation', 'store', 'transition', 'clear', 'original', 'transition', 'r′', 'r′', 'd′', 'else', 'repeat', 'step', 'calculate', 'r′', 'base', 'equation', 'k', 'γt', '−k1', 'γt−krt', 'st1', 'end', 'minibatch', 'datum', 'r′', 'get', 'next', 'action', 'at1', 'st1', 'target', 'value', 'base', 'equation', 'update', 'critic', 'base', 'equation', 'policy', 'update', 'frequency', 'update', 'equation', 'update', 'target', 'network', 'θ′', 'τ', 'θ′', 'end', 'end', 'end', 'initialize', 'soft', 'value', 'function', 'vξ', 'target', 'soft', 'value', 'function', '′', 'ξ', 'critic', 'network', 'qθ1', 'qθ2', 'actor', 'network', 'πϕ', 'random', 'parameter', '•', 'target', 'network', 'experience', 'buffer', 'temporary', 'experience', 'buffer', 'size', 'total', 'training', 'episode', 'episode', 'reset', 'initialize', 'state', 'd′', 'choose', 'action', 'base', 'current', 'state', 'sk', 'learn', 'policy', 'execute', 'action', 'observe', 'new', 'state', 'sk1', 'reward', 'signal', 'rk', 'store', 'transition', 'rk', 'sk1', 'd′', 'n', '≤', 'r′', 'a′', 'get', 'early', 'memory', 'calculate', 'r′', 'base', 'equation', 'store', 'transition', 'clear', 'original', 'transition', 'r′', 'r′', 'd′', 'else', 'repeat', 'step', 'calculate', 'r′', 'base', 'equation', 'k', 'γt', '−k1', 'γt−krt', 'end', 'minibatch', 'datum', 'r′', 'get', 'next', 'action', 'at1', 'st1', 'target', 'value', 'base', 'equation', 'update', 'critic', 'base', 'equation', 'st1', 'update', 'soft', 'value', 'function', 'base', 'original', 'sac', 'formualtion', 'update', 'original', 'sac', 'formulation', 'update', 'target', 'network', 'τ', 'end', 'end', 'initialize', 'θ′', 'ϕ′', 'critic', 'network', 'actornetwork', 'random', 'parameter', '•', 'target', 'network', '•', 'experience', 'buffer', 'temporary', 'experience', 'buffer', 'size', 'total', 'training', 'episode', 'episode', 'reset', 'initialize', 'state', 'd′', 'choose', 'action', 'base', 'current', 'state', 'sk', 'learn', 'policy', 'execute', 'action', 'observe', 'new', 'state', 'sk1', 'reward', 'signal', 'rk', 'store', 'transition', 'rk', 'sk1', 'd′', 'n', '≤', 'r′', 'a′', 'get', 'early', 'memory', 'calculate', 'r′', 'base', 'equation', 'store', 'transition', 'clear', 'original', 'transition', 'r′', 'r′', 'd′', 'else', 'repeat', 'step', 'calculate', 'r′', 'base', 'equation', 'k', 'γt', '−k1', 'γt−krt', 'end', 'minibatch', 'datum', 'r′', 'get', 'next', 'action', 'at1', 'st1', 'target', 'distribution', 'base', 'equation', 'st1', 'update', 'critic', 'base', 'equation', 'policy', 'update', 'frequency', 'update', 'equation', 'update', 'target', 'network', 'θ′', 'τ', 'θ′', 'end', 'end', 'end']"
