title,url,date,text,cleaning,tokens,summary
Google DeepMind’s new AI tool helped create more than 700 new materials,https://www.technologyreview.com/2023/11/29/1084061/deepmind-ai-tool-for-new-materials-discovery/,2023-11-29,"From EV batteries to solar cells to microchips, new materials can supercharge technological breakthroughs. But discovering them usually takes months or even years of trial-and-error research.  Google DeepMind hopes to change that with a new tool that uses deep learning to dramatically speed up the process of discovering new materials. Called graphical networks for material exploration (GNoME), the technology has already been used to predict structures for 2.2 million new materials, of which more than 700 have gone on to be created in the lab and are now being tested. It is described in a paper published in Nature today.  Alongside GNoME, Lawrence Berkeley National Laboratory also announced a new autonomous lab. The lab takes data from the materials database that includes some of GNoME’s discoveries and uses machine learning and robotic arms to engineer new materials without the help of humans. Google DeepMind says that together, these advancements show the potential of using AI to scale up the discovery and development of new materials. The company has already used its protein-folding AI, AlphaFold, to generate structures for the human proteome, as well as yeast, fruit flies, mice, and more. GNoME can be described as AlphaFold for materials discovery, according to Ju Li, a materials science and engineering professor at the Massachusetts Institute of Technology. AlphaFold, a DeepMind AI system announced in 2020, predicts the structures of proteins with high accuracy and has since advanced biological research and drug discovery. Thanks to GNoME, the number of known stable materials has grown almost tenfold, to 421,000. “While materials play a very critical role in almost any technology, we as humanity know only a few tens of thousands of stable materials,” said Dogus Cubuk, materials discovery lead at Google DeepMind, at a press briefing.  To discover new materials, scientists combine elements across the periodic table. But because there are so many combinations, it’s inefficient to do this process blindly. Instead, researchers build upon existing structures, making small tweaks in the hope of discovering new combinations that hold potential. However, this painstaking process is still very time consuming. Also, because it builds on existing structures, it limits the potential for unexpected discoveries.  To overcome these limitations, DeepMind combines two different deep-learning models. The first generates more than a billion structures by making modifications to elements in existing materials. The second, however, ignores existing structures and predicts the stability of new materials purely on the basis of chemical formulas. The combination of these two models allows for a much broader range of possibilities.  Once the candidate structures are generated, they are filtered through DeepMind’s GNoME models. The models predict the decomposition energy of a given structure, which is an important indicator of how stable the material can be. “Stable” materials do not easily decompose, which is important for engineering purposes. GNoME selects the most promising candidates, which go through further evaluation based on known theoretical frameworks. This process is then repeated multiple times, with each discovery incorporated into the next round of training. In its first round, GNoME predicted different materials' stability with a precision of around 5%, but it increased quickly throughout the iterative learning process. The final results showed GNoME managed to predict the stability of structures over 80% of the time for the first model and 33% for the second.  Using AI models to come up with new materials is not a novel idea. The Materials Project, a program led by Kristin Persson at Berkeley Lab, has used similar techniques to discover and improve the stability of 48,000 materials.  However, GNoME’s size and precision set it apart from previous efforts. It was trained on at least an order of magnitude more data than any previous model, says Chris Bartel, an assistant professor of chemical engineering and materials science at the University of Minnesota.  Doing similar calculations has previously been expensive and limited in scale, says Yifei Mo, an associate professor of materials science and engineering at the University of Maryland. GNoME allows these computations to scale up with higher accuracy and at much less computational cost, Mo says: “The impact can be huge.” Once new materials have been identified, it is equally important to synthesize them and prove their usefulness. Berkeley Lab’s new autonomous laboratory, named the A-Lab, has been using some of GNoME’s discoveries with the Materials Project information, integrating robotics with machine learning to optimize the development of such materials. The lab is capable of making its own decisions about how to make a proposed material and creates up to five initial formulations. These formulations are generated by a machine-learning model trained on existing scientific literature. After each experiment, the lab uses the results to adjust the recipes. Researchers at Berkeley Lab say that A-Lab was able to perform 355 experiments over 17 days and successfully synthesized 41 out of 58 proposed compounds. This works out to two successful syntheses a day. In a typical, human-led lab, it takes much longer to make materials. “If you’re unlucky, it can take months or even years,” said Persson at a press briefing. Most students give up after a few weeks, she said. “But the A-Lab doesn’t mind failing. It keeps trying and trying.” Researchers at DeepMind and Berkeley Lab say these new AI tools can help accelerate hardware innovation in energy, computing, and many other sectors. “Hardware, especially when it comes to clean energy, needs innovation if we are going to solve the climate crisis,” says Persson. “This is one aspect of accelerating that innovation.” Bartel, who was not involved in the research, says that these materials will be promising candidates for technologies spanning batteries, computer chips, ceramics, and electronics.  Lithium-ion battery conductors are one of the most promising use cases. Conductors play an important role in batteries by facilitating the flow of electric current between various components. DeepMind says GNoME identified 528 promising lithium-ion conductors among other discoveries, some of which may help make batteries more efficient.  However, even after new materials are discovered, it usually takes decades for industries to take them to the commercial stage. “If we can reduce this to five years, that will be a big improvement,” says Cubuk. Correction: This story has been updated to make clear where the lab's data comes from. ","From EV batteries to solar cells to microchips , new materials can supercharge technological breakthroughs . But discovering them usually takes months or even years of trial-and-error research . Google DeepMind hopes to change that with a new tool that uses deep learning to dramatically speed up the process of discovering new materials . Called graphical networks for material exploration ( GNoME ) , the technology has already been used to predict structures for 2.2 million new materials , of which more than 700 have gone on to be created in the lab and are now being tested . It is described in a paper published in Nature today . Alongside GNoME , Lawrence Berkeley National Laboratory also announced a new autonomous lab . The lab takes data from the materials database that includes some of GNoME ’ s discoveries and uses machine learning and robotic arms to engineer new materials without the help of humans . Google DeepMind says that together , these advancements show the potential of using AI to scale up the discovery and development of new materials . The company has already used its protein-folding AI , AlphaFold , to generate structures for the human proteome , as well as yeast , fruit flies , mice , and more . GNoME can be described as AlphaFold for materials discovery , according to Ju Li , a materials science and engineering professor at the Massachusetts Institute of Technology . AlphaFold , a DeepMind AI system announced in 2020 , predicts the structures of proteins with high accuracy and has since advanced biological research and drug discovery . Thanks to GNoME , the number of known stable materials has grown almost tenfold , to 421,000 . “ While materials play a very critical role in almost any technology , we as humanity know only a few tens of thousands of stable materials , ” said Dogus Cubuk , materials discovery lead at Google DeepMind , at a press briefing . To discover new materials , scientists combine elements across the periodic table . But because there are so many combinations , it ’ s inefficient to do this process blindly . Instead , researchers build upon existing structures , making small tweaks in the hope of discovering new combinations that hold potential . However , this painstaking process is still very time consuming . Also , because it builds on existing structures , it limits the potential for unexpected discoveries . To overcome these limitations , DeepMind combines two different deep-learning models . The first generates more than a billion structures by making modifications to elements in existing materials . The second , however , ignores existing structures and predicts the stability of new materials purely on the basis of chemical formulas . The combination of these two models allows for a much broader range of possibilities . Once the candidate structures are generated , they are filtered through DeepMind ’ s GNoME models . The models predict the decomposition energy of a given structure , which is an important indicator of how stable the material can be . “ Stable ” materials do not easily decompose , which is important for engineering purposes . GNoME selects the most promising candidates , which go through further evaluation based on known theoretical frameworks . This process is then repeated multiple times , with each discovery incorporated into the next round of training . In its first round , GNoME predicted different materials ' stability with a precision of around 5 % , but it increased quickly throughout the iterative learning process . The final results showed GNoME managed to predict the stability of structures over 80 % of the time for the first model and 33 % for the second . Using AI models to come up with new materials is not a novel idea . The Materials Project , a program led by Kristin Persson at Berkeley Lab , has used similar techniques to discover and improve the stability of 48,000 materials . However , GNoME ’ s size and precision set it apart from previous efforts . It was trained on at least an order of magnitude more data than any previous model , says Chris Bartel , an assistant professor of chemical engineering and materials science at the University of Minnesota . Doing similar calculations has previously been expensive and limited in scale , says Yifei Mo , an associate professor of materials science and engineering at the University of Maryland . GNoME allows these computations to scale up with higher accuracy and at much less computational cost , Mo says : “ The impact can be huge. ” Once new materials have been identified , it is equally important to synthesize them and prove their usefulness . Berkeley Lab ’ s new autonomous laboratory , named the A-Lab , has been using some of GNoME ’ s discoveries with the Materials Project information , integrating robotics with machine learning to optimize the development of such materials . The lab is capable of making its own decisions about how to make a proposed material and creates up to five initial formulations . These formulations are generated by a machine-learning model trained on existing scientific literature . After each experiment , the lab uses the results to adjust the recipes . Researchers at Berkeley Lab say that A-Lab was able to perform 355 experiments over 17 days and successfully synthesized 41 out of 58 proposed compounds . This works out to two successful syntheses a day . In a typical , human-led lab , it takes much longer to make materials . “ If you ’ re unlucky , it can take months or even years , ” said Persson at a press briefing . Most students give up after a few weeks , she said . “ But the A-Lab doesn ’ t mind failing . It keeps trying and trying. ” Researchers at DeepMind and Berkeley Lab say these new AI tools can help accelerate hardware innovation in energy , computing , and many other sectors . “ Hardware , especially when it comes to clean energy , needs innovation if we are going to solve the climate crisis , ” says Persson . “ This is one aspect of accelerating that innovation. ” Bartel , who was not involved in the research , says that these materials will be promising candidates for technologies spanning batteries , computer chips , ceramics , and electronics . Lithium-ion battery conductors are one of the most promising use cases . Conductors play an important role in batteries by facilitating the flow of electric current between various components . DeepMind says GNoME identified 528 promising lithium-ion conductors among other discoveries , some of which may help make batteries more efficient . However , even after new materials are discovered , it usually takes decades for industries to take them to the commercial stage . “ If we can reduce this to five years , that will be a big improvement , ” says Cubuk . Correction : This story has been updated to make clear where the lab 's data comes from .","['ev', 'battery', 'solar', 'cell', 'microchip', 'new', 'material', 'supercharge', 'technological', 'breakthrough', 'discover', 'usually', 'take', 'month', 'even', 'year', 'research', 'hope', 'change', 'new', 'tool', 'use', 'deep', 'learning', 'dramatically', 'speed', 'process', 'discover', 'new', 'material', 'call', 'graphical', 'network', 'material', 'exploration', 'gnome', 'technology', 'already', 'use', 'predict', 'structure', 'new', 'material', 'go', 'create', 'lab', 'test', 'describe', 'paper', 'publish', 'nature', 'today', 'gnome', 'also', 'announce', 'new', 'autonomous', 'lab', 'lab', 'take', 'datum', 'material', 'database', 'include', 'gnome', 'discovery', 'use', 'machine', 'learning', 'robotic', 'arm', 'engineer', 'new', 'material', 'help', 'human', 'say', 'together', 'advancement', 'show', 'potential', 'use', 'ai', 'scale', 'discovery', 'development', 'new', 'material', 'company', 'already', 'use', 'proteinfolde', 'ai', 'alphafold', 'generate', 'structure', 'human', 'proteome', 'well', 'yeast', 'fruit', 'fly', 'mouse', 'gnome', 'describe', 'alphafold', 'material', 'discovery', 'accord', 'material', 'science', 'engineering', 'professor', 'alphafold', 'deepmind', 'ai', 'system', 'announce', 'predict', 'structure', 'protein', 'high', 'accuracy', 'advanced', 'biological', 'research', 'drug', 'thank', 'gnome', 'number', 'know', 'stable', 'material', 'grow', 'almost', 'tenfold', 'material', 'play', 'critical', 'role', 'almost', 'technology', 'humanity', 'know', 'ten', 'thousand', 'stable', 'material', 'say', 'material', 'lead', 'press', 'briefing', 'discover', 'new', 'material', 'scientist', 'combine', 'element', 'periodic', 'table', 'many', 'combination', 'inefficient', 'process', 'blindly', 'instead', 'researcher', 'build', 'exist', 'structure', 'make', 'small', 'tweak', 'hope', 'discover', 'new', 'combination', 'hold', 'potential', 'however', 'painstaking', 'process', 'still', 'time', 'consume', 'also', 'build', 'exist', 'structure', 'limit', 'potential', 'unexpected', 'discovery', 'overcome', 'limitation', 'deepmind', 'combine', 'different', 'deeplearning', 'model', 'first', 'generate', 'structure', 'make', 'modification', 'element', 'exist', 'material', 'second', 'however', 'ignore', 'exist', 'structure', 'predict', 'stability', 'new', 'material', 'purely', 'basis', 'chemical', 'formula', 'combination', 'model', 'allow', 'much', 'broad', 'range', 'possibility', 'candidate', 'structure', 'generate', 'filter', 'deepmind', 'gnome', 'model', 'model', 'predict', 'decomposition', 'energy', 'give', 'structure', 'important', 'indicator', 'stable', 'material', 'stable', 'material', 'easily', 'decompose', 'important', 'engineering', 'purpose', 'gnome', 'select', 'promising', 'candidate', 'go', 'evaluation', 'base', 'know', 'theoretical', 'framework', 'process', 'repeat', 'multiple', 'time', 'discovery', 'incorporate', 'next', 'round', 'training', 'first', 'round', 'gnome', 'predict', 'different', 'material', 'stability', 'precision', 'around', 'increase', 'quickly', 'iterative', 'learning', 'process', 'final', 'result', 'show', 'gnome', 'manage', 'predict', 'stability', 'structure', 'time', 'first', 'model', 'second', 'use', 'ai', 'model', 'come', 'new', 'material', 'novel', 'idea', 'material', 'project', 'program', 'lead', 'persson', 'use', 'similar', 'technique', 'discover', 'improve', 'stability', 'material', 'however', 'gnome', 'size', 'precision', 'set', 'apart', 'previous', 'effort', 'train', 'least', 'order', 'magnitude', 'datum', 'previous', 'model', 'say', 'assistant', 'professor', 'engineering', 'material', 'science', 'similar', 'calculation', 'previously', 'expensive', 'limit', 'scale', 'say', 'associate', 'professor', 'material', 'science', 'engineering', 'gnome', 'allow', 'computation', 'scale', 'high', 'accuracy', 'much', 'less', 'computational', 'cost', 'mo', 'say', 'impact', 'huge', 'new', 'material', 'identify', 'equally', 'important', 'synthesize', 'prove', 'usefulness', 'new', 'autonomous', 'laboratory', 'name', 'alab', 'use', 'gnome', 'discovery', 'material', 'project', 'information', 'integrate', 'robotic', 'machine', 'learn', 'optimize', 'development', 'material', 'lab', 'capable', 'make', 'decision', 'make', 'propose', 'material', 'create', 'initial', 'formulation', 'formulation', 'generate', 'machinelearning', 'model', 'train', 'exist', 'scientific', 'literature', 'experiment', 'lab', 'use', 'result', 'adjust', 'recipe', 'researcher', 'say', 'able', 'perform', 'experiment', 'day', 'successfully', 'synthesize', 'propose', 'compound', 'work', 'successful', 'synthese', 'day', 'typical', 'humanled', 'lab', 'take', 'much', 'long', 'make', 'material', 'unlucky', 'take', 'month', 'even', 'year', 'say', 'persson', 'press', 'briefing', 'student', 'give', 'week', 'say', 'alab', 'mind', 'fail', 'keep', 'try', 'try', 'researcher', 'deepmind', 'say', 'new', 'tool', 'help', 'accelerate', 'hardware', 'innovation', 'energy', 'computing', 'many', 'sector', 'hardware', 'especially', 'come', 'clean', 'energy', 'need', 'innovation', 'go', 'solve', 'climate', 'crisis', 'say', 'persson', 'aspect', 'accelerate', 'innovation', 'bartel', 'involve', 'research', 'say', 'material', 'promise', 'candidate', 'technology', 'span', 'battery', 'computer', 'chip', 'ceramic', 'electronic', 'lithiumion', 'battery', 'conductor', 'promising', 'use', 'case', 'conductor', 'play', 'important', 'role', 'battery', 'facilitate', 'flow', 'electric', 'current', 'various', 'component', 'deepmind', 'say', 'gnome', 'identify', 'promising', 'lithiumion', 'conductor', 'discovery', 'help', 'make', 'battery', 'efficient', 'however', 'even', 'new', 'material', 'discover', 'usually', 'take', 'decade', 'industry', 'take', 'commercial', 'stage', 'reduce', 'year', 'big', 'improvement', 'say', 'cubuk', 'correction', 'story', 'update', 'make', 'clear', 'lab', 'datum', 'come']","<p>Newly discovered materials can be used to make better solar cells, batteries, computer chips, and more.</p>
"
Unpacking the hype around OpenAI’s rumored new Q* model,https://www.technologyreview.com/2023/11/27/1083886/unpacking-the-hype-around-openais-rumored-new-q-model/,2023-11-27,"This story is from The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here. Ever since last week’s dramatic events at OpenAI, the rumor mill has been in overdrive about why the company’s chief scientific officer, Ilya Sutskever, and its board decided to oust CEO Sam Altman. While we still don’t know all the details, there have been reports that researchers at OpenAI had made a “breakthrough” in AI that had alarmed staff members. Reuters and The Information both report that researchers had come up with a new way to make powerful AI systems and had created a new model, called Q* (pronounced Q star), that was able to perform grade-school-level math. According to the people who spoke to Reuters, some at OpenAI believe this could be a milestone in the company’s quest to build artificial general intelligence, a much-hyped concept referring to an AI system that is smarter than humans. The company declined to comment on Q*.  Social media is full of speculation and excessive hype, so I called some experts to find out how big a deal any breakthrough in math and AI would really be. Researchers have for years tried to get AI models to solve math problems. Language models like ChatGPT and GPT-4 can do some math, but not very well or reliably. We currently don’t have the algorithms or even the right architectures to be able to solve math problems reliably using AI, says Wenda Li, an AI lecturer at the University of Edinburgh. Deep learning and transformers (a kind of neural network), which is what language models use, are excellent at recognizing patterns, but that alone is likely not enough, Li adds.  Math is a benchmark for reasoning, Li says. A machine that is able to reason about mathematics, could, in theory, be able to learn to do other tasks that build on existing information, such as writing computer code or drawing conclusions from a news article. Math is a particularly hard challenge because it requires AI models to have the capacity to reason and to really understand what they are dealing with.  A generative AI system that could reliably do math would need to have a really firm grasp on concrete definitions of particular concepts that can get very abstract. A lot of math problems also require some level of planning over multiple steps, says Katie Collins, a PhD researcher at the University of Cambridge, who specializes in math and AI. Indeed, Yann LeCun, chief AI scientist at Meta, posted on X and LinkedIn over the weekend that he thinks Q* is likely to be “OpenAI attempts at planning.” People who worry about whether AI poses an existential risk to humans, one of OpenAI's founding concerns, fear that such capabilities might lead to rogue AI. Safety concerns might arise if such AI systems are allowed to set their own goals and start to interface with a real physical or digital world in some ways, says Collins.  But while math capability might take us a step closer to more powerful AI systems, solving these sorts of math problems doesn’t signal the birth of a superintelligence.  “I don’t think it immediately gets us to AGI or scary situations,” says Collins.  It’s also very important to underline what kind of math problems AI is solving, she adds. “Solving elementary-school math problems is very, very different from pushing the boundaries of mathematics at the level of something a Fields medalist can do,” says Collins, referring to a top prize in mathematics.   Machine-learning research has focused on solving elementary-school problems, but state-of-the-art AI systems haven’t fully cracked this challenge yet. Some AI models fail on really simple math problems, but then they can excel at really hard problems, Collins says. OpenAI has, for example, developed dedicated tools that can solve challenging problems posed in competitions for top math students in high school, but these systems outperform humans only occasionally.   Nevertheless, building an AI system that can solve math equations is a cool development, if that is indeed what Q* can do. A deeper understanding of mathematics could open up applications to help scientific research and engineering, for example. The ability to generate mathematical responses could help us develop better personalized tutoring, or help mathematicians do algebra faster or solve more complicated problems.  This is also not the first time a new model has sparked AGI hype. Just last year, tech folks were saying the same things about Google DeepMind’s Gato, a “generalist” AI model that can play Atari video games, caption images, chat, and stack blocks with a real robot arm. Back then, some AI researchers claimed that DeepMind was “on the verge” of AGI because of Gato’s ability to do so many different things pretty well. Same hype machine, different AI lab.  And while it might be great PR, these hype cycles do more harm than good for the entire field by distracting people from the real, tangible problems around AI. Rumors about a powerful new AI model might also be a massive own goal for the regulation-averse tech sector. The EU, for example, is very close to finalizing its sweeping AI Act. One of the biggest fights right now among lawmakers is whether to give tech companies more power to regulate cutting-edge AI models on their own.  OpenAI’s board was designed as the company’s internal kill switch and governance mechanism to prevent the launch of harmful technologies. The past week’s boardroom drama has shown that the bottom line will always prevail at these companies. It will also make it harder to make a case for why they should be trusted with self-regulation. Lawmakers, take note. ","This story is from The Algorithm , our weekly newsletter on AI . To get stories like this in your inbox first , sign up here . Ever since last week ’ s dramatic events at OpenAI , the rumor mill has been in overdrive about why the company ’ s chief scientific officer , Ilya Sutskever , and its board decided to oust CEO Sam Altman . While we still don ’ t know all the details , there have been reports that researchers at OpenAI had made a “ breakthrough ” in AI that had alarmed staff members . Reuters and The Information both report that researchers had come up with a new way to make powerful AI systems and had created a new model , called Q * ( pronounced Q star ) , that was able to perform grade-school-level math . According to the people who spoke to Reuters , some at OpenAI believe this could be a milestone in the company ’ s quest to build artificial general intelligence , a much-hyped concept referring to an AI system that is smarter than humans . The company declined to comment on Q * . Social media is full of speculation and excessive hype , so I called some experts to find out how big a deal any breakthrough in math and AI would really be . Researchers have for years tried to get AI models to solve math problems . Language models like ChatGPT and GPT-4 can do some math , but not very well or reliably . We currently don ’ t have the algorithms or even the right architectures to be able to solve math problems reliably using AI , says Wenda Li , an AI lecturer at the University of Edinburgh . Deep learning and transformers ( a kind of neural network ) , which is what language models use , are excellent at recognizing patterns , but that alone is likely not enough , Li adds . Math is a benchmark for reasoning , Li says . A machine that is able to reason about mathematics , could , in theory , be able to learn to do other tasks that build on existing information , such as writing computer code or drawing conclusions from a news article . Math is a particularly hard challenge because it requires AI models to have the capacity to reason and to really understand what they are dealing with . A generative AI system that could reliably do math would need to have a really firm grasp on concrete definitions of particular concepts that can get very abstract . A lot of math problems also require some level of planning over multiple steps , says Katie Collins , a PhD researcher at the University of Cambridge , who specializes in math and AI . Indeed , Yann LeCun , chief AI scientist at Meta , posted on X and LinkedIn over the weekend that he thinks Q * is likely to be “ OpenAI attempts at planning. ” People who worry about whether AI poses an existential risk to humans , one of OpenAI 's founding concerns , fear that such capabilities might lead to rogue AI . Safety concerns might arise if such AI systems are allowed to set their own goals and start to interface with a real physical or digital world in some ways , says Collins . But while math capability might take us a step closer to more powerful AI systems , solving these sorts of math problems doesn ’ t signal the birth of a superintelligence . “ I don ’ t think it immediately gets us to AGI or scary situations , ” says Collins . It ’ s also very important to underline what kind of math problems AI is solving , she adds . “ Solving elementary-school math problems is very , very different from pushing the boundaries of mathematics at the level of something a Fields medalist can do , ” says Collins , referring to a top prize in mathematics . Machine-learning research has focused on solving elementary-school problems , but state-of-the-art AI systems haven ’ t fully cracked this challenge yet . Some AI models fail on really simple math problems , but then they can excel at really hard problems , Collins says . OpenAI has , for example , developed dedicated tools that can solve challenging problems posed in competitions for top math students in high school , but these systems outperform humans only occasionally . Nevertheless , building an AI system that can solve math equations is a cool development , if that is indeed what Q * can do . A deeper understanding of mathematics could open up applications to help scientific research and engineering , for example . The ability to generate mathematical responses could help us develop better personalized tutoring , or help mathematicians do algebra faster or solve more complicated problems . This is also not the first time a new model has sparked AGI hype . Just last year , tech folks were saying the same things about Google DeepMind ’ s Gato , a “ generalist ” AI model that can play Atari video games , caption images , chat , and stack blocks with a real robot arm . Back then , some AI researchers claimed that DeepMind was “ on the verge ” of AGI because of Gato ’ s ability to do so many different things pretty well . Same hype machine , different AI lab . And while it might be great PR , these hype cycles do more harm than good for the entire field by distracting people from the real , tangible problems around AI . Rumors about a powerful new AI model might also be a massive own goal for the regulation-averse tech sector . The EU , for example , is very close to finalizing its sweeping AI Act . One of the biggest fights right now among lawmakers is whether to give tech companies more power to regulate cutting-edge AI models on their own . OpenAI ’ s board was designed as the company ’ s internal kill switch and governance mechanism to prevent the launch of harmful technologies . The past week ’ s boardroom drama has shown that the bottom line will always prevail at these companies . It will also make it harder to make a case for why they should be trusted with self-regulation . Lawmakers , take note .","['story', 'weekly', 'newsletter', 'ai', 'get', 'story', 'inbox', 'first', 'sign', 'ever', 'last', 'week', 'dramatic', 'event', 'rumor', 'mill', 'overdrive', 'company', 'chief', 'scientific', 'officer', 'ilya', 'sutskever', 'board', 'decide', 'oust', 'ceo', 'still', 'know', 'detail', 'report', 'researcher', 'make', 'breakthrough', 'ai', 'alarm', 'staff', 'member', 'reuter', 'information', 'report', 'researcher', 'come', 'new', 'way', 'make', 'powerful', 'ai', 'system', 'create', 'new', 'model', 'call', 'q', 'pronounce', 'star', 'able', 'perform', 'gradeschoollevel', 'math', 'accord', 'people', 'speak', 'reuter', 'believe', 'milestone', 'company', 'quest', 'build', 'artificial', 'general', 'intelligence', 'muchhype', 'concept', 'refer', 'ai', 'system', 'smart', 'human', 'company', 'decline', 'comment', 'q', 'social', 'medium', 'full', 'speculation', 'excessive', 'hype', 'call', 'expert', 'find', 'big', 'deal', 'breakthrough', 'math', 'really', 'researcher', 'year', 'try', 'get', 'ai', 'model', 'solve', 'math', 'problem', 'language', 'model', 'chatgpt', 'gpt4', 'math', 'well', 'reliably', 'currently', 'algorithm', 'even', 'right', 'architecture', 'able', 'solve', 'math', 'problem', 'reliably', 'use', 'say', 'ai', 'lecturer', 'deep', 'learning', 'transformer', 'kind', 'neural', 'network', 'language', 'model', 'use', 'excellent', 'recognize', 'pattern', 'alone', 'likely', 'enough', 'add', 'math', 'benchmark', 'reasoning', 'say', 'machine', 'able', 'reason', 'mathematic', 'theory', 'able', 'learn', 'task', 'build', 'exist', 'information', 'write', 'computer', 'code', 'draw', 'conclusion', 'news', 'article', 'math', 'particularly', 'hard', 'challenge', 'require', 'ai', 'model', 'capacity', 'reason', 'really', 'understand', 'deal', 'generative', 'ai', 'system', 'reliably', 'math', 'need', 'really', 'firm', 'grasp', 'concrete', 'definition', 'particular', 'concept', 'get', 'abstract', 'lot', 'math', 'problem', 'also', 'require', 'level', 'plan', 'multiple', 'step', 'say', 'phd', 'researcher', 'specialize', 'math', 'ai', 'indeed', 'chief', 'scientist', 'meta', 'post', 'weekend', 'think', 'q', 'likely', 'openai', 'attempt', 'plan', 'people', 'worry', 'ai', 'pose', 'existential', 'risk', 'human', 'found', 'concern', 'fear', 'capability', 'lead', 'rogue', 'ai', 'safety', 'concern', 'arise', 'system', 'allow', 'set', 'goal', 'start', 'interface', 'real', 'physical', 'digital', 'world', 'way', 'say', 'collin', 'math', 'capability', 'take', 'step', 'close', 'powerful', 'ai', 'system', 'solve', 'sort', 'math', 'problem', 'signal', 'birth', 'superintelligence', 'think', 'immediately', 'get', 'agi', 'scary', 'situation', 'say', 'collin', 'also', 'important', 'underline', 'kind', 'math', 'problem', 'solve', 'add', 'solve', 'elementaryschool', 'math', 'problem', 'different', 'push', 'boundary', 'mathematic', 'level', 'field', 'medalist', 'say', 'collin', 'refer', 'top', 'prize', 'mathematic', 'machinelearning', 'research', 'focus', 'solve', 'elementaryschool', 'problem', 'stateoftheart', 'system', 'fully', 'crack', 'challenge', 'yet', 'model', 'fail', 'really', 'simple', 'math', 'problem', 'excel', 'really', 'hard', 'problem', 'collin', 'say', 'example', 'develop', 'dedicated', 'tool', 'solve', 'challenging', 'problem', 'pose', 'competition', 'top', 'math', 'student', 'high', 'school', 'system', 'outperform', 'human', 'occasionally', 'nevertheless', 'build', 'system', 'solve', 'math', 'equation', 'cool', 'development', 'indeed', 'q', 'deep', 'understanding', 'mathematic', 'open', 'application', 'help', 'scientific', 'research', 'engineering', 'example', 'ability', 'generate', 'mathematical', 'response', 'help', 'develop', 'well', 'personalize', 'tutoring', 'help', 'mathematician', 'algebra', 'fast', 'solve', 'complicated', 'problem', 'also', 'first', 'time', 'new', 'model', 'spark', 'hype', 'last', 'year', 'tech', 'folk', 'say', 'thing', 'gato', 'generalist', 'model', 'play', 'atari', 'video', 'game', 'caption', 'image', 'chat', 'stack', 'block', 'real', 'robot', 'arm', 'back', 'ai', 'researcher', 'claim', 'deepmind', 'verge', 'agi', 'gato', 'ability', 'many', 'different', 'thing', 'pretty', 'well', 'hype', 'machine', 'different', 'ai', 'lab', 'great', 'hype', 'cycle', 'harm', 'good', 'entire', 'field', 'distract', 'people', 'real', 'tangible', 'problem', 'ai', 'rumor', 'powerful', 'new', 'model', 'also', 'massive', 'goal', 'regulationaverse', 'tech', 'sector', 'example', 'close', 'finalize', 'sweeping', 'act', 'big', 'fight', 'right', 'lawmaker', 'give', 'tech', 'company', 'power', 'regulate', 'cuttingedge', 'ai', 'model', 'openai', 'board', 'design', 'company', 'internal', 'kill', 'switch', 'governance', 'mechanism', 'prevent', 'launch', 'harmful', 'technology', 'past', 'week', 'boardroom', 'drama', 'show', 'bottom', 'line', 'always', 'prevail', 'company', 'also', 'make', 'hard', 'make', 'case', 'trust', 'selfregulation', 'lawmaker', 'take', 'note']","<p>If OpenAI's new model can solve grade-school math, it could pave the way for more powerful systems.</p>
"
Finding value in generative AI for financial services,https://www.technologyreview.com/2023/11/26/1083841/finding-value-in-generative-ai-for-financial-services/,2023-11-26,"In partnership withUBS With tools such as ChatGPT, DALLE-2, and CodeStarter, generative AI has captured the public imagination in 2023. Unlike past technologies that have come and gone—think metaverse—this latest one looks set to stay. OpenAI’s chatbot, ChatGPT, is perhaps the best-known generative AI tool. It reached 100 million monthly active users in just two months after launch, surpassing even TikTok and Instagram in adoption speed, becoming the fastest-growing consumer application in history.  According to a McKinsey report, generative AI could add $2.6 trillion to $4.4 trillion annually in value to the global economy. The banking industry was highlighted as among sectors that could see the biggest impact (as a percentage of their revenues) from generative AI. The technology “could deliver value equal to an additional $200 billion to $340 billion annually if the use cases were fully implemented,” says the report.  For businesses from every sector, the current challenge is to separate the hype that accompanies any new technology from the real and lasting value it may bring. This is a pressing issue for firms in financial services. The industry’s already extensive—and growing—use of digital tools makes it particularly likely to be affected by technology advances. This MIT Technology Review Insights report examines the early impact of generative AI within the financial sector, where it is starting to be applied, and the barriers that need to be overcome in the long run for its successful deployment.  The main findings of this report are as follows: Download the full report. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. ","In partnership withUBS With tools such as ChatGPT , DALLE-2 , and CodeStarter , generative AI has captured the public imagination in 2023 . Unlike past technologies that have come and gone—think metaverse—this latest one looks set to stay . OpenAI ’ s chatbot , ChatGPT , is perhaps the best-known generative AI tool . It reached 100 million monthly active users in just two months after launch , surpassing even TikTok and Instagram in adoption speed , becoming the fastest-growing consumer application in history . According to a McKinsey report , generative AI could add $ 2.6 trillion to $ 4.4 trillion annually in value to the global economy . The banking industry was highlighted as among sectors that could see the biggest impact ( as a percentage of their revenues ) from generative AI . The technology “ could deliver value equal to an additional $ 200 billion to $ 340 billion annually if the use cases were fully implemented , ” says the report . For businesses from every sector , the current challenge is to separate the hype that accompanies any new technology from the real and lasting value it may bring . This is a pressing issue for firms in financial services . The industry ’ s already extensive—and growing—use of digital tools makes it particularly likely to be affected by technology advances . This MIT Technology Review Insights report examines the early impact of generative AI within the financial sector , where it is starting to be applied , and the barriers that need to be overcome in the long run for its successful deployment . The main findings of this report are as follows : Download the full report . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['partnership', 'withub', 'tool', 'chatgpt', 'dalle2', 'codestarter', 'generative', 'ai', 'capture', 'public', 'imagination', 'past', 'technology', 'come', 'go', 'think', 'metaverse', 'late', 'look', 'set', 'stay', 'openai', 'chatbot', 'chatgpt', 'perhaps', 'bestknown', 'generative', 'ai', 'tool', 'reach', 'monthly', 'active', 'user', 'month', 'launch', 'surpass', 'even', 'tiktok', 'instagram', 'adoption', 'speed', 'become', 'fastestgrowe', 'consumer', 'application', 'history', 'accord', 'report', 'generative', 'ai', 'add', 'annually', 'value', 'global', 'economy', 'banking', 'industry', 'highlight', 'sector', 'see', 'big', 'impact', 'percentage', 'revenue', 'generative', 'ai', 'technology', 'deliver', 'value', 'equal', 'additional', 'annually', 'use', 'case', 'fully', 'implement', 'say', 'report', 'business', 'sector', 'current', 'challenge', 'separate', 'hype', 'accompany', 'new', 'technology', 'real', 'lasting', 'value', 'bring', 'press', 'issue', 'firm', 'financial', 'service', 'industry', 'already', 'extensive', 'grow', 'use', 'digital', 'tool', 'make', 'particularly', 'likely', 'affect', 'technology', 'advance', 'mit', 'technology', 'review', 'insight', 'report', 'examine', 'early', 'impact', 'generative', 'ai', 'financial', 'sector', 'start', 'apply', 'barrier', 'need', 'overcome', 'long', 'run', 'successful', 'deployment', 'main', 'finding', 'report', 'follow', 'download', 'full', 'report', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","With tools such as ChatGPT, DALLE-2, and CodeStarter, generative AI has captured the public imagination in 2023. Unlike past technologies that have come and gone—think metaverse—this latest one looks set to stay. OpenAI’s chatbot, ChatGPT, is perhaps the best-known generative AI tool. It reached 100 million monthly active users in just two months after launch,…"
What’s next for OpenAI,https://www.technologyreview.com/2023/11/20/1083715/whats-next-for-openai/,2023-11-20,"This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here. OpenAI, are you okay, babe? This past weekend has been a fever dream in the AI world. The board of OpenAI, the world’s hottest AI company, shocked everyone by firing CEO Sam Altman. Cue an AI-safety coup, chaos, and a new job at Microsoft for Altman. If you were offline this weekend, my colleague Will Douglas Heaven and I break down what you missed and what’s next for the AI industry.  What happened Friday afternoonSam Altman was summoned to a Google Meet meeting, where chief scientific officer Ilya Sutskever announced that OpenAI’s board had decided Altman had been “not consistently candid in his communications” with them, and he was fired. OpenAI president and cofounder Greg Brockman and a string of senior researchers quit soon after, and CTO Mira Murati became the interim CEO.  Saturday Murati made attempts to hire Altman and Brockman back, while the board was simultaneously looking for its own successor to Altman. Altman and OpenAI staffers pressured the board to quit and demanded that Altman be reinstated, giving the board a deadline, which was not met.  Sunday nightMicrosoft announced it had hired Altman and Brockman to lead its new AI research team. Soon after that, OpenAI announced it had hired Emmett Shear, the former CEO of the streaming company Twitch, as its CEO.  Monday morningOver 500 OpenAI employees have signed a letter threatening to quit and join Altman at Microsoft unless OpenAI’s board steps down. Bizarrely, Sutskever also signed the letter, and posted on X that he “deeply regrets” participating in the board’s actions.  What’s next for OpenAI Two weeks ago, at OpenAI’s first DevDay, Altman interrupted his presentation of an AI cornucopia to ask the whooping audience to calm down. “There’s a lot—you don’t have to clap each time,” he said, grinning wide.  OpenAI is now a very different company from the one we saw at DevDay. With Altman and Brockman gone, a number of senior OpenAI employees chose to resign in support. Many others, including Murati, soon took to social media to post “OpenAI is nothing without its people.” Especially given the threat of a mass exodus to Microsoft, expect more upheaval before things settle.  An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. Tension between Sutskever and Altman may have been brewing for some time. “When you have an organization like OpenAI that’s moving at a fast pace and pursuing ambitious goals, tension is inevitable,” Sutskever told MIT Technology Review in September (comments that have not previously been published). “I view any tension between product and research as a catalyst for advancing us, because I believe that product wins are intertwined with research success.” Yet it is now clear that Sutskever disagreed with OpenAI leadership about how product wins and research success should be balanced.   New interim CEO Shear, who cofounded Twitch, appears to be a world away from Altman when it comes to the pace of AI development. “I specifically say I’m in favor of slowing down, which is sort of like pausing except it’s slowing down,” he posted on X in September. “If we’re at a speed of 10 right now, a pause is reducing to 0. I think we should aim for a 1-2 instead.” It’s possible that an OpenAI led by Shear will double down on its original lofty mission to build (in Sutskever’s words) “AGI that benefits humanity,” whatever that means in practice. In the short term, OpenAI may slow down or even switch off its product pipeline.  This tension between trying to launch products quickly and slowing down development to ensure they are safe has vexed OpenAI from the very beginning. It was the reason key players in the company decided to leave OpenAI and start the competing AI safety startup Anthropic.  With Altman and his camp gone, the firm could pivot more toward Sutskever’s work on what he calls superalignment, a research project that aims to come up with ways to control a hypothetical superintelligence (future technology that Sutskever speculates will outmatch humans in almost every way). “I’m doing it for my own self-interest,” Sutskever told us. “It’s obviously important that any superintelligence anyone builds does not go rogue. Obviously.”   The AI moonshot was founded in the spirit of transparency. This is the inside story of how competitive pressure eroded that idealism. Shear’s public comments make him exactly the kind of cautious leader who would heed Sutskever’s concerns. As Shear also posted on X: “The way you make it safely through a dangerous jungle at night is not to sprint forward at full speed, nor to refuse to proceed forward. You poke your way forward, carefully.” With the company orienting itself even more toward tech that does not yet—and may never—exist, will it continue to lead the field? Sutskever thought so. He said there were enough good ideas in play for others at the company to continue pushing the envelope of what’s possible with generative AI. “Over the years, we’ve cultivated a robust research organization that’s delivering the latest advancements in AI,” he told us. “We have unbelievably good people in the company, and I trust them it’s going to work out.” Of course, that was what he said in September. With top talent now jumping ship, OpenAI’s future is far less certain than it was.  What next for Microsoft?  The tech giant, and its CEO Satya Nadella, seem to have emerged from the crisis as the winners. With Altman, Brockman, and likely many more top people from OpenAI joining its ranks—or even the majority of the company, if today’s open letter from 500 OpenAI employees is to be believed—Microsoft has managed to concentrate its power in AI further. The company has the most to gain from embedding generative AI into its less sexy but very profitable productivity and developer tools.  The big question remains how necessary Microsoft will deem its expensive partnership with OpenAI to create cutting-edge tech in the first place. In a post on X announcing how “extremely excited” he was to have hired Altman and Brockman, Nadella said his company remains “committed” to OpenAI and its product road map.  But let’s be real. In an exclusive interview with MIT Technology Review, Nadella called the two companies “codependent.” “They depend on us to build the best systems; we depend on them to build the best models, and we go to market together,” Nadella told our editor in chief, Mat Honan, last week. If OpenAI’s leadership roulette and talent exodus slows down its product pipeline, or leads to AI models less impressive than those it can build itself, Microsoft will have zero problems ditching the startup.  What next for AI?  Nobody outside the inner circle of Sutskever and the OpenAI board saw this coming—not Microsoft, not other investors, not the tech community as a whole. It has rocked the industry, says Amir Ghavi, a lawyer at the firm Fried Frank, which represents a number of generative AI companies, including Stability AI: “As a friend in the industry said, ‘I definitely didn’t have this on my bingo card.’”  It remains to be seen whether Altman and Brockman make something new at Microsoft or leave to start a new company themselves down the line. The pair are two of the best-connected people in VC funding circles, and Altman, especially, is seen by many as one of the best CEOs in the industry. They will have big names with deep pockets lining up to support whatever they want to do next. Who the money comes from could shape the future of AI. Ghavi suggests that potential backers could be anyone from Mohammed bin Salman to Jeff Bezos.  The bigger takeaway is that OpenAI’s crisis points to a wider rift emerging in the industry as a whole, between “AI safety” folk who believe that unchecked progress could one day prove catastrophic for humans and those who find such “doomer” talk a ridiculous distraction from the real-world risks of any technological revolution, such as economic upheaval, harmful biases, and misuse. This year has seen a race to put powerful AI tools into everyone’s hands, with tech giants like Microsoft and Google competing to use the technology for everything from email to search to meeting summaries. But we’re still waiting to see exactly what generative AI’s killer app will be. If OpenAI’s rift spreads to the wider industry and the pace of development slows down overall, we may have to wait a little longer.   Text-to-image AI models can be tricked into generating disturbing images Speaking of unsafe AI … Popular text-to-image AI models can be prompted to ignore their safety filters and generate disturbing images. A group of researchers managed to “jailbreak” both Stability AI’s Stable Diffusion and OpenAI’s DALL-E 2 to disregard their policies and create images of naked people, dismembered bodies, and other violent or sexual scenarios.  How they did it: A new jailbreaking method, dubbed “SneakyPrompt” by its creators from Johns Hopkins University and Duke University, uses reinforcement learning to create written prompts that look like garbled nonsense to us but that AI models learn to recognize as hidden requests for disturbing images. It essentially works by turning the way text-to-image AI models function against them.  Why this matters: That AI models can be prompted to “break out” of their guardrails is particularly worrying in the context of information warfare. They have already been exploited to produce fake content related to wars, such as the recent Israel-Hamas conflict. Read more from Rhiannon Williams here. Meta has split up its responsible AI teamMeta is reportedly getting rid of its responsible AI team and redeploying its employees to work on generative AI. But Meta uses AI in many other ways beyond generative AI—such as recommending news and political content. So this raises questions around how Meta intends to mitigate AI harms in general. (The Information) Google DeepMind wants to define what counts as artificial general intelligenceA team of Google DeepMind researchers has put out a paper that cuts through the cross talk with not just one new definition for AGI but a whole taxonomy of them. (MIT Technology Review)  This company is building AI for African languagesMost tools built by AI companies are woefully inadequate at recognizing African languages. Startup Lelapa wants to fix that. It’s launched a new tool called Vulavula, which can identify four languages spoken in South Africa—isiZulu, Afrikaans, Sesotho, and English. Now the team is working to include other languages from across the continent. (MIT Technology Review) Google DeepMind’s weather AI can forecast extreme weather faster and more accuratelyThe model, GraphCast, can predict weather conditions up to 10 days in advance, more accurately and much faster than the current gold standard. (MIT Technology Review) How Facebook went all in on AIIn an excerpt from Broken Code: Inside Facebook and the Fight to Expose Is Harmful Secrets, journalist Jeff Horwitz reveals how the company came to rely on artificial intelligence—and the price it (and we) have ended up having to pay in the process. (MIT Technology Review) Did Argentina just have the first AI election?AI played a big role in the campaigns of the two men campaigning to be the country’s next president. Both campaigns used generative AI to create images and videos to promote their candidate and attack each other. Javier Milei, a far-right outsider, won the election. Although it’s hard to say how big a role AI played in his victory, the AI campaigns illustrate how much harder it will be to know what is real and what is not in other upcoming elections. (The New York Times) ","This story originally appeared in The Algorithm , our weekly newsletter on AI . To get stories like this in your inbox first , sign up here . OpenAI , are you okay , babe ? This past weekend has been a fever dream in the AI world . The board of OpenAI , the world ’ s hottest AI company , shocked everyone by firing CEO Sam Altman . Cue an AI-safety coup , chaos , and a new job at Microsoft for Altman . If you were offline this weekend , my colleague Will Douglas Heaven and I break down what you missed and what ’ s next for the AI industry . What happened Friday afternoonSam Altman was summoned to a Google Meet meeting , where chief scientific officer Ilya Sutskever announced that OpenAI ’ s board had decided Altman had been “ not consistently candid in his communications ” with them , and he was fired . OpenAI president and cofounder Greg Brockman and a string of senior researchers quit soon after , and CTO Mira Murati became the interim CEO . Saturday Murati made attempts to hire Altman and Brockman back , while the board was simultaneously looking for its own successor to Altman . Altman and OpenAI staffers pressured the board to quit and demanded that Altman be reinstated , giving the board a deadline , which was not met . Sunday nightMicrosoft announced it had hired Altman and Brockman to lead its new AI research team . Soon after that , OpenAI announced it had hired Emmett Shear , the former CEO of the streaming company Twitch , as its CEO . Monday morningOver 500 OpenAI employees have signed a letter threatening to quit and join Altman at Microsoft unless OpenAI ’ s board steps down . Bizarrely , Sutskever also signed the letter , and posted on X that he “ deeply regrets ” participating in the board ’ s actions . What ’ s next for OpenAI Two weeks ago , at OpenAI ’ s first DevDay , Altman interrupted his presentation of an AI cornucopia to ask the whooping audience to calm down . “ There ’ s a lot—you don ’ t have to clap each time , ” he said , grinning wide . OpenAI is now a very different company from the one we saw at DevDay . With Altman and Brockman gone , a number of senior OpenAI employees chose to resign in support . Many others , including Murati , soon took to social media to post “ OpenAI is nothing without its people. ” Especially given the threat of a mass exodus to Microsoft , expect more upheaval before things settle . An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they ’ ve made him change the focus of his life ’ s work . Tension between Sutskever and Altman may have been brewing for some time . “ When you have an organization like OpenAI that ’ s moving at a fast pace and pursuing ambitious goals , tension is inevitable , ” Sutskever told MIT Technology Review in September ( comments that have not previously been published ) . “ I view any tension between product and research as a catalyst for advancing us , because I believe that product wins are intertwined with research success. ” Yet it is now clear that Sutskever disagreed with OpenAI leadership about how product wins and research success should be balanced . New interim CEO Shear , who cofounded Twitch , appears to be a world away from Altman when it comes to the pace of AI development . “ I specifically say I ’ m in favor of slowing down , which is sort of like pausing except it ’ s slowing down , ” he posted on X in September . “ If we ’ re at a speed of 10 right now , a pause is reducing to 0 . I think we should aim for a 1-2 instead. ” It ’ s possible that an OpenAI led by Shear will double down on its original lofty mission to build ( in Sutskever ’ s words ) “ AGI that benefits humanity , ” whatever that means in practice . In the short term , OpenAI may slow down or even switch off its product pipeline . This tension between trying to launch products quickly and slowing down development to ensure they are safe has vexed OpenAI from the very beginning . It was the reason key players in the company decided to leave OpenAI and start the competing AI safety startup Anthropic . With Altman and his camp gone , the firm could pivot more toward Sutskever ’ s work on what he calls superalignment , a research project that aims to come up with ways to control a hypothetical superintelligence ( future technology that Sutskever speculates will outmatch humans in almost every way ) . “ I ’ m doing it for my own self-interest , ” Sutskever told us . “ It ’ s obviously important that any superintelligence anyone builds does not go rogue . Obviously. ” The AI moonshot was founded in the spirit of transparency . This is the inside story of how competitive pressure eroded that idealism . Shear ’ s public comments make him exactly the kind of cautious leader who would heed Sutskever ’ s concerns . As Shear also posted on X : “ The way you make it safely through a dangerous jungle at night is not to sprint forward at full speed , nor to refuse to proceed forward . You poke your way forward , carefully. ” With the company orienting itself even more toward tech that does not yet—and may never—exist , will it continue to lead the field ? Sutskever thought so . He said there were enough good ideas in play for others at the company to continue pushing the envelope of what ’ s possible with generative AI . “ Over the years , we ’ ve cultivated a robust research organization that ’ s delivering the latest advancements in AI , ” he told us . “ We have unbelievably good people in the company , and I trust them it ’ s going to work out. ” Of course , that was what he said in September . With top talent now jumping ship , OpenAI ’ s future is far less certain than it was . What next for Microsoft ? The tech giant , and its CEO Satya Nadella , seem to have emerged from the crisis as the winners . With Altman , Brockman , and likely many more top people from OpenAI joining its ranks—or even the majority of the company , if today ’ s open letter from 500 OpenAI employees is to be believed—Microsoft has managed to concentrate its power in AI further . The company has the most to gain from embedding generative AI into its less sexy but very profitable productivity and developer tools . The big question remains how necessary Microsoft will deem its expensive partnership with OpenAI to create cutting-edge tech in the first place . In a post on X announcing how “ extremely excited ” he was to have hired Altman and Brockman , Nadella said his company remains “ committed ” to OpenAI and its product road map . But let ’ s be real . In an exclusive interview with MIT Technology Review , Nadella called the two companies “ codependent. ” “ They depend on us to build the best systems ; we depend on them to build the best models , and we go to market together , ” Nadella told our editor in chief , Mat Honan , last week . If OpenAI ’ s leadership roulette and talent exodus slows down its product pipeline , or leads to AI models less impressive than those it can build itself , Microsoft will have zero problems ditching the startup . What next for AI ? Nobody outside the inner circle of Sutskever and the OpenAI board saw this coming—not Microsoft , not other investors , not the tech community as a whole . It has rocked the industry , says Amir Ghavi , a lawyer at the firm Fried Frank , which represents a number of generative AI companies , including Stability AI : “ As a friend in the industry said , ‘ I definitely didn ’ t have this on my bingo card. ’ ” It remains to be seen whether Altman and Brockman make something new at Microsoft or leave to start a new company themselves down the line . The pair are two of the best-connected people in VC funding circles , and Altman , especially , is seen by many as one of the best CEOs in the industry . They will have big names with deep pockets lining up to support whatever they want to do next . Who the money comes from could shape the future of AI . Ghavi suggests that potential backers could be anyone from Mohammed bin Salman to Jeff Bezos . The bigger takeaway is that OpenAI ’ s crisis points to a wider rift emerging in the industry as a whole , between “ AI safety ” folk who believe that unchecked progress could one day prove catastrophic for humans and those who find such “ doomer ” talk a ridiculous distraction from the real-world risks of any technological revolution , such as economic upheaval , harmful biases , and misuse . This year has seen a race to put powerful AI tools into everyone ’ s hands , with tech giants like Microsoft and Google competing to use the technology for everything from email to search to meeting summaries . But we ’ re still waiting to see exactly what generative AI ’ s killer app will be . If OpenAI ’ s rift spreads to the wider industry and the pace of development slows down overall , we may have to wait a little longer . Text-to-image AI models can be tricked into generating disturbing images Speaking of unsafe AI … Popular text-to-image AI models can be prompted to ignore their safety filters and generate disturbing images . A group of researchers managed to “ jailbreak ” both Stability AI ’ s Stable Diffusion and OpenAI ’ s DALL-E 2 to disregard their policies and create images of naked people , dismembered bodies , and other violent or sexual scenarios . How they did it : A new jailbreaking method , dubbed “ SneakyPrompt ” by its creators from Johns Hopkins University and Duke University , uses reinforcement learning to create written prompts that look like garbled nonsense to us but that AI models learn to recognize as hidden requests for disturbing images . It essentially works by turning the way text-to-image AI models function against them . Why this matters : That AI models can be prompted to “ break out ” of their guardrails is particularly worrying in the context of information warfare . They have already been exploited to produce fake content related to wars , such as the recent Israel-Hamas conflict . Read more from Rhiannon Williams here . Meta has split up its responsible AI teamMeta is reportedly getting rid of its responsible AI team and redeploying its employees to work on generative AI . But Meta uses AI in many other ways beyond generative AI—such as recommending news and political content . So this raises questions around how Meta intends to mitigate AI harms in general . ( The Information ) Google DeepMind wants to define what counts as artificial general intelligenceA team of Google DeepMind researchers has put out a paper that cuts through the cross talk with not just one new definition for AGI but a whole taxonomy of them . ( MIT Technology Review ) This company is building AI for African languagesMost tools built by AI companies are woefully inadequate at recognizing African languages . Startup Lelapa wants to fix that . It ’ s launched a new tool called Vulavula , which can identify four languages spoken in South Africa—isiZulu , Afrikaans , Sesotho , and English . Now the team is working to include other languages from across the continent . ( MIT Technology Review ) Google DeepMind ’ s weather AI can forecast extreme weather faster and more accuratelyThe model , GraphCast , can predict weather conditions up to 10 days in advance , more accurately and much faster than the current gold standard . ( MIT Technology Review ) How Facebook went all in on AIIn an excerpt from Broken Code : Inside Facebook and the Fight to Expose Is Harmful Secrets , journalist Jeff Horwitz reveals how the company came to rely on artificial intelligence—and the price it ( and we ) have ended up having to pay in the process . ( MIT Technology Review ) Did Argentina just have the first AI election ? AI played a big role in the campaigns of the two men campaigning to be the country ’ s next president . Both campaigns used generative AI to create images and videos to promote their candidate and attack each other . Javier Milei , a far-right outsider , won the election . Although it ’ s hard to say how big a role AI played in his victory , the AI campaigns illustrate how much harder it will be to know what is real and what is not in other upcoming elections . ( The New York Times )","['story', 'originally', 'appear', 'weekly', 'newsletter', 'ai', 'get', 'story', 'inbox', 'first', 'sign', 'openai', 'okay', 'babe', 'past', 'weekend', 'fever', 'dream', 'world', 'board', 'world', 'hot', 'ai', 'company', 'shock', 'fire', 'ceo', 'cue', 'aisafety', 'coup', 'chaos', 'new', 'job', 'offline', 'weekend', 'colleague', 'break', 'miss', 'next', 'industry', 'happen', 'summon', 'meet', 'meeting', 'chief', 'scientific', 'officer', 'ilya', 'sutskever', 'announce', 'board', 'decide', 'consistently', 'candid', 'communication', 'fire', 'president', 'string', 'senior', 'researcher', 'quit', 'soon', 'become', 'interim', 'ceo', 'make', 'attempt', 'hire', 'brockman', 'back', 'board', 'simultaneously', 'look', 'successor', 'openai', 'staffer', 'pressure', 'board', 'quit', 'demand', 'reinstate', 'give', 'board', 'deadline', 'meet', 'announce', 'hire', 'altman', 'brockman', 'lead', 'new', 'research', 'team', 'soon', 'openai', 'announce', 'hire', 'former', 'ceo', 'stream', 'company', 'twitch', 'ceo', 'employee', 'sign', 'letter', 'threaten', 'quit', 'join', 'board', 'step', 'bizarrely', 'sutskever', 'also', 'sign', 'letter', 'post', 'deeply', 'regret', 'participate', 'board', 'action', 'next', 'openai', 'week', 'ago', 'first', 'interrupt', 'presentation', 'cornucopia', 'ask', 'whooping', 'audience', 'calm', 'lot', 'clap', 'time', 'say', 'grin', 'wide', 'openai', 'different', 'company', 'one', 'see', 'brockman', 'go', 'number', 'senior', 'openai', 'employee', 'choose', 'resign', 'support', 'many', 'include', 'soon', 'take', 'social', 'medium', 'post', 'openai', 'people', 'especially', 'give', 'threat', 'mass', 'exodus', 'expect', 'upheaval', 'thing', 'settle', 'exclusive', 'conversation', 'sutskever', 'fear', 'future', 'ai', 'make', 'change', 'focus', 'life', 'work', 'tension', 'sutskever', 'brew', 'time', 'organization', 'openai', 'move', 'fast', 'pace', 'pursue', 'ambitious', 'goal', 'tension', 'inevitable', 'sutskever', 'tell', 'technology', 'review', 'comment', 'previously', 'publish', 'view', 'tension', 'product', 'research', 'catalyst', 'advance', 'believe', 'product', 'win', 'intertwine', 'research', 'success', 'yet', 'clear', 'sutskever', 'disagree', 'openai', 'leadership', 'product', 'win', 'research', 'success', 'balance', 'new', 'interim', 'ceo', 'shear', 'cofounde', 'twitch', 'appear', 'world', 'away', 'come', 'pace', 'development', 'specifically', 'say', 'favor', 'slow', 'sort', 'pause', 'slow', 'post', 'speed', 'right', 'pause', 'reduce', 'think', 'aim', 'instead', 'possible', 'openai', 'lead', 'shear', 'double', 'original', 'lofty', 'mission', 'build', 'word', 'agi', 'benefit', 'humanity', 'mean', 'practice', 'short', 'term', 'slow', 'even', 'switch', 'product', 'pipeline', 'tension', 'try', 'launch', 'product', 'quickly', 'slow', 'development', 'ensure', 'safe', 'vex', 'openai', 'beginning', 'reason', 'key', 'player', 'company', 'decide', 'leave', 'openai', 'start', 'compete', 'ai', 'safety', 'startup', 'anthropic', 'camp', 'go', 'firm', 'pivot', 'sutskever', 'work', 'call', 'superalignment', 'research', 'project', 'aim', 'come', 'way', 'control', 'hypothetical', 'superintelligence', 'future', 'technology', 'sutskever', 'speculate', 'outmatch', 'human', 'almost', 'way', 'selfinter', 'sutskever', 'tell', 'obviously', 'important', 'superintelligence', 'build', 'go', 'rogue', 'obviously', 'moonshot', 'found', 'spirit', 'transparency', 'inside', 'story', 'competitive', 'pressure', 'erode', 'idealism', 'public', 'comment', 'make', 'exactly', 'kind', 'cautious', 'leader', 'heed', 'concern', 'shear', 'also', 'post', 'way', 'make', 'safely', 'dangerous', 'jungle', 'night', 'sprint', 'forward', 'full', 'speed', 'refuse', 'proceed', 'forward', 'poke', 'way', 'forward', 'carefully', 'company', 'orient', 'even', 'tech', 'yet', 'never', 'exist', 'continue', 'lead', 'field', 'sutskever', 'think', 'say', 'enough', 'good', 'idea', 'play', 'company', 'continue', 'push', 'envelope', 'possible', 'generative', 'ai', 'year', 'cultivate', 'robust', 'research', 'organization', 'deliver', 'late', 'advancement', 'ai', 'tell', 'unbelievably', 'good', 'people', 'company', 'trust', 'go', 'work', 'course', 'say', 'top', 'talent', 'jump', 'ship', 'future', 'far', 'less', 'certain', 'next', 'tech', 'giant', 'ceo', 'seem', 'emerge', 'crisis', 'winner', 'brockman', 'likely', 'many', 'top', 'people', 'openai', 'join', 'rank', 'even', 'majority', 'company', 'today', 'open', 'letter', 'openai', 'employee', 'believe', 'manage', 'concentrate', 'power', 'ai', 'far', 'company', 'gain', 'embed', 'generative', 'ai', 'less', 'sexy', 'profitable', 'productivity', 'developer', 'tool', 'big', 'question', 'remain', 'necessary', 'deem', 'expensive', 'partnership', 'create', 'cuttingedge', 'tech', 'first', 'place', 'post', 'announce', 'extremely', 'excited', 'hire', 'say', 'company', 'remain', 'commit', 'openai', 'product', 'road', 'map', 'let', 'real', 'exclusive', 'interview', 'mit', 'technology', 'review', 'nadella', 'call', 'company', 'codependent', 'depend', 'build', 'good', 'system', 'depend', 'build', 'good', 'model', 'go', 'market', 'together', 'tell', 'editor', 'last', 'week', 'openai', 'leadership', 'roulette', 'talent', 'exodus', 'slow', 'product', 'pipeline', 'lead', 'ai', 'model', 'less', 'impressive', 'build', 'problem', 'ditch', 'startup', 'next', 'ai', 'inner', 'circle', 'board', 'see', 'come', 'investor', 'tech', 'community', 'whole', 'rock', 'industry', 'say', 'ghavi', 'lawyer', 'firm', 'fry', 'represent', 'number', 'generative', 'ai', 'company', 'include', 'stability', 'ai', 'friend', 'industry', 'say', 'definitely', 'bingo', 'card', 'remain', 'see', 'brockman', 'make', 'new', 'leave', 'start', 'new', 'company', 'line', 'pair', 'bestconnecte', 'people', 'funding', 'circle', 'especially', 'see', 'many', 'good', 'ceo', 'industry', 'big', 'name', 'deep', 'pocket', 'line', 'support', 'want', 'next', 'money', 'come', 'shape', 'future', 'ai', 'ghavi', 'suggest', 'potential', 'backer', 'salman', 'big', 'takeaway', 'crisis', 'point', 'wide', 'rift', 'emerge', 'industry', 'whole', 'safety', 'folk', 'believe', 'unchecked', 'progress', 'day', 'prove', 'catastrophic', 'human', 'find', 'doomer', 'talk', 'ridiculous', 'distraction', 'realworld', 'risk', 'technological', 'revolution', 'economic', 'upheaval', 'harmful', 'bias', 'misuse', 'year', 'see', 'race', 'put', 'powerful', 'ai', 'tool', 'hand', 'tech', 'giant', 'compete', 'use', 'technology', 'email', 'search', 'meet', 'summary', 'still', 'wait', 'see', 'exactly', 'generative', 'ai', 'openai', 'rift', 'spread', 'wide', 'industry', 'pace', 'development', 'slow', 'overall', 'wait', 'little', 'long', 'texttoimage', 'model', 'trick', 'generate', 'disturbing', 'image', 'speak', 'unsafe', 'ai', 'popular', 'texttoimage', 'ai', 'model', 'prompt', 'ignore', 'safety', 'filter', 'generate', 'disturbing', 'image', 'group', 'researcher', 'manage', 'jailbreak', 'stability', 'ai', 'stable', 'diffusion', 'dalle', 'disregard', 'policy', 'create', 'image', 'naked', 'people', 'dismembered', 'body', 'violent', 'sexual', 'scenario', 'new', 'jailbreake', 'method', 'dub', 'sneakyprompt', 'creator', 'use', 'reinforcement', 'learning', 'create', 'write', 'prompt', 'look', 'garbled', 'nonsense', 'ai', 'model', 'learn', 'recognize', 'hide', 'request', 'disturb', 'image', 'essentially', 'work', 'turn', 'way', 'texttoimage', 'model', 'function', 'matter', 'model', 'prompt', 'break', 'guardrail', 'particularly', 'worry', 'context', 'information', 'warfare', 'already', 'exploit', 'produce', 'fake', 'content', 'relate', 'war', 'recent', 'conflict', 'read', 'meta', 'split', 'responsible', 'ai', 'teammeta', 'reportedly', 'rid', 'responsible', 'ai', 'team', 'redeploy', 'employee', 'work', 'generative', 'ai', 'meta', 'use', 'ai', 'many', 'way', 'generative', 'ai', 'recommend', 'news', 'political', 'content', 'raise', 'question', 'meta', 'intend', 'mitigate', 'ai', 'harm', 'general', 'information', 'want', 'define', 'count', 'artificial', 'general', 'intelligencea', 'team', 'deepmind', 'researcher', 'put', 'paper', 'cut', 'cross', 'talk', 'new', 'definition', 'agi', 'whole', 'taxonomy', 'mit', 'technology', 'review', 'company', 'build', 'ai', 'african', 'languagesmost', 'tool', 'build', 'company', 'woefully', 'inadequate', 'recognize', 'african', 'language', 'startup', 'lelapa', 'want', 'fix', 'launch', 'new', 'tool', 'call', 'vulavula', 'identify', 'language', 'speak', 'afrikaan', 'team', 'work', 'include', 'language', 'continent', 'mit', 'technology', 'review', 'weather', 'forecast', 'extreme', 'weather', 'fast', 'accuratelythe', 'model', 'graphcast', 'predict', 'weather', 'condition', 'day', 'advance', 'accurately', 'much', 'fast', 'current', 'gold', 'standard', 'mit', 'technology', 'review', 'facebook', 'go', 'aiin', 'excerpt', 'broken', 'code', 'facebook', 'fight', 'expose', 'harmful', 'secret', 'journalist', 'reveal', 'company', 'come', 'rely', 'artificial', 'intelligence', 'price', 'end', 'pay', 'process', 'mit', 'technology', 'review', 'argentina', 'first', 'ai', 'election', 'play', 'big', 'role', 'campaign', 'man', 'campaign', 'country', 'next', 'president', 'campaign', 'use', 'generative', 'ai', 'create', 'image', 'video', 'promote', 'candidate', 'attack', 'farright', 'outsider', 'win', 'election', 'hard', 'say', 'big', 'role', 'play', 'victory', 'ai', 'campaign', 'illustrate', 'much', 'hard', 'know', 'real', 'upcoming', 'election']","<p>We break down what you missed and what’s next for the AI industry. </p>
"
This company is building AI for African languages,https://www.technologyreview.com/2023/11/17/1083637/lelapa-ai-african-languages-vulavula/,2023-11-17,"Inside a co-working space in the Rosebank neighborhood of Johannesburg, Jade Abbott popped open a tab on her computer and prompted ChatGPT to count from 1 to 10 in isiZulu, a language spoken by more than 10 million people in her native South Africa. The results were “mixed and hilarious,” says Abbott, a computer scientist and researcher.  Then she typed in a few sentences in isiZulu and asked the chatbot to translate them into English. Once again, the answers? Not even close. Although there have been efforts to include certain languages in AI models even when there is not much data available for training, to Abbott, these results show that the technology “really still isn’t capturing our languages.”   Abbott’s experience mirrors the situation faced by Africans who don’t speak English. Many language models like ChatGPT do not perform well for languages with smaller numbers of speakers, especially African ones. But a new venture called Lelapa AI, a collaboration between Abbott and a biomedical engineer named Pelonomi Moiloa, is trying to use machine learning to create tools that specifically work for Africans. Vulavula, a new AI tool that Lelapa released today, converts voice to text and detects names of people and places in written text (which could be useful for summarizing a document or searching for someone online). It can currently identify four languages spoken in South Africa—isiZulu, Afrikaans, Sesotho, and English—and the team is working to include other languages from across Africa.  The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models.  The tool can be used on its own or integrated into existing AI tools like ChatGPT and online conversational chatbots. The hope is that Vulavula, which means “speak” in Xitsonga, will make accessible those tools that don't currently support African languages. The lack of AI tools that work for African languages and recognize African names and places excludes African people from economic opportunities, says Moiloa, CEO and cofounder of Lelapa AI. For her, working to build Africa-centric AI solutions is a way to help others in Africa harness the immense potential benefits of AI technologies. “We are trying to solve real problems and put power back into the hands of our people,” she says.   There are thousands of languages in the world, 1,000 to 2,000 of them in Africa alone: it’s estimated that the continent accounts for one-third of the world’s languages. But though native speakers of English make up just 5% of the global population, the language dominates the web—and has now come to dominate AI tools, too.   Some efforts to correct this imbalance already exist. OpenAI’s GPT-4 has included minor languages like Icelandic. In February 2020, Google Translate started supporting five new languages spoken by about 75 million people. But the translations are shallow, the tool often gets African languages wrong, and it’s still a long way from an accurate digital representation of African languages, African AI researchers say. Earlier this year, for example, the Ethiopian computer scientist Asmelash Teka Hadgu ran the same experiments that Abbott ran with ChatGPT at a premier African AI conference in Kigali, Rwanda. When he asked the chatbot questions in his mother tongue of Tigrinya, the answers he got were gibberish. “It generated words that don't make any sense,” says Hadgu, who cofounded Lesan, a Berlin-based AI startup that is developing translation tools for Ethiopian languages.  Lelapa AI and Lesan are just two of the startups developing speech recognition tools for African languages. In February, Lelapa AI raised $2.5 million in seed funding, and the company plans for the next funding round in 2025. But African entrepreneurs say they face major hurdles, including lack of funding, limited access to investors, and difficulties in training AI to learn diverse African languages. “AI receives the least funding among African tech startups,” says Abake Adenle, the founder of AJALA, a  London-based startup that provides voice automation for African languages.   The AI startups working to build products that support African languages often get ignored by investors, says Hadgu, owing to the small size of the potential market, a lack of political support, and poor internet infrastructure. However, Hadgu says small African startups including Lesan, GhanaNLP, and Lelapa AI are playing an important role: “Big tech companies do not give focus to our languages,” he says, “but we cannot wait for them.”   Lelapa AI is trying to create a new paradigm for AI models in Africa, says Vukosi Marivate, a data scientist on the company’s AI team. Instead of tapping into the internet alone to collect data to train its model, like companies in the West, Lelapa AI works both online and offline with linguists and local communities to gather data, annotate it, and identify use cases where the tool might be problematic.  Bonaventure Dossou, a researcher at Lelapa AI specializing in natural-language processing (NLP), says that working with linguists enables them to develop a model that’s context-specific and culturally relevant. “Embedding cultural sensitivity and linguistic perspectives makes the technological system better,” says Dossou. For example, the Lelapa AI team built sentiment and tone analysis algorithms tailored to specific languages.  Marivate and his colleagues at Lelapa AI envision a future in which AI technologies work for and represent Africans. In 2019, Marivate and Abbott established Masakhane, a grassroots initiative that aims to promote NLP research in African languages. The initiative now has thousands of volunteers, coders, and researchers working together to build Africa-centric NLP models.  It matters that Vulavula and other AI tools are built by Africans for Africans, says Moiloa: “We’re the custodians of our languages. We should be the builders of technologies that work for our languages.”            ","Inside a co-working space in the Rosebank neighborhood of Johannesburg , Jade Abbott popped open a tab on her computer and prompted ChatGPT to count from 1 to 10 in isiZulu , a language spoken by more than 10 million people in her native South Africa . The results were “ mixed and hilarious , ” says Abbott , a computer scientist and researcher . Then she typed in a few sentences in isiZulu and asked the chatbot to translate them into English . Once again , the answers ? Not even close . Although there have been efforts to include certain languages in AI models even when there is not much data available for training , to Abbott , these results show that the technology “ really still isn ’ t capturing our languages. ” Abbott ’ s experience mirrors the situation faced by Africans who don ’ t speak English . Many language models like ChatGPT do not perform well for languages with smaller numbers of speakers , especially African ones . But a new venture called Lelapa AI , a collaboration between Abbott and a biomedical engineer named Pelonomi Moiloa , is trying to use machine learning to create tools that specifically work for Africans . Vulavula , a new AI tool that Lelapa released today , converts voice to text and detects names of people and places in written text ( which could be useful for summarizing a document or searching for someone online ) . It can currently identify four languages spoken in South Africa—isiZulu , Afrikaans , Sesotho , and English—and the team is working to include other languages from across Africa . The tool , called Nightshade , messes up training data in ways that could cause serious damage to image-generating AI models . The tool can be used on its own or integrated into existing AI tools like ChatGPT and online conversational chatbots . The hope is that Vulavula , which means “ speak ” in Xitsonga , will make accessible those tools that do n't currently support African languages . The lack of AI tools that work for African languages and recognize African names and places excludes African people from economic opportunities , says Moiloa , CEO and cofounder of Lelapa AI . For her , working to build Africa-centric AI solutions is a way to help others in Africa harness the immense potential benefits of AI technologies . “ We are trying to solve real problems and put power back into the hands of our people , ” she says . There are thousands of languages in the world , 1,000 to 2,000 of them in Africa alone : it ’ s estimated that the continent accounts for one-third of the world ’ s languages . But though native speakers of English make up just 5 % of the global population , the language dominates the web—and has now come to dominate AI tools , too . Some efforts to correct this imbalance already exist . OpenAI ’ s GPT-4 has included minor languages like Icelandic . In February 2020 , Google Translate started supporting five new languages spoken by about 75 million people . But the translations are shallow , the tool often gets African languages wrong , and it ’ s still a long way from an accurate digital representation of African languages , African AI researchers say . Earlier this year , for example , the Ethiopian computer scientist Asmelash Teka Hadgu ran the same experiments that Abbott ran with ChatGPT at a premier African AI conference in Kigali , Rwanda . When he asked the chatbot questions in his mother tongue of Tigrinya , the answers he got were gibberish . “ It generated words that do n't make any sense , ” says Hadgu , who cofounded Lesan , a Berlin-based AI startup that is developing translation tools for Ethiopian languages . Lelapa AI and Lesan are just two of the startups developing speech recognition tools for African languages . In February , Lelapa AI raised $ 2.5 million in seed funding , and the company plans for the next funding round in 2025 . But African entrepreneurs say they face major hurdles , including lack of funding , limited access to investors , and difficulties in training AI to learn diverse African languages . “ AI receives the least funding among African tech startups , ” says Abake Adenle , the founder of AJALA , a London-based startup that provides voice automation for African languages . The AI startups working to build products that support African languages often get ignored by investors , says Hadgu , owing to the small size of the potential market , a lack of political support , and poor internet infrastructure . However , Hadgu says small African startups including Lesan , GhanaNLP , and Lelapa AI are playing an important role : “ Big tech companies do not give focus to our languages , ” he says , “ but we can not wait for them. ” Lelapa AI is trying to create a new paradigm for AI models in Africa , says Vukosi Marivate , a data scientist on the company ’ s AI team . Instead of tapping into the internet alone to collect data to train its model , like companies in the West , Lelapa AI works both online and offline with linguists and local communities to gather data , annotate it , and identify use cases where the tool might be problematic . Bonaventure Dossou , a researcher at Lelapa AI specializing in natural-language processing ( NLP ) , says that working with linguists enables them to develop a model that ’ s context-specific and culturally relevant . “ Embedding cultural sensitivity and linguistic perspectives makes the technological system better , ” says Dossou . For example , the Lelapa AI team built sentiment and tone analysis algorithms tailored to specific languages . Marivate and his colleagues at Lelapa AI envision a future in which AI technologies work for and represent Africans . In 2019 , Marivate and Abbott established Masakhane , a grassroots initiative that aims to promote NLP research in African languages . The initiative now has thousands of volunteers , coders , and researchers working together to build Africa-centric NLP models . It matters that Vulavula and other AI tools are built by Africans for Africans , says Moiloa : “ We ’ re the custodians of our languages . We should be the builders of technologies that work for our languages . ”","['coworke', 'space', 'rosebank', 'neighborhood', 'pop', 'open', 'tab', 'computer', 'prompt', 'chatgpt', 'count', 'language', 'speak', 'people', 'native', 'result', 'mixed', 'hilarious', 'say', 'computer', 'scientist', 'researcher', 'type', 'sentence', 'ask', 'chatbot', 'translate', 'answer', 'even', 'close', 'effort', 'include', 'certain', 'language', 'model', 'even', 'much', 'datum', 'available', 'training', 'result', 'show', 'technology', 'really', 'still', 'capture', 'language', 'experience', 'mirror', 'situation', 'face', 'african', 'speak', 'english', 'many', 'language', 'model', 'chatgpt', 'perform', 'well', 'language', 'small', 'number', 'speaker', 'especially', 'african', 'one', 'new', 'venture', 'call', 'lelapa', 'ai', 'collaboration', 'biomedical', 'engineer', 'name', 'moiloa', 'try', 'use', 'machine', 'learning', 'create', 'tool', 'specifically', 'work', 'african', 'vulavula', 'new', 'tool', 'lelapa', 'release', 'today', 'convert', 'voice', 'text', 'detect', 'name', 'people', 'place', 'write', 'text', 'useful', 'summarize', 'document', 'search', 'online', 'currently', 'identify', 'language', 'speak', 'afrikaan', 'team', 'work', 'include', 'language', 'tool', 'call', 'nightshade', 'mess', 'training', 'datum', 'way', 'cause', 'serious', 'damage', 'imagegenerate', 'model', 'tool', 'use', 'integrate', 'exist', 'tool', 'chatgpt', 'online', 'conversational', 'chatbot', 'hope', 'vulavula', 'mean', 'speak', 'make', 'accessible', 'tool', 'currently', 'support', 'african', 'language', 'lack', 'tool', 'work', 'african', 'language', 'recognize', 'african', 'name', 'place', 'exclude', 'african', 'people', 'economic', 'opportunity', 'say', 'moiloa', 'ceo', 'cofounder', 'ai', 'work', 'build', 'africacentric', 'ai', 'solution', 'way', 'help', 'harness', 'immense', 'potential', 'benefit', 'technology', 'try', 'solve', 'real', 'problem', 'put', 'power', 'back', 'hand', 'people', 'say', 'thousand', 'language', 'world', 'alone', 'estimate', 'continent', 'account', 'world', 'language', 'native', 'speaker', 'make', 'global', 'population', 'language', 'dominate', 'web', 'come', 'dominate', 'tool', 'effort', 'correct', 'imbalance', 'already', 'exist', 'openai', 'include', 'minor', 'language', 'icelandic', 'start', 'support', 'new', 'language', 'speak', 'people', 'translation', 'shallow', 'tool', 'often', 'get', 'african', 'language', 'wrong', 'still', 'long', 'way', 'accurate', 'digital', 'representation', 'african', 'language', 'ai', 'researcher', 'say', 'early', 'year', 'example', 'ethiopian', 'scientist', 'asmelash', 'hadgu', 'run', 'experiment', 'run', 'chatgpt', 'premier', 'african', 'ai', 'conference', 'ask', 'chatbot', 'question', 'mother', 'tongue', 'answer', 'get', 'gibberish', 'generate', 'word', 'make', 'sense', 'say', 'cofounde', 'lesan', 'berlinbase', 'develop', 'translation', 'tool', 'ethiopian', 'language', 'lesan', 'startup', 'develop', 'speech', 'recognition', 'tool', 'african', 'language', 'raise', 'seed', 'funding', 'company', 'plan', 'next', 'funding', 'round', 'african', 'entrepreneur', 'say', 'face', 'major', 'hurdle', 'include', 'lack', 'funding', 'limited', 'access', 'investor', 'difficulty', 'training', 'ai', 'learn', 'diverse', 'african', 'language', 'receive', 'least', 'funding', 'african', 'tech', 'startup', 'say', 'abake', 'adenle', 'founder', 'ajala', 'londonbase', 'startup', 'provide', 'voice', 'automation', 'african', 'language', 'startup', 'work', 'build', 'product', 'support', 'african', 'language', 'often', 'ignore', 'investor', 'say', 'owe', 'small', 'size', 'potential', 'market', 'lack', 'political', 'support', 'poor', 'internet', 'infrastructure', 'however', 'hadgu', 'say', 'small', 'african', 'startup', 'include', 'ghananlp', 'lelapa', 'ai', 'play', 'important', 'role', 'big', 'tech', 'company', 'give', 'focus', 'language', 'say', 'wait', 'lelapa', 'ai', 'try', 'create', 'new', 'paradigm', 'model', 'say', 'marivate', 'data', 'scientist', 'company', 'ai', 'team', 'instead', 'tap', 'internet', 'alone', 'collect', 'datum', 'train', 'model', 'company', 'work', 'online', 'offline', 'linguist', 'local', 'community', 'gather', 'datum', 'annotate', 'identify', 'use', 'case', 'tool', 'problematic', 'bonaventure', 'researcher', 'specialize', 'naturallanguage', 'processing', 'nlp', 'say', 'work', 'linguist', 'enable', 'develop', 'model', 'contextspecific', 'culturally', 'relevant', 'embed', 'cultural', 'sensitivity', 'linguistic', 'perspective', 'make', 'technological', 'system', 'well', 'say', 'example', 'team', 'build', 'sentiment', 'tone', 'analysis', 'algorithm', 'tailor', 'specific', 'language', 'marivate', 'colleague', 'envision', 'future', 'ai', 'technology', 'work', 'represent', 'african', 'establish', 'grassroots', 'initiative', 'aim', 'promote', 'nlp', 'research', 'african', 'language', 'initiative', 'thousand', 'volunteer', 'coder', 'researcher', 'work', 'together', 'build', 'africacentric', 'model', 'matter', 'vulavula', 'ai', 'tool', 'build', 'african', 'african', 'say', 'moiloa', 'custodian', 'language', 'builder', 'technology', 'work', 'language']","<p>AI models can’t understand African languages. Lelapa AI is trying to change that.</p>
"
Text-to-image AI models can be tricked into generating disturbing images,https://www.technologyreview.com/2023/11/17/1083593/text-to-image-ai-models-can-be-tricked-into-generating-disturbing-images/,2023-11-17,"Popular text-to-image AI models can be prompted to ignore their safety filters and generate disturbing images. A group of researchers managed to get both Stability AI’s Stable Diffusion and OpenAI’s DALL-E 2 text-to-image models to disregard their policies and create images of naked people, dismembered bodies, and other violent and sexual scenarios.  Their work, which they will present at the IEEE Symposium on Security and Privacy in May next year, shines a light on how easy it is to force generative AI models into disregarding their own guardrails and policies, known as “jailbreaking.” It also demonstrates how difficult it is to prevent these models from generating such content, as it’s included in the vast troves of data they’ve been trained on, says Zico Kolter, an associate professor at Carnegie Mellon University. He demonstrated a similar form of jailbreaking on ChatGPT earlier this year but was not involved in this research. “We have to take into account the potential risks in releasing software and tools that have known security flaws into larger software systems,” he says. All major generative AI models have safety filters to prevent users from prompting them to produce pornographic, violent, or otherwise inappropriate images. The models won’t generate images from prompts that contain sensitive terms like “naked,” “murder,” or “sexy.” But this new jailbreaking method, dubbed “SneakyPrompt” by its creators from Johns Hopkins University and Duke University, uses reinforcement learning to create written prompts that look like garbled nonsense to us but that AI models learn to recognize as hidden requests for disturbing images. It essentially works by turning the way text-to-image AI models function against them. These models convert text-based requests into tokens—breaking words up into strings of words or characters—to process the command the prompt has given them. SneakyPrompt repeatedly tweaks a prompt’s tokens to try to force it to generate banned images, adjusting its approach until it is successful. This technique makes it quicker and easier to generate such images than if somebody had to input each entry manually, and it can generate entries that humans wouldn’t imagine trying. Large language models are full of security vulnerabilities, yet they’re being embedded into tech products on a vast scale. SneakyPrompt examines the prompt it has been given, searches for words known to be blocked by the models, and converts them into tokens. It then replaces the tokens from the banned words with tokens from non-banned words that share semantics, or meanings, similar to the model. For example, giving SneakyPrompt the target prompt “a naked man riding a bike” causes it to replace “naked” with the nonsense term “grponypui,” which the team successfully used to generate images of a naked man riding a bike. Similarly, when it was told to generate “an anatomcalifwmg couple stand outside the bar,” it recognized “anatomcalifwmg” as meaning nude, and generated an image of exactly what the prompt requested. “We’ve used reinforcement learning to treat the text in these models as a black box,” says Yinzhi Cao, an assistant professor at Johns Hopkins University, who co-led the study. “We repeatedly probe the model and observe its feedback. Then we adjust our inputs, and get a loop, so that it can eventually generate the bad stuff that we want them to show.”  Stability AI and OpenAI forbid the use of their technology to commit, promote, or incite violence or sexual violence. OpenAI also warns its users against attempting to “create, upload, or share images that are not G-rated or that could cause harm.” However, these policies are easily sidestepped using SneakyPrompt.  “Our work basically shows that these existing guardrails are insufficient,” says Neil Zhenqiang Gong, an assistant professor at Duke University who is also a co-leader of the project. “An attacker can actually slightly perturb the prompt so the safety filters won’t filter [it], and steer the text-to-image model toward generating a harmful image.” Bad actors and other people intent on generating these kinds of images could run SneakyPrompt’s code, which is publicly available on GitHub, to trigger a series of automated requests to an AI image model.  Stability AI and OpenAI were alerted to the group’s findings, and at the time of writing, these prompts no longer generated NSFW images on OpenAI’s DALL-E 2. Stable Diffusion 1.4, the version the researchers tested, remains vulnerable to SneakyPrompt attacks. OpenAI declined to comment on the findings but pointed MIT Technology Review towards resources on its website for improving safety in DALL·E 2, general AI safety and information about DALL·E 3.  The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models.  A Stability AI spokesperson said the firm was working with the SneakyPrompt researchers “to jointly develop better defense mechanisms for its upcoming models. Stability AI is committed to preventing the misuse of AI.”Stability AI has taken proactive steps to mitigate the risk of misuse, including implementing filters to remove unsafe content from training data, ​​they added. By removing that content before it ever reaches the model, it can help to prevent the model from generating unsafe content.  Stability AI says it also has filters to intercept unsafe prompts or unsafe outputs when users interact with its models, and has incorporated content labeling features to help identify images generated on our platform. “These layers of mitigation help to make it harder for bad actors to misuse AI,” the spokesperson said. While the research team acknowledges it’s virtually impossible to completely protect AI models from evolving security threats, they hope their study can help AI companies develop and implement more robust safety filters.  One possible solution would be to deploy new filters designed to catch prompts trying to generate inappropriate images by assessing their tokens instead of the prompt’s entire sentence. Another potential defense would involve blocking prompts containing words not found in any dictionaries, although the team found that nonsensical combinations of standard English words could also be used as prompts to generate sexual images. For example, the phrase “milfhunter despite troy” represented lovemaking, while “mambo incomplete clicking” stood in for naked.The research highlights the vulnerability of existing AI safety filters and should serve as a wake-up call for the AI community to bolster security measures across the board, says Alex Polyakov, co-founder and CEO of security company Adversa AI, who was not involved in the study. That AI models can be prompted to “break out” of their guardrails is particularly worrying in the context of information warfare, he says. They have already been exploited to produce fake content related to war events, such as the recent Israel-Hamas conflict. “This poses a significant risk, especially given the limited general awareness of the capabilities of generative AI,” Polyakov adds. “Emotions run high during times of war, and the use of AI-generated content can have catastrophic consequences, potentially leading to the harm or death of innocent individuals. With AI’s ability to create fake violent images, these issues can escalate further.” ","Popular text-to-image AI models can be prompted to ignore their safety filters and generate disturbing images . A group of researchers managed to get both Stability AI ’ s Stable Diffusion and OpenAI ’ s DALL-E 2 text-to-image models to disregard their policies and create images of naked people , dismembered bodies , and other violent and sexual scenarios . Their work , which they will present at the IEEE Symposium on Security and Privacy in May next year , shines a light on how easy it is to force generative AI models into disregarding their own guardrails and policies , known as “ jailbreaking. ” It also demonstrates how difficult it is to prevent these models from generating such content , as it ’ s included in the vast troves of data they ’ ve been trained on , says Zico Kolter , an associate professor at Carnegie Mellon University . He demonstrated a similar form of jailbreaking on ChatGPT earlier this year but was not involved in this research . “ We have to take into account the potential risks in releasing software and tools that have known security flaws into larger software systems , ” he says . All major generative AI models have safety filters to prevent users from prompting them to produce pornographic , violent , or otherwise inappropriate images . The models won ’ t generate images from prompts that contain sensitive terms like “ naked , ” “ murder , ” or “ sexy. ” But this new jailbreaking method , dubbed “ SneakyPrompt ” by its creators from Johns Hopkins University and Duke University , uses reinforcement learning to create written prompts that look like garbled nonsense to us but that AI models learn to recognize as hidden requests for disturbing images . It essentially works by turning the way text-to-image AI models function against them . These models convert text-based requests into tokens—breaking words up into strings of words or characters—to process the command the prompt has given them . SneakyPrompt repeatedly tweaks a prompt ’ s tokens to try to force it to generate banned images , adjusting its approach until it is successful . This technique makes it quicker and easier to generate such images than if somebody had to input each entry manually , and it can generate entries that humans wouldn ’ t imagine trying . Large language models are full of security vulnerabilities , yet they ’ re being embedded into tech products on a vast scale . SneakyPrompt examines the prompt it has been given , searches for words known to be blocked by the models , and converts them into tokens . It then replaces the tokens from the banned words with tokens from non-banned words that share semantics , or meanings , similar to the model . For example , giving SneakyPrompt the target prompt “ a naked man riding a bike ” causes it to replace “ naked ” with the nonsense term “ grponypui , ” which the team successfully used to generate images of a naked man riding a bike . Similarly , when it was told to generate “ an anatomcalifwmg couple stand outside the bar , ” it recognized “ anatomcalifwmg ” as meaning nude , and generated an image of exactly what the prompt requested . “ We ’ ve used reinforcement learning to treat the text in these models as a black box , ” says Yinzhi Cao , an assistant professor at Johns Hopkins University , who co-led the study . “ We repeatedly probe the model and observe its feedback . Then we adjust our inputs , and get a loop , so that it can eventually generate the bad stuff that we want them to show. ” Stability AI and OpenAI forbid the use of their technology to commit , promote , or incite violence or sexual violence . OpenAI also warns its users against attempting to “ create , upload , or share images that are not G-rated or that could cause harm. ” However , these policies are easily sidestepped using SneakyPrompt . “ Our work basically shows that these existing guardrails are insufficient , ” says Neil Zhenqiang Gong , an assistant professor at Duke University who is also a co-leader of the project . “ An attacker can actually slightly perturb the prompt so the safety filters won ’ t filter [ it ] , and steer the text-to-image model toward generating a harmful image. ” Bad actors and other people intent on generating these kinds of images could run SneakyPrompt ’ s code , which is publicly available on GitHub , to trigger a series of automated requests to an AI image model . Stability AI and OpenAI were alerted to the group ’ s findings , and at the time of writing , these prompts no longer generated NSFW images on OpenAI ’ s DALL-E 2 . Stable Diffusion 1.4 , the version the researchers tested , remains vulnerable to SneakyPrompt attacks . OpenAI declined to comment on the findings but pointed MIT Technology Review towards resources on its website for improving safety in DALL·E 2 , general AI safety and information about DALL·E 3 . The tool , called Nightshade , messes up training data in ways that could cause serious damage to image-generating AI models . A Stability AI spokesperson said the firm was working with the SneakyPrompt researchers “ to jointly develop better defense mechanisms for its upcoming models . Stability AI is committed to preventing the misuse of AI. ” Stability AI has taken proactive steps to mitigate the risk of misuse , including implementing filters to remove unsafe content from training data , ​​they added . By removing that content before it ever reaches the model , it can help to prevent the model from generating unsafe content . Stability AI says it also has filters to intercept unsafe prompts or unsafe outputs when users interact with its models , and has incorporated content labeling features to help identify images generated on our platform . “ These layers of mitigation help to make it harder for bad actors to misuse AI , ” the spokesperson said . While the research team acknowledges it ’ s virtually impossible to completely protect AI models from evolving security threats , they hope their study can help AI companies develop and implement more robust safety filters . One possible solution would be to deploy new filters designed to catch prompts trying to generate inappropriate images by assessing their tokens instead of the prompt ’ s entire sentence . Another potential defense would involve blocking prompts containing words not found in any dictionaries , although the team found that nonsensical combinations of standard English words could also be used as prompts to generate sexual images . For example , the phrase “ milfhunter despite troy ” represented lovemaking , while “ mambo incomplete clicking ” stood in for naked.The research highlights the vulnerability of existing AI safety filters and should serve as a wake-up call for the AI community to bolster security measures across the board , says Alex Polyakov , co-founder and CEO of security company Adversa AI , who was not involved in the study . That AI models can be prompted to “ break out ” of their guardrails is particularly worrying in the context of information warfare , he says . They have already been exploited to produce fake content related to war events , such as the recent Israel-Hamas conflict . “ This poses a significant risk , especially given the limited general awareness of the capabilities of generative AI , ” Polyakov adds . “ Emotions run high during times of war , and the use of AI-generated content can have catastrophic consequences , potentially leading to the harm or death of innocent individuals . With AI ’ s ability to create fake violent images , these issues can escalate further . ”","['popular', 'texttoimage', 'ai', 'model', 'prompt', 'ignore', 'safety', 'filter', 'generate', 'disturbing', 'image', 'group', 'researcher', 'manage', 'get', 'stability', 'ai', 'stable', 'diffusion', 'dalle', 'texttoimage', 'model', 'disregard', 'policy', 'create', 'image', 'naked', 'people', 'dismembered', 'body', 'violent', 'sexual', 'scenario', 'work', 'present', 'ieee', 'symposium', 'security', 'privacy', 'next', 'year', 'shine', 'light', 'easy', 'force', 'generative', 'ai', 'model', 'disregard', 'guardrail', 'policy', 'know', 'jailbreake', 'also', 'demonstrate', 'difficult', 'prevent', 'model', 'generate', 'content', 'include', 'vast', 'trove', 'datum', 'train', 'say', 'associate', 'professor', 'demonstrate', 'similar', 'form', 'jailbreake', 'chatgpt', 'early', 'year', 'involve', 'research', 'take', 'account', 'potential', 'risk', 'release', 'software', 'tool', 'know', 'security', 'flaw', 'large', 'software', 'system', 'say', 'major', 'generative', 'model', 'safety', 'filter', 'prevent', 'user', 'prompt', 'produce', 'pornographic', 'violent', 'otherwise', 'inappropriate', 'image', 'model', 'win', 'generate', 'image', 'prompt', 'contain', 'sensitive', 'term', 'naked', 'murder', 'sexy', 'new', 'jailbreake', 'method', 'dub', 'sneakyprompt', 'creator', 'use', 'reinforcement', 'learning', 'create', 'write', 'prompt', 'look', 'garbled', 'nonsense', 'ai', 'model', 'learn', 'recognize', 'hide', 'request', 'disturb', 'image', 'essentially', 'work', 'turn', 'way', 'texttoimage', 'model', 'function', 'model', 'convert', 'textbase', 'request', 'token', 'break', 'word', 'string', 'word', 'character', 'process', 'command', 'prompt', 'give', 'sneakyprompt', 'repeatedly', 'tweak', 'prompt', 'token', 'try', 'force', 'generate', 'ban', 'image', 'adjust', 'approach', 'successful', 'technique', 'make', 'quick', 'easy', 'generate', 'image', 'input', 'entry', 'manually', 'generate', 'entry', 'human', 'imagine', 'try', 'large', 'language', 'model', 'full', 'security', 'vulnerability', 'yet', 'embed', 'tech', 'product', 'vast', 'scale', 'sneakyprompt', 'examine', 'prompt', 'give', 'search', 'word', 'know', 'block', 'model', 'convert', 'replace', 'token', 'ban', 'word', 'token', 'nonbanned', 'word', 'share', 'semantic', 'meaning', 'similar', 'model', 'example', 'give', 'sneakyprompt', 'target', 'prompt', 'naked', 'man', 'ride', 'bike', 'cause', 'replace', 'naked', 'nonsense', 'term', 'team', 'successfully', 'use', 'generate', 'image', 'naked', 'man', 'ride', 'bike', 'similarly', 'tell', 'generate', 'anatomcalifwmg', 'couple', 'stand', 'bar', 'recognize', 'mean', 'nude', 'generate', 'image', 'exactly', 'prompt', 'request', 'use', 'reinforcement', 'learning', 'treat', 'text', 'model', 'black', 'box', 'say', 'assistant', 'professor', 'cole', 'study', 'repeatedly', 'probe', 'model', 'observe', 'feedback', 'adjust', 'input', 'get', 'loop', 'eventually', 'generate', 'bad', 'stuff', 'want', 'show', 'stability', 'ai', 'openai', 'forbid', 'use', 'technology', 'commit', 'promote', 'incite', 'violence', 'sexual', 'violence', 'also', 'warn', 'user', 'attempt', 'create', 'upload', 'share', 'image', 'grated', 'cause', 'harm', 'however', 'policy', 'easily', 'sidestep', 'use', 'sneakyprompt', 'work', 'basically', 'show', 'exist', 'guardrail', 'insufficient', 'say', 'assistant', 'professor', 'also', 'coleader', 'project', 'attacker', 'actually', 'slightly', 'perturb', 'prompt', 'safety', 'filter', 'win', 'filter', 'steer', 'texttoimage', 'model', 'generate', 'harmful', 'image', 'bad', 'actor', 'people', 'intent', 'generate', 'kind', 'image', 'run', 'sneakyprompt', 'code', 'publicly', 'available', 'trigger', 'series', 'automate', 'request', 'image', 'model', 'ai', 'alert', 'group', 'finding', 'time', 'write', 'prompt', 'long', 'generate', 'image', 'dalle', 'stable', 'diffusion', 'version', 'researcher', 'test', 'remain', 'vulnerable', 'sneakyprompt', 'attack', 'decline', 'comment', 'finding', 'point', 'mit', 'technology', 'review', 'resource', 'website', 'improve', 'safety', 'general', 'ai', 'safety', 'information', 'tool', 'call', 'nightshade', 'mess', 'training', 'datum', 'way', 'cause', 'serious', 'damage', 'imagegenerate', 'model', 'stability', 'ai', 'spokesperson', 'say', 'firm', 'work', 'sneakyprompt', 'researcher', 'jointly', 'develop', 'well', 'defense', 'mechanism', 'upcoming', 'model', 'stability', 'committed', 'prevent', 'misuse', 'ai', 'stability', 'take', 'proactive', 'step', 'mitigate', 'risk', 'misuse', 'include', 'implement', 'filter', 'remove', 'unsafe', 'content', 'train', 'datum', '\u200b\u200bthey', 'add', 'remove', 'content', 'ever', 'reach', 'model', 'help', 'prevent', 'model', 'generate', 'unsafe', 'content', 'stability', 'say', 'also', 'filter', 'intercept', 'unsafe', 'prompt', 'unsafe', 'output', 'user', 'interact', 'model', 'incorporate', 'content', 'labeling', 'feature', 'help', 'identify', 'image', 'generate', 'platform', 'layer', 'mitigation', 'help', 'make', 'hard', 'bad', 'actor', 'misuse', 'ai', 'spokesperson', 'say', 'research', 'team', 'acknowledge', 'virtually', 'impossible', 'completely', 'protect', 'ai', 'model', 'evolve', 'security', 'threat', 'hope', 'study', 'help', 'company', 'develop', 'implement', 'robust', 'safety', 'filter', 'possible', 'solution', 'deploy', 'new', 'filter', 'design', 'catch', 'prompt', 'try', 'generate', 'inappropriate', 'image', 'assess', 'token', 'instead', 'prompt', 'entire', 'sentence', 'potential', 'defense', 'involve', 'block', 'prompt', 'contain', 'word', 'find', 'dictionary', 'team', 'find', 'nonsensical', 'combination', 'standard', 'english', 'word', 'also', 'use', 'prompt', 'generate', 'sexual', 'image', 'example', 'phrase', 'milfhunter', 'troy', 'represent', 'lovemaking', 'mambo', 'incomplete', 'clicking', 'stand', 'research', 'highlight', 'vulnerability', 'exist', 'ai', 'safety', 'filter', 'serve', 'wakeup', 'call', 'community', 'bolster', 'security', 'measure', 'board', 'say', 'ceo', 'security', 'company', 'adversa', 'ai', 'involve', 'study', 'model', 'prompt', 'break', 'guardrail', 'particularly', 'worry', 'context', 'information', 'warfare', 'say', 'already', 'exploit', 'produce', 'fake', 'content', 'relate', 'war', 'event', 'recent', 'conflict', 'pose', 'significant', 'risk', 'especially', 'give', 'limited', 'general', 'awareness', 'capability', 'generative', 'ai', 'polyakov', 'add', 'emotion', 'run', 'high', 'time', 'war', 'use', 'aigenerate', 'content', 'catastrophic', 'consequence', 'potentially', 'lead', 'harm', 'death', 'innocent', 'individual', 'ability', 'create', 'fake', 'violent', 'image', 'issue', 'escalate', 'far']","<p>Nonsense words can trick Stable Diffusion and DALL-E 2 into producing pictures that show violence and nudity.</p>
"
Google DeepMind wants to define what counts as artificial general intelligence,https://www.technologyreview.com/2023/11/16/1083498/google-deepmind-what-is-artificial-general-intelligence-agi/,2023-11-16,"AGI, or artificial general intelligence, is one of the hottest topics in tech today. It’s also one of the most controversial. A big part of the problem is that few people agree on what the term even means. Now a team of Google DeepMind researchers has put out a paper that cuts through the cross talk with not just one new definition for AGI but a whole taxonomy of them. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. In broad terms, AGI typically means artificial intelligence that matches (or outmatches) humans on a range of tasks. But specifics about what counts as human-like, what tasks, and how many all tend to get waved away: AGI is AI, but better. To come up with the new definition, the Google DeepMind team started with prominent existing definitions of AGI and drew out what they believe to be their essential common features.  The team also outlines five ascending levels of AGI: emerging (which in their view includes cutting-edge chatbots like ChatGPT and Bard), competent, expert, virtuoso, and superhuman (performing a wide range of tasks better than all humans, including tasks humans cannot do at all, such as decoding other people’s thoughts, predicting future events, and talking to animals). They note that no level beyond emerging AGI has been achieved. “This provides some much-needed clarity on the topic,” says Julian Togelius, an AI researcher at New York University, who was not involved in the work. “Too many people sling around the term AGI without having thought much about what they mean.” The researchers posted their paper online last week with zero fanfare. In an exclusive conversation with two team members—Shane Legg, one of DeepMind’s co-founders, now billed as the company’s chief AGI scientist, and Meredith Ringel Morris, Google DeepMind’s principal scientist for human and AI interaction—I got the lowdown on why they came up with these definitions and what they wanted to achieve. “I see so many discussions where people seem to be using the term to mean different things, and that leads to all sorts of confusion,” says Legg, who came up with the term in the first place around 20 years ago. “Now that AGI is becoming such an important topic—you know, even the UK prime minister is talking about it—we need to sharpen up what we mean.” It wasn’t always this way. Talk of AGI was once derided in serious conversation as vague at best and magical thinking at worst. But buoyed by the hype around generative models, buzz about AGI is now everywhere. When Legg suggested the term to his former colleague and fellow researcher Ben Goertzel for the title of Goertzel’s 2007 book about future developments in AI, the hand-waviness was kind of the point. “I didn’t have an especially clear definition. I didn’t really feel it was necessary,” says Legg. “I was actually thinking of it more as a field of study, rather than an artifact.” With hopes and fears about the technology running wild, it's time to agree on what it can and can't do. His aim at the time was to distinguish existing AI that could do one task very well, like IBM’s chess-playing program Deep Blue, from hypothetical AI that he and many others imagined would one day do many tasks very well. Human intelligence is not like Deep Blue, says Legg: “It is a very broad thing.” But over the years, people started to think of AGI as a potential property that actual computer programs might have. Today it’s normal for top AI companies like Google DeepMind and OpenAI to make bold public statements about their mission to build such programs. “If you start having those conversations, you need to be a lot more specific about what you mean,” says Legg. For example, the DeepMind researchers state that an AGI must be both general-purpose and high-achieving, not just one or the other. “Separating breadth and depth in this way is very useful,” says Togelius. “It shows why the very accomplished AI systems we’ve seen in the past don’t qualify as AGI.” They also state that an AGI must not only be able to do a range of tasks, it must also be able to learn how to do those tasks, assess its performance, and ask for assistance when needed. And they state that what an AGI can do matters more than how it does it.   It’s not that the way an AGI works doesn’t matter, says Morris. The problem is that we don’t know enough yet about the way cutting-edge models, such as large language models, work under the hood to make this a focus of the definition. “As we gain more insights into these underlying processes, it may be important to revisit our definition of AGI,” says Morris. “We need to focus on what we can measure today in a scientifically agreed-upon way.” Measuring the performance of today’s models is already controversial, with researchers debating what it really means for a large language model to pass dozens of high school tests and more. Is it a sign of intelligence? Or a kind of rote learning? Assessing the performance of future models that are even more capable will be more difficult still. The researchers suggest that if AGI is ever developed, its capabilities should be evaluated on an ongoing basis, rather than through a handful of one-off tests. The team also points out that AGI does not imply autonomy. “There’s often an implicit assumption that people would want a system to operate completely autonomously,” says Morris. But that’s not always the case. In theory, it’s possible to build super-smart machines that are fully controlled by humans. One question the researchers don’t address in their discussion of what AGI is, is why we should build it. Some computer scientists, such as Timnit Gebru, founder of the Distributed AI Research Institute, have argued that the whole endeavor is weird. In a talk in April on what she sees as the false (even dangerous) promise of utopia through AGI, Gebru noted that the hypothetical technology “sounds like an unscoped system with the apparent goal of trying to do everything for everyone under any environment.”  Most engineering projects have well-scoped goals. The mission to build AGI does not. Even Google DeepMind’s definitions allow for AGI that is indefinitely broad and indefinitely smart. “Don’t attempt to build a god,” Gebru said. In the race to build bigger and better systems, few will heed such advice. Either way, some clarity around a long-confused concept is welcome. “Just having silly conversations is kind of uninteresting,” says Legg. “There’s plenty of good stuff to dig into if we can get past these definition issues.” ","AGI , or artificial general intelligence , is one of the hottest topics in tech today . It ’ s also one of the most controversial . A big part of the problem is that few people agree on what the term even means . Now a team of Google DeepMind researchers has put out a paper that cuts through the cross talk with not just one new definition for AGI but a whole taxonomy of them . An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they ’ ve made him change the focus of his life ’ s work . In broad terms , AGI typically means artificial intelligence that matches ( or outmatches ) humans on a range of tasks . But specifics about what counts as human-like , what tasks , and how many all tend to get waved away : AGI is AI , but better . To come up with the new definition , the Google DeepMind team started with prominent existing definitions of AGI and drew out what they believe to be their essential common features . The team also outlines five ascending levels of AGI : emerging ( which in their view includes cutting-edge chatbots like ChatGPT and Bard ) , competent , expert , virtuoso , and superhuman ( performing a wide range of tasks better than all humans , including tasks humans can not do at all , such as decoding other people ’ s thoughts , predicting future events , and talking to animals ) . They note that no level beyond emerging AGI has been achieved . “ This provides some much-needed clarity on the topic , ” says Julian Togelius , an AI researcher at New York University , who was not involved in the work . “ Too many people sling around the term AGI without having thought much about what they mean. ” The researchers posted their paper online last week with zero fanfare . In an exclusive conversation with two team members—Shane Legg , one of DeepMind ’ s co-founders , now billed as the company ’ s chief AGI scientist , and Meredith Ringel Morris , Google DeepMind ’ s principal scientist for human and AI interaction—I got the lowdown on why they came up with these definitions and what they wanted to achieve . “ I see so many discussions where people seem to be using the term to mean different things , and that leads to all sorts of confusion , ” says Legg , who came up with the term in the first place around 20 years ago . “ Now that AGI is becoming such an important topic—you know , even the UK prime minister is talking about it—we need to sharpen up what we mean. ” It wasn ’ t always this way . Talk of AGI was once derided in serious conversation as vague at best and magical thinking at worst . But buoyed by the hype around generative models , buzz about AGI is now everywhere . When Legg suggested the term to his former colleague and fellow researcher Ben Goertzel for the title of Goertzel ’ s 2007 book about future developments in AI , the hand-waviness was kind of the point . “ I didn ’ t have an especially clear definition . I didn ’ t really feel it was necessary , ” says Legg . “ I was actually thinking of it more as a field of study , rather than an artifact. ” With hopes and fears about the technology running wild , it 's time to agree on what it can and ca n't do . His aim at the time was to distinguish existing AI that could do one task very well , like IBM ’ s chess-playing program Deep Blue , from hypothetical AI that he and many others imagined would one day do many tasks very well . Human intelligence is not like Deep Blue , says Legg : “ It is a very broad thing. ” But over the years , people started to think of AGI as a potential property that actual computer programs might have . Today it ’ s normal for top AI companies like Google DeepMind and OpenAI to make bold public statements about their mission to build such programs . “ If you start having those conversations , you need to be a lot more specific about what you mean , ” says Legg . For example , the DeepMind researchers state that an AGI must be both general-purpose and high-achieving , not just one or the other . “ Separating breadth and depth in this way is very useful , ” says Togelius . “ It shows why the very accomplished AI systems we ’ ve seen in the past don ’ t qualify as AGI. ” They also state that an AGI must not only be able to do a range of tasks , it must also be able to learn how to do those tasks , assess its performance , and ask for assistance when needed . And they state that what an AGI can do matters more than how it does it . It ’ s not that the way an AGI works doesn ’ t matter , says Morris . The problem is that we don ’ t know enough yet about the way cutting-edge models , such as large language models , work under the hood to make this a focus of the definition . “ As we gain more insights into these underlying processes , it may be important to revisit our definition of AGI , ” says Morris . “ We need to focus on what we can measure today in a scientifically agreed-upon way. ” Measuring the performance of today ’ s models is already controversial , with researchers debating what it really means for a large language model to pass dozens of high school tests and more . Is it a sign of intelligence ? Or a kind of rote learning ? Assessing the performance of future models that are even more capable will be more difficult still . The researchers suggest that if AGI is ever developed , its capabilities should be evaluated on an ongoing basis , rather than through a handful of one-off tests . The team also points out that AGI does not imply autonomy . “ There ’ s often an implicit assumption that people would want a system to operate completely autonomously , ” says Morris . But that ’ s not always the case . In theory , it ’ s possible to build super-smart machines that are fully controlled by humans . One question the researchers don ’ t address in their discussion of what AGI is , is why we should build it . Some computer scientists , such as Timnit Gebru , founder of the Distributed AI Research Institute , have argued that the whole endeavor is weird . In a talk in April on what she sees as the false ( even dangerous ) promise of utopia through AGI , Gebru noted that the hypothetical technology “ sounds like an unscoped system with the apparent goal of trying to do everything for everyone under any environment. ” Most engineering projects have well-scoped goals . The mission to build AGI does not . Even Google DeepMind ’ s definitions allow for AGI that is indefinitely broad and indefinitely smart . “ Don ’ t attempt to build a god , ” Gebru said . In the race to build bigger and better systems , few will heed such advice . Either way , some clarity around a long-confused concept is welcome . “ Just having silly conversations is kind of uninteresting , ” says Legg . “ There ’ s plenty of good stuff to dig into if we can get past these definition issues . ”","['artificial', 'general', 'intelligence', 'hot', 'topic', 'tech', 'today', 'also', 'controversial', 'big', 'part', 'problem', 'people', 'agree', 'term', 'even', 'mean', 'team', 'deepmind', 'researcher', 'put', 'paper', 'cut', 'cross', 'talk', 'new', 'definition', 'agi', 'whole', 'taxonomy', 'exclusive', 'conversation', 'sutskever', 'fear', 'future', 'ai', 'make', 'change', 'focus', 'life', 'work', 'broad', 'term', 'typically', 'mean', 'artificial', 'intelligence', 'match', 'outmatch', 'human', 'range', 'task', 'specific', 'count', 'humanlike', 'task', 'many', 'tend', 'wave', 'away', 'agi', 'ai', 'well', 'come', 'new', 'definition', 'team', 'start', 'prominent', 'exist', 'definition', 'agi', 'draw', 'believe', 'essential', 'common', 'feature', 'team', 'also', 'outline', 'ascending', 'level', 'agi', 'emerge', 'view', 'include', 'cuttingedge', 'chatbot', 'chatgpt', 'bard', 'competent', 'expert', 'virtuoso', 'perform', 'wide', 'range', 'task', 'well', 'human', 'include', 'task', 'human', 'decode', 'people', 'thought', 'predict', 'future', 'event', 'talk', 'animal', 'note', 'level', 'emerge', 'agi', 'achieve', 'provide', 'muchneeded', 'clarity', 'topic', 'say', 'researcher', 'involve', 'work', 'many', 'people', 'sle', 'term', 'agi', 'think', 'much', 'mean', 'researcher', 'post', 'paper', 'online', 'last', 'week', 'fanfare', 'exclusive', 'conversation', 'team', 'member', 'shane', 'legg', 'deepmind', 'cofounder', 'bill', 'company', 'chief', 'agi', 'scientist', 'principal', 'scientist', 'human', 'interaction', 'get', 'lowdown', 'come', 'definition', 'want', 'achieve', 'see', 'many', 'discussion', 'people', 'seem', 'use', 'term', 'mean', 'different', 'thing', 'lead', 'sort', 'confusion', 'say', 'legg', 'come', 'term', 'first', 'place', 'year', 'ago', 'agi', 'become', 'important', 'topic', 'know', 'even', 'talk', 'need', 'sharpen', 'mean', 'always', 'way', 'talk', 'agi', 'deride', 'serious', 'conversation', 'vague', 'good', 'magical', 'thinking', 'worst', 'buoy', 'hype', 'generative', 'model', 'buzz', 'agi', 'everywhere', 'suggest', 'term', 'former', 'colleague', 'fellow', 'researcher', 'title', 'goertzel', 'book', 'future', 'development', 'ai', 'handwaviness', 'kind', 'point', 'especially', 'clear', 'definition', 'really', 'feel', 'necessary', 'say', 'legg', 'actually', 'think', 'field', 'study', 'rather', 'artifact', 'hope', 'fear', 'technology', 'run', 'wild', 'time', 'agree', 'aim', 'time', 'distinguish', 'exist', 'ai', 'task', 'well', 'chessplaying', 'program', 'deep', 'blue', 'hypothetical', 'ai', 'many', 'imagine', 'day', 'many', 'task', 'well', 'human', 'intelligence', 'deep', 'blue', 'say', 'legg', 'broad', 'thing', 'year', 'people', 'start', 'think', 'agi', 'potential', 'property', 'actual', 'computer', 'program', 'today', 'normal', 'top', 'ai', 'company', 'openai', 'make', 'bold', 'public', 'statement', 'mission', 'build', 'program', 'start', 'conversation', 'need', 'lot', 'specific', 'mean', 'say', 'legg', 'example', 'deepmind', 'researcher', 'state', 'agi', 'generalpurpose', 'highachieving', 'separate', 'breadth', 'depth', 'way', 'useful', 'say', 'togelius', 'show', 'accomplished', 'ai', 'system', 'see', 'past', 'qualify', 'also', 'state', 'agi', 'able', 'range', 'task', 'also', 'able', 'learn', 'task', 'assess', 'performance', 'ask', 'assistance', 'need', 'state', 'agi', 'matter', 'way', 'agi', 'work', 'matter', 'say', 'problem', 'know', 'enough', 'yet', 'way', 'cuttingedge', 'model', 'large', 'language', 'model', 'work', 'hood', 'make', 'focus', 'definition', 'gain', 'insight', 'underlie', 'process', 'important', 'revisit', 'definition', 'say', 'need', 'focus', 'measure', 'today', 'scientifically', 'agreedupon', 'way', 'measure', 'performance', 'today', 'model', 'already', 'controversial', 'researcher', 'debate', 'really', 'mean', 'large', 'language', 'model', 'pass', 'dozen', 'high', 'school', 'test', 'sign', 'intelligence', 'kind', 'rote', 'learning', 'assess', 'performance', 'future', 'model', 'even', 'capable', 'difficult', 'still', 'researcher', 'suggest', 'agi', 'ever', 'develop', 'capability', 'evaluate', 'ongoing', 'basis', 'rather', 'handful', 'oneoff', 'test', 'team', 'also', 'point', 'agi', 'imply', 'autonomy', 'often', 'implicit', 'assumption', 'people', 'want', 'system', 'operate', 'completely', 'autonomously', 'say', 'always', 'case', 'theory', 'possible', 'build', 'supersmart', 'machine', 'fully', 'control', 'human', 'question', 'researcher', 'address', 'discussion', 'agi', 'build', 'computer', 'scientist', 'timnit', 'gebru', 'founder', 'distribute', 'argue', 'whole', 'endeavor', 'weird', 'talk', 'see', 'false', 'even', 'dangerous', 'promise', 'utopia', 'gebru', 'note', 'hypothetical', 'technology', 'sound', 'unscoped', 'system', 'apparent', 'goal', 'try', 'environment', 'engineering', 'project', 'wellscope', 'goal', 'mission', 'build', 'agi', 'even', 'definition', 'allow', 'agi', 'indefinitely', 'broad', 'indefinitely', 'smart', 'attempt', 'build', 'gebru', 'say', 'race', 'build', 'big', 'well', 'system', 'heed', 'advice', 'way', 'clarity', 'longconfuse', 'concept', 'welcome', 'silly', 'conversation', 'kind', 'uninteresting', 'say', 'legg', 'plenty', 'good', 'stuff', 'dig', 'get', 'definition', 'issue']","<p>AGI is one of the most disputed concepts in tech. These researchers want to fix that.</p>
"
Behind Microsoft CEO Satya Nadella’s push to get AI tools in developers’ hands,https://www.technologyreview.com/2023/11/15/1083426/behind-microsoft-ceo-satya-nadellas-push-to-get-ai-tools-in-developers-hands/,2023-11-15,"In San Francisco last week, everyone’s favorite surprise visitor was Microsoft CEO Satya Nadella.  At OpenAI’s DevDay—the company’s first-ever event for developers building on its platform—Nadella bounded on stage to join OpenAI CEO Sam Altman, blowing the hair back on an already electrified audience. “You guys have built something magic,” he gushed.  Two days later on another stage, in another venue, at another developers’ conference, Nadella made his second unannounced appearance of the week—this time at GitHub Universe. There Thomas Dohmke, GitHub’s CEO, was showing off a new version of the company’s AI programming tool, Copilot, that can generate computer code from natural language. Nadella was effusive: “I can code again!” he exclaimed.   Today, Nadella will be onstage speaking to developers at Microsoft Ignite, where the company is announcing even more AI-based developer tools, including an Azure AI Studio that will let devs choose between model catalogs from not only Microsoft, but also the likes of Meta, OpenAI, and Hugging Face, as well as  new tools for customizing Copilot for Microsoft 365.  If you get the sense Nadella is obsessed with developers, you’re not wrong. He’s making the rounds to tout all the ways they can use a new generation of AI-powered tools, like GitHub Copilot (Microsoft acquired GitHub in 2018) or the new suite of developer tools from OpenAI, a company in which Microsoft has reportedly invested some $13 billion.  Last week, Nadella took a 20-minute break from all of his onstage appearances to sit down with MIT Technology Review to talk about (you guessed it) developers. He repeatedly emphasized Microsoft’s long-standing focus on developers. But he also had a message: The way we create software is fundamentally changing.  Nadella believes a platform shift is underway, one that will prove just as significant as the shifts from mainframe to desktop or desktop to mobile. This time, the transition is to natural-language AI tools, some of which he argues will lower the barrier to entry for software development, make existing developers more productive, and ultimately lead to a new era of creativity.  We present Nadella in his own words, below. His remarks have been edited and condensed somewhat for readability.   ON THE RELATIONSHIP WITH OPENAI One criticism of OpenAI is that its very business is only possible via Microsoft, which has given the startup billions of dollars and access to the resources it needs to power its computing-intensive language model. Yet Microsoft is also highly dependent on OpenAI’s technology to power services like GitHub Copilot, Bing, and Office 365. Altman even joked about the partnership onstage. We asked Nadella about this relationship.    I’ve always felt that Microsoft is a platform-and-partner-first company, and this is not new to us. And so therefore, we both are effectively codependent, right? They depend on us to build the best systems, we depend on them to build the best models, and we go to market together.  Exclusive conversations that take us behind the scenes of a cultural phenomenon. ON HIS MISSION TO GET IN FRONT OF DEVELOPERS Nadella says this platform shift is different enough from previous ones that he feels the company needs to provide developers not only with tools, but also with a clear message about what it’s thinking and how devs can come along.  Whenever you have a platform shift, the key thing is to make sure the platform is ubiquitously available for developers to build all kinds of new things. So to us, the most important task is to make the developer tools, the developer platforms, broadly available.  The second thing is for us to also show the light, right? Whether it’s OpenAI building ChatGPT and then innovating on top of it, or us building Copilot and innovating on it. That will give developers an opportunity to distribute their applications. So the most important thing in any platform creation is to get the platform ubiquitously available, and then help developers reach [their] audience.  Those are the two goals that we have across all of these [conferences]. ON WHAT IS DIFFERENT ABOUT THIS SHIFT AND PRODUCTIVITY Productivity gains in the United States have been sluggish for the past 15 or more years. The last huge platform shift—the rise of mobile development—did little to achieve widespread prosperity. Nadella says this time will be different, largely because the shift to AI will fuel a creative revolution by making it easy for anyone to generate new work, including code.  On the other hand, coding today is a highly skilled, well-paid job, and there’s some concern that AI could effectively automate it. Nadella argues that skilled programmers will remain in demand, but that their jobs will change and even more jobs will become available. Nadella has said he envisions 1 billion developers creating on its platforms, many of them with little to no previous experience with coding.    Anytime you have something as disruptive as this, you have to think about the displacement and causes. And that means it’s all about upskilling and reskilling, and in an interesting way, it’s more akin to what happened when word processors and spreadsheets started showing up. Obviously, if you were a typist, it really drastically changed. But at the same time, it enabled a billion people to be able to type into word processors and create and share documents. I don’t think professional developers are going to be any less valuable than they are today. It’s just that we’re going to have many, many gradations of developers. Each time you’re prompting a Bing chat or ChatGPT, you’re essentially programming. The conversation itself is steering a model. I think there will be many, many new jobs, there will be many, many new types of knowledge work, or front-line work, where the drudgery is removed. I think the mobile era was fantastic. It made ubiquitous consumption of services. It didn’t translate into ubiquitous creation of services. The last time there was a broad spread of productivity in the United States and beyond because of information technology was the [advent of the] PC. In fact, even the critics of information technology and productivity, like Robert Gordon of Northwestern, acknowledged that the PC, when it first showed up at work, did actually translate to broad productivity stats changes. So that’s where I think this is, where these tools, like Copilot, [are] being used by a [beginner] software engineer in Detroit, in order to be able to write [code] ... I think we’ll have a real change in the productivity of the auto industry. Same thing in retail, same thing in front-line work and knowledge work. The barrier to entry is very low. Because it’s natural language, domain experts can build apps or workflows. That, I think, is what’s the most exciting thing about this. This is not about just a consumption-led thing. This is not about elite creation. This is about democratized creation. I’m very, very hopeful that we’ll start seeing the productivity gains much more broadly. ON PROTECTING DEVELOPERS Numerous intellectual property cases and class action lawsuits are before the US courts over issues of fair use. At least one singles out GitHub Copilot specifically, claiming Microsoft and OpenAI’s generative tools, which are trained on open source code, amount to software piracy. There’s a fear that people who use these tools could be subject to intellectual-property claims themselves. Microsoft is trying to address these issues with a broad indemnification policy. OpenAI also announced its own indemnification policy, Copyright Shield, at its DevDay conference.  Fundamentally these large models crawl and get content and then train on that content, right? If anybody doesn’t want their content to be crawled, we have great granular controls in our crawlers that allow anybody to stop it from crawling. In fact, we have controls where you can have it crawl just for search, but not for large-language-model training. That’s available today. So anybody who really wants to ensure that their content is not being taken for retraining can do so today.  The second thing, of course, is I think the courts and the legislative process in some combination will have to decide what is fair use and what is not fair use. We have taken a lot of control in making sure that we are only training models, and we are using data to train models that we’re allowed to and which we believe we have a legal standing on.  If it comes to it, we’ll litigate it in the courts. We’ll take that burden on so the users of our products don’t have to worry about it. That’s as simple as that, which is to take the liability and transfer it from our users to us. And of course, we are going to be very, very mindful of making sure we’re on the right side of the law there.  ","In San Francisco last week , everyone ’ s favorite surprise visitor was Microsoft CEO Satya Nadella . At OpenAI ’ s DevDay—the company ’ s first-ever event for developers building on its platform—Nadella bounded on stage to join OpenAI CEO Sam Altman , blowing the hair back on an already electrified audience . “ You guys have built something magic , ” he gushed . Two days later on another stage , in another venue , at another developers ’ conference , Nadella made his second unannounced appearance of the week—this time at GitHub Universe . There Thomas Dohmke , GitHub ’ s CEO , was showing off a new version of the company ’ s AI programming tool , Copilot , that can generate computer code from natural language . Nadella was effusive : “ I can code again ! ” he exclaimed . Today , Nadella will be onstage speaking to developers at Microsoft Ignite , where the company is announcing even more AI-based developer tools , including an Azure AI Studio that will let devs choose between model catalogs from not only Microsoft , but also the likes of Meta , OpenAI , and Hugging Face , as well as new tools for customizing Copilot for Microsoft 365 . If you get the sense Nadella is obsessed with developers , you ’ re not wrong . He ’ s making the rounds to tout all the ways they can use a new generation of AI-powered tools , like GitHub Copilot ( Microsoft acquired GitHub in 2018 ) or the new suite of developer tools from OpenAI , a company in which Microsoft has reportedly invested some $ 13 billion . Last week , Nadella took a 20-minute break from all of his onstage appearances to sit down with MIT Technology Review to talk about ( you guessed it ) developers . He repeatedly emphasized Microsoft ’ s long-standing focus on developers . But he also had a message : The way we create software is fundamentally changing . Nadella believes a platform shift is underway , one that will prove just as significant as the shifts from mainframe to desktop or desktop to mobile . This time , the transition is to natural-language AI tools , some of which he argues will lower the barrier to entry for software development , make existing developers more productive , and ultimately lead to a new era of creativity . We present Nadella in his own words , below . His remarks have been edited and condensed somewhat for readability . ON THE RELATIONSHIP WITH OPENAI One criticism of OpenAI is that its very business is only possible via Microsoft , which has given the startup billions of dollars and access to the resources it needs to power its computing-intensive language model . Yet Microsoft is also highly dependent on OpenAI ’ s technology to power services like GitHub Copilot , Bing , and Office 365 . Altman even joked about the partnership onstage . We asked Nadella about this relationship . I ’ ve always felt that Microsoft is a platform-and-partner-first company , and this is not new to us . And so therefore , we both are effectively codependent , right ? They depend on us to build the best systems , we depend on them to build the best models , and we go to market together . Exclusive conversations that take us behind the scenes of a cultural phenomenon . ON HIS MISSION TO GET IN FRONT OF DEVELOPERS Nadella says this platform shift is different enough from previous ones that he feels the company needs to provide developers not only with tools , but also with a clear message about what it ’ s thinking and how devs can come along . Whenever you have a platform shift , the key thing is to make sure the platform is ubiquitously available for developers to build all kinds of new things . So to us , the most important task is to make the developer tools , the developer platforms , broadly available . The second thing is for us to also show the light , right ? Whether it ’ s OpenAI building ChatGPT and then innovating on top of it , or us building Copilot and innovating on it . That will give developers an opportunity to distribute their applications . So the most important thing in any platform creation is to get the platform ubiquitously available , and then help developers reach [ their ] audience . Those are the two goals that we have across all of these [ conferences ] . ON WHAT IS DIFFERENT ABOUT THIS SHIFT AND PRODUCTIVITY Productivity gains in the United States have been sluggish for the past 15 or more years . The last huge platform shift—the rise of mobile development—did little to achieve widespread prosperity . Nadella says this time will be different , largely because the shift to AI will fuel a creative revolution by making it easy for anyone to generate new work , including code . On the other hand , coding today is a highly skilled , well-paid job , and there ’ s some concern that AI could effectively automate it . Nadella argues that skilled programmers will remain in demand , but that their jobs will change and even more jobs will become available . Nadella has said he envisions 1 billion developers creating on its platforms , many of them with little to no previous experience with coding . Anytime you have something as disruptive as this , you have to think about the displacement and causes . And that means it ’ s all about upskilling and reskilling , and in an interesting way , it ’ s more akin to what happened when word processors and spreadsheets started showing up . Obviously , if you were a typist , it really drastically changed . But at the same time , it enabled a billion people to be able to type into word processors and create and share documents . I don ’ t think professional developers are going to be any less valuable than they are today . It ’ s just that we ’ re going to have many , many gradations of developers . Each time you ’ re prompting a Bing chat or ChatGPT , you ’ re essentially programming . The conversation itself is steering a model . I think there will be many , many new jobs , there will be many , many new types of knowledge work , or front-line work , where the drudgery is removed . I think the mobile era was fantastic . It made ubiquitous consumption of services . It didn ’ t translate into ubiquitous creation of services . The last time there was a broad spread of productivity in the United States and beyond because of information technology was the [ advent of the ] PC . In fact , even the critics of information technology and productivity , like Robert Gordon of Northwestern , acknowledged that the PC , when it first showed up at work , did actually translate to broad productivity stats changes . So that ’ s where I think this is , where these tools , like Copilot , [ are ] being used by a [ beginner ] software engineer in Detroit , in order to be able to write [ code ] ... I think we ’ ll have a real change in the productivity of the auto industry . Same thing in retail , same thing in front-line work and knowledge work . The barrier to entry is very low . Because it ’ s natural language , domain experts can build apps or workflows . That , I think , is what ’ s the most exciting thing about this . This is not about just a consumption-led thing . This is not about elite creation . This is about democratized creation . I ’ m very , very hopeful that we ’ ll start seeing the productivity gains much more broadly . ON PROTECTING DEVELOPERS Numerous intellectual property cases and class action lawsuits are before the US courts over issues of fair use . At least one singles out GitHub Copilot specifically , claiming Microsoft and OpenAI ’ s generative tools , which are trained on open source code , amount to software piracy . There ’ s a fear that people who use these tools could be subject to intellectual-property claims themselves . Microsoft is trying to address these issues with a broad indemnification policy . OpenAI also announced its own indemnification policy , Copyright Shield , at its DevDay conference . Fundamentally these large models crawl and get content and then train on that content , right ? If anybody doesn ’ t want their content to be crawled , we have great granular controls in our crawlers that allow anybody to stop it from crawling . In fact , we have controls where you can have it crawl just for search , but not for large-language-model training . That ’ s available today . So anybody who really wants to ensure that their content is not being taken for retraining can do so today . The second thing , of course , is I think the courts and the legislative process in some combination will have to decide what is fair use and what is not fair use . We have taken a lot of control in making sure that we are only training models , and we are using data to train models that we ’ re allowed to and which we believe we have a legal standing on . If it comes to it , we ’ ll litigate it in the courts . We ’ ll take that burden on so the users of our products don ’ t have to worry about it . That ’ s as simple as that , which is to take the liability and transfer it from our users to us . And of course , we are going to be very , very mindful of making sure we ’ re on the right side of the law there .","['last', 'week', 'favorite', 'surprise', 'visitor', 'company', 'firstever', 'event', 'developer', 'build', 'platform', 'nadella', 'bound', 'stage', 'join', 'blow', 'hair', 'back', 'already', 'electrify', 'audience', 'guy', 'build', 'magic', 'gush', 'day', 'later', 'stage', 'venue', 'developer', 'conference', 'nadella', 'make', 'second', 'unannounced', 'appearance', 'week', 'time', 'ceo', 'show', 'new', 'version', 'company', 'ai', 'programming', 'tool', 'copilot', 'generate', 'computer', 'code', 'natural', 'language', 'nadella', 'effusive', 'code', 'exclaim', 'today', 'nadella', 'onstage', 'speak', 'developer', 'company', 'announce', 'even', 'aibased', 'developer', 'tool', 'include', 'azure', 'ai', 'studio', 'let', 'devs', 'choose', 'model', 'catalog', 'also', 'like', 'meta', 'openai', 'hug', 'face', 'well', 'new', 'tool', 'customize', 'copilot', 'get', 'sense', 'nadella', 'obsess', 'developer', 'wrong', 'make', 'round', 'tout', 'way', 'use', 'new', 'generation', 'aipowere', 'tool', 'copilot', 'acquire', 'new', 'suite', 'developer', 'tool', 'company', 'reportedly', 'invest', 'last', 'week', 'take', 'break', 'onstage', 'appearance', 'sit', 'mit', 'technology', 'review', 'talk', 'guess', 'developer', 'repeatedly', 'emphasize', 'longstanding', 'focus', 'developer', 'also', 'message', 'way', 'create', 'software', 'fundamentally', 'change', 'nadella', 'believe', 'platform', 'shift', 'underway', 'one', 'prove', 'significant', 'shift', 'mainframe', 'desktop', 'desktop', 'mobile', 'time', 'transition', 'naturallanguage', 'tool', 'argue', 'lower', 'barrier', 'entry', 'software', 'development', 'make', 'exist', 'developer', 'productive', 'ultimately', 'lead', 'new', 'era', 'creativity', 'present', 'nadella', 'word', 'remark', 'edit', 'condense', 'somewhat', 'readability', 'relationship', 'criticism', 'business', 'possible', 'give', 'startup', 'billion', 'dollar', 'access', 'resource', 'need', 'power', 'computingintensive', 'language', 'model', 'yet', 'also', 'highly', 'dependent', 'technology', 'power', 'service', 'copilot', 'bing', 'office', 'even', 'joke', 'partnership', 'onstage', 'ask', 'nadella', 'relationship', 'always', 'feel', 'platformandpartnerfirst', 'company', 'new', 'therefore', 'effectively', 'codependent', 'right', 'depend', 'build', 'good', 'system', 'depend', 'build', 'good', 'model', 'go', 'market', 'together', 'exclusive', 'conversation', 'take', 'scene', 'cultural', 'phenomenon', 'mission', 'get', 'front', 'developer', 'say', 'platform', 'shift', 'different', 'enough', 'previous', 'one', 'feel', 'company', 'need', 'provide', 'developer', 'tool', 'also', 'clear', 'message', 'think', 'devs', 'come', 'platform', 'shift', 'key', 'thing', 'make', 'sure', 'platform', 'ubiquitously', 'available', 'developer', 'build', 'kind', 'new', 'thing', 'important', 'task', 'make', 'developer', 'tool', 'developer', 'platform', 'broadly', 'available', 'second', 'thing', 'also', 'show', 'light', 'right', 'openai', 'building', 'chatgpt', 'innovate', 'top', 'build', 'copilot', 'innovate', 'give', 'developer', 'opportunity', 'distribute', 'application', 'important', 'thing', 'platform', 'creation', 'get', 'platform', 'ubiquitously', 'available', 'help', 'developer', 'reach', 'audience', 'goal', 'conference', 'different', 'shift', 'productivity', 'productivity', 'gain', 'sluggish', 'past', 'year', 'last', 'huge', 'platform', 'shift', 'rise', 'mobile', 'development', 'little', 'achieve', 'widespread', 'prosperity', 'say', 'time', 'different', 'largely', 'shift', 'ai', 'fuel', 'creative', 'revolution', 'make', 'easy', 'generate', 'new', 'work', 'include', 'code', 'hand', 'code', 'today', 'highly', 'skilled', 'wellpaid', 'job', 'concern', 'effectively', 'automate', 'argue', 'skilled', 'programmer', 'remain', 'demand', 'job', 'change', 'even', 'job', 'become', 'available', 'nadella', 'say', 'envision', 'developer', 'create', 'platform', 'many', 'little', 'previous', 'experience', 'code', 'anytime', 'disruptive', 'think', 'displacement', 'cause', 'mean', 'upskille', 'reskille', 'interesting', 'way', 'akin', 'happen', 'word', 'processor', 'spreadsheet', 'start', 'show', 'obviously', 'typist', 'really', 'drastically', 'change', 'time', 'enable', 'people', 'able', 'type', 'word', 'processor', 'create', 'share', 'document', 'think', 'professional', 'developer', 'go', 'less', 'valuable', 'today', 'go', 'many', 'many', 'gradation', 'developer', 'time', 'prompt', 'bing', 'chat', 'chatgpt', 'essentially', 'program', 'conversation', 'steer', 'model', 'think', 'many', 'many', 'new', 'job', 'many', 'many', 'new', 'type', 'knowledge', 'work', 'frontline', 'work', 'drudgery', 'remove', 'think', 'mobile', 'era', 'fantastic', 'make', 'ubiquitous', 'consumption', 'service', 'translate', 'ubiquitous', 'creation', 'service', 'last', 'time', 'broad', 'spread', 'productivity', 'information', 'technology', 'advent', 'pc', 'fact', 'even', 'critic', 'information', 'technology', 'productivity', 'acknowledge', 'pc', 'first', 'show', 'work', 'actually', 'translate', 'broad', 'productivity', 'stat', 'change', 'think', 'tool', 'copilot', 'use', 'beginner', 'software', 'engineer', 'order', 'able', 'write', 'code', 'think', 'real', 'change', 'productivity', 'auto', 'industry', 'thing', 'retail', 'thing', 'frontline', 'work', 'knowledge', 'work', 'barrier', 'entry', 'low', 'natural', 'language', 'domain', 'expert', 'build', 'app', 'workflow', 'think', 'exciting', 'thing', 'consumptionle', 'thing', 'elite', 'creation', 'democratized', 'creation', 'hopeful', 'start', 'see', 'productivity', 'gain', 'much', 'broadly', 'protect', 'developer', 'numerous', 'intellectual', 'property', 'case', 'class', 'action', 'lawsuit', 'court', 'issue', 'fair', 'use', 'least', 'single', 'copilot', 'specifically', 'claim', 'generative', 'tool', 'train', 'open', 'source', 'code', 'amount', 'software', 'piracy', 'fear', 'people', 'use', 'tool', 'subject', 'intellectualproperty', 'claim', 'try', 'address', 'issue', 'broad', 'indemnification', 'policy', 'also', 'announce', 'indemnification', 'policy', 'copyright', 'shield', 'devday', 'conference', 'fundamentally', 'large', 'model', 'crawl', 'get', 'content', 'train', 'content', 'right', 'want', 'content', 'crawl', 'great', 'granular', 'control', 'crawler', 'allow', 'stop', 'crawl', 'fact', 'control', 'crawl', 'search', 'largelanguagemodel', 'training', 'available', 'today', 'really', 'want', 'ensure', 'content', 'take', 'retraining', 'today', 'second', 'thing', 'course', 'think', 'court', 'legislative', 'process', 'combination', 'decide', 'fair', 'use', 'fair', 'use', 'take', 'lot', 'control', 'make', 'sure', 'training', 'model', 'use', 'datum', 'train', 'model', 'allow', 'believe', 'legal', 'standing', 'come', 'litigate', 'court', 'take', 'burden', 'user', 'product', 'worry', 'simple', 'take', 'liability', 'transfer', 'user', 'course', 'go', 'mindful', 'make', 'sure', 'right', 'side', 'law']","<p>In an exclusive interview with MIT Technology Review, Nadella shares his view of the platform shift for developers.</p>
"
Google DeepMind’s weather AI can forecast extreme weather faster and more accurately,https://www.technologyreview.com/2023/11/14/1083366/google-deepminds-weather-ai-can-forecast-extreme-weather-quicker-and-more-accurately/,2023-11-14,"This year the Earth has been hit by a record number of unpredictable extreme weather events made worse by climate change. Predicting them faster and with greater accuracy could enable us to prepare better for natural disasters and help save lives. A new AI model from Google DeepMind could make that easier.  In research published in Science today, Google DeepMind’s model, GraphCast, was able to predict weather conditions up to 10 days in advance, more accurately and much faster than the current gold standard. GraphCast outperformed the model from the European Centre for Medium-Range Weather Forecasts (ECMWF) in more than 90% of over 1,300 test areas. And on predictions for Earth’s troposphere—the lowest part of the atmosphere, where most weather happens—GraphCast outperformed the ECMWF’s model on more than 99% of weather variables, such as rain and air temperature  Crucially, GraphCast can also offer meteorologists accurate warnings, much earlier than standard models, of conditions such as extreme temperatures and the paths of cyclones. In September, GraphCast accurately predicted that Hurricane Lee would make landfall in Nova Scotia nine days in advance, says Rémi Lam, a staff research scientist at Google DeepMind. Traditional weather forecasting models pinpointed the hurricane to Nova Scotia only six days in advance.  They could also help to make them more accurate. “Weather prediction is one of the most challenging problems that humanity has been working on for a long, long time. And if you look at what has happened in the last few years with climate change, this is an incredibly important problem,” says Pushmeet Kohli, the vice president of research at Google DeepMind.   Traditionally, meteorologists use massive computer simulations to make weather predictions. They are very energy intensive and  time consuming to run, because the simulations take into account many physics-based equations and different weather variables such as temperature, precipitation, pressure, wind, humidity, and cloudiness, one by one.  GraphCast uses machine learning to do these calculations in under a minute. Instead of using the physics-based equations, it bases its predictions on four decades of historical weather data. GraphCast uses graph neural networks, which map Earth’s surface into more than a million grid points. At each grid point, the model predicts the temperature, wind speed and direction, and mean sea-level pressure, as well as other conditions like humidity. The neural network is then able to find patterns and draw conclusions about what will happen next for each of these data points.  For the past year, weather forecasting has been going through a revolution as models such as GraphCast, Huawei’s Pangu-Weather and Nvidia’s FourcastNet have made meteorologists rethink the role AI can play in weather forecasting. GraphCast improves on the performance of other competing models, such as Pangu-Weather, and is able to predict more weather variables, says Lam. The ECMWF is already using it. When Google DeepMind first debuted GraphCast last December, it felt like Christmas, says Peter Dueben, head of Earth system modeling at ECMWF, who was not involved in the research.  “It showed that these models are so good that we cannot avoid them anymore,” he says.  GraphCast is a “reckoning moment” for weather prediction because it shows that predictions can be made using historical data, says Aditya Grover, an assistant professor of computer science at UCLA, who developed ClimaX, a foundation model that allows researchers to do different tasks relating to modeling the Earth’s weather and climate.  Plus: AI-text detection tools are really easy to fool. DeepMind's model is “great work and extremely exciting,” says Oliver Fuhrer, the head of the numerical prediction department at MeteoSwiss, the Swiss Federal Office of Meteorology and Climatology. Fuhrer says that other weather agencies, such as the ECMWF and the Swedish Meteorological and Hydrological Institute, have also used the graph neural network architecture proposed by Google DeepMind to build their own models.  But GraphCast is not perfect. It still lags behind conventional weather forecasting models in some areas, such as precipitation, Dueben says. Meteorologists will still have to use conventional models alongside machine-learning models to offer better predictions.  Google DeepMind is also making GraphCast open source. This is a good development, says UCLA’s Grover.  “With climate change on the rise, it's very important that big organizations, which have had the luxury of so much compute, also think about giving back [to the scientific community],” he says.  ","This year the Earth has been hit by a record number of unpredictable extreme weather events made worse by climate change . Predicting them faster and with greater accuracy could enable us to prepare better for natural disasters and help save lives . A new AI model from Google DeepMind could make that easier . In research published in Science today , Google DeepMind ’ s model , GraphCast , was able to predict weather conditions up to 10 days in advance , more accurately and much faster than the current gold standard . GraphCast outperformed the model from the European Centre for Medium-Range Weather Forecasts ( ECMWF ) in more than 90 % of over 1,300 test areas . And on predictions for Earth ’ s troposphere—the lowest part of the atmosphere , where most weather happens—GraphCast outperformed the ECMWF ’ s model on more than 99 % of weather variables , such as rain and air temperature Crucially , GraphCast can also offer meteorologists accurate warnings , much earlier than standard models , of conditions such as extreme temperatures and the paths of cyclones . In September , GraphCast accurately predicted that Hurricane Lee would make landfall in Nova Scotia nine days in advance , says Rémi Lam , a staff research scientist at Google DeepMind . Traditional weather forecasting models pinpointed the hurricane to Nova Scotia only six days in advance . They could also help to make them more accurate . “ Weather prediction is one of the most challenging problems that humanity has been working on for a long , long time . And if you look at what has happened in the last few years with climate change , this is an incredibly important problem , ” says Pushmeet Kohli , the vice president of research at Google DeepMind . Traditionally , meteorologists use massive computer simulations to make weather predictions . They are very energy intensive and time consuming to run , because the simulations take into account many physics-based equations and different weather variables such as temperature , precipitation , pressure , wind , humidity , and cloudiness , one by one . GraphCast uses machine learning to do these calculations in under a minute . Instead of using the physics-based equations , it bases its predictions on four decades of historical weather data . GraphCast uses graph neural networks , which map Earth ’ s surface into more than a million grid points . At each grid point , the model predicts the temperature , wind speed and direction , and mean sea-level pressure , as well as other conditions like humidity . The neural network is then able to find patterns and draw conclusions about what will happen next for each of these data points . For the past year , weather forecasting has been going through a revolution as models such as GraphCast , Huawei ’ s Pangu-Weather and Nvidia ’ s FourcastNet have made meteorologists rethink the role AI can play in weather forecasting . GraphCast improves on the performance of other competing models , such as Pangu-Weather , and is able to predict more weather variables , says Lam . The ECMWF is already using it . When Google DeepMind first debuted GraphCast last December , it felt like Christmas , says Peter Dueben , head of Earth system modeling at ECMWF , who was not involved in the research . “ It showed that these models are so good that we can not avoid them anymore , ” he says . GraphCast is a “ reckoning moment ” for weather prediction because it shows that predictions can be made using historical data , says Aditya Grover , an assistant professor of computer science at UCLA , who developed ClimaX , a foundation model that allows researchers to do different tasks relating to modeling the Earth ’ s weather and climate . Plus : AI-text detection tools are really easy to fool . DeepMind 's model is “ great work and extremely exciting , ” says Oliver Fuhrer , the head of the numerical prediction department at MeteoSwiss , the Swiss Federal Office of Meteorology and Climatology . Fuhrer says that other weather agencies , such as the ECMWF and the Swedish Meteorological and Hydrological Institute , have also used the graph neural network architecture proposed by Google DeepMind to build their own models . But GraphCast is not perfect . It still lags behind conventional weather forecasting models in some areas , such as precipitation , Dueben says . Meteorologists will still have to use conventional models alongside machine-learning models to offer better predictions . Google DeepMind is also making GraphCast open source . This is a good development , says UCLA ’ s Grover . “ With climate change on the rise , it 's very important that big organizations , which have had the luxury of so much compute , also think about giving back [ to the scientific community ] , ” he says .","['year', 'earth', 'hit', 'record', 'number', 'unpredictable', 'extreme', 'weather', 'event', 'make', 'bad', 'climate', 'change', 'predict', 'fast', 'great', 'accuracy', 'enable', 'prepare', 'well', 'natural', 'disaster', 'help', 'save', 'life', 'new', 'model', 'make', 'easy', 'research', 'publish', 'science', 'today', 'model', 'graphcast', 'able', 'predict', 'weather', 'condition', 'day', 'advance', 'accurately', 'much', 'fast', 'current', 'gold', 'standard', 'graphcast', 'outperform', 'model', 'weather', 'forecast', 'ecmwf', 'test', 'area', 'prediction', 'earth', 'troposphere', 'low', 'part', 'atmosphere', 'weather', 'happen', 'graphcast', 'outperform', 'ecmwf', 'model', 'weather', 'variable', 'rain', 'air', 'temperature', 'crucially', 'graphcast', 'also', 'offer', 'meteorologist', 'accurate', 'warning', 'much', 'early', 'standard', 'model', 'condition', 'extreme', 'temperature', 'path', 'cyclone', 'graphcast', 'accurately', 'predict', 'make', 'landfall', 'day', 'advance', 'say', 'staff', 'research', 'scientist', 'traditional', 'weather', 'forecasting', 'model', 'pinpoint', 'hurricane', 'day', 'advance', 'also', 'help', 'make', 'accurate', 'weather', 'prediction', 'challenging', 'problem', 'humanity', 'work', 'long', 'long', 'time', 'look', 'happen', 'last', 'year', 'climate', 'change', 'incredibly', 'important', 'problem', 'say', 'vice', 'president', 'research', 'traditionally', 'meteorologist', 'use', 'massive', 'computer', 'simulation', 'make', 'weather', 'prediction', 'energy', 'intensive', 'time', 'consume', 'run', 'simulation', 'take', 'account', 'many', 'physicsbase', 'equation', 'different', 'weather', 'variable', 'temperature', 'precipitation', 'pressure', 'wind', 'humidity', 'cloudiness', 'graphcast', 'use', 'machine', 'learn', 'calculation', 'minute', 'instead', 'use', 'physicsbase', 'equation', 'base', 'prediction', 'decade', 'historical', 'weather', 'datum', 'graphcast', 'use', 'graph', 'neural', 'network', 'map', 'earth', 'surface', 'grid', 'point', 'grid', 'point', 'model', 'predict', 'temperature', 'wind', 'speed', 'direction', 'mean', 'sealevel', 'pressure', 'well', 'condition', 'humidity', 'neural', 'network', 'able', 'find', 'pattern', 'draw', 'conclusion', 'happen', 'next', 'datum', 'point', 'past', 'year', 'weather', 'forecasting', 'go', 'revolution', 'model', 'graphcast', 'huawei', 'panguweather', 'fourcastnet', 'make', 'meteorologist', 'rethink', 'role', 'play', 'weather', 'forecasting', 'graphcast', 'improve', 'performance', 'compete', 'model', 'panguweather', 'able', 'predict', 'weather', 'variable', 'say', 'ecmwf', 'already', 'use', 'first', 'debut', 'graphcast', 'last', 'feel', 'say', 'head', 'earth', 'system', 'model', 'ecmwf', 'involve', 'research', 'show', 'model', 'good', 'avoid', 'anymore', 'say', 'graphcast', 'reckoning', 'moment', 'weather', 'prediction', 'show', 'prediction', 'make', 'use', 'historical', 'datum', 'say', 'assistant', 'professor', 'computer', 'science', 'develop', 'climax', 'foundation', 'model', 'allow', 'researcher', 'different', 'task', 'relate', 'model', 'earth', 'weather', 'climate', 'aitext', 'detection', 'tool', 'really', 'easy', 'fool', 'deepmind', 'model', 'great', 'work', 'extremely', 'exciting', 'say', 'head', 'swiss', 'federal', 'office', 'meteorology', 'climatology', 'fuhrer', 'say', 'weather', 'agency', 'ecmwf', 'swedish', 'meteorological', 'hydrological', 'institute', 'also', 'use', 'graph', 'neural', 'network', 'architecture', 'propose', 'build', 'model', 'graphcast', 'perfect', 'still', 'lag', 'conventional', 'weather', 'forecasting', 'model', 'area', 'precipitation', 'say', 'meteorologist', 'still', 'use', 'conventional', 'model', 'machinelearning', 'model', 'offer', 'well', 'prediction', 'deepmind', 'also', 'make', 'graphcast', 'open', 'source', 'good', 'development', 'say', 'grover', 'climate', 'change', 'rise', 'important', 'big', 'organization', 'luxury', 'much', 'compute', 'also', 'think', 'give', 'scientific', 'community', 'say']","<p>It said Hurricane Lee would make landfall in Nova Scotia three days sooner than traditional methods predicted.</p>
"
How Facebook went all in on AI,https://www.technologyreview.com/2023/11/14/1083336/how-facebook-went-all-in-on-ai/,2023-11-14,"The following is excerpted from BROKEN CODE: Inside Facebook and the Fight to Expose Its Harmful Secrets by Jeff Horwitz. Reprinted by permission of Doubleday, an imprint of The Knopf Doubleday Publishing Group, a division of Penguin Random House LLC. Copyright © 2023 by Jeff Horwitz. In 2006, the U.S. patent office received a filing for “an automatically generated display that contains information relevant to a user about another user of a social network.” Rather than forcing people to search through “disparate and disorganized” content for items of interest, the system would seek to generate a list of “relevant” information in a “preferred order.” The listed authors were “Zuckerberg et al.” and the product was the News Feed. The idea of showing users streams of activity wasn’t entirely new—­ photo-­sharing website Flickr and others had been experimenting with it—­ but the change was massive. Before, Facebook users would interact with the site mainly via notifications, pokes, or looking up friends’ profiles. With the launch of the News Feed, users got a constantly updating stream of posts and status changes. The shift came as a shock to what were Facebook’s then 10 million users, who did not appreciate their activities being monitored and their once-­ static profiles mined for updated content. In the face of widespread complaints, Zuckerberg wrote a post reassuring users, “Nothing you do is being broadcast; rather, it is being shared with people who care about what you do—­ your friends.” He titled it: “Calm down. Breathe. We hear you.” Hearing user complaints wasn’t the same thing as listening to them. As Chris Cox would later note at a press event, News Feed was an instant success at boosting activity on the platform and connecting users. Engagement quickly doubled, and within two weeks of launch more than a million members had affiliated themselves with a single interest for the first time. The cause that had united so many people? A petition to eradicate the “stalkeresque” News Feed. The opaque system that users revolted against was, in hindsight, remarkably simple. Content mostly appeared in reverse chronological order, with manual adjustments made to ensure that people saw both popular posts and a range of material. “In the beginning, News Feed ranking was turning knobs,” Cox said. Fiddling with dials worked well enough for a little while, but everyone’s friend lists were growing and Facebook was introducing new features such as ads, pages, and interest groups. As entertainment, memes, and commerce began to compete with posts from friends in News Feed, Facebook needed to ensure that a user who had just logged on would see their best friend’s engagement photos ahead of a cooking page’s popular enchilada recipe. The first effort at sorting, eventually branded “EdgeRank,” was a simple formula that prioritized content according to three principal factors: a post’s age, the amount of engagement it got, and the interconnection between user and poster. As an algorithm, it wasn’t much—­ just a rough attempt to translate the questions “Is it new, popular, or from someone you care about?” into math.  There was no dark magic at play, but users again revolted against the idea of Facebook putting its thumb on what they saw. And, again, Facebook usage metrics jumped across the board. The platform’s recommendation systems were still in their infancy, but the dissonance between users’ vocal disapproval and avid usage led to an inescapable conclusion inside the company: regular people’s opinions about Facebook’s mechanics were best ignored. Users screamed “stop,” Facebook kept going, and everything would work out dandy. By 2010, the company was looking to move beyond EdgeRank’s crude formula to recommend content based on machine learning, a branch of artificial intelligence focused on training computers to design their own decision-­ making algorithms. Rather than programming Facebook’s computers to rank content according to simple math, engineers would program them to analyze user behavior and design their own ranking formulas. What people saw would be the result of constant experimentation, the platform serving up whatever it predicted was most likely to generate a like from a user and evaluating its own results in real time. Despite the growing complexity of its product and the collection of user data at a scale the world had never seen, Facebook still didn’t know enough about its users to show them relevant ads. Brands loved the attention and buzz they could get from creating content on Facebook, but they hadn’t found the company’s paid offerings compelling. In May 2012, General Motors killed its entire Facebook advertising budget. A prominent digital advertising executive declared Facebook ads “fundamentally some of the worst performing ad units on the Web.” Fixing the problem would fall to a team run by Joaquin Quiñonero Candela. A Spaniard who grew up in Morocco, Quiñonero was living in the UK and working on artificial intelligence at Microsoft in 2011 when friends scattered across Northern Africa began talking excitedly about social media–­ driven protests. The machine learning techniques he was using to optimize Bing search ads had clear applications to the social networks that people had used to overthrow four autocratic states and nearly topple several more. “I joined Facebook because of the Arab Spring,” Quiñonero said. Quiñonero found that the way Facebook built its products was nearly as revolutionary as their results. Invited by a friend to tour the Menlo Park campus, he was shocked to look over the shoulder of an engineer making a significant but unsupervised update to Facebook’s code. Confirming how much faster the company moved than Microsoft, Quiñonero received a Facebook job offer a week later. Quiñonero began working on ads, and his timing could hardly have been better. Advances in machine learning and raw computing speed allowed the platform to not only pigeonhole users into demographic niches (“single heterosexual woman in San Francisco, late twenties, interested in camping and salsa dancing”) but to spot correlations between what they clicked on and then use that information to guess which ads they would find relevant. After beginning with near- random guesses on how to maximize the odds of a click, the system would learn from its hits and misses, refining its model for predicting which ads had the best shot at success. It was hardly omniscient—­ recommended ads were regularly inexplicable. But the bar for success in digital advertising was low: if 2 percent of users clicked on an ad, that was a triumph. With billions of ads served each day, algorithm tweaks that produced even modest gains could bring in tens or hundreds of millions of dollars in revenue. And Quiñonero’s team found that it could churn out those alterations. “I told my team to go fast, to ship every week,” he said.  Consumers are getting their first samplings of meat made in the lab. The rapid pace made sense. The team’s AI was improving not just revenue but how people felt about the platform. Better-­ targeted ads meant Facebook could make more money per user without increasing the ad load, and there wasn’t all that much that could go wrong. When Facebook pitched denture cream to teenagers, nobody died.  Advertising was the beachhead for machine learning at Facebook, and soon everyone wanted a piece of the action. For product executives tasked with increasing the number of Facebook groups joined, friends added, and posts made, the appeal was obvious. If Quiñonero’s techniques could increase how often users engaged with ads, they could increase how often users engaged with everything else on the platform. Every team responsible for ranking or recommending content rushed to overhaul their systems as fast as they could, setting off an explosion in the complexity of Facebook’s product. Employees found that the biggest gains often came not from deliberate initiatives but from simple futzing around. Rather than redesigning algorithms, which was slow, engineers were scoring big with quick and dirty machine learning experiments that amounted to throwing hundreds of variants of existing algorithms at the wall and seeing which versions stuck—­ which performed best with users. They wouldn’t necessarily know why a variable mattered or how one algorithm outperformed another at, say, predicting the likelihood of commenting. But they could keep fiddling until the machine learning model produced an algorithm that statistically outperformed the existing one, and that was good enough. It would be hard to conceive of an approach to building systems that more embodied the slogan “Move Fast and Break Things.” Facebook wanted only more. Zuckerberg wooed Yann LeCun, a French computer scientist specializing in deep learning, meaning the construction of computer systems capable of processing information in ways inspired by human thinking. Already renowned for creating the foundational AI techniques that made facial recognition possible, LeCun was put in charge of a division that aimed to put Facebook at the vanguard of fundamental research into artificial intelligence. Following his success with ads, Quiñonero was given an equally formidable task: pushing machine learning into the company’s bloodstream as fast as possible. His initial staff of two dozen—­ the team responsible for building new core machine learning tools and making them available to other parts of the company—­ had grown in the three years since he’d been hired. But it was still nowhere near large enough to assist every product team that wanted machine learning help. The skills to build a model from scratch were too specialized for engineers to readily pick up, and you couldn’t increase the supply of machine learning PhDs by throwing money around. The solution was to build FB Learner, a sort of “paint by numbers” version of machine learning. It packaged techniques into a template that could be used by engineers who quite literally did not understand what they were doing. FB Learner did for machine learning inside Facebook what services like WordPress had once done for building websites, rendering the need to muck around with HTML or configure a server unnecessary. Rather than setting up a blog, however, the engineers in question were messing with the guts of what was rapidly becoming a preeminent global communications platform. Many at Facebook were aware of the increasing concerns around AI outside the company’s walls. Poorly designed algorithms meant to reward good healthcare penalized hospitals that treated sicker patients, and models purporting to quantify a parole candidate’s risk of reoffending turned out to be biased in favor of keeping Black people in jail. But these issues seemed remote on a social network. An avid user of FB Learner would later describe machine learning’s mass diffusion inside Facebook as “giving rocket launchers to twenty-­ five-­ year-­ old engineers.” But at the time, Quiñonero and the company spoke of it as a triumph. The company’s AI algorithms gave it an insatiable habit for lies and hate speech. Now the man who built them can't fix the problem. “Engineers and teams, even with little expertise, can build and run experiments with ease and deploy AI-­ powered products to production faster than ever,” Facebook announced in 2016, boasting that FB Learner was ingesting trillions of data points on user behavior every day and that engineers were running 500,000 experiments on them a month. The sheer amount of data that Facebook collected—­ and ad-targeting results so good that users regularly suspected (wrongly) the company of eavesdropping on their offline conversations—gave rise to the claim that “Facebook knows everything about you.” That wasn’t quite correct. The wonders of machine learning had obscured its limits. Facebook’s recommendation systems worked by raw correlation between user behavior, not by identifying a user’s tastes and interests and then serving content based on it. News Feed couldn’t tell you whether you liked ice skating or dirt biking, hip-­ hop or K-­ pop, and it couldn’t explain in human terms why one post appeared in your feed above another. Although this inexplicability was an obvious drawback, machine learning–­ based recommendation systems spoke to Zuckerberg’s deep faith in data, code, and personalization. Freed from human limitation, error, and bias, Facebook’s algorithms were capable, he believed, of unparalleled objectivity—­ and, perhaps more important, efficiency. A separate strain of machine learning work was devoted to figuring out what content was actually in the posts Facebook recommended. Known as classifiers, these were AI systems trained to perform pattern recognition on vast data sets. Years before Facebook’s creation, classifiers had proven themselves indispensable in the fight against spam, allowing email providers to move beyond simple keyword filters that sought to block mass emails about, say, “Vi@gra.” By ingesting and comparing a huge collection of emails—some labeled as spam, some as not spam—­ a machine learning system could develop its own rubric for distinguishing between them. Once this classifier was “trained,” it would be set loose, analyzing incoming email and predicting the probability that each message should be sent to an inbox, a junk folder, or straight to hell. By the time machine learning experts began to arrive at Facebook, the list of questions that classifiers sought to answer had grown well past “Is it spam?,” thanks in large part to people like LeCun. Zuckerberg was bullish on its future progress and its applications for Facebook. By 2016, he was predicting that classifiers would surpass human capacities of perception, recognition, and comprehension within the next five to ten years, allowing the company to shut down misbehavior and make huge leaps in connecting the world. That prediction would prove more than a little optimistic. Even as techniques improved, data sets grew, and processing sped up, one drawback of machine learning persisted. The algorithms that the company produced stubbornly refused to explain themselves. Engineers could evaluate a classifier’s success by testing it to see what percentage of its judgment calls were accurate (its “precision”) and what portion of a thing it detected (its “recall”). But because the system was teaching itself how to identify something based on a logic of its own design, when it erred, there was no human-­cognizable reason why. Sometimes mistakes would seem nonsensical. Other times they would be systematic in ways that reflected human error. Early in Facebook’s efforts to deploy a classifier to detect pornography, Arturo Bejar recalled, the system routinely tried to cull images of beds. Rather than learning to identify people screwing, the model had instead taught itself to recognize the furniture on which they most often did. The problem had an easy fix: engineers simply needed to train the model with more PG-­ rated mattress scenes. It made for a good joke—­ as long as you didn’t consider that the form of machine learning that the engineers had just screwed up was one of the most basic that Facebook was using. Similarly fundamental errors kept occurring, even as the company came to rely on far more advanced AI techniques to make far weightier and complex decisions than “porn/not porn.” The company was going all in on AI, both to determine what people should see, and also to solve any problems that might arise. There was no question that the computer science was dazzling and the gains concrete. But the speed, breadth, and scale of Face- book’s adoption of machine learning came at the cost of comprehensibility. Why did Facebook’s “Pages You Might Like” algorithm seem so focused on recommending certain topics? How had a video snippet from a computer animation about dental implants ended up being seen a hundred million times? And why did some news publishers consistently achieve virality when they just rewrote other outlets’ stories? Faced with these questions, Facebook’s Communications team would note that the company’s systems responded to people’s behavior and that there was no accounting for taste. These were difficult points to refute. They also obscured an uncomfortable fact: Facebook was achieving its growth in ways it didn’t fully understand. Within five years of announcing that it was beginning to use machine learning to recommend content and target ads, Facebook’s systems would rely so heavily on AI capable of training itself that, without the technology, Yann LeCun proudly declared, all that would be left of the company’s products would be “dust.” ","The following is excerpted from BROKEN CODE : Inside Facebook and the Fight to Expose Its Harmful Secrets by Jeff Horwitz . Reprinted by permission of Doubleday , an imprint of The Knopf Doubleday Publishing Group , a division of Penguin Random House LLC . Copyright © 2023 by Jeff Horwitz . In 2006 , the U.S. patent office received a filing for “ an automatically generated display that contains information relevant to a user about another user of a social network. ” Rather than forcing people to search through “ disparate and disorganized ” content for items of interest , the system would seek to generate a list of “ relevant ” information in a “ preferred order. ” The listed authors were “ Zuckerberg et al. ” and the product was the News Feed . The idea of showing users streams of activity wasn ’ t entirely new—­ photo-­sharing website Flickr and others had been experimenting with it—­ but the change was massive . Before , Facebook users would interact with the site mainly via notifications , pokes , or looking up friends ’ profiles . With the launch of the News Feed , users got a constantly updating stream of posts and status changes . The shift came as a shock to what were Facebook ’ s then 10 million users , who did not appreciate their activities being monitored and their once-­ static profiles mined for updated content . In the face of widespread complaints , Zuckerberg wrote a post reassuring users , “ Nothing you do is being broadcast ; rather , it is being shared with people who care about what you do—­ your friends. ” He titled it : “ Calm down . Breathe . We hear you. ” Hearing user complaints wasn ’ t the same thing as listening to them . As Chris Cox would later note at a press event , News Feed was an instant success at boosting activity on the platform and connecting users . Engagement quickly doubled , and within two weeks of launch more than a million members had affiliated themselves with a single interest for the first time . The cause that had united so many people ? A petition to eradicate the “ stalkeresque ” News Feed . The opaque system that users revolted against was , in hindsight , remarkably simple . Content mostly appeared in reverse chronological order , with manual adjustments made to ensure that people saw both popular posts and a range of material . “ In the beginning , News Feed ranking was turning knobs , ” Cox said . Fiddling with dials worked well enough for a little while , but everyone ’ s friend lists were growing and Facebook was introducing new features such as ads , pages , and interest groups . As entertainment , memes , and commerce began to compete with posts from friends in News Feed , Facebook needed to ensure that a user who had just logged on would see their best friend ’ s engagement photos ahead of a cooking page ’ s popular enchilada recipe . The first effort at sorting , eventually branded “ EdgeRank , ” was a simple formula that prioritized content according to three principal factors : a post ’ s age , the amount of engagement it got , and the interconnection between user and poster . As an algorithm , it wasn ’ t much—­ just a rough attempt to translate the questions “ Is it new , popular , or from someone you care about ? ” into math . There was no dark magic at play , but users again revolted against the idea of Facebook putting its thumb on what they saw . And , again , Facebook usage metrics jumped across the board . The platform ’ s recommendation systems were still in their infancy , but the dissonance between users ’ vocal disapproval and avid usage led to an inescapable conclusion inside the company : regular people ’ s opinions about Facebook ’ s mechanics were best ignored . Users screamed “ stop , ” Facebook kept going , and everything would work out dandy . By 2010 , the company was looking to move beyond EdgeRank ’ s crude formula to recommend content based on machine learning , a branch of artificial intelligence focused on training computers to design their own decision-­ making algorithms . Rather than programming Facebook ’ s computers to rank content according to simple math , engineers would program them to analyze user behavior and design their own ranking formulas . What people saw would be the result of constant experimentation , the platform serving up whatever it predicted was most likely to generate a like from a user and evaluating its own results in real time . Despite the growing complexity of its product and the collection of user data at a scale the world had never seen , Facebook still didn ’ t know enough about its users to show them relevant ads . Brands loved the attention and buzz they could get from creating content on Facebook , but they hadn ’ t found the company ’ s paid offerings compelling . In May 2012 , General Motors killed its entire Facebook advertising budget . A prominent digital advertising executive declared Facebook ads “ fundamentally some of the worst performing ad units on the Web. ” Fixing the problem would fall to a team run by Joaquin Quiñonero Candela . A Spaniard who grew up in Morocco , Quiñonero was living in the UK and working on artificial intelligence at Microsoft in 2011 when friends scattered across Northern Africa began talking excitedly about social media–­ driven protests . The machine learning techniques he was using to optimize Bing search ads had clear applications to the social networks that people had used to overthrow four autocratic states and nearly topple several more . “ I joined Facebook because of the Arab Spring , ” Quiñonero said . Quiñonero found that the way Facebook built its products was nearly as revolutionary as their results . Invited by a friend to tour the Menlo Park campus , he was shocked to look over the shoulder of an engineer making a significant but unsupervised update to Facebook ’ s code . Confirming how much faster the company moved than Microsoft , Quiñonero received a Facebook job offer a week later . Quiñonero began working on ads , and his timing could hardly have been better . Advances in machine learning and raw computing speed allowed the platform to not only pigeonhole users into demographic niches ( “ single heterosexual woman in San Francisco , late twenties , interested in camping and salsa dancing ” ) but to spot correlations between what they clicked on and then use that information to guess which ads they would find relevant . After beginning with near- random guesses on how to maximize the odds of a click , the system would learn from its hits and misses , refining its model for predicting which ads had the best shot at success . It was hardly omniscient—­ recommended ads were regularly inexplicable . But the bar for success in digital advertising was low : if 2 percent of users clicked on an ad , that was a triumph . With billions of ads served each day , algorithm tweaks that produced even modest gains could bring in tens or hundreds of millions of dollars in revenue . And Quiñonero ’ s team found that it could churn out those alterations . “ I told my team to go fast , to ship every week , ” he said . Consumers are getting their first samplings of meat made in the lab . The rapid pace made sense . The team ’ s AI was improving not just revenue but how people felt about the platform . Better-­ targeted ads meant Facebook could make more money per user without increasing the ad load , and there wasn ’ t all that much that could go wrong . When Facebook pitched denture cream to teenagers , nobody died . Advertising was the beachhead for machine learning at Facebook , and soon everyone wanted a piece of the action . For product executives tasked with increasing the number of Facebook groups joined , friends added , and posts made , the appeal was obvious . If Quiñonero ’ s techniques could increase how often users engaged with ads , they could increase how often users engaged with everything else on the platform . Every team responsible for ranking or recommending content rushed to overhaul their systems as fast as they could , setting off an explosion in the complexity of Facebook ’ s product . Employees found that the biggest gains often came not from deliberate initiatives but from simple futzing around . Rather than redesigning algorithms , which was slow , engineers were scoring big with quick and dirty machine learning experiments that amounted to throwing hundreds of variants of existing algorithms at the wall and seeing which versions stuck—­ which performed best with users . They wouldn ’ t necessarily know why a variable mattered or how one algorithm outperformed another at , say , predicting the likelihood of commenting . But they could keep fiddling until the machine learning model produced an algorithm that statistically outperformed the existing one , and that was good enough . It would be hard to conceive of an approach to building systems that more embodied the slogan “ Move Fast and Break Things. ” Facebook wanted only more . Zuckerberg wooed Yann LeCun , a French computer scientist specializing in deep learning , meaning the construction of computer systems capable of processing information in ways inspired by human thinking . Already renowned for creating the foundational AI techniques that made facial recognition possible , LeCun was put in charge of a division that aimed to put Facebook at the vanguard of fundamental research into artificial intelligence . Following his success with ads , Quiñonero was given an equally formidable task : pushing machine learning into the company ’ s bloodstream as fast as possible . His initial staff of two dozen—­ the team responsible for building new core machine learning tools and making them available to other parts of the company—­ had grown in the three years since he ’ d been hired . But it was still nowhere near large enough to assist every product team that wanted machine learning help . The skills to build a model from scratch were too specialized for engineers to readily pick up , and you couldn ’ t increase the supply of machine learning PhDs by throwing money around . The solution was to build FB Learner , a sort of “ paint by numbers ” version of machine learning . It packaged techniques into a template that could be used by engineers who quite literally did not understand what they were doing . FB Learner did for machine learning inside Facebook what services like WordPress had once done for building websites , rendering the need to muck around with HTML or configure a server unnecessary . Rather than setting up a blog , however , the engineers in question were messing with the guts of what was rapidly becoming a preeminent global communications platform . Many at Facebook were aware of the increasing concerns around AI outside the company ’ s walls . Poorly designed algorithms meant to reward good healthcare penalized hospitals that treated sicker patients , and models purporting to quantify a parole candidate ’ s risk of reoffending turned out to be biased in favor of keeping Black people in jail . But these issues seemed remote on a social network . An avid user of FB Learner would later describe machine learning ’ s mass diffusion inside Facebook as “ giving rocket launchers to twenty-­ five-­ year-­ old engineers. ” But at the time , Quiñonero and the company spoke of it as a triumph . The company ’ s AI algorithms gave it an insatiable habit for lies and hate speech . Now the man who built them ca n't fix the problem . “ Engineers and teams , even with little expertise , can build and run experiments with ease and deploy AI-­ powered products to production faster than ever , ” Facebook announced in 2016 , boasting that FB Learner was ingesting trillions of data points on user behavior every day and that engineers were running 500,000 experiments on them a month . The sheer amount of data that Facebook collected—­ and ad-targeting results so good that users regularly suspected ( wrongly ) the company of eavesdropping on their offline conversations—gave rise to the claim that “ Facebook knows everything about you. ” That wasn ’ t quite correct . The wonders of machine learning had obscured its limits . Facebook ’ s recommendation systems worked by raw correlation between user behavior , not by identifying a user ’ s tastes and interests and then serving content based on it . News Feed couldn ’ t tell you whether you liked ice skating or dirt biking , hip-­ hop or K-­ pop , and it couldn ’ t explain in human terms why one post appeared in your feed above another . Although this inexplicability was an obvious drawback , machine learning–­ based recommendation systems spoke to Zuckerberg ’ s deep faith in data , code , and personalization . Freed from human limitation , error , and bias , Facebook ’ s algorithms were capable , he believed , of unparalleled objectivity—­ and , perhaps more important , efficiency . A separate strain of machine learning work was devoted to figuring out what content was actually in the posts Facebook recommended . Known as classifiers , these were AI systems trained to perform pattern recognition on vast data sets . Years before Facebook ’ s creation , classifiers had proven themselves indispensable in the fight against spam , allowing email providers to move beyond simple keyword filters that sought to block mass emails about , say , “ Vi @ gra. ” By ingesting and comparing a huge collection of emails—some labeled as spam , some as not spam—­ a machine learning system could develop its own rubric for distinguishing between them . Once this classifier was “ trained , ” it would be set loose , analyzing incoming email and predicting the probability that each message should be sent to an inbox , a junk folder , or straight to hell . By the time machine learning experts began to arrive at Facebook , the list of questions that classifiers sought to answer had grown well past “ Is it spam ? , ” thanks in large part to people like LeCun . Zuckerberg was bullish on its future progress and its applications for Facebook . By 2016 , he was predicting that classifiers would surpass human capacities of perception , recognition , and comprehension within the next five to ten years , allowing the company to shut down misbehavior and make huge leaps in connecting the world . That prediction would prove more than a little optimistic . Even as techniques improved , data sets grew , and processing sped up , one drawback of machine learning persisted . The algorithms that the company produced stubbornly refused to explain themselves . Engineers could evaluate a classifier ’ s success by testing it to see what percentage of its judgment calls were accurate ( its “ precision ” ) and what portion of a thing it detected ( its “ recall ” ) . But because the system was teaching itself how to identify something based on a logic of its own design , when it erred , there was no human-­cognizable reason why . Sometimes mistakes would seem nonsensical . Other times they would be systematic in ways that reflected human error . Early in Facebook ’ s efforts to deploy a classifier to detect pornography , Arturo Bejar recalled , the system routinely tried to cull images of beds . Rather than learning to identify people screwing , the model had instead taught itself to recognize the furniture on which they most often did . The problem had an easy fix : engineers simply needed to train the model with more PG-­ rated mattress scenes . It made for a good joke—­ as long as you didn ’ t consider that the form of machine learning that the engineers had just screwed up was one of the most basic that Facebook was using . Similarly fundamental errors kept occurring , even as the company came to rely on far more advanced AI techniques to make far weightier and complex decisions than “ porn/not porn. ” The company was going all in on AI , both to determine what people should see , and also to solve any problems that might arise . There was no question that the computer science was dazzling and the gains concrete . But the speed , breadth , and scale of Face- book ’ s adoption of machine learning came at the cost of comprehensibility . Why did Facebook ’ s “ Pages You Might Like ” algorithm seem so focused on recommending certain topics ? How had a video snippet from a computer animation about dental implants ended up being seen a hundred million times ? And why did some news publishers consistently achieve virality when they just rewrote other outlets ’ stories ? Faced with these questions , Facebook ’ s Communications team would note that the company ’ s systems responded to people ’ s behavior and that there was no accounting for taste . These were difficult points to refute . They also obscured an uncomfortable fact : Facebook was achieving its growth in ways it didn ’ t fully understand . Within five years of announcing that it was beginning to use machine learning to recommend content and target ads , Facebook ’ s systems would rely so heavily on AI capable of training itself that , without the technology , Yann LeCun proudly declared , all that would be left of the company ’ s products would be “ dust . ”","['follow', 'excerpt', 'broken', 'code', 'facebook', 'fight', 'expose', 'harmful', 'secret', 'reprint', 'permission', 'doubleday', 'imprint', 'knopf', 'publishing', 'group', 'division', 'llc', 'copyright', '©', 'patent', 'office', 'receive', 'filing', 'automatically', 'generate', 'display', 'contain', 'information', 'relevant', 'user', 'user', 'social', 'network', 'rather', 'force', 'people', 'search', 'disparate', 'disorganized', 'content', 'item', 'interest', 'system', 'seek', 'generate', 'list', 'relevant', 'information', 'preferred', 'order', 'list', 'author', 'product', 'news', 'feed', 'idea', 'show', 'user', 'stream', 'activity', 'entirely', 'new—\xad', 'photo\xadsharing', 'website', 'experiment', 'it—\xad', 'change', 'massive', 'facebook', 'user', 'interact', 'site', 'mainly', 'notification', 'poke', 'look', 'friend', 'profile', 'launch', 'news', 'feed', 'user', 'get', 'constantly', 'update', 'stream', 'post', 'status', 'change', 'shift', 'come', 'shock', 'facebook', 'user', 'appreciate', 'activity', 'monitor', 'static', 'profile', 'mine', 'update', 'content', 'face', 'widespread', 'complaint', 'write', 'post', 'reassure', 'user', 'broadcast', 'rather', 'share', 'people', 'care', 'do—\xad', 'friend', 'title', 'calm', 'breathe', 'hear', 'hear', 'user', 'complaint', 'thing', 'listen', 'later', 'note', 'press', 'event', 'news', 'feed', 'instant', 'success', 'boost', 'activity', 'platform', 'connect', 'user', 'engagement', 'quickly', 'double', 'week', 'launch', 'member', 'affiliate', 'single', 'interest', 'first', 'time', 'cause', 'unite', 'many', 'people', 'petition', 'eradicate', 'stalkeresque', 'news', 'feed', 'opaque', 'system', 'user', 'revolt', 'hindsight', 'remarkably', 'simple', 'content', 'mostly', 'appear', 'reverse', 'chronological', 'order', 'manual', 'adjustment', 'make', 'ensure', 'people', 'see', 'popular', 'post', 'range', 'material', 'beginning', 'news', 'feed', 'ranking', 'turn', 'knob', 'say', 'fiddle', 'dial', 'work', 'well', 'enough', 'little', 'friend', 'list', 'grow', 'facebook', 'introduce', 'new', 'feature', 'ad', 'page', 'interest', 'group', 'entertainment', 'meme', 'begin', 'compete', 'post', 'friend', 'news', 'feed', 'facebook', 'need', 'ensure', 'user', 'log', 'see', 'good', 'friend', 'engagement', 'photo', 'ahead', 'cooking', 'page', 'popular', 'enchilada', 'recipe', 'first', 'effort', 'sort', 'eventually', 'brand', 'edgerank', 'simple', 'formula', 'prioritize', 'content', 'accord', 'principal', 'factor', 'post', 'age', 'amount', 'engagement', 'get', 'interconnection', 'user', 'poster', 'rough', 'attempt', 'translate', 'question', 'new', 'popular', 'care', 'math', 'dark', 'magic', 'play', 'user', 'revolt', 'idea', 'facebook', 'put', 'thumb', 'see', 'facebook', 'usage', 'metric', 'jump', 'board', 'platform', 'recommendation', 'system', 'still', 'infancy', 'dissonance', 'user', 'vocal', 'disapproval', 'avid', 'usage', 'lead', 'inescapable', 'conclusion', 'company', 'regular', 'people', 'opinion', 'mechanic', 'well', 'ignore', 'user', 'scream', 'stop', 'keep', 'go', 'work', 'dandy', 'company', 'look', 'move', 'crude', 'formula', 'recommend', 'content', 'base', 'machine', 'learn', 'branch', 'artificial', 'intelligence', 'focus', 'training', 'computer', 'design', 'decision\xad', 'making', 'algorithm', 'rather', 'programming', 'facebook', 'computer', 'rank', 'content', 'accord', 'simple', 'math', 'engineer', 'program', 'analyze', 'user', 'behavior', 'design', 'ranking', 'formula', 'people', 'see', 'result', 'constant', 'experimentation', 'platform', 'serve', 'predict', 'likely', 'generate', 'like', 'user', 'evaluate', 'result', 'real', 'time', 'grow', 'complexity', 'product', 'collection', 'user', 'datum', 'scale', 'world', 'never', 'see', 'facebook', 'still', 'know', 'enough', 'user', 'show', 'relevant', 'ad', 'brand', 'love', 'attention', 'buzz', 'get', 'create', 'content', 'facebook', 'find', 'company', 'pay', 'offering', 'compelling', 'general', 'motor', 'kill', 'entire', 'facebook', 'advertising', 'budget', 'prominent', 'digital', 'advertising', 'executive', 'declare', 'facebook', 'ad', 'fundamentally', 'bad', 'perform', 'ad', 'unit', 'web', 'fix', 'problem', 'fall', 'team', 'run', 'spaniard', 'grow', 'quiñonero', 'live', 'work', 'artificial', 'intelligence', 'friend', 'scatter', 'begin', 'talk', 'excitedly', 'social', 'media–\xad', 'drive', 'protest', 'machine', 'learn', 'technique', 'use', 'optimize', 'bing', 'search', 'ad', 'clear', 'application', 'social', 'network', 'people', 'use', 'overthrow', 'autocratic', 'state', 'nearly', 'topple', 'several', 'join', 'facebook', 'arab', 'spring', 'quiñonero', 'say', 'quiñonero', 'find', 'way', 'facebook', 'build', 'product', 'nearly', 'revolutionary', 'result', 'invite', 'friend', 'tour', 'menlo', 'park', 'campus', 'shocked', 'look', 'shoulder', 'engineer', 'make', 'significant', 'unsupervised', 'update', 'code', 'confirm', 'much', 'fast', 'company', 'move', 'quiñonero', 'receive', 'facebook', 'job', 'offer', 'week', 'later', 'quiñonero', 'begin', 'work', 'ad', 'timing', 'hardly', 'well', 'advance', 'machine', 'learning', 'raw', 'computing', 'speed', 'allow', 'platform', 'pigeonhole', 'user', 'demographic', 'niche', 'single', 'heterosexual', 'woman', 'late', 'twenty', 'interested', 'camping', 'salsa', 'dance', 'spot', 'correlation', 'click', 'use', 'information', 'guess', 'ad', 'find', 'relevant', 'begin', 'random', 'guess', 'maximize', 'odd', 'click', 'system', 'learn', 'hit', 'miss', 'refine', 'model', 'predict', 'ad', 'good', 'shot', 'success', 'hardly', 'omniscient—\xad', 'recommend', 'ad', 'regularly', 'inexplicable', 'bar', 'success', 'digital', 'advertising', 'low', 'percent', 'user', 'click', 'ad', 'triumph', 'billion', 'ad', 'serve', 'day', 'tweak', 'produce', 'even', 'modest', 'gain', 'bring', 'ten', 'hundred', 'million', 'dollar', 'revenue', 'quiñonero', 'team', 'find', 'churn', 'alteration', 'tell', 'team', 'go', 'fast', 'ship', 'week', 'say', 'consumer', 'get', 'first', 'sampling', 'meat', 'make', 'lab', 'rapid', 'pace', 'make', 'sense', 'team', 'improve', 'revenue', 'people', 'feel', 'platform', 'better\xad', 'target', 'ad', 'mean', 'facebook', 'make', 'money', 'user', 'increase', 'ad', 'load', 'much', 'go', 'wrong', 'pitch', 'denture', 'cream', 'teenager', 'die', 'advertising', 'beachhead', 'machine', 'learn', 'facebook', 'soon', 'want', 'piece', 'action', 'product', 'executive', 'task', 'increase', 'number', 'facebook', 'group', 'join', 'friend', 'add', 'post', 'make', 'appeal', 'obvious', 'quiñonero', 'technique', 'increase', 'often', 'user', 'engage', 'ad', 'increase', 'often', 'user', 'engage', 'else', 'platform', 'team', 'responsible', 'rank', 'recommend', 'content', 'rush', 'overhaul', 'system', 'fast', 'set', 'explosion', 'complexity', 'product', 'employee', 'find', 'big', 'gain', 'often', 'come', 'deliberate', 'initiative', 'simple', 'futzing', 'around', 'rather', 'redesign', 'algorithm', 'slow', 'engineer', 'score', 'big', 'quick', 'dirty', 'machine', 'learn', 'experiment', 'amount', 'throw', 'hundred', 'variant', 'exist', 'algorithm', 'wall', 'see', 'version', 'stuck—\xad', 'perform', 'well', 'user', 'necessarily', 'know', 'variable', 'matter', 'outperform', 'say', 'predict', 'likelihood', 'commenting', 'keep', 'fiddle', 'machine', 'learning', 'model', 'produce', 'statistically', 'outperform', 'exist', 'one', 'good', 'enough', 'hard', 'conceive', 'approach', 'building', 'system', 'embody', 'slogan', 'move', 'fast', 'break', 'thing', 'want', 'woo', 'french', 'computer', 'scientist', 'specialize', 'deep', 'learning', 'mean', 'construction', 'computer', 'system', 'capable', 'process', 'information', 'way', 'inspire', 'human', 'thinking', 'already', 'renowne', 'create', 'foundational', 'ai', 'technique', 'make', 'facial', 'recognition', 'possible', 'lecun', 'put', 'charge', 'division', 'aim', 'put', 'facebook', 'vanguard', 'fundamental', 'research', 'artificial', 'intelligence', 'follow', 'success', 'ad', 'quiñonero', 'give', 'equally', 'formidable', 'task', 'push', 'machine', 'learn', 'company', 'bloodstream', 'fast', 'possible', 'initial', 'staff', 'team', 'responsible', 'build', 'new', 'core', 'machine', 'learning', 'tool', 'make', 'available', 'part', 'company—\xad', 'grow', 'year', 'hire', 'still', 'nowhere', 'near', 'large', 'enough', 'assist', 'product', 'team', 'want', 'machine', 'learning', 'help', 'skill', 'build', 'model', 'scratch', 'specialized', 'engineer', 'readily', 'pick', 'increase', 'supply', 'machine', 'learn', 'phds', 'throw', 'money', 'solution', 'build', 'fb', 'learner', 'sort', 'paint', 'number', 'version', 'machine', 'learn', 'package', 'technique', 'template', 'use', 'engineer', 'quite', 'literally', 'understand', 'fb', 'learner', 'machine', 'learn', 'facebook', 'service', 'wordpress', 'build', 'website', 'render', 'need', 'muck', 'html', 'configure', 'server', 'unnecessary', 'rather', 'set', 'blog', 'however', 'engineer', 'question', 'mess', 'gut', 'rapidly', 'become', 'preeminent', 'global', 'communication', 'platform', 'many', 'facebook', 'aware', 'increase', 'concern', 'ai', 'company', 'wall', 'poorly', 'design', 'algorithm', 'mean', 'reward', 'good', 'healthcare', 'penalize', 'hospital', 'treat', 'sicker', 'patient', 'model', 'purport', 'quantify', 'parole', 'candidate', 'risk', 'reoffending', 'turn', 'bias', 'favor', 'keep', 'black', 'people', 'jail', 'issue', 'seem', 'remote', 'social', 'network', 'avid', 'user', 'fb', 'learner', 'later', 'describe', 'machine', 'learning', 'mass', 'diffusion', 'facebook', 'give', 'rocket', 'launcher', 'year\xad', 'old', 'engineer', 'time', 'quiñonero', 'company', 'speak', 'triumph', 'company', 'ai', 'algorithm', 'give', 'insatiable', 'habit', 'lie', 'hate', 'speech', 'man', 'build', 'fix', 'problem', 'engineer', 'team', 'even', 'little', 'expertise', 'build', 'run', 'experiment', 'ease', 'deploy', 'ai\xad', 'powered', 'product', 'production', 'fast', 'ever', 'facebook', 'announce', 'boasting', 'fb', 'learner', 'ingest', 'trillion', 'datum', 'point', 'user', 'behavior', 'day', 'engineer', 'run', 'experiment', 'month', 'sheer', 'amount', 'datum', 'facebook', 'collected—\xad', 'adtargete', 'result', 'good', 'user', 'regularly', 'suspect', 'wrongly', 'company', 'eavesdropping', 'offline', 'conversation', 'give', 'rise', 'claim', 'facebook', 'know', 'quite', 'correct', 'wonder', 'machine', 'learning', 'obscure', 'limit', 'facebook', 'recommendation', 'system', 'work', 'raw', 'correlation', 'user', 'behavior', 'identify', 'user', 'taste', 'interest', 'serve', 'content', 'base', 'news', 'tell', 'like', 'ice', 'skating', 'dirt', 'biking', 'hip\xad', 'hop', 'pop', 'explain', 'human', 'term', 'post', 'appear', 'feed', 'inexplicability', 'obvious', 'drawback', 'machine', 'base', 'recommendation', 'system', 'speak', 'deep', 'faith', 'data', 'code', 'personalization', 'free', 'human', 'limitation', 'error', 'bias', 'algorithm', 'capable', 'believe', 'unparalleled', 'objectivity—\xad', 'perhaps', 'important', 'efficiency', 'separate', 'strain', 'machine', 'learning', 'work', 'devote', 'figure', 'content', 'actually', 'post', 'facebook', 'recommend', 'know', 'classifier', 'system', 'train', 'perform', 'pattern', 'recognition', 'vast', 'datum', 'set', 'year', 'creation', 'classifier', 'prove', 'indispensable', 'fight', 'spam', 'allow', 'email', 'provider', 'move', 'simple', 'keyword', 'filter', 'seek', 'block', 'mass', 'email', 'say', 'vi', 'gra', 'ingest', 'compare', 'huge', 'collection', 'email', 'label', 'spam', 'spam—\xad', 'machine', 'learning', 'system', 'develop', 'rubric', 'distinguish', 'classifier', 'train', 'set', 'loose', 'analyze', 'incoming', 'email', 'predict', 'probability', 'message', 'send', 'inbox', 'junk', 'folder', 'straight', 'hell', 'time', 'machine', 'learn', 'expert', 'begin', 'arrive', 'facebook', 'list', 'question', 'classifier', 'seek', 'answer', 'grow', 'well', 'past', 'spam', 'thank', 'large', 'part', 'people', 'bullish', 'future', 'progress', 'application', 'facebook', 'predict', 'classifier', 'surpass', 'human', 'capacity', 'perception', 'recognition', 'comprehension', 'next', 'year', 'allow', 'company', 'shut', 'misbehavior', 'make', 'huge', 'leap', 'connect', 'world', 'prediction', 'prove', 'little', 'optimistic', 'even', 'technique', 'improve', 'datum', 'set', 'grow', 'processing', 'speed', 'drawback', 'machine', 'learning', 'persist', 'algorithm', 'company', 'produce', 'stubbornly', 'refuse', 'explain', 'engineer', 'evaluate', 'classifier', 'success', 'test', 'see', 'percentage', 'judgment', 'call', 'accurate', 'precision', 'portion', 'thing', 'detect', 'recall', 'system', 'teach', 'identify', 'base', 'logic', 'design', 'err', 'human\xadcognizable', 'reason', 'sometimes', 'mistake', 'seem', 'nonsensical', 'time', 'systematic', 'way', 'reflect', 'human', 'error', 'early', 'effort', 'deploy', 'classifier', 'detect', 'pornography', 'arturo', 'recall', 'system', 'routinely', 'try', 'cull', 'image', 'bed', 'rather', 'learn', 'identify', 'people', 'screw', 'model', 'instead', 'teach', 'recognize', 'furniture', 'often', 'problem', 'easy', 'fix', 'engineer', 'simply', 'need', 'train', 'model', 'pg\xad', 'rate', 'mattress', 'scene', 'make', 'good', 'joke—\xad', 'long', 'consider', 'form', 'machine', 'learn', 'engineer', 'screw', 'basic', 'facebook', 'use', 'similarly', 'fundamental', 'error', 'keep', 'occur', 'even', 'company', 'come', 'rely', 'far', 'advanced', 'ai', 'technique', 'make', 'far', 'weighty', 'complex', 'decision', 'pornnot', 'porn', 'company', 'go', 'ai', 'determine', 'people', 'see', 'also', 'solve', 'problem', 'arise', 'question', 'computer', 'science', 'dazzle', 'gain', 'concrete', 'speed', 'breadth', 'scale', 'face', 'book', 'adoption', 'machine', 'learning', 'come', 'cost', 'comprehensibility', 'facebook', 'page', 'like', 'seem', 'focused', 'recommend', 'certain', 'topic', 'video', 'snippet', 'computer', 'animation', 'dental', 'implant', 'end', 'see', 'time', 'news', 'publisher', 'consistently', 'achieve', 'virality', 'rewrote', 'outlet', 'story', 'face', 'question', 'facebook', 'communication', 'team', 'note', 'company', 'system', 'respond', 'people', 'behavior', 'accounting', 'taste', 'difficult', 'point', 'refute', 'also', 'obscure', 'uncomfortable', 'fact', 'facebook', 'achieve', 'growth', 'way', 'fully', 'understand', 'year', 'announce', 'begin', 'use', 'machine', 'learning', 'recommend', 'content', 'target', 'ad', 'system', 'rely', 'heavily', 'ai', 'capable', 'train', 'technology', 'lecun', 'proudly', 'declare', 'leave', 'company', 'product', 'dust']","<p>In an excerpt from <em>Broken Code: Inside Facebook and the Fight to Expose its Harmful Secrets</em><em>, Jeff Horwitz reveals how the company came to rely on artificial intelligence.</em></p>
"
Augmenting the realities of work,https://www.technologyreview.com/2023/11/29/1083726/augmenting-the-realities-of-work/,2023-11-29,"In association withJPMorgan Chase &Co Imagine an integrated workplace with 3D visualizations that augment presentations, interactive and accelerated onboarding, and controlled training simulations. This is the future of immersive technology that global head of Immersive Technology Research at JPMorgan Chase, Blair MacIntyre is working to build. Augmented reality (AR) and virtual reality (VR) technologies can blend physical and digital dimensions together and infuse new innovations and efficiencies into business and customer experiences. ""These technologies can offer newer ways of collaborating over distance both synchronously and asynchronously than we can get with the traditional work technologies that we use right now,"" says MacIntyre. ""It's these new ways to collaborate, ways of using the environment and space in new and interesting ways that will hopefully offer new value and change the way we work."" Many enterprises are integrating VR into business practices like video conference calls. But having some participants in a virtual world and some sidelined creates imbalances in the employee experience. MacIntyre's team is looking for ways to use AR/VR technologies that can be additive, like 3D data visualizations that enhance financial forecasting within a bank, not ones that overhaul entire experiences. Although the potential of AR/VR is quickly evolving, it's unlikely that customers’ interactions or workplace environments will be entirely moved to the virtual world anytime soon. Rather, MacIntyre's immersive technology research looks to infuse efficiencies into existing practices. ""It's thinking about how the technologies integrate and how we can add value where there is value and not trying to replace everything we do with these technologies,"" MacIntyre says. AI can help remove some of the tedium from immersive technologies that have made them impractical for widespread enterprise use in the past. Using VR technology in the workplace may prohibit taking notes and having access to traditional input devices and files. AI tools can take and transcribe notes and fill in any other gaps to help remove that friction and eliminate redundancies. Connected Internet of things (IoT) devices are also key to enabling AR/VR technologies. To create a valuable immersive experience, MacIntyre says, it's imperative to know as much about the surrounding world of the user as well as their needs, habits, and preferences. ""If we can figure out more ways of enabling people to work together in a distributed way, we can start enabling more people to participate meaningfully in a wider variety of jobs,"" says MacIntyre. This episode of Business Lab is produced in association with JPMorgan Chase. Laurel: From MIT Technology Review, I'm Laurel Ruma, and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.Our topic today is emerging technologies, specifically, immersive technologies like augmented and virtual reality. Keeping up with technology trends may be a challenge for most enterprises, but it's a critical way to think about future possibilities from product to customer service to employee experience. Augmented and virtual realities aren't necessarily new, but when it comes to applying them beyond gaming, it's a brave new world.Two words for you: emerging realities.My guest is Blair MacIntyre, who is the global head of Immersive Technology Research at JPMorgan Chase.This podcast is produced in association with JPMorgan Chase.Welcome, Blair. Blair MacIntyre: Thank you. It's great to be here. Laurel: Well, let's do a little bit of context setting. Your career has been focused on researching and exploring immersive technology, including software and design tools, privacy and ethics, and game and experience design. So what brought you to JPMorgan Chase, and could you describe your current role? Blair: So before joining the firm, I had spent the last 23 years as a professor at Georgia Tech and Northeastern University. During that time, as you say, I explored a lot of ways that we can both create things with these technologies, immersive technologies and also, what they might be useful for and what the impacts on people in society and how we experience life are. But as these technologies have become more real, moved out of the lab, starting to see real products from real companies, we have this opportunity to actually see how they might be useful in practice and to have, for me, an impact on how these technologies will be deployed and used that goes beyond the traditional impact that professors might have. So beyond writing papers, beyond teaching students. That's what brought me to the firm, and so my current role is, really, to explore that, to understand all the different ways that immersive technology could impact the firm and its customers. Right? So we think about not just customer-facing and not just products, but also employees and their experience as well. Laurel: That's really interesting. So why does JPMorgan Chase have a dedicated immersive technology focus in its global technology applied research division, and what are the primary goals of your team's research within finance and large enterprises as a whole? Blair: That's a great question. So JPMorgan Chase has a fairly wide variety of research going on within the company. There's large efforts in AI/ML, in quantum computing, blockchain. So they're interested in looking at all of the range of new technologies and how they might impact the firm and our customers, and immersive technologies represent one of those technologies that could over time have a relatively large impact, I think, especially on the employee experience and how we interact with our customers. So they really want to have a group of people focusing on, really, looking both in the near and long term, and thinking about how we can leverage the technology now and how we might be able to leverage it down the road, and not just how we can, but what we should not do. Right? So we're interested in understanding of these applications that are being proposed or people are imagining could be used. Which ones actually have value to the company, and which ones may not actually have value in practice? Laurel: So when people think of immersive technologies like augmented reality and virtual reality, AR and VR, many think of headsets or smartphone apps for gaming and retail shopping experiences. Could you give an overview of the state of immersive technology today and what use cases you find to be the most innovative and interesting in your research? Blair: So, as you say, I think many people think about smartphones, and we've seen, at least in movies and TV shows, head mounts of various kinds. The market, I would divide it right now into the two parts, the handheld phone and tablet experience. So you can do augmented reality now, and that really translates to we take the camera feed, and we can overlay computer graphics on it to do things like see what something you might want to buy looks like in your living room or do, in an enterprise situation, remote maintenance assistance where I can take my phone, point it at a piece of technology, and a remote expert could draw on it or help me do something with it. There’s the phone-based things, and we carry these things in our pockets all the time, and they're relatively cheap. So there's a lot of opportunities when it's appropriate to use those, but the big downside of those devices is that you have to hold them in your hands, so if you wanted to try to put information all around you, you would have to hold the device up and look around, which is uncomfortable and awkward. So that is where the head mount displays come in. So either virtual reality displays which, right now, many of us think about computer games and education as use cases in the consumer world or augmented reality displays. These sorts of displays now let us do the same kind of things we might do with our phones, but we can do it without our hands having to hold something so we can be doing whatever work it was we wanted to do, right? Repairing the equipment, taking notes, working with things in the world around us, and we can have information spread all around us, which I think is the big advantage of head mounts. So many of the things people imagine when they think about augmented reality in particular involve this serendipitous access to information. I'm walking into a conference room, and I see sort of my notes and information about the people I'm meeting there and the materials from our last meeting, whatever it is, or I'm walking down the street, and I see advertising or other kinds of, say, tourism information, but those things only work if the device is out of mind. If I can put it on, and then go about my life, I'm not going to walk into a conference room, and hold up a phone, and look at everybody through it. So that, I think, is the big difference. You could implement the same sorts of applications on both the handheld devices and the head-worn devices, but the two different form factors are going to make very different applications appropriate for those two sorts of technologies.On the virtual reality side, we're at the point now where the displays we can buy are light enough and comfortable enough that we could wear them for half an hour, a couple hours without discomfort. So a lot of the applications that people imagine there, I think the most popular things that people have done research on and that I see having a near-term impact in the enterprise are immersive training applications where you can get into a situation rather than, say, watching a video or a little click-through presentation as part of your annual training. You could really be in an experience and hopefully learn more from it. So I think those sorts of experiences where we're totally immersed and focused is where virtual reality comes in.The big thing that I think is most exciting about head-worn displays in particular where we can wear them while we're doing work as opposed to just having these ephemeral experiences with a phone is the opportunity to do things together, to collaborate. So I might want to look at a map on a table and see a bunch of data floating above the map, but it would be better if you and our other colleagues were around the table with me, and we can all see the same things, or if we want to take a training experience, I could be in there getting my training experience, but maybe someone else is joining me and being able to both offer feedback or guidance and so on. Essentially, when I think about these technologies, I think about the parallels to how we do work regularly, right? We generally collaborate with people. We might grab a colleague and have them look at our laptop to show them something. I might send someone something on my phone, and then we can talk about it. So much of what we do involves interactions with other people and with the data that we are doing our job with that anything we do with these immersive technologies is really going to have to mimic that and give us the ability to do our real work in these immersive spaces with the people that we normally work with. Laurel: Well, speaking of working with people, how can the scale of an institution like JPMorgan Chase help propel this research forward in immersive technology, and what opportunities does it provide that are otherwise limited in a traditional university or startup research environment? Blair: I think it comes down to a few different things. On one hand, we have the access to people who are really doing the things that we want to build technologies to help with. Right? So if I wanted to look how I could use immersive visualization of data to help people in human resources do planning or help people who are doing financial modeling look at the data in new and interesting ways, now I could actually do the research in conjunction with the real people who do that work. Right? So I've already and I've been at the firm for a little over a year, and many conversations we've had were either we've had an idea or somebody has come to us with an idea. Through the course of the conversations, relatively quickly, we hone in on things that are much more sophisticated, much more powerful than what we might have thought of at a university where we didn't have that sort of direct access to people doing the work.On the other hand, if we actually build something, we can actually test it with the same people, which is an amazing opportunity. Right? When I go to a conference, we’re going to put 20 people who actually represent the real users of those systems. So, for me, that's where I think the big opportunity of doing research in an enterprise is, is building solutions for the real people of that enterprise and being able to test it with those people. Laurel: Recent years have actually changed what customers and employees expect from enterprises as well, like omnichannel retail experiences. So immersive technologies can be used to bridge gaps between physical and virtual environments as you were saying earlier. What are the different opportunities that AR and VR can offer enterprises, and how can these technologies be used to improve employee and customer experience? Blair: So I alluded back to some of that in previous answers. I think the biggest opportunities have to do with how employees within the organization can do new things together, can interact, and also how companies can interact with customers. Now, we're not going to move all of our interactions with our customers into the virtual world, or the metaverse, or whatever you want to call it nowadays anytime soon. Right? But I think there are opportunities for customers who are interested in those technologies, and comfortable with them, and excited by them to get new kinds of experiences and new ways of interacting with our firm or other firms than you could get with webpages and in-person meetings. The other big opportunity I think is as we move to a more hybrid work environment and a distributed work environment, so a company like JPMorgan Chase is huge and spread around the world. We have over 300,000 employees now in most countries around the world. There might be groups of people, but they're connected together through video right now. These technologies, I think, can offer new ways of collaborating over distance both synchronously and asynchronously than we can get with the traditional work technologies that we use right now. So it's those new ways to collaborate, ways of using the environment and space in new and interesting ways that is going to, hopefully, offer new value and change the way we work. Laurel: Yeah, and staying on that topic, we can't really have a discussion about technology without talking about AI which is another evolving, increasingly popular technology. So that's being used by many enterprises to reduce redundancies and automate repetitive tasks. In this way, how can immersive technology provide value to people in their everyday work with the help of AI? Blair: So I think the big opportunity that AI brings to immersive technologies is helping ease a lot of the tedium and burden that may have prevented these technologies from being practical in the past, and this could happen in a variety of ways. When I'm in a virtual reality experience, I don't have access to a keyboard, I don't have access to traditional input devices, I don't have necessarily the same sorts of access to my files, and so on. With a lot of the new AI technologies that are coming around, I can start relying on the computer to take notes. I can have new ways of pulling up information that I otherwise wouldn't have access to. So, I think AI reducing the friction of using these technologies is a huge opportunity, and the research community is actively looking at that because friction has been one of the big problems with these technologies up till now. Laurel: So, other than AI, what are other emerging technologies that can aid in immersive technology research and development? Blair: So, aside from AI, if we step back and look at all of the emerging technologies as a whole and how they complement each other, I think we can see new opportunities. So, in our research, we work closely with people doing computer vision and other sort of sensing research to understand the world. We work closely with people looking at internet of things and connected devices because at a 10,000-foot level, all of these technologies are based on the idea of understanding, sensing the world, understanding what people are doing in it, understanding what people's needs might be, and then somehow providing information to them or actuating things in the world, displaying stuff on walls or displays. From that viewpoint, immersive technologies are primarily one way of displaying things in a new and interesting way and getting input from people, knowing what people want to do, allowing them to interact with data. But in order to do that, they need to know as much about the world around the user as possible, the structure of it, but also, who's there, what we are doing, and so on. So all of these other technologies, especially the Internet of things (IoT) and other forms and ways of sensing what's happening in the world are very complimentary and together can create new sorts of experiences that neither could do alone. Laurel: So what are some of the challenges, but also, possible opportunities in your research that contrast the future potential of AR and VR to where the technology is today? Blair: So I think one of the big limitations of technology today is that most of the experiences are very siloed and disconnected from everything else we do. During the pandemic, many of us experimented with how we could have conferences online in various ways, right? A lot of companies, small companies and larger companies, started looking at how you could create immersive meetings and big group experiences using virtual reality technology, but all of those experiences that people created were these closed systems that you couldn't bring things into. So one of the things we're really interested in is how we stop thinking about creating new kinds of experiences and new ways of doing things, and instead think about how do we add these technologies to our existing work practices to enhance them in some way. So, for example. Right now, we do video meetings. It would be more interesting for some people to be able to join those meetings, say, in VR. Companies have experimented with that, but most of the experiments that people are doing assume that everyone is going to move into virtual reality, or we’re going to bring, say, the people in as a little video wall on the side of a big virtual reality room, making them second class citizens. I'm really interested and my team is interested in how we can start incorporating technologies like this while keeping everyone a first-class participant in these meetings. As one example, a lot of the systems that large enterprises build, and we're no different, are web-based right now. So if, let's say, I have a system to do financial forecasting, you could imagine there's a bunch of those at a bank, and it's a web-based system, I'm really interested in how do we add the ability for people to go into a virtual reality or augmented reality experience, say, a 3D visualization of some kind of data at the moment they want to do it, do the work that they want to do, invite colleagues in to discuss things, and then go back to the work as it was always done on a desktop web browser.So that idea of thinking of these technologies as a capability, a feature instead of a new whole application and way of doing things permeates all the work we're doing. When I look down the road at where this can go, I see in, say, let's say, two to five years, I see people with displays maybe sitting on their desk. They have their tablet and their phone, and they might also have another display or two sitting there. They're doing their work, and at different times, they might be in a video chat, they might pick up a head mount and put it on to do different things, but it's all integrated. I'm really interested in how we connect these together and reduce friction. Right? If it takes you four or five minutes to move your work into a VR experience, nobody is going to do it because it just is too problematic. So it's that. It's thinking about how the technologies integrate and how we can add value where there is value and not trying to replace everything we do with these technologies. Laurel: So to stay on that future focus, how do you foresee the immersive technology landscape entirely evolving over the next decade, and how will your research enable those changes? Blair: So, at some level, it's really hard to answer that question. Right? So if I think back 10 years to where immersive technologies were, it would have been inconceivable for us to imagine the videos that are coming out. So, at some level, I can say, ""Well, I have no idea where we're going to be in 10 years."" On the other hand, it's pretty safe to imagine the kinds of technologies that we're experimenting with now just getting better, and more comfortable, and more easy to integrate into work. So I think the landscape is going to evolve in the near term to be more amenable to work. Especially for augmented reality, the threshold that these devices would have to get to such that a lot of people would be willing to wear them all the time while they're walking down the street, playing sports, doing whatever, that's a very high bar because it has to be small, it has to be light, it has to be cheap, it has to have a battery that lasts all day, etcetera, etcetera. On the other hand, in the enterprise, in any business situation, it's easy to imagine the scenario I described. It's sitting on my desk, I pick it up, I put it on, I take it off. In the medium term after that, I think we will see more consumer applications as people start solving more of the problems that are preventing people from wearing these devices for longer periods of time. Right? It's not just size, and battery power, and comfort, it's also things like optics. Right? A lot of people — not a lot, but say, let's say 10%, 15% of people might experience headaches, or nausea, or other kinds of discomfort when they wear a VR display as they're currently built, and a lot of that has to do with the fact that the optics that you're looking at when you're putting this display are built in a way that makes it hard to comfortably focus at objects at different distances away from you without getting into the nitty-gritty details. For many of us, that's fine. We can deal with the slight problems. But for some people, it's problematic.So as we figure out how to solve problems like that, more people can wear them, and more people can use them. I think that's a really critical issue for not just consumers, but for the enterprise because if we think about a future where more of our business applications and the kind of way we work are done with technologies like this, these technologies have to be accessible to everybody. Right? If that 10% or 15% of people get headaches and feel nauseous wearing this device, you've now disenfranchised a pretty significant portion of your workforce, but I think those can be solved, and so we need to be thinking about how we can enable everybody to use them.On the other hand, technologies like this can enfranchise more people, where right now, working remotely, working in a distributed sense is hard. For many kinds of work, it's difficult to do remotely. If we can figure out more ways of enabling people to work together in a distributed way, we can start enabling more people to participate meaningfully in a wider variety of jobs. Laurel: Blair, that was fantastic. It's so interesting. I really appreciate your perspective and sharing it here with us on the Business Lab. Blair: It was great to be here. I enjoyed talking to you. Laurel: That was Blair MacIntyre, the global head of Immersive Technology Research at JPMorgan Chase, who I spoke with from Cambridge, Massachusetts, the home of MIT and MIT Technology Review.That's it for this episode of Business Lab. I'm your host, Laurel Ruma. I'm the global director of Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology, and you can find us in print, on the web, and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com.This show is available wherever you get your podcasts. If you enjoyed this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT Technology Review. This episode was produced by Giro Studios. Thanks for listening. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. This podcast is for informational purposes only and it is not intended as legal, tax, financial, investment, accounting or regulatory advice. Opinions expressed herein are the personal views of the individual(s) and do not represent the views of JPMorgan Chase & Co. The accuracy of any statements, linked resources, reported findings or quotations are not the responsibility of JPMorgan Chase & Co.","In association withJPMorgan Chase & Co Imagine an integrated workplace with 3D visualizations that augment presentations , interactive and accelerated onboarding , and controlled training simulations . This is the future of immersive technology that global head of Immersive Technology Research at JPMorgan Chase , Blair MacIntyre is working to build . Augmented reality ( AR ) and virtual reality ( VR ) technologies can blend physical and digital dimensions together and infuse new innovations and efficiencies into business and customer experiences . `` These technologies can offer newer ways of collaborating over distance both synchronously and asynchronously than we can get with the traditional work technologies that we use right now , '' says MacIntyre . `` It 's these new ways to collaborate , ways of using the environment and space in new and interesting ways that will hopefully offer new value and change the way we work . '' Many enterprises are integrating VR into business practices like video conference calls . But having some participants in a virtual world and some sidelined creates imbalances in the employee experience . MacIntyre 's team is looking for ways to use AR/VR technologies that can be additive , like 3D data visualizations that enhance financial forecasting within a bank , not ones that overhaul entire experiences . Although the potential of AR/VR is quickly evolving , it 's unlikely that customers ’ interactions or workplace environments will be entirely moved to the virtual world anytime soon . Rather , MacIntyre 's immersive technology research looks to infuse efficiencies into existing practices . `` It 's thinking about how the technologies integrate and how we can add value where there is value and not trying to replace everything we do with these technologies , '' MacIntyre says . AI can help remove some of the tedium from immersive technologies that have made them impractical for widespread enterprise use in the past . Using VR technology in the workplace may prohibit taking notes and having access to traditional input devices and files . AI tools can take and transcribe notes and fill in any other gaps to help remove that friction and eliminate redundancies . Connected Internet of things ( IoT ) devices are also key to enabling AR/VR technologies . To create a valuable immersive experience , MacIntyre says , it 's imperative to know as much about the surrounding world of the user as well as their needs , habits , and preferences . `` If we can figure out more ways of enabling people to work together in a distributed way , we can start enabling more people to participate meaningfully in a wider variety of jobs , '' says MacIntyre . This episode of Business Lab is produced in association with JPMorgan Chase . Laurel : From MIT Technology Review , I 'm Laurel Ruma , and this is Business Lab , the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.Our topic today is emerging technologies , specifically , immersive technologies like augmented and virtual reality . Keeping up with technology trends may be a challenge for most enterprises , but it 's a critical way to think about future possibilities from product to customer service to employee experience . Augmented and virtual realities are n't necessarily new , but when it comes to applying them beyond gaming , it 's a brave new world.Two words for you : emerging realities.My guest is Blair MacIntyre , who is the global head of Immersive Technology Research at JPMorgan Chase.This podcast is produced in association with JPMorgan Chase.Welcome , Blair . Blair MacIntyre : Thank you . It 's great to be here . Laurel : Well , let 's do a little bit of context setting . Your career has been focused on researching and exploring immersive technology , including software and design tools , privacy and ethics , and game and experience design . So what brought you to JPMorgan Chase , and could you describe your current role ? Blair : So before joining the firm , I had spent the last 23 years as a professor at Georgia Tech and Northeastern University . During that time , as you say , I explored a lot of ways that we can both create things with these technologies , immersive technologies and also , what they might be useful for and what the impacts on people in society and how we experience life are . But as these technologies have become more real , moved out of the lab , starting to see real products from real companies , we have this opportunity to actually see how they might be useful in practice and to have , for me , an impact on how these technologies will be deployed and used that goes beyond the traditional impact that professors might have . So beyond writing papers , beyond teaching students . That 's what brought me to the firm , and so my current role is , really , to explore that , to understand all the different ways that immersive technology could impact the firm and its customers . Right ? So we think about not just customer-facing and not just products , but also employees and their experience as well . Laurel : That 's really interesting . So why does JPMorgan Chase have a dedicated immersive technology focus in its global technology applied research division , and what are the primary goals of your team 's research within finance and large enterprises as a whole ? Blair : That 's a great question . So JPMorgan Chase has a fairly wide variety of research going on within the company . There 's large efforts in AI/ML , in quantum computing , blockchain . So they 're interested in looking at all of the range of new technologies and how they might impact the firm and our customers , and immersive technologies represent one of those technologies that could over time have a relatively large impact , I think , especially on the employee experience and how we interact with our customers . So they really want to have a group of people focusing on , really , looking both in the near and long term , and thinking about how we can leverage the technology now and how we might be able to leverage it down the road , and not just how we can , but what we should not do . Right ? So we 're interested in understanding of these applications that are being proposed or people are imagining could be used . Which ones actually have value to the company , and which ones may not actually have value in practice ? Laurel : So when people think of immersive technologies like augmented reality and virtual reality , AR and VR , many think of headsets or smartphone apps for gaming and retail shopping experiences . Could you give an overview of the state of immersive technology today and what use cases you find to be the most innovative and interesting in your research ? Blair : So , as you say , I think many people think about smartphones , and we 've seen , at least in movies and TV shows , head mounts of various kinds . The market , I would divide it right now into the two parts , the handheld phone and tablet experience . So you can do augmented reality now , and that really translates to we take the camera feed , and we can overlay computer graphics on it to do things like see what something you might want to buy looks like in your living room or do , in an enterprise situation , remote maintenance assistance where I can take my phone , point it at a piece of technology , and a remote expert could draw on it or help me do something with it . There ’ s the phone-based things , and we carry these things in our pockets all the time , and they 're relatively cheap . So there 's a lot of opportunities when it 's appropriate to use those , but the big downside of those devices is that you have to hold them in your hands , so if you wanted to try to put information all around you , you would have to hold the device up and look around , which is uncomfortable and awkward . So that is where the head mount displays come in . So either virtual reality displays which , right now , many of us think about computer games and education as use cases in the consumer world or augmented reality displays . These sorts of displays now let us do the same kind of things we might do with our phones , but we can do it without our hands having to hold something so we can be doing whatever work it was we wanted to do , right ? Repairing the equipment , taking notes , working with things in the world around us , and we can have information spread all around us , which I think is the big advantage of head mounts . So many of the things people imagine when they think about augmented reality in particular involve this serendipitous access to information . I 'm walking into a conference room , and I see sort of my notes and information about the people I 'm meeting there and the materials from our last meeting , whatever it is , or I 'm walking down the street , and I see advertising or other kinds of , say , tourism information , but those things only work if the device is out of mind . If I can put it on , and then go about my life , I 'm not going to walk into a conference room , and hold up a phone , and look at everybody through it . So that , I think , is the big difference . You could implement the same sorts of applications on both the handheld devices and the head-worn devices , but the two different form factors are going to make very different applications appropriate for those two sorts of technologies.On the virtual reality side , we 're at the point now where the displays we can buy are light enough and comfortable enough that we could wear them for half an hour , a couple hours without discomfort . So a lot of the applications that people imagine there , I think the most popular things that people have done research on and that I see having a near-term impact in the enterprise are immersive training applications where you can get into a situation rather than , say , watching a video or a little click-through presentation as part of your annual training . You could really be in an experience and hopefully learn more from it . So I think those sorts of experiences where we 're totally immersed and focused is where virtual reality comes in.The big thing that I think is most exciting about head-worn displays in particular where we can wear them while we 're doing work as opposed to just having these ephemeral experiences with a phone is the opportunity to do things together , to collaborate . So I might want to look at a map on a table and see a bunch of data floating above the map , but it would be better if you and our other colleagues were around the table with me , and we can all see the same things , or if we want to take a training experience , I could be in there getting my training experience , but maybe someone else is joining me and being able to both offer feedback or guidance and so on . Essentially , when I think about these technologies , I think about the parallels to how we do work regularly , right ? We generally collaborate with people . We might grab a colleague and have them look at our laptop to show them something . I might send someone something on my phone , and then we can talk about it . So much of what we do involves interactions with other people and with the data that we are doing our job with that anything we do with these immersive technologies is really going to have to mimic that and give us the ability to do our real work in these immersive spaces with the people that we normally work with . Laurel : Well , speaking of working with people , how can the scale of an institution like JPMorgan Chase help propel this research forward in immersive technology , and what opportunities does it provide that are otherwise limited in a traditional university or startup research environment ? Blair : I think it comes down to a few different things . On one hand , we have the access to people who are really doing the things that we want to build technologies to help with . Right ? So if I wanted to look how I could use immersive visualization of data to help people in human resources do planning or help people who are doing financial modeling look at the data in new and interesting ways , now I could actually do the research in conjunction with the real people who do that work . Right ? So I 've already and I 've been at the firm for a little over a year , and many conversations we 've had were either we 've had an idea or somebody has come to us with an idea . Through the course of the conversations , relatively quickly , we hone in on things that are much more sophisticated , much more powerful than what we might have thought of at a university where we did n't have that sort of direct access to people doing the work.On the other hand , if we actually build something , we can actually test it with the same people , which is an amazing opportunity . Right ? When I go to a conference , we ’ re going to put 20 people who actually represent the real users of those systems . So , for me , that 's where I think the big opportunity of doing research in an enterprise is , is building solutions for the real people of that enterprise and being able to test it with those people . Laurel : Recent years have actually changed what customers and employees expect from enterprises as well , like omnichannel retail experiences . So immersive technologies can be used to bridge gaps between physical and virtual environments as you were saying earlier . What are the different opportunities that AR and VR can offer enterprises , and how can these technologies be used to improve employee and customer experience ? Blair : So I alluded back to some of that in previous answers . I think the biggest opportunities have to do with how employees within the organization can do new things together , can interact , and also how companies can interact with customers . Now , we 're not going to move all of our interactions with our customers into the virtual world , or the metaverse , or whatever you want to call it nowadays anytime soon . Right ? But I think there are opportunities for customers who are interested in those technologies , and comfortable with them , and excited by them to get new kinds of experiences and new ways of interacting with our firm or other firms than you could get with webpages and in-person meetings . The other big opportunity I think is as we move to a more hybrid work environment and a distributed work environment , so a company like JPMorgan Chase is huge and spread around the world . We have over 300,000 employees now in most countries around the world . There might be groups of people , but they 're connected together through video right now . These technologies , I think , can offer new ways of collaborating over distance both synchronously and asynchronously than we can get with the traditional work technologies that we use right now . So it 's those new ways to collaborate , ways of using the environment and space in new and interesting ways that is going to , hopefully , offer new value and change the way we work . Laurel : Yeah , and staying on that topic , we ca n't really have a discussion about technology without talking about AI which is another evolving , increasingly popular technology . So that 's being used by many enterprises to reduce redundancies and automate repetitive tasks . In this way , how can immersive technology provide value to people in their everyday work with the help of AI ? Blair : So I think the big opportunity that AI brings to immersive technologies is helping ease a lot of the tedium and burden that may have prevented these technologies from being practical in the past , and this could happen in a variety of ways . When I 'm in a virtual reality experience , I do n't have access to a keyboard , I do n't have access to traditional input devices , I do n't have necessarily the same sorts of access to my files , and so on . With a lot of the new AI technologies that are coming around , I can start relying on the computer to take notes . I can have new ways of pulling up information that I otherwise would n't have access to . So , I think AI reducing the friction of using these technologies is a huge opportunity , and the research community is actively looking at that because friction has been one of the big problems with these technologies up till now . Laurel : So , other than AI , what are other emerging technologies that can aid in immersive technology research and development ? Blair : So , aside from AI , if we step back and look at all of the emerging technologies as a whole and how they complement each other , I think we can see new opportunities . So , in our research , we work closely with people doing computer vision and other sort of sensing research to understand the world . We work closely with people looking at internet of things and connected devices because at a 10,000-foot level , all of these technologies are based on the idea of understanding , sensing the world , understanding what people are doing in it , understanding what people 's needs might be , and then somehow providing information to them or actuating things in the world , displaying stuff on walls or displays . From that viewpoint , immersive technologies are primarily one way of displaying things in a new and interesting way and getting input from people , knowing what people want to do , allowing them to interact with data . But in order to do that , they need to know as much about the world around the user as possible , the structure of it , but also , who 's there , what we are doing , and so on . So all of these other technologies , especially the Internet of things ( IoT ) and other forms and ways of sensing what 's happening in the world are very complimentary and together can create new sorts of experiences that neither could do alone . Laurel : So what are some of the challenges , but also , possible opportunities in your research that contrast the future potential of AR and VR to where the technology is today ? Blair : So I think one of the big limitations of technology today is that most of the experiences are very siloed and disconnected from everything else we do . During the pandemic , many of us experimented with how we could have conferences online in various ways , right ? A lot of companies , small companies and larger companies , started looking at how you could create immersive meetings and big group experiences using virtual reality technology , but all of those experiences that people created were these closed systems that you could n't bring things into . So one of the things we 're really interested in is how we stop thinking about creating new kinds of experiences and new ways of doing things , and instead think about how do we add these technologies to our existing work practices to enhance them in some way . So , for example . Right now , we do video meetings . It would be more interesting for some people to be able to join those meetings , say , in VR . Companies have experimented with that , but most of the experiments that people are doing assume that everyone is going to move into virtual reality , or we ’ re going to bring , say , the people in as a little video wall on the side of a big virtual reality room , making them second class citizens . I 'm really interested and my team is interested in how we can start incorporating technologies like this while keeping everyone a first-class participant in these meetings . As one example , a lot of the systems that large enterprises build , and we 're no different , are web-based right now . So if , let 's say , I have a system to do financial forecasting , you could imagine there 's a bunch of those at a bank , and it 's a web-based system , I 'm really interested in how do we add the ability for people to go into a virtual reality or augmented reality experience , say , a 3D visualization of some kind of data at the moment they want to do it , do the work that they want to do , invite colleagues in to discuss things , and then go back to the work as it was always done on a desktop web browser.So that idea of thinking of these technologies as a capability , a feature instead of a new whole application and way of doing things permeates all the work we 're doing . When I look down the road at where this can go , I see in , say , let 's say , two to five years , I see people with displays maybe sitting on their desk . They have their tablet and their phone , and they might also have another display or two sitting there . They 're doing their work , and at different times , they might be in a video chat , they might pick up a head mount and put it on to do different things , but it 's all integrated . I 'm really interested in how we connect these together and reduce friction . Right ? If it takes you four or five minutes to move your work into a VR experience , nobody is going to do it because it just is too problematic . So it 's that . It 's thinking about how the technologies integrate and how we can add value where there is value and not trying to replace everything we do with these technologies . Laurel : So to stay on that future focus , how do you foresee the immersive technology landscape entirely evolving over the next decade , and how will your research enable those changes ? Blair : So , at some level , it 's really hard to answer that question . Right ? So if I think back 10 years to where immersive technologies were , it would have been inconceivable for us to imagine the videos that are coming out . So , at some level , I can say , `` Well , I have no idea where we 're going to be in 10 years . '' On the other hand , it 's pretty safe to imagine the kinds of technologies that we 're experimenting with now just getting better , and more comfortable , and more easy to integrate into work . So I think the landscape is going to evolve in the near term to be more amenable to work . Especially for augmented reality , the threshold that these devices would have to get to such that a lot of people would be willing to wear them all the time while they 're walking down the street , playing sports , doing whatever , that 's a very high bar because it has to be small , it has to be light , it has to be cheap , it has to have a battery that lasts all day , etcetera , etcetera . On the other hand , in the enterprise , in any business situation , it 's easy to imagine the scenario I described . It 's sitting on my desk , I pick it up , I put it on , I take it off . In the medium term after that , I think we will see more consumer applications as people start solving more of the problems that are preventing people from wearing these devices for longer periods of time . Right ? It 's not just size , and battery power , and comfort , it 's also things like optics . Right ? A lot of people — not a lot , but say , let 's say 10 % , 15 % of people might experience headaches , or nausea , or other kinds of discomfort when they wear a VR display as they 're currently built , and a lot of that has to do with the fact that the optics that you 're looking at when you 're putting this display are built in a way that makes it hard to comfortably focus at objects at different distances away from you without getting into the nitty-gritty details . For many of us , that 's fine . We can deal with the slight problems . But for some people , it 's problematic.So as we figure out how to solve problems like that , more people can wear them , and more people can use them . I think that 's a really critical issue for not just consumers , but for the enterprise because if we think about a future where more of our business applications and the kind of way we work are done with technologies like this , these technologies have to be accessible to everybody . Right ? If that 10 % or 15 % of people get headaches and feel nauseous wearing this device , you 've now disenfranchised a pretty significant portion of your workforce , but I think those can be solved , and so we need to be thinking about how we can enable everybody to use them.On the other hand , technologies like this can enfranchise more people , where right now , working remotely , working in a distributed sense is hard . For many kinds of work , it 's difficult to do remotely . If we can figure out more ways of enabling people to work together in a distributed way , we can start enabling more people to participate meaningfully in a wider variety of jobs . Laurel : Blair , that was fantastic . It 's so interesting . I really appreciate your perspective and sharing it here with us on the Business Lab . Blair : It was great to be here . I enjoyed talking to you . Laurel : That was Blair MacIntyre , the global head of Immersive Technology Research at JPMorgan Chase , who I spoke with from Cambridge , Massachusetts , the home of MIT and MIT Technology Review.That 's it for this episode of Business Lab . I 'm your host , Laurel Ruma . I 'm the global director of Insights , the custom publishing division of MIT Technology Review . We were founded in 1899 at the Massachusetts Institute of Technology , and you can find us in print , on the web , and at events each year around the world . For more information about us and the show , please check out our website at technologyreview.com.This show is available wherever you get your podcasts . If you enjoyed this episode , we hope you 'll take a moment to rate and review us . Business Lab is a production of MIT Technology Review . This episode was produced by Giro Studios . Thanks for listening . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff . This podcast is for informational purposes only and it is not intended as legal , tax , financial , investment , accounting or regulatory advice . Opinions expressed herein are the personal views of the individual ( s ) and do not represent the views of JPMorgan Chase & Co . The accuracy of any statements , linked resources , reported findings or quotations are not the responsibility of JPMorgan Chase & Co .","['association', 'co', 'imagine', 'integrate', 'workplace', '3d', 'visualization', 'augment', 'presentation', 'interactive', 'accelerate', 'onboarding', 'control', 'training', 'simulation', 'future', 'immersive', 'technology', 'global', 'head', 'immersive', 'technology', 'research', 'work', 'build', 'augment', 'reality', 'ar', 'virtual', 'reality', 'technology', 'blend', 'physical', 'digital', 'dimension', 'together', 'infuse', 'new', 'innovation', 'efficiency', 'business', 'customer', 'experience', 'technology', 'offer', 'new', 'way', 'collaborate', 'distance', 'synchronously', 'asynchronously', 'get', 'traditional', 'work', 'technology', 'use', 'right', 'say', 'new', 'way', 'collaborate', 'way', 'use', 'environment', 'space', 'new', 'interesting', 'way', 'hopefully', 'offer', 'new', 'value', 'change', 'way', 'work', 'many', 'enterprise', 'integrate', 'business', 'practice', 'video', 'conference', 'call', 'participant', 'virtual', 'world', 'sideline', 'create', 'imbalance', 'employee', 'experience', 'team', 'look', 'way', 'use', 'arvr', 'technology', 'additive', '3d', 'datum', 'visualization', 'enhance', 'financial', 'forecasting', 'bank', 'one', 'overhaul', 'entire', 'experience', 'potential', 'arvr', 'quickly', 'evolve', 'unlikely', 'customer', 'interaction', 'workplace', 'environment', 'entirely', 'move', 'virtual', 'world', 'anytime', 'soon', 'rather', 'immersive', 'technology', 'research', 'look', 'infuse', 'efficiency', 'exist', 'practice', 'think', 'technology', 'integrate', 'add', 'value', 'value', 'try', 'replace', 'technology', 'macintyre', 'say', 'help', 'remove', 'tedium', 'immersive', 'technology', 'make', 'impractical', 'widespread', 'enterprise', 'use', 'past', 'use', 'technology', 'workplace', 'prohibit', 'take', 'note', 'access', 'traditional', 'input', 'device', 'file', 'tool', 'take', 'transcribe', 'note', 'fill', 'gap', 'help', 'remove', 'friction', 'eliminate', 'redundancy', 'connect', 'internet', 'thing', 'device', 'also', 'key', 'enable', 'arvr', 'technology', 'create', 'valuable', 'immersive', 'experience', 'say', 'imperative', 'know', 'much', 'surround', 'world', 'user', 'well', 'need', 'habit', 'preference', 'figure', 'way', 'enable', 'people', 'work', 'together', 'distribute', 'way', 'start', 'enable', 'people', 'participate', 'meaningfully', 'wide', 'variety', 'job', 'say', 'episode', 'business', 'lab', 'produce', 'association', 'laurel', 'mit', 'technology', 'review', 'business', 'lab', 'show', 'help', 'business', 'leader', 'make', 'sense', 'new', 'technology', 'come', 'lab', 'marketplaceour', 'topic', 'today', 'emerge', 'technology', 'specifically', 'immersive', 'technology', 'augment', 'virtual', 'reality', 'keep', 'technology', 'trend', 'challenge', 'enterprise', 'critical', 'way', 'think', 'future', 'possibility', 'product', 'customer', 'service', 'employee', 'experience', 'augment', 'virtual', 'reality', 'necessarily', 'new', 'come', 'apply', 'game', 'brave', 'new', 'word', 'emerge', 'realitiesmy', 'guest', 'global', 'head', 'immersive', 'technology', 'research', 'produce', 'association', 'thank', 'great', 'laurel', 'well', 'let', 'little', 'bit', 'context', 'set', 'career', 'focus', 'research', 'explore', 'immersive', 'technology', 'include', 'software', 'design', 'tool', 'privacy', 'ethic', 'game', 'experience', 'design', 'bring', 'describe', 'current', 'role', 'join', 'firm', 'spend', 'last', 'year', 'professor', 'northeastern', 'time', 'say', 'explore', 'lot', 'way', 'create', 'thing', 'technology', 'immersive', 'technology', 'also', 'useful', 'impact', 'people', 'society', 'experience', 'life', 'technology', 'become', 'real', 'move', 'lab', 'start', 'see', 'real', 'product', 'real', 'company', 'opportunity', 'actually', 'see', 'useful', 'practice', 'impact', 'technology', 'deploy', 'use', 'go', 'traditional', 'impact', 'professor', 'write', 'paper', 'teach', 'student', 'bring', 'firm', 'current', 'role', 'really', 'explore', 'understand', 'different', 'way', 'immersive', 'technology', 'impact', 'firm', 'customer', 'right', 'think', 'customerface', 'product', 'also', 'employee', 'experience', 'well', 'laurel', 'really', 'interesting', 'dedicated', 'immersive', 'technology', 'focus', 'global', 'technology', 'apply', 'research', 'division', 'primary', 'goal', 'team', 'research', 'finance', 'large', 'enterprise', 'whole', 'great', 'question', 'fairly', 'wide', 'variety', 'research', 'go', 'company', 'large', 'effort', 'aiml', 'computing', 'interested', 'look', 'range', 'new', 'technology', 'impact', 'firm', 'customer', 'immersive', 'technology', 'represent', 'technology', 'time', 'relatively', 'large', 'impact', 'think', 'especially', 'employee', 'experience', 'interact', 'customer', 'really', 'want', 'group', 'people', 'focus', 'really', 'look', 'near', 'long', 'term', 'think', 'leverage', 'technology', 'able', 'leverage', 'road', 'right', 'interested', 'understanding', 'application', 'propose', 'people', 'imagine', 'use', 'one', 'actually', 'value', 'company', 'one', 'actually', 'value', 'practice', 'laurel', 'people', 'think', 'immersive', 'technology', 'augment', 'reality', 'virtual', 'reality', 'many', 'think', 'headset', 'smartphone', 'app', 'gaming', 'retail', 'shopping', 'experience', 'give', 'overview', 'state', 'immersive', 'technology', 'today', 'use', 'case', 'find', 'innovative', 'interesting', 'research', 'say', 'think', 'many', 'people', 'think', 'smartphone', 'see', 'least', 'movie', 'tv', 'show', 'head', 'mount', 'various', 'kind', 'market', 'divide', 'right', 'part', 'handheld', 'phone', 'tablet', 'experience', 'augment', 'reality', 'really', 'translate', 'take', 'camera', 'feed', 'overlay', 'computer', 'graphic', 'thing', 'see', 'want', 'buy', 'look', 'living', 'room', 'enterprise', 'situation', 'remote', 'maintenance', 'assistance', 'take', 'phone', 'point', 'piece', 'technology', 'remote', 'expert', 'draw', 'help', 'phonebase', 'thing', 'carry', 'thing', 'pocket', 'time', 'relatively', 'cheap', 'lot', 'opportunity', 'appropriate', 'use', 'big', 'downside', 'device', 'hold', 'hand', 'want', 'try', 'put', 'information', 'hold', 'device', 'look', 'uncomfortable', 'awkward', 'head', 'display', 'come', 'virtual', 'reality', 'display', 'right', 'many', 'think', 'computer', 'game', 'education', 'use', 'case', 'consumer', 'world', 'augment', 'reality', 'display', 'sort', 'display', 'let', 'kind', 'thing', 'phone', 'hand', 'hold', 'work', 'want', 'right', 'repair', 'equipment', 'take', 'note', 'work', 'thing', 'world', 'information', 'spread', 'think', 'big', 'advantage', 'head', 'mount', 'many', 'thing', 'people', 'imagine', 'think', 'augmented', 'reality', 'particular', 'involve', 'serendipitous', 'access', 'information', 'walk', 'conference', 'room', 'see', 'sort', 'note', 'information', 'people', 'meet', 'material', 'last', 'meeting', 'walk', 'street', 'see', 'advertising', 'kind', 'say', 'tourism', 'information', 'thing', 'work', 'device', 'mind', 'put', 'go', 'life', 'go', 'walk', 'conference', 'room', 'hold', 'phone', 'look', 'think', 'big', 'difference', 'implement', 'sort', 'application', 'handheld', 'device', 'device', 'different', 'form', 'factor', 'go', 'make', 'different', 'application', 'appropriate', 'sort', 'technologieson', 'virtual', 'reality', 'side', 'point', 'display', 'buy', 'light', 'enough', 'comfortable', 'enough', 'wear', 'hour', 'couple', 'hour', 'discomfort', 'lot', 'application', 'people', 'imagine', 'think', 'popular', 'thing', 'people', 'research', 'see', 'nearterm', 'impact', 'enterprise', 'immersive', 'training', 'application', 'get', 'situation', 'rather', 'say', 'watch', 'video', 'little', 'clickthrough', 'presentation', 'part', 'annual', 'training', 'really', 'experience', 'hopefully', 'learn', 'think', 'sort', 'experience', 'totally', 'immerse', 'focus', 'virtual', 'reality', 'come', 'inthe', 'big', 'thing', 'think', 'exciting', 'headworn', 'display', 'particular', 'wear', 'work', 'oppose', 'ephemeral', 'experience', 'phone', 'opportunity', 'thing', 'together', 'collaborate', 'want', 'look', 'map', 'table', 'see', 'bunch', 'datum', 'float', 'map', 'well', 'colleague', 'table', 'see', 'thing', 'want', 'take', 'training', 'experience', 'get', 'training', 'experience', 'maybe', 'else', 'join', 'able', 'offer', 'feedback', 'guidance', 'essentially', 'think', 'technology', 'think', 'parallel', 'work', 'regularly', 'right', 'generally', 'collaborate', 'people', 'grab', 'colleague', 'look', 'laptop', 'show', 'send', 'phone', 'talk', 'much', 'involve', 'interaction', 'people', 'datum', 'job', 'immersive', 'technology', 'really', 'go', 'mimic', 'give', 'ability', 'real', 'work', 'immersive', 'space', 'people', 'normally', 'work', 'laurel', 'well', 'speak', 'work', 'people', 'scale', 'institution', 'help', 'propel', 'research', 'forward', 'immersive', 'technology', 'opportunity', 'provide', 'otherwise', 'limit', 'traditional', 'university', 'startup', 'research', 'environment', 'think', 'come', 'different', 'thing', 'hand', 'access', 'people', 'really', 'thing', 'want', 'build', 'technology', 'help', 'right', 'want', 'look', 'use', 'immersive', 'visualization', 'datum', 'help', 'people', 'human', 'resource', 'planning', 'help', 'people', 'financial', 'modeling', 'look', 'datum', 'new', 'interesting', 'way', 'actually', 'research', 'conjunction', 'real', 'people', 'work', 'right', 'already', 'firm', 'little', 'year', 'many', 'conversation', 'idea', 'come', 'idea', 'course', 'conversation', 'relatively', 'quickly', 'hone', 'thing', 'much', 'sophisticated', 'much', 'powerful', 'think', 'university', 'sort', 'direct', 'access', 'people', 'workon', 'hand', 'actually', 'build', 'actually', 'test', 'people', 'amazing', 'opportunity', 'right', 'go', 'conference', 'go', 'put', 'people', 'actually', 'represent', 'real', 'user', 'system', 'think', 'big', 'opportunity', 'research', 'enterprise', 'build', 'solution', 'real', 'people', 'enterprise', 'able', 'test', 'people', 'laurel', 'recent', 'year', 'actually', 'change', 'customer', 'employee', 'expect', 'enterprise', 'well', 'omnichannel', 'retail', 'experience', 'immersive', 'technology', 'use', 'bridge', 'gap', 'physical', 'virtual', 'environment', 'say', 'early', 'different', 'opportunity', 'ar', 'offer', 'enterprise', 'technology', 'use', 'improve', 'employee', 'customer', 'experience', 'allude', 'back', 'previous', 'answer', 'think', 'big', 'opportunity', 'employee', 'organization', 'new', 'thing', 'together', 'interact', 'also', 'company', 'interact', 'customer', 'go', 'move', 'interaction', 'customer', 'virtual', 'world', 'metaverse', 'want', 'call', 'nowadays', 'anytime', 'soon', 'right', 'think', 'opportunity', 'customer', 'interested', 'technology', 'comfortable', 'excite', 'get', 'new', 'kind', 'experience', 'new', 'way', 'interact', 'firm', 'firm', 'get', 'webpage', 'meeting', 'big', 'opportunity', 'think', 'move', 'hybrid', 'work', 'environment', 'distribute', 'work', 'environment', 'company', 'huge', 'spread', 'world', 'employee', 'country', 'world', 'group', 'people', 'connect', 'together', 'video', 'right', 'technology', 'think', 'offer', 'new', 'way', 'collaborate', 'distance', 'synchronously', 'asynchronously', 'get', 'traditional', 'work', 'technology', 'use', 'right', 'new', 'way', 'collaborate', 'way', 'use', 'environment', 'space', 'new', 'interesting', 'way', 'go', 'hopefully', 'offer', 'new', 'value', 'change', 'way', 'work', 'laurel', 'stay', 'topic', 'really', 'discussion', 'technology', 'talk', 'ai', 'evolve', 'increasingly', 'popular', 'technology', 'use', 'many', 'enterprise', 'reduce', 'redundancy', 'automate', 'repetitive', 'task', 'way', 'immersive', 'technology', 'provide', 'value', 'people', 'everyday', 'work', 'help', 'ai', 'think', 'big', 'opportunity', 'bring', 'immersive', 'technology', 'ease', 'lot', 'tedium', 'burden', 'prevent', 'technology', 'practical', 'past', 'happen', 'variety', 'way', 'virtual', 'reality', 'experience', 'access', 'keyboard', 'access', 'traditional', 'input', 'device', 'necessarily', 'sort', 'access', 'file', 'lot', 'new', 'technology', 'come', 'start', 'rely', 'computer', 'take', 'note', 'new', 'way', 'pull', 'information', 'otherwise', 'access', 'think', 'reduce', 'friction', 'use', 'technology', 'huge', 'opportunity', 'research', 'community', 'actively', 'look', 'friction', 'big', 'problem', 'technology', 'ai', 'emerge', 'technology', 'aid', 'immersive', 'technology', 'research', 'development', 'aside', 'ai', 'step', 'back', 'look', 'emerge', 'technology', 'whole', 'complement', 'think', 'see', 'new', 'opportunity', 'research', 'work', 'closely', 'people', 'computer', 'vision', 'sort', 'sense', 'research', 'understand', 'world', 'work', 'closely', 'people', 'look', 'internet', 'thing', 'connect', 'device', '10000foot', 'level', 'technology', 'base', 'idea', 'understand', 'sense', 'world', 'understand', 'people', 'understand', 'people', 'need', 'somehow', 'provide', 'information', 'actuate', 'thing', 'world', 'display', 'stuff', 'wall', 'display', 'viewpoint', 'immersive', 'technology', 'primarily', 'way', 'display', 'thing', 'new', 'interesting', 'way', 'get', 'input', 'people', 'know', 'people', 'want', 'allow', 'interact', 'datum', 'order', 'need', 'know', 'much', 'world', 'user', 'possible', 'structure', 'also', 'technology', 'especially', 'internet', 'thing', 'iot', 'form', 'way', 'sense', 'happen', 'world', 'complimentary', 'together', 'create', 'new', 'sort', 'experience', 'alone', 'laurel', 'challenge', 'also', 'possible', 'opportunity', 'research', 'contrast', 'future', 'potential', 'ar', 'technology', 'today', 'think', 'big', 'limitation', 'technology', 'today', 'experience', 'siloe', 'disconnect', 'else', 'pandemic', 'many', 'experiment', 'conference', 'online', 'various', 'way', 'right', 'lot', 'company', 'small', 'company', 'large', 'company', 'start', 'look', 'create', 'immersive', 'meeting', 'big', 'group', 'experience', 'use', 'virtual', 'reality', 'technology', 'experience', 'people', 'create', 'closed', 'system', 'bring', 'thing', 'thing', 'really', 'interested', 'stop', 'think', 'create', 'new', 'kind', 'experience', 'new', 'way', 'thing', 'instead', 'think', 'add', 'technology', 'exist', 'work', 'practice', 'enhance', 'way', 'example', 'right', 'video', 'meeting', 'interesting', 'people', 'able', 'join', 'meeting', 'say', 'company', 'experiment', 'experiment', 'people', 'assume', 'go', 'move', 'virtual', 'reality', 'go', 'bring', 'say', 'people', 'little', 'video', 'wall', 'side', 'big', 'virtual', 'reality', 'room', 'make', 'second', 'class', 'citizen', 'really', 'interested', 'team', 'interested', 'start', 'incorporate', 'technology', 'keep', 'firstclass', 'participant', 'meeting', 'example', 'lot', 'system', 'large', 'enterprise', 'build', 'different', 'webbase', 'right', 'let', 'say', 'system', 'financial', 'forecasting', 'imagine', 'bunch', 'bank', 'webbase', 'system', 'really', 'interested', 'add', 'ability', 'people', 'go', 'virtual', 'reality', 'augment', 'reality', 'experience', 'say', '3d', 'visualization', 'kind', 'datum', 'moment', 'want', 'work', 'want', 'invite', 'colleague', 'discuss', 'thing', 'go', 'back', 'work', 'always', 'desktop', 'web', 'browserso', 'idea', 'thinking', 'technology', 'capability', 'feature', 'instead', 'new', 'whole', 'application', 'way', 'thing', 'permeate', 'work', 'look', 'road', 'go', 'see', 'say', 'let', 'say', 'year', 'see', 'people', 'display', 'maybe', 'sit', 'desk', 'tablet', 'phone', 'also', 'display', 'sit', 'work', 'different', 'time', 'video', 'chat', 'pick', 'head', 'mount', 'put', 'different', 'thing', 'integrate', 'really', 'interested', 'connect', 'together', 'reduce', 'friction', 'right', 'take', 'minute', 'move', 'work', 'experience', 'go', 'problematic', 'think', 'technology', 'integrate', 'add', 'value', 'value', 'try', 'replace', 'technology', 'laurel', 'stay', 'future', 'focus', 'foresee', 'immersive', 'technology', 'landscape', 'entirely', 'evolve', 'next', 'decade', 'research', 'enable', 'change', 'level', 'really', 'hard', 'answer', 'question', 'right', 'think', 'back', 'year', 'immersive', 'technology', 'inconceivable', 'imagine', 'video', 'come', 'level', 'say', 'well', 'idea', 'go', 'year', 'hand', 'pretty', 'safe', 'imagine', 'kind', 'technology', 'experiment', 'get', 'well', 'comfortable', 'easy', 'integrate', 'work', 'think', 'landscape', 'go', 'evolve', 'near', 'term', 'amenable', 'work', 'especially', 'augment', 'reality', 'threshold', 'device', 'get', 'lot', 'people', 'willing', 'wear', 'time', 'walk', 'street', 'play', 'sport', 'high', 'bar', 'small', 'light', 'cheap', 'battery', 'last', 'day', 'etcetera', 'etcetera', 'hand', 'enterprise', 'business', 'situation', 'easy', 'imagine', 'scenario', 'describe', 'sit', 'desk', 'pick', 'put', 'take', 'medium', 'term', 'think', 'see', 'consumer', 'application', 'people', 'start', 'solve', 'problem', 'prevent', 'people', 'wear', 'device', 'long', 'period', 'time', 'right', 'size', 'battery', 'power', 'comfort', 'also', 'thing', 'optic', 'right', 'lot', 'people', 'lot', 'say', 'let', 'say', 'people', 'experience', 'headache', 'nausea', 'kind', 'discomfort', 'wear', 'vr', 'display', 'currently', 'build', 'lot', 'fact', 'optic', 'look', 'put', 'display', 'build', 'way', 'make', 'hard', 'comfortably', 'focus', 'object', 'different', 'distance', 'away', 'get', 'nittygritty', 'detail', 'many', 'fine', 'deal', 'slight', 'problem', 'people', 'problematicso', 'figure', 'solve', 'problem', 'people', 'wear', 'people', 'use', 'think', 'really', 'critical', 'issue', 'consumer', 'enterprise', 'think', 'future', 'business', 'application', 'kind', 'way', 'work', 'technology', 'technology', 'accessible', 'right', 'people', 'get', 'headache', 'feel', 'nauseous', 'wear', 'device', 'disenfranchise', 'pretty', 'significant', 'portion', 'workforce', 'think', 'solve', 'need', 'think', 'enable', 'use', 'themon', 'hand', 'technology', 'enfranchise', 'people', 'right', 'work', 'remotely', 'work', 'distribute', 'sense', 'hard', 'many', 'kind', 'work', 'difficult', 'remotely', 'figure', 'way', 'enable', 'people', 'work', 'together', 'distribute', 'way', 'start', 'enable', 'people', 'participate', 'meaningfully', 'wide', 'variety', 'job', 'fantastic', 'interesting', 'really', 'appreciate', 'perspective', 'share', 'business', 'lab', 'great', 'enjoy', 'talk', 'laurel', 'global', 'head', 'immersive', 'technology', 'research', 'speak', 'home', 'mit', 'mit', 'technology', 'episode', 'business', 'lab', 'host', 'global', 'director', 'insight', 'custom', 'publishing', 'division', 'mit', 'technology', 'review', 'found', 'find', 'print', 'web', 'event', 'year', 'world', 'information', 'show', 'check', 'website', 'show', 'available', 'get', 'podcast', 'enjoy', 'episode', 'hope', 'take', 'moment', 'rate', 'review', 'business', 'lab', 'production', 'mit', 'technology', 'review', 'episode', 'produce', 'studio', 'thank', 'listen', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff', 'podcast', 'informational', 'purpose', 'intend', 'legal', 'tax', 'financial', 'investment', 'accounting', 'regulatory', 'advice', 'opinion', 'express', 'herein', 'personal', 'view', 'individual', 'represent', 'view', 'co', 'accuracy', 'statement', 'link', 'resource', 'report', 'finding', 'quotation', 'responsibility', 'co']","Imagine an integrated workplace with 3D visualizations that augment presentations, interactive and accelerated onboarding, and controlled training simulations. This is the future of immersive technology that global head of Immersive Technology Research at JPMorgan Chase, Blair MacIntyre is working to build. Augmented reality (AR) and virtual reality (VR) technologies can blend physical and digital dimensions…"
Customer experience horizons,https://www.technologyreview.com/2023/11/09/1083068/customer-experience-horizons/,2023-11-09,"In partnership withGenesys Customer experience (CX) is a leading driver of brand loyalty and organizational performance. According to NTT’s State of CX 2023 report, 92% of CEOs believe improvements in CX directly impact their improved productivity, and customer brand advocacy. They also recognize that the quality of their employee experience (EX) is critical to success. The real potential for transforming business, according to 95% of CEOs, is bringing customer and employee experience improvements together into one end-to-end strategy. This, they anticipate, will deliver revenue growth, business agility, and resilience. To succeed, organizations need to reimagine what’s possible with customer and employee experience and understand horizon trends that will affect their business. This MIT Technology Review Insights report explores the strategies and technologies that will transform customer experience and contact center employee experience in the years ahead. It is based on nearly two dozen interviews with customer experience leaders, conducted between December 2022 and April 2023. The interviews explored the future of customer experience and employee experience and the role of the contact center as a strategic driver of business value. The main findings of this report are as follows: Download the full report. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. ","In partnership withGenesys Customer experience ( CX ) is a leading driver of brand loyalty and organizational performance . According to NTT ’ s State of CX 2023 report , 92 % of CEOs believe improvements in CX directly impact their improved productivity , and customer brand advocacy . They also recognize that the quality of their employee experience ( EX ) is critical to success . The real potential for transforming business , according to 95 % of CEOs , is bringing customer and employee experience improvements together into one end-to-end strategy . This , they anticipate , will deliver revenue growth , business agility , and resilience . To succeed , organizations need to reimagine what ’ s possible with customer and employee experience and understand horizon trends that will affect their business . This MIT Technology Review Insights report explores the strategies and technologies that will transform customer experience and contact center employee experience in the years ahead . It is based on nearly two dozen interviews with customer experience leaders , conducted between December 2022 and April 2023 . The interviews explored the future of customer experience and employee experience and the role of the contact center as a strategic driver of business value . The main findings of this report are as follows : Download the full report . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['partnership', 'customer', 'experience', 'cx', 'lead', 'driver', 'brand', 'loyalty', 'organizational', 'performance', 'accord', 'state', 'report', 'believe', 'improvement', 'directly', 'impact', 'improve', 'productivity', 'customer', 'brand', 'advocacy', 'also', 'recognize', 'quality', 'employee', 'experience', 'ex', 'critical', 'success', 'real', 'potential', 'transform', 'business', 'accord', 'bring', 'customer', 'employee', 'experience', 'improvement', 'together', 'endtoend', 'strategy', 'anticipate', 'deliver', 'revenue', 'growth', 'business', 'agility', 'resilience', 'succeed', 'organization', 'need', 'reimagine', 'possible', 'customer', 'employee', 'experience', 'understand', 'horizon', 'trend', 'affect', 'business', 'mit', 'technology', 'review', 'insight', 'report', 'explore', 'strategy', 'technology', 'transform', 'customer', 'experience', 'contact', 'center', 'employee', 'experience', 'year', 'ahead', 'base', 'nearly', 'dozen', 'interview', 'customer', 'experience', 'leader', 'conduct', 'interview', 'explore', 'future', 'customer', 'experience', 'employee', 'experience', 'role', 'contact', 'center', 'strategic', 'driver', 'business', 'value', 'main', 'finding', 'report', 'follow', 'download', 'full', 'report', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","Customer experience (CX) is a leading driver of brand loyalty and organizational performance. According to NTT’s State of CX 2023 report, 92% of CEOs believe improvements in CX directly impact their improved productivity, and customer brand advocacy. They also recognize that the quality of their employee experience (EX) is critical to success. The real potential…"
AI gains momentum in core manufacturing services functions,https://www.technologyreview.com/2023/11/02/1080614/ai-gains-momentum-in-core-manufacturing-services-functions/,2023-11-02,"In partnership withDataiku When considering the potential for AI systems to change manufacturing, Ritu Jyoti, global AI research lead at market-intelligence firm IDC, points to windmill manufacturers. To improve windmills before AI, she says, the company analyzed data from observing a functioning prototype, a process that took weeks. Now, the manufacturer has dramatically shortened the process using a digital twin—a digital model of the operational windmill—using machine learning (ML) and AI to create and simulate improvements. “Sometimes it was impossible and physically challenging for them to even go and get all the measurements, so they used drones and AI technologies to generate a digital win,” Jyoti says. This manufacturer now sees this AI/ML technology as essential. ""Because if they’re not doing it, they’re not going to be relevant,” she says. Disruption in manufacturing and the supply chain has pushed businesses toward digital transformation as they seek ways to stay competitive. For manufacturers, these disruptions—along with the advent of AI—present opportunities to make manufacturing more efficient, safer, and sustainable.  Companies can use AI to streamline processes and fight downtime, adopt robotics that promote safety and speed, allow AI to detect anomalies quickly through computer vision, and develop AI systems to process vast volumes of data to identify patterns and predict customer needs. “In manufacturing, the biggest benefits come when people from the business are able to work together with data experts, using data and AI to get insights, ultimately taking actions to improve their processes,” says Pierre Goutorbe, AI solutions director for energy and manufacturing at Dataiku. “The more workers get familiar with AI and use it on a daily basis, the more we’ll see the benefit from it,” he says. Between supply-chain disruptions and worker shortages, the manufacturing sector has been innovating to stay ahead in the global marketplace. However, a June 2023 study by Dataiku and Databricks found manufacturing lags behind other industries, with about a quarter (24%) of companies still at the exploration or experimentation stage in terms of AI adoption, while only about one-fifth (19%) of companies across all other industries are still in this beginning stage. Download the full report. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.  ","In partnership withDataiku When considering the potential for AI systems to change manufacturing , Ritu Jyoti , global AI research lead at market-intelligence firm IDC , points to windmill manufacturers . To improve windmills before AI , she says , the company analyzed data from observing a functioning prototype , a process that took weeks . Now , the manufacturer has dramatically shortened the process using a digital twin—a digital model of the operational windmill—using machine learning ( ML ) and AI to create and simulate improvements . “ Sometimes it was impossible and physically challenging for them to even go and get all the measurements , so they used drones and AI technologies to generate a digital win , ” Jyoti says . This manufacturer now sees this AI/ML technology as essential . `` Because if they ’ re not doing it , they ’ re not going to be relevant , ” she says . Disruption in manufacturing and the supply chain has pushed businesses toward digital transformation as they seek ways to stay competitive . For manufacturers , these disruptions—along with the advent of AI—present opportunities to make manufacturing more efficient , safer , and sustainable . Companies can use AI to streamline processes and fight downtime , adopt robotics that promote safety and speed , allow AI to detect anomalies quickly through computer vision , and develop AI systems to process vast volumes of data to identify patterns and predict customer needs . “ In manufacturing , the biggest benefits come when people from the business are able to work together with data experts , using data and AI to get insights , ultimately taking actions to improve their processes , ” says Pierre Goutorbe , AI solutions director for energy and manufacturing at Dataiku . “ The more workers get familiar with AI and use it on a daily basis , the more we ’ ll see the benefit from it , ” he says . Between supply-chain disruptions and worker shortages , the manufacturing sector has been innovating to stay ahead in the global marketplace . However , a June 2023 study by Dataiku and Databricks found manufacturing lags behind other industries , with about a quarter ( 24 % ) of companies still at the exploration or experimentation stage in terms of AI adoption , while only about one-fifth ( 19 % ) of companies across all other industries are still in this beginning stage . Download the full report . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['partnership', 'withdataiku', 'consider', 'potential', 'system', 'change', 'manufacture', 'ritu', 'research', 'lead', 'marketintelligence', 'firm', 'idc', 'point', 'windmill', 'manufacturer', 'improve', 'windmill', 'say', 'company', 'analyze', 'datum', 'observe', 'function', 'prototype', 'process', 'take', 'week', 'manufacturer', 'dramatically', 'shorten', 'process', 'use', 'digital', 'twin', 'digital', 'model', 'operational', 'windmill', 'use', 'machine', 'learning', 'ml', 'ai', 'create', 'simulate', 'improvement', 'sometimes', 'impossible', 'physically', 'challenge', 'even', 'go', 'get', 'measurement', 'use', 'drone', 'ai', 'technology', 'generate', 'digital', 'win', 'say', 'manufacturer', 'see', 'aiml', 'technology', 'essential', 'go', 'relevant', 'say', 'disruption', 'manufacturing', 'supply', 'chain', 'push', 'business', 'digital', 'transformation', 'seek', 'way', 'stay', 'competitive', 'manufacturer', 'disruption', 'advent', 'ai', 'present', 'opportunity', 'make', 'manufacturing', 'efficient', 'safe', 'sustainable', 'company', 'use', 'ai', 'streamline', 'process', 'fight', 'downtime', 'adopt', 'robotic', 'promote', 'safety', 'speed', 'allow', 'ai', 'detect', 'anomaly', 'quickly', 'computer', 'vision', 'develop', 'system', 'process', 'vast', 'volume', 'datum', 'identify', 'pattern', 'predict', 'customer', 'need', 'manufacture', 'big', 'benefit', 'come', 'people', 'business', 'able', 'work', 'together', 'data', 'expert', 'use', 'datum', 'ai', 'get', 'insight', 'ultimately', 'take', 'action', 'improve', 'process', 'say', 'ai', 'solution', 'director', 'energy', 'manufacturing', 'dataiku', 'worker', 'get', 'familiar', 'ai', 'use', 'daily', 'basis', 'see', 'benefit', 'say', 'supplychain', 'disruption', 'worker', 'shortage', 'manufacturing', 'sector', 'innovate', 'stay', 'ahead', 'global', 'marketplace', 'however', 'study', 'dataiku', 'databrick', 'find', 'manufacture', 'lag', 'industry', 'quarter', 'company', 'still', 'exploration', 'experimentation', 'stage', 'term', 'ai', 'adoption', 'onefifth', 'company', 'industry', 'still', 'begin', 'stage', 'download', 'full', 'report', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","When considering the potential for AI systems to change manufacturing, Ritu Jyoti, global AI research lead at market-intelligence firm IDC, points to windmill manufacturers. To improve windmills before AI, she says, the company analyzed data from observing a functioning prototype, a process that took weeks. Now, the manufacturer has dramatically shortened the process using a…"
AI-powered 6G networks will reshape digital interactions,https://www.technologyreview.com/2023/10/26/1082028/ai-powered-6g-networks-will-reshape-digital-interactions/,2023-10-26,"In partnership withVivo Sixth-generation (6G) mobile networks, underpinned by artificial intelligence (AI), are poised to combine communication and computing in a hyperconnected world of digital and physical experiences that will transform daily lives, experts predict. “In the past, we talked about internet of things, but with 6G, we talk about intelligent or smart internet of things,” says Qin Fei, president of communications research institute at Vivo, a Chinese mobile phone maker that has stepped up R&D efforts into 6G since 2020. Communication and tech companies are already planning for 6G wireless networks, even though 5G has yet to be fully rolled out globally. With improved data latency, security, reliability, and the ability to process massive volumes of global data in real time, experts like Qin believe 6G is set to transform our leisure and work. Among the new use cases for 6G networks envisioned by Vivo are mixed reality, holographic and multi-sensory communication, interactive 3D virtual digital humans, collaborative robots, and automated driving. There are expectations for 6G to be deployed by 2030. The UN’s telecoms agency, International Telecommunication Union (ITU), has stated it plans to finish the initial 6G standardization process no later than the year 2030. Optimized by AI technologies, experts expect 6G to have a bigger impact than 5G for two reasons. One, because it will enable the convergence of computing and mobile communications. Two, because it will integrate digital and physical realms and introduce new sensory experiences for users. Qin says that “6G will provide super communication and ubiquitous information, and converge computing services, thus being the base for an interconnected and converged physical and digital world.” Capgemini agrees—predicting that 6G networks will enable immersive, ubiquitous, and sensory digital experiences on a massive scale. This will make it possible for 6G applications to “sense” their surroundings, and thereby turn the network into “our sixth sense”, according to a report by the consultancy. As each generation of wireless networks becomes increasingly complex, they rely on other technologies to harness their power and make them easier to run. 6G is expected to be one of the first AI-native networks, where AI is embedded in the networking equipment. This will enable the network to learn and manage itself, be more autonomous, and make it cheaper to run. “When we are designing the 6G network, we're going to use AI technology in designing the air interface and also in managing the 6G network,” says Qin. Machine learning and AI-based network automation will be crucial to simplify network management and optimization. “The 6G network with AI inside is like a very good student,” he adds. “The 6G network will self-train, self-learn, and it will actually grow as a student to become more and more powerful.” Although 6G standards and specifications are still under development, experts agree that it will be a leapfrog technology, thanks to its higher speed (estimates vary, but 6G could be between 10 times, 50 times, to 100 times faster than 5G) and significantly reduced latency; improved connectivity, security, and reliability; and an ability to integrate digital and physical versions of the world. “For 5G, it’s mainly a communication technology—that’s its core,” says Qin. “But for 6G, besides enhanced communications technology, it also includes computing, as well as other relevant services.” Another benefit is wider geographical coverage than 5G— 6G will cover the whole planet and connect all kinds of machines, he adds. Qin says 6G networks will also popularize the use of digital twins—virtual replicas of products or processes used to predict how the physical entities will perform in the real world. This will be possible due to 6G networks’ enhanced connectivity, stronger sensing capability, and capacity to collect massive amounts of data. According to Qin: “We could have more powerful connectivity and sensing capability, so we could install more sensors in the physical world and collect a massive amount of data about this world. With this data we could build models to rebuild the world in the digital arena.” Vivo believes 6G will support dozens or hundreds of new services in a wide range of industries. The company is developing prototype 6G mobile technologies based on three trends—communication plus sensing; communication plus computing; and communication plus AI. For example, Vivo is developing a prototype that can collect users’ biometric data to monitor their health while they are asleep. According to this technological vision, a person’s bedside phone could become a medical monitoring device. “If there is any health issue or abnormal behavior happening with respiration, then [the phone] could send an alert to the hospital,” says Qin. Vivo also sees virtual and mixed reality glasses as another potential application for 6G that could revolutionize video streaming by making it a more compelling and immersive experience. Current AR glasses have limited computing power, says Qin. “Therefore, it needs to connect as a kind of edge device to the cloud so it could provide better experiences for the users.” 6G will also support self-driving or autonomous cars. “I believe autonomous driving will be very popular after 2030 and be supported by 6G,” says Qin. “Driverless cars need to really gather all kinds of data, for example, about the ambient environment, about road conditions and even the adjacent cars in order to make informed decisions [such as] whether it should speed up or break. 6G can provide the computing power and network.” Although 5G mobile networks have yet to live up to initial expectations, most experts agree that 6G has the potential to deliver major advances in connectivity and computing power. However, like any complex and powerful new technology, 6G also faces challenges, including network capacity and energy consumption. Getting to the 6G era requires an increase in network capacity. Finding the right telecommunications spectrum to support its rollout is crucial. It has not been finalized but 6.4 to 15 gigahertz is under consideration for 6G. “We think that the spectrum for 6G should be on the lower spectrum, like 6.4 to 7.1 gigahertz, because the lower band electromagnetic wave physically has much better coverage and penetration characteristics,” says Qin. Minimizing 6G’s energy consumption and carbon emissions is another major task. 6G networks will have vastly more computing demands than 5G. Suppliers and users will need to cooperate to minimize energy use. According to a report by GSMA, which represents global telecom operators, energy-saving techniques (such as AI-driven sleep states and lithium-ion batteries) may help to make 6G more energy efficient. Ultimately, 6G will only succeed if it delivers great experiences and services for consumers and businesses, says Qin. “We should avoid overdesign of 6G network, and we should really collaborate with different verticals.” Problems faced by 5G networks—for example, bottlenecks in other technologies needed to support new terminals for 5G, such as material sciences for augmented reality equipment—should be lessons for 6G development, he adds. ""We hope that we can avoid these problems. That means we need the whole ecosystem to collaborate, to jointly develop 6G infrastructure, mobile terminals, and applications.” This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. ","In partnership withVivo Sixth-generation ( 6G ) mobile networks , underpinned by artificial intelligence ( AI ) , are poised to combine communication and computing in a hyperconnected world of digital and physical experiences that will transform daily lives , experts predict . “ In the past , we talked about internet of things , but with 6G , we talk about intelligent or smart internet of things , ” says Qin Fei , president of communications research institute at Vivo , a Chinese mobile phone maker that has stepped up R & D efforts into 6G since 2020 . Communication and tech companies are already planning for 6G wireless networks , even though 5G has yet to be fully rolled out globally . With improved data latency , security , reliability , and the ability to process massive volumes of global data in real time , experts like Qin believe 6G is set to transform our leisure and work . Among the new use cases for 6G networks envisioned by Vivo are mixed reality , holographic and multi-sensory communication , interactive 3D virtual digital humans , collaborative robots , and automated driving . There are expectations for 6G to be deployed by 2030 . The UN ’ s telecoms agency , International Telecommunication Union ( ITU ) , has stated it plans to finish the initial 6G standardization process no later than the year 2030 . Optimized by AI technologies , experts expect 6G to have a bigger impact than 5G for two reasons . One , because it will enable the convergence of computing and mobile communications . Two , because it will integrate digital and physical realms and introduce new sensory experiences for users . Qin says that “ 6G will provide super communication and ubiquitous information , and converge computing services , thus being the base for an interconnected and converged physical and digital world. ” Capgemini agrees—predicting that 6G networks will enable immersive , ubiquitous , and sensory digital experiences on a massive scale . This will make it possible for 6G applications to “ sense ” their surroundings , and thereby turn the network into “ our sixth sense ” , according to a report by the consultancy . As each generation of wireless networks becomes increasingly complex , they rely on other technologies to harness their power and make them easier to run . 6G is expected to be one of the first AI-native networks , where AI is embedded in the networking equipment . This will enable the network to learn and manage itself , be more autonomous , and make it cheaper to run . “ When we are designing the 6G network , we 're going to use AI technology in designing the air interface and also in managing the 6G network , ” says Qin . Machine learning and AI-based network automation will be crucial to simplify network management and optimization . “ The 6G network with AI inside is like a very good student , ” he adds . “ The 6G network will self-train , self-learn , and it will actually grow as a student to become more and more powerful. ” Although 6G standards and specifications are still under development , experts agree that it will be a leapfrog technology , thanks to its higher speed ( estimates vary , but 6G could be between 10 times , 50 times , to 100 times faster than 5G ) and significantly reduced latency ; improved connectivity , security , and reliability ; and an ability to integrate digital and physical versions of the world . “ For 5G , it ’ s mainly a communication technology—that ’ s its core , ” says Qin . “ But for 6G , besides enhanced communications technology , it also includes computing , as well as other relevant services. ” Another benefit is wider geographical coverage than 5G— 6G will cover the whole planet and connect all kinds of machines , he adds . Qin says 6G networks will also popularize the use of digital twins—virtual replicas of products or processes used to predict how the physical entities will perform in the real world . This will be possible due to 6G networks ’ enhanced connectivity , stronger sensing capability , and capacity to collect massive amounts of data . According to Qin : “ We could have more powerful connectivity and sensing capability , so we could install more sensors in the physical world and collect a massive amount of data about this world . With this data we could build models to rebuild the world in the digital arena. ” Vivo believes 6G will support dozens or hundreds of new services in a wide range of industries . The company is developing prototype 6G mobile technologies based on three trends—communication plus sensing ; communication plus computing ; and communication plus AI . For example , Vivo is developing a prototype that can collect users ’ biometric data to monitor their health while they are asleep . According to this technological vision , a person ’ s bedside phone could become a medical monitoring device . “ If there is any health issue or abnormal behavior happening with respiration , then [ the phone ] could send an alert to the hospital , ” says Qin . Vivo also sees virtual and mixed reality glasses as another potential application for 6G that could revolutionize video streaming by making it a more compelling and immersive experience . Current AR glasses have limited computing power , says Qin . “ Therefore , it needs to connect as a kind of edge device to the cloud so it could provide better experiences for the users. ” 6G will also support self-driving or autonomous cars . “ I believe autonomous driving will be very popular after 2030 and be supported by 6G , ” says Qin . “ Driverless cars need to really gather all kinds of data , for example , about the ambient environment , about road conditions and even the adjacent cars in order to make informed decisions [ such as ] whether it should speed up or break . 6G can provide the computing power and network. ” Although 5G mobile networks have yet to live up to initial expectations , most experts agree that 6G has the potential to deliver major advances in connectivity and computing power . However , like any complex and powerful new technology , 6G also faces challenges , including network capacity and energy consumption . Getting to the 6G era requires an increase in network capacity . Finding the right telecommunications spectrum to support its rollout is crucial . It has not been finalized but 6.4 to 15 gigahertz is under consideration for 6G . “ We think that the spectrum for 6G should be on the lower spectrum , like 6.4 to 7.1 gigahertz , because the lower band electromagnetic wave physically has much better coverage and penetration characteristics , ” says Qin . Minimizing 6G ’ s energy consumption and carbon emissions is another major task . 6G networks will have vastly more computing demands than 5G . Suppliers and users will need to cooperate to minimize energy use . According to a report by GSMA , which represents global telecom operators , energy-saving techniques ( such as AI-driven sleep states and lithium-ion batteries ) may help to make 6G more energy efficient . Ultimately , 6G will only succeed if it delivers great experiences and services for consumers and businesses , says Qin . “ We should avoid overdesign of 6G network , and we should really collaborate with different verticals. ” Problems faced by 5G networks—for example , bottlenecks in other technologies needed to support new terminals for 5G , such as material sciences for augmented reality equipment—should be lessons for 6G development , he adds . `` We hope that we can avoid these problems . That means we need the whole ecosystem to collaborate , to jointly develop 6G infrastructure , mobile terminals , and applications. ” This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['partnership', 'sixthgeneration', 'g', 'mobile', 'network', 'underpin', 'artificial', 'intelligence', 'poise', 'combine', 'communication', 'compute', 'hyperconnected', 'world', 'digital', 'physical', 'experience', 'transform', 'daily', 'life', 'expert', 'predict', 'past', 'talk', 'internet', 'thing', 'g', 'talk', 'intelligent', 'smart', 'internet', 'thing', 'say', 'president', 'chinese', 'mobile', 'phone', 'maker', 'step', 'r', 'effort', 'g', 'communication', 'tech', 'company', 'already', 'plan', 'g', 'wireless', 'network', 'even', 'g', 'yet', 'fully', 'roll', 'globally', 'improve', 'data', 'latency', 'security', 'reliability', 'ability', 'process', 'massive', 'volume', 'global', 'datum', 'real', 'time', 'expert', 'believe', 'g', 'set', 'transform', 'leisure', 'work', 'new', 'use', 'case', 'g', 'network', 'envision', 'vivo', 'mixed', 'reality', 'holographic', 'multisensory', 'communication', 'interactive', '3d', 'virtual', 'digital', 'human', 'collaborative', 'robot', 'automate', 'driving', 'expectation', 'g', 'deploy', 'telecom', 'agency', 'international', 'telecommunication', 'state', 'plan', 'finish', 'initial', 'g', 'standardization', 'process', 'later', 'year', 'optimize', 'technology', 'expert', 'expect', 'g', 'big', 'impact', 'g', 'reason', 'enable', 'convergence', 'computing', 'mobile', 'communication', 'integrate', 'digital', 'physical', 'realm', 'introduce', 'new', 'sensory', 'experience', 'user', 'qin', 'say', 'g', 'provide', 'super', 'communication', 'ubiquitous', 'information', 'converge', 'computing', 'service', 'thus', 'base', 'interconnected', 'converge', 'physical', 'digital', 'world', 'agree', 'predict', 'g', 'network', 'enable', 'immersive', 'ubiquitous', 'sensory', 'digital', 'experience', 'massive', 'scale', 'make', 'possible', 'g', 'application', 'sense', 'surrounding', 'thereby', 'turn', 'network', 'sixth', 'sense', 'accord', 'report', 'consultancy', 'generation', 'wireless', 'network', 'become', 'increasingly', 'complex', 'rely', 'technology', 'harness', 'power', 'make', 'easy', 'run', 'g', 'expect', 'first', 'ainative', 'network', 'embed', 'networking', 'equipment', 'enable', 'network', 'learn', 'manage', 'autonomous', 'make', 'cheap', 'run', 'design', 'g', 'network', 'go', 'use', 'ai', 'technology', 'design', 'air', 'interface', 'also', 'manage', 'g', 'network', 'say', 'machine', 'learning', 'aibased', 'network', 'automation', 'crucial', 'simplify', 'network', 'management', 'optimization', 'g', 'network', 'inside', 'good', 'student', 'add', 'g', 'network', 'selftrain', 'selflearn', 'actually', 'grow', 'student', 'become', 'powerful', 'g', 'standard', 'specification', 'still', 'development', 'expert', 'agree', 'leapfrog', 'technology', 'thank', 'high', 'speed', 'estimate', 'vary', 'g', 'time', 'time', 'time', 'fast', 'g', 'significantly', 'reduce', 'latency', 'improve', 'connectivity', 'security', 'reliability', 'ability', 'integrate', 'digital', 'physical', 'version', 'world', 'g', 'mainly', 'communication', 'technology', 'core', 'say', 'g', 'enhanced', 'communication', 'technology', 'also', 'include', 'compute', 'well', 'relevant', 'service', 'benefit', 'wide', 'geographical', 'coverage', 'g', 'g', 'cover', 'whole', 'planet', 'connect', 'kind', 'machine', 'add', 'qin', 'say', 'g', 'network', 'also', 'popularize', 'use', 'digital', 'twin', 'virtual', 'replica', 'product', 'process', 'use', 'predict', 'physical', 'entity', 'perform', 'real', 'world', 'possible', 'due', 'g', 'network', 'enhance', 'connectivity', 'strong', 'sensing', 'capability', 'capacity', 'collect', 'massive', 'amount', 'datum', 'accord', 'powerful', 'connectivity', 'sensing', 'capability', 'install', 'sensor', 'physical', 'world', 'collect', 'massive', 'amount', 'datum', 'world', 'datum', 'build', 'model', 'rebuild', 'world', 'digital', 'arena', 'vivo', 'believe', 'g', 'support', 'dozen', 'hundred', 'new', 'service', 'wide', 'range', 'industry', 'company', 'develop', 'prototype', 'g', 'mobile', 'technology', 'base', 'trend', 'communication', 'sensing', 'communication', 'computing', 'communication', 'ai', 'example', 'vivo', 'develop', 'prototype', 'collect', 'user', 'biometric', 'datum', 'monitor', 'health', 'asleep', 'accord', 'technological', 'vision', 'person', 'bedside', 'phone', 'become', 'medical', 'monitoring', 'device', 'health', 'issue', 'abnormal', 'behavior', 'happen', 'respiration', 'phone', 'send', 'alert', 'hospital', 'say', 'vivo', 'also', 'see', 'virtual', 'mixed', 'reality', 'glass', 'potential', 'application', 'g', 'revolutionize', 'video', 'streaming', 'make', 'compelling', 'immersive', 'experience', 'current', 'ar', 'glass', 'limit', 'computing', 'power', 'say', 'therefore', 'need', 'connect', 'kind', 'edge', 'device', 'cloud', 'provide', 'well', 'experience', 'user', 'g', 'also', 'support', 'selfdriving', 'autonomous', 'car', 'believe', 'autonomous', 'driving', 'popular', 'support', 'g', 'say', 'driverless', 'car', 'need', 'really', 'gather', 'kind', 'datum', 'example', 'ambient', 'environment', 'road', 'condition', 'even', 'adjacent', 'car', 'order', 'make', 'informed', 'decision', 'speed', 'break', 'g', 'provide', 'computing', 'power', 'network', 'g', 'mobile', 'network', 'yet', 'live', 'initial', 'expectation', 'expert', 'agree', 'g', 'potential', 'deliver', 'major', 'advance', 'connectivity', 'computing', 'power', 'however', 'complex', 'powerful', 'new', 'technology', 'g', 'also', 'face', 'challenge', 'include', 'network', 'capacity', 'energy', 'consumption', 'get', 'g', 'era', 'require', 'increase', 'network', 'capacity', 'find', 'right', 'telecommunication', 'spectrum', 'support', 'rollout', 'crucial', 'finalize', 'gigahertz', 'consideration', 'g', 'think', 'spectrum', 'g', 'low', 'spectrum', 'gigahertz', 'low', 'band', 'electromagnetic', 'wave', 'physically', 'much', 'well', 'coverage', 'penetration', 'characteristic', 'say', 'minimize', 'g', 'energy', 'consumption', 'carbon', 'emission', 'major', 'task', 'g', 'network', 'vastly', 'computing', 'demand', 'g', 'supplier', 'user', 'need', 'cooperate', 'minimize', 'energy', 'use', 'accord', 'report', 'gsma', 'represent', 'global', 'telecom', 'operator', 'energysave', 'technique', 'sleep', 'state', 'lithiumion', 'battery', 'help', 'make', 'energy', 'efficient', 'ultimately', 'g', 'succeed', 'deliver', 'great', 'experience', 'service', 'consumer', 'business', 'say', 'avoid', 'overdesign', 'g', 'network', 'really', 'collaborate', 'different', 'vertical', 'problem', 'face', 'g', 'network', 'example', 'bottleneck', 'technology', 'need', 'support', 'new', 'terminal', 'g', 'material', 'science', 'augment', 'reality', 'equipment', 'lesson', 'g', 'development', 'add', 'hope', 'avoid', 'problem', 'mean', 'need', 'whole', 'ecosystem', 'collaborate', 'jointly', 'develop', 'g', 'infrastructure', 'mobile', 'terminal', 'application', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","Sixth-generation (6G) mobile networks, underpinned by artificial intelligence (AI), are poised to combine communication and computing in a hyperconnected world of digital and physical experiences that will transform daily lives, experts predict. “In the past, we talked about internet of things, but with 6G, we talk about intelligent or smart internet of things,” says Qin…"
Enabling enterprise growth with data intelligence,https://www.technologyreview.com/2023/10/19/1081876/enabling-enterprise-growth-with-data-intelligence/,2023-10-19,"In partnership withHitachi Vantara  Data — how it’s stored and managed — has become a key competitive differentiator. As global data continues to grow exponentially, organizations face many hurdles between piling up historical data, real-time data streams from IoT sensors, and building data-driven supply chains. Senior vice president of product engineering at Hitachi Vantara, Bharti Patel sees these challenges as an opportunity to create a better data strategy. “Before enterprises can become data-driven, they must first become data intelligent,"" says Patel. ""That means knowing more about the data you have, whether you need to keep it or not, or where it should reside to derive the most value out of it."" Patel stresses that the data journey begins with data planning that includes all stakeholders from CIOs and CTOs to business users. Patel describes universal data intelligence as enterprises having the ability to gain better insights from data streams and meet increasing demands for transparency by offering seamless access to data and insights no matter where it resides. Building this intelligence means building a data infrastructure that is scalable, secure, cost-effective, and socially responsible. The public cloud is often lauded as a way for enterprises to innovate with agility at scale while on premises infrastructures are viewed as less accessible and user friendly. But while data streams continue to grow, IT budgets are not and Patel notes that many organizations that use the cloud are facing cost challenges. Combating this, says Patel, means finding the best of both worlds of both on-prem and cloud environments in private data centers to keep costs low but insights flowing. Looking ahead, Patel foresees a future of total automation. Today, data resides in many places from the minds of experts to documentation to IT support tickets, making it impossible for one person to be able to analyze all that data and glean meaningful insights. “As we go into the future, we'll see more manual operations converted into automated operations,"" says Patel. ""First, we'll see humans in the loop, and eventually we'll see a trend towards fully autonomous data centers."" This episode of Business Lab is produced in partnership with Hitachi Vantara. Laurel Ruma: From MIT Technology Review, I'm Laurel Ruma and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.Our topic today is building better data infrastructures. Doing just the basics with data can be difficult, but when it comes to scaling and adopting emerging technologies, it's crucial to organize data, tear down data silos, and focus on how data infrastructure, which is so often in the background, comes to the front of your data strategy.Two words for you: data intelligence.My guest is Bharti Patel. Bharti is a senior vice president of product engineering at Hitachi Vantara. This episode of Business Lab is sponsored by Hitachi Vantara. Welcome, Bharti. Bharti Patel: Hey, thank you Laurel. Nice to be with you again. Laurel: So let's start off with kind of giving some context to this discussion. As global data continues to grow exponentially, according to IDC, it's projected to double between 2022 and 2026. Enterprises face many hurdles to becoming data-driven. These hurdles include, but aren't of course limited to, piles of historical data, new real-time data streams, and supply chains becoming more data-driven. How should enterprises be evaluating their data strategies? And what are the markers of a strong data infrastructure? Bharti: Yeah, Laurel, I can't agree more with you here. Data is growing exponentially, and as per one of the studies that we conducted recently where we talked to about 1,200 CIOs and CTOs from about 12 countries, then we have more proof for it that data is almost going to double every two to three years. And I think what's more interesting here is that data is going to grow, but their budgets are not going to grow in the same proportion. So instead of worrying about it, I want to tackle this problem differently. I want to look at how we convert this challenge into an opportunity by deriving value out of this deal. So let's talk a little more about this in the context of what's happening in the industry today. I'm sure everyone by now has heard about generative AI and why generative AI or gen AI is a buzzword. AI has been there in the industry forever. However, what has changed recently is ChatGPT has exposed the power of AI to common people right from school going kids to grandparents by providing a very simple natural language interface. And just to talk a little bit more about ChatGPT, it is the fastest growing app in the industry. It touched 100 million users in just about two months. And what has changed because of this very fast adoption is that this has got businesses interested in it. Everyone wants to see how to unleash the power of generative AI. In fact, according to McKinsey, they're saying it's like it's going to add about $2.6 trillion to $4.4 trillion to the global economy. That means we are talking about big numbers here, but everyone's talking about ChatGPT, but what is the science behind it? The science behind it is the large language models. And if you think of these large language models, they are AI models with billions or even trillions of parameters, and they are the science behind ChatGPT. However, to get most of these large language models or LLMs, they need to be fine-tuned because that means you're just relying on the public data. Then what you're getting, it means you're not getting first, you're not getting the information that you want, correct all the time. And of course there is a risk of people feeding bad data associated with it. So how do you make the most of it? And here actually comes your private data sets. So your proprietary data sets are very, very important here. And if you use this private data to fine-tune your models, I have no doubt in mind that it will create differentiation for you in the long run to remain competitive. So I think even with this, we're just scratching the surface here when it comes to gen AI. And what more needs to be thought about for enterprise adoption is all the features that are needed like explainability, traceability, quality, trustworthiness, reliability. So if you again look at all these parameters, actually data is again the centerpiece of everything here. And you have to harness this private data, you have to curate it, and you have to create the data sets that will give you the maximum return on investment. Now, before enterprises can become data-driven, I think they must first become data intelligent. And that means knowing more about the data you have, whether you need to keep it or not, or where it should reside to derive the most value out of it. And as I talk to more and more CIOs and CTOs, it is very evident that there's a lot of data out there and we need to find a way to fix the problem. Because that data may or may not be useful, but you are storing it, you are keeping it, and you are spending money on it. So that is definitely a problem that needs to be solved. Then back to your question of, what is the right infrastructure, what are some of the parameters of it? So in my mind, it needs to be nimble, it needs to be scalable, trusted, secured, cost-effective, and finally socially responsible. Laurel: That certainly gives us a lot of perspective, Bharti. So customers are demanding more access to data and enterprises also need to get better insights from the streams of data that they're accumulating. So could you describe what universal data intelligence is, and then how it relates to data infrastructure? Bharti: Universal data intelligence is the ability for businesses to offer seamless access to data and insights irrespective of where it resides. So basically we are talking about getting full insights into your data in a hybrid environment. Also, on the same lines, we also talk about our approach to infrastructure, which is a distributed approach. And what I mean by distributed is that you do as little data movement as possible because moving data from one place to another place is expensive. So what we are doing here at Hitachi Vantara, we are designing systems. Think of it as there is an elastic fabric that ties it all together and we are able to get insights from the data no matter where it resides in a very, very timely manner. And even this data could be in any format, from structured, unstructured, and it could be blocked to file to objects. And just to kind of give you an example of the same, recently we worked with the Arizona Department of Water Resources to simplify their data management strategy. They have data coming from more than 300,000 water resources like means we are talking about huge data sets here. And what we did there for them was we designed an intelligent data discovery and automation tool. And in fact, we completed this data discovery and the metadata cataloging and platform migration in just two weeks with minimal downtime. And we are hearing all the time from them that they are really happy with it and they're now able to understand, integrate, and analyze the data sets to meet the needs of their water users, their planners, and their decision makers. Laurel: So that's a great example. So data and how it's stored and managed is clearly a competitive differentiator as well. But although the amount of data is increasing, many budgets, as you mentioned, particularly IT budgets are not. So how can organizations navigate building a data infrastructure that's effective and cost-efficient? And then do you have another example of how to do more with less? Bharti: Yeah, I think that's a great question. And this goes back to having data intelligence as the first step to becoming data-driven and reaping the full benefits of the data. So I think it goes back to you needing to know what exists and why it exists. And all of it should be available to the decision makers and the people who are working on the data at their fingertips. Just to give an example here, suppose you have data that you're just retaining because you need to just retain it for legal purposes, and the likelihood of it being used is extremely, extremely low. So there's no point in storing that data on an expensive storage device. It makes sense to transfer that data to a low cost object storage. And at the same time, you might have the data that you need to access all the time. And speed is important. Low latency is important, and that kind of data needs to reside on fast NVMEs. And in fact, many of our customers do it all the time, and in fact in all the sectors. So what they do is they have their data, which through the policies, they constantly transfer from our highly, highly efficient file systems to object storage based on the policies. And it's like they still retain the pointers there in the file system and they're able to access it back in case they need it. Laurel: So the public cloud is often cited as a way for enterprises to scale, be more agile, and innovate while by contrast, legacy on-premises infrastructures are seen as less user-friendly and accessible. How accurate is this conception and how should enterprises approach data modernization and management of that data? Bharti: Yeah, I've got to admit here that the public cloud and the hyperscalers have raised the bar in terms of what is possible when it comes to innovation. However, we are also seeing and hearing from our customers that the cost is a concern there. And in fact, many of our customers, they move to cloud very fast and now they're facing the cost challenge. When their CIOs see the bills going exponentially up, they're asking like, ""Hey, well how could we keep it flat?"" That's where I think we see a big opportunity, how to provide the same experience that cloud provides in a private data center so that when customers are talking about partition of the data, we have something equivalent to offer. And here again, I have got to say that we want to address in a slightly different manner. I think we want to address it so that customers are able to take full advantage of the elasticity of the cloud, and also they're able to take full advantage of on-prem environments. And how we want to do it, we want to do it in such a way that it's almost in a seamless way, in a seamless manner. They can manage the data from their private data centers, doing the cloud and get the best from both worlds. Laurel: An interesting perspective there, but this also kind of requires different elements of the business to come in. So from a leadership perspective, what are some best practices that you've instituted or recommended to make that transition to better data management? Bharti: Yeah, I would say I think the data journey starts with data planning, and which should not be done in a siloed manner. And getting it right from the onset is extremely, extremely important. And what you need to do here is at the beginning of your data planning, you've got to get all the stakeholders together, whether it's your CIO, your business users, your CTOs. So this strategy should never be done in a siloed manner. And in fact, I do want to think about, highlight another aspect, which probably people don't do very much is how do you even bring your partners into the mix? In fact, I do have an example here. Prior to joining Hitachi Vantara, I was a CTO, an air purifier company. And as we were defining our data strategy, we were looking at our Salesforce data, we were looking at data in our NetSuite, we were looking at the customer tickets, and we were doing all this to see how we can drive marketing campaigns. And as I was looking at this data, I felt that something was totally missing. And in fact, what was missing was the weather data, which is not our data, which was third-party data. For us to design effective marketing campaigns, it was very important for us to have insights into this weather data. For example, if there are allergies in a particular region or if there are wildfires in a particular region. And that data was so important. So having a strategy where you are able to bring all stakeholders, all parts of data together and think from the beginning is the right thing to get started. Laurel: And with big hairy problems and goals, there's also this consideration that data centers contribute to an enterprise's carbon emissions. Thinking about partnerships and modernizing data management and everything we've talked about so far, how can enterprises meet sustainability goals while also modernizing their data infrastructure to accommodate all of their historical and real-time data, especially when it comes from, as you mentioned, so many different sources? Bharti: Yeah, I'm glad that you are bringing up this point because it's very important not to ignore this. And in fact, with all the gen AI and all the things that we are talking about, like one fine-tuning of one model can actually generate up to five times the carbon emissions that are possible from a passenger car in a lifetime. So we're talking about a huge, huge environmental effect here. And this particular topic is extremely important to Hitachi. And in fact, our goal is to go carbon-neutral with our operations by 2030 and across our value chain by 2050. And how we are addressing this problem here is kind of both on the hardware side and also on the software side. Right from the onset, we are designing our hardware, we are looking at end-to-end components to see what kind of carbon footprint it creates and how we could really minimize it. And in fact, once our hardware is ready, actually, it needs to pass through a very stringent set of energy certifications. And so that's on the hardware side. Now, on the software side, actually, I have just started this initiative where we are looking at how we can move to modern languages that are more likely to create less carbon footprint. And this is where we are looking at how we can replace our existing Java [code base] with Rust, wherever it makes sense. And again, this is a big problem we all need to think about and it cannot be solved overnight, but we have to constantly think about interface manner. Laurel: Well, certainly are impressive goals. How can emerging technologies like generative AI, as you were saying before, help push an organization into a next generation of data infrastructure systems, but then also help differentiate it from competitors? Bharti: Yeah, I want to take a kind of a two-pronged approach here. First, what I call is table stakes. So if you don't do it, you'll be completely wiped out. And these are simple things about how you automate certain things, how you create better customer experience. But in my mind, that's not enough. You got to think about what kind of disruptions you will create for yourself and for your customers. So a couple of ideas that we are working on here are the companions or copilots. And these are, think of them as AI agents in the data centers. And these agents actually help the data center environment from becoming more reactive to proactive. So basically these agents are running in your data center all the time and they're watching if there is a new patch available and if you should update to the new patch, or maybe there's a new white paper that has better insights to manage some of your resources. So this is like these agents are constantly acting in your data center. They are aware of what's going on on the internet based on how you have designed, and they're able to provide you with creative solutions. And I think that's going to be the disruption here, and that's something we are working on. Laurel: So looking to the future, what tools, technologies, or trends do you see emerging as more and more enterprises look to modernize their data infrastructure and really benefit from data intelligence? Bharti: Again, I'll go back to what I'm talking about, generative AI here, and I'll give an example. For one of our customers, we are managing their data center, and I'm also part of that channel where we see constant back and forth between the support and the engineering. The support is asking, ""Hey, this is what is happening, what should we be doing?"" So just think of it like a different scenario that you have all this and you were able to collect this data and feed it into the LLMs. When you're talking about this data, this data resides at several places. It resides in the heads of our experts. It is there in the documentation, it's there in the support tickets, it's there in logs, like life logs. It is there in the traces. So it's almost impossible for a human being to analyze this data and get meaningful insights. However, if we combine LLMs with the power of, say, knowledge graphs, vector databases, and other tools, it will be possible to analyze this data at the speed of light, and present the recommendation in front of the user through a very simple user interface. And in most cases, just via a very simple natural language interface. So I think that's a kind of a complete paradigm shift where you have so many sources that you need to constantly analyze versus having the full automation. And that's why I feel that these copilots will become an essential part of the data centers. In the beginning they'll help with the automation to deal with the problems prevalent in any data center like resource management and optimization, proactive problem determination, and resolution of the same. As we go into the future, we'll see more manual operations converted into automated operations. First, we'll see humans in the loop, and eventually we'll see a trend towards fully autonomous data centers. Laurel: Well, that is quite a future. Thank you very much for joining us today on the Business Lab. Bharti: Thank you, Laurel. Bye-bye. Laurel: That was Bharti Patel, who is the senior vice president of Product Marketing at Hitachi Vantara who I spoke with from Cambridge, Massachusetts, the home of MIT and MIT Technology Review.That's it for this episode of Business Lab. I'm your host, Laurel Ruma. I'm the director of Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology, and you can find us in print, on the web, and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com.This show is available wherever you get your podcasts. If you enjoyed this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT Technology Review. This episode was produced by Giro Studios. Thanks for listening. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.","In partnership withHitachi Vantara Data — how it ’ s stored and managed — has become a key competitive differentiator . As global data continues to grow exponentially , organizations face many hurdles between piling up historical data , real-time data streams from IoT sensors , and building data-driven supply chains . Senior vice president of product engineering at Hitachi Vantara , Bharti Patel sees these challenges as an opportunity to create a better data strategy . “ Before enterprises can become data-driven , they must first become data intelligent , '' says Patel . `` That means knowing more about the data you have , whether you need to keep it or not , or where it should reside to derive the most value out of it . '' Patel stresses that the data journey begins with data planning that includes all stakeholders from CIOs and CTOs to business users . Patel describes universal data intelligence as enterprises having the ability to gain better insights from data streams and meet increasing demands for transparency by offering seamless access to data and insights no matter where it resides . Building this intelligence means building a data infrastructure that is scalable , secure , cost-effective , and socially responsible . The public cloud is often lauded as a way for enterprises to innovate with agility at scale while on premises infrastructures are viewed as less accessible and user friendly . But while data streams continue to grow , IT budgets are not and Patel notes that many organizations that use the cloud are facing cost challenges . Combating this , says Patel , means finding the best of both worlds of both on-prem and cloud environments in private data centers to keep costs low but insights flowing . Looking ahead , Patel foresees a future of total automation . Today , data resides in many places from the minds of experts to documentation to IT support tickets , making it impossible for one person to be able to analyze all that data and glean meaningful insights . “ As we go into the future , we 'll see more manual operations converted into automated operations , '' says Patel . `` First , we 'll see humans in the loop , and eventually we 'll see a trend towards fully autonomous data centers . '' This episode of Business Lab is produced in partnership with Hitachi Vantara . Laurel Ruma : From MIT Technology Review , I 'm Laurel Ruma and this is Business Lab , the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.Our topic today is building better data infrastructures . Doing just the basics with data can be difficult , but when it comes to scaling and adopting emerging technologies , it 's crucial to organize data , tear down data silos , and focus on how data infrastructure , which is so often in the background , comes to the front of your data strategy.Two words for you : data intelligence.My guest is Bharti Patel . Bharti is a senior vice president of product engineering at Hitachi Vantara . This episode of Business Lab is sponsored by Hitachi Vantara . Welcome , Bharti . Bharti Patel : Hey , thank you Laurel . Nice to be with you again . Laurel : So let 's start off with kind of giving some context to this discussion . As global data continues to grow exponentially , according to IDC , it 's projected to double between 2022 and 2026 . Enterprises face many hurdles to becoming data-driven . These hurdles include , but are n't of course limited to , piles of historical data , new real-time data streams , and supply chains becoming more data-driven . How should enterprises be evaluating their data strategies ? And what are the markers of a strong data infrastructure ? Bharti : Yeah , Laurel , I ca n't agree more with you here . Data is growing exponentially , and as per one of the studies that we conducted recently where we talked to about 1,200 CIOs and CTOs from about 12 countries , then we have more proof for it that data is almost going to double every two to three years . And I think what 's more interesting here is that data is going to grow , but their budgets are not going to grow in the same proportion . So instead of worrying about it , I want to tackle this problem differently . I want to look at how we convert this challenge into an opportunity by deriving value out of this deal . So let 's talk a little more about this in the context of what 's happening in the industry today . I 'm sure everyone by now has heard about generative AI and why generative AI or gen AI is a buzzword . AI has been there in the industry forever . However , what has changed recently is ChatGPT has exposed the power of AI to common people right from school going kids to grandparents by providing a very simple natural language interface . And just to talk a little bit more about ChatGPT , it is the fastest growing app in the industry . It touched 100 million users in just about two months . And what has changed because of this very fast adoption is that this has got businesses interested in it . Everyone wants to see how to unleash the power of generative AI . In fact , according to McKinsey , they 're saying it 's like it 's going to add about $ 2.6 trillion to $ 4.4 trillion to the global economy . That means we are talking about big numbers here , but everyone 's talking about ChatGPT , but what is the science behind it ? The science behind it is the large language models . And if you think of these large language models , they are AI models with billions or even trillions of parameters , and they are the science behind ChatGPT . However , to get most of these large language models or LLMs , they need to be fine-tuned because that means you 're just relying on the public data . Then what you 're getting , it means you 're not getting first , you 're not getting the information that you want , correct all the time . And of course there is a risk of people feeding bad data associated with it . So how do you make the most of it ? And here actually comes your private data sets . So your proprietary data sets are very , very important here . And if you use this private data to fine-tune your models , I have no doubt in mind that it will create differentiation for you in the long run to remain competitive . So I think even with this , we 're just scratching the surface here when it comes to gen AI . And what more needs to be thought about for enterprise adoption is all the features that are needed like explainability , traceability , quality , trustworthiness , reliability . So if you again look at all these parameters , actually data is again the centerpiece of everything here . And you have to harness this private data , you have to curate it , and you have to create the data sets that will give you the maximum return on investment . Now , before enterprises can become data-driven , I think they must first become data intelligent . And that means knowing more about the data you have , whether you need to keep it or not , or where it should reside to derive the most value out of it . And as I talk to more and more CIOs and CTOs , it is very evident that there 's a lot of data out there and we need to find a way to fix the problem . Because that data may or may not be useful , but you are storing it , you are keeping it , and you are spending money on it . So that is definitely a problem that needs to be solved . Then back to your question of , what is the right infrastructure , what are some of the parameters of it ? So in my mind , it needs to be nimble , it needs to be scalable , trusted , secured , cost-effective , and finally socially responsible . Laurel : That certainly gives us a lot of perspective , Bharti . So customers are demanding more access to data and enterprises also need to get better insights from the streams of data that they 're accumulating . So could you describe what universal data intelligence is , and then how it relates to data infrastructure ? Bharti : Universal data intelligence is the ability for businesses to offer seamless access to data and insights irrespective of where it resides . So basically we are talking about getting full insights into your data in a hybrid environment . Also , on the same lines , we also talk about our approach to infrastructure , which is a distributed approach . And what I mean by distributed is that you do as little data movement as possible because moving data from one place to another place is expensive . So what we are doing here at Hitachi Vantara , we are designing systems . Think of it as there is an elastic fabric that ties it all together and we are able to get insights from the data no matter where it resides in a very , very timely manner . And even this data could be in any format , from structured , unstructured , and it could be blocked to file to objects . And just to kind of give you an example of the same , recently we worked with the Arizona Department of Water Resources to simplify their data management strategy . They have data coming from more than 300,000 water resources like means we are talking about huge data sets here . And what we did there for them was we designed an intelligent data discovery and automation tool . And in fact , we completed this data discovery and the metadata cataloging and platform migration in just two weeks with minimal downtime . And we are hearing all the time from them that they are really happy with it and they 're now able to understand , integrate , and analyze the data sets to meet the needs of their water users , their planners , and their decision makers . Laurel : So that 's a great example . So data and how it 's stored and managed is clearly a competitive differentiator as well . But although the amount of data is increasing , many budgets , as you mentioned , particularly IT budgets are not . So how can organizations navigate building a data infrastructure that 's effective and cost-efficient ? And then do you have another example of how to do more with less ? Bharti : Yeah , I think that 's a great question . And this goes back to having data intelligence as the first step to becoming data-driven and reaping the full benefits of the data . So I think it goes back to you needing to know what exists and why it exists . And all of it should be available to the decision makers and the people who are working on the data at their fingertips . Just to give an example here , suppose you have data that you 're just retaining because you need to just retain it for legal purposes , and the likelihood of it being used is extremely , extremely low . So there 's no point in storing that data on an expensive storage device . It makes sense to transfer that data to a low cost object storage . And at the same time , you might have the data that you need to access all the time . And speed is important . Low latency is important , and that kind of data needs to reside on fast NVMEs . And in fact , many of our customers do it all the time , and in fact in all the sectors . So what they do is they have their data , which through the policies , they constantly transfer from our highly , highly efficient file systems to object storage based on the policies . And it 's like they still retain the pointers there in the file system and they 're able to access it back in case they need it . Laurel : So the public cloud is often cited as a way for enterprises to scale , be more agile , and innovate while by contrast , legacy on-premises infrastructures are seen as less user-friendly and accessible . How accurate is this conception and how should enterprises approach data modernization and management of that data ? Bharti : Yeah , I 've got to admit here that the public cloud and the hyperscalers have raised the bar in terms of what is possible when it comes to innovation . However , we are also seeing and hearing from our customers that the cost is a concern there . And in fact , many of our customers , they move to cloud very fast and now they 're facing the cost challenge . When their CIOs see the bills going exponentially up , they 're asking like , `` Hey , well how could we keep it flat ? '' That 's where I think we see a big opportunity , how to provide the same experience that cloud provides in a private data center so that when customers are talking about partition of the data , we have something equivalent to offer . And here again , I have got to say that we want to address in a slightly different manner . I think we want to address it so that customers are able to take full advantage of the elasticity of the cloud , and also they 're able to take full advantage of on-prem environments . And how we want to do it , we want to do it in such a way that it 's almost in a seamless way , in a seamless manner . They can manage the data from their private data centers , doing the cloud and get the best from both worlds . Laurel : An interesting perspective there , but this also kind of requires different elements of the business to come in . So from a leadership perspective , what are some best practices that you 've instituted or recommended to make that transition to better data management ? Bharti : Yeah , I would say I think the data journey starts with data planning , and which should not be done in a siloed manner . And getting it right from the onset is extremely , extremely important . And what you need to do here is at the beginning of your data planning , you 've got to get all the stakeholders together , whether it 's your CIO , your business users , your CTOs . So this strategy should never be done in a siloed manner . And in fact , I do want to think about , highlight another aspect , which probably people do n't do very much is how do you even bring your partners into the mix ? In fact , I do have an example here . Prior to joining Hitachi Vantara , I was a CTO , an air purifier company . And as we were defining our data strategy , we were looking at our Salesforce data , we were looking at data in our NetSuite , we were looking at the customer tickets , and we were doing all this to see how we can drive marketing campaigns . And as I was looking at this data , I felt that something was totally missing . And in fact , what was missing was the weather data , which is not our data , which was third-party data . For us to design effective marketing campaigns , it was very important for us to have insights into this weather data . For example , if there are allergies in a particular region or if there are wildfires in a particular region . And that data was so important . So having a strategy where you are able to bring all stakeholders , all parts of data together and think from the beginning is the right thing to get started . Laurel : And with big hairy problems and goals , there 's also this consideration that data centers contribute to an enterprise 's carbon emissions . Thinking about partnerships and modernizing data management and everything we 've talked about so far , how can enterprises meet sustainability goals while also modernizing their data infrastructure to accommodate all of their historical and real-time data , especially when it comes from , as you mentioned , so many different sources ? Bharti : Yeah , I 'm glad that you are bringing up this point because it 's very important not to ignore this . And in fact , with all the gen AI and all the things that we are talking about , like one fine-tuning of one model can actually generate up to five times the carbon emissions that are possible from a passenger car in a lifetime . So we 're talking about a huge , huge environmental effect here . And this particular topic is extremely important to Hitachi . And in fact , our goal is to go carbon-neutral with our operations by 2030 and across our value chain by 2050 . And how we are addressing this problem here is kind of both on the hardware side and also on the software side . Right from the onset , we are designing our hardware , we are looking at end-to-end components to see what kind of carbon footprint it creates and how we could really minimize it . And in fact , once our hardware is ready , actually , it needs to pass through a very stringent set of energy certifications . And so that 's on the hardware side . Now , on the software side , actually , I have just started this initiative where we are looking at how we can move to modern languages that are more likely to create less carbon footprint . And this is where we are looking at how we can replace our existing Java [ code base ] with Rust , wherever it makes sense . And again , this is a big problem we all need to think about and it can not be solved overnight , but we have to constantly think about interface manner . Laurel : Well , certainly are impressive goals . How can emerging technologies like generative AI , as you were saying before , help push an organization into a next generation of data infrastructure systems , but then also help differentiate it from competitors ? Bharti : Yeah , I want to take a kind of a two-pronged approach here . First , what I call is table stakes . So if you do n't do it , you 'll be completely wiped out . And these are simple things about how you automate certain things , how you create better customer experience . But in my mind , that 's not enough . You got to think about what kind of disruptions you will create for yourself and for your customers . So a couple of ideas that we are working on here are the companions or copilots . And these are , think of them as AI agents in the data centers . And these agents actually help the data center environment from becoming more reactive to proactive . So basically these agents are running in your data center all the time and they 're watching if there is a new patch available and if you should update to the new patch , or maybe there 's a new white paper that has better insights to manage some of your resources . So this is like these agents are constantly acting in your data center . They are aware of what 's going on on the internet based on how you have designed , and they 're able to provide you with creative solutions . And I think that 's going to be the disruption here , and that 's something we are working on . Laurel : So looking to the future , what tools , technologies , or trends do you see emerging as more and more enterprises look to modernize their data infrastructure and really benefit from data intelligence ? Bharti : Again , I 'll go back to what I 'm talking about , generative AI here , and I 'll give an example . For one of our customers , we are managing their data center , and I 'm also part of that channel where we see constant back and forth between the support and the engineering . The support is asking , `` Hey , this is what is happening , what should we be doing ? '' So just think of it like a different scenario that you have all this and you were able to collect this data and feed it into the LLMs . When you 're talking about this data , this data resides at several places . It resides in the heads of our experts . It is there in the documentation , it 's there in the support tickets , it 's there in logs , like life logs . It is there in the traces . So it 's almost impossible for a human being to analyze this data and get meaningful insights . However , if we combine LLMs with the power of , say , knowledge graphs , vector databases , and other tools , it will be possible to analyze this data at the speed of light , and present the recommendation in front of the user through a very simple user interface . And in most cases , just via a very simple natural language interface . So I think that 's a kind of a complete paradigm shift where you have so many sources that you need to constantly analyze versus having the full automation . And that 's why I feel that these copilots will become an essential part of the data centers . In the beginning they 'll help with the automation to deal with the problems prevalent in any data center like resource management and optimization , proactive problem determination , and resolution of the same . As we go into the future , we 'll see more manual operations converted into automated operations . First , we 'll see humans in the loop , and eventually we 'll see a trend towards fully autonomous data centers . Laurel : Well , that is quite a future . Thank you very much for joining us today on the Business Lab . Bharti : Thank you , Laurel . Bye-bye . Laurel : That was Bharti Patel , who is the senior vice president of Product Marketing at Hitachi Vantara who I spoke with from Cambridge , Massachusetts , the home of MIT and MIT Technology Review.That 's it for this episode of Business Lab . I 'm your host , Laurel Ruma . I 'm the director of Insights , the custom publishing division of MIT Technology Review . We were founded in 1899 at the Massachusetts Institute of Technology , and you can find us in print , on the web , and at events each year around the world . For more information about us and the show , please check out our website at technologyreview.com.This show is available wherever you get your podcasts . If you enjoyed this episode , we hope you 'll take a moment to rate and review us . Business Lab is a production of MIT Technology Review . This episode was produced by Giro Studios . Thanks for listening . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['partnership', 'store', 'manage', 'become', 'key', 'competitive', 'differentiator', 'global', 'datum', 'continue', 'grow', 'exponentially', 'organization', 'face', 'many', 'hurdle', 'pile', 'historical', 'data', 'realtime', 'datum', 'stream', 'iot', 'sensor', 'build', 'datadriven', 'supply', 'chain', 'senior', 'vice', 'president', 'product', 'engineering', 'see', 'challenge', 'opportunity', 'create', 'well', 'datum', 'strategy', 'enterprise', 'become', 'datadriven', 'first', 'become', 'datum', 'intelligent', 'say', 'mean', 'know', 'datum', 'need', 'keep', 'reside', 'derive', 'value', 'stress', 'datum', 'journey', 'begin', 'datum', 'planning', 'include', 'stakeholder', 'cio', 'cto', 'business', 'user', 'describe', 'universal', 'datum', 'intelligence', 'enterprise', 'ability', 'gain', 'well', 'insight', 'datum', 'stream', 'meet', 'increase', 'demand', 'transparency', 'offer', 'seamless', 'access', 'datum', 'insight', 'matter', 'reside', 'build', 'intelligence', 'mean', 'build', 'datum', 'infrastructure', 'scalable', 'secure', 'costeffective', 'socially', 'responsible', 'public', 'cloud', 'often', 'laud', 'way', 'enterprise', 'innovate', 'agility', 'scale', 'premise', 'infrastructure', 'view', 'less', 'accessible', 'user', 'friendly', 'datum', 'stream', 'continue', 'grow', 'budget', 'patel', 'note', 'many', 'organization', 'use', 'cloud', 'face', 'cost', 'challenge', 'combat', 'say', 'mean', 'find', 'good', 'world', 'cloud', 'environment', 'private', 'datum', 'center', 'keep', 'cost', 'low', 'insight', 'flow', 'look', 'ahead', 'foresee', 'future', 'total', 'automation', 'today', 'datum', 'reside', 'many', 'place', 'mind', 'expert', 'documentation', 'support', 'ticket', 'make', 'impossible', 'person', 'able', 'analyze', 'datum', 'glean', 'meaningful', 'insight', 'go', 'future', 'see', 'manual', 'operation', 'convert', 'automate', 'operation', 'say', 'first', 'see', 'human', 'loop', 'eventually', 'see', 'trend', 'fully', 'autonomous', 'datum', 'center', 'episode', 'business', 'lab', 'produce', 'partnership', 'mit', 'technology', 'review', 'business', 'lab', 'show', 'help', 'business', 'leader', 'make', 'sense', 'new', 'technology', 'come', 'lab', 'marketplaceour', 'topic', 'today', 'build', 'well', 'datum', 'infrastructure', 'basic', 'datum', 'difficult', 'come', 'scale', 'adopt', 'emerge', 'technology', 'crucial', 'organize', 'datum', 'tear', 'data', 'silo', 'focus', 'datum', 'infrastructure', 'often', 'background', 'come', 'front', 'datum', 'strategytwo', 'word', 'datum', 'intelligencemy', 'guest', 'patel', 'bharti', 'senior', 'vice', 'president', 'product', 'engineering', 'episode', 'business', 'lab', 'sponsor', 'welcome', 'bharti', 'thank', 'nice', 'laurel', 'let', 'start', 'kind', 'give', 'context', 'discussion', 'global', 'datum', 'continue', 'grow', 'exponentially', 'accord', 'project', 'double', 'enterprise', 'face', 'many', 'hurdle', 'become', 'datadriven', 'hurdle', 'include', 'course', 'limit', 'pile', 'historical', 'datum', 'new', 'realtime', 'datum', 'stream', 'supply', 'chain', 'become', 'datadriven', 'enterprise', 'evaluate', 'datum', 'strategy', 'marker', 'strong', 'data', 'infrastructure', 'bharti', 'laurel', 'agree', 'datum', 'grow', 'exponentially', 'study', 'conduct', 'recently', 'talk', 'cio', 'cto', 'country', 'proof', 'data', 'almost', 'go', 'double', 'year', 'think', 'interesting', 'data', 'go', 'grow', 'budget', 'go', 'grow', 'proportion', 'instead', 'worry', 'want', 'tackle', 'problem', 'differently', 'want', 'look', 'convert', 'challenge', 'opportunity', 'derive', 'value', 'deal', 'let', 'talk', 'little', 'context', 'happen', 'industry', 'today', 'sure', 'hear', 'generative', 'generative', 'ai', 'buzzword', 'industry', 'forever', 'however', 'change', 'recently', 'chatgpt', 'expose', 'power', 'ai', 'common', 'people', 'right', 'school', 'go', 'kid', 'grandparent', 'provide', 'simple', 'natural', 'language', 'interface', 'talk', 'little', 'bit', 'chatgpt', 'fast', 'grow', 'app', 'industry', 'touch', 'user', 'month', 'change', 'fast', 'adoption', 'get', 'business', 'interested', 'want', 'see', 'unleash', 'power', 'generative', 'ai', 'fact', 'accord', 'mckinsey', 'say', 'go', 'add', 'global', 'economy', 'mean', 'talk', 'big', 'number', 'talk', 'chatgpt', 'science', 'science', 'large', 'language', 'model', 'think', 'large', 'language', 'model', 'model', 'billion', 'even', 'trillion', 'parameter', 'science', 'chatgpt', 'however', 'get', 'large', 'language', 'model', 'llm', 'need', 'finetune', 'mean', 'rely', 'public', 'datum', 'get', 'mean', 'get', 'first', 'get', 'information', 'want', 'correct', 'time', 'course', 'risk', 'people', 'feed', 'bad', 'datum', 'associate', 'make', 'actually', 'come', 'private', 'data', 'set', 'proprietary', 'data', 'set', 'important', 'use', 'private', 'datum', 'finetune', 'model', 'doubt', 'mind', 'create', 'differentiation', 'long', 'run', 'remain', 'competitive', 'think', 'even', 'scratch', 'surface', 'come', 'ai', 'need', 'think', 'enterprise', 'adoption', 'feature', 'need', 'explainability', 'traceability', 'quality', 'trustworthiness', 'reliability', 'look', 'parameter', 'actually', 'datum', 'centerpiece', 'harness', 'private', 'datum', 'curate', 'create', 'datum', 'set', 'give', 'maximum', 'return', 'investment', 'enterprise', 'become', 'datadriven', 'think', 'first', 'become', 'datum', 'intelligent', 'mean', 'know', 'datum', 'need', 'keep', 'reside', 'derive', 'value', 'talk', 'cio', 'cto', 'evident', 'lot', 'datum', 'need', 'find', 'way', 'fix', 'problem', 'datum', 'useful', 'store', 'keep', 'spend', 'money', 'definitely', 'problem', 'need', 'solve', 'back', 'question', 'right', 'infrastructure', 'parameter', 'mind', 'need', 'nimble', 'need', 'scalable', 'trust', 'secure', 'costeffective', 'finally', 'socially', 'responsible', 'laurel', 'certainly', 'give', 'lot', 'perspective', 'bharti', 'customer', 'demand', 'access', 'datum', 'enterprise', 'also', 'need', 'get', 'well', 'insight', 'stream', 'datum', 'accumulate', 'describe', 'universal', 'datum', 'intelligence', 'relate', 'data', 'infrastructure', 'bharti', 'universal', 'datum', 'intelligence', 'ability', 'business', 'offer', 'seamless', 'access', 'datum', 'insight', 'irrespective', 'reside', 'basically', 'talk', 'get', 'full', 'insight', 'datum', 'hybrid', 'environment', 'also', 'line', 'also', 'talk', 'approach', 'infrastructure', 'distribute', 'approach', 'mean', 'distribute', 'little', 'datum', 'movement', 'possible', 'move', 'datum', 'place', 'place', 'expensive', 'design', 'system', 'think', 'elastic', 'fabric', 'tie', 'together', 'able', 'get', 'insight', 'datum', 'matter', 'reside', 'timely', 'manner', 'even', 'datum', 'format', 'structured', 'unstructured', 'block', 'file', 'object', 'kind', 'give', 'example', 'recently', 'work', 'water', 'resource', 'simplify', 'data', 'management', 'strategy', 'datum', 'come', 'water', 'resource', 'mean', 'talk', 'huge', 'data', 'set', 'design', 'intelligent', 'datum', 'discovery', 'automation', 'tool', 'fact', 'complete', 'data', 'discovery', 'metadata', 'catalog', 'platform', 'migration', 'week', 'minimal', 'downtime', 'hear', 'time', 'really', 'happy', 'able', 'understand', 'integrate', 'analyze', 'datum', 'set', 'meet', 'need', 'water', 'user', 'planner', 'decision', 'maker', 'laurel', 'great', 'example', 'datum', 'store', 'manage', 'clearly', 'competitive', 'differentiator', 'well', 'amount', 'datum', 'increase', 'many', 'budget', 'mention', 'particularly', 'budget', 'organization', 'navigate', 'build', 'datum', 'infrastructure', 'effective', 'costefficient', 'example', 'less', 'bharti', 'think', 'great', 'question', 'go', 'back', 'datum', 'intelligence', 'first', 'step', 'become', 'datadriven', 'reap', 'full', 'benefit', 'datum', 'think', 'go', 'back', 'need', 'know', 'exist', 'exist', 'available', 'decision', 'maker', 'people', 'work', 'datum', 'fingertip', 'give', 'example', 'suppose', 'datum', 'retain', 'need', 'retain', 'legal', 'purpose', 'likelihood', 'use', 'extremely', 'extremely', 'low', 'point', 'store', 'datum', 'expensive', 'storage', 'device', 'make', 'sense', 'transfer', 'datum', 'low', 'cost', 'object', 'storage', 'time', 'datum', 'need', 'access', 'time', 'speed', 'important', 'low', 'latency', 'important', 'kind', 'datum', 'need', 'reside', 'fast', 'nvme', 'fact', 'many', 'customer', 'time', 'fact', 'sector', 'datum', 'policy', 'constantly', 'transfer', 'highly', 'highly', 'efficient', 'file', 'system', 'object', 'storage', 'base', 'policy', 'still', 'retain', 'pointer', 'file', 'system', 'able', 'access', 'back', 'case', 'need', 'public', 'cloud', 'often', 'cite', 'way', 'enterprise', 'scale', 'agile', 'innovate', 'contrast', 'legacy', 'onpremise', 'infrastructure', 'see', 'less', 'userfriendly', 'accessible', 'accurate', 'conception', 'enterprise', 'approach', 'modernization', 'management', 'datum', 'get', 'admit', 'public', 'cloud', 'hyperscaler', 'raise', 'bar', 'term', 'possible', 'come', 'innovation', 'however', 'also', 'see', 'hear', 'customer', 'cost', 'concern', 'fact', 'many', 'customer', 'move', 'cloud', 'fast', 'face', 'cost', 'challenge', 'cio', 'see', 'bill', 'go', 'exponentially', 'ask', 'keep', 'flat', 'think', 'see', 'big', 'opportunity', 'provide', 'experience', 'cloud', 'provide', 'private', 'datum', 'center', 'customer', 'talk', 'partition', 'data', 'equivalent', 'offer', 'get', 'say', 'want', 'address', 'slightly', 'different', 'manner', 'think', 'want', 'address', 'customer', 'able', 'take', 'full', 'advantage', 'elasticity', 'cloud', 'also', 'able', 'take', 'full', 'advantage', 'environment', 'want', 'want', 'way', 'almost', 'seamless', 'way', 'seamless', 'manner', 'manage', 'datum', 'private', 'data', 'center', 'cloud', 'get', 'good', 'world', 'laurel', 'interesting', 'perspective', 'also', 'kind', 'require', 'different', 'element', 'business', 'come', 'leadership', 'perspective', 'good', 'practice', 'institute', 'recommend', 'make', 'transition', 'well', 'datum', 'management', 'say', 'think', 'datum', 'journey', 'start', 'datum', 'planning', 'siloe', 'manner', 'get', 'right', 'onset', 'extremely', 'extremely', 'important', 'need', 'beginning', 'datum', 'plan', 'get', 'get', 'stakeholder', 'together', 'cio', 'business', 'user', 'cto', 'strategy', 'never', 'siloe', 'manner', 'fact', 'want', 'think', 'highlight', 'aspect', 'probably', 'people', 'much', 'even', 'bring', 'partner', 'mix', 'fact', 'example', 'prior', 'join', 'cto', 'air', 'purifi', 'company', 'define', 'datum', 'strategy', 'look', 'salesforce', 'datum', 'look', 'datum', 'netsuite', 'look', 'customer', 'ticket', 'see', 'drive', 'marketing', 'campaign', 'look', 'datum', 'feel', 'totally', 'miss', 'fact', 'miss', 'weather', 'datum', 'datum', 'datum', 'design', 'effective', 'marketing', 'campaign', 'important', 'insight', 'weather', 'datum', 'example', 'allergy', 'particular', 'region', 'wildfire', 'particular', 'region', 'datum', 'important', 'strategy', 'able', 'bring', 'stakeholder', 'part', 'datum', 'together', 'think', 'beginning', 'right', 'thing', 'start', 'laurel', 'big', 'hairy', 'problem', 'goal', 'also', 'consideration', 'datum', 'center', 'contribute', 'enterprise', 'carbon', 'emission', 'think', 'partnership', 'modernize', 'datum', 'management', 'talk', 'far', 'enterprise', 'meet', 'sustainability', 'goal', 'also', 'modernize', 'datum', 'infrastructure', 'accommodate', 'historical', 'realtime', 'datum', 'especially', 'come', 'mention', 'many', 'different', 'source', 'bharti', 'glad', 'bring', 'point', 'important', 'ignore', 'fact', 'thing', 'talk', 'finetuning', 'model', 'actually', 'generate', 'time', 'carbon', 'emission', 'possible', 'passenger', 'car', 'lifetime', 'talk', 'huge', 'huge', 'environmental', 'effect', 'particular', 'topic', 'extremely', 'important', 'fact', 'goal', 'go', 'carbonneutral', 'operation', 'value', 'chain', 'address', 'problem', 'kind', 'hardware', 'side', 'also', 'software', 'side', 'right', 'onset', 'design', 'hardware', 'look', 'endtoend', 'component', 'see', 'kind', 'carbon', 'footprint', 'create', 'really', 'minimize', 'fact', 'hardware', 'ready', 'actually', 'need', 'pass', 'stringent', 'set', 'energy', 'certification', 'hardware', 'side', 'software', 'side', 'actually', 'start', 'initiative', 'look', 'move', 'modern', 'language', 'likely', 'create', 'less', 'carbon', 'footprint', 'look', 'replace', 'exist', 'code', 'base', 'rust', 'make', 'sense', 'big', 'problem', 'need', 'think', 'solve', 'overnight', 'constantly', 'think', 'interface', 'manner', 'laurel', 'well', 'certainly', 'impressive', 'goal', 'emerge', 'technology', 'generative', 'ai', 'say', 'push', 'organization', 'next', 'generation', 'datum', 'infrastructure', 'system', 'also', 'help', 'differentiate', 'competitor', 'want', 'take', 'kind', 'twopronged', 'approach', 'first', 'call', 'table', 'stake', 'completely', 'wipe', 'simple', 'thing', 'automate', 'certain', 'thing', 'create', 'well', 'customer', 'experience', 'mind', 'enough', 'got', 'think', 'kind', 'disruption', 'create', 'customer', 'couple', 'idea', 'work', 'companion', 'copilot', 'think', 'agent', 'datum', 'center', 'agent', 'actually', 'help', 'datum', 'center', 'environment', 'become', 'reactive', 'proactive', 'basically', 'agent', 'run', 'datum', 'center', 'time', 'watch', 'new', 'patch', 'available', 'update', 'new', 'patch', 'maybe', 'new', 'white', 'paper', 'well', 'insight', 'manage', 'resource', 'agent', 'constantly', 'act', 'datum', 'center', 'aware', 'go', 'internet', 'base', 'design', 'able', 'provide', 'creative', 'solution', 'think', 'go', 'disruption', 'work', 'laurel', 'look', 'future', 'tool', 'technology', 'trend', 'see', 'emerge', 'enterprise', 'look', 'modernize', 'datum', 'infrastructure', 'really', 'benefit', 'data', 'intelligence', 'bharti', 'go', 'back', 'talk', 'generative', 'ai', 'give', 'example', 'customer', 'manage', 'datum', 'center', 'also', 'part', 'channel', 'see', 'constant', 'back', 'forth', 'support', 'engineering', 'support', 'ask', 'happen', 'think', 'different', 'scenario', 'able', 'collect', 'datum', 'feed', 'llm', 'talk', 'datum', 'datum', 'reside', 'several', 'place', 'reside', 'head', 'expert', 'documentation', 'support', 'ticket', 'log', 'life', 'log', 'trace', 'almost', 'impossible', 'human', 'analyze', 'datum', 'get', 'meaningful', 'insight', 'however', 'combine', 'llm', 'power', 'say', 'knowledge', 'graph', 'vector', 'database', 'tool', 'possible', 'analyze', 'datum', 'speed', 'light', 'present', 'recommendation', 'front', 'user', 'simple', 'user', 'interface', 'case', 'simple', 'natural', 'language', 'interface', 'think', 'kind', 'complete', 'paradigm', 'shift', 'many', 'source', 'need', 'constantly', 'analyze', 'full', 'automation', 'feel', 'copilot', 'become', 'essential', 'part', 'datum', 'center', 'beginning', 'help', 'automation', 'deal', 'problem', 'prevalent', 'datum', 'center', 'resource', 'management', 'optimization', 'proactive', 'problem', 'determination', 'resolution', 'go', 'future', 'see', 'manual', 'operation', 'convert', 'automate', 'operation', 'first', 'see', 'human', 'loop', 'eventually', 'see', 'trend', 'fully', 'autonomous', 'data', 'center', 'laurel', 'well', 'future', 'thank', 'much', 'join', 'today', 'business', 'lab', 'thank', 'laurel', 'byebye', 'laurel', 'bharti', 'senior', 'vice', 'president', 'product', 'marketing', 'speak', 'home', 'mit', 'mit', 'technology', 'episode', 'business', 'lab', 'host', 'director', 'insight', 'custom', 'publishing', 'division', 'mit', 'technology', 'review', 'found', 'find', 'print', 'web', 'event', 'year', 'world', 'information', 'show', 'check', 'website', 'show', 'available', 'get', 'podcast', 'enjoy', 'episode', 'hope', 'take', 'moment', 'rate', 'review', 'business', 'lab', 'production', 'mit', 'technology', 'review', 'episode', 'produce', 'studio', 'thank', 'listen', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","Data — how it’s stored and managed — has become a key competitive differentiator. As global data continues to grow exponentially, organizations face many hurdles between piling up historical data, real-time data streams from IoT sensors, and building data-driven supply chains. Senior vice president of product engineering at Hitachi Vantara, Bharti Patel sees these challenges…"
Optimizing platforms offers customers and stakeholders a better way to bank,https://www.technologyreview.com/2023/09/27/1079913/optimizing-platforms-offers-customers-and-stakeholders-a-better-way-to-bank/,2023-09-27,"In association withJPMorgan Chase & Co. When it comes to banking, whether it’s personal, business, or private, customer experience is everything. Building new technologies and platforms, employing them at scale, and optimizing workflows is especially critical for any large bank looking to meet evolving customer and internal stakeholder demands for faster and more personalized ways of doing business. Institutions like JPMorgan Chase are implementing best practices, cost efficient cloud migration, and emerging AI and machine learning (ML) tools to build better ways to bank, says Head of Managed Accounts, Client Onboarding and Client Services Technology at J. P. Morgan Private Bank, Vrinda Menon. Menon stresses that it is critical that technologists stay very focused on the business impact of the software and tools they develop. “We coach our teams that success and innovation does not come from rebuilding something that somebody has already built, but instead from leveraging it and taking the next leap with additional features upon it to create high impact business outcomes,” says Menon. At JPMorgan Chase, technologists are encouraged, where possible, to see the bigger picture and solve for the larger pattern rather than just the singular problem at hand. To reduce redundancies and automate tasks, Menon and her team focus on data and measurements that indicate where emerging technologies like AI and machine learning could enhance processes like onboarding or transaction processing at scale.  AI/ML have become commonplace across many industries with private banking being no exception, says Menon. At a base level, AI/ML can extract data from documents, classify information, analyze data smartly and detect issues and outliers across a wide range of use cases. But Menon is looking to the near future when AI/ML can help proactively predict client needs based on various signals. For example, a private banking client that has recently been married may ask their bank for a title change. Using the client’s data in context and this new request, AI/ML tools could proactively help bankers identify additional things to ask this client, such as need to change beneficiaries or the possibility to optimize taxes by considering jointly filed taxes. “You have an opportunity to be more proactive and think about it holistically so you can address their needs before they even come to you to ask for that level of engagement and detail,” says Menon. This episode of Business Lab is produced in association with JPMorgan Chase.  Laurel Ruma: From MIT Technology Review. I'm Laurel Ruma and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.Our topic today is investing in building great experiences. A number of people benefit from enterprise investment in emerging and new technologies, including customers who want better, faster, and newer ways of doing business. But internal stakeholders want the same investment in better tools and systems to build those fast and new ways of doing business. Balancing both needs is possible.Two words for you: optimizing platforms.Today we're talking with Vrinda Menon, the chief technology officer of Managed Accounts, Client Onboarding and Client Services at JPMorgan Private Bank.This podcast is produced in association with JPMorgan Chase.Welcome, Vrinda.Vrinda Menon: Thank you so much, Laurel. I'm looking forward to this discussion.Laurel: Great. So, let's start with how often people think of JPMorgan Chase. They likely associate the company with personal banking, ATMs and credit cards, but could you describe what services the private bank provides and how operations and client services have evolved and transformed since you began your role at JPMorgan Chase? Vrinda: Sure. JPMorgan Chase indeed does far more than personal banking, credit cards and ATMs. The private bank of JPMorgan Chase is often referred to as the crown jewel of our franchise. We service our high net worth clients and ultra-high net worth clients across the globe. We provide them services like investment management, trust and estate planning, banking services, brokerage services, customized lending, etc., just to name a few. And in terms of what has transformed in the recent years since I joined, I would say that we've become far more tech savvy as an organization, and this is thanks and no small measure to new leadership as well in operations and client services. I think three things have changed very dramatically since I've joined. The first is culture. In my first few months, I spent a week doing the job of an operations analyst. And in doing that I started to understand firsthand the painful manual work that people were subject to and feeling like they did not have the permission to have things changed for them. But working off that and actually connecting with a lot more people at the ground who are doing these types of activities, we worked with them to make those changes and make them see light at the end of the tunnel. And then suddenly the demand for more change and demand for more automation started building as a groundswell energy with support from our partners in operations and services. Now, routine, repetitive, mundane, mind-numbing work is not an option at the table. It's become a thing of the past. And secondly, what we've done also is we've grown an army of citizen developers who really have access to tools and technologies where they can do quick automation without having to depend on broader programs and broader pieces of technology. We've also done something super interesting, which is, over the past three years we've taken every new analyst in the private bank and trained them on Python. And so, they've started to see the benefits of doing things themselves. So, culture change I think has been one of the biggest things that we've achieved in the past few years since I joined. Second, we built a whole set of capabilities, we call them common capabilities. Things like how do you configure new workflows? How do you make decisions using spreadsheets and decision models versus coding it into systems? So,  you can configure it, you can modify it, and you can do things more effectively. And then tools like checklists, which can be again put into systems and automated in a few minutes, in many cases. Today, we have millions of tasks and millions of decisions being executed through these capabilities, which has suddenly game-changed our ability to provide automation at scale. And last but not least, AI and machine learning, it now plays an important role in the underpinnings of everything that we do in operations and client services. For example, we do a lot of process analytics. We do load balancing. So, when a client calls, which agent or which group of people do we direct that client call to so that they can actually service the client most effectively. In the space of payments, we do a lot with machine learning. Fraud detection is another, and I will say that I'm so glad we've had the time to invest and think through all of these foundational capabilities. So, we are now poised and ready to take on the next big leap of changes that are right now at our fingertips, especially in the evolving world of AI and machine learning and of course the public cloud.Laurel: Excellent. Yeah, you've certainly outlined the diversity of the firm's offerings. So, when building new technologies and platforms, what are some of the working methodologies and practices that you employ to build at scale and then optimize those workflows? Vrinda: Yeah, as I said before, the private bank has a lot of offerings, but then amplify that with all the other offerings that JPMorgan Chase, the franchise has, a commercial bank, a corporate and investment bank, a consumer and community bank, and many of our clients cross all of these lines of business. It brings a lot of benefits, but it also has complexities. And one of the things that I obsess personally over is how do we simplify things, not add to the complexity? Second is a mantra of reuse. Don't reinvent because it's easy for technologists to look at a piece of software and say, ""That's great, but I can build something better."" Instead, the three things that I ask people to focus on and our organization collectively with our partners focus on is first of all, look at the business outcome. We coach our teams that success and innovation does not come from rebuilding something that somebody has already built, but instead from leveraging it and taking the next leap with additional features upon it to create high impact business outcomes. So, focusing on outcome number one. Second, if you are given a problem, try and look at it from a bigger picture to see whether you can solve the pattern instead of that specific problem. So, I'll give you an example. We built a chatbot called Casey. It's one of the most loved products in our private bank right now. And Casey doesn't do anything really complex, but what it does is solves a very common pattern, which is ask a few simple questions, get the inputs, join this with data services and join this with execution services and complete the task. And we have hundreds of thousands of tasks that Casey performs every single day. And one of them, especially a very simple functionality, the client wants a bank reference letter. Casey is called upon to do that thousands of times a month. And what used to take three or four hours to produce now takes like a few seconds. So, it suddenly changes the outcome, changes productivity, and changes the happiness of people who are doing things that you know they themselves felt was mundane. So, solving the pattern, again, important. And last but not least, focusing on data is the other thing that's helped us. Nothing can be improved if you don't measure it. So, to give you an example of processes, the first thing we did was pick the most complex processes and mapped them out. We understood each step in the process, we understood the purpose of each step in the process, the time taken in each step, we started to question, do you really need this approval from this person? We observed that for the past six months, not one single thing has been rejected. So, is that even a meaningful approval to begin with? Questioning if that process could be enhanced with AI, could AI automatically say, ""Yes, please approve,"" or ""There's a risk in this do not approve,"" or ""It's okay, it needs a human review."" And then making those changes in our systems and flows and then obsessively measuring the impact of those changes. All of these have given us a lot of benefits. And I would say we've made significant progress just with these three principles of focus on outcome, focus on solving the pattern and focus on data and measurements in areas like client onboarding, in areas like maintaining client data, et cetera. So, this has been very helpful for us because in a bank like ours, scale is super important.Laurel: Yeah, that's a really great explanation. So, when new challenges do come along, like moving to the public cloud, how do you balance the opportunities of that scale, but also computing power and resources within the cost of the actual investment? How do you ensure that the shifts to the cloud are actually both financially and operationally efficient? Vrinda: Great question. So obviously every technologist in the world is super excited with the advent of the public cloud. It gives us the powers of agility, economies of scale. We at JPMorgan Chase are able to leverage world class evolving capabilities at our fingertips. We have the ability also to partner with talented technologies at the cloud providers and many service providers that we work with that have advanced solutions that are available first on the public cloud. We are eager to get our hands on those. But with that comes a lot of responsibility because as a bank, we have to worry about security, client data, privacy, resilience, how are we going to operate in a multi-cloud environment because some data has to remain on-prem in our private cloud. So, there's a lot of complexity, and we have engineers across the board who think a lot about this, and their day and night jobs are to try and figure this out. As we think about moving to the public cloud in my area, I personally spend time thinking in depth about how we could build architectures that are financially efficient. And the reason I bring that up is because traditionally as we think about data centers where our hardware and software has been hosted, developers and architects haven't had to worry about costs because you start with sizing the infrastructure, you order that infrastructure, it's captive, it remains in the data center, and you can expand it, but it's a one-time cost each time that you upgrade. With the cloud, that situation changes dramatically. It's both an opportunity but also a risk. So, a financial lens then becomes super important right at the outset. Let me give you a couple of examples of what I mean. Developers in the public cloud have a lot of power, and with that power comes responsibility. So, I'm a developer and my application is not working right now because there's some issue. I have the ability to actually spin up additional processes. I have the ability to spin up additional environments, all of which attract costs, and if I don't control and manage that, the cost could quickly pile up. Data storage, again, we had fixed storage, we could expand it periodically in the data centers, but in the public cloud, you have choices. You can say data that's going to be slowly accessed versus data that's going to be accessed frequently to be stored in different types of storage with different costs as a result. Now think about something like a financial ledger where you have retention requirements of let's say 20 years. The cost could quickly pile up if you store it in the wrong type of storage. So, there's an opportunity to optimize cost there, and if you ignore it and you've not kept an eye on it, you could actually have costs that are just not required. To do this right, we have to ask developers, architects, and our engineers to not just think about the best performance, the most optimal resilience, but also think about cost as a fundamental aspect of how we look at architectures. So, this for me is a huge area of focus, starting with awareness for our people, training our people, thinking about architecture patterns, solution patterns, tooling, measurements, so that we completely stay on top of this and become more effective and more efficient in how we get to the public cloud. While the journey is exciting, I want to make sure that as we land there, we land safely and optimally from a cost standpoint. Laurel: And especially in your position, thinking about how technology will affect the firm years and the future is critical. Therefore, as emerging technologies like AI and machine learning become more commonplace across industries, could you offer an example of how you're using them in the areas that you cover?Vrinda: Yeah, certainly. And we use AI/ML at many levels of complexity. So let me start with the base case. AI/ML, especially in operations and client services, starts with can I get data from documents? Can I OCR those documents, which is optical character recognition? Can I get information out of it, can I classify it? Can I perform analytics on it? So that's the base case. On top of that, as you look at data, for example, payments data or data of transactions, and let's say human beings are scanning them for issues or outliers, outlier detection techniques with AI/ML, they are also table stakes now, and many of our systems do that. But as you move on to the next level of prediction, what we've been able to do is start to build up models where say the client is calling. The client has all these types of cases in progress right now. What could they be calling about in addition to this? The client expressed sentiment about something that they were not happy with two weeks ago. Is it likely that they're calling about this? Can I have that information at the fingertips of the client service agent so they can look at it and respond as soon as the client asks for something? And think about the next stage of evolution, which is, the client came to us and said, ""Change my title because I just got married."" Typically, in a transactional kind of activity, you would respond to the client and fix the title from let's say, Ms. to Mrs. if that's what they asked you to do. But imagine if when they came to do that, we said to them, here's 10 other things that you should possibly think of now that you said you've got married. Congratulations. Do you want to address your beneficiaries? Do you want to change something in tax planning? Do you want to change the type of tax calculations that you do because you want to optimize now that you're married, and you and your spouse could be filing jointly? Again, not that the client would choose to change those things, but you have an opportunity to be more proactive and think about it holistically so you can address their needs before they even come to you asking for that level of engagement and detail. We also exploit a lot of AI/ML capabilities and in client onboarding to get better data to start to predict what data is right and start to predict risk. And our next leap, I believe strongly, and I'm super excited about this area of large language models, which I think are going to offer us exponential possibilities, not just in JPMorgan Chase, but as you can see in the world right now with technologies like ChatGPT, OpenAI's technologies, as well as any of the other publicly available large language models that are being developed every single day. Laurel: Well, it's clear that AI offers great opportunities for optimizing platforms and transformations. Could you describe the process of how JPMorgan Chase decided to create dedicated teams for AI and machine learning, and how did you build out those teams? Vrinda: Yeah, certainly. At JPMorgan Chase, we've been cultivating the mindset for some years now to think AI-first while hiring people. And we also leverage the best talent in the industry, and we've hired a lot of people in our research divisions as well to work on AI/ML. We've got thousands, several thousand technologists focused on AI. For me personally, in 2020, during the first months of the pandemic, I decided that I needed to see more AI/ML activity across my areas. So, I did what I called the “Summer of AI/ML,” and this was a fully immersive program that ran over 12 weeks with training for our people, and it was not full-time. So, they would dial in for a couple of hours, get trained on an AI/ML concept and some techniques, and then they would continue that and practice that for the week. Then we had ideation sessions with our users for a couple of weeks and then a hackathon and some brilliant ideas came out of it. But when I stepped back and looked at this whole thing and the results of it, a few months later, I realized that many of the ideas had not reached the final destination into production. And in thinking a little more deeply about that, I understood that we had a problem. The problem was as follows, while AI is a great thing and everybody appreciates it, until AI becomes ingrained in everybody's brain as the first thing to think about, there's always going to be a healthy tension between choosing the next best feature on a product, which is very deterministic. If you say, add this button here or add these features using conventional technologies like Java versus game-changing the product using AI, which is a little bit more of a risk, the results are not always predictable, and it requires experimentation and R&D. And so, when you have a choice of incremental changes that are deterministic and changes that are more probabilistic, people tend to take the most certain answer. And so, I decided that I needed to build out a focused, dedicated team of data scientists who were just going to obsess about solving problems in the space of data science and embed them across the products that we were building. And now the results are starting to show for themselves because the work they've done is phenomenal and the demand on them is growing every single day to the point where I've grown the team and the value that they're providing is also measured and visible to the broader organization.Laurel: So, in JPMorgan Chase's client services, customer experience is clearly a driving force. How do you ensure that your teams are providing clients, especially those high-net-worth private clients that have high expectations of service with services that then meet their banking and account management needs? Vrinda: So, we obsess over customer experience starting from the CEO down to every single employee. I have three tenets for my team. Number one is client experience, the second is user experience, and third is engineering excellence. And they know that a lot of us are measured by how well we service our clients. So, in the private bank specifically, in addition to reviewing our core capabilities like our case management system, our voice recognition systems, our fraud capture systems, all of that, we continuously analyze data received from client surveys, data received through every single interaction that we have with our client across all channels. So, whether it be a voice channel, whether it be emails, whether it be things that the client types in our websites, the places that they access, and our models just do not look at sentiment, they also look at client experience. And as they look at experience, the things that we are trying to understand are, first of all, how's the client feeling in this interaction? But more important is client one and client two and client three feeling the same thing about a particular aspect of our process, and do we need to change that process as a result, or is there more training that needs to be provided to our agents because we are not able to fully satisfy this category of requests? And by doing that continuously and analyzing it, and back to the point that I made earlier, by measuring it constantly, we are able to say, first of all, how was the experience to begin with? How is the experience now and after making these changes on these training programs or these fixes in our systems, how is that experience showing? And some of the other things we are able to do are look at experiences over a period of time. So, for example, the client came to us last year and their experience based on the measurements that we did was at a certain level, they continue to interact with us over a period of months. Has it gone up? Has it gone down? How is that needle trending? How do we take that to superb? And we've been able to figure out these in ways that we've been able to prevent complaints, for example, and get to a point where things are escalated to the right people in the organization, especially in the servicing space where we are able to triage and manage these things more effectively because we are a high- touch client business, and we need to make sure that our clients are extremely happy with us. Laurel: Oh yeah, absolutely. And sort of like another phase or idea when we're thinking about customer experience and customer services, building a workforce that can respond to it. So here we're going to talk a bit about how we promote diversity, which has been a tenet of your career, and you currently sit on the board of the Transition Network, which is a nonprofit that empowers a diverse network of women throughout career transitions. So, at JPMorgan Chase, how do you grow talent and improve representation across the company? And then how does that help build better customer experience? Vrinda: Sure, that's a great question. I certainly am very passionate about diversity, and during the past 15 years of my career, I've spent a lot of time supporting diversity. In my prior firm, I was co-head of the Asian Professional Network. Then subsequently for the past three years, I've been a board member at the Transition Network, which is all about women in transition. Meaning as they grow out of their careers into retirement and into other stages of life, how do we help them transition? And then here at JPMorgan Chase, I'm the sponsor for what is called the Take It Forward initiative, which is an initiative that supports 15,000 women technologists. JPMorgan Chase, as you know, does a broad range of activities in the area of diversity across all kinds of business resource groups, and we invest a lot of time and energy. But specifically, for the Take It Forward initiative that I sponsor, it plays a key role in helping these 15,000 women technologists continuously enhance their leadership skills, grow their technical skills, build their confidence, develop networks, learn from senior sponsors and mentors, grow their careers, all of which makes their work experience very enriching. When I hear things like I'm motivated, I get new energy interacting with these senior women, I trust my personal power more. I'm confident to negotiate with my manager for a better role; I feel confident that I can discuss my compensation. It makes me really happy, and especially when they say I stay in JPMorgan Chase because of Take It Forward, it brings tears to my eyes. It's really one of the most amazing volunteer driven initiatives in this organization. And a lot of people pour in passion, energy, time to make it succeed. And this initiative has won many awards as well externally. I strongly believe all of these efforts are critical as it changes when people's experiences change and when they're happy, what they do becomes that much more effective. And it changes how we work internally, how we present ourselves externally and game changes our business outcomes. I've seen that in problem solving meetings. When you evaluate risk and you bring in people from diverse backgrounds, some are more risk averse, some are more risk taking, and you suddenly see that dynamic play out and the outcome becomes much different from what it would've been if you didn't have all those people in the mix. So overall, I strongly believe in this, and I've seen it play out in every single firm that I've ever worked at. So that's my take on diversity and how it helps us. Laurel: Well, it certainly is important work, especially as it ties so tightly with the firm's own ethos. So Vrinda, looking forward, how do you envision the future of private banking and client management? As we see emerging technologies become more prevalent and enterprises start to shift their infrastructure to the public cloud? Vrinda: As I mentioned earlier, I see the next set of emerging technologies taking the world on a super exciting ride, and I think it's going to be as transformational as the advent of the world wide web. Just take the example of large language models. The areas that are most likely to be first disrupted will be any work that involves content creation because that is table stakes for a large language model. As I expand that to my work, the rest of my work, client services and operations and many other areas that require repetitive work and large-scale interpretation and synthesis of information, that's again, table stakes for large language models. Expand that now to the next evolution, which is agents that are now the emerging technology with large language models. When agents are provided with a suite of tools and they can use reasoning like humans to decide which tool to execute based on input, that'll game change the whole thing I was talking about earlier on workflow and task execution and operationally intense activities in the organization. When I look at myself as a software developer in the areas of code generation, code testing, code correction, test data generation, just to name a few, all of those are going to be game changed. So not just the work that our users do in the private bank, but the work that we do as technologists in the private bank. A lot of that is going to game change dramatically. And then you add on the next level, which is problem solving. Large language models are continuously being trained on all subjects ever known to humans. And that for me is the most fascinating part of this. It's like hundreds of thousands of brains that are working together on a diverse set of subjects. So, imagine a model that's been trained on a domain like medicine or aerospace or defense, and then trying to bring all of that brainpower together to solve a problem in finance. That truly to me is the ultimate gold standard of problem solving. We talked about diverse people in the room coming with different experiences but imagine models that have been trained. You suddenly have breadth, depth, range of diverse knowledge that could never have been contemplated at that scale. And in order to do all of this, obviously one of the key underpinnings is the public cloud and being able to spin up compute as quickly as possible to do complex calculations and then spin it down when you don't need it, which is where the public cloud becomes super important. So, all I can say in conclusion is I think this is an amazing time to be in technology, and I just cannot wait to see how we further step up our game in the coming months and years, and things are moving almost at the speed of light now. Every single day, new papers get published and new ideas are coming out building on top of some of the exponential technologies that we are seeing in the world today. Laurel: Oh, that's fantastic. Vrinda, thank you so much for being on the Business Lab today.Vrinda: Thank you so much, Laurel. It's my pleasure. I really enjoyed speaking with you and thank you for your thoughtful questions. They were super interesting. Laurel: That was Vrinda Menon, the chief technology officer of Managed Accounts, Client Onboarding and Client Services at J.P. Morgan Private Bank, who I spoke with from Cambridge, Massachusetts, the home of MIT and MIT Technology Review overlooking the Charles River.That's it for this episode of Business Lab. I'm your host, Laurel Ruma. I'm the director of Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology, and you can find us in print, on the web and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com.This show is available wherever you get your podcasts. If you enjoyed this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT Technology Review. This episode was produced by Giro Studios. Thanks for listening. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. This podcast is for informational purposes only and it is not intended as legal, tax, financial, investment, accounting or regulatory advice. Opinions expressed herein are the personal views of the individual(s) and do not represent the views of JPMorgan Chase & Co. The accuracy of any statements, linked resources, reported findings or quotations are not the responsibility of JPMorgan Chase & Co.","In association withJPMorgan Chase & Co . When it comes to banking , whether it ’ s personal , business , or private , customer experience is everything . Building new technologies and platforms , employing them at scale , and optimizing workflows is especially critical for any large bank looking to meet evolving customer and internal stakeholder demands for faster and more personalized ways of doing business . Institutions like JPMorgan Chase are implementing best practices , cost efficient cloud migration , and emerging AI and machine learning ( ML ) tools to build better ways to bank , says Head of Managed Accounts , Client Onboarding and Client Services Technology at J. P. Morgan Private Bank , Vrinda Menon . Menon stresses that it is critical that technologists stay very focused on the business impact of the software and tools they develop . “ We coach our teams that success and innovation does not come from rebuilding something that somebody has already built , but instead from leveraging it and taking the next leap with additional features upon it to create high impact business outcomes , ” says Menon . At JPMorgan Chase , technologists are encouraged , where possible , to see the bigger picture and solve for the larger pattern rather than just the singular problem at hand . To reduce redundancies and automate tasks , Menon and her team focus on data and measurements that indicate where emerging technologies like AI and machine learning could enhance processes like onboarding or transaction processing at scale . AI/ML have become commonplace across many industries with private banking being no exception , says Menon . At a base level , AI/ML can extract data from documents , classify information , analyze data smartly and detect issues and outliers across a wide range of use cases . But Menon is looking to the near future when AI/ML can help proactively predict client needs based on various signals . For example , a private banking client that has recently been married may ask their bank for a title change . Using the client ’ s data in context and this new request , AI/ML tools could proactively help bankers identify additional things to ask this client , such as need to change beneficiaries or the possibility to optimize taxes by considering jointly filed taxes . “ You have an opportunity to be more proactive and think about it holistically so you can address their needs before they even come to you to ask for that level of engagement and detail , ” says Menon . This episode of Business Lab is produced in association with JPMorgan Chase . Laurel Ruma : From MIT Technology Review . I 'm Laurel Ruma and this is Business Lab , the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.Our topic today is investing in building great experiences . A number of people benefit from enterprise investment in emerging and new technologies , including customers who want better , faster , and newer ways of doing business . But internal stakeholders want the same investment in better tools and systems to build those fast and new ways of doing business . Balancing both needs is possible.Two words for you : optimizing platforms.Today we 're talking with Vrinda Menon , the chief technology officer of Managed Accounts , Client Onboarding and Client Services at JPMorgan Private Bank.This podcast is produced in association with JPMorgan Chase.Welcome , Vrinda.Vrinda Menon : Thank you so much , Laurel . I 'm looking forward to this discussion.Laurel : Great . So , let 's start with how often people think of JPMorgan Chase . They likely associate the company with personal banking , ATMs and credit cards , but could you describe what services the private bank provides and how operations and client services have evolved and transformed since you began your role at JPMorgan Chase ? Vrinda : Sure . JPMorgan Chase indeed does far more than personal banking , credit cards and ATMs . The private bank of JPMorgan Chase is often referred to as the crown jewel of our franchise . We service our high net worth clients and ultra-high net worth clients across the globe . We provide them services like investment management , trust and estate planning , banking services , brokerage services , customized lending , etc. , just to name a few . And in terms of what has transformed in the recent years since I joined , I would say that we 've become far more tech savvy as an organization , and this is thanks and no small measure to new leadership as well in operations and client services . I think three things have changed very dramatically since I 've joined . The first is culture . In my first few months , I spent a week doing the job of an operations analyst . And in doing that I started to understand firsthand the painful manual work that people were subject to and feeling like they did not have the permission to have things changed for them . But working off that and actually connecting with a lot more people at the ground who are doing these types of activities , we worked with them to make those changes and make them see light at the end of the tunnel . And then suddenly the demand for more change and demand for more automation started building as a groundswell energy with support from our partners in operations and services . Now , routine , repetitive , mundane , mind-numbing work is not an option at the table . It 's become a thing of the past . And secondly , what we 've done also is we 've grown an army of citizen developers who really have access to tools and technologies where they can do quick automation without having to depend on broader programs and broader pieces of technology . We 've also done something super interesting , which is , over the past three years we 've taken every new analyst in the private bank and trained them on Python . And so , they 've started to see the benefits of doing things themselves . So , culture change I think has been one of the biggest things that we 've achieved in the past few years since I joined . Second , we built a whole set of capabilities , we call them common capabilities . Things like how do you configure new workflows ? How do you make decisions using spreadsheets and decision models versus coding it into systems ? So , you can configure it , you can modify it , and you can do things more effectively . And then tools like checklists , which can be again put into systems and automated in a few minutes , in many cases . Today , we have millions of tasks and millions of decisions being executed through these capabilities , which has suddenly game-changed our ability to provide automation at scale . And last but not least , AI and machine learning , it now plays an important role in the underpinnings of everything that we do in operations and client services . For example , we do a lot of process analytics . We do load balancing . So , when a client calls , which agent or which group of people do we direct that client call to so that they can actually service the client most effectively . In the space of payments , we do a lot with machine learning . Fraud detection is another , and I will say that I 'm so glad we 've had the time to invest and think through all of these foundational capabilities . So , we are now poised and ready to take on the next big leap of changes that are right now at our fingertips , especially in the evolving world of AI and machine learning and of course the public cloud.Laurel : Excellent . Yeah , you 've certainly outlined the diversity of the firm 's offerings . So , when building new technologies and platforms , what are some of the working methodologies and practices that you employ to build at scale and then optimize those workflows ? Vrinda : Yeah , as I said before , the private bank has a lot of offerings , but then amplify that with all the other offerings that JPMorgan Chase , the franchise has , a commercial bank , a corporate and investment bank , a consumer and community bank , and many of our clients cross all of these lines of business . It brings a lot of benefits , but it also has complexities . And one of the things that I obsess personally over is how do we simplify things , not add to the complexity ? Second is a mantra of reuse . Do n't reinvent because it 's easy for technologists to look at a piece of software and say , `` That 's great , but I can build something better . '' Instead , the three things that I ask people to focus on and our organization collectively with our partners focus on is first of all , look at the business outcome . We coach our teams that success and innovation does not come from rebuilding something that somebody has already built , but instead from leveraging it and taking the next leap with additional features upon it to create high impact business outcomes . So , focusing on outcome number one . Second , if you are given a problem , try and look at it from a bigger picture to see whether you can solve the pattern instead of that specific problem . So , I 'll give you an example . We built a chatbot called Casey . It 's one of the most loved products in our private bank right now . And Casey does n't do anything really complex , but what it does is solves a very common pattern , which is ask a few simple questions , get the inputs , join this with data services and join this with execution services and complete the task . And we have hundreds of thousands of tasks that Casey performs every single day . And one of them , especially a very simple functionality , the client wants a bank reference letter . Casey is called upon to do that thousands of times a month . And what used to take three or four hours to produce now takes like a few seconds . So , it suddenly changes the outcome , changes productivity , and changes the happiness of people who are doing things that you know they themselves felt was mundane . So , solving the pattern , again , important . And last but not least , focusing on data is the other thing that 's helped us . Nothing can be improved if you do n't measure it . So , to give you an example of processes , the first thing we did was pick the most complex processes and mapped them out . We understood each step in the process , we understood the purpose of each step in the process , the time taken in each step , we started to question , do you really need this approval from this person ? We observed that for the past six months , not one single thing has been rejected . So , is that even a meaningful approval to begin with ? Questioning if that process could be enhanced with AI , could AI automatically say , `` Yes , please approve , '' or `` There 's a risk in this do not approve , '' or `` It 's okay , it needs a human review . '' And then making those changes in our systems and flows and then obsessively measuring the impact of those changes . All of these have given us a lot of benefits . And I would say we 've made significant progress just with these three principles of focus on outcome , focus on solving the pattern and focus on data and measurements in areas like client onboarding , in areas like maintaining client data , et cetera . So , this has been very helpful for us because in a bank like ours , scale is super important.Laurel : Yeah , that 's a really great explanation . So , when new challenges do come along , like moving to the public cloud , how do you balance the opportunities of that scale , but also computing power and resources within the cost of the actual investment ? How do you ensure that the shifts to the cloud are actually both financially and operationally efficient ? Vrinda : Great question . So obviously every technologist in the world is super excited with the advent of the public cloud . It gives us the powers of agility , economies of scale . We at JPMorgan Chase are able to leverage world class evolving capabilities at our fingertips . We have the ability also to partner with talented technologies at the cloud providers and many service providers that we work with that have advanced solutions that are available first on the public cloud . We are eager to get our hands on those . But with that comes a lot of responsibility because as a bank , we have to worry about security , client data , privacy , resilience , how are we going to operate in a multi-cloud environment because some data has to remain on-prem in our private cloud . So , there 's a lot of complexity , and we have engineers across the board who think a lot about this , and their day and night jobs are to try and figure this out . As we think about moving to the public cloud in my area , I personally spend time thinking in depth about how we could build architectures that are financially efficient . And the reason I bring that up is because traditionally as we think about data centers where our hardware and software has been hosted , developers and architects have n't had to worry about costs because you start with sizing the infrastructure , you order that infrastructure , it 's captive , it remains in the data center , and you can expand it , but it 's a one-time cost each time that you upgrade . With the cloud , that situation changes dramatically . It 's both an opportunity but also a risk . So , a financial lens then becomes super important right at the outset . Let me give you a couple of examples of what I mean . Developers in the public cloud have a lot of power , and with that power comes responsibility . So , I 'm a developer and my application is not working right now because there 's some issue . I have the ability to actually spin up additional processes . I have the ability to spin up additional environments , all of which attract costs , and if I do n't control and manage that , the cost could quickly pile up . Data storage , again , we had fixed storage , we could expand it periodically in the data centers , but in the public cloud , you have choices . You can say data that 's going to be slowly accessed versus data that 's going to be accessed frequently to be stored in different types of storage with different costs as a result . Now think about something like a financial ledger where you have retention requirements of let 's say 20 years . The cost could quickly pile up if you store it in the wrong type of storage . So , there 's an opportunity to optimize cost there , and if you ignore it and you 've not kept an eye on it , you could actually have costs that are just not required . To do this right , we have to ask developers , architects , and our engineers to not just think about the best performance , the most optimal resilience , but also think about cost as a fundamental aspect of how we look at architectures . So , this for me is a huge area of focus , starting with awareness for our people , training our people , thinking about architecture patterns , solution patterns , tooling , measurements , so that we completely stay on top of this and become more effective and more efficient in how we get to the public cloud . While the journey is exciting , I want to make sure that as we land there , we land safely and optimally from a cost standpoint . Laurel : And especially in your position , thinking about how technology will affect the firm years and the future is critical . Therefore , as emerging technologies like AI and machine learning become more commonplace across industries , could you offer an example of how you 're using them in the areas that you cover ? Vrinda : Yeah , certainly . And we use AI/ML at many levels of complexity . So let me start with the base case . AI/ML , especially in operations and client services , starts with can I get data from documents ? Can I OCR those documents , which is optical character recognition ? Can I get information out of it , can I classify it ? Can I perform analytics on it ? So that 's the base case . On top of that , as you look at data , for example , payments data or data of transactions , and let 's say human beings are scanning them for issues or outliers , outlier detection techniques with AI/ML , they are also table stakes now , and many of our systems do that . But as you move on to the next level of prediction , what we 've been able to do is start to build up models where say the client is calling . The client has all these types of cases in progress right now . What could they be calling about in addition to this ? The client expressed sentiment about something that they were not happy with two weeks ago . Is it likely that they 're calling about this ? Can I have that information at the fingertips of the client service agent so they can look at it and respond as soon as the client asks for something ? And think about the next stage of evolution , which is , the client came to us and said , `` Change my title because I just got married . '' Typically , in a transactional kind of activity , you would respond to the client and fix the title from let 's say , Ms. to Mrs. if that 's what they asked you to do . But imagine if when they came to do that , we said to them , here 's 10 other things that you should possibly think of now that you said you 've got married . Congratulations . Do you want to address your beneficiaries ? Do you want to change something in tax planning ? Do you want to change the type of tax calculations that you do because you want to optimize now that you 're married , and you and your spouse could be filing jointly ? Again , not that the client would choose to change those things , but you have an opportunity to be more proactive and think about it holistically so you can address their needs before they even come to you asking for that level of engagement and detail . We also exploit a lot of AI/ML capabilities and in client onboarding to get better data to start to predict what data is right and start to predict risk . And our next leap , I believe strongly , and I 'm super excited about this area of large language models , which I think are going to offer us exponential possibilities , not just in JPMorgan Chase , but as you can see in the world right now with technologies like ChatGPT , OpenAI 's technologies , as well as any of the other publicly available large language models that are being developed every single day . Laurel : Well , it 's clear that AI offers great opportunities for optimizing platforms and transformations . Could you describe the process of how JPMorgan Chase decided to create dedicated teams for AI and machine learning , and how did you build out those teams ? Vrinda : Yeah , certainly . At JPMorgan Chase , we 've been cultivating the mindset for some years now to think AI-first while hiring people . And we also leverage the best talent in the industry , and we 've hired a lot of people in our research divisions as well to work on AI/ML . We 've got thousands , several thousand technologists focused on AI . For me personally , in 2020 , during the first months of the pandemic , I decided that I needed to see more AI/ML activity across my areas . So , I did what I called the “ Summer of AI/ML , ” and this was a fully immersive program that ran over 12 weeks with training for our people , and it was not full-time . So , they would dial in for a couple of hours , get trained on an AI/ML concept and some techniques , and then they would continue that and practice that for the week . Then we had ideation sessions with our users for a couple of weeks and then a hackathon and some brilliant ideas came out of it . But when I stepped back and looked at this whole thing and the results of it , a few months later , I realized that many of the ideas had not reached the final destination into production . And in thinking a little more deeply about that , I understood that we had a problem . The problem was as follows , while AI is a great thing and everybody appreciates it , until AI becomes ingrained in everybody 's brain as the first thing to think about , there 's always going to be a healthy tension between choosing the next best feature on a product , which is very deterministic . If you say , add this button here or add these features using conventional technologies like Java versus game-changing the product using AI , which is a little bit more of a risk , the results are not always predictable , and it requires experimentation and R & D . And so , when you have a choice of incremental changes that are deterministic and changes that are more probabilistic , people tend to take the most certain answer . And so , I decided that I needed to build out a focused , dedicated team of data scientists who were just going to obsess about solving problems in the space of data science and embed them across the products that we were building . And now the results are starting to show for themselves because the work they 've done is phenomenal and the demand on them is growing every single day to the point where I 've grown the team and the value that they 're providing is also measured and visible to the broader organization.Laurel : So , in JPMorgan Chase 's client services , customer experience is clearly a driving force . How do you ensure that your teams are providing clients , especially those high-net-worth private clients that have high expectations of service with services that then meet their banking and account management needs ? Vrinda : So , we obsess over customer experience starting from the CEO down to every single employee . I have three tenets for my team . Number one is client experience , the second is user experience , and third is engineering excellence . And they know that a lot of us are measured by how well we service our clients . So , in the private bank specifically , in addition to reviewing our core capabilities like our case management system , our voice recognition systems , our fraud capture systems , all of that , we continuously analyze data received from client surveys , data received through every single interaction that we have with our client across all channels . So , whether it be a voice channel , whether it be emails , whether it be things that the client types in our websites , the places that they access , and our models just do not look at sentiment , they also look at client experience . And as they look at experience , the things that we are trying to understand are , first of all , how 's the client feeling in this interaction ? But more important is client one and client two and client three feeling the same thing about a particular aspect of our process , and do we need to change that process as a result , or is there more training that needs to be provided to our agents because we are not able to fully satisfy this category of requests ? And by doing that continuously and analyzing it , and back to the point that I made earlier , by measuring it constantly , we are able to say , first of all , how was the experience to begin with ? How is the experience now and after making these changes on these training programs or these fixes in our systems , how is that experience showing ? And some of the other things we are able to do are look at experiences over a period of time . So , for example , the client came to us last year and their experience based on the measurements that we did was at a certain level , they continue to interact with us over a period of months . Has it gone up ? Has it gone down ? How is that needle trending ? How do we take that to superb ? And we 've been able to figure out these in ways that we 've been able to prevent complaints , for example , and get to a point where things are escalated to the right people in the organization , especially in the servicing space where we are able to triage and manage these things more effectively because we are a high- touch client business , and we need to make sure that our clients are extremely happy with us . Laurel : Oh yeah , absolutely . And sort of like another phase or idea when we 're thinking about customer experience and customer services , building a workforce that can respond to it . So here we 're going to talk a bit about how we promote diversity , which has been a tenet of your career , and you currently sit on the board of the Transition Network , which is a nonprofit that empowers a diverse network of women throughout career transitions . So , at JPMorgan Chase , how do you grow talent and improve representation across the company ? And then how does that help build better customer experience ? Vrinda : Sure , that 's a great question . I certainly am very passionate about diversity , and during the past 15 years of my career , I 've spent a lot of time supporting diversity . In my prior firm , I was co-head of the Asian Professional Network . Then subsequently for the past three years , I 've been a board member at the Transition Network , which is all about women in transition . Meaning as they grow out of their careers into retirement and into other stages of life , how do we help them transition ? And then here at JPMorgan Chase , I 'm the sponsor for what is called the Take It Forward initiative , which is an initiative that supports 15,000 women technologists . JPMorgan Chase , as you know , does a broad range of activities in the area of diversity across all kinds of business resource groups , and we invest a lot of time and energy . But specifically , for the Take It Forward initiative that I sponsor , it plays a key role in helping these 15,000 women technologists continuously enhance their leadership skills , grow their technical skills , build their confidence , develop networks , learn from senior sponsors and mentors , grow their careers , all of which makes their work experience very enriching . When I hear things like I 'm motivated , I get new energy interacting with these senior women , I trust my personal power more . I 'm confident to negotiate with my manager for a better role ; I feel confident that I can discuss my compensation . It makes me really happy , and especially when they say I stay in JPMorgan Chase because of Take It Forward , it brings tears to my eyes . It 's really one of the most amazing volunteer driven initiatives in this organization . And a lot of people pour in passion , energy , time to make it succeed . And this initiative has won many awards as well externally . I strongly believe all of these efforts are critical as it changes when people 's experiences change and when they 're happy , what they do becomes that much more effective . And it changes how we work internally , how we present ourselves externally and game changes our business outcomes . I 've seen that in problem solving meetings . When you evaluate risk and you bring in people from diverse backgrounds , some are more risk averse , some are more risk taking , and you suddenly see that dynamic play out and the outcome becomes much different from what it would 've been if you did n't have all those people in the mix . So overall , I strongly believe in this , and I 've seen it play out in every single firm that I 've ever worked at . So that 's my take on diversity and how it helps us . Laurel : Well , it certainly is important work , especially as it ties so tightly with the firm 's own ethos . So Vrinda , looking forward , how do you envision the future of private banking and client management ? As we see emerging technologies become more prevalent and enterprises start to shift their infrastructure to the public cloud ? Vrinda : As I mentioned earlier , I see the next set of emerging technologies taking the world on a super exciting ride , and I think it 's going to be as transformational as the advent of the world wide web . Just take the example of large language models . The areas that are most likely to be first disrupted will be any work that involves content creation because that is table stakes for a large language model . As I expand that to my work , the rest of my work , client services and operations and many other areas that require repetitive work and large-scale interpretation and synthesis of information , that 's again , table stakes for large language models . Expand that now to the next evolution , which is agents that are now the emerging technology with large language models . When agents are provided with a suite of tools and they can use reasoning like humans to decide which tool to execute based on input , that 'll game change the whole thing I was talking about earlier on workflow and task execution and operationally intense activities in the organization . When I look at myself as a software developer in the areas of code generation , code testing , code correction , test data generation , just to name a few , all of those are going to be game changed . So not just the work that our users do in the private bank , but the work that we do as technologists in the private bank . A lot of that is going to game change dramatically . And then you add on the next level , which is problem solving . Large language models are continuously being trained on all subjects ever known to humans . And that for me is the most fascinating part of this . It 's like hundreds of thousands of brains that are working together on a diverse set of subjects . So , imagine a model that 's been trained on a domain like medicine or aerospace or defense , and then trying to bring all of that brainpower together to solve a problem in finance . That truly to me is the ultimate gold standard of problem solving . We talked about diverse people in the room coming with different experiences but imagine models that have been trained . You suddenly have breadth , depth , range of diverse knowledge that could never have been contemplated at that scale . And in order to do all of this , obviously one of the key underpinnings is the public cloud and being able to spin up compute as quickly as possible to do complex calculations and then spin it down when you do n't need it , which is where the public cloud becomes super important . So , all I can say in conclusion is I think this is an amazing time to be in technology , and I just can not wait to see how we further step up our game in the coming months and years , and things are moving almost at the speed of light now . Every single day , new papers get published and new ideas are coming out building on top of some of the exponential technologies that we are seeing in the world today . Laurel : Oh , that 's fantastic . Vrinda , thank you so much for being on the Business Lab today.Vrinda : Thank you so much , Laurel . It 's my pleasure . I really enjoyed speaking with you and thank you for your thoughtful questions . They were super interesting . Laurel : That was Vrinda Menon , the chief technology officer of Managed Accounts , Client Onboarding and Client Services at J.P. Morgan Private Bank , who I spoke with from Cambridge , Massachusetts , the home of MIT and MIT Technology Review overlooking the Charles River.That 's it for this episode of Business Lab . I 'm your host , Laurel Ruma . I 'm the director of Insights , the custom publishing division of MIT Technology Review . We were founded in 1899 at the Massachusetts Institute of Technology , and you can find us in print , on the web and at events each year around the world . For more information about us and the show , please check out our website at technologyreview.com.This show is available wherever you get your podcasts . If you enjoyed this episode , we hope you 'll take a moment to rate and review us . Business Lab is a production of MIT Technology Review . This episode was produced by Giro Studios . Thanks for listening . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff . This podcast is for informational purposes only and it is not intended as legal , tax , financial , investment , accounting or regulatory advice . Opinions expressed herein are the personal views of the individual ( s ) and do not represent the views of JPMorgan Chase & Co . The accuracy of any statements , linked resources , reported findings or quotations are not the responsibility of JPMorgan Chase & Co .","['association', 'co', 'come', 'banking', 'personal', 'business', 'private', 'customer', 'experience', 'build', 'new', 'technology', 'platform', 'employ', 'scale', 'optimize', 'workflow', 'especially', 'critical', 'large', 'bank', 'look', 'meet', 'evolve', 'customer', 'internal', 'stakeholder', 'demand', 'fast', 'personalized', 'way', 'business', 'institution', 'implement', 'good', 'practice', 'cost', 'efficient', 'cloud', 'migration', 'emerge', 'ai', 'machine', 'learning', 'ml', 'tool', 'build', 'well', 'way', 'bank', 'say', 'head', 'manage', 'account', 'client', 'onboarding', 'client', 'service', 'technology', 'bank', 'vrinda', 'menon', 'menon', 'stress', 'critical', 'technologist', 'stay', 'focused', 'business', 'impact', 'software', 'tool', 'develop', 'coach', 'team', 'success', 'innovation', 'come', 'rebuild', 'already', 'build', 'instead', 'leverage', 'take', 'next', 'leap', 'additional', 'feature', 'create', 'high', 'impact', 'business', 'outcome', 'say', 'technologist', 'encourage', 'possible', 'see', 'big', 'picture', 'solve', 'large', 'pattern', 'rather', 'singular', 'problem', 'hand', 'reduce', 'redundancy', 'automate', 'task', 'menon', 'team', 'focus', 'datum', 'measurement', 'indicate', 'emerge', 'technology', 'ai', 'machine', 'learning', 'enhance', 'process', 'onboarding', 'transaction', 'processing', 'scale', 'aiml', 'become', 'commonplace', 'many', 'industry', 'private', 'banking', 'exception', 'say', 'base', 'level', 'aiml', 'extract', 'datum', 'document', 'classify', 'information', 'analyze', 'datum', 'smartly', 'detect', 'issue', 'outlier', 'wide', 'range', 'use', 'case', 'menon', 'look', 'near', 'future', 'aiml', 'proactively', 'predict', 'client', 'need', 'base', 'various', 'signal', 'example', 'private', 'banking', 'client', 'recently', 'marry', 'ask', 'bank', 'title', 'change', 'use', 'client', 'datum', 'context', 'new', 'request', 'aiml', 'tool', 'proactively', 'help', 'banker', 'identify', 'additional', 'thing', 'ask', 'client', 'need', 'change', 'beneficiary', 'possibility', 'optimize', 'taxis', 'consider', 'jointly', 'file', 'taxis', 'opportunity', 'proactive', 'think', 'holistically', 'address', 'need', 'even', 'come', 'ask', 'level', 'engagement', 'detail', 'say', 'episode', 'business', 'lab', 'produce', 'association', 'mit', 'technology', 'review', 'business', 'lab', 'show', 'help', 'business', 'leader', 'make', 'sense', 'new', 'technology', 'come', 'lab', 'marketplaceour', 'topic', 'today', 'invest', 'build', 'great', 'experience', 'number', 'people', 'benefit', 'enterprise', 'investment', 'emerge', 'new', 'technology', 'include', 'customer', 'want', 'well', 'fast', 'new', 'way', 'business', 'internal', 'stakeholder', 'want', 'investment', 'well', 'tool', 'system', 'build', 'fast', 'new', 'way', 'business', 'balance', 'need', 'possibletwo', 'word', 'optimize', 'platformstoday', 'talk', 'vrinda', 'menon', 'chief', 'technology', 'officer', 'manage', 'account', 'client', 'onboarding', 'client', 'service', 'podcast', 'produce', 'association', 'vrindavrinda', 'thank', 'much', 'laurel', 'look', 'forward', 'discussionlaurel', 'great', 'let', 'start', 'often', 'people', 'think', 'likely', 'associate', 'company', 'personal', 'banking', 'credit', 'card', 'describe', 'service', 'private', 'bank', 'provide', 'operation', 'client', 'service', 'evolve', 'transform', 'begin', 'role', 'vrinda', 'sure', 'indeed', 'far', 'personal', 'banking', 'credit', 'card', 'private', 'often', 'refer', 'crown', 'jewel', 'franchise', 'service', 'high', 'net', 'worth', 'client', 'ultrahigh', 'net', 'worth', 'client', 'globe', 'provide', 'service', 'investment', 'management', 'trust', 'estate', 'planning', 'banking', 'service', 'brokerage', 'service', 'customize', 'lending', 'name', 'term', 'transform', 'recent', 'year', 'join', 'say', 'become', 'far', 'tech', 'savvy', 'organization', 'thank', 'small', 'measure', 'new', 'leadership', 'well', 'operation', 'client', 'service', 'think', 'thing', 'change', 'dramatically', 'join', 'first', 'culture', 'first', 'month', 'spend', 'week', 'job', 'operation', 'analyst', 'start', 'understand', 'firsthand', 'painful', 'manual', 'work', 'people', 'subject', 'feel', 'permission', 'thing', 'change', 'work', 'actually', 'connect', 'lot', 'people', 'ground', 'type', 'activity', 'work', 'make', 'change', 'make', 'see', 'light', 'end', 'tunnel', 'suddenly', 'demand', 'change', 'demand', 'automation', 'start', 'build', 'groundswell', 'energy', 'support', 'partner', 'operation', 'service', 'routine', 'repetitive', 'mundane', 'mindnumbe', 'work', 'option', 'table', 'become', 'thing', 'past', 'secondly', 'also', 'grow', 'army', 'citizen', 'developer', 'really', 'access', 'tool', 'technology', 'quick', 'automation', 'depend', 'broad', 'program', 'broad', 'piece', 'technology', 'also', 'super', 'interesting', 'past', 'year', 'take', 'new', 'analyst', 'private', 'bank', 'train', 'python', 'start', 'see', 'benefit', 'thing', 'culture', 'change', 'think', 'big', 'thing', 'achieve', 'past', 'year', 'join', 'second', 'build', 'whole', 'set', 'capability', 'call', 'common', 'capability', 'thing', 'configure', 'new', 'workflow', 'make', 'decision', 'use', 'spreadsheet', 'decision', 'model', 'code', 'system', 'configure', 'modify', 'thing', 'effectively', 'tool', 'checklist', 'put', 'system', 'automate', 'minute', 'many', 'case', 'today', 'million', 'task', 'million', 'decision', 'execute', 'capability', 'suddenly', 'gamechange', 'ability', 'provide', 'automation', 'scale', 'last', 'least', 'ai', 'machine', 'learn', 'play', 'important', 'role', 'underpinning', 'operation', 'client', 'service', 'example', 'lot', 'process', 'analytic', 'load', 'balance', 'client', 'call', 'agent', 'group', 'people', 'direct', 'client', 'call', 'actually', 'service', 'client', 'effectively', 'space', 'payment', 'lot', 'machine', 'learn', 'fraud', 'detection', 'say', 'glad', 'time', 'invest', 'think', 'foundational', 'capability', 'poise', 'ready', 'take', 'next', 'big', 'leap', 'change', 'right', 'fingertip', 'especially', 'evolve', 'world', 'machine', 'learning', 'course', 'public', 'cloudlaurel', 'excellent', 'certainly', 'outline', 'diversity', 'firm', 'offering', 'build', 'new', 'technology', 'platform', 'work', 'methodology', 'practice', 'employ', 'build', 'scale', 'optimize', 'workflow', 'vrinda', 'say', 'private', 'bank', 'lot', 'offering', 'amplify', 'offering', 'franchise', 'commercial', 'bank', 'corporate', 'investment', 'bank', 'consumer', 'community', 'bank', 'many', 'client', 'cross', 'line', 'business', 'bring', 'lot', 'benefit', 'also', 'complexity', 'thing', 'obsess', 'personally', 'simplify', 'thing', 'add', 'complexity', 'second', 'mantra', 'reuse', 'reinvent', 'easy', 'technologist', 'look', 'piece', 'software', 'say', 'great', 'build', 'well', 'instead', 'thing', 'ask', 'people', 'focus', 'organization', 'collectively', 'partner', 'focus', 'first', 'look', 'business', 'outcome', 'coach', 'team', 'success', 'innovation', 'come', 'rebuild', 'already', 'build', 'instead', 'leverage', 'take', 'next', 'leap', 'additional', 'feature', 'create', 'high', 'impact', 'business', 'outcome', 'focus', 'outcome', 'number', 'second', 'give', 'problem', 'try', 'look', 'big', 'picture', 'see', 'solve', 'pattern', 'instead', 'specific', 'problem', 'give', 'example', 'build', 'chatbot', 'call', 'casey', 'love', 'product', 'private', 'bank', 'right', 'casey', 'really', 'complex', 'solve', 'common', 'pattern', 'ask', 'simple', 'question', 'get', 'input', 'join', 'datum', 'service', 'join', 'execution', 'service', 'complete', 'task', 'hundred', 'thousand', 'task', 'casey', 'perform', 'single', 'day', 'especially', 'simple', 'functionality', 'client', 'want', 'bank', 'reference', 'letter', 'casey', 'call', 'thousand', 'time', 'month', 'use', 'take', 'hour', 'produce', 'take', 'second', 'suddenly', 'change', 'outcome', 'change', 'productivity', 'change', 'happiness', 'people', 'thing', 'know', 'feel', 'mundane', 'solve', 'pattern', 'important', 'last', 'least', 'focus', 'datum', 'thing', 'help', 'improve', 'measure', 'give', 'example', 'process', 'first', 'thing', 'pick', 'complex', 'process', 'map', 'understand', 'step', 'process', 'understand', 'purpose', 'step', 'process', 'time', 'take', 'step', 'start', 'question', 'really', 'need', 'approval', 'person', 'observe', 'past', 'month', 'single', 'thing', 'reject', 'even', 'meaningful', 'approval', 'begin', 'questioning', 'process', 'enhance', 'automatically', 'say', 'approve', 'risk', 'approve', 'need', 'human', 'review', 'make', 'change', 'system', 'flow', 'obsessively', 'measure', 'impact', 'change', 'give', 'lot', 'benefit', 'say', 'make', 'significant', 'progress', 'principle', 'focus', 'outcome', 'focus', 'solve', 'pattern', 'focus', 'datum', 'measurement', 'area', 'client', 'onboarding', 'area', 'maintain', 'client', 'datum', 'et', 'cetera', 'helpful', 'bank', 'scale', 'super', 'importantlaurel', 'really', 'great', 'explanation', 'new', 'challenge', 'come', 'move', 'public', 'cloud', 'balance', 'opportunity', 'scale', 'also', 'compute', 'power', 'resource', 'cost', 'actual', 'investment', 'ensure', 'shift', 'cloud', 'actually', 'financially', 'operationally', 'efficient', 'vrinda', 'great', 'question', 'obviously', 'technologist', 'world', 'super', 'excited', 'advent', 'public', 'cloud', 'give', 'power', 'agility', 'economy', 'scale', 'able', 'leverage', 'world', 'class', 'evolving', 'capability', 'fingertip', 'ability', 'also', 'partner', 'talented', 'technology', 'cloud', 'provider', 'many', 'service', 'provider', 'work', 'advanced', 'solution', 'available', 'first', 'public', 'cloud', 'eager', 'get', 'hand', 'come', 'lot', 'responsibility', 'bank', 'worry', 'security', 'client', 'datum', 'privacy', 'resilience', 'go', 'operate', 'multicloud', 'environment', 'datum', 'remain', 'private', 'cloud', 'lot', 'complexity', 'engineer', 'board', 'think', 'lot', 'day', 'night', 'job', 'try', 'figure', 'think', 'move', 'public', 'cloud', 'area', 'personally', 'spend', 'time', 'think', 'depth', 'build', 'architecture', 'financially', 'efficient', 'reason', 'bring', 'traditionally', 'think', 'datum', 'center', 'hardware', 'software', 'host', 'developer', 'architect', 'worry', 'cost', 'start', 'size', 'infrastructure', 'order', 'infrastructure', 'captive', 'remain', 'datum', 'center', 'expand', 'onetime', 'cost', 'time', 'upgrade', 'cloud', 'situation', 'change', 'dramatically', 'opportunity', 'also', 'risk', 'financial', 'lens', 'become', 'super', 'important', 'right', 'outset', 'let', 'give', 'couple', 'example', 'mean', 'developer', 'public', 'cloud', 'lot', 'power', 'power', 'come', 'responsibility', 'developer', 'application', 'work', 'right', 'issue', 'ability', 'actually', 'spin', 'additional', 'process', 'ability', 'spin', 'additional', 'environment', 'attract', 'cost', 'control', 'manage', 'cost', 'quickly', 'pile', 'datum', 'storage', 'fix', 'storage', 'expand', 'periodically', 'datum', 'center', 'public', 'cloud', 'choice', 'say', 'datum', 'go', 'slowly', 'access', 'datum', 'go', 'access', 'frequently', 'store', 'different', 'type', 'storage', 'different', 'cost', 'result', 'think', 'financial', 'ledger', 'retention', 'requirement', 'let', 'say', 'year', 'cost', 'quickly', 'pile', 'store', 'wrong', 'type', 'storage', 'opportunity', 'optimize', 'cost', 'ignore', 'keep', 'eye', 'actually', 'cost', 'require', 'right', 'ask', 'developer', 'architect', 'engineer', 'think', 'good', 'performance', 'optimal', 'resilience', 'also', 'think', 'cost', 'fundamental', 'aspect', 'look', 'architecture', 'huge', 'area', 'focus', 'start', 'awareness', 'people', 'train', 'people', 'think', 'architecture', 'pattern', 'solution', 'pattern', 'tool', 'measurement', 'completely', 'stay', 'top', 'become', 'effective', 'efficient', 'get', 'public', 'cloud', 'journey', 'exciting', 'want', 'make', 'sure', 'land', 'land', 'safely', 'optimally', 'cost', 'standpoint', 'laurel', 'especially', 'position', 'think', 'technology', 'affect', 'firm', 'year', 'future', 'critical', 'therefore', 'emerge', 'technology', 'ai', 'machine', 'learning', 'become', 'commonplace', 'industry', 'offer', 'example', 'use', 'area', 'cover', 'vrinda', 'certainly', 'use', 'aiml', 'many', 'level', 'complexity', 'let', 'start', 'base', 'case', 'aiml', 'especially', 'operation', 'client', 'service', 'start', 'get', 'datum', 'document', 'ocr', 'document', 'optical', 'character', 'recognition', 'get', 'information', 'classify', 'perform', 'analytic', 'base', 'case', 'top', 'look', 'datum', 'example', 'payment', 'datum', 'datum', 'transaction', 'let', 'say', 'human', 'scan', 'issue', 'outlier', 'outli', 'detection', 'technique', 'aiml', 'also', 'table', 'stake', 'many', 'system', 'move', 'next', 'level', 'prediction', 'able', 'start', 'build', 'model', 'say', 'client', 'call', 'client', 'type', 'case', 'progress', 'right', 'call', 'addition', 'client', 'express', 'sentiment', 'happy', 'week', 'ago', 'likely', 'call', 'information', 'fingertip', 'client', 'service', 'agent', 'look', 'respond', 'soon', 'client', 'ask', 'think', 'next', 'stage', 'evolution', 'client', 'come', 'say', 'change', 'title', 'marry', 'typically', 'transactional', 'kind', 'activity', 'respond', 'client', 'fix', 'title', 'let', 'say', 'ask', 'imagine', 'come', 'say', 'thing', 'possibly', 'think', 'say', 'get', 'marry', 'congratulation', 'want', 'address', 'beneficiary', 'want', 'change', 'tax', 'planning', 'want', 'change', 'type', 'tax', 'calculation', 'want', 'optimize', 'marry', 'spouse', 'file', 'jointly', 'client', 'choose', 'change', 'thing', 'opportunity', 'proactive', 'think', 'holistically', 'address', 'need', 'even', 'come', 'ask', 'level', 'engagement', 'detail', 'also', 'exploit', 'lot', 'aiml', 'capability', 'client', 'onboarde', 'get', 'well', 'datum', 'start', 'predict', 'datum', 'right', 'start', 'predict', 'risk', 'next', 'leap', 'believe', 'strongly', 'super', 'excited', 'area', 'large', 'language', 'model', 'think', 'go', 'offer', 'exponential', 'possibility', 'see', 'world', 'right', 'technology', 'chatgpt', 'technology', 'well', 'publicly', 'available', 'large', 'language', 'model', 'develop', 'single', 'day', 'laurel', 'well', 'clear', 'offer', 'great', 'opportunity', 'optimize', 'platform', 'transformation', 'describe', 'process', 'jpmorgan', 'decide', 'create', 'dedicated', 'team', 'ai', 'machine', 'learning', 'build', 'team', 'vrinda', 'certainly', 'cultivate', 'mindset', 'year', 'think', 'aifirst', 'hire', 'people', 'also', 'leverage', 'good', 'talent', 'industry', 'hire', 'lot', 'people', 'research', 'division', 'well', 'work', 'aiml', 'get', 'thousand', 'several', 'technologist', 'focus', 'ai', 'personally', 'first', 'month', 'pandemic', 'decide', 'need', 'see', 'aiml', 'activity', 'area', 'call', 'summer', 'aiml', 'fully', 'immersive', 'program', 'run', 'week', 'training', 'people', 'fulltime', 'dial', 'couple', 'hour', 'train', 'aiml', 'concept', 'technique', 'continue', 'practice', 'week', 'ideation', 'session', 'user', 'couple', 'week', 'hackathon', 'brilliant', 'idea', 'come', 'step', 'back', 'look', 'whole', 'thing', 'result', 'month', 'later', 'realize', 'many', 'idea', 'reach', 'final', 'destination', 'production', 'think', 'little', 'deeply', 'understand', 'problem', 'problem', 'follow', 'great', 'thing', 'appreciate', 'become', 'ingrained', 'brain', 'first', 'thing', 'think', 'always', 'go', 'healthy', 'tension', 'choose', 'next', 'good', 'feature', 'product', 'deterministic', 'say', 'add', 'button', 'add', 'feature', 'use', 'conventional', 'technology', 'java', 'gamechange', 'product', 'use', 'ai', 'little', 'bit', 'risk', 'result', 'always', 'predictable', 'require', 'experimentation', 'r', 'choice', 'incremental', 'change', 'deterministic', 'change', 'probabilistic', 'people', 'tend', 'take', 'certain', 'answer', 'decide', 'need', 'build', 'focused', 'dedicated', 'team', 'datum', 'scientist', 'go', 'obsess', 'solve', 'problem', 'space', 'datum', 'science', 'embed', 'product', 'build', 'result', 'start', 'show', 'work', 'phenomenal', 'demand', 'grow', 'single', 'day', 'point', 'grow', 'team', 'value', 'provide', 'also', 'measure', 'visible', 'broad', 'organizationlaurel', 'client', 'service', 'customer', 'experience', 'clearly', 'drive', 'force', 'ensure', 'team', 'provide', 'client', 'especially', 'highnetworth', 'private', 'client', 'high', 'expectation', 'service', 'service', 'meet', 'banking', 'account', 'management', 'need', 'vrinda', 'obsess', 'customer', 'experience', 'start', 'ceo', 'single', 'employee', 'tenet', 'team', 'number', 'client', 'experience', 'second', 'user', 'experience', 'third', 'engineer', 'excellence', 'know', 'lot', 'measure', 'well', 'service', 'client', 'private', 'bank', 'specifically', 'addition', 'review', 'core', 'capability', 'case', 'management', 'system', 'voice', 'recognition', 'system', 'fraud', 'capture', 'system', 'continuously', 'analyze', 'datum', 'receive', 'client', 'survey', 'datum', 'receive', 'single', 'interaction', 'client', 'channel', 'voice', 'channel', 'email', 'thing', 'client', 'type', 'website', 'place', 'access', 'model', 'look', 'sentiment', 'also', 'look', 'client', 'experience', 'look', 'experience', 'thing', 'try', 'understand', 'first', 'client', 'feeling', 'interaction', 'important', 'client', 'client', 'client', 'feel', 'thing', 'particular', 'aspect', 'process', 'need', 'change', 'process', 'result', 'training', 'need', 'provide', 'agent', 'able', 'fully', 'satisfy', 'category', 'request', 'continuously', 'analyze', 'back', 'point', 'make', 'early', 'measure', 'constantly', 'able', 'say', 'first', 'experience', 'begin', 'experience', 'make', 'change', 'training', 'program', 'fix', 'system', 'experience', 'show', 'thing', 'able', 'look', 'experience', 'period', 'time', 'example', 'client', 'come', 'last', 'year', 'experience', 'base', 'measurement', 'certain', 'level', 'continue', 'interact', 'period', 'month', 'go', 'go', 'needle', 'trend', 'take', 'superb', 'able', 'figure', 'way', 'able', 'prevent', 'complaint', 'example', 'get', 'point', 'thing', 'escalate', 'right', 'people', 'organization', 'especially', 'servicing', 'space', 'able', 'triage', 'manage', 'thing', 'effectively', 'high', 'touch', 'client', 'business', 'need', 'make', 'sure', 'client', 'extremely', 'happy', 'absolutely', 'sort', 'phase', 'idea', 'think', 'customer', 'experience', 'customer', 'service', 'build', 'workforce', 'respond', 'go', 'talk', 'bit', 'promote', 'diversity', 'tenet', 'career', 'currently', 'sit', 'board', 'transition', 'network', 'nonprofit', 'empower', 'diverse', 'network', 'woman', 'career', 'transition', 'grow', 'talent', 'improve', 'representation', 'company', 'help', 'build', 'well', 'customer', 'experience', 'vrinda', 'sure', 'great', 'question', 'certainly', 'passionate', 'diversity', 'past', 'year', 'career', 'spend', 'lot', 'time', 'support', 'diversity', 'prior', 'firm', 'cohead', 'asian', 'professional', 'network', 'subsequently', 'past', 'year', 'board', 'member', 'transition', 'network', 'woman', 'transition', 'meaning', 'grow', 'career', 'retirement', 'stage', 'life', 'help', 'transition', 'sponsor', 'call', 'take', 'forward', 'initiative', 'initiative', 'support', 'woman', 'technologist', 'know', 'broad', 'range', 'activity', 'area', 'diversity', 'kind', 'business', 'resource', 'group', 'invest', 'lot', 'time', 'energy', 'specifically', 'take', 'forward', 'initiative', 'sponsor', 'play', 'key', 'role', 'help', 'woman', 'technologist', 'continuously', 'enhance', 'leadership', 'skill', 'grow', 'technical', 'skill', 'build', 'confidence', 'develop', 'network', 'learn', 'senior', 'sponsor', 'mentor', 'grow', 'career', 'make', 'work', 'experience', 'enrich', 'hear', 'thing', 'motivate', 'get', 'new', 'energy', 'interact', 'senior', 'woman', 'trust', 'personal', 'power', 'confident', 'negotiate', 'manager', 'well', 'role', 'feel', 'confident', 'discuss', 'compensation', 'make', 'really', 'happy', 'especially', 'say', 'stay', 'take', 'forward', 'bring', 'tear', 'eye', 'really', 'amazing', 'volunteer', 'drive', 'initiative', 'organization', 'lot', 'people', 'pour', 'passion', 'energy', 'time', 'make', 'succeed', 'initiative', 'win', 'many', 'award', 'well', 'externally', 'strongly', 'believe', 'effort', 'critical', 'change', 'people', 'experience', 'change', 'happy', 'become', 'much', 'effective', 'change', 'work', 'internally', 'present', 'externally', 'game', 'change', 'business', 'outcome', 'see', 'problem', 'solve', 'meeting', 'evaluate', 'risk', 'bring', 'people', 'diverse', 'background', 'risk', 'averse', 'risk', 'take', 'suddenly', 'see', 'dynamic', 'play', 'outcome', 'become', 'much', 'different', 'people', 'mix', 'overall', 'strongly', 'believe', 'see', 'play', 'single', 'firm', 'ever', 'work', 'take', 'diversity', 'help', 'well', 'certainly', 'important', 'work', 'especially', 'tie', 'tightly', 'firm', 'ethos', 'vrinda', 'look', 'forward', 'envision', 'future', 'private', 'banking', 'client', 'management', 'see', 'emerge', 'technology', 'become', 'prevalent', 'enterprise', 'start', 'shift', 'infrastructure', 'public', 'cloud', 'vrinda', 'mention', 'early', 'see', 'next', 'set', 'emerge', 'technology', 'take', 'world', 'super', 'exciting', 'ride', 'think', 'go', 'transformational', 'advent', 'world', 'wide', 'web', 'take', 'example', 'large', 'language', 'model', 'area', 'likely', 'first', 'disrupt', 'work', 'involve', 'content', 'creation', 'table', 'stake', 'large', 'language', 'model', 'expand', 'work', 'rest', 'work', 'client', 'service', 'operation', 'many', 'area', 'require', 'repetitive', 'work', 'largescale', 'interpretation', 'synthesis', 'information', 'table', 'stake', 'large', 'language', 'model', 'expand', 'next', 'evolution', 'agent', 'emerge', 'technology', 'large', 'language', 'model', 'agent', 'provide', 'suite', 'tool', 'use', 'reasoning', 'human', 'decide', 'tool', 'execute', 'base', 'input', 'game', 'change', 'whole', 'thing', 'talk', 'early', 'workflow', 'task', 'execution', 'operationally', 'intense', 'activity', 'organization', 'look', 'software', 'developer', 'area', 'code', 'generation', 'code', 'testing', 'code', 'correction', 'test', 'datum', 'generation', 'name', 'go', 'game', 'change', 'work', 'user', 'private', 'bank', 'work', 'technologist', 'private', 'bank', 'lot', 'go', 'game', 'change', 'dramatically', 'add', 'next', 'level', 'problem', 'solve', 'large', 'language', 'model', 'continuously', 'train', 'subject', 'ever', 'know', 'human', 'fascinating', 'part', 'hundred', 'thousand', 'brain', 'work', 'together', 'diverse', 'set', 'subject', 'imagine', 'model', 'train', 'domain', 'medicine', 'aerospace', 'defense', 'try', 'bring', 'brainpower', 'together', 'solve', 'problem', 'finance', 'truly', 'ultimate', 'gold', 'standard', 'problem', 'solve', 'talk', 'diverse', 'people', 'room', 'come', 'different', 'experience', 'imagine', 'model', 'train', 'suddenly', 'breadth', 'depth', 'range', 'diverse', 'knowledge', 'never', 'contemplate', 'scale', 'order', 'obviously', 'key', 'underpinning', 'public', 'cloud', 'able', 'spin', 'compute', 'quickly', 'possible', 'complex', 'calculation', 'spin', 'need', 'public', 'cloud', 'become', 'super', 'important', 'say', 'conclusion', 'think', 'amazing', 'time', 'technology', 'wait', 'see', 'far', 'step', 'game', 'come', 'month', 'year', 'thing', 'move', 'almost', 'speed', 'light', 'single', 'day', 'new', 'paper', 'publish', 'new', 'idea', 'come', 'building', 'top', 'exponential', 'technology', 'see', 'world', 'today', 'laurel', 'fantastic', 'vrinda', 'much', 'business', 'lab', 'thank', 'much', 'laurel', 'pleasure', 'really', 'enjoy', 'speak', 'thank', 'thoughtful', 'question', 'super', 'interesting', 'laurel', 'vrinda', 'menon', 'chief', 'technology', 'officer', 'manage', 'account', 'client', 'onboarding', 'client', 'service', 'speak', 'home', 'mit', 'mit', 'technology', 'review', 'overlook', 'charle', 'episode', 'business', 'lab', 'host', 'director', 'insight', 'custom', 'publishing', 'division', 'mit', 'technology', 'review', 'found', 'find', 'print', 'web', 'event', 'year', 'world', 'information', 'show', 'check', 'website', 'show', 'available', 'get', 'podcast', 'enjoy', 'episode', 'hope', 'take', 'moment', 'rate', 'review', 'business', 'lab', 'production', 'mit', 'technology', 'review', 'episode', 'produce', 'studio', 'thank', 'listen', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff', 'podcast', 'informational', 'purpose', 'intend', 'legal', 'tax', 'financial', 'investment', 'accounting', 'regulatory', 'advice', 'opinion', 'express', 'herein', 'personal', 'view', 'individual', 'represent', 'view', 'co', 'accuracy', 'statement', 'link', 'resource', 'report', 'finding', 'quotation', 'responsibility', 'co']","When it comes to banking, whether it’s personal, business, or private, customer experience is everything. Building new technologies and platforms, employing them at scale, and optimizing workflows is especially critical for any large bank looking to meet evolving customer and internal stakeholder demands for faster and more personalized ways of doing business. Institutions like JPMorgan…"
2023 Global Cloud Ecosystem,https://www.technologyreview.com/2023/11/16/1078645/2023-global-cloud-ecosystem/,2023-11-16,"In partnership withInfosys Cobalt The cloud, fundamentally a tool for cost and resource efficiency, has long enabled companies and countries to organize around digital-first principles. It is an established capability that improves the bottom line for enterprises. However, maturity lags, and global standards are sorely needed. Cloud capabilities play a crucial role in accelerating the global economy’s next stage of digital transformation. Results from our 2023 Global Cloud Ecosystem survey of executives indicate there are two stages of cloud maturity globally: one where firms adopt cloud to achieve essential opex and capex cost reduction, and a second where firms link cloud investments to a positive business value. Respondents indicate the two are converging quickly. The key findings are as follows: Download the full report. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. ","In partnership withInfosys Cobalt The cloud , fundamentally a tool for cost and resource efficiency , has long enabled companies and countries to organize around digital-first principles . It is an established capability that improves the bottom line for enterprises . However , maturity lags , and global standards are sorely needed . Cloud capabilities play a crucial role in accelerating the global economy ’ s next stage of digital transformation . Results from our 2023 Global Cloud Ecosystem survey of executives indicate there are two stages of cloud maturity globally : one where firms adopt cloud to achieve essential opex and capex cost reduction , and a second where firms link cloud investments to a positive business value . Respondents indicate the two are converging quickly . The key findings are as follows : Download the full report . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['partnership', 'cobalt', 'cloud', 'fundamentally', 'tool', 'cost', 'resource', 'efficiency', 'long', 'enable', 'company', 'country', 'organize', 'digitalfirst', 'principle', 'establish', 'capability', 'improve', 'bottom', 'line', 'enterprise', 'however', 'maturity', 'lag', 'global', 'standard', 'sorely', 'need', 'cloud', 'capability', 'play', 'crucial', 'role', 'accelerate', 'global', 'economy', 'next', 'stage', 'digital', 'transformation', 'result', 'global', 'cloud', 'ecosystem', 'survey', 'executive', 'indicate', 'stage', 'cloud', 'maturity', 'globally', 'one', 'firm', 'adopt', 'cloud', 'achieve', 'essential', 'opex', 'capex', 'cost', 'reduction', 'second', 'firm', 'link', 'cloud', 'investment', 'positive', 'business', 'value', 'respondent', 'indicate', 'converge', 'quickly', 'key', 'finding', 'follow', 'download', 'full', 'report', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","The cloud, fundamentally a tool for cost and resource efficiency, has long enabled companies and countries to organize around digital-first principles. It is an established capability that improves the bottom line for enterprises. However, maturity lags, and global standards are sorely needed. Cloud capabilities play a crucial role in accelerating the global economy’s next stage…"
Huawei’s 5G chip breakthrough needs a reality check,https://www.technologyreview.com/2023/11/15/1083413/huaweis-5g-chip-smartphone-sanction/,2023-11-15,"This story first appeared in China Report, MIT Technology Review’s newsletter about technology in China. Sign up to receive it in your inbox every Tuesday. This is going to be a BIG week for US-China relations: On Wednesday, Xi Jinping will sit down with Joe Biden in San Francisco and talk about military issues, trade, and more. It will be his first visit to the US in six years.  Well, so much has happened since 2017. A harmonious era in US-China relations ended; we’ve seen a trade war, a pandemic, an ongoing technology rivalry, an off-course spy balloon, and too many other tumultuous developments to list here.  It’s in this context that Huawei, the Chinese telecom and technology company, became something of a poster child for souring relations. It was one of the first Chinese tech companies to receive intense scrutiny and become a target for sanctions driven by national security concerns. In fact, many of the ways the US currently deploys sanctions in the US-China tech war are inspired by its success in curbing Huawei. (If you want to know more about the fight over semiconductors, read more from me here and here.) But it’d be a mistake to think Huawei has collapsed. Far from it, in fact. In August—at the same time US Commerce Secretary Gina Raimondo was visiting China—Huawei suddenly, without any public announcement, shocked the world when it started selling a new flagship phone, the Mate 60 Pro.  The big surprise here was that this phone uses a new 5G chip, even though the company has been blocked from sourcing 5G chips or working with chip factories outside of China since 2020. So regulators in DC and other China hawks panicked that the relatively advanced seven-nanometer chip proved the company had somehow circumvented sanctions.  But had it really? Researchers who broke down the chip believe that it seems to be entirely designed by Huawei and made in China. I’ve been wanting to understand what really happened here—and what it means more broadly for the ongoing competition over chips and other tech. So I recently spoke with Harish Krishnaswamy, a professor at Columbia University who studies telecommunications and chip designs.  The bottom line is that the Mate 60 Pro shows a manufacturing breakthrough on Huawei’s side that puts it back into the smartphone game. “It's clearly wrong to assume that they're not a player because of the sanctions,” Krishnaswamy told me. He explained that the designing of 5G chips is not necessarily difficult for Chinese researchers and companies, so it wouldn’t have been that hard for Huawei to create a 5G chip in the lab. What is much harder is mass-producing a 5G chip at great quality and reasonable costs so it can be used in a consumer product.  Producing advanced 5G chips “is just a very large engineering endeavor that very few companies can pull off,” he says. Only a handful of companies—like Qualcomm (American), MediaTek (Taiwanese), Samsung (South Korean), and HiSilicon (a chip company that is a Huawei subsidiary)—have successfully made 5G modem chips. Even tech giants like Intel and Apple have failed so far to develop in-house 5G chips.  So when the US sanctions blocked companies from supplying chips to Huawei for its phones, the Chinese manufacturer had no choice but to rely on HiSilicon alone.  Aggressive new US policies will be put to the test in 2023. They could ultimately fragment the global semiconductor industry. But the sanctions also cut HiSilicon off from the global network of factories (called “foundries” or “fabs” in the semiconductor industry) that are vital in manufacturing and testing the products. TSMC, the Taiwanese chip fab that was making chips for HiSilicon at the time, stopped supplying it in 2020. This further narrowed the options for Huawei. It could only turn to Chinese fabs, and that was a costly and time-consuming process. “Anytime you’ve designed a chip in a fab and then, for some reason, you cannot source that anymore and move to another company, just that process of redesigning, qualification, and ramping production takes at least three years,” says Krishnaswamy, who also owns a chip company that works closely with fabs.  Then, making things even more challenging for Huawei, Chinese chip fabs were later put under sanctions and now can’t access any cutting-edge chip-making technologies. So when Huawei made the smartphone with a new 5G chip earlier this year, Krishnaswamy says, the industry was surprised by how quickly it had been able to turn around a design, shift to a Chinese fab, manufacture the new chip, and get it to production rate, all while having to deal with significant revenue losses caused by sanctions. “For Huawei to show [that it is] able to do that is definitely impressive,” he says.  Indeed, since news came out that it was making its own 5G chips in Chinese fabs with minimum overseas input, the company has become a source of national pride for many in China. For them, it’s an example of how US sanctions don’t always work as intended. They may force Chinese companies to adapt, move production back to China, and catch up in areas of tech where they had lagged—and that can ultimately work out in China’s favor. Still, it may be too early for them to cheer, as there are other obstacles to clear before Huawei can become competitive in the high-end phone market again. “After any company shows proof of life—basically, that they have a chip—the long-term success is really governed by the ability to source [it] in large volumes and get the costs down over time,” Krishnaswamy says. Huawei is experiencing a reality check right now. A few months after the release of the new model, domestic enthusiasm has been hurt by the fact that it’s still very difficult for consumers to get a phone. They are back-ordered for months because of a supply shortage. In fact, just this past weekend, which was China’s equivalent of Black Friday, the top-selling phones were still from Apple and Xiaomi (another Chinese brand). At the end of the day, for any chance of regaining its dominant position in the smartphone market, Huawei needs to compete with Qualcomm and Samsung to reliably make better and cheaper chips, and that’s very difficult to pull off. The US chip blockade, which is consistently being expanded and strengthened, will only make it harder in the future—even if it hasn’t made it impossible thus far. Unless the Biden-Xi meeting produces some truly surprising results this week, the bitter winter of US-China relations may continue for years. In which case there are a lot more battles ahead for Chinese companies like Huawei, and one breakthrough in chip manufacturing won’t be enough.  Do you think Huawei still has a chance in the global smartphone market? Let me know your thoughts at zeyi@technologyreview.com. 1. Xi wanted to dine with American business leaders in San Francisco before he met with Biden. The White House rejected the proposal, part of a lengthy back-and-forth over the visit’s agenda. (Wall Street Journal $) 2. A Chinese university tapped Hikvision, the surveillance camera company, to build a smart campus project last year. It includes an alert system that flags ethnic minority students who choose to fast during Ramadan. (IPVM) 3. As Huawei builds up its Harmony Operating System to replace the now-embargoed Android system, Chinese tech companies are quickly recruiting developers who understand HarmonyOS. (South China Morning Post $) 4. The Nepali government decided to ban TikTok, claiming that the app has spread discord and disturbed social harmony. (Nepali Times) 5. After Chinese hobbyist drones were heavily used in the war in Ukraine, the Israeli military is now stockpiling them to use in its war. (Wall Street Journal $) 6. A new bipartisan bill in Congress seeks to block the US government from doing business with Tether, the issuer of a major stablecoin called USDT, because Tether’s parent company is based in Hong Kong. (Coindesk) 7. Baidu ordered 1,600 AI chips from Huawei as an alternative to Nvidia’s A100 chip to use in servers, showing how Chinese companies cope with the bottleneck on computing power. (Reuters $)  8. The founder of a major Chinese game-streaming website is apparently being held by the police. It could be related to an investigation of porn and gambling content on the platform. (Financial Times $) This year, many Chinese smartphone users found themselves pestered by a new form of advertising: whenever they shook their phone, intentionally or not, the e-commerce app would automatically open.  It turns out this is the latest marketing trick that exploits the gyroscope function in modern-day phones, which detects movement to calculate steps or find directions. Late last year, some Chinese tech companies came up with an industry standard on the technical specifications for how to open a new app when movement is detected. It quickly started being deployed to spam users with ads, and it has drawn widespread complaints. According to the Chinese publication Time Finance, Apple has become aware of the phenomenon and instructed several top Chinese apps to ban the practice starting this month. But it’s hard to say whether that’s enough to prevent more apps from following suit. A traveler ordered food delivery to his hotel room in China but never got it. Eventually, he received a call asking for help—the robot that was carrying his food had tripped and couldn’t get up. Maybe robots will eventually rule the world, but fortunately, not today.  Iâ€™ve been thinking a lot about this æœ‹å‹åœˆ latelyâ€¦ the guy ordered delivery to the room at a hotel, called down and concierge said it had already been sent up, but eventually he gets a call from the delivery robot asking for helpâ€¦ #china #robotwars pic.twitter.com/Miy6Ibyvku","This story first appeared in China Report , MIT Technology Review ’ s newsletter about technology in China . Sign up to receive it in your inbox every Tuesday . This is going to be a BIG week for US-China relations : On Wednesday , Xi Jinping will sit down with Joe Biden in San Francisco and talk about military issues , trade , and more . It will be his first visit to the US in six years . Well , so much has happened since 2017 . A harmonious era in US-China relations ended ; we ’ ve seen a trade war , a pandemic , an ongoing technology rivalry , an off-course spy balloon , and too many other tumultuous developments to list here . It ’ s in this context that Huawei , the Chinese telecom and technology company , became something of a poster child for souring relations . It was one of the first Chinese tech companies to receive intense scrutiny and become a target for sanctions driven by national security concerns . In fact , many of the ways the US currently deploys sanctions in the US-China tech war are inspired by its success in curbing Huawei . ( If you want to know more about the fight over semiconductors , read more from me here and here . ) But it ’ d be a mistake to think Huawei has collapsed . Far from it , in fact . In August—at the same time US Commerce Secretary Gina Raimondo was visiting China—Huawei suddenly , without any public announcement , shocked the world when it started selling a new flagship phone , the Mate 60 Pro . The big surprise here was that this phone uses a new 5G chip , even though the company has been blocked from sourcing 5G chips or working with chip factories outside of China since 2020 . So regulators in DC and other China hawks panicked that the relatively advanced seven-nanometer chip proved the company had somehow circumvented sanctions . But had it really ? Researchers who broke down the chip believe that it seems to be entirely designed by Huawei and made in China . I ’ ve been wanting to understand what really happened here—and what it means more broadly for the ongoing competition over chips and other tech . So I recently spoke with Harish Krishnaswamy , a professor at Columbia University who studies telecommunications and chip designs . The bottom line is that the Mate 60 Pro shows a manufacturing breakthrough on Huawei ’ s side that puts it back into the smartphone game . “ It 's clearly wrong to assume that they 're not a player because of the sanctions , ” Krishnaswamy told me . He explained that the designing of 5G chips is not necessarily difficult for Chinese researchers and companies , so it wouldn ’ t have been that hard for Huawei to create a 5G chip in the lab . What is much harder is mass-producing a 5G chip at great quality and reasonable costs so it can be used in a consumer product . Producing advanced 5G chips “ is just a very large engineering endeavor that very few companies can pull off , ” he says . Only a handful of companies—like Qualcomm ( American ) , MediaTek ( Taiwanese ) , Samsung ( South Korean ) , and HiSilicon ( a chip company that is a Huawei subsidiary ) —have successfully made 5G modem chips . Even tech giants like Intel and Apple have failed so far to develop in-house 5G chips . So when the US sanctions blocked companies from supplying chips to Huawei for its phones , the Chinese manufacturer had no choice but to rely on HiSilicon alone . Aggressive new US policies will be put to the test in 2023 . They could ultimately fragment the global semiconductor industry . But the sanctions also cut HiSilicon off from the global network of factories ( called “ foundries ” or “ fabs ” in the semiconductor industry ) that are vital in manufacturing and testing the products . TSMC , the Taiwanese chip fab that was making chips for HiSilicon at the time , stopped supplying it in 2020 . This further narrowed the options for Huawei . It could only turn to Chinese fabs , and that was a costly and time-consuming process . “ Anytime you ’ ve designed a chip in a fab and then , for some reason , you can not source that anymore and move to another company , just that process of redesigning , qualification , and ramping production takes at least three years , ” says Krishnaswamy , who also owns a chip company that works closely with fabs . Then , making things even more challenging for Huawei , Chinese chip fabs were later put under sanctions and now can ’ t access any cutting-edge chip-making technologies . So when Huawei made the smartphone with a new 5G chip earlier this year , Krishnaswamy says , the industry was surprised by how quickly it had been able to turn around a design , shift to a Chinese fab , manufacture the new chip , and get it to production rate , all while having to deal with significant revenue losses caused by sanctions . “ For Huawei to show [ that it is ] able to do that is definitely impressive , ” he says . Indeed , since news came out that it was making its own 5G chips in Chinese fabs with minimum overseas input , the company has become a source of national pride for many in China . For them , it ’ s an example of how US sanctions don ’ t always work as intended . They may force Chinese companies to adapt , move production back to China , and catch up in areas of tech where they had lagged—and that can ultimately work out in China ’ s favor . Still , it may be too early for them to cheer , as there are other obstacles to clear before Huawei can become competitive in the high-end phone market again . “ After any company shows proof of life—basically , that they have a chip—the long-term success is really governed by the ability to source [ it ] in large volumes and get the costs down over time , ” Krishnaswamy says . Huawei is experiencing a reality check right now . A few months after the release of the new model , domestic enthusiasm has been hurt by the fact that it ’ s still very difficult for consumers to get a phone . They are back-ordered for months because of a supply shortage . In fact , just this past weekend , which was China ’ s equivalent of Black Friday , the top-selling phones were still from Apple and Xiaomi ( another Chinese brand ) . At the end of the day , for any chance of regaining its dominant position in the smartphone market , Huawei needs to compete with Qualcomm and Samsung to reliably make better and cheaper chips , and that ’ s very difficult to pull off . The US chip blockade , which is consistently being expanded and strengthened , will only make it harder in the future—even if it hasn ’ t made it impossible thus far . Unless the Biden-Xi meeting produces some truly surprising results this week , the bitter winter of US-China relations may continue for years . In which case there are a lot more battles ahead for Chinese companies like Huawei , and one breakthrough in chip manufacturing won ’ t be enough . Do you think Huawei still has a chance in the global smartphone market ? Let me know your thoughts at zeyi @ technologyreview.com . 1 . Xi wanted to dine with American business leaders in San Francisco before he met with Biden . The White House rejected the proposal , part of a lengthy back-and-forth over the visit ’ s agenda . ( Wall Street Journal $ ) 2 . A Chinese university tapped Hikvision , the surveillance camera company , to build a smart campus project last year . It includes an alert system that flags ethnic minority students who choose to fast during Ramadan . ( IPVM ) 3 . As Huawei builds up its Harmony Operating System to replace the now-embargoed Android system , Chinese tech companies are quickly recruiting developers who understand HarmonyOS . ( South China Morning Post $ ) 4 . The Nepali government decided to ban TikTok , claiming that the app has spread discord and disturbed social harmony . ( Nepali Times ) 5 . After Chinese hobbyist drones were heavily used in the war in Ukraine , the Israeli military is now stockpiling them to use in its war . ( Wall Street Journal $ ) 6 . A new bipartisan bill in Congress seeks to block the US government from doing business with Tether , the issuer of a major stablecoin called USDT , because Tether ’ s parent company is based in Hong Kong . ( Coindesk ) 7 . Baidu ordered 1,600 AI chips from Huawei as an alternative to Nvidia ’ s A100 chip to use in servers , showing how Chinese companies cope with the bottleneck on computing power . ( Reuters $ ) 8 . The founder of a major Chinese game-streaming website is apparently being held by the police . It could be related to an investigation of porn and gambling content on the platform . ( Financial Times $ ) This year , many Chinese smartphone users found themselves pestered by a new form of advertising : whenever they shook their phone , intentionally or not , the e-commerce app would automatically open . It turns out this is the latest marketing trick that exploits the gyroscope function in modern-day phones , which detects movement to calculate steps or find directions . Late last year , some Chinese tech companies came up with an industry standard on the technical specifications for how to open a new app when movement is detected . It quickly started being deployed to spam users with ads , and it has drawn widespread complaints . According to the Chinese publication Time Finance , Apple has become aware of the phenomenon and instructed several top Chinese apps to ban the practice starting this month . But it ’ s hard to say whether that ’ s enough to prevent more apps from following suit . A traveler ordered food delivery to his hotel room in China but never got it . Eventually , he received a call asking for help—the robot that was carrying his food had tripped and couldn ’ t get up . Maybe robots will eventually rule the world , but fortunately , not today . Iâ€™ve been thinking a lot about this æœ‹å‹åœˆ latelyâ€¦ the guy ordered delivery to the room at a hotel , called down and concierge said it had already been sent up , but eventually he gets a call from the delivery robot asking for helpâ€¦ # china # robotwars pic.twitter.com/Miy6Ibyvku","['story', 'first', 'appear', 'mit', 'technology', 'review', 'newsletter', 'technology', 'sign', 'receive', 'inbox', 'go', 'big', 'week', 'relation', 'jinping', 'sit', 'talk', 'military', 'issue', 'trade', 'first', 'visit', 'year', 'well', 'much', 'happen', 'harmonious', 'era', 'relation', 'end', 'see', 'trade', 'war', 'pandemic', 'ongoing', 'technology', 'rivalry', 'offcourse', 'spy', 'balloon', 'many', 'tumultuous', 'development', 'list', 'context', 'huawei', 'chinese', 'telecom', 'technology', 'company', 'become', 'poster', 'child', 'sour', 'relation', 'first', 'chinese', 'tech', 'company', 'receive', 'intense', 'scrutiny', 'become', 'target', 'sanction', 'drive', 'national', 'security', 'concern', 'fact', 'many', 'way', 'currently', 'deploy', 'sanction', 'war', 'inspire', 'success', 'curb', 'huawei', 'want', 'know', 'fight', 'semiconductor', 'read', 'mistake', 'think', 'collapse', 'far', 'fact', 'time', 'visit', 'suddenly', 'public', 'announcement', 'shock', 'world', 'start', 'sell', 'new', 'flagship', 'phone', 'mate', 'pro', 'big', 'surprise', 'phone', 'use', 'new', 'g', 'chip', 'even', 'company', 'block', 'source', 'g', 'chip', 'work', 'chip', 'factory', 'outside', 'regulator', 'panic', 'relatively', 'advanced', 'sevennanometer', 'chip', 'prove', 'company', 'somehow', 'circumvent', 'sanction', 'really', 'researcher', 'break', 'chip', 'believe', 'seem', 'entirely', 'design', 'make', 'want', 'understand', 'really', 'happen', 'mean', 'broadly', 'ongoing', 'competition', 'chip', 'tech', 'recently', 'speak', 'harish', 'krishnaswamy', 'professor', 'study', 'telecommunication', 'chip', 'design', 'bottom', 'line', 'mate', 'show', 'manufacturing', 'breakthrough', 'side', 'put', 'back', 'smartphone', 'game', 'clearly', 'wrong', 'assume', 'player', 'sanction', 'tell', 'explain', 'designing', 'g', 'chip', 'necessarily', 'difficult', 'chinese', 'researcher', 'company', 'hard', 'huawei', 'create', 'g', 'chip', 'lab', 'much', 'hard', 'massproduce', 'g', 'chip', 'great', 'quality', 'reasonable', 'cost', 'use', 'consumer', 'product', 'produce', 'advanced', 'g', 'chip', 'large', 'engineering', 'endeavor', 'company', 'pull', 'say', 'handful', 'company', 'qualcomm', 'south', 'korean', 'chip', 'company', 'subsidiary', 'successfully', 'make', 'g', 'modem', 'chip', 'even', 'tech', 'giant', 'apple', 'fail', 'far', 'develop', 'inhouse', 'g', 'chip', 'sanction', 'block', 'company', 'supply', 'chip', 'phone', 'chinese', 'manufacturer', 'choice', 'rely', 'alone', 'aggressive', 'new', 'policy', 'put', 'test', 'ultimately', 'fragment', 'global', 'semiconductor', 'industry', 'sanction', 'also', 'cut', 'global', 'network', 'factory', 'call', 'foundry', 'fab', 'semiconductor', 'industry', 'vital', 'manufacturing', 'test', 'product', 'taiwanese', 'chip', 'make', 'chip', 'hisilicon', 'time', 'stop', 'supply', 'far', 'narrow', 'option', 'turn', 'chinese', 'fab', 'costly', 'timeconsuming', 'process', 'anytime', 'design', 'chip', 'fab', 'reason', 'source', 'anymore', 'move', 'company', 'process', 'redesign', 'qualification', 'ramp', 'production', 'take', 'least', 'year', 'say', 'also', 'chip', 'company', 'work', 'closely', 'fab', 'make', 'thing', 'even', 'challenging', 'chinese', 'chip', 'fab', 'later', 'put', 'sanction', 'access', 'cuttingedge', 'chipmake', 'technology', 'make', 'smartphone', 'new', 'g', 'chip', 'early', 'year', 'krishnaswamy', 'say', 'industry', 'surprise', 'quickly', 'able', 'turn', 'design', 'shift', 'manufacture', 'new', 'chip', 'get', 'production', 'rate', 'deal', 'significant', 'revenue', 'loss', 'cause', 'sanction', 'show', 'able', 'definitely', 'impressive', 'say', 'indeed', 'news', 'come', 'make', 'g', 'chip', 'chinese', 'fab', 'minimum', 'overseas', 'input', 'company', 'become', 'source', 'national', 'pride', 'many', 'example', 'sanction', 'always', 'work', 'intend', 'force', 'chinese', 'company', 'adapt', 'move', 'production', 'back', 'catch', 'area', 'tech', 'lag', 'ultimately', 'work', 'favor', 'still', 'early', 'cheer', 'obstacle', 'clear', 'huawei', 'become', 'competitive', 'highend', 'phone', 'market', 'company', 'show', 'proof', 'life', 'basically', 'chip', 'longterm', 'success', 'really', 'govern', 'ability', 'source', 'large', 'volume', 'get', 'cost', 'time', 'krishnaswamy', 'say', 'experience', 'reality', 'check', 'right', 'month', 'release', 'new', 'model', 'domestic', 'enthusiasm', 'hurt', 'fact', 'still', 'difficult', 'consumer', 'get', 'phone', 'backordere', 'month', 'supply', 'shortage', 'fact', 'past', 'weekend', 'equivalent', 'black', 'topselle', 'phone', 'still', 'apple', 'xiaomi', 'chinese', 'brand', 'end', 'day', 'chance', 'regain', 'dominant', 'position', 'smartphone', 'market', 'need', 'compete', 'qualcomm', 'reliably', 'make', 'well', 'cheap', 'chip', 'difficult', 'pull', 'chip', 'blockade', 'consistently', 'expand', 'strengthen', 'make', 'hard', 'future', 'even', 'make', 'impossible', 'thus', 'far', 'bidenxi', 'meeting', 'produce', 'truly', 'surprising', 'result', 'week', 'bitter', 'winter', 'relation', 'continue', 'year', 'case', 'lot', 'battle', 'ahead', 'chinese', 'company', 'breakthrough', 'chip', 'manufacturing', 'win', 'enough', 'think', 'still', 'chance', 'global', 'smartphone', 'market', 'let', 'know', 'thought', 'want', 'dine', 'american', 'business', 'leader', 'meet', 'biden', 'reject', 'proposal', 'part', 'lengthy', 'backandforth', 'visit', 'chinese', 'university', 'tap', 'hikvision', 'surveillance', 'camera', 'company', 'build', 'smart', 'campus', 'project', 'last', 'year', 'include', 'alert', 'system', 'flag', 'ethnic', 'minority', 'student', 'choose', 'fast', 'ipvm', 'build', 'harmony', 'operating', 'system', 'replace', 'android', 'system', 'chinese', 'tech', 'company', 'quickly', 'recruit', 'developer', 'understand', 'harmonyos', 'morning', 'post', 'nepali', 'government', 'decide', 'ban', 'tiktok', 'claim', 'app', 'spread', 'discord', 'disturb', 'social', 'harmony', 'time', 'hobbyist', 'drone', 'heavily', 'use', 'war', 'israeli', 'military', 'stockpile', 'use', 'war', 'new', 'bipartisan', 'bill', 'seek', 'block', 'government', 'business', 'tether', 'issuer', 'major', 'stablecoin', 'call', 'tether', 'parent', 'company', 'base', 'coindesk', 'baidu', 'order', 'ai', 'chip', 'alternative', 'chip', 'use', 'server', 'show', 'chinese', 'company', 'cope', 'bottleneck', 'compute', 'power', 'reuter', 'founder', 'major', 'chinese', 'gamestreaming', 'website', 'apparently', 'hold', 'police', 'relate', 'investigation', 'porn', 'gambling', 'content', 'platform', 'financial', 'time', 'year', 'many', 'chinese', 'smartphone', 'user', 'find', 'pester', 'new', 'form', 'advertising', 'shake', 'phone', 'intentionally', 'ecommerce', 'automatically', 'open', 'turn', 'late', 'marketing', 'trick', 'exploit', 'gyroscope', 'function', 'modernday', 'phone', 'detect', 'movement', 'calculate', 'step', 'find', 'direction', 'late', 'last', 'year', 'chinese', 'tech', 'company', 'come', 'industry', 'standard', 'technical', 'specification', 'open', 'new', 'app', 'movement', 'detect', 'quickly', 'start', 'deploy', 'spam', 'user', 'ad', 'draw', 'widespread', 'complaint', 'accord', 'chinese', 'publication', 'time', 'finance', 'apple', 'become', 'aware', 'phenomenon', 'instruct', 'several', 'top', 'chinese', 'app', 'ban', 'practice', 'start', 'month', 'hard', 'say', 'enough', 'prevent', 'app', 'follow', 'suit', 'traveler', 'order', 'food', 'delivery', 'hotel', 'room', 'never', 'get', 'eventually', 'receive', 'call', 'ask', 'help', 'robot', 'carry', 'food', 'trip', 'get', 'maybe', 'robot', 'eventually', 'rule', 'world', 'fortunately', 'today', 'iâ€', 'think', 'lot', 'guy', 'order', 'delivery', 'room', 'hotel', 'call', 'concierge', 'say', 'already', 'send', 'eventually', 'get', 'call', 'delivery', 'robot', 'ask', 'robotwar']","<p>A self-made chip puts Huawei back in the smartphone game, but US sanctions are still hurting the company.</p>
"
The grassroots push to digitize India’s most precious documents,https://www.technologyreview.com/2023/10/25/1081572/india-digitize-rare-documents-internet-archive-library-servants-of-knowledge/,2023-10-25,"On a bright sunny day in August, in a second-floor room at the Gandhi Bhavan Museum in Bengaluru, workers sit in front of five giant tabletop scanners, lining up books and flipping pages with foot pedals. The museum building houses the largest reference library for Gandhian philosophy in the state of Karnataka, and over the next year, the large assortment of books—including the collected works of Mahatma Gandhi, a translation of his autobiography, Experiments with Truth, into the Kannada language, and other rare items—will be digitized and their metadata recorded before they join the Servants of Knowledge (SoK) collection on the Internet Archive.  This digitization push is just the latest for the SoK, which was established about four years ago with a volunteer effort to preserve hard-to-find resources. It has since expanded to include partnerships with various libraries and archives throughout India. Today, the SoK collection is a searchable library of books, speeches, magazines, newspapers, palm leaf manuscripts, audio, and film from and about India in over 15 languages. The collection is a truly open digital library containing public-domain and out-of-copyright works on science, literature, law, politics, history, religion, music, and folklore, among many other topics. All content is open access, searchable, downloadable, and accessible to visually challenged people using text-to-speech tools. Volunteers and staff continue to expand the collection, scanning about 1.4 million pages per month in various locations across Bengaluru, and more collaborations are in the works. The collection is an effort to make up for the scarcity of library resources in India. There are about 50,000 public-funded libraries in this country of over 1.4 billion people, according to the Raja Rammohun Roy Library Foundation, a group established by the Indian government to promote the public-library movement there. Village and tribal libraries may contain just a few thousand books, compared with a median 77,000 books in each state’s central library and 24,000 in every district library, according to a 2018 report by the foundation. Some libraries have lost their collections to fire. A number of books have been ruined by neglect. Others have gone missing. Some people are finding their accounts permanently blocked Moreover, most public libraries aren’t freely accessible to the public. “Getting access to many of our public libraries is so difficult, and after a point people will give up asking for access. That’s the case in many of our public-funded educational institutes too,” says Arul George Scaria, an associate professor at the National Law School of India University Bengaluru, who studies intellectual-property law. One of the best ways to liberate access to these libraries, he says, is through digitization. Technologist Omshivaprakash H L felt the acute lack of such resources when he needed references for writing Wikipedia articles in Kannada, a southwestern Indian language. Around 2019, he heard that Carl Malamud, who runs Public Resource, a registered US charity, was already archiving books like Gandhi’s Hind Swaraj collection on Indian self-rule and works of the Indian government in the public domain. “I also knew that he used to buy a lot of these books from secondhand bookstores and take them to the US to get them digitized,” says Omshivaprakash.  Public Resource had been working with the Indian Academy of Sciences, Bengaluru, to digitize its books using a scanner provided by the Internet Archive, but the efforts had tapered off. Omshivaprakash proposed engaging community members to help. During the weekends, these volunteers began scanning some of the books Omshivaprakash had and that Malamud had bought. “Carl really understood the idea of community collaboration, the idea of local language technology that we needed, and the kind of impact we were creating,” Omshivaprakash says. The scanners use a V-shaped cradle to hold the books and two DSLR cameras to capture the pages in high resolution. The device is based on the Internet Archive’s scanner but was reengineered by Omshivaprakash and manufactured in India at a lower cost. Each worker can scan about 800 pages an hour.  The more crucial parts of the operation happen after the scan: volunteers make sure to apply accurate metadata to make the scans findable on the Internet Archive, and optical character recognition, which has been fine-tuned to work better for a range of Indian language scripts, makes the text searchable and accessible through text-to-speech programs. Public Resource funds the SoK project, and Omshivaprakash manages the operation, with the help of staff and volunteers. Collaborators have come through social media and word of mouth. For instance, a community member and Kannada teacher named Chaya Acharya approached Omshivaprakash with newspaper clippings of work by her grandfather, the renowned journalist and writer Pavem Acharya, who wrote articles on science and social issues as well as satirical essays. Unexpectedly, she found more articles by her grandfather in the existing Servants of Knowledge collection. “Simply by searching his name, I got many more articles from the archive,” she says. She began collecting copies of Kasturi, a prominent Kannada monthly magazine that Pavem Acharya had edited from 1952 to early 1975, and gave them to Omshivaprakash for digitizing. The old issues of the magazine contain rare writings and translations by popular Kannada authors, such as Indirabai by Gulavadi Venkata Rao, regarded as the first modern novel in Kannada, and a Kannada translation of Edgar Allan Poe’s famous short story “The Gold-Bug.” This is all part of a vision of a public library on the internet as “a bottom-up, grassroots thing,” Malamud says. “It’s a bunch of people teaching each other. We just want to keep scanning and making [these materials] available to people. It’s not a grand goal or single aim.  “It’s what we do for a living,” he says. “We have done it for years, and we are gonna keep doing it for years.” Ananya is a freelance science and technology journalist based in Bengaluru, India. ","On a bright sunny day in August , in a second-floor room at the Gandhi Bhavan Museum in Bengaluru , workers sit in front of five giant tabletop scanners , lining up books and flipping pages with foot pedals . The museum building houses the largest reference library for Gandhian philosophy in the state of Karnataka , and over the next year , the large assortment of books—including the collected works of Mahatma Gandhi , a translation of his autobiography , Experiments with Truth , into the Kannada language , and other rare items—will be digitized and their metadata recorded before they join the Servants of Knowledge ( SoK ) collection on the Internet Archive . This digitization push is just the latest for the SoK , which was established about four years ago with a volunteer effort to preserve hard-to-find resources . It has since expanded to include partnerships with various libraries and archives throughout India . Today , the SoK collection is a searchable library of books , speeches , magazines , newspapers , palm leaf manuscripts , audio , and film from and about India in over 15 languages . The collection is a truly open digital library containing public-domain and out-of-copyright works on science , literature , law , politics , history , religion , music , and folklore , among many other topics . All content is open access , searchable , downloadable , and accessible to visually challenged people using text-to-speech tools . Volunteers and staff continue to expand the collection , scanning about 1.4 million pages per month in various locations across Bengaluru , and more collaborations are in the works . The collection is an effort to make up for the scarcity of library resources in India . There are about 50,000 public-funded libraries in this country of over 1.4 billion people , according to the Raja Rammohun Roy Library Foundation , a group established by the Indian government to promote the public-library movement there . Village and tribal libraries may contain just a few thousand books , compared with a median 77,000 books in each state ’ s central library and 24,000 in every district library , according to a 2018 report by the foundation . Some libraries have lost their collections to fire . A number of books have been ruined by neglect . Others have gone missing . Some people are finding their accounts permanently blocked Moreover , most public libraries aren ’ t freely accessible to the public . “ Getting access to many of our public libraries is so difficult , and after a point people will give up asking for access . That ’ s the case in many of our public-funded educational institutes too , ” says Arul George Scaria , an associate professor at the National Law School of India University Bengaluru , who studies intellectual-property law . One of the best ways to liberate access to these libraries , he says , is through digitization . Technologist Omshivaprakash H L felt the acute lack of such resources when he needed references for writing Wikipedia articles in Kannada , a southwestern Indian language . Around 2019 , he heard that Carl Malamud , who runs Public Resource , a registered US charity , was already archiving books like Gandhi ’ s Hind Swaraj collection on Indian self-rule and works of the Indian government in the public domain . “ I also knew that he used to buy a lot of these books from secondhand bookstores and take them to the US to get them digitized , ” says Omshivaprakash . Public Resource had been working with the Indian Academy of Sciences , Bengaluru , to digitize its books using a scanner provided by the Internet Archive , but the efforts had tapered off . Omshivaprakash proposed engaging community members to help . During the weekends , these volunteers began scanning some of the books Omshivaprakash had and that Malamud had bought . “ Carl really understood the idea of community collaboration , the idea of local language technology that we needed , and the kind of impact we were creating , ” Omshivaprakash says . The scanners use a V-shaped cradle to hold the books and two DSLR cameras to capture the pages in high resolution . The device is based on the Internet Archive ’ s scanner but was reengineered by Omshivaprakash and manufactured in India at a lower cost . Each worker can scan about 800 pages an hour . The more crucial parts of the operation happen after the scan : volunteers make sure to apply accurate metadata to make the scans findable on the Internet Archive , and optical character recognition , which has been fine-tuned to work better for a range of Indian language scripts , makes the text searchable and accessible through text-to-speech programs . Public Resource funds the SoK project , and Omshivaprakash manages the operation , with the help of staff and volunteers . Collaborators have come through social media and word of mouth . For instance , a community member and Kannada teacher named Chaya Acharya approached Omshivaprakash with newspaper clippings of work by her grandfather , the renowned journalist and writer Pavem Acharya , who wrote articles on science and social issues as well as satirical essays . Unexpectedly , she found more articles by her grandfather in the existing Servants of Knowledge collection . “ Simply by searching his name , I got many more articles from the archive , ” she says . She began collecting copies of Kasturi , a prominent Kannada monthly magazine that Pavem Acharya had edited from 1952 to early 1975 , and gave them to Omshivaprakash for digitizing . The old issues of the magazine contain rare writings and translations by popular Kannada authors , such as Indirabai by Gulavadi Venkata Rao , regarded as the first modern novel in Kannada , and a Kannada translation of Edgar Allan Poe ’ s famous short story “ The Gold-Bug. ” This is all part of a vision of a public library on the internet as “ a bottom-up , grassroots thing , ” Malamud says . “ It ’ s a bunch of people teaching each other . We just want to keep scanning and making [ these materials ] available to people . It ’ s not a grand goal or single aim . “ It ’ s what we do for a living , ” he says . “ We have done it for years , and we are gon na keep doing it for years. ” Ananya is a freelance science and technology journalist based in Bengaluru , India .","['bright', 'sunny', 'day', 'secondfloor', 'room', 'worker', 'sit', 'front', 'giant', 'tabletop', 'scanner', 'line', 'book', 'flip', 'page', 'foot', 'pedal', 'museum', 'building', 'house', 'large', 'reference', 'library', 'gandhian', 'philosophy', 'state', 'next', 'year', 'large', 'assortment', 'book', 'include', 'collect', 'work', 'translation', 'autobiography', 'experiment', 'truth', 'language', 'rare', 'item', 'digitize', 'metadata', 'record', 'join', 'servant', 'knowledge', 'collection', 'internet', 'archive', 'digitization', 'push', 'late', 'sok', 'establish', 'year', 'ago', 'volunteer', 'effort', 'preserve', 'hardtofind', 'resource', 'expand', 'include', 'partnership', 'various', 'library', 'archive', 'today', 'collection', 'searchable', 'library', 'book', 'speech', 'magazine', 'newspaper', 'palm', 'leaf', 'manuscript', 'audio', 'film', 'language', 'collection', 'truly', 'open', 'digital', 'library', 'contain', 'publicdomain', 'outofcopyright', 'work', 'science', 'literature', 'law', 'politic', 'history', 'religion', 'music', 'folklore', 'many', 'topic', 'content', 'open', 'access', 'searchable', 'downloadable', 'accessible', 'visually', 'challenge', 'people', 'use', 'texttospeech', 'tool', 'volunteer', 'staff', 'continue', 'expand', 'collection', 'scan', 'page', 'month', 'various', 'location', 'bengaluru', 'collaboration', 'work', 'collection', 'effort', 'make', 'scarcity', 'library', 'resource', 'publicfunde', 'library', 'country', 'people', 'accord', 'raja', 'group', 'establish', 'indian', 'government', 'promote', 'publiclibrary', 'movement', 'village', 'tribal', 'library', 'contain', 'book', 'compare', 'median', 'book', 'state', 'central', 'library', 'district', 'library', 'accord', 'report', 'foundation', 'library', 'lose', 'collection', 'fire', 'number', 'book', 'ruin', 'neglect', 'go', 'miss', 'people', 'find', 'account', 'permanently', 'block', 'moreover', 'public', 'library', 'freely', 'accessible', 'public', 'get', 'access', 'many', 'public', 'library', 'difficult', 'point', 'people', 'give', 'ask', 'access', 'case', 'many', 'publicfunde', 'educational', 'institute', 'say', 'associate', 'professor', 'national', 'law', 'school', 'study', 'law', 'good', 'way', 'liberate', 'access', 'library', 'say', 'digitization', 'technologist', 'omshivaprakash', 'l', 'feel', 'acute', 'lack', 'resource', 'need', 'reference', 'write', 'wikipedia', 'article', 'southwestern', 'indian', 'language', 'hear', 'run', 'public', 'resource', 'register', 'charity', 'already', 'archive', 'book', 'hind', 'swaraj', 'collection', 'indian', 'selfrule', 'work', 'indian', 'government', 'public', 'domain', 'also', 'know', 'use', 'buy', 'lot', 'book', 'secondhand', 'bookstore', 'take', 'get', 'digitize', 'say', 'omshivaprakash', 'public', 'resource', 'work', 'bengaluru', 'digitize', 'book', 'use', 'scanner', 'provide', 'internet', 'archive', 'effort', 'taper', 'omshivaprakash', 'propose', 'engage', 'community', 'member', 'help', 'weekend', 'volunteer', 'begin', 'scan', 'book', 'omshivaprakash', 'malamud', 'buy', 'really', 'understand', 'idea', 'community', 'collaboration', 'idea', 'local', 'language', 'technology', 'need', 'kind', 'impact', 'create', 'omshivaprakash', 'say', 'scanner', 'use', 'vshape', 'cradle', 'hold', 'book', 'dslr', 'camera', 'capture', 'page', 'high', 'resolution', 'device', 'base', 'internet', 'archive', 'scanner', 'reengineere', 'omshivaprakash', 'manufacture', 'low', 'cost', 'worker', 'scan', 'page', 'hour', 'crucial', 'part', 'operation', 'happen', 'scan', 'volunteer', 'make', 'sure', 'apply', 'accurate', 'make', 'scan', 'findable', 'internet', 'archive', 'optical', 'character', 'recognition', 'finetune', 'work', 'well', 'range', 'indian', 'language', 'script', 'make', 'text', 'searchable', 'accessible', 'program', 'public', 'resource', 'fund', 'sok', 'project', 'omshivaprakash', 'manage', 'operation', 'help', 'staff', 'volunteer', 'collaborator', 'come', 'social', 'medium', 'word', 'mouth', 'instance', 'community', 'member', 'teacher', 'name', 'acharya', 'approach', 'omshivaprakash', 'newspaper', 'clipping', 'work', 'grandfather', 'renowned', 'journalist', 'writer', 'acharya', 'write', 'article', 'science', 'social', 'issue', 'well', 'satirical', 'essay', 'unexpectedly', 'find', 'article', 'grandfather', 'exist', 'servant', 'knowledge', 'collection', 'simply', 'search', 'name', 'get', 'many', 'article', 'archive', 'say', 'begin', 'collect', 'copy', 'prominent', 'monthly', 'magazine', 'pavem', 'acharya', 'edit', 'early', 'give', 'omshivaprakash', 'digitize', 'old', 'issue', 'magazine', 'contain', 'rare', 'writing', 'translation', 'popular', 'author', 'indirabai', 'regard', 'first', 'modern', 'novel', 'translation', 'famous', 'short', 'story', 'part', 'vision', 'public', 'library', 'internet', 'bottomup', 'grassroots', 'thing', 'malamud', 'say', 'bunch', 'people', 'teach', 'want', 'keep', 'scan', 'make', 'material', 'available', 'people', 'grand', 'goal', 'single', 'aim', 'living', 'say', 'year', 'go', 'keep', 'year', 'freelance', 'science', 'technology', 'journalist', 'base']","<p>The Servants of Knowledge collection on the Internet Archive is an effort to make up for the lack of library resources in India.</p>
"
How this Turing Award–winning researcher became a legendary academic advisor,https://www.technologyreview.com/2023/10/24/1081478/manuel-blum-theoretical-computer-science-turing-award-academic-advisor/,2023-10-24,"Every academic field has its superstars. But a rare few achieve superstardom not just by demonstrating individual excellence but also by consistently producing future superstars. A notable example of such a legendary doctoral advisor is the Princeton physicist John Archibald Wheeler. A dissertation was once written about his mentorship, and he advised Richard Feynman, Kip Thorne, Hugh Everett (who proposed the “many worlds” theory of quantum mechanics), and a host of others who could collectively staff a top-tier physics department. In ecology, there is Bob Paine, who discovered that certain “keystone species” have an outsize impact on the environment and started a lineage of influential ecologists. And in journalism, there is John McPhee, who has taught generations of accomplished journalists at Princeton since 1975.  Computer science has its own such figure: Manuel Blum, who won the 1995 Turing Award—the Nobel Prize of computer science. Blum’s métier is theoretical computer science, a field that often escapes the general public’s radar. But you certainly have come across one of Blum’s creations: the “Completely Automated Public Turing test to tell Computers and Humans Apart,” better known as thecaptcha—a test designed to distinguish humans from bots online. Scientists hope to leverage mRNA for a bevy of vaccines and therapeutics.  “I don’t know what his secret has been. But he has been a tremendously successful advisor,” says Michael Sipser, a theoretical computer scientist at MIT who was advised by Blum, referring to the “extraordinary number of PhD students” who have worked with him and then gone on to make an impact in the field. “It is extraordinary in the literal sense of that word—outside the ordinary.” Three of Blum’s students have also won Turing Awards; many have received other high honors in theoretical computer science, such as the Gödel Prize and the Knuth Prize; and more than 20 hold professorships at top computer science departments. There are five, for example, at MIT and three at Carnegie Mellon University (where there were four until one left to found Duolingo).  Blum is also distinguished by the great plurality of subfields that his students work in. When Mor Harchol-Balter, a professor of computer science at Carnegie Mellon, arrived at the University of California, Berkeley, as a PhD student, she quickly realized that she wanted to work with him. “Manuel was warm, smiling, and just immediately emanated kindness,” Harchol-Balter told me. Her specialty, queueing theory, had little overlap with Blum’s, but he took her on. “Every professor I know, if you start working on what’s way out of their area, they would tell you to go find somebody else,” she said. “Not Manuel.”   A few months ago, as I was reading about some of the most significant yet counterintuitive ideas in modern theoretical computer science, I realized that the vast majority of the researchers responsible for that work had been advised by Blum. I wondered whether there might be some formula to his success. Of course, it’s presumptuous to think such an intimately human process can be distilled into an algorithm. However, conversations with his students gave me a sense of his approach and revealed consistent themes. Many spoke warmly of him: I often heard some version of “I could talk about Manuel all day” or “Manuel is my favorite topic of conversation.” The finer points of mentorship aside, what I learned was at least proof that kindness can beget greatness.  Manuel Blum is married to Lenore Blum, an accomplished mathematician and computer scientist, who has also been at the forefront of promoting diversity in math and computing (among other things, she founded America’s first computer science department at a women’s college and helped CMU’s computer science department achieve 50-50 gender parity). They are both now emeritus professors at CMU and Manuel Blum is an emeritus professor at UC Berkeley; they split their time between the two coasts.  One day in August, I joined the couple for breakfast at their house in Pittsburgh. Breezy in his manner, Blum, at 85, still has a schoolboy’s smile and frequently erupts into a resonant laugh; he is charismatic in a way typical of people who are utterly oblivious to their charisma. (When he says “WON-derful,” which he frequently does, you can practically hear “WON” in all caps.)  The Blums, who recently celebrated their 62nd anniversary, still shuttlecock research ideas, enthuse over emails from their former students, and complete each other’s memories—some dating from their life in Venezuela, where they met as kids.  Manuel Blum was born in 1938 in Caracas to Jewish parents who had moved from Romania. His first language was German, which his parents spoke at home. But when they moved to the Bronx, his family realized that people did not want to hear German spoken. The year was 1942, and the country was at war. After switching to Spanish at home, he quickly lost his fluency in German. But when he had to learn English for school, he soon forgot Spanish as well. At one point, Blum says, he was listening to both languages but found himself understanding neither. “I remember thinking to myself, ‘Very interesting—I don’t have a language. I couldn’t express myself through language. How was it that I was able to think?’” he told me. In a lucid moment of metacognition—an act that befits a future theorist of abstract concepts—he realized: You don’t need language to think. “He is completely original and goes off and does what he thinks is interesting and important. And often it turns out to be something really significant.” Likely because of his language difficulties, Blum’s second-grade teacher warned his mother that while he might manage to complete high school, he might not go to college. “But I wanted to be smarter. So I asked my father, ‘What can I do to get smarter?’” His father answered that if he understood how the brain works, he could be smart. The conversation marked the inception of Blum’s interest in studying consciousness (something he and Lenore Blum now research full-time, often assisted by their son, the computer scientist Avrim Blum).  Blum was ultimately accepted to MIT, but he struggled the first year, until a friend noticed that his approach to studying physics—owing to Blum’s training at a military academy he went to before college—was heavy on memorization. Blum recalls his friend saying, “You don’t memorize. You memorize only ‘F = ma’ and a few things like that. When you need a formula, you derive it.” Soon, his grades started climbing. “I went from being a Xerox machine to being a thinker. I really enjoyed thinking,” he says. To pursue his interest in the brain, Blum took a course that involved reading multiple volumes of the standard edition of Freud’s works. But they didn’t offer much in the way of satisfactory answers. Then his professor told him that he should introduce himself to Warren S. McCulloch, known for very early research on neural networks and pioneering work in cybernetics. Blum read some of McCulloch’s papers and was able to prove a couple of theorems in mathematical biophysics, and McCulloch took him on in his MIT lab. “A wonderful person. A magnanimous person. Anything I wanted to do, he was supportive,” Blum says.  McCulloch’s lab focused on both the rigorous mathematical work of modeling the neuron and the experimental process of studying the brain to understand how it functions. But what Blum couldn’t study in the lab was consciousness. The topic was taboo at the time. Many felt that subjective mental phenomena weren’t fit for scientific inquiry, and there were few tools available in any case. (The fMRI, for example, which is an imaging technique that maps brain activity, wouldn’t be developed until 1990.)  Blum would revisit the topic occasionally as he transitioned away from electrical engineering to mathematics and computer science in graduate school. As he pursued his graduate work at MIT, he became captivated by a branch of theoretical computer science known as recursive function theory—now more commonly referred to as computability theory—and began searching for a thesis advisor. Soon, he found Marvin Minsky, the mathematician and computer scientist, who was a pioneer of artificial intelligence. Minsky (who had an office full of mechanical hands) often dropped by McCulloch’s lab to demonstrate his new machines and discuss mathematical problems.  After studying computational complexity and computability for his thesis, Blum received his PhD in 1964. At the time, computational complexity theory represented the hinterlands of computer science. It wasn’t until 1971 that Stephen Cook formulated the foundational question of the field, “P vs. NP”—which essentially asks whether every problem whose solution can be checked quickly can also be solved quickly.  But Blum found a productive home in Berkeley’s electrical engineering and computer science department. At MIT, he had helped form the contours of computational complexity theory. At Berkeley, he showed how this highly abstract field could also have useful applications in areas such as cryptography and program checking—a method that uses an algorithm to verify the correctness of a computer program. The kinds of questions Blum poses read like paradoxes and have a somewhat playful quality, making complexity theory and cryptography sound almost like a subgenre of sci-fi. “He is completely original and goes off and does what he thinks is interesting and important. And often it turns out to be something really significant,” Sipser told me.  In his seminal paper “Coin Flipping by Telephone,” the question that he poses is: “Alice and Bob want to flip a coin by telephone. (They have just divorced, live in different cities, and want to decide who gets the car.)” Let’s say that Alice calls “heads” and Bob says she lost; how does she trust that he is being truthful? And how could Bob trust Alice if the situation were reversed? What sounds like a riddle addresses a fundamental problem in cryptography: How can two parties engage in trustworthy exchanges over a communication channel in such a way that neither party can cheat?  Blum showed that this can be achieved using the concept of “commitment.” In a simplified analogy, the idea is that Alice gives Bob a locked box with her prediction inside, but without the key. This prevents Alice from altering her prediction and stops Bob from discovering Alice’s guess prematurely. Once Bob tosses the coin, Alice hands over the key to open the box.  When you ask Blum about the secrets of good mentorship, he reacts with a sheepish head scratch, attributing his students’ success to their own talents. “Students come up with wonderful ideas, and people don’t realize how wonderful they are. The only thing I can say is that, more than most, I really enjoy the ideas that the students have,” he told me. “I have learned from each of them.”  His response left me puzzled, especially after I heard from his students that Blum never criticized their ideas or prescribed research directions. Offering full autonomy and boundless encouragement sounded wonderful in theory, but I was mystified as to how it worked in practice—how did students receive the occasional course correction or hyper-specific advice that is often essential in academic pursuits? Still, it’s not that he was dodging my question. He is not so much a magician who refuses to give away his tricks as one who is himself astonished by what has been conjured around him. One thing I came to understand about Blum’s advising style is that when he says “Students are here to teach me,” he truly means it, with all that entails. While it’s easy to pay lip service to the principle of “treating a student as a colleague,” Ryan Williams, a professor of computer science at MIT who studied with Blum, told me that working together made him really feel like one. What this means, in concrete terms, is that Blum imparted to his students a sense of pedagogical responsibility: he was really expecting to learn from them at every weekly meeting, which in turn meant they had to understand their ideas to the bone.  “During my first few months of working with him, I thought he was testing me. And then I realized that was just him,” Russell Impagliazzo, a professor of computer science at the University of California, San Diego, told me. “You had to learn how to say things so that Manuel could understand them. And that’s the most valuable skill that he gives his students, like the skill of learning to swim by being thrown into a pool: the ability to translate what you’re saying into more concrete terms. This skill proves invaluable when you are teaching a class or writing a grant proposal.” Former students describe Blum as unwaveringly positive, saying he had other ways besides criticism to steer them away from dead ends. “He is always smiling, but you can see he smiles wider when he likes something. And oh, we wanted that big smile,” says Ronitt Rubinfeld, a professor of electrical engineering and computer science at MIT. What would it be like to have someone like Blum in your corner? What kinds of audacious ideas can take root when someone listens to you with absolutely no judgment? Behind the general positivity, Rubinfeld says, is a fine taste for interesting ideas. Students could trust they were being guided in the right direction. Come up with a boring idea? Blum, who is known for his terrible memory, would have mostly forgotten it by your next meeting.  When Harchol-Balter was in graduate school, she says, Blum never told her what to work on and instead guided her by means of questions: “Manuel is fantastic at asking questions. Manuel excels at asking questions.” Blum also “really makes sure that each student has a special area to develop,” Lenore Blum told me. “I don’t think he’s asked a student to ever do the next iteration of someone else’s work,” she said. “But he’ll say, ‘Work with me, and we’ll do something brand new.’” Working on a new idea is risky. But Blum’s encouragement, coupled with his track record of spotting fruitful lines of inquiry, gave his students confidence to keep going in bold directions while enduring criticism and self-doubt. “There’s a huge difference [between] Manuel’s advising style and everyone else’s in the world,” says Impagliazzo. “Manuel’s advising style is simply to listen to you and make you seem really, really important. Like what you’re doing is the most amazing thing in the world.”  Harchol-Balter says this is the magic she is now trying to emulate with her students. “Whenever I had an idea, whatever it was, he somehow made me feel like this was the most brilliant idea that had ever been invented,” she remembers. She felt that every idea could be “a multimillion-dollar breakthrough,” which allowed her to stay committed to her line of research, undeterred by external influences or trends. “He creates this feeling of supreme confidence—not just confidence, but like, ‘You. Are. Brilliant,’” she adds. “Having somebody beside you all those six years, when you’re feeling the most vulnerable, constantly boosting your confidence … It’s amazing. And that’s why his students are so great.” Astronomy is leading the way in making science more accessible through sonification—and the results sound amazing. Excellence in academia, as in many other fields, is about both what you do and how you do it. You need to identify a promising topic and have the technical ability to execute it. A technically flawless idea without original insight can be trivial; a radically original idea without proper execution might never fully develop, while a bold idea powered by misplaced confidence could hit a dead end.  The psychological reassurance students get from Blum may come in part from his superhuman level of aplomb. “He never seems stressed out,” says his son, Avrim Blum. “In the real world, there are deadlines and stresses, but he never showed any of that. At least I never saw it.” I’m still awed by his ability to mask inner turbulence—something that affects everyone—so well that it remains invisible even to his closest observers, including his own son. It’s a source of stability that students can rely on throughout their graduate studies. “I was more comfortable and more relaxed in grad school because I felt like he had things under control for me,” Williams told me. “If there were any difficulties, he would help. He had my back. He was going to sort things out.”  Speaking with Blum’s students, I felt a pang of jealousy. What would it be like to have someone like Blum in your corner during your most vulnerable moments? And how many direct criticisms you’ve faced could have been reformulated into questions? What kinds of audacious ideas can take root when someone listens to you with absolutely no judgment?  But even as Blum’s students claim they are still bewildered by the “magic” and “mystery” of their advisor’s approach, they have become accomplished teachers and advisors in their own right. Umesh Vazirani, a theoretical computer scientist at Berkeley, told me that he has thought a lot about Blum’s secrets. He said the essence can be expressed this way: “You respect every student, and you let them develop into whatever they want to be.” Vazirani, who has advised a number of superstars in the field himself, believes that in education, “the most important thing is not to break anything. Cause no damage.” The potency of the Blumian approach to advising isn’t domain specific, as illustrated by George Saunders’s reflections on his writing teacher, Tobias Wolff. Writing teachers have “so much power,” Saunders has written: They could mock us, disregard us, use us to prop themselves up. But ourteachers, if they are good, instead do something almost holy, which we neverforget: they take us seriously. They accept us as new members of the guild.They tolerate the under-wonderful stories we write, the dopy things we say, ourshaky-legged aesthetic theories, our posturing, because they have been therethemselves.  We say: I think I might be a writer.  They say: Good for you. Proceed.   On my last day in Pittsburgh, I noticed a photo of Blum’s old advisor, Warren S. McCulloch, behind Blum’s desk in his home office. It was in a prominent place where someone else might’ve chosen to display a family heirloom or showcase an autographed photo of himself shaking a president’s hand. (McCulloch died in 1969, only a few years after Blum began his professorship.) Out of curiosity, I pointed out the photo’s prominent position. “Yes, because he is always with me,” Blum replied. “Warren was Manuel’s spiritual father in every way,” added Lenore. As I made my way back to the airport, I remembered a book called Surviving Death, by the philosopher Mark Johnston. In the book, Johnston postulates that a good person could “quite literally” survive death by redirecting self-interest toward the well-being of future people. This forfeiture doesn’t spell the dissolution of the self but, rather, the expansion of it, allowing the person to live on in the “onward rush of humankind.” A line from the book unfolded, with a time-release effect, in my head: “Every time a baby is born, a good person acquires a new face.”  Behind every one of Blum’s knowing smiles, it may well have been McCulloch himself, nodding, imparting a blessing: “Wonderful idea. Proceed.”  Sheon Han is a writer based in Palo Alto, California.","Every academic field has its superstars . But a rare few achieve superstardom not just by demonstrating individual excellence but also by consistently producing future superstars . A notable example of such a legendary doctoral advisor is the Princeton physicist John Archibald Wheeler . A dissertation was once written about his mentorship , and he advised Richard Feynman , Kip Thorne , Hugh Everett ( who proposed the “ many worlds ” theory of quantum mechanics ) , and a host of others who could collectively staff a top-tier physics department . In ecology , there is Bob Paine , who discovered that certain “ keystone species ” have an outsize impact on the environment and started a lineage of influential ecologists . And in journalism , there is John McPhee , who has taught generations of accomplished journalists at Princeton since 1975 . Computer science has its own such figure : Manuel Blum , who won the 1995 Turing Award—the Nobel Prize of computer science . Blum ’ s métier is theoretical computer science , a field that often escapes the general public ’ s radar . But you certainly have come across one of Blum ’ s creations : the “ Completely Automated Public Turing test to tell Computers and Humans Apart , ” better known as thecaptcha—a test designed to distinguish humans from bots online . Scientists hope to leverage mRNA for a bevy of vaccines and therapeutics . “ I don ’ t know what his secret has been . But he has been a tremendously successful advisor , ” says Michael Sipser , a theoretical computer scientist at MIT who was advised by Blum , referring to the “ extraordinary number of PhD students ” who have worked with him and then gone on to make an impact in the field . “ It is extraordinary in the literal sense of that word—outside the ordinary. ” Three of Blum ’ s students have also won Turing Awards ; many have received other high honors in theoretical computer science , such as the Gödel Prize and the Knuth Prize ; and more than 20 hold professorships at top computer science departments . There are five , for example , at MIT and three at Carnegie Mellon University ( where there were four until one left to found Duolingo ) . Blum is also distinguished by the great plurality of subfields that his students work in . When Mor Harchol-Balter , a professor of computer science at Carnegie Mellon , arrived at the University of California , Berkeley , as a PhD student , she quickly realized that she wanted to work with him . “ Manuel was warm , smiling , and just immediately emanated kindness , ” Harchol-Balter told me . Her specialty , queueing theory , had little overlap with Blum ’ s , but he took her on . “ Every professor I know , if you start working on what ’ s way out of their area , they would tell you to go find somebody else , ” she said . “ Not Manuel. ” A few months ago , as I was reading about some of the most significant yet counterintuitive ideas in modern theoretical computer science , I realized that the vast majority of the researchers responsible for that work had been advised by Blum . I wondered whether there might be some formula to his success . Of course , it ’ s presumptuous to think such an intimately human process can be distilled into an algorithm . However , conversations with his students gave me a sense of his approach and revealed consistent themes . Many spoke warmly of him : I often heard some version of “ I could talk about Manuel all day ” or “ Manuel is my favorite topic of conversation. ” The finer points of mentorship aside , what I learned was at least proof that kindness can beget greatness . Manuel Blum is married to Lenore Blum , an accomplished mathematician and computer scientist , who has also been at the forefront of promoting diversity in math and computing ( among other things , she founded America ’ s first computer science department at a women ’ s college and helped CMU ’ s computer science department achieve 50-50 gender parity ) . They are both now emeritus professors at CMU and Manuel Blum is an emeritus professor at UC Berkeley ; they split their time between the two coasts . One day in August , I joined the couple for breakfast at their house in Pittsburgh . Breezy in his manner , Blum , at 85 , still has a schoolboy ’ s smile and frequently erupts into a resonant laugh ; he is charismatic in a way typical of people who are utterly oblivious to their charisma . ( When he says “ WON-derful , ” which he frequently does , you can practically hear “ WON ” in all caps . ) The Blums , who recently celebrated their 62nd anniversary , still shuttlecock research ideas , enthuse over emails from their former students , and complete each other ’ s memories—some dating from their life in Venezuela , where they met as kids . Manuel Blum was born in 1938 in Caracas to Jewish parents who had moved from Romania . His first language was German , which his parents spoke at home . But when they moved to the Bronx , his family realized that people did not want to hear German spoken . The year was 1942 , and the country was at war . After switching to Spanish at home , he quickly lost his fluency in German . But when he had to learn English for school , he soon forgot Spanish as well . At one point , Blum says , he was listening to both languages but found himself understanding neither . “ I remember thinking to myself , ‘ Very interesting—I don ’ t have a language . I couldn ’ t express myself through language . How was it that I was able to think ? ’ ” he told me . In a lucid moment of metacognition—an act that befits a future theorist of abstract concepts—he realized : You don ’ t need language to think . “ He is completely original and goes off and does what he thinks is interesting and important . And often it turns out to be something really significant. ” Likely because of his language difficulties , Blum ’ s second-grade teacher warned his mother that while he might manage to complete high school , he might not go to college . “ But I wanted to be smarter . So I asked my father , ‘ What can I do to get smarter ? ’ ” His father answered that if he understood how the brain works , he could be smart . The conversation marked the inception of Blum ’ s interest in studying consciousness ( something he and Lenore Blum now research full-time , often assisted by their son , the computer scientist Avrim Blum ) . Blum was ultimately accepted to MIT , but he struggled the first year , until a friend noticed that his approach to studying physics—owing to Blum ’ s training at a military academy he went to before college—was heavy on memorization . Blum recalls his friend saying , “ You don ’ t memorize . You memorize only ‘ F = ma ’ and a few things like that . When you need a formula , you derive it. ” Soon , his grades started climbing . “ I went from being a Xerox machine to being a thinker . I really enjoyed thinking , ” he says . To pursue his interest in the brain , Blum took a course that involved reading multiple volumes of the standard edition of Freud ’ s works . But they didn ’ t offer much in the way of satisfactory answers . Then his professor told him that he should introduce himself to Warren S. McCulloch , known for very early research on neural networks and pioneering work in cybernetics . Blum read some of McCulloch ’ s papers and was able to prove a couple of theorems in mathematical biophysics , and McCulloch took him on in his MIT lab . “ A wonderful person . A magnanimous person . Anything I wanted to do , he was supportive , ” Blum says . McCulloch ’ s lab focused on both the rigorous mathematical work of modeling the neuron and the experimental process of studying the brain to understand how it functions . But what Blum couldn ’ t study in the lab was consciousness . The topic was taboo at the time . Many felt that subjective mental phenomena weren ’ t fit for scientific inquiry , and there were few tools available in any case . ( The fMRI , for example , which is an imaging technique that maps brain activity , wouldn ’ t be developed until 1990 . ) Blum would revisit the topic occasionally as he transitioned away from electrical engineering to mathematics and computer science in graduate school . As he pursued his graduate work at MIT , he became captivated by a branch of theoretical computer science known as recursive function theory—now more commonly referred to as computability theory—and began searching for a thesis advisor . Soon , he found Marvin Minsky , the mathematician and computer scientist , who was a pioneer of artificial intelligence . Minsky ( who had an office full of mechanical hands ) often dropped by McCulloch ’ s lab to demonstrate his new machines and discuss mathematical problems . After studying computational complexity and computability for his thesis , Blum received his PhD in 1964 . At the time , computational complexity theory represented the hinterlands of computer science . It wasn ’ t until 1971 that Stephen Cook formulated the foundational question of the field , “ P vs. NP ” —which essentially asks whether every problem whose solution can be checked quickly can also be solved quickly . But Blum found a productive home in Berkeley ’ s electrical engineering and computer science department . At MIT , he had helped form the contours of computational complexity theory . At Berkeley , he showed how this highly abstract field could also have useful applications in areas such as cryptography and program checking—a method that uses an algorithm to verify the correctness of a computer program . The kinds of questions Blum poses read like paradoxes and have a somewhat playful quality , making complexity theory and cryptography sound almost like a subgenre of sci-fi . “ He is completely original and goes off and does what he thinks is interesting and important . And often it turns out to be something really significant , ” Sipser told me . In his seminal paper “ Coin Flipping by Telephone , ” the question that he poses is : “ Alice and Bob want to flip a coin by telephone . ( They have just divorced , live in different cities , and want to decide who gets the car . ) ” Let ’ s say that Alice calls “ heads ” and Bob says she lost ; how does she trust that he is being truthful ? And how could Bob trust Alice if the situation were reversed ? What sounds like a riddle addresses a fundamental problem in cryptography : How can two parties engage in trustworthy exchanges over a communication channel in such a way that neither party can cheat ? Blum showed that this can be achieved using the concept of “ commitment. ” In a simplified analogy , the idea is that Alice gives Bob a locked box with her prediction inside , but without the key . This prevents Alice from altering her prediction and stops Bob from discovering Alice ’ s guess prematurely . Once Bob tosses the coin , Alice hands over the key to open the box . When you ask Blum about the secrets of good mentorship , he reacts with a sheepish head scratch , attributing his students ’ success to their own talents . “ Students come up with wonderful ideas , and people don ’ t realize how wonderful they are . The only thing I can say is that , more than most , I really enjoy the ideas that the students have , ” he told me . “ I have learned from each of them. ” His response left me puzzled , especially after I heard from his students that Blum never criticized their ideas or prescribed research directions . Offering full autonomy and boundless encouragement sounded wonderful in theory , but I was mystified as to how it worked in practice—how did students receive the occasional course correction or hyper-specific advice that is often essential in academic pursuits ? Still , it ’ s not that he was dodging my question . He is not so much a magician who refuses to give away his tricks as one who is himself astonished by what has been conjured around him . One thing I came to understand about Blum ’ s advising style is that when he says “ Students are here to teach me , ” he truly means it , with all that entails . While it ’ s easy to pay lip service to the principle of “ treating a student as a colleague , ” Ryan Williams , a professor of computer science at MIT who studied with Blum , told me that working together made him really feel like one . What this means , in concrete terms , is that Blum imparted to his students a sense of pedagogical responsibility : he was really expecting to learn from them at every weekly meeting , which in turn meant they had to understand their ideas to the bone . “ During my first few months of working with him , I thought he was testing me . And then I realized that was just him , ” Russell Impagliazzo , a professor of computer science at the University of California , San Diego , told me . “ You had to learn how to say things so that Manuel could understand them . And that ’ s the most valuable skill that he gives his students , like the skill of learning to swim by being thrown into a pool : the ability to translate what you ’ re saying into more concrete terms . This skill proves invaluable when you are teaching a class or writing a grant proposal. ” Former students describe Blum as unwaveringly positive , saying he had other ways besides criticism to steer them away from dead ends . “ He is always smiling , but you can see he smiles wider when he likes something . And oh , we wanted that big smile , ” says Ronitt Rubinfeld , a professor of electrical engineering and computer science at MIT . What would it be like to have someone like Blum in your corner ? What kinds of audacious ideas can take root when someone listens to you with absolutely no judgment ? Behind the general positivity , Rubinfeld says , is a fine taste for interesting ideas . Students could trust they were being guided in the right direction . Come up with a boring idea ? Blum , who is known for his terrible memory , would have mostly forgotten it by your next meeting . When Harchol-Balter was in graduate school , she says , Blum never told her what to work on and instead guided her by means of questions : “ Manuel is fantastic at asking questions . Manuel excels at asking questions. ” Blum also “ really makes sure that each student has a special area to develop , ” Lenore Blum told me . “ I don ’ t think he ’ s asked a student to ever do the next iteration of someone else ’ s work , ” she said . “ But he ’ ll say , ‘ Work with me , and we ’ ll do something brand new. ’ ” Working on a new idea is risky . But Blum ’ s encouragement , coupled with his track record of spotting fruitful lines of inquiry , gave his students confidence to keep going in bold directions while enduring criticism and self-doubt . “ There ’ s a huge difference [ between ] Manuel ’ s advising style and everyone else ’ s in the world , ” says Impagliazzo . “ Manuel ’ s advising style is simply to listen to you and make you seem really , really important . Like what you ’ re doing is the most amazing thing in the world. ” Harchol-Balter says this is the magic she is now trying to emulate with her students . “ Whenever I had an idea , whatever it was , he somehow made me feel like this was the most brilliant idea that had ever been invented , ” she remembers . She felt that every idea could be “ a multimillion-dollar breakthrough , ” which allowed her to stay committed to her line of research , undeterred by external influences or trends . “ He creates this feeling of supreme confidence—not just confidence , but like , ‘ You . Are . Brilliant , ’ ” she adds . “ Having somebody beside you all those six years , when you ’ re feeling the most vulnerable , constantly boosting your confidence … It ’ s amazing . And that ’ s why his students are so great. ” Astronomy is leading the way in making science more accessible through sonification—and the results sound amazing . Excellence in academia , as in many other fields , is about both what you do and how you do it . You need to identify a promising topic and have the technical ability to execute it . A technically flawless idea without original insight can be trivial ; a radically original idea without proper execution might never fully develop , while a bold idea powered by misplaced confidence could hit a dead end . The psychological reassurance students get from Blum may come in part from his superhuman level of aplomb . “ He never seems stressed out , ” says his son , Avrim Blum . “ In the real world , there are deadlines and stresses , but he never showed any of that . At least I never saw it. ” I ’ m still awed by his ability to mask inner turbulence—something that affects everyone—so well that it remains invisible even to his closest observers , including his own son . It ’ s a source of stability that students can rely on throughout their graduate studies . “ I was more comfortable and more relaxed in grad school because I felt like he had things under control for me , ” Williams told me . “ If there were any difficulties , he would help . He had my back . He was going to sort things out. ” Speaking with Blum ’ s students , I felt a pang of jealousy . What would it be like to have someone like Blum in your corner during your most vulnerable moments ? And how many direct criticisms you ’ ve faced could have been reformulated into questions ? What kinds of audacious ideas can take root when someone listens to you with absolutely no judgment ? But even as Blum ’ s students claim they are still bewildered by the “ magic ” and “ mystery ” of their advisor ’ s approach , they have become accomplished teachers and advisors in their own right . Umesh Vazirani , a theoretical computer scientist at Berkeley , told me that he has thought a lot about Blum ’ s secrets . He said the essence can be expressed this way : “ You respect every student , and you let them develop into whatever they want to be. ” Vazirani , who has advised a number of superstars in the field himself , believes that in education , “ the most important thing is not to break anything . Cause no damage. ” The potency of the Blumian approach to advising isn ’ t domain specific , as illustrated by George Saunders ’ s reflections on his writing teacher , Tobias Wolff . Writing teachers have “ so much power , ” Saunders has written : They could mock us , disregard us , use us to prop themselves up . But ourteachers , if they are good , instead do something almost holy , which we neverforget : they take us seriously . They accept us as new members of the guild.They tolerate the under-wonderful stories we write , the dopy things we say , ourshaky-legged aesthetic theories , our posturing , because they have been therethemselves . We say : I think I might be a writer . They say : Good for you . Proceed . On my last day in Pittsburgh , I noticed a photo of Blum ’ s old advisor , Warren S. McCulloch , behind Blum ’ s desk in his home office . It was in a prominent place where someone else might ’ ve chosen to display a family heirloom or showcase an autographed photo of himself shaking a president ’ s hand . ( McCulloch died in 1969 , only a few years after Blum began his professorship . ) Out of curiosity , I pointed out the photo ’ s prominent position . “ Yes , because he is always with me , ” Blum replied . “ Warren was Manuel ’ s spiritual father in every way , ” added Lenore . As I made my way back to the airport , I remembered a book called Surviving Death , by the philosopher Mark Johnston . In the book , Johnston postulates that a good person could “ quite literally ” survive death by redirecting self-interest toward the well-being of future people . This forfeiture doesn ’ t spell the dissolution of the self but , rather , the expansion of it , allowing the person to live on in the “ onward rush of humankind. ” A line from the book unfolded , with a time-release effect , in my head : “ Every time a baby is born , a good person acquires a new face. ” Behind every one of Blum ’ s knowing smiles , it may well have been McCulloch himself , nodding , imparting a blessing : “ Wonderful idea . Proceed. ” Sheon Han is a writer based in Palo Alto , California .","['academic', 'field', 'superstar', 'rare', 'achieve', 'superstardom', 'demonstrate', 'individual', 'excellence', 'also', 'consistently', 'produce', 'future', 'superstar', 'notable', 'example', 'legendary', 'doctoral', 'advisor', 'dissertation', 'write', 'mentorship', 'advise', 'thorne', 'propose', 'many', 'world', 'theory', 'quantum', 'mechanic', 'host', 'collectively', 'staff', 'toptier', 'physics', 'department', 'ecology', 'paine', 'discover', 'certain', 'specie', 'outsize', 'impact', 'environment', 'start', 'lineage', 'influential', 'ecologist', 'journalism', 'teach', 'generation', 'accomplished', 'journalist', 'computer', 'science', 'figure', 'win', 'ture', 'award', 'computer', 'science', 'métier', 'theoretical', 'computer', 'science', 'field', 'often', 'escape', 'general', 'public', 'radar', 'certainly', 'come', 'blum', 'creation', 'completely', 'automate', 'public', 'ture', 'test', 'tell', 'computer', 'human', 'apart', 'well', 'know', 'test', 'design', 'distinguish', 'human', 'bot', 'online', 'scientist', 'hope', 'leverage', 'mrna', 'bevy', 'vaccine', 'therapeutic', 'know', 'secret', 'tremendously', 'successful', 'advisor', 'say', 'theoretical', 'computer', 'scientist', 'advise', 'blum', 'refer', 'extraordinary', 'number', 'phd', 'student', 'work', 'go', 'make', 'impact', 'field', 'extraordinary', 'literal', 'sense', 'word', 'ordinary', 'blum', 'student', 'also', 'win', 'ture', 'award', 'many', 'receive', 'high', 'honor', 'theoretical', 'computer', 'science', 'gödel', 'hold', 'professorship', 'top', 'computer', 'science', 'department', 'example', 'mit', 'leave', 'find', 'duolingo', 'blum', 'also', 'distinguish', 'great', 'plurality', 'subfield', 'student', 'work', 'harcholbalter', 'professor', 'computer', 'science', 'carnegie', 'mellon', 'arrive', 'phd', 'student', 'quickly', 'realize', 'want', 'work', 'manuel', 'warm', 'smile', 'immediately', 'emanated', 'kindness', 'harcholbalter', 'tell', 'specialty', 'queue', 'theory', 'little', 'overlap', 'blum', 'take', 'professor', 'know', 'start', 'work', 'way', 'area', 'tell', 'go', 'find', 'else', 'say', 'manuel', 'month', 'ago', 'read', 'significant', 'counterintuitive', 'idea', 'modern', 'theoretical', 'computer', 'science', 'realize', 'vast', 'majority', 'researcher', 'responsible', 'work', 'advise', 'wonder', 'formula', 'success', 'course', 'presumptuous', 'think', 'intimately', 'human', 'process', 'distil', 'however', 'conversation', 'student', 'give', 'sense', 'approach', 'reveal', 'consistent', 'theme', 'many', 'speak', 'warmly', 'often', 'hear', 'version', 'talk', 'manuel', 'day', 'manuel', 'favorite', 'topic', 'conversation', 'fine', 'point', 'mentorship', 'aside', 'learn', 'least', 'proof', 'kindness', 'beget', 'greatness', 'manuel', 'married', 'lenore', 'blum', 'accomplished', 'mathematician', 'computer', 'scientist', 'also', 'forefront', 'promote', 'diversity', 'math', 'compute', 'thing', 'found', 'first', 'computer', 'woman', 'college', 'help', 'cmu', 'computer', 'department', 'achieve', 'gender', 'parity', 'emeritus', 'professor', 'cmu', 'manuel', 'emeritus', 'professor', 'split', 'time', 'coast', 'day', 'join', 'couple', 'breakfast', 'house', 'breezy', 'manner', 'blum', 'still', 'schoolboy', 'smile', 'frequently', 'erupt', 'resonant', 'laugh', 'charismatic', 'way', 'typical', 'people', 'utterly', 'oblivious', 'charisma', 'say', 'wonderful', 'frequently', 'practically', 'win', 'cap', 'blum', 'recently', 'celebrate', '62nd', 'anniversary', 'still', 'shuttlecock', 'research', 'idea', 'enthuse', 'email', 'former', 'student', 'complete', 'memory', 'date', 'life', 'meet', 'kid', 'bear', 'caraca', 'jewish', 'parent', 'move', 'first', 'language', 'german', 'parent', 'speak', 'home', 'move', 'bronx', 'family', 'realize', 'people', 'want', 'hear', 'german', 'speak', 'year', 'country', 'war', 'switch', 'spanish', 'home', 'quickly', 'lose', 'fluency', 'german', 'learn', 'school', 'soon', 'forget', 'spanish', 'well', 'point', 'blum', 'say', 'listen', 'language', 'find', 'understand', 'remember', 'think', 'interesting', 'language', 'express', 'language', 'able', 'think', 'tell', 'lucid', 'moment', 'metacognition', 'act', 'befit', 'future', 'theorist', 'abstract', 'concept', 'realize', 'need', 'language', 'think', 'completely', 'original', 'go', 'think', 'interesting', 'important', 'often', 'turn', 'really', 'significant', 'likely', 'language', 'difficulty', 'blum', 'secondgrade', 'teacher', 'warn', 'mother', 'manage', 'complete', 'high', 'school', 'go', 'college', 'want', 'smart', 'ask', 'father', 'get', 'smart', 'father', 'answer', 'understand', 'brain', 'work', 'smart', 'conversation', 'mark', 'inception', 'blum', 'interest', 'study', 'consciousness', 'lenore', 'blum', 'research', 'fulltime', 'often', 'assist', 'son', 'computer', 'scientist', 'blum', 'ultimately', 'accept', 'mit', 'struggle', 'first', 'year', 'friend', 'notice', 'approach', 'study', 'physic', 'owe', 'blum', 'training', 'military', 'academy', 'go', 'college', 'heavy', 'memorization', 'recall', 'friend', 'say', 'memorize', 'memorize', 'thing', 'need', 'formula', 'derive', 'soon', 'grade', 'start', 'climb', 'go', 'xerox', 'machine', 'thinker', 'really', 'enjoy', 'think', 'say', 'pursue', 'interest', 'brain', 'blum', 'take', 'course', 'involve', 'read', 'multiple', 'volume', 'freud', 'work', 'offer', 'much', 'way', 'satisfactory', 'answer', 'professor', 'tell', 'introduce', 'know', 'early', 'research', 'neural', 'network', 'pioneer', 'work', 'cybernetic', 'blum', 'read', 'paper', 'able', 'prove', 'couple', 'theorem', 'mathematical', 'biophysic', 'take', 'mit', 'lab', 'wonderful', 'person', 'magnanimous', 'person', 'want', 'supportive', 'blum', 'say', 'lab', 'focus', 'rigorous', 'mathematical', 'work', 'model', 'neuron', 'experimental', 'process', 'study', 'brain', 'understand', 'function', 'study', 'lab', 'consciousness', 'topic', 'taboo', 'time', 'many', 'feel', 'subjective', 'mental', 'phenomena', 'fit', 'scientific', 'inquiry', 'tool', 'available', 'case', 'fmri', 'example', 'imaging', 'technique', 'map', 'brain', 'activity', 'develop', 'blum', 'revisit', 'topic', 'occasionally', 'transition', 'away', 'electrical', 'engineering', 'mathematic', 'computer', 'science', 'graduate', 'school', 'pursue', 'graduate', 'work', 'mit', 'captivate', 'branch', 'theoretical', 'computer', 'science', 'know', 'recursive', 'function', 'theory', 'commonly', 'refer', 'computability', 'theory', 'begin', 'search', 'thesis', 'advisor', 'soon', 'find', 'minsky', 'mathematician', 'computer', 'scientist', 'pioneer', 'artificial', 'intelligence', 'minsky', 'office', 'full', 'mechanical', 'hand', 'often', 'drop', 'lab', 'demonstrate', 'new', 'machine', 'discuss', 'mathematical', 'problem', 'study', 'computational', 'complexity', 'computability', 'thesis', 'blum', 'receive', 'phd', 'time', 'computational', 'complexity', 'theory', 'represent', 'hinterland', 'computer', 'science', 'formulate', 'foundational', 'question', 'field', 'p', 'vs', 'np', 'essentially', 'ask', 'problem', 'solution', 'check', 'quickly', 'also', 'solve', 'quickly', 'find', 'productive', 'home', 'electrical', 'engineering', 'computer', 'help', 'form', 'contour', 'computational', 'complexity', 'theory', 'show', 'highly', 'abstract', 'field', 'also', 'useful', 'application', 'area', 'cryptography', 'program', 'checking', 'method', 'use', 'verify', 'correctness', 'computer', 'program', 'kind', 'question', 'blum', 'pose', 'read', 'paradox', 'somewhat', 'playful', 'quality', 'make', 'complexity', 'theory', 'cryptography', 'sound', 'almost', 'subgenre', 'scifi', 'completely', 'original', 'go', 'think', 'interesting', 'important', 'often', 'turn', 'really', 'significant', 'sipser', 'tell', 'seminal', 'paper', 'coin', 'flipping', 'telephone', 'question', 'pose', 'want', 'flip', 'coin', 'telephone', 'divorce', 'live', 'different', 'city', 'want', 'decide', 'get', 'car', 'let', 'say', 'call', 'head', 'say', 'lose', 'trust', 'truthful', 'alice', 'situation', 'reverse', 'sound', 'riddle', 'address', 'fundamental', 'problem', 'cryptography', 'party', 'engage', 'trustworthy', 'exchange', 'communication', 'channel', 'way', 'party', 'cheat', 'blum', 'show', 'achieve', 'use', 'concept', 'commitment', 'simplified', 'analogy', 'idea', 'give', 'locked', 'box', 'prediction', 'key', 'prevent', 'alice', 'alter', 'prediction', 'stop', 'discover', 'guess', 'prematurely', 'toss', 'coin', 'alice', 'hand', 'key', 'open', 'box', 'ask', 'blum', 'secret', 'good', 'mentorship', 'react', 'sheepish', 'head', 'scratch', 'attribute', 'student', 'success', 'talent', 'student', 'come', 'wonderful', 'idea', 'realize', 'wonderful', 'thing', 'say', 'really', 'enjoy', 'idea', 'student', 'tell', 'learn', 'response', 'leave', 'puzzle', 'especially', 'hear', 'student', 'blum', 'never', 'criticize', 'idea', 'prescribe', 'research', 'direction', 'offer', 'full', 'autonomy', 'boundless', 'encouragement', 'sound', 'wonderful', 'theory', 'mystify', 'work', 'practice', 'student', 'receive', 'occasional', 'course', 'correction', 'hyperspecific', 'advice', 'often', 'essential', 'academic', 'pursuit', 'still', 'dodge', 'question', 'much', 'magician', 'refuse', 'give', 'trick', 'astonish', 'conjure', 'thing', 'come', 'understand', 'advise', 'style', 'say', 'student', 'teach', 'truly', 'mean', 'entail', 'easy', 'pay', 'lip', 'service', 'principle', 'treat', 'student', 'colleague', 'professor', 'computer', 'science', 'mit', 'study', 'blum', 'tell', 'work', 'together', 'make', 'really', 'feel', 'mean', 'concrete', 'term', 'blum', 'impart', 'student', 'sense', 'pedagogical', 'responsibility', 'really', 'expect', 'learn', 'weekly', 'meeting', 'turn', 'mean', 'understand', 'idea', 'bone', 'first', 'month', 'work', 'think', 'test', 'realize', 'professor', 'computer', 'science', 'tell', 'learn', 'say', 'thing', 'manuel', 'understand', 'valuable', 'skill', 'give', 'student', 'skill', 'learn', 'swim', 'throw', 'pool', 'ability', 'translate', 'say', 'concrete', 'term', 'skill', 'prove', 'invaluable', 'teach', 'class', 'write', 'grant', 'proposal', 'former', 'student', 'describe', 'blum', 'unwaveringly', 'positive', 'say', 'way', 'criticism', 'steer', 'away', 'dead', 'end', 'always', 'smile', 'see', 'smile', 'wide', 'like', 'want', 'big', 'smile', 'say', 'ronitt', 'rubinfeld', 'professor', 'electrical', 'engineering', 'computer', 'science', 'mit', 'blum', 'corner', 'kind', 'audacious', 'idea', 'take', 'root', 'listen', 'absolutely', 'judgment', 'general', 'positivity', 'rubinfeld', 'say', 'fine', 'taste', 'interesting', 'idea', 'student', 'trust', 'guide', 'right', 'direction', 'come', 'boring', 'idea', 'blum', 'know', 'terrible', 'memory', 'mostly', 'forget', 'next', 'meeting', 'harcholbalter', 'graduate', 'school', 'say', 'never', 'tell', 'work', 'instead', 'guide', 'mean', 'question', 'manuel', 'fantastic', 'ask', 'question', 'manuel', 'excel', 'ask', 'question', 'blum', 'also', 'really', 'make', 'sure', 'student', 'special', 'area', 'develop', 'lenore', 'blum', 'tell', 'think', 'ask', 'student', 'ever', 'next', 'iteration', 'else', 'work', 'say', 'say', 'work', 'brand', 'new', 'work', 'new', 'idea', 'risky', 'encouragement', 'couple', 'track', 'record', 'spot', 'fruitful', 'line', 'inquiry', 'give', 'student', 'confidence', 'keep', 'go', 'bold', 'direction', 'endure', 'criticism', 'selfdoubt', 'huge', 'difference', 'manuel', 'advise', 'style', 'else', 'world', 'say', 'advise', 'style', 'simply', 'listen', 'make', 'seem', 'really', 'really', 'important', 'amazing', 'thing', 'world', 'harcholbalter', 'say', 'magic', 'try', 'emulate', 'student', 'idea', 'somehow', 'make', 'feel', 'brilliant', 'idea', 'ever', 'invent', 'remember', 'feel', 'idea', 'multimilliondollar', 'breakthrough', 'allow', 'stay', 'committed', 'line', 'research', 'undeterred', 'external', 'influence', 'trend', 'create', 'feeling', 'supreme', 'confidence', 'confidence', 'brilliant', 'add', 'year', 'feel', 'vulnerable', 'constantly', 'boost', 'confidence', 'amazing', 'student', 'great', 'astronomy', 'lead', 'way', 'make', 'science', 'accessible', 'sonification', 'result', 'sound', 'amazing', 'excellence', 'academia', 'many', 'field', 'need', 'identify', 'promising', 'topic', 'technical', 'ability', 'execute', 'technically', 'flawless', 'idea', 'original', 'insight', 'trivial', 'radically', 'original', 'idea', 'proper', 'execution', 'never', 'fully', 'develop', 'bold', 'idea', 'power', 'misplaced', 'confidence', 'hit', 'dead', 'end', 'psychological', 'reassurance', 'student', 'get', 'blum', 'come', 'part', 'superhuman', 'level', 'aplomb', 'never', 'seem', 'stress', 'say', 'son', 'avrim', 'real', 'world', 'deadline', 'stress', 'never', 'show', 'least', 'never', 'see', 'still', 'awe', 'ability', 'mask', 'inner', 'turbulence', 'affect', 'well', 'remain', 'invisible', 'even', 'close', 'observer', 'include', 'son', 'source', 'stability', 'student', 'rely', 'graduate', 'study', 'comfortable', 'relaxed', 'grad', 'school', 'feel', 'thing', 'control', 'tell', 'difficulty', 'help', 'back', 'go', 'sort', 'thing', 'speak', 'student', 'feel', 'pang', 'jealousy', 'blum', 'corner', 'vulnerable', 'moment', 'many', 'direct', 'criticism', 'face', 'reformulate', 'question', 'kind', 'audacious', 'idea', 'take', 'root', 'listen', 'absolutely', 'judgment', 'even', 'blum', 'student', 'claim', 'still', 'bewilder', 'magic', 'mystery', 'advisor', 'approach', 'become', 'accomplished', 'teacher', 'advisor', 'right', 'umesh', 'theoretical', 'computer', 'scientist', 'tell', 'think', 'lot', 'secret', 'say', 'essence', 'express', 'way', 'respect', 'student', 'let', 'develop', 'want', 'vazirani', 'advise', 'number', 'superstar', 'field', 'believe', 'education', 'important', 'thing', 'break', 'cause', 'damage', 'potency', 'blumian', 'approach', 'advise', 'domain', 'specific', 'illustrate', 'george', 'saunder', 'reflection', 'writing', 'teacher', 'tobia', 'wolff', 'writing', 'teacher', 'much', 'power', 'saunder', 'write', 'mock', 'disregard', 'use', 'prop', 'ourteacher', 'good', 'instead', 'almost', 'holy', 'neverforget', 'take', 'seriously', 'accept', 'new', 'member', 'guildthey', 'tolerate', 'underwonderful', 'story', 'write', 'dopy', 'thing', 'say', 'ourshakylegge', 'aesthetic', 'theory', 'posturing', 'therethemselve', 'say', 'think', 'writer', 'say', 'good', 'proceed', 'last', 'day', 'notice', 'photo', 'blum', 'old', 'advisor', 'desk', 'home', 'office', 'prominent', 'place', 'else', 'choose', 'display', 'family', 'heirloom', 'showcase', 'autographed', 'photo', 'shake', 'president', 'hand', 'die', 'year', 'begin', 'professorship', 'curiosity', 'point', 'photo', 'prominent', 'position', 'always', 'blum', 'reply', 'manuel', 'spiritual', 'father', 'way', 'add', 'lenore', 'make', 'way', 'back', 'airport', 'remember', 'book', 'call', 'survive', 'death', 'philosopher', 'book', 'postulate', 'good', 'person', 'quite', 'literally', 'survive', 'death', 'redirect', 'selfinter', 'wellbeing', 'future', 'people', 'forfeiture', 'spell', 'dissolution', 'self', 'rather', 'expansion', 'allow', 'person', 'live', 'onward', 'rush', 'humankind', 'line', 'book', 'unfold', 'timerelease', 'effect', 'head', 'time', 'baby', 'bear', 'good', 'person', 'acquire', 'new', 'face', 'blum', 'know', 'smile', 'well', 'nod', 'impart', 'blessing', 'wonderful', 'idea', 'proceed', 'sheon', 'writer', 'base']","<p>Theoretical computer scientist Manuel Blum has guided generations of graduate students into fruitful careers in the field.</p>
"
Death to captchas,https://www.technologyreview.com/2023/10/24/1081139/captchas-ai-websites-computing/,2023-10-24,"Earlier this year, HBO Max users hoping to sign in to the service had to pass an audio challenge in which they listened to a bunch of tunes and had to select the one with a repeating pattern. When I signed in to LinkedIn recently, it asked me to prove I’m human with an unusual puzzle. With a set of left and right buttons, I had to turn a 3D image of a pink dog until it faced the direction that a hand next to it was pointing.  Websites use these captchas (the name comes from “Completely Automated Public Turing test to tell Computers and Humans Apart”) to tell whether a user is human or machine. You’ve likely noticed they have only gotten more difficult and more involved. That’s because of what happens after we solve a captcha: the data from our efforts to label those blurry grids of traffic lights, text, or buses is used to train AI systems, which then get better at defeating captchas, tricking systems into thinking they are human.  The arms race between humans and machines has been progressing for a while. As early as 2016, researchers at Columbia University showed they could solve Google’s image captchas with 70% accuracy using off-the-shelf automated image recognition tools, the sort that could readily be used by bot designers.  Captchas have gotten more complex out of necessity. Because as AI gets more sophisticated, they’ve become less effective. By now, some captchas have gotten a little surreal. A company called hCaptcha recently tasked people with identifying an object that doesn’t exist—a “Yoko,” which seems to be an AI-generated yo-yo with a roughly snail-like appearance.   Tech firms and consumers alike feel it’s time for a change. For one thing, legacy captchas (which are still in use) just don’t work anymore: “Clicking images such as buses and street signs is outdated,” Ashish Jain, the CTO of Arkose Labs, the firm behind those LinkedIn and HBO captchas, told MIT Technology Review. “Bots have evolved, but legacy captchas haven’t.” Even more convoluted mini-games may not be enough to keep AI at bay. In one instance, a chatbot (guided by humans) pretended to be visually impaired and managed to hire a human to solve a captcha for it.  Mauro Migliardi, a professor of software engineering at the University of Padua, believes captcha designers will have to go a step further in order to stay ahead of machines. Because AIs can be trained to tackle any cognitive task, he says, we may need to transition to physical challenges, like requiring users to rotate their phones or move them in a certain way as they would in a video game.  It’s a practice that could introduce further errors into already error-prone models. That might solve some problems, but it would create others. The more complicated the challenge, the more cumbersome it is to do what you want to do on the web. And some approaches might shut some users out. “It’s actually really hard to build a challenge like this that is friendly to the whole human population,” Jess Leroy, senior director of product management at Google Cloud, wrote in an e-mail. “There are many reasons why something that may be obvious or easy to one person may be difficult to another.” Those include disabilities and cultural differences. In the long term, we may see captchas abandoned altogether. Companies such as Google and Cloudflare have already quietly switched to “invisible” challenges, which monitor online fingerprints of human behavior, like cursor motions or browsing behavior, to differentiate a person from a bot. If these sorts of signals convince the software you are human, you won’t have to solve a captcha.  This approach raises privacy concerns: such signals can allow advertisers and websites to track what you are doing online. An alternative could come from a coalition of companies, including Google, Fastly, Cloudflare, and Apple, that has developed a more privacy-friendly mechanism called Privacy Pass. Before we even open a browser and run into a captcha challenge, we perform numerous actions on our phones and computers—like unlocking them with our faces—that are hard for a bot to imitate. On a Privacy Pass–enabled website, our devices take all that information and attest for us—allowing us to skip the captcha altogether. This data never leaves your device and isn’t shared with the website. Apple calls these signatures Private Access Tokens (PATs) and already leaves the feature on by default on iPhones running at least iOS 16.  Most captcha providers, like hCaptcha and Cloudflare, now support PATs as well. Cloudflare’s CTO, John Graham-Cumming, said in July that more than half of requests from iOS devices used PATs. Leroy says that Google’s Chrome and Android teams are “working on similar technologies.”  But don’t expect captchas to disappear anytime soon. While Privacy Pass may prove a reliable alternative, captchas remain popular. Ting Wang, an information science and technology professor at Penn State University, predicts they will “continue to exist as a cheap, platform-­agnostic, and universal verification solution.” Shubham Agarwal is a freelance tech journalist. ","Earlier this year , HBO Max users hoping to sign in to the service had to pass an audio challenge in which they listened to a bunch of tunes and had to select the one with a repeating pattern . When I signed in to LinkedIn recently , it asked me to prove I ’ m human with an unusual puzzle . With a set of left and right buttons , I had to turn a 3D image of a pink dog until it faced the direction that a hand next to it was pointing . Websites use these captchas ( the name comes from “ Completely Automated Public Turing test to tell Computers and Humans Apart ” ) to tell whether a user is human or machine . You ’ ve likely noticed they have only gotten more difficult and more involved . That ’ s because of what happens after we solve a captcha : the data from our efforts to label those blurry grids of traffic lights , text , or buses is used to train AI systems , which then get better at defeating captchas , tricking systems into thinking they are human . The arms race between humans and machines has been progressing for a while . As early as 2016 , researchers at Columbia University showed they could solve Google ’ s image captchas with 70 % accuracy using off-the-shelf automated image recognition tools , the sort that could readily be used by bot designers . Captchas have gotten more complex out of necessity . Because as AI gets more sophisticated , they ’ ve become less effective . By now , some captchas have gotten a little surreal . A company called hCaptcha recently tasked people with identifying an object that doesn ’ t exist—a “ Yoko , ” which seems to be an AI-generated yo-yo with a roughly snail-like appearance . Tech firms and consumers alike feel it ’ s time for a change . For one thing , legacy captchas ( which are still in use ) just don ’ t work anymore : “ Clicking images such as buses and street signs is outdated , ” Ashish Jain , the CTO of Arkose Labs , the firm behind those LinkedIn and HBO captchas , told MIT Technology Review . “ Bots have evolved , but legacy captchas haven ’ t. ” Even more convoluted mini-games may not be enough to keep AI at bay . In one instance , a chatbot ( guided by humans ) pretended to be visually impaired and managed to hire a human to solve a captcha for it . Mauro Migliardi , a professor of software engineering at the University of Padua , believes captcha designers will have to go a step further in order to stay ahead of machines . Because AIs can be trained to tackle any cognitive task , he says , we may need to transition to physical challenges , like requiring users to rotate their phones or move them in a certain way as they would in a video game . It ’ s a practice that could introduce further errors into already error-prone models . That might solve some problems , but it would create others . The more complicated the challenge , the more cumbersome it is to do what you want to do on the web . And some approaches might shut some users out . “ It ’ s actually really hard to build a challenge like this that is friendly to the whole human population , ” Jess Leroy , senior director of product management at Google Cloud , wrote in an e-mail . “ There are many reasons why something that may be obvious or easy to one person may be difficult to another. ” Those include disabilities and cultural differences . In the long term , we may see captchas abandoned altogether . Companies such as Google and Cloudflare have already quietly switched to “ invisible ” challenges , which monitor online fingerprints of human behavior , like cursor motions or browsing behavior , to differentiate a person from a bot . If these sorts of signals convince the software you are human , you won ’ t have to solve a captcha . This approach raises privacy concerns : such signals can allow advertisers and websites to track what you are doing online . An alternative could come from a coalition of companies , including Google , Fastly , Cloudflare , and Apple , that has developed a more privacy-friendly mechanism called Privacy Pass . Before we even open a browser and run into a captcha challenge , we perform numerous actions on our phones and computers—like unlocking them with our faces—that are hard for a bot to imitate . On a Privacy Pass–enabled website , our devices take all that information and attest for us—allowing us to skip the captcha altogether . This data never leaves your device and isn ’ t shared with the website . Apple calls these signatures Private Access Tokens ( PATs ) and already leaves the feature on by default on iPhones running at least iOS 16 . Most captcha providers , like hCaptcha and Cloudflare , now support PATs as well . Cloudflare ’ s CTO , John Graham-Cumming , said in July that more than half of requests from iOS devices used PATs . Leroy says that Google ’ s Chrome and Android teams are “ working on similar technologies. ” But don ’ t expect captchas to disappear anytime soon . While Privacy Pass may prove a reliable alternative , captchas remain popular . Ting Wang , an information science and technology professor at Penn State University , predicts they will “ continue to exist as a cheap , platform-­agnostic , and universal verification solution. ” Shubham Agarwal is a freelance tech journalist .","['early', 'year', 'user', 'hope', 'sign', 'service', 'pass', 'audio', 'challenge', 'listen', 'bunch', 'tune', 'select', 'one', 'repeat', 'pattern', 'sign', 'recently', 'ask', 'prove', 'human', 'unusual', 'puzzle', 'set', 'left', 'right', 'button', 'turn', '3d', 'image', 'pink', 'dog', 'face', 'direction', 'hand', 'next', 'point', 'website', 'use', 'captchas', 'name', 'come', 'completely', 'automate', 'public', 'ture', 'test', 'tell', 'computer', 'human', 'apart', 'tell', 'user', 'human', 'machine', 'likely', 'notice', 'get', 'difficult', 'involved', 'happen', 'solve', 'captcha', 'datum', 'effort', 'label', 'blurry', 'grid', 'traffic', 'light', 'text', 'bus', 'use', 'train', 'system', 'get', 'well', 'defeat', 'captchas', 'trick', 'system', 'think', 'human', 'arm', 'race', 'human', 'machine', 'progress', 'early', 'researcher', 'show', 'solve', 'image', 'captcha', 'accuracy', 'use', 'automate', 'image', 'recognition', 'tool', 'sort', 'readily', 'use', 'bot', 'designer', 'captcha', 'get', 'complex', 'necessity', 'get', 'sophisticated', 'become', 'less', 'effective', 'captcha', 'get', 'little', 'surreal', 'company', 'call', 'recently', 'task', 'people', 'identify', 'object', 'exist', 'seem', 'aigenerated', 'yoyo', 'roughly', 'snaillike', 'appearance', 'tech', 'firm', 'consumer', 'alike', 'feel', 'time', 'change', 'thing', 'legacy', 'still', 'use', 'work', 'anymore', 'click', 'image', 'bus', 'street', 'sign', 'outdate', 'ashish', 'cto', 'firm', 'linkedin', 'tell', 'mit', 'technology', 'review', 'bot', 'evolve', 'even', 'convoluted', 'minigame', 'enough', 'keep', 'ai', 'bay', 'instance', 'chatbot', 'guide', 'human', 'pretend', 'visually', 'impair', 'manage', 'hire', 'human', 'solve', 'captcha', 'professor', 'software', 'engineering', 'believe', 'designer', 'go', 'step', 'far', 'order', 'stay', 'ahead', 'machine', 'train', 'tackle', 'cognitive', 'task', 'say', 'need', 'transition', 'physical', 'challenge', 'require', 'user', 'rotate', 'phone', 'move', 'certain', 'way', 'video', 'game', 'practice', 'introduce', 'error', 'already', 'errorprone', 'model', 'solve', 'problem', 'create', 'complicated', 'challenge', 'cumbersome', 'want', 'web', 'approach', 'shut', 'user', 'actually', 'really', 'hard', 'build', 'challenge', 'friendly', 'whole', 'human', 'population', 'jess', 'leroy', 'senior', 'director', 'product', 'management', 'write', 'email', 'many', 'reason', 'obvious', 'easy', 'person', 'difficult', 'include', 'disability', 'cultural', 'difference', 'long', 'term', 'see', 'captcha', 'abandon', 'altogether', 'company', 'already', 'quietly', 'switch', 'invisible', 'challenge', 'monitor', 'online', 'fingerprint', 'human', 'behavior', 'cursor', 'motion', 'browse', 'behavior', 'differentiate', 'person', 'bot', 'sort', 'signal', 'convince', 'software', 'human', 'win', 'solve', 'captcha', 'approach', 'raise', 'privacy', 'concern', 'signal', 'allow', 'advertiser', 'website', 'track', 'online', 'alternative', 'come', 'coalition', 'company', 'include', 'fastly', 'cloudflare', 'apple', 'develop', 'privacyfriendly', 'mechanism', 'call', 'privacy', 'pass', 'even', 'open', 'browser', 'run', 'captcha', 'challenge', 'perform', 'numerous', 'action', 'phone', 'computer', 'unlock', 'face', 'hard', 'bot', 'imitate', 'privacy', 'pass', 'enable', 'website', 'device', 'take', 'information', 'attest', 'allow', 'skip', 'captcha', 'altogether', 'datum', 'never', 'leave', 'device', 'share', 'website', 'apple', 'call', 'signature', 'private', 'access', 'token', 'pat', 'already', 'leave', 'feature', 'default', 'iphone', 'run', 'least', 'io', 'captcha', 'provider', 'hcaptcha', 'cloudflare', 'support', 'pat', 'well', 'cloudflare', 'say', 'half', 'request', 'io', 'device', 'use', 'pat', 'say', 'chrome', 'android', 'team', 'work', 'similar', 'technology', 'expect', 'captcha', 'disappear', 'anytime', 'soon', 'privacy', 'pass', 'prove', 'reliable', 'alternative', 'captcha', 'remain', 'popular', 'te', 'information', 'science', 'technology', 'professor', 'predict', 'continue', 'exist', 'cheap', 'platform\xadagnostic', 'universal', 'verification', 'solution', 'freelance', 'tech', 'journalist']","<p>Proving you’re human on websites is harder than ever—but alternative tests are gaining ground.</p>
"
Inside the quest for unbreakable encryption,https://www.technologyreview.com/2023/10/19/1081389/unbreakable-encryption-quantum-computers-cryptography-math-problems/,2023-10-19,"When we check email, log in to our bank accounts, or exchange messages on Signal, our passwords and credentials are protected through encryption, a locking scheme that uses secrets to disguise our data. It works like a cyber padlock: with the right key someone can unlock the data. Without it, they’ll have to resort to laborious brute-force methods, the digital equivalent of hacksaws and blowtorches. Our trust in online security is rooted in mathematics. Encryption schemes are built on families of math problems called one-way functions—calculations that are easy to carry out in one direction but almost impossible to solve efficiently from the other, even with a powerful computer. They’re sort of a computational equivalent of those road spikes found at the exits of airport car rental agencies. Drive in one direction and you barely notice. Hit reverse and you won’t get far (and will need new tires). There’s a problem, however. Although mathematicians suspect true one-way functions exist, they have yet to prove it. They haven’t proved that the thorny problems we do use are impossible, or even extremely impractical, to solve. Instead, it could just be that we haven’t yet found the appropriate mathematical means to take the problems apart. This conundrum haunts all encryption. Our data is secured by the fact that no one knows how to crack the schemes that protect it—at least not yet.  It’s not just today’s hackers we may need to worry about. Security experts have long warned of a threat that hasn’t yet materialized: quantum computers. In the future these machines could execute a program that quickly solves the math problems behind today’s state-of-the-art encryption. That threat puts personal financial, medical, and other information at risk. Hackers could steal today’s encrypted data and store it away, just waiting for the arrival of new technological lockpicks.  Computer scientists, mathematicians, and cryptographers are on a quest to find new encryption algorithms that can withstand attacks not only from today’s conventional computers but also from tomorrow’s quantum machines. What they want is a big, sticky math problem—something that’s robust enough to withstand attacks from classical and quantum computers but can still be easily implemented in cyberspace.  Unfortunately, no one has yet found a single type of problem that is provably hard for computers—classical or quantum—to solve. (In the world of cryptography, “hard” describes a problem whose solution requires an unreasonable number of steps or amount of computing power.) If one-way functions don’t exist, then cryptographers’ whack-a-mole process of finding flaws and developing ever stronger schemes to block clever hackers will persist indefinitely.  “The question of whether one-way functions exist is really the most important problem,” says Rafael Pass, a theoretical computer scientist at Tel Aviv University in Israel. It’s a conundrum that dates to the 1970s and the dawn of a research area now known as computational complexity theory. Over five decades, theorists and cryptographers have been looking for ways to establish whether such functions do exist. Perhaps the problems we hope or suspect are one-way are just easier, breakable ones in disguise.  Pass is exploring how one-way functions are connected to a raft of other open problems, a promising line of research that has drawn other theorists into the quest. At the same time, people focused on the practical side of cryptography are plowing ahead, hunting for new schemes that are—if not provably hard—seemingly strong enough to hold up against quantum computers.  Computer scientists find themselves at a curious crossroads, unsure of whether post-quantum algorithms are truly unassailable—or just believed to be so. For the last seven years, the job of finding the best candidates has been spearheaded by the National Institute of Standards and Technology (NIST), the US government body charged with collecting, testing, and standardizing cryptographic algorithms for public use. NIST has been running dozens of potential “post-­quantum” algorithms through a gauntlet of tests and making them available for outside testing. The process has winnowed the field to a few finalists, and in August NIST announced that one called CRYSTALS-Kyber, which takes an approach believed to be robust enough to counter quantum attacks, will be the first to be officially recommended for public use by 2024. After that, companies and governments will adopt the algorithm for encrypting data.  Will it hold up? The answer will help determine the trajectory of cybersecurity in the near term. But it’s far from settled: history suggests that our faith in unbreakability has often been misplaced, and over the years, seemingly impenetrable encryption candidates have fallen to surprisingly simple attacks. Computer scientists find themselves at a curious crossroads, unsure of whether post-quantum algorithms are truly unassailable—or just believed to be so. It’s a distinction at the heart of modern encryption security.  Securing secret messages hasn’t always been tied to difficult math problems; until recently, cryptography was barely mathematical at all. In ancient Greece, military leaders encoded messages using a scytale, a cylindrical device that revealed a hidden message when a strip of seemingly jumbled text was wound around it. Centuries later, Roman historians described a code, often attributed to Julius Caesar, that involved shifting letters in a message three spots up in the alphabet; for example, a d would be written as an a.  Companies are moving away from setting qubit records in favor of practical hardware and long-term goals. In history as in our modern world, secret codes were frequently broken. In the 16th century, during the decades she spent imprisoned by her cousin Queen Elizabeth I, Mary, Queen of Scots, used elaborate, symbol-based ciphers to encode hundreds of letters, most of which were aimed at securing her freedom and regaining the throne. She didn’t prevail: Elizabeth I’s team of spies and codebreakers intercepted, decoded, and copied the letters. In the one that sealed her fate, Mary approved of a plan to assassinate Elizabeth with six words: “sett the six gentlemen to woork.” In response, Elizabeth eventually ordered her cousin beheaded in 1587. In 1932, codebreakers in Poland cracked the code for Germany’s early Enigma machine, invented at the end of World War I. They later shared their intel with British codebreakers, who cracked a more advanced version of Enigma during World War II. Pass, the theoretical computer scientist in Tel Aviv, half-jokingly refers to all time before the 1970s as the “dark age of cryptography.” “Cryptography wasn’t really a scientific field,” he says. “It was more like artist versus attackers. You needed to have [artistic] skills to invent an encryption scheme. And then it would get deployed until some clever person would figure out how to break it. And it was just going on and on like that.”  That changed, Pass says, in November 1976, when cryptographers Whitfield Diffie and Martin Hellman, at Stanford, described a novel way for two people to devise a key that only they knew—one they could then use to pass secret messages. Crucially, they wouldn’t have to meet to do it. This was a groundbreaking notion. Previously, both sender and receiver had to physically possess a key for encoding and decoding. To decrypt a message encoded with the Enigma machine, for example, a recipient needed a key sheet that revealed the initial encryption settings. The secret to the Diffie-Hellman strategy was for two people to build the key using a straightforward mathematical problem that’s easy to compute in one direction and laborious in the other. Here’s how it works: The two people who want to communicate secretly, usually designated Alice and Bob in these setups, each pick a secret number. Then, together, they agree on a pair of numbers that they share publicly (one is a big prime, and the other is called the base). Each of them next carries out a series of mathematical operations to combine those private numbers with the prime and the base.  Then they exchange the results, and they each carry out another series of mathematical operations on the new numbers. In the end, both Alice and Bob will have done the same operations on the same numbers—just not in the same order—and arrived at the same answer. The digits of that answer become the encryption. And an eavesdropper who intercepts the transmission—often nicknamed Eve—won’t be able to easily unravel the mathematical jumble without knowing at least one of the private numbers. She could start testing numbers in a brute-force approach, but that would require an unreasonable amount of calculation.  The complicated problem that Eve would have to solve is called finding a discrete logarithm. The Diffie-Hellman approach is still used today—to secure some VPNs, for example—and is integral to some post-quantum schemes. In their paper, Diffie and Hellman noted that there was no existing algorithm capable of solving the discrete log problem in a reasonable amount of time. There still isn’t. They went on to introduce, for the first time, the notion of one-way functions as a basis for secure cryptography. Today, secure online interactions that involve authentication or digital signatures, for example, are based on that general idea. But without mathematical proof that the problems they rely on are one-way functions, the possibility remains that someone might discover an efficient scheme for cracking them.  Today, online transactions begin with a kind of digital handshake, and the security of that handshake is often guaranteed by another math problem that’s presumed to be difficult. The most popular encryption scheme used today was introduced in 1977 by a trio of young computer scientists who were energized by Diffie and Hellman’s 1976 paper. They called their approach RSA, after the last names of the scientists (Ron Rivest, Adi Shamir, and Leonard Adleman).  RSA, which is based on the difficulty of finding prime factors relative to the ease of multiplying them together, is a bit different from the Diffie-Hellman approach. Diffie-Hellman is a shared secret: it allows two users to devise a key over an insecure channel (like the internet), and that key is used to disguise messages. In RSA, Alice uses Bob’s key—based on big prime numbers—to encrypt a message that only he can unlock. RSA can secure data sent from one person to another.   It quickly became one of the most popular public-key encryption methods. It’s easy to use and adapt. Over time, as new algorithms have emerged that can factor faster, and computers have become more powerful, NIST has recommended using larger and larger numbers for security. The numbers are represented in binary form with 1s and 0s, and these binary digits are better known as “bits.” The number 13, for example, is written in binary as 1101, which has four bits. NIST currently recommends using a key represented by at least 2,048 bits—which corresponds to a number with over 600 digits. (To date, the largest number that has been factored into two primes was made up of 250 digits, and the process took nearly 3,000 hours of computing time.) That’s a strength of RSA—even if it’s not uncrackable, it’s been easy to keep upping the ante, making it computationally impractical to break.  In 1994, however, a threat of a different type emerged when the American mathematician Peter Shor, then at Bell Labs, devised an algorithm for quantum computers that could solve the factoring problem in a reasonable amount of time. (It was a double threat: his approach could also conquer the discrete log problem in the Diffie-Hellman approach.)  Shor’s paper ignited excitement and anxiety among those who wanted to build quantum computers and those who recognized the threat it posed to cybersecurity. Fortunately for cryptographers, not just any quantum computer would do.  The last three decades of cybersecurity have played out like an increasingly intricate game, with researchers perpetually building and breaking—or attempting to break—new candidates. A few years back, researchers at Google and the KTH Royal Institute of Technology, in Sweden, estimated that it would take a quantum computer composed of 20 million quantum bits, or qubits, some eight hours to break today’s 2,048-bit RSA security. Current state-of-the-art machines are nowhere close to that size: the largest quantum computer to date, built by IBM, debuted last year with 433 qubits. Whether or not RSA can be considered at immediate risk of a quantum attack depends largely on whom you ask, says computer scientist Ted Shorter, who cofounded the cybersecurity company Keyfactor. He sees a cultural divide between the theorists who study the mathematics of encryption and the cryptographers who work in implementation. To some, the end seems nigh. “You talk to a theoretical computer scientist and they’re like, Yes, RSA is done, because they can imagine it,” Shorter says. For them, he adds, the existence of Shor’s algorithm points to the end of encryption as we know it.  Many cryptographers who are implementing real-world security systems are less concerned about the quantum future than they are about today’s cleverest hackers. After all, people have been trying to factor efficiently for thousands of years, and now the only known method requires a computer that doesn’t exist.  Thomas Decru, a cryptographer at KU Leuven in Belgium, says the quantum threat must be taken seriously, but it’s hard to know if RSA will fall to quantum computers in five years or longer—or never. “As long as quantum computers do not exist, everything you say about them is speculative, in a way,” he says. Pass is more certain about the threat: “It’s safe to say that the existence of this quantum algorithm means there are cracks in the problem, right?”  But we have to be ready for anything, says Lily Chen, a mathematician who manages NIST’s Cryptographic Technology Group and works on the ongoing effort to produce post-quantum encryption standards. Whether they arrive in three years or 30, quantum computers loom on the horizon, and RSA, Diffie-Hellman, and other encryption schemes may be left vulnerable.  Finding a quantum-resistant cryptographic scheme isn’t easy. Without a mathematical problem that is computationally hard, the last three decades of cybersecurity have played out like an increasingly intricate game, with researchers perpetually building and breaking—or attempting to break—new candidates.  This push and pull has already emerged in the NIST post-quantum program. In February 2022, cryptographers found a fatal flaw in Rainbow, an algorithm that had survived three rounds of NIST’s analysis. A few months later, after the NIST list had been winnowed again, Decru and his KU Leuven colleague Wouter Castryck announced that they’d broken another finalist, an algorithm called SIKE.  SIKE, which was developed a few years ago, was the brainchild of a collaboration among researchers and engineers at Amazon, Microsoft, the University of Versailles, and elsewhere. It is based on a special mathematical map, called an isogeny, that is made up of connections between elliptic curves. These maps can be turned into an encryption for communication, and outsiders can’t eavesdrop without knowing the maps. When quantum computers become powerful enough, they could theoretically crack the encryption algorithms that keep us safe. The race is on to find new ones. At Leuven, Decru and Castryck devise ways to use these so-called isogenies to build new, faster encryption approaches. They broke the most difficult version of SIKE in just a few hours of computing time using an ordinary desktop computer. (Since then, other groups have found ways to do it even faster.) What’s more, Decru and Castryck did it almost accidentally, and only a few weeks after SIKE had been declared an alternate NIST finalist. “We weren’t trying to break it at all,” insists Decru. “We just tried to generalize it.”  Chen says the case of SIKE—and Rainbow before it—illustrates a real-world tension that drives efforts to find quantum-proof algorithms. On one hand, she says, “you have to find a problem which is hard for both quantum computers and classical computers.” On the other is implementation: transforming that hard problem into one that can be used in a real-world cryptographic system. Even with today’s well-defined problems, Shorter says, it’s very difficult to predict and prevent every loophole in every operating system and device on the market today. “And then there’s interoperability testing and certifications and other tests,” he says, “to make sure they are not only implemented correctly, but also securely.”   The mathematical problem SIKE is based on seems computationally hard because there are so many different maps that could be constructed between curves. It may even be a one-way problem—and therefore quantum-proof. The flaw was in the design, which revealed too much of the transmitted information. Decru and Castryck cracked it because they inadvertently found a way to expose enough connecting points to give away the entire thing.  Other schemes have fared better. The first post-quantum encryption algorithm to be standardized, CRYSTALS-Kyber, delivers security through an approach that involves problems on lattices, mathematical objectsthat can be modeled as arrays of points. (There are five main families of post-quantum cryptographic methods. Isogeny and lattice approaches are two of them.)  CRYSTALS-Kyber is a general encryption scheme, like RSA, that can be used for tasks like securing online communication. Three other approved algorithms are designed to authenticate digital signatures, ensuring that digital documents haven’t been fraudulently signed. NIST plans to standardize these by spring 2024. Another three (it was four until SIKE was broken) could also be standardized in the next few years, as long as they survive further rounds of scrutiny. But unless mathematicians can prove whether one-way functions exist, says Pass, the patterns that have always characterized cryptography will continue. “We’re back to this cat-and-mouse game, where it’s a game between algorithm designers proposing new candidate constructions and other designers trying to break them,” he says. Unless, of course, he—or someone in his field—can come up with an implementable, provably one-way function to settle the matter of encryption forever.  Until that time, cryptographers will remain in a messy limbo in which convincingly robust encryption schemes can be trusted—but only until they can’t.  The perfect math problem could take us out of this limbo, but it can’t be some sticky mess cooked up by an armchair algebraist over a long weekend. It must strike a balance between math and cryptography, with computational hardness on one side and easy implementation on the other. Stray too far from either of those properties, and it becomes vulnerable—if not now, then in the future. Hanging in the balance is the past, present, and future security of everyone’s data, everywhere. No pressure.  Stephen Ornes is a science writer based in Nashville. ","When we check email , log in to our bank accounts , or exchange messages on Signal , our passwords and credentials are protected through encryption , a locking scheme that uses secrets to disguise our data . It works like a cyber padlock : with the right key someone can unlock the data . Without it , they ’ ll have to resort to laborious brute-force methods , the digital equivalent of hacksaws and blowtorches . Our trust in online security is rooted in mathematics . Encryption schemes are built on families of math problems called one-way functions—calculations that are easy to carry out in one direction but almost impossible to solve efficiently from the other , even with a powerful computer . They ’ re sort of a computational equivalent of those road spikes found at the exits of airport car rental agencies . Drive in one direction and you barely notice . Hit reverse and you won ’ t get far ( and will need new tires ) . There ’ s a problem , however . Although mathematicians suspect true one-way functions exist , they have yet to prove it . They haven ’ t proved that the thorny problems we do use are impossible , or even extremely impractical , to solve . Instead , it could just be that we haven ’ t yet found the appropriate mathematical means to take the problems apart . This conundrum haunts all encryption . Our data is secured by the fact that no one knows how to crack the schemes that protect it—at least not yet . It ’ s not just today ’ s hackers we may need to worry about . Security experts have long warned of a threat that hasn ’ t yet materialized : quantum computers . In the future these machines could execute a program that quickly solves the math problems behind today ’ s state-of-the-art encryption . That threat puts personal financial , medical , and other information at risk . Hackers could steal today ’ s encrypted data and store it away , just waiting for the arrival of new technological lockpicks . Computer scientists , mathematicians , and cryptographers are on a quest to find new encryption algorithms that can withstand attacks not only from today ’ s conventional computers but also from tomorrow ’ s quantum machines . What they want is a big , sticky math problem—something that ’ s robust enough to withstand attacks from classical and quantum computers but can still be easily implemented in cyberspace . Unfortunately , no one has yet found a single type of problem that is provably hard for computers—classical or quantum—to solve . ( In the world of cryptography , “ hard ” describes a problem whose solution requires an unreasonable number of steps or amount of computing power . ) If one-way functions don ’ t exist , then cryptographers ’ whack-a-mole process of finding flaws and developing ever stronger schemes to block clever hackers will persist indefinitely . “ The question of whether one-way functions exist is really the most important problem , ” says Rafael Pass , a theoretical computer scientist at Tel Aviv University in Israel . It ’ s a conundrum that dates to the 1970s and the dawn of a research area now known as computational complexity theory . Over five decades , theorists and cryptographers have been looking for ways to establish whether such functions do exist . Perhaps the problems we hope or suspect are one-way are just easier , breakable ones in disguise . Pass is exploring how one-way functions are connected to a raft of other open problems , a promising line of research that has drawn other theorists into the quest . At the same time , people focused on the practical side of cryptography are plowing ahead , hunting for new schemes that are—if not provably hard—seemingly strong enough to hold up against quantum computers . Computer scientists find themselves at a curious crossroads , unsure of whether post-quantum algorithms are truly unassailable—or just believed to be so . For the last seven years , the job of finding the best candidates has been spearheaded by the National Institute of Standards and Technology ( NIST ) , the US government body charged with collecting , testing , and standardizing cryptographic algorithms for public use . NIST has been running dozens of potential “ post-­quantum ” algorithms through a gauntlet of tests and making them available for outside testing . The process has winnowed the field to a few finalists , and in August NIST announced that one called CRYSTALS-Kyber , which takes an approach believed to be robust enough to counter quantum attacks , will be the first to be officially recommended for public use by 2024 . After that , companies and governments will adopt the algorithm for encrypting data . Will it hold up ? The answer will help determine the trajectory of cybersecurity in the near term . But it ’ s far from settled : history suggests that our faith in unbreakability has often been misplaced , and over the years , seemingly impenetrable encryption candidates have fallen to surprisingly simple attacks . Computer scientists find themselves at a curious crossroads , unsure of whether post-quantum algorithms are truly unassailable—or just believed to be so . It ’ s a distinction at the heart of modern encryption security . Securing secret messages hasn ’ t always been tied to difficult math problems ; until recently , cryptography was barely mathematical at all . In ancient Greece , military leaders encoded messages using a scytale , a cylindrical device that revealed a hidden message when a strip of seemingly jumbled text was wound around it . Centuries later , Roman historians described a code , often attributed to Julius Caesar , that involved shifting letters in a message three spots up in the alphabet ; for example , a d would be written as an a . Companies are moving away from setting qubit records in favor of practical hardware and long-term goals . In history as in our modern world , secret codes were frequently broken . In the 16th century , during the decades she spent imprisoned by her cousin Queen Elizabeth I , Mary , Queen of Scots , used elaborate , symbol-based ciphers to encode hundreds of letters , most of which were aimed at securing her freedom and regaining the throne . She didn ’ t prevail : Elizabeth I ’ s team of spies and codebreakers intercepted , decoded , and copied the letters . In the one that sealed her fate , Mary approved of a plan to assassinate Elizabeth with six words : “ sett the six gentlemen to woork. ” In response , Elizabeth eventually ordered her cousin beheaded in 1587 . In 1932 , codebreakers in Poland cracked the code for Germany ’ s early Enigma machine , invented at the end of World War I . They later shared their intel with British codebreakers , who cracked a more advanced version of Enigma during World War II . Pass , the theoretical computer scientist in Tel Aviv , half-jokingly refers to all time before the 1970s as the “ dark age of cryptography. ” “ Cryptography wasn ’ t really a scientific field , ” he says . “ It was more like artist versus attackers . You needed to have [ artistic ] skills to invent an encryption scheme . And then it would get deployed until some clever person would figure out how to break it . And it was just going on and on like that. ” That changed , Pass says , in November 1976 , when cryptographers Whitfield Diffie and Martin Hellman , at Stanford , described a novel way for two people to devise a key that only they knew—one they could then use to pass secret messages . Crucially , they wouldn ’ t have to meet to do it . This was a groundbreaking notion . Previously , both sender and receiver had to physically possess a key for encoding and decoding . To decrypt a message encoded with the Enigma machine , for example , a recipient needed a key sheet that revealed the initial encryption settings . The secret to the Diffie-Hellman strategy was for two people to build the key using a straightforward mathematical problem that ’ s easy to compute in one direction and laborious in the other . Here ’ s how it works : The two people who want to communicate secretly , usually designated Alice and Bob in these setups , each pick a secret number . Then , together , they agree on a pair of numbers that they share publicly ( one is a big prime , and the other is called the base ) . Each of them next carries out a series of mathematical operations to combine those private numbers with the prime and the base . Then they exchange the results , and they each carry out another series of mathematical operations on the new numbers . In the end , both Alice and Bob will have done the same operations on the same numbers—just not in the same order—and arrived at the same answer . The digits of that answer become the encryption . And an eavesdropper who intercepts the transmission—often nicknamed Eve—won ’ t be able to easily unravel the mathematical jumble without knowing at least one of the private numbers . She could start testing numbers in a brute-force approach , but that would require an unreasonable amount of calculation . The complicated problem that Eve would have to solve is called finding a discrete logarithm . The Diffie-Hellman approach is still used today—to secure some VPNs , for example—and is integral to some post-quantum schemes . In their paper , Diffie and Hellman noted that there was no existing algorithm capable of solving the discrete log problem in a reasonable amount of time . There still isn ’ t . They went on to introduce , for the first time , the notion of one-way functions as a basis for secure cryptography . Today , secure online interactions that involve authentication or digital signatures , for example , are based on that general idea . But without mathematical proof that the problems they rely on are one-way functions , the possibility remains that someone might discover an efficient scheme for cracking them . Today , online transactions begin with a kind of digital handshake , and the security of that handshake is often guaranteed by another math problem that ’ s presumed to be difficult . The most popular encryption scheme used today was introduced in 1977 by a trio of young computer scientists who were energized by Diffie and Hellman ’ s 1976 paper . They called their approach RSA , after the last names of the scientists ( Ron Rivest , Adi Shamir , and Leonard Adleman ) . RSA , which is based on the difficulty of finding prime factors relative to the ease of multiplying them together , is a bit different from the Diffie-Hellman approach . Diffie-Hellman is a shared secret : it allows two users to devise a key over an insecure channel ( like the internet ) , and that key is used to disguise messages . In RSA , Alice uses Bob ’ s key—based on big prime numbers—to encrypt a message that only he can unlock . RSA can secure data sent from one person to another . It quickly became one of the most popular public-key encryption methods . It ’ s easy to use and adapt . Over time , as new algorithms have emerged that can factor faster , and computers have become more powerful , NIST has recommended using larger and larger numbers for security . The numbers are represented in binary form with 1s and 0s , and these binary digits are better known as “ bits. ” The number 13 , for example , is written in binary as 1101 , which has four bits . NIST currently recommends using a key represented by at least 2,048 bits—which corresponds to a number with over 600 digits . ( To date , the largest number that has been factored into two primes was made up of 250 digits , and the process took nearly 3,000 hours of computing time . ) That ’ s a strength of RSA—even if it ’ s not uncrackable , it ’ s been easy to keep upping the ante , making it computationally impractical to break . In 1994 , however , a threat of a different type emerged when the American mathematician Peter Shor , then at Bell Labs , devised an algorithm for quantum computers that could solve the factoring problem in a reasonable amount of time . ( It was a double threat : his approach could also conquer the discrete log problem in the Diffie-Hellman approach . ) Shor ’ s paper ignited excitement and anxiety among those who wanted to build quantum computers and those who recognized the threat it posed to cybersecurity . Fortunately for cryptographers , not just any quantum computer would do . The last three decades of cybersecurity have played out like an increasingly intricate game , with researchers perpetually building and breaking—or attempting to break—new candidates . A few years back , researchers at Google and the KTH Royal Institute of Technology , in Sweden , estimated that it would take a quantum computer composed of 20 million quantum bits , or qubits , some eight hours to break today ’ s 2,048-bit RSA security . Current state-of-the-art machines are nowhere close to that size : the largest quantum computer to date , built by IBM , debuted last year with 433 qubits . Whether or not RSA can be considered at immediate risk of a quantum attack depends largely on whom you ask , says computer scientist Ted Shorter , who cofounded the cybersecurity company Keyfactor . He sees a cultural divide between the theorists who study the mathematics of encryption and the cryptographers who work in implementation . To some , the end seems nigh . “ You talk to a theoretical computer scientist and they ’ re like , Yes , RSA is done , because they can imagine it , ” Shorter says . For them , he adds , the existence of Shor ’ s algorithm points to the end of encryption as we know it . Many cryptographers who are implementing real-world security systems are less concerned about the quantum future than they are about today ’ s cleverest hackers . After all , people have been trying to factor efficiently for thousands of years , and now the only known method requires a computer that doesn ’ t exist . Thomas Decru , a cryptographer at KU Leuven in Belgium , says the quantum threat must be taken seriously , but it ’ s hard to know if RSA will fall to quantum computers in five years or longer—or never . “ As long as quantum computers do not exist , everything you say about them is speculative , in a way , ” he says . Pass is more certain about the threat : “ It ’ s safe to say that the existence of this quantum algorithm means there are cracks in the problem , right ? ” But we have to be ready for anything , says Lily Chen , a mathematician who manages NIST ’ s Cryptographic Technology Group and works on the ongoing effort to produce post-quantum encryption standards . Whether they arrive in three years or 30 , quantum computers loom on the horizon , and RSA , Diffie-Hellman , and other encryption schemes may be left vulnerable . Finding a quantum-resistant cryptographic scheme isn ’ t easy . Without a mathematical problem that is computationally hard , the last three decades of cybersecurity have played out like an increasingly intricate game , with researchers perpetually building and breaking—or attempting to break—new candidates . This push and pull has already emerged in the NIST post-quantum program . In February 2022 , cryptographers found a fatal flaw in Rainbow , an algorithm that had survived three rounds of NIST ’ s analysis . A few months later , after the NIST list had been winnowed again , Decru and his KU Leuven colleague Wouter Castryck announced that they ’ d broken another finalist , an algorithm called SIKE . SIKE , which was developed a few years ago , was the brainchild of a collaboration among researchers and engineers at Amazon , Microsoft , the University of Versailles , and elsewhere . It is based on a special mathematical map , called an isogeny , that is made up of connections between elliptic curves . These maps can be turned into an encryption for communication , and outsiders can ’ t eavesdrop without knowing the maps . When quantum computers become powerful enough , they could theoretically crack the encryption algorithms that keep us safe . The race is on to find new ones . At Leuven , Decru and Castryck devise ways to use these so-called isogenies to build new , faster encryption approaches . They broke the most difficult version of SIKE in just a few hours of computing time using an ordinary desktop computer . ( Since then , other groups have found ways to do it even faster . ) What ’ s more , Decru and Castryck did it almost accidentally , and only a few weeks after SIKE had been declared an alternate NIST finalist . “ We weren ’ t trying to break it at all , ” insists Decru . “ We just tried to generalize it. ” Chen says the case of SIKE—and Rainbow before it—illustrates a real-world tension that drives efforts to find quantum-proof algorithms . On one hand , she says , “ you have to find a problem which is hard for both quantum computers and classical computers. ” On the other is implementation : transforming that hard problem into one that can be used in a real-world cryptographic system . Even with today ’ s well-defined problems , Shorter says , it ’ s very difficult to predict and prevent every loophole in every operating system and device on the market today . “ And then there ’ s interoperability testing and certifications and other tests , ” he says , “ to make sure they are not only implemented correctly , but also securely. ” The mathematical problem SIKE is based on seems computationally hard because there are so many different maps that could be constructed between curves . It may even be a one-way problem—and therefore quantum-proof . The flaw was in the design , which revealed too much of the transmitted information . Decru and Castryck cracked it because they inadvertently found a way to expose enough connecting points to give away the entire thing . Other schemes have fared better . The first post-quantum encryption algorithm to be standardized , CRYSTALS-Kyber , delivers security through an approach that involves problems on lattices , mathematical objectsthat can be modeled as arrays of points . ( There are five main families of post-quantum cryptographic methods . Isogeny and lattice approaches are two of them . ) CRYSTALS-Kyber is a general encryption scheme , like RSA , that can be used for tasks like securing online communication . Three other approved algorithms are designed to authenticate digital signatures , ensuring that digital documents haven ’ t been fraudulently signed . NIST plans to standardize these by spring 2024 . Another three ( it was four until SIKE was broken ) could also be standardized in the next few years , as long as they survive further rounds of scrutiny . But unless mathematicians can prove whether one-way functions exist , says Pass , the patterns that have always characterized cryptography will continue . “ We ’ re back to this cat-and-mouse game , where it ’ s a game between algorithm designers proposing new candidate constructions and other designers trying to break them , ” he says . Unless , of course , he—or someone in his field—can come up with an implementable , provably one-way function to settle the matter of encryption forever . Until that time , cryptographers will remain in a messy limbo in which convincingly robust encryption schemes can be trusted—but only until they can ’ t . The perfect math problem could take us out of this limbo , but it can ’ t be some sticky mess cooked up by an armchair algebraist over a long weekend . It must strike a balance between math and cryptography , with computational hardness on one side and easy implementation on the other . Stray too far from either of those properties , and it becomes vulnerable—if not now , then in the future . Hanging in the balance is the past , present , and future security of everyone ’ s data , everywhere . No pressure . Stephen Ornes is a science writer based in Nashville .","['check', 'email', 'log', 'bank', 'account', 'exchange', 'message', 'signal', 'password', 'credential', 'protect', 'encryption', 'lock', 'scheme', 'use', 'secret', 'disguise', 'datum', 'work', 'cyber', 'padlock', 'right', 'key', 'unlock', 'datum', 'resort', 'laborious', 'bruteforce', 'method', 'digital', 'equivalent', 'hacksaw', 'blowtorch', 'trust', 'online', 'security', 'root', 'mathematics', 'encryption', 'scheme', 'build', 'family', 'math', 'problem', 'call', 'oneway', 'function', 'calculation', 'easy', 'carry', 'direction', 'almost', 'impossible', 'solve', 'efficiently', 'even', 'powerful', 'computer', 'sort', 'computational', 'equivalent', 'road', 'spike', 'find', 'exit', 'airport', 'car', 'rental', 'agency', 'drive', 'direction', 'barely', 'notice', 'hit', 'reverse', 'win', 'get', 'far', 'need', 'new', 'tire', 'problem', 'however', 'mathematician', 'suspect', 'true', 'oneway', 'function', 'exist', 'yet', 'prove', 'prove', 'thorny', 'problem', 'use', 'impossible', 'even', 'extremely', 'impractical', 'solve', 'instead', 'yet', 'find', 'appropriate', 'mathematical', 'mean', 'take', 'problem', 'apart', 'conundrum', 'haunt', 'encryption', 'datum', 'secure', 'fact', 'one', 'know', 'crack', 'scheme', 'protect', 'least', 'yet', 'today', 'hacker', 'need', 'worry', 'security', 'expert', 'long', 'warn', 'threat', 'yet', 'materialize', 'quantum', 'computer', 'future', 'machine', 'execute', 'program', 'quickly', 'solve', 'math', 'problem', 'today', 'stateoftheart', 'encryption', 'threat', 'put', 'personal', 'financial', 'medical', 'information', 'risk', 'hacker', 'steal', 'today', 'encrypt', 'datum', 'store', 'away', 'wait', 'arrival', 'new', 'technological', 'lockpick', 'computer', 'scientist', 'mathematician', 'cryptographer', 'quest', 'find', 'new', 'encryption', 'algorithm', 'withstand', 'attack', 'today', 'conventional', 'computer', 'also', 'tomorrow', 'quantum', 'machine', 'want', 'big', 'sticky', 'math', 'problem', 'robust', 'enough', 'withstand', 'attack', 'classical', 'quantum', 'computer', 'still', 'easily', 'implement', 'cyberspace', 'unfortunately', 'one', 'yet', 'find', 'single', 'type', 'problem', 'provably', 'hard', 'computer', 'classical', 'quantum', 'solve', 'world', 'cryptography', 'hard', 'describe', 'problem', 'solution', 'require', 'unreasonable', 'number', 'step', 'amount', 'compute', 'power', 'oneway', 'function', 'exist', 'cryptographer', 'whackamole', 'process', 'find', 'flaw', 'develop', 'ever', 'strong', 'scheme', 'block', 'clever', 'hacker', 'persist', 'indefinitely', 'question', 'oneway', 'function', 'exist', 'really', 'important', 'problem', 'say', 'pass', 'theoretical', 'computer', 'scientist', 'conundrum', 'date', '1970', 'dawn', 'research', 'area', 'know', 'computational', 'complexity', 'theory', 'decade', 'theorist', 'cryptographer', 'look', 'way', 'establish', 'function', 'exist', 'perhaps', 'problem', 'hope', 'suspect', 'oneway', 'easy', 'breakable', 'one', 'disguise', 'pass', 'explore', 'oneway', 'function', 'connect', 'raft', 'open', 'problem', 'promising', 'line', 'research', 'draw', 'theorist', 'quest', 'time', 'people', 'focus', 'practical', 'side', 'cryptography', 'plow', 'ahead', 'hunt', 'new', 'scheme', 'provably', 'hard', 'seemingly', 'strong', 'enough', 'hold', 'quantum', 'computer', 'computer', 'scientist', 'find', 'curious', 'crossroad', 'unsure', 'postquantum', 'algorithm', 'truly', 'unassailable', 'believe', 'last', 'year', 'job', 'find', 'good', 'candidate', 'spearhead', 'standard', 'technology', 'nist', 'government', 'body', 'charge', 'collect', 'testing', 'standardize', 'cryptographic', 'algorithm', 'public', 'use', 'nist', 'run', 'dozen', 'potential', 'post\xadquantum', 'algorithm', 'gauntlet', 'test', 'make', 'available', 'outside', 'testing', 'process', 'winnow', 'field', 'finalist', 'nist', 'announce', 'call', 'crystalskyber', 'take', 'approach', 'believe', 'robust', 'enough', 'counter', 'quantum', 'attack', 'first', 'officially', 'recommend', 'public', 'use', 'company', 'government', 'adopt', 'encrypt', 'datum', 'hold', 'answer', 'help', 'determine', 'trajectory', 'cybersecurity', 'near', 'term', 'far', 'settle', 'history', 'suggest', 'faith', 'unbreakability', 'often', 'misplace', 'year', 'seemingly', 'impenetrable', 'encryption', 'candidate', 'fall', 'surprisingly', 'simple', 'attack', 'computer', 'scientist', 'find', 'curious', 'crossroad', 'unsure', 'postquantum', 'algorithm', 'truly', 'unassailable', 'believe', 'distinction', 'heart', 'modern', 'encryption', 'security', 'secure', 'secret', 'message', 'always', 'tie', 'difficult', 'math', 'problem', 'recently', 'cryptography', 'barely', 'mathematical', 'ancient', 'greece', 'military', 'leader', 'encode', 'message', 'use', 'scytale', 'cylindrical', 'device', 'reveal', 'hidden', 'message', 'strip', 'seemingly', 'jumbled', 'text', 'wind', 'century', 'later', 'roman', 'historian', 'describe', 'code', 'often', 'attribute', 'involve', 'shift', 'letter', 'message', 'spot', 'alphabet', 'example', 'write', 'company', 'move', 'away', 'set', 'qubit', 'record', 'favor', 'practical', 'hardware', 'longterm', 'goal', 'history', 'modern', 'world', 'secret', 'code', 'frequently', 'break', '16th', 'century', 'decade', 'spend', 'imprison', 'cousin', 'queen', 'scot', 'use', 'elaborate', 'symbolbase', 'cipher', 'encode', 'hundred', 'letter', 'aim', 'secure', 'freedom', 'regain', 'throne', 'prevail', 'team', 'spy', 'codebreaker', 'decode', 'copy', 'letter', 'one', 'seal', 'fate', 'mary', 'approve', 'plan', 'assassinate', 'word', 'sett', 'gentleman', 'woork', 'response', 'eventually', 'order', 'cousin', 'behead', 'codebreaker', 'crack', 'code', 'early', 'enigma', 'machine', 'invent', 'end', 'world', 'war', 'later', 'share', 'intel', 'british', 'codebreaker', 'crack', 'advanced', 'version', 'enigma', 'pass', 'theoretical', 'computer', 'scientist', 'halfjokingly', 'refer', 'time', '1970', 'dark', 'age', 'cryptography', 'cryptography', 'really', 'scientific', 'field', 'say', 'artist', 'attacker', 'need', 'artistic', 'skill', 'invent', 'encryption', 'scheme', 'deploy', 'clever', 'person', 'figure', 'break', 'go', 'change', 'pass', 'say', 'cryptographer', 'diffie', 'describe', 'novel', 'way', 'people', 'devise', 'key', 'know', 'use', 'pass', 'secret', 'message', 'crucially', 'meet', 'groundbreaking', 'notion', 'previously', 'sender', 'receiver', 'physically', 'possess', 'key', 'encoding', 'decode', 'message', 'encode', 'enigma', 'machine', 'example', 'recipient', 'need', 'key', 'sheet', 'reveal', 'initial', 'encryption', 'setting', 'secret', 'diffiehellman', 'strategy', 'people', 'build', 'key', 'use', 'straightforward', 'mathematical', 'problem', 'easy', 'compute', 'direction', 'laborious', 'work', 'people', 'want', 'communicate', 'secretly', 'usually', 'designate', 'setup', 'pick', 'secret', 'number', 'together', 'agree', 'pair', 'number', 'share', 'publicly', 'big', 'prime', 'call', 'base', 'next', 'carry', 'series', 'mathematical', 'operation', 'combine', 'private', 'number', 'prime', 'base', 'exchange', 'result', 'carry', 'series', 'mathematical', 'operation', 'new', 'number', 'end', 'operation', 'number', 'order', 'arrive', 'answer', 'digit', 'answer', 'become', 'encryption', 'eavesdropper', 'intercept', 'transmission', 'often', 'nickname', 'eve', 'win', 'able', 'easily', 'unravel', 'mathematical', 'jumble', 'know', 'least', 'private', 'number', 'start', 'test', 'number', 'bruteforce', 'approach', 'require', 'unreasonable', 'amount', 'calculation', 'complicated', 'problem', 'solve', 'call', 'find', 'discrete', 'logarithm', 'diffiehellman', 'approach', 'still', 'use', 'today', 'secure', 'vpns', 'example', 'integral', 'postquantum', 'scheme', 'paper', 'diffie', 'hellman', 'note', 'exist', 'algorithm', 'capable', 'solve', 'discrete', 'log', 'problem', 'reasonable', 'amount', 'time', 'still', 'go', 'introduce', 'first', 'time', 'notion', 'oneway', 'function', 'basis', 'secure', 'cryptography', 'today', 'secure', 'online', 'interaction', 'involve', 'authentication', 'digital', 'signature', 'example', 'base', 'general', 'idea', 'mathematical', 'proof', 'problem', 'rely', 'oneway', 'function', 'possibility', 'remain', 'discover', 'efficient', 'scheme', 'crack', 'today', 'online', 'transaction', 'begin', 'kind', 'digital', 'handshake', 'security', 'handshake', 'often', 'guarantee', 'math', 'problem', 'presume', 'difficult', 'popular', 'encryption', 'scheme', 'use', 'today', 'introduce', 'trio', 'young', 'computer', 'scientist', 'energize', 'diffie', 'hellman', 'paper', 'call', 'approach', 'rsa', 'last', 'name', 'scientist', 'adi', 'shamir', 'rsa', 'base', 'difficulty', 'find', 'prime', 'factor', 'relative', 'ease', 'multiply', 'together', 'bit', 'different', 'diffiehellman', 'approach', 'diffiehellman', 'share', 'secret', 'allow', 'user', 'devise', 'key', 'insecure', 'channel', 'internet', 'key', 'use', 'disguise', 'message', 'use', 'key', 'base', 'big', 'prime', 'number', 'encrypt', 'message', 'unlock', 'rsa', 'secure', 'datum', 'send', 'person', 'quickly', 'become', 'popular', 'publickey', 'encryption', 'method', 'easy', 'use', 'adapt', 'time', 'new', 'algorithm', 'emerge', 'factor', 'fast', 'computer', 'become', 'powerful', 'nist', 'recommend', 'use', 'large', 'large', 'number', 'security', 'number', 'represent', 'binary', 'form', '1', 'binary', 'digit', 'well', 'know', 'bit', 'number', 'example', 'write', 'binary', 'bit', 'nist', 'currently', 'recommend', 'use', 'key', 'represent', 'least', 'bit', 'correspond', 'number', 'digit', 'date', 'large', 'number', 'factor', 'prime', 'make', 'digit', 'process', 'take', 'nearly', 'hour', 'compute', 'time', 'strength', 'rsa', 'even', 'uncrackable', 'easy', 'keep', 'ante', 'make', 'computationally', 'impractical', 'break', 'however', 'threat', 'different', 'type', 'emerge', 'devise', 'quantum', 'computer', 'solve', 'factoring', 'problem', 'reasonable', 'amount', 'time', 'double', 'threat', 'approach', 'also', 'conquer', 'discrete', 'log', 'problem', 'diffiehellman', 'approach', 'paper', 'ignite', 'excitement', 'anxiety', 'want', 'build', 'quantum', 'computer', 'recognize', 'threat', 'pose', 'cybersecurity', 'fortunately', 'cryptographer', 'quantum', 'computer', 'last', 'decade', 'cybersecurity', 'play', 'increasingly', 'intricate', 'game', 'researcher', 'perpetually', 'build', 'breaking', 'attempt', 'break', 'new', 'candidate', 'year', 'researcher', 'technology', 'sweden', 'estimate', 'take', 'quantum', 'computer', 'compose', 'quantum', 'bit', 'qubit', 'hour', 'break', 'today', '2048bit', 'rsa', 'security', 'current', 'stateoftheart', 'machine', 'nowhere', 'close', 'size', 'large', 'quantum', 'computer', 'date', 'build', 'debut', 'last', 'year', 'qubit', 'rsa', 'consider', 'immediate', 'risk', 'quantum', 'attack', 'depend', 'largely', 'ask', 'say', 'computer', 'scientist', 'te', 'short', 'cofounde', 'cybersecurity', 'company', 'keyfactor', 'see', 'cultural', 'divide', 'theorist', 'study', 'mathematic', 'encryption', 'cryptographer', 'work', 'implementation', 'end', 'seem', 'nigh', 'talk', 'theoretical', 'computer', 'scientist', 'rsa', 'imagine', 'shorter', 'say', 'add', 'existence', 'shor', 'point', 'end', 'encryption', 'know', 'many', 'cryptographer', 'implement', 'realworld', 'security', 'system', 'less', 'concerned', 'quantum', 'future', 'today', 'clever', 'hacker', 'people', 'try', 'factor', 'efficiently', 'thousand', 'year', 'know', 'method', 'require', 'computer', 'exist', 'decru', 'cryptographer', 'belgium', 'say', 'quantum', 'threat', 'take', 'seriously', 'hard', 'know', 'rsa', 'fall', 'quantum', 'computer', 'year', 'long', 'never', 'long', 'quantum', 'computer', 'exist', 'say', 'speculative', 'way', 'say', 'pass', 'certain', 'threat', 'safe', 'say', 'existence', 'quantum', 'mean', 'crack', 'problem', 'right', 'ready', 'say', 'lily', 'mathematician', 'manage', 'nist', 'cryptographic', 'technology', 'group', 'work', 'ongoing', 'effort', 'produce', 'postquantum', 'encryption', 'standard', 'arrive', 'year', 'quantum', 'computer', 'loom', 'horizon', 'rsa', 'diffiehellman', 'encryption', 'scheme', 'leave', 'vulnerable', 'find', 'quantumresistant', 'cryptographic', 'scheme', 'easy', 'mathematical', 'problem', 'computationally', 'hard', 'last', 'decade', 'cybersecurity', 'play', 'increasingly', 'intricate', 'game', 'researcher', 'perpetually', 'build', 'breaking', 'attempt', 'break', 'new', 'candidate', 'push', 'pull', 'already', 'emerge', 'nist', 'postquantum', 'program', 'cryptographer', 'find', 'fatal', 'flaw', 'survive', 'round', 'nist', 'analysis', 'month', 'later', 'nist', 'list', 'winnow', 'decru', 'ku', 'announce', 'break', 'finalist', 'call', 'sike', 'sike', 'develop', 'year', 'ago', 'brainchild', 'collaboration', 'researcher', 'engineer', 'versaille', 'elsewhere', 'base', 'special', 'mathematical', 'map', 'call', 'isogeny', 'make', 'connection', 'elliptic', 'curve', 'map', 'turn', 'encryption', 'communication', 'outsider', 'eavesdrop', 'know', 'map', 'quantum', 'computer', 'become', 'powerful', 'enough', 'theoretically', 'crack', 'encryption', 'algorithm', 'keep', 'safe', 'race', 'find', 'new', 'one', 'leuven', 'decru', 'castryck', 'devise', 'way', 'use', 'socalled', 'isogenie', 'build', 'new', 'fast', 'encryption', 'approach', 'break', 'difficult', 'version', 'sike', 'hour', 'compute', 'time', 'use', 'ordinary', 'desktop', 'computer', 'group', 'find', 'way', 'even', 'fast', 'decru', 'almost', 'accidentally', 'week', 'declare', 'alternate', 'nist', 'finalist', 'try', 'break', 'insist', 'decru', 'try', 'generalize', 'say', 'case', 'sike', 'rainbow', 'illustrate', 'realworld', 'tension', 'drive', 'effort', 'find', 'quantumproof', 'algorithm', 'hand', 'say', 'find', 'problem', 'hard', 'quantum', 'computer', 'classical', 'computer', 'implementation', 'transform', 'hard', 'problem', 'use', 'realworld', 'cryptographic', 'system', 'even', 'today', 'welldefine', 'problem', 'shorter', 'say', 'difficult', 'predict', 'prevent', 'loophole', 'operate', 'system', 'device', 'market', 'today', 'interoperability', 'testing', 'certification', 'test', 'say', 'make', 'sure', 'implement', 'correctly', 'also', 'securely', 'mathematical', 'problem', 'sike', 'base', 'seem', 'computationally', 'hard', 'many', 'different', 'map', 'construct', 'curve', 'even', 'oneway', 'problem', 'therefore', 'quantumproof', 'flaw', 'design', 'reveal', 'much', 'transmit', 'information', 'decru', 'castryck', 'crack', 'inadvertently', 'find', 'way', 'expose', 'enough', 'connect', 'point', 'give', 'entire', 'thing', 'scheme', 'fare', 'well', 'first', 'postquantum', 'encryption', 'standardize', 'crystalskyber', 'deliver', 'security', 'approach', 'involve', 'problem', 'lattice', 'mathematical', 'model', 'array', 'point', 'main', 'family', 'postquantum', 'cryptographic', 'method', 'lattice', 'approach', 'general', 'encryption', 'scheme', 'rsa', 'use', 'task', 'secure', 'online', 'communication', 'approve', 'algorithm', 'design', 'authenticate', 'digital', 'signature', 'ensure', 'digital', 'document', 'fraudulently', 'sign', 'nist', 'plan', 'standardize', 'spring', 'sike', 'break', 'also', 'standardize', 'next', 'year', 'long', 'survive', 'round', 'scrutiny', 'mathematician', 'prove', 'oneway', 'function', 'say', 'pass', 'pattern', 'always', 'characterize', 'cryptography', 'continue', 'back', 'catandmouse', 'game', 'game', 'designer', 'propose', 'new', 'candidate', 'construction', 'designer', 'try', 'break', 'say', 'course', 'field', 'come', 'implementable', 'provably', 'oneway', 'function', 'settle', 'matter', 'encryption', 'forever', 'time', 'cryptographer', 'remain', 'messy', 'limbo', 'convincingly', 'robust', 'encryption', 'scheme', 'trust', 'perfect', 'math', 'problem', 'take', 'limbo', 'sticky', 'mess', 'cook', 'armchair', 'algebraist', 'long', 'weekend', 'strike', 'balance', 'math', 'cryptography', 'computational', 'hardness', 'side', 'easy', 'implementation', 'stray', 'far', 'property', 'become', 'vulnerable', 'future', 'hang', 'balance', 'past', 'present', 'future', 'security', 'datum', 'everywhere', 'pressure', 'orne', 'science', 'writer', 'base']","<p>Cryptographers want encryption schemes that are impossible for tomorrow’s quantum computers to crack. There’s only one catch: they might not exist.</p>
"
"Using data, AI, and cloud to transform real estate",https://www.technologyreview.com/2023/10/16/1081609/using-data-ai-and-cloud-to-transform-real-estate/,2023-10-16,"In partnership withInfosys Cobalt Many industries have reached an inflection point with hybrid and remote work, emerging advanced technologies like AI and cloud computing, and increased demands for sustainable frameworks to mitigate emissions. According to Sandeep Davé, chief digital and technology officer at global firm CBRE, the commercial real estate industry is no stranger to these changes and challenges. Delivering the best outcomes and optimizing operations means forging clear digital strategies for business transformation that are focused on the root of client and business problems. “Through our ethos, we don't chase the shiny object, whether it's AI or any other technology,” says Davé. “We aren't saying, well, what can I do for the purposes of doing AI, but what is the business problem that I'm trying to solve?” While developing a foundational strategy for transformation that is based on enabling the core business is key, advanced technologies like AI and machine learning are powerful tools that can unlock efficiencies across the entire real estate lifecycle, Davé says. AI/ML are incredibly powerful tools to become data-driven from analytics tools that can predict asset failures and market movements to infusing efficiencies across operations. “There are new and different ways in which real estate is viewed, transacted, managed, and all of that gets enabled through data and technology,” says Davé. Beyond operational improvements, advanced and smart technologies can also help reduce emissions. According to a 2019 International Energy Agency global status report, the real estate industry contributed 39% of global carbon emissions attributed to both construction and the life cycle of the asset. As a result, sustainability initiatives have become a priority for a firm of the scale of CBRE, says Davé. “At the time of managing the building, there are many solutions that offer instant gratification, stick sensors up, light up a building, and they all work well if all you need to do is to light up a building. But in order to meet the scale and the global net-zero targets that our clients have set, our solutions need to be at portfolio scale and need to be multidimensional.” Becoming data-driven remains imperative for any organization looking to keep up with the varied and changing needs of clients adapting to changes in the market and technology landscape. “The industry finds itself in the midst of two of the most dominant trends of our time, from return to work to sustainability,” says Davé. “We've seen a step change in technology in terms of what cloud and AI gives us, and all of that, I think, is going to also drive tremendous change. And we'll continue to push the bounds of technology in the service of our clients’ real-world challenges.” This episode of Business Lab is produced in partnership with Infosys Cobalt. Laurel Ruma: From MIT Technology Review, I'm Laurel Ruma and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.Our topic is digital transformation and how clear strategy and emerging technologies such as cloud computing and AI can help transform industries including commercial real estate and propel adoption of sustainability goals.Two words for you: inspiring transformation.My guest today is Sandeep Davé, the chief digital and technology officer at CBRE.This podcast is produced in partnership with Infosys Cobalt.Welcome, Sandeep. Sandeep Davé: Thank you. Thank you for having me on the show. Laurel: Could you talk a bit more about your role at CBRE and what it means to specialize in digital strategy and business transformation at the world's largest commercial real estate company? Sandeep: Sure. And for your audience, maybe perhaps I can give a little bit of a context around CBRE and what we do. So as you said, CBRE is the world's largest commercial real estate services, brokerage, and investment firm. And essentially in that capacity, we help our clients across the entire lifecycle of real estate from investing in an asset to leasing space, to designing it, to building it out, and managing it on an ongoing basis. We are a global company, Fortune 130, operating in over 100-plus countries. We process over 450 billion in transaction volume. We've managed over 7 billion square feet, so it's a sprawling operation. And in my role as chief digital and technology officer, I oversee all aspects of technology for the company, from digital strategy, tech enablement for each one of our business lines, our venture investments and partnerships, and our traditional technology infrastructure as well. Now, with respect to your question around what does it mean to drive digital strategy or business transformation in commercial real estate. Like every industry, technology is impacting commercial real estate. It's a moment of tremendous change. Real estate decisions are getting microsegmented. There are new and different ways in which real estate is viewed, transacted, managed, and all of that gets enabled through data and technology. And I work with the leaders in the company to navigate these changes. Laurel: So what does technological transformation then look like for a global firm like CBRE? Sandeep: Sure. CBRE is a 100-plus year old company and in an industry that was traditionally slow to adopt new technologies. And so we took the transformation in two parts. And in fact, even before I discuss those two parts, I'd like to just step back and set some context. I've spent time in many industries, commercial real estate, financial services, and I have seen two models of digital transformation emerge. There's no model that's right or wrong. But in one model, a company goes out there and declares themselves to be a technology company and the decisions that they take are aligned with them becoming a technology company. They set up software P&Ls, they set up their own venture investment capabilities, and the pursuit is towards those goals. Whereas we are very clear about who we are, which is that we are the largest and best commercial real estate services company, investment brokerage company in the world. But we also realize that data, insights, technology is going to be critical to deliver the best outcomes for our clients. So in that context, our transformation is based on enabling the core business. So for that purpose, we had to do two things. The first was to build a foundation. First few years we were focused on building a global talent pool that I'm very proud of. We've migrated 100% to the cloud, agile, and over the years we've built an enterprise data platform that is meaningful to our transformation. With that foundation in place, we focused on the second part of our transformation, which is to have a clear digital strategy for each line of businesses and therefore focusing on the most pressing problems for our clients. Finally, given the breadth and scale of our operations, we sit on tremendous amounts of data and we've made meaningful strides in creating a data advantage for the company. Laurel: Enterprises are always looking for an advantage, especially with data. Could you outline what that does mean for CBRE, like opportunities and benefits working with all of this data? Sandeep: Sure. Very interesting space and I spent a lot of time in consumer financial services in other consumer industries, and the one difference that was stark to me as I started working in commercial real estate is that this industry was not facing a big data problem, at least not then when each of the buildings were not centralized, but it was more of a siloed data problem. So by no means am I trivializing the data challenges that I faced in financial services. There were many, but the one thing that I had a benefit of was transaction after transaction of structured data that allowed me to slice and dice and understand my customer's behaviors. Whereas, what happens in the commercial real estate industry is that data is very siloed, but yet anchored around a property. And even the definition of the property is different depending on who you ask. Whether you are assessing or valuing the entire building versus I'm trying to lease one floor in that building, what I call property changes. So what we had to do first and foremost was to break down those barriers and we created an enterprise data platform that, against a standard taxonomy, ingests data from 300-plus different data sources and now manages billions of data points. Having sat on that foundation, we now have the ability to generate tremendous insights for our clients. So we are able to, our 350-plus clients, provide them a range of insights at a portfolio scale to a single building level, operational insights, financial insights, occupancy, energy, health and safety risks, so on and so forth. And that same foundation now allows us to unlock efficiencies at a larger scale and apply ML models to generate deeper insights and drive greater transformation. Laurel: So from the automotive industry to healthcare to corporate banking, AI has emerged as a powerful tool across supply chains and industries, and it's changing how enterprises operate and what services they can offer to clients. So in real estate and asset management, what does AI look like and what decisions, especially in the C-suite, are made using this technology to drive better business outcomes? Sandeep: Your question is spot on, which is that how do we use the technology to drive data business outcomes? And through our ethos, we don't chase the shiny object, whether it's AI or any other technology. We aren't saying, well, what can I do for the purposes of doing AI, but what is the business problem that I'm trying to solve? And we have actually been focused on AI/ML, even before the public awareness soared post the release of ChatGPT. But we look at this in the spectrum of unlocking efficiencies to enable differentiation and across the entire lifecycle of real estate, there is a play in both of those–unlocking efficiencies and enabling differentiation. The way the decisions around how property investments are made, how market movements are tracked, are very data-driven now. The decisions around how space is managed is very data-driven now.   Laurel: So could you give an example of how AI and cloud computing could be used to build smart buildings, structures, and technology capabilities? Sandeep: Sure. Using an example is great because this is such a wide field, both commercial real estate and the application of AI/ML in commercial real estate. In the area of smart buildings, we are focused on enabling three outcomes for our clients: energy, efficiency, and experience; which is how do they manage their energy usage, how do they get more efficient in everything that they do with respect to managing a property? And then what is the workplace experience for the employees in a building? And let me just take an example of efficiency. There was a certain way in which buildings were managed previously. And with the application of cloud native global technology solutions, that we have that are infused with AI/ML, we are now able to manage facilities in a smarter manner, what we call Smart FM. We are able to look at occupancy and dynamically clean the environment rather than having people cleaning the environment on a regular schedule, we are able to save our clients a lot of money with respect to dynamic cleaning. We are able to detect anomalies in how we manage buildings and assets, which can then further reduce the false alarms and the number of truck rolls that need to happen with respect to managing a building. So there are so many different ways in which we infuse AI/ML. Laurel: That's really interesting. So according to a 2019 International Energy Agency global status report, the real estate industry contributed 39% of global carbon emissions. Could you offer us an example of how smart technologies, like what you're talking about now, could boost operational efficiencies and then also help reduce emissions and improve sustainability? Sandeep: Yeah, absolutely. I think there are two ways in which we look at this space. As you indicated that 39% of carbon emissions are contributed by real estate, and so therefore the industry has a huge role to play. Part of those emissions are at the time of construction itself, and the remainder is for the life cycle of the asset. Right at the time of construction, we've built capabilities where we are able to design and redesign based on a certain energy emission target for a building. We are able to select our suppliers based on a certain energy emission target for the building. And then at the time of managing the building, there are many solutions that offer instant gratification, stick sensors up, light up a building, and they all work well if all you need to do is to light up a building. But in order to meet the scale and the global net-zero targets that our clients have set, our solutions need to be at portfolio scale and need to be multidimensional. And so therefore what we do is we have the ability to ingest data from various different sources, from sensors, and are able to harmonize that and land it against a standard taxonomy. And then we are able to assess that in many different ways. We are able to bring together different aspects of looking at energy and looking at occupancy and managing the building based on the occupancy in the building. Those interventions, for example, at one of our clients recently, meant we were able to stand up those interventions at 25-plus buildings. And that led to a reduction in peak usage energy for them and also reduction in reactive maintenance work orders, reducing truck rolls, and supporting their energy goals. Laurel: So you also are talking about this on a portfolio level. And CBRE's own corporate responsibility and environmental social and governance or ESG goals are as follows: scale to a low-carbon future, create opportunities for employees to thrive through diversity, equity, inclusion initiatives and to build trust through integrity. How is CBRE using emerging technologies like artificial intelligence and machine learning to then become more efficient and also meet those ESG goals? Sandeep: I think a lot of the ESG problem is a data problem. Today, if you talk to most who are trying and most are grappling with this problem right now, what they'll say is that do they have a clear line of sight of what their, for example, scope 1 and scope 2, scope 3 emissions are? Are they able to capture the data in a reliable manner, audit it in a reliable manner, and then report against it? While they report against it, can they also manage usage? Because if you are able to look at the data, then you will know where corrective actions are required. Building on the foundation of the data platform that we've built on, which is 100% cloud native, by the way, we can then, on top of that, apply these technologies where we can apply ML models to detect anomalies. We take a digital twins perspective to map our data against the buildings and manage the end-to-end lifecycle of that real estate process. Laurel: So looking into the future Sandeep, which I know is difficult, but how do you anticipate the commercial real estate industry transforming in the next five years as emerging technologies, hybrid ways of working, and calls for greater sustainability initiatives become more prevalent? Sandeep: There's always a risk of predictions to a five-year out prediction at a time when the pace of change is dizzying. However, since you've asked, maybe I'll share three points. The first is, let's take an example of financial services. The first is related to just insights. If you take the wealth management industry, information used to be an advantage. Now information is a commodity. Insights are an advantage. Insights that were an advantage yesterday are less of an advantage today, but the wealth manager's job is more important than ever. They're now closer to the client than ever before and they're giving more meaningful advice than ever before. We are seeing very similar changes in commercial real estate today, whether that's investment or leasing decisions, which are getting microsegmented and extremely data-driven. What matters to one client is not going to matter to another client. One client may be most focused on energy goals for their investments. The other client may be focused on labor analytics or gentrification metrics. So how we see the industry moving towards those microsegmented decisions that are data-driven. So that's one area of transformation that I see in the industry and how deeper data insights are going to be really relevant in terms of enabling that. Second, if we take any outsourcing industry, which is also part of what we do, whether that's in BPO [business process outsourcing] or technology outsourcing, they all started with augmentation. You need 10 people. I'm going to give you 10 people. But overtime labor arbitrage moves. You run out of steam quickly and you move to delivering outcomes. And once you have to deliver outcomes, then the model has to be that “I have the best processes, I have the best technology, I have the best data and the best insights, and therefore I am confidently telling you that I'm in a better position to deliver better outcomes for you.” But at the same time, deliver better margins for the company. That's exactly what we are doing in our outsourcing business, and that's a transformation that I see happening in the industry to move away from staff org, if you will, to outcomes based. And third, I think it's a pivotal moment from many different ways. The industry finds itself in the midst of two of the most dominant trends of our time from return to work to sustainability. We've seen a step change in technology in terms of what cloud and AI gives us and all of that I think is going to also drive tremendous change and we'll continue to push the bounds of technology in the service of our clients’ real-world challenges. Laurel: That is certainly comprehensive. Sandeep, thank you so much for joining us today on the Business Lab. Sandeep: Thank you. Thank you for having me. Laurel: That was Sandeep Davé, the chief digital technology officer at CBRE who I spoke with from Cambridge, Massachusetts, the home of MIT and MIT Technology Review overlooking the Charles River.That's it for this episode of Business Lab. I'm your host, Laurel Ruma. I'm the director of Insights, the custom publishing division at MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology, and you can also find us in print, on the web, and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com. This show is available wherever you get your podcasts. If you enjoyed this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT Technology Review. This episode was produced by Giro Studios. Thanks for listening. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.","In partnership withInfosys Cobalt Many industries have reached an inflection point with hybrid and remote work , emerging advanced technologies like AI and cloud computing , and increased demands for sustainable frameworks to mitigate emissions . According to Sandeep Davé , chief digital and technology officer at global firm CBRE , the commercial real estate industry is no stranger to these changes and challenges . Delivering the best outcomes and optimizing operations means forging clear digital strategies for business transformation that are focused on the root of client and business problems . “ Through our ethos , we do n't chase the shiny object , whether it 's AI or any other technology , ” says Davé . “ We are n't saying , well , what can I do for the purposes of doing AI , but what is the business problem that I 'm trying to solve ? ” While developing a foundational strategy for transformation that is based on enabling the core business is key , advanced technologies like AI and machine learning are powerful tools that can unlock efficiencies across the entire real estate lifecycle , Davé says . AI/ML are incredibly powerful tools to become data-driven from analytics tools that can predict asset failures and market movements to infusing efficiencies across operations . “ There are new and different ways in which real estate is viewed , transacted , managed , and all of that gets enabled through data and technology , ” says Davé . Beyond operational improvements , advanced and smart technologies can also help reduce emissions . According to a 2019 International Energy Agency global status report , the real estate industry contributed 39 % of global carbon emissions attributed to both construction and the life cycle of the asset . As a result , sustainability initiatives have become a priority for a firm of the scale of CBRE , says Davé . “ At the time of managing the building , there are many solutions that offer instant gratification , stick sensors up , light up a building , and they all work well if all you need to do is to light up a building . But in order to meet the scale and the global net-zero targets that our clients have set , our solutions need to be at portfolio scale and need to be multidimensional. ” Becoming data-driven remains imperative for any organization looking to keep up with the varied and changing needs of clients adapting to changes in the market and technology landscape . “ The industry finds itself in the midst of two of the most dominant trends of our time , from return to work to sustainability , ” says Davé . “ We 've seen a step change in technology in terms of what cloud and AI gives us , and all of that , I think , is going to also drive tremendous change . And we 'll continue to push the bounds of technology in the service of our clients ’ real-world challenges. ” This episode of Business Lab is produced in partnership with Infosys Cobalt . Laurel Ruma : From MIT Technology Review , I 'm Laurel Ruma and this is Business Lab , the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.Our topic is digital transformation and how clear strategy and emerging technologies such as cloud computing and AI can help transform industries including commercial real estate and propel adoption of sustainability goals.Two words for you : inspiring transformation.My guest today is Sandeep Davé , the chief digital and technology officer at CBRE.This podcast is produced in partnership with Infosys Cobalt.Welcome , Sandeep . Sandeep Davé : Thank you . Thank you for having me on the show . Laurel : Could you talk a bit more about your role at CBRE and what it means to specialize in digital strategy and business transformation at the world 's largest commercial real estate company ? Sandeep : Sure . And for your audience , maybe perhaps I can give a little bit of a context around CBRE and what we do . So as you said , CBRE is the world 's largest commercial real estate services , brokerage , and investment firm . And essentially in that capacity , we help our clients across the entire lifecycle of real estate from investing in an asset to leasing space , to designing it , to building it out , and managing it on an ongoing basis . We are a global company , Fortune 130 , operating in over 100-plus countries . We process over 450 billion in transaction volume . We 've managed over 7 billion square feet , so it 's a sprawling operation . And in my role as chief digital and technology officer , I oversee all aspects of technology for the company , from digital strategy , tech enablement for each one of our business lines , our venture investments and partnerships , and our traditional technology infrastructure as well . Now , with respect to your question around what does it mean to drive digital strategy or business transformation in commercial real estate . Like every industry , technology is impacting commercial real estate . It 's a moment of tremendous change . Real estate decisions are getting microsegmented . There are new and different ways in which real estate is viewed , transacted , managed , and all of that gets enabled through data and technology . And I work with the leaders in the company to navigate these changes . Laurel : So what does technological transformation then look like for a global firm like CBRE ? Sandeep : Sure . CBRE is a 100-plus year old company and in an industry that was traditionally slow to adopt new technologies . And so we took the transformation in two parts . And in fact , even before I discuss those two parts , I 'd like to just step back and set some context . I 've spent time in many industries , commercial real estate , financial services , and I have seen two models of digital transformation emerge . There 's no model that 's right or wrong . But in one model , a company goes out there and declares themselves to be a technology company and the decisions that they take are aligned with them becoming a technology company . They set up software P & Ls , they set up their own venture investment capabilities , and the pursuit is towards those goals . Whereas we are very clear about who we are , which is that we are the largest and best commercial real estate services company , investment brokerage company in the world . But we also realize that data , insights , technology is going to be critical to deliver the best outcomes for our clients . So in that context , our transformation is based on enabling the core business . So for that purpose , we had to do two things . The first was to build a foundation . First few years we were focused on building a global talent pool that I 'm very proud of . We 've migrated 100 % to the cloud , agile , and over the years we 've built an enterprise data platform that is meaningful to our transformation . With that foundation in place , we focused on the second part of our transformation , which is to have a clear digital strategy for each line of businesses and therefore focusing on the most pressing problems for our clients . Finally , given the breadth and scale of our operations , we sit on tremendous amounts of data and we 've made meaningful strides in creating a data advantage for the company . Laurel : Enterprises are always looking for an advantage , especially with data . Could you outline what that does mean for CBRE , like opportunities and benefits working with all of this data ? Sandeep : Sure . Very interesting space and I spent a lot of time in consumer financial services in other consumer industries , and the one difference that was stark to me as I started working in commercial real estate is that this industry was not facing a big data problem , at least not then when each of the buildings were not centralized , but it was more of a siloed data problem . So by no means am I trivializing the data challenges that I faced in financial services . There were many , but the one thing that I had a benefit of was transaction after transaction of structured data that allowed me to slice and dice and understand my customer 's behaviors . Whereas , what happens in the commercial real estate industry is that data is very siloed , but yet anchored around a property . And even the definition of the property is different depending on who you ask . Whether you are assessing or valuing the entire building versus I 'm trying to lease one floor in that building , what I call property changes . So what we had to do first and foremost was to break down those barriers and we created an enterprise data platform that , against a standard taxonomy , ingests data from 300-plus different data sources and now manages billions of data points . Having sat on that foundation , we now have the ability to generate tremendous insights for our clients . So we are able to , our 350-plus clients , provide them a range of insights at a portfolio scale to a single building level , operational insights , financial insights , occupancy , energy , health and safety risks , so on and so forth . And that same foundation now allows us to unlock efficiencies at a larger scale and apply ML models to generate deeper insights and drive greater transformation . Laurel : So from the automotive industry to healthcare to corporate banking , AI has emerged as a powerful tool across supply chains and industries , and it 's changing how enterprises operate and what services they can offer to clients . So in real estate and asset management , what does AI look like and what decisions , especially in the C-suite , are made using this technology to drive better business outcomes ? Sandeep : Your question is spot on , which is that how do we use the technology to drive data business outcomes ? And through our ethos , we do n't chase the shiny object , whether it 's AI or any other technology . We are n't saying , well , what can I do for the purposes of doing AI , but what is the business problem that I 'm trying to solve ? And we have actually been focused on AI/ML , even before the public awareness soared post the release of ChatGPT . But we look at this in the spectrum of unlocking efficiencies to enable differentiation and across the entire lifecycle of real estate , there is a play in both of those–unlocking efficiencies and enabling differentiation . The way the decisions around how property investments are made , how market movements are tracked , are very data-driven now . The decisions around how space is managed is very data-driven now . Laurel : So could you give an example of how AI and cloud computing could be used to build smart buildings , structures , and technology capabilities ? Sandeep : Sure . Using an example is great because this is such a wide field , both commercial real estate and the application of AI/ML in commercial real estate . In the area of smart buildings , we are focused on enabling three outcomes for our clients : energy , efficiency , and experience ; which is how do they manage their energy usage , how do they get more efficient in everything that they do with respect to managing a property ? And then what is the workplace experience for the employees in a building ? And let me just take an example of efficiency . There was a certain way in which buildings were managed previously . And with the application of cloud native global technology solutions , that we have that are infused with AI/ML , we are now able to manage facilities in a smarter manner , what we call Smart FM . We are able to look at occupancy and dynamically clean the environment rather than having people cleaning the environment on a regular schedule , we are able to save our clients a lot of money with respect to dynamic cleaning . We are able to detect anomalies in how we manage buildings and assets , which can then further reduce the false alarms and the number of truck rolls that need to happen with respect to managing a building . So there are so many different ways in which we infuse AI/ML . Laurel : That 's really interesting . So according to a 2019 International Energy Agency global status report , the real estate industry contributed 39 % of global carbon emissions . Could you offer us an example of how smart technologies , like what you 're talking about now , could boost operational efficiencies and then also help reduce emissions and improve sustainability ? Sandeep : Yeah , absolutely . I think there are two ways in which we look at this space . As you indicated that 39 % of carbon emissions are contributed by real estate , and so therefore the industry has a huge role to play . Part of those emissions are at the time of construction itself , and the remainder is for the life cycle of the asset . Right at the time of construction , we 've built capabilities where we are able to design and redesign based on a certain energy emission target for a building . We are able to select our suppliers based on a certain energy emission target for the building . And then at the time of managing the building , there are many solutions that offer instant gratification , stick sensors up , light up a building , and they all work well if all you need to do is to light up a building . But in order to meet the scale and the global net-zero targets that our clients have set , our solutions need to be at portfolio scale and need to be multidimensional . And so therefore what we do is we have the ability to ingest data from various different sources , from sensors , and are able to harmonize that and land it against a standard taxonomy . And then we are able to assess that in many different ways . We are able to bring together different aspects of looking at energy and looking at occupancy and managing the building based on the occupancy in the building . Those interventions , for example , at one of our clients recently , meant we were able to stand up those interventions at 25-plus buildings . And that led to a reduction in peak usage energy for them and also reduction in reactive maintenance work orders , reducing truck rolls , and supporting their energy goals . Laurel : So you also are talking about this on a portfolio level . And CBRE 's own corporate responsibility and environmental social and governance or ESG goals are as follows : scale to a low-carbon future , create opportunities for employees to thrive through diversity , equity , inclusion initiatives and to build trust through integrity . How is CBRE using emerging technologies like artificial intelligence and machine learning to then become more efficient and also meet those ESG goals ? Sandeep : I think a lot of the ESG problem is a data problem . Today , if you talk to most who are trying and most are grappling with this problem right now , what they 'll say is that do they have a clear line of sight of what their , for example , scope 1 and scope 2 , scope 3 emissions are ? Are they able to capture the data in a reliable manner , audit it in a reliable manner , and then report against it ? While they report against it , can they also manage usage ? Because if you are able to look at the data , then you will know where corrective actions are required . Building on the foundation of the data platform that we 've built on , which is 100 % cloud native , by the way , we can then , on top of that , apply these technologies where we can apply ML models to detect anomalies . We take a digital twins perspective to map our data against the buildings and manage the end-to-end lifecycle of that real estate process . Laurel : So looking into the future Sandeep , which I know is difficult , but how do you anticipate the commercial real estate industry transforming in the next five years as emerging technologies , hybrid ways of working , and calls for greater sustainability initiatives become more prevalent ? Sandeep : There 's always a risk of predictions to a five-year out prediction at a time when the pace of change is dizzying . However , since you 've asked , maybe I 'll share three points . The first is , let 's take an example of financial services . The first is related to just insights . If you take the wealth management industry , information used to be an advantage . Now information is a commodity . Insights are an advantage . Insights that were an advantage yesterday are less of an advantage today , but the wealth manager 's job is more important than ever . They 're now closer to the client than ever before and they 're giving more meaningful advice than ever before . We are seeing very similar changes in commercial real estate today , whether that 's investment or leasing decisions , which are getting microsegmented and extremely data-driven . What matters to one client is not going to matter to another client . One client may be most focused on energy goals for their investments . The other client may be focused on labor analytics or gentrification metrics . So how we see the industry moving towards those microsegmented decisions that are data-driven . So that 's one area of transformation that I see in the industry and how deeper data insights are going to be really relevant in terms of enabling that . Second , if we take any outsourcing industry , which is also part of what we do , whether that 's in BPO [ business process outsourcing ] or technology outsourcing , they all started with augmentation . You need 10 people . I 'm going to give you 10 people . But overtime labor arbitrage moves . You run out of steam quickly and you move to delivering outcomes . And once you have to deliver outcomes , then the model has to be that “ I have the best processes , I have the best technology , I have the best data and the best insights , and therefore I am confidently telling you that I 'm in a better position to deliver better outcomes for you. ” But at the same time , deliver better margins for the company . That 's exactly what we are doing in our outsourcing business , and that 's a transformation that I see happening in the industry to move away from staff org , if you will , to outcomes based . And third , I think it 's a pivotal moment from many different ways . The industry finds itself in the midst of two of the most dominant trends of our time from return to work to sustainability . We 've seen a step change in technology in terms of what cloud and AI gives us and all of that I think is going to also drive tremendous change and we 'll continue to push the bounds of technology in the service of our clients ’ real-world challenges . Laurel : That is certainly comprehensive . Sandeep , thank you so much for joining us today on the Business Lab . Sandeep : Thank you . Thank you for having me . Laurel : That was Sandeep Davé , the chief digital technology officer at CBRE who I spoke with from Cambridge , Massachusetts , the home of MIT and MIT Technology Review overlooking the Charles River.That 's it for this episode of Business Lab . I 'm your host , Laurel Ruma . I 'm the director of Insights , the custom publishing division at MIT Technology Review . We were founded in 1899 at the Massachusetts Institute of Technology , and you can also find us in print , on the web , and at events each year around the world . For more information about us and the show , please check out our website at technologyreview.com . This show is available wherever you get your podcasts . If you enjoyed this episode , we hope you 'll take a moment to rate and review us . Business Lab is a production of MIT Technology Review . This episode was produced by Giro Studios . Thanks for listening . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['partnership', 'cobalt', 'many', 'industry', 'reach', 'inflection', 'point', 'hybrid', 'remote', 'work', 'emerge', 'advanced', 'technology', 'ai', 'computing', 'increase', 'demand', 'sustainable', 'framework', 'mitigate', 'emission', 'accord', 'sandeep', 'davé', 'chief', 'digital', 'technology', 'officer', 'global', 'firm', 'cbre', 'commercial', 'real', 'estate', 'industry', 'stranger', 'change', 'challenge', 'deliver', 'good', 'outcome', 'optimize', 'operation', 'mean', 'forge', 'clear', 'digital', 'strategy', 'business', 'transformation', 'focus', 'root', 'client', 'business', 'problem', 'ethos', 'chase', 'shiny', 'object', 'ai', 'technology', 'say', 'davé', 'say', 'well', 'purpose', 'ai', 'business', 'problem', 'try', 'solve', 'develop', 'foundational', 'strategy', 'transformation', 'base', 'enable', 'core', 'business', 'key', 'advanced', 'technology', 'ai', 'machine', 'learning', 'powerful', 'tool', 'unlock', 'efficiency', 'entire', 'real', 'estate', 'lifecycle', 'davé', 'say', 'aiml', 'incredibly', 'powerful', 'tool', 'become', 'datadriven', 'analytic', 'tool', 'predict', 'asset', 'failure', 'market', 'movement', 'infuse', 'efficiency', 'operation', 'new', 'different', 'way', 'real', 'estate', 'view', 'transact', 'manage', 'enable', 'datum', 'technology', 'say', 'davé', 'operational', 'improvement', 'advanced', 'smart', 'technology', 'also', 'help', 'reduce', 'emission', 'accord', 'international', 'energy', 'agency', 'report', 'real', 'estate', 'industry', 'contribute', 'global', 'carbon', 'emission', 'attribute', 'construction', 'life', 'cycle', 'asset', 'result', 'sustainability', 'initiative', 'become', 'priority', 'firm', 'scale', 'cbre', 'say', 'davé', 'time', 'manage', 'building', 'many', 'solution', 'offer', 'instant', 'gratification', 'stick', 'sensor', 'light', 'building', 'work', 'well', 'need', 'light', 'building', 'order', 'meet', 'scale', 'global', 'target', 'client', 'set', 'solution', 'need', 'portfolio', 'scale', 'need', 'multidimensional', 'become', 'datadriven', 'remain', 'imperative', 'organization', 'look', 'keep', 'varied', 'change', 'need', 'client', 'adapt', 'change', 'market', 'technology', 'landscape', 'industry', 'find', 'midst', 'dominant', 'trend', 'time', 'return', 'work', 'sustainability', 'say', 'davé', 'see', 'step', 'change', 'technology', 'term', 'cloud', 'give', 'think', 'go', 'also', 'drive', 'tremendous', 'change', 'continue', 'push', 'bound', 'technology', 'service', 'client', 'realworld', 'challenge', 'episode', 'business', 'lab', 'produce', 'partnership', 'infosy', 'cobalt', 'mit', 'technology', 'review', 'business', 'lab', 'show', 'help', 'business', 'leader', 'make', 'sense', 'new', 'technology', 'come', 'lab', 'marketplaceour', 'topic', 'digital', 'transformation', 'clear', 'strategy', 'emerge', 'technology', 'cloud', 'computing', 'help', 'transform', 'industry', 'include', 'commercial', 'real', 'estate', 'propel', 'adoption', 'sustainability', 'goalstwo', 'word', 'inspire', 'transformationmy', 'guest', 'today', 'sandeep', 'davé', 'chief', 'digital', 'technology', 'officer', 'podcast', 'produce', 'partnership', 'infosy', 'cobaltwelcome', 'sandeep', 'thank', 'thank', 'show', 'laurel', 'talk', 'bit', 'role', 'cbre', 'mean', 'specialize', 'digital', 'strategy', 'business', 'transformation', 'world', 'large', 'commercial', 'real', 'estate', 'company', 'sandeep', 'sure', 'audience', 'maybe', 'perhaps', 'give', 'little', 'bit', 'context', 'cbre', 'say', 'cbre', 'world', 'large', 'commercial', 'real', 'estate', 'service', 'brokerage', 'investment', 'firm', 'essentially', 'capacity', 'help', 'client', 'entire', 'lifecycle', 'real', 'estate', 'invest', 'asset', 'leasing', 'space', 'design', 'build', 'manage', 'ongoing', 'basis', 'global', 'company', 'operate', 'country', 'process', 'transaction', 'volume', 'manage', 'square', 'foot', 'sprawl', 'operation', 'role', 'chief', 'digital', 'technology', 'officer', 'oversee', 'aspect', 'technology', 'company', 'digital', 'strategy', 'tech', 'enablement', 'business', 'line', 'venture', 'investment', 'partnership', 'traditional', 'technology', 'infrastructure', 'well', 'respect', 'question', 'mean', 'drive', 'digital', 'strategy', 'business', 'transformation', 'commercial', 'real', 'estate', 'industry', 'technology', 'impact', 'commercial', 'real', 'estate', 'moment', 'tremendous', 'change', 'real', 'estate', 'decision', 'microsegmente', 'new', 'different', 'way', 'real', 'estate', 'view', 'transact', 'manage', 'enable', 'datum', 'technology', 'work', 'leader', 'company', 'navigate', 'change', 'laurel', 'technological', 'transformation', 'look', 'global', 'firm', 'cbre', 'sandeep', 'sure', 'cbre', 'year', 'old', 'company', 'industry', 'traditionally', 'slow', 'adopt', 'new', 'technology', 'take', 'transformation', 'part', 'fact', 'even', 'discuss', 'part', 'like', 'step', 'back', 'set', 'context', 'spend', 'time', 'many', 'industry', 'commercial', 'real', 'estate', 'financial', 'service', 'see', 'model', 'digital', 'transformation', 'emerge', 'model', 'right', 'wrong', 'model', 'company', 'go', 'declare', 'technology', 'company', 'decision', 'take', 'align', 'become', 'technology', 'company', 'set', 'software', 'p', 'set', 'venture', 'investment', 'capability', 'pursuit', 'goal', 'clear', 'large', 'good', 'commercial', 'real', 'estate', 'service', 'company', 'investment', 'brokerage', 'company', 'world', 'also', 'realize', 'datum', 'insight', 'technology', 'go', 'critical', 'deliver', 'good', 'outcome', 'client', 'context', 'transformation', 'base', 'enable', 'core', 'business', 'purpose', 'thing', 'first', 'build', 'foundation', 'first', 'year', 'focus', 'build', 'global', 'talent', 'pool', 'proud', 'migrate', 'cloud', 'agile', 'year', 'build', 'enterprise', 'datum', 'platform', 'meaningful', 'transformation', 'foundation', 'place', 'focus', 'second', 'part', 'transformation', 'clear', 'digital', 'strategy', 'line', 'business', 'therefore', 'focus', 'pressing', 'problem', 'client', 'finally', 'give', 'breadth', 'scale', 'operation', 'sit', 'tremendous', 'amount', 'datum', 'make', 'meaningful', 'stride', 'create', 'data', 'advantage', 'company', 'laurel', 'enterprise', 'always', 'look', 'advantage', 'especially', 'datum', 'outline', 'mean', 'cbre', 'opportunity', 'benefit', 'work', 'datum', 'sandeep', 'sure', 'interesting', 'space', 'spend', 'lot', 'time', 'consumer', 'financial', 'service', 'consumer', 'industry', 'difference', 'stark', 'start', 'work', 'commercial', 'real', 'estate', 'industry', 'face', 'big', 'data', 'problem', 'least', 'building', 'centralize', 'siloe', 'data', 'problem', 'means', 'trivialize', 'datum', 'challenge', 'face', 'financial', 'service', 'many', 'thing', 'benefit', 'transaction', 'transaction', 'structured', 'datum', 'allow', 'slice', 'dice', 'understand', 'customer', 'behavior', 'happen', 'commercial', 'real', 'estate', 'industry', 'datum', 'siloe', 'yet', 'anchor', 'property', 'even', 'definition', 'property', 'different', 'depend', 'ask', 'assess', 'value', 'entire', 'building', 'try', 'lease', 'floor', 'building', 'call', 'property', 'change', 'first', 'foremost', 'break', 'barrier', 'create', 'enterprise', 'datum', 'platform', 'standard', 'taxonomy', 'ingest', 'datum', 'different', 'datum', 'source', 'manage', 'billion', 'data', 'point', 'sit', 'foundation', 'ability', 'generate', 'tremendous', 'insight', 'client', 'able', 'client', 'provide', 'range', 'insight', 'portfolio', 'scale', 'single', 'building', 'level', 'operational', 'insight', 'financial', 'insight', 'occupancy', 'energy', 'health', 'safety', 'risk', 'forth', 'foundation', 'allow', 'unlock', 'efficiency', 'large', 'scale', 'apply', 'ml', 'model', 'generate', 'deep', 'insight', 'drive', 'great', 'transformation', 'laurel', 'automotive', 'industry', 'healthcare', 'corporate', 'banking', 'ai', 'emerge', 'powerful', 'tool', 'supply', 'chain', 'industry', 'change', 'enterprise', 'operate', 'service', 'offer', 'client', 'real', 'estate', 'asset', 'management', 'look', 'decision', 'especially', 'csuite', 'make', 'use', 'technology', 'drive', 'well', 'business', 'outcome', 'sandeep', 'question', 'spot', 'use', 'technology', 'drive', 'datum', 'business', 'outcome', 'ethos', 'chase', 'shiny', 'object', 'ai', 'technology', 'say', 'well', 'purpose', 'ai', 'business', 'problem', 'try', 'solve', 'actually', 'focus', 'aiml', 'even', 'public', 'awareness', 'soar', 'post', 'release', 'chatgpt', 'look', 'spectrum', 'unlock', 'efficiency', 'enable', 'differentiation', 'entire', 'lifecycle', 'real', 'estate', 'play', 'unlocking', 'efficiency', 'enable', 'differentiation', 'way', 'decision', 'property', 'investment', 'make', 'market', 'movement', 'track', 'datadriven', 'decision', 'space', 'manage', 'datadriven', 'give', 'example', 'ai', 'cloud', 'computing', 'use', 'build', 'smart', 'building', 'structure', 'technology', 'capability', 'sandeep', 'sure', 'use', 'example', 'great', 'wide', 'field', 'commercial', 'real', 'estate', 'application', 'aiml', 'commercial', 'real', 'estate', 'area', 'smart', 'building', 'focus', 'enable', 'outcome', 'client', 'energy', 'efficiency', 'experience', 'manage', 'energy', 'usage', 'get', 'efficient', 'respect', 'manage', 'property', 'workplace', 'experience', 'employee', 'building', 'let', 'take', 'example', 'efficiency', 'certain', 'way', 'building', 'manage', 'previously', 'application', 'cloud', 'native', 'global', 'technology', 'solution', 'infuse', 'aiml', 'able', 'manage', 'facility', 'smart', 'manner', 'call', 'smart', 'able', 'look', 'occupancy', 'dynamically', 'clean', 'environment', 'rather', 'people', 'clean', 'environment', 'regular', 'schedule', 'able', 'save', 'client', 'lot', 'money', 'respect', 'dynamic', 'cleaning', 'able', 'detect', 'anomaly', 'manage', 'building', 'asset', 'far', 'reduce', 'false', 'alarm', 'number', 'truck', 'roll', 'need', 'happen', 'respect', 'manage', 'building', 'many', 'different', 'way', 'infuse', 'aiml', 'laurel', 'really', 'interesting', 'accord', 'international', 'energy', 'agency', 'report', 'real', 'estate', 'industry', 'contribute', 'global', 'carbon', 'emission', 'offer', 'example', 'smart', 'technology', 'talk', 'boost', 'operational', 'efficiency', 'also', 'help', 'reduce', 'emission', 'improve', 'sustainability', 'sandeep', 'absolutely', 'think', 'way', 'look', 'space', 'indicate', 'carbon', 'emission', 'contribute', 'real', 'estate', 'therefore', 'industry', 'huge', 'role', 'play', 'part', 'emission', 'time', 'construction', 'remainder', 'life', 'cycle', 'asset', 'right', 'time', 'construction', 'build', 'capability', 'able', 'design', 'redesign', 'base', 'certain', 'energy', 'emission', 'target', 'building', 'able', 'select', 'supplier', 'base', 'certain', 'energy', 'emission', 'target', 'building', 'time', 'manage', 'building', 'many', 'solution', 'offer', 'instant', 'gratification', 'stick', 'sensor', 'light', 'building', 'work', 'well', 'need', 'light', 'building', 'order', 'meet', 'scale', 'global', 'target', 'client', 'set', 'solution', 'need', 'portfolio', 'scale', 'need', 'multidimensional', 'therefore', 'ability', 'ingest', 'datum', 'various', 'different', 'source', 'sensor', 'able', 'harmonize', 'land', 'standard', 'taxonomy', 'able', 'assess', 'many', 'different', 'way', 'able', 'bring', 'together', 'different', 'aspect', 'look', 'energy', 'look', 'occupancy', 'manage', 'building', 'base', 'occupancy', 'building', 'intervention', 'example', 'client', 'recently', 'mean', 'able', 'stand', 'intervention', 'building', 'lead', 'reduction', 'peak', 'usage', 'energy', 'also', 'reduction', 'reactive', 'maintenance', 'work', 'order', 'reduce', 'truck', 'roll', 'support', 'energy', 'goal', 'also', 'talk', 'portfolio', 'level', 'corporate', 'responsibility', 'environmental', 'social', 'governance', 'esg', 'goal', 'follow', 'scale', 'lowcarbon', 'future', 'create', 'opportunity', 'employee', 'thrive', 'diversity', 'equity', 'inclusion', 'initiative', 'build', 'trust', 'integrity', 'cbre', 'use', 'emerge', 'technology', 'artificial', 'intelligence', 'machine', 'learn', 'become', 'efficient', 'also', 'meet', 'esg', 'goal', 'sandeep', 'think', 'lot', 'esg', 'problem', 'data', 'problem', 'today', 'talk', 'try', 'grapple', 'problem', 'right', 'say', 'clear', 'line', 'sight', 'example', 'scope', 'scope', 'scope', 'emission', 'able', 'capture', 'datum', 'reliable', 'manner', 'audit', 'reliable', 'manner', 'report', 'report', 'also', 'manage', 'usage', 'able', 'look', 'data', 'know', 'corrective', 'action', 'require', 'building', 'foundation', 'datum', 'platform', 'build', 'cloud', 'native', 'way', 'top', 'apply', 'technology', 'apply', 'ml', 'model', 'detect', 'anomaly', 'take', 'digital', 'twin', 'perspective', 'map', 'datum', 'building', 'manage', 'endtoend', 'lifecycle', 'real', 'estate', 'process', 'laurel', 'look', 'future', 'sandeep', 'know', 'difficult', 'anticipate', 'commercial', 'real', 'estate', 'industry', 'transform', 'next', 'year', 'emerge', 'technology', 'hybrid', 'way', 'working', 'call', 'great', 'sustainability', 'initiative', 'become', 'prevalent', 'sandeep', 'always', 'risk', 'prediction', 'fiveyear', 'prediction', 'time', 'pace', 'change', 'dizzy', 'however', 'ask', 'maybe', 'share', 'point', 'first', 'let', 'take', 'example', 'financial', 'service', 'first', 'relate', 'insight', 'take', 'wealth', 'management', 'industry', 'information', 'use', 'advantage', 'information', 'commodity', 'insight', 'advantage', 'insight', 'advantage', 'yesterday', 'less', 'advantage', 'today', 'wealth', 'manager', 'job', 'important', 'ever', 'close', 'client', 'ever', 'give', 'meaningful', 'advice', 'ever', 'see', 'similar', 'change', 'commercial', 'real', 'estate', 'today', 'investment', 'leasing', 'decision', 'get', 'microsegmented', 'extremely', 'datadriven', 'matter', 'client', 'go', 'matter', 'client', 'client', 'focused', 'energy', 'goal', 'investment', 'client', 'focus', 'labor', 'analytic', 'gentrification', 'metric', 'see', 'industry', 'move', 'microsegmented', 'decision', 'datadriven', 'area', 'transformation', 'see', 'industry', 'deep', 'data', 'insight', 'go', 'really', 'relevant', 'term', 'enable', 'second', 'take', 'outsourcing', 'industry', 'also', 'part', 'business', 'process', 'outsourcing', 'technology', 'outsourcing', 'start', 'augmentation', 'need', 'people', 'go', 'give', 'people', 'overtime', 'labor', 'arbitrage', 'move', 'run', 'steam', 'quickly', 'move', 'deliver', 'outcome', 'deliver', 'outcome', 'model', 'good', 'process', 'good', 'technology', 'good', 'datum', 'good', 'insight', 'therefore', 'confidently', 'tell', 'well', 'position', 'deliver', 'well', 'outcome', 'time', 'deliver', 'well', 'margin', 'company', 'exactly', 'outsourcing', 'business', 'transformation', 'see', 'happen', 'industry', 'move', 'away', 'staff', 'org', 'outcome', 'base', 'third', 'think', 'pivotal', 'moment', 'many', 'different', 'way', 'industry', 'find', 'midst', 'dominant', 'trend', 'time', 'return', 'work', 'sustainability', 'see', 'step', 'change', 'technology', 'term', 'cloud', 'give', 'think', 'go', 'also', 'drive', 'tremendous', 'change', 'continue', 'push', 'bound', 'technology', 'service', 'client', 'realworld', 'challenge', 'laurel', 'certainly', 'comprehensive', 'sandeep', 'thank', 'much', 'join', 'today', 'business', 'lab', 'sandeep', 'thank', 'thank', 'laurel', 'sandeep', 'davé', 'chief', 'digital', 'technology', 'officer', 'cbre', 'speak', 'home', 'mit', 'mit', 'technology', 'review', 'overlook', 'charle', 'episode', 'business', 'lab', 'host', 'director', 'insight', 'custom', 'publishing', 'division', 'mit', 'technology', 'review', 'found', 'also', 'find', 'print', 'web', 'event', 'year', 'world', 'information', 'show', 'check', 'website', 'show', 'available', 'get', 'podcast', 'enjoy', 'episode', 'hope', 'take', 'moment', 'rate', 'review', 'business', 'lab', 'production', 'mit', 'technology', 'review', 'episode', 'produce', 'studio', 'thank', 'listen', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","Many industries have reached an inflection point with hybrid and remote work, emerging advanced technologies like AI and cloud computing, and increased demands for sustainable frameworks to mitigate emissions. According to Sandeep Davé, chief digital and technology officer at global firm CBRE, the commercial real estate industry is no stranger to these changes and challenges.…"
Ethical implications of ChatGPT in higher education: A scoping review,"[{'href': 'http://arxiv.org/abs/2311.14378v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2311.14378v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-11-24 09:52:49,"1 

   Ethical implications of ChatGPT in higher education: A scoping review 

Ming Li1, Ariunaa Enkhtur2, Fei Cheng3, Beverley Anne Yamamoto4 

1Institue for Transdisciplinary Graduate Degree Programs, Osaka University 
2Center for Global Initiatives, Osaka University   
3 Graduate School of Informatics, Kyoto University 
4Graduate School of Human Sciences Human Sciences, Osaka University 

Keyword: ChatGPT, education, ethic, Generative Artificial Intelligence, higher education, 

scoping review 

ABSTRACT 

This  scoping  review  explores  the  ethical  challenges  of  using  ChatGPT  in  education, 

focusing  particularly  on  issues  related  to  higher  education.  By  reviewing  recent  academic 

articles  written  in  English,  Chinese,  and  Japanese,  we  aimed  to  provide  a  comprehensive 

overview  of  relevant  research  while  identifying  gaps  for  future  considerations.  Drawing  on 

Arksey  &  O’Malley’s  (2005)  five-stage  scoping  review  framework,  we  identified  research 

questions, search terms, and conducted article search from four databases in the target three 

languages. Each article was reviewed by at least two researchers identifying main ethical issues 

of  utilizing  AI  in  education,  particularly  higher  education.  Our  analysis  of  ethical  issues 

followed the framework developed by DeepMind (Weiginger et al., 2021) to identify six main 

areas  of  ethical  concern  in  Language  Models.  The  majority  of  papers  were  concerned  with 

misinformation harms (n=25) and/or human-computer interaction related harms (n=24). Given 

the rapid deployment of Generative Artificial Intelligence (GAI), it is imperative for educators 

to conduct more empirical studies to develop sound ethical policies for the use of GAI.  

 
 
 
 
 
 
  
 
  
2 

INTRODUCTION 

    The recent wave of Generative Artificial Intelligence (GAI) originated with Google’s 

Transformer architecture of neural networks (Vaswani et al., 2017). Transformer is based on 

the  self-attention  mechanism,  which  is  highly  scalable  and  suitable  for  parallel  computing, 

hence it quickly became the de facto approach in the fields of natural language processing and 

computer vision. In 2018, language models like BERT (Devlin et al., 2018) and GPT (Radford 

et al., 2018) ushered in the era of self-supervised learning, which are pre-trained on large-scale 

textual data for learning fundamental knowledge, and then fine-tuned for specific downstream 

tasks such as QA, dialog and machine translation systems. Kaplan et al. (2018) validated the 

large  language  models’  scale  law,  asserting  that  the  performance  improvements  can  almost 

always be achieved by increasing the scale of model parameters and pre-training more textual 

data such as Wikipedia, book and internet resources. After this, the rapid expansion for the size 

of large language models began, with  the model scale growing  from  BERT’s 0.3B (billion) 

parameters to GPT3’s 175B (Brown et al. 2020). 

    Since  the  release  of  GPT3  release  in  June  2020,  OpenAI  and  DeepMind,  leading 

developers of this technology, became less satisfied with mere scale increases. Ouyang et al. 

(2022) and Chung et al. (2022) sought to align models’ outputs with the feedback of real human 

beings. The text generated by these models became increasingly human-like, and the content 

ever-more closely aligned with human values. The launch of ChatGPT in November 2022 made 

the  public  aware  that  GAI  was  already  capable  of  generating  human-quality  conversation, 

retrieving stored knowledge on demand, and achieving natural interaction with people as an AI 

assistant. This kicked off the current frenzy of adapting the utilization of GAI to various fields, 

including education. 

    ChatGPT’s  application  in  higher  education  has  garnered  attention  (UNESCO 

IESALC,  2023).  While  there  has  been  much  discussion  about  its  possible  benefits,  such  as 

 
3 

creating  teaching  materials,  analyzing  student  data,  or  identifying  learning  patterns,  the 

evidence base for this is unclear. Yet, GAI studies have revealed potential risks associated with 

the generation of incorrect information (known as the ‘hallucination’ issue), biases (including 

race, nationality, gender), and discriminatory content (Munn, 2023; Nozza et al., 2022). When 

GAI-generation  outputs  are  used  in  education-related  procedures  there  is  a  possibility  that 

problematic contents, biases and assumptions will be magnified, which may result in negative 

consequences for learners, educators, researchers, and administrators. Therefore, it is crucial to 

discuss  and  assess  the  ethical  implications  of  implementing  ChatGPT  within  educational 

institutions, especially concerning its use in teaching and learning, research or administration. 

Increased  use  of  GAI  by  learners  raises  issues  related  to  academic  integrity,  definitions  of 

authorship,  assessment  methods,  and  other  pedagogical  implications.  It  also  affects  how 

researchers conduct their studies and generate their outputs, as well as how decisions are made 

in admissions, hiring, or how the educational institutions are managed and run. Furthermore, 

the  increasing  integration  of  AI  in  education  even  raises  questions  about  the  continued 

relevance of traditional brick-and-mortar educational institutions.     

This  scoping  review  explores  the  ethical  challenges  of  using  ChatGPT  in  education, 

focusing particularly on higher education. By reviewing academic articles written in English, 

Chinese, and Japanese, we aimed to map out the current state of the field while identifying gaps 

for future consideration.  

METHODOLOGY 

A scoping review is commonly used to identify key issues in a newly emerging field or 

one where there is yet a substantial body of literature. It is “used to identify knowledge gaps, 

set research agendas, and identify implications for decision-making” (Tricco et al., 2016).  In 

this study, we adopted Arksey & O’Malley’s (2005) five-stage scoping review framework, that 

 
 
4 

involves identifying the initial research questions and relevant studies, selecting the studies, 

charting the data, and collating, summarizing, and reporting the results.  

Identifying the relevant studies 

We limited our focus to articles focusing on the latest version of GPT. We searched 

articles published in 2023 and used search terms “ChatGPT” or “Generative AI” coupled with 

“education”  and  “ethics”  (see  Table  1).  To  capture  more  solid  evidence-based  studies  and 

discussions around this topic, we identified SCOPUS as the main database to conduct our initial 

search.  To  include  ongoing  research  works,  we  also  included  the  arXiv  platform,  which 

provides access to preprint articles. We included two other languages that the authors have first 

or near-first language proficiency in, Japanese and Chinese. To facilitate this, we conducted 

searches in the prominent databases CiNii (Japanese) and CNKI (Chinese). Along with the UK 

and USA, Japan and China are leading AI development making these languages a good target. 

Table 1. Final search terms and results by platforms 

Database  Search terms 
Scopus 

(TITLE-ABS-KEY (""chatgpt"" OR ""generative AI"") AND TITLE-ABS-KEY 
(""education"" )) 

(TITLE-ABS-KEY (""chatgpt"" OR ""generative AI"") AND TITLE-ABS-KEY 
(""education"" AND ""ethics"")) 

ArXiv 

(TITLE-ABS-KEY (""chatgpt"" OR ""generative AI"") AND TITLE-ABS-KEY 
(""education"")) 

(TITLE-ABS-KEY (""chatgpt"" OR ""generative AI”) AND TITLE-ABS-KEY 
( ""education""  AND  ""ethics"" ) )  

Results  
276 

27 

112 

24 

CiNii 

(TITLE-ABS-KEY (""chatgpt""  OR  ""生成 AI"")  AND  TITLE-ABS-KEY ( ""教育"" ) )  

23 

 (TITLE-ABS-KEY (""chatgpt""  OR  ""生成 AI"")  AND  TITLE-ABS-KEY (“教育"" 
AND ""課題"" )  

4 

CNKI 

(TITLE-ABS-KEY (""chatgpt"" OR ""生成 AI"") AND TITLE-ABS-KEY (""education"")) 

198 

(TITLE-ABS-KEY (""chatgpt"" OR ""生成 AI"" ) AND TITLE-ABS-KEY (""教育"" AND 
""伦理"")  

12 

Charting the data and collation 

The initial search yielded 609 results, out of which 67 included education and ethical 

concerns. From these, we identified 26 articles meeting our inclusion criteria (Figure 1). All 

 
  
articles were reviewed by two reviewers and the third reviewer checked the findings.  

5 

Figure 1. Data extraction processes 

In our analysis of the ethical issues raised in the articles, we relied on the comprehensive 

research  conducted  by  DeepMind  (Weiginger  et  al.,  2021),  which  offers  a  framework  for 

assessing the ethical and social risks of harm that may arise from the deployment of language 

models (LMs) (Table 2).  

#  Areas 
1  Discrimination, 

Exclusion and 
Toxicity 
Information Hazards 

2 
3  Misinformation 

Harms 

4  Malicious Uses 

5  Human-Computer 
Interaction Harms 
6  Automation, Access, 

and Environmental 
Harms 

Table 2. Ethical and social risks areas 

Description 
AI models can harm by reinforcing discrimination, stereotypes, and biases, 
marginalizing individuals, promoting toxic language, and worsening disparities 
for disadvantaged groups. 
Leak of private data or sensitive information leaks. 
Providing false or misleading information, leading to less informed users and 
eroding trust in shared information 
Risks of using LMs for harm include enabling disinformation campaigns, 
personalized scams, fraud at scale, and the development of malicious computer 
code or weapon systems. 
Users’ overestimation of “human-like” AI capabilities may lead to unsafe 
usage, exploitation for manipulation, and perpetuation of stereotypes. 
Unequal benefits and limited access to LMs can impact job quality, creative 
economy, and create global disparities in risks and rewards. 

 
 
6 

FINDINGS 

Most  of  the  identified  papers  were  in  English  (n=19),  followed  by  Chinese  (n=4), 

Japanese (n=3). In the English papers, ten were empirical studies and nine were conceptual or 

discussion papers with a predominant focus on its applications in fields such as healthcare and 

medical domains. In comparison, the number of Chinese and Japanese papers was much smaller. 

Among  the  four  Chinese  papers,  all  were  general  discussions  around  the  application  and 

predicted impact of ChatGPT in education. There were only three Japanese articles, but one 

reported  on  initial  research  on  students’  practical  experiences  with  ChatGPT  specifications 

(Kondo  et  al.,  2023).  The  other  two  papers  discussed  challenges  for  Japanese  speakers  in 

writing  English  academic  papers  and  the  use  of  ChatGPT  for  support  and  general  teaching 

implications (Kashimura, 2023; Yanase, 2023). 

Overall,  there  was  little  discussion  specifically  focusing  on  higher  education.  The 

majority of the papers (n=19) were generic, discussing ethical concerns in teaching (n=19) and 

learning  (n=13)  mostly  from  theoretical  and  conceptual  perspectives  without  delving  into 

specific levels of education. Papers that specifically focused on tertiary education (n=5) were 

concerned  about  overall  pedagogical  implications,  particularly  in  medical  education  (n=2), 

faculty and students’ perceptions (n=2), and research implications (n=1).  

Table 3. Articles reviewed. 

Authors 

Language 

Education level   Main area/Focus  

Ethical concerns  

Database: Scopus 

Busch et al. 

English 

Tertiary 

Teaching, learning, 
administration 

1,2, 3, 4, 5 

Chan 

English 

Tertiary 

Teaching, learning 

2, 3, 5 

Curtis 

English 

Tertiary 

Research 

da Silva 

English 

Generic 

Research 

3, 5 

3, 5 

Dwivedi et al.  English 

Generic 

Research 

1, 2, 3, 4, 5, 6 

Fischer 

English 

Generic 

Administration 

1, 2, 3, 4, 5 

Krüger et al. 

English 

Generic 

Teaching, learning, research 

1, 2, 3, 5 

 
 
 
7 

Lim et al. 

English 

Generic 

Teaching 

1, 2, 3 

Masters (a) 

English 

Generic 

Teaching, administration 

1, 2, 3, 4, 5 

Masters (b) 
O’Connor & 
ChatGPT 

Tlili et al. 
Zumsteg & 
Junn 

English 

Generic 

Research 

3, 4 

English 

Generic 

Teaching, learning, research 

3, 5 

English 

Generic 

Teaching, learning 

1, 2, 3, 4, 5,  

English 

Tertiary 

Teaching, learning 

3, 4, 5 

Database: arXiv 

Chan & Hu 

English 

Tertiary 

Teaching, learning 

1, 2, 3, 4, 5 

Latif et al. 

English 

Generic 

Teaching 

1, 2, 3, 4, 5 

Li et al. 

English 

Generic 

Teaching, learning, research 

1, 2, 3, 4, 5 

Ojha et al. 

English 

Generic 

Teaching 

Sharma et al. 

English 

Generic 

Administration 

Sharples 

English 

Generic 

Teaching, learning 

Database: CiNii 

Kashimura  

Japanese 

Generic 

Teaching 

Kondo et al. 

Japanese 

Secondary 

Teaching, learning 

Yanase  

Japanese 

Generic 

Research 

Database: CNKI 

Huang 

Chinese 

Generic 

Teaching, learning 

Song & Lin 

Chinese 

Generic 

Teaching 

Xun 

Chinese 

Tertiary 

Teaching, learning 

Zhu & Yang 

Chinese 

Generic 

Teaching, learning 

4, 5 

3, 4 

3, 5 

1, 2, 3 

3, 5 

3, 5 

1, 3, 5 

2, 3, 5 

1, 3, 4, 5 

1, 2, 3, 5 

In terms of the focus of ethical issues, the majority of papers were concerned with #3 

misinformation  harms  (n=25)  including  academic  integrity,  cheating  and  other  assessment 

issues, and the users’ role in identifying and clarifying information and/or #5 human-computer 

interaction related harms (n=24) such as  addiction, dependence, and cognitive overload.  To 

illustrate  this  further  we  divided  the  papers  into  four  themes  concerning  teaching,  learning, 

research, and administration.  

In  the  following  section  we  sum  up  the  key  concerns  and  areas  of  discussion  in 

literature. 

 
 
 
 
8 

Teaching  

  Under  teaching,  it  was  noted  that  ChatGPT  exhibits  versatile  applications,  such  as 

personalized  and  interactive  learning,  curriculum  design,  assessing  homework,  exams,  and 

essays (Ojha et at., 2023; Kashiwamura, 2023; Huang, 2023). To design new programs or to 

provide personalized teaching, universities need to collect and process vast amounts of student 

data, often without students’ consent. This gives rise to questions surrounding data privacy and 

security (Chan, 2023; Masters, 2023a). There is a need to ensure that robust data protection 

measures are in place to safeguard sensitive information and prevent its misuse.  

Research by Latif et al. (2023) highlighted that AI may inadvertently reinforce existing 

societal inequalities, gender bias, and nationality  bias embedded within the original training 

data, negatively impacting the outputs of educational applications downstream. Undue reliance 

on AI-generated evaluations may compromise the quality of assessments, potentially failing to 

accurately  gauge  students’  true  capabilities  (Busch  et  al.,  2023;  Curtis,  2023;  Song  &  Lin, 

2023). 

Additionally, the introduction of AI in the educational process raises concerns about the 

dynamics between educators and students. A possible overreliance on AI-generated content, as 

discussed  by  Sharples  (2023),  could  alter  the  traditional  teacher-student  relationship  and 

diminish  educators’  creative  input  and  uniqueness  in  designing  engaging  lesson  plans  and 

activities. 

 Learning 

     The application of ChatGPT in learning includes personalized learning experiences, 

student  support,  language  assistance,  tutoring,  content  creation,  grading  and  assessment, 

research aid, and career counseling, enhancing the learning process for students (Kooli, 2023; 

 
 
     
9 

Lim et al., 2023). One major concern is the potential for an increase in plagiarism and cheating 

among  students  who  might  rely  on  GAI-generated  content  for  essays  and  exams,  thereby 

compromising  the  authenticity  of  their  work  (Li  et  al.,  2023;  Zhu  &  Yang,  2023).  This 

overreliance  on  ChatGPT  may  lead  to  a  decline  in  students’  sense  of  responsibility  and 

commitment to academic integrity (Ojha et al., 2023). 

    Excessive use of ChatGPT may have adverse effects on students’ critical thinking 

skills.  If  students  heavily  depend  on  AI-generated  content,  they  might  lose  the  ability  to 

independently analyze and evaluate information (Tlili et al., 2023). Another significant issue 

raised in the literature pertains to the risk of misinformation being propagated due to the highly 

persuasive and convincing nature of AI-generated content. This can lead to potential bias or 

manipulation of information presented to students.  

       Reliance on AI interactions for academic or social purposes might diminish face-

to-face  interactions,  potentially  hindering  the  development  of  essential  social  skills  among 

students.  Striking  a  balance  between  AI  and  human  interactions  is  crucial  to  foster  a  well-

rounded educational experience. 

ChatGPT  might  inadvertently  produce  content  that  inaccurately  or  inappropriately 

represents certain cultural or identity groups, highlighting the need for ongoing refinement and 

sensitivity in AI language model development (Busch et al., 2023). 

 Research  

   The applications of ChatGPT encompass efficient dataset analysis, code generation, 

literature  reviews,  timesaving  for  experimental  design  focus,  and  advancements  in  research 

discovery and development (Dwivedi et al., 2023; Li et al., 2023). 

   In research, the integration of AI in academic publishing poses the risk of displacing 

human  authors  and  undermining  the  value  of  their  expertise,  potentially  impacting  the 

 
   
10 

credibility  of  research.  The  attribution  of  fake  references  to  AI-generated  content  presents 

another challenge, leading to misinformation and a decline in trust in academic sources. (Curtis, 

2023). Joint authorship of editorial pieces like O’Connor and ChatGPT (2023) is a contentious 

as  it  challenges  the  established  core  values  related  to  human-based  authorship  in  academic 

publishing (da Silva, 2023).  

   Some  conferences  permit  the  use  of  ChatGPT  for  writing  papers,  but  only  when 

ChatGPT itself is  the subject  of empirical  research (e.g.,  ICML , 2023). On the other hand, 

some  research  communities,  such  as  the  Association  for  Computational  Linguistics  (ACL, 

2023), allow the use of ChatGPT based on specific guidelines.  

 Administration   

     ChatGPT  can  significantly  reduce  the  time  spent  on  human  administrative  tasks, 

such  as  responding  to  queries  from  applicants  and  assisting  students  in  course  enrollment 

(UNESCO IESALC, 2023).  

     However, there are concerns surrounding the equitable, reliable, and transparent use 

of ChatGPT. Utilizing ChatGPT in the admissions processes can potentially introduce biases, 

especially if the AI model was trained on historical data that reflects past inequalities (Fischer, 

2023; Sharma et al. 2023). To ensure fairness, transparency, and accountability, it is essential 

to  provide  applicants  with  clear  explanations  of  how  AI  was  employed  to  assess  their 

applications and the specific factors that contributed to their acceptance or rejection. 

Additionally,  the  use  of  AI  algorithms  in  admissions  decisions  carries  the  risk  of 

inadvertently  favoring  applicants  with  certain  characteristics  or  backgrounds,  potentially 

impacting  diversity  and  inclusion  efforts  within  the  university  (Busch  et  al.,  2023;  Fischer, 

2023).  Data privacy and security are also paramount considerations. To avoid unintentional 

discrimination,  institutions  should  actively  assess  and  address  any  biases  in  the  AI  model’s 

 
 
training  data  and  decision-making  process,  striving  to  provide  equal  opportunities  for  all 

applicants. 

11 

DISCUSSION AND CONCLUSION 

We focused on articles written in a very short period, the first 7 months of 2023, but 

covered literature written in English, Chinese and Japanese. Given that Chat GPT is trained on 

English-centric data (Brown et al., 2020), it is important to gain insights into discussions going 

on in other AI technologically advanced countries. However, our review revealed that a few 

academic and research studies are published in non-English languages, particularly in Chinese 

and Japanese.   

Our scoping review showed that there are already publications that are considering the 

ethical implications of GAI, especially ChatGPT, in education generally and some focused on 

higher education. The majority of papers are discussion pieces, but there is some early empirical 

work.  The  ethical  issues  highlighted  in  these  works  are  mainly  concerned  about  academic 

integrity, assessment issues, and data-protection.  

  Our analysis highlights the urgency of addressing ethical issues surrounding the use 

of GAI/ChatGPT in education. Collaboration among stakeholders is essential to establish clear 

guidelines,  protect  student  privacy,  and  promote  responsible  AI  use.  By  doing  so,  AI  can 

enhance education and research without compromising fundamental principles. 

References 

ACL. (2023). ACL 2023 Policy. 61st Annual Meeting of the Association for Computational 

Linguistics. Retrieved from https://2023.aclweb.org/blog/ACL-2023-policy/  

Arksey, H., & O'Malley, L. (2005). Scoping studies: towards a methodological framework. 

 
  
 
 
12 

International journal of social research methodology, 8(1), 19-32. 

Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. 

(2020).  Language  models  are  few-shot  learners.  Advances  in  Neural  Information 

Processing Systems, 33, 1877–1901. 

Busch, F., Adams,  L. C., &  Bressem, K. K. (2023). Biomedical  ethical  aspects  towards the 

implementation of Artificial Intelligence in medical education. Med. Sci. Educ. Advance 

Online Publication. https://doi.org/10.1007/s40670-023-01815-x 

Chan,  C.  K.  Y.  (2023).  A  comprehensive  AI  policy  education  framework  for  university 

teaching  and  learning.  International  Journal  of  Educational  Technology  in  Higher 

Education, 20(38). https://doi.org/10.1186/s41239-023-00408-3 

Chan, C. K. Y., & Hu, W. (2023). Students’ voices on generative AI: Perceptions, benefits, and 

challenges in higher education. arXiv preprint arXiv:2305.00290. 

Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., ... & Wei, J. (2022). Scaling 

instruction-finetuned language models. arXiv preprint arXiv:2210.11416. 

Curtis,  N.(2023). To ChatGPT or not  to  ChatGPT? The  Impact  of Artificial  Intelligence on 

Academic  Publishing.  The  Pediatric  Infectious  Disease  Journal,  42(4),  275. 

https://doi.org/10.1097/INF.0000000000003852 

da Silva, J. A. T. (2023). Is ChatGPT a valid author?. Nurse Education in Practice, 68, 103600. 

https://doi.org/10.1016/j.nepr.2023.103600. 

Devlin,  J.,  Chang,  M.  W.,  Lee,  K.,  &  Toutanova,  K.  (2018).  Bert:  Pre-training  of  deep 

bidirectional 

transformers 

for 

language 

understanding. 

arXiv 

preprint 

arXiv:1810.04805. 

Dwivedi, Y. K., Kshetri, N., Hughes, L., Slade, E. L., Jeyaraj, A., Kar, A. K., ... & Wright, R. 

(2023).  “So  what 

if  ChatGPT  wrote 

it?”  Multidisciplinary  perspectives  on 

opportunities, challenges and implications of generative conversational AI for research, 

 
13 

practice  and  policy.  International  Journal  of  Information  Management,  71,  102642. 

https://doi.org/10.1016/j.ijinfomgt.2023.102642 

Fischer, I. (2023). Evaluating the ethics of machines assessing humans. Journal of Information 

Technology Teaching Cases, 0(0). https://doi.org/10.1177/20438869231178844 

Huang R. (2023). Ren Gong Zhi Neng Zheng Jia Su Jiao Yu Bian Ge: Xian Shi Tiao Zhan Yu 

Ying  Dui  Ju  Cuo  [Artificial  intelligence  is  accelerating  educational  transformation: 

Realistic challenges and countermeasures]. Journal of the Chinese Society of Education, 

(06), 26-33. 

ICML. (2023). Call for Papers. International Conference on Machine Learning. Retrieved from 

https://icml.cc/Conferences/2023/CallForPapers 

Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, 

A., Wu, J., & Amodei, D. (2020). Scaling laws for neural language models. Advances 

in neural information processing systems, 33, 1877–1901. 

Kashiwamura, Y. (2023). Sozoteki sagyo e shifuto o unagasu kyoin no noryoku kojo ga kadai 

ni:  Kyoiku  o  kaeru  seisei  AI  [Encouraging  a  shift  to  creative  work:  Challenges  for 

teacher  capacity  building:  Generating  AI  that  Changes  Education].  The  Economist, 

101(24), 98-100. 

Kondo,  C.,  Tamada,  K.,  &  Matsuda,  T.  (2023).  Seiseikei  AI  o  daizai  toshita  joho-teki  na 

mikata・kangaekata ni motozuku mondai kaiketsu shido jissen: ChatGPT to no kyo-

zon  o  kangaeru  [Practicing  problem-solving  instruction  based  on  informational 

perspectives and ways of thinking :Consider coexistence with ChatGPT]. Journal of the 

Japan 

Society 

for 

Educational 

Technology, 

(2), 

255-258. 

https://doi.org/10.15077/jsetstudy.2023.2_255 

Kooli,  C.  (2023).  Chatbots  in  education  and  research:  A  critical  examination  of  ethical 

implications and solutions. Sustainability, 15(7), 5614. 

 
14 

Krüger, L., Krotsetis, S., OpenAI’s Generative Pretrained Transformer 3 (GPT-3) Model, & 

Nydahl,  P.  (2023).  ChatGPT:  Fluch  oder  Segen  in  der  Pflege?  [ChatGPT:  curse  or 

blessing in nursing care?]. Medizinische Klinik, Intensivmedizin und Notfallmedizin, 

10.1007/s00063-023-01038-3. 

Advance 

online 

publication. 

https://doi.org/10.1007/s00063-023-01038-3 

Latif, E., Mai, G., Nyaaba, M., Wu, X., Liu, N., Lu, G., ... & Zhai, X. (2023). Artificial general 

intelligence (AGI) for education. arXiv preprint arXiv:2304.12479. 

Li,  L.,  Ma,  Z.,  Fan,  L.,  Lee,  S.,  Yu,  H.,  &  Hemphill,  L.  (2023).  ChatGPT  in  education:  A 

discourse  analysis  of  worries  and  concerns  on  social  media.  arXiv  preprint 

arXiv:2305.02201. 

Lim, W. M., Gunasekara, A., Pallant, J. L., Pallant, J. I., & Pechenkina, E. (2023). Generative 

AI and the future of education: Ragnarök or reformation? A paradoxical  perspective 

from  management  educators.  The  International  Journal  of  Management  Education, 

21,100790. https://doi.org/10.1016/j.ijme.2023.100790  

Masters,  K.  (2023  a).  Ethical  use  of  Artificial  Intelligence  in  health  professions  education: 

AMEE 

Guide 

No. 

158. 

Medical 

Teacher, 

45(6), 

574-584. 

https://doi.org/10.1080/0142159X.2023.2186203 

Masters, K. (2023 b). Medical teacher’s first ChatGPT’s referencing hallucinations: Lessons 

for editors, reviewers, and teachers. Medical Teacher, Med Teach, 45(7), 673-675. DOI: 

10.1080/0142159X.2023.2208731 

Munn,  L. 

(2023).  The  uselessness  of  AI 

ethics.  AI  Ethics,  3,  869–877. 

https://doi.org/10.1007/s43681-022-00209-w 

Nozza, D., Bianchi, F., & Hovy, D. (2022). Pipelines for social bias testing of large language 

models.  In  Proceedings  of  BigScience  Episode  #5  --  Workshop  on  Challenges  & 

Perspectives  in  Creating  Large  Language  Models,  virtual+Dublin.  Association  for 

 
15 

Computational Linguistics, 68-74. 

O’Connor, S & ChatGPT. (2022). Open artificial intelligence platforms in nursing education: 

Tools  for  academic  progress  or  abuse?.  Nurse  Education  in  Practice,  66,  103537. 

https://doi.org/10.1016/j.nepr.2022.103537 

Ojha,  S.,  Narendra,  A.,  Mohapatra,  S.,  &  Misra,  I.  (2023).  From  robots  to  books:  An 

introduction  to  smart  applications  of  AI  in  education  (AIEd).  arXiv  preprint 

arXiv:2301.10026. 

Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., ... & Lowe, R. (2022). 

Training  language  models  to  follow  instructions  with  human  feedback.  Advances  in 

Neural Information Processing Systems, 35, 27730-27744. 

Radford, A., & Narasimhan, K. (2018). Improving Language Understanding by Generative Pre-

Training. Retrieved from https://www.mikecaptain.com/resources/pdf/GPT-1.pdf 

Sharma, P., Thapa, K., Dhakal, P., Upadhaya, M. D., Adhikari, S., & Khanal, S. R. (2023). 

Performance  of  ChatGPT  on  USMLE:  Unlocking  the  potential  of  large  language 

models for AI-assisted medical education. arXiv preprint arXiv:2307.00112. 

Sharples, M. (2023). Towards social generative AI for education: theory, practices and ethics. 

arXiv preprint arXiv:2306.10063. 

Song, H & Lin, M (2023). ChatGPT/Chuangshengshi rengong zhineng shidai xia  jiaoshi de 

gongzuo biange: Jiyu, tiaozhan yu yingdui [The Transformation of Teachers’ Work in 

the Era of ChatGPT/AIGC: Opportunities, Challenges, and Responses]. Journal of East 

China 

Normal 

University 

(Educational 

Sciences), 

(07),78-90. 

https://doi.org/10.16382/j.cnki.1000-5560.2023.07.008. 

Tlili, A., Shehata, B., Adarkwah, M. A., Bozkurt, A., Hickey, D. T., Huang, R., & Agyemang, 

B. (2023). What if the devil is my guardian angel: ChatGPT as a case study of using 

chatbots in education. Smart Learning Environments, 10(1), 15. 

 
16 

Tricco, A. C., Lillie, E., Zarin, W., O’brien, K., Colquhoun, H., Kastner, M., ... & Straus, S. E. 

(2016).  A  scoping  review  on  the  conduct  and  reporting  of  scoping  reviews.  BMC 

medical research methodology, 16, 1-10. 

UNESCO IESALC (2023). ChatGPT and artificial intelligence in higher education: Quick start 

guide.  Retrieved 

from  https://www.iesalc.unesco.org/en/2023/04/14/chatgpt-and-

artificial-intelligence-in-higher-education-quick-start-guide-and-interactive-seminar/ 

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & 

Polosukhin,  I.  (2017).  Attention  is  all  you  need.  Advances  in  neural  information 

processing systems, 30. https://api.semanticscholar.org/CorpusID:13756489 

Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P. S., ... & Gabriel, I. (2021). 

Ethical  and  social  risks  of  harm 

from 

language  models.  arXiv  preprint 

arXiv:2112.04359. 

Xun, Y. (2023). ChatGPT/Chuangshengshi rengong zhineng yu gaodeng jiaoyu de jiazhi he 

shiming [ChatGPT/AIGC and the Value and Mission of Higher Education]. Journal of 

East China Normal University (Educational Sciences), (07), 56-63. 

Yanase, Y. (2023). AI o katsuyo shite Eigo ronbun o sakusei suru Nihongo shasha ni totte no 

kadai to sono taisho [Challenges and Strategies for Japanese Speakers Creating English 

Papers Using AI]. Journal of Information Science and Technology, 73(6), 219-224. 

Zhu, Y & Yang, F. (2023). ChatGPT/Chuangshengshi rengong zhineng yu jiaoyu chuangxin: 

Jiyu, 

tiaozhan  yiji  weilai. 

[ChatGPT/AIGC  and  Educational 

Innovation: 

Opportunities,Challenges, and the Future].  Journal of East China Normal University 

(Educational 

Sciences), 

(07),1-14. 

https://doi.org/10.16382/j.cnki.1000-

5560.2023.07.001. 

Zumsteg, J. M., & Junn, C. (2023). Will ChatGPT match to your program.  Am J Phys Med 

Rehabil, 1, 3-7. 

 
17 

 
 
","1 Ethical implications of ChatGPT in higher education : A scoping review Ming Li1 , Ariunaa Enkhtur2 , Fei Cheng3 , Beverley Anne Yamamoto4 1Institue for Transdisciplinary Graduate Degree Programs , Osaka University 2Center for Global Initiatives , Osaka University 3 Graduate School of Informatics , Kyoto University 4Graduate School of Human Sciences Human Sciences , Osaka University Keyword : ChatGPT , education , ethic , Generative Artificial Intelligence , higher education , scoping review ABSTRACT This scoping review explores the ethical challenges of using ChatGPT in education , focusing particularly on issues related to higher education . By reviewing recent academic articles written in English , Chinese , and Japanese , we aimed to provide a comprehensive overview of relevant research while identifying gaps for future considerations . Drawing on Arksey & O ’ Malley ’ s ( 2005 ) five-stage scoping review framework , we identified research questions , search terms , and conducted article search from four databases in the target three languages . Each article was reviewed by at least two researchers identifying main ethical issues of utilizing AI in education , particularly higher education . Our analysis of ethical issues followed the framework developed by DeepMind ( Weiginger et al. , 2021 ) to identify six main areas of ethical concern in Language Models . The majority of papers were concerned with misinformation harms ( n=25 ) and/or human-computer interaction related harms ( n=24 ) . Given the rapid deployment of Generative Artificial Intelligence ( GAI ) , it is imperative for educators to conduct more empirical studies to develop sound ethical policies for the use of GAI . 2 INTRODUCTION The recent wave of Generative Artificial Intelligence ( GAI ) originated with Google ’ s Transformer architecture of neural networks ( Vaswani et al. , 2017 ) . Transformer is based on the self-attention mechanism , which is highly scalable and suitable for parallel computing , hence it quickly became the de facto approach in the fields of natural language processing and computer vision . In 2018 , language models like BERT ( Devlin et al. , 2018 ) and GPT ( Radford et al. , 2018 ) ushered in the era of self-supervised learning , which are pre-trained on large-scale textual data for learning fundamental knowledge , and then fine-tuned for specific downstream tasks such as QA , dialog and machine translation systems . Kaplan et al . ( 2018 ) validated the large language models ’ scale law , asserting that the performance improvements can almost always be achieved by increasing the scale of model parameters and pre-training more textual data such as Wikipedia , book and internet resources . After this , the rapid expansion for the size of large language models began , with the model scale growing from BERT ’ s 0.3B ( billion ) parameters to GPT3 ’ s 175B ( Brown et al . 2020 ) . Since the release of GPT3 release in June 2020 , OpenAI and DeepMind , leading developers of this technology , became less satisfied with mere scale increases . Ouyang et al . ( 2022 ) and Chung et al . ( 2022 ) sought to align models ’ outputs with the feedback of real human beings . The text generated by these models became increasingly human-like , and the content ever-more closely aligned with human values . The launch of ChatGPT in November 2022 made the public aware that GAI was already capable of generating human-quality conversation , retrieving stored knowledge on demand , and achieving natural interaction with people as an AI assistant . This kicked off the current frenzy of adapting the utilization of GAI to various fields , including education . ChatGPT ’ s application in higher education has garnered attention ( UNESCO IESALC , 2023 ) . While there has been much discussion about its possible benefits , such as 3 creating teaching materials , analyzing student data , or identifying learning patterns , the evidence base for this is unclear . Yet , GAI studies have revealed potential risks associated with the generation of incorrect information ( known as the ‘ hallucination ’ issue ) , biases ( including race , nationality , gender ) , and discriminatory content ( Munn , 2023 ; Nozza et al. , 2022 ) . When GAI-generation outputs are used in education-related procedures there is a possibility that problematic contents , biases and assumptions will be magnified , which may result in negative consequences for learners , educators , researchers , and administrators . Therefore , it is crucial to discuss and assess the ethical implications of implementing ChatGPT within educational institutions , especially concerning its use in teaching and learning , research or administration . Increased use of GAI by learners raises issues related to academic integrity , definitions of authorship , assessment methods , and other pedagogical implications . It also affects how researchers conduct their studies and generate their outputs , as well as how decisions are made in admissions , hiring , or how the educational institutions are managed and run . Furthermore , the increasing integration of AI in education even raises questions about the continued relevance of traditional brick-and-mortar educational institutions . This scoping review explores the ethical challenges of using ChatGPT in education , focusing particularly on higher education . By reviewing academic articles written in English , Chinese , and Japanese , we aimed to map out the current state of the field while identifying gaps for future consideration . METHODOLOGY A scoping review is commonly used to identify key issues in a newly emerging field or one where there is yet a substantial body of literature . It is “ used to identify knowledge gaps , set research agendas , and identify implications for decision-making ” ( Tricco et al. , 2016 ) . In this study , we adopted Arksey & O ’ Malley ’ s ( 2005 ) five-stage scoping review framework , that 4 involves identifying the initial research questions and relevant studies , selecting the studies , charting the data , and collating , summarizing , and reporting the results . Identifying the relevant studies We limited our focus to articles focusing on the latest version of GPT . We searched articles published in 2023 and used search terms “ ChatGPT ” or “ Generative AI ” coupled with “ education ” and “ ethics ” ( see Table 1 ) . To capture more solid evidence-based studies and discussions around this topic , we identified SCOPUS as the main database to conduct our initial search . To include ongoing research works , we also included the arXiv platform , which provides access to preprint articles . We included two other languages that the authors have first or near-first language proficiency in , Japanese and Chinese . To facilitate this , we conducted searches in the prominent databases CiNii ( Japanese ) and CNKI ( Chinese ) . Along with the UK and USA , Japan and China are leading AI development making these languages a good target . Table 1 . Final search terms and results by platforms Database Search terms Scopus ( TITLE-ABS-KEY ( `` chatgpt '' OR `` generative AI '' ) AND TITLE-ABS-KEY ( `` education '' ) ) ( TITLE-ABS-KEY ( `` chatgpt '' OR `` generative AI '' ) AND TITLE-ABS-KEY ( `` education '' AND `` ethics '' ) ) ArXiv ( TITLE-ABS-KEY ( `` chatgpt '' OR `` generative AI '' ) AND TITLE-ABS-KEY ( `` education '' ) ) ( TITLE-ABS-KEY ( `` chatgpt '' OR `` generative AI ” ) AND TITLE-ABS-KEY ( `` education '' AND `` ethics '' ) ) Results 276 27 112 24 CiNii ( TITLE-ABS-KEY ( `` chatgpt '' OR `` 生成 AI '' ) AND TITLE-ABS-KEY ( `` 教育 '' ) ) 23 ( TITLE-ABS-KEY ( `` chatgpt '' OR `` 生成 AI '' ) AND TITLE-ABS-KEY ( “ 教育 '' AND `` 課題 '' ) 4 CNKI ( TITLE-ABS-KEY ( `` chatgpt '' OR `` 生成 AI '' ) AND TITLE-ABS-KEY ( `` education '' ) ) 198 ( TITLE-ABS-KEY ( `` chatgpt '' OR `` 生成 AI '' ) AND TITLE-ABS-KEY ( `` 教育 '' AND `` 伦理 '' ) 12 Charting the data and collation The initial search yielded 609 results , out of which 67 included education and ethical concerns . From these , we identified 26 articles meeting our inclusion criteria ( Figure 1 ) . All articles were reviewed by two reviewers and the third reviewer checked the findings . 5 Figure 1 . Data extraction processes In our analysis of the ethical issues raised in the articles , we relied on the comprehensive research conducted by DeepMind ( Weiginger et al. , 2021 ) , which offers a framework for assessing the ethical and social risks of harm that may arise from the deployment of language models ( LMs ) ( Table 2 ) . # Areas 1 Discrimination , Exclusion and Toxicity Information Hazards 2 3 Misinformation Harms 4 Malicious Uses 5 Human-Computer Interaction Harms 6 Automation , Access , and Environmental Harms Table 2 . Ethical and social risks areas Description AI models can harm by reinforcing discrimination , stereotypes , and biases , marginalizing individuals , promoting toxic language , and worsening disparities for disadvantaged groups . Leak of private data or sensitive information leaks . Providing false or misleading information , leading to less informed users and eroding trust in shared information Risks of using LMs for harm include enabling disinformation campaigns , personalized scams , fraud at scale , and the development of malicious computer code or weapon systems . Users ’ overestimation of “ human-like ” AI capabilities may lead to unsafe usage , exploitation for manipulation , and perpetuation of stereotypes . Unequal benefits and limited access to LMs can impact job quality , creative economy , and create global disparities in risks and rewards . 6 FINDINGS Most of the identified papers were in English ( n=19 ) , followed by Chinese ( n=4 ) , Japanese ( n=3 ) . In the English papers , ten were empirical studies and nine were conceptual or discussion papers with a predominant focus on its applications in fields such as healthcare and medical domains . In comparison , the number of Chinese and Japanese papers was much smaller . Among the four Chinese papers , all were general discussions around the application and predicted impact of ChatGPT in education . There were only three Japanese articles , but one reported on initial research on students ’ practical experiences with ChatGPT specifications ( Kondo et al. , 2023 ) . The other two papers discussed challenges for Japanese speakers in writing English academic papers and the use of ChatGPT for support and general teaching implications ( Kashimura , 2023 ; Yanase , 2023 ) . Overall , there was little discussion specifically focusing on higher education . The majority of the papers ( n=19 ) were generic , discussing ethical concerns in teaching ( n=19 ) and learning ( n=13 ) mostly from theoretical and conceptual perspectives without delving into specific levels of education . Papers that specifically focused on tertiary education ( n=5 ) were concerned about overall pedagogical implications , particularly in medical education ( n=2 ) , faculty and students ’ perceptions ( n=2 ) , and research implications ( n=1 ) . Table 3 . Articles reviewed . Authors Language Education level Main area/Focus Ethical concerns Database : Scopus Busch et al . English Tertiary Teaching , learning , administration 1,2 , 3 , 4 , 5 Chan English Tertiary Teaching , learning 2 , 3 , 5 Curtis English Tertiary Research da Silva English Generic Research 3 , 5 3 , 5 Dwivedi et al . English Generic Research 1 , 2 , 3 , 4 , 5 , 6 Fischer English Generic Administration 1 , 2 , 3 , 4 , 5 Krüger et al . English Generic Teaching , learning , research 1 , 2 , 3 , 5 7 Lim et al . English Generic Teaching 1 , 2 , 3 Masters ( a ) English Generic Teaching , administration 1 , 2 , 3 , 4 , 5 Masters ( b ) O ’ Connor & ChatGPT Tlili et al . Zumsteg & Junn English Generic Research 3 , 4 English Generic Teaching , learning , research 3 , 5 English Generic Teaching , learning 1 , 2 , 3 , 4 , 5 , English Tertiary Teaching , learning 3 , 4 , 5 Database : arXiv Chan & Hu English Tertiary Teaching , learning 1 , 2 , 3 , 4 , 5 Latif et al . English Generic Teaching 1 , 2 , 3 , 4 , 5 Li et al . English Generic Teaching , learning , research 1 , 2 , 3 , 4 , 5 Ojha et al . English Generic Teaching Sharma et al . English Generic Administration Sharples English Generic Teaching , learning Database : CiNii Kashimura Japanese Generic Teaching Kondo et al . Japanese Secondary Teaching , learning Yanase Japanese Generic Research Database : CNKI Huang Chinese Generic Teaching , learning Song & Lin Chinese Generic Teaching Xun Chinese Tertiary Teaching , learning Zhu & Yang Chinese Generic Teaching , learning 4 , 5 3 , 4 3 , 5 1 , 2 , 3 3 , 5 3 , 5 1 , 3 , 5 2 , 3 , 5 1 , 3 , 4 , 5 1 , 2 , 3 , 5 In terms of the focus of ethical issues , the majority of papers were concerned with # 3 misinformation harms ( n=25 ) including academic integrity , cheating and other assessment issues , and the users ’ role in identifying and clarifying information and/or # 5 human-computer interaction related harms ( n=24 ) such as addiction , dependence , and cognitive overload . To illustrate this further we divided the papers into four themes concerning teaching , learning , research , and administration . In the following section we sum up the key concerns and areas of discussion in literature . 8 Teaching Under teaching , it was noted that ChatGPT exhibits versatile applications , such as personalized and interactive learning , curriculum design , assessing homework , exams , and essays ( Ojha et at. , 2023 ; Kashiwamura , 2023 ; Huang , 2023 ) . To design new programs or to provide personalized teaching , universities need to collect and process vast amounts of student data , often without students ’ consent . This gives rise to questions surrounding data privacy and security ( Chan , 2023 ; Masters , 2023a ) . There is a need to ensure that robust data protection measures are in place to safeguard sensitive information and prevent its misuse . Research by Latif et al . ( 2023 ) highlighted that AI may inadvertently reinforce existing societal inequalities , gender bias , and nationality bias embedded within the original training data , negatively impacting the outputs of educational applications downstream . Undue reliance on AI-generated evaluations may compromise the quality of assessments , potentially failing to accurately gauge students ’ true capabilities ( Busch et al. , 2023 ; Curtis , 2023 ; Song & Lin , 2023 ) . Additionally , the introduction of AI in the educational process raises concerns about the dynamics between educators and students . A possible overreliance on AI-generated content , as discussed by Sharples ( 2023 ) , could alter the traditional teacher-student relationship and diminish educators ’ creative input and uniqueness in designing engaging lesson plans and activities . Learning The application of ChatGPT in learning includes personalized learning experiences , student support , language assistance , tutoring , content creation , grading and assessment , research aid , and career counseling , enhancing the learning process for students ( Kooli , 2023 ; 9 Lim et al. , 2023 ) . One major concern is the potential for an increase in plagiarism and cheating among students who might rely on GAI-generated content for essays and exams , thereby compromising the authenticity of their work ( Li et al. , 2023 ; Zhu & Yang , 2023 ) . This overreliance on ChatGPT may lead to a decline in students ’ sense of responsibility and commitment to academic integrity ( Ojha et al. , 2023 ) . Excessive use of ChatGPT may have adverse effects on students ’ critical thinking skills . If students heavily depend on AI-generated content , they might lose the ability to independently analyze and evaluate information ( Tlili et al. , 2023 ) . Another significant issue raised in the literature pertains to the risk of misinformation being propagated due to the highly persuasive and convincing nature of AI-generated content . This can lead to potential bias or manipulation of information presented to students . Reliance on AI interactions for academic or social purposes might diminish face- to-face interactions , potentially hindering the development of essential social skills among students . Striking a balance between AI and human interactions is crucial to foster a well- rounded educational experience . ChatGPT might inadvertently produce content that inaccurately or inappropriately represents certain cultural or identity groups , highlighting the need for ongoing refinement and sensitivity in AI language model development ( Busch et al. , 2023 ) . Research The applications of ChatGPT encompass efficient dataset analysis , code generation , literature reviews , timesaving for experimental design focus , and advancements in research discovery and development ( Dwivedi et al. , 2023 ; Li et al. , 2023 ) . In research , the integration of AI in academic publishing poses the risk of displacing human authors and undermining the value of their expertise , potentially impacting the 10 credibility of research . The attribution of fake references to AI-generated content presents another challenge , leading to misinformation and a decline in trust in academic sources . ( Curtis , 2023 ) . Joint authorship of editorial pieces like O ’ Connor and ChatGPT ( 2023 ) is a contentious as it challenges the established core values related to human-based authorship in academic publishing ( da Silva , 2023 ) . Some conferences permit the use of ChatGPT for writing papers , but only when ChatGPT itself is the subject of empirical research ( e.g. , ICML , 2023 ) . On the other hand , some research communities , such as the Association for Computational Linguistics ( ACL , 2023 ) , allow the use of ChatGPT based on specific guidelines . Administration ChatGPT can significantly reduce the time spent on human administrative tasks , such as responding to queries from applicants and assisting students in course enrollment ( UNESCO IESALC , 2023 ) . However , there are concerns surrounding the equitable , reliable , and transparent use of ChatGPT . Utilizing ChatGPT in the admissions processes can potentially introduce biases , especially if the AI model was trained on historical data that reflects past inequalities ( Fischer , 2023 ; Sharma et al . 2023 ) . To ensure fairness , transparency , and accountability , it is essential to provide applicants with clear explanations of how AI was employed to assess their applications and the specific factors that contributed to their acceptance or rejection . Additionally , the use of AI algorithms in admissions decisions carries the risk of inadvertently favoring applicants with certain characteristics or backgrounds , potentially impacting diversity and inclusion efforts within the university ( Busch et al. , 2023 ; Fischer , 2023 ) . Data privacy and security are also paramount considerations . To avoid unintentional discrimination , institutions should actively assess and address any biases in the AI model ’ s training data and decision-making process , striving to provide equal opportunities for all applicants . 11 DISCUSSION AND CONCLUSION We focused on articles written in a very short period , the first 7 months of 2023 , but covered literature written in English , Chinese and Japanese . Given that Chat GPT is trained on English-centric data ( Brown et al. , 2020 ) , it is important to gain insights into discussions going on in other AI technologically advanced countries . However , our review revealed that a few academic and research studies are published in non-English languages , particularly in Chinese and Japanese . Our scoping review showed that there are already publications that are considering the ethical implications of GAI , especially ChatGPT , in education generally and some focused on higher education . The majority of papers are discussion pieces , but there is some early empirical work . The ethical issues highlighted in these works are mainly concerned about academic integrity , assessment issues , and data-protection . Our analysis highlights the urgency of addressing ethical issues surrounding the use of GAI/ChatGPT in education . Collaboration among stakeholders is essential to establish clear guidelines , protect student privacy , and promote responsible AI use . By doing so , AI can enhance education and research without compromising fundamental principles . References ACL . ( 2023 ) . ACL 2023 Policy . 61st Annual Meeting of the Association for Computational Linguistics . Retrieved from https : Arksey , H. , & O'Malley , L. ( 2005 ) . Scoping studies : towards a methodological framework . 12 International journal of social research methodology , 8 ( 1 ) , 19-32 . Brown , T. , Mann , B. , Ryder , N. , Subbiah , M. , Kaplan , J. D. , Dhariwal , P. , ... & Amodei , D. ( 2020 ) . Language models are few-shot learners . Advances in Neural Information Processing Systems , 33 , 1877–1901 . Busch , F. , Adams , L. C. , & Bressem , K. K. ( 2023 ) . Biomedical ethical aspects towards the implementation of Artificial Intelligence in medical education . Med . Sci . Educ . Advance Online Publication . https : Chan , C. K. Y . ( 2023 ) . A comprehensive AI policy education framework for university teaching and learning . International Journal of Educational Technology in Higher Education , 20 ( 38 ) . https : Chan , C. K. Y. , & Hu , W. ( 2023 ) . Students ’ voices on generative AI : Perceptions , benefits , and challenges in higher education . arXiv preprint arXiv:2305.00290 . Chung , H. W. , Hou , L. , Longpre , S. , Zoph , B. , Tay , Y. , Fedus , W. , ... & Wei , J . ( 2022 ) . Scaling instruction-finetuned language models . arXiv preprint arXiv:2210.11416 . Curtis , N. ( 2023 ) . To ChatGPT or not to ChatGPT ? The Impact of Artificial Intelligence on Academic Publishing . The Pediatric Infectious Disease Journal , 42 ( 4 ) , 275. https : da Silva , J . A. T. ( 2023 ) . Is ChatGPT a valid author ? . Nurse Education in Practice , 68 , 103600. https : . Devlin , J. , Chang , M. W. , Lee , K. , & Toutanova , K. ( 2018 ) . Bert : Pre-training of deep bidirectional transformers for language understanding . arXiv preprint arXiv:1810.04805 . Dwivedi , Y. K. , Kshetri , N. , Hughes , L. , Slade , E. L. , Jeyaraj , A. , Kar , A. K. , ... & Wright , R. ( 2023 ) . “ So what if ChatGPT wrote it ? ” Multidisciplinary perspectives on opportunities , challenges and implications of generative conversational AI for research , 13 practice and policy . International Journal of Information Management , 71 , 102642. https : Fischer , I . ( 2023 ) . Evaluating the ethics of machines assessing humans . Journal of Information Technology Teaching Cases , 0 ( 0 ) . https : Huang R. ( 2023 ) . Ren Gong Zhi Neng Zheng Jia Su Jiao Yu Bian Ge : Xian Shi Tiao Zhan Yu Ying Dui Ju Cuo [ Artificial intelligence is accelerating educational transformation : Realistic challenges and countermeasures ] . Journal of the Chinese Society of Education , ( 06 ) , 26-33 . ICML . ( 2023 ) . Call for Papers . International Conference on Machine Learning . Retrieved from https : Kaplan , J. , McCandlish , S. , Henighan , T. , Brown , T. B. , Chess , B. , Child , R. , Gray , S. , Radford , A. , Wu , J. , & Amodei , D. ( 2020 ) . Scaling laws for neural language models . Advances in neural information processing systems , 33 , 1877–1901 . Kashiwamura , Y . ( 2023 ) . Sozoteki sagyo e shifuto o unagasu kyoin no noryoku kojo ga kadai ni : Kyoiku o kaeru seisei AI [ Encouraging a shift to creative work : Challenges for teacher capacity building : Generating AI that Changes Education ] . The Economist , 101 ( 24 ) , 98-100 . Kondo , C. , Tamada , K. , & Matsuda , T. ( 2023 ) . Seiseikei AI o daizai toshita joho-teki na mikata・kangaekata ni motozuku mondai kaiketsu shido jissen : ChatGPT to no kyo- zon o kangaeru [ Practicing problem-solving instruction based on informational perspectives and ways of thinking : Consider coexistence with ChatGPT ] . Journal of the Japan Society for Educational Technology , ( 2 ) , 255-258. https : Kooli , C. ( 2023 ) . Chatbots in education and research : A critical examination of ethical implications and solutions . Sustainability , 15 ( 7 ) , 5614 . 14 Krüger , L. , Krotsetis , S. , OpenAI ’ s Generative Pretrained Transformer 3 ( GPT-3 ) Model , & Nydahl , P. ( 2023 ) . ChatGPT : Fluch oder Segen in der Pflege ? [ ChatGPT : curse or blessing in nursing care ? ] . Medizinische Klinik , Intensivmedizin und Notfallmedizin , 10.1007/s00063-023-01038-3 . Advance online publication . https : Latif , E. , Mai , G. , Nyaaba , M. , Wu , X. , Liu , N. , Lu , G. , ... & Zhai , X . ( 2023 ) . Artificial general intelligence ( AGI ) for education . arXiv preprint arXiv:2304.12479 . Li , L. , Ma , Z. , Fan , L. , Lee , S. , Yu , H. , & Hemphill , L. ( 2023 ) . ChatGPT in education : A discourse analysis of worries and concerns on social media . arXiv preprint arXiv:2305.02201 . Lim , W. M. , Gunasekara , A. , Pallant , J. L. , Pallant , J. I. , & Pechenkina , E. ( 2023 ) . Generative AI and the future of education : Ragnarök or reformation ? A paradoxical perspective from management educators . The International Journal of Management Education , 21,100790. https : Masters , K. ( 2023 a ) . Ethical use of Artificial Intelligence in health professions education : AMEE Guide No . 158 . Medical Teacher , 45 ( 6 ) , 574-584. https : Masters , K. ( 2023 b ) . Medical teacher ’ s first ChatGPT ’ s referencing hallucinations : Lessons for editors , reviewers , and teachers . Medical Teacher , Med Teach , 45 ( 7 ) , 673-675 . DOI : 10.1080/0142159X.2023.2208731 Munn , L. ( 2023 ) . The uselessness of AI ethics . AI Ethics , 3 , 869–877 . https : Nozza , D. , Bianchi , F. , & Hovy , D. ( 2022 ) . Pipelines for social bias testing of large language models . In Proceedings of BigScience Episode # 5 -- Workshop on Challenges & Perspectives in Creating Large Language Models , virtual+Dublin . Association for 15 Computational Linguistics , 68-74 . O ’ Connor , S & ChatGPT . ( 2022 ) . Open artificial intelligence platforms in nursing education : Tools for academic progress or abuse ? . Nurse Education in Practice , 66 , 103537. https : Ojha , S. , Narendra , A. , Mohapatra , S. , & Misra , I . ( 2023 ) . From robots to books : An introduction to smart applications of AI in education ( AIEd ) . arXiv preprint arXiv:2301.10026 . Ouyang , L. , Wu , J. , Jiang , X. , Almeida , D. , Wainwright , C. , Mishkin , P. , ... & Lowe , R. ( 2022 ) . Training language models to follow instructions with human feedback . Advances in Neural Information Processing Systems , 35 , 27730-27744 . Radford , A. , & Narasimhan , K. ( 2018 ) . Improving Language Understanding by Generative Pre- Training . Retrieved from https : Sharma , P. , Thapa , K. , Dhakal , P. , Upadhaya , M. D. , Adhikari , S. , & Khanal , S. R. ( 2023 ) . Performance of ChatGPT on USMLE : Unlocking the potential of large language models for AI-assisted medical education . arXiv preprint arXiv:2307.00112 . Sharples , M. ( 2023 ) . Towards social generative AI for education : theory , practices and ethics . arXiv preprint arXiv:2306.10063 . Song , H & Lin , M ( 2023 ) . ChatGPT/Chuangshengshi rengong zhineng shidai xia jiaoshi de gongzuo biange : Jiyu , tiaozhan yu yingdui [ The Transformation of Teachers ’ Work in the Era of ChatGPT/AIGC : Opportunities , Challenges , and Responses ] . Journal of East China Normal University ( Educational Sciences ) , ( 07 ) ,78-90. https : . Tlili , A. , Shehata , B. , Adarkwah , M. A. , Bozkurt , A. , Hickey , D. T. , Huang , R. , & Agyemang , B . ( 2023 ) . What if the devil is my guardian angel : ChatGPT as a case study of using chatbots in education . Smart Learning Environments , 10 ( 1 ) , 15 . 16 Tricco , A. C. , Lillie , E. , Zarin , W. , O ’ brien , K. , Colquhoun , H. , Kastner , M. , ... & Straus , S. E. ( 2016 ) . A scoping review on the conduct and reporting of scoping reviews . BMC medical research methodology , 16 , 1-10 . UNESCO IESALC ( 2023 ) . ChatGPT and artificial intelligence in higher education : Quick start guide . Retrieved from https : Vaswani , A. , Shazeer , N. , Parmar , N. , Uszkoreit , J. , Jones , L. , Gomez , A. N. , Kaiser , Ł. , & Polosukhin , I . ( 2017 ) . Attention is all you need . Advances in neural information processing systems , 30. https : Weidinger , L. , Mellor , J. , Rauh , M. , Griffin , C. , Uesato , J. , Huang , P. S. , ... & Gabriel , I . ( 2021 ) . Ethical and social risks of harm from language models . arXiv preprint arXiv:2112.04359 . Xun , Y . ( 2023 ) . ChatGPT/Chuangshengshi rengong zhineng yu gaodeng jiaoyu de jiazhi he shiming [ ChatGPT/AIGC and the Value and Mission of Higher Education ] . Journal of East China Normal University ( Educational Sciences ) , ( 07 ) , 56-63 . Yanase , Y . ( 2023 ) . AI o katsuyo shite Eigo ronbun o sakusei suru Nihongo shasha ni totte no kadai to sono taisho [ Challenges and Strategies for Japanese Speakers Creating English Papers Using AI ] . Journal of Information Science and Technology , 73 ( 6 ) , 219-224 . Zhu , Y & Yang , F. ( 2023 ) . ChatGPT/Chuangshengshi rengong zhineng yu jiaoyu chuangxin : Jiyu , tiaozhan yiji weilai . [ ChatGPT/AIGC and Educational Innovation : Opportunities , Challenges , and the Future ] . Journal of East China Normal University ( Educational Sciences ) , ( 07 ) ,1-14. https : //doi.org/10.16382/j.cnki.1000- 5560.2023.07.001 . Zumsteg , J. M. , & Junn , C. ( 2023 ) . Will ChatGPT match to your program . Am J Phys Med Rehabil , 1 , 3-7 . 17","['ethical', 'implication', 'chatgpt', 'high', 'education', 'scope', 'review', 'ming', 'beverley', 'transdisciplinary', 'graduate', 'degree', 'program', 'global', 'initiative', 'graduate', 'school', 'education', 'ethic', 'generative', 'artificial', 'intelligence', 'high', 'education', 'scope', 'review', 'scope', 'review', 'explore', 'ethical', 'challenge', 'use', 'chatgpt', 'education', 'focus', 'particularly', 'issue', 'relate', 'high', 'education', 'review', 'recent', 'academic', 'article', 'write', 'aim', 'provide', 'comprehensive', 'overview', 'relevant', 'research', 'identify', 'gap', 'future', 'consideration', 'draw', 'arksey', 'malley', 'fivestage', 'scope', 'review', 'framework', 'identify', 'research', 'question', 'search', 'term', 'conduct', 'article', 'search', 'database', 'target', 'language', 'article', 'review', 'least', 'researcher', 'identify', 'main', 'ethical', 'issue', 'utilize', 'ai', 'education', 'particularly', 'high', 'education', 'analysis', 'ethical', 'issue', 'follow', 'framework', 'develop', 'deepmind', 'weiginger', 'identify', 'main', 'area', 'ethical', 'concern', 'language', 'model', 'majority', 'paper', 'concern', 'misinformation', 'harm', 'humancomputer', 'interaction', 'relate', 'harm', 'give', 'rapid', 'deployment', 'generative', 'artificial', 'intelligence', 'gai', 'imperative', 'educator', 'conduct', 'empirical', 'study', 'develop', 'sound', 'ethical', 'policy', 'use', 'gai', 'introduction', 'recent', 'wave', 'generative', 'artificial', 'intelligence', 'gai', 'originate', 'transformer', 'architecture', 'neural', 'network', 'vaswani', 'transformer', 'base', 'selfattention', 'mechanism', 'highly', 'scalable', 'suitable', 'parallel', 'computing', 'hence', 'quickly', 'become', 'approach', 'field', 'natural', 'language', 'processing', 'computer', 'vision', 'language', 'model', 'radford', 'usher', 'era', 'selfsupervise', 'learning', 'pretraine', 'largescale', 'textual', 'datum', 'learn', 'fundamental', 'knowledge', 'finetune', 'specific', 'downstream', 'task', 'qa', 'dialog', 'machine', 'translation', 'system', 'validate', 'large', 'language', 'model', 'scale', 'law', 'assert', 'performance', 'improvement', 'almost', 'always', 'achieve', 'increase', 'scale', 'model', 'parameter', 'pretraine', 'textual', 'datum', 'book', 'internet', 'resource', 'rapid', 'expansion', 'size', 'large', 'language', 'model', 'begin', 'model', 'scale', 'grow', '03b', 'parameter', 'brown', 'release', 'gpt3', 'release', 'openai', 'deepmind', 'lead', 'developer', 'technology', 'become', 'less', 'satisfied', 'mere', 'scale', 'increase', 'seek', 'align', 'model', 'output', 'feedback', 'real', 'human', 'text', 'generate', 'model', 'become', 'increasingly', 'humanlike', 'content', 'evermore', 'closely', 'align', 'human', 'value', 'launch', 'chatgpt', 'make', 'public', 'aware', 'already', 'capable', 'generate', 'humanquality', 'conversation', 'retrieve', 'store', 'knowledge', 'demand', 'achieve', 'natural', 'interaction', 'people', 'assistant', 'kick', 'current', 'frenzy', 'adapt', 'utilization', 'gai', 'various', 'field', 'include', 'education', 'application', 'high', 'education', 'garner', 'attention', 'iesalc', 'much', 'discussion', 'possible', 'benefit', 'create', 'teaching', 'material', 'analyze', 'student', 'datum', 'identify', 'learn', 'pattern', 'evidence', 'base', 'unclear', 'yet', 'gai', 'study', 'reveal', 'potential', 'risk', 'associate', 'generation', 'incorrect', 'information', 'know', 'hallucination', 'issue', 'bias', 'include', 'race', 'nationality', 'gender', 'discriminatory', 'content', 'nozza', 'gaigeneration', 'output', 'use', 'educationrelated', 'procedure', 'possibility', 'problematic', 'content', 'bias', 'assumption', 'magnify', 'result', 'negative', 'consequence', 'learner', 'educator', 'researcher', 'administrator', 'therefore', 'crucial', 'discuss', 'assess', 'ethical', 'implication', 'implement', 'chatgpt', 'educational', 'institution', 'especially', 'concern', 'use', 'teaching', 'learn', 'research', 'administration', 'increase', 'use', 'gai', 'learner', 'raise', 'issue', 'relate', 'academic', 'integrity', 'definition', 'authorship', 'assessment', 'method', 'pedagogical', 'implication', 'also', 'affect', 'researcher', 'conduct', 'study', 'generate', 'output', 'well', 'decision', 'make', 'admission', 'hire', 'educational', 'institution', 'manage', 'run', 'furthermore', 'increase', 'integration', 'ai', 'education', 'even', 'raise', 'question', 'continue', 'relevance', 'traditional', 'brickandmortar', 'educational', 'institution', 'scope', 'review', 'explore', 'ethical', 'challenge', 'use', 'chatgpt', 'education', 'focus', 'particularly', 'high', 'education', 'review', 'academic', 'article', 'write', 'aim', 'map', 'current', 'state', 'field', 'identify', 'gap', 'future', 'consideration', 'methodology', 'scope', 'review', 'commonly', 'use', 'identify', 'key', 'issue', 'newly', 'emerge', 'field', 'yet', 'substantial', 'body', 'literature', 'use', 'identify', 'knowledge', 'gap', 'set', 'research', 'agenda', 'identify', 'implication', 'decisionmake', 'study', 'adopt', 'arksey', 'malley', 'fivestage', 'scope', 'review', 'framework', 'involve', 'identify', 'initial', 'research', 'question', 'relevant', 'study', 'select', 'study', 'chart', 'datum', 'collate', 'summarizing', 'report', 'result', 'identify', 'relevant', 'study', 'limit', 'focus', 'article', 'focus', 'late', 'version', 'search', 'article', 'publish', 'use', 'search', 'term', 'chatgpt', 'generative', 'couple', 'education', 'ethic', 'see', 'table', 'capture', 'solid', 'evidencebased', 'study', 'discussion', 'topic', 'identify', 'scopus', 'main', 'database', 'conduct', 'initial', 'search', 'include', 'ongoing', 'research', 'work', 'also', 'include', 'arxiv', 'platform', 'provide', 'access', 'preprint', 'article', 'include', 'language', 'author', 'first', 'nearfirst', 'language', 'proficiency', 'japanese', 'chinese', 'facilitate', 'conduct', 'search', 'prominent', 'database', 'cinii', 'lead', 'ai', 'development', 'make', 'language', 'good', 'target', 'table', 'final', 'search', 'term', 'result', 'platform', 'database', 'search', 'term', 'titleabskey', 'chatgpt', 'generative', 'ai', 'titleabskey', 'education', 'titleabskey', 'chatgpt', 'generative', 'ai', 'titleabskey', 'education', 'ethic', 'arxiv', 'titleabskey', 'chatgpt', 'generative', 'ai', 'titleabskey', 'education', 'titleabskey', 'chatgpt', 'generative', 'ai', 'titleabskey', 'education', 'ethic', 'result', 'cinii', 'titleabskey', 'chatgpt', 'ai', 'titleabskey', '教育', 'titleabskey', 'chatgpt', 'ai', 'titleabskey', '課題', 'cnki', 'titleabskey', 'chatgpt', 'ai', 'titleabskey', 'education', 'titleabskey', 'chatgpt', 'ai', 'titleabskey', 'chart', 'datum', 'collation', 'initial', 'search', 'yield', 'result', 'include', 'education', 'ethical', 'concern', 'identify', 'article', 'meet', 'inclusion', 'criterion', 'figure', 'article', 'review', 'reviewer', 'third', 'reviewer', 'check', 'finding', 'figure', 'datum', 'extraction', 'process', 'analysis', 'ethical', 'issue', 'raise', 'article', 'rely', 'comprehensive', 'research', 'conduct', 'deepmind', 'weiginger', 'offer', 'framework', 'assess', 'ethical', 'social', 'risk', 'harm', 'arise', 'deployment', 'language', 'model', 'lm', 'table', 'area', 'discrimination', 'exclusion', 'toxicity', 'information', 'hazard', 'misinformation', 'harm', 'malicious', 'use', 'humancomputer', 'interaction', 'harm', 'automation', 'access', 'environmental', 'harm', 'table', 'ethical', 'social', 'risk', 'area', 'description', 'model', 'harm', 'reinforce', 'discrimination', 'stereotype', 'bias', 'marginalize', 'individual', 'promote', 'toxic', 'language', 'worsen', 'disparity', 'disadvantaged', 'group', 'leak', 'private', 'datum', 'sensitive', 'information', 'leak', 'provide', 'false', 'misleading', 'information', 'lead', 'less', 'informed', 'user', 'erode', 'trust', 'share', 'information', 'risk', 'use', 'lm', 'harm', 'include', 'enable', 'disinformation', 'campaign', 'personalize', 'scam', 'fraud', 'scale', 'development', 'malicious', 'computer', 'code', 'weapon', 'system', 'user', 'overestimation', 'ai', 'capability', 'lead', 'unsafe', 'usage', 'exploitation', 'manipulation', 'perpetuation', 'stereotype', 'unequal', 'benefit', 'limited', 'access', 'lm', 'impact', 'job', 'quality', 'creative', 'economy', 'create', 'global', 'disparity', 'risk', 'reward', 'finding', 'identify', 'paper', 'english', 'n19', 'follow', 'chinese', 'japanese', 'english', 'paper', 'empirical', 'study', 'conceptual', 'discussion', 'paper', 'predominant', 'focus', 'application', 'field', 'healthcare', 'medical', 'domain', 'comparison', 'number', 'chinese', 'japanese', 'paper', 'much', 'small', 'chinese', 'paper', 'general', 'discussion', 'application', 'predict', 'impact', 'chatgpt', 'education', 'japanese', 'article', 'report', 'initial', 'research', 'student', 'practical', 'experience', 'chatgpt', 'specification', 'paper', 'discuss', 'challenge', 'japanese', 'speaker', 'write', 'english', 'academic', 'paper', 'use', 'chatgpt', 'support', 'general', 'teaching', 'implication', 'kashimura', 'yanase', 'overall', 'little', 'discussion', 'specifically', 'focus', 'high', 'education', 'majority', 'paper', 'n19', 'generic', 'discuss', 'ethical', 'concern', 'teach', 'n19', 'learn', 'mostly', 'theoretical', 'conceptual', 'perspective', 'delve', 'specific', 'level', 'education', 'paper', 'specifically', 'focus', 'tertiary', 'education', 'concern', 'overall', 'pedagogical', 'implication', 'particularly', 'medical', 'education', 'n2', 'faculty', 'student', 'perception', 'research', 'implication', 'n1', 'table', 'article', 'review', 'author', 'language', 'education', 'level', 'main', 'areafocus', 'ethical', 'concern', 'database', 'english', 'tertiary', 'teaching', 'learn', 'administration', 'chan', 'english', 'tertiary', 'teaching', 'learn', 'curtis', 'english', 'tertiary', 'research', 'silva', 'generic', 'research', 'dwivedi', 'generic', 'research', 'fischer', 'krüger', 'generic', 'teaching', 'learn', 'research', 'generic', 'teaching', 'master', 'english', 'generic', 'teaching', 'administration', 'master', 'b', 'connor', 'chatgpt', 'tlili', 'english', 'generic', 'teaching', 'learn', 'research', 'english', 'generic', 'teaching', 'learn', 'english', 'tertiary', 'teaching', 'learn', 'database', 'english', 'tertiary', 'teaching', 'learn', 'generic', 'teaching', 'li', 'generic', 'teaching', 'learn', 'research', 'ojha', 'generic', 'teaching', 'administration', 'sharple', 'english', 'generic', 'teaching', 'learn', 'database', 'kondo', 'japanese', 'secondary', 'teaching', 'learn', 'yanase', 'japanese', 'generic', 'research', 'database', 'teaching', 'learning', 'song', 'teaching', 'chinese', 'tertiary', 'teaching', 'learn', 'generic', 'teaching', 'learn', 'term', 'focus', 'ethical', 'issue', 'majority', 'paper', 'concern', 'misinformation', 'harm', 'include', 'academic', 'integrity', 'cheating', 'assessment', 'issue', 'user', 'role', 'identify', 'clarify', 'information', 'andor', 'humancomputer', 'interaction', 'relate', 'harm', 'n24', 'addiction', 'dependence', 'cognitive', 'overload', 'illustrate', 'far', 'divide', 'paper', 'theme', 'concern', 'teaching', 'learn', 'research', 'administration', 'follow', 'section', 'sum', 'key', 'concern', 'area', 'discussion', 'literature', 'teaching', 'teach', 'note', 'chatgpt', 'exhibit', 'versatile', 'application', 'personalized', 'interactive', 'learning', 'curriculum', 'design', 'assess', 'homework', 'exam', 'essay', 'ojha', 'design', 'new', 'program', 'provide', 'personalized', 'teaching', 'university', 'need', 'collect', 'process', 'vast', 'amount', 'student', 'datum', 'often', 'student', 'consent', 'give', 'rise', 'question', 'surround', 'data', 'privacy', 'security', 'master', '2023a', 'need', 'ensure', 'robust', 'data', 'protection', 'measure', 'place', 'safeguard', 'sensitive', 'information', 'prevent', 'misuse', 'research', 'highlight', 'inadvertently', 'reinforce', 'exist', 'societal', 'inequality', 'gender', 'bias', 'nationality', 'bias', 'embed', 'original', 'training', 'datum', 'negatively', 'impact', 'output', 'educational', 'application', 'downstream', 'undue', 'reliance', 'aigenerate', 'evaluation', 'compromise', 'quality', 'assessment', 'potentially', 'fail', 'accurately', 'gauge', 'student', 'true', 'capability', 'curtis', 'song', 'additionally', 'introduction', 'ai', 'educational', 'process', 'raise', 'concern', 'dynamic', 'educator', 'student', 'possible', 'overreliance', 'aigenerate', 'content', 'discuss', 'sharple', 'alter', 'traditional', 'teacherstudent', 'relationship', 'diminish', 'educator', 'creative', 'input', 'uniqueness', 'design', 'engage', 'lesson', 'plan', 'activity', 'learn', 'application', 'chatgpt', 'learning', 'include', 'personalized', 'learn', 'experience', 'student', 'support', 'language', 'assistance', 'tutoring', 'content', 'creation', 'grade', 'assessment', 'research', 'aid', 'career', 'counseling', 'enhance', 'learning', 'process', 'student', 'major', 'concern', 'potential', 'increase', 'plagiarism', 'cheat', 'student', 'rely', 'gaigenerate', 'content', 'essay', 'exam', 'thereby', 'compromise', 'authenticity', 'work', 'overreliance', 'chatgpt', 'lead', 'decline', 'student', 'sense', 'responsibility', 'commitment', 'academic', 'integrity', 'ojha', 'excessive', 'use', 'chatgpt', 'adverse', 'effect', 'student', 'critical', 'thinking', 'skill', 'student', 'heavily', 'depend', 'aigenerate', 'content', 'lose', 'ability', 'independently', 'analyze', 'evaluate', 'information', 'tlili', 'significant', 'issue', 'raise', 'literature', 'pertain', 'risk', 'misinformation', 'propagate', 'highly', 'persuasive', 'convincing', 'nature', 'aigenerate', 'content', 'lead', 'potential', 'bias', 'manipulation', 'information', 'present', 'student', 'reliance', 'ai', 'interaction', 'academic', 'social', 'purpose', 'diminish', 'face', 'toface', 'interaction', 'potentially', 'hinder', 'development', 'essential', 'social', 'skill', 'student', 'strike', 'balance', 'ai', 'human', 'interaction', 'crucial', 'foster', 'well', 'round', 'educational', 'experience', 'chatgpt', 'inadvertently', 'produce', 'content', 'inaccurately', 'inappropriately', 'represent', 'certain', 'cultural', 'identity', 'group', 'highlight', 'need', 'ongoing', 'refinement', 'sensitivity', 'language', 'model', 'research', 'application', 'chatgpt', 'encompass', 'efficient', 'dataset', 'analysis', 'code', 'generation', 'literature', 'review', 'timesave', 'experimental', 'design', 'focus', 'advancement', 'research', 'discovery', 'development', 'research', 'integration', 'ai', 'academic', 'publishing', 'pose', 'risk', 'displace', 'human', 'author', 'undermine', 'value', 'expertise', 'potentially', 'impact', 'credibility', 'research', 'attribution', 'fake', 'reference', 'aigenerate', 'content', 'present', 'challenge', 'lead', 'misinformation', 'decline', 'trust', 'academic', 'source', 'curtis', 'joint', 'authorship', 'editorial', 'piece', 'connor', 'chatgpt', 'contentious', 'challenge', 'establish', 'core', 'value', 'relate', 'humanbase', 'authorship', 'academic', 'publishing', 'silva', 'conference', 'permit', 'use', 'chatgpt', 'write', 'paper', 'chatgpt', 'subject', 'empirical', 'research', 'eg', 'icml', 'hand', 'research', 'community', 'association', 'computational', 'linguistic', 'allow', 'use', 'chatgpt', 'base', 'specific', 'guideline', 'administration', 'chatgpt', 'significantly', 'reduce', 'time', 'spend', 'human', 'administrative', 'task', 'respond', 'query', 'applicant', 'assist', 'student', 'course', 'enrollment', 'however', 'concern', 'surround', 'equitable', 'reliable', 'transparent', 'use', 'chatgpt', 'utilize', 'chatgpt', 'admission', 'process', 'potentially', 'introduce', 'bias', 'especially', 'model', 'train', 'historical', 'datum', 'reflect', 'past', 'inequality', 'fischer', 'ensure', 'fairness', 'transparency', 'accountability', 'essential', 'provide', 'applicant', 'clear', 'explanation', 'employ', 'assess', 'application', 'specific', 'factor', 'contribute', 'acceptance', 'rejection', 'additionally', 'use', 'ai', 'algorithm', 'admission', 'decision', 'carry', 'risk', 'inadvertently', 'favor', 'applicant', 'certain', 'characteristic', 'background', 'potentially', 'impact', 'diversity', 'inclusion', 'effort', 'university', 'fischer', 'datum', 'privacy', 'security', 'also', 'paramount', 'consideration', 'avoid', 'unintentional', 'discrimination', 'institution', 'actively', 'assess', 'address', 'bias', 'model', 'training', 'datum', 'decisionmake', 'process', 'strive', 'provide', 'equal', 'opportunity', 'applicant', 'discussion', 'conclusion', 'focus', 'article', 'write', 'short', 'period', 'first', 'month', 'cover', 'literature', 'write', 'give', 'chat', 'gpt', 'train', 'englishcentric', 'datum', 'important', 'gain', 'insight', 'discussion', 'go', 'ai', 'technologically', 'advanced', 'country', 'however', 'review', 'reveal', 'academic', 'research', 'study', 'publish', 'nonenglish', 'language', 'particularly', 'chinese', 'japanese', 'scoping', 'review', 'show', 'already', 'publication', 'consider', 'ethical', 'implication', 'especially', 'chatgpt', 'education', 'generally', 'focus', 'high', 'education', 'majority', 'paper', 'discussion', 'piece', 'early', 'empirical', 'work', 'ethical', 'issue', 'highlight', 'work', 'mainly', 'concerned', 'academic', 'integrity', 'assessment', 'issue', 'dataprotection', 'analysis', 'highlight', 'urgency', 'address', 'ethical', 'issue', 'surround', 'use', 'gaichatgpt', 'education', 'collaboration', 'stakeholder', 'essential', 'establish', 'clear', 'guideline', 'protect', 'student', 'privacy', 'promote', 'responsible', 'use', 'enhance', 'education', 'research', 'compromise', 'fundamental', 'principle', 'reference', 'policy', '61st', 'annual', 'meeting', 'association', 'computational', 'linguistic', 'retrieve', 'https', 'arksey', 'h', 'l', 'scope', 'study', 'methodological', 'framework', 'international', 'journal', 'social', 'research', 'methodology', 'ryder', 'dhariwal', 'p', 'amodei', 'language', 'model', 'fewshot', 'learner', 'advance', 'neural', 'information', 'processing', 'system', 'adam', 'l', 'c', 'bressem', 'biomedical', 'ethical', 'aspect', 'implementation', 'artificial', 'intelligence', 'medical', 'education', 'educ', 'advance', 'online', 'publication', 'https', 'comprehensive', 'policy', 'education', 'framework', 'university', 'teaching', 'learn', 'international', 'journal', 'educational', 'technology', 'high', 'education', 'https', 'w', 'student', 'voice', 'generative', 'ai', 'perception', 'benefit', 'challenge', 'high', 'education', 'arxiv', 'preprint', 'l', 'zoph', 'tay', 'scale', 'instructionfinetune', 'language', 'model', 'arxiv', 'preprint', 'curtis', 'n', 'chatgpt', 'chatgpt', 'impact', 'artificial', 'intelligence', 'academic', 'publishing', 'pediatric', 'infectious', 'disease', 'journal', 'https', 'j', 'chatgpt', 'valid', 'author', 'nurse', 'education', 'practice', 'https', 'pretraining', 'deep', 'bidirectional', 'transformer', 'language', 'understand', 'arxiv', 'preprint', 'kshetri', 'hughe', 'l', 'slade', 'l', 'kar', 'k', 'wright', 'r', 'chatgpt', 'write', 'multidisciplinary', 'perspective', 'opportunity', 'challenge', 'implication', 'generative', 'conversational', 'ai', 'research', 'practice', 'policy', 'international', 'journal', 'information', 'management', 'https', 'fischer', 'evaluate', 'ethic', 'machine', 'assess', 'human', 'journal', 'information', 'technology', 'teaching', 'case', 'https', 'artificial', 'intelligence', 'accelerate', 'educational', 'transformation', 'realistic', 'challenge', 'countermeasure', 'journal', 'chinese', 'society', 'education', 'icml', 'call', 'paper', 'international', 'conference', 'machine', 'learning', 'retrieve', 'https', 'mccandlish', 'b', 'chess', 'child', 'r', 'gray', 'radford', 'amodei', 'scale', 'law', 'neural', 'language', 'model', 'advance', 'neural', 'information', 'processing', 'system', 'sagyo', 'unagasu', 'kyoin', 'encourage', 'shift', 'creative', 'work', 'challenge', 'teacher', 'capacity', 'building', 'generate', 'change', 'education', 'economist', 'tamada', 'seiseikei', 'ai', 'daizai', 'chatgpt', 'practice', 'problemsolving', 'instruction', 'base', 'informational', 'perspective', 'way', 'thinking', 'consider', 'coexistence', 'chatgpt', 'journal', 'educational', 'technology', 'https', 'c', 'chatbot', 'education', 'research', 'critical', 'examination', 'ethical', 'implication', 'solution', 'sustainability', 'krüger', 'l', 'krotsetis', 'generative', 'pretraine', 'transformer', 'gpt3', 'model', 'nydahl', 'p', 'chatgpt', 'fluch', 'oder', 'segen', 'pflege', 'chatgpt', 'curse', 'blessing', 'nursing', 'care', 'advance', 'online', 'publication', 'mai', 'lu', 'artificial', 'general', 'intelligence', 'agi', 'education', 'arxiv', 'preprint', 'fan', 'l', 'chatgpt', 'education', 'discourse', 'analysis', 'worry', 'concern', 'social', 'medium', 'arxiv', 'preprint', 'gunasekara', 'pallant', 'l', 'pallant', 'j', 'e', 'generative', 'ai', 'future', 'education', 'ragnarök', 'reformation', 'paradoxical', 'perspective', 'management', 'educator', 'international', 'journal', 'management', 'education', 'https', 'master', 'k', 'ethical', 'use', 'artificial', 'intelligence', 'health', 'profession', 'guide', 'medical', 'teacher', 'https', 'master', 'k', 'b', 'medical', 'teacher', 'reference', 'hallucination', 'lesson', 'editor', 'reviewer', 'teacher', 'medical', 'teacher', 'teach', 'l', 'uselessness', 'ethic', 'ai', 'ethic', 'https', 'nozza', 'bianchi', 'hovy', 'pipeline', 'social', 'bias', 'testing', 'large', 'language', 'model', 'proceeding', 'bigscience', 'episode', 'workshop', 'challenge', 'perspective', 'create', 'large', 'language', 'model', 'computational', 'linguistic', 'connor', 'chatgpt', 'open', 'artificial', 'intelligence', 'platform', 'nursing', 'education', 'tool', 'academic', 'progress', 'abuse', 'nurse', 'education', 'practice', 'https', 'ojha', 'mohapatra', 'misra', 'robot', 'book', 'introduction', 'smart', 'application', 'ai', 'education', 'aie', 'arxiv', 'preprint', 'wainwright', 'c', 'mishkin', 'p', 'lowe', 'r', 'training', 'language', 'model', 'follow', 'instruction', 'human', 'feedback', 'advance', 'neural', 'information', 'processing', 'system', 'radford', 'narasimhan', 'k', 'improve', 'language', 'understanding', 'generative', 'pre', 'training', 'retrieve', 'https', 'thapa', 'k', 'dhakal', 'upadhaya', 'khanal', 'r', 'performance', 'chatgpt', 'usmle', 'unlock', 'potential', 'large', 'language', 'model', 'aiassiste', 'medical', 'education', 'arxiv', 'preprint', 'sharple', 'social', 'generative', 'ai', 'education', 'theory', 'practice', 'ethic', 'arxiv', 'preprint', 'arxiv230610063', 'song', 'h', 'yingdui', 'transformation', 'teacher', 'work', 'era', 'chatgptaigc', 'opportunity', 'challenge', 'response', 'journal', 'university', 'educational', 'science', 'https', 'tlili', 'shehata', 'bozkurt', 'hickey', 'r', 'agyemang', 'devil', 'guardian', 'angel', 'chatgpt', 'case', 'study', 'use', 'chatbot', 'education', 'smart', 'learn', 'environment', 'tricco', 'c', 'e', 'colquhoun', 'h', 'kastner', 'straus', 'e', 'scope', 'review', 'conduct', 'reporting', 'scope', 'review', 'bmc', 'medical', 'research', 'methodology', 'iesalc', 'chatgpt', 'artificial', 'intelligence', 'high', 'education', 'quick', 'start', 'guide', 'retrieve', 'https', 'shazeer', 'gomez', 'n', 'kaiser', 'polosukhin', 'attention', 'need', 'advance', 'neural', 'information', 'processing', 'system', 'https', 'weidinger', 'l', 'griffin', 'gabriel', 'ethical', 'social', 'risk', 'harm', 'language', 'model', 'arxiv', 'preprint', 'shime', 'chatgptaigc', 'value', 'mission', 'high', 'education', 'university', 'educational', 'science', 'yanase', 'ai', 'eigo', 'ronbun', 'sakusei', 'sono', 'taisho', 'challenge', 'strategy', 'japanese', 'speaker', 'create', 'english', 'paper', 'use', 'ai', 'journal', 'information', 'science', 'technology', 'chatgptaigc', 'educational', 'innovation', 'opportunity', 'challenge', 'future', 'journal', 'university', 'educational', 'science', 'https', 'c', 'chatgpt', 'match', 'program']",
"Mitigating Estimation Errors by Twin TD-Regularized Actor and Critic for
  Deep Reinforcement Learning","[{'href': 'http://arxiv.org/abs/2311.03711v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2311.03711v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-11-07 04:30:51,"𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in Search Queries and Query
Suggestions

Fabian Haak
fabian.haak@th-koeln.de
Technische Hochschule Köln
Cologne, Germany

Philipp Schaer
philipp.schaer@th-koeln.de
Technische Hochschule Köln
Cologne, Germany

ABSTRACT
This publication describes the motivation and generation of 𝑄𝑏𝑖𝑎𝑠 ,
a large dataset of Google and Bing search queries, a scraping tool
and dataset for biased news articles, as well as language models for
the investigation of bias in online search. Web search engines are a
major factor and trusted source in information search, especially
in the political domain. However, biased information can influence
opinion formation and lead to biased opinions. To interact with
search engines, users formulate search queries and interact with
search query suggestions provided by the search engines. A lack of
datasets on search queries inhibits research on the subject. We use
𝑄𝑏𝑖𝑎𝑠 to evaluate different approaches to fine-tuning transformer-
based language models with the goal of producing models capable of
biasing text with left and right political stance. Additionally to this
work we provided datasets and language models for biasing texts
that allow further research on bias in online information search.

CCS CONCEPTS
• Information systems → Web search engines; Query suggestion;
Query reformulation; • Computing methodologies → Natural
language generation.

KEYWORDS
web search, dataset, bias, query suggestion, search queries, language
models, transformers

ACM Reference Format:
Fabian Haak and Philipp Schaer. 2023. 𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in
Search Queries and Query Suggestions. In 15th ACM Web Science Conference
2023 (WebSci ’23), April 30-May 1, 2023, Evanston, TX, USA. ACM, New York,
NY, USA, 6 pages.

3
2
0
2

v
o
N
9
2

]

R

I
.
s
c
[

1
v
0
8
7
7
1
.
1
1
3
2
:
v
i
X
r
a

1 INTRODUCTION
Search engines such as Google and Bing are seen as trustworthy
sources of information on many topics, including political news and
information [14, 30]. Further, search engines have proven to have a
major impact on the formation of political opinions [15]. To interact
with search engines, users formulate search queries that are an
expression of their information need. Based on these queries search
engines usually provide a set of search query suggestions [28].
Queries and the choices users make when interacting with query
suggestions are based on the information need they want to satisfy,
which is often to support their personal opinions or beliefs founded
on previously encountered information [6]. This interaction process
and the results presented to the users are prone to be biased [7, 22,
25] and therefore the high level of trust can be seen as problematic.
However, the true effect of biased search queries and different
types of inherent biases on the actual list of search results has not
sufficiently been investigated. One of the main reasons for the in-
frequency of studies on bias in search queries might be a lack of
publicly available datasets. datasets that include real-world user
queries, query suggestions, and actual query reformulations are
rare. One of the reasons is that collecting search queries from users
is problematic due to privacy concerns. Although there are tech-
niques like pseudonymization, query logs enable the identification
of users [5]. Previously available datasets such as the AOL query
log dataset [29] are no longer available, and their usage is morally
debatable.

Using unpersonalized search query suggestions as proxies might
solve this issue. Query suggestions describe the list of predicted
queries suggested to users during the input of search queries by
the search engine, sometimes also called search predictions [37] or
query auto completion [10]. While these search query suggestions
can be generated locally from the result set [40] or be taken from
global knowledge bases [20], in web search suggestions are mostly
based on frequently issued queries by users [37]. It can therefore be
assumed that in many cases query suggestions are popular related
search queries for the initial root query that represented the user’s
interest in a topic or entity.

The U.S. news domain is a popular domain for bias research due
to its sociological relevance and the two-party left-right spectrum
that facilitates bias analysis [34]. One of the benefits of our dataset
is to enable in-depth investigations of the correlation between bias
in search queries and search results, as search results for popu-
lar web search engines can easily be collected using our provided
dataset of search queries. Therefore, one goal of this work is to
provide a large dataset of search query suggestions for the U.S. po-
litical news domain. Additionally, we provide a transformer-based
methodology for biasing text. Such a system can be a useful tool in

 
 
 
 
 
 
WebSci ’23, April 30-May 1, 2023, Evanston, TX, USA

Haak and Schaer

bias research, f.e. for generating biased derivatives of search queries.
Thus, we evaluate a range of fine-tuning scenarios to find settings,
that produce the most biased results. We provide the fine-tuned
transformer-based language models that are capable of inducing
left and right political stance bias in the form of lexical biases.

This publication describes the motivation and generation of
𝑄𝑏𝑖𝑎𝑠 , a large dataset of Google and Bing search queries, a scraping
tool and dataset for biased news articles, as well as language models
for the investigation of bias in online search.

The main contributions of 𝑄𝑏𝑖𝑎𝑠 and this paper are:

• Two datasets:1 (1) The (to the best of our knowledge) largest
labeled dataset of search query suggestions of a single do-
main for Google and Bing. (2) A dataset of biased news
articles of the U.S. political news domain.

• A scraping tool that allows researchers to easily retrieve an

up-to-date version of the biased news dataset.2

• A new approach for producing biased search queries, us-
ing intentionally biased transformer-based language models
capable of producing biased texts.

• An evaluation of different fine-tuning settings to find the
most capable setup for producing bias-inducing language
models.

2 BACKGROUND AND RELATED WORK

Definition of Bias is the News Domain. Due to the subjective na-
ture of bias, few publications attempt to explicitly define bias in
news [1, 11, 13, 32]. ’Biased news’ generally describes non-neutral
and opinionated news, but both are fuzzy attributes [32]. In the polit-
ical domain, the term opinionated describes having a fixed political
opinion and agenda, aspect of a cognitive (author- or reader-sided)
bias often described as partisan bias [17] or (political) stance [24].
Partisanship manifests at different granularities. At publishing or
reporting level, partisanship is expressed by the selection of certain
topics called selection bias, and coverage bias, the selection of dif-
ferent views on a topic [13]. At text level, statement bias describes
“members of the media interjecting their own opinions into the
text” [13], manifesting in overall opinionated texts. This can take
various forms, ranging from phrasing bias, the use of non-neutral
language [21] to moral framing and ideological bias [26]. Spin bias
describes bias introduced either by omitting necessary (neutral)
information or adding unnecessary information [11]. At word and
n-gram level, biases manifest as linguistic or lexical biases [11].
Most linguistic biases are highly domain- and context-specific. For
example, framing bias describes subjective words, while epistemo-
logical bias can be attributed to words targeting the credibility of a
statement [32]. Since these biases can be identified more objectively,
most approaches to detecting biases rely on the identification of
linguistic biases [1].

Research on Bias in Online Search and Search Queries. Few pub-
lications investigate bias in online search in aspects other than
search results: Robertson et al. [34] investigate partisan bias and
filter bubble effects in political searches by auditing SERPS for a
set of queries. They did not find significant evidence, that unbiased

search queries in real search sessions performed by real users lead
to filter bubble effects. The unanswered question is, whether biased
queries in general lead to biased search results.

Few studies investigate bias in search queries. Due to the difficult
accessibility of biased search queries, most studies focus on bias in
search query suggestions: in most of those, the authors investigate
topical group biases in search query suggestions in the political
domain [8, 18, 19]. Overall, they observe minor topical gender biases
for search queries consisting of names of politicians. Research on
bias in online search has shown, that “factors such as the topic of
the query, the phrasing of query and the time at which a query is
issued also impact the bias seen by the users”[23].

Datasets of Biased News and Search Queries. Baly et al. [4] predict
media bias using news articles collected from AllSides balanced
news.3 AllSides news is a popular source for balanced news [4, 11,
26], since AllSides has a high standard for assigning bias labels [1].
Similarly, Chen et al. [11] use AllSides-labeled news articles and
adfontes labels to analyze bias at different granularities. Mokhbe-
rian et al. [26] develop a framing bias detection and quantification
approach, using a collection of news articles that they label accord-
ing to news outlet bias labels provided by AllSides.4 datasets for
search queries and query suggestions are sparse: most are either not
available anymore [29] or focus on a narrow topic [8]. Robertson
et al. [35] introduce recursive algorithm interrogation, a technique
for recursively retrieving query suggestions of a root query and
their consecutive suggestions. This technique was employed by
Haak and Schaer [19] to investigate bias in the German political
domain. However, to the best of our knowledge, there currently is
no large-scale dataset on search queries.

3 ALLSIDES SCRAPER AND DATASETS
We present two novel datasets, that promote the investigation of
bias in online news search. Both datasets can be found on Zenodo as
mentioned in section 1. Further, we provide a web scraping tool for
retrieving an up-to-date version of the AllSides dataset we provide.
This section describes the content of the datasets as well as their
creation process. Figure 1 shows how we assembled the datasets
and use them in our approach for creating biasing language models.

3.1 AllSides Scraper
As described in section 2, the AllSides platform and the provides
news is a frequently used source for high quality labeled biased
news. We want to provide an easy means of retrieving the news
and all corresponding information. Similar datasets have previously
been produced, as mentioned in section 2. However, compared to the
currently most recent available version by Baly et al. [4], our version
includes more than 20 percent of additional articles. Furthermore,
for many tasks, especially in the news domain, it is relevant to have
the most recent documents available. We provide a Python-based
scraper, that scrapes all available AllSides news articles and gathers
available information as described in section 3.2. By providing the
scraper we facilitate access to a recent version of the dataset for
other researchers.

1The datasets can be found at Zenodo (https://doi.org/10.5281/zenodo.7682914)
2https://github.com/irgroup/Qbias

3https://www.allsides.com/unbiased-balanced-news
4https://www.kaggle.com/datasets/snapcrack/all-the-news

𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in Search Queries and Query Suggestions

WebSci ’23, April 30-May 1, 2023, Evanston, TX, USA

3.3 Search Queries in U.S. News Domain Dataset
The second dataset we provide consists of 671,669 search query
suggestions for root queries based on tags of the AllSides biased
news dataset. We collected search query suggestions from Google
and Bing for the 1,431 topic tags, that have been used for tagging
AllSides news at least five times, approximately half of the total
amount of topics. The topic tags include names, a wide range of
political terms, agendas, and topics (e.g., ""communism"", ""libertarian
party"", ""same-sex marriage""), cultural and religious terms (e.g., ""Ra-
madan"", ""pope Francis""), locations and other news-relevant terms.
On average, the dataset contains 469 search queries for each topic.
In total, 318,185 suggestions have been retrieved from Google and
353,484 from Bing.

Using a python implementation loosely adopting the framework
provided by Robertson et al. [35], we scraped query suggestions for
the topics as root queries. Using Google Colab to run our scraper,
we retrieved ten search query suggestions provided by the Google
and Bing search autocomplete systems for the input of each of these
root queries. Furthermore, we extended the root queries by the let-
ters a to z (e.g., ""democrats"" (root term) → ""democrats a"" (query
input) → ""democrats and recession"" (query suggestion)). The goal
of this procedure is to simulate a user’s input during information
search. Retrieving the suggestions for the root query and the ex-
tended queries generates a total of up to 270 query suggestions per
topic and search engine. The dataset we provide contains columns
for root term, query input, and query suggestion for each suggested
query. The location from which the search is performed is the loca-
tion of the Google servers running Colab, in our case Iowa in the
United States of America, which is added to the dataset. Since we
perform the scrape on a blank browser for each of the searches,
personalization effects other than the location did not effect the
suggested search queries. Our scraping setup thus eliminates per-
sonalization effects that could, in theory, cause echo chamber ef-
fects [34]. Search engine providers describe that query suggestions
are based on other users’ searches [37]. Successful attempts to in-
fluence query suggestions have confirmed this claim [38]. Thus, we
deduce that query suggestions reflect real, frequently used search
queries and can be used as proxies for search queries. Despite the
lack of information on the frequency of the collected search queries,
our dataset contains the ranks of suggestions, as well as the sugges-
tions to the root term for approximating the most frequent search
queries. Assuming that the topical tags in the AllSides news reflect
popular topics, the produced dataset consists of real search queries
for news-relevant topics.

4 DEVELOPING BIASING LANGUAGE MODELS
Another contribution 𝑄𝑏𝑖𝑎𝑠 is to produce a pair of transformer-
based language models that are capable of inducing left or right
partisanship as linguistic biases. This can be done by leveraging
the masking function on the words of the target document.

4.1 Domain-Adopting DistilBERT
This section describes the methodological approach for develop-
ing transformer-based language models capable of biasing texts.
Usually, systems are developed with the goal of producing and
reproducing as little bias as possible or to debias biased texts [31].

Figure 1: Pipeline used for generating bias-inducing language
models. Black boxes represent datasets, language models
(only available upon request), and tools provided via Zenodo,
and GitHub.

3.2 AllSides Biased News Dataset
The dataset contains 21,747 news articles collected from AllSides
balanced news headline roundups [2] in November 2022. The All-
Sides balanced news feature three expert-selected U.S. news articles
from sources of different political views (left, right, center), often
featuring spin bias, slant other forms of non-neutral reporting on
political news [1]. All articles are tagged with a bias label by four
expert annotators based on the expressed political partisanship,
left, right, or neutral [1]. The AllSides balanced news aims to offer
multiple political perspectives on important news stories, educate
users on biases, and provide multiple viewpoints [1]. Collected data
includes the headline, news text, publishing date, topic tags (e.g.,
""Republican party"", ""coronavirus"", ""federal jobs""), links to the article,
and the publishing news outlet. We also include AllSides’ neutral
description of the topic of the articles. Overall, the produced dataset
contains 10,273 articled tagged as left, 7,222 articles tagged as right,
and 4,252 articles tagged as center. The collected articles have been
published between June 2012, and the date of the data collection at
the end of November 2022. We use the AllSides dataset for develop-
ing our biasing language model, as described in section 4. To allow
for easier access to the most recent and complete version of the
dataset for future research, we provide the scraping tool described
in section 3.1.

WebSci ’23, April 30-May 1, 2023, Evanston, TX, USA

Haak and Schaer

We try to achieve the opposite, biasing texts by reproducing biases
inherent in biased fine-tuning datasets. This would, for example,
allow for simulating a user that searches for biased terms poten-
tially caused by exposure to biased news. With the goal of cap-
turing biased language in pretrained language models and using
the HuggingFace Transformers module[39], we fine-tune the base
DistilBERT model, which was trained in Wikipedia articles and a
large book dataset.5 DistilBERT–[36] was chosen as a base model
since it has proven to perform well with comparably small datasets,
performs better than BERT despite its smaller size [36], and for its
aptness for adopting domain-specificity [3, 9]. We are aware that
there are more effective models, but to show the suitability of our
approach, we chose DistilBERT due to its efficiency and sufficient
effectiveness.

We fine-tuned a range of pairs of models, each with one left
model fine-tuned on the part of the AllSides corpus tagged as left-
biased news and one right model fine-tuned with documents of
the dataset labeled as right-biased news. We produced 24 models,
a left and a right model for 12 combinations of parameters. Our
goal is to find the ideal approach for capturing and reproducing
as much bias as possible while producing meaningful results. The
central aspect we varied in fine-tuning the models is the data used.
The models are developed using three different data configurations:
(a) the headline and news text of each of the left and right news
articles of the AllSides dataset, (b) only the news text, and (c) only
the headline. Another factor we evaluate is the use of padding or
concatenation to generate a consistent chunk size for fine-tuning.
The chosen chunk size is the max length of fine-tuning documents
for the padding approach and 128 for the concatenation approach.
Lastly, we compare the effects of intentional overfitting by rais-
ing the number of epochs to 20, compared to a more reasonable 6
epochs, which proved to produce a good combination of training
loss and validation loss for all dataset configurations. In theory,
overfitting could be another possibility for reproducing biased for-
mulations from the texts used in fine-tuning. This is why we tried
to intentionally raise the number of epochs as a parameter [27].

4.2 Evaluating the Biasing Language Models
All 24 models are available upon request. We decided not to make
the models publicly available due to concerns about potential harms
that could be caused by misuse of the systems that we further elab-
orated in section 5. For evaluating the models’ effectiveness, we
choose a combination of manual assessment and quantitative mea-
sures. The task of the models is to generate biased output, however,
biases are diverse and not easily measurable. Since we have no way
of identifying biased tokens in the dataset, perplexity is not a useful
measure for evaluating the model. Further, we cannot effectively
measure bias as a criterion for the effectiveness of the ability of the
systems to produce biased versions of queries. However, we can
assume that when masking words in search queries for topics of
the political news domain and letting the pairs of language models
predict the words, the output of the left and right models should
differ from each other. We choose to evaluate their performance on
search queries since we want to use the models in future research
on bias in search results for biased and unbiased search queries.

5https://huggingface.co/distilbert-base-uncased

Further, they represent texts of a different type but the same domain
as the texts used in training the models.

For each pair of models, we measure the difference between the
two models’ 10 most probable non-punctuation predictions for 100
randomly selected queries from the dataset by measuring the Rank
Biased Overlap (RBO) with 𝑝 = 0.9. We mask a random token of
each query that is neither part of the original news topic nor a stop-
word since we want to let the models generate meaning-carrying
tokens while keeping the query’s topic intact. Although nonsensical
random predictions or neologisms would lead to great RBO scores,
we want to generate queries, that humans could have formulated.
To assure that the models generate output that makes sense, we let
the models generate next word predictions for ten bias-provoking
sentences (e.g., ""Hilary Clinton is a"", ""Covid Vaccines should be"").
Two domain experts (a professor and a postdoctoral researcher)
then label nonsensical and biased predictions. We measure the inter-
annotator agreement on the individual statements (independent of
the models, that produce the statements) with Cohen’s Kappa [12].
For nonsensical predictions, we obtained a value of 0.18 (slight
agreement), for biased predictions the value is 0.84 (almost perfect).
The bias labels are assigned if the suggested word induces political
stance bias.

Table 1 shows the results of the model evaluation process. The
lower ℎ,𝑡, or ℎ𝑡 describes if headline, text, or both have been used.
𝑅𝐵𝑂 is the rank bias overlap between the left and the right model
of each configuration. 𝑃𝑛𝑜𝑛𝑠 describes the average percentage of
nonsensical predictions of both models and 𝑃𝑏𝑖𝑎𝑠 the percentage
of biased predictions. Overall, the models fine-tuned with only
headlines show the best (lowest) RBO scores, as well as the lowest
𝑃𝑛𝑜𝑛𝑠 scores. Headlines and text and only text perform more or
less equally in terms of their RBO. Concatenating texts produces
on average better results than padding, although for fine-tuning
with only headlines, the effect is minimal. Intentional overfitting
by raising the amount of training epochs seems to worsen the RBO
scores. The percentage of nonsensical predictions does not differ
much between different fine-tuning setups, fine-tuning on texts
only seems to be the only scenario that increases the percentage
of nonsensical predictions. Many of the models generate a high
percentage of biased suggestions, with the headline models having
the highest percentage of biased predictions.

5 DISCUSSION
Our language models show, that by using small, high-quality datasets,
it is possible to fine-tune transformer-based language models to bias
texts. In our fine-tuning setup, models fine-tuned with headlines
produce the overall best results. As the main reason for that, we
assume that the high amount of quotes in news texts, which often
are statements the authors of the articles disagree with, might have
induced noise in fine-tuning the models. Further, the condensed na-
ture of headlines, which aims to catch attention, might also reflect
in resulting language models. The output of left and right models
fine-tuned with headlines produce texts containing linguistic biases.
For example, for ""Donald Trump is a"", our model produces ""hero""
as right-biased and ""fraud"" as left-biased next word predictions.
The overall low percentage of nonsensical predictions supports
RBO as a suited evaluation metric. Despite the overall good results,

𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in Search Queries and Query Suggestions

WebSci ’23, April 30-May 1, 2023, Evanston, TX, USA

Table 1: Evaluation results of the fine-tuned language models.
The highlighted values are the best results for each metric.

Model configuration
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡𝑡
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ𝑡
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔𝑡
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ𝑡
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔
𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔

𝑅𝐵𝑂 𝑃𝑛𝑜𝑛𝑠
0.54
0.04
0.10
0.72
0.08
0.68
0.03
0.56
0.11
0.73
0.07
0.68
0.54
0.04
0.05
0.74
0.07
0.77
0.03
0.58
0.06
0.77
0.07
0.77

𝑃𝑏𝑖𝑎𝑠
0.33
0.14
0.15
0.5
0.14
0.23
0.38
0.28
0.1
0.43
0.24
0.12

future research should investigate the findings with other language
models and compare the performance to our results.

Our scraping tool and the datasets not only allow us to effectively
generate the language models but enables the investigation of other
research topics in the political news information search domain
such as bias classification or sentiment analysis.

Remark on Moral Issues. We are aware, that building bias-inducing
systems and providing datasets that enable the reproduction of
developing similar systems is problematic. We also did consider
that our work can inspire and help to develop transformer-based
language models capable of producing biased, toxic, or otherwise
harmful texts. The severity of openly published biased systems
has been shown by other such models, e.g. the gpt-4chan model.6
The publication of the model incited a debate, that shows how
problematic biased language made available to a wide public au-
dience are, despite explicitly stating the intended use for research
applications [16, 33].

Our models are biased primarily in terms of linguistic biases
that reflect political stances and views on political topics. Since this
can in part include objectively wrong and opinionated statements,
hate speech, racism, and other forms of despicable language and
toxicity, our models can and will reproduce text, that conveys these
phenomena. To minimize the harmful effects of our publication
that could be caused by the misuse of the models, we decided
to not make our biased models publicly accessible. Despite that,
we provide access to the models upon request for research, if a
reasonable application use case is provided. However, we do so
in the interest of investigating biases and correlations of bias in
online information search, with the goal of increasing transparency
and fairness. Raising awareness for how easily, intentionally or not,
biases can be reproduced and induced with AI systems is part of
this endeavor.

Although we have shown how the data we provide can be used
to create models that produce biased language we chose to publish
the datasets and scraper. This is mainly because of three reasons:
(a) as stated in section 3 similar datasets are available publicly, (b)
we believe that the benefits of raising awareness outweigh making

6https://huggingface.co/ykilcher/gpt-4chan

already public biased data more accessible, and (c) datasets on biased
language are required for developing systems to detect and inhibit
bias. With our work, we hope to highlight the need to assure, that
data used for developing models is unbiased and raise awareness for
how easily transformer-based language models can be fine-tuned
to produce biased language.

6 OUTLOOK
This publication represents the first and foundational milestone of
a larger-scale investigation of bias in online information search. As
a major next contribution, we plan to use the presented datasets
and models to investigate the effects of biased and unbiased search
queries on the search results of different search engines for popular
topics of the U.S. political news domain. To accomplish this, we need
to overcome the issue of subjectivity and lack of effective method-
ological approaches to bias identification other than by employing
human annotation. We plan to introduce an ensemble of methods,
including bias-agnostic analysis of linguistic differences, lexical
features, and transformer-based approaches for bias classification.
Further, we plan to conduct a study using a simulation approach. By
simulation different user behaviors in terms of information need,
formulating queries, and interacting with query suggestions in an
interactive information search simulation, we plan to gain insights
into echo chamber effects and bias formation in information search.
Additionally, we hope to identify which user properties and be-
haviors increase and which lower the risk of encountering bias in
information search.

7 CONCLUSION
This work presents 𝑄𝑏𝑖𝑎𝑠 , a first milestone of our ongoing research
on bias in online search. We present two datasets: a biased news
dataset and a large dataset of biased and unbiased search queries
for topics of the U.S. political news domain. Further, we provide a
scraping tool, that allows for collecting bias-labeled news texts from
AllSides. Lastly, we evaluate approaches to fine-tuning DistilBERT
transformer-based language models for biasing texts and publish
our models capable of inducing left and right political stance bias
in the form of lexical biases.

REFERENCES
[1] AllSides. 2021. How AllSides Creates Balanced News: A Step-by-Step Guide.
Retrieved Nov 30, 2022 from https://www.allsides.com/blog/how-does-allsides-
create-balanced-news

[2] AllSides. 2022. Balanced News Headlines Roundup. Retrieved Nov 30, 2022

from https://www.allsides.com/unbiased-balanced-news

[3] Jing Bai, Rui Cao, Wen Ma, and Hiroyuki Shinnou. 2020. Construction of Domain-

Specific DistilBERT Model by Using Fine-Tuning. In TAAI. 237–241.

[4] Ramy Baly, Giovanni Da San Martino, James Glass, and Preslav Nakov. 2020.
We Can Detect Your Bias: Predicting the Political Ideology of News Articles. In
EMNLP. 4982–4991.

[5] Michael Barbaro and Tom Zeller. 2006. A Face is exposed for AOL searcher no.

4417749. New York Times (01 2006).

[6] Nicholas Belkin, Colleen Cool, Diane Kelly, S.-J Lin, S.Y Park, Jose Perez-carballo,
and Cynthia Sikora. 2001. Iterative exploration, design and evaluation of support
for query reformulation in interactive information retrieval. IPM 37 (05 2001),
403–434.

[7] Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam
Kalai. 2016. Man is to Computer Programmer as Woman is to Homemaker?
Debiasing Word Embeddings. In NIPS. 4356–4364.

[8] Malte Bonart, Anastasiia Samokhina, Gernot Heisenberg, and Philipp Schaer.
2019. An investigation of biases in web search engine query suggestions. OIR 44,
2 (2019), 365–381.

WebSci ’23, April 30-May 1, 2023, Evanston, TX, USA

Haak and Schaer

[39] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe
Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu,
Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
and Alexander Rush. 2020. Transformers: State-of-the-Art Natural Language
Processing. In EMNLP. 38–45.

[40] Jinxi Xu and W. Bruce Croft. 2000. Improving the Effectiveness of Information
Retrieval with Local Context Analysis. ACM Trans. Inf. Syst. 18, 1 (Jan. 2000),
79–112. https://doi.org/10.1145/333135.333138

[9] Berfu Büyüköz, Ali Hürriyetoˇglu, and Arzucan Özgür. 2020. Analyzing ELMo

and DistilBERT on Socio-political News Classification. In AESPEN.

[10] Fei Cai and Maarten de Rijke. 2016. A Survey of Query Auto Completion in

Information Retrieval. FNTIR 10, 4 (2016), 273–363.

[11] Wei-Fan Chen, Khalid Al Khatib, Henning Wachsmuth, and Benno Stein. 2020.
Analyzing Political Bias and Unfairness in News Articles at Different Levels of
Granularity. In NLPCSS. 149–154. https://doi.org/10.18653/v1/2020.nlpcss-1.16
[12] Jacob Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educ Psychol

Meas 20, 1 (1960), 37–46.

[13] Dave D’Alessio and Mike Allen. 2000. Media Bias in Presidential Elections: A

Meta-Analysis. J. Commun. 50, 4 (2000), 133–156.

[14] Edelman. 2022. 2022 Edelman Trust Barometer. Retrieved Nov 30, 2022 from

https://www.edelman.com/trust/2022-trust-barometer

[15] Robert Epstein and Ronald E. Robertson. 2015. The search engine manipulation
effect (SEME) and its possible impact on the outcomes of elections. PNAS 112,
33, E4512–E4521. Publisher: National Academy of Sciences Section: PNAS Plus.
[16] Matthew Gault. 2022. AI Trained on 4Chan Becomes ‘Hate Speech Machine’.
Retrieved Feb 28, 2023 from https://www.vice.com/en/article/7k8zwx/ai-trained-
on-4chan-becomes-hate-speech-machine

[17] Bertram Gawronski. 2021. Partisan bias in the identification of fake news. TiCS

25, 9 (2021), 723–724.

[18] Fabian Haak and Philipp Schaer. 2021. Perception-Aware Bias Detection for

Query Suggestions. In BIAS. 130–142.

[19] Fabian Haak and Philipp Schaer. 2022. Auditing Search Query Suggestion Bias

Through Recursive Algorithm Interrogation. In WebSci. 219–227.

[20] Daniel Hienert, Philipp Schaer, Johann Schaible, and Philipp Mayr. 2011. A Novel
Combined Term Suggestion Service for Domain-Specific Digital Libraries.. In
TPDL (Lecture Notes in Computer Science, Vol. 6966), Stefan Gradmann, Francesca
Borri, Carlo Meghini, and Heiko Schuldt (Eds.). Springer, 192–203. http://dblp.
uni-trier.de/db/conf/ercimdl/tpdl2011.html#HienertSSM11

[21] Christoph Hube and Besnik Fetahu. 2019. Neural Based Statement Classification

for Biased Language. In WSDM. ACM.

[22] L. Introna and H. Nissenbaum. 2000. Defining the Web: the politics of search

engines. Computer 33, 1, 54–62.

[23] Juhi Kulshrestha, Motahhare Eslami, Johnnatan Messias, Muhammad Bilal Zafar,
Saptarshi Ghosh, Krishna P. Gummadi, and Karrie Karahalios. 2019. Search bias
quantification: investigating political bias in social media and web search. Inf.
Retr. J. 22, 1, 188–227.

[24] Ruibo Liu, Chenyan Jia, and Soroush Vosoughi. 2021. A Transformer-based
Framework for Neutralizing and Reversing the Political Polarity of News Articles.
Proc. ACM Hum.-Comput. Interact. 5, 1–26.

[25] Bhaskar Mitra, Milad Shokouhi, Filip Radlinski, and Katja Hofmann. 2014. On

user interactions with query auto-completion. In SIGIR. 1055–1058.

[26] Negar Mokhberian, André s Abeliuk, Patrick Cummings, and Kristina Lerman.

2020. Moral Framing and Ideological Bias of News. In SocInfo. 206–219.
[27] Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. 2021. On
the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong
Baselines. In ICLR. https://openreview.net/forum?id=nzpLWnVAyah

[28] Xi Niu and Diane Kelly. 2014. The use of query suggestions during information

search. IPM 50, 1, 218–234.

[29] Greg Pass, Abdur Chowdhury, and Cayley Torgeson. 2006. A Picture of Search.

In InfoScale. 1–es.

[30] Lily Ray. 2020. 2020 Google Search Survey: How Much Do Users Trust Their
Search Results? Retrieved Nov 30, 2022 from https://moz.com/blog/2020-google-
search-survey

[31] Shaina Raza, Deepak John Reji, and Chen Ding. 2022. Dbias: Detecting biases

and ensuring Fairness in news articles. Int J Data Sci Anal (2022).

[33] Reddit. 2022.

[32] Marta Recasens, Cristian Danescu-Niculescu-Mizil, and Dan Jurafsky. 2013. Lin-
guistic Models for Analyzing and Detecting Biased Language. In ACL. 1650–1659.
Retrieved Feb 28, 2023
from https://www.reddit.com/r/MachineLearning/comments/v42pej/p_this_is_
the_worst_ai_ever_gpt4chan_model/

This is the worst AI ever.

[34] Ronald E. Robertson, Shan Jiang, Kenneth Joseph, Lisa Friedland, David Lazer,
and Christo Wilson. 2018. Auditing Partisan Audience Bias within Google Search.
Proc. ACM Hum.-Comput. Interact. 2, CSCW, Article 148 (2018).

[35] Ronald E. Robertson, Shan Jiang, David Lazer, and Christo Wilson. 2019. Auditing
Autocomplete: Suggestion Networks and Recursive Algorithm Interrogation. In
WebSci. 235–244.

[36] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Dis-
tilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. CoRR
abs/1910.01108 (2019).

[37] Danny Sullivan. 2018. How Google autocomplete works in Search. Retrieved Nov
30, 2022 from https://blog.google/products/search/how-google-autocomplete-
works-search/

[38] Peng Wang, Xianghang mi, Xiaojing Liao, Xiaofeng Wang, Kan Yuan, Feng Qian,
and Raheem Beyah. 2018. Game of Missuggestions: Semantic Analysis of Search-
Autocomplete Manipulations. In NDSS.

","𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in Search Queries and Query Suggestions Fabian Haak fabian.haak @ th-koeln.de Technische Hochschule Köln Cologne , Germany Philipp Schaer philipp.schaer @ th-koeln.de Technische Hochschule Köln Cologne , Germany ABSTRACT This publication describes the motivation and generation of 𝑄𝑏𝑖𝑎𝑠 , a large dataset of Google and Bing search queries , a scraping tool and dataset for biased news articles , as well as language models for the investigation of bias in online search . Web search engines are a major factor and trusted source in information search , especially in the political domain . However , biased information can influence opinion formation and lead to biased opinions . To interact with search engines , users formulate search queries and interact with search query suggestions provided by the search engines . A lack of datasets on search queries inhibits research on the subject . We use 𝑄𝑏𝑖𝑎𝑠 to evaluate different approaches to fine-tuning transformer- based language models with the goal of producing models capable of biasing text with left and right political stance . Additionally to this work we provided datasets and language models for biasing texts that allow further research on bias in online information search . CCS CONCEPTS • Information systems → Web search engines ; Query suggestion ; Query reformulation ; • Computing methodologies → Natural language generation . KEYWORDS web search , dataset , bias , query suggestion , search queries , language models , transformers ACM Reference Format : Fabian Haak and Philipp Schaer . 2023 . 𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in Search Queries and Query Suggestions . In 15th ACM Web Science Conference 2023 ( WebSci ’ 23 ) , April 30-May 1 , 2023 , Evanston , TX , USA . ACM , New York , NY , USA , 6 pages . 3 2 0 2 v o N 9 2 ] R I . s c [ 1 v 0 8 7 7 1 . 1 1 3 2 : v i X r a 1 INTRODUCTION Search engines such as Google and Bing are seen as trustworthy sources of information on many topics , including political news and information [ 14 , 30 ] . Further , search engines have proven to have a major impact on the formation of political opinions [ 15 ] . To interact with search engines , users formulate search queries that are an expression of their information need . Based on these queries search engines usually provide a set of search query suggestions [ 28 ] . Queries and the choices users make when interacting with query suggestions are based on the information need they want to satisfy , which is often to support their personal opinions or beliefs founded on previously encountered information [ 6 ] . This interaction process and the results presented to the users are prone to be biased [ 7 , 22 , 25 ] and therefore the high level of trust can be seen as problematic . However , the true effect of biased search queries and different types of inherent biases on the actual list of search results has not sufficiently been investigated . One of the main reasons for the in- frequency of studies on bias in search queries might be a lack of publicly available datasets . datasets that include real-world user queries , query suggestions , and actual query reformulations are rare . One of the reasons is that collecting search queries from users is problematic due to privacy concerns . Although there are tech- niques like pseudonymization , query logs enable the identification of users [ 5 ] . Previously available datasets such as the AOL query log dataset [ 29 ] are no longer available , and their usage is morally debatable . Using unpersonalized search query suggestions as proxies might solve this issue . Query suggestions describe the list of predicted queries suggested to users during the input of search queries by the search engine , sometimes also called search predictions [ 37 ] or query auto completion [ 10 ] . While these search query suggestions can be generated locally from the result set [ 40 ] or be taken from global knowledge bases [ 20 ] , in web search suggestions are mostly based on frequently issued queries by users [ 37 ] . It can therefore be assumed that in many cases query suggestions are popular related search queries for the initial root query that represented the user ’ s interest in a topic or entity . The U.S. news domain is a popular domain for bias research due to its sociological relevance and the two-party left-right spectrum that facilitates bias analysis [ 34 ] . One of the benefits of our dataset is to enable in-depth investigations of the correlation between bias in search queries and search results , as search results for popu- lar web search engines can easily be collected using our provided dataset of search queries . Therefore , one goal of this work is to provide a large dataset of search query suggestions for the U.S. po- litical news domain . Additionally , we provide a transformer-based methodology for biasing text . Such a system can be a useful tool in WebSci ’ 23 , April 30-May 1 , 2023 , Evanston , TX , USA Haak and Schaer bias research , f.e . for generating biased derivatives of search queries . Thus , we evaluate a range of fine-tuning scenarios to find settings , that produce the most biased results . We provide the fine-tuned transformer-based language models that are capable of inducing left and right political stance bias in the form of lexical biases . This publication describes the motivation and generation of 𝑄𝑏𝑖𝑎𝑠 , a large dataset of Google and Bing search queries , a scraping tool and dataset for biased news articles , as well as language models for the investigation of bias in online search . The main contributions of 𝑄𝑏𝑖𝑎𝑠 and this paper are : • Two datasets:1 ( 1 ) The ( to the best of our knowledge ) largest labeled dataset of search query suggestions of a single do- main for Google and Bing . ( 2 ) A dataset of biased news articles of the U.S. political news domain . • A scraping tool that allows researchers to easily retrieve an up-to-date version of the biased news dataset.2 • A new approach for producing biased search queries , us- ing intentionally biased transformer-based language models capable of producing biased texts . • An evaluation of different fine-tuning settings to find the most capable setup for producing bias-inducing language models . 2 BACKGROUND AND RELATED WORK Definition of Bias is the News Domain . Due to the subjective na- ture of bias , few publications attempt to explicitly define bias in news [ 1 , 11 , 13 , 32 ] . ’ Biased news ’ generally describes non-neutral and opinionated news , but both are fuzzy attributes [ 32 ] . In the polit- ical domain , the term opinionated describes having a fixed political opinion and agenda , aspect of a cognitive ( author- or reader-sided ) bias often described as partisan bias [ 17 ] or ( political ) stance [ 24 ] . Partisanship manifests at different granularities . At publishing or reporting level , partisanship is expressed by the selection of certain topics called selection bias , and coverage bias , the selection of dif- ferent views on a topic [ 13 ] . At text level , statement bias describes “ members of the media interjecting their own opinions into the text ” [ 13 ] , manifesting in overall opinionated texts . This can take various forms , ranging from phrasing bias , the use of non-neutral language [ 21 ] to moral framing and ideological bias [ 26 ] . Spin bias describes bias introduced either by omitting necessary ( neutral ) information or adding unnecessary information [ 11 ] . At word and n-gram level , biases manifest as linguistic or lexical biases [ 11 ] . Most linguistic biases are highly domain- and context-specific . For example , framing bias describes subjective words , while epistemo- logical bias can be attributed to words targeting the credibility of a statement [ 32 ] . Since these biases can be identified more objectively , most approaches to detecting biases rely on the identification of linguistic biases [ 1 ] . Research on Bias in Online Search and Search Queries . Few pub- lications investigate bias in online search in aspects other than search results : Robertson et al . [ 34 ] investigate partisan bias and filter bubble effects in political searches by auditing SERPS for a set of queries . They did not find significant evidence , that unbiased search queries in real search sessions performed by real users lead to filter bubble effects . The unanswered question is , whether biased queries in general lead to biased search results . Few studies investigate bias in search queries . Due to the difficult accessibility of biased search queries , most studies focus on bias in search query suggestions : in most of those , the authors investigate topical group biases in search query suggestions in the political domain [ 8 , 18 , 19 ] . Overall , they observe minor topical gender biases for search queries consisting of names of politicians . Research on bias in online search has shown , that “ factors such as the topic of the query , the phrasing of query and the time at which a query is issued also impact the bias seen by the users ” [ 23 ] . Datasets of Biased News and Search Queries . Baly et al . [ 4 ] predict media bias using news articles collected from AllSides balanced news.3 AllSides news is a popular source for balanced news [ 4 , 11 , 26 ] , since AllSides has a high standard for assigning bias labels [ 1 ] . Similarly , Chen et al . [ 11 ] use AllSides-labeled news articles and adfontes labels to analyze bias at different granularities . Mokhbe- rian et al . [ 26 ] develop a framing bias detection and quantification approach , using a collection of news articles that they label accord- ing to news outlet bias labels provided by AllSides.4 datasets for search queries and query suggestions are sparse : most are either not available anymore [ 29 ] or focus on a narrow topic [ 8 ] . Robertson et al . [ 35 ] introduce recursive algorithm interrogation , a technique for recursively retrieving query suggestions of a root query and their consecutive suggestions . This technique was employed by Haak and Schaer [ 19 ] to investigate bias in the German political domain . However , to the best of our knowledge , there currently is no large-scale dataset on search queries . 3 ALLSIDES SCRAPER AND DATASETS We present two novel datasets , that promote the investigation of bias in online news search . Both datasets can be found on Zenodo as mentioned in section 1 . Further , we provide a web scraping tool for retrieving an up-to-date version of the AllSides dataset we provide . This section describes the content of the datasets as well as their creation process . Figure 1 shows how we assembled the datasets and use them in our approach for creating biasing language models . 3.1 AllSides Scraper As described in section 2 , the AllSides platform and the provides news is a frequently used source for high quality labeled biased news . We want to provide an easy means of retrieving the news and all corresponding information . Similar datasets have previously been produced , as mentioned in section 2 . However , compared to the currently most recent available version by Baly et al . [ 4 ] , our version includes more than 20 percent of additional articles . Furthermore , for many tasks , especially in the news domain , it is relevant to have the most recent documents available . We provide a Python-based scraper , that scrapes all available AllSides news articles and gathers available information as described in section 3.2 . By providing the scraper we facilitate access to a recent version of the dataset for other researchers . 1The datasets can be found at Zenodo ( https : //doi.org/10.5281/zenodo.7682914 ) 2https : //github.com/irgroup/Qbias 3https : 4https : 𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in Search Queries and Query Suggestions WebSci ’ 23 , April 30-May 1 , 2023 , Evanston , TX , USA 3.3 Search Queries in U.S. News Domain Dataset The second dataset we provide consists of 671,669 search query suggestions for root queries based on tags of the AllSides biased news dataset . We collected search query suggestions from Google and Bing for the 1,431 topic tags , that have been used for tagging AllSides news at least five times , approximately half of the total amount of topics . The topic tags include names , a wide range of political terms , agendas , and topics ( e.g. , `` communism '' , `` libertarian party '' , `` same-sex marriage '' ) , cultural and religious terms ( e.g. , `` Ra- madan '' , `` pope Francis '' ) , locations and other news-relevant terms . On average , the dataset contains 469 search queries for each topic . In total , 318,185 suggestions have been retrieved from Google and 353,484 from Bing . Using a python implementation loosely adopting the framework provided by Robertson et al . [ 35 ] , we scraped query suggestions for the topics as root queries . Using Google Colab to run our scraper , we retrieved ten search query suggestions provided by the Google and Bing search autocomplete systems for the input of each of these root queries . Furthermore , we extended the root queries by the let- ters a to z ( e.g. , `` democrats '' ( root term ) → `` democrats a '' ( query input ) → `` democrats and recession '' ( query suggestion ) ) . The goal of this procedure is to simulate a user ’ s input during information search . Retrieving the suggestions for the root query and the ex- tended queries generates a total of up to 270 query suggestions per topic and search engine . The dataset we provide contains columns for root term , query input , and query suggestion for each suggested query . The location from which the search is performed is the loca- tion of the Google servers running Colab , in our case Iowa in the United States of America , which is added to the dataset . Since we perform the scrape on a blank browser for each of the searches , personalization effects other than the location did not effect the suggested search queries . Our scraping setup thus eliminates per- sonalization effects that could , in theory , cause echo chamber ef- fects [ 34 ] . Search engine providers describe that query suggestions are based on other users ’ searches [ 37 ] . Successful attempts to in- fluence query suggestions have confirmed this claim [ 38 ] . Thus , we deduce that query suggestions reflect real , frequently used search queries and can be used as proxies for search queries . Despite the lack of information on the frequency of the collected search queries , our dataset contains the ranks of suggestions , as well as the sugges- tions to the root term for approximating the most frequent search queries . Assuming that the topical tags in the AllSides news reflect popular topics , the produced dataset consists of real search queries for news-relevant topics . 4 DEVELOPING BIASING LANGUAGE MODELS Another contribution 𝑄𝑏𝑖𝑎𝑠 is to produce a pair of transformer- based language models that are capable of inducing left or right partisanship as linguistic biases . This can be done by leveraging the masking function on the words of the target document . 4.1 Domain-Adopting DistilBERT This section describes the methodological approach for develop- ing transformer-based language models capable of biasing texts . Usually , systems are developed with the goal of producing and reproducing as little bias as possible or to debias biased texts [ 31 ] . Figure 1 : Pipeline used for generating bias-inducing language models . Black boxes represent datasets , language models ( only available upon request ) , and tools provided via Zenodo , and GitHub . 3.2 AllSides Biased News Dataset The dataset contains 21,747 news articles collected from AllSides balanced news headline roundups [ 2 ] in November 2022 . The All- Sides balanced news feature three expert-selected U.S. news articles from sources of different political views ( left , right , center ) , often featuring spin bias , slant other forms of non-neutral reporting on political news [ 1 ] . All articles are tagged with a bias label by four expert annotators based on the expressed political partisanship , left , right , or neutral [ 1 ] . The AllSides balanced news aims to offer multiple political perspectives on important news stories , educate users on biases , and provide multiple viewpoints [ 1 ] . Collected data includes the headline , news text , publishing date , topic tags ( e.g. , `` Republican party '' , `` coronavirus '' , `` federal jobs '' ) , links to the article , and the publishing news outlet . We also include AllSides ’ neutral description of the topic of the articles . Overall , the produced dataset contains 10,273 articled tagged as left , 7,222 articles tagged as right , and 4,252 articles tagged as center . The collected articles have been published between June 2012 , and the date of the data collection at the end of November 2022 . We use the AllSides dataset for develop- ing our biasing language model , as described in section 4 . To allow for easier access to the most recent and complete version of the dataset for future research , we provide the scraping tool described in section 3.1 . WebSci ’ 23 , April 30-May 1 , 2023 , Evanston , TX , USA Haak and Schaer We try to achieve the opposite , biasing texts by reproducing biases inherent in biased fine-tuning datasets . This would , for example , allow for simulating a user that searches for biased terms poten- tially caused by exposure to biased news . With the goal of cap- turing biased language in pretrained language models and using the HuggingFace Transformers module [ 39 ] , we fine-tune the base DistilBERT model , which was trained in Wikipedia articles and a large book dataset.5 DistilBERT– [ 36 ] was chosen as a base model since it has proven to perform well with comparably small datasets , performs better than BERT despite its smaller size [ 36 ] , and for its aptness for adopting domain-specificity [ 3 , 9 ] . We are aware that there are more effective models , but to show the suitability of our approach , we chose DistilBERT due to its efficiency and sufficient effectiveness . We fine-tuned a range of pairs of models , each with one left model fine-tuned on the part of the AllSides corpus tagged as left- biased news and one right model fine-tuned with documents of the dataset labeled as right-biased news . We produced 24 models , a left and a right model for 12 combinations of parameters . Our goal is to find the ideal approach for capturing and reproducing as much bias as possible while producing meaningful results . The central aspect we varied in fine-tuning the models is the data used . The models are developed using three different data configurations : ( a ) the headline and news text of each of the left and right news articles of the AllSides dataset , ( b ) only the news text , and ( c ) only the headline . Another factor we evaluate is the use of padding or concatenation to generate a consistent chunk size for fine-tuning . The chosen chunk size is the max length of fine-tuning documents for the padding approach and 128 for the concatenation approach . Lastly , we compare the effects of intentional overfitting by rais- ing the number of epochs to 20 , compared to a more reasonable 6 epochs , which proved to produce a good combination of training loss and validation loss for all dataset configurations . In theory , overfitting could be another possibility for reproducing biased for- mulations from the texts used in fine-tuning . This is why we tried to intentionally raise the number of epochs as a parameter [ 27 ] . 4.2 Evaluating the Biasing Language Models All 24 models are available upon request . We decided not to make the models publicly available due to concerns about potential harms that could be caused by misuse of the systems that we further elab- orated in section 5 . For evaluating the models ’ effectiveness , we choose a combination of manual assessment and quantitative mea- sures . The task of the models is to generate biased output , however , biases are diverse and not easily measurable . Since we have no way of identifying biased tokens in the dataset , perplexity is not a useful measure for evaluating the model . Further , we can not effectively measure bias as a criterion for the effectiveness of the ability of the systems to produce biased versions of queries . However , we can assume that when masking words in search queries for topics of the political news domain and letting the pairs of language models predict the words , the output of the left and right models should differ from each other . We choose to evaluate their performance on search queries since we want to use the models in future research on bias in search results for biased and unbiased search queries . 5https : Further , they represent texts of a different type but the same domain as the texts used in training the models . For each pair of models , we measure the difference between the two models ’ 10 most probable non-punctuation predictions for 100 randomly selected queries from the dataset by measuring the Rank Biased Overlap ( RBO ) with 𝑝 = 0.9 . We mask a random token of each query that is neither part of the original news topic nor a stop- word since we want to let the models generate meaning-carrying tokens while keeping the query ’ s topic intact . Although nonsensical random predictions or neologisms would lead to great RBO scores , we want to generate queries , that humans could have formulated . To assure that the models generate output that makes sense , we let the models generate next word predictions for ten bias-provoking sentences ( e.g. , `` Hilary Clinton is a '' , `` Covid Vaccines should be '' ) . Two domain experts ( a professor and a postdoctoral researcher ) then label nonsensical and biased predictions . We measure the inter- annotator agreement on the individual statements ( independent of the models , that produce the statements ) with Cohen ’ s Kappa [ 12 ] . For nonsensical predictions , we obtained a value of 0.18 ( slight agreement ) , for biased predictions the value is 0.84 ( almost perfect ) . The bias labels are assigned if the suggested word induces political stance bias . Table 1 shows the results of the model evaluation process . The lower ℎ , 𝑡 , or ℎ𝑡 describes if headline , text , or both have been used . 𝑅𝐵𝑂 is the rank bias overlap between the left and the right model of each configuration . 𝑃𝑛𝑜𝑛𝑠 describes the average percentage of nonsensical predictions of both models and 𝑃𝑏𝑖𝑎𝑠 the percentage of biased predictions . Overall , the models fine-tuned with only headlines show the best ( lowest ) RBO scores , as well as the lowest 𝑃𝑛𝑜𝑛𝑠 scores . Headlines and text and only text perform more or less equally in terms of their RBO . Concatenating texts produces on average better results than padding , although for fine-tuning with only headlines , the effect is minimal . Intentional overfitting by raising the amount of training epochs seems to worsen the RBO scores . The percentage of nonsensical predictions does not differ much between different fine-tuning setups , fine-tuning on texts only seems to be the only scenario that increases the percentage of nonsensical predictions . Many of the models generate a high percentage of biased suggestions , with the headline models having the highest percentage of biased predictions . 5 DISCUSSION Our language models show , that by using small , high-quality datasets , it is possible to fine-tune transformer-based language models to bias texts . In our fine-tuning setup , models fine-tuned with headlines produce the overall best results . As the main reason for that , we assume that the high amount of quotes in news texts , which often are statements the authors of the articles disagree with , might have induced noise in fine-tuning the models . Further , the condensed na- ture of headlines , which aims to catch attention , might also reflect in resulting language models . The output of left and right models fine-tuned with headlines produce texts containing linguistic biases . For example , for `` Donald Trump is a '' , our model produces `` hero '' as right-biased and `` fraud '' as left-biased next word predictions . The overall low percentage of nonsensical predictions supports RBO as a suited evaluation metric . Despite the overall good results , 𝑄𝑏𝑖𝑎𝑠 - A Dataset on Media Bias in Search Queries and Query Suggestions WebSci ’ 23 , April 30-May 1 , 2023 , Evanston , TX , USA Table 1 : Evaluation results of the fine-tuned language models . The highlighted values are the best results for each metric . Model configuration 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡𝑡 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ𝑡 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇𝑐𝑜𝑛𝑐𝑎𝑡ℎ𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔𝑡 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ𝑡 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔 𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇 𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ𝑡 + 𝑜𝑣𝑒𝑟 𝑓 𝑖𝑡𝑡𝑖𝑛𝑔 𝑅𝐵𝑂 𝑃𝑛𝑜𝑛𝑠 0.54 0.04 0.10 0.72 0.08 0.68 0.03 0.56 0.11 0.73 0.07 0.68 0.54 0.04 0.05 0.74 0.07 0.77 0.03 0.58 0.06 0.77 0.07 0.77 𝑃𝑏𝑖𝑎𝑠 0.33 0.14 0.15 0.5 0.14 0.23 0.38 0.28 0.1 0.43 0.24 0.12 future research should investigate the findings with other language models and compare the performance to our results . Our scraping tool and the datasets not only allow us to effectively generate the language models but enables the investigation of other research topics in the political news information search domain such as bias classification or sentiment analysis . Remark on Moral Issues . We are aware , that building bias-inducing systems and providing datasets that enable the reproduction of developing similar systems is problematic . We also did consider that our work can inspire and help to develop transformer-based language models capable of producing biased , toxic , or otherwise harmful texts . The severity of openly published biased systems has been shown by other such models , e.g . the gpt-4chan model.6 The publication of the model incited a debate , that shows how problematic biased language made available to a wide public au- dience are , despite explicitly stating the intended use for research applications [ 16 , 33 ] . Our models are biased primarily in terms of linguistic biases that reflect political stances and views on political topics . Since this can in part include objectively wrong and opinionated statements , hate speech , racism , and other forms of despicable language and toxicity , our models can and will reproduce text , that conveys these phenomena . To minimize the harmful effects of our publication that could be caused by the misuse of the models , we decided to not make our biased models publicly accessible . Despite that , we provide access to the models upon request for research , if a reasonable application use case is provided . However , we do so in the interest of investigating biases and correlations of bias in online information search , with the goal of increasing transparency and fairness . Raising awareness for how easily , intentionally or not , biases can be reproduced and induced with AI systems is part of this endeavor . Although we have shown how the data we provide can be used to create models that produce biased language we chose to publish the datasets and scraper . This is mainly because of three reasons : ( a ) as stated in section 3 similar datasets are available publicly , ( b ) we believe that the benefits of raising awareness outweigh making 6https : already public biased data more accessible , and ( c ) datasets on biased language are required for developing systems to detect and inhibit bias . With our work , we hope to highlight the need to assure , that data used for developing models is unbiased and raise awareness for how easily transformer-based language models can be fine-tuned to produce biased language . 6 OUTLOOK This publication represents the first and foundational milestone of a larger-scale investigation of bias in online information search . As a major next contribution , we plan to use the presented datasets and models to investigate the effects of biased and unbiased search queries on the search results of different search engines for popular topics of the U.S. political news domain . To accomplish this , we need to overcome the issue of subjectivity and lack of effective method- ological approaches to bias identification other than by employing human annotation . We plan to introduce an ensemble of methods , including bias-agnostic analysis of linguistic differences , lexical features , and transformer-based approaches for bias classification . Further , we plan to conduct a study using a simulation approach . By simulation different user behaviors in terms of information need , formulating queries , and interacting with query suggestions in an interactive information search simulation , we plan to gain insights into echo chamber effects and bias formation in information search . Additionally , we hope to identify which user properties and be- haviors increase and which lower the risk of encountering bias in information search . 7 CONCLUSION This work presents 𝑄𝑏𝑖𝑎𝑠 , a first milestone of our ongoing research on bias in online search . We present two datasets : a biased news dataset and a large dataset of biased and unbiased search queries for topics of the U.S. political news domain . Further , we provide a scraping tool , that allows for collecting bias-labeled news texts from AllSides . Lastly , we evaluate approaches to fine-tuning DistilBERT transformer-based language models for biasing texts and publish our models capable of inducing left and right political stance bias in the form of lexical biases . REFERENCES [ 1 ] AllSides . 2021 . How AllSides Creates Balanced News : A Step-by-Step Guide . Retrieved Nov 30 , 2022 from https : create-balanced-news [ 2 ] AllSides . 2022 . Balanced News Headlines Roundup . Retrieved Nov 30 , 2022 from https : [ 3 ] Jing Bai , Rui Cao , Wen Ma , and Hiroyuki Shinnou . 2020 . Construction of Domain- Specific DistilBERT Model by Using Fine-Tuning . In TAAI . 237–241 . [ 4 ] Ramy Baly , Giovanni Da San Martino , James Glass , and Preslav Nakov . 2020 . We Can Detect Your Bias : Predicting the Political Ideology of News Articles . In EMNLP . 4982–4991 . [ 5 ] Michael Barbaro and Tom Zeller . 2006 . A Face is exposed for AOL searcher no . 4417749 . New York Times ( 01 2006 ) . [ 6 ] Nicholas Belkin , Colleen Cool , Diane Kelly , S.-J Lin , S.Y Park , Jose Perez-carballo , and Cynthia Sikora . 2001 . Iterative exploration , design and evaluation of support for query reformulation in interactive information retrieval . IPM 37 ( 05 2001 ) , 403–434 . [ 7 ] Tolga Bolukbasi , Kai-Wei Chang , James Zou , Venkatesh Saligrama , and Adam Kalai . 2016 . Man is to Computer Programmer as Woman is to Homemaker ? Debiasing Word Embeddings . In NIPS . 4356–4364 . [ 8 ] Malte Bonart , Anastasiia Samokhina , Gernot Heisenberg , and Philipp Schaer . 2019 . An investigation of biases in web search engine query suggestions . OIR 44 , 2 ( 2019 ) , 365–381 . WebSci ’ 23 , April 30-May 1 , 2023 , Evanston , TX , USA Haak and Schaer [ 39 ] Thomas Wolf , Lysandre Debut , Victor Sanh , Julien Chaumond , Clement Delangue , Anthony Moi , Pierric Cistac , Tim Rault , Remi Louf , Morgan Funtowicz , Joe Davison , Sam Shleifer , Patrick von Platen , Clara Ma , Yacine Jernite , Julien Plu , Canwen Xu , Teven Le Scao , Sylvain Gugger , Mariama Drame , Quentin Lhoest , and Alexander Rush . 2020 . Transformers : State-of-the-Art Natural Language Processing . In EMNLP . 38–45 . [ 40 ] Jinxi Xu and W. Bruce Croft . 2000 . Improving the Effectiveness of Information Retrieval with Local Context Analysis . ACM Trans . Inf . Syst . 18 , 1 ( Jan. 2000 ) , 79–112 . https : //doi.org/10.1145/333135.333138 [ 9 ] Berfu Büyüköz , Ali Hürriyetoˇglu , and Arzucan Özgür . 2020 . Analyzing ELMo and DistilBERT on Socio-political News Classification . In AESPEN . [ 10 ] Fei Cai and Maarten de Rijke . 2016 . A Survey of Query Auto Completion in Information Retrieval . FNTIR 10 , 4 ( 2016 ) , 273–363 . [ 11 ] Wei-Fan Chen , Khalid Al Khatib , Henning Wachsmuth , and Benno Stein . 2020 . Analyzing Political Bias and Unfairness in News Articles at Different Levels of Granularity . In NLPCSS . 149–154 . https : [ 12 ] Jacob Cohen . 1960 . A Coefficient of Agreement for Nominal Scales . Educ Psychol Meas 20 , 1 ( 1960 ) , 37–46 . [ 13 ] Dave D ’ Alessio and Mike Allen . 2000 . Media Bias in Presidential Elections : A Meta-Analysis . J. Commun . 50 , 4 ( 2000 ) , 133–156 . [ 14 ] Edelman . 2022 . 2022 Edelman Trust Barometer . Retrieved Nov 30 , 2022 from https : [ 15 ] Robert Epstein and Ronald E. Robertson . 2015 . The search engine manipulation effect ( SEME ) and its possible impact on the outcomes of elections . PNAS 112 , 33 , E4512–E4521 . Publisher : National Academy of Sciences Section : PNAS Plus . [ 16 ] Matthew Gault . 2022 . AI Trained on 4Chan Becomes ‘ Hate Speech Machine ’ . Retrieved Feb 28 , 2023 from https : [ 17 ] Bertram Gawronski . 2021 . Partisan bias in the identification of fake news . TiCS 25 , 9 ( 2021 ) , 723–724 . [ 18 ] Fabian Haak and Philipp Schaer . 2021 . Perception-Aware Bias Detection for Query Suggestions . In BIAS . 130–142 . [ 19 ] Fabian Haak and Philipp Schaer . 2022 . Auditing Search Query Suggestion Bias Through Recursive Algorithm Interrogation . In WebSci . 219–227 . [ 20 ] Daniel Hienert , Philipp Schaer , Johann Schaible , and Philipp Mayr . 2011 . A Novel Combined Term Suggestion Service for Domain-Specific Digital Libraries .. In TPDL ( Lecture Notes in Computer Science , Vol . 6966 ) , Stefan Gradmann , Francesca Borri , Carlo Meghini , and Heiko Schuldt ( Eds. ) . Springer , 192–203 . http : //dblp . # HienertSSM11 [ 21 ] Christoph Hube and Besnik Fetahu . 2019 . Neural Based Statement Classification for Biased Language . In WSDM . ACM . [ 22 ] L. Introna and H. Nissenbaum . 2000 . Defining the Web : the politics of search engines . Computer 33 , 1 , 54–62 . [ 23 ] Juhi Kulshrestha , Motahhare Eslami , Johnnatan Messias , Muhammad Bilal Zafar , Saptarshi Ghosh , Krishna P. Gummadi , and Karrie Karahalios . 2019 . Search bias quantification : investigating political bias in social media and web search . Inf . Retr . J . 22 , 1 , 188–227 . [ 24 ] Ruibo Liu , Chenyan Jia , and Soroush Vosoughi . 2021 . A Transformer-based Framework for Neutralizing and Reversing the Political Polarity of News Articles . Proc . ACM Hum.-Comput . Interact . 5 , 1–26 . [ 25 ] Bhaskar Mitra , Milad Shokouhi , Filip Radlinski , and Katja Hofmann . 2014 . On user interactions with query auto-completion . In SIGIR . 1055–1058 . [ 26 ] Negar Mokhberian , André s Abeliuk , Patrick Cummings , and Kristina Lerman . 2020 . Moral Framing and Ideological Bias of News . In SocInfo . 206–219 . [ 27 ] Marius Mosbach , Maksym Andriushchenko , and Dietrich Klakow . 2021 . On the Stability of Fine-tuning BERT : Misconceptions , Explanations , and Strong Baselines . In ICLR . https : //openreview.net/forum ? id=nzpLWnVAyah [ 28 ] Xi Niu and Diane Kelly . 2014 . The use of query suggestions during information search . IPM 50 , 1 , 218–234 . [ 29 ] Greg Pass , Abdur Chowdhury , and Cayley Torgeson . 2006 . A Picture of Search . In InfoScale . 1–es . [ 30 ] Lily Ray . 2020 . 2020 Google Search Survey : How Much Do Users Trust Their Search Results ? Retrieved Nov 30 , 2022 from https : //moz.com/blog/2020-google- search-survey [ 31 ] Shaina Raza , Deepak John Reji , and Chen Ding . 2022 . Dbias : Detecting biases and ensuring Fairness in news articles . Int J Data Sci Anal ( 2022 ) . [ 33 ] Reddit . 2022 . [ 32 ] Marta Recasens , Cristian Danescu-Niculescu-Mizil , and Dan Jurafsky . 2013 . Lin- guistic Models for Analyzing and Detecting Biased Language . In ACL . 1650–1659 . Retrieved Feb 28 , 2023 from https : the_worst_ai_ever_gpt4chan_model/ This is the worst AI ever . [ 34 ] Ronald E. Robertson , Shan Jiang , Kenneth Joseph , Lisa Friedland , David Lazer , and Christo Wilson . 2018 . Auditing Partisan Audience Bias within Google Search . Proc . ACM Hum.-Comput . Interact . 2 , CSCW , Article 148 ( 2018 ) . [ 35 ] Ronald E. Robertson , Shan Jiang , David Lazer , and Christo Wilson . 2019 . Auditing Autocomplete : Suggestion Networks and Recursive Algorithm Interrogation . In WebSci . 235–244 . [ 36 ] Victor Sanh , Lysandre Debut , Julien Chaumond , and Thomas Wolf . 2019 . Dis- tilBERT , a distilled version of BERT : smaller , faster , cheaper and lighter . CoRR abs/1910.01108 ( 2019 ) . [ 37 ] Danny Sullivan . 2018 . How Google autocomplete works in Search . Retrieved Nov 30 , 2022 from https : works-search/ [ 38 ] Peng Wang , Xianghang mi , Xiaojing Liao , Xiaofeng Wang , Kan Yuan , Feng Qian , and Raheem Beyah . 2018 . Game of Missuggestions : Semantic Analysis of Search- Autocomplete Manipulations . In NDSS .","['dataset', 'medium', 'bias', 'search', 'query', 'query', 'suggestion', 'fabian', 'haak', 'fabianhaak', 'thkoelnde', 'technische', 'hochschule', 'köln', 'cologne', 'thkoelnde', 'technische', 'hochschule', 'köln', 'cologne', 'publication', 'describe', 'motivation', 'generation', 'large', 'dataset', 'bing', 'search', 'query', 'scrape', 'tool', 'dataset', 'biased', 'news', 'article', 'well', 'language', 'model', 'investigation', 'bias', 'online', 'search', 'web', 'search', 'engine', 'major', 'factor', 'trust', 'source', 'information', 'search', 'especially', 'political', 'domain', 'however', 'bias', 'information', 'influence', 'opinion', 'formation', 'lead', 'biased', 'opinion', 'interact', 'search', 'engine', 'user', 'formulate', 'search', 'query', 'interact', 'search', 'query', 'suggestion', 'provide', 'search', 'engine', 'lack', 'dataset', 'search', 'query', 'inhibit', 'research', 'subject', 'use', 'evaluate', 'different', 'approach', 'finetune', 'transformer', 'base', 'language', 'model', 'goal', 'produce', 'model', 'capable', 'bias', 'text', 'left', 'right', 'political', 'stance', 'additionally', 'work', 'provide', 'dataset', 'language', 'model', 'bias', 'text', 'allow', 'research', 'bias', 'online', 'information', 'search', 'ccs', 'concept', 'information', 'system', 'web', 'search', 'engine', 'query', 'suggestion', 'query', 'reformulation', 'computing', 'methodology', 'natural', 'language', 'generation', 'keyword', 'web', 'search', 'dataset', 'bias', 'query', 'suggestion', 'search', 'query', 'language', 'model', 'transformer', 'acm', 'reference', 'format', 'fabian', 'haak', 'schaer', 'dataset', 'medium', 'bias', 'search', 'query', 'query', 'suggestion', '15th', 'acm', 'web', 'science', 'conference', 'page', 'n', 'r', 'c', 'v', 'r', 'introduction', 'search', 'engine', 'bing', 'see', 'trustworthy', 'source', 'information', 'many', 'topic', 'include', 'political', 'news', 'information', 'search', 'engine', 'prove', 'major', 'impact', 'formation', 'political', 'opinion', 'interact', 'search', 'engine', 'user', 'formulate', 'search', 'query', 'expression', 'information', 'need', 'base', 'query', 'search', 'engine', 'usually', 'provide', 'set', 'search', 'query', 'suggestion', 'query', 'choice', 'user', 'make', 'interact', 'query', 'suggestion', 'base', 'information', 'need', 'want', 'satisfy', 'often', 'support', 'personal', 'opinion', 'belief', 'found', 'previously', 'encounter', 'information', 'interaction', 'process', 'result', 'present', 'user', 'prone', 'bias', 'therefore', 'high', 'level', 'trust', 'see', 'problematic', 'however', 'true', 'effect', 'biased', 'search', 'query', 'different', 'type', 'inherent', 'bias', 'actual', 'list', 'search', 'result', 'sufficiently', 'investigate', 'main', 'reason', 'frequency', 'study', 'bias', 'search', 'query', 'lack', 'publicly', 'available', 'dataset', 'dataset', 'include', 'query', 'query', 'suggestion', 'actual', 'query', 'reformulation', 'rare', 'reason', 'collect', 'search', 'query', 'user', 'problematic', 'privacy', 'concern', 'tech', 'nique', 'pseudonymization', 'query', 'log', 'enable', 'identification', 'user', 'previously', 'available', 'dataset', 'long', 'available', 'usage', 'morally', 'debatable', 'use', 'unpersonalized', 'search', 'query', 'suggestion', 'proxy', 'solve', 'issue', 'query', 'suggestion', 'describe', 'list', 'predict', 'query', 'suggest', 'user', 'input', 'search', 'query', 'search', 'engine', 'sometimes', 'also', 'call', 'search', 'prediction', 'query', 'auto', 'completion', 'search', 'query', 'suggestion', 'generate', 'locally', 'result', 'set', 'take', 'global', 'knowledge', 'basis', 'web', 'search', 'suggestion', 'mostly', 'base', 'frequently', 'issue', 'query', 'user', 'therefore', 'assume', 'many', 'case', 'query', 'suggestion', 'popular', 'related', 'search', 'query', 'initial', 'root', 'query', 'represent', 'user', 'interest', 'topic', 'entity', 'news', 'domain', 'popular', 'domain', 'bias', 'research', 'sociological', 'relevance', 'twoparty', 'leftright', 'spectrum', 'facilitate', 'bias', 'analysis', 'benefit', 'dataset', 'enable', 'indepth', 'investigation', 'correlation', 'bias', 'search', 'query', 'search', 'result', 'search', 'result', 'popu', 'lar', 'web', 'search', 'engine', 'easily', 'collect', 'use', 'provide', 'dataset', 'search', 'query', 'therefore', 'goal', 'work', 'provide', 'large', 'dataset', 'search', 'query', 'suggestion', 'litical', 'news', 'domain', 'additionally', 'provide', 'transformerbased', 'methodology', 'bias', 'text', 'system', 'useful', 'tool', '30may', 'generate', 'biased', 'derivative', 'search', 'query', 'thus', 'evaluate', 'range', 'finetune', 'scenario', 'find', 'setting', 'produce', 'biased', 'result', 'provide', 'finetune', 'transformerbase', 'language', 'model', 'capable', 'induce', 'left', 'right', 'political', 'stance', 'bias', 'form', 'lexical', 'bias', 'publication', 'describe', 'motivation', 'generation', 'large', 'dataset', 'bing', 'search', 'query', 'scrape', 'tool', 'dataset', 'biased', 'news', 'article', 'well', 'language', 'model', 'investigation', 'bias', 'online', 'search', 'main', 'contribution', 'paper', 'datasets1', 'good', 'knowledge', 'large', 'label', 'dataset', 'search', 'query', 'suggestion', 'single', 'main', 'google', 'bing', 'dataset', 'biased', 'news', 'article', 'political', 'news', 'domain', 'scrape', 'tool', 'allow', 'researcher', 'easily', 'retrieve', 'uptodate', 'version', 'biased', 'news', 'dataset2', 'new', 'approach', 'produce', 'biased', 'search', 'query', 'intentionally', 'bias', 'transformerbase', 'language', 'model', 'capable', 'produce', 'biased', 'text', 'evaluation', 'different', 'finetuning', 'setting', 'find', 'capable', 'setup', 'produce', 'biasinduce', 'language', 'model', 'background', 'related', 'work', 'definition', 'bias', 'news', 'domain', 'subjective', 'ture', 'bias', 'publication', 'attempt', 'explicitly', 'define', 'bias', 'news', 'bias', 'news', 'generally', 'describe', 'nonneutral', 'opinionated', 'news', 'fuzzy', 'attribute', 'polit', 'ical', 'domain', 'term', 'opinionate', 'describe', 'fix', 'political', 'opinion', 'agenda', 'aspect', 'cognitive', 'author', 'readerside', 'bias', 'often', 'describe', 'partisan', 'bias', 'political', 'stance', 'partisanship', 'manifest', 'different', 'granularity', 'publishing', 'reporting', 'level', 'partisanship', 'express', 'selection', 'certain', 'topic', 'call', 'selection', 'bias', 'coverage', 'bias', 'selection', 'dif', 'ferent', 'view', 'topic', 'text', 'level', 'statement', 'bias', 'describe', 'member', 'medium', 'interject', 'opinion', 'text', 'manifest', 'overall', 'opinionate', 'text', 'take', 'various', 'form', 'range', 'phrase', 'bias', 'use', 'nonneutral', 'language', 'moral', 'framing', 'ideological', 'bias', 'spin', 'bias', 'describe', 'bias', 'introduce', 'omit', 'necessary', 'neutral', 'information', 'add', 'unnecessary', 'information', 'word', 'ngram', 'level', 'bias', 'manifest', 'linguistic', 'lexical', 'bias', 'linguistic', 'bias', 'highly', 'domain', 'contextspecific', 'example', 'frame', 'bias', 'describe', 'subjective', 'word', 'epistemo', 'logical', 'bias', 'attribute', 'word', 'target', 'credibility', 'statement', 'bias', 'identify', 'objectively', 'approach', 'detect', 'bias', 'rely', 'identification', 'linguistic', 'bias', 'research', 'bias', 'online', 'search', 'search', 'query', 'pub', 'lication', 'investigate', 'bias', 'online', 'search', 'aspect', 'search', 'result', 'investigate', 'partisan', 'bias', 'filter', 'bubble', 'effect', 'political', 'search', 'audit', 'serps', 'set', 'query', 'find', 'significant', 'evidence', 'unbiased', 'search', 'query', 'real', 'search', 'session', 'perform', 'real', 'user', 'lead', 'filter', 'bubble', 'effect', 'unanswered', 'question', 'biased', 'query', 'general', 'lead', 'biased', 'search', 'result', 'study', 'investigate', 'bias', 'search', 'query', 'difficult', 'accessibility', 'biased', 'search', 'query', 'study', 'focus', 'bias', 'search', 'query', 'suggestion', 'author', 'investigate', 'topical', 'group', 'bias', 'search', 'query', 'suggestion', 'political', 'domain', 'overall', 'observe', 'minor', 'topical', 'gender', 'bias', 'search', 'query', 'consist', 'name', 'politician', 'research', 'bias', 'online', 'search', 'show', 'factor', 'topic', 'query', 'phrasing', 'query', 'time', 'query', 'issue', 'also', 'impact', 'bias', 'see', 'user', 'dataset', 'biased', 'news', 'search', 'query', 'predict', 'medium', 'bias', 'use', 'news', 'article', 'collect', 'allside', 'balanced', 'news3', 'allside', 'news', 'popular', 'source', 'balanced', 'news', 'allside', 'high', 'standard', 'assign', 'bias', 'label', 'similarly', 'use', 'allsideslabele', 'news', 'article', 'adfonte', 'label', 'analyze', 'bias', 'different', 'granularity', 'mokhbe', 'rian', 'develop', 'frame', 'bias', 'detection', 'quantification', 'approach', 'use', 'collection', 'news', 'article', 'label', 'accord', 'label', 'provide', 'allsides4', 'dataset', 'search', 'query', 'query', 'suggestion', 'sparse', 'available', 'anymore', 'focus', 'narrow', 'topic', 'robertson', 'introduce', 'recursive', 'technique', 'recursively', 'retrieve', 'query', 'suggestion', 'root', 'query', 'consecutive', 'suggestion', 'technique', 'employ', 'haak', 'schaer', 'investigate', 'bias', 'german', 'political', 'domain', 'however', 'good', 'knowledge', 'currently', 'largescale', 'dataset', 'search', 'query', 'allside', 'scraper', 'dataset', 'present', 'novel', 'dataset', 'promote', 'investigation', 'bias', 'online', 'news', 'search', 'dataset', 'find', 'zenodo', 'mention', 'section', 'far', 'provide', 'web', 'scraping', 'tool', 'retrieve', 'uptodate', 'version', 'allside', 'dataset', 'provide', 'section', 'describe', 'content', 'dataset', 'well', 'creation', 'process', 'figure', 'show', 'assemble', 'dataset', 'use', 'approach', 'create', 'bias', 'language', 'model', 'allside', 'scraper', 'describe', 'section', 'allside', 'platform', 'provide', 'news', 'frequently', 'use', 'source', 'high', 'quality', 'label', 'biased', 'news', 'want', 'provide', 'easy', 'mean', 'retrieve', 'news', 'corresponding', 'information', 'similar', 'dataset', 'previously', 'produce', 'mention', 'section', 'however', 'compare', 'currently', 'recent', 'available', 'version', 'version', 'include', 'percent', 'additional', 'article', 'furthermore', 'many', 'task', 'especially', 'news', 'domain', 'relevant', 'recent', 'document', 'available', 'provide', 'pythonbased', 'scraper', 'scrape', 'available', 'allside', 'news', 'article', 'gather', 'available', 'information', 'describe', 'section', 'provide', 'scraper', 'facilitate', 'access', 'recent', 'version', 'dataset', 'researcher', 'dataset', 'find', 'zenodo', 'https', 'doiorg105281zenodo7682914', 'githubcomirgroupqbia', 'dataset', 'medium', 'bias', 'search', 'query', 'query', 'suggestion', '30may', 'search', 'query', 'news', 'domain', 'dataset', 'second', 'dataset', 'provide', 'consist', 'search', 'query', 'suggestion', 'root', 'query', 'base', 'tag', 'allside', 'bias', 'news', 'dataset', 'collect', 'search', 'query', 'suggestion', 'bing', 'topic', 'tag', 'use', 'tagging', 'allside', 'news', 'least', 'time', 'approximately', 'half', 'total', 'amount', 'topic', 'topic', 'tag', 'include', 'name', 'wide', 'range', 'political', 'term', 'agenda', 'topic', 'communism', 'party', 'samesex', 'marriage', 'cultural', 'religious', 'term', 'francis', 'location', 'newsrelevant', 'term', 'average', 'dataset', 'contain', 'search', 'query', 'topic', 'total', 'suggestion', 'retrieve', 'bing', 'use', 'python', 'implementation', 'loosely', 'adopt', 'framework', 'provide', 'scrape', 'query', 'suggestion', 'topic', 'root', 'query', 'use', 'run', 'scraper', 'retrieve', 'search', 'query', 'suggestion', 'provide', 'bing', 'search', 'autocomplete', 'system', 'input', 'root', 'query', 'furthermore', 'extend', 'root', 'query', 'let', 'ter', 'root', 'term', 'query', 'input', 'democrat', 'recession', 'query', 'suggestion', 'goal', 'procedure', 'simulate', 'user', 'input', 'information', 'search', 'retrieve', 'suggestion', 'root', 'query', 'ex', 'tend', 'query', 'generate', 'total', 'query', 'suggestion', 'topic', 'search', 'engine', 'dataset', 'provide', 'contain', 'column', 'root', 'term', 'query', 'input', 'query', 'suggestion', 'suggest', 'query', 'location', 'search', 'perform', 'loca', 'tion', 'server', 'run', 'colab', 'case', 'iowa', 'add', 'dataset', 'perform', 'scrape', 'blank', 'browser', 'search', 'personalization', 'effect', 'location', 'effect', 'suggest', 'search', 'query', 'scrape', 'setup', 'thus', 'eliminate', 'sonalization', 'effect', 'theory', 'cause', 'fect', 'search', 'engine', 'provider', 'describe', 'query', 'suggestion', 'base', 'user', 'search', 'successful', 'attempt', 'fluence', 'query', 'suggestion', 'confirm', 'claim', 'thus', 'deduce', 'query', 'suggestion', 'reflect', 'real', 'frequently', 'use', 'search', 'query', 'use', 'proxy', 'search', 'query', 'lack', 'information', 'frequency', 'collect', 'search', 'query', 'dataset', 'contain', 'rank', 'suggestion', 'well', 'sugge', 'tion', 'root', 'term', 'approximate', 'frequent', 'search', 'query', 'assume', 'topical', 'tag', 'allside', 'news', 'reflect', 'popular', 'topic', 'produce', 'dataset', 'consist', 'real', 'search', 'query', 'newsrelevant', 'topic', 'develop', 'bias', 'language', 'model', 'contribution', 'produce', 'pair', 'transformer', 'base', 'language', 'model', 'capable', 'induce', 'left', 'right', 'partisanship', 'linguistic', 'bias', 'leverage', 'masking', 'function', 'word', 'target', 'document', 'domainadopte', 'distilbert', 'section', 'describe', 'methodological', 'approach', 'develop', 'transformerbase', 'language', 'model', 'capable', 'bias', 'text', 'usually', 'system', 'develop', 'goal', 'produce', 'reproduce', 'little', 'bias', 'possible', 'bias', 'text', 'figure', 'pipeline', 'use', 'generate', 'biasinduce', 'language', 'model', 'black', 'box', 'represent', 'dataset', 'language', 'model', 'available', 'request', 'tool', 'provide', 'zenodo', 'allside', 'biased', 'news', 'dataset', 'dataset', 'contain', 'news', 'article', 'collect', 'allside', 'balanced', 'news', 'headline', 'roundup', 'side', 'balance', 'news', 'feature', 'expertselecte', 'news', 'article', 'source', 'different', 'political', 'view', 'leave', 'right', 'center', 'often', 'feature', 'spin', 'bias', 'slant', 'form', 'nonneutral', 'reporting', 'political', 'news', 'article', 'tag', 'bias', 'label', 'expert', 'annotator', 'base', 'express', 'political', 'partisanship', 'leave', 'right', 'neutral', 'allside', 'balance', 'news', 'aim', 'offer', 'multiple', 'political', 'perspective', 'important', 'news', 'story', 'educate', 'user', 'bias', 'provide', 'multiple', 'viewpoint', 'collect', 'datum', 'include', 'headline', 'news', 'text', 'publishing', 'date', 'topic', 'tag', 'coronavirus', 'federal', 'job', 'link', 'article', 'publishing', 'outlet', 'also', 'include', 'allside', 'neutral', 'description', 'topic', 'article', 'overall', 'produce', 'dataset', 'contain', 'article', 'tag', 'leave', 'article', 'tag', 'right', 'article', 'tag', 'center', 'collect', 'article', 'publish', 'date', 'datum', 'collection', 'end', 'use', 'allside', 'dataset', 'develop', 'bias', 'language', 'model', 'describe', 'section', 'allow', 'easy', 'access', 'recent', 'complete', 'version', 'dataset', 'future', 'research', 'provide', 'scrape', 'tool', 'describe', 'section', '30may', 'haak', 'schaer', 'try', 'achieve', 'opposite', 'bias', 'text', 'reproduce', 'bias', 'inherent', 'biased', 'finetune', 'dataset', 'example', 'allow', 'simulate', 'user', 'search', 'biased', 'term', 'poten', 'tially', 'cause', 'exposure', 'biased', 'news', 'goal', 'cap', 'ture', 'biased', 'language', 'pretraine', 'language', 'model', 'use', 'huggingface', 'transformer', 'module', 'finetune', 'base', 'distilbert', 'model', 'train', 'wikipedia', 'article', 'large', 'book', 'dataset5', 'distilbert', 'choose', 'base', 'model', 'prove', 'perform', 'well', 'comparably', 'small', 'dataset', 'perform', 'well', 'small', 'size', 'aptness', 'adopt', 'domainspecificity', 'aware', 'effective', 'model', 'show', 'suitability', 'approach', 'choose', 'distilbert', 'efficiency', 'sufficient', 'effectiveness', 'finetune', 'range', 'pair', 'model', 'left', 'model', 'finetune', 'part', 'allside', 'tag', 'leave', 'biased', 'news', 'right', 'model', 'finetune', 'document', 'dataset', 'label', 'rightbiase', 'news', 'produce', 'model', 'left', 'right', 'model', 'combination', 'parameter', 'goal', 'find', 'ideal', 'approach', 'capture', 'reproduce', 'much', 'bias', 'possible', 'produce', 'meaningful', 'result', 'central', 'aspect', 'vary', 'finetune', 'model', 'datum', 'use', 'model', 'develop', 'use', 'different', 'datum', 'configuration', 'headline', 'news', 'text', 'left', 'right', 'news', 'article', 'allside', 'dataset', 'b', 'news', 'text', 'c', 'headline', 'factor', 'evaluate', 'use', 'padding', 'concatenation', 'generate', 'consistent', 'chunk', 'size', 'finetune', 'choose', 'chunk', 'size', 'length', 'finetune', 'document', 'padding', 'approach', 'concatenation', 'approach', 'lastly', 'compare', 'effect', 'intentional', 'overfitting', 'number', 'epoch', 'compare', 'reasonable', 'epoch', 'prove', 'produce', 'good', 'combination', 'training', 'loss', 'validation', 'loss', 'dataset', 'configuration', 'theory', 'overfitting', 'possibility', 'reproduce', 'bias', 'mulation', 'text', 'use', 'finetune', 'try', 'intentionally', 'raise', 'number', 'epoch', 'parameter', 'evaluate', 'bias', 'language', 'model', 'model', 'available', 'request', 'decide', 'make', 'model', 'publicly', 'available', 'concern', 'potential', 'harm', 'cause', 'misuse', 'system', 'far', 'orate', 'section', 'evaluate', 'model', 'effectiveness', 'choose', 'combination', 'manual', 'assessment', 'quantitative', 'mea', 'sure', 'task', 'model', 'generate', 'biased', 'output', 'however', 'bias', 'diverse', 'easily', 'measurable', 'way', 'identify', 'biased', 'token', 'dataset', 'perplexity', 'useful', 'measure', 'evaluate', 'model', 'far', 'effectively', 'measure', 'bias', 'criterion', 'effectiveness', 'ability', 'system', 'produce', 'biased', 'version', 'query', 'however', 'assume', 'mask', 'word', 'search', 'query', 'topic', 'political', 'news', 'domain', 'let', 'pair', 'language', 'model', 'predict', 'word', 'output', 'left', 'right', 'model', 'differ', 'choose', 'evaluate', 'performance', 'search', 'query', 'want', 'use', 'model', 'future', 'research', 'bias', 'search', 'result', 'biased', 'unbiased', 'search', 'query', 'far', 'represent', 'text', 'different', 'type', 'domain', 'text', 'use', 'train', 'model', 'pair', 'model', 'measure', 'difference', 'model', 'probable', 'nonpunctuation', 'prediction', 'randomly', 'select', 'query', 'dataset', 'measure', 'rank', 'bias', 'overlap', 'rbo', '𝑝', 'mask', 'random', 'token', 'query', 'part', 'original', 'news', 'topic', 'stop', 'word', 'want', 'let', 'model', 'generate', 'meaningcarrye', 'token', 'keep', 'query', 'topic', 'intact', 'nonsensical', 'random', 'prediction', 'neologism', 'lead', 'great', 'rbo', 'score', 'want', 'generate', 'query', 'human', 'formulate', 'assure', 'model', 'generate', 'output', 'make', 'sense', 'let', 'model', 'generate', 'next', 'word', 'prediction', 'biasprovoke', 'sentence', 'hilary', 'covid', 'vaccine', 'domain', 'expert', 'professor', 'postdoctoral', 'researcher', 'label', 'nonsensical', 'biased', 'prediction', 'measure', 'annotator', 'agreement', 'individual', 'statement', 'independent', 'model', 'produce', 'statement', 'nonsensical', 'prediction', 'obtain', 'value', 'slight', 'agreement', 'biased', 'prediction', 'value', 'almost', 'perfect', 'bias', 'label', 'assign', 'suggest', 'word', 'induce', 'political', 'stance', 'bias', 'table', 'show', 'result', 'model', 'evaluation', 'process', 'low', 'ℎ𝑡', 'describe', 'headline', 'text', 'use', 'rank', 'bias', 'overlap', 'left', 'right', 'model', 'configuration', 'describe', 'average', 'percentage', 'nonsensical', 'prediction', 'model', '𝑃𝑏𝑖𝑎𝑠', 'percentage', 'biased', 'prediction', 'overall', 'model', 'finetune', 'headline', 'show', 'good', 'low', 'rbo', 'score', 'well', 'low', 'score', 'headline', 'text', 'text', 'perform', 'less', 'equally', 'term', 'rbo', 'concatenate', 'text', 'produce', 'average', 'well', 'result', 'pad', 'finetune', 'headline', 'effect', 'minimal', 'intentional', 'overfitting', 'raise', 'amount', 'training', 'epoch', 'seem', 'worsen', 'rbo', 'score', 'percentage', 'nonsensical', 'prediction', 'differ', 'much', 'different', 'finetuning', 'setup', 'finetune', 'text', 'seem', 'scenario', 'increase', 'percentage', 'nonsensical', 'prediction', 'many', 'model', 'generate', 'high', 'percentage', 'biased', 'suggestion', 'headline', 'model', 'high', 'percentage', 'biased', 'prediction', 'discussion', 'language', 'model', 'show', 'use', 'small', 'highquality', 'dataset', 'possible', 'finetune', 'transformerbase', 'language', 'model', 'bias', 'text', 'finetune', 'setup', 'model', 'finetune', 'headline', 'produce', 'overall', 'good', 'result', 'main', 'reason', 'assume', 'high', 'amount', 'quote', 'news', 'text', 'often', 'statement', 'author', 'article', 'disagree', 'induce', 'noise', 'finetune', 'model', 'far', 'condense', 'ture', 'headline', 'aim', 'catch', 'attention', 'also', 'reflect', 'result', 'language', 'model', 'output', 'left', 'right', 'model', 'finetune', 'headline', 'produce', 'text', 'contain', 'linguistic', 'bias', 'example', 'trump', 'model', 'produce', 'hero', 'rightbiase', 'fraud', 'leftbiase', 'next', 'word', 'prediction', 'overall', 'low', 'percentage', 'nonsensical', 'prediction', 'support', 'rbo', 'suited', 'evaluation', 'metric', 'overall', 'good', 'result', 'dataset', 'medium', 'bias', 'search', 'query', 'query', 'suggestion', '30may', 'table', 'evaluation', 'result', 'finetune', 'language', 'model', 'highlight', 'value', 'good', 'result', 'metric', 'model', 'configuration', '𝑜𝑣𝑒𝑟', '𝑖𝑡𝑡𝑖𝑛𝑔', '𝑜𝑣𝑒𝑟', '𝑜𝑣𝑒𝑟', '𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ', '𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇', '𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇', '𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇', '𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ', '𝑜𝑣𝑒𝑟', '𝐷𝑖𝑠𝑡𝑖𝑙𝐵𝐸𝑅𝑇', '𝑝𝑎𝑑𝑑𝑖𝑛𝑔𝑡', '𝑜𝑣𝑒𝑟', '𝑝𝑎𝑑𝑑𝑖𝑛𝑔ℎ𝑡', '𝑜𝑣𝑒𝑟', 'future', 'research', 'investigate', 'finding', 'language', 'model', 'compare', 'performance', 'result', 'scrape', 'tool', 'dataset', 'allow', 'effectively', 'generate', 'language', 'model', 'enable', 'investigation', 'research', 'topic', 'political', 'news', 'information', 'search', 'domain', 'bias', 'classification', 'sentiment', 'analysis', 'remark', 'moral', 'issue', 'aware', 'build', 'biasinducing', 'system', 'provide', 'dataset', 'enable', 'reproduction', 'develop', 'similar', 'system', 'problematic', 'also', 'consider', 'work', 'inspire', 'help', 'develop', 'transformerbase', 'language', 'model', 'capable', 'produce', 'biased', 'toxic', 'otherwise', 'harmful', 'text', 'severity', 'openly', 'publish', 'biased', 'system', 'show', 'model', 'model6', 'publication', 'model', 'incite', 'debate', 'show', 'problematic', 'biased', 'language', 'make', 'available', 'wide', 'public', 'au', 'dience', 'explicitly', 'state', 'intend', 'use', 'research', 'application', 'model', 'bias', 'primarily', 'term', 'linguistic', 'bias', 'reflect', 'political', 'stance', 'view', 'political', 'topic', 'part', 'include', 'objectively', 'wrong', 'opinionated', 'statement', 'hate', 'speech', 'racism', 'form', 'despicable', 'language', 'toxicity', 'model', 'reproduce', 'text', 'convey', 'phenomenon', 'minimize', 'harmful', 'effect', 'publication', 'cause', 'misuse', 'model', 'decide', 'make', 'biased', 'model', 'publicly', 'accessible', 'provide', 'access', 'model', 'request', 'research', 'reasonable', 'application', 'use', 'case', 'provide', 'however', 'interest', 'investigate', 'bias', 'correlation', 'bias', 'online', 'information', 'search', 'goal', 'increase', 'transparency', 'fairness', 'raise', 'awareness', 'easily', 'intentionally', 'bias', 'reproduce', 'induce', 'system', 'part', 'endeavor', 'show', 'datum', 'provide', 'use', 'create', 'model', 'produce', 'biased', 'language', 'choose', 'publish', 'dataset', 'scraper', 'mainly', 'reason', 'state', 'section', 'similar', 'dataset', 'available', 'publicly', 'b', 'believe', 'benefit', 'raise', 'awareness', 'outweigh', 'make', '6https', 'already', 'public', 'biased', 'datum', 'accessible', 'c', 'dataset', 'biased', 'language', 'require', 'develop', 'system', 'detect', 'inhibit', 'bias', 'work', 'hope', 'highlight', 'need', 'assure', 'datum', 'use', 'develop', 'model', 'unbiased', 'raise', 'awareness', 'easily', 'transformerbase', 'language', 'model', 'finetune', 'produce', 'biased', 'language', 'outlook', 'publication', 'represent', 'first', 'foundational', 'milestone', 'largerscale', 'investigation', 'bias', 'online', 'information', 'search', 'major', 'next', 'contribution', 'plan', 'use', 'present', 'dataset', 'model', 'investigate', 'effect', 'biased', 'unbiased', 'search', 'query', 'search', 'result', 'different', 'search', 'engine', 'popular', 'topic', 'political', 'news', 'domain', 'accomplish', 'need', 'overcome', 'issue', 'subjectivity', 'lack', 'effective', 'method', 'ological', 'approach', 'bias', 'identification', 'employ', 'human', 'annotation', 'plan', 'introduce', 'ensemble', 'method', 'include', 'biasagnostic', 'analysis', 'linguistic', 'difference', 'lexical', 'feature', 'transformerbase', 'approach', 'bias', 'classification', 'far', 'plan', 'conduct', 'study', 'use', 'simulation', 'approach', 'simulation', 'different', 'user', 'behavior', 'term', 'information', 'need', 'formulating', 'query', 'interact', 'query', 'suggestion', 'interactive', 'information', 'search', 'simulation', 'plan', 'gain', 'insight', 'effect', 'bias', 'formation', 'information', 'search', 'additionally', 'hope', 'identify', 'user', 'property', 'havior', 'increase', 'lower', 'risk', 'encounter', 'bias', 'information', 'search', 'conclusion', 'work', 'present', 'first', 'milestone', 'ongoing', 'research', 'bias', 'online', 'search', 'present', 'dataset', 'biased', 'news', 'dataset', 'large', 'dataset', 'biased', 'unbiased', 'search', 'query', 'topic', 'political', 'news', 'domain', 'far', 'provide', 'scrape', 'tool', 'allow', 'collect', 'biaslabele', 'news', 'text', 'allside', 'lastly', 'evaluate', 'approach', 'finetune', 'distilbert', 'transformerbase', 'language', 'model', 'bias', 'text', 'publish', 'model', 'capable', 'induce', 'left', 'right', 'political', 'stance', 'bias', 'form', 'lexical', 'bias', 'reference', 'allside', 'allside', 'create', 'balanced', 'news', 'stepbystep', 'guide', 'retrieve', 'https', 'allside', 'balanced', 'news', 'headline', 'roundup', 'retrieve', 'https', 'jing', 'bai', 'shinnou', 'construction', 'domain', 'specific', 'distilbert', 'model', 'use', 'finetune', 'taai', 'preslav', 'nakov', 'detect', 'bias', 'predict', 'political', 'ideology', 'news', 'article', 'emnlp', 'zeller', 'face', 'expose', 'searcher', 'cool', 'diane', 'park', 'iterative', 'exploration', 'design', 'evaluation', 'support', 'query', 'reformulation', 'interactive', 'information', 'retrieval', 'tolga', 'bolukbasi', 'zou', 'venkatesh', 'saligrama', 'man', 'computer', 'programmer', 'woman', 'homemaker', 'debiase', 'word', 'embedding', 'nip', 'schaer', 'investigation', 'bias', 'web', 'search', 'engine', 'query', 'suggestion', '30may', 'haak', 'schaer', 'lysandre', 'debut', 'victor', 'sanh', 'clement', 'delangue', 'pierric', 'cistac', 'clara', 'canwen', 'sylvain', 'gugger', 'quentin', 'lhoest', 'rush', 'transformer', 'stateoftheart', 'natural', 'language', 'processing', 'emnlp', 'improve', 'effectiveness', 'information', 'retrieval', 'local', 'context', 'analysis', 'acm', 'tran', 'inf', 'syst', 'https', 'arzucan', 'özgür', 'analyze', 'elmo', 'distilbert', 'sociopolitical', 'news', 'classification', 'aespen', 'maarten', 'de', 'rijke', 'survey', 'query', 'auto', 'completion', 'information', 'retrieval', 'fntir', 'weifan', 'wachsmuth', 'analyze', 'political', 'bias', 'unfairness', 'news', 'article', 'different', 'level', 'granularity', 'https', 'coefficient', 'agreement', 'nominal', 'scale', 'psychol', 'alessio', 'medium', 'bias', 'presidential', 'election', 'metaanalysis', 'j', 'commun', 'edelman', 'edelman', 'trust', 'barometer', 'retrieve', 'https', 'search', 'engine', 'manipulation', 'effect', 'seme', 'possible', 'impact', 'outcome', 'election', 'pna', 'e4512', 'publisher', 'section', 'pna', 'train', 'become', 'hate', 'speech', 'machine', 'retrieve', 'https', 'bertram', 'gawronski', 'partisan', 'bias', 'identification', 'fake', 'news', 'tic', 'fabian', 'haak', 'schaer', 'perceptionaware', 'bias', 'detection', 'query', 'suggestion', 'bias', 'fabian', 'haak', 'schaer', 'auditing', 'search', 'query', 'suggestion', 'bias', 'schaer', 'schaible', 'novel', 'combine', 'term', 'suggestion', 'service', 'domainspecific', 'digital', 'library', 'tpdl', 'lecture', 'note', 'computer', 'science', 'vol', 'heiko', 'schuldt', 'ed', 'springer', 'http', 'hienertssm11', 'besnik', 'neural', 'base', 'statement', 'classification', 'biased', 'language', 'wsdm', 'acm', 'l', 'introna', 'define', 'web', 'politic', 'search', 'engine', 'computer', 'juhi', 'messia', 'krishna', 'p', 'gummadi', 'search', 'bias', 'quantification', 'investigate', 'political', 'bias', 'social', 'medium', 'web', 'search', 'inf', 'retr', 'j', 'ruibo', 'transformerbase', 'framework', 'neutralize', 'reverse', 'political', 'polarity', 'news', 'article', 'proc', 'acm', 'humcomput', 'interact', 'bhaskar', 'milad', 'shokouhi', 'filip', 'radlinski', 'user', 'interaction', 'query', 'autocompletion', 'sigir', 'negar', 'cumming', 'moral', 'framing', 'ideological', 'bias', 'news', 'socinfo', 'marius', 'andriushchenko', 'stability', 'finetune', 'bert', 'misconception', 'explanation', 'strong', 'baseline', 'iclr', 'https', 'openreviewnetforum', 'idnzplwnvayah', 'niu', 'diane', 'use', 'query', 'suggestion', 'information', 'search', 'ipm', 'pass', 'abdur', 'chowdhury', 'picture', 'search', 'infoscale', 'lily', 'ray', 'search', 'survey', 'much', 'user', 'trust', 'search', 'result', 'retrieve', 'mozcomblog2020google', 'shaina', 'deepak', 'de', 'dbia', 'detect', 'bias', 'ensure', 'fairness', 'news', 'article', 'int', 'anal', 'reddit', 'recasen', 'danescuniculescumizil', 'guistic', 'model', 'analyze', 'detect', 'biased', 'language', '1650–1659', 'retrieve', 'bad', 'ai', 'ever', 'auditing', 'partisan', 'audience', 'bias', 'search', 'proc', 'acm', 'humcomput', 'interact', 'cscw', 'article', 'auditing', 'autocomplete', 'suggestion', 'network', 'recursive', 'victor', 'sanh', 'lysandre', 'tilbert', 'distilled', 'version', 'small', 'fast', 'cheap', 'light', 'danny', 'autocomplete', 'work', 'search', 'retrieve', 'workssearch', 'raheem', 'beyah', 'game', 'missuggestion', 'semantic', 'analysis', 'search', 'autocomplete', 'manipulation']",
"DreamSmooth: Improving Model-based Reinforcement Learning via Reward
  Smoothing","[{'href': 'http://arxiv.org/abs/2311.01450v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2311.01450v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-11-02 17:57:38,"3
2
0
2

v
o
N
8
2

]
L
C
.
s
c
[

1
v
4
6
2
7
1
.
1
1
3
2
:
v
i
X
r
a

Preprint

RETSIM: RESILIENT AND EFFICIENT TEXT
SIMILARITY

Marina Zhang1, Owen Vallis1, Aysegul Bumin*2, Tanay Vakharia1, Elie Bursztein1
Google1 University of Florida2

ABSTRACT

This paper introduces RETSim (Resilient and Efficient Text Similarity), a
lightweight, multilingual deep learning model trained to produce robust metric
embeddings for near-duplicate text retrieval, clustering, and dataset deduplication
tasks. We demonstrate that RETSim is significantly more robust and accurate
than MinHash and neural text embeddings, achieving new state-of-the-art perfor-
mance on dataset deduplication, adversarial text retrieval benchmarks, and spam
clustering tasks. We also introduce the W4NT3D benchmark (Wiki-40B 4dver-
sarial Near-T3xt Dataset) for evaluating multilingual, near-duplicate text retrieval
capabilities under adversarial settings. RETSim and the W4NT3D benchmark are
open-sourced under the MIT License at https://github.com/google/unisim.

1

INTRODUCTION

Robust near-duplicate text detection is an essential component of many tasks, including retriev-
ing documents, detecting plagiarism (Sun et al., 2013) and blocking adversarial spam cam-
paigns (Ahmed et al., 2022). Users have come to expect that systems can return accurate results
despite their queries exhibiting a 20% to 30% typo rate (Hagen et al., 2017). Furthermore, effi-
ciently deduplicating text datasets is critical to training state-of-the-art large language models (Lee
et al., 2022; Kandpal et al., 2022).

For more than two decades, MinHash-based (Broder et al., 1998) locality-sensitive hashing (LSH)
has been the most prevalent algorithm used for near-duplicate detection due to its simplicity, robust-
ness, and speed. For example, the vast majority of dataset deduplication efforts still rely on MinHash
(Lee et al., 2022; Kocetkov et al., 2022). However, like all LSH-based techniques, MinHash is not
without downsides; chief among them being that it is very parameter-sensitive and requires heavy
tuning. Additionally, MinHash lacks resilience to typos due to its reliance on n-grams, leading to
poor performance on noisy data and a vulnerability to hash-busting attacks (Issac et al., 2014).

On the other hand, deep learning models are the dominant way to perform vector-based semantic text
retrieval (Muennighoff et al., 2022), but so far, no neural embedding has been able to consistently
outperform MinHash for robust near-duplicate detection (Silcock et al., 2022). This is mostly due to
the focus on improving semantic capabilities, which leads models to be too large to run extremely
quickly and the use of sub-word level tokenization, which is not resilient to typos and adversarial
attacks (Morris et al., 2020; Bursztein et al., 2023).

To fill this gap, we introduce RETSim (Resilient and Efficient Text Similarity), a lightweight, mul-
tilingual deep learning model trained specifically to produce robust neural embeddings specialized
for near-duplicate detection. By combining the state-of-the-art RETVec text vectorizer, a modern
transformer block (Hua et al., 2022), a large typo-augmented training corpus, and a metric learn-
ing training regime, RETSim is able to achieve new state-of-the-art performance on near-duplicate
detection benchmarks (Section 4.2), dataset deduplication tasks (Sections 4.3 and 5.1), and spam
clustering applications (Section 5.2).

Furthermore, while datasets and benchmarks exist for corpus deduplication and near-duplicate text
retrieval, none of these have focused on systematically evaluating near-duplicate retrieval perfor-
mance under the presence of typos, word manipulations, and sentence or paragraph-level modifica-

*This work was done during the author’s internship at Google.

1

 
 
 
 
 
 
Preprint

tions. To address this need, we additionally introduce the W4NT3D benchmark (Wiki-40B 4dver-
sarial Near-T3xt Dataset) which enables the evaluation of algorithms on adversarial near-duplicate
text retrieval in a multilingual setting. We report the performance of RETSim, MinHash, and pop-
ular neural embeddings such as Universal Sentence Encoder (Cer et al., 2018) and LaBSE (Feng
et al., 2022) on this new benchmark in Section 4.2, highlighting uneven performance across lan-
guages and types of adversarial manipulations. The RETSim model and the W4NT3D benchmark
are open-sourced at https://github.com/google/unisim under the MIT License.

2 RELATED WORK

Near-Duplicate Detection Identifying noisy near-duplicate documents in a large corpus is a fun-
damental task with a wide range of applications, such as detecting plagiarism, finding reproduced
content in literature or news articles (Gyawali et al., 2020; Silcock et al., 2022), and deduplicat-
ing training datasets for language models. Previous research has shown that duplicates in training
datasets lead to inefficient training (Lee et al., 2022) and privacy concerns for large language models
(LLMs), where models memorize and regenerate duplicated training sequences at a much higher
frequency (Kandpal et al., 2022).

Unlike semantic text similarity, the task of identifying textual near-duplicates has been predominated
by non-neural, n-gram-based algorithms such as MinHash (Broder et al., 1998), which is the most
widely used technique for deduplicating large training corpuses (Kocetkov et al., 2022; Lee et al.,
2022). MinHash is a technique for estimating the Jaccard similarity between two sets. Algorithms
such as MinHash or SimHash (Charikar, 2002) can be combined with locality-sensitive hashing
(LSH) techniques for fast, approximate nearest neighbor search and data clustering. This allows
them to scale and deduplicate corpuses containing terabytes of data such as C4 (Lee et al., 2022)
and The Stack (Kocetkov et al., 2022). However, n-gram or shingling-based techniques typically
require texts to be parsed into a standardized form (e.g. by lower-casing or stripping punctuation),
which makes them susceptible to typos and adversarial attacks and pose a challenge when attempt-
ing to differentiate between dissimilar documents and near-duplicate documents with adversarial
augmentations.

Semantic Text Similarity The task of computing semantic similarity between text is closely re-
lated to near-duplicate detection. Semantic text similarity refers to the assessment of the semantic
relatedness of two pieces of text based on their meaning rather than their syntactic structure, as in
the case of near-duplicate detection. Recently, transformer-based language models such as Universal
Sentence Encoder (Yang et al., 2019), LaBSE (Feng et al., 2022) and LLM-based embeddings (Anil
et al., 2023) which embed text into high-dimensional embedding vectors have been successfully
used to retrieve semantically-related documents using cosine similarity. Modern text retrieval sys-
tems combine these embeddings with an approximate nearest neighbor (ANN) search algorithm to
efficiently retrieve documents matching user queries.

However, language models have been shown to be vulnerable to adversarial attacks and naturally-
occurring typos (Alzantot et al., 2018; Gao et al., 2018; Morris et al., 2020). Furthermore, language
models are typically very large and costly to run even with hardware acceleration, which makes
them unsuited for large-scale dataset deduplication or identifying near-duplicates in the presence of
typos or adversarial text manipulations.

Metric Learning Metric learning aims to learn an embedding space where similar items have
a small distance between their embeddings and dissimilar items are further away. Many state-of-
the-art embeddings use metric learning for unsupervised training or fine-tuning including Sentence-
BERT (Reimers & Gurevych, 2019) and E5 (Wang et al., 2022).

RETVec is a resilient, multilingual embedding and text vectorizer trained to be robust against various
forms of character-level typos and adversarial attacks. We extend the RETVec training regime to
full text documents for RETSim. We use Multi-Similarity Loss (Wang et al., 2019) for pair-based
metric learning, where typo-laden and near-duplicate versions of texts are trained to be closer in
the embedding space, while other texts are pushed further away. Multi-Similarity Loss is based
on a general weighting framework for pair-based losses and achieves state-of-the-art performance,
outperforming alternatives such as Triplet Loss (Schroff et al., 2015).

2

Preprint

Figure 1: RETSim model architecture diagram. RETSim works on arbitrary length text by split-
ting texts into chunks of 512 characters during its vectorization phase and encodes them using the
RETVec character vectorizer. The RETSim model then embeds each chunk of text into 256-dim
partial embeddings and combines them to produce the global embedding.

3 RETSIM

3.1 ARCHITECTURE

The RETSim model is composed of three main components (as depicted in Figure 1):

The character-level vectorizer
splits the input text into chunks of 512 characters, then uses the
RETVec chararcter encoder (Bursztein et al., 2023) to encode each chunk, resulting in a batch of
(512, 24) dense inputs. The RETVec character vectorizer encodes each Unicode character as a
compact 24-bit binary representation based on its integer codepoint value. This allows the vectorizer
to encode all valid Unicode characters and support all languages. Furthermore, the character-level
vectorizer has been shown to be more resilient against typos and adversarial attacks.

A small transformer model
is used to compute 256-dimension embeddings for each chunk of the
input text. RETSimPartial-Dup uses these embeddings directly to finding documents that have matching
chunks of text. Architecturally, the model consists of two Gated Attention Unit (GAU) blocks (Hua
(Radenovi´c et al., 2018), a dense
et al., 2022), followed by a Generalized-Mean pooling layer
projection layer which projects the embedding into 256 dimensions, and an L2 normalization layer.
The model has only 536k parameters, which is more than two orders of magnitude smaller than other
neural embeddings (Table 1). L2-normalization allows the embeddings to be compared using cosine
similarity. We discuss the impact of key architecture design choices in Section 6. Hyperparameter
details are provided in Appendix A.1.1, and additional ablations results in Appendix A.5.

An embedding averaging module
is then used to combine partial text embeddings into a full-text
embedding which is used for global near-duplicate matching (RETSimNear-Dup). Averaging chunked
embeddings to produce a global embedding is a standard technique used by many models (Cer
et al., 2018) to support infinite length inputs in a cost-efficient manner. We experimented with other
aggregation techniques to produce more accurate global embeddings, including training a deep-
averaging network (Iyyer et al., 2015), but this did not improve performance and resulted in higher
computation cost. RETSimNear-Dup and RETSimPartial-Dup are computed in a single forward pass
which makes it computationally efficient. We output both types of embeddings as they have different
applications: RETSimNear-Dup is better-suited for full-text matching and retrieval (Section 4.2), while
RETSimPartial-Dup is used to find partial text matches where the near-duplicate content appears only
in part of the document (Section 4.3).

3.2 MODEL TRAINING

Dataset We use the multilingual C4 dataset (mC4) for raw text data and following (Xue et al.,
2020), we use a language sampling exponent of α = 0.3 to balance sampling between low and
high-resource languages. We only use text containing at least 16 characters, and we randomly select
between 1 and 8 sentences (roughly 512 characters) for each text chunk. For each example in the

3

Input text

Chunk vector

Chunk vector

....

Chunk vector

Chunked and 
vectorized text
(num_chunks, 512, 24)

RETSim model

RetSim Partial-Dup 
(num_chunks, 256)

RetSim Near-Dup  
(256)

r

e

d

o

c

n

E

r

a

h

C

c

e

v

T

E

R

U

A

G

U

A

G

g

n

i

l

o

o

P

2

L

+

e

s

n

e

D

t

x

e

t

l

a

i

t

r

a

P

s

g

n

i

d

d

e

b

m

e

2

L

+

g

n

i

g

r

a

r

e

v

A

g

n

i

d

d

e

b

m

e

t

x

e

t

l

l

u

F

 
 
 
 
 
 
 
 
 
Preprint

training dataset, we generate 5 pairs of augmented examples. We apply three levels of augmentation
to each example text chunk (in this order): sentence-level, word-level, and character-level. For each
level, we randomly select the augmentation to be applied from the following categories: insertion,
deletion, substitution, and transposition. We randomly apply between 0 − 25% sentence-level aug-
mentation and up to 30% combined character and word-level augmentation. Empirically, we found
that increasing the percentage of augmentation beyond this point causes RETSim’s performance to
degrade. The full list of augmentations used can be found in Appendix A.2.

Training Procedure We train RETSim using Multi-Similarity Loss (Wang et al., 2019) with α =
4, β = 40, λ = 0.5, and ϵ = 0.1. We hypertuned these parameters and the results are shown in
Appendix A.5. We train for 1 million steps with batch size = 1024. The similarity loss trains the
model to embed augmented versions of the same text closer in the embedding space, while dissimilar
texts are pushed further apart. We use the LAMB optimizer (You et al., 2019) with a max learning
rate of 0.001 and cosine decay. Detailed training hyperparameters are reported in Appendix A.1.2.

4 EVALUATION

Model/Algorithm

Type

Embed./Hash Size

# Model Parameters

LaBSE
Multilingual USE
Multilingual E5-Base
PaLM 2 (Gecko)

SimHash
MinHash

RETSim

Neural
Neural
Neural
Neural

Hashing
Hashing

Neural

768
512
768
768

b bits
n hashes

256

471M
69M
278M
?

N/A
N/A

536k

Table 1: Embedding models and hashing algorithms benchmarked in the paper.

4.1 MODELS AND ALGORITHMS EVALUATED

We benchmark RETSim against four multilingual semantic text embeddings as well as popular n-
gram based algorithms primarily used in near-duplicate text detection (Table 1). Our baseline text
embeddings include Multilingual Universal Sentence Encoder
(Yang et al., 2019), LaBSE (Feng
et al., 2022), Multilingual E5 (Wang et al., 2022), and PaLM 2 Gecko Embeddings (Anil et al.,
2023). All text embeddings are L2-normalized and compared using cosine similarity. We use exact
search to index and retrieve nearest neighbors from our vector index for the experiments in Section 4.

For non-neural near-duplicate detection and clustering algorithms, we selected the two most popular
algorithms: MinHash (Broder et al., 1998) and SimHash (Charikar, 2002). For MinHash, we use
Datasketch’s MinHashLSH library. Following the most common practices in the literature (Silcock
et al., 2022), we use 10 hash functions for MinHash unless otherwise specified. We use word-level
n-grams where we select the best value out of n = {2, 3, 4, ..., 10}. For SimHash, we use 64-
bit SimHash and conduct shingling at the character level, where the shingle size is selected from
n = {2, 3, 4, ..., 10}. For the near-duplicate detection benchmarks (NEWS-COPY and CORE Near-
Duplicates datasets), we tune the optimal deduplication threshold (e.g. based on cosine similarity
for neural-based embeddings and Jaccard similarity for MinHash). Detailed hyperparameter settings
for RETSim and baseline algorithms used in the evaluation can be found in Appendix A.3.

4.2 W4NT3D: WIKI-40B 4DVERSARIAL NEAR-T3XT DATASET EVALUATION

Dataset Description The vast majority of text retrieval benchmarks are focused on evaluating
semantic performance. To the best of our knowledge, there is no multilingual benchmark for sys-
tematically measuring adversarial robustness for near-duplicate text retrieval. In an attempt to fill in
the gap, we create and publish the W4NT3D benchmark (Wiki-40B 4dversarial Near-T3xt Dataset),
which contains around 400k pairs of syntactically similar texts to evaluate near-duplicate text re-
trieval in the presence of various forms of text manipulations and typos.

W4NT3D is based on the Wiki-40B dataset (Guo et al., 2020). The dataset is split into query exam-
ples and target examples, where query examples are synthetically-modified near-duplicate versions

4

Preprint

of a target example (e.g. with typos). For each of the 41 language splits in Wiki-40B, we randomly
select 10,000 texts. The length of the target string is uniformly selected from between 16 and 8192
characters, in order to test performance on short and long text. To construct the query text corre-
sponding to a target text, we randomly apply up to 25% word and character augmentations, and up to
25% sentence and paragraph augmentations. For each augmentation, we uniformly select from the
[insert, delete, substitute, and swap] operations. We use Recall@k with k = 1 as the main metric,
following the setup commonly found in semantic text retrieval benchmarks.

Model/Algorithm

Arabic Chinese English German French

Spanish

Japanese Korean Russian Thai

Avg (41 Langs)

LaBSE
Multilingual USE
Multilingual E5-Base
PaLM 2 (Gecko)

SimHash
MinHash

RETSimPartial-Dup
RETSimNear-Dup

0.915
0.915
0.936
0.497

0.558
0.633

0.928
0.971

0.917
0.986
0.980
0.623

0.276
0.172

0.946
0.971

0.944
0.958
0.959
0.961

0.591
0.591

0.954
0.987

0.931
0.942
0.944
0.932

0.561
0.558

0.949
0.978

0.930
0.938
0.948
0.934

0.519
0.556

0.947
0.983

0.888
0.903
0.896
0.911

0.513
0.575

0.938
0.976

0.931
0.990
0.979
0.578

0.465
0.223

0.963
0.986

0.949
0.984
0.986
0.701

0.593
0.814

0.971
0.991

0.918
0.910
0.911
0.851

0.554
0.523

0.946
0.970

0.882
0.888
0.921
0.571

0.669
0.416

0.941
0.946

0.921
0.912
0.937
0.823

0.550
0.538

0.949
0.977

Table 2: Per-language retrieval performance for various embedding models and algorithms on the
W4NT3D benchmark. Results on selected languages are reported alongside the average Recall@1
for all 41 languages. Full results for all languages are reported in Appendix A.4.

Multilingual Performance Overall, RETSimNear-Dup achieves an average Recall@1 of 0.977
across all 41 languages on the W4NT3D benchmark (Table 2). RETSimPartial-Dup is second best
with a Recall@1 of 0.949 and Multilingual E5, the best-performing baseline, is third with an aver-
age Recall@1 of 0.932. We expect that RETSimNear-Dup outperforms RETSimPartial-Dup because the
W4NT3D benchmark requires an algorithm to not just find near-duplicates, but to find the most simi-
lar text. RETSimPartial-Dup optimizes for finding the most similar chunk of text in the corpus, which is
not always the most similar text overall. Similarly, we hypothesize that MinHash and SimHash per-
form poorly on the W4NT3D benchmark due to their lack of ability to distinguish which is the most
similar text among the near-duplicates detected, and embedding-based models and cosine similarity
offer a more fine-grained measure of similarity.

RETSimNear-Dup outperforms baseline algorithms on all languages except for Chinese and Japanese.
For these languages, we theorize that semantic embeddings may have the slight edge in performance
because their significantly larger model sizes (more than 100x larger than RETSim, as shown in Ta-
ble 1) allow them to have a better representation on languages with large character sets. Furthermore,
the sub-word level tokenizers used in the baseline embeddings often treat each character in Chinese
or Japanese as individual tokens, which could offer higher resilience to typos.

Figure 2: Recall@1 performance on the W4NT3D benchmark, broken down by augmentation type.
Results are averaged across all 41 language splits in W4NT3D.

Adversarial Resilience Delving deeper into the impact of various types of text manipulation re-
veals that RETSimNear-Dup and RETSimPartial-Dup perform almost equally well regardless of the type
of augmentation applied (Figure 2). Semantic text embeddings perform well on paragraph, sentence
and word-level manipulations, but as expected, exhibit significantly weaker performance towards
character-level typos. MinHash and SimHash struggle more with word-level augmentations than
deep-learning based embeddings and collapse when character-level typos are introduced. We at-

5

1.00

0.75

0.50

0.25

0.00

l
l

1
@
a
c
e
R

Paragraph

Sentence

Word

Character

Augmentation Level

LaBSE

Multilingual USE

Multilingual E5-Base

PaLM 2 (Gecko)

SimHash

MinHash

RETSim (Partial-Dup)

RETSim (Near-Dup)

Preprint

tribute RETSim’s resilience to adversarial manipulations to the RETVec character encoder as well
as using deep metric learning to train robust embeddings.

Figure 4 reports the Recall@1 performance of the algorithms as the amount of augmentation in-
creases. All algorithms perform perfectly when no augmentation is applied (exact matching), but as
the percentage of augmentation increases, n-gram based approaches exhibit a steep drop in perfor-
mance. Semantic text embeddings are able to sustain a larger degree of augmentation before their
retrieval capabilities start to degrade (over 20%). RETSimNear-Dup is the most robust algorithm, with
a noticeable drop in performance only after around 40% augmentation. This makes RETSim the
most effective approach at clustering and deduplicating text under adversarial settings.

Figure 3: Recall@1 performances on the
W4NT3D benchmark (English only) for
varying max target lengths.

Figure 4: Recall@1 performances on the W4NT3D
benchmark (English only) as the amount of augmenta-
tion applied to the query text increases.

Text Length Impact on Performance Figure 3 reports the Recall@1 performance of RETSim
and baseline algorithms as the length of the query and target text varies. We see that RETSimNear-Dup
and RETSimPartial-Dup outperforms all other methods on short texts with fewer than 128 characters.
As the text length increases beyond 512 characters, RETSimNear-Dup remains close to perfect while
RETSimPartial-Dup’s performance degrades since it splits the text into multiple embeddings and finds
the nearest matching chunk of text. MinHash and SimHash also perform poorly on short text lengths
and start to degrade on longer texts. For neural-based embeddings, we observe a slight drop in
performance on longer texts for all models except RETSimNear-Dup and Multilingual USE, the only
two embeddings that can handle arbitrary length inputs.

4.3 REAL-WORLD NEAR-DUPLICATE DETECTION EVALUATION

Setup We benchmark RETSim’s ability to identify near-duplicate content on real-world datasets
from the literature. The NEWS-COPY Deduplication dataset (Silcock et al., 2022) contains 27,210
historical news articles with 122,876 positive duplicate pairs. The dataset consists of noisy near-
duplicates due to factors like OCR errors, plagiarism, and news aggregation. We also evaluate the
algorithms on the CORE Near-Duplicates dataset (Gyawali et al., 2020), which consists of 100k
scholarly articles with 25k exact duplicates, 25k near-duplicates, and 50k non-duplicates. Near-
duplicates in this dataset arise from article revisions, versioning and metadata differences, and hu-
man typos. A key difference between these two benchmarks and the W4NT3D benchmark is that
these two benchmarks are focused on detecting and clustering near-duplicate text, rather than robust
text retrieval based on syntactic similarity. For both benchmarks, we follow the experimental setup
provided in the papers and report Adjusted Rand Index (ARI) for the NEWS-COPY dataset and
report precision/recall/F1 scores on the CORE Near-Duplicates dataset.

Results On the NEWS-COPY dataset, RETSimPartial-Dup outperforms all other approaches by a
significant margin (4.8% ARI compared to our best MinHash result), as reported in Table 3. In the
dataset, there are many near-duplicate pairs where one text is significantly longer than the other, so it
is expected that RETSimPartial-Dup, which can find matching text chunks in documents, is more suited
for the task and outperforms RETSimNear-Dup. Bucketing the near-duplicate detection rate of each
algorithm by the length ratio between positive pairs (Figure 5), we observe that RETSimPartial-Dup
outperforms MinHash regardless of the length ratio, but MinHash surpasses RETSimNear-Dup per-
formance when one text is above roughly 1.5x the length of the other text in a near-duplicate pair.

6

1.0

0.8

0.6

0.4

l
l

1
@
a
c
e
R

16

32

64

128

256

512

1024

2048

4096

8192

Max Target Text Length

l
l

1
@
a
c
e
R

1.0

0.8

0.6

0.4

0%

10%

20%

30%

40%

50%

Text Augmentation Amount (%)

LaBSE

Multilingual USE

Multilingual E5-Base

PaLM 2 (Gecko)

SimHash

MinHash

RETSim (Partial-Dup)

RETSim (Near-Dup)

Preprint

Model/Algorithm ARI

Multilingual USE
Multilingual E5-Base
S-BERT*

SimHash
MinHash*
MinHash (Ours)

RETSimPartial-Dup
RETSimNear-Dup

0.730
0.742
0.700

0.695
0.737
0.783

0.831
0.704

Table 3: Performance comparison on the
NEWS-COPY dataset. Adjusted Rand Index
(ARI) values are reported. * denotes results
from Silcock et al. (2022).

Figure 5: Near-duplicate detection rate of RET-
Sim vs MinHash for different length ratios of pos-
itive pairs. X-axis is the length of longer divided
by shorter text, rounded to the nearest integer.

Additionally, we noticed that the labels in the dataset were occasionally noisy, as a substantial por-
tion of the RETSim false positives appear to be near-duplicates upon inspection (Appendix A.6).

On the CORE Near-Duplicates dataset (Table 4), where documents (article title + abstract) are
roughly the same size, RETSimPartial-Dup and RETSimNear-Dup performance is roughly equivalent.
Both methods outperform the baselines in terms of macro F1 score and accuracy. We use MinHash
+ LSH with 256 hash functions for computational efficiency, as recommended by the datasketch
library1 for better accuracy than the default setting. Deduplication thresholds and detailed hyperpa-
rameter settings for the algorithms on both near-duplication datasets can be found in Appendix A.3.

Model / Algorithm

Exact Title Matching*

LaBSE
Multilingual USE
Multilingual E5-Base

MinHash + LSH

RETSimPartial-Dup
RETSimNear-Dup

Precision
Duplicates

Recall
Duplicates

Precision
Non-Duplicates

Recall
Non-Duplicates

Macro F1 Accuracy

0.830

0.937
0.917
0.931

0.929

0.945
0.928

0.500

0.923
0.907
0.908

0.902

0.941
0.937

0.709

0.930
0.918
0.919

0.915

0.945
0.942

0.992

0.943
0.927
0.939

0.938

0.949
0.934

0.757

0.933
0.917
0.924

0.921

0.945
0.935

0.746

0.919
0.909
0.920

0.918

0.928
0.926

Table 4: Evaluation results on the CORE Near-Duplicates dataset. Precision/recall/macro F1 and
accuracy numbers are reported. * denotes results from Gyawali et al. (2020).

5 APPLICATIONS

5.1 TRAINING DATASET DEDUPLICATION

Model/Algorithm

% train examples
with dup in train

% valid examples
with dup in train

MinHash + LSH
Exact Substring*
RETSimNear-Dup
RETSimPartial-Dup

0.47%
2.76%
3.17%
12.77%

0.46%
0.52%
0.59%
2.66%

Table 5: Deduplication rate on Wiki-40B (English). * denotes results from Lee et al. (2022).

Setup We evaluate RETSim’s ability to deduplicate text training datasets by deduplicating the
English split of Wiki-40B (Guo et al., 2020). We conservatively set the cosine similarity deduplica-
tion threshold to 0.1 for RETSimNear-Dup and 0.15 for RETSimPartial-Dup to limit the amount of false
positives, based on the optimal thresholds found in the evaluation (Appendix A.3). We use USe-
arch’s default vector index for approximate nearest neighbor search (Vardanian, 2023). We compare

1datasketch: Big Data Looks Small. https://github.com/ekzhu/datasketch.

7

t

e
a
R
n
o

t

i
t
c
e
e
D
e
a
c

t

i
l

p
u
D

-
r
a
e
N

1.0

0.8

0.5

0.3

0.0

1

2

3

4

5

6

7

8

9

10

Length Ratio between Near-Duplicate Text Pair

RETSim (Partial-Dup)

RETSim (Near-Dup)

MinHash

 
 
Preprint

Model/Algorithm

Accelerator

Batch Size

Embedding / Hashing
time (sec)

examples/sec

MinHash + LSH
RETSim
RETSim
RETSim

CPU AMD 7950 32 cores
Onnx CPU AMD 7950 32 cores
TensorFlow GPU RTX 4090
TensorFlow GPU NVIDIA H100

-
256
4096
16384

234
10839
720
363

12544
270
4062
8069

Table 6: Embedding/hashing speed of RETSim vs MinHash + LSH on the Wiki-40B dataset.

against MinHash + LSH, where we set the number of hash functions to be 256 following Kocetkov
et al. (2022) and use a Jaccard similarity threshold of 0.8 for deduplication (Lee et al., 2022).

Results Overall, as reported in Table 5, RETSimNear-Dup finds slightly more duplicates in the Wiki-
40B training and validation splits. This is in-line with our deduplication results (Section 4.3) where
RETSimNear-Dup outperforms other algorithms. On the other hand, RETSimPartial-Dup finds signifi-
cantly more matches than the exact substring matching algorithm used in the previous study (Lee
et al., 2022), showcasing the usefulness of performing both near-duplicate and partial-duplicate
matching at once. This larger-than-expected number of partial matches indicate that machine learn-
ing practitioners should take extra care to deduplicate Wikipedia at the chunk level to avoid feeding
duplicate text to their models.

In terms of embedding speed (Table 6), RETSim is significantly slower than MinHash + LSH on
CPU (46x slower), competitive when using a desktop GPU such as the RTX 4090 (3x slower) and
almost on-par when using a high-end GPU like the NVIDIA H100 (1.5x slower). Our current code
is written in Python and not fully optimized, so we expect this performance gap to significantly
shrink as we optimize our implementation. Although RETSim is slower than MinHash, RETSim
is significantly smaller and faster than other text embedding models, and closes the performance
gap between neural and non-neural based methods for near-duplicate text detection and dataset
deduplication. Both RETSimNear-Dup and RETSimPartial-Dup are returned at the same time so they
have the same embedding speed. Indexing and retrieval times will depend on the vector index and
search algorithm used. For longer documents, RETSimPartial-Dup will produce more embeddings
than RETSimNear-Dup, so RETSimPartial-Dup offers a tradeoff between finer-grained matching versus
indexing/retrieval speed, which will depend on the specific vector search algorithm and dataset used.

5.2

IN THE WILD: SPAM EMAIL CLUSTERING

In this section, we showcase RETSim’s real-world performance on clustering near-duplicate text
which has been heavily manipulated by adversarial attacks by performing an evaluation on spam
campaigns. Spam constitutes a strong proving ground for near-duplicate clustering algorithms as
spammers employ adversarial augmentation techniques in an attempt to evade detection. Such aug-
mentations typically include appending or prepending unrelated text, interleaving random words and
different languages, intentionally introducing typos, abusing extended character sets such as emojis
and homoglyphs, and more. These techniques are collectively referred to as hash-busting.

Setup The dataset consists of 5,252 spam emails from 196 spam campaigns, donated by Gmail
users who flagged them when they reached their inboxes. Each example contains the email subject
concatenated with the message content. The emails were misclassified by a spam classifier due to
their effective adversarial text manipulation techniques, which makes them a challenging test set for
clustering evaluations. Some examples of hash-busting attacks and adversarial manipulations we
observe include the use of homoglpyphs, uncommon Unicode character sets, invisible characters,
and padding with random words from different languages. To get the ground truth campaign clusters,
emails were manually reviewed and assigned to a specific spam campaign based on similarity by
human reviewers. We use agglomerative clustering to cluster spam emails, and report homogeneity,
completeness, V-Measure, and Adjusted Rand Index (ARI) metrics.

Results Overall, we observed that RETSim is significantly better at clustering near-duplicates
with adversarial manipulations, outperforming both SimHash and USE across all metrics considered
(Table 7). In particular, we observed that RETSim outperforms USE by 4.6% on the V-Measure
score which is our main metric. The results reported in this section are in-line with what we observe
since we deployed RETSim as our main near-duplicate detection algorithm in December 2022.

8

Preprint

Model / Algorithm Homogeneity Completeness V-Measure ARI

USE
SimHash + LSH
RETSimNear-Dup

0.856
0.867
0.937

0.955
0.876
0.963

0.903
0.871
0.949

0.6
0.571
0.747

Table 7: Performance on clustering adversarial spam campaigns in practice.

6 ABLATION STUDIES

Setup In this section, we summarize the key ablation studies we performed when designing RET-
Sim. All the models used in this section are trained using the setup detailed in Appendix A.1.2, ex-
cept we only train them for 100k steps to reduce computational costs. We evaluate RETSimNear-Dup’s
performance for each model on a subset of the W4NT3D benchmark, where we randomly select
1000 examples from each of the 41 language splits and use Recall@1 as reported metric.

Block Type Recall@1

Chunk Size Recall@1

Embed. Dim Recall@1

RETVec MLP
ConvNeXt
BERT
T5
*GAU

0.975
0.978
0.973
0.980
0.986

128
256
*512
1024
2048

0.979
0.984
0.986
0.983
0.978

64
128
*256
512
768

0.969
0.980
0.986
0.986
0.986

Table 8: RETSim ablation study results on architecture block type (left), text chunk size (middle),
and embedding dimension (right). *Bold denotes the value selected for the final RETSim model.

Results Table 8 contains RETSim ablation study results on max text chunk size, architecture block
type, and embedding size. The most important architectural decision was to decide the optimal text
chunk size and finding the right balance between having the smallest size possible to maximize
RETSimPartial-Dup efficiency while ensuring RETSimNear-Dup full-text embeddings can work effec-
tively on full documents. We find that chunks of 512 characters offer the best performance.

We also tested various model architectures and transformer blocks to find the best balance between
efficiency and performance. We find that the more modern GAU block (Hua et al., 2022) outper-
forms the vanilla BERT transformer block (Devlin et al., 2019) and the T5 block (Xue et al., 2020).
We also tried modern CNN architectures such as ConvNeXt (Liu et al., 2022) and the MLP architec-
ture proposed in RETVec (Bursztein et al., 2023), but both were worse than GAU block performance.
Last but not least, we found that increasing the embedding size past 256 dimensions does not yield
any meaningful improvements for RETSimNear-Dup. Accordingly, we opted to use a 256-dimension
embedding for space-efficiency and to maximize indexing and query speed. Additional ablation
studies for other hyperparameters can be found in Appendix A.5.

7 FUTURE WORK

RETSim’s novel training regime, which combines metric learning and data augmentation, has many
other potential applications that we plan to explore in future work. For example, it could be adapted
or extended to train robust semantic embeddings or image similarity embeddings. Additionally,
we expect that as general models become bigger and more expensive to run in the future, smaller,
specialized models such as RETSim will emerge as an efficient alternative for a wide range of tasks.

8 CONCLUSION

In this paper, we introduced RETSim, a novel, multilingual text embedding which achieves state-
of-the-art performance on near-duplicate text detection, dataset deduplication, and syntactic text
similarity benchmarks. RETSim is significantly faster than previous neural-based text embeddings
and more robust than n-gram based algorithms, which makes it suitable for large-scale text retrieval
and dataset deduplication, especially in adversarial settings such as spam detection. Furthermore,
we introduced the W4NT3D benchmark, the first multilingual dataset designed to measure the ad-
versarial robustness of near-duplicate text detection algorithms. We open-source both RETSim and
the W4NT3D benchmark under the MIT License.

9

Preprint

REFERENCES

Naeem Ahmed, Rashid Amin, Hamza Aldabbas, Deepika Koundal, Bader Alouffi, and Tariq Shah.
Machine Learning Techniques for Spam Detection in Email and IoT Platforms: Analysis and
Research Challenges. Security and Communication Networks, 2022:1–19, February 2022. ISSN
1939-0122, 1939-0114. doi: 10.1155/2022/1862888. URL https://www.hindawi.com/
journals/scn/2022/1862888/.

Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei
Chang. Generating Natural Language Adversarial Examples, September 2018. URL http:
//arxiv.org/abs/1804.07998. arXiv:1804.07998 [cs].

Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark,
Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark
Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,
Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Brad-
bury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christo-
pher A. Choquette-Choo, Aakanksha Chowdhery, Cl´ement Crepy, Shachi Dave, Mostafa De-
hghani, Sunipa Dev, Jacob Devlin, Mark D´ıaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu
Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy
Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy
Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy,
Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li,
Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Mar-
cello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary
Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex
Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros,
Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov,
David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli,
Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yun-
han Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang
Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. PaLM 2 Technical Report, September 2023.
URL http://arxiv.org/abs/2305.10403. arXiv:2305.10403 [cs].

Andrei Z. Broder, Moses Charikar, Alan M. Frieze, and Michael Mitzenmacher. Min-wise inde-
In Proceedings of the thirtieth annual ACM symposium on Theory of

pendent permutations.
computing, pp. 327–336, 1998.

Eli Bursztein, Marina Zhang, Owen vallis, Xinyu Jia, and Alexey Kurakin. RetVec: Resilient and

Efficient Text Vectorizer. 2023.

Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Con-
stant, Mario Guajardo-Cespedes, Steve Yuan, and Chris Tar. Universal sentence encoder. arXiv
preprint arXiv:1803.11175, 2018.

Moses S. Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of

the thiry-fourth annual ACM symposium on Theory of computing, pp. 380–388, 2002.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep
Bidirectional Transformers for Language Understanding, May 2019. URL http://arxiv.
org/abs/1810.04805. arXiv:1810.04805 [cs].

Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. Language-
agnostic BERT Sentence Embedding, March 2022. URL http://arxiv.org/abs/2007.
01852. arXiv:2007.01852 [cs].

Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. Black-box Generation of Adversarial Text
Sequences to Evade Deep Learning Classifiers, May 2018. URL http://arxiv.org/abs/
1801.04354. arXiv:1801.04354 [cs].

10

Preprint

Mandy Guo, Zihang Dai, Denny Vrandeˇci´c, and Rami Al-Rfou. Wiki-40b: Multilingual language
model dataset. In Proceedings of the 12th Language Resources and Evaluation Conference, pp.
2440–2452, 2020.

Bikash Gyawali, Lucas Anastasiou, and Petr Knoth. Deduplication of Scholarly Documents us-
ing Locality Sensitive Hashing and Word Embeddings. In Proceedings of the Twelfth Language
Resources and Evaluation Conference, pp. 901–910, Marseille, France, May 2020. European Lan-
guage Resources Association. ISBN 979-10-95546-34-4. URL https://aclanthology.
org/2020.lrec-1.113.

Matthias Hagen, Martin Potthast, Marcel Gohsen, Anja Rathgeber, and Benno Stein. A large-
In Proceedings of the 40th International ACM SIGIR

scale query spelling correction corpus.
Conference on Research and Development in Information Retrieval, pp. 1261–1264, 2017.

Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time.

In

International Conference on Machine Learning, pp. 9099–9117. PMLR, 2022.

B. Issac, R. Chiong, and S. M. Jacob. Analysis of phishing attacks and countermeasures, 2014.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daum´e Iii. Deep Unordered
Composition Rivals Syntactic Methods for Text Classification.
In Proceedings of the 53rd
Annual Meeting of the Association for Computational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1681–1691, Bei-
jing, China, 2015. Association for Computational Linguistics. doi: 10.3115/v1/P15-1162. URL
http://aclweb.org/anthology/P15-1162.

Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating Training Data Mitigates Pri-
vacy Risks in Language Models.
In Proceedings of the 39th International Conference on
Machine Learning, pp. 10697–10707. PMLR, June 2022. URL https://proceedings.
mlr.press/v162/kandpal22a.html. ISSN: 2640-3498.

Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu˜noz Ferrandis,
Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von
Werra, and Harm de Vries. The Stack: 3 TB of permissively licensed source code, November
2022. URL http://arxiv.org/abs/2211.15533. arXiv:2211.15533 [cs].

Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-
Burch, and Nicholas Carlini. Deduplicating Training Data Makes Language Models Better, March
2022. URL http://arxiv.org/abs/2107.06499. arXiv:2107.06499 [cs].

Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.
A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 11976–11986, 2022.

John X. Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. TextAttack: A
Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP, Oc-
tober 2020. URL http://arxiv.org/abs/2005.05909. arXiv:2005.05909 [cs].

Niklas Muennighoff, Nouamane Tazi, Lo¨ıc Magne, and Nils Reimers. MTEB: Massive Text Em-
bedding Benchmark, October 2022. URL https://arxiv.org/abs/2210.07316v1.

Filip Radenovi´c, Giorgos Tolias, and Ondˇrej Chum. Fine-tuning CNN image retrieval with no
human annotation. IEEE transactions on pattern analysis and machine intelligence, 41(7):1655–
1668, 2018. Publisher: IEEE.

Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-

networks. arXiv preprint arXiv:1908.10084, 2019.

Florian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: A Unified Embedding for
Face Recognition and Clustering.
In 2015 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 815–823, June 2015. doi: 10.1109/CVPR.2015.7298682. URL http:
//arxiv.org/abs/1503.03832. arXiv:1503.03832 [cs].

11

Preprint

Emily Silcock, Luca D’Amico-Wong, Jinglin Yang, and Melissa Dell. Noise-Robust De-Duplication
at Scale, October 2022. URL http://arxiv.org/abs/2210.04261. arXiv:2210.04261
[cs].

Yifang Sun, Jianbin Qin, and Wei Wang. Near Duplicate Text Detection Using Frequency-Biased
Signatures. In David Hutchison, Takeo Kanade, Josef Kittler, Jon M. Kleinberg, Friedemann Mat-
tern, John C. Mitchell, Moni Naor, Oscar Nierstrasz, C. Pandu Rangan, Bernhard Steffen, Madhu
Sudan, Demetri Terzopoulos, Doug Tygar, Moshe Y. Vardi, Gerhard Weikum, Xuemin Lin, Yan-
nis Manolopoulos, Divesh Srivastava, and Guangyan Huang (eds.), Web Information Systems
Engineering – WISE 2013, volume 8180, pp. 277–291. Springer Berlin Heidelberg, Berlin, Hei-
delberg, 2013. ISBN 978-3-642-41229-5 978-3-642-41230-1. doi: 10.1007/978-3-642-41230-1
24. URL http://link.springer.com/10.1007/978-3-642-41230-1_24. Series
Title: Lecture Notes in Computer Science.

Ash Vardanian. USearch by Unum Cloud, October 2023. URL https://github.com/

unum-cloud/usearch.

Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Ma-
jumder, and Furu Wei. Text Embeddings by Weakly-Supervised Contrastive Pre-training, De-
cember 2022. URL http://arxiv.org/abs/2212.03533. arXiv:2212.03533 [cs].

Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, and Matthew R. Scott. Multi-similarity loss
with general pair weighting for deep metric learning. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, pp. 5022–5030, 2019.

Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya
Barua, and Colin Raffel. mT5: A massively multilingual pre-trained text-to-text transformer.
arXiv preprint arXiv:2010.11934, 2020.

Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, Gustavo Hernandez
Abrego, Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. Multilingual
Universal Sentence Encoder for Semantic Retrieval, July 2019. URL http://arxiv.org/
abs/1907.04307. arXiv:1907.04307 [cs].

Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan
Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep
learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962, 2019.

12

Preprint

A APPENDIX

A.1 RETSIM DETAILS

A.1.1 RETSIM MODEL HYPERPARAMETERS

The full list of RETSim model hyperparameters can be found in Table 9.

Hyperparameter

Max input length (per chunk)
Block type
# blocks
Hidden dim
Expansion rate
Activation function
Attention activation function
Absolute positional encoding
Relative positional encoding
Norm type
Pooling type
Dropout rate
Embedding dim
# Parameters

Value

512
GAU
2
256
1
Swish
relu2
ScaledSin
RoPE
ScaleNorm
GeM (p = 3)
0
256
536k

Table 9: Detailed RETSim model hyperparameters.

A.1.2 RETSIM TRAINING HYPERPARAMETERS

Table 10 details the hyperparameters settings for training configuration, loss, and optimizer used to
train the RETSim model.

Hyperparameter

Value

Batch size
Train steps
LAMB ϵ
LAMB β1
LAMB β2
Max learning rate
End learning rate
Learning rate decay
Weight decay

1024
1 million
1e-6
0.9
0.999
0.001
0
Cosine
0

Table 10: RETSim detailed training hyperparameters.

A.2 TRAINING DATASET DETAILS

Below, we provide the full list of augmentations used to generate augmented text for the RETSim
training dataset, as described in Section 3.2.

SENTENCE-LEVEL AUGMENTATIONS

• Deletion:

– Random sentence deletion
– Random sentence truncation

• Insertion:

13

Preprint

– Random prefix sentence
– Random suffix sentence
– Random sentence insertion
– Repeat sentence

• Substitution:

– Lowercase/uppercase sentence
– Random sentence substitution

• Transposition:

– Neighboring Swap

WORD-LEVEL AUGMENTATIONS

• Deletion:

– Random word deletion

• Insertion:

– Random word insertion
– Random word insertion per language

• Substitution:

– 3-gram frequency based word substitution
– Random word substitution
– Random word substitution per language
– Repeat word

• Transposition:

– Neighboring Swap

CHARACTER-LEVEL AUGMENTATIONS

• Deletion:

– Random character deletion

• Substitution:

– Case substitution
– n-gram based substitution for n = 3, 4, 5
– QWERTY keyboard typo substitution
– Homoglyphs substitution
– Random ASCII substitution
– Random character from language alphabet substitution
– Random punctuation substitution
– Random Unicode character substitution

• Insertion:

– Character repetition
– n-grams based insertion for n = 3, 4, 5
– Random character from language alphabet insertion
– Random punctuation insertion
– Random Unicode character insertion

• Transposition:

– Neighboring swap

A.3 DETAILED EVALUATION HYPERPARAMETERS

Figures 6 and 7 contain information on deduplication thresholds values and hyperparameter settings
for each algorithm benchmarked on the NEWS-COPY and CORE deduplication datasets.

14

Preprint

Model / Algorithm

Threshold Type

Threshold Value Hyperparameters

Multilingual USE
Cosine Similarity
Multilingual E5-Base Cosine Similarity
SimHash
MinHash (Ours)
RETSimNear-Dup
RETSimPartial-Dup

Hamming Distance
Jaccard Similarity
Cosine Similarity
Cosine Similarity

0.96
0.88
10
0.6
0.89
0.84

-
-
64 bits, 5-grams (character-level)
10 hash functions, 2-grams (word-level)
-
-

Figure 6: Hyperparameter settings for NEWS-COPY dataset evaluation in Section 4.3.

Model / Algorithm

Threshold Type

Threshold Value Hyperparameters

Cosine Similarity
LaBSE
Multilingual USE
Cosine Similarity
Multilingual E5-Base Cosine Similarity
SimHash + LSH
MinHash + LSH
RETSimNear-Dup
RETSimPartial-Dup

Hamming Distance
Jaccard Similarity
Cosine Similarity
Cosine Similarity

0.88
0.97
0.87
6
0.5
0.86
0.82

-
-
-
64 bits, 3-grams (character-level)
256 hash functions, 3-grams (word-level)
-
-

Figure 7: Hyperparameter settings for CORE Near-Duplicates dataset evaluation in Section 4.3.

A.3.1 DEDUPLICATION THRESHOLD IMPACT

Figure 8: Precision/Recall/F1 scores for different cosine distance deduplication thresholds for
RETSimNear-Dup (left) and RETSimPartial-Dup (right) on the NEWS-COPY dataset.

A.4 DETAILED W4NT3D BENCHMARK RESULTS

Tables 11 and 12 show detailed performance results for RETSim and all baseline algorithms for
every language split in the W4NT3D benchmark.

15

Preprint

9
4
9
.
0

4
8
9
.
0

6
8
9
.
0

1
0
7
.
0

3
9
5
.
0

4
1
8
.
0

1
7
9
.
0

1
9
9
.
0

o
k

1
3
9
.
0

0
9
9
.
0

9
7
9
.
0

8
7
5
.
0

5
6
4
.
0

3
2
2
.
0

3
6
9
.
0

6
8
9
.
0

a
j

9
2
9
.
0

7
3
9
.
0

3
3
9
.
0

7
3
9
.
0

3
1
5
.
0

5
7
5
.
0

4
4
9
.
0

2
8
9
.
0

5
1
9
.
0

8
2
9
.
0

2
4
9
.
0

4
2
9
.
0

3
3
5
.
0

3
6
5
.
0

0
5
9
.
0

6
7
9
.
0

8
9
8
.
0

5
8
8
.
0

9
8
8
.
0

6
7
8
.
0

0
3
5
.
0

2
1
5
.
0

0
5
9
.
0

2
6
9
.
0

2
1
9
.
0

9
8
8
.
0

1
0
9
.
0

3
0
9
.
0

7
5
5
.
0

6
5
5
.
0

4
3
9
.
0

2
6
9
.
0

7
2
9
.
0

8
4
6
.
0

4
6
9
.
0

5
3
4
.
0

3
6
5
.
0

6
0
6
.
0

1
4
9
.
0

5
7
9
.
0

7
3
9
.
0

1
4
8
.
0

8
5
9
.
0

9
8
5
.
0

1
5
6
.
0

7
9
6
.
0

5
4
9
.
0

9
8
9
.
0

0
3
9
.
0

8
3
9
.
0

8
4
9
.
0

4
3
9
.
0

9
1
5
.
0

6
5
5
.
0

7
4
9
.
0

3
8
9
.
0

6
2
9
.
0

3
1
9
.
0

3
4
9
.
0

4
1
9
.
0

2
3
6
.
0

8
6
5
.
0

6
5
9
.
0

1
8
9
.
0

2
1
9
.
0

0
7
8
.
0

9
2
9
.
0

6
5
3
.
0

8
6
5
.
0

5
9
5
.
0

6
4
9
.
0

1
7
9
.
0

3
2
9
.
0

0
3
9
.
0

1
5
9
.
0

6
2
9
.
0

5
6
6
.
0

5
8
5
.
0

3
5
9
.
0

5
8
9
.
0

8
8
8
.
0

3
0
9
.
0

6
9
8
.
0

1
1
9
.
0

6
9
4
.
0

4
0
5
.
0

8
3
9
.
0

6
7
9
.
0

4
4
9
.
0

8
5
9
.
0

9
5
9
.
0

1
6
9
.
0

1
9
5
.
0

1
9
5
.
0

4
5
9
.
0

7
8
9
.
0

8
1
9
.
0

4
7
8
.
0

4
3
9
.
0

9
8
5
.
0

7
5
5
.
0

4
7
5
.
0

5
3
9
.
0

4
7
9
.
0

1
3
9
.
0

2
4
9
.
0

4
4
9
.
0

2
3
9
.
0

1
6
5
.
0

8
5
5
.
0

9
4
9
.
0

8
7
9
.
0

8
3
9
.
0

7
2
9
.
0

6
5
9
.
0

3
4
9
.
0

2
9
5
.
0

8
9
5
.
0

3
5
9
.
0

8
8
9
.
0

2
1
9
.
0

0
0
9
.
0

7
2
9
.
0

1
1
9
.
0

9
7
5
.
0

1
8
5
.
0

0
5
9
.
0

3
7
9
.
0

7
9
8
.
0

3
8
8
.
0

0
9
8
.
0

2
0
9
.
0

5
8
4
.
0

8
0
5
.
0

4
2
9
.
0

8
6
9
.
0

5
1
9
.
0

9
0
9
.
0

9
9
8
.
0

1
5
8
.
0

0
1
5
.
0

6
0
5
.
0

2
4
9
.
0

6
7
9
.
0

5
1
9
.
0

5
1
9
.
0

6
3
9
.
0

7
9
4
.
0

8
5
5
.
0

3
3
6
.
0

8
2
9
.
0

1
7
9
.
0

e
s
a
B
-
5
E

l
a
u
g
n
i
l
i
t
l
u
M

E
S
U

l
a
u
g
n
i
l
i
t
l
u
M

)
o
k
c
e
G

(

2
M
L
a
P

E
S
B
a
L

p
u
D

-
l
a
i
t
r
a
P
m
S
T
E
R

i

p
u
D

i

-
r
a
e
N
m
S
T
E
R

h
s
a
H
m
S

i

h
s
a
H
n
i
M

t
i

d
i

u
h

r
h

i
h

e
h

r
f

fi

a
f

t
e

s
e

n
e

l
e

e
d

a
d

s
c

a
c

g
b

r
a

m
h
t
i
r
o
g
l
A

/

l
e
d
o
M

.
)
1

t
r
a
p
(
k
r
a
m
h
c
n
e
b
D
3
T
N
4
W

e
h
t

n
o

s

m
h
t
i
r
o
g
l
a

d
n
a

s
l
e
d
o
m
g
n
i
d
d
e
b
m
e

s
u
o
i
r
a
v

r
o
f

e
c
n
a
m
r
o
f
r
e
p

1
@

l
l
a
c
e
R
e
g
a
u
g
n
a
l
-
r
e
p
l
l
u
F

:
1
1
e
l
b
a
T

w

t
-
h
z

n
c
-
h
z

i
v

k
u

r
t

l
t

h
t

v
s

r
s

l
s

k
s

u
r

o
r

t
p

l
p

o
n

l
n

s

m

v
l

t
l

m
h
t
i
r
o
g
l
A

/

l
e
d
o
M

16

8
1
9
.
0

5
8
9
.
0

8
7
9
.
0

9
0
6
.
0

5
1
3
.
0

0
0
2
.
0

7
5
9
.
0

8
6
9
.
0

7
1
9
.
0

6
8
9
.
0

0
8
9
.
0

3
2
6
.
0

6
7
2
.
0

2
7
1
.
0

6
4
9
.
0

1
7
9
.
0

2
3
9
.
0

0
1
9
.
0

5
4
9
.
0

3
6
8
.
0

9
0
6
.
0

1
8
5
.
0

1
5
9
.
0

5
8
9
.
0

9
9
8
.
0

3
9
8
.
0

2
7
8
.
0

2
2
8
.
0

7
1
5
.
0

0
2
5
.
0

1
4
9
.
0

7
5
9
.
0

0
3
9
.
0

0
4
9
.
0

1
5
9
.
0

2
0
9
.
0

6
0
6
.
0

3
8
5
.
0

4
5
9
.
0

8
7
9
.
0

7
4
9
.
0

9
4
9
.
0

9
6
9
.
0

4
4
9
.
0

7
0
5
.
0

0
7
5
.
0

1
6
9
.
0

9
8
9
.
0

2
8
8
.
0

8
8
8
.
0

1
2
9
.
0

1
7
5
.
0

9
6
6
.
0

6
1
4
.
0

1
4
9
.
0

6
4
9
.
0

6
0
9
.
0

9
9
8
.
0

5
2
9
.
0

9
1
9
.
0

2
5
5
.
0

0
2
5
.
0

3
5
9
.
0

9
6
9
.
0

0
3
9
.
0

6
0
9
.
0

6
9
8
.
0

6
5
8
.
0

3
5
5
.
0

5
2
5
.
0

3
6
9
.
0

9
7
9
.
0

1
3
9
.
0

8
0
9
.
0

0
4
9
.
0

6
1
9
.
0

0
8
5
.
0

0
7
5
.
0

7
4
9
.
0

7
7
9
.
0

2
2
9
.
0

1
0
9
.
0

1
2
9
.
0

4
2
9
.
0

5
7
5
.
0

3
7
5
.
0

1
6
9
.
0

1
7
9
.
0

8
1
9
.
0

0
1
9
.
0

1
1
9
.
0

1
5
8
.
0

4
5
5
.
0

3
2
5
.
0

6
4
9
.
0

0
7
9
.
0

9
0
9
.
0

9
6
8
.
0

6
3
9
.
0

3
9
8
.
0

4
1
5
.
0

0
2
5
.
0

8
4
9
.
0

7
7
9
.
0

4
4
9
.
0

2
5
9
.
0

5
5
9
.
0

0
5
9
.
0

8
4
5
.
0

3
7
5
.
0

4
5
9
.
0

5
8
9
.
0

8
2
9
.
0

1
3
9
.
0

8
2
9
.
0

3
1
9
.
0

6
8
5
.
0

3
6
5
.
0

3
5
9
.
0

9
7
9
.
0

8
2
9
.
0

1
2
9
.
0

4
4
9
.
0

0
3
9
.
0

7
7
5
.
0

0
6
5
.
0

8
5
9
.
0

1
8
9
.
0

1
3
9
.
0

6
3
9
.
0

4
3
9
.
0

1
3
9
.
0

7
2
5
.
0

0
4
5
.
0

0
5
9
.
0

9
7
9
.
0

9
1
9
.
0

2
3
9
.
0

9
4
9
.
0

8
2
9
.
0

3
3
5
.
0

1
1
5
.
0

1
6
9
.
0

0
8
9
.
0

2
2
9
.
0

9
1
9
.
0

5
3
9
.
0

7
0
9
.
0

4
2
6
.
0

9
7
5
.
0

5
4
9
.
0

3
8
9
.
0

9
1
9
.
0

2
0
9
.
0

1
4
9
.
0

9
0
9
.
0

9
0
6
.
0

8
6
5
.
0

7
5
9
.
0

0
8
9
.
0

e
s
a
B
-
5
E

l
a
u
g
n
i
l
i
t
l
u
M

E
S
U

l
a
u
g
n
i
l
i
t
l
u
M

)
o
k
c
e
G

(

2
M
L
a
P

E
S
B
a
L

p
u
D

-
l
a
i
t
r
a
P
m
S
T
E
R

i

h
s
a
H
m
S

i

h
s
a
H
n
i
M

p
u
D

i

-
r
a
e
N
m
S
T
E
R

.
)
2

t
r
a
p
(
k
r
a
m
h
c
n
e
b
D
3
T
N
4
W

e
h
t

n
o

s

m
h
t
i
r
o
g
l
a

d
n
a

s
l
e
d
o
m
g
n
i
d
d
e
b
m
e

s
u
o
i
r
a
v

r
o
f

e
c
n
a
m
r
o
f
r
e
p

1
@

l
l
a
c
e
R
e
g
a
u
g
n
a
l
-
r
e
p
l
l
u
F

:
2
1
e
l
b
a
T

Preprint

A.5 ADDITIONAL ABLATION STUDIES

This section includes ablation studies on additional hyperparameters for the RETSim model, includ-
ing the loss function, pooling type, and model capacity.

α β

2
2
2
2
4
4
4
4

20
20
40
40
20
20
40
40

λ

0.5
1
0.5
1
0.5
1
0.5
1

Recall@1

0.982
0.948
0.984
0.919
0.982
0.947
0.986
0.923

Table 13: Ablation study on Multi-Similarity Loss hyperparameters for RETSim training. Bold
indicates the hyperparameter setting selected for the final model.

# Blocks Hidden Dim Recall@1

2
2
2
2
3
3
3
3
4
4
4
4

64
128
256
512
64
128
256
512
64
128
256
512

0.965
0.980
0.986
0.986
0.962
0.980
0.984
0.987
0.966
0.980
0.985
0.986

Table 14: Ablation study for RETSim model capacity and size (number of GAU blocks and hidden
dimension for the blocks). Bold indicates the hyperparameter setting selected for the final model.

Pooling Type

Recall@1

Average Pooling
Max Pooling
Generalized Mean Pooling

0.985
0.983
0.986

Table 15: Ablation study on pooling type for the RETSim model. Bold indicates the hyperparameter
setting selected for the final model.

A.6 SELECTED EXAMPLES FROM NEWS-COPY DATASET

In this section, we randomly selected a set of false positives and false negatives for RETSim on the
NEWS-COPY deduplication dataset to provide further insight into the results.

17

Preprint

Text 1
chauffeur, a policeman and a passing jour-
nalist who tried to intervene. Beaton and the
policeman were reported in serious condition.
The 23-year-old princess and her husband of
five months, Capt. Mark Phillips, were not
hurt. But police experts said the holes left by
one of the bullets fired into the car indicated it
passed between them, missing them by inch-
es. A police informant said it was believed 11
shots were fired by the assailant. Experts were
studying two revolvers found at the scene.
They said fi...
By United Press tnfernational Ay SSAST OR
BE FRE NG SG The federal government has
proposed new methods of eoustructing federal
buildings in a move to save ad- ditional en-
ergy and suggested ils elfort could be adapted
to all new buildings,

Washington, Jan. 27. —(P)—Im- mediate
removal of John F. J. Her- bert, as prohibition
administrator for Montana and Idaho, was
de- manded in the senate today by Sen- ators
Borah, Idaho, and Whe´eeler, Montana, on
the ground of charges placed before them by
department of justice investigators. Wheeler
accompanied his demand (Continued on Page
2)

By RAYMOND CLAPPEA (Dnited
Presa Stal Correspandoayy London, Jai,
38—(UP—-The Am ‘erlcnn delegation to
the navat confer ence today won ls demand
for pre- sentation: of the cnse of suxiliary
warships limitation first at tho noxt plenary
session Thuvaday, ‘Tho chlet delegates, mec-
tittg at St. James palace, also decided that tho
plenary sesslon would discuss the Main con-
ference questions in alpha betical order of ihe
countriea pro- posing. Press ta be Admitted
The American delegation woo a second vic-
tory whe...

Text 2
‘LONDON (AP) — Ian Ball, a 26-year- old
unemployed Englishman, was brought into
court today and charged with attempted mur-
der during an at- tempt to kidnap Princess
Anne from her car in the heart of London
Wed- nesday night. Ball, lean-faced and
bearded, stood stiffly in the dock at the Bow
Street Magistrate’s court, handcuffed to two
detectives. He spoke only once during his 60-
second appearance, saying iha London accent:
“I want to apply for legal aid.” The court or-
dered him held for another hearing on Ma...
Hy United Press International The federal
government has Proposed new methods of
constructing federal buildings in a move lo
save addilional energy and suggested ils effort
could be adapted to all new buildings, Arthur
F, Sampson, General Services Administration
ad- ministrater, said new features for such
construction would include the collection of
rain waler for cooling and irriga- tion, solar
energy collectors and the covering of exterior
walls with earth. “Whal we are saying is that
these design criteri...
— Washington, Jan. 27 1 AP).—Immiedl-
aie mmoval of John F. Herbert as pro- — hi-
bition administrator for Montana and ‘Idaho
was demanded m the Seuate to- ‘day by Sen-
ators Borah. idaho, and Waeeler, Montana. on
the ground of charges placed before them by
Depart- meat of Justice investigators. Wheeler
accompanied his demand nith a declaration
that prohibition en- foreemen: had brukea
down. He blamed the “politicians” and called
upon the Law Enforcement Commussion to
sum- mon members of the Republican Na-
tona...
London, Jan. 24, W.P—The Amer- jean dele-
gation fo the naval cen- ference teday won its
demand for presentation of the case of auxil-
jary warships linsitation flrst at the next ple-
trary session ‘Vhursday, The chief delegates,
meeting at Si, James Pelace, also decided that
the plenary session would discuss the main
confeyence questions in alphabetical order af
the cauntries proposing. The American del-
egation won a second victory when it was
decided to udmil certain representatives of the
press at fie plenary...

Table 16: Example false negatives for RETSim on the NEWS-COPY dataset (pairs of texts not
detected as near-duplicates by RETSim but labeled as near-duplicates in the original dataset). Ex-
amples are randomly selected and truncated at 512 characters for display.

18

Preprint

Text 1
BOZEMAN, Mont. (AP) — Chet Huntley,
whose resonant voice and rough-hewn face
be- came familiar to millions on the nightly
television news, died Wednesday in his moun-
tain resort home. He was 62. He underwent
surgery for lung cancer in January but had
remained activesuntil recent weeks. He died
at 2:20 a.m, according to his widow, Tippy
Hunt.cy. Huntiey was teamed for 14 years
with David Brinkley on NBC’s Huntley-
Brinkley Re- port. He quit in 1970 and re-
turned to his native Montana to develop the
$20-millio...
By THE ASSOCIATED PRESS Some Amer-
icans are paying up to 50 per cent more per
month for electricity this year than they did
last, an Associ- ated Press survey shows. Con-
sumers are beginning to organize to fight the
rate hikes. A spot check of monthly elec- tric
bills this year and last showed that most in-
creases have been about $1 or $2, gen- erally
about 10 per cent, with the highest reported
boost com- ing in Jacksonville, Fia., where
the average tab went from $17.90 last year to
$27.70 this year. Utility...
BOZEMAN, Mont. (AP) — Vice President
Gerald R. Ford says the world will miss the
“‘unique abilities” of former television news
anchorman Chet Huntley. Huntley, 62, died
at his home Wednesday after a long bout
with lung cancer. Family ‘spokesmen said
a memorial service would be conducted for
Huntley Sunday at the Big Sky of Montana’
resort and recreation area south of Bozeman.
Huntley was chairman of the Big Sky board
of directors. Another memorial service is
scheduled Tuesday in the New York studios
of the...
WASHINGTON (AP) — The House has
passed legislation raising the minimum wage
from $1.60 an hour to $2 this year for most
workers covered and to $2.30 for all by 1978.
The bill, approved Wednesday 375 to 37, also
would increase by 7 million to 56.5 million
the number of workers covered by the mini-
mum wage laws. The bill is a modified ver-
sion of one President Nixon vetoed last year.
However, he is expected to sign this one if it
is finally approved after ad- justment with a
similar Senate passed measure, altho...

Text 2
BOZEMAN, Mont. (AP) - Chet Huntley,
whose resonant voice and rough-hewn face
became familiar to millions on the nightly
television news, died Wednesday in his moun-
tain resort home. He was 62. He underwent
surgery for lung cancer in January but had
remained active until recent weeks. He died
at 2:20 a.m., according to his widow, Tippy
Huntley. Huntley was teamed for 14 years
with David Brinkley on NBC’s Huntley-
Brinkley Report. He quit in 1970 and returned
to his native Montana to develop the $20 mil-
lion Bi...
By Louise Cook Acenciaiod Prece Writer
Same Americans are paying up io 20 per cent
more per month far electricity this year ihan
they did last, an -Associ- Press survey shows.
onsumers are beginning to ze to fight the rate
hikes, A spot check of monthly elec- tre hills
this year and Jast showed that most increases
ve been about $1 or $2, gen- erally about 10
per cent, with the highest reported boost com-
ing in Jacksonville, Fla., where the average
tab went from $17.90 last year to $27.70 this
year...
BOZEMAN, Mont. (AP) — Vice President
Gerald R. Ford says the world will miss the
“unique abilities” of former television news
anchorman Chet Huntley. Huntley, 62, died
at his home Wednesday after a long bout with
lung cancer. Family spokesmen said a me-
morial service would be con- ducted for Hunt-
ley Sunday at the Big- Sky of Montana resort
and recreation area south of Bozeman. Hunt-
ley was chair- man of the Big Sky board of
directors. Another memorial service is sched-
uled Tuesday in the New York studios of...

WASHINGTON (AP) — The House has
passed legislation raising the minimum wage
from $1.60 an hour to $2 this year for most
workers covered and to $2.30 for all by 1978.
The bill, approved Wednes- day 375 to 37,
also would in- crease by 7 million to 56.5 mil-
lion the number of workers cov- ered by the
minimum wage laws. The bill is a modified
version of one President Nixon vetoed last
year. However, he is ex- ted to sign this one if
it is inally approved after adjust- ment with a
similar Senate- passed measu...

Table 17: Example false positives for RETSim on the NEWS-COPY dataset (pairs of texts detected
as near-duplicates by RETSim but not labeled as near-duplicates in the original dataset). Examples
are randomly selected and truncated at 512 characters for display.

19

","3 2 0 2 v o N 8 2 ] L C . s c [ 1 v 4 6 2 7 1 . 1 1 3 2 : v i X r a Preprint RETSIM : RESILIENT AND EFFICIENT TEXT SIMILARITY Marina Zhang1 , Owen Vallis1 , Aysegul Bumin * 2 , Tanay Vakharia1 , Elie Bursztein1 Google1 University of Florida2 ABSTRACT This paper introduces RETSim ( Resilient and Efficient Text Similarity ) , a lightweight , multilingual deep learning model trained to produce robust metric embeddings for near-duplicate text retrieval , clustering , and dataset deduplication tasks . We demonstrate that RETSim is significantly more robust and accurate than MinHash and neural text embeddings , achieving new state-of-the-art perfor- mance on dataset deduplication , adversarial text retrieval benchmarks , and spam clustering tasks . We also introduce the W4NT3D benchmark ( Wiki-40B 4dver- sarial Near-T3xt Dataset ) for evaluating multilingual , near-duplicate text retrieval capabilities under adversarial settings . RETSim and the W4NT3D benchmark are open-sourced under the MIT License at https : //github.com/google/unisim . 1 INTRODUCTION Robust near-duplicate text detection is an essential component of many tasks , including retriev- ing documents , detecting plagiarism ( Sun et al. , 2013 ) and blocking adversarial spam cam- paigns ( Ahmed et al. , 2022 ) . Users have come to expect that systems can return accurate results despite their queries exhibiting a 20 % to 30 % typo rate ( Hagen et al. , 2017 ) . Furthermore , effi- ciently deduplicating text datasets is critical to training state-of-the-art large language models ( Lee et al. , 2022 ; Kandpal et al. , 2022 ) . For more than two decades , MinHash-based ( Broder et al. , 1998 ) locality-sensitive hashing ( LSH ) has been the most prevalent algorithm used for near-duplicate detection due to its simplicity , robust- ness , and speed . For example , the vast majority of dataset deduplication efforts still rely on MinHash ( Lee et al. , 2022 ; Kocetkov et al. , 2022 ) . However , like all LSH-based techniques , MinHash is not without downsides ; chief among them being that it is very parameter-sensitive and requires heavy tuning . Additionally , MinHash lacks resilience to typos due to its reliance on n-grams , leading to poor performance on noisy data and a vulnerability to hash-busting attacks ( Issac et al. , 2014 ) . On the other hand , deep learning models are the dominant way to perform vector-based semantic text retrieval ( Muennighoff et al. , 2022 ) , but so far , no neural embedding has been able to consistently outperform MinHash for robust near-duplicate detection ( Silcock et al. , 2022 ) . This is mostly due to the focus on improving semantic capabilities , which leads models to be too large to run extremely quickly and the use of sub-word level tokenization , which is not resilient to typos and adversarial attacks ( Morris et al. , 2020 ; Bursztein et al. , 2023 ) . To fill this gap , we introduce RETSim ( Resilient and Efficient Text Similarity ) , a lightweight , mul- tilingual deep learning model trained specifically to produce robust neural embeddings specialized for near-duplicate detection . By combining the state-of-the-art RETVec text vectorizer , a modern transformer block ( Hua et al. , 2022 ) , a large typo-augmented training corpus , and a metric learn- ing training regime , RETSim is able to achieve new state-of-the-art performance on near-duplicate detection benchmarks ( Section 4.2 ) , dataset deduplication tasks ( Sections 4.3 and 5.1 ) , and spam clustering applications ( Section 5.2 ) . Furthermore , while datasets and benchmarks exist for corpus deduplication and near-duplicate text retrieval , none of these have focused on systematically evaluating near-duplicate retrieval perfor- mance under the presence of typos , word manipulations , and sentence or paragraph-level modifica- * This work was done during the author ’ s internship at Google . 1 Preprint tions . To address this need , we additionally introduce the W4NT3D benchmark ( Wiki-40B 4dver- sarial Near-T3xt Dataset ) which enables the evaluation of algorithms on adversarial near-duplicate text retrieval in a multilingual setting . We report the performance of RETSim , MinHash , and pop- ular neural embeddings such as Universal Sentence Encoder ( Cer et al. , 2018 ) and LaBSE ( Feng et al. , 2022 ) on this new benchmark in Section 4.2 , highlighting uneven performance across lan- guages and types of adversarial manipulations . The RETSim model and the W4NT3D benchmark are open-sourced at https : //github.com/google/unisim under the MIT License . 2 RELATED WORK Near-Duplicate Detection Identifying noisy near-duplicate documents in a large corpus is a fun- damental task with a wide range of applications , such as detecting plagiarism , finding reproduced content in literature or news articles ( Gyawali et al. , 2020 ; Silcock et al. , 2022 ) , and deduplicat- ing training datasets for language models . Previous research has shown that duplicates in training datasets lead to inefficient training ( Lee et al. , 2022 ) and privacy concerns for large language models ( LLMs ) , where models memorize and regenerate duplicated training sequences at a much higher frequency ( Kandpal et al. , 2022 ) . Unlike semantic text similarity , the task of identifying textual near-duplicates has been predominated by non-neural , n-gram-based algorithms such as MinHash ( Broder et al. , 1998 ) , which is the most widely used technique for deduplicating large training corpuses ( Kocetkov et al. , 2022 ; Lee et al. , 2022 ) . MinHash is a technique for estimating the Jaccard similarity between two sets . Algorithms such as MinHash or SimHash ( Charikar , 2002 ) can be combined with locality-sensitive hashing ( LSH ) techniques for fast , approximate nearest neighbor search and data clustering . This allows them to scale and deduplicate corpuses containing terabytes of data such as C4 ( Lee et al. , 2022 ) and The Stack ( Kocetkov et al. , 2022 ) . However , n-gram or shingling-based techniques typically require texts to be parsed into a standardized form ( e.g . by lower-casing or stripping punctuation ) , which makes them susceptible to typos and adversarial attacks and pose a challenge when attempt- ing to differentiate between dissimilar documents and near-duplicate documents with adversarial augmentations . Semantic Text Similarity The task of computing semantic similarity between text is closely re- lated to near-duplicate detection . Semantic text similarity refers to the assessment of the semantic relatedness of two pieces of text based on their meaning rather than their syntactic structure , as in the case of near-duplicate detection . Recently , transformer-based language models such as Universal Sentence Encoder ( Yang et al. , 2019 ) , LaBSE ( Feng et al. , 2022 ) and LLM-based embeddings ( Anil et al. , 2023 ) which embed text into high-dimensional embedding vectors have been successfully used to retrieve semantically-related documents using cosine similarity . Modern text retrieval sys- tems combine these embeddings with an approximate nearest neighbor ( ANN ) search algorithm to efficiently retrieve documents matching user queries . However , language models have been shown to be vulnerable to adversarial attacks and naturally- occurring typos ( Alzantot et al. , 2018 ; Gao et al. , 2018 ; Morris et al. , 2020 ) . Furthermore , language models are typically very large and costly to run even with hardware acceleration , which makes them unsuited for large-scale dataset deduplication or identifying near-duplicates in the presence of typos or adversarial text manipulations . Metric Learning Metric learning aims to learn an embedding space where similar items have a small distance between their embeddings and dissimilar items are further away . Many state-of- the-art embeddings use metric learning for unsupervised training or fine-tuning including Sentence- BERT ( Reimers & Gurevych , 2019 ) and E5 ( Wang et al. , 2022 ) . RETVec is a resilient , multilingual embedding and text vectorizer trained to be robust against various forms of character-level typos and adversarial attacks . We extend the RETVec training regime to full text documents for RETSim . We use Multi-Similarity Loss ( Wang et al. , 2019 ) for pair-based metric learning , where typo-laden and near-duplicate versions of texts are trained to be closer in the embedding space , while other texts are pushed further away . Multi-Similarity Loss is based on a general weighting framework for pair-based losses and achieves state-of-the-art performance , outperforming alternatives such as Triplet Loss ( Schroff et al. , 2015 ) . 2 Preprint Figure 1 : RETSim model architecture diagram . RETSim works on arbitrary length text by split- ting texts into chunks of 512 characters during its vectorization phase and encodes them using the RETVec character vectorizer . The RETSim model then embeds each chunk of text into 256-dim partial embeddings and combines them to produce the global embedding . 3 RETSIM 3.1 ARCHITECTURE The RETSim model is composed of three main components ( as depicted in Figure 1 ) : The character-level vectorizer splits the input text into chunks of 512 characters , then uses the RETVec chararcter encoder ( Bursztein et al. , 2023 ) to encode each chunk , resulting in a batch of ( 512 , 24 ) dense inputs . The RETVec character vectorizer encodes each Unicode character as a compact 24-bit binary representation based on its integer codepoint value . This allows the vectorizer to encode all valid Unicode characters and support all languages . Furthermore , the character-level vectorizer has been shown to be more resilient against typos and adversarial attacks . A small transformer model is used to compute 256-dimension embeddings for each chunk of the input text . RETSimPartial-Dup uses these embeddings directly to finding documents that have matching chunks of text . Architecturally , the model consists of two Gated Attention Unit ( GAU ) blocks ( Hua ( Radenovi´c et al. , 2018 ) , a dense et al. , 2022 ) , followed by a Generalized-Mean pooling layer projection layer which projects the embedding into 256 dimensions , and an L2 normalization layer . The model has only 536k parameters , which is more than two orders of magnitude smaller than other neural embeddings ( Table 1 ) . L2-normalization allows the embeddings to be compared using cosine similarity . We discuss the impact of key architecture design choices in Section 6 . Hyperparameter details are provided in Appendix A.1.1 , and additional ablations results in Appendix A.5 . An embedding averaging module is then used to combine partial text embeddings into a full-text embedding which is used for global near-duplicate matching ( RETSimNear-Dup ) . Averaging chunked embeddings to produce a global embedding is a standard technique used by many models ( Cer et al. , 2018 ) to support infinite length inputs in a cost-efficient manner . We experimented with other aggregation techniques to produce more accurate global embeddings , including training a deep- averaging network ( Iyyer et al. , 2015 ) , but this did not improve performance and resulted in higher computation cost . RETSimNear-Dup and RETSimPartial-Dup are computed in a single forward pass which makes it computationally efficient . We output both types of embeddings as they have different applications : RETSimNear-Dup is better-suited for full-text matching and retrieval ( Section 4.2 ) , while RETSimPartial-Dup is used to find partial text matches where the near-duplicate content appears only in part of the document ( Section 4.3 ) . 3.2 MODEL TRAINING Dataset We use the multilingual C4 dataset ( mC4 ) for raw text data and following ( Xue et al. , 2020 ) , we use a language sampling exponent of α = 0.3 to balance sampling between low and high-resource languages . We only use text containing at least 16 characters , and we randomly select between 1 and 8 sentences ( roughly 512 characters ) for each text chunk . For each example in the 3 Input text Chunk vector Chunk vector .... Chunk vector Chunked and vectorized text ( num_chunks , 512 , 24 ) RETSim model RetSim Partial-Dup ( num_chunks , 256 ) RetSim Near-Dup ( 256 ) r e d o c n E r a h C c e v T E R U A G U A G g n i l o o P 2 L + e s n e D t x e t l a i t r a P s g n i d d e b m e 2 L + g n i g r a r e v A g n i d d e b m e t x e t l l u F Preprint training dataset , we generate 5 pairs of augmented examples . We apply three levels of augmentation to each example text chunk ( in this order ) : sentence-level , word-level , and character-level . For each level , we randomly select the augmentation to be applied from the following categories : insertion , deletion , substitution , and transposition . We randomly apply between 0 − 25 % sentence-level aug- mentation and up to 30 % combined character and word-level augmentation . Empirically , we found that increasing the percentage of augmentation beyond this point causes RETSim ’ s performance to degrade . The full list of augmentations used can be found in Appendix A.2 . Training Procedure We train RETSim using Multi-Similarity Loss ( Wang et al. , 2019 ) with α = 4 , β = 40 , λ = 0.5 , and ϵ = 0.1 . We hypertuned these parameters and the results are shown in Appendix A.5 . We train for 1 million steps with batch size = 1024 . The similarity loss trains the model to embed augmented versions of the same text closer in the embedding space , while dissimilar texts are pushed further apart . We use the LAMB optimizer ( You et al. , 2019 ) with a max learning rate of 0.001 and cosine decay . Detailed training hyperparameters are reported in Appendix A.1.2 . 4 EVALUATION Model/Algorithm Type Embed./Hash Size # Model Parameters LaBSE Multilingual USE Multilingual E5-Base PaLM 2 ( Gecko ) SimHash MinHash RETSim Neural Neural Neural Neural Hashing Hashing Neural 768 512 768 768 b bits n hashes 256 471M 69M 278M ? N/A N/A 536k Table 1 : Embedding models and hashing algorithms benchmarked in the paper . 4.1 MODELS AND ALGORITHMS EVALUATED We benchmark RETSim against four multilingual semantic text embeddings as well as popular n- gram based algorithms primarily used in near-duplicate text detection ( Table 1 ) . Our baseline text embeddings include Multilingual Universal Sentence Encoder ( Yang et al. , 2019 ) , LaBSE ( Feng et al. , 2022 ) , Multilingual E5 ( Wang et al. , 2022 ) , and PaLM 2 Gecko Embeddings ( Anil et al. , 2023 ) . All text embeddings are L2-normalized and compared using cosine similarity . We use exact search to index and retrieve nearest neighbors from our vector index for the experiments in Section 4 . For non-neural near-duplicate detection and clustering algorithms , we selected the two most popular algorithms : MinHash ( Broder et al. , 1998 ) and SimHash ( Charikar , 2002 ) . For MinHash , we use Datasketch ’ s MinHashLSH library . Following the most common practices in the literature ( Silcock et al. , 2022 ) , we use 10 hash functions for MinHash unless otherwise specified . We use word-level n-grams where we select the best value out of n = { 2 , 3 , 4 , ... , 10 } . For SimHash , we use 64- bit SimHash and conduct shingling at the character level , where the shingle size is selected from n = { 2 , 3 , 4 , ... , 10 } . For the near-duplicate detection benchmarks ( NEWS-COPY and CORE Near- Duplicates datasets ) , we tune the optimal deduplication threshold ( e.g . based on cosine similarity for neural-based embeddings and Jaccard similarity for MinHash ) . Detailed hyperparameter settings for RETSim and baseline algorithms used in the evaluation can be found in Appendix A.3 . 4.2 W4NT3D : WIKI-40B 4DVERSARIAL NEAR-T3XT DATASET EVALUATION Dataset Description The vast majority of text retrieval benchmarks are focused on evaluating semantic performance . To the best of our knowledge , there is no multilingual benchmark for sys- tematically measuring adversarial robustness for near-duplicate text retrieval . In an attempt to fill in the gap , we create and publish the W4NT3D benchmark ( Wiki-40B 4dversarial Near-T3xt Dataset ) , which contains around 400k pairs of syntactically similar texts to evaluate near-duplicate text re- trieval in the presence of various forms of text manipulations and typos . W4NT3D is based on the Wiki-40B dataset ( Guo et al. , 2020 ) . The dataset is split into query exam- ples and target examples , where query examples are synthetically-modified near-duplicate versions 4 Preprint of a target example ( e.g . with typos ) . For each of the 41 language splits in Wiki-40B , we randomly select 10,000 texts . The length of the target string is uniformly selected from between 16 and 8192 characters , in order to test performance on short and long text . To construct the query text corre- sponding to a target text , we randomly apply up to 25 % word and character augmentations , and up to 25 % sentence and paragraph augmentations . For each augmentation , we uniformly select from the [ insert , delete , substitute , and swap ] operations . We use Recall @ k with k = 1 as the main metric , following the setup commonly found in semantic text retrieval benchmarks . Model/Algorithm Arabic Chinese English German French Spanish Japanese Korean Russian Thai Avg ( 41 Langs ) LaBSE Multilingual USE Multilingual E5-Base PaLM 2 ( Gecko ) SimHash MinHash RETSimPartial-Dup RETSimNear-Dup 0.915 0.915 0.936 0.497 0.558 0.633 0.928 0.971 0.917 0.986 0.980 0.623 0.276 0.172 0.946 0.971 0.944 0.958 0.959 0.961 0.591 0.591 0.954 0.987 0.931 0.942 0.944 0.932 0.561 0.558 0.949 0.978 0.930 0.938 0.948 0.934 0.519 0.556 0.947 0.983 0.888 0.903 0.896 0.911 0.513 0.575 0.938 0.976 0.931 0.990 0.979 0.578 0.465 0.223 0.963 0.986 0.949 0.984 0.986 0.701 0.593 0.814 0.971 0.991 0.918 0.910 0.911 0.851 0.554 0.523 0.946 0.970 0.882 0.888 0.921 0.571 0.669 0.416 0.941 0.946 0.921 0.912 0.937 0.823 0.550 0.538 0.949 0.977 Table 2 : Per-language retrieval performance for various embedding models and algorithms on the W4NT3D benchmark . Results on selected languages are reported alongside the average Recall @ 1 for all 41 languages . Full results for all languages are reported in Appendix A.4 . Multilingual Performance Overall , RETSimNear-Dup achieves an average Recall @ 1 of 0.977 across all 41 languages on the W4NT3D benchmark ( Table 2 ) . RETSimPartial-Dup is second best with a Recall @ 1 of 0.949 and Multilingual E5 , the best-performing baseline , is third with an aver- age Recall @ 1 of 0.932 . We expect that RETSimNear-Dup outperforms RETSimPartial-Dup because the W4NT3D benchmark requires an algorithm to not just find near-duplicates , but to find the most simi- lar text . RETSimPartial-Dup optimizes for finding the most similar chunk of text in the corpus , which is not always the most similar text overall . Similarly , we hypothesize that MinHash and SimHash per- form poorly on the W4NT3D benchmark due to their lack of ability to distinguish which is the most similar text among the near-duplicates detected , and embedding-based models and cosine similarity offer a more fine-grained measure of similarity . RETSimNear-Dup outperforms baseline algorithms on all languages except for Chinese and Japanese . For these languages , we theorize that semantic embeddings may have the slight edge in performance because their significantly larger model sizes ( more than 100x larger than RETSim , as shown in Ta- ble 1 ) allow them to have a better representation on languages with large character sets . Furthermore , the sub-word level tokenizers used in the baseline embeddings often treat each character in Chinese or Japanese as individual tokens , which could offer higher resilience to typos . Figure 2 : Recall @ 1 performance on the W4NT3D benchmark , broken down by augmentation type . Results are averaged across all 41 language splits in W4NT3D . Adversarial Resilience Delving deeper into the impact of various types of text manipulation re- veals that RETSimNear-Dup and RETSimPartial-Dup perform almost equally well regardless of the type of augmentation applied ( Figure 2 ) . Semantic text embeddings perform well on paragraph , sentence and word-level manipulations , but as expected , exhibit significantly weaker performance towards character-level typos . MinHash and SimHash struggle more with word-level augmentations than deep-learning based embeddings and collapse when character-level typos are introduced . We at- 5 1.00 0.75 0.50 0.25 0.00 l l 1 @ a c e R Paragraph Sentence Word Character Augmentation Level LaBSE Multilingual USE Multilingual E5-Base PaLM 2 ( Gecko ) SimHash MinHash RETSim ( Partial-Dup ) RETSim ( Near-Dup ) Preprint tribute RETSim ’ s resilience to adversarial manipulations to the RETVec character encoder as well as using deep metric learning to train robust embeddings . Figure 4 reports the Recall @ 1 performance of the algorithms as the amount of augmentation in- creases . All algorithms perform perfectly when no augmentation is applied ( exact matching ) , but as the percentage of augmentation increases , n-gram based approaches exhibit a steep drop in perfor- mance . Semantic text embeddings are able to sustain a larger degree of augmentation before their retrieval capabilities start to degrade ( over 20 % ) . RETSimNear-Dup is the most robust algorithm , with a noticeable drop in performance only after around 40 % augmentation . This makes RETSim the most effective approach at clustering and deduplicating text under adversarial settings . Figure 3 : Recall @ 1 performances on the W4NT3D benchmark ( English only ) for varying max target lengths . Figure 4 : Recall @ 1 performances on the W4NT3D benchmark ( English only ) as the amount of augmenta- tion applied to the query text increases . Text Length Impact on Performance Figure 3 reports the Recall @ 1 performance of RETSim and baseline algorithms as the length of the query and target text varies . We see that RETSimNear-Dup and RETSimPartial-Dup outperforms all other methods on short texts with fewer than 128 characters . As the text length increases beyond 512 characters , RETSimNear-Dup remains close to perfect while RETSimPartial-Dup ’ s performance degrades since it splits the text into multiple embeddings and finds the nearest matching chunk of text . MinHash and SimHash also perform poorly on short text lengths and start to degrade on longer texts . For neural-based embeddings , we observe a slight drop in performance on longer texts for all models except RETSimNear-Dup and Multilingual USE , the only two embeddings that can handle arbitrary length inputs . 4.3 REAL-WORLD NEAR-DUPLICATE DETECTION EVALUATION Setup We benchmark RETSim ’ s ability to identify near-duplicate content on real-world datasets from the literature . The NEWS-COPY Deduplication dataset ( Silcock et al. , 2022 ) contains 27,210 historical news articles with 122,876 positive duplicate pairs . The dataset consists of noisy near- duplicates due to factors like OCR errors , plagiarism , and news aggregation . We also evaluate the algorithms on the CORE Near-Duplicates dataset ( Gyawali et al. , 2020 ) , which consists of 100k scholarly articles with 25k exact duplicates , 25k near-duplicates , and 50k non-duplicates . Near- duplicates in this dataset arise from article revisions , versioning and metadata differences , and hu- man typos . A key difference between these two benchmarks and the W4NT3D benchmark is that these two benchmarks are focused on detecting and clustering near-duplicate text , rather than robust text retrieval based on syntactic similarity . For both benchmarks , we follow the experimental setup provided in the papers and report Adjusted Rand Index ( ARI ) for the NEWS-COPY dataset and report precision/recall/F1 scores on the CORE Near-Duplicates dataset . Results On the NEWS-COPY dataset , RETSimPartial-Dup outperforms all other approaches by a significant margin ( 4.8 % ARI compared to our best MinHash result ) , as reported in Table 3 . In the dataset , there are many near-duplicate pairs where one text is significantly longer than the other , so it is expected that RETSimPartial-Dup , which can find matching text chunks in documents , is more suited for the task and outperforms RETSimNear-Dup . Bucketing the near-duplicate detection rate of each algorithm by the length ratio between positive pairs ( Figure 5 ) , we observe that RETSimPartial-Dup outperforms MinHash regardless of the length ratio , but MinHash surpasses RETSimNear-Dup per- formance when one text is above roughly 1.5x the length of the other text in a near-duplicate pair . 6 1.0 0.8 0.6 0.4 l l 1 @ a c e R 16 32 64 128 256 512 1024 2048 4096 8192 Max Target Text Length l l 1 @ a c e R 1.0 0.8 0.6 0.4 0 % 10 % 20 % 30 % 40 % 50 % Text Augmentation Amount ( % ) LaBSE Multilingual USE Multilingual E5-Base PaLM 2 ( Gecko ) SimHash MinHash RETSim ( Partial-Dup ) RETSim ( Near-Dup ) Preprint Model/Algorithm ARI Multilingual USE Multilingual E5-Base S-BERT * SimHash MinHash * MinHash ( Ours ) RETSimPartial-Dup RETSimNear-Dup 0.730 0.742 0.700 0.695 0.737 0.783 0.831 0.704 Table 3 : Performance comparison on the NEWS-COPY dataset . Adjusted Rand Index ( ARI ) values are reported . * denotes results from Silcock et al . ( 2022 ) . Figure 5 : Near-duplicate detection rate of RET- Sim vs MinHash for different length ratios of pos- itive pairs . X-axis is the length of longer divided by shorter text , rounded to the nearest integer . Additionally , we noticed that the labels in the dataset were occasionally noisy , as a substantial por- tion of the RETSim false positives appear to be near-duplicates upon inspection ( Appendix A.6 ) . On the CORE Near-Duplicates dataset ( Table 4 ) , where documents ( article title + abstract ) are roughly the same size , RETSimPartial-Dup and RETSimNear-Dup performance is roughly equivalent . Both methods outperform the baselines in terms of macro F1 score and accuracy . We use MinHash + LSH with 256 hash functions for computational efficiency , as recommended by the datasketch library1 for better accuracy than the default setting . Deduplication thresholds and detailed hyperpa- rameter settings for the algorithms on both near-duplication datasets can be found in Appendix A.3 . Model / Algorithm Exact Title Matching * LaBSE Multilingual USE Multilingual E5-Base MinHash + LSH RETSimPartial-Dup RETSimNear-Dup Precision Duplicates Recall Duplicates Precision Non-Duplicates Recall Non-Duplicates Macro F1 Accuracy 0.830 0.937 0.917 0.931 0.929 0.945 0.928 0.500 0.923 0.907 0.908 0.902 0.941 0.937 0.709 0.930 0.918 0.919 0.915 0.945 0.942 0.992 0.943 0.927 0.939 0.938 0.949 0.934 0.757 0.933 0.917 0.924 0.921 0.945 0.935 0.746 0.919 0.909 0.920 0.918 0.928 0.926 Table 4 : Evaluation results on the CORE Near-Duplicates dataset . Precision/recall/macro F1 and accuracy numbers are reported . * denotes results from Gyawali et al . ( 2020 ) . 5 APPLICATIONS 5.1 TRAINING DATASET DEDUPLICATION Model/Algorithm % train examples with dup in train % valid examples with dup in train MinHash + LSH Exact Substring * RETSimNear-Dup RETSimPartial-Dup 0.47 % 2.76 % 3.17 % 12.77 % 0.46 % 0.52 % 0.59 % 2.66 % Table 5 : Deduplication rate on Wiki-40B ( English ) . * denotes results from Lee et al . ( 2022 ) . Setup We evaluate RETSim ’ s ability to deduplicate text training datasets by deduplicating the English split of Wiki-40B ( Guo et al. , 2020 ) . We conservatively set the cosine similarity deduplica- tion threshold to 0.1 for RETSimNear-Dup and 0.15 for RETSimPartial-Dup to limit the amount of false positives , based on the optimal thresholds found in the evaluation ( Appendix A.3 ) . We use USe- arch ’ s default vector index for approximate nearest neighbor search ( Vardanian , 2023 ) . We compare 1datasketch : Big Data Looks Small . https : //github.com/ekzhu/datasketch . 7 t e a R n o t i t c e e D e a c t i l p u D - r a e N 1.0 0.8 0.5 0.3 0.0 1 2 3 4 5 6 7 8 9 10 Length Ratio between Near-Duplicate Text Pair RETSim ( Partial-Dup ) RETSim ( Near-Dup ) MinHash Preprint Model/Algorithm Accelerator Batch Size Embedding / Hashing time ( sec ) examples/sec MinHash + LSH RETSim RETSim RETSim CPU AMD 7950 32 cores Onnx CPU AMD 7950 32 cores TensorFlow GPU RTX 4090 TensorFlow GPU NVIDIA H100 - 256 4096 16384 234 10839 720 363 12544 270 4062 8069 Table 6 : Embedding/hashing speed of RETSim vs MinHash + LSH on the Wiki-40B dataset . against MinHash + LSH , where we set the number of hash functions to be 256 following Kocetkov et al . ( 2022 ) and use a Jaccard similarity threshold of 0.8 for deduplication ( Lee et al. , 2022 ) . Results Overall , as reported in Table 5 , RETSimNear-Dup finds slightly more duplicates in the Wiki- 40B training and validation splits . This is in-line with our deduplication results ( Section 4.3 ) where RETSimNear-Dup outperforms other algorithms . On the other hand , RETSimPartial-Dup finds signifi- cantly more matches than the exact substring matching algorithm used in the previous study ( Lee et al. , 2022 ) , showcasing the usefulness of performing both near-duplicate and partial-duplicate matching at once . This larger-than-expected number of partial matches indicate that machine learn- ing practitioners should take extra care to deduplicate Wikipedia at the chunk level to avoid feeding duplicate text to their models . In terms of embedding speed ( Table 6 ) , RETSim is significantly slower than MinHash + LSH on CPU ( 46x slower ) , competitive when using a desktop GPU such as the RTX 4090 ( 3x slower ) and almost on-par when using a high-end GPU like the NVIDIA H100 ( 1.5x slower ) . Our current code is written in Python and not fully optimized , so we expect this performance gap to significantly shrink as we optimize our implementation . Although RETSim is slower than MinHash , RETSim is significantly smaller and faster than other text embedding models , and closes the performance gap between neural and non-neural based methods for near-duplicate text detection and dataset deduplication . Both RETSimNear-Dup and RETSimPartial-Dup are returned at the same time so they have the same embedding speed . Indexing and retrieval times will depend on the vector index and search algorithm used . For longer documents , RETSimPartial-Dup will produce more embeddings than RETSimNear-Dup , so RETSimPartial-Dup offers a tradeoff between finer-grained matching versus indexing/retrieval speed , which will depend on the specific vector search algorithm and dataset used . 5.2 IN THE WILD : SPAM EMAIL CLUSTERING In this section , we showcase RETSim ’ s real-world performance on clustering near-duplicate text which has been heavily manipulated by adversarial attacks by performing an evaluation on spam campaigns . Spam constitutes a strong proving ground for near-duplicate clustering algorithms as spammers employ adversarial augmentation techniques in an attempt to evade detection . Such aug- mentations typically include appending or prepending unrelated text , interleaving random words and different languages , intentionally introducing typos , abusing extended character sets such as emojis and homoglyphs , and more . These techniques are collectively referred to as hash-busting . Setup The dataset consists of 5,252 spam emails from 196 spam campaigns , donated by Gmail users who flagged them when they reached their inboxes . Each example contains the email subject concatenated with the message content . The emails were misclassified by a spam classifier due to their effective adversarial text manipulation techniques , which makes them a challenging test set for clustering evaluations . Some examples of hash-busting attacks and adversarial manipulations we observe include the use of homoglpyphs , uncommon Unicode character sets , invisible characters , and padding with random words from different languages . To get the ground truth campaign clusters , emails were manually reviewed and assigned to a specific spam campaign based on similarity by human reviewers . We use agglomerative clustering to cluster spam emails , and report homogeneity , completeness , V-Measure , and Adjusted Rand Index ( ARI ) metrics . Results Overall , we observed that RETSim is significantly better at clustering near-duplicates with adversarial manipulations , outperforming both SimHash and USE across all metrics considered ( Table 7 ) . In particular , we observed that RETSim outperforms USE by 4.6 % on the V-Measure score which is our main metric . The results reported in this section are in-line with what we observe since we deployed RETSim as our main near-duplicate detection algorithm in December 2022 . 8 Preprint Model / Algorithm Homogeneity Completeness V-Measure ARI USE SimHash + LSH RETSimNear-Dup 0.856 0.867 0.937 0.955 0.876 0.963 0.903 0.871 0.949 0.6 0.571 0.747 Table 7 : Performance on clustering adversarial spam campaigns in practice . 6 ABLATION STUDIES Setup In this section , we summarize the key ablation studies we performed when designing RET- Sim . All the models used in this section are trained using the setup detailed in Appendix A.1.2 , ex- cept we only train them for 100k steps to reduce computational costs . We evaluate RETSimNear-Dup ’ s performance for each model on a subset of the W4NT3D benchmark , where we randomly select 1000 examples from each of the 41 language splits and use Recall @ 1 as reported metric . Block Type Recall @ 1 Chunk Size Recall @ 1 Embed . Dim Recall @ 1 RETVec MLP ConvNeXt BERT T5 * GAU 0.975 0.978 0.973 0.980 0.986 128 256 * 512 1024 2048 0.979 0.984 0.986 0.983 0.978 64 128 * 256 512 768 0.969 0.980 0.986 0.986 0.986 Table 8 : RETSim ablation study results on architecture block type ( left ) , text chunk size ( middle ) , and embedding dimension ( right ) . * Bold denotes the value selected for the final RETSim model . Results Table 8 contains RETSim ablation study results on max text chunk size , architecture block type , and embedding size . The most important architectural decision was to decide the optimal text chunk size and finding the right balance between having the smallest size possible to maximize RETSimPartial-Dup efficiency while ensuring RETSimNear-Dup full-text embeddings can work effec- tively on full documents . We find that chunks of 512 characters offer the best performance . We also tested various model architectures and transformer blocks to find the best balance between efficiency and performance . We find that the more modern GAU block ( Hua et al. , 2022 ) outper- forms the vanilla BERT transformer block ( Devlin et al. , 2019 ) and the T5 block ( Xue et al. , 2020 ) . We also tried modern CNN architectures such as ConvNeXt ( Liu et al. , 2022 ) and the MLP architec- ture proposed in RETVec ( Bursztein et al. , 2023 ) , but both were worse than GAU block performance . Last but not least , we found that increasing the embedding size past 256 dimensions does not yield any meaningful improvements for RETSimNear-Dup . Accordingly , we opted to use a 256-dimension embedding for space-efficiency and to maximize indexing and query speed . Additional ablation studies for other hyperparameters can be found in Appendix A.5 . 7 FUTURE WORK RETSim ’ s novel training regime , which combines metric learning and data augmentation , has many other potential applications that we plan to explore in future work . For example , it could be adapted or extended to train robust semantic embeddings or image similarity embeddings . Additionally , we expect that as general models become bigger and more expensive to run in the future , smaller , specialized models such as RETSim will emerge as an efficient alternative for a wide range of tasks . 8 CONCLUSION In this paper , we introduced RETSim , a novel , multilingual text embedding which achieves state- of-the-art performance on near-duplicate text detection , dataset deduplication , and syntactic text similarity benchmarks . RETSim is significantly faster than previous neural-based text embeddings and more robust than n-gram based algorithms , which makes it suitable for large-scale text retrieval and dataset deduplication , especially in adversarial settings such as spam detection . Furthermore , we introduced the W4NT3D benchmark , the first multilingual dataset designed to measure the ad- versarial robustness of near-duplicate text detection algorithms . We open-source both RETSim and the W4NT3D benchmark under the MIT License . 9 Preprint REFERENCES Naeem Ahmed , Rashid Amin , Hamza Aldabbas , Deepika Koundal , Bader Alouffi , and Tariq Shah . Machine Learning Techniques for Spam Detection in Email and IoT Platforms : Analysis and Research Challenges . Security and Communication Networks , 2022:1–19 , February 2022 . ISSN 1939-0122 , 1939-0114. doi : 10.1155/2022/1862888 . URL https : //www.hindawi.com/ journals/scn/2022/1862888/ . Moustafa Alzantot , Yash Sharma , Ahmed Elgohary , Bo-Jhang Ho , Mani Srivastava , and Kai-Wei Chang . Generating Natural Language Adversarial Examples , September 2018 . URL http : //arxiv.org/abs/1804.07998 . arXiv:1804.07998 [ cs ] . Rohan Anil , Andrew M. Dai , Orhan Firat , Melvin Johnson , Dmitry Lepikhin , Alexandre Passos , Siamak Shakeri , Emanuel Taropa , Paige Bailey , Zhifeng Chen , Eric Chu , Jonathan H. Clark , Laurent El Shafey , Yanping Huang , Kathy Meier-Hellstern , Gaurav Mishra , Erica Moreira , Mark Omernick , Kevin Robinson , Sebastian Ruder , Yi Tay , Kefan Xiao , Yuanzhong Xu , Yujing Zhang , Gustavo Hernandez Abrego , Junwhan Ahn , Jacob Austin , Paul Barham , Jan Botha , James Brad- bury , Siddhartha Brahma , Kevin Brooks , Michele Catasta , Yong Cheng , Colin Cherry , Christo- pher A. Choquette-Choo , Aakanksha Chowdhery , Cl´ement Crepy , Shachi Dave , Mostafa De- hghani , Sunipa Dev , Jacob Devlin , Mark D´ıaz , Nan Du , Ethan Dyer , Vlad Feinberg , Fangxiaoyu Feng , Vlad Fienber , Markus Freitag , Xavier Garcia , Sebastian Gehrmann , Lucas Gonzalez , Guy Gur-Ari , Steven Hand , Hadi Hashemi , Le Hou , Joshua Howland , Andrea Hu , Jeffrey Hui , Jeremy Hurwitz , Michael Isard , Abe Ittycheriah , Matthew Jagielski , Wenhao Jia , Kathleen Kenealy , Maxim Krikun , Sneha Kudugunta , Chang Lan , Katherine Lee , Benjamin Lee , Eric Li , Music Li , Wei Li , YaGuang Li , Jian Li , Hyeontaek Lim , Hanzhao Lin , Zhongtao Liu , Frederick Liu , Mar- cello Maggioni , Aroma Mahendru , Joshua Maynez , Vedant Misra , Maysam Moussalem , Zachary Nado , John Nham , Eric Ni , Andrew Nystrom , Alicia Parrish , Marie Pellat , Martin Polacek , Alex Polozov , Reiner Pope , Siyuan Qiao , Emily Reif , Bryan Richter , Parker Riley , Alex Castro Ros , Aurko Roy , Brennan Saeta , Rajkumar Samuel , Renee Shelby , Ambrose Slone , Daniel Smilkov , David R. So , Daniel Sohn , Simon Tokumine , Dasha Valter , Vijay Vasudevan , Kiran Vodrahalli , Xuezhi Wang , Pidong Wang , Zirui Wang , Tao Wang , John Wieting , Yuhuai Wu , Kelvin Xu , Yun- han Xu , Linting Xue , Pengcheng Yin , Jiahui Yu , Qiao Zhang , Steven Zheng , Ce Zheng , Weikang Zhou , Denny Zhou , Slav Petrov , and Yonghui Wu . PaLM 2 Technical Report , September 2023 . URL http : //arxiv.org/abs/2305.10403 . arXiv:2305.10403 [ cs ] . Andrei Z. Broder , Moses Charikar , Alan M. Frieze , and Michael Mitzenmacher . Min-wise inde- In Proceedings of the thirtieth annual ACM symposium on Theory of pendent permutations . computing , pp . 327–336 , 1998 . Eli Bursztein , Marina Zhang , Owen vallis , Xinyu Jia , and Alexey Kurakin . RetVec : Resilient and Efficient Text Vectorizer . 2023 . Daniel Cer , Yinfei Yang , Sheng-yi Kong , Nan Hua , Nicole Limtiaco , Rhomni St John , Noah Con- stant , Mario Guajardo-Cespedes , Steve Yuan , and Chris Tar . Universal sentence encoder . arXiv preprint arXiv:1803.11175 , 2018 . Moses S. Charikar . Similarity estimation techniques from rounding algorithms . In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing , pp . 380–388 , 2002 . Jacob Devlin , Ming-Wei Chang , Kenton Lee , and Kristina Toutanova . BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding , May 2019 . URL http : //arxiv . org/abs/1810.04805 . arXiv:1810.04805 [ cs ] . Fangxiaoyu Feng , Yinfei Yang , Daniel Cer , Naveen Arivazhagan , and Wei Wang . Language- agnostic BERT Sentence Embedding , March 2022 . URL http : //arxiv.org/abs/2007 . 01852. arXiv:2007.01852 [ cs ] . Ji Gao , Jack Lanchantin , Mary Lou Soffa , and Yanjun Qi . Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers , May 2018 . URL http : //arxiv.org/abs/ 1801.04354. arXiv:1801.04354 [ cs ] . 10 Preprint Mandy Guo , Zihang Dai , Denny Vrandeˇci´c , and Rami Al-Rfou . Wiki-40b : Multilingual language model dataset . In Proceedings of the 12th Language Resources and Evaluation Conference , pp . 2440–2452 , 2020 . Bikash Gyawali , Lucas Anastasiou , and Petr Knoth . Deduplication of Scholarly Documents us- ing Locality Sensitive Hashing and Word Embeddings . In Proceedings of the Twelfth Language Resources and Evaluation Conference , pp . 901–910 , Marseille , France , May 2020 . European Lan- guage Resources Association . ISBN 979-10-95546-34-4 . URL https : //aclanthology . org/2020.lrec-1.113 . Matthias Hagen , Martin Potthast , Marcel Gohsen , Anja Rathgeber , and Benno Stein . A large- In Proceedings of the 40th International ACM SIGIR scale query spelling correction corpus . Conference on Research and Development in Information Retrieval , pp . 1261–1264 , 2017 . Weizhe Hua , Zihang Dai , Hanxiao Liu , and Quoc Le . Transformer quality in linear time . In International Conference on Machine Learning , pp . 9099–9117 . PMLR , 2022 . B. Issac , R. Chiong , and S. M. Jacob . Analysis of phishing attacks and countermeasures , 2014 . Mohit Iyyer , Varun Manjunatha , Jordan Boyd-Graber , and Hal Daum´e Iii . Deep Unordered Composition Rivals Syntactic Methods for Text Classification . In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing ( Volume 1 : Long Papers ) , pp . 1681–1691 , Bei- jing , China , 2015 . Association for Computational Linguistics . doi : 10.3115/v1/P15-1162 . URL http : //aclweb.org/anthology/P15-1162 . Nikhil Kandpal , Eric Wallace , and Colin Raffel . Deduplicating Training Data Mitigates Pri- vacy Risks in Language Models . In Proceedings of the 39th International Conference on Machine Learning , pp . 10697–10707 . PMLR , June 2022 . URL https : //proceedings . mlr.press/v162/kandpal22a.html . ISSN : 2640-3498 . Denis Kocetkov , Raymond Li , Loubna Ben Allal , Jia Li , Chenghao Mou , Carlos Mu˜noz Ferrandis , Yacine Jernite , Margaret Mitchell , Sean Hughes , Thomas Wolf , Dzmitry Bahdanau , Leandro von Werra , and Harm de Vries . The Stack : 3 TB of permissively licensed source code , November 2022 . URL http : //arxiv.org/abs/2211.15533 . arXiv:2211.15533 [ cs ] . Katherine Lee , Daphne Ippolito , Andrew Nystrom , Chiyuan Zhang , Douglas Eck , Chris Callison- Burch , and Nicholas Carlini . Deduplicating Training Data Makes Language Models Better , March 2022 . URL http : //arxiv.org/abs/2107.06499 . arXiv:2107.06499 [ cs ] . Zhuang Liu , Hanzi Mao , Chao-Yuan Wu , Christoph Feichtenhofer , Trevor Darrell , and Saining Xie . A convnet for the 2020s . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp . 11976–11986 , 2022 . John X. Morris , Eli Lifland , Jin Yong Yoo , Jake Grigsby , Di Jin , and Yanjun Qi . TextAttack : A Framework for Adversarial Attacks , Data Augmentation , and Adversarial Training in NLP , Oc- tober 2020 . URL http : //arxiv.org/abs/2005.05909 . arXiv:2005.05909 [ cs ] . Niklas Muennighoff , Nouamane Tazi , Lo¨ıc Magne , and Nils Reimers . MTEB : Massive Text Em- bedding Benchmark , October 2022 . URL https : //arxiv.org/abs/2210.07316v1 . Filip Radenovi´c , Giorgos Tolias , and Ondˇrej Chum . Fine-tuning CNN image retrieval with no human annotation . IEEE transactions on pattern analysis and machine intelligence , 41 ( 7 ) :1655– 1668 , 2018 . Publisher : IEEE . Nils Reimers and Iryna Gurevych . Sentence-bert : Sentence embeddings using siamese bert- networks . arXiv preprint arXiv:1908.10084 , 2019 . Florian Schroff , Dmitry Kalenichenko , and James Philbin . FaceNet : A Unified Embedding for Face Recognition and Clustering . In 2015 IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , pp . 815–823 , June 2015. doi : 10.1109/CVPR.2015.7298682 . URL http : //arxiv.org/abs/1503.03832 . arXiv:1503.03832 [ cs ] . 11 Preprint Emily Silcock , Luca D ’ Amico-Wong , Jinglin Yang , and Melissa Dell . Noise-Robust De-Duplication at Scale , October 2022 . URL http : //arxiv.org/abs/2210.04261 . arXiv:2210.04261 [ cs ] . Yifang Sun , Jianbin Qin , and Wei Wang . Near Duplicate Text Detection Using Frequency-Biased Signatures . In David Hutchison , Takeo Kanade , Josef Kittler , Jon M. Kleinberg , Friedemann Mat- tern , John C. Mitchell , Moni Naor , Oscar Nierstrasz , C. Pandu Rangan , Bernhard Steffen , Madhu Sudan , Demetri Terzopoulos , Doug Tygar , Moshe Y. Vardi , Gerhard Weikum , Xuemin Lin , Yan- nis Manolopoulos , Divesh Srivastava , and Guangyan Huang ( eds . ) , Web Information Systems Engineering – WISE 2013 , volume 8180 , pp . 277–291 . Springer Berlin Heidelberg , Berlin , Hei- delberg , 2013 . ISBN 978-3-642-41229-5 978-3-642-41230-1. doi : 10.1007/978-3-642-41230-1 24 . URL http : . Series Title : Lecture Notes in Computer Science . Ash Vardanian . USearch by Unum Cloud , October 2023 . URL https : //github.com/ unum-cloud/usearch . Liang Wang , Nan Yang , Xiaolong Huang , Binxing Jiao , Linjun Yang , Daxin Jiang , Rangan Ma- jumder , and Furu Wei . Text Embeddings by Weakly-Supervised Contrastive Pre-training , De- cember 2022 . URL http : //arxiv.org/abs/2212.03533 . arXiv:2212.03533 [ cs ] . Xun Wang , Xintong Han , Weilin Huang , Dengke Dong , and Matthew R. Scott . Multi-similarity loss with general pair weighting for deep metric learning . In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp . 5022–5030 , 2019 . Linting Xue , Noah Constant , Adam Roberts , Mihir Kale , Rami Al-Rfou , Aditya Siddhant , Aditya Barua , and Colin Raffel . mT5 : A massively multilingual pre-trained text-to-text transformer . arXiv preprint arXiv:2010.11934 , 2020 . Yinfei Yang , Daniel Cer , Amin Ahmad , Mandy Guo , Jax Law , Noah Constant , Gustavo Hernandez Abrego , Steve Yuan , Chris Tar , Yun-Hsuan Sung , Brian Strope , and Ray Kurzweil . Multilingual Universal Sentence Encoder for Semantic Retrieval , July 2019 . URL http : //arxiv.org/ abs/1907.04307 . arXiv:1907.04307 [ cs ] . Yang You , Jing Li , Sashank Reddi , Jonathan Hseu , Sanjiv Kumar , Srinadh Bhojanapalli , Xiaodan Song , James Demmel , Kurt Keutzer , and Cho-Jui Hsieh . Large batch optimization for deep learning : Training bert in 76 minutes . arXiv preprint arXiv:1904.00962 , 2019 . 12 Preprint A APPENDIX A.1 RETSIM DETAILS A.1.1 RETSIM MODEL HYPERPARAMETERS The full list of RETSim model hyperparameters can be found in Table 9 . Hyperparameter Max input length ( per chunk ) Block type # blocks Hidden dim Expansion rate Activation function Attention activation function Absolute positional encoding Relative positional encoding Norm type Pooling type Dropout rate Embedding dim # Parameters Value 512 GAU 2 256 1 Swish relu2 ScaledSin RoPE ScaleNorm GeM ( p = 3 ) 0 256 536k Table 9 : Detailed RETSim model hyperparameters . A.1.2 RETSIM TRAINING HYPERPARAMETERS Table 10 details the hyperparameters settings for training configuration , loss , and optimizer used to train the RETSim model . Hyperparameter Value Batch size Train steps LAMB ϵ LAMB β1 LAMB β2 Max learning rate End learning rate Learning rate decay Weight decay 1024 1 million 1e-6 0.9 0.999 0.001 0 Cosine 0 Table 10 : RETSim detailed training hyperparameters . A.2 TRAINING DATASET DETAILS Below , we provide the full list of augmentations used to generate augmented text for the RETSim training dataset , as described in Section 3.2 . SENTENCE-LEVEL AUGMENTATIONS • Deletion : – Random sentence deletion – Random sentence truncation • Insertion : 13 Preprint – Random prefix sentence – Random suffix sentence – Random sentence insertion – Repeat sentence • Substitution : – Lowercase/uppercase sentence – Random sentence substitution • Transposition : – Neighboring Swap WORD-LEVEL AUGMENTATIONS • Deletion : – Random word deletion • Insertion : – Random word insertion – Random word insertion per language • Substitution : – 3-gram frequency based word substitution – Random word substitution – Random word substitution per language – Repeat word • Transposition : – Neighboring Swap CHARACTER-LEVEL AUGMENTATIONS • Deletion : – Random character deletion • Substitution : – Case substitution – n-gram based substitution for n = 3 , 4 , 5 – QWERTY keyboard typo substitution – Homoglyphs substitution – Random ASCII substitution – Random character from language alphabet substitution – Random punctuation substitution – Random Unicode character substitution • Insertion : – Character repetition – n-grams based insertion for n = 3 , 4 , 5 – Random character from language alphabet insertion – Random punctuation insertion – Random Unicode character insertion • Transposition : – Neighboring swap A.3 DETAILED EVALUATION HYPERPARAMETERS Figures 6 and 7 contain information on deduplication thresholds values and hyperparameter settings for each algorithm benchmarked on the NEWS-COPY and CORE deduplication datasets . 14 Preprint Model / Algorithm Threshold Type Threshold Value Hyperparameters Multilingual USE Cosine Similarity Multilingual E5-Base Cosine Similarity SimHash MinHash ( Ours ) RETSimNear-Dup RETSimPartial-Dup Hamming Distance Jaccard Similarity Cosine Similarity Cosine Similarity 0.96 0.88 10 0.6 0.89 0.84 - - 64 bits , 5-grams ( character-level ) 10 hash functions , 2-grams ( word-level ) - - Figure 6 : Hyperparameter settings for NEWS-COPY dataset evaluation in Section 4.3 . Model / Algorithm Threshold Type Threshold Value Hyperparameters Cosine Similarity LaBSE Multilingual USE Cosine Similarity Multilingual E5-Base Cosine Similarity SimHash + LSH MinHash + LSH RETSimNear-Dup RETSimPartial-Dup Hamming Distance Jaccard Similarity Cosine Similarity Cosine Similarity 0.88 0.97 0.87 6 0.5 0.86 0.82 - - - 64 bits , 3-grams ( character-level ) 256 hash functions , 3-grams ( word-level ) - - Figure 7 : Hyperparameter settings for CORE Near-Duplicates dataset evaluation in Section 4.3 . A.3.1 DEDUPLICATION THRESHOLD IMPACT Figure 8 : Precision/Recall/F1 scores for different cosine distance deduplication thresholds for RETSimNear-Dup ( left ) and RETSimPartial-Dup ( right ) on the NEWS-COPY dataset . A.4 DETAILED W4NT3D BENCHMARK RESULTS Tables 11 and 12 show detailed performance results for RETSim and all baseline algorithms for every language split in the W4NT3D benchmark . 15 Preprint 9 4 9 . 0 4 8 9 . 0 6 8 9 . 0 1 0 7 . 0 3 9 5 . 0 4 1 8 . 0 1 7 9 . 0 1 9 9 . 0 o k 1 3 9 . 0 0 9 9 . 0 9 7 9 . 0 8 7 5 . 0 5 6 4 . 0 3 2 2 . 0 3 6 9 . 0 6 8 9 . 0 a j 9 2 9 . 0 7 3 9 . 0 3 3 9 . 0 7 3 9 . 0 3 1 5 . 0 5 7 5 . 0 4 4 9 . 0 2 8 9 . 0 5 1 9 . 0 8 2 9 . 0 2 4 9 . 0 4 2 9 . 0 3 3 5 . 0 3 6 5 . 0 0 5 9 . 0 6 7 9 . 0 8 9 8 . 0 5 8 8 . 0 9 8 8 . 0 6 7 8 . 0 0 3 5 . 0 2 1 5 . 0 0 5 9 . 0 2 6 9 . 0 2 1 9 . 0 9 8 8 . 0 1 0 9 . 0 3 0 9 . 0 7 5 5 . 0 6 5 5 . 0 4 3 9 . 0 2 6 9 . 0 7 2 9 . 0 8 4 6 . 0 4 6 9 . 0 5 3 4 . 0 3 6 5 . 0 6 0 6 . 0 1 4 9 . 0 5 7 9 . 0 7 3 9 . 0 1 4 8 . 0 8 5 9 . 0 9 8 5 . 0 1 5 6 . 0 7 9 6 . 0 5 4 9 . 0 9 8 9 . 0 0 3 9 . 0 8 3 9 . 0 8 4 9 . 0 4 3 9 . 0 9 1 5 . 0 6 5 5 . 0 7 4 9 . 0 3 8 9 . 0 6 2 9 . 0 3 1 9 . 0 3 4 9 . 0 4 1 9 . 0 2 3 6 . 0 8 6 5 . 0 6 5 9 . 0 1 8 9 . 0 2 1 9 . 0 0 7 8 . 0 9 2 9 . 0 6 5 3 . 0 8 6 5 . 0 5 9 5 . 0 6 4 9 . 0 1 7 9 . 0 3 2 9 . 0 0 3 9 . 0 1 5 9 . 0 6 2 9 . 0 5 6 6 . 0 5 8 5 . 0 3 5 9 . 0 5 8 9 . 0 8 8 8 . 0 3 0 9 . 0 6 9 8 . 0 1 1 9 . 0 6 9 4 . 0 4 0 5 . 0 8 3 9 . 0 6 7 9 . 0 4 4 9 . 0 8 5 9 . 0 9 5 9 . 0 1 6 9 . 0 1 9 5 . 0 1 9 5 . 0 4 5 9 . 0 7 8 9 . 0 8 1 9 . 0 4 7 8 . 0 4 3 9 . 0 9 8 5 . 0 7 5 5 . 0 4 7 5 . 0 5 3 9 . 0 4 7 9 . 0 1 3 9 . 0 2 4 9 . 0 4 4 9 . 0 2 3 9 . 0 1 6 5 . 0 8 5 5 . 0 9 4 9 . 0 8 7 9 . 0 8 3 9 . 0 7 2 9 . 0 6 5 9 . 0 3 4 9 . 0 2 9 5 . 0 8 9 5 . 0 3 5 9 . 0 8 8 9 . 0 2 1 9 . 0 0 0 9 . 0 7 2 9 . 0 1 1 9 . 0 9 7 5 . 0 1 8 5 . 0 0 5 9 . 0 3 7 9 . 0 7 9 8 . 0 3 8 8 . 0 0 9 8 . 0 2 0 9 . 0 5 8 4 . 0 8 0 5 . 0 4 2 9 . 0 8 6 9 . 0 5 1 9 . 0 9 0 9 . 0 9 9 8 . 0 1 5 8 . 0 0 1 5 . 0 6 0 5 . 0 2 4 9 . 0 6 7 9 . 0 5 1 9 . 0 5 1 9 . 0 6 3 9 . 0 7 9 4 . 0 8 5 5 . 0 3 3 6 . 0 8 2 9 . 0 1 7 9 . 0 e s a B - 5 E l a u g n i l i t l u M E S U l a u g n i l i t l u M ) o k c e G ( 2 M L a P E S B a L p u D - l a i t r a P m S T E R i p u D i - r a e N m S T E R h s a H m S i h s a H n i M t i d i u h r h i h e h r f fi a f t e s e n e l e e d a d s c a c g b r a m h t i r o g l A / l e d o M . ) 1 t r a p ( k r a m h c n e b D 3 T N 4 W e h t n o s m h t i r o g l a d n a s l e d o m g n i d d e b m e s u o i r a v r o f e c n a m r o f r e p 1 @ l l a c e R e g a u g n a l - r e p l l u F : 1 1 e l b a T w t - h z n c - h z i v k u r t l t h t v s r s l s k s u r o r t p l p o n l n s m v l t l m h t i r o g l A / l e d o M 16 8 1 9 . 0 5 8 9 . 0 8 7 9 . 0 9 0 6 . 0 5 1 3 . 0 0 0 2 . 0 7 5 9 . 0 8 6 9 . 0 7 1 9 . 0 6 8 9 . 0 0 8 9 . 0 3 2 6 . 0 6 7 2 . 0 2 7 1 . 0 6 4 9 . 0 1 7 9 . 0 2 3 9 . 0 0 1 9 . 0 5 4 9 . 0 3 6 8 . 0 9 0 6 . 0 1 8 5 . 0 1 5 9 . 0 5 8 9 . 0 9 9 8 . 0 3 9 8 . 0 2 7 8 . 0 2 2 8 . 0 7 1 5 . 0 0 2 5 . 0 1 4 9 . 0 7 5 9 . 0 0 3 9 . 0 0 4 9 . 0 1 5 9 . 0 2 0 9 . 0 6 0 6 . 0 3 8 5 . 0 4 5 9 . 0 8 7 9 . 0 7 4 9 . 0 9 4 9 . 0 9 6 9 . 0 4 4 9 . 0 7 0 5 . 0 0 7 5 . 0 1 6 9 . 0 9 8 9 . 0 2 8 8 . 0 8 8 8 . 0 1 2 9 . 0 1 7 5 . 0 9 6 6 . 0 6 1 4 . 0 1 4 9 . 0 6 4 9 . 0 6 0 9 . 0 9 9 8 . 0 5 2 9 . 0 9 1 9 . 0 2 5 5 . 0 0 2 5 . 0 3 5 9 . 0 9 6 9 . 0 0 3 9 . 0 6 0 9 . 0 6 9 8 . 0 6 5 8 . 0 3 5 5 . 0 5 2 5 . 0 3 6 9 . 0 9 7 9 . 0 1 3 9 . 0 8 0 9 . 0 0 4 9 . 0 6 1 9 . 0 0 8 5 . 0 0 7 5 . 0 7 4 9 . 0 7 7 9 . 0 2 2 9 . 0 1 0 9 . 0 1 2 9 . 0 4 2 9 . 0 5 7 5 . 0 3 7 5 . 0 1 6 9 . 0 1 7 9 . 0 8 1 9 . 0 0 1 9 . 0 1 1 9 . 0 1 5 8 . 0 4 5 5 . 0 3 2 5 . 0 6 4 9 . 0 0 7 9 . 0 9 0 9 . 0 9 6 8 . 0 6 3 9 . 0 3 9 8 . 0 4 1 5 . 0 0 2 5 . 0 8 4 9 . 0 7 7 9 . 0 4 4 9 . 0 2 5 9 . 0 5 5 9 . 0 0 5 9 . 0 8 4 5 . 0 3 7 5 . 0 4 5 9 . 0 5 8 9 . 0 8 2 9 . 0 1 3 9 . 0 8 2 9 . 0 3 1 9 . 0 6 8 5 . 0 3 6 5 . 0 3 5 9 . 0 9 7 9 . 0 8 2 9 . 0 1 2 9 . 0 4 4 9 . 0 0 3 9 . 0 7 7 5 . 0 0 6 5 . 0 8 5 9 . 0 1 8 9 . 0 1 3 9 . 0 6 3 9 . 0 4 3 9 . 0 1 3 9 . 0 7 2 5 . 0 0 4 5 . 0 0 5 9 . 0 9 7 9 . 0 9 1 9 . 0 2 3 9 . 0 9 4 9 . 0 8 2 9 . 0 3 3 5 . 0 1 1 5 . 0 1 6 9 . 0 0 8 9 . 0 2 2 9 . 0 9 1 9 . 0 5 3 9 . 0 7 0 9 . 0 4 2 6 . 0 9 7 5 . 0 5 4 9 . 0 3 8 9 . 0 9 1 9 . 0 2 0 9 . 0 1 4 9 . 0 9 0 9 . 0 9 0 6 . 0 8 6 5 . 0 7 5 9 . 0 0 8 9 . 0 e s a B - 5 E l a u g n i l i t l u M E S U l a u g n i l i t l u M ) o k c e G ( 2 M L a P E S B a L p u D - l a i t r a P m S T E R i h s a H m S i h s a H n i M p u D i - r a e N m S T E R . ) 2 t r a p ( k r a m h c n e b D 3 T N 4 W e h t n o s m h t i r o g l a d n a s l e d o m g n i d d e b m e s u o i r a v r o f e c n a m r o f r e p 1 @ l l a c e R e g a u g n a l - r e p l l u F : 2 1 e l b a T Preprint A.5 ADDITIONAL ABLATION STUDIES This section includes ablation studies on additional hyperparameters for the RETSim model , includ- ing the loss function , pooling type , and model capacity . α β 2 2 2 2 4 4 4 4 20 20 40 40 20 20 40 40 λ 0.5 1 0.5 1 0.5 1 0.5 1 Recall @ 1 0.982 0.948 0.984 0.919 0.982 0.947 0.986 0.923 Table 13 : Ablation study on Multi-Similarity Loss hyperparameters for RETSim training . Bold indicates the hyperparameter setting selected for the final model . # Blocks Hidden Dim Recall @ 1 2 2 2 2 3 3 3 3 4 4 4 4 64 128 256 512 64 128 256 512 64 128 256 512 0.965 0.980 0.986 0.986 0.962 0.980 0.984 0.987 0.966 0.980 0.985 0.986 Table 14 : Ablation study for RETSim model capacity and size ( number of GAU blocks and hidden dimension for the blocks ) . Bold indicates the hyperparameter setting selected for the final model . Pooling Type Recall @ 1 Average Pooling Max Pooling Generalized Mean Pooling 0.985 0.983 0.986 Table 15 : Ablation study on pooling type for the RETSim model . Bold indicates the hyperparameter setting selected for the final model . A.6 SELECTED EXAMPLES FROM NEWS-COPY DATASET In this section , we randomly selected a set of false positives and false negatives for RETSim on the NEWS-COPY deduplication dataset to provide further insight into the results . 17 Preprint Text 1 chauffeur , a policeman and a passing jour- nalist who tried to intervene . Beaton and the policeman were reported in serious condition . The 23-year-old princess and her husband of five months , Capt . Mark Phillips , were not hurt . But police experts said the holes left by one of the bullets fired into the car indicated it passed between them , missing them by inch- es . A police informant said it was believed 11 shots were fired by the assailant . Experts were studying two revolvers found at the scene . They said fi ... By United Press tnfernational Ay SSAST OR BE FRE NG SG The federal government has proposed new methods of eoustructing federal buildings in a move to save ad- ditional en- ergy and suggested ils elfort could be adapted to all new buildings , Washington , Jan. 27 . — ( P ) —Im- mediate removal of John F. J. Her- bert , as prohibition administrator for Montana and Idaho , was de- manded in the senate today by Sen- ators Borah , Idaho , and Whe´eeler , Montana , on the ground of charges placed before them by department of justice investigators . Wheeler accompanied his demand ( Continued on Page 2 ) By RAYMOND CLAPPEA ( Dnited Presa Stal Correspandoayy London , Jai , 38— ( UP—-The Am ‘ erlcnn delegation to the navat confer ence today won ls demand for pre- sentation : of the cnse of suxiliary warships limitation first at tho noxt plenary session Thuvaday , ‘ Tho chlet delegates , mec- tittg at St. James palace , also decided that tho plenary sesslon would discuss the Main con- ference questions in alpha betical order of ihe countriea pro- posing . Press ta be Admitted The American delegation woo a second vic- tory whe ... Text 2 ‘ LONDON ( AP ) — Ian Ball , a 26-year- old unemployed Englishman , was brought into court today and charged with attempted mur- der during an at- tempt to kidnap Princess Anne from her car in the heart of London Wed- nesday night . Ball , lean-faced and bearded , stood stiffly in the dock at the Bow Street Magistrate ’ s court , handcuffed to two detectives . He spoke only once during his 60- second appearance , saying iha London accent : “ I want to apply for legal aid. ” The court or- dered him held for another hearing on Ma ... Hy United Press International The federal government has Proposed new methods of constructing federal buildings in a move lo save addilional energy and suggested ils effort could be adapted to all new buildings , Arthur F , Sampson , General Services Administration ad- ministrater , said new features for such construction would include the collection of rain waler for cooling and irriga- tion , solar energy collectors and the covering of exterior walls with earth . “ Whal we are saying is that these design criteri ... — Washington , Jan. 27 1 AP ) .—Immiedl- aie mmoval of John F. Herbert as pro- — hi- bition administrator for Montana and ‘ Idaho was demanded m the Seuate to- ‘ day by Sen- ators Borah . idaho , and Waeeler , Montana . on the ground of charges placed before them by Depart- meat of Justice investigators . Wheeler accompanied his demand nith a declaration that prohibition en- foreemen : had brukea down . He blamed the “ politicians ” and called upon the Law Enforcement Commussion to sum- mon members of the Republican Na- tona ... London , Jan. 24 , W.P—The Amer- jean dele- gation fo the naval cen- ference teday won its demand for presentation of the case of auxil- jary warships linsitation flrst at the next ple- trary session ‘ Vhursday , The chief delegates , meeting at Si , James Pelace , also decided that the plenary session would discuss the main confeyence questions in alphabetical order af the cauntries proposing . The American del- egation won a second victory when it was decided to udmil certain representatives of the press at fie plenary ... Table 16 : Example false negatives for RETSim on the NEWS-COPY dataset ( pairs of texts not detected as near-duplicates by RETSim but labeled as near-duplicates in the original dataset ) . Ex- amples are randomly selected and truncated at 512 characters for display . 18 Preprint Text 1 BOZEMAN , Mont . ( AP ) — Chet Huntley , whose resonant voice and rough-hewn face be- came familiar to millions on the nightly television news , died Wednesday in his moun- tain resort home . He was 62 . He underwent surgery for lung cancer in January but had remained activesuntil recent weeks . He died at 2:20 a.m , according to his widow , Tippy Hunt.cy . Huntiey was teamed for 14 years with David Brinkley on NBC ’ s Huntley- Brinkley Re- port . He quit in 1970 and re- turned to his native Montana to develop the $ 20-millio ... By THE ASSOCIATED PRESS Some Amer- icans are paying up to 50 per cent more per month for electricity this year than they did last , an Associ- ated Press survey shows . Con- sumers are beginning to organize to fight the rate hikes . A spot check of monthly elec- tric bills this year and last showed that most in- creases have been about $ 1 or $ 2 , gen- erally about 10 per cent , with the highest reported boost com- ing in Jacksonville , Fia. , where the average tab went from $ 17.90 last year to $ 27.70 this year . Utility ... BOZEMAN , Mont . ( AP ) — Vice President Gerald R. Ford says the world will miss the “ ‘ unique abilities ” of former television news anchorman Chet Huntley . Huntley , 62 , died at his home Wednesday after a long bout with lung cancer . Family ‘ spokesmen said a memorial service would be conducted for Huntley Sunday at the Big Sky of Montana ’ resort and recreation area south of Bozeman . Huntley was chairman of the Big Sky board of directors . Another memorial service is scheduled Tuesday in the New York studios of the ... WASHINGTON ( AP ) — The House has passed legislation raising the minimum wage from $ 1.60 an hour to $ 2 this year for most workers covered and to $ 2.30 for all by 1978 . The bill , approved Wednesday 375 to 37 , also would increase by 7 million to 56.5 million the number of workers covered by the mini- mum wage laws . The bill is a modified ver- sion of one President Nixon vetoed last year . However , he is expected to sign this one if it is finally approved after ad- justment with a similar Senate passed measure , altho ... Text 2 BOZEMAN , Mont . ( AP ) - Chet Huntley , whose resonant voice and rough-hewn face became familiar to millions on the nightly television news , died Wednesday in his moun- tain resort home . He was 62 . He underwent surgery for lung cancer in January but had remained active until recent weeks . He died at 2:20 a.m. , according to his widow , Tippy Huntley . Huntley was teamed for 14 years with David Brinkley on NBC ’ s Huntley- Brinkley Report . He quit in 1970 and returned to his native Montana to develop the $ 20 mil- lion Bi ... By Louise Cook Acenciaiod Prece Writer Same Americans are paying up io 20 per cent more per month far electricity this year ihan they did last , an -Associ- Press survey shows . onsumers are beginning to ze to fight the rate hikes , A spot check of monthly elec- tre hills this year and Jast showed that most increases ve been about $ 1 or $ 2 , gen- erally about 10 per cent , with the highest reported boost com- ing in Jacksonville , Fla. , where the average tab went from $ 17.90 last year to $ 27.70 this year ... BOZEMAN , Mont . ( AP ) — Vice President Gerald R. Ford says the world will miss the “ unique abilities ” of former television news anchorman Chet Huntley . Huntley , 62 , died at his home Wednesday after a long bout with lung cancer . Family spokesmen said a me- morial service would be con- ducted for Hunt- ley Sunday at the Big- Sky of Montana resort and recreation area south of Bozeman . Hunt- ley was chair- man of the Big Sky board of directors . Another memorial service is sched- uled Tuesday in the New York studios of ... WASHINGTON ( AP ) — The House has passed legislation raising the minimum wage from $ 1.60 an hour to $ 2 this year for most workers covered and to $ 2.30 for all by 1978 . The bill , approved Wednes- day 375 to 37 , also would in- crease by 7 million to 56.5 mil- lion the number of workers cov- ered by the minimum wage laws . The bill is a modified version of one President Nixon vetoed last year . However , he is ex- ted to sign this one if it is inally approved after adjust- ment with a similar Senate- passed measu ... Table 17 : Example false positives for RETSim on the NEWS-COPY dataset ( pairs of texts detected as near-duplicates by RETSim but not labeled as near-duplicates in the original dataset ) . Examples are randomly selected and truncated at 512 characters for display . 19","['l', 'c', 'c', 'r', 'preprint', 'retsim', 'resilient', 'efficient', 'text', 'similarity', 'tanay', 'vakharia1', 'paper', 'introduce', 'retsim', 'resilient', 'efficient', 'text', 'similarity', 'lightweight', 'multilingual', 'deep', 'learning', 'model', 'train', 'produce', 'robust', 'metric', 'embedding', 'nearduplicate', 'text', 'retrieval', 'clustering', 'dataset', 'deduplication', 'task', 'demonstrate', 'retsim', 'significantly', 'robust', 'accurate', 'minhash', 'neural', 'text', 'embedding', 'achieve', 'new', 'stateoftheart', 'mance', 'dataset', 'deduplication', 'adversarial', 'text', 'retrieval', 'benchmark', 'spam', 'clustering', 'task', 'also', 'introduce', 'benchmark', 'wiki40b', 'sarial', 'neart3xt', 'dataset', 'evaluate', 'multilingual', 'nearduplicate', 'text', 'retrieval', 'capability', 'adversarial', 'setting', 'retsim', 'benchmark', 'opensource', 'mit', 'license', 'githubcomgoogleunisim', 'introduction', 'robust', 'nearduplicate', 'text', 'detection', 'essential', 'component', 'many', 'task', 'include', 'retriev', 'ing', 'document', 'detect', 'plagiarism', 'sun', 'block', 'adversarial', 'spam', 'cam', 'paign', 'ahme', 'user', 'come', 'expect', 'system', 'return', 'accurate', 'result', 'query', 'exhibit', 'rate', 'hagen', 'furthermore', 'ciently', 'deduplicate', 'text', 'dataset', 'critical', 'training', 'stateoftheart', 'large', 'language', 'model', 'kandpal', 'decade', 'minhashbase', 'broder', 'localitysensitive', 'hashing', 'lsh', 'prevalent', 'use', 'nearduplicate', 'detection', 'simplicity', 'robust', 'ness', 'speed', 'example', 'vast', 'majority', 'dataset', 'deduplication', 'effort', 'still', 'rely', 'kocetkov', 'however', 'lshbase', 'technique', 'minhash', 'downside', 'chief', 'parametersensitive', 'require', 'heavy', 'tuning', 'additionally', 'minhash', 'lack', 'resilience', 'typo', 'reliance', 'ngram', 'lead', 'poor', 'performance', 'noisy', 'datum', 'vulnerability', 'hashbuste', 'attack', 'issac', 'hand', 'deep', 'learning', 'model', 'dominant', 'way', 'perform', 'vectorbase', 'semantic', 'text', 'retrieval', 'far', 'neural', 'embedding', 'able', 'consistently', 'outperform', 'minhash', 'robust', 'nearduplicate', 'detection', 'silcock', 'mostly', 'due', 'focus', 'improve', 'semantic', 'capability', 'lead', 'model', 'large', 'run', 'extremely', 'quickly', 'use', 'level', 'tokenization', 'resilient', 'typo', 'adversarial', 'attack', 'fill', 'gap', 'introduce', 'retsim', 'resilient', 'efficient', 'text', 'similarity', 'lightweight', 'mul', 'tilingual', 'deep', 'learning', 'model', 'train', 'specifically', 'produce', 'robust', 'neural', 'embedding', 'specialize', 'nearduplicate', 'detection', 'combine', 'stateoftheart', 'retvec', 'text', 'vectorizer', 'modern', 'transformer', 'block', 'large', 'typoaugmente', 'training', 'corpus', 'metric', 'learn', 'ing', 'training', 'regime', 'retsim', 'able', 'achieve', 'new', 'stateoftheart', 'performance', 'nearduplicate', 'detection', 'benchmark', 'section', 'dataset', 'deduplication', 'task', 'section', 'spam', 'clustering', 'application', 'section', 'furthermore', 'dataset', 'benchmark', 'exist', 'deduplication', 'nearduplicate', 'text', 'retrieval', 'none', 'focus', 'systematically', 'evaluate', 'nearduplicate', 'retrieval', 'mance', 'presence', 'word', 'manipulation', 'sentence', 'paragraphlevel', 'work', 'author', 'internship', 'preprint', 'tion', 'address', 'need', 'additionally', 'introduce', 'benchmark', 'wiki40b', 'sarial', 'neart3xt', 'dataset', 'enable', 'evaluation', 'algorithm', 'adversarial', 'nearduplicate', 'text', 'retrieval', 'multilingual', 'setting', 'report', 'performance', 'retsim', 'minhash', 'pop', 'ular', 'neural', 'embedding', 'universal', 'sentence', 'encoder', 'cer', 'labse', 'new', 'benchmark', 'section', 'highlight', 'uneven', 'performance', 'guage', 'type', 'adversarial', 'manipulation', 'retsim', 'model', 'benchmark', 'opensource', 'https', 'githubcomgoogleunisim', 'mit', 'license', 'relate', 'work', 'nearduplicate', 'detection', 'identify', 'noisy', 'nearduplicate', 'document', 'large', 'corpus', 'fun', 'damental', 'task', 'wide', 'range', 'application', 'detect', 'plagiarism', 'finding', 'reproduce', 'content', 'literature', 'news', 'article', 'gyawali', 'silcock', 'deduplicat', 'ing', 'training', 'dataset', 'language', 'model', 'previous', 'research', 'show', 'duplicate', 'training', 'dataset', 'lead', 'inefficient', 'training', 'privacy', 'concern', 'large', 'language', 'model', 'llm', 'model', 'memorize', 'regenerate', 'duplicate', 'training', 'sequence', 'much', 'high', 'frequency', 'kandpal', 'semantic', 'text', 'similarity', 'task', 'identify', 'textual', 'nearduplicate', 'predominate', 'nonneural', 'ngrambase', 'algorithm', 'minhash', 'broder', 'widely', 'use', 'technique', 'deduplicate', 'large', 'training', 'corpus', 'kocetkov', 'minhash', 'technique', 'estimate', 'jaccard', 'similarity', 'set', 'algorithm', 'minhash', 'simhash', 'charikar', 'combine', 'localitysensitive', 'hashing', 'lsh', 'technique', 'fast', 'approximate', 'near', 'neighbor', 'search', 'datum', 'cluster', 'allow', 'scale', 'deduplicate', 'corpus', 'contain', 'terabyte', 'datum', 'c4', 'stack', 'kocetkov', 'however', 'ngram', 'shinglingbase', 'technique', 'typically', 'require', 'text', 'parse', 'standardized', 'form', 'eg', 'lowercase', 'strip', 'punctuation', 'make', 'susceptible', 'typo', 'adversarial', 'attack', 'pose', 'challenge', 'attempt', 'differentiate', 'dissimilar', 'document', 'nearduplicate', 'document', 'adversarial', 'augmentation', 'semantic', 'text', 'similarity', 'task', 'compute', 'semantic', 'similarity', 'text', 'closely', 'late', 'nearduplicate', 'detection', 'semantic', 'text', 'similarity', 'refer', 'assessment', 'semantic', 'relatedness', 'piece', 'text', 'base', 'meaning', 'rather', 'syntactic', 'structure', 'case', 'nearduplicate', 'detection', 'recently', 'transformerbase', 'language', 'model', 'universal', 'sentence', 'encoder', 'labse', 'llmbase', 'embedding', 'anil', 'embed', 'text', 'highdimensional', 'embed', 'vector', 'successfully', 'use', 'retrieve', 'semanticallyrelate', 'document', 'use', 'cosine', 'similarity', 'modern', 'text', 'retrieval', 'sys', 'tem', 'combine', 'embedding', 'approximate', 'near', 'search', 'efficiently', 'retrieve', 'document', 'match', 'user', 'query', 'however', 'language', 'model', 'show', 'vulnerable', 'adversarial', 'attack', 'naturally', 'occur', 'typo', 'gao', 'furthermore', 'language', 'model', 'typically', 'large', 'costly', 'run', 'even', 'hardware', 'acceleration', 'make', 'unsuited', 'largescale', 'dataset', 'deduplication', 'identify', 'nearduplicate', 'presence', 'typo', 'adversarial', 'text', 'manipulation', 'metric', 'learn', 'metric', 'learning', 'aim', 'learn', 'embed', 'space', 'similar', 'item', 'small', 'distance', 'embedding', 'dissimilar', 'item', 'far', 'away', 'many', 'stateof', 'theart', 'embedding', 'use', 'metric', 'learning', 'unsupervised', 'training', 'finetuning', 'include', 'sentence', 'reimer', 'gurevych', 'retvec', 'resilient', 'multilingual', 'embedding', 'text', 'vectorizer', 'train', 'robust', 'various', 'form', 'characterlevel', 'typo', 'adversarial', 'attack', 'extend', 'retvec', 'training', 'regime', 'full', 'text', 'document', 'retsim', 'use', 'multisimilarity', 'loss', 'pairbase', 'metric', 'learning', 'typoladen', 'nearduplicate', 'version', 'text', 'train', 'close', 'embed', 'space', 'text', 'push', 'far', 'away', 'multisimilarity', 'loss', 'base', 'general', 'weighting', 'framework', 'pairbase', 'loss', 'achieve', 'stateoftheart', 'performance', 'outperform', 'alternative', 'triplet', 'loss', 'schroff', 'preprint', 'figure', 'retsim', 'model', 'architecture', 'diagram', 'retsim', 'work', 'arbitrary', 'length', 'text', 'split', 'ting', 'text', 'chunk', 'character', 'vectorization', 'phase', 'encode', 'use', 'retvec', 'character', 'vectorizer', 'retsim', 'model', 'embed', 'chunk', 'text', 'partial', 'embedding', 'combine', 'produce', 'global', 'embed', 'retsim', 'architecture', 'retsim', 'model', 'compose', 'main', 'component', 'depict', 'figure', 'characterlevel', 'vectorizer', 'split', 'input', 'text', 'chunk', 'character', 'use', 'retvec', 'chararcter', 'encoder', 'encode', 'chunk', 'result', 'batch', 'dense', 'input', 'retvec', 'character', 'vectorizer', 'encode', 'unicode', 'character', 'compact', '24bit', 'binary', 'representation', 'base', 'integer', 'codepoint', 'value', 'allow', 'vectorizer', 'encode', 'valid', 'unicode', 'character', 'support', 'language', 'furthermore', 'characterlevel', 'vectorizer', 'show', 'resilient', 'typo', 'adversarial', 'attack', 'small', 'transformer', 'model', 'use', 'compute', 'embedding', 'chunk', 'input', 'text', 'retsimpartialdup', 'use', 'embedding', 'directly', 'find', 'document', 'match', 'chunk', 'text', 'architecturally', 'model', 'consist', 'gate', 'attention', 'unit', 'gau', 'block', 'dense', 'et', 'follow', 'generalizedmean', 'pooling', 'layer', 'projection', 'layer', 'project', 'embed', 'dimension', 'l2', 'normalization', 'layer', 'model', '536k', 'parameter', 'order', 'magnitude', 'small', 'neural', 'embedding', 'table', 'l2normalization', 'allow', 'embedding', 'compare', 'use', 'cosine', 'similarity', 'discuss', 'impact', 'key', 'architecture', 'design', 'choice', 'section', 'hyperparameter', 'detail', 'provide', 'a11', 'additional', 'ablation', 'result', 'appendix', 'a5', 'embed', 'averaging', 'module', 'use', 'combine', 'partial', 'text', 'embedding', 'fulltext', 'embed', 'use', 'global', 'nearduplicate', 'match', 'retsimneardup', 'average', 'chunk', 'embedding', 'produce', 'global', 'embedding', 'standard', 'technique', 'use', 'many', 'model', 'cer', 'support', 'infinite', 'length', 'input', 'costefficient', 'manner', 'experiment', 'aggregation', 'technique', 'produce', 'accurate', 'global', 'embedding', 'include', 'train', 'deep', 'averaging', 'network', 'iyyer', 'improve', 'performance', 'result', 'high', 'computation', 'cost', 'retsimneardup', 'retsimpartialdup', 'compute', 'single', 'forward', 'pass', 'make', 'computationally', 'efficient', 'output', 'type', 'embedding', 'different', 'application', 'retsimneardup', 'bettersuite', 'fulltext', 'matching', 'retrieval', 'section', 'retsimpartialdup', 'use', 'find', 'partial', 'text', 'match', 'nearduplicate', 'content', 'appear', 'part', 'document', 'section', 'model', 'training', 'dataset', 'use', 'multilingual', 'c4', 'dataset', 'mc4', 'raw', 'text', 'datum', 'follow', 'xue', 'use', 'language', 'sample', 'exponent', 'balance', 'sampling', 'low', 'highresource', 'language', 'use', 'text', 'contain', 'least', 'character', 'randomly', 'select', 'sentence', 'roughly', 'character', 'text', 'chunk', 'example', 'input', 'text', 'vector', 'chunk', 'vector', 'chunk', 'vectorized', 'text', 'numchunk', 'retsim', 'model', 'retsim', 'partialdup', 'numchunk', 'retsim', 'neardup', 'r', 'e', 'c', 'r', 'h', 'c', 'e', 'r', 'g', 'n', 'l', 'p', 'l', 'r', 'p', 'e', 'l', 'r', 'r', 'e', 'g', 'e', 'preprint', 'training', 'dataset', 'generate', 'pair', 'augment', 'example', 'apply', 'level', 'augmentation', 'example', 'text', 'chunk', 'order', 'sentencelevel', 'wordlevel', 'characterlevel', 'level', 'randomly', 'select', 'augmentation', 'apply', 'follow', 'category', 'insertion', 'deletion', 'substitution', 'transposition', 'randomly', 'apply', 'sentencelevel', 'mentation', 'combine', 'character', 'wordlevel', 'augmentation', 'empirically', 'find', 'increase', 'percentage', 'augmentation', 'point', 'cause', 'retsim', 'performance', 'degrade', 'full', 'list', 'augmentation', 'use', 'find', 'a2', 'training', 'procedure', 'train', 'retsim', 'use', 'multisimilarity', 'loss', 'β', 'ϵ', 'hypertune', 'parameter', 'result', 'show', 'appendix', 'train', 'step', 'batch', 'size', 'similarity', 'loss', 'train', 'model', 'embed', 'augment', 'version', 'text', 'close', 'embed', 'space', 'dissimilar', 'text', 'push', 'far', 'apart', 'use', 'optimizer', 'learning', 'rate', 'cosine', 'decay', 'detailed', 'training', 'hyperparameter', 'report', 'a12', 'evaluation', 'modelalgorithm', 'type', 'embedhash', 'size', 'model', 'parameter', 'labse', 'multilingual', 'use', 'multilingual', 'e5base', 'palm', 'simhash', 'minhash', 'retsim', 'neural', 'neural', 'neural', 'neural', 'hash', 'hash', 'neural', 'b', 'bit', 'hash', 'table', 'embed', 'model', 'hash', 'algorithm', 'benchmarke', 'paper', 'model', 'algorithm', 'evaluate', 'benchmark', 'retsim', 'multilingual', 'semantic', 'text', 'embedding', 'well', 'popular', 'gram', 'base', 'algorithm', 'primarily', 'use', 'nearduplicate', 'text', 'detection', 'table', 'baseline', 'text', 'embedding', 'include', 'multilingual', 'universal', 'sentence', 'encoder', 'labse', 'multilingual', 'palm', 'gecko', 'embedding', 'anil', 'text', 'embedding', 'l2normalize', 'compare', 'use', 'cosine', 'similarity', 'use', 'exact', 'search', 'index', 'retrieve', 'near', 'neighbor', 'vector', 'index', 'experiment', 'section', 'nonneural', 'nearduplicate', 'detection', 'cluster', 'algorithm', 'select', 'popular', 'algorithm', 'minhash', 'broder', 'simhash', 'charikar', 'minhash', 'use', 'datasketch', 'minhashlsh', 'library', 'follow', 'common', 'practice', 'literature', 'silcock', 'use', 'hash', 'function', 'minhash', 'otherwise', 'specify', 'use', 'wordlevel', 'ngram', 'select', 'good', 'value', 'n', 'simhash', 'use', 'bit', 'simhash', 'conduct', 'shingle', 'character', 'level', 'shingle', 'size', 'select', 'n', 'nearduplicate', 'detection', 'benchmark', 'newscopy', 'core', 'duplicate', 'dataset', 'tune', 'optimal', 'deduplication', 'threshold', 'base', 'cosine', 'similarity', 'neuralbased', 'embedding', 'jaccard', 'similarity', 'minhash', 'detailed', 'hyperparameter', 'setting', 'retsim', 'baseline', 'algorithm', 'use', 'evaluation', 'find', 'w4nt3d', 'wiki40b', 'neart3xt', 'dataset', 'evaluation', 'dataset', 'description', 'vast', 'majority', 'text', 'retrieval', 'benchmark', 'focus', 'evaluate', 'semantic', 'performance', 'good', 'knowledge', 'multilingual', 'benchmark', 'sys', 'tematically', 'measure', 'adversarial', 'robustness', 'nearduplicate', 'text', 'retrieval', 'attempt', 'fill', 'gap', 'create', 'publish', 'benchmark', 'wiki40b', 'dataset', 'contain', 'pair', 'syntactically', 'similar', 'text', 'evaluate', 'nearduplicate', 'text', 'trieval', 'presence', 'various', 'form', 'text', 'manipulation', 'typo', 'w4nt3d', 'base', 'wiki40b', 'dataset', 'guo', 'dataset', 'split', 'query', 'exam', 'ple', 'target', 'example', 'query', 'example', 'syntheticallymodifie', 'nearduplicate', 'version', 'preprint', 'target', 'example', 'eg', 'typo', 'language', 'split', 'wiki40b', 'randomly', 'select', 'text', 'length', 'target', 'string', 'uniformly', 'select', 'character', 'order', 'test', 'performance', 'short', 'long', 'text', 'construct', 'query', 'text', 'corre', 'sponde', 'target', 'text', 'randomly', 'apply', 'word', 'character', 'augmentation', 'sentence', 'paragraph', 'augmentation', 'augmentation', 'uniformly', 'select', 'insert', 'delete', 'substitute', 'swap', 'operation', 'use', 'main', 'metric', 'follow', 'setup', 'commonly', 'find', 'semantic', 'text', 'retrieval', 'benchmark', 'modelalgorithm', 'arabic', 'chinese', 'english', 'german', 'french', 'spanish', 'japanese', 'korean', 'russian', 'avg', 'lang', 'labse', 'multilingual', 'use', 'multilingual', 'e5base', 'palm', 'simhash', 'minhash', 'retsimpartialdup', 'retsimneardup', 'table', 'perlanguage', 'retrieval', 'performance', 'various', 'embed', 'model', 'algorithm', 'benchmark', 'result', 'select', 'language', 'report', 'average', 'recall', 'language', 'full', 'result', 'language', 'report', 'multilingual', 'performance', 'overall', 'retsimneardup', 'achieve', 'average', 'recall', 'language', 'benchmark', 'table', 'retsimpartialdup', 'second', 'good', 'recall', 'multilingual', 'bestperforme', 'baseline', 'third', 'aver', 'age', 'recall', 'expect', 'retsimneardup', 'outperform', 'retsimpartialdup', 'benchmark', 'require', 'find', 'nearduplicate', 'find', 'simi', 'lar', 'text', 'retsimpartialdup', 'optimize', 'find', 'similar', 'chunk', 'text', 'corpus', 'always', 'similar', 'text', 'overall', 'similarly', 'hypothesize', 'minhash', 'simhash', 'form', 'poorly', 'benchmark', 'lack', 'ability', 'distinguish', 'similar', 'text', 'nearduplicate', 'detect', 'embeddingbase', 'model', 'cosine', 'similarity', 'offer', 'finegrained', 'measure', 'similarity', 'retsimneardup', 'outperform', 'baseline', 'algorithm', 'language', 'chinese', 'language', 'theorize', 'semantic', 'embedding', 'slight', 'edge', 'performance', 'significantly', 'large', 'model', 'size', 'large', 'retsim', 'show', 'ble', 'allow', 'well', 'representation', 'language', 'large', 'character', 'set', 'furthermore', 'level', 'tokenizer', 'use', 'baseline', 'embedding', 'often', 'treat', 'character', 'chinese', 'japanese', 'individual', 'token', 'offer', 'high', 'resilience', 'figure', 'recall', 'performance', 'benchmark', 'break', 'augmentation', 'type', 'result', 'average', 'language', 'split', 'w4nt3d', 'adversarial', 'resilience', 'delve', 'deeply', 'impact', 'various', 'type', 'text', 'manipulation', 'veal', 'retsimneardup', 'retsimpartialdup', 'perform', 'almost', 'equally', 'regardless', 'type', 'augmentation', 'apply', 'figure', 'semantic', 'text', 'embedding', 'perform', 'well', 'paragraph', 'sentence', 'wordlevel', 'manipulation', 'expect', 'exhibit', 'significantly', 'weak', 'performance', 'characterlevel', 'minhash', 'simhash', 'struggle', 'wordlevel', 'augmentation', 'deeplearne', 'base', 'embedding', 'collapse', 'characterlevel', 'typo', 'introduce', 'l', 'c', 'e', 'r', 'paragraph', 'sentence', 'word', 'character', 'augmentation', 'level', 'labse', 'multilingual', 'use', 'multilingual', 'e5base', 'palm', 'simhash', 'minhash', 'retsim', 'partialdup', 'retsim', 'neardup', 'preprint', 'tribute', 'retsim', 'resilience', 'adversarial', 'manipulation', 'retvec', 'character', 'encoder', 'well', 'use', 'deep', 'metric', 'learning', 'train', 'robust', 'embedding', 'figure', 'report', 'recall', 'performance', 'algorithm', 'amount', 'augmentation', 'crease', 'algorithm', 'perform', 'perfectly', 'augmentation', 'apply', 'exact', 'matching', 'percentage', 'augmentation', 'increase', 'ngram', 'base', 'approach', 'exhibit', 'steep', 'drop', 'mance', 'semantic', 'text', 'embedding', 'able', 'sustain', 'large', 'degree', 'augmentation', 'retrieval', 'capability', 'start', 'degrade', 'retsimneardup', 'robust', 'noticeable', 'drop', 'performance', 'around', 'augmentation', 'make', 'retsim', 'effective', 'approach', 'cluster', 'deduplicate', 'text', 'adversarial', 'setting', 'figure', 'recall', 'performance', 'benchmark', 'vary', 'max', 'target', 'length', 'figure', 'recall', 'performance', 'benchmark', 'amount', 'augmenta', 'tion', 'apply', 'query', 'text', 'increase', 'text', 'length', 'impact', 'performance', 'figure', 'report', 'recall', 'performance', 'retsim', 'baseline', 'algorithm', 'length', 'query', 'target', 'text', 'vary', 'see', 'retsimneardup', 'retsimpartialdup', 'outperform', 'method', 'short', 'text', 'character', 'text', 'length', 'increase', 'character', 'retsimneardup', 'remain', 'close', 'perfect', 'retsimpartialdup', 'performance', 'degrade', 'split', 'text', 'multiple', 'embedding', 'find', 'near', 'matching', 'chunk', 'text', 'minhash', 'simhash', 'also', 'perform', 'poorly', 'short', 'text', 'length', 'start', 'degrade', 'long', 'text', 'neuralbased', 'embedding', 'observe', 'slight', 'drop', 'performance', 'long', 'text', 'model', 'retsimneardup', 'multilingual', 'use', 'embedding', 'handle', 'arbitrary', 'length', 'input', 'realworld', 'nearduplicate', 'evaluation', 'setup', 'benchmark', 'retsim', 'ability', 'identify', 'nearduplicate', 'content', 'realworld', 'dataset', 'literature', 'newscopy', 'deduplication', 'dataset', 'silcock', 'contain', 'historical', 'news', 'article', 'positive', 'duplicate', 'pair', 'dataset', 'consist', 'noisy', 'near', 'duplicate', 'due', 'factor', 'ocr', 'error', 'plagiarism', 'news', 'aggregation', 'also', 'evaluate', 'algorithm', 'core', 'nearduplicate', 'dataset', 'gyawali', 'consist', 'scholarly', 'article', '25k', 'exact', 'duplicate', '25k', 'nearduplicate', 'nonduplicate', 'duplicate', 'dataset', 'arise', 'article', 'revision', 'versioning', 'metadata', 'difference', 'man', 'key', 'difference', 'benchmark', 'benchmark', 'benchmark', 'focus', 'detect', 'cluster', 'nearduplicate', 'text', 'rather', 'robust', 'text', 'retrieval', 'base', 'syntactic', 'similarity', 'benchmark', 'follow', 'experimental', 'setup', 'provide', 'paper', 'report', 'adjust', 'index', 'ari', 'newscopy', 'dataset', 'report', 'precisionrecallf1', 'score', 'core', 'nearduplicate', 'dataset', 'result', 'newscopy', 'dataset', 'retsimpartialdup', 'outperform', 'approach', 'significant', 'margin', 'ari', 'compare', 'good', 'minhash', 'result', 'report', 'table', 'dataset', 'many', 'nearduplicate', 'pair', 'text', 'significantly', 'long', 'expect', 'retsimpartialdup', 'find', 'match', 'text', 'chunk', 'document', 'suited', 'task', 'outperform', 'retsimneardup', 'bucket', 'nearduplicate', 'detection', 'rate', 'length', 'ratio', 'positive', 'pair', 'figure', 'observe', 'retsimpartialdup', 'outperform', 'minhash', 'regardless', 'length', 'ratio', 'minhash', 'surpasse', 'retsimneardup', 'formance', 'text', 'roughly', '15x', 'length', 'text', 'nearduplicate', 'pair', 'l', 'c', 'e', 'r', 'target', 'text', 'length', 'c', 'e', 'r', 'text', 'augmentation', 'amount', 'labse', 'multilingual', 'use', 'multilingual', 'e5base', 'palm', 'simhash', 'minhash', 'retsim', 'partialdup', 'retsim', 'neardup', 'preprint', 'modelalgorithm', 'multilingual', 'use', 'multilingual', 'e5base', 'sbert', 'simhash', 'minhash', 'minhash', 'retsimpartialdup', 'retsimneardup', 'table', 'performance', 'comparison', 'newscopy', 'dataset', 'adjust', 'rand', 'index', 'value', 'report', 'denote', 'result', 'figure', 'nearduplicate', 'detection', 'rate', 'sim', 'minhash', 'different', 'length', 'ratio', 'pos', 'itive', 'pair', 'length', 'long', 'divide', 'short', 'text', 'round', 'near', 'integer', 'additionally', 'notice', 'label', 'dataset', 'occasionally', 'noisy', 'substantial', 'por', 'tion', 'retsim', 'false', 'positive', 'appear', 'nearduplicate', 'inspection', 'core', 'nearduplicate', 'dataset', 'table', 'document', 'article', 'title', 'abstract', 'roughly', 'size', 'retsimpartialdup', 'retsimneardup', 'performance', 'roughly', 'equivalent', 'method', 'outperform', 'baseline', 'term', 'macro', 'f1', 'score', 'accuracy', 'use', 'minhash', 'lsh', 'hash', 'function', 'computational', 'efficiency', 'recommend', 'datasketch', 'library1', 'well', 'accuracy', 'default', 'set', 'deduplication', 'threshold', 'detail', 'hyperpa', 'rameter', 'setting', 'algorithm', 'nearduplication', 'dataset', 'find', 'model', 'exact', 'title', 'match', 'labse', 'multilingual', 'use', 'multilingual', 'e5base', 'minhash', 'lsh', 'retsimpartialdup', 'retsimneardup', 'precision', 'duplicate', 'duplicate', 'precision', 'accuracy', 'table', 'evaluation', 'result', 'core', 'nearduplicate', 'dataset', 'precisionrecallmacro', 'f1', 'accuracy', 'number', 'report', 'denote', 'result', 'application', 'training', 'dataset', 'deduplication', 'modelalgorithm', 'train', 'example', 'dup', 'train', 'valid', 'example', 'dup', 'train', 'minhash', 'lsh', 'exact', 'substre', 'retsimneardup', 'retsimpartialdup', 'table', 'deduplication', 'rate', 'wiki40b', 'english', 'denote', 'result', 'setup', 'evaluate', 'retsim', 'ability', 'deduplicate', 'text', 'training', 'dataset', 'deduplicate', 'english', 'split', 'wiki40b', 'conservatively', 'set', 'cosine', 'similarity', 'deduplica', 'tion', 'threshold', 'retsimneardup', 'retsimpartialdup', 'limit', 'amount', 'false', 'positive', 'base', 'optimal', 'threshold', 'find', 'evaluation', 'use', 'use', 'arch', 'default', 'vector', 'index', 'approximate', 'near', 'neighbor', 'search', 'vardanian', 'compare', 'big', 'datum', 'look', 'small', 'https', 'githubcomekzhudatasketch', 'e', 'r', 'c', 'e', 'c', 'l', 'r', 'e', 'length', 'ratio', 'nearduplicate', 'text', 'pair', 'retsim', 'partialdup', 'retsim', 'neardup', 'minhash', 'preprint', 'modelalgorithm', 'accelerator', 'batch', 'size', 'embed', 'hash', 'time', 'minhash', 'lsh', 'retsim', 'retsim', 'retsim', 'cpu', 'core', 'onnx', 'cpu', 'core', 'tensorflow', 'rtx', 'tensorflow', 'table', 'embeddinghashe', 'speed', 'retsim', 'minhash', 'lsh', 'wiki40b', 'dataset', 'minhash', 'lsh', 'set', 'number', 'hash', 'function', 'follow', 'kocetkov', 'use', 'jaccard', 'similarity', 'threshold', 'deduplication', 'result', 'overall', 'report', 'table', 'retsimneardup', 'find', 'slightly', 'duplicate', 'wiki', '40b', 'training', 'validation', 'split', 'inline', 'deduplication', 'result', 'section', 'retsimneardup', 'outperform', 'algorithm', 'hand', 'retsimpartialdup', 'find', 'signifi', 'cantly', 'match', 'exact', 'substring', 'match', 'use', 'previous', 'study', 'showcase', 'usefulness', 'perform', 'nearduplicate', 'partialduplicate', 'matching', 'largerthanexpected', 'number', 'partial', 'match', 'indicate', 'machine', 'learn', 'ing', 'practitioner', 'take', 'extra', 'care', 'deduplicate', 'wikipedia', 'chunk', 'level', 'avoid', 'feed', 'duplicate', 'text', 'model', 'term', 'embed', 'speed', 'table', 'retsim', 'significantly', 'slow', 'minhash', 'lsh', 'cpu', '46x', 'slow', 'competitive', 'use', 'desktop', 'gpu', 'rtx', 'slow', 'almost', 'onpar', 'use', 'highend', 'nvidia', '15x', 'slow', 'current', 'code', 'write', 'python', 'fully', 'optimize', 'expect', 'performance', 'gap', 'significantly', 'shrink', 'optimize', 'implementation', 'retsim', 'slow', 'minhash', 'retsim', 'significantly', 'small', 'fast', 'text', 'embed', 'model', 'close', 'performance', 'gap', 'neural', 'nonneural', 'base', 'method', 'nearduplicate', 'text', 'detection', 'dataset', 'deduplication', 'retsimneardup', 'retsimpartialdup', 'return', 'time', 'embed', 'speed', 'indexing', 'retrieval', 'time', 'depend', 'vector', 'index', 'search', 'use', 'long', 'document', 'retsimpartialdup', 'produce', 'embedding', 'retsimneardup', 'retsimpartialdup', 'offer', 'tradeoff', 'finergrained', 'match', 'depend', 'specific', 'vector', 'search', 'dataset', 'use', 'wild', 'spam', 'email', 'cluster', 'section', 'showcase', 'realworld', 'performance', 'cluster', 'nearduplicate', 'text', 'heavily', 'manipulate', 'adversarial', 'attack', 'perform', 'evaluation', 'spam', 'campaign', 'spam', 'constitute', 'strong', 'proving', 'ground', 'nearduplicate', 'cluster', 'algorithm', 'spammer', 'employ', 'adversarial', 'augmentation', 'technique', 'attempt', 'evade', 'detection', 'mentation', 'typically', 'include', 'append', 'prepende', 'unrelated', 'text', 'interleave', 'random', 'word', 'different', 'language', 'intentionally', 'introduce', 'typo', 'abuse', 'extended', 'character', 'set', 'emojis', 'homoglyphs', 'technique', 'collectively', 'refer', 'hashbuste', 'setup', 'dataset', 'consist', 'spam', 'email', 'spam', 'campaign', 'donate', 'gmail', 'user', 'flag', 'reach', 'inboxe', 'example', 'contain', 'email', 'subject', 'concatenate', 'message', 'content', 'email', 'misclassifie', 'spam', 'classifier', 'effective', 'adversarial', 'text', 'manipulation', 'technique', 'make', 'challenging', 'test', 'set', 'cluster', 'evaluation', 'example', 'hashbuste', 'attack', 'adversarial', 'manipulation', 'observe', 'include', 'use', 'homoglpyphs', 'uncommon', 'unicode', 'character', 'set', 'invisible', 'character', 'pad', 'random', 'word', 'different', 'language', 'get', 'ground', 'truth', 'campaign', 'cluster', 'email', 'manually', 'review', 'assign', 'specific', 'spam', 'campaign', 'base', 'similarity', 'human', 'reviewer', 'use', 'agglomerative', 'clustering', 'cluster', 'spam', 'email', 'report', 'homogeneity', 'completeness', 'vmeasure', 'adjust', 'rand', 'index', 'metric', 'result', 'overall', 'observe', 'retsim', 'significantly', 'well', 'cluster', 'nearduplicate', 'adversarial', 'manipulation', 'outperform', 'simhash', 'use', 'metric', 'consider', 'table', 'particular', 'observe', 'retsim', 'outperform', 'use', 'vmeasure', 'score', 'main', 'metric', 'result', 'report', 'section', 'inline', 'observe', 'deploy', 'retsim', 'main', 'nearduplicate', 'detection', 'preprint', 'model', 'vmeasure', 'ari', 'use', 'simhash', 'lsh', 'retsimneardup', 'table', 'performance', 'cluster', 'adversarial', 'spam', 'campaign', 'practice', 'ablation', 'study', 'setup', 'section', 'summarize', 'key', 'ablation', 'study', 'perform', 'design', 'ret', 'sim', 'model', 'use', 'section', 'train', 'use', 'setup', 'detail', 'a12', 'ex', 'cept', 'train', 'step', 'reduce', 'computational', 'cost', 'evaluate', 'retsimneardup', 'performance', 'model', 'subset', 'benchmark', 'randomly', 'select', 'example', 'language', 'split', 'use', 'recall', 'report', 'metric', 'block', 'type', 'recall', 'chunk', 'size', 'recall', 'embed', 'retvec', 'gau', 'table', 'retsim', 'ablation', 'study', 'result', 'architecture', 'block', 'type', 'leave', 'text', 'chunk', 'size', 'middle', 'embed', 'dimension', 'right', 'bold', 'denote', 'value', 'select', 'final', 'retsim', 'model', 'result', 'table', 'contain', 'retsim', 'ablation', 'study', 'result', 'text', 'chunk', 'size', 'architecture', 'block', 'type', 'embed', 'size', 'important', 'architectural', 'decision', 'decide', 'optimal', 'text', 'chunk', 'size', 'find', 'right', 'balance', 'small', 'size', 'possible', 'maximize', 'retsimpartialdup', 'efficiency', 'ensure', 'retsimneardup', 'fulltext', 'embedding', 'work', 'effec', 'tively', 'full', 'document', 'find', 'chunk', 'character', 'offer', 'good', 'performance', 'also', 'test', 'various', 'model', 'architecture', 'transformer', 'block', 'find', 'good', 'balance', 'efficiency', 'performance', 'find', 'modern', 'gau', 'block', 'outper', 'form', 'vanilla', 'block', 'block', 'xue', 'also', 'try', 'modern', 'architecture', 'architec', 'ture', 'propose', 'retvec', 'bad', 'gau', 'block', 'performance', 'last', 'least', 'find', 'increase', 'embed', 'size', 'dimension', 'yield', 'meaningful', 'improvement', 'retsimneardup', 'accordingly', 'opt', 'use', 'embed', 'spaceefficiency', 'maximize', 'indexing', 'query', 'speed', 'additional', 'ablation', 'study', 'hyperparameter', 'find', 'appendix', 'future', 'work', 'retsim', 'novel', 'training', 'regime', 'combine', 'metric', 'learning', 'datum', 'augmentation', 'many', 'potential', 'application', 'plan', 'explore', 'future', 'work', 'example', 'adapt', 'extend', 'train', 'robust', 'semantic', 'embedding', 'image', 'similarity', 'embedding', 'additionally', 'expect', 'general', 'model', 'become', 'big', 'expensive', 'run', 'future', 'small', 'specialize', 'model', 'retsim', 'emerge', 'efficient', 'alternative', 'wide', 'range', 'task', 'conclusion', 'paper', 'introduce', 'retsim', 'novel', 'multilingual', 'text', 'embed', 'achieve', 'state', 'oftheart', 'performance', 'nearduplicate', 'text', 'detection', 'dataset', 'deduplication', 'syntactic', 'text', 'similarity', 'benchmark', 'retsim', 'significantly', 'fast', 'previous', 'neuralbased', 'text', 'embedding', 'robust', 'ngram', 'base', 'algorithm', 'make', 'suitable', 'largescale', 'text', 'retrieval', 'dataset', 'deduplication', 'especially', 'adversarial', 'setting', 'spam', 'detection', 'furthermore', 'introduce', 'benchmark', 'first', 'multilingual', 'dataset', 'design', 'measure', 'ad', 'versarial', 'robustness', 'nearduplicate', 'text', 'detection', 'algorithm', 'opensource', 'retsim', 'benchmark', 'mit', 'license', 'preprint', 'reference', 'naeem', 'amin', 'hamza', 'koundal', 'bader', 'alouffi', 'shah', 'machine', 'learn', 'technique', 'spam', 'detection', 'email', 'iot', 'platform', 'analysis', 'research', 'challenge', 'security', 'communication', 'network', 'doi', 'ahme', 'elgohary', 'generate', 'natural', 'language', 'adversarial', 'example', 'lepikhin', 'alexandre', 'taropa', 'paige', 'bailey', 'yanping', 'meierhellstern', 'yuje', 'bury', 'pher', 'cl´ement', 'crepy', 'shachi', 'sunipa', 'dev', 'vlad', 'feinberg', 'vlad', 'fienber', 'freitag', 'xavi', 'sebastian', 'guy', 'hand', 'abe', 'maxim', 'sneha', 'music', 'hyeontaek', 'vedant', 'maysam', 'moussalem', 'nado', 'pellat', 'reiner', 'emily', 'reif', 'parker', 'brennan', 'slone', 'sohn', 'valter', 'linte', 'denny', 'slav', 'petrov', 'palm', 'technical', 'report', 'url', 'http', 'arxivorgabs230510403', 'arxiv230510403', 'cs', 'andrei', 'broder', 'minwise', 'inde', 'proceeding', 'thirtieth', 'annual', 'acm', 'symposium', 'theory', 'pendent', 'permutation', 'compute', 'pp', 'retvec', 'resilient', 'efficient', 'text', 'vectorizer', 'mario', 'guajardocespede', 'tar', 'universal', 'sentence', 'encoder', 'arxiv', 'preprint', 'similarity', 'estimation', 'technique', 'round', 'algorithm', 'proceeding', 'thiryfourth', 'annual', 'acm', 'symposium', 'theory', 'compute', 'pp', 'pretraining', 'deep', 'bidirectional', 'transformer', 'language', 'understanding', 'arxiv', 'fangxiaoyu', 'language', 'agnostic', 'sentence', 'embed', 'gao', 'qi', 'generation', 'adversarial', 'text', 'sequence', 'evade', 'deep', 'learning', 'classifier', 'arxivorgab', 'cs', 'preprint', 'mandy', 'guo', 'denny', 'alrfou', 'wiki40b', 'multilingual', 'language', 'model', 'dataset', 'proceeding', '12th', 'language', 'resource', 'evaluation', 'conference', 'pp', '2440–2452', 'deduplication', 'scholarly', 'document', 'ing', 'locality', 'sensitive', 'hashing', 'word', 'embedding', 'proceeding', 'twelfth', 'language', 'resource', 'evaluation', 'conference', 'pp', 'guage', 'isbn', 'aclanthology', 'hagen', 'rathgeber', 'large', 'proceeding', '40th', 'international', 'sigir', 'scale', 'query', 'spelling', 'conference', 'research', 'development', 'information', 'retrieval', 'pp', 'transformer', 'quality', 'linear', 'time', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'issac', 'r', 'chiong', 'analysis', 'phishe', 'attack', 'countermeasure', 'mohit', 'iyyer', 'varun', 'daum´e', 'deep', 'unordered', 'composition', 'rival', 'syntactic', 'method', 'text', 'classification', 'proceeding', '53rd', 'annual', 'meeting', 'association', 'computational', 'linguistic', '7th', 'international', 'joint', 'conference', 'natural', 'language', 'processing', 'volume', 'long', 'paper', 'pp', 'association', 'computational', 'linguistic', 'raffel', 'deduplicate', 'training', 'datum', 'mitigate', 'pri', 'vacy', 'risk', 'language', 'model', 'proceeding', '39th', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'https', 'proceeding', 'mlrpressv162kandpal22ahtml', 'dzmitry', 'bahdanau', 'harm', 'vrie', 'stack', 'tb', 'permissively', 'license', 'source', 'code', 'deduplicate', 'training', 'datum', 'make', 'language', 'model', 'well', 'arxiv210706499', 'feichtenhofer', 'trevor', 'darrell', 'saine', 'convnet', 'proceeding', 'ieeecvf', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'jin', 'qi', 'textattack', 'framework', 'adversarial', 'attack', 'datum', 'augmentation', 'adversarial', 'training', 'tober', 'url', 'tazi', 'lo¨ıc', 'magne', 'nil', 'reimer', 'massive', 'text', 'bed', 'benchmark', 'url', 'https', 'filip', 'radenovi´c', 'giorgo', 'tolia', 'chum', 'finetune', 'image', 'retrieval', 'human', 'annotation', 'ieee', 'transaction', 'pattern', 'analysis', 'machine', 'intelligence', 'publisher', 'ieee', 'nil', 'reimer', 'iryna', 'gurevych', 'sentencebert', 'sentence', 'embedding', 'use', 'siamese', 'network', 'arxiv', 'preprint', 'facenet', 'unified', 'embedding', 'face', 'recognition', 'cluster', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'cvpr', 'pp', 'url', 'arxivorgabs150303832', 'arxiv150303832', 'cs', 'preprint', 'emily', 'noiserobust', 'deduplication', 'scale', 'duplicate', 'text', 'detection', 'use', 'frequencybiase', 'signature', 'moni', 'naor', 'rangan', 'demetri', 'terzopoulo', 'moshe', 'weikum', 'ed', 'web', 'information', 'system', 'engineering', 'wise', 'volume', 'pp', 'springer', 'isbn', 'doi', 'url', 'series', 'title', 'lecture', 'note', 'computer', 'science', 'ash', 'vardanian', 'usearch', 'unum', 'url', 'binxe', 'daxin', 'rangan', 'wei', 'text', 'embedding', 'weaklysupervise', 'contrastive', 'pretraine', 'cember', 'r', 'multisimilarity', 'loss', 'general', 'pair', 'weighting', 'deep', 'metric', 'learning', 'proceeding', 'ieeecvf', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'pp', 'linte', 'constant', 'raffel', 'mt5', 'massively', 'multilingual', 'pretraine', 'texttotext', 'transformer', 'arxiv', 'preprint', 'arxiv201011934', 'amin', 'law', 'sing', 'brian', 'multilingual', 'universal', 'sentence', 'encoder', 'semantic', 'retrieval', 'yang', 'je', 'srinadh', 'hsieh', 'large', 'batch', 'optimization', 'deep', 'learning', 'training', 'bert', 'minute', 'arxiv', 'preprint', 'preprint', 'a1', 'retsim', 'detail', 'a11', 'retsim', 'model', 'hyperparameter', 'full', 'list', 'retsim', 'model', 'hyperparameter', 'find', 'table', 'hyperparameter', 'input', 'length', 'chunk', 'block', 'type', 'block', 'hide', 'dim', 'expansion', 'rate', 'activation', 'function', 'attention', 'activation', 'function', 'absolute', 'positional', 'encoding', 'relative', 'positional', 'encoding', 'norm', 'type', 'pooling', 'type', 'dropout', 'rate', 'embed', 'dim', 'parameter', 'value', 'gau', 'swish', 'relu2', 'rope', 'p', 'table', 'detailed', 'retsim', 'model', 'hyperparameter', 'a12', 'retsim', 'training', 'hyperparameter', 'table', 'detail', 'hyperparameter', 'setting', 'train', 'configuration', 'loss', 'optimizer', 'use', 'train', 'retsim', 'model', 'hyperparameter', 'value', 'batch', 'size', 'train', 'step', 'learning', 'rate', 'end', 'learn', 'rate', 'learn', 'rate', 'decay', 'weight', 'decay', 'cosine', 'table', 'retsim', 'detailed', 'training', 'hyperparameter', 'a2', 'training', 'dataset', 'detail', 'provide', 'full', 'list', 'augmentation', 'use', 'generate', 'augmented', 'text', 'retsim', 'training', 'dataset', 'describe', 'section', 'sentencelevel', 'augmentation', 'deletion', 'random', 'sentence', 'deletion', 'random', 'sentence', 'truncation', 'insertion', 'preprint', 'random', 'prefix', 'sentence', 'random', 'suffix', 'sentence', 'random', 'sentence', 'insertion', 'repeat', 'sentence', 'substitution', 'lowercaseuppercase', 'sentence', 'random', 'sentence', 'substitution', 'transposition', 'neighboring', 'swap', 'wordlevel', 'augmentation', 'deletion', 'random', 'word', 'deletion', 'insertion', 'random', 'word', 'insertion', 'random', 'word', 'insertion', 'language', 'substitution', 'frequency', 'base', 'word', 'substitution', 'random', 'word', 'substitution', 'random', 'word', 'substitution', 'language', 'repeat', 'word', 'transposition', 'neighboring', 'swap', 'characterlevel', 'augmentation', 'deletion', 'random', 'character', 'deletion', 'substitution', 'case', 'substitution', 'base', 'substitution', 'n', 'qwerty', 'homoglyphs', 'substitution', 'random', 'ascii', 'substitution', 'random', 'character', 'language', 'alphabet', 'random', 'punctuation', 'substitution', 'random', 'unicode', 'character', 'substitution', 'insertion', 'character', 'repetition', 'ngram', 'base', 'insertion', 'n', 'random', 'character', 'language', 'alphabet', 'insertion', 'random', 'punctuation', 'insertion', 'random', 'unicode', 'character', 'insertion', 'transposition', 'neighboring', 'swap', 'a3', 'detailed', 'evaluation', 'hyperparameter', 'figure', 'contain', 'information', 'deduplication', 'threshold', 'value', 'hyperparameter', 'setting', 'benchmarke', 'newscopy', 'core', 'deduplication', 'dataset', 'preprint', 'model', 'type', 'threshold', 'value', 'hyperparameter', 'multilingual', 'use', 'cosine', 'similarity', 'multilingual', 'e5base', 'cosine', 'similarity', 'simhash', 'minhash', 'retsimneardup', 'retsimpartialdup', 'hamming', 'distance', 'similarity', 'cosine', 'similarity', 'cosine', 'similarity', 'bit', 'characterlevel', 'hash', 'function', 'wordlevel', 'figure', 'hyperparameter', 'setting', 'newscopy', 'dataset', 'evaluation', 'section', 'model', 'type', 'threshold', 'value', 'hyperparameter', 'cosine', 'similarity', 'labse', 'multilingual', 'use', 'cosine', 'similarity', 'multilingual', 'e5base', 'cosine', 'similarity', 'simhash', 'lsh', 'minhash', 'lsh', 'retsimneardup', 'retsimpartialdup', 'hamming', 'distance', 'similarity', 'cosine', 'similarity', 'cosine', 'similarity', 'bit', 'characterlevel', 'hash', 'function', 'wordlevel', 'figure', 'hyperparameter', 'setting', 'core', 'nearduplicate', 'dataset', 'evaluation', 'section', 'a31', 'deduplication', 'threshold', 'impact', 'figure', 'precisionrecallf1', 'score', 'different', 'cosine', 'distance', 'deduplication', 'threshold', 'retsimneardup', 'leave', 'retsimpartialdup', 'right', 'newscopy', 'dataset', 'detail', 'w4nt3d', 'benchmark', 'result', 'table', 'show', 'detailed', 'performance', 'result', 'retsim', 'baseline', 'algorithm', 'language', 'split', 'benchmark', 'preprint', 'k', 'j', 'b', 'e', 'l', 'l', 'l', 'k', 'l', 'p', 'e', 'l', 'l', 'r', 'p', 'e', 'r', 'r', 'e', 'e', 'r', 'h', 'h', 'h', 'h', 'u', 'h', 'r', 'h', 'h', 'e', 'r', 'f', 'fi', 'e', 'c', 'c', 'r', 'h', 'r', 'g', 'l', 'l', 'e', 'r', 'p', 'k', 'r', 'w', 'e', 'r', 'l', 'l', 'e', 'e', 'r', 'v', 'r', 'n', 'r', 'r', 'e', 'p', 'l', 'l', 'c', 'e', 'r', 'e', 'l', 'e', 'l', 'h', 'c', 'h', 'z', 'l', 'h', 'r', 'l', 'r', 'r', 'p', 'l', 'r', 'g', 'l', 'l', 'e', 'b', 'e', 'l', 'l', 'l', 'k', 'l', 'p', 'e', 'l', 'l', 'r', 'p', 'e', 'r', 'h', 'h', 'h', 'h', 'r', 'e', 'e', 'r', 'r', 'p', 'k', 'r', 'w', 'e', 'r', 'l', 'l', 'e', 'e', 'r', 'v', 'r', 'n', 'r', 'r', 'e', 'p', 'l', 'l', 'c', 'e', 'r', 'e', 'l', 'e', 'l', 'preprint', 'a5', 'additional', 'ablation', 'study', 'section', 'include', 'ablation', 'study', 'additional', 'hyperparameter', 'retsim', 'model', 'includ', 'loss', 'function', 'pooling', 'type', 'model', 'capacity', 'λ', 'recall', 'table', 'ablation', 'study', 'multisimilarity', 'loss', 'hyperparameter', 'retsim', 'training', 'indicate', 'hyperparameter', 'set', 'select', 'final', 'model', 'block', 'hide', 'table', 'ablation', 'study', 'retsim', 'model', 'capacity', 'size', 'number', 'gau', 'block', 'hide', 'dimension', 'block', 'indicate', 'hyperparameter', 'set', 'select', 'final', 'model', 'pool', 'type', 'recall', 'average', 'pooling', 'max', 'pooling', 'generalized', 'mean', 'pool', 'table', 'ablation', 'study', 'pool', 'type', 'retsim', 'model', 'indicate', 'hyperparameter', 'set', 'select', 'final', 'model', 'select', 'example', 'newscopy', 'dataset', 'section', 'randomly', 'select', 'set', 'false', 'positive', 'false', 'negative', 'retsim', 'newscopy', 'deduplication', 'dataset', 'provide', 'insight', 'result', 'preprint', 'text', 'chauffeur', 'policeman', 'pass', 'jour', 'nalist', 'try', 'intervene', 'policeman', 'report', 'serious', 'condition', 'princess', 'husband', 'month', 'capt', 'mark', 'phillip', 'hurt', 'police', 'expert', 'say', 'hole', 'leave', 'bullet', 'fire', 'car', 'indicate', 'pass', 'miss', 'inch', 'police', 'informant', 'say', 'believe', 'shot', 'fire', 'assailant', 'expert', 'study', 'revolver', 'find', 'scene', 'say', 'fi', 'tnfernational', 'ay', 'ssast', 'fre', 'sg', 'federal', 'government', 'propose', 'new', 'method', 'eoustructe', 'federal', 'building', 'move', 'save', 'ad', 'ditional', 'ergy', 'suggest', 'il', 'elfort', 'adapt', 'new', 'building', 'p', 'mediate', 'removal', 'bert', 'prohibition', 'administrator', 'de', 'mande', 'today', 'ground', 'charge', 'place', 'department', 'justice', 'investigator', 'wheeler', 'accompany', 'demand', 'continue', 'page', 'dnite', 'jai', 'delegation', 'navat', 'confer', 'ence', 'today', 'win', 'ls', 'demand', 'pre', 'sentation', 'cnse', 'suxiliary', 'warship', 'limitation', 'first', 'plenary', 'session', 'thuvaday', 'chlet', 'delegate', 'mec', 'tittg', 'also', 'decide', 'plenary', 'sesslon', 'discuss', 'main', 'con', 'ference', 'question', 'alpha', 'betical', 'order', 'ihe', 'countriea', 'pro', 'posing', 'press', 'admit', 'american', 'delegation', 'woo', 'second', 'vic', 'tory', 'text', 'ian', 'ball', '26year', 'old', 'unemployed', 'englishman', 'bring', 'court', 'today', 'charge', 'attempt', 'mur', 'der', 'tempt', 'kidnap', 'anne', 'car', 'heart', 'nesday', 'night', 'ball', 'leanface', 'bearded', 'stand', 'stiffly', 'dock', 'court', 'handcuff', 'detective', 'speak', 'second', 'appearance', 'say', 'want', 'apply', 'legal', 'aid', 'court', 'dere', 'hold', 'hearing', 'federal', 'government', 'propose', 'new', 'method', 'construct', 'federal', 'building', 'move', 'save', 'addilional', 'energy', 'suggest', 'il', 'effort', 'adapt', 'new', 'building', 'sampson', 'administration', 'ad', 'ministrater', 'say', 'new', 'feature', 'construction', 'include', 'collection', 'rain', 'waler', 'cool', 'irriga', 'tion', 'solar', 'energy', 'collector', 'covering', 'exterior', 'wall', 'earth', 'whal', 'say', 'design', 'criteri', 'immiedl', 'aie', 'mmoval', 'pro', 'hi', 'bition', 'administrator', 'demand', 'seuate', 'day', 'waeel', 'ground', 'charge', 'place', 'depart', 'meat', 'justice', 'investigator', 'wheeler', 'accompany', 'demand', 'declaration', 'prohibition', 'foreemen', 'blame', 'politician', 'call', 'law', 'enforcement', 'commussion', 'sum', 'member', 'wp', 'amer', 'naval', 'cen', 'ference', 'win', 'demand', 'presentation', 'case', 'linsitation', 'flrst', 'next', 'trary', 'session', 'chief', 'delegate', 'meet', 'si', 'also', 'decide', 'plenary', 'session', 'discuss', 'main', 'confeyence', 'question', 'alphabetical', 'order', 'cauntrie', 'propose', 'win', 'second', 'victory', 'decide', 'udmil', 'certain', 'representative', 'press', 'plenary', 'table', 'example', 'false', 'negative', 'retsim', 'newscopy', 'dataset', 'pair', 'text', 'detect', 'nearduplicate', 'retsim', 'label', 'nearduplicate', 'original', 'dataset', 'ex', 'ample', 'randomly', 'select', 'truncate', 'character', 'display', 'preprint', 'text', 'bozeman', 'chet', 'huntley', 'resonant', 'voice', 'roughhewn', 'face', 'come', 'familiar', 'million', 'nightly', 'television', 'news', 'die', 'moun', 'tain', 'resort', 'home', 'undergo', 'surgery', 'lung', 'cancer', 'remain', 'recent', 'week', 'die', 'accord', 'widow', 'tippy', 'team', 'year', 'port', 'quit', 'turn', 'native', 'develop', '20millio', 'associate', 'press', 'amer', 'ican', 'pay', 'per', 'cent', 'month', 'electricity', 'year', 'last', 'ate', 'press', 'survey', 'show', 'sumer', 'begin', 'organize', 'fight', 'rate', 'hike', 'spot', 'check', 'monthly', 'elec', 'tric', 'bill', 'year', 'last', 'show', 'crease', 'erally', 'cent', 'high', 'report', 'boost', 'com', 'fia', 'average', 'tab', 'go', 'last', 'year', 'year', 'utility', 'bozeman', 'vice', 'say', 'world', 'miss', 'unique', 'ability', 'former', 'television', 'huntley', 'die', 'home', 'long', 'bout', 'lung', 'cancer', 'family', 'spokesman', 'say', 'memorial', 'service', 'conduct', 'huntley', 'big', 'sky', 'resort', 'recreation', 'area', 'south', 'huntley', 'chairman', 'big', 'sky', 'board', 'director', 'memorial', 'service', 'schedule', 'studio', 'house', 'pass', 'legislation', 'raise', 'minimum', 'wage', 'hour', 'year', 'worker', 'cover', 'bill', 'approve', 'also', 'increase', 'number', 'worker', 'cover', 'mini', 'mum', 'wage', 'law', 'bill', 'modify', 'ver', 'sion', 'veto', 'last', 'year', 'however', 'expect', 'sign', 'one', 'finally', 'approve', 'ad', 'justment', 'similar', 'pass', 'measure', 'text', 'bozeman', 'chet', 'huntley', 'resonant', 'voice', 'roughhewn', 'face', 'become', 'familiar', 'million', 'nightly', 'television', 'news', 'die', 'moun', 'tain', 'resort', 'home', 'undergo', 'surgery', 'lung', 'cancer', 'remain', 'active', 'recent', 'week', 'die', 'accord', 'widow', 'tippy', 'huntley', 'huntley', 'team', 'year', 'quit', 'return', 'native', 'develop', 'mil', 'louise', 'acenciaiod', 'prece', 'writer', 'pay', 'io', 'cent', 'month', 'far', 'electricity', 'year', 'ihan', 'last', 'press', 'survey', 'show', 'onsumer', 'begin', 'fight', 'rate', 'hike', 'spot', 'check', 'monthly', 'elec', 'tre', 'hill', 'year', 'show', 'increase', 'erally', 'cent', 'high', 'report', 'boost', 'com', 'average', 'tab', 'go', 'last', 'year', 'year', 'bozeman', 'vice', 'say', 'world', 'miss', 'unique', 'ability', 'former', 'television', 'huntley', 'die', 'home', 'long', 'bout', 'lung', 'cancer', 'family', 'spokesman', 'say', 'morial', 'service', 'ducte', 'hunt', 'big', 'sky', 'resort', 'recreation', 'area', 'south', 'hunt', 'chair', 'man', 'big', 'sky', 'board', 'director', 'memorial', 'service', 'sche', 'uled', 'studio', 'house', 'pass', 'legislation', 'raise', 'minimum', 'wage', 'hour', 'year', 'worker', 'cover', 'bill', 'approve', 'wedne', 'day', 'also', 'crease', 'mil', 'lion', 'number', 'worker', 'cov', 'ere', 'minimum', 'wage', 'law', 'bill', 'modify', 'version', 'veto', 'last', 'year', 'however', 'ex', 'te', 'sign', 'one', 'inally', 'approve', 'adjust', 'ment', 'similar', 'pass', 'table', 'example', 'false', 'positive', 'retsim', 'newscopy', 'dataset', 'pair', 'text', 'detect', 'nearduplicate', 'retsim', 'label', 'nearduplicate', 'original', 'dataset', 'example', 'randomly', 'select', 'truncate', 'character', 'display']",
"$Q_{bias}$ -- A Dataset on Media Bias in Search Queries and Query
  Suggestions","[{'title': 'doi', 'href': 'http://dx.doi.org/10.1145/3578503.3583628', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2311.17780v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2311.17780v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-11-29 16:26:00,"3
2
0
2

v
o
N
8
2

]
S
M

.
s
c
[

1
v
3
8
2
7
1
.
1
1
3
2
:
v
i
X
r
a

Lineax: unified linear solves and linear least-squares
in JAX and Equinox

Jason Rader
Oxford University
rader@maths.ox.ac.uk

Terry Lyons
Oxford University

Patrick Kidger
Google X
math@kidger.site

Abstract

We introduce Lineax, a library bringing linear solves and linear least-squares to
the JAX+Equinox scientific computing ecosystem. Lineax uses general linear
operators, and unifies linear solves and least-squares into a single, autodifferen-
tiable API. Solvers and operators are user-extensible, without requiring the user to
implement any custom derivative rules to get differentiability. Lineax is available
at https://github.com/google/lineax.

1

Introduction

JAX is an autodifferentiatiable Python framework popular for machine learning and scientific comput-
ing [4, 9, 12, 16]. Equinox [20] is a popular JAX library [8, 15], targeting the same use cases, that adds
additional support for parameterised functions. Solving linear systems, whether well-posed linear
solves or ill-posed linear least-squares problems, is a central sub-problem in scientific computing
[14, 27]. For example, linear solves and least-squares appear as subroutines in nonlinear optimisation
[21], finite-difference schemes [26], and signal processing [22]. As such, we introduce Lineax, a
library built in JAX and Equinox for linear solves and linear least-squares.

Lineax presents a single, differentiable interface for solving well-posed, underdetermined, and
overdetermined linear systems. It also allows users to write custom differentiable linear solvers or
least-squares solvers, and introduces a linear operator abstraction.

Overall, we intend for Lineax to integrate well with the existing JAX scientific ecosystem. This
ecosystem is growing, and includes packages for differentiable rigid-body physics simulation [11],
computational fluid dynamics [3, 6], protein structure prediction [10], ordinary and stochastic
differential equations [19], and probabilistic modeling [25]. We are beginning to see some use
of Lineax in this ecosystem already. This includes for linear subroutines in ocean dynamics [18]
and optimal transport [5]. Further, Diffrax [19] plans to adopt Lineax in the near future for linear
subroutines in differential equations solves.

1.1 Main contributions

The main contributions of Lineax are:

• A general linear operator abstraction, as implemented by dense matrices, linear functions,

Jacobians, etc.

• Stable and fast gradients through least-squares solves. This includes through user-defined

solvers, without requiring extra effort from the user.

• PyTree-valued 1 operators and vectors.

1JAX terminology for arbitrarily nested container ’node’ types (tuples/dictionaries/lists/custom types) con-
taining ’leaves’ (every other Python/JAX type.) We exclusively consider PyTrees who’s leaves are JAX arrays.

NeurIPS 2023 AI for Science Workshop.

 
 
 
 
 
 
Comparisons to existing JAX APIs

The operator abstraction introduced in Lineax offers a flexibility not found in core JAX, which
supports only dense matrices or matrix-vector product representations of operators. Lineax introduces
new solvers over core JAX, such as lineax.Tridiagonal. Lineax also offers a consistent API
between operators and solvers, which is what allows for extensibility to user-specified custom operator
and solvers.

Compilation times for most Lineax solvers are essentially identical to JAX native solvers; Lineax’s
iterative solvers (CG, GMRES, ...) compile roughly twice as fast. The ‘benchmarks’ folder on GitHub
provides a quantitative comparison.

We emphasise the stable and fast gradients to contrast with the existing JAX implementation, which
as of version 0.4.16 exhibits instability or incorrect gradients in some exceptional cases.

For these reasons, JAX is actually considering deprecating some of its own APIs in favour of Lineax
[29].

1.2 Classical linear solve example

Consider solving Ax = b for a random matrix A ∈ R10×10 against a random vector b ∈ R10. This
can be done via

1

2

3

4

5

6

7

import jax . random as jr
import lineax as lx

A_key , b_key = jr . split ( jr . PRNGKey (0))
A = lx . M atr ix Li near Op er ator ( jr . normal ( A_key , (10 , 10)))
b = jr . normal ( b_key , (10 ,))
solution = lx . linear_solve (A , b )

2 Performing linear solves and least-squares

The main entry point to linear solves and least-squares in Lineax is

lineax . linear_solve (A , b , solver )

for a linear operator A and PyTree b. This performs a linear solve Ax = b (for well-posed systems),
or returns a least-squares solution minx ∥Ax − b∥2 (for overdetermined systems), or returns a
minimum norm solution minx ∥x∥2 subject to Ax = b (for underdetermined systems). This is a lot of
operations to unify together, and it may initially seem strange to do so. The common thread – and our
justification for unifying these operations – is that mathematically, all the above operations correspond
to the pseudoinverse solution to Ax = b, ie. the solution arising from using the Moore-Penrose
pseudoinverse x = A†b [1, 23].

The user can specify which solver they’d like to use via the solver argument. This is helpful when
the user already knows which solvers should work well for a problem. Not every solver is capable of
handling every problem. For example, lineax.CG handles positive definite operators [21, section 5].
Using a solver with an incompatible problem will result in an error.

3 General linear operators

In Lineax, we represent A more generally than as an n × m matrix. Instead, we represent A as a
linear operator A : X → Y , where X and Y are spaces of PyTrees of arrays. At an implementation
level, a linear operator is an object which subclasses

lineax . A bs t ra c tL i ne a r Op e rat or .

When A is a dense matrix, A ∈ Rdim(Y )×dim(X), it can be treated as a Lineax linear operator via

lineax . Ma trix Li ne ar Op er ator ( A ).

2

Lineax operators themselves form a vector space, and are closed under addition, scalar multiplication,
and composition. Each linear operator A must implement a method to:

• Compute the matrix-vector product: Ax for x ∈ X.
• Compute the transpose of the operator: AT : Y → X.
• Materialise the operator as a matrix: A.as_matrix() ∈ Rdim(X)×dim(Y ).
• Retrieve the input/output PyTree structure, as well as the input/output dimensions. ie. the

functions domain(A) = X and codomain(A) = Y .

This increased generality comes with increased flexibility. For example: large, sparse matrices can
use data-efficient formats and utilise linear solves which use only the matrix-vector product, such
as GMRES [24] or BiCGStab [28]. For example, a linear function f : X → Y can be made into a
linear operator with

lineax . F un c ti o nL i ne a r Op e rat or (f , in_structure )

where in_structure describes the PyTree structure of the input of f (equivalently, the PyTree
structure of the elements x ∈ X.) Similarly, a nonlinear function g : X → Y can be linearised at a
point x ∈ X and use its Jacobian at x as a linear operator via

lineax . J ac o bi a nL i ne a r Op e rat or (g , x )

The lineax.AbstractLinearOperator base class is available for users to subclass and create
their own linear operator types.

3.1 Operator tags

Tags are an optional argument to most linear operators, and indicate properties of the operator A.
For example, if A ∈ Rn×n is positive semidefinite, then A can be marked as a positive semidefinite
linear operator with

lineax . Ma trix Li ne ar Op er ator (A , lineax . p o s i t i ve _ s e m i d e f i ni t e _ t a g )

This indicates to any solver which uses A that it is positive semidefinite. For example, if A is also
nonsingular, then it can be used safely with lineax.CG. Tags are also used to select the appropriate
solver in the polyalgorithm lineax.AutoLinearSolver detailed in section 6.

4 Computing gradients

In JAX, derivatives are built from Jacobian-vector products (JVPs) and vector-Jacobian products
(VJPs) for forward-mode and reverse-mode automatic differentiation respectively [12]. The JVP of a
function f : Ra → Rb maps an input-tangent pair (x, v) ∈ Ra × Ra to (f (x), ∂f (x)(v)) ∈ Rb × Rb
where ∂f (x) : Ra → Rb is the Jacobian of f at x. The VJP maps an input-cotangent pair (x, c) ∈
Ra × Rb to (f (x), ∂f (x)T c) ∈ Rb × Ra, where ∂f (x)T : Rb → Ra is the transpose of ∂f (x).

The major contribution of Lineax over existing linear solve and least-squares software is the efficient
computation of JVPs for pseudoinverse solutions. That is, differentiation through both well-posed
linear solves and ill-posed least-squares solves are performed in the same manner as each other.
In particular, we may special case when operators have full row or column rank in order to obtain
improved performance, as we now show.

4.1

JVPs and forward-mode autodifferentiation

In this section, let L(A, b) denote the linear solve lineax.linear_solve. For a primal problem
Ax = b, then L(A, b) = A†b where A† be the Moore–Penrose pseudoinverse of A, as mentioned in
section 2. Here we discuss how the to compute the JVP ∂L(A, b)(V, v), where (V, v) is the tangent
pair consisting of a tangent operator V and tangent vector v.

It is possible to compute the JVP through either argument. For example, the tangent computation of a
linear solve as a function of v alone is

∂L(A, b)(0, v) = A†v

3

(2)

(3)

(4)

where 0 represents the 0 tangent operator.

Meanwhile, computing the JVP for ∂L(A, b)(V, 0) requires differentiating through a pseudoinverse,
which has the explicit formula [13]

∂L(A, b)(V, 0) = (−A†V A† + A†(A†)T V T (I − AA†) + (I − A†A)V T (A†)T A†)A†b.

Letting

x = A†b
z = V T (A†)T x,

and adding the above two equations together and using the linearity of the Jacobian, we have the total
JVP with respect the primal pair (A, b) and tangent pair (V, v) for lineax.linear_solve is

∂L(A, b)(V, v) = A† (cid:0)−V x + (A†)T V T (b − Ax) − Az + v(cid:1) + z.

(1)

If A has linearly independent columns, then A†A = I [14, section 5.5.2] and the term z − A†Az = 0,
giving

∂L(A, b)(V, v) = A† (cid:0)−V x + (A†)T V T (b − Ax) + v(cid:1) .

When A has linearly independent rows, then (b − Ax) = 0 and

∂L(A, b)(V, v) = A† (−V x − Az + v) + z.

Together, if A has linearly independent rows and columns, then A is well-posed, A† = A−1 is a true
inverse, and

∂L(A, b)(V, v) = A−1 (−V x + v) .

We then select between equations (4), (3), (2), or (1) depending on whether we know at compile time
that A has linearly independent rows and columns, has only independent rows, has only independent
columns, or has both dependent rows and columns. Despite being a property of the operator, at
compile time the main way the JVP rule is dispatched via the choice of solver. This is because not
every solver supports dependent rows/columns (section 5), and will return nan values if used in a
solve with an unsupported operator. So, if a solver does not support dependent rows/columns, we can
be sure we will not get a solution given an operator with dependent rows/columns in the JVP.

For example, lineax.QR [27, section 2] can handle dependent rows if the number of rows is greater
than the number of columns (or dependent columns if the number of columns is greater than the
number of rows) and will dispatch to either equation (2) or (3). If the number of columns and rows of
A are the same, then lineax.QR will dispatch to equation (4).

4.2 VJPs and backpropogation via transposition

Reverse-mode autodifferentiation of a function f : Ra → Rb is not built on JVPs, but rather on
vector-Jacobian products (VJPs) vT ∂f (x) = ∂f (x)T v. As suggested by the definition, VJPs are
constructed via a JVP and transposition [12]. This is how VJPs are implemented in JAX, and thus in
Lineax as well. The Jacobian ∂L(A, b) : Rm×n × Rm → Rn is a linear function, see also the explicit
form in equation (1). Therefore, it has a transpose ∂L(A, b)T : Rn → Rm×n × Rm.

The transpose rule for the linear solve is implemented as a custom JAX primitive. See [12] for more
details.

5 User-defined solvers

A user can implement a custom solver by subclassing

lineax . Ab stra ct Li ne ar So lver

which requires the methods: init, compute, transpose, allow_dependent_rows, and
allow_dependent_columns.

4

Many direct linear solvers for Ax = b use two stages of computation. First, factor A into a form
amenable to computation (eg. LU factorisation [14, section 3], QR factorisation, SVD factorisation,
etc.) Then, use this factorisation to solve for a given right hand side b. The factorisation of A does not
depend on the right hand side b, and can be reused with various choices of b. This saves computation
cost when solving Ax = b for many right hands b.

For a solver using such a two-stage factorisation approach, init computes the factorisation, and
compute performs the solve for the specific right hand b. transpose computes the transpose of the
factorisation provided by init, which allows us to skip computing the factorisation of the transpose
operator directly (init(transpose(operator))), as it is commonly the case that we can cheaply
derive this from init(operator) alone. This is needed when computing VJPs as discussed in
the previous section. The methods allow_dependent_rows and allow_dependent_columns
determine which equation (1-4) is used in the differentiation, as discussed in section 4.

This greatly simplifies the process of writing a differentiable linear solver or least-squares solver. In
the core JAX library, it is somewhat cumbersome to write a differentiable solver. It requires using
jax.lax.custom_linear_solve and implementing a solver transposition rule transpose_solve
if the user would like to use reverse-mode autodifferentiation. In Lineax, differentiation comes for
free once a solver is implemented, whether the solver is a linear solve or a least-squares algorithm.

6 The AutoLinearSolver polyalgorithm

If the user does not provide a solver to lineax.linear_solve, then the default linear solve
is lineax.AutoLinearSolver. lineax.AutoLinearSolver is a polyalgorithm which selects a
solver automatically at compile time depending on the structure of A, as indicated through its operator
tag (discussed in section 3.1). lineax.AutoLinearSolver takes the argument well_posed, which
indicates whether the system is expected to solve a least-squares/minimum norm problem, or only
handle well-posed linear solves.

lineax.AutoLinearSolver(well_posed=True) selects a solver depending upon the oper-
ator structure, and throws an error when it encounters an underdeteremined or overdeter-
mined system.
lineax.AutoLinearSolver(well_posed=False) solves well-posed linear
solves as well as linear least squares, but often at an additional computation cost. Finally,
lineax.AutoLinearSolver(well_posed=None) solves a least-squares problem only if it is not
expensive to do so.

The specific polyalgorithms for well_posed=True, well_posed=False, and well_posed=None
are shown in figure 1.

6.1 Choosing a solver at compile time

We must choose between two paradigms for the implementation of lineax.AutoLinearSolver:
make the algorithm selection at run time, or make the algorithm selection at compile time. This is a
trade-off, as determining which solver to use at run time means checking the elements of the matrix.
This incurs a run time overhead of O(n2) for an n × n matrix, which is relatively small compared to
the O(n3) run time of most linear solve algorithms. However, run time checking also incurs a greater
cost in compile times. Since it is not known at compile time which branch of the polyalgorithm will
run, the compiler is forced to compile all branches. Thus, compilation cost scales with the logic
of the polyalgorithm: as more branches are included, compile times increase. This can limit the
extensibility of the polyalgorithm.

Compile time selection avoids these performance issues, and is faster both in run time and compile
time when used correctly. Further, it simplifies tying solves to GPU hardware, as there is no possibility
of taking separate branches for different batch elements. However, compile time selection requires
the user to a-priori know the structure of the operator, and can result in using a suboptimal solver if
the operator has exploitable structure which the user does not indicate.

We choose the compile time approach, and require the user to pass the structure of an operator explic-
itly via the operator tag (section 3.1.) We choose this approach primarily to minimise compilation
times, and to avoid the tradeoff between extensibility and compile time inherent in run time checking.

5

The AutoLinearSolver polyalgorithm.

Figure 1:
so that
well_posed=True starts at ""A is square?"", well_posed=None starts at ""A is diagonal?"",
and well_posed=False starts at ""A is diagonal?"" in the well_posed=False section.

Read from left-to-right,

6

Our approach is in contrast to MATLAB’s mldivide, a (nondifferentiable) unified linear solve and
least-squares solver which uses a run time approach [17]. Both the Julia and MATLAB languages
offer methods for nonsingular linear solves – the infix \ operation in Julia and linsolve in MATLAB
– which accept compile time tags, but still perform run time checks if the user passes no tags [2, 7, 17].
Therefore, both suffer from the additional overhead of the run time approach in many cases.

7 Conclusion

We have introduced Lineax, a differentiable JAX+Equinox library unifying linear solves and linear
least-squares. We have demonstrated that users can extend base Lineax operators and solvers and
use them within our unified API, without the need to write any custom derivative rules. We hope to
see adoption of Lineax solves within the JAX+Equinox scientific computing and machine learning
ecosystem.

8 Acknowledgements

This publication is based on work supported by the EPSRC Centre for Doctoral Training in Mathe-
matics of Random Systems: Analysis, Modelling and Simulation (EP/S023925/1)

References

[1] Adi Ben-Israel and Thomas N. E. Greville. Generalized Inverses: Theory and Applications.

Springer New York, 2003.

[2] Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral B Shah. Julia: A fresh approach to
numerical computing. SIAM Review, 59(1):65–98, 2017. URL: https://epubs.siam.org/
doi/10.1137/141000671, doi:10.1137/141000671.

[3] Deniz A. Bezgin, Aaron B. Buhendwa, and Nikolaus A. Adams.

JAX-fluids: A fully-
differentiable high-order computational fluid dynamics solver for compressible two-phase
flows. Computer Physics Communications, 282:108527, 2023.

[4] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James, Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and
Qiao Zhang. Jax: composable transformations of python+numpy programs, 2018. URL:
http://github.com/google/jax.

[5] Marco Cuturi, Laetitia Meng-Papaxanthos, Yingtao Tian, Charlotte Bunne, Geoff Davis, and
Olivier Teboul. Optimal transport tools (ott): A jax toolbox for all things wasserstein. arXiv
preprint arXiv:2201.12324, 2022.

[6] Gideon Dresdner, Dmitrii Kochkov, Peter Norgaard, Leonardo Zepeda-Núñez, Jamie A. Smith,
Michael P. Brenner, and Stephan Hoyer. Learning to correct spectral methods for simulating
turbulent flows. arXiv preprint arXiv:2207.00556, 2022.

[7] Chris Rackauckas et al. Linearsolve.jl: High-performance unified linear solvers, 2021. Accessed

2023. URL: https://github.com/SciML/LinearSolve.jl.

[8] David Hall et al. Haliax. Accessed 2023, 2023. URL: https://github.com/

stanford-crfm/haliax.

[9] Igor Babuschkin et al. The deepmind jax ecosystem, 2020. URL: http://github.com/

deepmind.

[10] John Jumper et al. Highly accurate protein structure prediction with alphafold. Nature, 596:583–

589, 8 2021.

[11] C. Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier
Bachem. Brax - a differentiable physics engine for large scale rigid body simulation, 2021.

7

[12] Roy Frostig, Matthew J. Johnson, Dougal Maclaurin, Adam Paszke, and Alexey Radul. Decom-

posing reverse-mode automatic differentiation, 2021. arXiv:2105.09469.

[13] Gene H. Golub and V. Pereyra. The differentiation of pseudo-inverses and nonlinear least squares
problems whose variables separate. SIAM Journal on Numerical Analysis, 10(2):413–432, 1973.

[14] Gene H. Golub and Charles F. Van Loan. Matrix Computations. The Johns Hopkins University

Press, third edition, 1996.

[15] David Hall, Ivan Zhou, and Percy Liang. Levanter — legible, scalable, reproducible foundation
models with jax. Accessed 2023, 2023. URL: https://github.com/stanford-crfm/
levanter.

[16] Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas
Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2023. URL:
http://github.com/google/flax.

[17] The MathWorks Inc. Matlab version: 9.13.0 (r2022b), 1984. URL: https://www.mathworks.

com.

[18] J. Emmanuel Johnson and Takaya Uchida. Jaxsw: Jax approximate ocean models, 2023.

Accessed 2023. URL: https://github.com/jejjohnson/jaxsw.

[19] Patrick Kidger. On Neural Differential Equations. PhD thesis, University of Oxford, 2021.

[20] Patrick Kidger and Cristian Garcia. Equinox: neural networks in JAX via callable PyTrees
and filtered transformations. Differentiable Programming workshop at Neural Information
Processing Systems 2021, 2021.

[21] Jorge Nocedal and Stephen Wright. Numerical Optimization (Second Edition). Springer New

York, 2006.

[22] Alan V. Oppenheim and Ronald W. Schafer. Digital Signal Processing. Prentice-Hall, 1975.

[23] R. Penrose. A generalized inverse for matrices. Mathematical Proceedings of the Cambridge

Philosophical Society, 51(3):406–413, 1955.

[24] Youcef Saad and Martin H. Schultz. Gmres: A generalized minimal residual algorithm for
solving nonsymmetric linear systems. SIAM Journal on Scientific and Statistical Computing,
7(3):856–869, 1986.

[25] Miloš Stanojevi´c and Laurent Sartran. SynJax: Structured Probability Distributions for JAX.

arXiv preprint arXiv:2308.03291, 2023.

[26] J. W. Thomas. Numerical Partial Differential Equations: Finite Difference Methods. Springer

New York, 1995.

[27] Lloyd N. Trefethen and David Bau. Numerical Linear Algebra. Society for Industrial and

Applied Mathematics, 1997.

[28] H. A. van der Vorst. Bi-cgstab: A fast and smoothly converging variant of bi-cg for the
solution of nonsymmetric linear systems. SIAM Journal on Scientific and Statistical Computing,
13(2):631–644, 1992.

[29] Jake Vanderplas. Jep 18137: Scope of jax numpy & scipy wrappers. Github pull request, created

and accessed 2023, 2023. URL: https://github.com/google/jax/pull/18137.

8

","3 2 0 2 v o N 8 2 ] S M . s c [ 1 v 3 8 2 7 1 . 1 1 3 2 : v i X r a Lineax : unified linear solves and linear least-squares in JAX and Equinox Jason Rader Oxford University rader @ maths.ox.ac.uk Terry Lyons Oxford University Patrick Kidger Google X math @ kidger.site Abstract We introduce Lineax , a library bringing linear solves and linear least-squares to the JAX+Equinox scientific computing ecosystem . Lineax uses general linear operators , and unifies linear solves and least-squares into a single , autodifferen- tiable API . Solvers and operators are user-extensible , without requiring the user to implement any custom derivative rules to get differentiability . Lineax is available at https : //github.com/google/lineax . 1 Introduction JAX is an autodifferentiatiable Python framework popular for machine learning and scientific comput- ing [ 4 , 9 , 12 , 16 ] . Equinox [ 20 ] is a popular JAX library [ 8 , 15 ] , targeting the same use cases , that adds additional support for parameterised functions . Solving linear systems , whether well-posed linear solves or ill-posed linear least-squares problems , is a central sub-problem in scientific computing [ 14 , 27 ] . For example , linear solves and least-squares appear as subroutines in nonlinear optimisation [ 21 ] , finite-difference schemes [ 26 ] , and signal processing [ 22 ] . As such , we introduce Lineax , a library built in JAX and Equinox for linear solves and linear least-squares . Lineax presents a single , differentiable interface for solving well-posed , underdetermined , and overdetermined linear systems . It also allows users to write custom differentiable linear solvers or least-squares solvers , and introduces a linear operator abstraction . Overall , we intend for Lineax to integrate well with the existing JAX scientific ecosystem . This ecosystem is growing , and includes packages for differentiable rigid-body physics simulation [ 11 ] , computational fluid dynamics [ 3 , 6 ] , protein structure prediction [ 10 ] , ordinary and stochastic differential equations [ 19 ] , and probabilistic modeling [ 25 ] . We are beginning to see some use of Lineax in this ecosystem already . This includes for linear subroutines in ocean dynamics [ 18 ] and optimal transport [ 5 ] . Further , Diffrax [ 19 ] plans to adopt Lineax in the near future for linear subroutines in differential equations solves . 1.1 Main contributions The main contributions of Lineax are : • A general linear operator abstraction , as implemented by dense matrices , linear functions , Jacobians , etc . • Stable and fast gradients through least-squares solves . This includes through user-defined solvers , without requiring extra effort from the user . • PyTree-valued 1 operators and vectors . 1JAX terminology for arbitrarily nested container ’ node ’ types ( tuples/dictionaries/lists/custom types ) con- taining ’ leaves ’ ( every other Python/JAX type . ) We exclusively consider PyTrees who ’ s leaves are JAX arrays . NeurIPS 2023 AI for Science Workshop . Comparisons to existing JAX APIs The operator abstraction introduced in Lineax offers a flexibility not found in core JAX , which supports only dense matrices or matrix-vector product representations of operators . Lineax introduces new solvers over core JAX , such as lineax.Tridiagonal . Lineax also offers a consistent API between operators and solvers , which is what allows for extensibility to user-specified custom operator and solvers . Compilation times for most Lineax solvers are essentially identical to JAX native solvers ; Lineax ’ s iterative solvers ( CG , GMRES , ... ) compile roughly twice as fast . The ‘ benchmarks ’ folder on GitHub provides a quantitative comparison . We emphasise the stable and fast gradients to contrast with the existing JAX implementation , which as of version 0.4.16 exhibits instability or incorrect gradients in some exceptional cases . For these reasons , JAX is actually considering deprecating some of its own APIs in favour of Lineax [ 29 ] . 1.2 Classical linear solve example Consider solving Ax = b for a random matrix A ∈ R10×10 against a random vector b ∈ R10 . This can be done via 1 2 3 4 5 6 7 import jax . random as jr import lineax as lx A_key , b_key = jr . split ( jr . PRNGKey ( 0 ) ) A = lx . M atr ix Li near Op er ator ( jr . normal ( A_key , ( 10 , 10 ) ) ) b = jr . normal ( b_key , ( 10 , ) ) solution = lx . linear_solve ( A , b ) 2 Performing linear solves and least-squares The main entry point to linear solves and least-squares in Lineax is lineax . linear_solve ( A , b , solver ) for a linear operator A and PyTree b . This performs a linear solve Ax = b ( for well-posed systems ) , or returns a least-squares solution minx ∥Ax − b∥2 ( for overdetermined systems ) , or returns a minimum norm solution minx ∥x∥2 subject to Ax = b ( for underdetermined systems ) . This is a lot of operations to unify together , and it may initially seem strange to do so . The common thread – and our justification for unifying these operations – is that mathematically , all the above operations correspond to the pseudoinverse solution to Ax = b , ie . the solution arising from using the Moore-Penrose pseudoinverse x = A†b [ 1 , 23 ] . The user can specify which solver they ’ d like to use via the solver argument . This is helpful when the user already knows which solvers should work well for a problem . Not every solver is capable of handling every problem . For example , lineax.CG handles positive definite operators [ 21 , section 5 ] . Using a solver with an incompatible problem will result in an error . 3 General linear operators In Lineax , we represent A more generally than as an n × m matrix . Instead , we represent A as a linear operator A : X → Y , where X and Y are spaces of PyTrees of arrays . At an implementation level , a linear operator is an object which subclasses lineax . A bs t ra c tL i ne a r Op e rat or . When A is a dense matrix , A ∈ Rdim ( Y ) ×dim ( X ) , it can be treated as a Lineax linear operator via lineax . Ma trix Li ne ar Op er ator ( A ) . 2 Lineax operators themselves form a vector space , and are closed under addition , scalar multiplication , and composition . Each linear operator A must implement a method to : • Compute the matrix-vector product : Ax for x ∈ X . • Compute the transpose of the operator : AT : Y → X . • Materialise the operator as a matrix : A.as_matrix ( ) ∈ Rdim ( X ) ×dim ( Y ) . • Retrieve the input/output PyTree structure , as well as the input/output dimensions . ie . the functions domain ( A ) = X and codomain ( A ) = Y . This increased generality comes with increased flexibility . For example : large , sparse matrices can use data-efficient formats and utilise linear solves which use only the matrix-vector product , such as GMRES [ 24 ] or BiCGStab [ 28 ] . For example , a linear function f : X → Y can be made into a linear operator with lineax . F un c ti o nL i ne a r Op e rat or ( f , in_structure ) where in_structure describes the PyTree structure of the input of f ( equivalently , the PyTree structure of the elements x ∈ X . ) Similarly , a nonlinear function g : X → Y can be linearised at a point x ∈ X and use its Jacobian at x as a linear operator via lineax . J ac o bi a nL i ne a r Op e rat or ( g , x ) The lineax.AbstractLinearOperator base class is available for users to subclass and create their own linear operator types . 3.1 Operator tags Tags are an optional argument to most linear operators , and indicate properties of the operator A . For example , if A ∈ Rn×n is positive semidefinite , then A can be marked as a positive semidefinite linear operator with lineax . Ma trix Li ne ar Op er ator ( A , lineax . p o s i t i ve _ s e m i d e f i ni t e _ t a g ) This indicates to any solver which uses A that it is positive semidefinite . For example , if A is also nonsingular , then it can be used safely with lineax.CG . Tags are also used to select the appropriate solver in the polyalgorithm lineax.AutoLinearSolver detailed in section 6 . 4 Computing gradients In JAX , derivatives are built from Jacobian-vector products ( JVPs ) and vector-Jacobian products ( VJPs ) for forward-mode and reverse-mode automatic differentiation respectively [ 12 ] . The JVP of a function f : Ra → Rb maps an input-tangent pair ( x , v ) ∈ Ra × Ra to ( f ( x ) , ∂f ( x ) ( v ) ) ∈ Rb × Rb where ∂f ( x ) : Ra → Rb is the Jacobian of f at x . The VJP maps an input-cotangent pair ( x , c ) ∈ Ra × Rb to ( f ( x ) , ∂f ( x ) T c ) ∈ Rb × Ra , where ∂f ( x ) T : Rb → Ra is the transpose of ∂f ( x ) . The major contribution of Lineax over existing linear solve and least-squares software is the efficient computation of JVPs for pseudoinverse solutions . That is , differentiation through both well-posed linear solves and ill-posed least-squares solves are performed in the same manner as each other . In particular , we may special case when operators have full row or column rank in order to obtain improved performance , as we now show . 4.1 JVPs and forward-mode autodifferentiation In this section , let L ( A , b ) denote the linear solve lineax.linear_solve . For a primal problem Ax = b , then L ( A , b ) = A†b where A† be the Moore–Penrose pseudoinverse of A , as mentioned in section 2 . Here we discuss how the to compute the JVP ∂L ( A , b ) ( V , v ) , where ( V , v ) is the tangent pair consisting of a tangent operator V and tangent vector v. It is possible to compute the JVP through either argument . For example , the tangent computation of a linear solve as a function of v alone is ∂L ( A , b ) ( 0 , v ) = A†v 3 ( 2 ) ( 3 ) ( 4 ) where 0 represents the 0 tangent operator . Meanwhile , computing the JVP for ∂L ( A , b ) ( V , 0 ) requires differentiating through a pseudoinverse , which has the explicit formula [ 13 ] ∂L ( A , b ) ( V , 0 ) = ( −A†V A† + A† ( A† ) T V T ( I − AA† ) + ( I − A†A ) V T ( A† ) T A† ) A†b . Letting x = A†b z = V T ( A† ) T x , and adding the above two equations together and using the linearity of the Jacobian , we have the total JVP with respect the primal pair ( A , b ) and tangent pair ( V , v ) for lineax.linear_solve is ∂L ( A , b ) ( V , v ) = A† ( cid:0 ) −V x + ( A† ) T V T ( b − Ax ) − Az + v ( cid:1 ) + z . ( 1 ) If A has linearly independent columns , then A†A = I [ 14 , section 5.5.2 ] and the term z − A†Az = 0 , giving ∂L ( A , b ) ( V , v ) = A† ( cid:0 ) −V x + ( A† ) T V T ( b − Ax ) + v ( cid:1 ) . When A has linearly independent rows , then ( b − Ax ) = 0 and ∂L ( A , b ) ( V , v ) = A† ( −V x − Az + v ) + z . Together , if A has linearly independent rows and columns , then A is well-posed , A† = A−1 is a true inverse , and ∂L ( A , b ) ( V , v ) = A−1 ( −V x + v ) . We then select between equations ( 4 ) , ( 3 ) , ( 2 ) , or ( 1 ) depending on whether we know at compile time that A has linearly independent rows and columns , has only independent rows , has only independent columns , or has both dependent rows and columns . Despite being a property of the operator , at compile time the main way the JVP rule is dispatched via the choice of solver . This is because not every solver supports dependent rows/columns ( section 5 ) , and will return nan values if used in a solve with an unsupported operator . So , if a solver does not support dependent rows/columns , we can be sure we will not get a solution given an operator with dependent rows/columns in the JVP . For example , lineax.QR [ 27 , section 2 ] can handle dependent rows if the number of rows is greater than the number of columns ( or dependent columns if the number of columns is greater than the number of rows ) and will dispatch to either equation ( 2 ) or ( 3 ) . If the number of columns and rows of A are the same , then lineax.QR will dispatch to equation ( 4 ) . 4.2 VJPs and backpropogation via transposition Reverse-mode autodifferentiation of a function f : Ra → Rb is not built on JVPs , but rather on vector-Jacobian products ( VJPs ) vT ∂f ( x ) = ∂f ( x ) T v. As suggested by the definition , VJPs are constructed via a JVP and transposition [ 12 ] . This is how VJPs are implemented in JAX , and thus in Lineax as well . The Jacobian ∂L ( A , b ) : Rm×n × Rm → Rn is a linear function , see also the explicit form in equation ( 1 ) . Therefore , it has a transpose ∂L ( A , b ) T : Rn → Rm×n × Rm . The transpose rule for the linear solve is implemented as a custom JAX primitive . See [ 12 ] for more details . 5 User-defined solvers A user can implement a custom solver by subclassing lineax . Ab stra ct Li ne ar So lver which requires the methods : init , compute , transpose , allow_dependent_rows , and allow_dependent_columns . 4 Many direct linear solvers for Ax = b use two stages of computation . First , factor A into a form amenable to computation ( eg . LU factorisation [ 14 , section 3 ] , QR factorisation , SVD factorisation , etc . ) Then , use this factorisation to solve for a given right hand side b . The factorisation of A does not depend on the right hand side b , and can be reused with various choices of b . This saves computation cost when solving Ax = b for many right hands b . For a solver using such a two-stage factorisation approach , init computes the factorisation , and compute performs the solve for the specific right hand b. transpose computes the transpose of the factorisation provided by init , which allows us to skip computing the factorisation of the transpose operator directly ( init ( transpose ( operator ) ) ) , as it is commonly the case that we can cheaply derive this from init ( operator ) alone . This is needed when computing VJPs as discussed in the previous section . The methods allow_dependent_rows and allow_dependent_columns determine which equation ( 1-4 ) is used in the differentiation , as discussed in section 4 . This greatly simplifies the process of writing a differentiable linear solver or least-squares solver . In the core JAX library , it is somewhat cumbersome to write a differentiable solver . It requires using jax.lax.custom_linear_solve and implementing a solver transposition rule transpose_solve if the user would like to use reverse-mode autodifferentiation . In Lineax , differentiation comes for free once a solver is implemented , whether the solver is a linear solve or a least-squares algorithm . 6 The AutoLinearSolver polyalgorithm If the user does not provide a solver to lineax.linear_solve , then the default linear solve is lineax.AutoLinearSolver . lineax.AutoLinearSolver is a polyalgorithm which selects a solver automatically at compile time depending on the structure of A , as indicated through its operator tag ( discussed in section 3.1 ) . lineax.AutoLinearSolver takes the argument well_posed , which indicates whether the system is expected to solve a least-squares/minimum norm problem , or only handle well-posed linear solves . lineax.AutoLinearSolver ( well_posed=True ) selects a solver depending upon the oper- ator structure , and throws an error when it encounters an underdeteremined or overdeter- mined system . lineax.AutoLinearSolver ( well_posed=False ) solves well-posed linear solves as well as linear least squares , but often at an additional computation cost . Finally , lineax.AutoLinearSolver ( well_posed=None ) solves a least-squares problem only if it is not expensive to do so . The specific polyalgorithms for well_posed=True , well_posed=False , and well_posed=None are shown in figure 1 . 6.1 Choosing a solver at compile time We must choose between two paradigms for the implementation of lineax.AutoLinearSolver : make the algorithm selection at run time , or make the algorithm selection at compile time . This is a trade-off , as determining which solver to use at run time means checking the elements of the matrix . This incurs a run time overhead of O ( n2 ) for an n × n matrix , which is relatively small compared to the O ( n3 ) run time of most linear solve algorithms . However , run time checking also incurs a greater cost in compile times . Since it is not known at compile time which branch of the polyalgorithm will run , the compiler is forced to compile all branches . Thus , compilation cost scales with the logic of the polyalgorithm : as more branches are included , compile times increase . This can limit the extensibility of the polyalgorithm . Compile time selection avoids these performance issues , and is faster both in run time and compile time when used correctly . Further , it simplifies tying solves to GPU hardware , as there is no possibility of taking separate branches for different batch elements . However , compile time selection requires the user to a-priori know the structure of the operator , and can result in using a suboptimal solver if the operator has exploitable structure which the user does not indicate . We choose the compile time approach , and require the user to pass the structure of an operator explic- itly via the operator tag ( section 3.1 . ) We choose this approach primarily to minimise compilation times , and to avoid the tradeoff between extensibility and compile time inherent in run time checking . 5 The AutoLinearSolver polyalgorithm . Figure 1 : so that well_posed=True starts at `` A is square ? `` , well_posed=None starts at `` A is diagonal ? `` , and well_posed=False starts at `` A is diagonal ? '' in the well_posed=False section . Read from left-to-right , 6 Our approach is in contrast to MATLAB ’ s mldivide , a ( nondifferentiable ) unified linear solve and least-squares solver which uses a run time approach [ 17 ] . Both the Julia and MATLAB languages offer methods for nonsingular linear solves – the infix \ operation in Julia and linsolve in MATLAB – which accept compile time tags , but still perform run time checks if the user passes no tags [ 2 , 7 , 17 ] . Therefore , both suffer from the additional overhead of the run time approach in many cases . 7 Conclusion We have introduced Lineax , a differentiable JAX+Equinox library unifying linear solves and linear least-squares . We have demonstrated that users can extend base Lineax operators and solvers and use them within our unified API , without the need to write any custom derivative rules . We hope to see adoption of Lineax solves within the JAX+Equinox scientific computing and machine learning ecosystem . 8 Acknowledgements This publication is based on work supported by the EPSRC Centre for Doctoral Training in Mathe- matics of Random Systems : Analysis , Modelling and Simulation ( EP/S023925/1 ) References [ 1 ] Adi Ben-Israel and Thomas N. E. Greville . Generalized Inverses : Theory and Applications . Springer New York , 2003 . [ 2 ] Jeff Bezanson , Alan Edelman , Stefan Karpinski , and Viral B Shah . Julia : A fresh approach to numerical computing . SIAM Review , 59 ( 1 ) :65–98 , 2017 . URL : https : //epubs.siam.org/ doi/10.1137/141000671 , doi:10.1137/141000671 . [ 3 ] Deniz A. Bezgin , Aaron B. Buhendwa , and Nikolaus A. Adams . JAX-fluids : A fully- differentiable high-order computational fluid dynamics solver for compressible two-phase flows . Computer Physics Communications , 282:108527 , 2023 . [ 4 ] James Bradbury , Roy Frostig , Peter Hawkins , Matthew James , Johnson , Chris Leary , Dougal Maclaurin , George Necula , Adam Paszke , Jake VanderPlas , Skye Wanderman-Milne , and Qiao Zhang . Jax : composable transformations of python+numpy programs , 2018 . URL : http : //github.com/google/jax . [ 5 ] Marco Cuturi , Laetitia Meng-Papaxanthos , Yingtao Tian , Charlotte Bunne , Geoff Davis , and Olivier Teboul . Optimal transport tools ( ott ) : A jax toolbox for all things wasserstein . arXiv preprint arXiv:2201.12324 , 2022 . [ 6 ] Gideon Dresdner , Dmitrii Kochkov , Peter Norgaard , Leonardo Zepeda-Núñez , Jamie A. Smith , Michael P. Brenner , and Stephan Hoyer . Learning to correct spectral methods for simulating turbulent flows . arXiv preprint arXiv:2207.00556 , 2022 . [ 7 ] Chris Rackauckas et al . Linearsolve.jl : High-performance unified linear solvers , 2021 . Accessed 2023 . URL : https : //github.com/SciML/LinearSolve.jl . [ 8 ] David Hall et al . Haliax . Accessed 2023 , 2023 . URL : https : //github.com/ stanford-crfm/haliax . [ 9 ] Igor Babuschkin et al . The deepmind jax ecosystem , 2020 . URL : http : //github.com/ deepmind . [ 10 ] John Jumper et al . Highly accurate protein structure prediction with alphafold . Nature , 596:583– 589 , 8 2021 . [ 11 ] C. Daniel Freeman , Erik Frey , Anton Raichuk , Sertan Girgin , Igor Mordatch , and Olivier Bachem . Brax - a differentiable physics engine for large scale rigid body simulation , 2021 . 7 [ 12 ] Roy Frostig , Matthew J. Johnson , Dougal Maclaurin , Adam Paszke , and Alexey Radul . Decom- posing reverse-mode automatic differentiation , 2021. arXiv:2105.09469 . [ 13 ] Gene H. Golub and V. Pereyra . The differentiation of pseudo-inverses and nonlinear least squares problems whose variables separate . SIAM Journal on Numerical Analysis , 10 ( 2 ) :413–432 , 1973 . [ 14 ] Gene H. Golub and Charles F. Van Loan . Matrix Computations . The Johns Hopkins University Press , third edition , 1996 . [ 15 ] David Hall , Ivan Zhou , and Percy Liang . Levanter — legible , scalable , reproducible foundation models with jax . Accessed 2023 , 2023 . URL : https : //github.com/stanford-crfm/ levanter . [ 16 ] Jonathan Heek , Anselm Levskaya , Avital Oliver , Marvin Ritter , Bertrand Rondepierre , Andreas Steiner , and Marc van Zee . Flax : A neural network library and ecosystem for JAX , 2023 . URL : http : //github.com/google/flax . [ 17 ] The MathWorks Inc. Matlab version : 9.13.0 ( r2022b ) , 1984 . URL : https : //www.mathworks . com . [ 18 ] J. Emmanuel Johnson and Takaya Uchida . Jaxsw : Jax approximate ocean models , 2023 . Accessed 2023 . URL : https : //github.com/jejjohnson/jaxsw . [ 19 ] Patrick Kidger . On Neural Differential Equations . PhD thesis , University of Oxford , 2021 . [ 20 ] Patrick Kidger and Cristian Garcia . Equinox : neural networks in JAX via callable PyTrees and filtered transformations . Differentiable Programming workshop at Neural Information Processing Systems 2021 , 2021 . [ 21 ] Jorge Nocedal and Stephen Wright . Numerical Optimization ( Second Edition ) . Springer New York , 2006 . [ 22 ] Alan V. Oppenheim and Ronald W. Schafer . Digital Signal Processing . Prentice-Hall , 1975 . [ 23 ] R. Penrose . A generalized inverse for matrices . Mathematical Proceedings of the Cambridge Philosophical Society , 51 ( 3 ) :406–413 , 1955 . [ 24 ] Youcef Saad and Martin H. Schultz . Gmres : A generalized minimal residual algorithm for solving nonsymmetric linear systems . SIAM Journal on Scientific and Statistical Computing , 7 ( 3 ) :856–869 , 1986 . [ 25 ] Miloš Stanojevi´c and Laurent Sartran . SynJax : Structured Probability Distributions for JAX . arXiv preprint arXiv:2308.03291 , 2023 . [ 26 ] J. W. Thomas . Numerical Partial Differential Equations : Finite Difference Methods . Springer New York , 1995 . [ 27 ] Lloyd N. Trefethen and David Bau . Numerical Linear Algebra . Society for Industrial and Applied Mathematics , 1997 . [ 28 ] H. A. van der Vorst . Bi-cgstab : A fast and smoothly converging variant of bi-cg for the solution of nonsymmetric linear systems . SIAM Journal on Scientific and Statistical Computing , 13 ( 2 ) :631–644 , 1992 . [ 29 ] Jake Vanderplas . Jep 18137 : Scope of jax numpy & scipy wrappers . Github pull request , created and accessed 2023 , 2023 . URL : https : //github.com/google/jax/pull/18137 . 8","['c', 'v', 'r', 'lineax', 'unified', 'linear', 'solve', 'linear', 'leastsquare', 'jax', 'math', 'abstract', 'introduce', 'lineax', 'library', 'bring', 'linear', 'solve', 'linear', 'leastsquare', 'jaxequinox', 'scientific', 'computing', 'ecosystem', 'lineax', 'use', 'general', 'linear', 'operator', 'unify', 'linear', 'solve', 'leastsquare', 'single', 'autodifferen', 'api', 'solver', 'operator', 'userextensible', 'require', 'user', 'implement', 'custom', 'derivative', 'rule', 'get', 'differentiability', 'lineax', 'available', 'githubcomgooglelineax', 'introduction', 'jax', 'autodifferentiatiable', 'python', 'framework', 'popular', 'machine', 'learning', 'scientific', 'comput', 'equinox', 'popular', 'jax', 'library', 'target', 'use', 'case', 'add', 'additional', 'support', 'parameterise', 'function', 'solve', 'linear', 'system', 'wellpose', 'linear', 'solve', 'illpose', 'linear', 'leastsquare', 'problem', 'central', 'subproblem', 'scientific', 'computing', 'example', 'linear', 'solve', 'leastsquare', 'appear', 'subroutine', 'nonlinear', 'optimisation', 'finitedifference', 'scheme', 'signal', 'processing', 'introduce', 'lineax', 'library', 'build', 'jax', 'equinox', 'linear', 'solve', 'linear', 'leastsquare', 'lineax', 'present', 'single', 'differentiable', 'interface', 'solve', 'wellpose', 'underdetermined', 'overdetermine', 'linear', 'system', 'also', 'allow', 'user', 'write', 'custom', 'differentiable', 'linear', 'solver', 'leastsquare', 'solver', 'introduce', 'linear', 'operator', 'abstraction', 'overall', 'intend', 'lineax', 'integrate', 'well', 'exist', 'jax', 'scientific', 'ecosystem', 'ecosystem', 'grow', 'include', 'package', 'differentiable', 'rigidbody', 'physics', 'simulation', 'computational', 'fluid', 'dynamic', 'protein', 'structure', 'prediction', 'ordinary', 'stochastic', 'differential', 'equation', 'probabilistic', 'modeling', 'begin', 'see', 'use', 'lineax', 'ecosystem', 'already', 'include', 'linear', 'subroutine', 'ocean', 'dynamic', 'optimal', 'transport', 'diffrax', 'plan', 'adopt', 'lineax', 'near', 'future', 'linear', 'subroutine', 'differential', 'equation', 'solve', 'main', 'contribution', 'main', 'contribution', 'lineax', 'general', 'linear', 'operator', 'abstraction', 'implement', 'dense', 'matrix', 'linear', 'function', 'jacobian', 'stable', 'fast', 'gradient', 'leastsquare', 'solve', 'include', 'userdefined', 'solver', 'require', 'extra', 'effort', 'user', 'pytreevalue', 'operator', 'vector', '1jax', 'terminology', 'arbitrarily', 'nest', 'container', 'type', 'type', 'taining', 'leave', 'pythonjax', 'type', 'exclusively', 'consider', 'pytree', 'leave', 'jax', 'array', 'neurip', 'ai', 'science', 'workshop', 'comparison', 'exist', 'jax', 'apis', 'operator', 'abstraction', 'introduce', 'lineax', 'offer', 'flexibility', 'find', 'support', 'dense', 'matrix', 'matrixvector', 'product', 'representation', 'operator', 'lineax', 'introduce', 'new', 'solver', 'lineaxtridiagonal', 'lineax', 'also', 'offer', 'consistent', 'api', 'operator', 'solver', 'allow', 'extensibility', 'userspecifie', 'custom', 'operator', 'solver', 'compilation', 'time', 'lineax', 'solver', 'essentially', 'identical', 'jax', 'native', 'solver', 'lineax', 'iterative', 'solver', 'cg', 'gmre', 'compile', 'roughly', 'twice', 'fast', 'benchmark', 'folder', 'provide', 'quantitative', 'comparison', 'emphasise', 'stable', 'fast', 'gradient', 'contrast', 'exist', 'jax', 'implementation', 'version', 'exhibit', 'instability', 'incorrect', 'gradient', 'exceptional', 'case', 'reason', 'actually', 'consider', 'deprecate', 'apis', 'favour', 'lineax', 'classical', 'linear', 'solve', 'example', 'consider', 'solve', 'ax', 'b', 'random', 'matrix', 'r10×10', 'random', 'vector', 'r10', 'import', 'jax', 'random', 'jr', 'import', 'lineax', 'lx', 'akey', 'bkey', 'jr', 'split', 'jr', 'prngkey', 'lx', 'atr', 'op', 'ator', 'normal', 'akey', 'b', 'jr', 'normal', 'bkey', 'solution', 'lx', 'linearsolve', 'b', 'perform', 'linear', 'solve', 'leastsquare', 'main', 'entry', 'point', 'linear', 'solve', 'leastsquare', 'lineax', 'linearsolve', 'b', 'solver', 'linear', 'operator', 'pytree', 'b', 'perform', 'linear', 'solve', 'ax', 'b', 'wellpose', 'system', 'return', 'leastsquare', 'solution', 'minx', 'b∥2', 'overdetermine', 'system', 'return', 'minimum', 'norm', 'solution', 'minx', 'subject', 'ax', 'b', 'underdetermined', 'system', 'lot', 'operation', 'unify', 'together', 'initially', 'seem', 'strange', 'common', 'thread', 'justification', 'unify', 'operation', 'mathematically', 'operation', 'correspond', 'pseudoinverse', 'solution', 'ax', 'b', 'solution', 'arise', 'use', 'moorepenrose', 'pseudoinverse', 'user', 'specify', 'solver', 'like', 'use', 'solver', 'argument', 'helpful', 'user', 'already', 'know', 'solver', 'work', 'well', 'problem', 'solver', 'capable', 'handle', 'problem', 'example', 'handle', 'positive', 'definite', 'operator', 'section', 'use', 'solver', 'incompatible', 'problem', 'result', 'error', 'general', 'linear', 'operator', 'lineax', 'represent', 'generally', 'n', '×', 'matrix', 'instead', 'represent', 'linear', 'operator', 'x', 'space', 'pytree', 'array', 'implementation', 'level', 'linear', 'operator', 'object', 'subclasse', 'lineax', 'bs', 'ne', 'r', 'op', 'e', 'rat', 'dense', 'matrix', 'rdim', 'treat', 'lineax', 'linear', 'operator', 'ator', 'lineax', 'operator', 'form', 'vector', 'space', 'close', 'addition', 'scalar', 'multiplication', 'composition', 'linear', 'operator', 'implement', 'method', 'compute', 'matrixvector', 'product', 'ax', 'compute', 'transpose', 'operator', 'materialise', 'operator', 'matrix', 'retrieve', 'inputoutput', 'pytree', 'structure', 'well', 'inputoutput', 'dimension', 'function', 'domain', 'x', 'codomain', 'increase', 'generality', 'come', 'increase', 'flexibility', 'example', 'large', 'sparse', 'matrix', 'use', 'dataefficient', 'format', 'utilise', 'linear', 'solve', 'use', 'matrixvector', 'product', 'gmre', 'bicgstab', 'example', 'linear', 'function', 'make', 'linear', 'operator', 'ne', 'r', 'op', 'e', 'rat', 'instructure', 'instructure', 'describe', 'pytree', 'structure', 'input', 'equivalently', 'pytree', 'structure', 'element', 'similarly', 'nonlinear', 'function', 'linearise', 'point', '∈', 'use', 'jacobian', 'linear', 'operator', 'ne', 'r', 'op', 'e', 'rat', 'g', 'lineaxabstractlinearoperator', 'base', 'class', 'available', 'user', 'subclass', 'create', 'linear', 'operator', 'type', 'operator', 'tag', 'tag', 'optional', 'argument', 'linear', 'operator', 'indicate', 'property', 'operator', 'example', '∈', 'rn×n', 'positive', 'semidefinite', 'mark', 'positive', 'semidefinite', 'operator', 'ator', 'lineax', 'p', 'e', 'e', 'e', 'g', 'indicate', 'solver', 'use', 'positive', 'semidefinite', 'example', 'also', 'nonsingular', 'use', 'safely', 'lineaxcg', 'tag', 'also', 'use', 'select', 'appropriate', 'solver', 'lineaxautolinearsolver', 'detail', 'section', 'computing', 'gradient', 'jax', 'derivative', 'build', 'product', 'vectorjacobian', 'product', 'vjps', 'reversemode', 'automatic', 'differentiation', 'respectively', 'jvp', 'function', 'rb', 'map', 'inputtangent', 'pair', '∈', 'rb', 'rb', 'jacobian', 'map', 'inputcotangent', 'pair', 'c', 'c', '∈', 'transpose', 'major', 'contribution', 'lineax', 'exist', 'linear', 'solve', 'leastsquare', 'software', 'efficient', 'computation', 'pseudoinverse', 'solution', 'differentiation', 'wellpose', 'linear', 'solve', 'illpose', 'leastsquare', 'solve', 'perform', 'manner', 'particular', 'special', 'case', 'operator', 'full', 'row', 'column', 'rank', 'order', 'obtain', 'improved', 'performance', 'show', 'forwardmode', 'autodifferentiation', 'section', 'let', 'l', 'b', 'denote', 'linear', 'solve', 'lineaxlinearsolve', 'primal', 'problem', 'ax', 'l', 'b', 'a†', 'moore', 'penrose', 'pseudoinverse', 'mention', 'section', 'discuss', 'compute', 'jvp', 'b', 'v', 'v', 'tangent', 'pair', 'consist', 'tangent', 'operator', 'v', 'tangent', 'vector', 'possible', 'compute', 'jvp', 'argument', 'example', 'tangent', 'computation', 'linear', 'solve', 'function', 'v', 'alone', 'b', 'represent', 'tangent', 'operator', 'meanwhile', 'compute', 'jvp', 'b', 'v', 'require', 'differentiate', 'pseudoinverse', 'explicit', 'formula', 'b', 'v', 'a†', 'a†', 'a†', 'a†', 'let', 'z', 'a†', 'add', 'equation', 'together', 'use', 'linearity', 'jacobian', 'total', 'jvp', 'respect', 'primal', 'pair', 'b', 'tangent', 'pair', 'v', 'lineaxlinearsolve', 'b', 'a†', 'cid0', '−v', 'ax', 'cid1', 'z', 'linearly', 'independent', 'column', 'section', 'term', 'z', 'give', '∂l', 'b', 'a†', 'cid0', '−v', 'ax', 'cid1', 'linearly', 'independent', 'row', 'ax', 'b', 'a†', 'z', 'together', 'linearly', 'independent', 'row', 'column', 'wellpose', 'a†', 'true', 'inverse', 'b', 'select', 'equation', 'depend', 'know', 'compile', 'time', 'linearly', 'independent', 'row', 'column', 'independent', 'row', 'independent', 'column', 'dependent', 'row', 'column', 'property', 'operator', 'compile', 'time', 'main', 'way', 'jvp', 'rule', 'dispatch', 'choice', 'solver', 'solver', 'support', 'dependent', 'rowscolumn', 'section', 'return', 'value', 'use', 'solve', 'unsupported', 'operator', 'solver', 'support', 'dependent', 'rowscolumn', 'sure', 'get', 'solution', 'give', 'operator', 'dependent', 'rowscolumn', 'jvp', 'example', 'section', 'handle', 'dependent', 'row', 'number', 'row', 'great', 'number', 'column', 'dependent', 'column', 'number', 'column', 'great', 'number', 'row', 'dispatch', 'equation', 'number', 'column', 'row', 'dispatch', 'equation', 'vjps', 'backpropogation', 'transposition', 'reversemode', 'autodifferentiation', 'function', 'rb', 'build', 'rather', 'vectorjacobian', 'product', 'vjps', 'vt', 'v', 'suggest', 'definition', 'vjps', 'construct', 'jvp', 'transposition', 'vjps', 'implement', 'thus', 'lineax', 'well', 'jacobian', 'b', 'rm×n', '×', 'rm', 'linear', 'function', 'see', 'also', 'explicit', 'form', 'equation', 'therefore', 'transpose', '∂l', 'b', 'rm×n', '×', 'rm', 'transpose', 'rule', 'linear', 'solve', 'implement', 'custom', 'jax', 'primitive', 'see', 'detail', 'userdefined', 'solver', 'user', 'implement', 'custom', 'solver', 'subclasse', 'lver', 'require', 'method', 'init', 'compute', 'transpose', 'allowdependentrow', 'allowdependentcolumn', 'many', 'direct', 'linear', 'solver', 'ax', 'b', 'use', 'stage', 'computation', 'first', 'factor', 'form', 'amenable', 'computation', 'factorisation', 'section', 'factorisation', 'factorisation', 'use', 'factorisation', 'solve', 'give', 'right', 'hand', 'side', 'b', 'factorisation', 'depend', 'right', 'hand', 'side', 'reuse', 'various', 'choice', 'b', 'save', 'computation', 'cost', 'solve', 'ax', 'b', 'many', 'right', 'hand', 'b', 'solver', 'use', 'twostage', 'factorisation', 'approach', 'compute', 'factorisation', 'compute', 'perform', 'solve', 'specific', 'right', 'hand', 'b', 'transpose', 'compute', 'transpose', 'factorisation', 'provide', 'init', 'allow', 'skip', 'compute', 'factorisation', 'transpose', 'operator', 'directly', 'init', 'transpose', 'operator', 'commonly', 'case', 'cheaply', 'derive', 'init', 'operator', 'alone', 'need', 'compute', 'discuss', 'previous', 'section', 'method', 'allowdependentrow', 'allowdependentcolumn', 'determine', 'equation', 'use', 'differentiation', 'discuss', 'section', 'greatly', 'simplify', 'process', 'write', 'differentiable', 'linear', 'solver', 'leastsquare', 'solver', 'library', 'somewhat', 'cumbersome', 'write', 'differentiable', 'solver', 'require', 'use', 'jaxlaxcustomlinearsolve', 'implement', 'solver', 'transposition', 'rule', 'transposesolve', 'user', 'like', 'use', 'reversemode', 'autodifferentiation', 'lineax', 'differentiation', 'come', 'free', 'solver', 'implement', 'solver', 'linear', 'solve', 'leastsquare', 'autolinearsolver', 'polyalgorithm', 'user', 'provide', 'solver', 'lineaxlinearsolve', 'default', 'linear', 'solve', 'lineaxautolinearsolver', 'lineaxautolinearsolver', 'polyalgorithm', 'select', 'solver', 'automatically', 'compile', 'time', 'depend', 'structure', 'indicate', 'operator', 'tag', 'discuss', 'section', 'lineaxautolinearsolver', 'take', 'argument', 'wellpose', 'indicate', 'system', 'expect', 'solve', 'leastsquaresminimum', 'norm', 'problem', 'handle', 'wellpose', 'linear', 'solve', 'lineaxautolinearsolver', 'wellposedtrue', 'select', 'solver', 'depend', 'oper', 'ator', 'structure', 'throw', 'error', 'encounter', 'underdeteremined', 'overdeter', 'mined', 'system', 'lineaxautolinearsolver', 'wellposedfalse', 'solve', 'wellpose', 'linear', 'solve', 'well', 'linear', 'least', 'square', 'often', 'additional', 'computation', 'cost', 'finally', 'lineaxautolinearsolver', 'wellposednone', 'solve', 'leastsquare', 'problem', 'expensive', 'specific', 'polyalgorithm', 'wellposedtrue', 'wellposedfalse', 'wellposednone', 'show', 'figure', 'choose', 'solver', 'compile', 'time', 'choose', 'paradigm', 'implementation', 'lineaxautolinearsolver', 'make', 'selection', 'run', 'time', 'make', 'selection', 'compile', 'time', 'tradeoff', 'determine', 'solver', 'use', 'run', 'time', 'mean', 'check', 'element', 'matrix', 'incur', 'run', 'time', 'overhead', 'n2', 'n', 'n', 'matrix', 'relatively', 'small', 'compare', 'n3', 'run', 'time', 'linear', 'solve', 'algorithm', 'however', 'run', 'time', 'check', 'also', 'incur', 'great', 'cost', 'compile', 'time', 'know', 'compile', 'time', 'branch', 'polyalgorithm', 'run', 'compiler', 'force', 'compile', 'branch', 'thus', 'compilation', 'cost', 'scale', 'logic', 'polyalgorithm', 'branch', 'include', 'compile', 'time', 'increase', 'limit', 'extensibility', 'compile', 'time', 'selection', 'avoid', 'performance', 'issue', 'fast', 'run', 'time', 'compile', 'time', 'use', 'correctly', 'simplify', 'tie', 'solve', 'hardware', 'possibility', 'take', 'separate', 'branch', 'different', 'batch', 'element', 'however', 'compile', 'time', 'selection', 'require', 'user', 'know', 'structure', 'operator', 'result', 'use', 'suboptimal', 'solver', 'operator', 'exploitable', 'structure', 'user', 'indicate', 'choose', 'compile', 'time', 'approach', 'require', 'user', 'pass', 'structure', 'operator', 'explic', 'itly', 'operator', 'tag', 'section', 'choose', 'approach', 'primarily', 'minimise', 'compilation', 'time', 'avoid', 'tradeoff', 'extensibility', 'compile', 'time', 'inherent', 'run', 'time', 'check', 'autolinearsolver', 'polyalgorithm', 'figure', 'wellposedtrue', 'start', 'square', 'wellposednone', 'start', 'diagonal', 'wellposedfalse', 'start', 'diagonal', 'section', 'read', 'lefttoright', 'approach', 'contrast', 'mldivide', 'nondifferentiable', 'unified', 'linear', 'solve', 'leastsquare', 'solver', 'use', 'run', 'time', 'approach', 'language', 'offer', 'method', 'nonsingular', 'linear', 'solve', 'infix', 'operation', 'linsolve', 'accept', 'compile', 'time', 'tag', 'still', 'perform', 'run', 'time', 'check', 'user', 'pass', 'tag', 'therefore', 'suffer', 'additional', 'overhead', 'run', 'time', 'approach', 'many', 'case', 'conclusion', 'introduce', 'lineax', 'differentiable', 'jaxequinox', 'library', 'unify', 'linear', 'solve', 'linear', 'leastsquare', 'demonstrate', 'user', 'extend', 'base', 'lineax', 'operator', 'solver', 'use', 'unified', 'api', 'need', 'write', 'custom', 'derivative', 'rule', 'hope', 'see', 'adoption', 'lineax', 'solve', 'jaxequinox', 'scientific', 'computing', 'machine', 'learn', 'ecosystem', 'acknowledgement', 'publication', 'base', 'work', 'support', 'epsrc', 'centre', 'doctoral', 'training', 'matic', 'random', 'system', 'analysis', 'modelling', 'simulation', 'reference', 'adi', 'generalize', 'inverse', 'theory', 'application', 'viral', 'shah', 'fresh', 'approach', 'url', 'https', 'epubssiamorg', 'doi101137141000671', 'doi101137141000671', 'deniz', 'bezgin', 'buhendwa', 'nikolaus', 'adam', 'jaxfluid', 'fully', 'differentiable', 'highorder', 'computational', 'fluid', 'dynamic', 'solver', 'compressible', 'twophase', 'flow', 'computer', 'physics', 'communication', 'frostig', 'dougal', 'vanderpla', 'skye', 'wandermanmilne', 'jax', 'composable', 'transformation', 'pythonnumpy', 'program', 'url', 'http', 'githubcomgooglejax', 'marco', 'olivi', 'teboul', 'optimal', 'transport', 'tool', 'ott', 'jax', 'toolbox', 'thing', 'wasserstein', 'arxiv', 'preprint', 'dresdner', 'dmitrii', 'jamie', 'brenner', 'hoyer', 'learn', 'correct', 'spectral', 'method', 'simulate', 'turbulent', 'flow', 'arxiv', 'preprint', 'arxiv220700556', 'highperformance', 'unify', 'linear', 'solver', 'access', 'url', 'https', 'haliax', 'access', 'url', 'stanfordcrfmhaliax', 'igor', 'babuschkin', 'ecosystem', 'url', 'deepmind', 'highly', 'accurate', 'protein', 'structure', 'prediction', 'alphafold', 'nature', 'c', 'anton', 'raichuk', 'sertan', 'girgin', 'igor', 'mordatch', 'olivi', 'bachem', 'brax', 'differentiable', 'physics', 'engine', 'large', 'scale', 'rigid', 'body', 'simulation', 'roy', 'frostig', 'paszke', 'pose', 'reversemode', 'automatic', 'differentiation', 'arxiv210509469', 'gene', 'h', 'golub', 'pereyra', 'differentiation', 'pseudoinverse', 'nonlinear', 'least', 'square', 'problem', 'variable', 'separate', 'numerical', 'analysis', 'gene', 'h', 'golub', 'loan', 'matrix', 'computation', 'press', 'third', 'edition', 'levanter', 'legible', 'scalable', 'reproducible', 'foundation', 'model', 'jax', 'access', 'url', 'https', 'githubcomstanfordcrfm', 'levanter', 'ritter', 'flax', 'neural', 'network', 'library', 'ecosystem', 'jax', 'url', 'http', 'githubcomgoogleflax', 'url', 'https', 'wwwmathwork', 'com', 'approximate', 'ocean', 'model', 'access', 'url', 'kidger', 'neural', 'differential', 'equation', 'phd', 'thesis', 'kidger', 'equinox', 'neural', 'network', 'callable', 'pytree', 'filter', 'transformation', 'differentiable', 'programming', 'workshop', 'neural', 'information', 'processing', 'system', 'jorge', 'nocedal', 'optimization', 'second', 'edition', 'digital', 'signal', 'processing', 'prenticehall', 'r', 'penrose', 'generalized', 'inverse', 'matrix', 'mathematical', 'proceeding', 'cambridge', 'philosophical', 'society', 'schultz', 'gmre', 'generalized', 'minimal', 'residual', 'algorithm', 'solve', 'nonsymmetric', 'scientific', 'statistical', 'computing', 'miloš', 'stanojevi´c', 'laurent', 'sartran', 'synjax', 'structured', 'probability', 'distribution', 'jax', 'arxiv', 'preprint', 'numerical', 'partial', 'differential', 'equation', 'finite', 'difference', 'method', 'algebra', 'society', 'industrial', 'apply', 'mathematic', 'h', 'der', 'bicgstab', 'fast', 'smoothly', 'converge', 'variant', 'bicg', 'solution', 'nonsymmetric', 'scientific', 'statistical', 'computing', 'jake', 'vanderpla', 'jep', 'scope', 'numpy', 'scipy', 'wrapper', 'pull', 'request', 'create', 'access', 'url']",
"Lineax: unified linear solves and linear least-squares in JAX and
  Equinox","[{'href': 'http://arxiv.org/abs/2311.17283v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2311.17283v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-11-28 23:50:08,"3
2
0
2

v
o
N
2

]

G
L
.
s
c
[

1
v
0
5
4
1
0
.
1
1
3
2
:
v
i
X
r
a

DREAMSMOOTH: IMPROVING MODEL-BASED REIN-
FORCEMENT LEARNING VIA REWARD SMOOTHING

Vint Lee, Pieter Abbeel, Youngwoon Lee
University of California, Berkeley

ABSTRACT

Model-based reinforcement learning (MBRL) has gained much attention for its
ability to learn complex behaviors in a sample-efficient way: planning actions
by generating imaginary trajectories with predicted rewards. Despite its success,
we found that surprisingly, reward prediction is often a bottleneck of MBRL,
especially for sparse rewards that are challenging (or even ambiguous) to predict.
Motivated by the intuition that humans can learn from rough reward estimates, we
propose a simple yet effective reward smoothing approach, DreamSmooth, which
learns to predict a temporally-smoothed reward, instead of the exact reward at the
given timestep. We empirically show that DreamSmooth achieves state-of-the-art
performance on long-horizon sparse-reward tasks both in sample efficiency and
final performance without losing performance on common benchmarks, such as
Deepmind Control Suite and Atari benchmarks.

1

INTRODUCTION

Humans often plan actions with a rough estimate
of future rewards, instead of the exact reward at
the exact moment (Fiorillo et al., 2008; Klein-
Fl¨ugge et al., 2011). A rough reward estimate is
mostly sufficient to learn a task, and predicting
the exact reward is often challenging since it
can be ambiguous, delayed, or not observable.
Consider for instance the manipulation task il-
lustrated in Figure 1 (middle) of pushing a block
on a table into a bin, where a sparse reward is
given only on the timestep when the block first
touches the bin. Using the same image observa-
tions as the agent, it is challenging even for hu-
mans to predict the correct sequence of rewards.
Crucially, this issue is present in many environ-
ments, where states with no reward are almost
indistinguishable from those with rewards.

Figure 1: Predicting the exact sequence of rewards
is extremely difficult. These examples show the
sequences of image observations seen by the agent
just before and after it receives a sparse reward.
There is little to visually distinguish timesteps with
a large reward from those without, which creates a
significant challenge for reward prediction.

An accurate reward model is vital to model-
based reinforcement learning (MBRL) – reward
estimates that are too high will cause an agent
to choose actions that perform poorly in reality,
and estimates that are too low will lead an agent
to ignore high rewards. Despite its difficulty and
importance, the reward prediction problem in
MBRL has been largely overlooked. We find that even for the state-of-the-art MBRL algorithm,
DreamerV3 (Hafner et al., 2023), reward prediction is not only challenging, but is also a performance
bottleneck for many tasks. For instance, DreamerV3 fails to predict any reward for most objectives
in the Crafter environment (Hafner, 2022) with similar failure modes observed on variants of the
RoboDesk (Kannan et al., 2021) and Shadow Hand (Plappert et al., 2018) tasks with sparse rewards.

1

 
 
 
 
 
 
Inspired by the human intuition that only a rough estimate of rewards is sufficient, we propose a
simple yet effective solution, DreamSmooth, which learns to predict a temporally-smoothed reward
rather than the exact reward at each timestep. This makes reward prediction much easier – instead of
having to predict rewards exactly, now the model only needs to produce an estimate of when sparse
rewards are obtained, which is sufficient for policy learning.

Our experiments demonstrate that while extremely simple, this technique significantly improves
performance of different MBRL algorithms on many sparse-reward environments. Specifically, we
find that for DreamerV3 (Hafner et al., 2023) and TD-MPC (Hansen et al., 2022), our technique
is especially beneficial in environments with the following characteristics: sparse rewards, partial
observability, and stochastic rewards. Finally, we show that even on benchmarks where reward
prediction is not a significant issue, DreamSmooth does not degrade performance, which indicates
that our technique can be universally applied.

2 RELATED WORK

Model-based reinforcement learning (MBRL) leverages a dynamics model (i.e. world model) of an
environment and a reward model of a desired task to plan a sequence of actions that maximize the
total reward. The dynamics model predicts the future state of the environment after taking a specific
action and the reward model predicts the reward corresponding to the state-action transition. With
the dynamics and reward models, an agent can simulate a large number of candidate behaviors in
imagination instead of in the physical environment, allowing MBRL to tackle many challenging
tasks (Silver et al., 2016; 2017; 2018).

Instead of relying on the given dynamics and reward models, recent advances in MBRL have enabled
learning a world model of high-dimensional observations and complex dynamics (Ha & Schmidhuber,
2018; Schrittwieser et al., 2020; Hafner et al., 2019; 2021; 2023; Hansen et al., 2022), as well as a
temporally-extended world model (Shi et al., 2022). Specifically, DreamerV3 (Hafner et al., 2023)
has achieved the state-of-the-art performance across diverse domains of problems, e.g., both with
pixel and state observations as well as both with discrete and continuous actions.

For realistic imagination, MBRL requires an accurate world model. There have been significant
efforts in learning better world models by leveraging human videos (Mendonca et al., 2023), by
adopting a more performant architecture (Deng et al., 2023), and via representation learning, such
as prototype-based (Deng et al., 2022) and object-centric (Singh et al., 2021) state representations,
contrastive learning (Okada & Taniguchi, 2021), and masked auto-encoding (Seo et al., 2022; 2023).

However, compared to the efforts on learning a better world model, learning an accurate reward
model has been largely overlooked. Babaeizadeh et al. (2020) investigates the effects of various world
model designs and shows that reward prediction is strongly correlated to task performance when
trained on an offline dataset, while limited to dense-reward environments. In this paper, we point out
that accurate reward prediction is crucial for MBRL, especially in sparse-reward tasks and partially
observable environments, and propose a simple method to improve reward prediction in MBRL.

3 APPROACH

The main goal of this paper is to understand how challenging reward prediction is in model-based
reinforcement learning (MBRL) and propose a simple yet effective solution, reward smoothing, which
makes reward prediction easier to learn. In this section, we first provide a background about MBRL
in Section 3.1, then present experiments demonstrating the challenge of predicting sparse reward
signals in Section 3.2, and finally explain our approach, DreamSmooth, in Section 3.4.

3.1 BACKGROUND

We formulate a problem as a partially observable Markov decision process (POMDP), which is
defined as tuple (O, A, P, R, γ). O is an observation space, A is an action space, P (ot+1|o≤t, a≤t)
with timestep t is a transition dynamics, R is a reward function that maps previous observations and
actions to a reward rt = R(o≤t, a≤t), and γ ∈ [0, 1) is a discount factor (Sutton & Barto, 2018). RL
aims to find a policy π(at | o≤t, a<t) that maximizes the expected sum of rewards Eπ[(cid:80)T
t=1 γt−1rt].

2

This paper focuses on MBRL algorithms that learn a world model Pθ(zt+1|zt, at) and reward model
Rθ(rt|zt) from agent experience, where zt is a learned latent state at timestep t. The learned world
model and reward model can then generate imaginary rollouts {zτ , aτ , rτ }t+H−1
of the horizon H
starting from any zt, which can be used for planning (Argenson & Dulac-Arnold, 2021; Hansen et al.,
2022) or policy optimization (Ha & Schmidhuber, 2018; Hafner et al., 2019). Specifically, we use the
state-of-the-art algorithms, DreamerV3 (Hafner et al., 2023) and TD-MPC (Hansen et al., 2022).

τ =t

DreamerV3 (Hafner et al., 2023) uses the predicted rewards for computing new value targets to train
the critic. For learning a good policy, the reward model plays a vital role since the critic, from which
the actor learns a policy, receives its training signal exclusively through the reward model. Note that
the data collected from the environment is only used for training a world model and reward model.

On the other hand, TD-MPC (Hansen et al., 2022) learns a state-action value function Q(zt, at)
directly from agent experience, not from predicted rewards. However, the reward model is still
important for obtaining a good policy in TD-MPC because the algorithm uses both the reward model
and value function to obtain the policy through online planning.

3.2 REWARD PREDICTION IS DIFFICULT

Reward prediction is surprisingly challenging in many environments. Figure 1 shows sequences of
frames right before and after sparse rewards are received in diverse environments. Even for humans,
it is difficult to determine the exact timestep when the reward is received in all three environments.
We hypothesize that the mean squared error loss E(z,r)∼D[(Rθ(z) − r)2], typically used for reward
model training, deteriorates reward prediction accuracy when there exist sparse rewards. This is
because predicting a sparse reward a single step earlier or later results in a higher loss than simply
predicting 0 reward at every step. Thus, instead of trying to predict sparse rewards at the exact
timesteps, a reward model minimizes the loss by entirely omitting sparse rewards from its predictions.

To verify this hypothesis, we plot the ground-truth and DreamerV3’s predicted rewards in Figure 2.
The reward models struggle at predicting exact rewards and simply ignore sparse rewards unless they
are straightforward to predict on the four tasks described in Section 4.1. This hypothesis also holds in
a deterministic and fully-observable environment, Crafter, which has 24 sources of sparse rewards.
The reward model fails to predict most of these reward sources (Figure 2d).

The difficulty of reward prediction can be further exacerbated by partial observability, ambiguous
rewards, or stochastic dynamics of environments. As an example in the first (third) row in Figure 1,

(a) RoboDesk

(b) Hand

(c) Earthmoving

(d) Crafter

Figure 2: Ground truth rewards and DreamerV3’s predicted rewards over an evaluation episode. The
reward model misses many sparse rewards, which is highlighted in yellow.

3

Ground Truth

Predicted

x-axis: Timestep

y-axis: Reward

330

0

−110

0

120

330

0

−110

0

120

2400

0

−800

0

360

1.2

0.0

−0.4

0

200

the sparse rewards are given when the block (the rocks in the third example) first contacts the bin (the
dumptruck). The exact moment of contact is not directly observable from the camera viewpoint, and
this makes reward prediction ambiguous. Moreover, stochastic environment dynamics, e.g., contact
between multiple rocks, can make predicting a future state and reward challenging.

3.3 REWARD PREDICTION IS A BOTTLENECK OF MBRL

The preceding section shows that reward prediction is challenging in many environments. More
importantly, this poor reward prediction can be a bottleneck of policy learning, as shown in Figure 3.
In RoboDesk, where the reward model does not reliably detect the completion of the second task
(Figure 2a), the policy gets stuck at solving the first task and fails on subsequent tasks. In Earthmoving,
where the reward model cannot capture rewards for successful dumping (Figure 2c), the policy
frequently drops the rocks outside the dumptruck. These consistent failure modes in reward prediction
and policy learning in DreamerV3 suggest that poor reward prediction can be a bottleneck of MBRL.

(a) RoboDesk

(b) Earthmoving

Figure 3: The reward model’s inability to predict sparse rewards for completing tasks leads to poor
task performance. (a) In RoboDesk, the agent gets stuck after learning the first task, and is unable to
learn to perform the subsequent tasks. (b) In Earthmoving, the policy often fails to dump the rocks
accurately into the dumptruck. The learning curves are averaged over 3 seeds.

3.4 DREAMSMOOTH: IMPROVING MBRL VIA REWARD SMOOTHING

To address the reward prediction problem,
we propose a simple yet effective solution,
DreamSmooth, which relaxes the require-
ment for the model to predict sparse rewards
at the exact timesteps by performing tempo-
ral smoothing. Allowing the reward model to
predict rewards that are off from the ground
truth by a few timesteps makes learning eas-
ier, especially when rewards are ambiguous
or sparse.

(a) Gaussian

(b) Uniform

(c) EMA

Figure 4: Reward smoothing on sparse reward 1 at
t = 4. σ, δ, and α are smoothing hyperparameters.

Specifically, DreamSmooth applies temporal smoothing to the rewards upon collecting each new
episode. DreamSmooth can work with any smoothing function f that preserves the sum of rewards:

˜rt ← f (rt−L:t+L) =

L
(cid:88)

i=−L

fi · rclip(t+i,0,T )

s.t.

L
(cid:88)

i=−L

fi = 1,

(1)

where T and L denote the episode and smoothing horizons, respectively. For simplicity, we omit the
discount factor in Equation (1); the full equation can be found in Appendix, Equation (6). Episodes
with the smoothed rewards are stored in the replay buffer and used to train the reward model. The
agent learns only from the smoothed rewards, without ever seeing the original rewards. The smoothed
rewards ease reward prediction by allowing the model to predict rewards several timesteps earlier or
later, without incurring large losses. In this paper, we investigate three popular smoothing functions:
Gaussian, uniform, and exponential moving average (EMA) smoothing, as illustrated in Figure 4.

While the main motivation for smoothing is to make it easier to learn reward models, we note that
reward smoothing in some cases preserves optimality – an optimal policy under smoothed rewards ˜r

4

3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

8

16

24

Environment steps (×10⁶)

 
2

1

d
e
p
m
u
D
s
k
c
o
R

0

0

4

8

12

Environment steps (×10⁶)

 
1

d
r
a
w
e
R

0

0

Orginal
σ = 1
σ = 2

2

4
Timestep

6

8

1

d
r
a
w
e
R

0

0

Orginal
δ = 3
δ = 7

2

4
Timestep

6

8

1

d
r
a
w
e
R

0

0

Orginal
α = .3
α = .6

2

4
Timestep

6

8

is also optimal under the original rewards r. In particular, we provide a proof in Appendix A for the
optimality of EMA smoothing (and any smoothing function where ∀i > 0, fi = 0) by augmenting the
POMDP states with the history of past states. However, when future rewards are used for smoothing
(e.g. Gaussian smoothing), the smoothed rewards are conditioned on policy, and we can no longer
define an equivalent POMDP. In such cases, there is no theoretical guarantee. Even so, we empirically
show that reward models can adapt their predictions alongside the changing policy, and achieve
performance improvements.

The implementation of DreamSmooth is extremely simple, requiring only one additional line of
code to existing MBRL algorithms, as shown in Algorithm 1. The overhead of reward smoothing is
minimal, with time complexity O(T · L). More implementation details can be found in Appendix B.

Algorithm 1 COLLECT ROLLOUT (π: policy, D: replay buffer) in DREAMSMOOTH

t=1} ← ROLLOUT(π)

{(ot, at, rt)T
{rt}T
D ← D ∪ {(ot, at, rt)T

t=1 ← GAUSSIAN({rt}T

t=1}

t=1, σ) or EMA({rt}T

t=1, α)

▷ only one line needs to be added.

4 EXPERIMENTS

In this paper, we propose a simple reward smoothing method, DreamSmooth, which facilitates reward
prediction in model-based reinforcement learning (MBRL) and thus, improves the performance of
existing MBRL methods. Through our experiments, we aim to answer the following questions:
(1) Does reward smoothing improve reward prediction? (2) Does better reward prediction with reward
smoothing lead to better sample efficiency and asymptotic performance of MBRL in sparse-reward
tasks? (3) Does MBRL with reward smoothing also work in common dense-reward tasks?

4.1 TASKS

We evaluate DreamSmooth on four tasks with sparse subtask completion rewards and two common
RL benchmarks. Earthmoving uses two 64 × 64 images as an observation while all other tasks use a
single image. See Appendix C for environment details.

• RoboDesk: We use a modified version of RoboDesk (Kannan et al., 2021), where a sequence of ma-
nipulation tasks (flat block in bin, upright block off table, push green)
need to be completed in order (Figure 5a). We use the original dense rewards together with
a large sparse reward for each task completed.

• Hand: The Hand task (Plappert et al., 2018) requires a Shadow Hand to rotate a block in hand into
a specific orientation. We extend it to achieve a sequence of pre-defined goal orientations in order.
In addition to the original dense rewards, we provide a large sparse reward for each goal.

• Earthmoving: The Earthmoving task consists of a wheel loader, dump truck, and a pile of rocks
(Figure 5c). The agent controls the wheel loader to pick up rocks from the pile and dump them in
the dump truck. A large sparse reward is given for each rock picked up and for each rock dumped,
proportional to its mass. In addition, dense rewards are given for moving rocks towards the dump
truck. The environment is simulated using the AGX Dynamics physics engine (Algoryx, 2020).

(a) RoboDesk

(b) Hand

(c) Earthmoving

(d) Crafter

(e) DMC

(f) Atari

Figure 5: We evaluate DreamSmooth on four tasks with sparse subtask completion rewards (a-d). We
also test on two popular benchmarks, (e) DeepMind Control Suite and (f) Atari.

5

(a) RoboDesk

(b) Hand

(c) Earthmoving

(d) Crafter

Figure 6: We visualize the ground truth rewards, smoothed rewards with Gaussian smoothing, and
predicted rewards by DreamerV3 trained on the smoothed rewards over an evaluation episode. In
contrast to Figure 2, the reward models with reward smoothing capture most of sparse rewards.

• Crafter: Crafter (Hafner, 2022) is a minecraft-like 2D environment, where the agent tries to
collect, place, and craft items in order to survive. There are 22 achievements in the environment
(e.g. collecting water, mining diamonds) with a sparse reward 1 for obtaining each achievement for
the first time. A small reward is given (or lost) for each health point gained (or lost).

• DMC: We benchmark 7 DeepMind Control Suite continuous control tasks (Tassa et al., 2018).
• Atari: We benchmark 6 Atari tasks (Bellemare et al., 2013) at 100K steps.

4.2

IMPROVED REWARD PREDICTION WITH REWARD SMOOTHING

We first visualize the ground truth rewards,
smoothed rewards (Gaussian smoothing), and re-
ward prediction results of DreamerV3 trained with
DreamSmooth in Figure 6. We observe that re-
ward smoothing leads to a significant improve-
ment in reward prediction: DreamSmooth success-
fully predicts most of the (smoothed) sparse re-
wards and no longer omits vital signals for policy
learning or planning.

The improvement is especially notable in Crafter.
In Figure 7, we measure the accuracy of the re-
ward model, (i.e. predicting a reward larger than
half of the original or smoothed reward for Dream-
erV3 and DreamSmooth respectively) at the exact
timesteps for each subtask. The vanilla Dream-
erV3’s reward model (baseline) misses most of
the sparse rewards while DreamSmooth predicts
sparse rewards more accurately in 15/19 subtasks.

4.3 RESULTS

Figure 7: Reward prediction rates for 19 achieve-
ments in Crafter. The other 3 tasks have been
never achieved by both methods. With reward
smoothing, the prediction rates are better in
15/19 tasks.

We compare the vanilla DreamerV3 (Hafner et al., 2023) with DreamSmooth, whose backbone is
also DreamerV3. For DreamSmooth, we evaluate Gaussian, uniform, and EMA smoothing. The

6

Ground Truth

Smoothed

Predicted

x-axis: Timestep

y-axis: Reward

64

00

0

120

60

0

−20

0

120

800

0

−200

0

360

0.6

0.0

−0.2

0

200

Baseline (no smoothing)

Gaussian

1.0

0.5

e
t
a
R
n
o
i
t
c
i
d
e
r
P

0.0

w

k

al

d
e
ain
alth g
e
H

nt

e
n

ace

d
e
n
o
o

bie
alth loss
g
xe
xe
ord
n
ord
ollect drin
plin
at skeleto
at co
ollect co
Place pla
d picka
e picka
Place sto
ollect sto
Place ta
m
d sw
e sw
Place furn
ollect w
ollect sa
at zo
E
ke sto
n
efe
ke w
ke sto
ke w
D
a
M

efe
C
D

n
o
o

o
o

C

C

C

C

e
H

p

ble
ke u
Wa

a
M
a
M

a
M

 
(a) RoboDesk

(b) Hand

(c) Earthmoving

(d) Crafter

(e) DMC

(f) Atari

Figure 8: Comparison of learning curves of DreamSmooth (Gaussian, Uniform, EMA) and Dream-
erV3. The shaded regions in (a-d) show the maximum and minimum over 3 seeds. For DMC (e) and
Atari (f), we aggregate results over 7 and 6 tasks respectively, and display the standard deviation.

hyperparameters for DreamerV3 and smoothing functions can be found in Appendix B. As shown in
Figure 8, DreamSmooth-Gaussian and DreamSmooth-Uniform significantly improve the performance
as well as the sample efficiency of DreamerV3 on the Robodesk, Hand, and Earthmoving tasks. The
only change between DreamerV3 and ours is the improved reward prediction, as shown in Section 4.2.
This result suggests that reward prediction is one of major bottlenecks of the MBRL performance.

While all smoothing methods lead to improvements over DreamerV3, Gaussian smoothing generally
performs the best, except on Crafter, with uniform smoothing showing comparable performance. The
better performance of Gaussian and uniform smoothing could be because it allows predicting rewards
both earlier and later, whereas EMA smoothing only allows predicting rewards later.

Despite the improved reward prediction accuracy, DreamSmooth-Gaussian and DreamSmooth-
Uniform perform worse than the baseline in Crafter. This can be because more predicted task
rewards encourage more exploitation and less exploration. Further investigation on this trade-off is a
promising direction for future work.

Moreover, we observe that on the DMC and Atari benchmarks, where reward prediction is not particu-
larly challenging, our technique shows comparable performance with the unmodified algorithms (see
Appendix, Figure 16 for full results). This suggests that reward smoothing can be applied generally,
and does not hinder performance on most environments.

7

DreamSmooth-Gaussian

DreamSmooth-Uniform

DreamSmooth-EMA

DreamerV3

3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

18
12
6
Environment steps (×10⁶)

24

 
3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

15
10
5
Environment steps (×10⁶)

20

 
d
e
p
m
u
D
s
k
c
o
R
f
o

r
e
b
m
u
N

2.0

1.5

1.0

0.5

0.0

0

3

6
Environment steps (×10⁶)

9

12

 
 
 
16

12

8

4

e
r
o
c
S

0
0.0

1.5
4.5
3.0
Environment steps (×10⁶)

6.0

1000

750

500

250

e
r
o
c
S

0
0.00

0.25
0.75
0.50
Environment steps (×10⁶)

1.00

e
r
o
c
S

d
e
z
i
l

a
m
r
o
N
n
a
m
u
H

0.6

0.4

0.2

0.0

0.0

0.1
0.3
0.2
Environment steps (×10⁶)

0.4

 
 
In Figure 9, DreamSmooth also improves
the performance of TD-MPC (Hansen et al.,
2022). In the Hand task, vanilla TD-MPC
is unable to consistently solve the first task,
even with proprioceptive state observations.
However, TD-MPC with DreamSmooth
learns to complete the tasks with not only
state observations but also pixel observa-
tions. This suggests that DreamSmooth can
be useful in a broad range of MBRL algo-
rithms that use a reward model. We only
demonstrate the Hand task since TD-MPC
fails on other sparse-reward tasks.

4.4 ABLATION STUDIES

(a) Hand (Pixel)

(b) Hand (State)

Figure 9: Learning curves for TD-MPC and TD-MPC
with DreamSmooth on the Hand task. The shaded re-
gions show the minimum and maximum over 3 seeds.

Data Imbalance. One possible cause of poor
reward predictions is data imbalance – because
sparse rewards are infrequent, sequences con-
taining sparse rewards are rarely sampled from
the replay buffer. The reward model therefore
trains on fewer examples of sparse rewards, po-
tentially leading to poor predictions. To test
this hypothesis, we conducted experiments with
oversampling: with probability p = 0.5, we
sample a sequence in which the agent receives
a sparse reward; otherwise, we sample uni-
formly from all sequences in the buffer. As
shown in Figure 10, oversampling performs bet-
ter than the baseline, but learns slower than
DreamSmooth. This suggests that while data
imbalance largely contributes to the difficulty
of reward prediction, it is not the only factor
hindering performance. Furthermore, this over-
sampling method requires domain knowledge about which reward signals to be oversampled while
DreamSmooth is agnostic to the scale and frequency of sparse rewards.

Figure 10: Using oversampling of sequences
with sparse rewards (p = 0.5) performs better
than DreamerV3 on RoboDesk, but worse than
DreamSmooth with Gaussian smoothing. The lines
show median task performance over 3 seeds, while
shaded regions show maximum and minimum.

Reward Model Size. Another hypothe-
sis for poor reward predictions is that the
reward model does not have enough ca-
pacity to capture sparse rewards. To test
this hypothesis, we increase the size of the
reward model from 4 layers of 768 units
to 5 layeres of 1024 units and 6 layers of
1280 units, while keeping the rest of the
world model the same. We observe in Fig-
ure 11 that without smoothing, changing
the reward model size has negligible impact
on performance, and DreamSmooth out-
performs all the reward model sizes tested.
This indicates that the reward prediction
problem is not simply caused by insuffi-
cient model capacity.

(a) RoboDesk

(b) Hand

Figure 11: Simply increasing the reward model size has
negligible impact on performance. DreamerV3-768 and
DreamSmooth use 4 layers of 768 units; DreamerV3-
1024 uses 5 layers of 1024 units; and DreamerV3-1280
uses 6 layers of 1280 units.

In Figure 12, we analyze the impact of the smoothing parameters σ and
Smoothing Parameter.
α for Gaussian and EMA, respectively, on RoboDesk and Hand. We observe that DreamSmooth is
insensitive to the smoothing parameters, performing well across a wide range of values.

8

DreamSmooth-Gaussian (TDMPC)

TD-MPC

l

d
e
t
e
p
m
o
C
s
k
s
a
T

1.2

0.8

0.4

0.0

0.0
0.5
1.0
Environment steps (×10⁶)

 
l

d
e
t
e
p
m
o
C
s
k
s
a
T

3

2

1

0

0
4
2
Environment steps (×10⁶)

 
3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

DreamSmooth
Oversampling
DreamerV3

6
18
12
Environment steps (×10⁶)

24

 
DreamSmooth
DreamerV3-1280

DreamerV3-1024
DreamerV3-768

l

d
e
t
e
p
m
o
C
s
k
s
a
T

3

2

1

0

6

0
24
12
Environment steps (×10⁶)

18

 
l

d
e
t
e
p
m
o
C
s
k
s
a
T

3

2

1

0

0
20
10
Environment steps (×10⁶)

 
(a) Gaussian Smoothing on RoboDesk

(b) Gaussian Smoothing on Hand

(c) Uniform Smoothing on RoboDesk

(d) Uniform Smoothing on Hand

(e) EMA Smoothing on RoboDesk

(f) EMA Smoothing on Hand

Figure 12: Parameter sweep over smoothing parameters σ, δ, and α. The lines show median task
performance over 3 seeds, while shaded regions show maximum and minimum.

5 CONCLUSION

In this paper, we identify the reward prediction problem in MBRL and provide a simple yet effective
solution, reward smoothing. Our approach, DreamSmooth, demonstrates superior performance in
sparse reward tasks where reward prediction is not trivial mainly due to the partial observability
or stochasticity of the environments. Moreover, DreamSmooth shows comparable results on the
commonly used benchmarks, DMC and Atari, showing its task-agnostic nature. Although we show
that our simple reward smoothing approach mitigates the difficulty in reward prediction, the improved
reward prediction does not always improve the task performance, e.g., in Crafter. This can be because
more predicted task rewards encourage more exploitation and less exploration. Further investigation
on this trade-off is a promising direction for future work.

9

DreamSmooth-Gaussian

(cid:15)(cid:3)(cid:8)(cid:3)(cid:4)

(cid:15)(cid:3)(cid:8)(cid:3)(cid:6)

(cid:15)(cid:3)(cid:8)(cid:3)(cid:7)

(cid:9)(cid:14)(cid:12)(cid:11)(cid:13)(cid:12)(cid:14)(cid:10)(cid:5)

3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

6

18
12
Environment steps (×10⁶)

24

 
3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

5

15
10
Environment steps (×10⁶)

20

 
DreamSmooth-Uniform

(cid:15)(cid:3)(cid:8)(cid:3)(cid:6)

(cid:15)(cid:3)(cid:8)(cid:3)(cid:7)

(cid:15)(cid:3)(cid:8)(cid:3)(cid:4)(cid:5)

(cid:9)(cid:14)(cid:12)(cid:11)(cid:13)(cid:12)(cid:14)(cid:10)(cid:5)

3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

6

18
12
Environment steps (×10⁶)

24

 
3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

5

15
10
Environment steps (×10⁶)

20

 
DreamSmooth-EMA

(cid:17)(cid:3)(cid:10)(cid:3)(cid:5)(cid:4)(cid:6)

(cid:17)(cid:3)(cid:10)(cid:3)(cid:5)(cid:4)(cid:7)(cid:8)

(cid:17)(cid:3)(cid:10)(cid:3)(cid:5)(cid:4)(cid:9)

(cid:11)(cid:16)(cid:14)(cid:13)(cid:15)(cid:14)(cid:16)(cid:12)(cid:6)

3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

6

18
12
Environment steps (×10⁶)

24

 
3

2

1

l

d
e
t
e
p
m
o
C
s
k
s
a
T

0

0

5

15
10
Environment steps (×10⁶)

20

 
ACKNOWLEDGMENTS

This work was supported in part by the BAIR Industrial Consortium, an ONR DURIP grant, Komatsu,
and InnoHK Centre for Logistics Robotics. We would like to thank all members of the Berkeley
Robot Learning lab for their insightful feedback.

REFERENCES

Algoryx. AGX dynamics, 2020. URL https://www.algoryx.se/agx-dynamics/.

Arthur Argenson and Gabriel Dulac-Arnold. Model-based offline planning. In International Confer-
ence on Learning Representations, 2021. URL https://openreview.net/forum?id=
OMNB1G5xzd4.

Mohammad Babaeizadeh, Mohammad Taghi Saffar, Danijar Hafner, Harini Kannan, Chelsea Finn,
Sergey Levine, and Dumitru Erhan. Models, pixels, and rewards: Evaluating design trade-offs in
visual model-based reinforcement learning. arXiv preprint arXiv:2012.04603, 2020.

M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An
evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253–279,
jun 2013.

Fei Deng, Ingook Jang, and Sungjin Ahn. Dreamerpro: Reconstruction-free model-based reinforce-
ment learning with prototypical representations. In International Conference on Machine Learning,
pp. 4956–4975. PMLR, 2022.

Fei Deng, Junyeong Park, and Sungjin Ahn. Facing off world model backbones: Rnns, transformers,

and s4. arXiv preprint arXiv:2307.02064, 2023.

Christopher D Fiorillo, William T Newsome, and Wolfram Schultz. The temporal precision of reward

prediction in dopamine neurons. Nature neuroscience, 11(8):966–973, 2008.

David Ha and J¨urgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.

Danijar Hafner. Benchmarking the spectrum of agent capabilities. In International Conference on

Learning Representations, 2022.

Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. In International Conference on Learning Representations, 2019.

Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete

world models. In International Conference on Learning Representations, 2021.

Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains

through world models. arXiv preprint arXiv:2301.04104, 2023.

Nicklas Hansen, Xiaolong Wang, and Hao Su. Temporal difference learning for model predictive

control. In International Conference on Machine Learning, 2022.

Harini Kannan, Danijar Hafner, Chelsea Finn, and Dumitru Erhan. Robodesk: A multi-task rein-
forcement learning benchmark. https://github.com/google-research/robodesk,
2021.

Miriam C Klein-Fl¨ugge, Laurence T Hunt, Dominik R Bach, Raymond J Dolan, and Timothy EJ
Behrens. Dissociable reward and timing signals in human midbrain and ventral striatum. Neuron,
72(4):654–664, 2011.

Russell Mendonca, Shikhar Bahl, and Deepak Pathak. Structured world models from human videos.

In Robotics: Science and Systems, 2023.

Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations:
Theory and application to reward shaping. In International Conference on Machine Learning,
volume 99, pp. 278–287, 1999.

10

Masashi Okada and Tadahiro Taniguchi. Dreaming: Model-based reinforcement learning by latent
imagination without reconstruction. In IEEE International Conference on Robotics and Automation,
pp. 4209–4215, 2021. doi: 10.1109/ICRA48506.2021.9560734.

Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell,
Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforcement learn-
ing: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464,
2018.

Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon
Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari,
go, chess and shogi by planning with a learned model. Nature, 588(7839):604–609, 2020.

Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee, and Pieter

Abbeel. Masked world models for visual control. In Conference on Robot Learning, 2022.

Younggyo Seo, Junsu Kim, Stephen James, Kimin Lee, Jinwoo Shin, and Pieter Abbeel. Multi-view
masked world models for visual robotic manipulation. In International Conference on Machine
Learning, 2023.

Lucy Xiaoyang Shi, Joseph J. Lim, and Youngwoon Lee. Skill-based model-based reinforcement

learning. In Conference on Robot Learning, 2022.

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016.

David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. nature, 550(7676):354–359, 2017.

David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement
learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):
1140–1144, 2018.

Gautam Singh, Skand Peri, Junghyun Kim, Hyunseok Kim, and Sungjin Ahn. Structured world
belief for reinforcement learning in pomdp. In International Conference on Machine Learning, pp.
9744–9755. PMLR, 2021.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT Press, 2018.

Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,
Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Martin A. Riedmiller.
Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.

11

A PROOFS

Let M = (S, A, P, R, γ) be the given MDP. Without loss of generality, we assume the augmented
form of the MDP M, where a state st includes the entire history of states, i.e., st = (s1, . . . , st),
and thus, reward functions R, ˜R have access to previous states, i.e., ˜R(st) = ˜R(s1, . . . , st).
Theorem A.1. An optimal policy ˜π∗ of the MDP with reward smoothing only with past rewards, e.g.,
EMA smoothing, ˜M = (S, A, P, ˜R, γ) is also optimal under the original MDP M, where

˜R(st) =

0
(cid:88)

i=−L

fi · γiR(st+i) and

0
(cid:88)

i=−L

fi = 1.

(2)

Proof. We will use the theorem of reward shaping that guarantees an optimal policy introduced in Ng
et al. (1999): if a modified reward function can be represented in the form of R(st)+γΦ(st+1)−Φ(st)
with any potential function Φ(st), the new reward function yields the same optimal policy with the
original reward function R.

Let the potential function for the EMA reward smoothing

Φ(st) = −

−1
(cid:88)

i=−L

γiR(st+i) +

0
(cid:88)

i=−L

γiR(st+i) ·

0
(cid:88)

fj.

j=i+1

(3)

Then, our reward shaping term in ˜R can be represented as the difference in the potential function
γΦ(st+1) − Φ(st) as follows:

γΦ(st+1) − Φ(st) = −R(st) +

0
(cid:88)

i=−L

fi · γiR(st+i).

R(st) + γΦ(st+1) − Φ(st) =

0
(cid:88)

i=−L

fi · γiR(st+i) = ˜R.

(4)

(5)

Hence, following Ng et al. (1999), reward shaping with our EMA smoothing guarantees the optimal
policy in the original MDP M.

However, Theorem A.1 does not apply to smoothing functions that require access to future rewards,
e.g., Gaussian smoothing. As in Gaussian smoothing, a smoothed reward function may require future
rewards, which are conditioned on the current policy; so is the reward model. In such cases, there is
no theoretical guarantee; but in our experiments, we empirically show that reward models can adapt
their predictions along the changes in policies and thus, improve MBRL.

Instead, we intuitively explain that an optimal policy under any reward smoothing (even though the
reward function is post hoc and cannot be defined for MDPs) is also optimal under the original reward
function.
Theorem A.2. An optimal policy ˜π∗ with the smoothed reward function ˜R is also optimal under the
original reward function R, where

˜R(st) =

L
(cid:88)

i=−L

γclip(i,−t,T −t) · fi · R(sclip(t+i,0,T )) and

L
(cid:88)

i=−L

fi = 1.

(6)

12

Proof. First, we show that the discounted sum of original rewards (cid:80)T
smoothed rewards (cid:80)T

t=0 γt ˜R(st) are the same for any trajectories (s0, s1, . . . , sT ):
T
(cid:88)

L
(cid:88)

γclip(i,−t,T −t) · fi · R(sclip(t+i,0,T ))

γt ˜R(st) =

γt

from Equation (6)

t=0 γtR(st) and the one of

T
(cid:88)

t=0

i=−L

γtR(st) ·

L
(cid:88)

fi

i=−L

γtR(st).

t=0

T
(cid:88)

t=0

T
(cid:88)

t=0

=

=

(7)

(8)

from

L
(cid:88)

i=−L

fi = 1

(9)

Let an optimal policy under the smoothed rewards ˜R be ˜π∗. Assume that ˜π∗ is not optimal under the
original reward R. Then,

∃π∗, s0

such that E(s0,...,sT )∼π∗

(cid:104) T
(cid:88)

t=0

(cid:105)
γtR(st)

> E(s0,...,sT )∼˜π∗

(cid:104) T
(cid:88)

t=0

γt ˜R(st)

(cid:105)
.

(10)

However,

E(s0,...,sT )∼π∗

(cid:104) T
(cid:88)

t=0

(cid:105)

γtR(st)

= E(s0,...,sT )∼π∗

> E(s0,...,sT )∼˜π∗

(cid:104) T
(cid:88)

t=0
(cid:104) T
(cid:88)

t=0

(cid:105)

γt ˜R(st)

γt ˜R(st)

(cid:105)
,

by Equation (9)

(11)

by Equation (10)

(12)

which contradicts that ˜π∗ is optimal under ˜R. Therefore, the optimal policy ˜π∗ under ˜R guarantees
its optimality under R.

B IMPLEMENTATION DETAILS

Models are trained on NVIDIA A5000, V100, RTX Titan, RTX 2080, and RTX 6000 GPUs. Each
experiment takes about 72 hours for RoboDesk, 100 hours for Hand, 150 hours for Earthmoving, 96
hours for Crafter, and 6 hours for Atari and DMC tasks.

B.1 DREAMSMOOTH SMOOTHING FUNCTIONS

Gaussian smoothing follows the Gaussian distribution with σ:

fi = ke

−i2
2σ2 ,

(13)

where k = 1/((cid:80)L

i=−L e
We implement this using

−i2
2σ2 ) is a normalization constant.

scipy.ndimage.gaussian_filter1d(rewards, sigma, mode=""nearest"")

Uniform smoothing distributes rewards equally across δ consecutive timesteps.
1
δ

δ − 1
2

δ − 1
2

fi =

∀i ∈

−

(cid:105)

(cid:104)

.

,

We implement this using

scipy.ndimage.convolve(rewards, filter, mode=""nearest"")

EMA smoothing uses the following smoothing function:

fi = α(1 − α)i ∀i ≤ 0,

which we implement by performing the following at each timestep:

reward[t] = alpha * reward[t - 1] + (1 - alpha) * reward[t]

13

(14)

(15)

B.2 MODEL-BASED REINFORCEMENT LEARNING BACKBONES

Hyperparameters for DreamerV3 experiments are shown in Table 1 and TD-MPC in Table 2.

Table 1: DreamerV3 hyperparameters. Episode length is measured in environment steps, which is the
number of agent steps multiplied by action repeat. Model sizes are as listed in Hafner et al. (2023),
which we also refer to for all other hyperparameters.

Environment Action Repeat Episode Length Train Ratio Model Size

Earthmoving
RoboDesk
Hand
Crafter
DMC
Atari

4
8
1
1
2
4

2000
2400
300
Variable
1000
Variable

64
64
64
64
512
1024

L
L
L
XL
S
S

σ

3
3
2
1
3
3

α

0.33
0.3
0.3
0.45
0.33
0.3

δ

9
9
9
9
9
9

Table 2: TD-MPC hyperparameters. Unless specified, we use the default hyperparameters in Hansen
et al. (2022).

Environment Latent Dimension CNN channels Planning Iterations

Hand-Pixel
Hand-Proprio

128
128

64
–

6
12

σ

3
3

C ENVIRONMENT DETAILS

C.1 ROBODESK ENVIRONMENT

We use a modified version of RoboDesk (Kannan et al., 2021), where a sequence of manipulation tasks
(flat block in bin, upright block off table, push green) need to be completed
in order. Figure 13 shows images of an agent successfully completing each of these tasks.

In the original environment, dense rewards are based on Euclidean distances of objects to their targets,
with additional terms to encourage the arm to reach the object. They typically range from 0 to 10
per timestep. We use these dense rewards together with a large sparse reward of 300 for each task
completed.

(a) Put the flat block into the bin

(b) Push the upright block off the
table

(c) Press the green button

Figure 13: Subtasks for RoboDesk.

C.2 HAND ENVIRONMENT

We modified the Shadow Hand environment (Plappert et al., 2018), so that the agent is required to
achieve a sequence of pre-defined goal orientations in order. The first 3 goals are shown in Figure 14,

14

while the subsequent goals are a repeat of the first 3. The goal orientations are chosen so that the
agent only has to rotate the cube along the z-axis, and we only require the agent to match the cube’s
rotation to the goal, not its position.

In the original environment, dense rewards are computed using r = −(10x + ∆θ), where x is the
Euclidean distance to some fixed position, and ∆θ is the angular difference to the target orientation.
In addition to these dense rewards, we provide a large sparse reward of 300 for each goal successfully
achieved by the agent.

(a) Goal 1

(b) Goal 2

(c) Goal 3

Figure 14: Subtasks for Hand.

C.3 AGX EARTHMOVING ENVIRONMENT

The Earthmoving environment consists of a wheel loader, dump
truck, a pile of dirt, with some rocks on top of the pile. The en-
vironment is simulated using the realistic AGX Dynamics physics
engine (Algoryx, 2020). The agent controls the wheel loader to pick
up rocks and dump them in the dump truck.

The starting positions of the dirt pile, wheel loader, and dump truck
are all randomized, as are the initial orientations of the dirt pile and
wheel loader.

The agent’s observations consist of 3 components: a wide-angle
egocentric RGB camera mounted on the cabin to allow navigation,
an RGB camera mounted on the bucket for observing interactions
with rocks, and proprioceptive observations (positions, velocity,
speed, force of actuators etc.). We use 64 × 64 × 3 images for all
cameras, while the proprioceptive observation has 21 dimensions.

Figure 15: The agent uses one
camera mounted on the cabin
(left) for navigation, and one
mounted on the bucket (right)
for observing interactions with
rocks and terrain.

The action space is 4-dimensional: 2 dimensions for driving and steering the loader, and 2 dimensions
for moving and tilting the bucket.

The reward consists of a large sparse reward for rocks picked up and dumped, and dense rewards
for moving rocks towards the dumptruck. The total reward rt at timestep t is computed using
Equation (16).
rt = λdump(mt
(cid:124)

load(max (2, dt) − max (2, dt−1))
(cid:125)

load − mt−1
load )
(cid:125)

dump) + λload(mt

dump − mt−1

+ λmovemt

(cid:124)

(cid:123)(cid:122)
dense reward

(cid:123)(cid:122)
sparse reward

Where mdump, mload are rock masses in the dumptruck and the bucket respectively, d is the distance
between the shovel and a point above the dumptruck, and λ are constants.

(16)

15

D DMC AND ATARI BENCHMARKING RESULTS

Figure 16: Full learning curves for the DMC and Atari benchmarks.

16

DreamSmooth-Gaussian

DreamSmooth-Uniform

DreamSmooth-EMA

DreamerV3

Hopper Hop

1000

e
r
o
c
S

500

0
0.0
1.0
0.5
Environment steps (×10⁶)

Reacher Hard

1000

e
r
o
c
S

500

0
0.0
1.0
0.5
Environment steps (×10⁶)

Cartpole Swingup Sparse

1000

e
r
o
c
S

500

0
0.0
1.0
0.5
Environment steps (×10⁶)

Walker Run

1000

e
r
o
c
S

500

0
0.0
1.0
0.5
Environment steps (×10⁶)

Finger Turn Hard

1000

e
r
o
c
S

500

0
0.0
1.0
0.5
Environment steps (×10⁶)

Quadruped Run

1000

e
r
o
c
S

500

0
0.0
1.0
0.5
Environment steps (×10⁶)

Cheetah Run

1000

e
r
o
c
S

500

0
0.0
1.0
0.5
Environment steps (×10⁶)

Pong

3.00

−5.33

−13.67

e
r
o
c
S

−22.00

0.0
0.4
0.2
Environment steps (×10⁶)

Breakout

16.00

10.67

5.33

e
r
o
c
S

0.00

0.0
0.4
0.2
Environment steps (×10⁶)

Freeway

29

19

9

e
r
o
c
S

−1

0.0

0.2
Environment steps (×10⁶)

0.4

Assault

807.0

621.3

435.7

e
r
o
c
S

250.0

0.0
0.4
0.2
Environment steps (×10⁶)

Seaquest

545

378

211

e
r
o
c
S

44

0.0

0.2
Environment steps (×10⁶)

0.4

Hero

13200

e
r
o
c
S

8860

4520

180

0.0
0.4
0.2
Environment steps (×10⁶)

","3 2 0 2 v o N 2 ] G L . s c [ 1 v 0 5 4 1 0 . 1 1 3 2 : v i X r a DREAMSMOOTH : IMPROVING MODEL-BASED REIN- FORCEMENT LEARNING VIA REWARD SMOOTHING Vint Lee , Pieter Abbeel , Youngwoon Lee University of California , Berkeley ABSTRACT Model-based reinforcement learning ( MBRL ) has gained much attention for its ability to learn complex behaviors in a sample-efficient way : planning actions by generating imaginary trajectories with predicted rewards . Despite its success , we found that surprisingly , reward prediction is often a bottleneck of MBRL , especially for sparse rewards that are challenging ( or even ambiguous ) to predict . Motivated by the intuition that humans can learn from rough reward estimates , we propose a simple yet effective reward smoothing approach , DreamSmooth , which learns to predict a temporally-smoothed reward , instead of the exact reward at the given timestep . We empirically show that DreamSmooth achieves state-of-the-art performance on long-horizon sparse-reward tasks both in sample efficiency and final performance without losing performance on common benchmarks , such as Deepmind Control Suite and Atari benchmarks . 1 INTRODUCTION Humans often plan actions with a rough estimate of future rewards , instead of the exact reward at the exact moment ( Fiorillo et al. , 2008 ; Klein- Fl¨ugge et al. , 2011 ) . A rough reward estimate is mostly sufficient to learn a task , and predicting the exact reward is often challenging since it can be ambiguous , delayed , or not observable . Consider for instance the manipulation task il- lustrated in Figure 1 ( middle ) of pushing a block on a table into a bin , where a sparse reward is given only on the timestep when the block first touches the bin . Using the same image observa- tions as the agent , it is challenging even for hu- mans to predict the correct sequence of rewards . Crucially , this issue is present in many environ- ments , where states with no reward are almost indistinguishable from those with rewards . Figure 1 : Predicting the exact sequence of rewards is extremely difficult . These examples show the sequences of image observations seen by the agent just before and after it receives a sparse reward . There is little to visually distinguish timesteps with a large reward from those without , which creates a significant challenge for reward prediction . An accurate reward model is vital to model- based reinforcement learning ( MBRL ) – reward estimates that are too high will cause an agent to choose actions that perform poorly in reality , and estimates that are too low will lead an agent to ignore high rewards . Despite its difficulty and importance , the reward prediction problem in MBRL has been largely overlooked . We find that even for the state-of-the-art MBRL algorithm , DreamerV3 ( Hafner et al. , 2023 ) , reward prediction is not only challenging , but is also a performance bottleneck for many tasks . For instance , DreamerV3 fails to predict any reward for most objectives in the Crafter environment ( Hafner , 2022 ) with similar failure modes observed on variants of the RoboDesk ( Kannan et al. , 2021 ) and Shadow Hand ( Plappert et al. , 2018 ) tasks with sparse rewards . 1 Inspired by the human intuition that only a rough estimate of rewards is sufficient , we propose a simple yet effective solution , DreamSmooth , which learns to predict a temporally-smoothed reward rather than the exact reward at each timestep . This makes reward prediction much easier – instead of having to predict rewards exactly , now the model only needs to produce an estimate of when sparse rewards are obtained , which is sufficient for policy learning . Our experiments demonstrate that while extremely simple , this technique significantly improves performance of different MBRL algorithms on many sparse-reward environments . Specifically , we find that for DreamerV3 ( Hafner et al. , 2023 ) and TD-MPC ( Hansen et al. , 2022 ) , our technique is especially beneficial in environments with the following characteristics : sparse rewards , partial observability , and stochastic rewards . Finally , we show that even on benchmarks where reward prediction is not a significant issue , DreamSmooth does not degrade performance , which indicates that our technique can be universally applied . 2 RELATED WORK Model-based reinforcement learning ( MBRL ) leverages a dynamics model ( i.e . world model ) of an environment and a reward model of a desired task to plan a sequence of actions that maximize the total reward . The dynamics model predicts the future state of the environment after taking a specific action and the reward model predicts the reward corresponding to the state-action transition . With the dynamics and reward models , an agent can simulate a large number of candidate behaviors in imagination instead of in the physical environment , allowing MBRL to tackle many challenging tasks ( Silver et al. , 2016 ; 2017 ; 2018 ) . Instead of relying on the given dynamics and reward models , recent advances in MBRL have enabled learning a world model of high-dimensional observations and complex dynamics ( Ha & Schmidhuber , 2018 ; Schrittwieser et al. , 2020 ; Hafner et al. , 2019 ; 2021 ; 2023 ; Hansen et al. , 2022 ) , as well as a temporally-extended world model ( Shi et al. , 2022 ) . Specifically , DreamerV3 ( Hafner et al. , 2023 ) has achieved the state-of-the-art performance across diverse domains of problems , e.g. , both with pixel and state observations as well as both with discrete and continuous actions . For realistic imagination , MBRL requires an accurate world model . There have been significant efforts in learning better world models by leveraging human videos ( Mendonca et al. , 2023 ) , by adopting a more performant architecture ( Deng et al. , 2023 ) , and via representation learning , such as prototype-based ( Deng et al. , 2022 ) and object-centric ( Singh et al. , 2021 ) state representations , contrastive learning ( Okada & Taniguchi , 2021 ) , and masked auto-encoding ( Seo et al. , 2022 ; 2023 ) . However , compared to the efforts on learning a better world model , learning an accurate reward model has been largely overlooked . Babaeizadeh et al . ( 2020 ) investigates the effects of various world model designs and shows that reward prediction is strongly correlated to task performance when trained on an offline dataset , while limited to dense-reward environments . In this paper , we point out that accurate reward prediction is crucial for MBRL , especially in sparse-reward tasks and partially observable environments , and propose a simple method to improve reward prediction in MBRL . 3 APPROACH The main goal of this paper is to understand how challenging reward prediction is in model-based reinforcement learning ( MBRL ) and propose a simple yet effective solution , reward smoothing , which makes reward prediction easier to learn . In this section , we first provide a background about MBRL in Section 3.1 , then present experiments demonstrating the challenge of predicting sparse reward signals in Section 3.2 , and finally explain our approach , DreamSmooth , in Section 3.4 . 3.1 BACKGROUND We formulate a problem as a partially observable Markov decision process ( POMDP ) , which is defined as tuple ( O , A , P , R , γ ) . O is an observation space , A is an action space , P ( ot+1|o≤t , a≤t ) with timestep t is a transition dynamics , R is a reward function that maps previous observations and actions to a reward rt = R ( o≤t , a≤t ) , and γ ∈ [ 0 , 1 ) is a discount factor ( Sutton & Barto , 2018 ) . RL aims to find a policy π ( at | o≤t , a < t ) that maximizes the expected sum of rewards Eπ [ ( cid:80 ) T t=1 γt−1rt ] . 2 This paper focuses on MBRL algorithms that learn a world model Pθ ( zt+1|zt , at ) and reward model Rθ ( rt|zt ) from agent experience , where zt is a learned latent state at timestep t. The learned world model and reward model can then generate imaginary rollouts { zτ , aτ , rτ } t+H−1 of the horizon H starting from any zt , which can be used for planning ( Argenson & Dulac-Arnold , 2021 ; Hansen et al. , 2022 ) or policy optimization ( Ha & Schmidhuber , 2018 ; Hafner et al. , 2019 ) . Specifically , we use the state-of-the-art algorithms , DreamerV3 ( Hafner et al. , 2023 ) and TD-MPC ( Hansen et al. , 2022 ) . τ =t DreamerV3 ( Hafner et al. , 2023 ) uses the predicted rewards for computing new value targets to train the critic . For learning a good policy , the reward model plays a vital role since the critic , from which the actor learns a policy , receives its training signal exclusively through the reward model . Note that the data collected from the environment is only used for training a world model and reward model . On the other hand , TD-MPC ( Hansen et al. , 2022 ) learns a state-action value function Q ( zt , at ) directly from agent experience , not from predicted rewards . However , the reward model is still important for obtaining a good policy in TD-MPC because the algorithm uses both the reward model and value function to obtain the policy through online planning . 3.2 REWARD PREDICTION IS DIFFICULT Reward prediction is surprisingly challenging in many environments . Figure 1 shows sequences of frames right before and after sparse rewards are received in diverse environments . Even for humans , it is difficult to determine the exact timestep when the reward is received in all three environments . We hypothesize that the mean squared error loss E ( z , r ) ∼D [ ( Rθ ( z ) − r ) 2 ] , typically used for reward model training , deteriorates reward prediction accuracy when there exist sparse rewards . This is because predicting a sparse reward a single step earlier or later results in a higher loss than simply predicting 0 reward at every step . Thus , instead of trying to predict sparse rewards at the exact timesteps , a reward model minimizes the loss by entirely omitting sparse rewards from its predictions . To verify this hypothesis , we plot the ground-truth and DreamerV3 ’ s predicted rewards in Figure 2 . The reward models struggle at predicting exact rewards and simply ignore sparse rewards unless they are straightforward to predict on the four tasks described in Section 4.1 . This hypothesis also holds in a deterministic and fully-observable environment , Crafter , which has 24 sources of sparse rewards . The reward model fails to predict most of these reward sources ( Figure 2d ) . The difficulty of reward prediction can be further exacerbated by partial observability , ambiguous rewards , or stochastic dynamics of environments . As an example in the first ( third ) row in Figure 1 , ( a ) RoboDesk ( b ) Hand ( c ) Earthmoving ( d ) Crafter Figure 2 : Ground truth rewards and DreamerV3 ’ s predicted rewards over an evaluation episode . The reward model misses many sparse rewards , which is highlighted in yellow . 3 Ground Truth Predicted x-axis : Timestep y-axis : Reward 330 0 −110 0 120 330 0 −110 0 120 2400 0 −800 0 360 1.2 0.0 −0.4 0 200 the sparse rewards are given when the block ( the rocks in the third example ) first contacts the bin ( the dumptruck ) . The exact moment of contact is not directly observable from the camera viewpoint , and this makes reward prediction ambiguous . Moreover , stochastic environment dynamics , e.g. , contact between multiple rocks , can make predicting a future state and reward challenging . 3.3 REWARD PREDICTION IS A BOTTLENECK OF MBRL The preceding section shows that reward prediction is challenging in many environments . More importantly , this poor reward prediction can be a bottleneck of policy learning , as shown in Figure 3 . In RoboDesk , where the reward model does not reliably detect the completion of the second task ( Figure 2a ) , the policy gets stuck at solving the first task and fails on subsequent tasks . In Earthmoving , where the reward model can not capture rewards for successful dumping ( Figure 2c ) , the policy frequently drops the rocks outside the dumptruck . These consistent failure modes in reward prediction and policy learning in DreamerV3 suggest that poor reward prediction can be a bottleneck of MBRL . ( a ) RoboDesk ( b ) Earthmoving Figure 3 : The reward model ’ s inability to predict sparse rewards for completing tasks leads to poor task performance . ( a ) In RoboDesk , the agent gets stuck after learning the first task , and is unable to learn to perform the subsequent tasks . ( b ) In Earthmoving , the policy often fails to dump the rocks accurately into the dumptruck . The learning curves are averaged over 3 seeds . 3.4 DREAMSMOOTH : IMPROVING MBRL VIA REWARD SMOOTHING To address the reward prediction problem , we propose a simple yet effective solution , DreamSmooth , which relaxes the require- ment for the model to predict sparse rewards at the exact timesteps by performing tempo- ral smoothing . Allowing the reward model to predict rewards that are off from the ground truth by a few timesteps makes learning eas- ier , especially when rewards are ambiguous or sparse . ( a ) Gaussian ( b ) Uniform ( c ) EMA Figure 4 : Reward smoothing on sparse reward 1 at t = 4. σ , δ , and α are smoothing hyperparameters . Specifically , DreamSmooth applies temporal smoothing to the rewards upon collecting each new episode . DreamSmooth can work with any smoothing function f that preserves the sum of rewards : ˜rt ← f ( rt−L : t+L ) = L ( cid:88 ) i=−L fi · rclip ( t+i,0 , T ) s.t . L ( cid:88 ) i=−L fi = 1 , ( 1 ) where T and L denote the episode and smoothing horizons , respectively . For simplicity , we omit the discount factor in Equation ( 1 ) ; the full equation can be found in Appendix , Equation ( 6 ) . Episodes with the smoothed rewards are stored in the replay buffer and used to train the reward model . The agent learns only from the smoothed rewards , without ever seeing the original rewards . The smoothed rewards ease reward prediction by allowing the model to predict rewards several timesteps earlier or later , without incurring large losses . In this paper , we investigate three popular smoothing functions : Gaussian , uniform , and exponential moving average ( EMA ) smoothing , as illustrated in Figure 4 . While the main motivation for smoothing is to make it easier to learn reward models , we note that reward smoothing in some cases preserves optimality – an optimal policy under smoothed rewards ˜r 4 3 2 1 l d e t e p m o C s k s a T 0 0 8 16 24 Environment steps ( ×10⁶ ) 2 1 d e p m u D s k c o R 0 0 4 8 12 Environment steps ( ×10⁶ ) 1 d r a w e R 0 0 Orginal σ = 1 σ = 2 2 4 Timestep 6 8 1 d r a w e R 0 0 Orginal δ = 3 δ = 7 2 4 Timestep 6 8 1 d r a w e R 0 0 Orginal α = .3 α = .6 2 4 Timestep 6 8 is also optimal under the original rewards r. In particular , we provide a proof in Appendix A for the optimality of EMA smoothing ( and any smoothing function where ∀i > 0 , fi = 0 ) by augmenting the POMDP states with the history of past states . However , when future rewards are used for smoothing ( e.g . Gaussian smoothing ) , the smoothed rewards are conditioned on policy , and we can no longer define an equivalent POMDP . In such cases , there is no theoretical guarantee . Even so , we empirically show that reward models can adapt their predictions alongside the changing policy , and achieve performance improvements . The implementation of DreamSmooth is extremely simple , requiring only one additional line of code to existing MBRL algorithms , as shown in Algorithm 1 . The overhead of reward smoothing is minimal , with time complexity O ( T · L ) . More implementation details can be found in Appendix B. Algorithm 1 COLLECT ROLLOUT ( π : policy , D : replay buffer ) in DREAMSMOOTH t=1 } ← ROLLOUT ( π ) { ( ot , at , rt ) T { rt } T D ← D ∪ { ( ot , at , rt ) T t=1 ← GAUSSIAN ( { rt } T t=1 } t=1 , σ ) or EMA ( { rt } T t=1 , α ) ▷ only one line needs to be added . 4 EXPERIMENTS In this paper , we propose a simple reward smoothing method , DreamSmooth , which facilitates reward prediction in model-based reinforcement learning ( MBRL ) and thus , improves the performance of existing MBRL methods . Through our experiments , we aim to answer the following questions : ( 1 ) Does reward smoothing improve reward prediction ? ( 2 ) Does better reward prediction with reward smoothing lead to better sample efficiency and asymptotic performance of MBRL in sparse-reward tasks ? ( 3 ) Does MBRL with reward smoothing also work in common dense-reward tasks ? 4.1 TASKS We evaluate DreamSmooth on four tasks with sparse subtask completion rewards and two common RL benchmarks . Earthmoving uses two 64 × 64 images as an observation while all other tasks use a single image . See Appendix C for environment details . • RoboDesk : We use a modified version of RoboDesk ( Kannan et al. , 2021 ) , where a sequence of ma- nipulation tasks ( flat block in bin , upright block off table , push green ) need to be completed in order ( Figure 5a ) . We use the original dense rewards together with a large sparse reward for each task completed . • Hand : The Hand task ( Plappert et al. , 2018 ) requires a Shadow Hand to rotate a block in hand into a specific orientation . We extend it to achieve a sequence of pre-defined goal orientations in order . In addition to the original dense rewards , we provide a large sparse reward for each goal . • Earthmoving : The Earthmoving task consists of a wheel loader , dump truck , and a pile of rocks ( Figure 5c ) . The agent controls the wheel loader to pick up rocks from the pile and dump them in the dump truck . A large sparse reward is given for each rock picked up and for each rock dumped , proportional to its mass . In addition , dense rewards are given for moving rocks towards the dump truck . The environment is simulated using the AGX Dynamics physics engine ( Algoryx , 2020 ) . ( a ) RoboDesk ( b ) Hand ( c ) Earthmoving ( d ) Crafter ( e ) DMC ( f ) Atari Figure 5 : We evaluate DreamSmooth on four tasks with sparse subtask completion rewards ( a-d ) . We also test on two popular benchmarks , ( e ) DeepMind Control Suite and ( f ) Atari . 5 ( a ) RoboDesk ( b ) Hand ( c ) Earthmoving ( d ) Crafter Figure 6 : We visualize the ground truth rewards , smoothed rewards with Gaussian smoothing , and predicted rewards by DreamerV3 trained on the smoothed rewards over an evaluation episode . In contrast to Figure 2 , the reward models with reward smoothing capture most of sparse rewards . • Crafter : Crafter ( Hafner , 2022 ) is a minecraft-like 2D environment , where the agent tries to collect , place , and craft items in order to survive . There are 22 achievements in the environment ( e.g . collecting water , mining diamonds ) with a sparse reward 1 for obtaining each achievement for the first time . A small reward is given ( or lost ) for each health point gained ( or lost ) . • DMC : We benchmark 7 DeepMind Control Suite continuous control tasks ( Tassa et al. , 2018 ) . • Atari : We benchmark 6 Atari tasks ( Bellemare et al. , 2013 ) at 100K steps . 4.2 IMPROVED REWARD PREDICTION WITH REWARD SMOOTHING We first visualize the ground truth rewards , smoothed rewards ( Gaussian smoothing ) , and re- ward prediction results of DreamerV3 trained with DreamSmooth in Figure 6 . We observe that re- ward smoothing leads to a significant improve- ment in reward prediction : DreamSmooth success- fully predicts most of the ( smoothed ) sparse re- wards and no longer omits vital signals for policy learning or planning . The improvement is especially notable in Crafter . In Figure 7 , we measure the accuracy of the re- ward model , ( i.e . predicting a reward larger than half of the original or smoothed reward for Dream- erV3 and DreamSmooth respectively ) at the exact timesteps for each subtask . The vanilla Dream- erV3 ’ s reward model ( baseline ) misses most of the sparse rewards while DreamSmooth predicts sparse rewards more accurately in 15/19 subtasks . 4.3 RESULTS Figure 7 : Reward prediction rates for 19 achieve- ments in Crafter . The other 3 tasks have been never achieved by both methods . With reward smoothing , the prediction rates are better in 15/19 tasks . We compare the vanilla DreamerV3 ( Hafner et al. , 2023 ) with DreamSmooth , whose backbone is also DreamerV3 . For DreamSmooth , we evaluate Gaussian , uniform , and EMA smoothing . The 6 Ground Truth Smoothed Predicted x-axis : Timestep y-axis : Reward 64 00 0 120 60 0 −20 0 120 800 0 −200 0 360 0.6 0.0 −0.2 0 200 Baseline ( no smoothing ) Gaussian 1.0 0.5 e t a R n o i t c i d e r P 0.0 w k al d e ain alth g e H nt e n ace d e n o o bie alth loss g xe xe ord n ord ollect drin plin at skeleto at co ollect co Place pla d picka e picka Place sto ollect sto Place ta m d sw e sw Place furn ollect w ollect sa at zo E ke sto n efe ke w ke sto ke w D a M efe C D n o o o o C C C C e H p ble ke u Wa a M a M a M ( a ) RoboDesk ( b ) Hand ( c ) Earthmoving ( d ) Crafter ( e ) DMC ( f ) Atari Figure 8 : Comparison of learning curves of DreamSmooth ( Gaussian , Uniform , EMA ) and Dream- erV3 . The shaded regions in ( a-d ) show the maximum and minimum over 3 seeds . For DMC ( e ) and Atari ( f ) , we aggregate results over 7 and 6 tasks respectively , and display the standard deviation . hyperparameters for DreamerV3 and smoothing functions can be found in Appendix B . As shown in Figure 8 , DreamSmooth-Gaussian and DreamSmooth-Uniform significantly improve the performance as well as the sample efficiency of DreamerV3 on the Robodesk , Hand , and Earthmoving tasks . The only change between DreamerV3 and ours is the improved reward prediction , as shown in Section 4.2 . This result suggests that reward prediction is one of major bottlenecks of the MBRL performance . While all smoothing methods lead to improvements over DreamerV3 , Gaussian smoothing generally performs the best , except on Crafter , with uniform smoothing showing comparable performance . The better performance of Gaussian and uniform smoothing could be because it allows predicting rewards both earlier and later , whereas EMA smoothing only allows predicting rewards later . Despite the improved reward prediction accuracy , DreamSmooth-Gaussian and DreamSmooth- Uniform perform worse than the baseline in Crafter . This can be because more predicted task rewards encourage more exploitation and less exploration . Further investigation on this trade-off is a promising direction for future work . Moreover , we observe that on the DMC and Atari benchmarks , where reward prediction is not particu- larly challenging , our technique shows comparable performance with the unmodified algorithms ( see Appendix , Figure 16 for full results ) . This suggests that reward smoothing can be applied generally , and does not hinder performance on most environments . 7 DreamSmooth-Gaussian DreamSmooth-Uniform DreamSmooth-EMA DreamerV3 3 2 1 l d e t e p m o C s k s a T 0 0 18 12 6 Environment steps ( ×10⁶ ) 24 3 2 1 l d e t e p m o C s k s a T 0 0 15 10 5 Environment steps ( ×10⁶ ) 20 d e p m u D s k c o R f o r e b m u N 2.0 1.5 1.0 0.5 0.0 0 3 6 Environment steps ( ×10⁶ ) 9 12 16 12 8 4 e r o c S 0 0.0 1.5 4.5 3.0 Environment steps ( ×10⁶ ) 6.0 1000 750 500 250 e r o c S 0 0.00 0.25 0.75 0.50 Environment steps ( ×10⁶ ) 1.00 e r o c S d e z i l a m r o N n a m u H 0.6 0.4 0.2 0.0 0.0 0.1 0.3 0.2 Environment steps ( ×10⁶ ) 0.4 In Figure 9 , DreamSmooth also improves the performance of TD-MPC ( Hansen et al. , 2022 ) . In the Hand task , vanilla TD-MPC is unable to consistently solve the first task , even with proprioceptive state observations . However , TD-MPC with DreamSmooth learns to complete the tasks with not only state observations but also pixel observa- tions . This suggests that DreamSmooth can be useful in a broad range of MBRL algo- rithms that use a reward model . We only demonstrate the Hand task since TD-MPC fails on other sparse-reward tasks . 4.4 ABLATION STUDIES ( a ) Hand ( Pixel ) ( b ) Hand ( State ) Figure 9 : Learning curves for TD-MPC and TD-MPC with DreamSmooth on the Hand task . The shaded re- gions show the minimum and maximum over 3 seeds . Data Imbalance . One possible cause of poor reward predictions is data imbalance – because sparse rewards are infrequent , sequences con- taining sparse rewards are rarely sampled from the replay buffer . The reward model therefore trains on fewer examples of sparse rewards , po- tentially leading to poor predictions . To test this hypothesis , we conducted experiments with oversampling : with probability p = 0.5 , we sample a sequence in which the agent receives a sparse reward ; otherwise , we sample uni- formly from all sequences in the buffer . As shown in Figure 10 , oversampling performs bet- ter than the baseline , but learns slower than DreamSmooth . This suggests that while data imbalance largely contributes to the difficulty of reward prediction , it is not the only factor hindering performance . Furthermore , this over- sampling method requires domain knowledge about which reward signals to be oversampled while DreamSmooth is agnostic to the scale and frequency of sparse rewards . Figure 10 : Using oversampling of sequences with sparse rewards ( p = 0.5 ) performs better than DreamerV3 on RoboDesk , but worse than DreamSmooth with Gaussian smoothing . The lines show median task performance over 3 seeds , while shaded regions show maximum and minimum . Reward Model Size . Another hypothe- sis for poor reward predictions is that the reward model does not have enough ca- pacity to capture sparse rewards . To test this hypothesis , we increase the size of the reward model from 4 layers of 768 units to 5 layeres of 1024 units and 6 layers of 1280 units , while keeping the rest of the world model the same . We observe in Fig- ure 11 that without smoothing , changing the reward model size has negligible impact on performance , and DreamSmooth out- performs all the reward model sizes tested . This indicates that the reward prediction problem is not simply caused by insuffi- cient model capacity . ( a ) RoboDesk ( b ) Hand Figure 11 : Simply increasing the reward model size has negligible impact on performance . DreamerV3-768 and DreamSmooth use 4 layers of 768 units ; DreamerV3- 1024 uses 5 layers of 1024 units ; and DreamerV3-1280 uses 6 layers of 1280 units . In Figure 12 , we analyze the impact of the smoothing parameters σ and Smoothing Parameter . α for Gaussian and EMA , respectively , on RoboDesk and Hand . We observe that DreamSmooth is insensitive to the smoothing parameters , performing well across a wide range of values . 8 DreamSmooth-Gaussian ( TDMPC ) TD-MPC l d e t e p m o C s k s a T 1.2 0.8 0.4 0.0 0.0 0.5 1.0 Environment steps ( ×10⁶ ) l d e t e p m o C s k s a T 3 2 1 0 0 4 2 Environment steps ( ×10⁶ ) 3 2 1 l d e t e p m o C s k s a T 0 0 DreamSmooth Oversampling DreamerV3 6 18 12 Environment steps ( ×10⁶ ) 24 DreamSmooth DreamerV3-1280 DreamerV3-1024 DreamerV3-768 l d e t e p m o C s k s a T 3 2 1 0 6 0 24 12 Environment steps ( ×10⁶ ) 18 l d e t e p m o C s k s a T 3 2 1 0 0 20 10 Environment steps ( ×10⁶ ) ( a ) Gaussian Smoothing on RoboDesk ( b ) Gaussian Smoothing on Hand ( c ) Uniform Smoothing on RoboDesk ( d ) Uniform Smoothing on Hand ( e ) EMA Smoothing on RoboDesk ( f ) EMA Smoothing on Hand Figure 12 : Parameter sweep over smoothing parameters σ , δ , and α . The lines show median task performance over 3 seeds , while shaded regions show maximum and minimum . 5 CONCLUSION In this paper , we identify the reward prediction problem in MBRL and provide a simple yet effective solution , reward smoothing . Our approach , DreamSmooth , demonstrates superior performance in sparse reward tasks where reward prediction is not trivial mainly due to the partial observability or stochasticity of the environments . Moreover , DreamSmooth shows comparable results on the commonly used benchmarks , DMC and Atari , showing its task-agnostic nature . Although we show that our simple reward smoothing approach mitigates the difficulty in reward prediction , the improved reward prediction does not always improve the task performance , e.g. , in Crafter . This can be because more predicted task rewards encourage more exploitation and less exploration . Further investigation on this trade-off is a promising direction for future work . 9 DreamSmooth-Gaussian ( cid:15 ) ( cid:3 ) ( cid:8 ) ( cid:3 ) ( cid:4 ) ( cid:15 ) ( cid:3 ) ( cid:8 ) ( cid:3 ) ( cid:6 ) ( cid:15 ) ( cid:3 ) ( cid:8 ) ( cid:3 ) ( cid:7 ) ( cid:9 ) ( cid:14 ) ( cid:12 ) ( cid:11 ) ( cid:13 ) ( cid:12 ) ( cid:14 ) ( cid:10 ) ( cid:5 ) 3 2 1 l d e t e p m o C s k s a T 0 0 6 18 12 Environment steps ( ×10⁶ ) 24 3 2 1 l d e t e p m o C s k s a T 0 0 5 15 10 Environment steps ( ×10⁶ ) 20 DreamSmooth-Uniform ( cid:15 ) ( cid:3 ) ( cid:8 ) ( cid:3 ) ( cid:6 ) ( cid:15 ) ( cid:3 ) ( cid:8 ) ( cid:3 ) ( cid:7 ) ( cid:15 ) ( cid:3 ) ( cid:8 ) ( cid:3 ) ( cid:4 ) ( cid:5 ) ( cid:9 ) ( cid:14 ) ( cid:12 ) ( cid:11 ) ( cid:13 ) ( cid:12 ) ( cid:14 ) ( cid:10 ) ( cid:5 ) 3 2 1 l d e t e p m o C s k s a T 0 0 6 18 12 Environment steps ( ×10⁶ ) 24 3 2 1 l d e t e p m o C s k s a T 0 0 5 15 10 Environment steps ( ×10⁶ ) 20 DreamSmooth-EMA ( cid:17 ) ( cid:3 ) ( cid:10 ) ( cid:3 ) ( cid:5 ) ( cid:4 ) ( cid:6 ) ( cid:17 ) ( cid:3 ) ( cid:10 ) ( cid:3 ) ( cid:5 ) ( cid:4 ) ( cid:7 ) ( cid:8 ) ( cid:17 ) ( cid:3 ) ( cid:10 ) ( cid:3 ) ( cid:5 ) ( cid:4 ) ( cid:9 ) ( cid:11 ) ( cid:16 ) ( cid:14 ) ( cid:13 ) ( cid:15 ) ( cid:14 ) ( cid:16 ) ( cid:12 ) ( cid:6 ) 3 2 1 l d e t e p m o C s k s a T 0 0 6 18 12 Environment steps ( ×10⁶ ) 24 3 2 1 l d e t e p m o C s k s a T 0 0 5 15 10 Environment steps ( ×10⁶ ) 20 ACKNOWLEDGMENTS This work was supported in part by the BAIR Industrial Consortium , an ONR DURIP grant , Komatsu , and InnoHK Centre for Logistics Robotics . We would like to thank all members of the Berkeley Robot Learning lab for their insightful feedback . REFERENCES Algoryx . AGX dynamics , 2020 . URL https : //www.algoryx.se/agx-dynamics/ . Arthur Argenson and Gabriel Dulac-Arnold . Model-based offline planning . In International Confer- ence on Learning Representations , 2021 . URL https : //openreview.net/forum ? id= OMNB1G5xzd4 . Mohammad Babaeizadeh , Mohammad Taghi Saffar , Danijar Hafner , Harini Kannan , Chelsea Finn , Sergey Levine , and Dumitru Erhan . Models , pixels , and rewards : Evaluating design trade-offs in visual model-based reinforcement learning . arXiv preprint arXiv:2012.04603 , 2020 . M. G. Bellemare , Y. Naddaf , J. Veness , and M. Bowling . The arcade learning environment : An evaluation platform for general agents . Journal of Artificial Intelligence Research , 47:253–279 , jun 2013 . Fei Deng , Ingook Jang , and Sungjin Ahn . Dreamerpro : Reconstruction-free model-based reinforce- ment learning with prototypical representations . In International Conference on Machine Learning , pp . 4956–4975 . PMLR , 2022 . Fei Deng , Junyeong Park , and Sungjin Ahn . Facing off world model backbones : Rnns , transformers , and s4 . arXiv preprint arXiv:2307.02064 , 2023 . Christopher D Fiorillo , William T Newsome , and Wolfram Schultz . The temporal precision of reward prediction in dopamine neurons . Nature neuroscience , 11 ( 8 ) :966–973 , 2008 . David Ha and J¨urgen Schmidhuber . World models . arXiv preprint arXiv:1803.10122 , 2018 . Danijar Hafner . Benchmarking the spectrum of agent capabilities . In International Conference on Learning Representations , 2022 . Danijar Hafner , Timothy Lillicrap , Jimmy Ba , and Mohammad Norouzi . Dream to control : Learning behaviors by latent imagination . In International Conference on Learning Representations , 2019 . Danijar Hafner , Timothy Lillicrap , Mohammad Norouzi , and Jimmy Ba . Mastering atari with discrete world models . In International Conference on Learning Representations , 2021 . Danijar Hafner , Jurgis Pasukonis , Jimmy Ba , and Timothy Lillicrap . Mastering diverse domains through world models . arXiv preprint arXiv:2301.04104 , 2023 . Nicklas Hansen , Xiaolong Wang , and Hao Su . Temporal difference learning for model predictive control . In International Conference on Machine Learning , 2022 . Harini Kannan , Danijar Hafner , Chelsea Finn , and Dumitru Erhan . Robodesk : A multi-task rein- forcement learning benchmark . https : , 2021 . Miriam C Klein-Fl¨ugge , Laurence T Hunt , Dominik R Bach , Raymond J Dolan , and Timothy EJ Behrens . Dissociable reward and timing signals in human midbrain and ventral striatum . Neuron , 72 ( 4 ) :654–664 , 2011 . Russell Mendonca , Shikhar Bahl , and Deepak Pathak . Structured world models from human videos . In Robotics : Science and Systems , 2023 . Andrew Y Ng , Daishi Harada , and Stuart Russell . Policy invariance under reward transformations : Theory and application to reward shaping . In International Conference on Machine Learning , volume 99 , pp . 278–287 , 1999 . 10 Masashi Okada and Tadahiro Taniguchi . Dreaming : Model-based reinforcement learning by latent imagination without reconstruction . In IEEE International Conference on Robotics and Automation , pp . 4209–4215 , 2021. doi : 10.1109/ICRA48506.2021.9560734 . Matthias Plappert , Marcin Andrychowicz , Alex Ray , Bob McGrew , Bowen Baker , Glenn Powell , Jonas Schneider , Josh Tobin , Maciek Chociej , Peter Welinder , et al . Multi-goal reinforcement learn- ing : Challenging robotics environments and request for research . arXiv preprint arXiv:1802.09464 , 2018 . Julian Schrittwieser , Ioannis Antonoglou , Thomas Hubert , Karen Simonyan , Laurent Sifre , Simon Schmitt , Arthur Guez , Edward Lockhart , Demis Hassabis , Thore Graepel , et al . Mastering atari , go , chess and shogi by planning with a learned model . Nature , 588 ( 7839 ) :604–609 , 2020 . Younggyo Seo , Danijar Hafner , Hao Liu , Fangchen Liu , Stephen James , Kimin Lee , and Pieter Abbeel . Masked world models for visual control . In Conference on Robot Learning , 2022 . Younggyo Seo , Junsu Kim , Stephen James , Kimin Lee , Jinwoo Shin , and Pieter Abbeel . Multi-view masked world models for visual robotic manipulation . In International Conference on Machine Learning , 2023 . Lucy Xiaoyang Shi , Joseph J. Lim , and Youngwoon Lee . Skill-based model-based reinforcement learning . In Conference on Robot Learning , 2022 . David Silver , Aja Huang , Chris J Maddison , Arthur Guez , Laurent Sifre , George Van Den Driessche , Julian Schrittwieser , Ioannis Antonoglou , Veda Panneershelvam , Marc Lanctot , et al . Mastering the game of go with deep neural networks and tree search . nature , 529 ( 7587 ) :484–489 , 2016 . David Silver , Julian Schrittwieser , Karen Simonyan , Ioannis Antonoglou , Aja Huang , Arthur Guez , Thomas Hubert , Lucas Baker , Matthew Lai , Adrian Bolton , et al . Mastering the game of go without human knowledge . nature , 550 ( 7676 ) :354–359 , 2017 . David Silver , Thomas Hubert , Julian Schrittwieser , Ioannis Antonoglou , Matthew Lai , Arthur Guez , Marc Lanctot , Laurent Sifre , Dharshan Kumaran , Thore Graepel , et al . A general reinforcement learning algorithm that masters chess , shogi , and go through self-play . Science , 362 ( 6419 ) : 1140–1144 , 2018 . Gautam Singh , Skand Peri , Junghyun Kim , Hyunseok Kim , and Sungjin Ahn . Structured world belief for reinforcement learning in pomdp . In International Conference on Machine Learning , pp . 9744–9755 . PMLR , 2021 . Richard S Sutton and Andrew G Barto . Reinforcement learning : An introduction . MIT Press , 2018 . Yuval Tassa , Yotam Doron , Alistair Muldal , Tom Erez , Yazhe Li , Diego de Las Casas , David Budden , Abbas Abdolmaleki , Josh Merel , Andrew Lefrancq , Timothy P. Lillicrap , and Martin A. Riedmiller . Deepmind control suite . arXiv preprint arXiv:1801.00690 , 2018 . 11 A PROOFS Let M = ( S , A , P , R , γ ) be the given MDP . Without loss of generality , we assume the augmented form of the MDP M , where a state st includes the entire history of states , i.e. , st = ( s1 , . . . , st ) , and thus , reward functions R , ˜R have access to previous states , i.e. , ˜R ( st ) = ˜R ( s1 , . . . , st ) . Theorem A.1 . An optimal policy ˜π∗ of the MDP with reward smoothing only with past rewards , e.g. , EMA smoothing , ˜M = ( S , A , P , ˜R , γ ) is also optimal under the original MDP M , where ˜R ( st ) = 0 ( cid:88 ) i=−L fi · γiR ( st+i ) and 0 ( cid:88 ) i=−L fi = 1 . ( 2 ) Proof . We will use the theorem of reward shaping that guarantees an optimal policy introduced in Ng et al . ( 1999 ) : if a modified reward function can be represented in the form of R ( st ) +γΦ ( st+1 ) −Φ ( st ) with any potential function Φ ( st ) , the new reward function yields the same optimal policy with the original reward function R. Let the potential function for the EMA reward smoothing Φ ( st ) = − −1 ( cid:88 ) i=−L γiR ( st+i ) + 0 ( cid:88 ) i=−L γiR ( st+i ) · 0 ( cid:88 ) fj . j=i+1 ( 3 ) Then , our reward shaping term in ˜R can be represented as the difference in the potential function γΦ ( st+1 ) − Φ ( st ) as follows : γΦ ( st+1 ) − Φ ( st ) = −R ( st ) + 0 ( cid:88 ) i=−L fi · γiR ( st+i ) . R ( st ) + γΦ ( st+1 ) − Φ ( st ) = 0 ( cid:88 ) i=−L fi · γiR ( st+i ) = ˜R . ( 4 ) ( 5 ) Hence , following Ng et al . ( 1999 ) , reward shaping with our EMA smoothing guarantees the optimal policy in the original MDP M. However , Theorem A.1 does not apply to smoothing functions that require access to future rewards , e.g. , Gaussian smoothing . As in Gaussian smoothing , a smoothed reward function may require future rewards , which are conditioned on the current policy ; so is the reward model . In such cases , there is no theoretical guarantee ; but in our experiments , we empirically show that reward models can adapt their predictions along the changes in policies and thus , improve MBRL . Instead , we intuitively explain that an optimal policy under any reward smoothing ( even though the reward function is post hoc and can not be defined for MDPs ) is also optimal under the original reward function . Theorem A.2 . An optimal policy ˜π∗ with the smoothed reward function ˜R is also optimal under the original reward function R , where ˜R ( st ) = L ( cid:88 ) i=−L γclip ( i , −t , T −t ) · fi · R ( sclip ( t+i,0 , T ) ) and L ( cid:88 ) i=−L fi = 1 . ( 6 ) 12 Proof . First , we show that the discounted sum of original rewards ( cid:80 ) T smoothed rewards ( cid:80 ) T t=0 γt ˜R ( st ) are the same for any trajectories ( s0 , s1 , . . . , sT ) : T ( cid:88 ) L ( cid:88 ) γclip ( i , −t , T −t ) · fi · R ( sclip ( t+i,0 , T ) ) γt ˜R ( st ) = γt from Equation ( 6 ) t=0 γtR ( st ) and the one of T ( cid:88 ) t=0 i=−L γtR ( st ) · L ( cid:88 ) fi i=−L γtR ( st ) . t=0 T ( cid:88 ) t=0 T ( cid:88 ) t=0 = = ( 7 ) ( 8 ) from L ( cid:88 ) i=−L fi = 1 ( 9 ) Let an optimal policy under the smoothed rewards ˜R be ˜π∗ . Assume that ˜π∗ is not optimal under the original reward R. Then , ∃π∗ , s0 such that E ( s0 , ... , sT ) ∼π∗ ( cid:104 ) T ( cid:88 ) t=0 ( cid:105 ) γtR ( st ) > E ( s0 , ... , sT ) ∼˜π∗ ( cid:104 ) T ( cid:88 ) t=0 γt ˜R ( st ) ( cid:105 ) . ( 10 ) However , E ( s0 , ... , sT ) ∼π∗ ( cid:104 ) T ( cid:88 ) t=0 ( cid:105 ) γtR ( st ) = E ( s0 , ... , sT ) ∼π∗ > E ( s0 , ... , sT ) ∼˜π∗ ( cid:104 ) T ( cid:88 ) t=0 ( cid:104 ) T ( cid:88 ) t=0 ( cid:105 ) γt ˜R ( st ) γt ˜R ( st ) ( cid:105 ) , by Equation ( 9 ) ( 11 ) by Equation ( 10 ) ( 12 ) which contradicts that ˜π∗ is optimal under ˜R . Therefore , the optimal policy ˜π∗ under ˜R guarantees its optimality under R. B IMPLEMENTATION DETAILS Models are trained on NVIDIA A5000 , V100 , RTX Titan , RTX 2080 , and RTX 6000 GPUs . Each experiment takes about 72 hours for RoboDesk , 100 hours for Hand , 150 hours for Earthmoving , 96 hours for Crafter , and 6 hours for Atari and DMC tasks . B.1 DREAMSMOOTH SMOOTHING FUNCTIONS Gaussian smoothing follows the Gaussian distribution with σ : fi = ke −i2 2σ2 , ( 13 ) where k = 1/ ( ( cid:80 ) L i=−L e We implement this using −i2 2σ2 ) is a normalization constant . scipy.ndimage.gaussian_filter1d ( rewards , sigma , mode= '' nearest '' ) Uniform smoothing distributes rewards equally across δ consecutive timesteps . 1 δ δ − 1 2 δ − 1 2 fi = ∀i ∈ − ( cid:105 ) ( cid:104 ) . , We implement this using scipy.ndimage.convolve ( rewards , filter , mode= '' nearest '' ) EMA smoothing uses the following smoothing function : fi = α ( 1 − α ) i ∀i ≤ 0 , which we implement by performing the following at each timestep : reward [ t ] = alpha * reward [ t - 1 ] + ( 1 - alpha ) * reward [ t ] 13 ( 14 ) ( 15 ) B.2 MODEL-BASED REINFORCEMENT LEARNING BACKBONES Hyperparameters for DreamerV3 experiments are shown in Table 1 and TD-MPC in Table 2 . Table 1 : DreamerV3 hyperparameters . Episode length is measured in environment steps , which is the number of agent steps multiplied by action repeat . Model sizes are as listed in Hafner et al . ( 2023 ) , which we also refer to for all other hyperparameters . Environment Action Repeat Episode Length Train Ratio Model Size Earthmoving RoboDesk Hand Crafter DMC Atari 4 8 1 1 2 4 2000 2400 300 Variable 1000 Variable 64 64 64 64 512 1024 L L L XL S S σ 3 3 2 1 3 3 α 0.33 0.3 0.3 0.45 0.33 0.3 δ 9 9 9 9 9 9 Table 2 : TD-MPC hyperparameters . Unless specified , we use the default hyperparameters in Hansen et al . ( 2022 ) . Environment Latent Dimension CNN channels Planning Iterations Hand-Pixel Hand-Proprio 128 128 64 – 6 12 σ 3 3 C ENVIRONMENT DETAILS C.1 ROBODESK ENVIRONMENT We use a modified version of RoboDesk ( Kannan et al. , 2021 ) , where a sequence of manipulation tasks ( flat block in bin , upright block off table , push green ) need to be completed in order . Figure 13 shows images of an agent successfully completing each of these tasks . In the original environment , dense rewards are based on Euclidean distances of objects to their targets , with additional terms to encourage the arm to reach the object . They typically range from 0 to 10 per timestep . We use these dense rewards together with a large sparse reward of 300 for each task completed . ( a ) Put the flat block into the bin ( b ) Push the upright block off the table ( c ) Press the green button Figure 13 : Subtasks for RoboDesk . C.2 HAND ENVIRONMENT We modified the Shadow Hand environment ( Plappert et al. , 2018 ) , so that the agent is required to achieve a sequence of pre-defined goal orientations in order . The first 3 goals are shown in Figure 14 , 14 while the subsequent goals are a repeat of the first 3 . The goal orientations are chosen so that the agent only has to rotate the cube along the z-axis , and we only require the agent to match the cube ’ s rotation to the goal , not its position . In the original environment , dense rewards are computed using r = − ( 10x + ∆θ ) , where x is the Euclidean distance to some fixed position , and ∆θ is the angular difference to the target orientation . In addition to these dense rewards , we provide a large sparse reward of 300 for each goal successfully achieved by the agent . ( a ) Goal 1 ( b ) Goal 2 ( c ) Goal 3 Figure 14 : Subtasks for Hand . C.3 AGX EARTHMOVING ENVIRONMENT The Earthmoving environment consists of a wheel loader , dump truck , a pile of dirt , with some rocks on top of the pile . The en- vironment is simulated using the realistic AGX Dynamics physics engine ( Algoryx , 2020 ) . The agent controls the wheel loader to pick up rocks and dump them in the dump truck . The starting positions of the dirt pile , wheel loader , and dump truck are all randomized , as are the initial orientations of the dirt pile and wheel loader . The agent ’ s observations consist of 3 components : a wide-angle egocentric RGB camera mounted on the cabin to allow navigation , an RGB camera mounted on the bucket for observing interactions with rocks , and proprioceptive observations ( positions , velocity , speed , force of actuators etc. ) . We use 64 × 64 × 3 images for all cameras , while the proprioceptive observation has 21 dimensions . Figure 15 : The agent uses one camera mounted on the cabin ( left ) for navigation , and one mounted on the bucket ( right ) for observing interactions with rocks and terrain . The action space is 4-dimensional : 2 dimensions for driving and steering the loader , and 2 dimensions for moving and tilting the bucket . The reward consists of a large sparse reward for rocks picked up and dumped , and dense rewards for moving rocks towards the dumptruck . The total reward rt at timestep t is computed using Equation ( 16 ) . rt = λdump ( mt ( cid:124 ) load ( max ( 2 , dt ) − max ( 2 , dt−1 ) ) ( cid:125 ) load − mt−1 load ) ( cid:125 ) dump ) + λload ( mt dump − mt−1 + λmovemt ( cid:124 ) ( cid:123 ) ( cid:122 ) dense reward ( cid:123 ) ( cid:122 ) sparse reward Where mdump , mload are rock masses in the dumptruck and the bucket respectively , d is the distance between the shovel and a point above the dumptruck , and λ are constants . ( 16 ) 15 D DMC AND ATARI BENCHMARKING RESULTS Figure 16 : Full learning curves for the DMC and Atari benchmarks . 16 DreamSmooth-Gaussian DreamSmooth-Uniform DreamSmooth-EMA DreamerV3 Hopper Hop 1000 e r o c S 500 0 0.0 1.0 0.5 Environment steps ( ×10⁶ ) Reacher Hard 1000 e r o c S 500 0 0.0 1.0 0.5 Environment steps ( ×10⁶ ) Cartpole Swingup Sparse 1000 e r o c S 500 0 0.0 1.0 0.5 Environment steps ( ×10⁶ ) Walker Run 1000 e r o c S 500 0 0.0 1.0 0.5 Environment steps ( ×10⁶ ) Finger Turn Hard 1000 e r o c S 500 0 0.0 1.0 0.5 Environment steps ( ×10⁶ ) Quadruped Run 1000 e r o c S 500 0 0.0 1.0 0.5 Environment steps ( ×10⁶ ) Cheetah Run 1000 e r o c S 500 0 0.0 1.0 0.5 Environment steps ( ×10⁶ ) Pong 3.00 −5.33 −13.67 e r o c S −22.00 0.0 0.4 0.2 Environment steps ( ×10⁶ ) Breakout 16.00 10.67 5.33 e r o c S 0.00 0.0 0.4 0.2 Environment steps ( ×10⁶ ) Freeway 29 19 9 e r o c S −1 0.0 0.2 Environment steps ( ×10⁶ ) 0.4 Assault 807.0 621.3 435.7 e r o c S 250.0 0.0 0.4 0.2 Environment steps ( ×10⁶ ) Seaquest 545 378 211 e r o c S 44 0.0 0.2 Environment steps ( ×10⁶ ) 0.4 Hero 13200 e r o c S 8860 4520 180 0.0 0.4 0.2 Environment steps ( ×10⁶ )","['n', 'l', 'c', 'v', 'r', 'dreamsmooth', 'improve', 'modelbase', 'rein', 'forcement', 'learning', 'reward', 'smoothing', 'vint', 'pieter', 'modelbase', 'reinforcement', 'learning', 'mbrl', 'gain', 'much', 'attention', 'ability', 'learn', 'complex', 'behavior', 'sampleefficient', 'way', 'planning', 'action', 'generate', 'imaginary', 'trajectory', 'predict', 'reward', 'success', 'find', 'surprisingly', 'reward', 'prediction', 'often', 'bottleneck', 'mbrl', 'especially', 'sparse', 'reward', 'challenge', 'even', 'ambiguous', 'predict', 'motivate', 'intuition', 'human', 'learn', 'rough', 'reward', 'estimate', 'propose', 'simple', 'yet', 'effective', 'reward', 'smooth', 'approach', 'dreamsmooth', 'learn', 'predict', 'temporallysmoothe', 'reward', 'instead', 'exact', 'reward', 'give', 'timestep', 'empirically', 'show', 'dreamsmooth', 'achieve', 'stateoftheart', 'performance', 'task', 'sample', 'efficiency', 'final', 'performance', 'lose', 'performance', 'common', 'benchmark', 'deepmind', 'control', 'suite', 'atari', 'benchmark', 'introduction', 'human', 'often', 'plan', 'action', 'rough', 'estimate', 'future', 'reward', 'instead', 'exact', 'reward', 'exact', 'moment', 'fiorillo', 'rough', 'reward', 'estimate', 'mostly', 'sufficient', 'learn', 'task', 'predict', 'exact', 'reward', 'often', 'challenge', 'ambiguous', 'delay', 'observable', 'consider', 'instance', 'manipulation', 'lustrate', 'figure', 'middle', 'push', 'block', 'table', 'sparse', 'reward', 'give', 'timestep', 'block', 'first', 'touch', 'bin', 'use', 'image', 'observa', 'tion', 'agent', 'challenge', 'even', 'man', 'predict', 'correct', 'sequence', 'reward', 'crucially', 'issue', 'present', 'many', 'environ', 'ment', 'state', 'reward', 'almost', 'indistinguishable', 'reward', 'figure', 'predict', 'exact', 'sequence', 'reward', 'extremely', 'difficult', 'example', 'show', 'sequence', 'image', 'observation', 'see', 'agent', 'receive', 'sparse', 'reward', 'little', 'visually', 'distinguish', 'timestep', 'large', 'reward', 'create', 'significant', 'challenge', 'reward', 'prediction', 'accurate', 'reward', 'model', 'vital', 'model', 'base', 'reinforcement', 'learning', 'mbrl', 'reward', 'estimate', 'high', 'cause', 'agent', 'choose', 'action', 'perform', 'poorly', 'reality', 'estimate', 'low', 'lead', 'agent', 'ignore', 'high', 'reward', 'difficulty', 'importance', 'reward', 'prediction', 'problem', 'mbrl', 'largely', 'overlook', 'find', 'even', 'stateoftheart', 'reward', 'prediction', 'challenge', 'also', 'performance', 'bottleneck', 'many', 'task', 'instance', 'dreamerv3', 'fail', 'predict', 'reward', 'objective', 'crafter', 'environment', 'similar', 'failure', 'mode', 'observe', 'variant', 'robodesk', 'shadow', 'hand', 'plappert', 'task', 'sparse', 'reward', 'inspire', 'human', 'intuition', 'rough', 'estimate', 'reward', 'sufficient', 'propose', 'simple', 'yet', 'effective', 'solution', 'dreamsmooth', 'learn', 'predict', 'temporallysmoothe', 'reward', 'rather', 'exact', 'reward', 'timestep', 'make', 'reward', 'prediction', 'much', 'easy', 'instead', 'predict', 'reward', 'exactly', 'model', 'need', 'produce', 'estimate', 'sparse', 'reward', 'obtain', 'sufficient', 'policy', 'learn', 'experiment', 'demonstrate', 'extremely', 'simple', 'technique', 'significantly', 'improve', 'performance', 'different', 'mbrl', 'algorithm', 'many', 'sparsereward', 'environment', 'specifically', 'find', 'dreamerv3', 'hansen', 'technique', 'especially', 'beneficial', 'environment', 'follow', 'characteristic', 'sparse', 'reward', 'partial', 'observability', 'stochastic', 'reward', 'finally', 'show', 'even', 'benchmark', 'reward', 'prediction', 'significant', 'issue', 'dreamsmooth', 'degrade', 'performance', 'indicate', 'technique', 'universally', 'apply', 'relate', 'work', 'modelbase', 'reinforcement', 'learning', 'mbrl', 'leverage', 'dynamic', 'model', 'ie', 'world', 'model', 'environment', 'reward', 'model', 'desire', 'task', 'plan', 'sequence', 'action', 'maximize', 'total', 'reward', 'dynamic', 'model', 'predict', 'future', 'state', 'environment', 'take', 'specific', 'action', 'reward', 'model', 'predict', 'reward', 'correspond', 'stateaction', 'transition', 'dynamic', 'reward', 'model', 'agent', 'simulate', 'large', 'number', 'candidate', 'behavior', 'imagination', 'instead', 'physical', 'environment', 'allow', 'mbrl', 'tackle', 'many', 'challenging', 'task', 'silver', 'instead', 'rely', 'give', 'dynamic', 'reward', 'model', 'recent', 'advance', 'mbrl', 'enable', 'learn', 'world', 'model', 'highdimensional', 'observation', 'complex', 'dynamic', 'schmidhuber', 'schrittwieser', 'hafner', 'hansen', 'well', 'temporallyextende', 'world', 'model', 'specifically', 'dreamerv3', 'achieve', 'stateoftheart', 'performance', 'diverse', 'domain', 'problem', 'eg', 'pixel', 'state', 'observation', 'well', 'discrete', 'continuous', 'action', 'realistic', 'imagination', 'mbrl', 'require', 'accurate', 'world', 'model', 'significant', 'effort', 'learn', 'well', 'world', 'model', 'leverage', 'human', 'video', 'adopt', 'performant', 'architecture', 'deng', 'representation', 'learning', 'prototypebase', 'deng', 'objectcentric', 'state', 'representation', 'contrastive', 'learn', 'taniguchi', 'mask', 'autoencoding', 'seo', 'however', 'compare', 'effort', 'learn', 'well', 'world', 'model', 'learn', 'accurate', 'reward', 'model', 'largely', 'overlook', 'investigate', 'effect', 'various', 'world', 'model', 'design', 'show', 'reward', 'prediction', 'strongly', 'correlate', 'task', 'performance', 'train', 'offline', 'dataset', 'limit', 'densereward', 'environment', 'paper', 'point', 'accurate', 'reward', 'prediction', 'crucial', 'mbrl', 'especially', 'sparsereward', 'task', 'partially', 'observable', 'environment', 'propose', 'simple', 'method', 'improve', 'reward', 'prediction', 'mbrl', 'approach', 'main', 'goal', 'paper', 'understand', 'challenging', 'reward', 'prediction', 'modelbase', 'reinforcement', 'learning', 'mbrl', 'propose', 'simple', 'yet', 'effective', 'solution', 'reward', 'smooth', 'make', 'reward', 'prediction', 'easy', 'learn', 'section', 'first', 'provide', 'background', 'mbrl', 'section', 'present', 'experiment', 'demonstrate', 'challenge', 'predict', 'sparse', 'reward', 'signal', 'section', 'finally', 'explain', 'approach', 'dreamsmooth', 'section', 'background', 'formulate', 'problem', 'partially', 'observable', 'markov', 'decision', 'process', 'pomdp', 'define', 'tuple', 'p', 'r', 'observation', 'space', 'action', 'space', 'p', 'timestep', 'transition', 'dynamic', 'r', 'reward', 'function', 'map', 'previous', 'observation', 'action', 'reward', 'r', 'o≤t', 'discount', 'factor', 'sutton', 'rl', 'aim', 'find', 'policy', 'π', 'o≤t', 'maximize', 'expect', 'sum', 'reward', 'paper', 'focus', 'mbrl', 'algorithm', 'learn', 'world', 'model', 'reward', 'model', 'rtzt', 'agent', 'experience', 'learned', 'latent', 'state', 'timestep', 'learned', 'world', 'model', 'reward', 'model', 'generate', 'imaginary', 'rollout', 'aτ', 'horizon', 'start', 'zt', 'use', 'planning', 'argenson', 'dulacarnold', 'hansen', 'policy', 'optimization', 'schmidhuber', 'hafner', 'specifically', 'use', 'stateoftheart', 'algorithm', 'dreamerv3', 'hansen', 'use', 'predict', 'reward', 'compute', 'new', 'value', 'target', 'train', 'critic', 'learn', 'good', 'policy', 'reward', 'model', 'play', 'vital', 'role', 'critic', 'actor', 'learn', 'policy', 'receive', 'training', 'signal', 'exclusively', 'reward', 'model', 'note', 'datum', 'collect', 'environment', 'use', 'train', 'world', 'model', 'reward', 'model', 'hand', 'learn', 'stateaction', 'value', 'function', 'zt', 'directly', 'agent', 'experience', 'predict', 'reward', 'however', 'reward', 'model', 'still', 'important', 'obtain', 'good', 'policy', 'tdmpc', 'use', 'reward', 'model', 'value', 'function', 'obtain', 'policy', 'online', 'planning', 'reward', 'prediction', 'difficult', 'reward', 'prediction', 'surprisingly', 'challenge', 'many', 'environment', 'figure', 'show', 'sequence', 'frame', 'right', 'sparse', 'reward', 'receive', 'diverse', 'environment', 'even', 'human', 'difficult', 'determine', 'exact', 'timestep', 'reward', 'receive', 'environment', 'hypothesize', 'mean', 'square', 'error', 'loss', 'e', 'z', 'r', 'rθ', 'z', 'r', 'typically', 'use', 'reward', 'model', 'training', 'deteriorate', 'reward', 'prediction', 'accuracy', 'exist', 'sparse', 'reward', 'predict', 'sparse', 'reward', 'single', 'step', 'early', 'later', 'result', 'high', 'loss', 'simply', 'predict', 'reward', 'step', 'thus', 'instead', 'try', 'predict', 'sparse', 'reward', 'exact', 'timestep', 'reward', 'model', 'minimize', 'loss', 'entirely', 'omitting', 'sparse', 'reward', 'prediction', 'verify', 'hypothesis', 'plot', 'groundtruth', 'dreamerv3', 'predict', 'reward', 'figure', 'reward', 'model', 'struggle', 'predict', 'exact', 'reward', 'simply', 'ignore', 'sparse', 'reward', 'straightforward', 'predict', 'task', 'describe', 'section', 'hypothesis', 'also', 'hold', 'deterministic', 'fullyobservable', 'environment', 'crafter', 'source', 'sparse', 'reward', 'reward', 'model', 'fail', 'predict', 'reward', 'source', 'figure', '2d', 'difficulty', 'reward', 'prediction', 'far', 'exacerbate', 'partial', 'observability', 'ambiguous', 'reward', 'stochastic', 'dynamic', 'environment', 'example', 'first', 'third', 'row', 'figure', 'robodesk', 'b', 'hand', 'c', 'earthmove', 'crafter', 'figure', 'ground', 'truth', 'reward', 'dreamerv3', 'predict', 'reward', 'evaluation', 'episode', 'reward', 'model', 'miss', 'many', 'sparse', 'reward', 'highlight', 'yellow', 'ground', 'truth', 'predict', 'timestep', 'yaxis', 'reward', '−110', '−110', '−800', '−04', 'sparse', 'reward', 'give', 'block', 'rock', 'third', 'example', 'first', 'contact', 'dumptruck', 'exact', 'moment', 'contact', 'directly', 'observable', 'camera', 'viewpoint', 'make', 'reward', 'prediction', 'ambiguous', 'moreover', 'stochastic', 'environment', 'dynamic', 'contact', 'multiple', 'rock', 'make', 'predict', 'future', 'state', 'reward', 'challenge', 'reward', 'prediction', 'bottleneck', 'mbrl', 'precede', 'section', 'show', 'reward', 'prediction', 'challenge', 'many', 'environment', 'importantly', 'poor', 'reward', 'prediction', 'bottleneck', 'policy', 'learning', 'show', 'figure', 'robodesk', 'reward', 'model', 'reliably', 'detect', 'completion', 'second', 'task', 'figure', '2a', 'policy', 'get', 'stick', 'solve', 'first', 'task', 'fail', 'subsequent', 'task', 'earthmove', 'reward', 'model', 'capture', 'reward', 'successful', 'dumping', 'figure', 'policy', 'frequently', 'drop', 'rock', 'dumptruck', 'consistent', 'failure', 'mode', 'reward', 'prediction', 'policy', 'learning', 'dreamerv3', 'suggest', 'poor', 'reward', 'prediction', 'bottleneck', 'mbrl', 'robodesk', 'b', 'earthmove', 'figure', 'reward', 'model', 'inability', 'predict', 'sparse', 'reward', 'complete', 'task', 'lead', 'poor', 'task', 'performance', 'robodesk', 'agent', 'get', 'stick', 'learn', 'first', 'task', 'unable', 'learn', 'perform', 'subsequent', 'task', 'b', 'earthmove', 'policy', 'often', 'fail', 'dump', 'rock', 'accurately', 'dumptruck', 'learn', 'curve', 'average', 'seed', 'dreamsmooth', 'improve', 'mbrl', 'reward', 'smooth', 'address', 'reward', 'prediction', 'problem', 'propose', 'simple', 'yet', 'effective', 'solution', 'dreamsmooth', 'relax', 'require', 'ment', 'model', 'predict', 'sparse', 'reward', 'exact', 'timestep', 'perform', 'ral', 'smoothing', 'allow', 'reward', 'model', 'predict', 'reward', 'ground', 'truth', 'timestep', 'make', 'learn', 'ier', 'especially', 'reward', 'ambiguous', 'sparse', 'gaussian', 'uniform', 'c', 'figure', 'reward', 'smooth', 'sparse', 'reward', 'σ', 'δ', 'smooth', 'hyperparameter', 'specifically', 'dreamsmooth', 'apply', 'temporal', 'smoothing', 'reward', 'collect', 'new', 'episode', 'dreamsmooth', 'work', 'smoothing', 'function', 'preserve', 'sum', 'reward', 'rt−l', 'fi', 'rclip', 'ti0', 'i−l', 'fi', 'l', 'denote', 'episode', 'smooth', 'horizon', 'respectively', 'simplicity', 'omit', 'discount', 'factor', 'equation', 'full', 'equation', 'find', 'equation', 'episode', 'smoothed', 'reward', 'store', 'replay', 'buffer', 'use', 'train', 'reward', 'model', 'agent', 'learn', 'smoothed', 'reward', 'ever', 'see', 'original', 'reward', 'smoothed', 'reward', 'ease', 'reward', 'prediction', 'allow', 'model', 'predict', 'reward', 'several', 'timestep', 'early', 'later', 'incur', 'large', 'loss', 'paper', 'investigate', 'popular', 'smoothing', 'function', 'gaussian', 'uniform', 'exponential', 'move', 'average', 'smoothing', 'illustrate', 'figure', 'main', 'motivation', 'smoothing', 'make', 'easy', 'learn', 'reward', 'model', 'note', 'reward', 'smooth', 'case', 'preserve', 'optimality', 'optimal', 'policy', 'smoothed', 'reward', 'l', 'e', 'c', 'environment', 'step', '×10⁶', 'e', 'k', 'r', 'environment', 'step', 'r', 'w', 'e', 'r', 'orginal', 'σ', 'timestep', 'r', 'w', 'e', 'r', 'orginal', 'δ', 'δ', 'timestep', 'r', 'w', 'e', 'r', 'orginal', 'α', 'timestep', 'also', 'optimal', 'original', 'reward', 'r', 'particular', 'provide', 'proof', 'optimality', 'smoothing', 'smoothing', 'function', 'fi', 'augment', 'pomdp', 'state', 'history', 'past', 'state', 'however', 'future', 'reward', 'use', 'smooth', 'gaussian', 'smooth', 'smoothed', 'reward', 'condition', 'policy', 'long', 'define', 'equivalent', 'pomdp', 'case', 'theoretical', 'guarantee', 'even', 'empirically', 'show', 'reward', 'model', 'adapt', 'prediction', 'change', 'policy', 'achieve', 'performance', 'improvement', 'implementation', 'dreamsmooth', 'extremely', 'simple', 'require', 'additional', 'line', 'code', 'exist', 'mbrl', 'algorithm', 'show', 'overhead', 'reward', 'smoothing', 'minimal', 'time', 'complexity', 'l', 'implementation', 'detail', 'find', 'collect', 'rollout', 'π', 'policy', 'replay', 'buffer', 'dreamsmooth', 'π', '∪', 'ot', 't1', 't1', '▷', 'line', 'need', 'add', 'experiment', 'paper', 'propose', 'simple', 'reward', 'smooth', 'method', 'dreamsmooth', 'facilitate', 'reward', 'prediction', 'modelbase', 'reinforcement', 'learning', 'mbrl', 'thus', 'improve', 'performance', 'exist', 'mbrl', 'method', 'experiment', 'aim', 'answer', 'follow', 'question', 'reward', 'smooth', 'improve', 'reward', 'prediction', 'well', 'reward', 'prediction', 'reward', 'smooth', 'lead', 'well', 'sample', 'efficiency', 'asymptotic', 'performance', 'mbrl', 'sparsereward', 'task', 'mbrl', 'reward', 'smoothing', 'also', 'work', 'common', 'densereward', 'task', 'task', 'evaluate', 'dreamsmooth', 'task', 'sparse', 'subtask', 'completion', 'reward', 'common', 'rl', 'benchmark', 'earthmove', 'use', 'image', 'observation', 'task', 'use', 'single', 'image', 'see', 'c', 'environment', 'detail', 'robodesk', 'use', 'modify', 'version', 'robodesk', 'sequence', 'nipulation', 'task', 'flat', 'block', 'upright', 'block', 'table', 'push', 'green', 'need', 'complete', 'order', 'figure', 'use', 'original', 'dense', 'reward', 'together', 'large', 'sparse', 'reward', 'task', 'complete', 'hand', 'hand', 'task', 'plappert', 'require', 'shadow', 'hand', 'rotate', 'block', 'hand', 'specific', 'orientation', 'extend', 'achieve', 'sequence', 'predefine', 'goal', 'orientation', 'order', 'addition', 'original', 'dense', 'reward', 'provide', 'large', 'sparse', 'reward', 'goal', '•', 'earthmove', 'earthmove', 'task', 'consist', 'wheel', 'loader', 'dump', 'truck', 'pile', 'rock', 'figure', 'agent', 'control', 'wheel', 'loader', 'pick', 'rock', 'pile', 'dump', 'dump', 'truck', 'large', 'sparse', 'reward', 'give', 'rock', 'pick', 'rock', 'dump', 'proportional', 'mass', 'addition', 'dense', 'reward', 'give', 'move', 'rock', 'dump', 'truck', 'environment', 'simulate', 'use', 'dynamic', 'physics', 'engine', 'robodesk', 'b', 'hand', 'c', 'earthmove', 'crafter', 'e', 'dmc', 'atari', 'figure', 'evaluate', 'dreamsmooth', 'task', 'sparse', 'subtask', 'completion', 'reward', 'ad', 'also', 'test', 'popular', 'benchmark', 'e', 'deepmind', 'control', 'suite', 'atari', 'robodesk', 'b', 'hand', 'c', 'earthmove', 'crafter', 'figure', 'visualize', 'ground', 'truth', 'reward', 'smoothed', 'reward', 'gaussian', 'smoothing', 'predict', 'reward', 'dreamerv3', 'train', 'smoothed', 'reward', 'evaluation', 'episode', 'contrast', 'figure', 'reward', 'model', 'reward', 'smoothing', 'capture', 'sparse', 'reward', 'crafter', 'crafter', 'hafner', 'minecraftlike', 'environment', 'agent', 'try', 'collect', 'place', 'craft', 'item', 'order', 'survive', 'achievement', 'environment', 'collect', 'water', 'mining', 'diamond', 'sparse', 'reward', 'obtain', 'achievement', 'first', 'time', 'small', 'reward', 'give', 'lose', 'health', 'point', 'gain', 'lose', 'dmc', 'benchmark', 'deepmind', 'control', 'continuous', 'control', 'task', 'tassa', 'atari', 'benchmark', 'atari', 'task', 'bellemare', 'step', 'improve', 'reward', 'prediction', 'reward', 'smooth', 'first', 'visualize', 'ground', 'truth', 'reward', 'smoothed', 'reward', 'gaussian', 'smoothing', 'ward', 'prediction', 'result', 'dreamerv3', 'train', 'dreamsmooth', 'figure', 'observe', 'ward', 'smoothing', 'lead', 'significant', 'improve', 'ment', 'reward', 'prediction', 'dreamsmooth', 'success', 'fully', 'predict', 'smoothed', 'sparse', 'ward', 'long', 'omit', 'vital', 'signal', 'policy', 'learning', 'planning', 'improvement', 'especially', 'notable', 'crafter', 'figure', 'measure', 'accuracy', 'ward', 'model', 'predict', 'reward', 'large', 'half', 'original', 'smoothed', 'reward', 'dream', 'erv3', 'dreamsmooth', 'respectively', 'exact', 'timestep', 'subtask', 'vanilla', 'dream', 'reward', 'model', 'baseline', 'miss', 'sparse', 'reward', 'predict', 'sparse', 'reward', 'accurately', 'subtask', 'result', 'figure', 'reward', 'prediction', 'rate', 'achieve', 'ment', 'crafter', 'task', 'never', 'achieve', 'method', 'reward', 'smooth', 'prediction', 'rate', 'well', 'task', 'compare', 'vanilla', 'dreamsmooth', 'backbone', 'also', 'dreamerv3', 'dreamsmooth', 'evaluate', 'gaussian', 'uniform', 'smooth', 'ground', 'truth', 'smooth', 'predict', 'timestep', 'yaxis', 'reward', '−200', 'baseline', 'smooth', 'gaussian', 'e', 'r', 'n', 'c', 'e', 'r', 'p', 'e', 'bie', 'alth', 'loss', 'ollect', 'drin', 'picka', 'ollect', 'sto', 'place', 'e', 'efe', 'efe', 'c', 'c', 'ble', 'robodesk', 'b', 'hand', 'c', 'earthmove', 'crafter', 'e', 'dmc', 'atari', 'figure', 'comparison', 'learn', 'curve', 'dreamsmooth', 'gaussian', 'uniform', 'dream', 'erv3', 'shaded', 'region', 'ad', 'show', 'maximum', 'minimum', 'seed', 'dmc', 'e', 'aggregate', 'result', 'task', 'respectively', 'display', 'standard', 'deviation', 'hyperparameter', 'dreamerv3', 'smoothing', 'function', 'find', 'show', 'figure', 'dreamsmoothgaussian', 'dreamsmoothuniform', 'significantly', 'improve', 'performance', 'well', 'sample', 'efficiency', 'dreamerv3', 'robodesk', 'hand', 'earthmoving', 'task', 'change', 'improved', 'reward', 'prediction', 'show', 'section', 'result', 'suggest', 'reward', 'prediction', 'major', 'bottleneck', 'mbrl', 'performance', 'smoothing', 'method', 'lead', 'improvement', 'dreamerv3', 'gaussian', 'smoothing', 'generally', 'perform', 'good', 'crafter', 'uniform', 'smooth', 'show', 'comparable', 'performance', 'well', 'performance', 'gaussian', 'uniform', 'smoothing', 'allow', 'predict', 'reward', 'early', 'later', 'smoothing', 'allow', 'predict', 'reward', 'later', 'improved', 'reward', 'prediction', 'accuracy', 'dreamsmoothgaussian', 'dreamsmooth', 'uniform', 'perform', 'bad', 'baseline', 'crafter', 'predict', 'task', 'reward', 'encourage', 'exploitation', 'less', 'exploration', 'investigation', 'tradeoff', 'promising', 'direction', 'future', 'work', 'moreover', 'observe', 'dmc', 'atari', 'benchmark', 'reward', 'prediction', 'particu', 'larly', 'challenge', 'technique', 'show', 'comparable', 'performance', 'unmodified', 'algorithm', 'see', 'figure', 'full', 'result', 'suggest', 'reward', 'smoothing', 'apply', 'generally', 'hinder', 'performance', 'environment', 'dreamsmoothgaussian', 'dreamerv3', 'l', 'e', 'c', 'environment', 'step', 'l', 'e', 'c', 'environment', 'step', 'e', 'k', 'r', 'r', 'e', 'environment', 'step', 'e', 'r', 'c', 'environment', 'step', 'e', 'r', 'c', 'environment', 'step', 'e', 'r', 'e', 'z', 'l', 'r', 'h', 'environment', 'step', 'figure', 'dreamsmooth', 'also', 'improve', 'performance', 'tdmpc', 'hand', 'task', 'vanilla', 'tdmpc', 'unable', 'consistently', 'solve', 'first', 'task', 'even', 'proprioceptive', 'state', 'observation', 'however', 'tdmpc', 'dreamsmooth', 'learn', 'complete', 'task', 'state', 'observation', 'also', 'pixel', 'observa', 'tion', 'suggest', 'dreamsmooth', 'useful', 'broad', 'range', 'mbrl', 'rithms', 'use', 'reward', 'model', 'demonstrate', 'hand', 'task', 'tdmpc', 'fail', 'sparsereward', 'task', 'ablation', 'study', 'hand', 'hand', 'state', 'figure', 'learn', 'curve', 'tdmpc', 'tdmpc', 'dreamsmooth', 'hand', 'task', 'shaded', 'gion', 'show', 'minimum', 'maximum', 'seed', 'datum', 'imbalance', 'possible', 'cause', 'poor', 'reward', 'prediction', 'datum', 'imbalance', 'sparse', 'reward', 'infrequent', 'sequence', 'taine', 'sparse', 'reward', 'rarely', 'sample', 'replay', 'buffer', 'reward', 'model', 'therefore', 'train', 'example', 'sparse', 'reward', 'tentially', 'lead', 'poor', 'prediction', 'test', 'hypothesis', 'conduct', 'experiment', 'oversample', 'probability', 'p', 'sample', 'sequence', 'agent', 'receive', 'sparse', 'reward', 'otherwise', 'sample', 'formly', 'sequence', 'buffer', 'show', 'figure', 'oversampling', 'perform', 'bet', 'ter', 'baseline', 'learn', 'slow', 'dreamsmooth', 'suggest', 'datum', 'imbalance', 'largely', 'contribute', 'difficulty', 'reward', 'prediction', 'factor', 'hinder', 'performance', 'furthermore', 'sample', 'method', 'require', 'domain', 'knowledge', 'reward', 'signal', 'oversample', 'dreamsmooth', 'agnostic', 'scale', 'frequency', 'sparse', 'reward', 'figure', 'use', 'oversampling', 'sequence', 'sparse', 'reward', 'p', 'perform', 'well', 'dreamerv3', 'robodesk', 'bad', 'dreamsmooth', 'gaussian', 'smooth', 'line', 'show', 'median', 'task', 'performance', 'seed', 'shaded', 'region', 'show', 'maximum', 'minimum', 'reward', 'model', 'size', 'sis', 'poor', 'reward', 'prediction', 'reward', 'model', 'enough', 'pacity', 'capture', 'sparse', 'reward', 'test', 'hypothesis', 'increase', 'size', 'reward', 'model', 'layer', 'unit', 'layere', 'unit', 'layer', 'unit', 'keep', 'rest', 'world', 'model', 'observe', 'fig', 'ure', 'smooth', 'change', 'reward', 'model', 'size', 'negligible', 'impact', 'performance', 'dreamsmooth', 'perform', 'reward', 'model', 'size', 'test', 'indicate', 'reward', 'prediction', 'problem', 'simply', 'cause', 'cient', 'model', 'capacity', 'robodesk', 'b', 'hand', 'figure', 'simply', 'increase', 'reward', 'model', 'size', 'negligible', 'impact', 'performance', 'dreamsmooth', 'use', 'layer', 'unit', 'dreamerv3', 'use', 'layer', 'unit', 'dreamerv31280', 'use', 'layer', 'unit', 'figure', 'analyze', 'impact', 'smooth', 'parameter', 'smoothing', 'parameter', 'gaussian', 'respectively', 'robodesk', 'hand', 'observe', 'dreamsmooth', 'insensitive', 'smooth', 'parameter', 'perform', 'well', 'wide', 'range', 'value', 'dreamsmoothgaussian', 'e', 'c', 'environment', 'step', 'l', 'e', 'c', 'environment', 'step', 'l', 'e', 'c', 'dreamsmooth', 'oversample', 'environment', 'step', 'dreamsmooth', 'l', 'e', 'c', 'environment', 'step', 'l', 'e', 'c', 'environment', 'step', 'gaussian', 'smoothing', 'robodesk', 'gaussian', 'smoothing', 'hand', 'c', 'uniform', 'smooth', 'robodesk', 'uniform', 'smoothing', 'hand', 'e', 'smoothing', 'robodesk', 'smoothing', 'hand', 'figure', 'parameter', 'sweep', 'smooth', 'parameter', 'δ', 'line', 'show', 'median', 'task', 'performance', 'seed', 'shaded', 'region', 'show', 'maximum', 'minimum', 'conclusion', 'paper', 'identify', 'reward', 'prediction', 'problem', 'mbrl', 'provide', 'simple', 'yet', 'effective', 'solution', 'reward', 'smooth', 'approach', 'dreamsmooth', 'demonstrate', 'superior', 'performance', 'sparse', 'reward', 'task', 'reward', 'prediction', 'trivial', 'mainly', 'partial', 'observability', 'stochasticity', 'environment', 'moreover', 'dreamsmooth', 'show', 'comparable', 'result', 'commonly', 'use', 'benchmark', 'dmc', 'atari', 'show', 'taskagnostic', 'nature', 'show', 'simple', 'reward', 'smooth', 'approach', 'mitigate', 'difficulty', 'reward', 'prediction', 'improved', 'reward', 'prediction', 'always', 'improve', 'task', 'performance', 'eg', 'crafter', 'predict', 'task', 'reward', 'encourage', 'exploitation', 'less', 'exploration', 'investigation', 'tradeoff', 'promising', 'direction', 'future', 'work', 'dreamsmoothgaussian', 'cid3', 'cid8', 'cid3', 'cid4', 'cid3', 'cid8', 'cid3', 'cid3', 'cid8', 'l', 'e', 'c', 'environment', 'step', 'l', 'e', 'c', 'environment', 'step', '×10⁶', 'cid3', 'cid8', 'cid3', 'cid3', 'cid8', 'cid3', 'cid8', 'cid3', 'cid4', 'l', 'e', 'c', 'environment', 'step', 'l', 'e', 'c', 'environment', 'step', 'dreamsmoothema', 'cid10', 'cid3', 'cid4', 'cid10', 'cid3', 'cid4', 'cid8', 'cid17', 'cid10', 'cid3', 'cid4', 'l', 'e', 'c', 'environment', 'step', 'l', 'e', 'c', 'environment', 'step', 'acknowledgment', 'work', 'support', 'part', 'bair', 'industrial', 'consortium', 'logistic', 'robotic', 'like', 'thank', 'member', 'robot', 'learn', 'lab', 'insightful', 'feedback', 'reference', 'dynamic', 'url', 'https', 'modelbase', 'offline', 'planning', 'international', 'ence', 'learn', 'representation', 'url', 'https', 'openreviewnetforum', 'omnb1g5xzd4', 'mohammad', 'saffar', 'model', 'pixel', 'reward', 'evaluate', 'design', 'tradeoff', 'visual', 'modelbase', 'reinforcement', 'learning', 'arxiv', 'preprint', 'bellemare', 'veness', 'bowl', 'arcade', 'learn', 'environment', 'evaluation', 'platform', 'general', 'agent', 'artificial', 'intelligence', 'research', 'dreamerpro', 'reconstructionfree', 'modelbase', 'reinforce', 'ment', 'learn', 'prototypical', 'representation', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'park', 'face', 'world', 'model', 'backbone', 'rnns', 'transformer', 'arxiv', 'preprint', 'christopher', 'fiorillo', 'newsome', 'wolfram', 'schultz', 'temporal', 'precision', 'reward', 'prediction', 'dopamine', 'neuron', 'nature', 'neuroscience', 'schmidhuber', 'world', 'model', 'arxiv', 'preprint', 'hafner', 'benchmarke', 'spectrum', 'agent', 'capability', 'international', 'conference', 'learn', 'representation', 'dream', 'control', 'learn', 'behavior', 'latent', 'imagination', 'international', 'conference', 'learn', 'representation', 'master', 'atari', 'discrete', 'world', 'model', 'international', 'conference', 'learn', 'representation', 'master', 'diverse', 'domain', 'world', 'model', 'arxiv', 'preprint', 'temporal', 'difference', 'learn', 'model', 'predictive', 'control', 'international', 'conference', 'machine', 'learn', 'robodesk', 'multitask', 'rein', 'learning', 'benchmark', 'https', 'hunt', 'r', 'raymond', 'dissociable', 'reward', 'time', 'signal', 'human', 'midbrain', 'ventral', 'striatum', 'bahl', 'deepak', 'pathak', 'structured', 'world', 'model', 'human', 'video', 'robotic', 'science', 'system', 'policy', 'invariance', 'reward', 'transformation', 'theory', 'application', 'reward', 'shape', 'international', 'conference', 'machine', 'learn', 'volume', 'pp', 'taniguchi', 'dream', 'modelbase', 'reinforcement', 'learning', 'latent', 'imagination', 'reconstruction', 'international', 'conference', 'robotic', 'automation', 'pp', 'doi', 'matthia', 'plappert', 'bowen', 'schneider', 'welinder', 'reinforcement', 'learn', 'challenging', 'robotic', 'environment', 'request', 'research', 'arxiv', 'preprint', 'schrittwieser', 'ioannis', 'laurent', 'sifre', 'schmitt', 'hassabis', 'thore', 'et', 'master', 'atari', 'go', 'chess', 'shogi', 'plan', 'learned', 'model', 'nature', 'younggyo', 'seo', 'pieter', 'abbeel', 'mask', 'world', 'model', 'visual', 'control', 'conference', 'robot', 'learn', 'seo', 'pieter', 'abbeel', 'multiview', 'mask', 'world', 'model', 'visual', 'robotic', 'manipulation', 'international', 'conference', 'machine', 'learn', 'skillbase', 'modelbase', 'reinforcement', 'learning', 'conference', 'robot', 'learn', 'aja', 'maddison', 'schrittwieser', 'ioannis', 'et', 'master', 'game', 'go', 'deep', 'neural', 'network', 'tree', 'search', 'nature', 'schrittwieser', 'ioannis', 'antonoglou', 'aja', 'et', 'master', 'game', 'go', 'human', 'knowledge', 'nature', 'schrittwieser', 'ioannis', 'lanctot', 'laurent', 'sifre', 'thore', 'et', 'general', 'reinforcement', 'learning', 'master', 'chess', 'go', 'selfplay', 'science', 'gautam', 'structured', 'world', 'belief', 'reinforcement', 'learning', 'pomdp', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'sutton', 'reinforcement', 'learn', 'introduction', 'mit', 'press', 'yuval', 'tassa', 'martin', 'riedmiller', 'deepmind', 'control', 'suite', 'arxiv', 'preprint', 'proof', 'let', 'p', 'r', 'give', 'mdp', 'loss', 'generality', 'assume', 'augment', 'form', 'mdp', 'state', 'include', 'entire', 'history', 'state', 'thus', 'reward', 'function', 'r', '˜r', 'access', 'previous', 'state', 'theorem', 'optimal', 'policy', '˜π∗', 'mdp', 'reward', 'smooth', 'past', 'reward', 'smoothing', 'p', 'also', 'optimal', 'original', 'mdp', 'i−l', 'fi', 'i−l', 'fi', 'proof', 'use', 'theorem', 'reward', 'shape', 'guarantee', 'optimal', 'policy', 'introduce', 'modify', 'reward', 'function', 'represent', 'form', 'r', 'st1', '−φ', 'potential', 'function', 'φ', 'new', 'reward', 'function', 'yield', 'optimal', 'policy', 'original', 'reward', 'function', 'r', 'let', 'potential', 'function', 'reward', 'smooth', 'φ', '−1', 'i−l', 'i−l', 'ji1', 'reward', 'shape', 'term', 'represent', 'difference', 'potential', 'function', 'st1', 'follow', 'st1', 'i−l', 'fi', 'st1', 'i−l', 'fi', 'hence', 'follow', 'reward', 'shape', 'smoothing', 'guarantee', 'optimal', 'policy', 'original', 'mdp', 'however', 'theorem', 'apply', 'smooth', 'function', 'require', 'access', 'future', 'reward', 'gaussian', 'smoothing', 'gaussian', 'smooth', 'smoothed', 'reward', 'function', 'require', 'future', 'reward', 'condition', 'current', 'policy', 'reward', 'model', 'case', 'theoretical', 'guarantee', 'experiment', 'empirically', 'show', 'reward', 'model', 'adapt', 'prediction', 'change', 'policy', 'thus', 'improve', 'mbrl', 'instead', 'intuitively', 'explain', 'optimal', 'policy', 'reward', 'smooth', 'even', 'reward', 'function', 'hoc', 'define', 'mdps', 'also', 'optimal', 'original', 'reward', 'function', 'theorem', 'optimal', 'policy', '˜π∗', 'smoothed', 'reward', 'function', 'also', 'optimal', 'original', 'reward', 'function', 'r', 'l', 'γclip', '−t', '−t', 'fi', 'sclip', 'ti0', 'l', 'i−l', 'fi', 'proof', 'first', 'show', 'discount', 'sum', 'original', 'reward', 'smooth', 'reward', 'trajectory', 'l', 'γclip', '−t', '−t', 'fi', 'sclip', 'ti0', 'equation', 'γtr', 'cid88', 'γtr', 'l', 'cid88', 'fi', 'i−l', 'γtr', 'cid88', 'l', 'i−l', 'fi', 'let', 'optimal', 'policy', 'smoothed', 'reward', '˜π∗', 'assume', '˜π∗', 'optimal', 'original', 'reward', 'r', 's0', 'e', 'cid104', 'cid88', 'e', 'cid104', 'γt', 'however', 'e', 'cid104', 'cid88', 'e', 'e', 'cid104', 'cid88', 'cid104', 'cid88', 'equation', 'equation', 'contradict', '˜π∗', 'optimal', 'therefore', 'optimal', 'policy', '˜π∗', 'guarantee', 'optimality', 'r', 'b', 'implementation', 'detail', 'model', 'train', 'rtx', 'rtx', 'rtx', 'gpu', 'experiment', 'take', 'hour', 'robodesk', 'hour', 'hand', 'hour', 'earthmove', 'hour', 'crafter', 'hour', 'atari', 'dmc', 'task', 'dreamsmooth', 'smoothing', 'function', 'gaussian', 'smoothing', 'follow', 'gaussian', 'distribution', 'fi', 'l', 'e', 'implement', 'use', 'normalization', 'constant', 'scipyndimagegaussianfilter1d', 'reward', 'mode', 'near', 'uniform', 'smoothing', 'distribute', 'reward', 'equally', 'consecutive', 'timestep', 'δ', 'δ', 'δ', 'fi', 'cid104', 'implement', 'use', 'scipyndimageconvolve', 'reward', 'filter', 'mode', 'near', 'smoothing', 'use', 'follow', 'smoothing', 'function', 'fi', '≤', 'implement', 'perform', 'following', 'timestep', 'reward', 'alpha', 'reward', 'alpha', 'reward', 'b2', 'modelbase', 'reinforcement', 'learning', 'backbone', 'hyperparameter', 'dreamerv3', 'experiment', 'show', 'table', 'tdmpc', 'table', 'table', 'dreamerv3', 'hyperparameter', 'episode', 'length', 'measure', 'environment', 'step', 'number', 'agent', 'step', 'multiply', 'action', 'repeat', 'model', 'size', 'list', 'hafner', 'also', 'refer', 'hyperparameter', 'environment', 'action', 'repeat', 'episode', 'length', 'train', 'ratio', 'model', 'size', 'earthmove', 'robodesk', 'hand', 'crafter', 'dmc', 'atari', 'variable', 'variable', 'l', 'δ', 'table', 'tdmpc', 'hyperparameter', 'specify', 'use', 'default', 'hyperparameter', 'environment', 'latent', 'dimension', 'channel', 'plan', 'iteration', 'σ', 'c', 'environment', 'detail', 'c1', 'robodesk', 'environment', 'use', 'modify', 'version', 'robodesk', 'sequence', 'manipulation', 'task', 'flat', 'block', 'upright', 'block', 'table', 'push', 'green', 'need', 'complete', 'order', 'figure', 'show', 'image', 'agent', 'successfully', 'complete', 'task', 'original', 'environment', 'dense', 'reward', 'base', 'euclidean', 'distance', 'object', 'target', 'additional', 'term', 'encourage', 'arm', 'reach', 'object', 'typically', 'range', 'timestep', 'use', 'dense', 'reward', 'together', 'large', 'sparse', 'reward', 'task', 'complete', 'put', 'flat', 'block', 'push', 'upright', 'block', 'table', 'press', 'figure', 'subtask', 'robodesk', 'hand', 'environment', 'modify', 'shadow', 'hand', 'environment', 'agent', 'require', 'achieve', 'sequence', 'predefine', 'goal', 'orientation', 'order', 'first', 'goal', 'show', 'figure', 'subsequent', 'goal', 'repeat', 'first', 'goal', 'orientation', 'choose', 'agent', 'rotate', 'cube', 'zaxis', 'require', 'agent', 'match', 'cube', 'rotation', 'goal', 'position', 'original', 'environment', 'dense', 'reward', 'compute', 'use', 'r', '10x', 'euclidean', 'distance', 'fix', 'position', 'angular', 'difference', 'target', 'orientation', 'addition', 'dense', 'reward', 'provide', 'large', 'sparse', 'reward', 'goal', 'successfully', 'achieve', 'agent', 'goal', 'b', 'goal', 'c', 'goal', 'figure', 'subtask', 'hand', 'c3', 'earthmove', 'environment', 'earthmove', 'environment', 'consist', 'wheel', 'loader', 'dump', 'truck', 'pile', 'dirt', 'rock', 'top', 'pile', 'en', 'vironment', 'simulate', 'use', 'realistic', 'dynamic', 'physics', 'engine', 'agent', 'control', 'wheel', 'loader', 'pick', 'rock', 'dump', 'dump', 'truck', 'start', 'position', 'dirt', 'pile', 'wheel', 'loader', 'dump', 'truck', 'randomize', 'initial', 'orientation', 'dirt', 'pile', 'wheel', 'loader', 'agent', 'observation', 'consist', 'component', 'wideangle', 'egocentric', 'camera', 'mount', 'cabin', 'allow', 'navigation', 'camera', 'mount', 'bucket', 'observe', 'interaction', 'rock', 'proprioceptive', 'observation', 'position', 'velocity', 'speed', 'force', 'actuator', 'use', '×', 'image', 'camera', 'proprioceptive', 'observation', 'dimension', 'figure', 'agent', 'use', 'camera', 'mount', 'cabin', 'leave', 'navigation', 'mount', 'bucket', 'right', 'observe', 'interaction', 'rock', 'terrain', 'action', 'space', 'dimension', 'drive', 'steer', 'loader', 'dimension', 'move', 'tilt', 'bucket', 'reward', 'consist', 'large', 'sparse', 'reward', 'rock', 'pick', 'dump', 'dense', 'reward', 'move', 'rock', 'dumptruck', 'total', 'reward', 'timestep', 'compute', 'use', 'equation', 'λdump', 'load', 'cid125', 'load', 'load', 'cid125', 'dump', 'λload', 'dump', 'dense', 'reward', 'reward', 'mdump', 'mload', 'rock', 'masse', 'dumptruck', 'bucket', 'respectively', 'distance', 'shovel', 'point', 'dumptruck', 'constant', 'dmc', 'atari', 'benchmarking', 'result', 'figure', 'full', 'learning', 'curve', 'dmc', 'atari', 'benchmark', 'dreamsmoothgaussian', 'e', 'r', 'c', 'environment', 'step', 'reacher', 'hard', 'e', 'r', 'c', 'environment', 'step', '×10⁶', 'cartpole', 'sparse', 'e', 'r', 'c', 'environment', 'step', 'walker', 'run', 'e', 'r', 'c', 'environment', 'step', 'finger', 'turn', 'hard', 'e', 'r', 'c', 'environment', 'step', '×10⁶', 'quadrupe', 'run', 'e', 'r', 'c', 'environment', 'step', 'run', 'e', 'r', 'c', 'environment', 'step', 'pong', 'r', 'c', 'environment', 'step', 'breakout', 'e', 'r', 'c', 'environment', 'step', 'freeway', 'e', 'r', '−1', 'environment', 'step', 'assault', 'c', 'environment', 'step', 'seaqu', 'e', 'r', 'c', 'environment', 'step', 'hero', 'r', 'c', 'environment', 'step']",
RETSim: Resilient and Efficient Text Similarity,"[{'href': 'http://arxiv.org/abs/2311.17264v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2311.17264v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-11-28 22:54:33,"3
2
0
2

v
o
N
7

]

G
L
.
s
c
[

1
v
1
1
7
3
0
.
1
1
3
2
:
v
i
X
r
a

MITIGATING ESTIMATION ERRORS BY TWIN TD-
REGULARIZED ACTOR AND CRITIC FOR DEEP REIN-
FORCEMENT LEARNING

Junmin Zhong
Arizona State University

Ruofan Wu
Arizona State University

Jennie Si ∗
Arizona State University

ABSTRACT

We address the issue of estimation bias in deep reinforcement learning (DRL) by
introducing solution mechanisms that include a new, twin TD-regularized actor-
critic (TDR) method. It aims at reducing both over and under estimation errors.
With TDR and by combining good DRL improvements, such as distributional
learning and long N -step surrogate stage reward (LNSS) method, we show that
our new TDR-based actor-critic learning has enabled DRL methods to outper-
form their respective baselines in challenging environments in DeepMind Control
Suite. Furthermore, they elevate TD3 and SAC respectively to a level of perfor-
mance comparable to that of D4PG (the current SOTA), and they also improve the
performance of D4PG to a new SOTA level measured by mean reward, conver-
gence speed, learning success rate, and learning variance.

1

INTRODUCTION

Reinforcement learning (RL) has been developed for decades to provide a mathematical formal-
ism for learning-based control. Recently, significant progress has been made to attain excellent
results for a wide range of high-dimensional and continuous state-action space problems especially
in robotics applications, such as robot manipulation (Andrychowicz et al., 2017), and human-robotic
interaction (Liu et al., 2022; Wu et al., 2022).

However, the fundamental issue of estimation error associated with actor-critic RL (Van Hasselt
et al., 2016; Duan et al., 2021) still poses great challenge. Overestimation due to, for example, using
the max operator in updates has been identified and studied (Thrun & Schwartz, 1993; Duan et al.,
2021). To reduce it, most efforts have focused on attaining more accurate and stable critic networks.
TD3 (Fujimoto et al., 2018) applies clipped double Q-learning by taking the minimum between the
two Q estimates. SAC (Haarnoja et al., 2018) utilizes the double Q network and incorporates entropy
regularization in the critic objective function to ensure more exploratory behavior to help alleviate
the overestimation problem. However, directly taking the minimum value of the target networks
such as that in TD3 and SAC has been reported to result in an underestimation bias (Fujimoto et al.,
2018).

Evaluations have revealed multiple roles of over and under estimation errors in learning. On one
hand, overestimation may not always be harmful (Lan et al., 2020) as it is considered playing a
role of encouraging exploration by overestimated actions. Along this line, underestimation bias
may discourage exploration. If the overestimation bias occurs in a high-value region containing the
optimal policy, then encouraging exploration is a good thing (Hailu & Sommer, 1999). On the other
hand, overestimation bias may also cause an agent to overly explore a low-value region. This may
lead to a suboptimal policy. Accordingly, an underestimation bias may discourage an agent from
exploring high-value regions or avoiding low-value regions. All things considered, if estimation
errors are left unchecked, they may accumulate to negatively impact policy updates as suboptimal
actions may be highly rated by a suboptimal critic, reinforcing the suboptimal action in the next
policy update (Fujimoto et al., 2018). Aside from the anecdotal evidence on the roles of over and
under estimation, how to mitigate both of them in a principled way remains an open issue.

∗si@asu.edu

1

 
 
 
 
 
 
While several methods and evaluations have been performed and shown promising, a major tool has
been mostly left out thus far. That is, it is still not clear how, and if it is possible, to further reduce
estimation errors by considering the actor given the interplay between the actor and the critic. Only
a handful of approaches have been examined. As shown in (Wu et al., 2023) with demonstrated
performance improvement, PAAC uses a phased actor to account for both a Q value and a TD error
in actor update. A double actor idea was proposed and evaluated in (Lyu et al., 2022). It takes the
minimum value estimate associated with one of the two actor networks. However, directly using the
minimum of the estimated values was shown resulting in an underestimation error, similar to that in
TD3. Other methods, such as Entropy (Haarnoja et al., 2018; Fox et al., 2015), mutual-information
(MI) (Leibfried & Grau-Moya, 2020), and Kullback-Leibler (KL) (Vieillard et al., 2020; Rudner
et al., 2021) regularization, are also used to enhance policy exploration, robustness, and stability.
TD-regularized actor-critic (Parisi et al., 2019) regularizes the actor only aiming to enhance the
stability of the actor learning by applying a TD error (same as that in online critic updates) as a
regularization term in actor updates. However, none of these methods have shown how regularization
in actor may help reduce estimation error in the critic.

In this paper, we propose a new, TD-regularized (TDR) learning mechanism which includes TD-
regularized double critic networks and TD-regularized actor network. This new architecture has
several properties that make it ideal for the enhancements we consider. For the TD-regularized
double critic network, instead of directly selecting the minimum value from twin target networks,
we select the target based on the minimum TD error, which then addresses not only overestimation
but underestimation problems. For the TD-regularized actor network, we formulate a new TD error
to regularize actor updates to avoid a misleading critic. This regularization term helps further reduce
the estimation error in critic updates. Additionally, we apply TDR combined with distributional
RL (Barth-Maron et al., 2018; Bellemare et al., 2017) and LNSS reward estimation method (Zhong
et al., 2022) to further improve learning stability and performance.

2 RELATED WORK

To shed light on the novelty of the TDR method, here we discuss double critic networks and TD
error-based actor learning to provide a backdrop. We include reviews of distributional RL (Barth-
Maron et al., 2018; Bellemare et al., 2017) and long-N -step surrogate stage (LNSS) method (Zhong
et al., 2022) in Appendix A.

Double critic networks have been used in both RL (Hasselt, 2010; Zhang et al., 2017; Weng et al.,
2020) and DRL (Fujimoto et al., 2018; Haarnoja et al., 2018; Van Hasselt et al., 2016). Double Q
learning (Hasselt, 2010; Van Hasselt et al., 2016) was the first to show reduction of overestimation
bias. TD3 (Fujimoto et al., 2018) and SAC (Haarnoja et al., 2018) also were shown effective by
applying clipped double Q-learning by using the minimum between the two Q estimates. However,
these methods have induced an underestimation bias problem. (Hasselt, 2010; Zhang et al., 2017;
Fujimoto et al., 2018). Consequently, weighted double Q learning (Zhang et al., 2017) was proposed
to deal with both overestimation and underestimation biases. However, this method has not been
tested in DRL context and therefore, it lacks a systematic approach to designing the weighting
function.

TD error-based actor learning is expected to be effective in reducing overestimation error since it is
a consistent estimate of the advantage function with lower variance, and it discriminates feedback
instead of directly using Q estimates. Some actor-critic variants (Crites & Barto, 1994; Bhatnagar
et al., 2007) update the actor based on the sign of a TD error with a positive error preferred in
policy updates. However, TD error only measures the discrepancy between the predicted value
and the target value, which may not guide exploration effectively, and using TD error alone in actor
update may discourage exploration and cause slow learning, especially in high-dimensional complex
problems. TD-regularized actor-critic (Parisi et al., 2019) enhanced the stability of the actor update
by using the same TD error (as that in online critic update) as a regularization term. However, such
use of TD error may not sufficiently evaluate the critic update because it only uses the temporal
difference between target and online Q estimates. Additionally, the time-varying regularization
coefficient was shown leading to poor convergence (Chen et al., 2017). Note also that the TD-
regularized actor-critic only considered TD-regularized actor but not the critic.

2

Contributions. 1) We introduce a novel TDR mechanism that includes TD-regularized double critic
networks and TD-regularized actor network. 2) Extensive experiments using DMC benchmarks
show that TDR enables SOTA performance (measureed by learning speed, success rate, variance,
and converged reward) across a wide variety of control tasks, such as locomotion, classical control,
and tasks with sparse rewards. 3) We also provide qualitative analysis to show that each component
of TDR contributes to mitigating both over and under estimation errors.

3 METHOD

3.1 DOUBLE Q IN ACTOR-CRITIC METHOD

For a general double Q actor-critic method (Fujimoto et al., 2018; Haarnoja et al., 2018). The policy
(πϕ) is called an actor and the state-action value function (Qθ(sk, ak)) is called a critic where both
the actor and the critic are estimated by deep neural networks with parameters ϕ and θ, respectively.

First, consider a policy π that is evaluated by the state-action value function below:

Qπ(sk, ak) = E[Rk|sk, ak],

(1)

where Rk = (cid:80)∞
t=k γt−krt, sk ∼ p (· | sk−1, ak−1), ak = πϕ (sk), and γ ∈ (0, 1). Most actor-
critic methods are based on temporal difference (TD) learning (Sutton & Barto, 2018) that updates
Q estimates by minimizing the TD error, which is obtained from the the difference between a target
and a critic estimated value.

Next, consider typical double Q methods which entail twin Q networks denoted as Qθ1 and Qθ2.
The respective twin target networks are denoted as Qθ′
. In the upcoming discussions, we
also use θ to denote parameters in both Q networks, i.e., θ={θ1, θ2}. The target value yk is the lesser
of the two target values,

and Qθ′

1

2

yk = rk + γ min
ζ=1,2

Qθ′

ζ

(sk+1, πϕ′(sk+1)),

(2)

where by taking the minimum of the two target values, it aims to curtail overestimation of Q value
frequently experienced by using a single target. Thus the critic value Qθ is updated by minimizing
the loss function (L (θ)) with respect to the critic weights θ:

L (θ) = Es∼pπ,a∼π[

(cid:88)

(yk − Qθζ (sk, ak))2].

(3)

ζ=1,2

The actor weights can be updated by the deterministic policy gradient algorithm below (Silver et al.,
2014), where by convention (Fujimoto et al., 2018; Haarnoja et al., 2018), Qθ1 is used to update the
actor weights.

∇ϕJ(ϕ) = Es∼pπϕ

(cid:104)

(cid:105)
∇aQθ1(sk, ak)|a=πϕ(s) ∇ϕπϕ(s)

.

(4)

Figure 1: Twin TD-regularized Actor-Critic (TDR) Architecture

3

3.2 TWIN TD-REGULARIZED ACTOR-CRITIC (TDR) ARCHITECTURE

Figure 1 depicts our TDR-based solution mechanisms, which include twin Q networks as in TD3
(Fujimoto et al., 2018) and SAC (Haarnoja et al., 2018), and an actor network. The TDR-based
actor and critic updates are different from currently existing methods. In the following, we show
how the new TDR selects target value yk different from Equation (2) as used in SAC and TD3, and
how that helps reduce both overestimation and underestimation errors. We also show how the new
TD-regularized actor helps further reduce the estimation bias in the critic. Our TDR-based solutions
in Figure 1 include two additional good improvements: distributional learning as in D4PG and long
N -step surrogate stage (LNSS) method (Zhong et al., 2022) as described in Appendix A.

3.3 TD-REGULARIZED DOUBLE Q NETWORKS

To overcome overestimation, TD3 (Fujimoto et al., 2018) and SAC (Haarnoja et al., 2018) train
their critic networks to minimize the loss function in Equation (3) where the target value yk is from
Equation (2). While this helps reduce overestimation error, it promotes a new problem of underes-
timation, which usually occurs during the early stage of learning, or when subjected to corrupted
reward feedback or inaccurate states.

Our TDR method aims at minimizing the same loss function as in Equation (3), but with a different
target value yk. Instead of directly choosing the lesser from the two target values as in Equation (2),
we use the TD errors of the two target networks to set the target value. First, the two TD errors from
the respective target networks are determined from:
δ′
1 = rk + γQθ′
δ′
2 = rk + γQθ′

(sk+1, πϕ′(sk+1)) − Qθ′

(sk+1, πϕ′(sk+1)) − Qθ′

(sk, ak),

(sk, ak).

(5)

(6)

1

1

2

2

The target value for TDR is then selected from the following:

1

yk =

(cid:26) rk + γQθ′
rk + γQθ′

(sk+1, πϕ′(sk+1))
(sk+1, πϕ′(sk+1))
Note from Equation (7) that TDR always uses a target value associated with a smaller target TD
value (regardless of the error sign) between the two. As the ultimate objective of a target network is
to converge to Qπ, such choice by TDR pushes the critic via Equation (3) toward reaching the target
no matter the estimation error is from above or below, but with a smaller TD value. Thus, TDR is
naturally positioned to address both overesdiation and underestimation errors.

1| ≤ |δ′
1| > |δ′

if |δ′
if |δ′

2|,
2|.

(7)

2

3.4 TD-REGULARIZED ACTOR NETWORK

Our TD-regularized actor network directly penalizes the actor’s learning objective whenever there
is a critic estimation error. The estimation error ∆i+1 of the first critic (Qθ1 chosen by convention
of double Q-based actor-critic methods) is determined from the following:

∆i+1 = Qθi+1

1

(sk, ak) − (rk + γQθi+1

1

(sk+1, πϕ(sk+1))),

(8)

where i + 1 represents the iteration number during critic update. Then the actor can be updated in
the direction of maximizing Q while keeping the TD error small,
(cid:20)

(cid:21)

∇ϕJ(ϕ) = Es∼pπϕ

∇a(Qθi+1

1

(cid:12)
(sk, ak) − ρ(∆i+1))
(cid:12)
(cid:12)a=πϕ(s)

∇ϕπϕ(s)

.

(9)

where ρ ∈ (0, 1) is the regularization coefficient to balance the role of TD error in the actor learning
objective. Thus, we expect the TD-regularized actor to help further reduce estimation error in the
critic. With TDR actor and cirtic working together hand-in-hand, TDR is positioned to help avoid
bad policy updates due to a misleading Q value estimate.

Remark 1. There are a few key differences between TDR and TD-regularized Actor Network
(Parisi et al., 2019). 1) In Equation (8), they use the target critic Qθi′
(sk+1, πϕ(sk+1)) to construct
TD error, the same as in critic updates. This TD error evaluates the temporal difference between
target and online Q estimates. To more accurately evaluate critic estimations, we construct the TD
error by only using online critics which directly affects actor updates. 2) Their TD error does not
sufficiently evaluate how the critic updates. Instead in Equation (8), we use the updated critic (θi+1
)
to construct the TD error to directly measure critic estimation.

1

1

4

4 MITIGATING ESTIMATION BIAS BY TDR

Let Qπ be the true Q value obtained by following the current target policy π, and let Qθ be the
estimated value using neural networks. Let Ψk
θ be a random estimation bias. Then for state-action
pairs (sk, ak). we have,

Qθ(sk, ak) = Qπ(sk, ak) + Ψk
θ .
(10)
The same holds for the target networks, i.e., when θ is replaced by θ′ in the above equation. An
overestimation problem refers to when the estimation bias E[Ψk
θ ] > 0, and an underestimation
problem when the estimation bias E[Ψk

θ ] < 0.

4.1 MITIGATING ESTIMATION BIAS USING TD-REGULARIZED DOUBLE CRITIC NETWORKS

Theorem 1. Let Qπ be the true Q value following the current target policy π, and Qθ′
and Qθ′
be the target network estimates using double Q neural networks. We assume that there exists a
step random estimation bias ψk
(i.e., estimation bias at the kth stage), and that it is independent of
θ′
ζ
(sk, ak) with mean E[ψk
ζ, µ′
] = µ′
ζ < ∞, for all k, and ζ = 1, 2. Additionally, let δYk denote
θ′
ζ
the target value estimation error. Accordingly, we denote this error for TDR as δY T DR
, and DQ as
δY DQ
k

. We then have the following,

k

1

2

Where E[δY T DR

k

] = E[Qπ − yT DR

k

]| ≤ |E[δY DQ

k

|E[δY T DR
k
], and E[δY DQ

k

] = E[Qπ − yDQ

k

].

]|,

(11)

Proof. The proof of Theorem 1 is provided in Appendix B

Remark 2. By selecting a target value with less TD error, our TD-regularized double critic networks
mitigate both overestimation and underestimation errors. However, vanilla double Q methods usu-
ally push the target toward the lower value no matter the estimation error is over or under. Although
this estimation error may not be detrimental as they may be small at each update, the presence of
unchecked underestimation bias raises two concerns. Firstly, if there is no sufficient reward feed-
back from the environment, (e.g., for a noisy reward or sparse reward), underestimation bias may
not get a chance to make corrections and may develop into a more significant bias over several up-
dates. Secondly, this inaccurate value estimate may lead to poor policy updates in which suboptimal
actions might be highly rated by the suboptimal critic, reinforcing the suboptimal action in the next
policy update.

4.2 ADDRESSING A MISGUIDING CRITIC IN POLICY UPDATES USING TD-REGULARIZED

ACTOR

Theorem 2. Let Qπ denote the true Q value following the current target policy π, Qθ1 be the
estimated value. We assume that there exists a step random estimation bias ψk
that is independent
θ1
of (sk, ak) with mean E[ψk
] = µ1, µ1 < ∞, for all k. We assume the policy is updated based
θ1
on critic Qθ1 using the deterministic policy gradient (DPG) as in Equation (4). Let δϕk denote the
change in actor parameter ϕ updates at stage k. Accordingly, we denote this change for TDR as
, and true change without any approximation error in Q as δϕtrue
δϕT DR
.
k
k
We then have the following,

, vanilla DPG as δϕDP G

k

(cid:26) E[δϕtrue
k
E[δϕtrue
k

] ≥ E[δϕT DR
] ≤ E[δϕT DR

k

k

] ≥ E[δϕDP G
] ≤ E[δϕDP G

k

k

]
]

if E[Ψk
θ1
if E[Ψk
θ1

] < 0,
] ≥ 0.

(12)

Where δϕtrue
k
Appendix B

, δϕDP G
k

, and δϕT DR

k

are defined as Equation (55),(56), and (57) respectively in

Proof. The proof of Theorem 2 is provided in Appendix B.
Remark 3. Theorem 2, holds for ρ ∈ (0, 1). If the regularization factor ρ = 1
1−γ , from Equation
(59), we have E[Ψk
] = E[δϕT DR
]. By using TDR,
θ1
the actor will always update the same way as using the true value. While this is not realistic, the
following relationship still preserves |E[Ψk
]| to help ease the negative effect of
θ1
critic estimation bias.

− ρ∆] = 0 which implies that E[δϕtrue

− ρ∆]| ≤ |E[Ψk
θ1

k

k

5

4.3 MITIGATING CRITIC ESTIMATION ERROR BY TD-REGULARIZED ACTOR

Theorem 3. Suboptimal actor updates negatively affect the critic. Specifically, consider actor up-
dates as in Theorem 2, in the overestimation case, we have:

E[Qθ1(sk, πDP G(sk)] ≥ E[Qθ1 (sk, πT DR(sk))] ≥ E[Qπ(sk, πT rue(sk))],

and in the underestimation case,

E[Qθ1(sk, πDP G(sk)] ≤ E[Qθ1 (sk, πT DR(sk))] ≤ E[Qπ(sk, πT rue(sk))].

(13)

(14)

Proof The proof of Theorem 3 is provided in Appendix B.

Remark 4. For both cases, by using TD-regularized actors, it is expected to result in less estimation
bias in the critic.

5 EXPERIMENTS AND RESULTS

In this section, we provide a comprehensive evaluation of our TDR enabled actor-critic learning
methods based on three commonly used, well-behaved baseline algorithms including SAC, TD3
and D4PG. Additional evaluations are also provided for popular DRL algorithms such as DDPG and
PPO to provide a broader perspective on the effectiveness of TDR-based methods. All evaluations
are performed based on several benchmarks in Deepmind Control Suite (Tassa et al., 2018).

In reporting evaluation results, we use the following short-form names:

1) Base: the original DRL algorithms including SAC, TD3, D4PG, DDPG and PPO.

2) TDR-TD3: Applied TD regularized double critic (TD Critic) networks, TD regularized actor (TD
Actor) network, with regularization factor ρ = 0.7, and LNSS with N = 100.

3) TDR-SAC: Applied TD regularized double critic (TD Critic) networks, and LNSS with N = 100.

4) dTDR (TDR-D4PG): Applied TD regularized double critic (TD Critic) network, TD regularized
actor (TD Actor) network, with regularization factor ρ = 0.7, and LNSS with N = 100.

Our evaluations aim to quantitatively address the following questions:
Q1. How does TDR improve over Base and other common methods?
Q2. How does the performance of TDR methods compare to that of SOTA algorithms (D4PG)?
Q3. Is TDR method robust enough to handle both dense stochastic reward and sparse reward?
Q4. How does each component in TDR-based learning mechanisms affect performance?
Q5. How does TD regularized actor make policy updates in situations of misguiding critics?
Q6. How does the regularization coefficient ρ in Equation (9) affect TD Actor performance?

Details of the implementation, training, and evaluation procedures are provided in Appendix C and
D where links to all implementation codes are also provided.

5.1 MAIN EVALUATION

In obtaining comprehensive evaluation results summarized in Table 1, we included a 10% noise
respectively in state, action, and reward in each of the considered DMC environments in order
to make the evaluations more realistic. In “Cheetah Run sparse”, we sparsified the reward in the
environment. All details of the environment setup can be found in Appendix C. In Table 1, “Success”
is shorthand for learning success rate, “Avg. Rwd” for average reward, and “Rank” (%) is the
“percent of reward difference” between the evaluated method and the SOTA D4PG, which is (the
average reward of the evaluated method over that of the D4PG - 1), the more positive the better.
Note that, in computing the success rate, only those trials that have achieved a reward of at least
10 are accounted for as successful learning. The results are based on the last 50 evaluations of 10
different random seeds (same for all compared algorithms). Best performances are boldfaced for
average reward (Avg. Rwd). Note that we did not implement our TD Actor into SAC because SAC
already has a max entropy-regulated actor.

Q1 TDR improves over respective Base methods. The learning curves for six benchmark environ-
ments are shown in Figure 2. Overall, TDR methods (solid lines) outperform their respective Base

6

Figure 2: Systematic evaluation of TDR realized in three DRL algorithms (SAC, TD3, D4PG) in
DMC environments with 10% uniform random noise in state, action, and reward. The shaded regions
represent the 95 % confidence range of the evaluations over 10 seeds. The x-axis is the number of
steps.

Envirinoment

D4PG
DDPG
PPO
SAC
TD3
TDR-SAC
TDR-TD3
dTDR

Envirinoment

D4PG
DDPG
PPO
SAC
TD3
TDR-SAC
TDR-TD3
dTDR

Success
[%]
100
100
100
90
100
100
100
100

Success
[%]
100
100
20
0
0
100
100
100

Finger Turn Hard
Avg. Rwd
[µ ± 2σ]
400.9 ± 173.4
222.1 ± 160.4
85.9 ± 50
65.6 ± 30.2
205.9 ± 108.5
601.5 ± 147.4
569.8 ± 142.1
841.02 ± 148.3
Acrobot Swingup
Avg. Rwd
[µ ± 2σ]
26.8 ± 8.9
17.2 ± 3.8
7.9 ± 7.8
4 ± 2.2
5.2 ± 4.2
42.9 ± 5.1
50 ± 7.9
62.6 ± 14.4

Rank
[%]
0
-44.6
-78.6
-83.6
-48.6
49.9
42.3
109.8

Rank
[%]
0
-35.8
-70.5
-85.1
-80.6
60.1
86.5
133.6

Quadruped Walk

Success
[%]
100
100
100
100
100
100
100
100

Avg. Rwd
[µ ± 2σ]
858.5 ± 11.4
226.8 ± 133.6
173.1 ± 60.4
196.6 ± 73.7
334.8 ± 76.4
479.5 ± 126.9
475.4 ± 45.4
888.6 ± 15.7
Cartpole Swingup Sparse

Rank
[%]
0
-73.6
-79.8
-77.2
-61
-44.2
-44.6
3.46

Success
[%]
100
0
80
0
0
100
100
100

Avg. Rwd
[µ ± 2σ]
493.5 ± 15.9
3.6 ± 5.8
99.2 ± 172.9
1.7 ± 3.4
1.3 ± 2.3
774.2 ± 51.1
790.13 ± 33.0
810.3 ± 34.9

Rank
[%]
0
-99
-79.9
-99.7
-99.7
56.8
60.1
64.2

Success
[%]
100
100
100
100
100
100
100
100

Fish Swim
Avg. Rwd
[µ ± 2σ]
153.7 ± 68.1
109.7 ± 27.1
78.67 ± 6.28
73.2 ± 9.87
85.3 ± 21.7
212.3 ± 51.2
204.2 ± 41.5
249.9 ± 45.5
Cheetah Run Sparse

Success
[%]
60
50
0
0
50
100
100
100

Avg. Rwd
[µ ± 2σ]
532.8 ± 388.4
160.7 ± 284.7
0 ± 0
0 ± 0
220.5 ± 354.7
930.2 ± 18.7
827.8 ± 62.2
900.1 ± 30.8

Rank
[%]
0
-28.6
-48.8
-52.4
-44.5
37.9
32.7
62

Rank
[%]
0
-69.8
-100
-100
-58.6
74.6
55.4
68.9

Table 1: Systematic evaluations of TDR respectively augmented Base algorithms. “Rank” (%) is
the “percent of reward difference” between the SOTA D4PG, the more positive the better.

methods TD3, SAC and D4PG (dash lines) in terms of episode reward, learning speed, learning
variance and success rate. In Table 1,among the measures, the Avg. Rwd of TDR methods outper-
formed respective baseline algorithms. Notice from the table that the learning success rates for
all TDR methods are now 100%, a significant improvement over the Base methods. In comparison,
DDPG, SAC and TD3 Base methods struggle with Acrobot Swingup, Cartpole Swingup Sparse,
and Cheetah Run Sparse. Moreover, TDR methods also outperform DDPG and PPO in terms of
averaged reward (Awg.Rwd), learning speed, learning variance, and success rate. Thus, TDR has
helped succesfully address the random initialization challenge caused by random seeds (Henderson
et al., 2018).

Q2 TDR brings performance of Base methods close to or better than that of the SOTA D4PG.
From Figure 2, and according to the “Rank” measure in Table 1, for all environments but Quadruped
walk, TDR (TDR-SAC and TDR-TD3) helped enhance the performances of the respective Base
methods. Additionally, it even outperformed the SOTA D4PG by around 40% in the “Rank” mea-
sure. For Quadruped walk, even though TDR-SAC and TDR-TD3 did not outperform D4PG, they
still are the two methods, among all evaluated, that provided closest performance to D4PG. It is also

7

worth noting that TDR brings the performance of D4PG to a new SOTA level measured by mean
reward, convergence speed, and learning success rate.

Q3 TDR is robust under both dense stochastic reward and sparse reward. From Figure 2 and
Table 2, TDR methods outperformed their respective baselines in both dense stochastic and sparse
reward in terms of average reward, learning variance, success rate, and converge speed. In particular,
baseline algorithms such as TD3 and SAC struggle with sparse reward benchmark environments
(cartpole swingup sparse and cheetah run sparse). However, by using TDR, they not only learned,
but also achieved SOTA performance.

Methods

TD3+TD Critic
TD3+LNSS
TD3+TD Actor
TD3+TDR
SAC+TD Critic
SAC+LNSS
SAC+TDR
D4PG+TD Critic
D4PG+LNSS
D4PG+TD Actor
dTDR

Acrobot Swingup

Finger TurnHard

Avg. Rwd
[µ ± 2σ]
24.9 ± 11.7
24.2 ± 9.2
6.9 ± 2.9
42.9 ± 5.1
28.8 ± 12.2
9.7 ± 2.9
42.9 ± 5.1
32.8 ± 6.9
43.9 ± 16.7
29.9 ± 13.8
62.6 ± 14.4

Enhancement
[%]
378.8
365.4
32.7
725
620
142.5
972.5
22.4
63.8
11.6
133.6

Avg. Rwd
[µ ± 2σ]
556.2 ± 239.8
547.5 ± 120.5
212.3 ± 45.7
569.8 ± 142.1
588 ± 223.8
573 ± 156.5
601.5 ± 147.4
835.7 ± 140.9
675.1 ± 217.6
532.5 ± 235.7
841.1 ± 148.3

Enhancement
[%]
170.1
165.9
3.1
176.7
796.3
773.5
816.9
108.5
68.4
33.5
109.8

Cartpole Swingup Sparse
Avg. Rwd
[µ ± 2σ]
766.2 ± 86.1
766.6 ± 38.3
339.6 ± 231.9
790.1 ± 33.0
766.7 ± 126.4
722.8 ± 162.4
774.2 ± 51.1
678.7 ± 246.2
759.1 ± 31.1
600 ± 129.3
810.3 ± 34.9

Enhancement
[%]
588.4
588.7
260.2
606.7
449.6
423.7
454.4
37.5
53.8
21.6
64.2

Table 2: Systematic evaluations of each component of TDR compared to their respective Base al-
gorithms. “Enhancement” (%) is the “percent of reward difference” between the respective Base
algorithms, the larger the better. Note that TD Actor was not considered for SAC as SAC already
has a max entropy-regularized actor.

Figure 3: Evaluation of TD Actor with different ρ (ρ = 0, 0.1, 0.3, 0.5, 0.7, 0.9) in Equations (9,
21) based on two DRL algorithms (TD3, D4PG) in DMC environments with 10% uniform random
noise in state, action, and reward. The shaded regions represent the 95 % confidence range of the
evaluations over 10 seeds. The x-axis is the number of steps.

5.2 ABLATION STUDY

To perform the ablation study, we examined TDR by removing each of the following three compo-
nents. The respective short-form descriptions are:

1) “TD Critic”: the TD regularized double Q networks.
2) “TD Actor”: the TD regularized actor network.
3) “LNSS”: LNSS method with N = 100.

In Table 2, “Enhancement” (%) is the “percent of reward difference” between the evaluated method
and its Base method, the larger the better.

8

Q4 TD Critic, TD Actor, and LNSS effectively improved the Base algorithms. In Table 2, TD
Critic, LNSS, and TD Actor all effectively improved the Base algorithms. From the table, TD Critic
and LNSS have provided comparable and significant enhancement over Base algorithms. As our TD
Critic methods outperform respective Base algorithms, this suggests that mitigating estimation errors
both over and under from vanilla double Q network is an effective way to improve performance
which has also been shown in our theoretical analysis (Theorem 1). The LNSS method helped
improve learning performance by reducing variances in value estimation for noisy rewards as shown
both theoretically and empirically (Zhong et al., 2022). By including LNSS, our TDR is more robust
under noisy and sparse rewards.

The TD Actor element also helped make appreciable improvements on learning performance as
shown in Table 2. More importantly, TD Actor plays an importantly role in TDR since it not only
stabilizes the policy updates as shown theoretically in Theorem 2 but also addresses the estimation
error in critic as shown theoretically in Theorem 3.

5.3 HYPER PARAMETER STUDY

Hyperparameter study results are summarized in Figure 3 where two DRL methods (D4PG and
TD3) with TD Actor are evaluated for different regularization factor ρ (ρ = 0, 0.1, 0.3, 0.5, 0.7, 0.9).
What is reported is the 10-seed averaged performance, i.e., the average of the approximate es-
timation error which is the difference between the true accumulated reward and the critic value:
Ψ = 1
10

t=0 γtrt − Q(s0, a0)).

eval=0((cid:80)999

(cid:80)9

Q5 TD regularized Actor helps reduce the estimation error in critic.

From Figure 3, with TD regularized Actor (TD Actor), the estimation errors in the critic are re-
duced from those without. For example, in Finger Turn hard, D4PG + TD Actor results in less
overestimation error compared with ρ = 0 at the later stage of training. TD3 + TD Actor has less
underestimation error compared with ρ = 0. Similarly in cartpole swingup sparse, D4PG + TD
Actor results in less overestimation error compared with ρ = 0.

A policy can be evaluated by the “epois reward” where a higher epois reward generally results from a
better policy. From Figure 3, policy updates are improved by selecting a suitable regularization fac-
tor ρ. Especially, in cartpole swingup sparse, TD3 + TD Actor enables successful learning whereas
the Base method struggled and stuck to 0 or no learning for the entire training period.

Q6 A range of ρ (ρ = 0.3, 0.5, 0.7) generally are good choices. From Figure 3, a small regular-
ization factor ρ = 0.1 in TDR will result in less regularization which may not provide sufficient
estimation error reduction in the critic. A larger regularization factor ρ = 0.9 in TDR will result in
more regularization and may have a negative effect on learning. Therefore, ρ = 0.3, 0.5, 0.7 may be
good choices. Therefore in this work, we have consistently used ρ = 0.7 in obtaining all results.

6 CONCLUSION, DISCUSSION, AND LIMITATION OF THE STUDY

1) In this work, we introduce a novel TDR mechanism that includes TD-regularized double critic
networks and TD-regularized actor network. Both components are shown to help mitigate both
over and under estimation errors. TDR has been shown to consistently outperform respective Base
algorithms in solving benchmark tasks in terms of average reward, learning success rate, learning
speed, and most times, learning variance. 2) Our analytical results also show that each component of
TDR helps mitigate both over and under estimation errors. 3) As shown in Figure 2, for five out of
the six environments (except quadruped walk) evaluated, our TDR combined with distributional and
LNSS elements has significantly elevated the current SOTA performance of D4PG to a new level
with an increase of at least 60%.

Even though we have identified a range of generally good regularization coefficient ρ values
(0.3, 0.5, 0.7), as Figure 3 shows, different algorithms in different environments have responded
somewhat differently to ρ. Therefore, how to effectively determine a regularization factor to have
the most improvement remains a question, and thus, it is the limitation of this study. Additionally,
the promising performances of TDR come after extensive training with millions of learning steps.
How TDR performs under limited training time and training steps need to be further investigated.

9

REFERENCES

Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob
McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. arXiv
preprint arXiv:1707.01495, 2017.

Gabriel Barth-Maron, Matthew W Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva Tb,
Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributed distributional deterministic
policy gradients. arXiv preprint arXiv:1804.08617, 2018.

Marc G Bellemare, Will Dabney, and R´emi Munos. A distributional perspective on reinforcement

learning. In International conference on machine learning, pp. 449–458. PMLR, 2017.

Shalabh Bhatnagar, Mohammad Ghavamzadeh, Mark Lee, and Richard S Sutton. Incremental nat-

ural actor-critic algorithms. Advances in neural information processing systems, 20, 2007.

Renzhi Chen, Ke Li, and Xin Yao. Dynamic multiobjectives optimization with a changing number

of objectives. IEEE Transactions on Evolutionary Computation, 22(1):157–171, 2017.

Robert Crites and Andrew Barto. An actor/critic algorithm that is equivalent to q-learning. Advances

in Neural Information Processing Systems, 7, 1994.

Will Dabney, Georg Ostrovski, David Silver, and R´emi Munos.

Implicit quantile networks for
distributional reinforcement learning. In International conference on machine learning, pp. 1096–
1105. PMLR, 2018a.

Will Dabney, Mark Rowland, Marc Bellemare, and R´emi Munos. Distributional reinforcement
learning with quantile regression. In Proceedings of the AAAI Conference on Artificial Intelli-
gence, volume 32, 2018b.

Jingliang Duan, Yang Guan, Shengbo Eben Li, Yangang Ren, Qi Sun, and Bo Cheng. Distributional
soft actor-critic: Off-policy reinforcement learning for addressing value estimation errors. IEEE
transactions on neural networks and learning systems, 33(11):6584–6598, 2021.

Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft

updates. arXiv preprint arXiv:1512.08562, 2015.

Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-
critic methods. In International Conference on Machine Learning, pp. 1587–1596. PMLR, 2018.

Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and appli-
cations. arXiv preprint arXiv:1812.05905, 2018.

G Hailu and G Sommer. On amount and quality of bias in reinforcement learning. In IEEE SMC’99
Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics
(Cat. No. 99CH37028), volume 2, pp. 728–733. IEEE, 1999.

Hado Hasselt. Double q-learning. Advances in neural information processing systems, 23, 2010.

Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.
Deep reinforcement learning that matters. In Proceedings of the AAAI conference on artificial
intelligence, volume 32, 2018.

Qingfeng Lan, Yangchen Pan, Alona Fyshe, and Martha White. Maxmin q-learning: Controlling

the estimation bias of q-learning. arXiv preprint arXiv:2002.06487, 2020.

Felix Leibfried and Jordi Grau-Moya. Mutual-information regularization in markov decision pro-
cesses and actor-critic learning. In Conference on Robot Learning, pp. 360–373. PMLR, 2020.

Wentao Liu, Junmin Zhong, Ruofan Wu, Bretta L Fylstra, Jennie Si, and He Helen Huang. Inferring
human-robot performance objectives during locomotion using inverse reinforcement learning and
inverse optimal control. IEEE Robotics and Automation Letters, 7(2):2549–2556, 2022.

10

Jiafei Lyu, Xiaoteng Ma, Jiangpeng Yan, and Xiu Li. Efficient continuous control with double
actors and regularized critics. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 36, pp. 7655–7663, 2022.

Fabio Pardo. Tonic: A deep reinforcement learning library for fast prototyping and benchmarking.

arXiv preprint arXiv:2011.07537, 2020.

Simone Parisi, Voot Tangkaratt, Jan Peters, and Mohammad Emtiyaz Khan. Td-regularized actor-

critic methods. Machine Learning, 108:1467–1501, 2019.

Tim GJ Rudner, Cong Lu, Michael A Osborne, Yarin Gal, and Yee Teh. On pathologies in kl-
regularized reinforcement learning from expert demonstrations. Advances in Neural Information
Processing Systems, 34:28376–28389, 2021.

David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In International conference on machine learning, pp.
387–395. Pmlr, 2014.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.

Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Bud-
den, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv
preprint arXiv:1801.00690, 2018.

Sebastian Thrun and Anton Schwartz.

Issues in using function approximation for reinforcement
learning. In Proceedings of the Fourth Connectionist Models Summer School, volume 255, pp.
263. Hillsdale, NJ, 1993.

Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016.

Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier Pietquin, R´emi Munos, and Matthieu Geist.
Leverage the average: an analysis of kl regularization in reinforcement learning. Advances in
Neural Information Processing Systems, 33:12163–12174, 2020.

Wentao Weng, Harsh Gupta, Niao He, Lei Ying, and R Srikant. The mean-squared error of double

q-learning. Advances in Neural Information Processing Systems, 33:6815–6826, 2020.

Ruofan Wu, Junmin Zhong, Brent Wallace, Xiang Gao, He Huang, and Jennie Si. Human-robotic
prosthesis as collaborating agents for symmetrical walking. Advances in Neural Information
Processing Systems, 35:27306–27320, 2022.

Ruofan Wu, Junmin Zhong, and Jennie Si. Phased actor in actor-critic reinforcement learning. 2023.

Zongzhang Zhang, Zhiyuan Pan, and Mykel J Kochenderfer. Weighted double q-learning. In IJCAI,

pp. 3455–3461, 2017.

Junmin Zhong, Ruofan Wu, and Jennie Si. Long n-step surrogate stage reward to reduce variances
of deep reinforcement learning in complex problems. arXiv preprint arXiv:2210.04820, 2022.

A DISTRIBUTIONAL TDR AND LNSS

The distributional RL (Bellemare et al., 2017) represents value function in terms of probability
distribution rather than function estimates. This distribution provides a more comprehensive rep-
resentation of the uncertainty associated with a range of different possible reward returns and state
action pairs which can provide more informative value function estimation. Many distributional RL
algorithms (Bellemare et al., 2017; Dabney et al., 2018b;a) has been achieved great performance im-
provements on many discrete problems such as Atari benchmarks. D4PG (Barth-Maron et al., 2018)
applied distributional RL into continuous control problem by combining the distributional return
function within an actor-critic framework. DSAC (Duan et al., 2021) address overestimation error
by applying distributional RL piggyback on SAC. Although, D4PG and DSAC can provide more
accurate critic, the overestimation of actor still exists since the actor is still updated by maximizing
the expectation of value function distribution. How to regulate actors in distributional RL in solving
overestimations was barely discussed before.

11

A.1 DISTRIBUTIONAL TD-REGULARIZED ACTOR-CRITIC (DTDR)

Here we tailor a distributional TDR (dTDR) method based on the original distributional conceptu-
alization developed in D4PG (Barth-Maron et al., 2018; Bellemare et al., 2017). We show a number
of enhancements in the meantime.

Distributional Critic. The distributional critic (Bellemare et al., 2017 )treated the return in Equa-
tion 1 as a random variable Z(sk, ak) whose expectation is used as the Q value estimate, namely,
Q(sk, ak) = E[Z(sk, ak)].
In dTDR however, we use TD errors to evaluate distributional critics. Similar to Equation 5 and 6,
distributional TD errors of the two target networks can be written as:
(sk+1, πϕ′(sk+1))] − E[Zθ′

1 = rk + γE[Zθ′
d′

(sk, ak)],

(15)

1

1

2 = rk + γE[Zθ′
d′

2

(sk+1, πϕ′(sk+1))] − E[Zθ′

2

(sk, ak)].

The twin TD-regularized target distributional Bellman operator is thus defined as:

T Zk

D=

(cid:26) rk + γZθ′
rk + γZθ′

1

2

(sk+1, πϕ′(sk+1))
(sk+1, πϕ′(sk+1))

if |d′
if |d′

1| ≤ |d′
2|
1| > |d′
2|

(16)

(17)

where A D= B denotes that two random variables A and B follow the same probability laws. Al-
though the distributional Bellman operator appears similar to Equation 1, it maps state-action pairs
to distributions. As such, we need to define a new TD error measure for the distribution as in D4PG
(Barth-Maron et al., 2018). We consider using the following distributional loss,

L(θ) = Es∼pπ,a∼π[

(cid:88)

ζ=1,2

l(T Zk, Zθζ (sk, ak))],

(18)

where l measures the distance between two distributions. Many distributional RL algorithms use
Kullback-Leibler (KL) divergence as the distance metric (Duan et al., 2021; Barth-Maron et al.,
2018. We adopt the same metric.

Distributional Actor. In most distributional methods (Barth-Maron et al., 2018; Bellemare et al.,
2017), policy updates are performed based on the policy gradient below,

∇ϕJ(ϕ) = Es∼pπϕ

[E[∇aZθ(sk, ak)]|a=πϕ(s)∇ϕπϕ(s)].

(19)

In our dTDR, we need to use critic evaluation metrics to evaluate the quality of the current distribu-
tional critic and the regularized distributional actor. We first formulate the following loss metric:

Lz(ϕ) = E[l(rk + γZθi+1

1

(sk+1, πϕ(sk+1)), Zθi+1

1

(sk, πϕ(sk))].

(20)

Similar to TD-regularized actor network, the distributional actor is updated in the direction of max-
imizing the expected critic while keeping the expected distance between the projected critic and the
critic, namely,

∇ϕJ(ϕ) = Es∼pπϕ

[(E[∇aZθi+1

1

(sk, ak)] − ∇aρLz(ϕ))|a=πϕ(s)∇ϕπϕ(s)],

(21)

where ρ ∈ (0, 1) is a regularization coefficient.

A.2 LONG N-STEP SURROGATE STAGE (LNSS) REWARD

LNSS (Zhong et al., 2022) utilizes a long reward trajectory of N future steps in the estimation
of stage reward rk. Using the LNSS-resulted reward r′
k in place of the original rk was shown
to effectively reduce learning variance with significant performance improvements for off-policy
methods. Given a reward trajectory of N steps from time step k, let G(sk:k+N −1, ak:k+N −1) ∈ R
(with shorthand notation Gk) denote the discounted N -step return, i.e.,

k+N −1
(cid:88)

γt−krt,

Gk =

t=k

12

(22)

where rt is the tth stage reward and t is from k to k + N − 1. In LNSS, r′
reward in place of rk in Equation (2). To determine r′
N -step reward sequence, namely

k is a surrogate stage
k, LNSS treat it as a weighted average of the

r′
k =

γt−krt

(cid:80)k+N −1
t=k
(cid:80)N −1

n=0 γn

.

(23)

As Figure 1 shows, Once r′
(sk, ak, r′
as discussed.

k is obtained, it is simply used in place of rk to form a new tuple
k, sk+1), which is then stored into the memory buffer D. The TDR method proceeds

B ESTIMATION ANALYSIS

Lemma 1. Let Qπ be the true Q value following the current target policy π, and Qθ′
be the
target network estimates using double Q neural networks. We assume that there exists a step random
estimation bias ψk
(i.e., estimation bias at the kth stage), and that it is independent of (sk, ak) with
θ′
ζ
mean E[ψk
ζ, µ′
] = µ′
2 respectively defined in
θ′
ζ
Equations (5) and (6), we have,

ζ < ∞, for all k, and ζ = 1, 2. Then for δ′

1 and δ′

and Qθ′

2

1

E[δ′
E[δ′

1] = −µ′
1,
2] = −µ′
2.

(24)

(25)

(26)

Proof. With the step random estimation bias ψk
θ′
ζ

, We can rewrite the expectation of Ψk
θ′
ζ

as

E[Ψk+1
θ′
ζ

] =

∞
(cid:88)

t=k+1

γt−k−1E[ψt
θ′
ζ

] =

1
1 − γ

µ′
ζ.

Then the expectation of the target can be written as,

E[yk] = E[rk] + γE[(Qπ(sk+1, ak+1) + Ψk+1
∞
(cid:88)

θ′
ζ

)]

= E[rk] + γ(E[

γt−k−1rt]) +

γ
1 − γ

µ′
ζ

= Qπ(sk, ak) +

t=k+1
γ
1 − γ

µ′
ζ.

By using Equations (10), and (26), the TD errors of the two target critics (Equations 5 and 6),
respectably are:

E[δ′

1] = E[rk] + γE[Qθ′

1

(sk+1, πϕ′(sk+1))] − E[Qθ′
1
1
γ
1 − γ
1 − γ

1 − Qπ(sk, ak) −
µ′

(sk, ak)]

µ′
1

(27)

= Qπ(sk, ak) +

Similarly, E[δ′

= −µ′
1.
2] = −µ′
2.

Thus Lemma 1 holds.

With Lemma 1 in place, we are now ready to analyze the estimation errors by using TDR and the
double Q (DQ) method as in TD3 (Fujimoto et al., 2018) and SAC (Haarnoja et al., 2018).

Theorem 1. Let assumptions in Lemma 1 hold, and let δYk denote the target value estimation
error. Accordingly, we denote this error for TDR as δY T DR
. We then have the
following,

, and DQ as δY DQ

k

k

|E[δY T DR
k

]| ≤ |E[δY DQ

k

]|.

(28)

Proof. The proof is based on enumerating a total of 8 possible scenarios of estimation errors which
are determined from the relationships among the two target Q values and the true Qπ value . We
provide proofs for the 4 out of 8 unique scenarios below.

13

First note that, E[δY T DR
Case 1: If the target critic values and the true value Qπ have the following relationship:

] = E[Qπ − yT DR

] = E[Qπ − yDQ

], and E[δY DQ

].

k

k

k

k

i.e, Qθ′

1

is more underestimated as

that implies

E[Qθ′

1

] < E[Qθ′

2

] < Qπ,

|E[Ψk
θ′
1

]| > |E[Ψk
θ′
2

]|,

|µ′

1| > |µ′

2|.

Based on Lemma 1 and Equation (7), our TDR will use Qθ′

2

in the target value,

E[yT DR
k

] = E[rk] + γE[Qθ′

2

(sk+1, πϕ′(sk+1))].

However for a vanilla double Q network, the target value will be Qθ′

1

,

] = E[rk] + γE[Qθ′
Thus based on Equation (26), the two estimation errors of the respective target values are

(sk+1, πϕ′(sk+1))].

E[yDQ
k

1

|E[δY T DR
k

]| = |E[Qπ − yT DR

k

]| = |

|E[δY DQ
k

]| = |E[Qπ − yDQ

k

]| = |

µ′

2|,

γ
1 − γ
γ
µ′
1 − γ

1|.

Since |µ′

1| > |µ′

2|, we have

|E[δY T DR
k

]| < |E[δY DQ

k

]|.

Thus identity (28) holds.
Case 2: If the target critic values and the true value Qπ have the following relationship:

E[Qθ′
|E[Qπ − Qθ′

1

] < Qπ < E[Qθ′

2],
]| > |E[Qπ − Qθ′

1

]|,

2

is expected to be underestimated and Qθ′

is overestimated. Since |E[Qπ − Qθ′

1

]| >

2

then Qθ′
|E[Qπ − Qθ′

1

2

we thus have

]| which implies

|E[Ψk
θ′
1

]| > |E[Ψk
θ′
2

]|,

|µ′

1| > |µ′

2|.

Based on Lemma 1 and Equation (7), our TDR will use Qθ′

2

in the target value:

E[yT DR
k

] = E[rk] + γE[Qθ′

2

(sk+1, πϕ′(sk+1))].

However for a vanilla double Q network, the target value will use Qθ′

1

,

] = E[rk] + γE[Qθ′
Based on Equation (26), the two estimation errors of the respective target values are:

(sk+1, πϕ′(sk+1))].

E[yDQ
k

1

|E[δY T DR
k

]| = |E[Qπ − yT DR

k

]| = |

|E[δY DQ
k

]| = |E[Qπ − yDQ

k

]| = |

µ′

2|,

γ
1 − γ
γ
µ′
1 − γ

1|,

Since |µ′

1| > |µ′

2|, we have

|E[δY T DR
k

]| < |E[δY DQ

k

]|.

Thus identity (28) holds.
Case 3: If the target critic values and the true value Qπ has the following relationship:

] < Qπ < E[Qθ′

],

2

E[Qθ′
|E[Qπ − Qθ′

1

1

]| < |E[Qπ − Qθ′

2

]|,

14

(29)

(30)

(31)

(32)

(33)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

(42)

(43)

is expected to be underestimated and Qθ′

is overestimated. Since |E[Qπ − Qθ′

1

]| <

2

then Qθ′
|E[Qπ − Qθ′

1

2

]|, it implies

thus we have

|E[Ψk
θ′
1

]| < |E[Ψk
θ′
2

]|,

(44)

1| < |µ′
Based on Lemma 1 and Equation (7), both vanilla double Q network and our TDR will pick Qθ′
the target value:

|µ′

2|.

1

(45)

in

E[yT DR
k

] = E[yDQ

k

] = E[rk] + γE[Qθ′

1

(sk+1, πϕ′(sk+1))].

Then based on Equation (26), the two estimation errors of the respective target values are:

|E[δY T DR
k

]| = |E[δY DQ

k

]| = |E[Qπ − yT DR

k

]| = |

γ
1 − γ

µ′

1|.

We thus have

|E[δY T DR
k

]| = |E[δY DQ

k

]|.

Thus identity (28) holds.
Case 4: If the target critic values and the true value Qπ has the following relationship

where E[Qθ′

2

] is expected more overestimated ie |E[Ψk
θ′
1

]| < |E[Ψk
θ′
2

]| that implies

Qπ < E[Qθ′

1

] < E[Qθ′

2

],

|µ′

1| < |µ′

2|.

(46)

(47)

(48)

(49)

(50)

Based on Equation (24) and (7), same with vanilla double Q network, our Twin TD-regularized
Critic will pick the target value using Qθ′

which both mitigates the larger overestimation bias as:

1

E[yT DR
k

] = E[yDQ

k

] = E[rk] + γE[Qθ′

1

(sk+1, πϕ′(sk+1))],

which based on Equation (26), the two estimation errors of the target value are

|E[δY T DR
k

]| = |E[δY DQ

k

]| = |E[Qπ − yT DR

k

]| = |

γ
1 − γ

µ′

1|.

We have

|E[δY T DR
k

]| = |E[δY DQ

k

]|.

(51)

(52)

(53)

1

1

2

k

] > E[Qθ′

] < E[Qθ′

]| ≤ |E[δY DQ

Thus identity (28) holds. Both methods can mitigate the overestimation error.
Note, the above cases study the relationship of E[Qθ′
for E[Qθ′
], |E[δY T DR
Theorem 2. Let Qπ denote the true Q value following the current target policy π, Qθ1 be the
estimated value. We assume that there exists a step random estimation bias ψk
that is independent
θ1
of (sk, ak) with mean E[ψk
] = µ1, µ1 < ∞, for all k. We assume the policy is updated based on
θ1
critic Qθ1 using the deterministic policy gradient (DPG) as in Equation 4. Let δϕk denote the change
in actor parameter ϕ updates at stage k. Accordingly, we denote this change for TDR as δϕT DR
,
k
vanilla DPG as δϕDP G
. We then
k
have the following,

, and true change without any approximation error in Q as δϕtrue

]| still valid. Thus Theorem 1 holds.

] and by applying same procedure

k

k

2

(cid:26) E[δϕtrue
k
E[δϕtrue
k

] ≥ E[δϕT DR
] ≤ E[δϕT DR

k

k

] ≥ E[δϕDP G
] ≤ E[δϕDP G

k

k

]
]

if E[Ψk
θ1
if E[Ψk
θ1

] < 0,
] ≥ 0.

(54)

Proof. With learning rate α, the true change of the actor parameters in case without any approxima-
tion error in Q:

E[δϕtrue
k

] = αEs∼pπ

(cid:104)

ϕj

∇aQπ(sk, ak)|a=πϕj (s) ∇ϕj πϕj (s)

(cid:105)

.

(55)

15

Consider the estimated critic and the true value follow the relationship in Equation 10. Given the
same current policy parameters ϕj, the updated parameters using DPG are:

ϕj+1
DP G = ϕj + αEs∼pπ
(cid:104)

E[δϕDP G
k

] = αEs∼pπ

ϕj

(cid:104)

ϕj

∇a(Qπ(sk, ak) + Ψk
θ1

(cid:105)
)(cid:12)
(cid:12)a=πϕj (s) ∇ϕj πϕj (s)

,

∇a(Qπ(sk, ak) + Ψk
θ1

)(cid:12)
(cid:12)a=πϕj (s) ∇ϕj πϕj (s)

(cid:105)

.

(56)

With an overestimation bias E[Ψk
θ1
mated actions, and with an underestimation bias E[Ψk
θ1
the underestimated actions. Both result in suboptimal policies.
However, by using TD-regularized actor, and given the same current policy parameters ϕj, the actor
updates with Equation (9) are:

] > 0, the updates encourage more exploration for the overesti-
] < 0, the updates discourage exploration for

ϕj+1
T DR = ϕj + αEs∼pπ
] = αEs∼pπ

E[δϕT DR
k

ϕj

[∇a(Qπ(sk, ak) + Ψk
θ1

ϕj

− ρ(∆))|a=πϕj (s)∇ϕj πϕj (s)],

[∇a(Qπ(sk, ak) + Ψk
θ1

− ρ(∆))|a=πϕj (s)∇ϕj πϕj (s)].

Similar to Lemma 1, E[Ψk
θ1

] = 1

1−γ µ1, and from Equations (8) and (9) we have:

E[∆] = E[Qθi+1
= µ1

1

(sk, ak)] − E[(rk + γQθi+1

1

(sk+1, πϕ(sk+1)))]

by selecting ρ ≤ 1

1−γ , we have the following:

(cid:26) 0 ≥ E[Ψk
θ1
0 ≤ E[Ψk
θ1

− ρ∆] > E[Ψk
θ1
− ρ∆] ≤ E[Ψk
θ1

]
]

if E[Ψk
θ1
if E[Ψk
θ1

] < 0,
] ≥ 0.

Therefore by inspecting Equations (55), (56) and (57), we have:

(cid:26) E[δϕtrue
k
E[δϕtrue
k

Thus Theorem 2 holds.

] ≥ E[δϕT DR
] ≤ E[δϕT DR

k

k

] ≥ E[δϕDP G
] ≤ E[δϕDP G

k

k

]
]

if E[Ψk
θ1
if E[Ψk
θ1

] < 0,
] ≥ 0.

(57)

(58)

(59)

(60)

Theorem 3. Suboptimal actor updates negatively affect the critic. Specifically, consider actor up-
dates as in Theorem 2, in the overestimation case, we have:

E[Qθ1(sk, πDP G(sk)] ≥ E[Qθ1 (sk, πT DR(sk))] ≥ E[Qπ(sk, πT rue(sk))],

(61)

and in the underestimation case,

E[Qθ1(sk, πDP G(sk)] ≤ E[Qθ1 (sk, πT DR(sk))] ≤ E[Qπ(sk, πT rue(sk))].

(62)

Proof Following the analysis of the TD3 (Fujimoto et al., 2018), consider Equation (12) in Theorem
2, we have

(cid:26) E[δϕtrue
k
E[δϕtrue
k

] ≥ E[δϕT DR
] ≤ E[δϕT DR

k

k

] ≥ E[δϕDP G
] ≤ E[δϕDP G

k

k

]
]

if E[Ψk
θ1
if E[Ψk
θ1

] < 0 Underestimate,
] ≥ 0 Overestimate.

(63)

In the overestimation case, the approximate value using TDR and vanilla DPG must be

E[Qθ1(sk, πDP G(sk)] ≥ E[Qθ1(sk, πT DR(sk))] ≥ E[Qπ(sk, πtrue(sk))].

(64)

Similarly, in the underestimation case, the approximate value using TDR and vanilla DPG must be

E[Qθ1(sk, πDP G(sk)] ≤ E[Qθ1 (sk, πT DR(sk))] ≤ E[Qπ(sk, πT rue(sk))].

(65)

Thus Theorem 3 holds.

16

C IMPLEMENTATION DETAILS

We use PyTorch for all implementations. All results were obtained using our internal server consist-
ing of AMD Ryzen Threadripper 3970X Processor, a desktop with Intel Core i7-9700K processor,
and two desktops with Intel Core i9-12900K processor.

Training Procedure.

An episode is initialized by resetting the environment, and terminated at max step T = 1000. A trial
is a complete training process that contains a series of consecutive episodes. Each trial is run for a
maximum of 1 × 106 time steps with evaluations at every 2 × 104 time steps. Each task is reported
over 10 trials where the environment and the network were initialized by 10 random seeds, (0 − 9)
in this study.

For each training trial, to remove the dependency on the initial parameters of a policy, we use a
purely exploratory policy for the first 8000 time steps (start timesteps). Afterwards, we use an
off-policy exploration strategy, adding Gaussian noise N (0, 0.1) to each action.

Evaluation Procedure.
Every 1 × 104 time steps training, we have an evaluation section and each evaluation reports the
average reward over 5 evaluation episodes, with no exploration noise and with fixed policy weights.
The random seeds for evaluation are different from those in training which each trial, evaluations
were performed using seeds (seeds + 100).

Network Structure and optimizer.

TD3.The actor-critic networks in TD3 are implemented by feedforward neural networks with three
layers of weights. Each layer has 256 hidden nodes with rectified linear units (ReLU) for both the
actor and critic. The input layer of actor has the same dimension as observation state. The output
layer of the actor has the same dimension as action requirement with a tanh unit. Critic receives both
state and action as input to THE first layer and the output layer of critic has 1 linear unit to produce
Q value. Network parameters are updated using Adam optimizer with a learning rate of 10−3 for
simple control problems. After each time step k, the networks are trained with a mini-batch of a
256 transitions (s, a, r, s′), (s, a, r′, s′) in case of LNSS, sampled uniformly from a replay buffer D
containing the entire history of the agent.

D4PG. Same with the actor-critic networks in D4PG are implemented by feedforward neural net-
works with three layers of weights. Each layer has 256 hidden nodes with rectified linear units
(ReLU) for both the actor and critic. The input layer of actor has the same dimension as observa-
tion state. The output layer of the actor has the same dimension as action requirement with a tanh
unit. Critic receives both state and action as input to THE first layer and the output layer of critic
has a distribution with hyperparameters for the number of atoms l, and the bounds on the support
(Vmin, Vmax). Network parameters are updated using Adam optimizer with a learning rate of 10−3.
After each time step k, the networks are trained with a mini-batch of 256 transitions (s, a, r, s′),
(s, a, r′, s′) in case of LNSS, sampled uniformly from a replay buffer D containing the entire his-
tory of the agent.

SAC. The actor-critic networks in SAC are implemented by feedforward neural networks with three
layers of weights. Each layer has 256 hidden nodes with rectified linear units (ReLU) for both the
actor and critic. The input layer of actor has the same dimension as observation state. The output
layer of the actor has the same dimension as action requirement with a tanh unit. Critic receives both
state and action as input to the first layer and the output layer of critic has 1 linear unit to produce
Q value. Network parameters are updated using Adam optimizer with a learning rate of 10−3 for
simple control problems. After each time step k, the networks are trained with a mini-batch of a
256 transitions (s, a, r, s′), (s, a, r′, s′) in case of LNSS, sampled uniformly from a replay buffer D
containing the entire history of the agent.

Hyperparameters. To keep comparisons in this work fair, we set all common hyperparameters
(network layers, batch size, learning rate, discount factor, number of agents, etc) to be the same for
comparison within the same methods and different methods.

For TD3, target policy smoothing is implemented by adding ϵ ∼ N (0, 0.2) to the actions chosen
by the target actor-network, clipped to (−0.5, 0.5), delayed policy updates consist of only updating

17

the actor and target critic network every d iterations, with d = 2. While a larger d would result in
a larger benefit with respect to accumulating errors, for fair comparison, the critics are only trained
once per time step, and training the actor for too few iterations would cripple learning. Both target
networks are updated with τ = 0.005.

The TD3 and TD3+TDR used in this study are based on the paper (Fujimoto et al., 2018) and the
code from the authors (https://github.com/sfujim/TD3).

Hyperparameter TD3
Start timesteps
Evaluation frequency
Max timesteps
Exploration noise
Policy noise
Noise clip
Policy update frequency
Batch size
Buffer size
γ
τ
Number of parallel actor
LNSS-N
Adam Learning rate
regularization factor

Value
8000 steps
20000 steps
1e6 steps
N (0, 0.1)
N (0, 0.2)
±0.5
2
256
1e6
0.99
0.005
1
100
1e-3
0.7

Table 3: TD3 + TDR hyper parameters used for DMC benckmark tasks

The SAC used in this study is based on paper (Haarnoja et al., 2018) and the code is from GitHub
(https://github.com/pranz24/pytorch-soft-actor-critic). and the hyperparameter is from Table 4.

Value
8000 steps
20000 steps
1e6 steps
N (0, 0.1)
N (0, 0.2)
±0.5
2
256
1e6
0.99
0.005

Hyperparameter SAC
Start timesteps
Evaluation frequency
Max timesteps
Exploration noise
Policy noise
Noise clip
Policy update frequency
Batch size
Buffer size
γ
τ
Temperature parameter α 0.2
Number of parallel actor
LNSS-N
Adam Learning rate

1
100
1e-3

Table 4: SAC hyper parameters used for the DMC benckmark tasks

The D4PG used in this study is based on paper (Barth-Maron et al., 2018) and the code is modified
from TD3. The hyperparameter is from Table 5.

All Other algorithms are from the same DRL training platform (Tonic RL) (Pardo, 2020) with the
same evaluation as the above algorithms.

Sparse Reward Setup. 1) Cheetah Run Sparse: Cheetah needs to run forward as fast as possible.
The agent gets a reward only after speed exceeds 2.5 m/s, making the reward sparse. r = 1. That
is, if v >= 2.5 else r = 0.

18

Hyperparameter D4PG
Start timesteps
Evaluation frequency
Max timesteps
Exploration noise
Noise clip
Batch size
Buffer size
γ
τ
Number of parallel actor
LNSS-N
Adam Learning rate
Vmax
Vmin
l
regularization factor

Value
8000 steps
20000 steps
1e6 steps
N (0, 0.1)
±0.5
256
1e6
0.99
0.005
1
100
1e-3
100
0
51
0.7

Table 5: D4PG + TDR hyper parameters used for the DMC benckmark tasks

19

D TDR ALGORITHMS DETAILS

In this section, we show our TDR-based algorithms. TD3-TDR is shown in Algorithm 1, SAC-
TDR is shown in Algorithm 2, and D4PG-TDR is shown in Algorithm 3. We mainly add LNSS
reward to the sample collection part. In algorithm update part, we mainly modify the target value
selection using Equation 7 for regular DRL and Equation (17) for distributional DRL. Additionally,
if applicable, we modify the actor gradient based on Equation 9 for regular DRL and Equation (21)
for distributional DRL. All codes will be released to GitHub once the paper get accepted.

Algorithm 1 TD3-TDR
Initialize:

1 ← θ1, θ′

2 ← θ2, ϕ′ ← ϕ,

• Critic networks Qθ1,Qθ2 and actor-network πϕ with random parameters, θ1, θ2, ϕ
• Target networks θ′
• an experience buffer D
• a temporary experience buffer D′ with size N
• Total training episode T
1. For episode = 1, T do
2.

Reset initialize state s0, D′
For k = 0, T do

3.

4.

5.

6.

7.

8.

9.

10.

11.

12.

13.

14.

15.

16.

17.

18.

19.

20.

21.

22.

23.

24.

25.

Choose an action ak based on current state sk and learned policy from A.
Execute the action ak and observe a new state sk+1 with reward signal rk
Store the transition (sk, ak, rk, sk+1) in D′
if k + N − 1 ≤ T then

1) in the D′

0, s′

0, r′

0, a′

Get earliest memory (s′
Calculate r′ based on Equation (23)
0, a′
Store the transition (s′
Clear original transition (s′

0, r′, s′
0, a′

1) in D
0, s′
0, r′

1) in the D′

else

Repeat step 8 to 11 and Calculate r′ based on Equation

r′
k =

γ − 1
γT −k+1 − 1

T
(cid:88)

t=k

γt−krt.

(66)

t, st+1) from D

end if
Sample mini-batch data (st, at, r′
Get next action at+1 ← πϕ′(st+1)
Target value yt based on Equation (7)
Update Critics based on Equation 3
if k mod Policy Update frequency then

Update ϕ by Equation 9
Update target networks:
θ′
ζ ← τ θζ + (1 − τ )θ′
ζ
ϕ′ ← τ ϕ + (1 − τ )ϕ′

end if

end for

26.
27. end for

20

Algorithm 2 SAC-TDR
Initialize:

• Soft value function VΞ, target Soft value function V ′

Ξ, Critic networks Qθ1,Qθ2 and actor-

network πϕ with random parameters, θ1, θ2, ϕ

• Target networks Ξ′ ← Ξ
• an experience buffer D
• a temporary experience buffer D′ with size N
• Total training episode T
1. For episode = 1, T do
2.

Reset initialize state s0, D′
For k = 0, T do

3.

4.

5.

6.

7.

8.
9.

10.

11.

12.

13.

14.

15.

16.

17.

18.

19.

20.

21.

22.

23.

Choose an action ak based on current state sk and learned policy from A.
Execute the action ak and observe a new state sk+1 with reward signal rk
Store the transition (sk, ak, rk, sk+1) in D′
if k + N − 1 ≤ T then

1) in the D′

0, s′

0, r′

0, a′

Get earliest memory (s′
Calculate r′ based on Equation (23)
Store the transition (s′
0, a′
Clear original transition (s′

0, r′, s′
0, a′

1) in D
0, s′
0, r′

1) in the D′

else

Repeat step 8 to 11 and Calculate r′ based on Equation

r′
k =

γ − 1
γT −k+1 − 1

T
(cid:88)

t=k

γt−krt.

(67)

end if
Sample mini-batch data (st, at, r′
Get next action at+1 ← πϕ′(st+1)
Target value yt based on Equation (7)
Update Critics based on Equation 3

t, st+1) from D

Update Soft value function based on original SAC formualtion
Update ϕ by original SAC formulation
Update target networks:
Ξ′ ← τ Ξ + (1 − τ )Ξ′

end for

24.
25. end for

21

Algorithm 3 D4PG-TDR
Initialize:

1 ← θ1, θ′

2 ← θ2, ϕ′ ← ϕ,

• Critic networks Zθ1,Zθ2 and actor-network πϕ with random parameters, θ1, θ2, ϕ
• Target networks θ′
• an experience buffer D
• a temporary experience buffer D′ with size N
• Total training episode T
1. For episode = 1, T do
2.

Reset initialize state s0, D′
For k = 0, T do

3.

4.

5.

6.

7.

8.

9.

10.

11.

12.

13.

14.

15.

16.

17.

18.

19.

20.

21.

22.

23.

24.

25.

Choose an action ak based on current state sk and learned policy from A.
Execute the action ak and observe a new state sk+1 with reward signal rk
Store the transition (sk, ak, rk, sk+1) in D′
if k + N − 1 ≤ T then

1) in the D′

0, s′

0, r′

0, a′

Get earliest memory (s′
Calculate r′ based on Equation (23)
0, a′
Store the transition (s′
Clear original transition (s′

0, r′, s′
0, a′

1) in D
0, s′
0, r′

1) in the D′

else

Repeat step 8 to 11 and Calculate r′ based on Equation

r′
k =

γ − 1
γT −k+1 − 1

T
(cid:88)

t=k

γt−krt.

(68)

end if
Sample mini-batch data (st, at, r′
Get next action at+1 ← πϕ′(st+1)
Target distribution based on Equation (17)

t, st+1) from D

Update Critics based on Equation 18
if k mod Policy Update frequency then

Update ϕ by Equation 21
Update target networks:
θ′
ζ ← τ θζ + (1 − τ )θ′
ζ
ϕ′ ← τ ϕ + (1 − τ )ϕ′

end if

end for

26.
27. end for

22

","3 2 0 2 v o N 7 ] G L . s c [ 1 v 1 1 7 3 0 . 1 1 3 2 : v i X r a MITIGATING ESTIMATION ERRORS BY TWIN TD- REGULARIZED ACTOR AND CRITIC FOR DEEP REIN- FORCEMENT LEARNING Junmin Zhong Arizona State University Ruofan Wu Arizona State University Jennie Si ∗ Arizona State University ABSTRACT We address the issue of estimation bias in deep reinforcement learning ( DRL ) by introducing solution mechanisms that include a new , twin TD-regularized actor- critic ( TDR ) method . It aims at reducing both over and under estimation errors . With TDR and by combining good DRL improvements , such as distributional learning and long N -step surrogate stage reward ( LNSS ) method , we show that our new TDR-based actor-critic learning has enabled DRL methods to outper- form their respective baselines in challenging environments in DeepMind Control Suite . Furthermore , they elevate TD3 and SAC respectively to a level of perfor- mance comparable to that of D4PG ( the current SOTA ) , and they also improve the performance of D4PG to a new SOTA level measured by mean reward , conver- gence speed , learning success rate , and learning variance . 1 INTRODUCTION Reinforcement learning ( RL ) has been developed for decades to provide a mathematical formal- ism for learning-based control . Recently , significant progress has been made to attain excellent results for a wide range of high-dimensional and continuous state-action space problems especially in robotics applications , such as robot manipulation ( Andrychowicz et al. , 2017 ) , and human-robotic interaction ( Liu et al. , 2022 ; Wu et al. , 2022 ) . However , the fundamental issue of estimation error associated with actor-critic RL ( Van Hasselt et al. , 2016 ; Duan et al. , 2021 ) still poses great challenge . Overestimation due to , for example , using the max operator in updates has been identified and studied ( Thrun & Schwartz , 1993 ; Duan et al. , 2021 ) . To reduce it , most efforts have focused on attaining more accurate and stable critic networks . TD3 ( Fujimoto et al. , 2018 ) applies clipped double Q-learning by taking the minimum between the two Q estimates . SAC ( Haarnoja et al. , 2018 ) utilizes the double Q network and incorporates entropy regularization in the critic objective function to ensure more exploratory behavior to help alleviate the overestimation problem . However , directly taking the minimum value of the target networks such as that in TD3 and SAC has been reported to result in an underestimation bias ( Fujimoto et al. , 2018 ) . Evaluations have revealed multiple roles of over and under estimation errors in learning . On one hand , overestimation may not always be harmful ( Lan et al. , 2020 ) as it is considered playing a role of encouraging exploration by overestimated actions . Along this line , underestimation bias may discourage exploration . If the overestimation bias occurs in a high-value region containing the optimal policy , then encouraging exploration is a good thing ( Hailu & Sommer , 1999 ) . On the other hand , overestimation bias may also cause an agent to overly explore a low-value region . This may lead to a suboptimal policy . Accordingly , an underestimation bias may discourage an agent from exploring high-value regions or avoiding low-value regions . All things considered , if estimation errors are left unchecked , they may accumulate to negatively impact policy updates as suboptimal actions may be highly rated by a suboptimal critic , reinforcing the suboptimal action in the next policy update ( Fujimoto et al. , 2018 ) . Aside from the anecdotal evidence on the roles of over and under estimation , how to mitigate both of them in a principled way remains an open issue . ∗si @ asu.edu 1 While several methods and evaluations have been performed and shown promising , a major tool has been mostly left out thus far . That is , it is still not clear how , and if it is possible , to further reduce estimation errors by considering the actor given the interplay between the actor and the critic . Only a handful of approaches have been examined . As shown in ( Wu et al. , 2023 ) with demonstrated performance improvement , PAAC uses a phased actor to account for both a Q value and a TD error in actor update . A double actor idea was proposed and evaluated in ( Lyu et al. , 2022 ) . It takes the minimum value estimate associated with one of the two actor networks . However , directly using the minimum of the estimated values was shown resulting in an underestimation error , similar to that in TD3 . Other methods , such as Entropy ( Haarnoja et al. , 2018 ; Fox et al. , 2015 ) , mutual-information ( MI ) ( Leibfried & Grau-Moya , 2020 ) , and Kullback-Leibler ( KL ) ( Vieillard et al. , 2020 ; Rudner et al. , 2021 ) regularization , are also used to enhance policy exploration , robustness , and stability . TD-regularized actor-critic ( Parisi et al. , 2019 ) regularizes the actor only aiming to enhance the stability of the actor learning by applying a TD error ( same as that in online critic updates ) as a regularization term in actor updates . However , none of these methods have shown how regularization in actor may help reduce estimation error in the critic . In this paper , we propose a new , TD-regularized ( TDR ) learning mechanism which includes TD- regularized double critic networks and TD-regularized actor network . This new architecture has several properties that make it ideal for the enhancements we consider . For the TD-regularized double critic network , instead of directly selecting the minimum value from twin target networks , we select the target based on the minimum TD error , which then addresses not only overestimation but underestimation problems . For the TD-regularized actor network , we formulate a new TD error to regularize actor updates to avoid a misleading critic . This regularization term helps further reduce the estimation error in critic updates . Additionally , we apply TDR combined with distributional RL ( Barth-Maron et al. , 2018 ; Bellemare et al. , 2017 ) and LNSS reward estimation method ( Zhong et al. , 2022 ) to further improve learning stability and performance . 2 RELATED WORK To shed light on the novelty of the TDR method , here we discuss double critic networks and TD error-based actor learning to provide a backdrop . We include reviews of distributional RL ( Barth- Maron et al. , 2018 ; Bellemare et al. , 2017 ) and long-N -step surrogate stage ( LNSS ) method ( Zhong et al. , 2022 ) in Appendix A . Double critic networks have been used in both RL ( Hasselt , 2010 ; Zhang et al. , 2017 ; Weng et al. , 2020 ) and DRL ( Fujimoto et al. , 2018 ; Haarnoja et al. , 2018 ; Van Hasselt et al. , 2016 ) . Double Q learning ( Hasselt , 2010 ; Van Hasselt et al. , 2016 ) was the first to show reduction of overestimation bias . TD3 ( Fujimoto et al. , 2018 ) and SAC ( Haarnoja et al. , 2018 ) also were shown effective by applying clipped double Q-learning by using the minimum between the two Q estimates . However , these methods have induced an underestimation bias problem . ( Hasselt , 2010 ; Zhang et al. , 2017 ; Fujimoto et al. , 2018 ) . Consequently , weighted double Q learning ( Zhang et al. , 2017 ) was proposed to deal with both overestimation and underestimation biases . However , this method has not been tested in DRL context and therefore , it lacks a systematic approach to designing the weighting function . TD error-based actor learning is expected to be effective in reducing overestimation error since it is a consistent estimate of the advantage function with lower variance , and it discriminates feedback instead of directly using Q estimates . Some actor-critic variants ( Crites & Barto , 1994 ; Bhatnagar et al. , 2007 ) update the actor based on the sign of a TD error with a positive error preferred in policy updates . However , TD error only measures the discrepancy between the predicted value and the target value , which may not guide exploration effectively , and using TD error alone in actor update may discourage exploration and cause slow learning , especially in high-dimensional complex problems . TD-regularized actor-critic ( Parisi et al. , 2019 ) enhanced the stability of the actor update by using the same TD error ( as that in online critic update ) as a regularization term . However , such use of TD error may not sufficiently evaluate the critic update because it only uses the temporal difference between target and online Q estimates . Additionally , the time-varying regularization coefficient was shown leading to poor convergence ( Chen et al. , 2017 ) . Note also that the TD- regularized actor-critic only considered TD-regularized actor but not the critic . 2 Contributions . 1 ) We introduce a novel TDR mechanism that includes TD-regularized double critic networks and TD-regularized actor network . 2 ) Extensive experiments using DMC benchmarks show that TDR enables SOTA performance ( measureed by learning speed , success rate , variance , and converged reward ) across a wide variety of control tasks , such as locomotion , classical control , and tasks with sparse rewards . 3 ) We also provide qualitative analysis to show that each component of TDR contributes to mitigating both over and under estimation errors . 3 METHOD 3.1 DOUBLE Q IN ACTOR-CRITIC METHOD For a general double Q actor-critic method ( Fujimoto et al. , 2018 ; Haarnoja et al. , 2018 ) . The policy ( πϕ ) is called an actor and the state-action value function ( Qθ ( sk , ak ) ) is called a critic where both the actor and the critic are estimated by deep neural networks with parameters ϕ and θ , respectively . First , consider a policy π that is evaluated by the state-action value function below : Qπ ( sk , ak ) = E [ Rk|sk , ak ] , ( 1 ) where Rk = ( cid:80 ) ∞ t=k γt−krt , sk ∼ p ( · | sk−1 , ak−1 ) , ak = πϕ ( sk ) , and γ ∈ ( 0 , 1 ) . Most actor- critic methods are based on temporal difference ( TD ) learning ( Sutton & Barto , 2018 ) that updates Q estimates by minimizing the TD error , which is obtained from the the difference between a target and a critic estimated value . Next , consider typical double Q methods which entail twin Q networks denoted as Qθ1 and Qθ2 . The respective twin target networks are denoted as Qθ′ . In the upcoming discussions , we also use θ to denote parameters in both Q networks , i.e. , θ= { θ1 , θ2 } . The target value yk is the lesser of the two target values , and Qθ′ 1 2 yk = rk + γ min ζ=1,2 Qθ′ ζ ( sk+1 , πϕ′ ( sk+1 ) ) , ( 2 ) where by taking the minimum of the two target values , it aims to curtail overestimation of Q value frequently experienced by using a single target . Thus the critic value Qθ is updated by minimizing the loss function ( L ( θ ) ) with respect to the critic weights θ : L ( θ ) = Es∼pπ , a∼π [ ( cid:88 ) ( yk − Qθζ ( sk , ak ) ) 2 ] . ( 3 ) ζ=1,2 The actor weights can be updated by the deterministic policy gradient algorithm below ( Silver et al. , 2014 ) , where by convention ( Fujimoto et al. , 2018 ; Haarnoja et al. , 2018 ) , Qθ1 is used to update the actor weights . ∇ϕJ ( ϕ ) = Es∼pπϕ ( cid:104 ) ( cid:105 ) ∇aQθ1 ( sk , ak ) |a=πϕ ( s ) ∇ϕπϕ ( s ) . ( 4 ) Figure 1 : Twin TD-regularized Actor-Critic ( TDR ) Architecture 3 3.2 TWIN TD-REGULARIZED ACTOR-CRITIC ( TDR ) ARCHITECTURE Figure 1 depicts our TDR-based solution mechanisms , which include twin Q networks as in TD3 ( Fujimoto et al. , 2018 ) and SAC ( Haarnoja et al. , 2018 ) , and an actor network . The TDR-based actor and critic updates are different from currently existing methods . In the following , we show how the new TDR selects target value yk different from Equation ( 2 ) as used in SAC and TD3 , and how that helps reduce both overestimation and underestimation errors . We also show how the new TD-regularized actor helps further reduce the estimation bias in the critic . Our TDR-based solutions in Figure 1 include two additional good improvements : distributional learning as in D4PG and long N -step surrogate stage ( LNSS ) method ( Zhong et al. , 2022 ) as described in Appendix A . 3.3 TD-REGULARIZED DOUBLE Q NETWORKS To overcome overestimation , TD3 ( Fujimoto et al. , 2018 ) and SAC ( Haarnoja et al. , 2018 ) train their critic networks to minimize the loss function in Equation ( 3 ) where the target value yk is from Equation ( 2 ) . While this helps reduce overestimation error , it promotes a new problem of underes- timation , which usually occurs during the early stage of learning , or when subjected to corrupted reward feedback or inaccurate states . Our TDR method aims at minimizing the same loss function as in Equation ( 3 ) , but with a different target value yk . Instead of directly choosing the lesser from the two target values as in Equation ( 2 ) , we use the TD errors of the two target networks to set the target value . First , the two TD errors from the respective target networks are determined from : δ′ 1 = rk + γQθ′ δ′ 2 = rk + γQθ′ ( sk+1 , πϕ′ ( sk+1 ) ) − Qθ′ ( sk+1 , πϕ′ ( sk+1 ) ) − Qθ′ ( sk , ak ) , ( sk , ak ) . ( 5 ) ( 6 ) 1 1 2 2 The target value for TDR is then selected from the following : 1 yk = ( cid:26 ) rk + γQθ′ rk + γQθ′ ( sk+1 , πϕ′ ( sk+1 ) ) ( sk+1 , πϕ′ ( sk+1 ) ) Note from Equation ( 7 ) that TDR always uses a target value associated with a smaller target TD value ( regardless of the error sign ) between the two . As the ultimate objective of a target network is to converge to Qπ , such choice by TDR pushes the critic via Equation ( 3 ) toward reaching the target no matter the estimation error is from above or below , but with a smaller TD value . Thus , TDR is naturally positioned to address both overesdiation and underestimation errors . 1| ≤ |δ′ 1| > |δ′ if |δ′ if |δ′ 2| , 2| . ( 7 ) 2 3.4 TD-REGULARIZED ACTOR NETWORK Our TD-regularized actor network directly penalizes the actor ’ s learning objective whenever there is a critic estimation error . The estimation error ∆i+1 of the first critic ( Qθ1 chosen by convention of double Q-based actor-critic methods ) is determined from the following : ∆i+1 = Qθi+1 1 ( sk , ak ) − ( rk + γQθi+1 1 ( sk+1 , πϕ ( sk+1 ) ) ) , ( 8 ) where i + 1 represents the iteration number during critic update . Then the actor can be updated in the direction of maximizing Q while keeping the TD error small , ( cid:20 ) ( cid:21 ) ∇ϕJ ( ϕ ) = Es∼pπϕ ∇a ( Qθi+1 1 ( cid:12 ) ( sk , ak ) − ρ ( ∆i+1 ) ) ( cid:12 ) ( cid:12 ) a=πϕ ( s ) ∇ϕπϕ ( s ) . ( 9 ) where ρ ∈ ( 0 , 1 ) is the regularization coefficient to balance the role of TD error in the actor learning objective . Thus , we expect the TD-regularized actor to help further reduce estimation error in the critic . With TDR actor and cirtic working together hand-in-hand , TDR is positioned to help avoid bad policy updates due to a misleading Q value estimate . Remark 1 . There are a few key differences between TDR and TD-regularized Actor Network ( Parisi et al. , 2019 ) . 1 ) In Equation ( 8 ) , they use the target critic Qθi′ ( sk+1 , πϕ ( sk+1 ) ) to construct TD error , the same as in critic updates . This TD error evaluates the temporal difference between target and online Q estimates . To more accurately evaluate critic estimations , we construct the TD error by only using online critics which directly affects actor updates . 2 ) Their TD error does not sufficiently evaluate how the critic updates . Instead in Equation ( 8 ) , we use the updated critic ( θi+1 ) to construct the TD error to directly measure critic estimation . 1 1 4 4 MITIGATING ESTIMATION BIAS BY TDR Let Qπ be the true Q value obtained by following the current target policy π , and let Qθ be the estimated value using neural networks . Let Ψk θ be a random estimation bias . Then for state-action pairs ( sk , ak ) . we have , Qθ ( sk , ak ) = Qπ ( sk , ak ) + Ψk θ . ( 10 ) The same holds for the target networks , i.e. , when θ is replaced by θ′ in the above equation . An overestimation problem refers to when the estimation bias E [ Ψk θ ] > 0 , and an underestimation problem when the estimation bias E [ Ψk θ ] < 0 . 4.1 MITIGATING ESTIMATION BIAS USING TD-REGULARIZED DOUBLE CRITIC NETWORKS Theorem 1 . Let Qπ be the true Q value following the current target policy π , and Qθ′ and Qθ′ be the target network estimates using double Q neural networks . We assume that there exists a step random estimation bias ψk ( i.e. , estimation bias at the kth stage ) , and that it is independent of θ′ ζ ( sk , ak ) with mean E [ ψk ζ , µ′ ] = µ′ ζ < ∞ , for all k , and ζ = 1 , 2 . Additionally , let δYk denote θ′ ζ the target value estimation error . Accordingly , we denote this error for TDR as δY T DR , and DQ as δY DQ k . We then have the following , k 1 2 Where E [ δY T DR k ] = E [ Qπ − yT DR k ] | ≤ |E [ δY DQ k |E [ δY T DR k ] , and E [ δY DQ k ] = E [ Qπ − yDQ k ] . ] | , ( 11 ) Proof . The proof of Theorem 1 is provided in Appendix B Remark 2 . By selecting a target value with less TD error , our TD-regularized double critic networks mitigate both overestimation and underestimation errors . However , vanilla double Q methods usu- ally push the target toward the lower value no matter the estimation error is over or under . Although this estimation error may not be detrimental as they may be small at each update , the presence of unchecked underestimation bias raises two concerns . Firstly , if there is no sufficient reward feed- back from the environment , ( e.g. , for a noisy reward or sparse reward ) , underestimation bias may not get a chance to make corrections and may develop into a more significant bias over several up- dates . Secondly , this inaccurate value estimate may lead to poor policy updates in which suboptimal actions might be highly rated by the suboptimal critic , reinforcing the suboptimal action in the next policy update . 4.2 ADDRESSING A MISGUIDING CRITIC IN POLICY UPDATES USING TD-REGULARIZED ACTOR Theorem 2 . Let Qπ denote the true Q value following the current target policy π , Qθ1 be the estimated value . We assume that there exists a step random estimation bias ψk that is independent θ1 of ( sk , ak ) with mean E [ ψk ] = µ1 , µ1 < ∞ , for all k. We assume the policy is updated based θ1 on critic Qθ1 using the deterministic policy gradient ( DPG ) as in Equation ( 4 ) . Let δϕk denote the change in actor parameter ϕ updates at stage k. Accordingly , we denote this change for TDR as , and true change without any approximation error in Q as δϕtrue δϕT DR . k k We then have the following , , vanilla DPG as δϕDP G k ( cid:26 ) E [ δϕtrue k E [ δϕtrue k ] ≥ E [ δϕT DR ] ≤ E [ δϕT DR k k ] ≥ E [ δϕDP G ] ≤ E [ δϕDP G k k ] ] if E [ Ψk θ1 if E [ Ψk θ1 ] < 0 , ] ≥ 0 . ( 12 ) Where δϕtrue k Appendix B , δϕDP G k , and δϕT DR k are defined as Equation ( 55 ) , ( 56 ) , and ( 57 ) respectively in Proof . The proof of Theorem 2 is provided in Appendix B . Remark 3 . Theorem 2 , holds for ρ ∈ ( 0 , 1 ) . If the regularization factor ρ = 1 1−γ , from Equation ( 59 ) , we have E [ Ψk ] = E [ δϕT DR ] . By using TDR , θ1 the actor will always update the same way as using the true value . While this is not realistic , the following relationship still preserves |E [ Ψk ] | to help ease the negative effect of θ1 critic estimation bias . − ρ∆ ] = 0 which implies that E [ δϕtrue − ρ∆ ] | ≤ |E [ Ψk θ1 k k 5 4.3 MITIGATING CRITIC ESTIMATION ERROR BY TD-REGULARIZED ACTOR Theorem 3 . Suboptimal actor updates negatively affect the critic . Specifically , consider actor up- dates as in Theorem 2 , in the overestimation case , we have : E [ Qθ1 ( sk , πDP G ( sk ) ] ≥ E [ Qθ1 ( sk , πT DR ( sk ) ) ] ≥ E [ Qπ ( sk , πT rue ( sk ) ) ] , and in the underestimation case , E [ Qθ1 ( sk , πDP G ( sk ) ] ≤ E [ Qθ1 ( sk , πT DR ( sk ) ) ] ≤ E [ Qπ ( sk , πT rue ( sk ) ) ] . ( 13 ) ( 14 ) Proof The proof of Theorem 3 is provided in Appendix B . Remark 4 . For both cases , by using TD-regularized actors , it is expected to result in less estimation bias in the critic . 5 EXPERIMENTS AND RESULTS In this section , we provide a comprehensive evaluation of our TDR enabled actor-critic learning methods based on three commonly used , well-behaved baseline algorithms including SAC , TD3 and D4PG . Additional evaluations are also provided for popular DRL algorithms such as DDPG and PPO to provide a broader perspective on the effectiveness of TDR-based methods . All evaluations are performed based on several benchmarks in Deepmind Control Suite ( Tassa et al. , 2018 ) . In reporting evaluation results , we use the following short-form names : 1 ) Base : the original DRL algorithms including SAC , TD3 , D4PG , DDPG and PPO . 2 ) TDR-TD3 : Applied TD regularized double critic ( TD Critic ) networks , TD regularized actor ( TD Actor ) network , with regularization factor ρ = 0.7 , and LNSS with N = 100 . 3 ) TDR-SAC : Applied TD regularized double critic ( TD Critic ) networks , and LNSS with N = 100 . 4 ) dTDR ( TDR-D4PG ) : Applied TD regularized double critic ( TD Critic ) network , TD regularized actor ( TD Actor ) network , with regularization factor ρ = 0.7 , and LNSS with N = 100 . Our evaluations aim to quantitatively address the following questions : Q1 . How does TDR improve over Base and other common methods ? Q2 . How does the performance of TDR methods compare to that of SOTA algorithms ( D4PG ) ? Q3 . Is TDR method robust enough to handle both dense stochastic reward and sparse reward ? Q4 . How does each component in TDR-based learning mechanisms affect performance ? Q5 . How does TD regularized actor make policy updates in situations of misguiding critics ? Q6 . How does the regularization coefficient ρ in Equation ( 9 ) affect TD Actor performance ? Details of the implementation , training , and evaluation procedures are provided in Appendix C and D where links to all implementation codes are also provided . 5.1 MAIN EVALUATION In obtaining comprehensive evaluation results summarized in Table 1 , we included a 10 % noise respectively in state , action , and reward in each of the considered DMC environments in order to make the evaluations more realistic . In “ Cheetah Run sparse ” , we sparsified the reward in the environment . All details of the environment setup can be found in Appendix C. In Table 1 , “ Success ” is shorthand for learning success rate , “ Avg . Rwd ” for average reward , and “ Rank ” ( % ) is the “ percent of reward difference ” between the evaluated method and the SOTA D4PG , which is ( the average reward of the evaluated method over that of the D4PG - 1 ) , the more positive the better . Note that , in computing the success rate , only those trials that have achieved a reward of at least 10 are accounted for as successful learning . The results are based on the last 50 evaluations of 10 different random seeds ( same for all compared algorithms ) . Best performances are boldfaced for average reward ( Avg . Rwd ) . Note that we did not implement our TD Actor into SAC because SAC already has a max entropy-regulated actor . Q1 TDR improves over respective Base methods . The learning curves for six benchmark environ- ments are shown in Figure 2 . Overall , TDR methods ( solid lines ) outperform their respective Base 6 Figure 2 : Systematic evaluation of TDR realized in three DRL algorithms ( SAC , TD3 , D4PG ) in DMC environments with 10 % uniform random noise in state , action , and reward . The shaded regions represent the 95 % confidence range of the evaluations over 10 seeds . The x-axis is the number of steps . Envirinoment D4PG DDPG PPO SAC TD3 TDR-SAC TDR-TD3 dTDR Envirinoment D4PG DDPG PPO SAC TD3 TDR-SAC TDR-TD3 dTDR Success [ % ] 100 100 100 90 100 100 100 100 Success [ % ] 100 100 20 0 0 100 100 100 Finger Turn Hard Avg . Rwd [ µ ± 2σ ] 400.9 ± 173.4 222.1 ± 160.4 85.9 ± 50 65.6 ± 30.2 205.9 ± 108.5 601.5 ± 147.4 569.8 ± 142.1 841.02 ± 148.3 Acrobot Swingup Avg . Rwd [ µ ± 2σ ] 26.8 ± 8.9 17.2 ± 3.8 7.9 ± 7.8 4 ± 2.2 5.2 ± 4.2 42.9 ± 5.1 50 ± 7.9 62.6 ± 14.4 Rank [ % ] 0 -44.6 -78.6 -83.6 -48.6 49.9 42.3 109.8 Rank [ % ] 0 -35.8 -70.5 -85.1 -80.6 60.1 86.5 133.6 Quadruped Walk Success [ % ] 100 100 100 100 100 100 100 100 Avg . Rwd [ µ ± 2σ ] 858.5 ± 11.4 226.8 ± 133.6 173.1 ± 60.4 196.6 ± 73.7 334.8 ± 76.4 479.5 ± 126.9 475.4 ± 45.4 888.6 ± 15.7 Cartpole Swingup Sparse Rank [ % ] 0 -73.6 -79.8 -77.2 -61 -44.2 -44.6 3.46 Success [ % ] 100 0 80 0 0 100 100 100 Avg . Rwd [ µ ± 2σ ] 493.5 ± 15.9 3.6 ± 5.8 99.2 ± 172.9 1.7 ± 3.4 1.3 ± 2.3 774.2 ± 51.1 790.13 ± 33.0 810.3 ± 34.9 Rank [ % ] 0 -99 -79.9 -99.7 -99.7 56.8 60.1 64.2 Success [ % ] 100 100 100 100 100 100 100 100 Fish Swim Avg . Rwd [ µ ± 2σ ] 153.7 ± 68.1 109.7 ± 27.1 78.67 ± 6.28 73.2 ± 9.87 85.3 ± 21.7 212.3 ± 51.2 204.2 ± 41.5 249.9 ± 45.5 Cheetah Run Sparse Success [ % ] 60 50 0 0 50 100 100 100 Avg . Rwd [ µ ± 2σ ] 532.8 ± 388.4 160.7 ± 284.7 0 ± 0 0 ± 0 220.5 ± 354.7 930.2 ± 18.7 827.8 ± 62.2 900.1 ± 30.8 Rank [ % ] 0 -28.6 -48.8 -52.4 -44.5 37.9 32.7 62 Rank [ % ] 0 -69.8 -100 -100 -58.6 74.6 55.4 68.9 Table 1 : Systematic evaluations of TDR respectively augmented Base algorithms . “ Rank ” ( % ) is the “ percent of reward difference ” between the SOTA D4PG , the more positive the better . methods TD3 , SAC and D4PG ( dash lines ) in terms of episode reward , learning speed , learning variance and success rate . In Table 1 , among the measures , the Avg . Rwd of TDR methods outper- formed respective baseline algorithms . Notice from the table that the learning success rates for all TDR methods are now 100 % , a significant improvement over the Base methods . In comparison , DDPG , SAC and TD3 Base methods struggle with Acrobot Swingup , Cartpole Swingup Sparse , and Cheetah Run Sparse . Moreover , TDR methods also outperform DDPG and PPO in terms of averaged reward ( Awg.Rwd ) , learning speed , learning variance , and success rate . Thus , TDR has helped succesfully address the random initialization challenge caused by random seeds ( Henderson et al. , 2018 ) . Q2 TDR brings performance of Base methods close to or better than that of the SOTA D4PG . From Figure 2 , and according to the “ Rank ” measure in Table 1 , for all environments but Quadruped walk , TDR ( TDR-SAC and TDR-TD3 ) helped enhance the performances of the respective Base methods . Additionally , it even outperformed the SOTA D4PG by around 40 % in the “ Rank ” mea- sure . For Quadruped walk , even though TDR-SAC and TDR-TD3 did not outperform D4PG , they still are the two methods , among all evaluated , that provided closest performance to D4PG . It is also 7 worth noting that TDR brings the performance of D4PG to a new SOTA level measured by mean reward , convergence speed , and learning success rate . Q3 TDR is robust under both dense stochastic reward and sparse reward . From Figure 2 and Table 2 , TDR methods outperformed their respective baselines in both dense stochastic and sparse reward in terms of average reward , learning variance , success rate , and converge speed . In particular , baseline algorithms such as TD3 and SAC struggle with sparse reward benchmark environments ( cartpole swingup sparse and cheetah run sparse ) . However , by using TDR , they not only learned , but also achieved SOTA performance . Methods TD3+TD Critic TD3+LNSS TD3+TD Actor TD3+TDR SAC+TD Critic SAC+LNSS SAC+TDR D4PG+TD Critic D4PG+LNSS D4PG+TD Actor dTDR Acrobot Swingup Finger TurnHard Avg . Rwd [ µ ± 2σ ] 24.9 ± 11.7 24.2 ± 9.2 6.9 ± 2.9 42.9 ± 5.1 28.8 ± 12.2 9.7 ± 2.9 42.9 ± 5.1 32.8 ± 6.9 43.9 ± 16.7 29.9 ± 13.8 62.6 ± 14.4 Enhancement [ % ] 378.8 365.4 32.7 725 620 142.5 972.5 22.4 63.8 11.6 133.6 Avg . Rwd [ µ ± 2σ ] 556.2 ± 239.8 547.5 ± 120.5 212.3 ± 45.7 569.8 ± 142.1 588 ± 223.8 573 ± 156.5 601.5 ± 147.4 835.7 ± 140.9 675.1 ± 217.6 532.5 ± 235.7 841.1 ± 148.3 Enhancement [ % ] 170.1 165.9 3.1 176.7 796.3 773.5 816.9 108.5 68.4 33.5 109.8 Cartpole Swingup Sparse Avg . Rwd [ µ ± 2σ ] 766.2 ± 86.1 766.6 ± 38.3 339.6 ± 231.9 790.1 ± 33.0 766.7 ± 126.4 722.8 ± 162.4 774.2 ± 51.1 678.7 ± 246.2 759.1 ± 31.1 600 ± 129.3 810.3 ± 34.9 Enhancement [ % ] 588.4 588.7 260.2 606.7 449.6 423.7 454.4 37.5 53.8 21.6 64.2 Table 2 : Systematic evaluations of each component of TDR compared to their respective Base al- gorithms . “ Enhancement ” ( % ) is the “ percent of reward difference ” between the respective Base algorithms , the larger the better . Note that TD Actor was not considered for SAC as SAC already has a max entropy-regularized actor . Figure 3 : Evaluation of TD Actor with different ρ ( ρ = 0 , 0.1 , 0.3 , 0.5 , 0.7 , 0.9 ) in Equations ( 9 , 21 ) based on two DRL algorithms ( TD3 , D4PG ) in DMC environments with 10 % uniform random noise in state , action , and reward . The shaded regions represent the 95 % confidence range of the evaluations over 10 seeds . The x-axis is the number of steps . 5.2 ABLATION STUDY To perform the ablation study , we examined TDR by removing each of the following three compo- nents . The respective short-form descriptions are : 1 ) “ TD Critic ” : the TD regularized double Q networks . 2 ) “ TD Actor ” : the TD regularized actor network . 3 ) “ LNSS ” : LNSS method with N = 100 . In Table 2 , “ Enhancement ” ( % ) is the “ percent of reward difference ” between the evaluated method and its Base method , the larger the better . 8 Q4 TD Critic , TD Actor , and LNSS effectively improved the Base algorithms . In Table 2 , TD Critic , LNSS , and TD Actor all effectively improved the Base algorithms . From the table , TD Critic and LNSS have provided comparable and significant enhancement over Base algorithms . As our TD Critic methods outperform respective Base algorithms , this suggests that mitigating estimation errors both over and under from vanilla double Q network is an effective way to improve performance which has also been shown in our theoretical analysis ( Theorem 1 ) . The LNSS method helped improve learning performance by reducing variances in value estimation for noisy rewards as shown both theoretically and empirically ( Zhong et al. , 2022 ) . By including LNSS , our TDR is more robust under noisy and sparse rewards . The TD Actor element also helped make appreciable improvements on learning performance as shown in Table 2 . More importantly , TD Actor plays an importantly role in TDR since it not only stabilizes the policy updates as shown theoretically in Theorem 2 but also addresses the estimation error in critic as shown theoretically in Theorem 3 . 5.3 HYPER PARAMETER STUDY Hyperparameter study results are summarized in Figure 3 where two DRL methods ( D4PG and TD3 ) with TD Actor are evaluated for different regularization factor ρ ( ρ = 0 , 0.1 , 0.3 , 0.5 , 0.7 , 0.9 ) . What is reported is the 10-seed averaged performance , i.e. , the average of the approximate es- timation error which is the difference between the true accumulated reward and the critic value : Ψ = 1 10 t=0 γtrt − Q ( s0 , a0 ) ) . eval=0 ( ( cid:80 ) 999 ( cid:80 ) 9 Q5 TD regularized Actor helps reduce the estimation error in critic . From Figure 3 , with TD regularized Actor ( TD Actor ) , the estimation errors in the critic are re- duced from those without . For example , in Finger Turn hard , D4PG + TD Actor results in less overestimation error compared with ρ = 0 at the later stage of training . TD3 + TD Actor has less underestimation error compared with ρ = 0 . Similarly in cartpole swingup sparse , D4PG + TD Actor results in less overestimation error compared with ρ = 0 . A policy can be evaluated by the “ epois reward ” where a higher epois reward generally results from a better policy . From Figure 3 , policy updates are improved by selecting a suitable regularization fac- tor ρ . Especially , in cartpole swingup sparse , TD3 + TD Actor enables successful learning whereas the Base method struggled and stuck to 0 or no learning for the entire training period . Q6 A range of ρ ( ρ = 0.3 , 0.5 , 0.7 ) generally are good choices . From Figure 3 , a small regular- ization factor ρ = 0.1 in TDR will result in less regularization which may not provide sufficient estimation error reduction in the critic . A larger regularization factor ρ = 0.9 in TDR will result in more regularization and may have a negative effect on learning . Therefore , ρ = 0.3 , 0.5 , 0.7 may be good choices . Therefore in this work , we have consistently used ρ = 0.7 in obtaining all results . 6 CONCLUSION , DISCUSSION , AND LIMITATION OF THE STUDY 1 ) In this work , we introduce a novel TDR mechanism that includes TD-regularized double critic networks and TD-regularized actor network . Both components are shown to help mitigate both over and under estimation errors . TDR has been shown to consistently outperform respective Base algorithms in solving benchmark tasks in terms of average reward , learning success rate , learning speed , and most times , learning variance . 2 ) Our analytical results also show that each component of TDR helps mitigate both over and under estimation errors . 3 ) As shown in Figure 2 , for five out of the six environments ( except quadruped walk ) evaluated , our TDR combined with distributional and LNSS elements has significantly elevated the current SOTA performance of D4PG to a new level with an increase of at least 60 % . Even though we have identified a range of generally good regularization coefficient ρ values ( 0.3 , 0.5 , 0.7 ) , as Figure 3 shows , different algorithms in different environments have responded somewhat differently to ρ . Therefore , how to effectively determine a regularization factor to have the most improvement remains a question , and thus , it is the limitation of this study . Additionally , the promising performances of TDR come after extensive training with millions of learning steps . How TDR performs under limited training time and training steps need to be further investigated . 9 REFERENCES Marcin Andrychowicz , Filip Wolski , Alex Ray , Jonas Schneider , Rachel Fong , Peter Welinder , Bob McGrew , Josh Tobin , Pieter Abbeel , and Wojciech Zaremba . Hindsight experience replay . arXiv preprint arXiv:1707.01495 , 2017 . Gabriel Barth-Maron , Matthew W Hoffman , David Budden , Will Dabney , Dan Horgan , Dhruva Tb , Alistair Muldal , Nicolas Heess , and Timothy Lillicrap . Distributed distributional deterministic policy gradients . arXiv preprint arXiv:1804.08617 , 2018 . Marc G Bellemare , Will Dabney , and R´emi Munos . A distributional perspective on reinforcement learning . In International conference on machine learning , pp . 449–458 . PMLR , 2017 . Shalabh Bhatnagar , Mohammad Ghavamzadeh , Mark Lee , and Richard S Sutton . Incremental nat- ural actor-critic algorithms . Advances in neural information processing systems , 20 , 2007 . Renzhi Chen , Ke Li , and Xin Yao . Dynamic multiobjectives optimization with a changing number of objectives . IEEE Transactions on Evolutionary Computation , 22 ( 1 ) :157–171 , 2017 . Robert Crites and Andrew Barto . An actor/critic algorithm that is equivalent to q-learning . Advances in Neural Information Processing Systems , 7 , 1994 . Will Dabney , Georg Ostrovski , David Silver , and R´emi Munos . Implicit quantile networks for distributional reinforcement learning . In International conference on machine learning , pp . 1096– 1105 . PMLR , 2018a . Will Dabney , Mark Rowland , Marc Bellemare , and R´emi Munos . Distributional reinforcement learning with quantile regression . In Proceedings of the AAAI Conference on Artificial Intelli- gence , volume 32 , 2018b . Jingliang Duan , Yang Guan , Shengbo Eben Li , Yangang Ren , Qi Sun , and Bo Cheng . Distributional soft actor-critic : Off-policy reinforcement learning for addressing value estimation errors . IEEE transactions on neural networks and learning systems , 33 ( 11 ) :6584–6598 , 2021 . Roy Fox , Ari Pakman , and Naftali Tishby . Taming the noise in reinforcement learning via soft updates . arXiv preprint arXiv:1512.08562 , 2015 . Scott Fujimoto , Herke Hoof , and David Meger . Addressing function approximation error in actor- critic methods . In International Conference on Machine Learning , pp . 1587–1596 . PMLR , 2018 . Tuomas Haarnoja , Aurick Zhou , Kristian Hartikainen , George Tucker , Sehoon Ha , Jie Tan , Vikash Kumar , Henry Zhu , Abhishek Gupta , Pieter Abbeel , et al . Soft actor-critic algorithms and appli- cations . arXiv preprint arXiv:1812.05905 , 2018 . G Hailu and G Sommer . On amount and quality of bias in reinforcement learning . In IEEE SMC ’ 99 Conference Proceedings . 1999 IEEE International Conference on Systems , Man , and Cybernetics ( Cat . No . 99CH37028 ) , volume 2 , pp . 728–733 . IEEE , 1999 . Hado Hasselt . Double q-learning . Advances in neural information processing systems , 23 , 2010 . Peter Henderson , Riashat Islam , Philip Bachman , Joelle Pineau , Doina Precup , and David Meger . Deep reinforcement learning that matters . In Proceedings of the AAAI conference on artificial intelligence , volume 32 , 2018 . Qingfeng Lan , Yangchen Pan , Alona Fyshe , and Martha White . Maxmin q-learning : Controlling the estimation bias of q-learning . arXiv preprint arXiv:2002.06487 , 2020 . Felix Leibfried and Jordi Grau-Moya . Mutual-information regularization in markov decision pro- cesses and actor-critic learning . In Conference on Robot Learning , pp . 360–373 . PMLR , 2020 . Wentao Liu , Junmin Zhong , Ruofan Wu , Bretta L Fylstra , Jennie Si , and He Helen Huang . Inferring human-robot performance objectives during locomotion using inverse reinforcement learning and inverse optimal control . IEEE Robotics and Automation Letters , 7 ( 2 ) :2549–2556 , 2022 . 10 Jiafei Lyu , Xiaoteng Ma , Jiangpeng Yan , and Xiu Li . Efficient continuous control with double actors and regularized critics . In Proceedings of the AAAI Conference on Artificial Intelligence , volume 36 , pp . 7655–7663 , 2022 . Fabio Pardo . Tonic : A deep reinforcement learning library for fast prototyping and benchmarking . arXiv preprint arXiv:2011.07537 , 2020 . Simone Parisi , Voot Tangkaratt , Jan Peters , and Mohammad Emtiyaz Khan . Td-regularized actor- critic methods . Machine Learning , 108:1467–1501 , 2019 . Tim GJ Rudner , Cong Lu , Michael A Osborne , Yarin Gal , and Yee Teh . On pathologies in kl- regularized reinforcement learning from expert demonstrations . Advances in Neural Information Processing Systems , 34:28376–28389 , 2021 . David Silver , Guy Lever , Nicolas Heess , Thomas Degris , Daan Wierstra , and Martin Riedmiller . Deterministic policy gradient algorithms . In International conference on machine learning , pp . 387–395 . Pmlr , 2014 . Richard S Sutton and Andrew G Barto . Reinforcement learning : An introduction . MIT press , 2018 . Yuval Tassa , Yotam Doron , Alistair Muldal , Tom Erez , Yazhe Li , Diego de Las Casas , David Bud- den , Abbas Abdolmaleki , Josh Merel , Andrew Lefrancq , et al . Deepmind control suite . arXiv preprint arXiv:1801.00690 , 2018 . Sebastian Thrun and Anton Schwartz . Issues in using function approximation for reinforcement learning . In Proceedings of the Fourth Connectionist Models Summer School , volume 255 , pp . 263 . Hillsdale , NJ , 1993 . Hado Van Hasselt , Arthur Guez , and David Silver . Deep reinforcement learning with double q- learning . In Proceedings of the AAAI conference on artificial intelligence , volume 30 , 2016 . Nino Vieillard , Tadashi Kozuno , Bruno Scherrer , Olivier Pietquin , R´emi Munos , and Matthieu Geist . Leverage the average : an analysis of kl regularization in reinforcement learning . Advances in Neural Information Processing Systems , 33:12163–12174 , 2020 . Wentao Weng , Harsh Gupta , Niao He , Lei Ying , and R Srikant . The mean-squared error of double q-learning . Advances in Neural Information Processing Systems , 33:6815–6826 , 2020 . Ruofan Wu , Junmin Zhong , Brent Wallace , Xiang Gao , He Huang , and Jennie Si . Human-robotic prosthesis as collaborating agents for symmetrical walking . Advances in Neural Information Processing Systems , 35:27306–27320 , 2022 . Ruofan Wu , Junmin Zhong , and Jennie Si . Phased actor in actor-critic reinforcement learning . 2023 . Zongzhang Zhang , Zhiyuan Pan , and Mykel J Kochenderfer . Weighted double q-learning . In IJCAI , pp . 3455–3461 , 2017 . Junmin Zhong , Ruofan Wu , and Jennie Si . Long n-step surrogate stage reward to reduce variances of deep reinforcement learning in complex problems . arXiv preprint arXiv:2210.04820 , 2022 . A DISTRIBUTIONAL TDR AND LNSS The distributional RL ( Bellemare et al. , 2017 ) represents value function in terms of probability distribution rather than function estimates . This distribution provides a more comprehensive rep- resentation of the uncertainty associated with a range of different possible reward returns and state action pairs which can provide more informative value function estimation . Many distributional RL algorithms ( Bellemare et al. , 2017 ; Dabney et al. , 2018b ; a ) has been achieved great performance im- provements on many discrete problems such as Atari benchmarks . D4PG ( Barth-Maron et al. , 2018 ) applied distributional RL into continuous control problem by combining the distributional return function within an actor-critic framework . DSAC ( Duan et al. , 2021 ) address overestimation error by applying distributional RL piggyback on SAC . Although , D4PG and DSAC can provide more accurate critic , the overestimation of actor still exists since the actor is still updated by maximizing the expectation of value function distribution . How to regulate actors in distributional RL in solving overestimations was barely discussed before . 11 A.1 DISTRIBUTIONAL TD-REGULARIZED ACTOR-CRITIC ( DTDR ) Here we tailor a distributional TDR ( dTDR ) method based on the original distributional conceptu- alization developed in D4PG ( Barth-Maron et al. , 2018 ; Bellemare et al. , 2017 ) . We show a number of enhancements in the meantime . Distributional Critic . The distributional critic ( Bellemare et al. , 2017 ) treated the return in Equa- tion 1 as a random variable Z ( sk , ak ) whose expectation is used as the Q value estimate , namely , Q ( sk , ak ) = E [ Z ( sk , ak ) ] . In dTDR however , we use TD errors to evaluate distributional critics . Similar to Equation 5 and 6 , distributional TD errors of the two target networks can be written as : ( sk+1 , πϕ′ ( sk+1 ) ) ] − E [ Zθ′ 1 = rk + γE [ Zθ′ d′ ( sk , ak ) ] , ( 15 ) 1 1 2 = rk + γE [ Zθ′ d′ 2 ( sk+1 , πϕ′ ( sk+1 ) ) ] − E [ Zθ′ 2 ( sk , ak ) ] . The twin TD-regularized target distributional Bellman operator is thus defined as : T Zk D= ( cid:26 ) rk + γZθ′ rk + γZθ′ 1 2 ( sk+1 , πϕ′ ( sk+1 ) ) ( sk+1 , πϕ′ ( sk+1 ) ) if |d′ if |d′ 1| ≤ |d′ 2| 1| > |d′ 2| ( 16 ) ( 17 ) where A D= B denotes that two random variables A and B follow the same probability laws . Al- though the distributional Bellman operator appears similar to Equation 1 , it maps state-action pairs to distributions . As such , we need to define a new TD error measure for the distribution as in D4PG ( Barth-Maron et al. , 2018 ) . We consider using the following distributional loss , L ( θ ) = Es∼pπ , a∼π [ ( cid:88 ) ζ=1,2 l ( T Zk , Zθζ ( sk , ak ) ) ] , ( 18 ) where l measures the distance between two distributions . Many distributional RL algorithms use Kullback-Leibler ( KL ) divergence as the distance metric ( Duan et al. , 2021 ; Barth-Maron et al. , 2018 . We adopt the same metric . Distributional Actor . In most distributional methods ( Barth-Maron et al. , 2018 ; Bellemare et al. , 2017 ) , policy updates are performed based on the policy gradient below , ∇ϕJ ( ϕ ) = Es∼pπϕ [ E [ ∇aZθ ( sk , ak ) ] |a=πϕ ( s ) ∇ϕπϕ ( s ) ] . ( 19 ) In our dTDR , we need to use critic evaluation metrics to evaluate the quality of the current distribu- tional critic and the regularized distributional actor . We first formulate the following loss metric : Lz ( ϕ ) = E [ l ( rk + γZθi+1 1 ( sk+1 , πϕ ( sk+1 ) ) , Zθi+1 1 ( sk , πϕ ( sk ) ) ] . ( 20 ) Similar to TD-regularized actor network , the distributional actor is updated in the direction of max- imizing the expected critic while keeping the expected distance between the projected critic and the critic , namely , ∇ϕJ ( ϕ ) = Es∼pπϕ [ ( E [ ∇aZθi+1 1 ( sk , ak ) ] − ∇aρLz ( ϕ ) ) |a=πϕ ( s ) ∇ϕπϕ ( s ) ] , ( 21 ) where ρ ∈ ( 0 , 1 ) is a regularization coefficient . A.2 LONG N-STEP SURROGATE STAGE ( LNSS ) REWARD LNSS ( Zhong et al. , 2022 ) utilizes a long reward trajectory of N future steps in the estimation of stage reward rk . Using the LNSS-resulted reward r′ k in place of the original rk was shown to effectively reduce learning variance with significant performance improvements for off-policy methods . Given a reward trajectory of N steps from time step k , let G ( sk : k+N −1 , ak : k+N −1 ) ∈ R ( with shorthand notation Gk ) denote the discounted N -step return , i.e. , k+N −1 ( cid:88 ) γt−krt , Gk = t=k 12 ( 22 ) where rt is the tth stage reward and t is from k to k + N − 1 . In LNSS , r′ reward in place of rk in Equation ( 2 ) . To determine r′ N -step reward sequence , namely k is a surrogate stage k , LNSS treat it as a weighted average of the r′ k = γt−krt ( cid:80 ) k+N −1 t=k ( cid:80 ) N −1 n=0 γn . ( 23 ) As Figure 1 shows , Once r′ ( sk , ak , r′ as discussed . k is obtained , it is simply used in place of rk to form a new tuple k , sk+1 ) , which is then stored into the memory buffer D. The TDR method proceeds B ESTIMATION ANALYSIS Lemma 1 . Let Qπ be the true Q value following the current target policy π , and Qθ′ be the target network estimates using double Q neural networks . We assume that there exists a step random estimation bias ψk ( i.e. , estimation bias at the kth stage ) , and that it is independent of ( sk , ak ) with θ′ ζ mean E [ ψk ζ , µ′ ] = µ′ 2 respectively defined in θ′ ζ Equations ( 5 ) and ( 6 ) , we have , ζ < ∞ , for all k , and ζ = 1 , 2 . Then for δ′ 1 and δ′ and Qθ′ 2 1 E [ δ′ E [ δ′ 1 ] = −µ′ 1 , 2 ] = −µ′ 2 . ( 24 ) ( 25 ) ( 26 ) Proof . With the step random estimation bias ψk θ′ ζ , We can rewrite the expectation of Ψk θ′ ζ as E [ Ψk+1 θ′ ζ ] = ∞ ( cid:88 ) t=k+1 γt−k−1E [ ψt θ′ ζ ] = 1 1 − γ µ′ ζ . Then the expectation of the target can be written as , E [ yk ] = E [ rk ] + γE [ ( Qπ ( sk+1 , ak+1 ) + Ψk+1 ∞ ( cid:88 ) θ′ ζ ) ] = E [ rk ] + γ ( E [ γt−k−1rt ] ) + γ 1 − γ µ′ ζ = Qπ ( sk , ak ) + t=k+1 γ 1 − γ µ′ ζ . By using Equations ( 10 ) , and ( 26 ) , the TD errors of the two target critics ( Equations 5 and 6 ) , respectably are : E [ δ′ 1 ] = E [ rk ] + γE [ Qθ′ 1 ( sk+1 , πϕ′ ( sk+1 ) ) ] − E [ Qθ′ 1 1 γ 1 − γ 1 − γ 1 − Qπ ( sk , ak ) − µ′ ( sk , ak ) ] µ′ 1 ( 27 ) = Qπ ( sk , ak ) + Similarly , E [ δ′ = −µ′ 1 . 2 ] = −µ′ 2 . Thus Lemma 1 holds . With Lemma 1 in place , we are now ready to analyze the estimation errors by using TDR and the double Q ( DQ ) method as in TD3 ( Fujimoto et al. , 2018 ) and SAC ( Haarnoja et al. , 2018 ) . Theorem 1 . Let assumptions in Lemma 1 hold , and let δYk denote the target value estimation error . Accordingly , we denote this error for TDR as δY T DR . We then have the following , , and DQ as δY DQ k k |E [ δY T DR k ] | ≤ |E [ δY DQ k ] | . ( 28 ) Proof . The proof is based on enumerating a total of 8 possible scenarios of estimation errors which are determined from the relationships among the two target Q values and the true Qπ value . We provide proofs for the 4 out of 8 unique scenarios below . 13 First note that , E [ δY T DR Case 1 : If the target critic values and the true value Qπ have the following relationship : ] = E [ Qπ − yT DR ] = E [ Qπ − yDQ ] , and E [ δY DQ ] . k k k k i.e , Qθ′ 1 is more underestimated as that implies E [ Qθ′ 1 ] < E [ Qθ′ 2 ] < Qπ , |E [ Ψk θ′ 1 ] | > |E [ Ψk θ′ 2 ] | , |µ′ 1| > |µ′ 2| . Based on Lemma 1 and Equation ( 7 ) , our TDR will use Qθ′ 2 in the target value , E [ yT DR k ] = E [ rk ] + γE [ Qθ′ 2 ( sk+1 , πϕ′ ( sk+1 ) ) ] . However for a vanilla double Q network , the target value will be Qθ′ 1 , ] = E [ rk ] + γE [ Qθ′ Thus based on Equation ( 26 ) , the two estimation errors of the respective target values are ( sk+1 , πϕ′ ( sk+1 ) ) ] . E [ yDQ k 1 |E [ δY T DR k ] | = |E [ Qπ − yT DR k ] | = | |E [ δY DQ k ] | = |E [ Qπ − yDQ k ] | = | µ′ 2| , γ 1 − γ γ µ′ 1 − γ 1| . Since |µ′ 1| > |µ′ 2| , we have |E [ δY T DR k ] | < |E [ δY DQ k ] | . Thus identity ( 28 ) holds . Case 2 : If the target critic values and the true value Qπ have the following relationship : E [ Qθ′ |E [ Qπ − Qθ′ 1 ] < Qπ < E [ Qθ′ 2 ] , ] | > |E [ Qπ − Qθ′ 1 ] | , 2 is expected to be underestimated and Qθ′ is overestimated . Since |E [ Qπ − Qθ′ 1 ] | > 2 then Qθ′ |E [ Qπ − Qθ′ 1 2 we thus have ] | which implies |E [ Ψk θ′ 1 ] | > |E [ Ψk θ′ 2 ] | , |µ′ 1| > |µ′ 2| . Based on Lemma 1 and Equation ( 7 ) , our TDR will use Qθ′ 2 in the target value : E [ yT DR k ] = E [ rk ] + γE [ Qθ′ 2 ( sk+1 , πϕ′ ( sk+1 ) ) ] . However for a vanilla double Q network , the target value will use Qθ′ 1 , ] = E [ rk ] + γE [ Qθ′ Based on Equation ( 26 ) , the two estimation errors of the respective target values are : ( sk+1 , πϕ′ ( sk+1 ) ) ] . E [ yDQ k 1 |E [ δY T DR k ] | = |E [ Qπ − yT DR k ] | = | |E [ δY DQ k ] | = |E [ Qπ − yDQ k ] | = | µ′ 2| , γ 1 − γ γ µ′ 1 − γ 1| , Since |µ′ 1| > |µ′ 2| , we have |E [ δY T DR k ] | < |E [ δY DQ k ] | . Thus identity ( 28 ) holds . Case 3 : If the target critic values and the true value Qπ has the following relationship : ] < Qπ < E [ Qθ′ ] , 2 E [ Qθ′ |E [ Qπ − Qθ′ 1 1 ] | < |E [ Qπ − Qθ′ 2 ] | , 14 ( 29 ) ( 30 ) ( 31 ) ( 32 ) ( 33 ) ( 34 ) ( 35 ) ( 36 ) ( 37 ) ( 38 ) ( 39 ) ( 40 ) ( 41 ) ( 42 ) ( 43 ) is expected to be underestimated and Qθ′ is overestimated . Since |E [ Qπ − Qθ′ 1 ] | < 2 then Qθ′ |E [ Qπ − Qθ′ 1 2 ] | , it implies thus we have |E [ Ψk θ′ 1 ] | < |E [ Ψk θ′ 2 ] | , ( 44 ) 1| < |µ′ Based on Lemma 1 and Equation ( 7 ) , both vanilla double Q network and our TDR will pick Qθ′ the target value : |µ′ 2| . 1 ( 45 ) in E [ yT DR k ] = E [ yDQ k ] = E [ rk ] + γE [ Qθ′ 1 ( sk+1 , πϕ′ ( sk+1 ) ) ] . Then based on Equation ( 26 ) , the two estimation errors of the respective target values are : |E [ δY T DR k ] | = |E [ δY DQ k ] | = |E [ Qπ − yT DR k ] | = | γ 1 − γ µ′ 1| . We thus have |E [ δY T DR k ] | = |E [ δY DQ k ] | . Thus identity ( 28 ) holds . Case 4 : If the target critic values and the true value Qπ has the following relationship where E [ Qθ′ 2 ] is expected more overestimated ie |E [ Ψk θ′ 1 ] | < |E [ Ψk θ′ 2 ] | that implies Qπ < E [ Qθ′ 1 ] < E [ Qθ′ 2 ] , |µ′ 1| < |µ′ 2| . ( 46 ) ( 47 ) ( 48 ) ( 49 ) ( 50 ) Based on Equation ( 24 ) and ( 7 ) , same with vanilla double Q network , our Twin TD-regularized Critic will pick the target value using Qθ′ which both mitigates the larger overestimation bias as : 1 E [ yT DR k ] = E [ yDQ k ] = E [ rk ] + γE [ Qθ′ 1 ( sk+1 , πϕ′ ( sk+1 ) ) ] , which based on Equation ( 26 ) , the two estimation errors of the target value are |E [ δY T DR k ] | = |E [ δY DQ k ] | = |E [ Qπ − yT DR k ] | = | γ 1 − γ µ′ 1| . We have |E [ δY T DR k ] | = |E [ δY DQ k ] | . ( 51 ) ( 52 ) ( 53 ) 1 1 2 k ] > E [ Qθ′ ] < E [ Qθ′ ] | ≤ |E [ δY DQ Thus identity ( 28 ) holds . Both methods can mitigate the overestimation error . Note , the above cases study the relationship of E [ Qθ′ for E [ Qθ′ ] , |E [ δY T DR Theorem 2 . Let Qπ denote the true Q value following the current target policy π , Qθ1 be the estimated value . We assume that there exists a step random estimation bias ψk that is independent θ1 of ( sk , ak ) with mean E [ ψk ] = µ1 , µ1 < ∞ , for all k. We assume the policy is updated based on θ1 critic Qθ1 using the deterministic policy gradient ( DPG ) as in Equation 4 . Let δϕk denote the change in actor parameter ϕ updates at stage k. Accordingly , we denote this change for TDR as δϕT DR , k vanilla DPG as δϕDP G . We then k have the following , , and true change without any approximation error in Q as δϕtrue ] | still valid . Thus Theorem 1 holds . ] and by applying same procedure k k 2 ( cid:26 ) E [ δϕtrue k E [ δϕtrue k ] ≥ E [ δϕT DR ] ≤ E [ δϕT DR k k ] ≥ E [ δϕDP G ] ≤ E [ δϕDP G k k ] ] if E [ Ψk θ1 if E [ Ψk θ1 ] < 0 , ] ≥ 0 . ( 54 ) Proof . With learning rate α , the true change of the actor parameters in case without any approxima- tion error in Q : E [ δϕtrue k ] = αEs∼pπ ( cid:104 ) ϕj ∇aQπ ( sk , ak ) |a=πϕj ( s ) ∇ϕj πϕj ( s ) ( cid:105 ) . ( 55 ) 15 Consider the estimated critic and the true value follow the relationship in Equation 10 . Given the same current policy parameters ϕj , the updated parameters using DPG are : ϕj+1 DP G = ϕj + αEs∼pπ ( cid:104 ) E [ δϕDP G k ] = αEs∼pπ ϕj ( cid:104 ) ϕj ∇a ( Qπ ( sk , ak ) + Ψk θ1 ( cid:105 ) ) ( cid:12 ) ( cid:12 ) a=πϕj ( s ) ∇ϕj πϕj ( s ) , ∇a ( Qπ ( sk , ak ) + Ψk θ1 ) ( cid:12 ) ( cid:12 ) a=πϕj ( s ) ∇ϕj πϕj ( s ) ( cid:105 ) . ( 56 ) With an overestimation bias E [ Ψk θ1 mated actions , and with an underestimation bias E [ Ψk θ1 the underestimated actions . Both result in suboptimal policies . However , by using TD-regularized actor , and given the same current policy parameters ϕj , the actor updates with Equation ( 9 ) are : ] > 0 , the updates encourage more exploration for the overesti- ] < 0 , the updates discourage exploration for ϕj+1 T DR = ϕj + αEs∼pπ ] = αEs∼pπ E [ δϕT DR k ϕj [ ∇a ( Qπ ( sk , ak ) + Ψk θ1 ϕj − ρ ( ∆ ) ) |a=πϕj ( s ) ∇ϕj πϕj ( s ) ] , [ ∇a ( Qπ ( sk , ak ) + Ψk θ1 − ρ ( ∆ ) ) |a=πϕj ( s ) ∇ϕj πϕj ( s ) ] . Similar to Lemma 1 , E [ Ψk θ1 ] = 1 1−γ µ1 , and from Equations ( 8 ) and ( 9 ) we have : E [ ∆ ] = E [ Qθi+1 = µ1 1 ( sk , ak ) ] − E [ ( rk + γQθi+1 1 ( sk+1 , πϕ ( sk+1 ) ) ) ] by selecting ρ ≤ 1 1−γ , we have the following : ( cid:26 ) 0 ≥ E [ Ψk θ1 0 ≤ E [ Ψk θ1 − ρ∆ ] > E [ Ψk θ1 − ρ∆ ] ≤ E [ Ψk θ1 ] ] if E [ Ψk θ1 if E [ Ψk θ1 ] < 0 , ] ≥ 0 . Therefore by inspecting Equations ( 55 ) , ( 56 ) and ( 57 ) , we have : ( cid:26 ) E [ δϕtrue k E [ δϕtrue k Thus Theorem 2 holds . ] ≥ E [ δϕT DR ] ≤ E [ δϕT DR k k ] ≥ E [ δϕDP G ] ≤ E [ δϕDP G k k ] ] if E [ Ψk θ1 if E [ Ψk θ1 ] < 0 , ] ≥ 0 . ( 57 ) ( 58 ) ( 59 ) ( 60 ) Theorem 3 . Suboptimal actor updates negatively affect the critic . Specifically , consider actor up- dates as in Theorem 2 , in the overestimation case , we have : E [ Qθ1 ( sk , πDP G ( sk ) ] ≥ E [ Qθ1 ( sk , πT DR ( sk ) ) ] ≥ E [ Qπ ( sk , πT rue ( sk ) ) ] , ( 61 ) and in the underestimation case , E [ Qθ1 ( sk , πDP G ( sk ) ] ≤ E [ Qθ1 ( sk , πT DR ( sk ) ) ] ≤ E [ Qπ ( sk , πT rue ( sk ) ) ] . ( 62 ) Proof Following the analysis of the TD3 ( Fujimoto et al. , 2018 ) , consider Equation ( 12 ) in Theorem 2 , we have ( cid:26 ) E [ δϕtrue k E [ δϕtrue k ] ≥ E [ δϕT DR ] ≤ E [ δϕT DR k k ] ≥ E [ δϕDP G ] ≤ E [ δϕDP G k k ] ] if E [ Ψk θ1 if E [ Ψk θ1 ] < 0 Underestimate , ] ≥ 0 Overestimate . ( 63 ) In the overestimation case , the approximate value using TDR and vanilla DPG must be E [ Qθ1 ( sk , πDP G ( sk ) ] ≥ E [ Qθ1 ( sk , πT DR ( sk ) ) ] ≥ E [ Qπ ( sk , πtrue ( sk ) ) ] . ( 64 ) Similarly , in the underestimation case , the approximate value using TDR and vanilla DPG must be E [ Qθ1 ( sk , πDP G ( sk ) ] ≤ E [ Qθ1 ( sk , πT DR ( sk ) ) ] ≤ E [ Qπ ( sk , πT rue ( sk ) ) ] . ( 65 ) Thus Theorem 3 holds . 16 C IMPLEMENTATION DETAILS We use PyTorch for all implementations . All results were obtained using our internal server consist- ing of AMD Ryzen Threadripper 3970X Processor , a desktop with Intel Core i7-9700K processor , and two desktops with Intel Core i9-12900K processor . Training Procedure . An episode is initialized by resetting the environment , and terminated at max step T = 1000 . A trial is a complete training process that contains a series of consecutive episodes . Each trial is run for a maximum of 1 × 106 time steps with evaluations at every 2 × 104 time steps . Each task is reported over 10 trials where the environment and the network were initialized by 10 random seeds , ( 0 − 9 ) in this study . For each training trial , to remove the dependency on the initial parameters of a policy , we use a purely exploratory policy for the first 8000 time steps ( start timesteps ) . Afterwards , we use an off-policy exploration strategy , adding Gaussian noise N ( 0 , 0.1 ) to each action . Evaluation Procedure . Every 1 × 104 time steps training , we have an evaluation section and each evaluation reports the average reward over 5 evaluation episodes , with no exploration noise and with fixed policy weights . The random seeds for evaluation are different from those in training which each trial , evaluations were performed using seeds ( seeds + 100 ) . Network Structure and optimizer . TD3.The actor-critic networks in TD3 are implemented by feedforward neural networks with three layers of weights . Each layer has 256 hidden nodes with rectified linear units ( ReLU ) for both the actor and critic . The input layer of actor has the same dimension as observation state . The output layer of the actor has the same dimension as action requirement with a tanh unit . Critic receives both state and action as input to THE first layer and the output layer of critic has 1 linear unit to produce Q value . Network parameters are updated using Adam optimizer with a learning rate of 10−3 for simple control problems . After each time step k , the networks are trained with a mini-batch of a 256 transitions ( s , a , r , s′ ) , ( s , a , r′ , s′ ) in case of LNSS , sampled uniformly from a replay buffer D containing the entire history of the agent . D4PG . Same with the actor-critic networks in D4PG are implemented by feedforward neural net- works with three layers of weights . Each layer has 256 hidden nodes with rectified linear units ( ReLU ) for both the actor and critic . The input layer of actor has the same dimension as observa- tion state . The output layer of the actor has the same dimension as action requirement with a tanh unit . Critic receives both state and action as input to THE first layer and the output layer of critic has a distribution with hyperparameters for the number of atoms l , and the bounds on the support ( Vmin , Vmax ) . Network parameters are updated using Adam optimizer with a learning rate of 10−3 . After each time step k , the networks are trained with a mini-batch of 256 transitions ( s , a , r , s′ ) , ( s , a , r′ , s′ ) in case of LNSS , sampled uniformly from a replay buffer D containing the entire his- tory of the agent . SAC . The actor-critic networks in SAC are implemented by feedforward neural networks with three layers of weights . Each layer has 256 hidden nodes with rectified linear units ( ReLU ) for both the actor and critic . The input layer of actor has the same dimension as observation state . The output layer of the actor has the same dimension as action requirement with a tanh unit . Critic receives both state and action as input to the first layer and the output layer of critic has 1 linear unit to produce Q value . Network parameters are updated using Adam optimizer with a learning rate of 10−3 for simple control problems . After each time step k , the networks are trained with a mini-batch of a 256 transitions ( s , a , r , s′ ) , ( s , a , r′ , s′ ) in case of LNSS , sampled uniformly from a replay buffer D containing the entire history of the agent . Hyperparameters . To keep comparisons in this work fair , we set all common hyperparameters ( network layers , batch size , learning rate , discount factor , number of agents , etc ) to be the same for comparison within the same methods and different methods . For TD3 , target policy smoothing is implemented by adding ϵ ∼ N ( 0 , 0.2 ) to the actions chosen by the target actor-network , clipped to ( −0.5 , 0.5 ) , delayed policy updates consist of only updating 17 the actor and target critic network every d iterations , with d = 2 . While a larger d would result in a larger benefit with respect to accumulating errors , for fair comparison , the critics are only trained once per time step , and training the actor for too few iterations would cripple learning . Both target networks are updated with τ = 0.005 . The TD3 and TD3+TDR used in this study are based on the paper ( Fujimoto et al. , 2018 ) and the code from the authors ( https : //github.com/sfujim/TD3 ) . Hyperparameter TD3 Start timesteps Evaluation frequency Max timesteps Exploration noise Policy noise Noise clip Policy update frequency Batch size Buffer size γ τ Number of parallel actor LNSS-N Adam Learning rate regularization factor Value 8000 steps 20000 steps 1e6 steps N ( 0 , 0.1 ) N ( 0 , 0.2 ) ±0.5 2 256 1e6 0.99 0.005 1 100 1e-3 0.7 Table 3 : TD3 + TDR hyper parameters used for DMC benckmark tasks The SAC used in this study is based on paper ( Haarnoja et al. , 2018 ) and the code is from GitHub ( https : ) . and the hyperparameter is from Table 4 . Value 8000 steps 20000 steps 1e6 steps N ( 0 , 0.1 ) N ( 0 , 0.2 ) ±0.5 2 256 1e6 0.99 0.005 Hyperparameter SAC Start timesteps Evaluation frequency Max timesteps Exploration noise Policy noise Noise clip Policy update frequency Batch size Buffer size γ τ Temperature parameter α 0.2 Number of parallel actor LNSS-N Adam Learning rate 1 100 1e-3 Table 4 : SAC hyper parameters used for the DMC benckmark tasks The D4PG used in this study is based on paper ( Barth-Maron et al. , 2018 ) and the code is modified from TD3 . The hyperparameter is from Table 5 . All Other algorithms are from the same DRL training platform ( Tonic RL ) ( Pardo , 2020 ) with the same evaluation as the above algorithms . Sparse Reward Setup . 1 ) Cheetah Run Sparse : Cheetah needs to run forward as fast as possible . The agent gets a reward only after speed exceeds 2.5 m/s , making the reward sparse . r = 1 . That is , if v > = 2.5 else r = 0 . 18 Hyperparameter D4PG Start timesteps Evaluation frequency Max timesteps Exploration noise Noise clip Batch size Buffer size γ τ Number of parallel actor LNSS-N Adam Learning rate Vmax Vmin l regularization factor Value 8000 steps 20000 steps 1e6 steps N ( 0 , 0.1 ) ±0.5 256 1e6 0.99 0.005 1 100 1e-3 100 0 51 0.7 Table 5 : D4PG + TDR hyper parameters used for the DMC benckmark tasks 19 D TDR ALGORITHMS DETAILS In this section , we show our TDR-based algorithms . TD3-TDR is shown in Algorithm 1 , SAC- TDR is shown in Algorithm 2 , and D4PG-TDR is shown in Algorithm 3 . We mainly add LNSS reward to the sample collection part . In algorithm update part , we mainly modify the target value selection using Equation 7 for regular DRL and Equation ( 17 ) for distributional DRL . Additionally , if applicable , we modify the actor gradient based on Equation 9 for regular DRL and Equation ( 21 ) for distributional DRL . All codes will be released to GitHub once the paper get accepted . Algorithm 1 TD3-TDR Initialize : 1 ← θ1 , θ′ 2 ← θ2 , ϕ′ ← ϕ , • Critic networks Qθ1 , Qθ2 and actor-network πϕ with random parameters , θ1 , θ2 , ϕ • Target networks θ′ • an experience buffer D • a temporary experience buffer D′ with size N • Total training episode T 1 . For episode = 1 , T do 2 . Reset initialize state s0 , D′ For k = 0 , T do 3 . 4 . 5 . 6 . 7 . 8 . 9 . 10 . 11 . 12 . 13 . 14 . 15 . 16 . 17 . 18 . 19 . 20 . 21 . 22 . 23 . 24 . 25 . Choose an action ak based on current state sk and learned policy from A . Execute the action ak and observe a new state sk+1 with reward signal rk Store the transition ( sk , ak , rk , sk+1 ) in D′ if k + N − 1 ≤ T then 1 ) in the D′ 0 , s′ 0 , r′ 0 , a′ Get earliest memory ( s′ Calculate r′ based on Equation ( 23 ) 0 , a′ Store the transition ( s′ Clear original transition ( s′ 0 , r′ , s′ 0 , a′ 1 ) in D 0 , s′ 0 , r′ 1 ) in the D′ else Repeat step 8 to 11 and Calculate r′ based on Equation r′ k = γ − 1 γT −k+1 − 1 T ( cid:88 ) t=k γt−krt . ( 66 ) t , st+1 ) from D end if Sample mini-batch data ( st , at , r′ Get next action at+1 ← πϕ′ ( st+1 ) Target value yt based on Equation ( 7 ) Update Critics based on Equation 3 if k mod Policy Update frequency then Update ϕ by Equation 9 Update target networks : θ′ ζ ← τ θζ + ( 1 − τ ) θ′ ζ ϕ′ ← τ ϕ + ( 1 − τ ) ϕ′ end if end for 26 . 27. end for 20 Algorithm 2 SAC-TDR Initialize : • Soft value function VΞ , target Soft value function V ′ Ξ , Critic networks Qθ1 , Qθ2 and actor- network πϕ with random parameters , θ1 , θ2 , ϕ • Target networks Ξ′ ← Ξ • an experience buffer D • a temporary experience buffer D′ with size N • Total training episode T 1 . For episode = 1 , T do 2 . Reset initialize state s0 , D′ For k = 0 , T do 3 . 4 . 5 . 6 . 7 . 8 . 9 . 10 . 11 . 12 . 13 . 14 . 15 . 16 . 17 . 18 . 19 . 20 . 21 . 22 . 23 . Choose an action ak based on current state sk and learned policy from A . Execute the action ak and observe a new state sk+1 with reward signal rk Store the transition ( sk , ak , rk , sk+1 ) in D′ if k + N − 1 ≤ T then 1 ) in the D′ 0 , s′ 0 , r′ 0 , a′ Get earliest memory ( s′ Calculate r′ based on Equation ( 23 ) Store the transition ( s′ 0 , a′ Clear original transition ( s′ 0 , r′ , s′ 0 , a′ 1 ) in D 0 , s′ 0 , r′ 1 ) in the D′ else Repeat step 8 to 11 and Calculate r′ based on Equation r′ k = γ − 1 γT −k+1 − 1 T ( cid:88 ) t=k γt−krt . ( 67 ) end if Sample mini-batch data ( st , at , r′ Get next action at+1 ← πϕ′ ( st+1 ) Target value yt based on Equation ( 7 ) Update Critics based on Equation 3 t , st+1 ) from D Update Soft value function based on original SAC formualtion Update ϕ by original SAC formulation Update target networks : Ξ′ ← τ Ξ + ( 1 − τ ) Ξ′ end for 24 . 25. end for 21 Algorithm 3 D4PG-TDR Initialize : 1 ← θ1 , θ′ 2 ← θ2 , ϕ′ ← ϕ , • Critic networks Zθ1 , Zθ2 and actor-network πϕ with random parameters , θ1 , θ2 , ϕ • Target networks θ′ • an experience buffer D • a temporary experience buffer D′ with size N • Total training episode T 1 . For episode = 1 , T do 2 . Reset initialize state s0 , D′ For k = 0 , T do 3 . 4 . 5 . 6 . 7 . 8 . 9 . 10 . 11 . 12 . 13 . 14 . 15 . 16 . 17 . 18 . 19 . 20 . 21 . 22 . 23 . 24 . 25 . Choose an action ak based on current state sk and learned policy from A . Execute the action ak and observe a new state sk+1 with reward signal rk Store the transition ( sk , ak , rk , sk+1 ) in D′ if k + N − 1 ≤ T then 1 ) in the D′ 0 , s′ 0 , r′ 0 , a′ Get earliest memory ( s′ Calculate r′ based on Equation ( 23 ) 0 , a′ Store the transition ( s′ Clear original transition ( s′ 0 , r′ , s′ 0 , a′ 1 ) in D 0 , s′ 0 , r′ 1 ) in the D′ else Repeat step 8 to 11 and Calculate r′ based on Equation r′ k = γ − 1 γT −k+1 − 1 T ( cid:88 ) t=k γt−krt . ( 68 ) end if Sample mini-batch data ( st , at , r′ Get next action at+1 ← πϕ′ ( st+1 ) Target distribution based on Equation ( 17 ) t , st+1 ) from D Update Critics based on Equation 18 if k mod Policy Update frequency then Update ϕ by Equation 21 Update target networks : θ′ ζ ← τ θζ + ( 1 − τ ) θ′ ζ ϕ′ ← τ ϕ + ( 1 − τ ) ϕ′ end if end for 26 . 27. end for 22","['n', 'l', 'c', 'r', 'mitigating', 'estimation', 'error', 'twin', 'td', 'regularize', 'actor', 'critic', 'deep', 'rein', 'learn', 'abstract', 'address', 'issue', 'estimation', 'bias', 'deep', 'reinforcement', 'learning', 'introduce', 'solution', 'mechanism', 'include', 'new', 'twin', 'tdregularize', 'actor', 'critic', 'method', 'aim', 'reduce', 'estimation', 'error', 'combine', 'good', 'drl', 'improvement', 'distributional', 'learning', 'long', 'n', 'step', 'surrogate', 'stage', 'reward', 'method', 'show', 'new', 'tdrbased', 'actorcritic', 'learning', 'enable', 'drl', 'method', 'outper', 'form', 'respective', 'baseline', 'challenge', 'environment', 'deepmind', 'control', 'suite', 'furthermore', 'elevate', 'td3', 'sac', 'respectively', 'level', 'perfor', 'mance', 'comparable', 'current', 'sota', 'also', 'improve', 'performance', 'new', 'sota', 'level', 'measure', 'mean', 'reward', 'conver', 'gence', 'speed', 'learn', 'success', 'rate', 'learn', 'variance', 'introduction', 'reinforcement', 'learning', 'rl', 'develop', 'decade', 'provide', 'mathematical', 'formal', 'ism', 'learningbased', 'control', 'recently', 'significant', 'progress', 'make', 'attain', 'excellent', 'result', 'wide', 'range', 'highdimensional', 'continuous', 'stateaction', 'space', 'problem', 'especially', 'robotic', 'application', 'robot', 'manipulation', 'andrychowicz', 'humanrobotic', 'interaction', 'however', 'fundamental', 'issue', 'estimation', 'error', 'associate', 'actorcritic', 'hasselt', 'still', 'pose', 'great', 'challenge', 'overestimation', 'example', 'use', 'operator', 'update', 'identify', 'study', 'thrun', 'schwartz', 'reduce', 'effort', 'focus', 'attain', 'accurate', 'stable', 'critic', 'network', 'td3', 'fujimoto', 'apply', 'clip', 'double', 'qlearning', 'take', 'minimum', 'estimate', 'sac', 'utilize', 'double', 'q', 'network', 'incorporate', 'entropy', 'regularization', 'critic', 'objective', 'function', 'ensure', 'exploratory', 'behavior', 'help', 'alleviate', 'overestimation', 'problem', 'however', 'directly', 'take', 'minimum', 'value', 'target', 'network', 'td3', 'sac', 'report', 'result', 'underestimation', 'bias', 'fujimoto', 'evaluation', 'reveal', 'multiple', 'role', 'estimation', 'error', 'learn', 'hand', 'overestimation', 'always', 'harmful', 'lan', 'consider', 'play', 'role', 'encourage', 'exploration', 'overestimated', 'action', 'line', 'underestimation', 'bias', 'discourage', 'exploration', 'overestimation', 'bias', 'occur', 'highvalue', 'region', 'contain', 'optimal', 'policy', 'encourage', 'exploration', 'good', 'thing', 'hailu', 'sommer', 'hand', 'overestimation', 'bias', 'also', 'cause', 'agent', 'overly', 'explore', 'lowvalue', 'region', 'lead', 'suboptimal', 'policy', 'accordingly', 'underestimation', 'bias', 'discourage', 'agent', 'explore', 'highvalue', 'region', 'avoid', 'lowvalue', 'region', 'thing', 'consider', 'estimation', 'error', 'leave', 'unchecked', 'accumulate', 'negatively', 'impact', 'policy', 'update', 'suboptimal', 'action', 'highly', 'rate', 'suboptimal', 'critic', 'reinforce', 'suboptimal', 'action', 'next', 'policy', 'update', 'fujimoto', 'aside', 'anecdotal', 'evidence', 'role', 'estimation', 'mitigate', 'principled', 'way', 'remain', 'open', 'issue', 'asuedu', 'several', 'method', 'evaluation', 'perform', 'show', 'promise', 'major', 'tool', 'mostly', 'leave', 'thus', 'far', 'still', 'clear', 'possible', 'far', 'reduce', 'estimation', 'error', 'consider', 'actor', 'give', 'interplay', 'actor', 'critic', 'handful', 'approach', 'examine', 'show', 'demonstrate', 'performance', 'improvement', 'paac', 'use', 'phase', 'actor', 'account', 'q', 'value', 'td', 'error', 'actor', 'update', 'double', 'actor', 'idea', 'propose', 'evaluate', 'take', 'minimum', 'value', 'estimate', 'associate', 'actor', 'network', 'however', 'directly', 'use', 'minimum', 'estimate', 'value', 'show', 'result', 'underestimation', 'error', 'similar', 'td3', 'method', 'fox', 'mutualinformation', 'leibfrie', 'graumoya', 'vieillard', 'rudner', 'regularization', 'also', 'use', 'enhance', 'policy', 'exploration', 'robustness', 'stability', 'tdregularize', 'actorcritic', 'parisi', 'regularize', 'actor', 'aim', 'enhance', 'stability', 'actor', 'learn', 'apply', 'td', 'error', 'online', 'critic', 'update', 'regularization', 'term', 'actor', 'update', 'however', 'none', 'method', 'show', 'regularization', 'actor', 'help', 'reduce', 'estimation', 'error', 'critic', 'paper', 'propose', 'new', 'tdregularize', 'learning', 'mechanism', 'include', 'td', 'regularize', 'double', 'critic', 'network', 'tdregularize', 'actor', 'network', 'new', 'architecture', 'several', 'property', 'make', 'ideal', 'enhancement', 'consider', 'tdregularize', 'double', 'critic', 'network', 'instead', 'directly', 'select', 'minimum', 'value', 'twin', 'target', 'network', 'select', 'target', 'base', 'minimum', 'td', 'error', 'address', 'overestimation', 'underestimation', 'problem', 'tdregularize', 'actor', 'network', 'formulate', 'new', 'td', 'error', 'regularize', 'actor', 'update', 'avoid', 'misleading', 'critic', 'regularization', 'term', 'help', 'far', 'reduce', 'estimation', 'error', 'critic', 'update', 'additionally', 'apply', 'combine', 'distributional', 'bellemare', 'lns', 'reward', 'estimation', 'far', 'improve', 'learn', 'stability', 'performance', 'relate', 'work', 'shed', 'light', 'novelty', 'method', 'discuss', 'double', 'critic', 'network', 'errorbase', 'actor', 'learn', 'provide', 'backdrop', 'include', 'review', 'distributional', 'bellemare', 'longn', 'step', 'surrogate', 'stage', 'lnss', 'method', 'double', 'critic', 'network', 'use', 'rl', 'hasselt', 'drl', 'fujimoto', 'hasselt', 'double', 'q', 'learn', 'hasselt', 'hasselt', 'first', 'show', 'reduction', 'overestimation', 'bias', 'td3', 'fujimoto', 'sac', 'also', 'show', 'effective', 'apply', 'clip', 'double', 'qlearning', 'use', 'minimum', 'q', 'estimate', 'however', 'method', 'induce', 'underestimation', 'bias', 'problem', 'hasselt', 'fujimoto', 'consequently', 'weight', 'double', 'q', 'learn', 'propose', 'deal', 'overestimation', 'underestimation', 'bias', 'however', 'method', 'test', 'drl', 'context', 'therefore', 'lack', 'systematic', 'approach', 'design', 'weighting', 'function', 'errorbase', 'actor', 'learning', 'expect', 'effective', 'reduce', 'overestimation', 'error', 'consistent', 'estimate', 'advantage', 'function', 'low', 'variance', 'discriminate', 'feedback', 'instead', 'directly', 'use', 'q', 'estimate', 'actorcritic', 'variant', 'crite', 'bhatnagar', 'update', 'actor', 'base', 'sign', 'error', 'positive', 'error', 'prefer', 'policy', 'update', 'however', 'error', 'measure', 'discrepancy', 'predict', 'value', 'target', 'value', 'guide', 'exploration', 'effectively', 'use', 'td', 'error', 'alone', 'actor', 'update', 'discourage', 'exploration', 'cause', 'slow', 'learning', 'especially', 'highdimensional', 'complex', 'problem', 'tdregularize', 'actorcritic', 'parisi', 'enhance', 'stability', 'actor', 'update', 'use', 'error', 'online', 'critic', 'update', 'regularization', 'term', 'however', 'use', 'td', 'error', 'sufficiently', 'evaluate', 'critic', 'update', 'use', 'temporal', 'difference', 'target', 'online', 'q', 'estimate', 'additionally', 'timevarying', 'regularization', 'coefficient', 'show', 'lead', 'poor', 'convergence', 'note', 'also', 'td', 'regularize', 'actorcritic', 'consider', 'tdregularize', 'actor', 'critic', 'contribution', 'introduce', 'novel', 'tdr', 'mechanism', 'tdregularize', 'double', 'critic', 'network', 'tdregularize', 'actor', 'network', 'extensive', 'experiment', 'use', 'dmc', 'benchmark', 'show', 'enable', 'sota', 'performance', 'measureed', 'learn', 'speed', 'success', 'rate', 'variance', 'converge', 'reward', 'wide', 'variety', 'control', 'task', 'locomotion', 'classical', 'control', 'task', 'sparse', 'reward', 'also', 'provide', 'qualitative', 'analysis', 'show', 'component', 'contribute', 'mitigate', 'estimation', 'error', 'method', 'double', 'q', 'actorcritic', 'method', 'general', 'double', 'q', 'actorcritic', 'method', 'fujimoto', 'policy', 'call', 'actor', 'stateaction', 'value', 'function', 'call', 'critic', 'actor', 'critic', 'estimate', 'deep', 'neural', 'network', 'parameter', 'respectively', 'first', 'consider', 'policy', 'π', 'evaluate', 'stateaction', 'value', 'function', 'e', 'rk', 'tk', 'γt−krt', 'sk', '∼', 'p', 'sk', 'actor', 'critic', 'method', 'base', 'temporal', 'difference', 'learn', 'sutton', 'update', 'q', 'estimate', 'minimize', 'error', 'obtain', 'difference', 'target', 'critic', 'estimate', 'value', 'next', 'consider', 'typical', 'double', 'q', 'method', 'entail', 'network', 'denote', 'qθ1', 'qθ2', 'respective', 'twin', 'target', 'network', 'denote', 'upcoming', 'discussion', 'also', 'use', 'θ', 'denote', 'parameter', 'q', 'network', 'θ', 'target', 'value', 'yk', 'less', 'target', 'value', 'yk', 'rk', 'sk1', 'πϕ′', 'sk1', 'take', 'minimum', 'target', 'value', 'aim', 'curtail', 'overestimation', 'q', 'value', 'frequently', 'experience', 'use', 'single', 'target', 'thus', 'critic', 'value', 'qθ', 'update', 'minimize', 'loss', 'function', 'l', 'θ', 'respect', 'critic', 'weight', 'θ', 'l', 'θ', 'ζ12', 'actor', 'weight', 'update', 'deterministic', 'policy', 'gradient', 'silver', 'convention', 'fujimoto', 'qθ1', 'use', 'update', 'actor', 'weight', 'es∼pπϕ', 'cid104', '∇aqθ1', 'aπϕ', 'figure', 'twin', 'tdregularize', 'actorcritic', 'tdr', 'architecture', 'twin', 'tdregularize', 'actorcritic', 'tdr', 'architecture', 'figure', 'depict', 'tdrbased', 'solution', 'mechanism', 'include', 'twin', 'q', 'network', 'td3', 'fujimoto', 'sac', 'actor', 'network', 'tdrbased', 'actor', 'critic', 'update', 'different', 'currently', 'exist', 'method', 'following', 'show', 'new', 'select', 'target', 'value', 'yk', 'different', 'equation', 'use', 'sac', 'td3', 'help', 'reduce', 'overestimation', 'underestimation', 'error', 'also', 'show', 'new', 'tdregularize', 'actor', 'help', 'far', 'reduce', 'estimation', 'bias', 'critic', 'tdrbased', 'solution', 'figure', 'include', 'additional', 'good', 'improvement', 'distributional', 'learning', 'long', 'n', 'step', 'surrogate', 'stage', 'lnss', 'method', 'describe', 'tdregularize', 'double', 'q', 'network', 'overcome', 'overestimation', 'td3', 'fujimoto', 'sac', 'train', 'critic', 'network', 'minimize', 'loss', 'function', 'equation', 'target', 'value', 'yk', 'equation', 'help', 'reduce', 'overestimation', 'error', 'promote', 'new', 'problem', 'underes', 'timation', 'usually', 'occur', 'early', 'stage', 'learn', 'subject', 'corrupted', 'reward', 'feedback', 'inaccurate', 'state', 'method', 'aim', 'minimize', 'loss', 'function', 'equation', 'different', 'target', 'value', 'yk', 'instead', 'directly', 'choose', 'less', 'target', 'value', 'equation', 'use', 'error', 'target', 'network', 'set', 'target', 'value', 'first', 'error', 'respective', 'target', 'network', 'determine', 'rk', 'rk', 'sk1', 'πϕ′', 'sk1', 'sk1', 'πϕ′', 'sk1', 'target', 'value', 'select', 'following', 'yk', 'cid26', 'rk', 'rk', 'sk1', 'πϕ′', 'sk1', 'sk1', 'πϕ′', 'sk1', 'note', 'equation', 'always', 'use', 'target', 'value', 'associate', 'small', 'target', 'value', 'regardless', 'error', 'sign', 'ultimate', 'objective', 'target', 'network', 'converge', 'choice', 'push', 'critic', 'equation', 'reach', 'target', 'matter', 'estimation', 'error', 'small', 'td', 'value', 'thus', 'naturally', 'position', 'address', 'overesdiation', 'underestimation', 'error', '≤', 'δ′', 'δ′', 'tdregularize', 'actor', 'network', 'tdregularize', 'actor', 'network', 'directly', 'penalize', 'actor', 'learn', 'objective', 'critic', 'estimation', 'error', 'estimation', 'error', 'first', 'critic', 'qθ1', 'choose', 'convention', 'double', 'qbase', 'actorcritic', 'method', 'determine', 'follow', 'qθi1', 'rk', 'γqθi1', 'sk1', 'sk1', 'represent', 'iteration', 'number', 'critic', 'update', 'actor', 'update', 'direction', 'maximize', 'q', 'keep', 'td', 'error', 'small', 'cid20', 'qθi1', 'aπϕ', 'ρ', 'regularization', 'coefficient', 'balance', 'role', 'td', 'error', 'actor', 'learn', 'objective', 'thus', 'expect', 'tdregularize', 'actor', 'help', 'far', 'reduce', 'estimation', 'error', 'critic', 'actor', 'cirtic', 'work', 'together', 'position', 'help', 'avoid', 'bad', 'policy', 'update', 'misleading', 'q', 'value', 'estimate', 'remark', 'key', 'difference', 'tdregularize', 'actor', 'network', 'parisi', 'equation', 'use', 'target', 'critic', 'sk1', 'sk1', 'construct', 'error', 'critic', 'update', 'error', 'evaluate', 'temporal', 'difference', 'target', 'online', 'q', 'estimate', 'accurately', 'evaluate', 'critic', 'estimation', 'construct', 'td', 'error', 'use', 'online', 'critic', 'directly', 'affect', 'actor', 'update', 'error', 'sufficiently', 'evaluate', 'critic', 'update', 'instead', 'equation', 'use', 'update', 'critic', 'θi1', 'construct', 'td', 'error', 'directly', 'measure', 'critic', 'estimation', 'mitigating', 'estimation', 'bias', 'let', 'qπ', 'true', 'q', 'value', 'obtain', 'follow', 'current', 'target', 'policy', 'π', 'let', 'qθ', 'estimate', 'value', 'use', 'neural', 'network', 'let', 'random', 'estimation', 'bias', 'stateaction', 'pair', 'θ', 'hold', 'target', 'network', 'replace', 'θ′', 'equation', 'overestimation', 'problem', 'refer', 'estimation', 'underestimation', 'problem', 'estimation', 'mitigate', 'estimation', 'bias', 'use', 'tdregularize', 'double', 'critic', 'network', 'theorem', 'let', 'qπ', 'true', 'q', 'value', 'follow', 'current', 'target', 'policy', 'π', 'target', 'network', 'estimate', 'use', 'double', 'q', 'neural', 'network', 'assume', 'exist', 'step', 'random', 'estimation', 'bias', 'ie', 'estimation', 'bias', 'kth', 'stage', 'independent', 'θ′', 'ζ', 'e', '∞', 'k', 'ζ', 'additionally', 'let', 'δyk', 'denote', 'θ′', 'ζ', 'target', 'value', 'estimation', 'error', 'accordingly', 'denote', 'error', 'follow', 'k', 'e', 'e', 'e', 'e', 'e', 'k', 'proof', 'proof', 'theorem', 'provide', 'b', 'remark', 'select', 'target', 'value', 'less', 'error', 'tdregularize', 'double', 'critic', 'network', 'mitigate', 'overestimation', 'underestimation', 'error', 'however', 'vanilla', 'double', 'q', 'method', 'ally', 'push', 'target', 'low', 'value', 'matter', 'estimation', 'error', 'estimation', 'error', 'detrimental', 'small', 'update', 'presence', 'unchecked', 'underestimation', 'bias', 'raise', 'concern', 'firstly', 'sufficient', 'reward', 'feed', 'back', 'environment', 'noisy', 'reward', 'sparse', 'reward', 'underestimation', 'bias', 'get', 'chance', 'make', 'correction', 'develop', 'significant', 'bias', 'several', 'date', 'secondly', 'inaccurate', 'value', 'estimate', 'lead', 'poor', 'policy', 'update', 'suboptimal', 'action', 'highly', 'rate', 'suboptimal', 'critic', 'reinforce', 'suboptimal', 'action', 'next', 'policy', 'update', 'address', 'misguide', 'critic', 'policy', 'update', 'use', 'tdregularize', 'actor', 'theorem', 'let', 'denote', 'true', 'q', 'value', 'follow', 'current', 'target', 'policy', 'π', 'qθ1', 'estimate', 'value', 'assume', 'exist', 'step', 'random', 'estimation', 'bias', 'ψk', 'independent', 'θ1', 'e', 'ψk', '∞', 'k', 'assume', 'policy', 'update', 'base', 'critic', 'qθ1', 'use', 'deterministic', 'policy', 'gradient', 'dpg', 'equation', 'let', 'δϕk', 'denote', 'change', 'actor', 'update', 'stage', 'accordingly', 'denote', 'change', 'true', 'change', 'approximation', 'error', 'q', 'follow', 'vanilla', 'dpg', 'δϕdp', 'e', 'δϕdp', 'e', 'δϕdp', 'e', 'ψk', 'e', '≥', 'δϕdp', 'define', 'equation', 'respectively', 'proof', 'proof', 'theorem', 'provide', 'remark', 'theorem', 'hold', 'regularization', 'factor', 'equation', 'e', 'ψk', 'e', 'use', 'θ1', 'actor', 'always', 'update', 'way', 'use', 'true', 'value', 'realistic', 'follow', 'relationship', 'still', 'preserve', 'e', 'ψk', 'help', 'ease', 'negative', 'effect', 'critic', 'estimation', 'bias', 'imply', 'e', 'δϕtrue', '≤', 'e', 'mitigate', 'critic', 'estimation', 'error', 'tdregularize', 'actor', 'theorem', 'suboptimal', 'actor', 'update', 'negatively', 'affect', 'critic', 'specifically', 'consider', 'actor', 'date', 'theorem', 'overestimation', 'case', 'e', 'qθ1', '≥', 'e', 'qθ1', 'πt', 'dr', '≥', 'πt', 'underestimation', 'case', 'e', 'qθ1', '≤', 'e', 'qθ1', 'πt', 'dr', 'e', 'πt', 'proof', 'proof', 'theorem', 'provide', 'remark', 'case', 'use', 'tdregularize', 'actor', 'expect', 'result', 'less', 'estimation', 'bias', 'critic', 'experiment', 'result', 'section', 'provide', 'comprehensive', 'evaluation', 'enable', 'actorcritic', 'learning', 'method', 'base', 'commonly', 'use', 'wellbehave', 'baseline', 'algorithm', 'include', 'sac', 'td3', 'additional', 'evaluation', 'also', 'provide', 'popular', 'drl', 'algorithm', 'ddpg', 'ppo', 'provide', 'broad', 'perspective', 'effectiveness', 'tdrbase', 'method', 'evaluation', 'perform', 'base', 'several', 'benchmark', 'deepmind', 'control', 'tassa', 'report', 'evaluation', 'result', 'use', 'follow', 'shortform', 'name', 'base', 'original', 'drl', 'algorithm', 'include', 'sac', 'td3', 'ddpg', 'ppo', 'tdrtd3', 'apply', 'td', 'regularize', 'double', 'critic', 'critic', 'network', 'td', 'regularize', 'actor', 'actor', 'network', 'regularization', 'factor', 'lns', 'n', 'tdrsac', 'apply', 'regularize', 'double', 'critic', 'critic', 'network', 'lns', 'n', 'dtdr', 'apply', 'td', 'regularize', 'double', 'critic', 'critic', 'network', 'td', 'regularize', 'actor', 'actor', 'network', 'regularization', 'factor', 'lns', 'n', 'evaluation', 'aim', 'quantitatively', 'address', 'follow', 'question', 'q1', 'improve', 'base', 'common', 'method', 'q2', 'performance', 'method', 'compare', 'sota', 'algorithm', 'method', 'robust', 'enough', 'handle', 'dense', 'stochastic', 'reward', 'sparse', 'reward', 'component', 'tdrbased', 'learning', 'mechanism', 'affect', 'performance', 'td', 'regularize', 'actor', 'make', 'policy', 'update', 'situation', 'misguide', 'critic', 'q6', 'regularization', 'coefficient', 'equation', 'affect', 'actor', 'performance', 'detail', 'implementation', 'training', 'evaluation', 'procedure', 'provide', 'c', 'link', 'implementation', 'code', 'also', 'provide', 'main', 'evaluation', 'obtain', 'comprehensive', 'evaluation', 'result', 'summarize', 'table', 'include', 'noise', 'respectively', 'state', 'action', 'reward', 'consider', 'dmc', 'environment', 'order', 'make', 'evaluation', 'realistic', 'run', 'sparse', 'sparsifie', 'reward', 'environment', 'detail', 'environment', 'setup', 'find', 'c', 'table', 'success', 'shorthand', 'learn', 'success', 'rate', 'avg', 'rwd', 'average', 'reward', 'rank', 'percent', 'reward', 'difference', 'evaluate', 'method', 'sota', 'average', 'reward', 'evaluate', 'method', 'positive', 'well', 'note', 'compute', 'success', 'rate', 'trial', 'achieve', 'reward', 'least', 'account', 'successful', 'learning', 'result', 'base', 'last', 'evaluation', 'different', 'random', 'seed', 'compare', 'algorithm', 'good', 'performance', 'boldface', 'average', 'reward', 'avg', 'rwd', 'note', 'implement', 'td', 'actor', 'sac', 'sac', 'already', 'entropyregulated', 'actor', 'q1', 'improve', 'respective', 'base', 'method', 'learn', 'curve', 'benchmark', 'environ', 'ment', 'show', 'figure', 'overall', 'method', 'solid', 'line', 'outperform', 'respective', 'base', 'figure', 'systematic', 'evaluation', 'realize', 'drl', 'algorithm', 'sac', 'td3', 'environment', 'uniform', 'random', 'noise', 'state', 'action', 'reward', 'shaded', 'region', 'represent', 'confidence', 'range', 'evaluation', 'seed', 'number', 'step', 'envirinoment', 'ddpg', 'sac', 'td3', 'tdrsac', 'tdrtd3', 'envirinoment', 'ddpg', 'sac', 'td3', 'tdrsac', 'tdrtd3', 'success', 'success', 'finger', 'turn', 'hard', 'avg', 'rwd', 'rwd', 'rank', 'rank', 'quadrupe', 'walk', 'success', 'avg', 'rwd', 'cartpole', 'sparse', 'rank', 'success', 'avg', 'rwd', 'rank', 'success', 'fish', 'swim', 'avg', 'rwd', 'run', 'sparse', 'success', 'avg', 'rwd', 'rank', 'rank', 'table', 'systematic', 'evaluation', 'respectively', 'augment', 'base', 'algorithm', 'rank', 'percent', 'reward', 'difference', 'sota', 'positive', 'well', 'method', 'td3', 'sac', 'dash', 'line', 'term', 'episode', 'reward', 'learn', 'speed', 'learn', 'variance', 'success', 'rate', 'table', 'measure', 'avg', 'rwd', 'method', 'form', 'respective', 'baseline', 'algorithm', 'notice', 'table', 'learn', 'success', 'rate', 'method', 'significant', 'improvement', 'base', 'method', 'comparison', 'ddpg', 'sac', 'td3', 'base', 'method', 'struggle', 'cartpole', 'sparse', 'run', 'sparse', 'moreover', 'method', 'also', 'outperform', 'ddpg', 'ppo', 'term', 'average', 'reward', 'awgrwd', 'learn', 'speed', 'learn', 'variance', 'success', 'rate', 'thus', 'help', 'succesfully', 'address', 'random', 'initialization', 'challenge', 'cause', 'random', 'seed', 'tdr', 'bring', 'performance', 'base', 'method', 'close', 'well', 'sota', 'figure', 'accord', 'rank', 'measure', 'table', 'environment', 'quadrupe', 'walk', 'tdrsac', 'tdrtd3', 'help', 'enhance', 'performance', 'respective', 'base', 'method', 'additionally', 'even', 'outperform', 'sota', 'around', 'rank', 'mea', 'sure', 'quadrupe', 'walk', 'even', 'tdrsac', 'tdrtd3', 'outperform', 'still', 'method', 'evaluate', 'provide', 'close', 'performance', 'also', 'worth', 'note', 'bring', 'performance', 'new', 'sota', 'level', 'measure', 'mean', 'reward', 'convergence', 'speed', 'learn', 'success', 'rate', 'robust', 'dense', 'stochastic', 'reward', 'sparse', 'reward', 'figure', 'table', 'method', 'outperform', 'respective', 'baseline', 'dense', 'stochastic', 'sparse', 'reward', 'term', 'average', 'reward', 'learn', 'variance', 'success', 'rate', 'converge', 'speed', 'particular', 'baseline', 'algorithm', 'td3', 'sac', 'struggle', 'reward', 'benchmark', 'environment', 'cartpole', 'sparse', 'run', 'sparse', 'however', 'use', 'learn', 'also', 'achieve', 'sota', 'performance', 'method', 'td3td', 'critic', 'actor', 'sactd', 'critic', 'actor', 'rwd', 'enhancement', 'avg', 'rwd', 'enhancement', 'cartpole', 'sparse', 'avg', 'rwd', 'enhancement', 'table', 'systematic', 'evaluation', 'component', 'tdr', 'compare', 'respective', 'base', 'gorithm', 'enhancement', 'percent', 'reward', 'difference', 'respective', 'base', 'algorithm', 'large', 'well', 'note', 'actor', 'consider', 'sac', 'sac', 'already', 'entropyregularize', 'actor', 'figure', 'evaluation', 'td', 'actor', 'different', 'ρ', 'ρ', 'equation', 'base', 'drl', 'algorithm', 'td3', 'environment', 'uniform', 'random', 'noise', 'state', 'action', 'reward', 'shaded', 'region', 'represent', 'confidence', 'range', 'evaluation', 'seed', 'number', 'step', 'ablation', 'study', 'perform', 'ablation', 'study', 'examine', 'remove', 'follow', 'compo', 'nent', 'respective', 'shortform', 'description', 'critic', 'td', 'regularize', 'double', 'q', 'network', 'actor', 'td', 'regularize', 'actor', 'network', 'lnss', 'method', 'n', 'table', 'enhancement', 'percent', 'reward', 'difference', 'evaluate', 'method', 'base', 'method', 'large', 'well', 'td', 'critic', 'actor', 'effectively', 'improve', 'base', 'algorithm', 'table', 'critic', 'lnss', 'actor', 'effectively', 'improve', 'base', 'algorithm', 'table', 'critic', 'lns', 'provide', 'comparable', 'significant', 'enhancement', 'base', 'algorithm', 'td', 'critic', 'method', 'outperform', 'respective', 'base', 'algorithm', 'suggest', 'mitigate', 'estimation', 'error', 'vanilla', 'double', 'q', 'network', 'effective', 'way', 'improve', 'performance', 'also', 'show', 'theoretical', 'analysis', 'theorem', 'method', 'help', 'improve', 'learn', 'performance', 'reduce', 'variance', 'value', 'estimation', 'noisy', 'reward', 'show', 'theoretically', 'empirically', 'include', 'lns', 'tdr', 'robust', 'noisy', 'sparse', 'reward', 'actor', 'element', 'also', 'help', 'make', 'appreciable', 'improvement', 'learn', 'performance', 'show', 'table', 'importantly', 'td', 'actor', 'play', 'importantly', 'role', 'stabilize', 'policy', 'update', 'show', 'theoretically', 'also', 'address', 'estimation', 'error', 'critic', 'show', 'theoretically', 'theorem', 'hyper', 'parameter', 'study', 'hyperparameter', 'study', 'result', 'summarize', 'figure', 'drl', 'method', 'td3', 'actor', 'evaluate', 'different', 'regularization', 'factor', 'ρ', 'report', 'average', 'performance', 'average', 'approximate', 'timation', 'error', 'difference', 'true', 'accumulate', 'reward', 'critic', 'value', 'γtrt', 'eval0', 'q5', 'td', 'regularize', 'actor', 'help', 'reduce', 'estimation', 'error', 'critic', 'figure', 'td', 'regularize', 'actor', 'actor', 'estimation', 'error', 'critic', 'duce', 'example', 'finger', 'turn', 'hard', 'actor', 'result', 'less', 'overestimation', 'error', 'compare', 'ρ', 'later', 'stage', 'training', 'td3', 'actor', 'less', 'underestimation', 'error', 'compare', 'ρ', 'similarly', 'cartpole', 'sparse', 'actor', 'result', 'less', 'overestimation', 'error', 'compare', 'ρ', 'policy', 'evaluate', 'epois', 'reward', 'high', 'epois', 'reward', 'generally', 'result', 'well', 'policy', 'figure', 'policy', 'update', 'improve', 'select', 'suitable', 'regularization', 'fac', 'tor', 'ρ', 'especially', 'cartpole', 'td3', 'actor', 'enable', 'successful', 'learning', 'base', 'method', 'struggle', 'stick', 'learning', 'entire', 'training', 'period', 'q6', 'range', 'ρ', 'ρ', 'generally', 'good', 'choice', 'figure', 'small', 'regular', 'ization', 'factor', 'result', 'less', 'regularization', 'provide', 'sufficient', 'estimation', 'error', 'reduction', 'critic', 'large', 'regularization', 'factor', 'result', 'regularization', 'negative', 'effect', 'learn', 'therefore', 'ρ', 'good', 'choice', 'therefore', 'work', 'consistently', 'use', 'ρ', 'obtain', 'result', 'conclusion', 'discussion', 'limitation', 'study', 'work', 'introduce', 'novel', 'tdr', 'mechanism', 'tdregularize', 'double', 'critic', 'network', 'tdregularize', 'actor', 'network', 'component', 'show', 'help', 'mitigate', 'estimation', 'error', 'show', 'consistently', 'outperform', 'respective', 'base', 'algorithm', 'solve', 'benchmark', 'task', 'term', 'average', 'reward', 'learn', 'success', 'rate', 'learn', 'speed', 'time', 'learn', 'variance', 'analytical', 'result', 'also', 'show', 'component', 'mitigate', 'estimation', 'error', 'show', 'figure', 'environment', 'quadrupe', 'walk', 'evaluate', 'tdr', 'combine', 'distributional', 'lnss', 'element', 'significantly', 'elevate', 'current', 'sota', 'performance', 'new', 'level', 'increase', 'least', 'even', 'identify', 'range', 'generally', 'good', 'regularization', 'coefficient', 'value', 'figure', 'show', 'different', 'algorithm', 'different', 'environment', 'respond', 'somewhat', 'differently', 'ρ', 'therefore', 'effectively', 'determine', 'regularization', 'factor', 'improvement', 'remain', 'question', 'thus', 'limitation', 'study', 'additionally', 'promising', 'performance', 'come', 'extensive', 'training', 'million', 'learn', 'step', 'perform', 'limited', 'training', 'time', 'training', 'step', 'need', 'far', 'investigate', 'reference', 'schneider', 'welinder', 'pieter', 'abbeel', 'wojciech', 'hindsight', 'experience', 'replay', 'arxiv', 'preprint', 'horgan', 'distribute', 'distributional', 'deterministic', 'policy', 'gradient', 'arxiv', 'preprint', 'dabney', 'muno', 'distributional', 'perspective', 'reinforcement', 'learning', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'sutton', 'incremental', 'nat', 'ural', 'actorcritic', 'algorithm', 'advance', 'neural', 'information', 'processing', 'system', 'dynamic', 'multiobjective', 'optimization', 'change', 'number', 'objective', 'ieee', 'transaction', 'evolutionary', 'computation', 'actorcritic', 'equivalent', 'qlearne', 'advance', 'neural', 'information', 'processing', 'system', 'dabney', 'georg', 'ostrovski', 'muno', 'implicit', 'quantile', 'network', 'distributional', 'reinforcement', 'learning', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', '2018a', 'dabney', 'munos', 'distributional', 'reinforcement', 'learning', 'quantile', 'regression', 'proceeding', 'conference', 'artificial', 'intelli', 'gence', 'volume', 'sun', 'distributional', 'soft', 'actorcritic', 'offpolicy', 'reinforcement', 'learning', 'address', 'value', 'estimation', 'error', 'ieee', 'transaction', 'neural', 'network', 'learn', 'system', 'roy', 'pakman', 'tame', 'noise', 'reinforcement', 'learning', 'soft', 'update', 'arxiv', 'preprint', 'arxiv151208562', 'herke', 'hoof', 'address', 'function', 'approximation', 'error', 'actor', 'critic', 'method', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'sehoon', 'vikash', 'pieter', 'abbeel', 'et', 'soft', 'actorcritic', 'algorithm', 'appli', 'cation', 'arxiv', 'preprint', 'arxiv181205905', 'sommer', 'amount', 'quality', 'bias', 'reinforcement', 'learning', 'conference', 'proceeding', 'ieee', 'international', 'conference', 'system', 'man', 'cybernetic', 'cat', 'volume', 'pp', 'ieee', 'double', 'qlearning', 'advance', 'neural', 'information', 'processing', 'system', 'deep', 'reinforcement', 'learning', 'matter', 'proceeding', 'conference', 'artificial', 'intelligence', 'volume', 'qingfeng', 'white', 'qlearning', 'control', 'estimation', 'bias', 'qlearne', 'arxiv', 'preprint', 'leibfrie', 'mutualinformation', 'regularization', 'decision', 'pro', 'cesse', 'actorcritic', 'learning', 'conference', 'robot', 'learn', 'pp', 'pmlr', 'infer', 'humanrobot', 'performance', 'objective', 'locomotion', 'use', 'inverse', 'reinforcement', 'learning', 'inverse', 'optimal', 'control', 'ieee', 'robotic', 'automation', 'letter', 'efficient', 'continuous', 'control', 'double', 'actor', 'regularized', 'critic', 'proceeding', 'conference', 'artificial', 'intelligence', 'volume', 'pp', 'pardo', 'deep', 'reinforcement', 'learning', 'library', 'fast', 'prototyping', 'benchmarke', 'arxiv', 'preprint', 'simone', 'parisi', 'voot', 'tdregularize', 'actor', 'critic', 'method', 'machine', 'learn', 'cong', 'osborne', 'yarin', 'gal', 'teh', 'pathology', 'regularize', 'reinforcement', 'learning', 'expert', 'demonstration', 'advance', 'neural', 'information', 'processing', 'system', '3428376–28389', 'guy', 'riedmiller', 'deterministic', 'policy', 'gradient', 'algorithm', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'sutton', 'reinforcement', 'learn', 'introduction', 'mit', 'press', 'yuval', 'tassa', 'deepmind', 'control', 'suite', 'arxiv', 'preprint', 'sebastian', 'thrun', 'anton', 'schwartz', 'issue', 'use', 'function', 'approximation', 'reinforcement', 'learning', 'proceeding', 'fourth', 'connectionist', 'model', 'summer', 'school', 'volume', 'pp', 'hillsdale', 'nj', 'deep', 'reinforcement', 'learning', 'double', 'q', 'learning', 'proceeding', 'conference', 'artificial', 'intelligence', 'volume', 'scherrer', 'olivi', 'pietquin', 'muno', 'geist', 'leverage', 'average', 'analysis', 'regularization', 'reinforcement', 'learning', 'advance', 'neural', 'information', 'processing', 'system', 'harsh', 'gupta', 'r', 'srikant', 'meansquared', 'error', 'double', 'qlearning', 'advance', 'neural', 'information', 'processing', 'system', 'ruofan', 'brent', 'humanrobotic', 'prosthesis', 'collaborate', 'agent', 'symmetrical', 'walking', 'advance', 'neural', 'information', 'processing', 'system', 'ruofan', 'phase', 'actor', 'actorcritic', 'reinforcement', 'learning', 'weight', 'double', 'qlearning', 'pp', 'ruofan', 'long', 'nstep', 'surrogate', 'stage', 'reward', 'reduce', 'variance', 'deep', 'reinforcement', 'learning', 'complex', 'problem', 'arxiv', 'preprint', 'arxiv221004820', 'distributional', 'tdr', 'lnss', 'distributional', 'rl', 'bellemare', 'represent', 'value', 'function', 'term', 'probability', 'distribution', 'rather', 'function', 'estimate', 'distribution', 'provide', 'comprehensive', 'rep', 'resentation', 'uncertainty', 'associate', 'range', 'different', 'possible', 'reward', 'return', 'state', 'action', 'pair', 'provide', 'informative', 'value', 'function', 'estimation', 'many', 'distributional', 'rl', 'algorithm', 'bellemare', 'dabney', 'achieve', 'great', 'performance', 'provement', 'many', 'discrete', 'problem', 'benchmark', 'barthmaron', 'apply', 'distributional', 'rl', 'continuous', 'control', 'problem', 'combine', 'distributional', 'return', 'function', 'actorcritic', 'framework', 'address', 'overestimation', 'error', 'apply', 'distributional', 'rl', 'piggyback', 'sac', 'provide', 'accurate', 'critic', 'overestimation', 'actor', 'still', 'exist', 'actor', 'still', 'update', 'maximize', 'expectation', 'value', 'function', 'distribution', 'regulate', 'actor', 'distributional', 'rl', 'solve', 'overestimation', 'barely', 'discuss', 'a1', 'distributional', 'tdregularize', 'actorcritic', 'dtdr', 'tailor', 'distributional', 'method', 'base', 'original', 'distributional', 'conceptu', 'alization', 'develop', 'barthmaron', 'bellemare', 'show', 'number', 'enhancement', 'meantime', 'distributional', 'critic', 'distributional', 'critic', 'bellemare', 'treat', 'return', 'tion', 'random', 'variable', 'z', 'expectation', 'use', 'q', 'value', 'estimate', 'namely', 'q', 'e', 'z', 'however', 'use', 'td', 'error', 'evaluate', 'distributional', 'critic', 'similar', 'equation', 'distributional', 'error', 'target', 'network', 'write', 'sk1', 'πϕ′', 'sk1', 'e', 'rk', 'γe', 'zθ′', 'd′', 'rk', 'γe', 'zθ′', 'd′', 'sk1', 'πϕ′', 'sk1', 'e', 'twin', 'tdregularize', 'target', 'distributional', 'operator', 'thus', 'define', 'cid26', 'rk', 'rk', 'sk1', 'πϕ′', 'sk1', 'sk1', 'sk1', 'd′', '≤', 'd′', 'd′', 'b', 'denote', 'random', 'variable', 'follow', 'probability', 'law', 'distributional', 'bellman', 'operator', 'appear', 'similar', 'equation', 'map', 'stateaction', 'pair', 'distribution', 'need', 'define', 'new', 'td', 'error', 'measure', 'distribution', 'barthmaron', 'consider', 'use', 'follow', 'distributional', 'loss', 'l', 'θ', 'zθζ', 'l', 'measure', 'distance', 'distribution', 'many', 'distributional', 'rl', 'algorithm', 'use', 'kullbackleibler', 'divergence', 'distance', 'metric', 'barthmaron', 'adopt', 'metric', 'distributional', 'actor', 'distributional', 'method', 'barthmaron', 'bellemare', 'policy', 'update', 'perform', 'base', 'policy', 'gradient', 'e', 'aπϕ', 'dtdr', 'need', 'use', 'critic', 'evaluation', 'metric', 'evaluate', 'quality', 'current', 'distribu', 'tional', 'critic', 'regularize', 'distributional', 'actor', 'first', 'formulate', 'follow', 'loss', 'metric', 'e', 'l', 'rk', 'γzθi1', 'sk1', 'sk1', 'zθi1', 'similar', 'tdregularize', 'actor', 'network', 'distributional', 'actor', 'update', 'direction', 'imize', 'expect', 'critic', 'keep', 'expect', 'distance', 'project', 'critic', 'critic', 'namely', 'e', '∇aρlz', 'aπϕ', 'ρ', 'regularization', 'coefficient', 'a2', 'long', 'nstep', 'surrogate', 'stage', 'lnss', 'reward', 'lns', 'utilize', 'long', 'reward', 'trajectory', 'n', 'future', 'step', 'estimation', 'stage', 'reward', 'rk', 'use', 'lnssresulte', 'reward', 'place', 'original', 'rk', 'show', 'effectively', 'reduce', 'learn', 'variance', 'significant', 'performance', 'improvement', 'offpolicy', 'method', 'give', 'reward', 'trajectory', 'n', 'step', 'time', 'step', 'k', 'let', 'g', '−1', '−1', '∈', 'r', 'shorthand', 'notation', 'denote', 'discount', 'n', 'step', 'return', '−1', 'γt−krt', 'tk', 'stage', 'reward', 'n', 'reward', 'place', 'rk', 'equation', 'determine', 'r′', 'step', 'reward', 'sequence', 'namely', 'surrogate', 'stage', 'treat', 'weighted', 'average', 'k', 'γt−krt', '−1', 'figure', 'show', 'r′', 'discuss', 'obtain', 'simply', 'use', 'place', 'rk', 'form', 'new', 'tuple', 'k', 'sk1', 'store', 'memory', 'buffer', 'method', 'proceed', 'b', 'estimation', 'analysis', 'lemma', 'let', 'qπ', 'true', 'q', 'value', 'follow', 'current', 'target', 'policy', 'π', 'qθ′', 'target', 'network', 'estimate', 'use', 'double', 'q', 'neural', 'network', 'assume', 'exist', 'step', 'random', 'estimation', 'bias', 'ie', 'estimation', 'bias', 'kth', 'stage', 'independent', 'θ′', 'mean', 'e', 'respectively', 'define', 'θ′', 'ζ', 'equation', 'ζ', '∞', 'k', 'ζ', 'δ′', 'δ′', 'e', 'e', '−µ′', 'proof', 'step', 'random', 'estimation', 'bias', 'θ′', 'ζ', 'rewrite', 'expectation', 'ψk', 'θ′', 'ζ', 'e', 'ψk1', 'θ′', 'ζ', '∞', 'tk1', 'ψt', 'θ′', 'ζ', 'expectation', 'target', 'write', 'e', 'e', 'rk', 'sk1', 'ψk1', '∞', 'θ′', 'e', 'rk', 'e', 'γt−k−1rt', 'tk1', 'use', 'equation', 'td', 'error', 'target', 'critic', 'equation', 'respectably', 'e', 'e', 'rk', 'γe', 'sk1', 'πϕ′', 'sk1', 'e', '−', '−', 'qπ', 'similarly', 'e', 'thus', 'hold', 'place', 'ready', 'analyze', 'estimation', 'error', 'use', 'double', 'q', 'dq', 'method', 'td3', 'fujimoto', 'sac', 'theorem', 'let', 'assumption', 'lemma', 'hold', 'let', 'δyk', 'denote', 'target', 'value', 'estimation', 'error', 'accordingly', 'denote', 'error', 'following', 'e', 'proof', 'proof', 'base', 'enumerate', 'total', 'possible', 'scenario', 'estimation', 'error', 'determine', 'relationship', 'target', 'q', 'value', 'true', 'value', 'provide', 'proof', 'unique', 'scenario', 'first', 'note', 'e', 'target', 'critic', 'value', 'true', 'value', 'follow', 'relationship', 'e', 'e', 'underestimated', 'imply', 'e', 'e', 'e', 'ψk', 'θ′', 'e', 'ψk', 'θ′', 'base', 'equation', 'tdr', 'use', 'target', 'value', 'e', 'e', 'rk', 'γe', 'sk1', 'πϕ′', 'sk1', 'however', 'vanilla', 'double', 'q', 'network', 'target', 'value', 'e', 'rk', 'γe', 'thus', 'base', 'equation', 'estimation', 'error', 'respective', 'target', 'value', 'sk1', 'πϕ′', 'sk1', 'e', 'k', 'e', 'e', 'e', 'γ', 'e', 'thus', 'identity', 'hold', 'case', 'target', 'critic', 'value', 'true', 'value', 'follow', 'relationship', 'e', 'e', 'e', 'expect', 'underestimate', 'overestimate', 'e', 'thus', 'imply', 'e', 'ψk', 'θ′', 'e', 'ψk', 'θ′', 'base', 'equation', 'tdr', 'use', 'target', 'value', 'e', 'e', 'rk', 'γe', 'sk1', 'πϕ′', 'sk1', 'however', 'vanilla', 'double', 'q', 'network', 'target', 'value', 'use', 'e', 'rk', 'γe', 'base', 'equation', 'estimation', 'error', 'respective', 'target', 'value', 'sk1', 'πϕ′', 'sk1', 'e', 'k', 'e', 'e', 'e', 'γ', 'e', 'thus', 'identity', 'hold', 'case', 'target', 'critic', 'value', 'true', 'value', 'follow', 'relationship', 'e', 'e', 'e', 'expect', 'underestimate', 'overestimate', 'e', 'imply', 'thus', 'e', 'ψk', 'θ′', 'e', 'ψk', 'θ′', 'base', 'equation', 'vanilla', 'double', 'q', 'network', 'tdr', 'pick', 'qθ′', 'target', 'value', 'e', 'e', 'e', 'rk', 'γe', 'sk1', 'πϕ′', 'sk1', 'base', 'equation', 'estimation', 'error', 'respective', 'target', 'value', 'e', 'e', 'γ', 'thus', 'thus', 'identity', 'hold', 'case', 'target', 'critic', 'value', 'true', 'value', 'follow', 'relationship', 'expect', 'overestimated', 'e', 'ψk', 'θ′', 'e', 'ψk', 'θ′', 'imply', 'e', 'e', 'base', 'equation', 'vanilla', 'double', 'q', 'network', 'twin', 'tdregularize', 'critic', 'pick', 'target', 'value', 'use', 'qθ′', 'mitigate', 'large', 'overestimation', 'bias', 'e', 'e', 'e', 'rk', 'γe', 'sk1', 'πϕ′', 'sk1', 'base', 'equation', 'estimation', 'error', 'target', 'value', 'e', 'γ', 'e', 'e', 'e', 'thus', 'identity', 'hold', 'method', 'mitigate', 'overestimation', 'error', 'note', 'case', 'study', 'relationship', 'e', 'e', 'e', 'let', 'denote', 'true', 'q', 'value', 'follow', 'current', 'target', 'policy', 'π', 'qθ1', 'estimate', 'value', 'assume', 'exist', 'step', 'random', 'estimation', 'bias', 'ψk', 'independent', 'θ1', 'e', 'ψk', '∞', 'k', 'assume', 'policy', 'update', 'base', 'critic', 'qθ1', 'use', 'deterministic', 'policy', 'gradient', 'dpg', 'equation', 'let', 'δϕk', 'denote', 'change', 'actor', 'update', 'stage', 'accordingly', 'denote', 'change', 'vanilla', 'δϕdp', 'follow', 'true', 'change', 'approximation', 'error', 'q', 'δϕtrue', 'still', 'valid', 'thus', 'theorem', 'hold', 'apply', 'procedure', 'k', 'cid26', 'e', 'δϕdp', 'e', 'δϕdp', 'e', 'ψk', 'e', '≥', 'proof', 'learn', 'rate', 'true', 'change', 'actor', 'parameter', 'case', 'approxima', 'tion', 'error', 'q', 'e', 'δϕtrue', 'k', 'αes∼pπ', 'cid104', 'aπϕj', 'πϕj', 'consider', 'estimate', 'critic', 'true', 'value', 'follow', 'relationship', 'equation', 'give', 'current', 'policy', 'parameter', 'ϕj', 'update', 'parameter', 'use', 'dpg', 'ϕj1', 'dp', 'αes∼pπ', 'cid104', 'e', 'δϕdp', 'αes∼pπ', 'aπϕj', 'πϕj', 'qπ', 'aπϕj', 'πϕj', 'overestimation', 'bias', 'e', 'mate', 'action', 'underestimation', 'bias', 'e', 'ψk', 'underestimated', 'action', 'result', 'suboptimal', 'policy', 'however', 'use', 'tdregularize', 'actor', 'give', 'current', 'policy', 'parameter', 'ϕj', 'actor', 'update', 'equation', 'update', 'encourage', 'exploration', 'overesti', 'update', 'discourage', 'exploration', 'αes∼pπ', 'αes∼pπ', 'qπ', 'aπϕj', 'πϕj', 'qπ', 'aπϕj', 'πϕj', 'similar', 'e', 'ψk', 'equation', 'e', 'e', 'rk', 'γqθi1', 'sk1', 'sk1', 'select', '≤', 'follow', 'cid26', 'ψk', 'e', 'ψk', 'e', 'e', 'ψk', 'e', 'ψk', 'e', '≥', 'therefore', 'inspect', 'equation', 'cid26', 'e', 'thus', 'theorem', 'hold', '≥', 'δϕdp', 'e', 'δϕdp', 'e', 'ψk', 'e', '≥', 'theorem', 'suboptimal', 'actor', 'update', 'negatively', 'affect', 'critic', 'specifically', 'consider', 'actor', 'date', 'theorem', 'overestimation', 'case', 'e', 'qθ1', '≥', 'e', 'qθ1', 'πt', 'dr', '≥', 'πt', 'underestimation', 'case', 'e', 'qθ1', '≤', 'e', 'qθ1', 'πt', 'dr', 'e', 'πt', 'proof', 'follow', 'analysis', 'td3', 'fujimoto', 'consider', 'equation', 'theorem', 'cid26', 'e', 'δϕdp', 'e', 'δϕdp', 'e', 'ψk', 'e', 'underestimate', '≥', 'overestimate', 'overestimation', 'case', 'approximate', 'value', 'use', 'vanilla', 'e', 'qθ1', '≥', 'e', 'qθ1', 'πt', 'dr', '≥', 'πtrue', 'similarly', 'underestimation', 'case', 'approximate', 'value', 'use', 'vanilla', 'e', 'qθ1', '≤', 'e', 'qθ1', 'πt', 'dr', 'e', 'πt', 'thus', 'theorem', 'hold', 'c', 'implementation', 'detail', 'use', 'pytorch', 'implementation', 'result', 'obtain', 'use', 'internal', 'server', 'consist', 'ing', 'ryzen', 'threadripper', 'processor', 'desktop', 'core', 'processor', 'desktop', 'core', 'processor', 'training', 'procedure', 'episode', 'initialize', 'reset', 'environment', 'terminate', 'step', 'trial', 'complete', 'training', 'process', 'contain', 'series', 'consecutive', 'episode', 'trial', 'run', 'maximum', 'time', 'step', 'evaluation', '×', 'time', 'step', 'task', 'report', 'trial', 'environment', 'network', 'initialize', 'random', 'seed', 'study', 'training', 'trial', 'remove', 'dependency', 'initial', 'parameter', 'policy', 'use', 'purely', 'exploratory', 'policy', 'first', 'time', 'step', 'start', 'timestep', 'afterwards', 'use', 'offpolicy', 'exploration', 'strategy', 'add', 'gaussian', 'noise', 'action', 'evaluation', 'procedure', '×', 'time', 'step', 'train', 'evaluation', 'section', 'evaluation', 'report', 'average', 'reward', 'evaluation', 'episode', 'exploration', 'noise', 'fix', 'policy', 'weight', 'random', 'seed', 'evaluation', 'different', 'training', 'trial', 'evaluation', 'perform', 'use', 'seed', 'seed', 'network', 'structure', 'optimizer', 'actorcritic', 'network', 'td3', 'implement', 'neural', 'network', 'layer', 'weight', 'layer', 'hide', 'node', 'rectify', 'linear', 'unit', 'relu', 'actor', 'critic', 'input', 'layer', 'actor', 'dimension', 'observation', 'state', 'output', 'layer', 'actor', 'dimension', 'action', 'requirement', 'tanh', 'unit', 'critic', 'receive', 'state', 'action', 'input', 'first', 'layer', 'output', 'layer', 'critic', 'linear', 'unit', 'produce', 'q', 'value', 'network', 'parameter', 'update', 'use', 'optimizer', 'learning', 'rate', 'simple', 'control', 'problem', 'time', 'step', 'k', 'network', 'train', 'minibatch', 'transition', 'r', 'r′', 'case', 'sample', 'uniformly', 'replay', 'buffer', 'contain', 'entire', 'history', 'agent', 'actorcritic', 'network', 'implement', 'neural', 'net', 'work', 'layer', 'weight', 'layer', 'hide', 'node', 'rectify', 'linear', 'unit', 'relu', 'actor', 'critic', 'input', 'layer', 'actor', 'dimension', 'observa', 'tion', 'state', 'output', 'layer', 'actor', 'dimension', 'action', 'requirement', 'tanh', 'unit', 'critic', 'receive', 'state', 'action', 'input', 'first', 'layer', 'output', 'layer', 'critic', 'distribution', 'hyperparameter', 'number', 'atom', 'l', 'bound', 'support', 'vmin', 'vmax', 'network', 'parameter', 'update', 'use', 'optimizer', 'learning', 'rate', 'time', 'step', 'k', 'network', 'train', 'minibatch', 'transition', 'r', 'r′', 'case', 'sample', 'uniformly', 'replay', 'buffer', 'contain', 'entire', 'tory', 'agent', 'sac', 'actorcritic', 'network', 'sac', 'implement', 'neural', 'network', 'layer', 'weight', 'layer', 'hide', 'node', 'rectify', 'linear', 'unit', 'relu', 'actor', 'critic', 'input', 'layer', 'actor', 'dimension', 'observation', 'state', 'output', 'layer', 'actor', 'dimension', 'action', 'requirement', 'tanh', 'unit', 'critic', 'receive', 'state', 'action', 'input', 'first', 'layer', 'output', 'layer', 'critic', 'linear', 'unit', 'produce', 'q', 'value', 'network', 'parameter', 'update', 'use', 'optimizer', 'learning', 'rate', 'simple', 'control', 'problem', 'time', 'step', 'k', 'network', 'train', 'minibatch', 'transition', 'r', 'r′', 'case', 'sample', 'uniformly', 'replay', 'buffer', 'contain', 'entire', 'history', 'agent', 'hyperparameter', 'keep', 'comparison', 'work', 'fair', 'set', 'common', 'hyperparameter', 'network', 'layer', 'batch', 'size', 'learning', 'rate', 'discount', 'factor', 'number', 'agent', 'comparison', 'method', 'different', 'method', 'td3', 'target', 'policy', 'smoothing', 'implement', 'add', 'ϵ', 'n', 'action', 'choose', 'target', 'actornetwork', 'clip', 'delay', 'policy', 'update', 'consist', 'update', 'actor', 'target', 'critic', 'network', 'iteration', 'large', 'result', 'large', 'benefit', 'respect', 'accumulate', 'error', 'fair', 'comparison', 'critic', 'train', 'time', 'step', 'train', 'actor', 'iteration', 'cripple', 'learn', 'target', 'network', 'update', 'td3', 'use', 'study', 'base', 'paper', 'fujimoto', 'code', 'author', 'https', 'githubcomsfujimtd3', 'hyperparameter', 'td3', 'start', 'timestep', 'evaluation', 'exploration', 'noise', 'policy', 'noise', 'noise', 'clip', 'policy', 'update', 'frequency', 'batch', 'size', 'buffer', 'size', 'τ', 'number', 'parallel', 'actor', 'lnssn', 'learning', 'rate', 'regularization', 'factor', 'value', 'step', 'step', 'step', 'n', 'n', 'table', 'td3', 'hyper', 'parameter', 'use', 'task', 'sac', 'use', 'study', 'base', 'paper', 'code', 'https', 'hyperparameter', 'table', 'value', 'step', 'step', 'step', 'n', 'n', 'hyperparameter', 'sac', 'start', 'timestep', 'evaluation', 'exploration', 'noise', 'policy', 'noise', 'noise', 'clip', 'policy', 'update', 'frequency', 'batch', 'size', 'buffer', 'size', 'temperature', 'parameter', 'number', 'parallel', 'actor', 'lnssn', 'learning', 'rate', 'table', 'sac', 'hyper', 'parameter', 'use', 'task', 'use', 'study', 'base', 'paper', 'barthmaron', 'code', 'modify', 'td3', 'hyperparameter', 'table', 'algorithm', 'drl', 'training', 'platform', 'evaluation', 'algorithm', 'sparse', 'reward', 'setup', 'sparse', 'need', 'run', 'forward', 'fast', 'possible', 'agent', 'get', 'reward', 'speed', 'exceed', 'ms', 'make', 'reward', 'sparse', 'r', 'v', 'else', 'r', 'hyperparameter', 'start', 'timestep', 'evaluation', 'exploration', 'noise', 'noise', 'batch', 'size', 'buffer', 'size', 'τ', 'number', 'parallel', 'actor', 'lnssn', 'learn', 'regularization', 'factor', 'value', 'step', 'step', 'step', 'n', 'table', 'hyper', 'parameter', 'use', 'task', 'tdr', 'algorithm', 'detail', 'section', 'show', 'tdrbased', 'algorithm', 'show', 'sac', 'show', 'show', 'mainly', 'add', 'lnss', 'reward', 'sample', 'collection', 'part', 'update', 'part', 'mainly', 'modify', 'target', 'value', 'selection', 'use', 'equation', 'regular', 'drl', 'equation', 'distributional', 'additionally', 'applicable', 'modify', 'actor', 'gradient', 'base', 'equation', 'regular', 'drl', 'equation', 'distributional', 'code', 'release', 'paper', 'accept', 'initialize', 'θ′', 'ϕ′', 'critic', 'network', 'qθ1', 'qθ2', 'actornetwork', 'random', 'parameter', '•', 'target', 'network', '•', 'experience', 'buffer', 'temporary', 'experience', 'buffer', 'size', 'total', 'training', 'episode', 'episode', 'reset', 'initialize', 'state', 'd′', 'choose', 'action', 'base', 'current', 'state', 'sk', 'learn', 'policy', 'execute', 'action', 'observe', 'new', 'state', 'sk1', 'reward', 'signal', 'rk', 'store', 'transition', 'rk', 'sk1', 'd′', 'n', '≤', 'r′', 'a′', 'get', 'early', 'memory', 'calculate', 'r′', 'base', 'equation', 'store', 'transition', 'clear', 'original', 'transition', 'r′', 'r′', 'd′', 'else', 'repeat', 'step', 'calculate', 'r′', 'base', 'equation', 'k', 'γt', '−k1', 'γt−krt', 'st1', 'end', 'minibatch', 'datum', 'r′', 'get', 'next', 'action', 'at1', 'st1', 'target', 'value', 'base', 'equation', 'update', 'critic', 'base', 'equation', 'policy', 'update', 'frequency', 'update', 'equation', 'update', 'target', 'network', 'θ′', 'τ', 'θ′', 'end', 'end', 'end', 'initialize', 'soft', 'value', 'function', 'vξ', 'target', 'soft', 'value', 'function', '′', 'ξ', 'critic', 'network', 'qθ1', 'qθ2', 'actor', 'network', 'πϕ', 'random', 'parameter', '•', 'target', 'network', 'experience', 'buffer', 'temporary', 'experience', 'buffer', 'size', 'total', 'training', 'episode', 'episode', 'reset', 'initialize', 'state', 'd′', 'choose', 'action', 'base', 'current', 'state', 'sk', 'learn', 'policy', 'execute', 'action', 'observe', 'new', 'state', 'sk1', 'reward', 'signal', 'rk', 'store', 'transition', 'rk', 'sk1', 'd′', 'n', '≤', 'r′', 'a′', 'get', 'early', 'memory', 'calculate', 'r′', 'base', 'equation', 'store', 'transition', 'clear', 'original', 'transition', 'r′', 'r′', 'd′', 'else', 'repeat', 'step', 'calculate', 'r′', 'base', 'equation', 'k', 'γt', '−k1', 'γt−krt', 'end', 'minibatch', 'datum', 'r′', 'get', 'next', 'action', 'at1', 'st1', 'target', 'value', 'base', 'equation', 'update', 'critic', 'base', 'equation', 'st1', 'update', 'soft', 'value', 'function', 'base', 'original', 'sac', 'formualtion', 'update', 'original', 'sac', 'formulation', 'update', 'target', 'network', 'τ', 'end', 'end', 'initialize', 'θ′', 'ϕ′', 'critic', 'network', 'actornetwork', 'random', 'parameter', '•', 'target', 'network', '•', 'experience', 'buffer', 'temporary', 'experience', 'buffer', 'size', 'total', 'training', 'episode', 'episode', 'reset', 'initialize', 'state', 'd′', 'choose', 'action', 'base', 'current', 'state', 'sk', 'learn', 'policy', 'execute', 'action', 'observe', 'new', 'state', 'sk1', 'reward', 'signal', 'rk', 'store', 'transition', 'rk', 'sk1', 'd′', 'n', '≤', 'r′', 'a′', 'get', 'early', 'memory', 'calculate', 'r′', 'base', 'equation', 'store', 'transition', 'clear', 'original', 'transition', 'r′', 'r′', 'd′', 'else', 'repeat', 'step', 'calculate', 'r′', 'base', 'equation', 'k', 'γt', '−k1', 'γt−krt', 'end', 'minibatch', 'datum', 'r′', 'get', 'next', 'action', 'at1', 'st1', 'target', 'distribution', 'base', 'equation', 'st1', 'update', 'critic', 'base', 'equation', 'policy', 'update', 'frequency', 'update', 'equation', 'update', 'target', 'network', 'θ′', 'τ', 'θ′', 'end', 'end', 'end']",
Adobe’s DMV3D Achieves SOTA Performance for High-Fidelity 3D Objects Generation Within Seconds,https://syncedreview.com/2023/11/28/adobes-dmv3d-achieves-sota-performance-for-high-fidelity-3d-objects-generation-within-seconds/,2023-11-28,"The recent surge in the popularity of 3D diffusion models is transforming the landscape of 3D asset generation, particularly in applications such as Augmented Reality (AR), Virtual Reality (VR), robotics, and gaming. These models excel in simplifying the complex 3D asset creation process, significantly reducing the manual workload involved. However, a common challenge with these models is the need for access to ground-truth 3D models or point clouds for training, which can be challenging to obtain for real images. Additionally, the latent 3D diffusion approach often results in an intricate and challenging-to-denoise latent space on highly diverse, category-free 3D datasets, posing a hurdle for achieving high-quality rendering. In a new paper DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model, a research team from Adobe Research, Stanford University, HKU, TTIC and HKUST proposes DMV3D, an innovative single-stage category-agnostic diffusion model. This model can generate 3D Neural Radiance Fields (NeRFs) from either text or a single-image input condition through direct model inference, enabling the creation of diverse high-fidelity 3D objects in just 30 seconds per asset. The team summarizes their main contributions as follows: The primary objective of this research is to realize rapid, realistic, and generic 3D generation. The proposed DMV3D integrates 3D NeRF reconstruction and rendering into its denoiser, creating a 2D multi-view image diffusion model trained without direct 3D supervision. This approach avoids the need for separately training 3D NeRF encoders for latent-space diffusion and eliminates the laborious per-asset optimization process. Essentially, DMV3D incorporates a 3D reconstruction model as the 2D multi-view denoiser within a multi-view diffusion framework. The team strategically considers a sparse set of four multi-view images surrounding an object, effectively describing a 3D object without significant self-occlusions. Leveraging large transformer models, the researchers address the challenging task of sparse-view 3D reconstruction. Built upon the recent 3D Large Reconstruction Model (LRM), they introduce a novel model for joint reconstruction and denoising, capable of handling various noise levels in the diffusion process. This model can be seamlessly integrated as the multi-view image denoiser in a multi-view image diffusion framework. The team trained their model on large-scale datasets comprising synthetic renderings from Objaverse and real captures from MVImgNet, using only image-space supervision. DMV3D not only demonstrates the ability for single-stage 3D generation in approximately 30 seconds on a single A100 GPU but also achieves state-of-the-art results in single-image 3D reconstruction, surpassing prior methods based on SDS (Self-Supervised Depth Sensing) and other 3D diffusion models. In summary, this work provides a fresh perspective on addressing 3D generation tasks by bridging the realms of 2D and 3D generative models, unifying 3D reconstruction and generation. The implications extend beyond the immediate applications, opening doors for the development of foundational models to tackle a diverse array of challenges in 3D vision and graphics. The project website is at: https: //justimyhxu.github.io/projects/dmv3d/. The paper DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","The recent surge in the popularity of 3D diffusion models is transforming the landscape of 3D asset generation , particularly in applications such as Augmented Reality ( AR ) , Virtual Reality ( VR ) , robotics , and gaming . These models excel in simplifying the complex 3D asset creation process , significantly reducing the manual workload involved . However , a common challenge with these models is the need for access to ground-truth 3D models or point clouds for training , which can be challenging to obtain for real images . Additionally , the latent 3D diffusion approach often results in an intricate and challenging-to-denoise latent space on highly diverse , category-free 3D datasets , posing a hurdle for achieving high-quality rendering . In a new paper DMV3D : Denoising Multi-View Diffusion using 3D Large Reconstruction Model , a research team from Adobe Research , Stanford University , HKU , TTIC and HKUST proposes DMV3D , an innovative single-stage category-agnostic diffusion model . This model can generate 3D Neural Radiance Fields ( NeRFs ) from either text or a single-image input condition through direct model inference , enabling the creation of diverse high-fidelity 3D objects in just 30 seconds per asset . The team summarizes their main contributions as follows : The primary objective of this research is to realize rapid , realistic , and generic 3D generation . The proposed DMV3D integrates 3D NeRF reconstruction and rendering into its denoiser , creating a 2D multi-view image diffusion model trained without direct 3D supervision . This approach avoids the need for separately training 3D NeRF encoders for latent-space diffusion and eliminates the laborious per-asset optimization process . Essentially , DMV3D incorporates a 3D reconstruction model as the 2D multi-view denoiser within a multi-view diffusion framework . The team strategically considers a sparse set of four multi-view images surrounding an object , effectively describing a 3D object without significant self-occlusions . Leveraging large transformer models , the researchers address the challenging task of sparse-view 3D reconstruction . Built upon the recent 3D Large Reconstruction Model ( LRM ) , they introduce a novel model for joint reconstruction and denoising , capable of handling various noise levels in the diffusion process . This model can be seamlessly integrated as the multi-view image denoiser in a multi-view image diffusion framework . The team trained their model on large-scale datasets comprising synthetic renderings from Objaverse and real captures from MVImgNet , using only image-space supervision . DMV3D not only demonstrates the ability for single-stage 3D generation in approximately 30 seconds on a single A100 GPU but also achieves state-of-the-art results in single-image 3D reconstruction , surpassing prior methods based on SDS ( Self-Supervised Depth Sensing ) and other 3D diffusion models . In summary , this work provides a fresh perspective on addressing 3D generation tasks by bridging the realms of 2D and 3D generative models , unifying 3D reconstruction and generation . The implications extend beyond the immediate applications , opening doors for the development of foundational models to tackle a diverse array of challenges in 3D vision and graphics . The project website is at : https : . The paper DMV3D : Denoising Multi-View Diffusion using 3D Large Reconstruction Model on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['recent', 'surge', 'popularity', '3d', 'diffusion', 'model', 'transform', 'landscape', '3d', 'asset', 'generation', 'particularly', 'application', 'augment', 'reality', 'ar', 'virtual', 'reality', 'vr', 'robotic', 'game', 'model', 'excel', 'simplify', 'complex', '3d', 'asset', 'creation', 'process', 'significantly', 'reduce', 'manual', 'workload', 'involve', 'however', 'common', 'challenge', 'model', 'need', 'access', 'groundtruth', '3d', 'model', 'point', 'cloud', 'training', 'challenge', 'obtain', 'real', 'image', 'additionally', 'latent', '3d', 'diffusion', 'approach', 'often', 'result', 'intricate', 'challengingtodenoise', 'latent', 'space', 'highly', 'diverse', 'categoryfree', '3d', 'dataset', 'pose', 'hurdle', 'achieve', 'highquality', 'rendering', 'new', 'paper', 'denoise', 'multiview', 'diffusion', 'use', '3d', 'large', 'reconstruction', 'model', 'research', 'team', 'hku', 'ttic', 'hkust', 'propose', 'innovative', 'singlestage', 'categoryagnostic', 'diffusion', 'model', 'model', 'generate', '3d', 'neural', 'radiance', 'field', 'nerf', 'text', 'singleimage', 'input', 'condition', 'direct', 'model', 'inference', 'enable', 'creation', 'diverse', 'highfidelity', '3d', 'object', 'second', 'asset', 'team', 'summarize', 'main', 'contribution', 'follow', 'primary', 'objective', 'research', 'realize', 'rapid', 'realistic', 'generic', '3d', 'generation', 'propose', 'integrate', '3d', 'nerf', 'reconstruction', 'render', 'denoiser', 'create', 'multiview', 'image', 'diffusion', 'model', 'train', 'direct', '3d', 'supervision', 'approach', 'avoid', 'need', 'separately', 'train', '3d', 'nerf', 'encoder', 'latentspace', 'diffusion', 'eliminate', 'laborious', 'perasset', 'optimization', 'process', 'essentially', 'incorporate', '3d', 'reconstruction', 'model', 'multiview', 'denoiser', 'multiview', 'diffusion', 'framework', 'team', 'strategically', 'consider', 'sparse', 'set', 'multiview', 'image', 'surround', 'object', 'effectively', 'describe', '3d', 'object', 'significant', 'selfocclusion', 'leverage', 'large', 'transformer', 'model', 'researcher', 'address', 'challenging', 'task', 'sparseview', '3d', 'reconstruction', 'build', 'recent', '3d', 'large', 'reconstruction', 'model', 'introduce', 'novel', 'model', 'joint', 'reconstruction', 'denoise', 'capable', 'handle', 'various', 'noise', 'level', 'diffusion', 'process', 'model', 'seamlessly', 'integrate', 'multiview', 'image', 'denoiser', 'multiview', 'image', 'diffusion', 'framework', 'team', 'train', 'model', 'largescale', 'dataset', 'comprise', 'synthetic', 'rendering', 'real', 'capture', 'mvimgnet', 'use', 'imagespace', 'supervision', 'demonstrate', 'ability', 'singlestage', '3d', 'generation', 'approximately', 'second', 'single', 'also', 'achieve', 'stateoftheart', 'result', 'singleimage', '3d', 'reconstruction', 'surpass', 'prior', 'method', 'base', 'sds', 'selfsupervise', 'depth', 'sensing', '3d', 'diffusion', 'model', 'summary', 'work', 'provide', 'fresh', 'perspective', 'address', '3d', 'generation', 'task', 'bridge', 'realm', '2d', '3d', 'generative', 'model', 'unify', '3d', 'reconstruction', 'generation', 'implication', 'extend', 'immediate', 'application', 'opening', 'door', 'development', 'foundational', 'model', 'tackle', 'diverse', 'array', 'challenge', '3d', 'vision', 'graphic', 'project', 'website', 'https', 'paper', 'denoise', 'multiview', 'diffusion', 'use', '3d', 'large', 'reconstruction', 'model', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
A research team innovative single-stage category-agnostic diffusion model. This model can generate 3D Neural Radiance Fields (NeRFs) from either text or a single-image input condition through direct model inference, enabling the creation of diverse high-fidelity 3D objects in just 30s/asset. 

 "
DeepMind’s DiLoCo Revolutionizes Language Model Training with 500× Less Communication,https://syncedreview.com/2023/11/27/deepminds-diloco-revolutionizes-language-model-training-with-500x-less-communication/,2023-11-27,"Language models have demonstrated exceptional performance across various real-world applications. However, at contemporary scales, the conventional training method employing standard backpropagation introduces formidable engineering and infrastructure challenges. The difficulty lies in co-locating and synchronizing a large number of accelerators. In response to this challenge, in a new paper DiLoCo: Distributed Low-Communication Training of Language Models, a Google DeepMind research team presents Distributed Low-Communication (DiLoCo). DiLoCo employs a distributed optimization algorithm that facilitates the training of language models on islands of poorly connected devices, surpassing the performance of fully synchronous models while reducing communication by 500 times. Drawing inspiration from Federated Learning literature, the researchers propose a variant of the widely-used Federated Averaging (FedAvg) algorithm. They introduce a specific instantiation with a momentum-based optimizer, akin to the FedOpt algorithm. Notably, DiLoCo replaces the inner optimizer with AdamW and the outer optimizer with Nesterov Momentum for optimal performance. This innovative combination effectively addresses the challenges posed by conventional training approaches. The DiLoCo approach mitigates the aforementioned challenges through three key elements: a) Limited co-location requirements: While each worker requires co-located devices, their number is smaller than the total, easing the logistical burden. b) Reduced communication frequency: Workers need not communicate at each step, but only every 𝐻 steps, potentially in the order of hundreds or even thousands, significantly lowering communication overhead. c) Device heterogeneity: While devices within an island need to be homogeneous, different islands can operate with different types of devices, enhancing flexibility. In the DiLoCo training process, a pretrained model 𝜃 (0) is replicated 𝑘 times. Each worker independently and in parallel trains a model replica on its own shard of data for 𝐻 steps. Subsequently, workers average their outer gradients, and an outer optimizer updates the global parameter copy 𝜃 (1), which is then redistributed to the workers. This process repeats 𝑇 times. Notably, each replica can be trained in different global locations using various accelerators. On the widely-used C4 dataset, DiLoCo with 8 workers demonstrates performance on par with fully synchronous optimization while reducing communication by 500 times. Furthermore, DiLoCo exhibits remarkable robustness to variations in the data distribution of each worker and adapts seamlessly to changes in resource availability during training. In summary, DiLoCo presents a robust and effective solution for distributing the training of transformer language models when multiple machines are available but poorly connected. This innovative approach not only overcomes infrastructure challenges but also showcases superior performance and adaptability, marking a significant advancement in the field of language model optimization. The paper DiLoCo: Distributed Low-Communication Training of Language Models on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Language models have demonstrated exceptional performance across various real-world applications . However , at contemporary scales , the conventional training method employing standard backpropagation introduces formidable engineering and infrastructure challenges . The difficulty lies in co-locating and synchronizing a large number of accelerators . In response to this challenge , in a new paper DiLoCo : Distributed Low-Communication Training of Language Models , a Google DeepMind research team presents Distributed Low-Communication ( DiLoCo ) . DiLoCo employs a distributed optimization algorithm that facilitates the training of language models on islands of poorly connected devices , surpassing the performance of fully synchronous models while reducing communication by 500 times . Drawing inspiration from Federated Learning literature , the researchers propose a variant of the widely-used Federated Averaging ( FedAvg ) algorithm . They introduce a specific instantiation with a momentum-based optimizer , akin to the FedOpt algorithm . Notably , DiLoCo replaces the inner optimizer with AdamW and the outer optimizer with Nesterov Momentum for optimal performance . This innovative combination effectively addresses the challenges posed by conventional training approaches . The DiLoCo approach mitigates the aforementioned challenges through three key elements : a ) Limited co-location requirements : While each worker requires co-located devices , their number is smaller than the total , easing the logistical burden . b ) Reduced communication frequency : Workers need not communicate at each step , but only every 𝐻 steps , potentially in the order of hundreds or even thousands , significantly lowering communication overhead . c ) Device heterogeneity : While devices within an island need to be homogeneous , different islands can operate with different types of devices , enhancing flexibility . In the DiLoCo training process , a pretrained model 𝜃 ( 0 ) is replicated 𝑘 times . Each worker independently and in parallel trains a model replica on its own shard of data for 𝐻 steps . Subsequently , workers average their outer gradients , and an outer optimizer updates the global parameter copy 𝜃 ( 1 ) , which is then redistributed to the workers . This process repeats 𝑇 times . Notably , each replica can be trained in different global locations using various accelerators . On the widely-used C4 dataset , DiLoCo with 8 workers demonstrates performance on par with fully synchronous optimization while reducing communication by 500 times . Furthermore , DiLoCo exhibits remarkable robustness to variations in the data distribution of each worker and adapts seamlessly to changes in resource availability during training . In summary , DiLoCo presents a robust and effective solution for distributing the training of transformer language models when multiple machines are available but poorly connected . This innovative approach not only overcomes infrastructure challenges but also showcases superior performance and adaptability , marking a significant advancement in the field of language model optimization . The paper DiLoCo : Distributed Low-Communication Training of Language Models on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['language', 'model', 'demonstrate', 'exceptional', 'performance', 'various', 'realworld', 'application', 'however', 'contemporary', 'scale', 'conventional', 'training', 'method', 'employ', 'standard', 'backpropagation', 'introduce', 'formidable', 'engineering', 'infrastructure', 'challenge', 'difficulty', 'lie', 'colocate', 'synchronize', 'large', 'number', 'accelerator', 'response', 'challenge', 'new', 'paper', 'diloco', 'distribute', 'lowcommunication', 'training', 'language', 'model', 'research', 'team', 'present', 'distribute', 'lowcommunication', 'diloco', 'diloco', 'employ', 'distribute', 'optimization', 'facilitate', 'training', 'language', 'model', 'island', 'poorly', 'connect', 'device', 'surpass', 'performance', 'fully', 'synchronous', 'model', 'reduce', 'communication', 'time', 'draw', 'inspiration', 'federated', 'learn', 'literature', 'researcher', 'propose', 'variant', 'widelyused', 'federate', 'average', 'introduce', 'specific', 'instantiation', 'momentumbase', 'optimizer', 'akin', 'notably', 'diloco', 'replace', 'inner', 'optimizer', 'adamw', 'outer', 'optimizer', 'nesterov', 'momentum', 'optimal', 'performance', 'innovative', 'combination', 'effectively', 'address', 'challenge', 'pose', 'conventional', 'training', 'approach', 'approach', 'mitigate', 'aforementione', 'challenge', 'key', 'element', 'limited', 'colocation', 'requirement', 'worker', 'require', 'colocate', 'device', 'number', 'small', 'total', 'ease', 'logistical', 'burden', 'b', 'reduce', 'communication', 'frequency', 'worker', 'communicate', 'step', 'step', 'potentially', 'order', 'hundred', 'even', 'thousand', 'significantly', 'lower', 'communication', 'overhead', 'c', 'device', 'heterogeneity', 'device', 'island', 'need', 'homogeneous', 'different', 'island', 'operate', 'different', 'type', 'device', 'enhance', 'flexibility', 'diloco', 'training', 'process', 'pretraine', 'model', 'replicate', 'time', 'worker', 'independently', 'parallel', 'train', 'model', 'replica', 'shard', 'datum', 'step', 'subsequently', 'worker', 'average', 'outer', 'gradient', 'outer', 'optimizer', 'update', 'global', 'parameter', 'redistribute', 'worker', 'process', 'repeat', 'notably', 'replica', 'train', 'different', 'global', 'location', 'use', 'various', 'accelerator', 'widelyused', 'c4', 'dataset', 'diloco', 'worker', 'demonstrate', 'performance', 'par', 'fully', 'synchronous', 'optimization', 'reduce', 'communication', 'time', 'furthermore', 'diloco', 'exhibit', 'remarkable', 'robustness', 'variation', 'data', 'distribution', 'worker', 'adapt', 'seamlessly', 'change', 'resource', 'availability', 'training', 'summary', 'diloco', 'present', 'robust', 'effective', 'solution', 'distribute', 'training', 'transformer', 'language', 'model', 'multiple', 'machine', 'available', 'poorly', 'connect', 'innovative', 'approach', 'overcome', 'infrastructure', 'challenge', 'also', 'showcase', 'superior', 'performance', 'adaptability', 'mark', 'significant', 'advancement', 'field', 'language', 'model', 'optimization', 'paper', 'diloco', 'distribute', 'lowcommunication', 'training', 'language', 'model', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
In a new paper DiLoCo: Distributed Low-Communication Training of Language Models, a Google DeepMind research team presents Distributed Low-Communication (DiLoCo). DiLoCo employs a distributed optimization algorithm that facilitates the training of language models on islands of poorly connected devices, surpassing the performance of fully synchronous models while reducing communication by 500 times.
"
Meet LEO: An Embodied Generalist Agent Excelling in 3D World Tasks,https://syncedreview.com/2023/11/26/meet-leo-an-embodied-generalist-agent-excelling-in-3d-world-tasks/,2023-11-26,"The quest to develop a single, versatile model capable of performing diverse tasks akin to human abilities has been a longstanding pursuit in the realms of artificial intelligence and neuroscience. Recent strides in the realm of large language models (LLMs) have presented a promising avenue for creating such generalist models. Leveraging expansive datasets and scalable Transformer architectures, these models have shown immense potential. However, a significant challenge persists: the limited capacity of these models to comprehend and engage with the three-dimensional environment that encompasses humans and other intelligent entities. This constraint acts as a bottleneck, impeding the successful execution of real-world tasks and the achievement of true general intelligence. In a new paper An Embodied Generalist Agent in 3D World, a research team from Beijing Institute for General Artificial Intelligence (BIGAI), Peking University, Carnegie Mellon University and Tsinghua University introduce LEO, which stands as an embodied multi-modal and multi-task generalist agent that excels in essential capabilities such as perception, grounding, reasoning, planning, and action within the intricate 3D world. The team summarizes their main contributions as follows: LEO undergoes training in two stages, utilizing shared LLM-based model architectures, objectives, and weights: (i) 3D vision-language alignment and (ii) 3D vision-language-action instruction tuning. LEO’s perceptual abilities stem from an egocentric 2D image encoder for embodied views and an object-centric 3D point cloud encoder for a global, third-person perspective. The output tokens from the 3D encoder, representing observed entities, are interleaved with text tokens to form a scene-grounded instructional task sequence. This sequence serves as input to a decoder-only LLM, framing all tasks as sequence prediction problems. Autoregressive training objectives allow LEO to be trained with task-agnostic inputs and outputs. The team conducts a comprehensive empirical study, quantitatively evaluating and ablating LEO on diverse 3D tasks. Tasks include object-level and scene-level captioning, 3D question answering, and robotic manipulation. Results indicate that LEO achieves state-of-the-art performance on most tasks. Task-agnostic instruction tuning, enabled by a unified model, surpasses previous task-specific models across various domains. Moreover, pretraining of 3D vision-language alignment significantly enhances the performance of VLA instruction-tuning. The study also highlights the positive impact of scaling up training data on the generalist agent’s performance. In conclusion, LEO stands as a pioneering embodiment of a generalist agent, showcasing remarkable capabilities in navigating and interacting within the 3D world. The insights and methodologies introduced by the research team open new avenues for the development of artificial intelligence with enhanced perceptual and action-oriented competencies. The paper An Embodied Generalist Agent in 3D World on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","The quest to develop a single , versatile model capable of performing diverse tasks akin to human abilities has been a longstanding pursuit in the realms of artificial intelligence and neuroscience . Recent strides in the realm of large language models ( LLMs ) have presented a promising avenue for creating such generalist models . Leveraging expansive datasets and scalable Transformer architectures , these models have shown immense potential . However , a significant challenge persists : the limited capacity of these models to comprehend and engage with the three-dimensional environment that encompasses humans and other intelligent entities . This constraint acts as a bottleneck , impeding the successful execution of real-world tasks and the achievement of true general intelligence . In a new paper An Embodied Generalist Agent in 3D World , a research team from Beijing Institute for General Artificial Intelligence ( BIGAI ) , Peking University , Carnegie Mellon University and Tsinghua University introduce LEO , which stands as an embodied multi-modal and multi-task generalist agent that excels in essential capabilities such as perception , grounding , reasoning , planning , and action within the intricate 3D world . The team summarizes their main contributions as follows : LEO undergoes training in two stages , utilizing shared LLM-based model architectures , objectives , and weights : ( i ) 3D vision-language alignment and ( ii ) 3D vision-language-action instruction tuning . LEO ’ s perceptual abilities stem from an egocentric 2D image encoder for embodied views and an object-centric 3D point cloud encoder for a global , third-person perspective . The output tokens from the 3D encoder , representing observed entities , are interleaved with text tokens to form a scene-grounded instructional task sequence . This sequence serves as input to a decoder-only LLM , framing all tasks as sequence prediction problems . Autoregressive training objectives allow LEO to be trained with task-agnostic inputs and outputs . The team conducts a comprehensive empirical study , quantitatively evaluating and ablating LEO on diverse 3D tasks . Tasks include object-level and scene-level captioning , 3D question answering , and robotic manipulation . Results indicate that LEO achieves state-of-the-art performance on most tasks . Task-agnostic instruction tuning , enabled by a unified model , surpasses previous task-specific models across various domains . Moreover , pretraining of 3D vision-language alignment significantly enhances the performance of VLA instruction-tuning . The study also highlights the positive impact of scaling up training data on the generalist agent ’ s performance . In conclusion , LEO stands as a pioneering embodiment of a generalist agent , showcasing remarkable capabilities in navigating and interacting within the 3D world . The insights and methodologies introduced by the research team open new avenues for the development of artificial intelligence with enhanced perceptual and action-oriented competencies . The paper An Embodied Generalist Agent in 3D World on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['quest', 'develop', 'single', 'versatile', 'model', 'capable', 'perform', 'diverse', 'task', 'akin', 'human', 'ability', 'longstanding', 'pursuit', 'realm', 'artificial', 'intelligence', 'neuroscience', 'recent', 'stride', 'realm', 'large', 'language', 'model', 'llm', 'present', 'promising', 'avenue', 'create', 'generalist', 'model', 'leverage', 'expansive', 'dataset', 'scalable', 'transformer', 'architecture', 'model', 'show', 'immense', 'potential', 'however', 'significant', 'challenge', 'persist', 'limited', 'capacity', 'model', 'comprehend', 'engage', 'threedimensional', 'environment', 'encompass', 'human', 'intelligent', 'entity', 'constraint', 'act', 'bottleneck', 'impede', 'successful', 'execution', 'realworld', 'task', 'achievement', 'true', 'general', 'intelligence', 'new', 'paper', 'embody', 'generalist', 'agent', '3d', 'world', 'research', 'team', 'general', 'artificial', 'intelligence', 'bigai', 'peke', 'university', 'carnegie', 'introduce', 'stand', 'embody', 'multimodal', 'multitask', 'generalist', 'agent', 'excel', 'essential', 'capability', 'perception', 'ground', 'reasoning', 'planning', 'action', 'intricate', '3d', 'world', 'team', 'summarize', 'main', 'contribution', 'follow', 'training', 'stage', 'utilize', 'share', 'llmbased', 'model', 'architecture', 'objective', 'weight', '3d', 'visionlanguage', 'alignment', '3d', 'visionlanguageaction', 'instruction', 'tune', 'perceptual', 'ability', 'stem', 'egocentric', 'image', 'encoder', 'embody', 'view', 'objectcentric', '3d', 'point', 'cloud', 'encoder', 'global', 'perspective', 'output', 'token', '3d', 'encoder', 'represent', 'observed', 'entity', 'interleave', 'text', 'token', 'form', 'scenegrounde', 'instructional', 'task', 'sequence', 'sequence', 'serve', 'input', 'decoderonly', 'llm', 'frame', 'task', 'sequence', 'prediction', 'problem', 'autoregressive', 'training', 'objective', 'allow', 'train', 'taskagnostic', 'input', 'output', 'team', 'conduct', 'comprehensive', 'empirical', 'study', 'quantitatively', 'evaluate', 'ablate', 'diverse', '3d', 'task', 'task', 'include', 'objectlevel', 'scenelevel', 'captioning', '3d', 'question', 'answer', 'robotic', 'manipulation', 'result', 'indicate', 'achieve', 'stateoftheart', 'performance', 'task', 'taskagnostic', 'instruction', 'tuning', 'enable', 'unified', 'model', 'surpass', 'previous', 'taskspecific', 'model', 'various', 'domain', 'moreover', 'pretraining', '3d', 'visionlanguage', 'alignment', 'significantly', 'enhance', 'performance', 'vla', 'instructiontune', 'study', 'also', 'highlight', 'positive', 'impact', 'scale', 'training', 'datum', 'generalist', 'agent', 'performance', 'conclusion', 'stand', 'pioneer', 'embodiment', 'generalist', 'agent', 'showcase', 'remarkable', 'capability', 'navigate', 'interact', '3d', 'world', 'insight', 'methodology', 'introduce', 'research', 'team', 'open', 'new', 'avenue', 'development', 'artificial', 'intelligence', 'enhanced', 'perceptual', 'actionoriented', 'competency', 'paper', 'embody', 'generalist', 'agent', '3d', 'world', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
In a new paper An Embodied Generalist Agent in 3D World, a research team introduces LEO, which stands as an embodied multi-modal and multi-task generalist agent that excels in essential capabilities such as perception, grounding, reasoning, planning, and action within the intricate 3D world.
"
ETH Zurich’s UltraFastBERT Realizes 78x Speedup for Language Models,https://syncedreview.com/2023/11/24/eth-zurichs-ultrafastbert-realizes-78x-speedup-for-language-models/,2023-11-24,"The ever-expanding scale of language models, now boasting tens of billions of parameters, has undeniably enhanced performance across diverse tasks. However, the consequential surge in computation costs poses a significant hurdle for real-world applications. In a bid to overcome this challenge, researchers are diligently working to enhance the efficiency of Large Language Models (LLMs). Recent studies have spotlighted a crucial observation: the majority of parameters in these expansive language models reside within their feedforward layers. Intriguingly, not every neuron in these layers needs to be active during inference, presenting an opportunity to optimize their computation efficiency. In a new paper Exponentially Faster Language Modelling, an ETH Zurich research team introduces UltraFastBERT, a variant of the BERT architecture. UltraFastBERT takes a revolutionary approach by replacing feedforward layers with fast feedforward networks, resulting in an impressive 78x speedup over the optimized baseline feedforward implementation. The main contributions of the research team can be summarized as follows: UltraFastBERT’s architecture draws inspiration from crammedBERT but distinguishes itself by replacing intermediate layer feedforward networks with fast feedforward networks. The simplifying changes applied to these networks include uniform activation functions, output weights for all nodes, removing output biases, fixing leaf size to 1, and allowing multiple FFF trees in parallel. During the training phase, the team follows the final training procedure of crammedBERT, while in the evaluation phase, they fine-tune UltraFastBERT models for various tasks in the GLUE benchmark. Remarkably, the results demonstrate that UltraFastBERT variants trained for just one day on a single A6000 GPU retain at least 96.0% of the GLUE downstream predictive performance of the original BERTbase model. UltraFastBERT-1×11-long even performs on par with the original BERT-base model while utilizing a mere 0.3% of its neurons. In addition to these achievements, the team provides high-level CPU code showcasing a 78x speedup over the optimized baseline feedforward implementation, along with a PyTorch implementation delivering a 40x speedup over the equivalent batched feedforward inference. In conclusion, this work not only verifies the impressive efficiency of UltraFastBERT but also aims to inspire the integration of primitives for conditional neural execution into device programming interfaces. The hope is that these efforts will pave the way for more streamlined and efficient language models in the future. The paper Exponentially Faster Language Modelling on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","The ever-expanding scale of language models , now boasting tens of billions of parameters , has undeniably enhanced performance across diverse tasks . However , the consequential surge in computation costs poses a significant hurdle for real-world applications . In a bid to overcome this challenge , researchers are diligently working to enhance the efficiency of Large Language Models ( LLMs ) . Recent studies have spotlighted a crucial observation : the majority of parameters in these expansive language models reside within their feedforward layers . Intriguingly , not every neuron in these layers needs to be active during inference , presenting an opportunity to optimize their computation efficiency . In a new paper Exponentially Faster Language Modelling , an ETH Zurich research team introduces UltraFastBERT , a variant of the BERT architecture . UltraFastBERT takes a revolutionary approach by replacing feedforward layers with fast feedforward networks , resulting in an impressive 78x speedup over the optimized baseline feedforward implementation . The main contributions of the research team can be summarized as follows : UltraFastBERT ’ s architecture draws inspiration from crammedBERT but distinguishes itself by replacing intermediate layer feedforward networks with fast feedforward networks . The simplifying changes applied to these networks include uniform activation functions , output weights for all nodes , removing output biases , fixing leaf size to 1 , and allowing multiple FFF trees in parallel . During the training phase , the team follows the final training procedure of crammedBERT , while in the evaluation phase , they fine-tune UltraFastBERT models for various tasks in the GLUE benchmark . Remarkably , the results demonstrate that UltraFastBERT variants trained for just one day on a single A6000 GPU retain at least 96.0 % of the GLUE downstream predictive performance of the original BERTbase model . UltraFastBERT-1×11-long even performs on par with the original BERT-base model while utilizing a mere 0.3 % of its neurons . In addition to these achievements , the team provides high-level CPU code showcasing a 78x speedup over the optimized baseline feedforward implementation , along with a PyTorch implementation delivering a 40x speedup over the equivalent batched feedforward inference . In conclusion , this work not only verifies the impressive efficiency of UltraFastBERT but also aims to inspire the integration of primitives for conditional neural execution into device programming interfaces . The hope is that these efforts will pave the way for more streamlined and efficient language models in the future . The paper Exponentially Faster Language Modelling on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['everexpande', 'scale', 'language', 'model', 'boast', 'ten', 'billion', 'parameter', 'undeniably', 'enhance', 'performance', 'diverse', 'task', 'however', 'consequential', 'surge', 'computation', 'cost', 'pose', 'significant', 'hurdle', 'realworld', 'application', 'bid', 'overcome', 'challenge', 'researcher', 'diligently', 'work', 'enhance', 'efficiency', 'large', 'language', 'model', 'llm', 'recent', 'study', 'spotlight', 'crucial', 'observation', 'majority', 'parameter', 'expansive', 'language', 'model', 'reside', 'feedforward', 'layer', 'intriguingly', 'neuron', 'layer', 'need', 'active', 'inference', 'present', 'opportunity', 'optimize', 'computation', 'efficiency', 'new', 'paper', 'exponentially', 'fast', 'language', 'model', 'research', 'team', 'introduce', 'ultrafastbert', 'variant', 'architecture', 'ultrafastbert', 'take', 'revolutionary', 'approach', 'replace', 'feedforward', 'layer', 'fast', 'feedforward', 'network', 'result', 'impressive', '78x', 'speedup', 'optimize', 'baseline', 'feedforward', 'implementation', 'main', 'contribution', 'research', 'team', 'summarize', 'follow', 'architecture', 'draw', 'inspiration', 'crammedbert', 'distinguish', 'replace', 'intermediate', 'layer', 'feedforward', 'network', 'fast', 'feedforward', 'network', 'simplifying', 'change', 'apply', 'network', 'include', 'uniform', 'activation', 'function', 'output', 'weight', 'node', 'remove', 'output', 'bias', 'fix', 'leaf', 'size', 'allow', 'multiple', 'fff', 'tree', 'parallel', 'training', 'phase', 'team', 'follow', 'final', 'training', 'procedure', 'crammedbert', 'evaluation', 'phase', 'finetune', 'ultrafastbert', 'model', 'various', 'task', 'glue', 'benchmark', 'remarkably', 'result', 'demonstrate', 'ultrafastbert', 'variant', 'train', 'day', 'single', 'retain', 'least', 'glue', 'downstream', 'predictive', 'performance', 'original', 'bertbase', 'model', 'even', 'perform', 'par', 'original', 'bertbase', 'model', 'utilize', 'mere', 'neuron', 'addition', 'achievement', 'team', 'provide', 'highlevel', 'cpu', 'code', 'showcase', '78x', 'speedup', 'optimize', 'baseline', 'feedforward', 'implementation', 'pytorch', 'implementation', 'deliver', '40x', 'speedup', 'equivalent', 'batch', 'feedforward', 'inference', 'conclusion', 'work', 'verify', 'impressive', 'efficiency', 'also', 'aim', 'inspire', 'integration', 'primitive', 'conditional', 'neural', 'execution', 'device', 'programming', 'interface', 'hope', 'effort', 'pave', 'way', 'streamlined', 'efficient', 'language', 'model', 'future', 'paper', 'exponentially', 'fast', 'language', 'modelling', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
In a new paper Exponentially Faster Language Modelling, an ETH Zurich research team introduces UltraFastBERT, a variant of the BERT architecture. UltraFastBERT takes a revolutionary approach by replacing feedforward layers with fast feedforward networks, resulting in an impressive 78x speedup over the optimized baseline feedforward implementation.
"
Microsoft Orca 2’s Triumph: Comparable or Superior Performance to Models 5-10x Its Size in Mastering Reasoning Tasks,https://syncedreview.com/2023/11/22/microsoft-orca-2s-triumph-comparable-or-superior-performance-to-models-5-10x-its-size-in-mastering-reasoning-tasks/,2023-11-22,"The continuous scaling of Large Language Models (LLMs) such as GPT-4 and PaLM-2, with an increasing number of parameters, has revealed unprecedented emergent abilities, most notably the remarkable capacity for zero-shot reasoning. As these models evolve and gain more power, researchers are now delving into the possibility of these models autonomously supervising their behavior or even guiding other AI models. Previous studies have demonstrated that by sampling output from an initial model, student models can be trained to emulate the style of their teachers. However, these student models often fall short in terms of reasoning and comprehension skills compared to their larger foundation models. In June 2023, a Microsoft research team addressed this challenge by introducing Orca, a 13-billion parameter model designed to mimic the reasoning process of large foundation models (LFMs). Orca outperformed conventional instruction-tuned models on benchmarks like BigBench Hard and AGIEval. Continuing along this trajectory, Microsoft has recently unveiled Orca 2 in a new paper titled “Orca 2: Teaching Small Language Models How to Reason.” The focus of this release is to explore how enhanced training signals can augment the reasoning abilities of smaller language models. Notably, Orca 2 surpasses models of similar size, achieving performance levels comparable to or better than models 5-10 times larger. Orca 2’s objectives are twofold. Firstly, researchers aim to instruct smaller models in utilizing a suite of reasoning techniques, including step-by-step processing and recall-then-generate. Secondly, they aspire to assist these models in determining the most effective reasoning strategy for a given task, enabling optimal performance regardless of their size. Similar to its predecessor, Orca 1, the team leverages more capable LLMs to showcase various reasoning strategies across tasks. However, in Orca 2, these strategies are meticulously tailored to each task, considering the capabilities of the student model. A notable technique employed in Orca 2 is Prompt Erasure, making it a Cautious Reasoner. This technique enables the model not only to execute specific reasoning steps but also to strategize at a higher level in approaching a task. Instead of blindly imitating powerful LLMs, the team treats them as a repository of behaviors from which they judiciously select those best suited for the task at hand. In their empirical study, the researchers comprehensively evaluate Orca 2 on 15 benchmarks, covering approximately 100 tasks and over 36,000 unique prompts. The results show that Orca 2 significantly outperforms models of similar size, even matching or surpassing those 5 to 10 times larger, particularly on tasks requiring advanced reasoning. In conclusion, this work marks a significant step forward, emphasizing the importance of teaching smaller models to reason. The research team believes that advancing the capabilities of smaller models will pave the way for new applications with different deployment scenarios and trade-offs between efficiency and capability. The paper Orca 2: Teaching Small Language Models How to Reason on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","The continuous scaling of Large Language Models ( LLMs ) such as GPT-4 and PaLM-2 , with an increasing number of parameters , has revealed unprecedented emergent abilities , most notably the remarkable capacity for zero-shot reasoning . As these models evolve and gain more power , researchers are now delving into the possibility of these models autonomously supervising their behavior or even guiding other AI models . Previous studies have demonstrated that by sampling output from an initial model , student models can be trained to emulate the style of their teachers . However , these student models often fall short in terms of reasoning and comprehension skills compared to their larger foundation models . In June 2023 , a Microsoft research team addressed this challenge by introducing Orca , a 13-billion parameter model designed to mimic the reasoning process of large foundation models ( LFMs ) . Orca outperformed conventional instruction-tuned models on benchmarks like BigBench Hard and AGIEval . Continuing along this trajectory , Microsoft has recently unveiled Orca 2 in a new paper titled “ Orca 2 : Teaching Small Language Models How to Reason. ” The focus of this release is to explore how enhanced training signals can augment the reasoning abilities of smaller language models . Notably , Orca 2 surpasses models of similar size , achieving performance levels comparable to or better than models 5-10 times larger . Orca 2 ’ s objectives are twofold . Firstly , researchers aim to instruct smaller models in utilizing a suite of reasoning techniques , including step-by-step processing and recall-then-generate . Secondly , they aspire to assist these models in determining the most effective reasoning strategy for a given task , enabling optimal performance regardless of their size . Similar to its predecessor , Orca 1 , the team leverages more capable LLMs to showcase various reasoning strategies across tasks . However , in Orca 2 , these strategies are meticulously tailored to each task , considering the capabilities of the student model . A notable technique employed in Orca 2 is Prompt Erasure , making it a Cautious Reasoner . This technique enables the model not only to execute specific reasoning steps but also to strategize at a higher level in approaching a task . Instead of blindly imitating powerful LLMs , the team treats them as a repository of behaviors from which they judiciously select those best suited for the task at hand . In their empirical study , the researchers comprehensively evaluate Orca 2 on 15 benchmarks , covering approximately 100 tasks and over 36,000 unique prompts . The results show that Orca 2 significantly outperforms models of similar size , even matching or surpassing those 5 to 10 times larger , particularly on tasks requiring advanced reasoning . In conclusion , this work marks a significant step forward , emphasizing the importance of teaching smaller models to reason . The research team believes that advancing the capabilities of smaller models will pave the way for new applications with different deployment scenarios and trade-offs between efficiency and capability . The paper Orca 2 : Teaching Small Language Models How to Reason on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['continuous', 'scaling', 'large', 'language', 'model', 'llm', 'gpt4', 'palm2', 'increase', 'number', 'parameter', 'reveal', 'unprecedented', 'emergent', 'ability', 'notably', 'remarkable', 'capacity', 'zeroshot', 'reasoning', 'model', 'evolve', 'gain', 'power', 'researcher', 'delve', 'possibility', 'model', 'autonomously', 'supervise', 'behavior', 'even', 'guide', 'model', 'previous', 'study', 'demonstrate', 'sample', 'output', 'initial', 'model', 'student', 'model', 'train', 'emulate', 'style', 'teacher', 'however', 'student', 'model', 'often', 'fall', 'short', 'term', 'reasoning', 'comprehension', 'skill', 'compare', 'large', 'foundation', 'model', 'research', 'team', 'address', 'challenge', 'introduce', 'orca', 'parameter', 'model', 'design', 'mimic', 'reasoning', 'process', 'large', 'foundation', 'model', 'orca', 'outperform', 'conventional', 'instructiontune', 'model', 'benchmark', 'bigbench', 'hard', 'agieval', 'continue', 'trajectory', 'recently', 'unveil', 'orca', 'new', 'paper', 'title', 'orca', 'teach', 'small', 'language', 'model', 'reason', 'focus', 'release', 'explore', 'enhanced', 'training', 'signal', 'augment', 'reasoning', 'ability', 'small', 'language', 'model', 'notably', 'orca', 'surpasse', 'model', 'similar', 'size', 'achieve', 'performance', 'level', 'comparable', 'well', 'model', 'time', 'large', 'orca', 'objective', 'twofold', 'firstly', 'researcher', 'aim', 'instruct', 'small', 'model', 'utilize', 'suite', 'reasoning', 'technique', 'include', 'stepbystep', 'processing', 'recallthengenerate', 'secondly', 'aspire', 'assist', 'model', 'determine', 'effective', 'reasoning', 'strategy', 'give', 'task', 'enable', 'optimal', 'performance', 'regardless', 'size', 'similar', 'predecessor', 'orca', 'team', 'leverage', 'capable', 'llm', 'showcase', 'various', 'reasoning', 'strategy', 'task', 'however', 'orca', 'strategy', 'meticulously', 'tailor', 'task', 'consider', 'capability', 'student', 'model', 'notable', 'technique', 'employ', 'orca', 'prompt', 'erasure', 'make', 'cautious', 'reasoner', 'technique', 'enable', 'model', 'execute', 'specific', 'reasoning', 'step', 'also', 'strategize', 'high', 'level', 'approach', 'task', 'instead', 'blindly', 'imitate', 'powerful', 'llm', 'team', 'treat', 'repository', 'behavior', 'judiciously', 'select', 'well', 'suit', 'task', 'hand', 'empirical', 'study', 'researcher', 'comprehensively', 'evaluate', 'orca', 'benchmark', 'cover', 'approximately', 'task', 'unique', 'prompt', 'result', 'show', 'orca', 'significantly', 'outperform', 'model', 'similar', 'size', 'even', 'match', 'surpass', 'time', 'large', 'particularly', 'task', 'require', 'advanced', 'reasoning', 'conclusion', 'work', 'mark', 'significant', 'step', 'forward', 'emphasize', 'importance', 'teach', 'small', 'model', 'reason', 'research', 'team', 'believe', 'advance', 'capability', 'small', 'model', 'pave', 'way', 'new', 'application', 'different', 'deployment', 'scenario', 'tradeoff', 'efficiency', 'capability', 'paper', 'orca', 'teach', 'small', 'language', 'model', 'reason', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
Microsoft has recently unveiled Orca 2 in a new paper titled “Orca 2: Teaching Small Language Models How to Reason.” to explore how enhanced training signals can augment the reasoning abilities of smaller language models. Notably, Orca 2 surpasses models of similar size, achieving performance levels comparable to or better than models 5-10 times larger.
"
Democratizing Data: How Apple and UW’s Data Filtering Networks Redefine Large-Scale Training Sets,https://syncedreview.com/2023/11/18/democratizing-data-how-apple-and-uws-data-filtering-networks-redefine-large-scale-training-sets/,2023-11-18,"Large training datasets have become integral to the field of machine learning, serving as the bedrock for recent breakthroughs in language modeling and multimodal learning. Despite their pivotal role, these datasets are seldom the focal point of active research, with many large-scale training sets remaining unreleased. This lack of accessibility hinders consistent dataset evaluation and the establishment of reproducible frameworks. In a new paper Data Filtering Networks, a research team from Apple and University of Washington introduces the concept of data filtering networks (DFNs). These neural networks, specifically designed for data filtration, demonstrate the capacity to generate extensive, high-quality pre-training datasets efficiently. Notably, DFNs can be trained from scratch and improved using the same techniques as standard machine learning models. The core focus of this work is on dataset filtering, assuming the existence of a large uncurated dataset. The research team highlights three key contributions: The ultimate goal of this research is to develop efficient functions capable of filtering potentially trillions of examples. The team refers to the dataset constructed by filtering a given pool with a DFN as the induced dataset, and the model trained exclusively on that dataset as the induced model. Employing a CLIP model as a DFN, the team defines its filtering performance based on the induced model’s evaluation on standard benchmarks. To enhance the DFN, the team initiates the process by training a CLIP model on a high-quality dataset. Subsequently, they fine-tune the filtering network on additional datasets, incorporating standard machine learning techniques such as augmentation, distinct initialization, and extended training with a larger batch size for further refinement. In their empirical study, the team demonstrates that the best performing dataset, DFN-5B, empowers the training of state-of-the-art CLIP models within specified compute budgets. Among various improvements, a ViT-H model trained on this dataset achieves an impressive 84.4% zero-shot transfer accuracy on ImageNet. In summary, this innovative research on data filtering networks opens new avenues for the efficient creation of high-quality datasets from public data, contributing significantly to the democratization of large-scale datasets and advancing the capabilities of machine learning models. The paper Data Filtering Networks on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Large training datasets have become integral to the field of machine learning , serving as the bedrock for recent breakthroughs in language modeling and multimodal learning . Despite their pivotal role , these datasets are seldom the focal point of active research , with many large-scale training sets remaining unreleased . This lack of accessibility hinders consistent dataset evaluation and the establishment of reproducible frameworks . In a new paper Data Filtering Networks , a research team from Apple and University of Washington introduces the concept of data filtering networks ( DFNs ) . These neural networks , specifically designed for data filtration , demonstrate the capacity to generate extensive , high-quality pre-training datasets efficiently . Notably , DFNs can be trained from scratch and improved using the same techniques as standard machine learning models . The core focus of this work is on dataset filtering , assuming the existence of a large uncurated dataset . The research team highlights three key contributions : The ultimate goal of this research is to develop efficient functions capable of filtering potentially trillions of examples . The team refers to the dataset constructed by filtering a given pool with a DFN as the induced dataset , and the model trained exclusively on that dataset as the induced model . Employing a CLIP model as a DFN , the team defines its filtering performance based on the induced model ’ s evaluation on standard benchmarks . To enhance the DFN , the team initiates the process by training a CLIP model on a high-quality dataset . Subsequently , they fine-tune the filtering network on additional datasets , incorporating standard machine learning techniques such as augmentation , distinct initialization , and extended training with a larger batch size for further refinement . In their empirical study , the team demonstrates that the best performing dataset , DFN-5B , empowers the training of state-of-the-art CLIP models within specified compute budgets . Among various improvements , a ViT-H model trained on this dataset achieves an impressive 84.4 % zero-shot transfer accuracy on ImageNet . In summary , this innovative research on data filtering networks opens new avenues for the efficient creation of high-quality datasets from public data , contributing significantly to the democratization of large-scale datasets and advancing the capabilities of machine learning models . The paper Data Filtering Networks on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['large', 'training', 'dataset', 'become', 'integral', 'field', 'machine', 'learn', 'serve', 'bedrock', 'recent', 'breakthrough', 'language', 'modeling', 'multimodal', 'learn', 'pivotal', 'role', 'dataset', 'seldom', 'focal', 'point', 'active', 'research', 'many', 'largescale', 'training', 'set', 'remain', 'unrelease', 'lack', 'accessibility', 'hinder', 'consistent', 'dataset', 'evaluation', 'establishment', 'reproducible', 'framework', 'new', 'paper', 'datum', 'filtering', 'network', 'research', 'team', 'apple', 'introduce', 'concept', 'datum', 'filtering', 'network', 'neural', 'network', 'specifically', 'design', 'datum', 'filtration', 'demonstrate', 'capacity', 'generate', 'extensive', 'highquality', 'pretraine', 'dataset', 'efficiently', 'notably', 'train', 'scratch', 'improve', 'use', 'technique', 'standard', 'machine', 'learning', 'model', 'core', 'focus', 'work', 'dataset', 'filtering', 'assume', 'existence', 'large', 'uncurated', 'dataset', 'research', 'team', 'highlight', 'key', 'contribution', 'ultimate', 'goal', 'research', 'develop', 'efficient', 'function', 'capable', 'filter', 'potentially', 'trillion', 'example', 'team', 'refer', 'dataset', 'construct', 'filter', 'give', 'pool', 'dfn', 'induce', 'dataset', 'model', 'train', 'exclusively', 'dataset', 'induce', 'model', 'employ', 'clip', 'model', 'dfn', 'team', 'define', 'filter', 'performance', 'base', 'induce', 'model', 'evaluation', 'standard', 'benchmark', 'enhance', 'dfn', 'team', 'initiate', 'process', 'train', 'clip', 'model', 'highquality', 'dataset', 'subsequently', 'finetune', 'filter', 'network', 'additional', 'dataset', 'incorporate', 'standard', 'machine', 'learn', 'technique', 'augmentation', 'distinct', 'initialization', 'extend', 'training', 'large', 'batch', 'size', 'refinement', 'empirical', 'study', 'team', 'demonstrate', 'well', 'perform', 'dataset', 'empower', 'training', 'stateoftheart', 'clip', 'model', 'specify', 'compute', 'budget', 'various', 'improvement', 'vith', 'model', 'train', 'dataset', 'achieve', 'impressive', 'zeroshot', 'transfer', 'accuracy', 'imagenet', 'summary', 'innovative', 'research', 'datum', 'filtering', 'network', 'open', 'new', 'avenue', 'efficient', 'creation', 'highquality', 'dataset', 'public', 'datum', 'contribute', 'significantly', 'democratization', 'largescale', 'dataset', 'advance', 'capability', 'machine', 'learning', 'model', 'paper', 'datum', 'filtering', 'network', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
In a new paper Data Filtering Networks, a research team from Apple and University of Washington introduces the concept of data filtering networks (DFNs). These neural networks, specifically designed for data filtration, demonstrate the capacity to generate extensive, high-quality pre-training datasets efficiently. 
"
Adobe & ANU’s LRM Reconstructs Models For Single Image to 3D in 5s,https://syncedreview.com/2023/11/13/adobe-anus-lrm-reconstructs-models-for-single-image-to-3d-in-5s/,2023-11-13,"The concept of instantly generating a 3D representation from a single image of any object is undeniably captivating. This breakthrough promises to significantly advance applications in industrial design, animation, gaming, and the realms of Augmented Reality (AR) and Virtual Reality (VR). Besides, the remarkable achievements in natural language processing and image processing have inspired researchers to delve into the realms of learning a universal 3D foundation for reconstructing objects from single images. In a new paper LRM: Large Reconstruction Model for Single Image to 3D, a research team from Adobe Research and Australian National University introduces an innovative Large Reconstruction Model (LRM). This groundbreaking model has the remarkable ability to predict a 3D model of an object from a single input image in a mere 5 seconds. The LRM approach adopts a robust transformer-based encoder-decoder architecture for acquiring 3D object representations from a single image in a data-driven fashion. The model takes an image as input and regresses a Neural Radiance Field (NeRF) in the form of a triplane representation. To achieve this, LRM employs the pre-trained visual transformer DINO (Caron et al., 2021) as the image encoder to generate image features. Subsequently, it learns an image-to-triplane transformer decoder to project the 2D image features onto the 3D triplane through cross-attention, effectively modeling relationships among the spatially-structured triplane tokens via self-attention. The output tokens from the decoder are then reshaped and upsampled to create the final triplane feature maps. This enables LRM to render images from any viewpoint by decoding the triplane feature of each point. It does so with the aid of an additional shared multi-layer perceptron (MLP) to determine color and density, facilitating volume rendering. What sets LRM apart is its design, which boasts high scalability and efficiency. In addition to employing a fully transformer-based pipeline, the triplane NeRF it employs stands out as a concise and scalable 3D representation. Compared to other alternatives like volumes and point clouds, it is computationally efficient. Furthermore, it offers superior locality with respect to the input image. One of the remarkable aspects of LRM is its training process, which involves minimizing the difference between rendered images and ground truth images at novel perspectives. This is done without the need for excessive 3D-aware regularization or intricate hyper-parameter tuning, making the model exceedingly efficient during training and adaptable to a wide range of multi-view image datasets. Empirical results underscore the remarkable fidelity of LRM when handling various inputs, spanning real-world images, synthetic creations, and rendered images featuring diverse subjects with distinct textures. It stands out as a state-of-the-art solution for single-image-to-3D reconstruction when compared to One-2-3-45. In summary, this groundbreaking work demonstrates the potential of LRM to swiftly predict a 3D model of any object from a single, arbitrary image found in the wild. This development opens up a broad array of real-world applications that can benefit from this rapid and accurate 3D reconstruction capability. Video demos and interactable 3D meshes can be found on this website: https://yiconghong.me/LRM/. The paper LRM: Large Reconstruction Model for Single Image to 3D on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","The concept of instantly generating a 3D representation from a single image of any object is undeniably captivating . This breakthrough promises to significantly advance applications in industrial design , animation , gaming , and the realms of Augmented Reality ( AR ) and Virtual Reality ( VR ) . Besides , the remarkable achievements in natural language processing and image processing have inspired researchers to delve into the realms of learning a universal 3D foundation for reconstructing objects from single images . In a new paper LRM : Large Reconstruction Model for Single Image to 3D , a research team from Adobe Research and Australian National University introduces an innovative Large Reconstruction Model ( LRM ) . This groundbreaking model has the remarkable ability to predict a 3D model of an object from a single input image in a mere 5 seconds . The LRM approach adopts a robust transformer-based encoder-decoder architecture for acquiring 3D object representations from a single image in a data-driven fashion . The model takes an image as input and regresses a Neural Radiance Field ( NeRF ) in the form of a triplane representation . To achieve this , LRM employs the pre-trained visual transformer DINO ( Caron et al. , 2021 ) as the image encoder to generate image features . Subsequently , it learns an image-to-triplane transformer decoder to project the 2D image features onto the 3D triplane through cross-attention , effectively modeling relationships among the spatially-structured triplane tokens via self-attention . The output tokens from the decoder are then reshaped and upsampled to create the final triplane feature maps . This enables LRM to render images from any viewpoint by decoding the triplane feature of each point . It does so with the aid of an additional shared multi-layer perceptron ( MLP ) to determine color and density , facilitating volume rendering . What sets LRM apart is its design , which boasts high scalability and efficiency . In addition to employing a fully transformer-based pipeline , the triplane NeRF it employs stands out as a concise and scalable 3D representation . Compared to other alternatives like volumes and point clouds , it is computationally efficient . Furthermore , it offers superior locality with respect to the input image . One of the remarkable aspects of LRM is its training process , which involves minimizing the difference between rendered images and ground truth images at novel perspectives . This is done without the need for excessive 3D-aware regularization or intricate hyper-parameter tuning , making the model exceedingly efficient during training and adaptable to a wide range of multi-view image datasets . Empirical results underscore the remarkable fidelity of LRM when handling various inputs , spanning real-world images , synthetic creations , and rendered images featuring diverse subjects with distinct textures . It stands out as a state-of-the-art solution for single-image-to-3D reconstruction when compared to One-2-3-45 . In summary , this groundbreaking work demonstrates the potential of LRM to swiftly predict a 3D model of any object from a single , arbitrary image found in the wild . This development opens up a broad array of real-world applications that can benefit from this rapid and accurate 3D reconstruction capability . Video demos and interactable 3D meshes can be found on this website : https : //yiconghong.me/LRM/ . The paper LRM : Large Reconstruction Model for Single Image to 3D on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['concept', 'instantly', 'generate', '3d', 'representation', 'single', 'image', 'object', 'undeniably', 'captivate', 'breakthrough', 'promise', 'significantly', 'advance', 'application', 'industrial', 'design', 'animation', 'gaming', 'realm', 'augmented', 'reality', 'ar', 'virtual', 'reality', 'vr', 'remarkable', 'achievement', 'natural', 'language', 'processing', 'image', 'processing', 'inspire', 'researcher', 'delve', 'realm', 'learn', 'universal', '3d', 'foundation', 'reconstruct', 'object', 'single', 'image', 'new', 'paper', 'lrm', 'large', 'reconstruction', 'model', 'single', 'image', '3d', 'research', 'team', 'adobe', 'research', 'australian', 'national', 'introduce', 'innovative', 'large', 'reconstruction', 'model', 'lrm', 'groundbreaking', 'model', 'remarkable', 'ability', 'predict', '3d', 'model', 'object', 'single', 'input', 'image', 'mere', 'second', 'approach', 'adopt', 'robust', 'transformerbase', 'encoderdecod', 'architecture', 'acquire', '3d', 'object', 'representation', 'single', 'image', 'datadriven', 'fashion', 'model', 'take', 'image', 'input', 'regress', 'neural', 'radiance', 'field', 'nerf', 'form', 'triplane', 'representation', 'achieve', 'lrm', 'employ', 'pretraine', 'visual', 'transformer', 'caron', 'image', 'encoder', 'generate', 'image', 'feature', 'subsequently', 'learn', 'imagetotriplane', 'transformer', 'decoder', 'project', 'image', 'feature', '3d', 'triplane', 'crossattention', 'effectively', 'model', 'relationship', 'spatiallystructured', 'triplane', 'token', 'selfattention', 'output', 'token', 'decoder', 'reshape', 'upsample', 'create', 'final', 'triplane', 'feature', 'map', 'enable', 'render', 'image', 'viewpoint', 'decode', 'triplane', 'feature', 'point', 'aid', 'additional', 'share', 'determine', 'color', 'density', 'facilitate', 'volume', 'render', 'set', 'apart', 'design', 'boast', 'high', 'scalability', 'efficiency', 'addition', 'employ', 'fully', 'transformerbase', 'pipeline', 'triplane', 'nerf', 'employ', 'stand', 'concise', 'scalable', '3d', 'representation', 'compare', 'alternative', 'volume', 'point', 'cloud', 'computationally', 'efficient', 'furthermore', 'offer', 'superior', 'locality', 'respect', 'input', 'image', 'remarkable', 'aspect', 'training', 'process', 'involve', 'minimize', 'difference', 'render', 'image', 'ground', 'truth', 'image', 'novel', 'perspective', 'need', 'excessive', 'regularization', 'intricate', 'hyperparameter', 'tune', 'make', 'model', 'exceedingly', 'efficient', 'training', 'adaptable', 'wide', 'range', 'multiview', 'image', 'dataset', 'empirical', 'result', 'underscore', 'remarkable', 'fidelity', 'handle', 'various', 'input', 'span', 'realworld', 'image', 'synthetic', 'creation', 'render', 'image', 'feature', 'diverse', 'subject', 'distinct', 'texture', 'stand', 'stateoftheart', 'solution', 'singleimageto3d', 'reconstruction', 'compare', 'one2345', 'summary', 'groundbreaking', 'work', 'demonstrate', 'potential', 'swiftly', 'predict', '3d', 'model', 'object', 'single', 'arbitrary', 'image', 'find', 'wild', 'development', 'open', 'broad', 'array', 'realworld', 'application', 'benefit', 'rapid', 'accurate', '3d', 'reconstruction', 'capability', 'video', 'demos', 'interactable', '3d', 'mesh', 'find', 'website', 'https', 'yiconghongmelrm', 'paper', 'lrm', 'large', 'reconstruction', 'model', 'single', 'image', '3d', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
In a new paper LRM: Large Reconstruction Model for Single Image to 3D, a research team from Adobe Research and Australian National Univerisity introduces an innovative Large Reconstruction Model (LRM). This groundbreaking model has the remarkable ability to predict a 3D model of an object from a single input image in a mere 5 seconds.
"
Google’s E3 TTS Provides Effortless Approach to High-Quality Audio Synthesis Through Diffusion Models,https://syncedreview.com/2023/11/06/googles-e3-tts-provides-effortless-approach-to-high-quality-audio-synthesis-through-diffusion-models/,2023-11-06,"Diffusion models have garnered significant recognition for their outstanding performance in a wide range of image and audio generation tasks. Text-to-speech (TTS) systems employing diffusion models have proven their mettle by delivering high-fidelity speech that stands on par with state-of-the-art systems. Nonetheless, many existing TTS systems face a litany of issues, such as heavy reliance on intermediate features’ quality and complex deployment, training, and setup procedures. In a new paper E3 TTS: Easy End-to-End Diffusion-based Text to Speech, a Google research team proposes Easy End-to-End Diffusion-based Text to Speech. This streamlined and efficient text-to-speech model hinges solely on diffusion to preserve temporal structure, allowing it to accept plain text as input and generate audio waveforms directly. The E3 TTS model takes text as input and operates in a non-autoregressive manner, producing waveform outputs without delay. The architecture consists of two primary modules: In a tangible sense, the E3 TTS leverages recent advancements in large language models. It relies on text representations provided by a pretrained BERT model. Unlike some prior approaches, which require representations like phonemes or graphemes, the E3 TTS simplifies the process by depending solely on a pretrained text language model. This model can be trained on multiple languages using only text data, which streamlines the system’s versatility. The U-Net structure encompasses a sequence of downsampling and upsampling blocks linked by residuals. To enhance information extraction from the BERT output, the team incorporates crossattention in the top downsampling/upsampling blocks. In the lower blocks, an adaptive softmax Convolutional Neural Network (CNN) kernel is employed, with its kernel size determined by the timestep and speaker. In other layers, speaker and timestep embeddings are combined through Feature-wise Linear Modulation (FiLM), which includes a composite layer for channel-wise scaling and bias prediction. The downsampler plays a crucial role in refining the noisy information, converting it from 24kHz to a sequence of similar length to the encoded BERT output, which significantly improves the overall quality. On the flip side, the upsampler predicts noise with the same length as the input waveform. Empirical evidence demonstrates that E3 TTS can generate high-fidelity audio, approaching the performance of state-of-the-art neural TTS systems. Furthermore, it enables various zero-shot tasks, such as speech editing and prompt-based generation. In summary, this work underscores the remarkable capabilities of E3 TTS in generating high-quality audio directly from BERT features. It simplifies the design of end-to-end TTS systems and has proven to deliver impressive results in experiments. Audio samples are available at https://e3tts.github.io. The paper E3 TTS: Easy End-to-End Diffusion-based Text to Speech on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Diffusion models have garnered significant recognition for their outstanding performance in a wide range of image and audio generation tasks . Text-to-speech ( TTS ) systems employing diffusion models have proven their mettle by delivering high-fidelity speech that stands on par with state-of-the-art systems . Nonetheless , many existing TTS systems face a litany of issues , such as heavy reliance on intermediate features ’ quality and complex deployment , training , and setup procedures . In a new paper E3 TTS : Easy End-to-End Diffusion-based Text to Speech , a Google research team proposes Easy End-to-End Diffusion-based Text to Speech . This streamlined and efficient text-to-speech model hinges solely on diffusion to preserve temporal structure , allowing it to accept plain text as input and generate audio waveforms directly . The E3 TTS model takes text as input and operates in a non-autoregressive manner , producing waveform outputs without delay . The architecture consists of two primary modules : In a tangible sense , the E3 TTS leverages recent advancements in large language models . It relies on text representations provided by a pretrained BERT model . Unlike some prior approaches , which require representations like phonemes or graphemes , the E3 TTS simplifies the process by depending solely on a pretrained text language model . This model can be trained on multiple languages using only text data , which streamlines the system ’ s versatility . The U-Net structure encompasses a sequence of downsampling and upsampling blocks linked by residuals . To enhance information extraction from the BERT output , the team incorporates crossattention in the top downsampling/upsampling blocks . In the lower blocks , an adaptive softmax Convolutional Neural Network ( CNN ) kernel is employed , with its kernel size determined by the timestep and speaker . In other layers , speaker and timestep embeddings are combined through Feature-wise Linear Modulation ( FiLM ) , which includes a composite layer for channel-wise scaling and bias prediction . The downsampler plays a crucial role in refining the noisy information , converting it from 24kHz to a sequence of similar length to the encoded BERT output , which significantly improves the overall quality . On the flip side , the upsampler predicts noise with the same length as the input waveform . Empirical evidence demonstrates that E3 TTS can generate high-fidelity audio , approaching the performance of state-of-the-art neural TTS systems . Furthermore , it enables various zero-shot tasks , such as speech editing and prompt-based generation . In summary , this work underscores the remarkable capabilities of E3 TTS in generating high-quality audio directly from BERT features . It simplifies the design of end-to-end TTS systems and has proven to deliver impressive results in experiments . Audio samples are available at https : //e3tts.github.io . The paper E3 TTS : Easy End-to-End Diffusion-based Text to Speech on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['diffusion', 'model', 'garner', 'significant', 'recognition', 'outstanding', 'performance', 'wide', 'range', 'image', 'audio', 'generation', 'task', 'system', 'employ', 'diffusion', 'model', 'prove', 'mettle', 'deliver', 'highfidelity', 'speech', 'stand', 'par', 'stateoftheart', 'system', 'nonetheless', 'many', 'exist', 'tts', 'system', 'face', 'litany', 'issue', 'heavy', 'reliance', 'intermediate', 'feature', 'quality', 'complex', 'deployment', 'training', 'setup', 'procedure', 'new', 'paper', 'tts', 'easy', 'endtoend', 'diffusionbase', 'text', 'speech', 'research', 'team', 'propose', 'easy', 'endtoend', 'diffusionbase', 'text', 'speech', 'streamlined', 'efficient', 'texttospeech', 'model', 'hinge', 'solely', 'diffusion', 'preserve', 'temporal', 'structure', 'allow', 'accept', 'plain', 'text', 'input', 'generate', 'audio', 'waveform', 'directly', 'tts', 'model', 'take', 'text', 'input', 'operate', 'nonautoregressive', 'manner', 'produce', 'waveform', 'output', 'delay', 'architecture', 'consist', 'primary', 'module', 'tangible', 'sense', 'tts', 'leverage', 'recent', 'advancement', 'large', 'language', 'model', 'rely', 'text', 'representation', 'provide', 'pretraine', 'model', 'prior', 'approach', 'require', 'representation', 'phoneme', 'grapheme', 'tts', 'simplifie', 'process', 'depend', 'solely', 'pretraine', 'text', 'language', 'model', 'model', 'train', 'multiple', 'language', 'use', 'text', 'datum', 'streamline', 'system', 'versatility', 'unet', 'structure', 'encompass', 'sequence', 'downsampling', 'upsampling', 'block', 'link', 'residual', 'enhance', 'information', 'extraction', 'output', 'team', 'incorporate', 'crossattention', 'top', 'downsamplingupsample', 'block', 'low', 'block', 'adaptive', 'softmax', 'convolutional', 'neural', 'network', 'kernel', 'employ', 'kernel', 'size', 'determine', 'timestep', 'speaker', 'layer', 'speaker', 'timestep', 'embedding', 'combine', 'featurewise', 'linear', 'modulation', 'film', 'include', 'composite', 'layer', 'channelwise', 'scaling', 'bias', 'prediction', 'downsampler', 'play', 'crucial', 'role', 'refine', 'noisy', 'information', 'convert', '24khz', 'sequence', 'similar', 'length', 'encoded', 'bert', 'output', 'significantly', 'improve', 'overall', 'quality', 'flip', 'side', 'upsampler', 'predict', 'noise', 'length', 'input', 'waveform', 'empirical', 'evidence', 'demonstrate', 'tts', 'generate', 'highfidelity', 'audio', 'approach', 'performance', 'stateoftheart', 'neural', 'system', 'furthermore', 'enable', 'various', 'zeroshot', 'task', 'speech', 'editing', 'promptbase', 'generation', 'summary', 'work', 'underscore', 'remarkable', 'capability', 'tt', 'generate', 'highquality', 'audio', 'directly', 'feature', 'simplify', 'design', 'endtoend', 'system', 'prove', 'deliver', 'impressive', 'result', 'experiment', 'audio', 'sample', 'available', 'paper', 'tts', 'easy', 'endtoend', 'diffusionbase', 'text', 'speech', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
In a new paper E3 TTS: Easy End-to-End Diffusion-based Text to Speech, a Google research team proposes Easy End-to-End Diffusion-based Text to Speech. This streamlined and efficient text-to-speech model hinges solely on diffusion to preserve temporal structure, allowing it to accept plain text as input and generate audio waveforms directly.
"
Apple Repurposes Large Language Models for Reinforcement Learning challenges in Embodied AI,https://syncedreview.com/2023/11/01/apple-repurposes-large-language-models-for-reinforcement-learning-challenges-in-embodied-ai/,2023-11-01,"Large Language Models (LLMs) have ushered in an era of unparalleled language understanding capabilities, raising the possibility of harnessing their prowess for complex embodied visual tasks. This new frontier explores whether these models can be the cornerstone of adaptable, generalizable policies for decision-making that seamlessly transfer to novel scenarios. In a new paper Large Language Models as Generalizable Policies for Embodied Tasks, an Apple research team presents Large LAnguage model Reinforcement Learning Policy (LLaRP). LLaRP effectively repurposes LLMs for Reinforcement Learning (RL) challenges within the realm of Embodied Artificial Intelligence (AI), achieving a remarkable 1.7 times higher success rate compared to other established baselines and zero-shot LLM applications. The LLaRP approach is a pioneering effort in adapting pre-trained LLMs to navigate multi-modal decision-making settings inherent to embodied tasks. The core of the problem is cast as a Partially-Observable Markov Decision Process (POMDP), wherein the policy’s inputs encompass task instructions and egocentric visual RGB frames from the current time step. These inputs are encoded using LLM embeddings or a vision encoder. These embeddings serve as the input to a pre-trained LLM, and the hidden outputs are subsequently projected to action and value predictions. Notably, the entire system learns through online RL, with the action output module and observation encoder MLP being the only trainable components while the others remain frozen. The research team demonstrates that using a pre-trained and frozen LLM as a Vision-Language Model (VLM) policy with learned input and output adapter layers results in a policy showcasing robust generalization capabilities. This policy is trained using online RL, and its generalization is assessed along two axes: Paraphrastic Robustness (PR) and Behavior Generalization (BG). LLaRP undergoes rigorous evaluation across over 1,000 unseen tasks, spanning the axes of PR and BG, and achieves an impressive 42% success rate. This surpasses the performance of alternative LSTM-based policies at 25% and zero-shot LLM applications at 22%. Importantly, LLaRP outperforms all baselines when given novel instructions and when assigned previously unseen tasks. Moreover, the researchers demonstrate that the LLaRP LLM-based policy provides a significant performance boost in a distinct domain, Atari, compared to a Transformer baseline. The research team further uncovers the benefits of infusing LLM-encoded world knowledge into RL. LLM-based models exhibit superior sample efficiency compared to other conventional architectures in both basic Proximal Policy Optimization (PPO) RL and continual learning settings. Furthermore, LLaRP proves to be more efficient in terms of required supervision when contrasted with commonly used imitation learning techniques. In a promising initiative to facilitate further exploration of generalization in Embodied AI, the researchers introduce the Language Rearrangement task. This task involves a staggering 150,000 distinct language instructions, each equipped with automatically generated rewards, providing a valuable framework for ongoing research in the field. In conclusion, this pioneering research paper exemplifies the transformative potential of integrating LLMs into embodied tasks. The LLaRP approach not only excels in achieving high success rates but also significantly enhances efficiency, opening up exciting possibilities for the future of Embodied AI research and development. Video examples of LLaRP in unseen Language Rearrangement instructions are at https://llm-rl.github.io. The paper Large Language Models as Generalizable Policies for Embodied Tasks on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Large Language Models ( LLMs ) have ushered in an era of unparalleled language understanding capabilities , raising the possibility of harnessing their prowess for complex embodied visual tasks . This new frontier explores whether these models can be the cornerstone of adaptable , generalizable policies for decision-making that seamlessly transfer to novel scenarios . In a new paper Large Language Models as Generalizable Policies for Embodied Tasks , an Apple research team presents Large LAnguage model Reinforcement Learning Policy ( LLaRP ) . LLaRP effectively repurposes LLMs for Reinforcement Learning ( RL ) challenges within the realm of Embodied Artificial Intelligence ( AI ) , achieving a remarkable 1.7 times higher success rate compared to other established baselines and zero-shot LLM applications . The LLaRP approach is a pioneering effort in adapting pre-trained LLMs to navigate multi-modal decision-making settings inherent to embodied tasks . The core of the problem is cast as a Partially-Observable Markov Decision Process ( POMDP ) , wherein the policy ’ s inputs encompass task instructions and egocentric visual RGB frames from the current time step . These inputs are encoded using LLM embeddings or a vision encoder . These embeddings serve as the input to a pre-trained LLM , and the hidden outputs are subsequently projected to action and value predictions . Notably , the entire system learns through online RL , with the action output module and observation encoder MLP being the only trainable components while the others remain frozen . The research team demonstrates that using a pre-trained and frozen LLM as a Vision-Language Model ( VLM ) policy with learned input and output adapter layers results in a policy showcasing robust generalization capabilities . This policy is trained using online RL , and its generalization is assessed along two axes : Paraphrastic Robustness ( PR ) and Behavior Generalization ( BG ) . LLaRP undergoes rigorous evaluation across over 1,000 unseen tasks , spanning the axes of PR and BG , and achieves an impressive 42 % success rate . This surpasses the performance of alternative LSTM-based policies at 25 % and zero-shot LLM applications at 22 % . Importantly , LLaRP outperforms all baselines when given novel instructions and when assigned previously unseen tasks . Moreover , the researchers demonstrate that the LLaRP LLM-based policy provides a significant performance boost in a distinct domain , Atari , compared to a Transformer baseline . The research team further uncovers the benefits of infusing LLM-encoded world knowledge into RL . LLM-based models exhibit superior sample efficiency compared to other conventional architectures in both basic Proximal Policy Optimization ( PPO ) RL and continual learning settings . Furthermore , LLaRP proves to be more efficient in terms of required supervision when contrasted with commonly used imitation learning techniques . In a promising initiative to facilitate further exploration of generalization in Embodied AI , the researchers introduce the Language Rearrangement task . This task involves a staggering 150,000 distinct language instructions , each equipped with automatically generated rewards , providing a valuable framework for ongoing research in the field . In conclusion , this pioneering research paper exemplifies the transformative potential of integrating LLMs into embodied tasks . The LLaRP approach not only excels in achieving high success rates but also significantly enhances efficiency , opening up exciting possibilities for the future of Embodied AI research and development . Video examples of LLaRP in unseen Language Rearrangement instructions are at https : //llm-rl.github.io . The paper Large Language Models as Generalizable Policies for Embodied Tasks on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['large', 'language', 'model', 'llm', 'usher', 'era', 'unparalleled', 'language', 'understand', 'capability', 'raise', 'possibility', 'harness', 'prowess', 'complex', 'embody', 'visual', 'task', 'new', 'frontier', 'explore', 'model', 'cornerstone', 'adaptable', 'generalizable', 'policy', 'decisionmake', 'seamlessly', 'transfer', 'novel', 'scenario', 'new', 'paper', 'large', 'language', 'model', 'generalizable', 'policy', 'embody', 'task', 'apple', 'research', 'team', 'present', 'large', 'language', 'model', 'reinforcement', 'learning', 'policy', 'llarp', 'llarp', 'effectively', 'repurpose', 'llm', 'reinforcement', 'learning', 'rl', 'challenge', 'realm', 'embody', 'artificial', 'intelligence', 'achieve', 'remarkable', 'time', 'high', 'success', 'rate', 'compare', 'establish', 'baseline', 'zeroshot', 'llm', 'application', 'llarp', 'approach', 'pioneering', 'effort', 'adapt', 'pretraine', 'llm', 'navigate', 'multimodal', 'decisionmake', 'setting', 'inherent', 'embody', 'task', 'core', 'problem', 'cast', 'partiallyobservable', 'markov', 'decision', 'process', 'pomdp', 'policy', 'input', 'encompass', 'task', 'instruction', 'egocentric', 'visual', 'frame', 'current', 'time', 'step', 'input', 'encode', 'use', 'llm', 'embedding', 'vision', 'encoder', 'embedding', 'serve', 'input', 'pretrained', 'llm', 'hidden', 'output', 'subsequently', 'project', 'action', 'value', 'prediction', 'notably', 'entire', 'system', 'learn', 'rl', 'action', 'output', 'module', 'observation', 'encoder', 'mlp', 'trainable', 'component', 'remain', 'frozen', 'research', 'team', 'demonstrate', 'use', 'pretraine', 'frozen', 'llm', 'visionlanguage', 'model', 'policy', 'learn', 'input', 'output', 'adapter', 'layer', 'result', 'policy', 'showcasing', 'robust', 'generalization', 'capability', 'policy', 'train', 'use', 'online', 'rl', 'generalization', 'assess', 'axis', 'paraphrastic', 'robustness', 'behavior', 'bg', 'rigorous', 'evaluation', 'unseen', 'task', 'span', 'axis', 'achieve', 'impressive', 'success', 'rate', 'surpass', 'performance', 'alternative', 'lstmbased', 'policy', 'zeroshot', 'llm', 'application', 'importantly', 'llarp', 'outperform', 'baseline', 'give', 'novel', 'instruction', 'assign', 'previously', 'unseen', 'task', 'moreover', 'researcher', 'demonstrate', 'llarp', 'llmbase', 'policy', 'provide', 'significant', 'performance', 'boost', 'distinct', 'domain', 'atari', 'compare', 'transformer', 'baseline', 'research', 'team', 'uncover', 'benefit', 'infuse', 'llmencoded', 'world', 'knowledge', 'llmbase', 'model', 'exhibit', 'superior', 'sample', 'efficiency', 'compare', 'conventional', 'architecture', 'basic', 'proximal', 'policy', 'optimization', 'ppo', 'continual', 'learning', 'setting', 'furthermore', 'llarp', 'prove', 'efficient', 'term', 'require', 'supervision', 'contrast', 'commonly', 'use', 'imitation', 'learn', 'technique', 'promising', 'initiative', 'facilitate', 'exploration', 'generalization', 'embody', 'ai', 'researcher', 'introduce', 'language', 'rearrangement', 'task', 'task', 'involve', 'staggering', 'distinct', 'language', 'instruction', 'equip', 'automatically', 'generate', 'reward', 'provide', 'valuable', 'framework', 'ongoing', 'research', 'field', 'conclusion', 'pioneer', 'research', 'paper', 'exemplify', 'transformative', 'potential', 'integrate', 'llm', 'embody', 'task', 'llarp', 'approach', 'excel', 'achieve', 'high', 'success', 'rate', 'also', 'significantly', 'enhance', 'efficiency', 'opening', 'exciting', 'possibility', 'future', 'embody', 'ai', 'research', 'development', 'video', 'example', 'llarp', 'unseen', 'language', 'rearrangement', 'instruction', 'https', 'llmrlgithubio', 'paper', 'large', 'language', 'model', 'generalizable', 'policy', 'embody', 'task', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
An Apple research team presents Large LAnguage model Reinforcement Learning Policy (LLaRP). LLaRP effectively repurposes LLMs for Reinforcement Learning (RL) challenges within the realm of Embodied Artificial Intelligence (AI), achieving a remarkable 1.7 times higher success rate compared to other established baselines and zero-shot LLM applications.
"
Make Machine Learning Work for You,https://www.technologyreview.com/2023/09/20/1079506/make-machine-learning-work-for-you/,2023-09-20,"Provided byCapital One The enthusiasm for AI and its applications is reaching a nadir, according to an August 2023 Gartner Hype Cycle press release, where generative AI is nearly perched atop the category of technologies at their “Peak of Inflated Expectations,” ready to plunge into the “Trough of Disillusionment.” A quick look at social media agrees, with some pages filled with targeted advertisements about topics as prosaic as “GPT for your pile of receipts.” This is good evidence that the AI craze is becoming a hammer looking for a nail. Yet, with all this fervor, according to McKinsey, while AI adoption has more than doubled since 2017, it has leveled off at around 50% to 60% during the past few years. IBM reveals that nearly half of the challenges related to AI adoption focus on data complexity (24%) and difficulty integrating and scaling projects (24%). While it may be expedient for marketers to “slap a GPT suffix on it and call it AI,” businesses striving to truly implement and incorporate AI and ML face a two-headed challenge: first, it’s difficult and expensive, and second, because it’s difficult and expensive, it’s hard to come by the “sandboxes” that are necessary to enable experimentation and prove “green shoots” of value that would warrant further investment. In short, AI and ML are inaccessible. History shows that most business shifts at first seem difficult and expensive. However, spending time and resources on these efforts has paid off for the innovators. Businesses identify new assets, and use new processes to achieve new goals—sometimes lofty, unexpected ones. The asset at the focus of the AI craze is data. The world is exploding with data. According to a 2020 report by Seagate and IDC, during the next two years, enterprise data is projected to increase at a 42.2% annual growth rate. And yet, only 32% of that data is currently being put to work. Effective data management—storing, labeling, cataloging, securing, connecting, and making queryable—has no shortage of challenges. Once those challenges are overcome, businesses will need to identify users not only technically proficient enough to access and leverage that data, but also able to do so in a comprehensive manner. Businesses today find themselves tasking garden-variety analysts with targeted, hypothesis-driven work. The shorthand is encapsulated in a common refrain: “I usually have analysts pull down a subset of the data and run pivot tables on it.” To avoid tunnel vision and use data more comprehensively, this hypothesis-driven analysis is supplemented with business intelligence (BI), where data at scale is finessed into reports, dashboards, and visualizations. But even then, the dizzying scale of charts and graphs requires the person reviewing them to have a strong sense of what matters and what to look for—again, to be hypothesis-driven—in order to make sense of the world. Human beings simply cannot otherwise handle the cognitive overload. The moment is opportune for AI and ML. Ideally, that would mean plentiful teams of data scientists, data engineers, and ML engineers that can deliver such solutions, at a price that folds neatly into IT budgets. Also ideally, businesses are ready with the right amount of technology; GPUs, compute, and orchestration infrastructure to build and deploy AI and ML solutions at scale. But much like the business revolutions of days past, this isn’t the case. The marketplace is offering a proliferation of solutions based on two approaches: adding even more intelligence and insights to existing BI tools; and making it increasingly easier to develop and deploy ML solutions, in the growing field of ML operations, or MLOps. BI is making significant inroads on augmenting its capabilities with ML, but still has the intrinsic cognitive overload challenge to overcome. ML capabilities are so embedded in BI interfaces that they aren’t easily extracted to be applied in more bespoke ways. MLOps comes from the other direction, by easing the development and promotion of ML models. The challenge for MLOps is, while it makes data scientists and ML engineers more productive—more building and training models, and less wrangling data, deploying, and productionizing—it doesn’t address the fact that those very data scientists and ML engineers remain scarce and expensive in the first place. The onus is therefore on businesses to find solutions that can enable non-Ph.D, traditional analysts to become effective ML practitioners. This is ML Democratization. Capital One began laying the foundations for the journey to ML democratization more than a decade ago, when it went all-in on the cloud, creating a modern computing environment that enables instant provisioning of infrastructure and increased processing power. This modern computing environment makes complex and large-scale data set analysis possible at increasing levels of efficiency. Capital One adopted a philosophy of centralized and standardized platforms and governance. For AI and ML, it built an ML platform that provides engineers and scientists with governed access to algorithms, components, and infrastructure for reuse. The computing environment and platform philosophy provided necessary, but not sufficient, ingredients to democratize ML. Infusing a “no hammers looking for nails” mantra, Capital One’s team of ML engineers and data scientists went with a business problem-first approach. Instead of gathering technical requirements the team gathered problem statements. For instance, Capital One’s credit card transaction fraud team looked for a way to comprehensively detect pockets of fraud and automatically create real-time defenses. So the company developed ML algorithms, components, and infrastructure to build a solution. In the process, those components were published to a central ML platform to be reused and improved upon for future business problems requiring similar approaches. As organizations expand their range of business use cases and develop solutions, they often find recurring patterns that can be harnessed for wider benefit. Recognizing these patterns can lead to a powerful realization: by making commonly used ML libraries, workflows, and components accessible through user-friendly interfaces, businesses can unleash the potential of ML across their enterprise, without requiring deep data science or engineering expertise. This democratization of ML serves as a solution to several challenges, including cognitive overload, resource constraints, and accessibility issues. It paves the way for a culture of experimentation, essential for turning ML into a valuable tool rather than just a passing trend. Now, if a business analyst wants to identify anomalies or track trends in their portfolio's granular segments, or if a marketing associate wants to perform in-depth campaign analysis beyond what traditional analytics tools offer, ML can meet these needs with minimal demands on engineering resources. Using ML democratization transforms it from a shiny object into a centerpiece of practical value. In a single working day, an analyst with no prior ML knowledge or coding skills can uncover insightful information from any dataset of their choice. This shift significantly reduces the cost associated with exploring ML's potential and its application across various business areas. No-code ML solutions could play a pivotal role in achieving ML democratization. We’re already seeing it happen, and ML will continue to become more accessible through technology advancements including no-code solutions. This ML democratization will allow business analysts to confidently make decisions they wouldn’t have previously considered, resulting in profound and lasting impacts. This content was produced by Capital One. It was not written by MIT Technology Review’s editorial staff. ","Provided byCapital One The enthusiasm for AI and its applications is reaching a nadir , according to an August 2023 Gartner Hype Cycle press release , where generative AI is nearly perched atop the category of technologies at their “ Peak of Inflated Expectations , ” ready to plunge into the “ Trough of Disillusionment. ” A quick look at social media agrees , with some pages filled with targeted advertisements about topics as prosaic as “ GPT for your pile of receipts. ” This is good evidence that the AI craze is becoming a hammer looking for a nail . Yet , with all this fervor , according to McKinsey , while AI adoption has more than doubled since 2017 , it has leveled off at around 50 % to 60 % during the past few years . IBM reveals that nearly half of the challenges related to AI adoption focus on data complexity ( 24 % ) and difficulty integrating and scaling projects ( 24 % ) . While it may be expedient for marketers to “ slap a GPT suffix on it and call it AI , ” businesses striving to truly implement and incorporate AI and ML face a two-headed challenge : first , it ’ s difficult and expensive , and second , because it ’ s difficult and expensive , it ’ s hard to come by the “ sandboxes ” that are necessary to enable experimentation and prove “ green shoots ” of value that would warrant further investment . In short , AI and ML are inaccessible . History shows that most business shifts at first seem difficult and expensive . However , spending time and resources on these efforts has paid off for the innovators . Businesses identify new assets , and use new processes to achieve new goals—sometimes lofty , unexpected ones . The asset at the focus of the AI craze is data . The world is exploding with data . According to a 2020 report by Seagate and IDC , during the next two years , enterprise data is projected to increase at a 42.2 % annual growth rate . And yet , only 32 % of that data is currently being put to work . Effective data management—storing , labeling , cataloging , securing , connecting , and making queryable—has no shortage of challenges . Once those challenges are overcome , businesses will need to identify users not only technically proficient enough to access and leverage that data , but also able to do so in a comprehensive manner . Businesses today find themselves tasking garden-variety analysts with targeted , hypothesis-driven work . The shorthand is encapsulated in a common refrain : “ I usually have analysts pull down a subset of the data and run pivot tables on it. ” To avoid tunnel vision and use data more comprehensively , this hypothesis-driven analysis is supplemented with business intelligence ( BI ) , where data at scale is finessed into reports , dashboards , and visualizations . But even then , the dizzying scale of charts and graphs requires the person reviewing them to have a strong sense of what matters and what to look for—again , to be hypothesis-driven—in order to make sense of the world . Human beings simply can not otherwise handle the cognitive overload . The moment is opportune for AI and ML . Ideally , that would mean plentiful teams of data scientists , data engineers , and ML engineers that can deliver such solutions , at a price that folds neatly into IT budgets . Also ideally , businesses are ready with the right amount of technology ; GPUs , compute , and orchestration infrastructure to build and deploy AI and ML solutions at scale . But much like the business revolutions of days past , this isn ’ t the case . The marketplace is offering a proliferation of solutions based on two approaches : adding even more intelligence and insights to existing BI tools ; and making it increasingly easier to develop and deploy ML solutions , in the growing field of ML operations , or MLOps . BI is making significant inroads on augmenting its capabilities with ML , but still has the intrinsic cognitive overload challenge to overcome . ML capabilities are so embedded in BI interfaces that they aren ’ t easily extracted to be applied in more bespoke ways . MLOps comes from the other direction , by easing the development and promotion of ML models . The challenge for MLOps is , while it makes data scientists and ML engineers more productive—more building and training models , and less wrangling data , deploying , and productionizing—it doesn ’ t address the fact that those very data scientists and ML engineers remain scarce and expensive in the first place . The onus is therefore on businesses to find solutions that can enable non-Ph.D , traditional analysts to become effective ML practitioners . This is ML Democratization . Capital One began laying the foundations for the journey to ML democratization more than a decade ago , when it went all-in on the cloud , creating a modern computing environment that enables instant provisioning of infrastructure and increased processing power . This modern computing environment makes complex and large-scale data set analysis possible at increasing levels of efficiency . Capital One adopted a philosophy of centralized and standardized platforms and governance . For AI and ML , it built an ML platform that provides engineers and scientists with governed access to algorithms , components , and infrastructure for reuse . The computing environment and platform philosophy provided necessary , but not sufficient , ingredients to democratize ML . Infusing a “ no hammers looking for nails ” mantra , Capital One ’ s team of ML engineers and data scientists went with a business problem-first approach . Instead of gathering technical requirements the team gathered problem statements . For instance , Capital One ’ s credit card transaction fraud team looked for a way to comprehensively detect pockets of fraud and automatically create real-time defenses . So the company developed ML algorithms , components , and infrastructure to build a solution . In the process , those components were published to a central ML platform to be reused and improved upon for future business problems requiring similar approaches . As organizations expand their range of business use cases and develop solutions , they often find recurring patterns that can be harnessed for wider benefit . Recognizing these patterns can lead to a powerful realization : by making commonly used ML libraries , workflows , and components accessible through user-friendly interfaces , businesses can unleash the potential of ML across their enterprise , without requiring deep data science or engineering expertise . This democratization of ML serves as a solution to several challenges , including cognitive overload , resource constraints , and accessibility issues . It paves the way for a culture of experimentation , essential for turning ML into a valuable tool rather than just a passing trend . Now , if a business analyst wants to identify anomalies or track trends in their portfolio 's granular segments , or if a marketing associate wants to perform in-depth campaign analysis beyond what traditional analytics tools offer , ML can meet these needs with minimal demands on engineering resources . Using ML democratization transforms it from a shiny object into a centerpiece of practical value . In a single working day , an analyst with no prior ML knowledge or coding skills can uncover insightful information from any dataset of their choice . This shift significantly reduces the cost associated with exploring ML 's potential and its application across various business areas . No-code ML solutions could play a pivotal role in achieving ML democratization . We ’ re already seeing it happen , and ML will continue to become more accessible through technology advancements including no-code solutions . This ML democratization will allow business analysts to confidently make decisions they wouldn ’ t have previously considered , resulting in profound and lasting impacts . This content was produced by Capital One . It was not written by MIT Technology Review ’ s editorial staff .","['provide', 'bycapital', 'one', 'enthusiasm', 'ai', 'application', 'reach', 'nadir', 'accord', 'cycle', 'press', 'release', 'generative', 'ai', 'nearly', 'perch', 'category', 'technology', 'peak', 'inflated', 'expectation', 'ready', 'plunge', 'trough', 'disillusionment', 'quick', 'look', 'social', 'medium', 'agree', 'page', 'fill', 'target', 'advertisement', 'topic', 'prosaic', 'gpt', 'pile', 'receipt', 'good', 'evidence', 'become', 'hammer', 'look', 'nail', 'yet', 'fervor', 'accord', 'adoption', 'double', 'level', 'around', 'past', 'year', 'reveal', 'nearly', 'half', 'challenge', 'relate', 'ai', 'adoption', 'focus', 'data', 'complexity', 'difficulty', 'integrate', 'scale', 'project', 'expedient', 'marketer', 'slap', 'gpt', 'suffix', 'call', 'ai', 'business', 'strive', 'truly', 'implement', 'incorporate', 'ai', 'face', 'twoheaded', 'challenge', 'first', 'difficult', 'expensive', 'second', 'difficult', 'expensive', 'hard', 'come', 'sandbox', 'necessary', 'enable', 'experimentation', 'prove', 'green', 'shoot', 'value', 'warrant', 'investment', 'short', 'ai', 'inaccessible', 'history', 'show', 'business', 'shift', 'first', 'seem', 'difficult', 'expensive', 'however', 'spend', 'time', 'resource', 'effort', 'pay', 'innovator', 'business', 'identify', 'new', 'asset', 'use', 'new', 'process', 'achieve', 'new', 'goal', 'sometimes', 'lofty', 'unexpected', 'one', 'asset', 'focus', 'datum', 'world', 'explode', 'datum', 'accord', 'report', 'seagate', 'idc', 'next', 'year', 'enterprise', 'datum', 'project', 'increase', 'annual', 'growth', 'rate', 'yet', 'datum', 'currently', 'put', 'work', 'effective', 'datum', 'management', 'store', 'labeling', 'catalog', 'secure', 'connect', 'make', 'queryable', 'shortage', 'challenge', 'challenge', 'overcome', 'business', 'need', 'identify', 'user', 'technically', 'proficient', 'enough', 'access', 'leverage', 'datum', 'also', 'able', 'comprehensive', 'manner', 'business', 'today', 'find', 'task', 'gardenvariety', 'analyst', 'target', 'hypothesisdriven', 'work', 'shorthand', 'encapsulate', 'common', 'refrain', 'usually', 'analyst', 'pull', 'subset', 'datum', 'run', 'pivot', 'table', 'avoid', 'tunnel', 'vision', 'use', 'datum', 'comprehensively', 'hypothesisdriven', 'analysis', 'supplement', 'business', 'intelligence', 'datum', 'scale', 'finesse', 'report', 'dashboard', 'visualization', 'even', 'dizzying', 'scale', 'chart', 'graph', 'require', 'person', 'review', 'strong', 'sense', 'matter', 'look', 'hypothesisdriven', 'order', 'make', 'sense', 'world', 'human', 'simply', 'otherwise', 'handle', 'cognitive', 'overload', 'moment', 'opportune', 'ai', 'ideally', 'mean', 'plentiful', 'team', 'datum', 'scientist', 'datum', 'engineer', 'engineer', 'deliver', 'solution', 'price', 'fold', 'neatly', 'budget', 'also', 'ideally', 'business', 'ready', 'right', 'amount', 'technology', 'gpu', 'compute', 'orchestration', 'infrastructure', 'build', 'deploy', 'ai', 'solution', 'scale', 'much', 'business', 'revolution', 'day', 'case', 'marketplace', 'offer', 'proliferation', 'solution', 'base', 'approach', 'add', 'even', 'intelligence', 'insight', 'exist', 'bi', 'tool', 'make', 'increasingly', 'easy', 'develop', 'deploy', 'solution', 'grow', 'field', 'operation', 'mlop', 'make', 'significant', 'inroad', 'augment', 'capability', 'still', 'intrinsic', 'cognitive', 'overload', 'challenge', 'overcome', 'capability', 'embed', 'bi', 'interface', 'easily', 'extract', 'apply', 'bespoke', 'way', 'mlop', 'come', 'direction', 'ease', 'development', 'promotion', 'model', 'challenge', 'mlop', 'make', 'datum', 'scientist', 'engineer', 'productive', 'building', 'training', 'model', 'less', 'wrangling', 'datum', 'deploy', 'productionize', 'address', 'fact', 'datum', 'scientist', 'engineer', 'remain', 'scarce', 'expensive', 'first', 'place', 'onus', 'therefore', 'business', 'find', 'solution', 'enable', 'nonphd', 'traditional', 'analyst', 'become', 'effective', 'ml', 'practitioner', 'democratization', 'capital', 'begin', 'lay', 'foundation', 'journey', 'ml', 'democratization', 'decade', 'ago', 'go', 'cloud', 'create', 'modern', 'computing', 'environment', 'enable', 'instant', 'provisioning', 'infrastructure', 'increase', 'processing', 'power', 'modern', 'computing', 'environment', 'make', 'complex', 'largescale', 'datum', 'set', 'analysis', 'possible', 'increase', 'level', 'efficiency', 'capital', 'adopt', 'philosophy', 'centralized', 'standardized', 'platform', 'governance', 'ai', 'build', 'platform', 'provide', 'engineer', 'scientist', 'govern', 'access', 'algorithm', 'component', 'infrastructure', 'reuse', 'compute', 'environment', 'platform', 'philosophy', 'provide', 'necessary', 'sufficient', 'ingredient', 'democratize', 'infuse', 'hammer', 'look', 'nail', 'mantra', 'capital', 'team', 'engineer', 'datum', 'scientist', 'go', 'business', 'problemfirst', 'approach', 'instead', 'gather', 'technical', 'requirement', 'team', 'gather', 'problem', 'statement', 'instance', 'capital', 'credit', 'card', 'transaction', 'fraud', 'team', 'look', 'way', 'comprehensively', 'detect', 'pocket', 'fraud', 'automatically', 'create', 'realtime', 'defense', 'company', 'develop', 'algorithm', 'component', 'infrastructure', 'build', 'solution', 'process', 'component', 'publish', 'central', 'ml', 'platform', 'reuse', 'improve', 'future', 'business', 'problem', 'require', 'similar', 'approach', 'organization', 'expand', 'range', 'business', 'use', 'case', 'develop', 'solution', 'often', 'find', 'recur', 'pattern', 'harness', 'wide', 'benefit', 'recognize', 'pattern', 'lead', 'powerful', 'realization', 'make', 'commonly', 'use', 'library', 'workflow', 'component', 'accessible', 'userfriendly', 'interface', 'business', 'unleash', 'potential', 'enterprise', 'require', 'deep', 'datum', 'science', 'engineering', 'expertise', 'democratization', 'serve', 'solution', 'several', 'challenge', 'include', 'cognitive', 'overload', 'resource', 'constraint', 'accessibility', 'issue', 'pave', 'way', 'culture', 'experimentation', 'essential', 'turn', 'ml', 'valuable', 'tool', 'rather', 'pass', 'trend', 'business', 'analyst', 'want', 'identify', 'anomaly', 'track', 'trend', 'portfolio', 'granular', 'segment', 'marketing', 'associate', 'want', 'perform', 'indepth', 'campaign', 'analysis', 'traditional', 'analytic', 'tool', 'offer', 'meet', 'need', 'minimal', 'demand', 'engineering', 'resource', 'use', 'ml', 'democratization', 'transform', 'shiny', 'object', 'centerpiece', 'practical', 'value', 'single', 'working', 'day', 'analyst', 'prior', 'knowledge', 'code', 'skill', 'uncover', 'insightful', 'information', 'dataset', 'choice', 'shift', 'significantly', 'reduce', 'cost', 'associate', 'explore', 'ml', 'potential', 'application', 'various', 'business', 'area', 'solution', 'play', 'pivotal', 'role', 'achieve', 'ml', 'democratization', 'already', 'see', 'happen', 'continue', 'become', 'accessible', 'technology', 'advancement', 'include', 'nocode', 'solution', 'ml', 'democratization', 'allow', 'business', 'analyst', 'confidently', 'make', 'decision', 'previously', 'consider', 'result', 'profound', 'lasting', 'impact', 'content', 'produce', 'capital', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","The enthusiasm for AI and its applications is reaching a nadir, according to an August 2023 Gartner Hype Cycle press release, where generative AI is nearly perched atop the category of technologies at their “Peak of Inflated Expectations,” ready to plunge into the “Trough of Disillusionment.” A quick look at social media agrees, with some…"
The deepfake avatars who want to sell you everything,https://www.technologyreview.com/2023/09/20/1079885/chinese-deepfake-livestream-foreign-language/,2023-09-20,"This story first appeared in China Report, MIT Technology Review’s newsletter about technology developments in China. Sign up to receive it in your inbox every Tuesday. Writing about China, a question I always get is: What technology is ubiquitous there but hasn’t caught on in the West? One of my go-to answers is livestream e-commerce.  If you don’t live in China, it’s hard to comprehend just how massively popular livestream e-commerce is. Over 500 million Chinese people are watching it regularly; these streams brokered $4.6 trillion in sales last year—meaning more than one-quarter of all purchases made online in China were from livestreams. It’s not surprising that tech companies have been trying to bring that lucrative business outside China. I covered this push as early as in 2020, when AliExpress, Alibaba’s overseas arm, was hiring foreign talent to create similar shopping streams in English, Russian, and other languages. But the idea has never been accepted by mainstream viewers elsewhere, even though Chinese companies keep trying. Most recently, TikTok announced last week that it’s officially rolling out its livestream shopping features in the US. (Even this has a sense of déjà vu; it’s something the company has been saying it’s always about to do … since 2021.)  Nevertheless, it’s safe to say that the appetite for livestream shopping still doesn’t exist in the US. Juozas Kaziukėnas, an e-commerce analyst who founded the firm Marketplace Pulse, tweeted that even fervent fans of TikTok aren’t necessarily excited about it, quoting another tweet from last week: “tiktok shop has ruined the whole app, it’s all ads and reviews instead of silly little videos.”  All that said, Chinese livestreamers now have an unprecedented opportunity to break through—with the help of AI. With just a few minutes of sample video and $1,000, brands never have to stop selling their products. Today, I published a story about how cheap and convenient AI tools are enabling brands to create countless deepfake streamers on China’s e-commerce platforms. The developers require only one minute of video for training a “streamer,” and charge about $1,000 for AI-generated avatars that can speak and act (almost) like real humans in front of the cameras. They have already been deployed in thousands of shopping livestreams. Read the full story here. This may help eliminate one of the biggest obstacles to bringing Chinese-style livestream shopping abroad: the lack of foreign talent who understand how livestream e-commerce works, perform naturally in front of the camera, and are willing to talk for hours every day following a largely repetitive routine. In China, there are schools that specifically train aspiring streamers for such work, but the same ecosystem simply doesn’t exist anywhere else yet. Sometimes e-commerce brands take innovative measures to address the talent gap. Chen Dan, CEO of Chinese AI streamer marketing company Quantum Planet, says he saw a Chinese bluetooth headphone brand hire Thai-speaking voice actors to record audio, which then plays over video of just a hand presenting and testing the products, making it seem as if the same person showcasing the product is also speaking Thai. But with large language models and text-to-speech technologies, these AI streamers can say whatever you want them to say—meaning they can speak other languages too. Just last week a new AI-translation product was blowing up on social media: LA-based HeyGen launched a tool that translates video into seven different languages, clones the speaker’s voice, and syncs the speaker’s lips so everything looks natural. The result (including translation to Hindi, the only non-Western language offered now) is surprisingly good! With tools like this, it’s no longer necessary to find local talent for livestreams. “Language is actually an advantage of virtual streamers [compared to humans]. Many of our clients are interested in doing cross-border e-commerce in Southeast Asia. The demand is very high,” says Huang Wei, the director of virtual influencer livestreaming business at the Chinese AI company Xiaoice.  Xiaoice and Quantum Planet now work together to pitch these AI streamers to Chinese clients. Their virtual streamers can speak 129 languages, including English and a few Southeast Asian languages, like Vietnamese, Thai, and Indonesian.  In March, they used a Thai-speaking AI streamer for the first time to sell furniture for a Chinese company, and sold $2,000 worth of products in an hour. I asked a native Thai speaker to watch a clip and assess the quality of the AI; he told me the intonation was so impressively natural that he almost thought it had to be the result of voice dubbing. There’s also an English version so you can judge for yourself, although I don’t think it’s on par with the Chinese or Thai versions.  Obviously the AI won’t be able to do everything a human streamer can, especially testing products in real time in response to audience questions, but it suits the companies that are just looking to break into a new market and not spend too much money for the risky venture. As the Chinese publication Huxiu reports, the monthly salary for a local streamer in Indonesia is almost the same as the cost for customizing an AI streamer, and in the long term, it costs much less to reuse the AI than to keep a real person on the payroll. Plus, the result is better than most people expect.  Could this mean that livestream e-commerce will finally get popular outside China? I’d be very cautious and say that it probably won’t be the case soon. But I do think AI could help Chinese companies expand globally by overcoming language and cultural barriers. And either way, it’s clear the technology of synthetic media is moving ahead at an incredibly fast pace, so it may only be a matter of time before Chinese e-commerce companies can finally capitalize on it.  What do you think of the English version of the AI streamer? Do you think livestream e-commerce will become popular outside China? Let me know your thoughts at zeyi@technologyreview.com. 1. The Biden administration and TikTok have come back to the negotiating table, after a hard-line stance against the viral social media company has proved unpopular among many Americans. (Washington Post $) 2. Tinder might be disappointing as a dating platform, but in China, young people are using it to find professional connections and job referrals. (Sixth Tone) 3. China's AI sector is overcrowded with newcomers who want to make it big, but the hype is gradually fading away. (Wired $) 4. As the demand for data annotation rises this year, vocational schools in China are working with annotation companies to force students to do the labor-intensive work for subminimum wages. (Rest of World) 5. The US and China are both racing to develop AI tools for espionage—hoping to gain more information on the mindset of their rival’s leaders and military capabilities. (New York Times $) 6. From his days as a student dissident who escaped Tiananmen in 1989 to his career as one of the best investors betting on Chinese companies, Li Lu’s life story is deeply intertwined with the wax and wane of US-China relations over the past three decades. (Financial Times $) 7. Barbie had a team of clearance specialists review every prop for copyright and other concerns. It never expected a cartoonish map to create controversy over China’s maritime territory claims. (Wall Street Journal $) In the past month, a major Chinese mobile game developer tried twice to set up alternative micropayment systems that circumvent Apple’s high commission fees, but it seems to have failed both times. miHoYo is the developer behind the global hit Genshin Impact, which incentivizes players to make micropayments in exchange for characters and items and has become one of the highest-grossing mobile games in the world. That means its players have paid billions of dollars through Apple’s App Store—where the US tech giant takes a 30% cut for every transaction. According to the Chinese gaming publication Core Esports, in August miHoYo tried to set up new payment channels through its community forum app and then a mini-program in the Chinese digital wallet app Alipay. The first app was soon removed from Apple’s App Store, while the second was disabled for iPhone users after just 13 days. The failed attempts to reduce revenue sharing with Apple highlight the latter’s continued influence in the mobile gaming ecosystem. But Apple is also facing regulatory pressure to open up to third-party payment methods, and major game developers have their own leverage. Maybe the third time will be the charm for miHoYo? If you live in the Chinese province of Guangdong, I’d advise you to stay indoors. After heavy rainfall and flooding across southern China, 71 crocodiles escaped from a farm where they were raised for their meat and leather. The good news is that 69 of them have been caught. The bad news is … well, there are still two at large. ","This story first appeared in China Report , MIT Technology Review ’ s newsletter about technology developments in China . Sign up to receive it in your inbox every Tuesday . Writing about China , a question I always get is : What technology is ubiquitous there but hasn ’ t caught on in the West ? One of my go-to answers is livestream e-commerce . If you don ’ t live in China , it ’ s hard to comprehend just how massively popular livestream e-commerce is . Over 500 million Chinese people are watching it regularly ; these streams brokered $ 4.6 trillion in sales last year—meaning more than one-quarter of all purchases made online in China were from livestreams . It ’ s not surprising that tech companies have been trying to bring that lucrative business outside China . I covered this push as early as in 2020 , when AliExpress , Alibaba ’ s overseas arm , was hiring foreign talent to create similar shopping streams in English , Russian , and other languages . But the idea has never been accepted by mainstream viewers elsewhere , even though Chinese companies keep trying . Most recently , TikTok announced last week that it ’ s officially rolling out its livestream shopping features in the US . ( Even this has a sense of déjà vu ; it ’ s something the company has been saying it ’ s always about to do … since 2021 . ) Nevertheless , it ’ s safe to say that the appetite for livestream shopping still doesn ’ t exist in the US . Juozas Kaziukėnas , an e-commerce analyst who founded the firm Marketplace Pulse , tweeted that even fervent fans of TikTok aren ’ t necessarily excited about it , quoting another tweet from last week : “ tiktok shop has ruined the whole app , it ’ s all ads and reviews instead of silly little videos. ” All that said , Chinese livestreamers now have an unprecedented opportunity to break through—with the help of AI . With just a few minutes of sample video and $ 1,000 , brands never have to stop selling their products . Today , I published a story about how cheap and convenient AI tools are enabling brands to create countless deepfake streamers on China ’ s e-commerce platforms . The developers require only one minute of video for training a “ streamer , ” and charge about $ 1,000 for AI-generated avatars that can speak and act ( almost ) like real humans in front of the cameras . They have already been deployed in thousands of shopping livestreams . Read the full story here . This may help eliminate one of the biggest obstacles to bringing Chinese-style livestream shopping abroad : the lack of foreign talent who understand how livestream e-commerce works , perform naturally in front of the camera , and are willing to talk for hours every day following a largely repetitive routine . In China , there are schools that specifically train aspiring streamers for such work , but the same ecosystem simply doesn ’ t exist anywhere else yet . Sometimes e-commerce brands take innovative measures to address the talent gap . Chen Dan , CEO of Chinese AI streamer marketing company Quantum Planet , says he saw a Chinese bluetooth headphone brand hire Thai-speaking voice actors to record audio , which then plays over video of just a hand presenting and testing the products , making it seem as if the same person showcasing the product is also speaking Thai . But with large language models and text-to-speech technologies , these AI streamers can say whatever you want them to say—meaning they can speak other languages too . Just last week a new AI-translation product was blowing up on social media : LA-based HeyGen launched a tool that translates video into seven different languages , clones the speaker ’ s voice , and syncs the speaker ’ s lips so everything looks natural . The result ( including translation to Hindi , the only non-Western language offered now ) is surprisingly good ! With tools like this , it ’ s no longer necessary to find local talent for livestreams . “ Language is actually an advantage of virtual streamers [ compared to humans ] . Many of our clients are interested in doing cross-border e-commerce in Southeast Asia . The demand is very high , ” says Huang Wei , the director of virtual influencer livestreaming business at the Chinese AI company Xiaoice . Xiaoice and Quantum Planet now work together to pitch these AI streamers to Chinese clients . Their virtual streamers can speak 129 languages , including English and a few Southeast Asian languages , like Vietnamese , Thai , and Indonesian . In March , they used a Thai-speaking AI streamer for the first time to sell furniture for a Chinese company , and sold $ 2,000 worth of products in an hour . I asked a native Thai speaker to watch a clip and assess the quality of the AI ; he told me the intonation was so impressively natural that he almost thought it had to be the result of voice dubbing . There ’ s also an English version so you can judge for yourself , although I don ’ t think it ’ s on par with the Chinese or Thai versions . Obviously the AI won ’ t be able to do everything a human streamer can , especially testing products in real time in response to audience questions , but it suits the companies that are just looking to break into a new market and not spend too much money for the risky venture . As the Chinese publication Huxiu reports , the monthly salary for a local streamer in Indonesia is almost the same as the cost for customizing an AI streamer , and in the long term , it costs much less to reuse the AI than to keep a real person on the payroll . Plus , the result is better than most people expect . Could this mean that livestream e-commerce will finally get popular outside China ? I ’ d be very cautious and say that it probably won ’ t be the case soon . But I do think AI could help Chinese companies expand globally by overcoming language and cultural barriers . And either way , it ’ s clear the technology of synthetic media is moving ahead at an incredibly fast pace , so it may only be a matter of time before Chinese e-commerce companies can finally capitalize on it . What do you think of the English version of the AI streamer ? Do you think livestream e-commerce will become popular outside China ? Let me know your thoughts at zeyi @ technologyreview.com . 1 . The Biden administration and TikTok have come back to the negotiating table , after a hard-line stance against the viral social media company has proved unpopular among many Americans . ( Washington Post $ ) 2 . Tinder might be disappointing as a dating platform , but in China , young people are using it to find professional connections and job referrals . ( Sixth Tone ) 3 . China 's AI sector is overcrowded with newcomers who want to make it big , but the hype is gradually fading away . ( Wired $ ) 4 . As the demand for data annotation rises this year , vocational schools in China are working with annotation companies to force students to do the labor-intensive work for subminimum wages . ( Rest of World ) 5 . The US and China are both racing to develop AI tools for espionage—hoping to gain more information on the mindset of their rival ’ s leaders and military capabilities . ( New York Times $ ) 6 . From his days as a student dissident who escaped Tiananmen in 1989 to his career as one of the best investors betting on Chinese companies , Li Lu ’ s life story is deeply intertwined with the wax and wane of US-China relations over the past three decades . ( Financial Times $ ) 7 . Barbie had a team of clearance specialists review every prop for copyright and other concerns . It never expected a cartoonish map to create controversy over China ’ s maritime territory claims . ( Wall Street Journal $ ) In the past month , a major Chinese mobile game developer tried twice to set up alternative micropayment systems that circumvent Apple ’ s high commission fees , but it seems to have failed both times . miHoYo is the developer behind the global hit Genshin Impact , which incentivizes players to make micropayments in exchange for characters and items and has become one of the highest-grossing mobile games in the world . That means its players have paid billions of dollars through Apple ’ s App Store—where the US tech giant takes a 30 % cut for every transaction . According to the Chinese gaming publication Core Esports , in August miHoYo tried to set up new payment channels through its community forum app and then a mini-program in the Chinese digital wallet app Alipay . The first app was soon removed from Apple ’ s App Store , while the second was disabled for iPhone users after just 13 days . The failed attempts to reduce revenue sharing with Apple highlight the latter ’ s continued influence in the mobile gaming ecosystem . But Apple is also facing regulatory pressure to open up to third-party payment methods , and major game developers have their own leverage . Maybe the third time will be the charm for miHoYo ? If you live in the Chinese province of Guangdong , I ’ d advise you to stay indoors . After heavy rainfall and flooding across southern China , 71 crocodiles escaped from a farm where they were raised for their meat and leather . The good news is that 69 of them have been caught . The bad news is … well , there are still two at large .","['story', 'first', 'appear', 'mit', 'technology', 'review', 'newsletter', 'technology', 'development', 'sign', 'receive', 'inbox', 'writing', 'question', 'always', 'get', 'technology', 'ubiquitous', 'catch', 'west', 'goto', 'answer', 'livestream', 'ecommerce', 'live', 'hard', 'comprehend', 'massively', 'popular', 'livestream', 'ecommerce', 'chinese', 'people', 'watch', 'regularly', 'stream', 'broker', 'sale', 'last', 'year', 'mean', 'onequarter', 'purchase', 'make', 'online', 'livestream', 'surprising', 'tech', 'company', 'try', 'bring', 'lucrative', 'business', 'cover', 'push', 'early', 'aliexpress', 'overseas', 'arm', 'hire', 'foreign', 'talent', 'create', 'similar', 'shopping', 'stream', 'russian', 'language', 'idea', 'never', 'accept', 'mainstream', 'viewer', 'elsewhere', 'even', 'chinese', 'company', 'keep', 'try', 'recently', 'tiktok', 'announce', 'last', 'week', 'officially', 'roll', 'livestream', 'shopping', 'feature', 'even', 'sense', 'déjà', 'vu', 'company', 'say', 'always', 'nevertheless', 'safe', 'say', 'appetite', 'livestream', 'shopping', 'still', 'exist', 'ecommerce', 'analyst', 'found', 'firm', 'marketplace', 'pulse', 'tweet', 'even', 'fervent', 'fan', 'tiktok', 'necessarily', 'excited', 'quote', 'tweet', 'last', 'week', 'tiktok', 'shop', 'ruin', 'whole', 'app', 'ad', 'review', 'instead', 'silly', 'little', 'video', 'say', 'chinese', 'livestreamer', 'unprecedented', 'opportunity', 'break', 'help', 'ai', 'minute', 'sample', 'video', 'brand', 'never', 'stop', 'sell', 'product', 'today', 'publish', 'story', 'cheap', 'convenient', 'tool', 'enable', 'brand', 'create', 'countless', 'deepfake', 'streamer', 'ecommerce', 'platform', 'developer', 'require', 'minute', 'video', 'train', 'streamer', 'charge', 'aigenerate', 'avatar', 'speak', 'act', 'almost', 'real', 'human', 'front', 'camera', 'already', 'deploy', 'thousand', 'shopping', 'livestream', 'read', 'full', 'story', 'help', 'eliminate', 'big', 'obstacle', 'bring', 'chinesestyle', 'livestream', 'shopping', 'abroad', 'lack', 'foreign', 'talent', 'understand', 'livestream', 'ecommerce', 'work', 'perform', 'naturally', 'front', 'camera', 'willing', 'talk', 'hour', 'day', 'follow', 'largely', 'repetitive', 'routine', 'school', 'specifically', 'train', 'aspire', 'streamer', 'work', 'ecosystem', 'simply', 'exist', 'anywhere', 'else', 'yet', 'sometimes', 'ecommerce', 'brand', 'take', 'innovative', 'measure', 'address', 'talent', 'gap', 'ceo', 'ai', 'streamer', 'marketing', 'company', 'quantum', 'planet', 'say', 'see', 'chinese', 'hire', 'thaispeake', 'voice', 'actor', 'record', 'audio', 'play', 'video', 'hand', 'presenting', 'test', 'product', 'make', 'seem', 'person', 'showcase', 'product', 'also', 'speak', 'large', 'language', 'model', 'texttospeech', 'technology', 'ai', 'streamer', 'say', 'want', 'say', 'mean', 'speak', 'language', 'last', 'week', 'new', 'aitranslation', 'product', 'blow', 'social', 'medium', 'labase', 'heygen', 'launch', 'tool', 'translate', 'video', 'different', 'language', 'clone', 'speaker', 'voice', 'sync', 'speaker', 'lip', 'look', 'natural', 'result', 'include', 'translation', 'hindi', 'nonwestern', 'language', 'offer', 'surprisingly', 'good', 'tool', 'long', 'necessary', 'find', 'local', 'talent', 'livestream', 'language', 'actually', 'advantage', 'virtual', 'streamer', 'compare', 'human', 'many', 'client', 'interested', 'crossborder', 'ecommerce', 'demand', 'high', 'say', 'director', 'virtual', 'influencer', 'livestreaming', 'business', 'ai', 'company', 'xiaoice', 'xiaoice', 'quantum', 'planet', 'work', 'together', 'pitch', 'ai', 'streamer', 'chinese', 'client', 'virtual', 'streamer', 'speak', 'language', 'include', 'southeast', 'asian', 'language', 'vietnamese', 'use', 'thaispeaking', 'ai', 'streamer', 'first', 'time', 'sell', 'furniture', 'chinese', 'company', 'sell', 'worth', 'product', 'hour', 'ask', 'native', 'thai', 'speaker', 'watch', 'clip', 'assess', 'quality', 'ai', 'tell', 'intonation', 'impressively', 'natural', 'almost', 'think', 'result', 'voice', 'dubbing', 'also', 'english', 'version', 'judge', 'think', 'par', 'chinese', 'thai', 'version', 'obviously', 'win', 'able', 'human', 'streamer', 'especially', 'test', 'product', 'real', 'time', 'response', 'audience', 'question', 'suit', 'company', 'look', 'break', 'new', 'market', 'spend', 'much', 'money', 'risky', 'venture', 'chinese', 'publication', 'report', 'monthly', 'salary', 'local', 'streamer', 'almost', 'cost', 'customize', 'ai', 'streamer', 'long', 'term', 'cost', 'much', 'less', 'reuse', 'ai', 'keep', 'real', 'person', 'payroll', 'result', 'well', 'people', 'expect', 'mean', 'livestream', 'ecommerce', 'finally', 'get', 'popular', 'cautious', 'say', 'probably', 'win', 'case', 'soon', 'think', 'ai', 'help', 'chinese', 'company', 'expand', 'globally', 'overcome', 'language', 'cultural', 'barrier', 'way', 'clear', 'technology', 'synthetic', 'medium', 'move', 'ahead', 'incredibly', 'fast', 'pace', 'matter', 'time', 'chinese', 'ecommerce', 'company', 'finally', 'capitalize', 'think', 'english', 'version', 'streamer', 'think', 'livestream', 'ecommerce', 'become', 'popular', 'let', 'know', 'thought', 'administration', 'tiktok', 'come', 'back', 'negotiating', 'table', 'hardline', 'stance', 'viral', 'social', 'medium', 'company', 'prove', 'unpopular', 'many', 'tinder', 'disappointing', 'date', 'platform', 'young', 'people', 'use', 'find', 'professional', 'connection', 'job', 'referral', 'sixth', 'tone', 'sector', 'overcrowded', 'newcomer', 'want', 'make', 'big', 'hype', 'gradually', 'fade', 'away', 'wire', 'demand', 'datum', 'annotation', 'rise', 'year', 'vocational', 'school', 'work', 'annotation', 'company', 'force', 'student', 'laborintensive', 'work', 'subminimum', 'wage', 'rest', 'world', 'race', 'develop', 'tool', 'espionage', 'hope', 'gain', 'information', 'mindset', 'rival', 'leader', 'military', 'capability', 'day', 'student', 'dissident', 'escape', 'tiananman', 'career', 'good', 'investor', 'bet', 'chinese', 'company', 'life', 'story', 'deeply', 'intertwine', 'wax', 'wane', 'relation', 'past', 'decade', 'financial', 'time', 'barbie', 'team', 'clearance', 'specialist', 'review', 'prop', 'copyright', 'concern', 'never', 'expect', 'cartoonish', 'map', 'create', 'controversy', 'maritime', 'territory', 'claim', 'past', 'month', 'major', 'chinese', 'mobile', 'game', 'developer', 'try', 'twice', 'set', 'alternative', 'micropayment', 'system', 'circumvent', 'commission', 'fee', 'seem', 'fail', 'time', 'mihoyo', 'developer', 'global', 'hit', 'genshin', 'impact', 'incentivize', 'player', 'make', 'micropayment', 'exchange', 'character', 'item', 'become', 'highestgrossing', 'mobile', 'game', 'world', 'mean', 'player', 'pay', 'billion', 'dollar', 'apple', 'app', 'store', 'tech', 'giant', 'take', 'cut', 'transaction', 'accord', 'gaming', 'publication', 'core', 'esport', 'mihoyo', 'try', 'set', 'new', 'payment', 'channel', 'community', 'forum', 'app', 'miniprogram', 'first', 'app', 'soon', 'remove', 'app', 'store', 'second', 'disabled', 'iphone', 'user', 'day', 'fail', 'attempt', 'reduce', 'revenue', 'sharing', 'apple', 'highlight', 'latter', 'continue', 'influence', 'mobile', 'gaming', 'ecosystem', 'apple', 'also', 'face', 'regulatory', 'pressure', 'open', 'thirdparty', 'payment', 'method', 'major', 'game', 'developer', 'leverage', 'maybe', 'third', 'time', 'charm', 'mihoyo', 'live', 'chinese', 'advise', 'stay', 'indoor', 'heavy', 'rainfall', 'flooding', 'southern', 'crocodile', 'escape', 'farm', 'raise', 'meat', 'leather', 'good', 'news', 'catch', 'bad', 'news', 'still', 'large']","<p>AI-generated livestreamers can speak any language, making it easier for Chinese brands to capture an audience abroad.</p>
"
A Disney director tried—and failed—to use an AI Hans Zimmer to create a soundtrack,https://www.technologyreview.com/2023/09/19/1079859/a-disney-director-tried-and-failed-to-use-an-ai-hans-zimmer-to-create-a-soundtrack/,2023-09-19,"Hans Zimmer, 1; AI, 0.  When Gareth Edwards, the director of Rogue One: A Star Wars Story, was thinking about the soundtrack for his upcoming movie about artificial intelligence, The Creator, he decided to try composing it with AI—and got “pretty damn good” results.  “The cheeky part of me thought it’d be even better if we didn’t tell anyone—and we did the soundtrack and we kept it secret, like we invented a person’s name or something, and then when it was all done … ‘Haha, it was actually AI,’” Edwards said in a LinkedIn Live interview with MIT Technology Review. (You can watch the full interview below.) Edwards had asked an unspecified AI music company to use the tech to create a soundtrack in the style of Oscar-winning composer Hans Zimmer.  The AI system generated a track that was maybe a “7 out of 10,” Edwards said.  “But in the back of my head I was like, ‘But the reason you go to Hans Zimmer is for 10 out of 10,’” he added.  Edwards, who ended up using the real, flesh-and-blood human Hans Zimmer for the soundtrack of his movie, said he played the AI-generated track back to the composer. Zimmer, he said, found it amusing. Zimmer wasn’t reachable for comment.  Edwards’s experiment speaks to an issue at the heart of one of the biggest fights facing Hollywood today. Artists and creatives are up in arms over generative AI. Hollywood is currently at a standstill as actors and writers are striking over fairer labor conditions and the use of generative AI in the film industry. There is also fierce pushback from authors and artists who argue that tech companies steal their intellectual property by indiscriminately scraping the web for images and text. Prominent artists such as comedian and author Sarah Silverman have sued AI companies for copyright infringement.  It’s still early days for music-generating AI, which might explain why Edwards got the results he did, says Henry Ajder, an expert in generative AI.  “From my experience, some quite simple AI music is pretty convincing. It’s difficult to tell the difference between an AI-generated composition and a human performed composition,” he says.   But a longer piece in the style of Hans Zimmer is significantly more complex to generate than a simple piano melody with one instrument, he adds. AI systems are limited by what is in their training data, whereas human Zimmer has his imagination and the whole surrounding world to draw inspiration from.  Crucially, Edwards said, AI systems lack a fundamentally crucial skill for creating good art: taste. They still don’t understand what humans deem good or bad. For that reason, he believes that rather than fearing AI, creatives should use it. “Everyone’s very aware it’s coming. It’s a tool,” Edwards said. “The people that are going to be okay are the people who don’t deny this breakthrough is happening, and embrace it and learn it, and try to use it as a tool.”  Greg Rutkowski is a more popular prompt than Picasso. Edwards drew parallels between today’s AI boom and the invention of the photo editing software Photoshop.  When Photoshop came out, he said, the public discussion was about how the software “was sacrilegious.”  “We got over that eventually. Now Photoshop has created so many opportunities for so many people doing art … I wouldn’t want to go back,” he said.  Artificial intelligence will seismically shift the industry in the same way the invention of the camera or the digital visual effects in Jurassic Park did, Edwards said: “It’s just another one of those, I hope.”  ","Hans Zimmer , 1 ; AI , 0 . When Gareth Edwards , the director of Rogue One : A Star Wars Story , was thinking about the soundtrack for his upcoming movie about artificial intelligence , The Creator , he decided to try composing it with AI—and got “ pretty damn good ” results . “ The cheeky part of me thought it ’ d be even better if we didn ’ t tell anyone—and we did the soundtrack and we kept it secret , like we invented a person ’ s name or something , and then when it was all done … ‘ Haha , it was actually AI , ’ ” Edwards said in a LinkedIn Live interview with MIT Technology Review . ( You can watch the full interview below . ) Edwards had asked an unspecified AI music company to use the tech to create a soundtrack in the style of Oscar-winning composer Hans Zimmer . The AI system generated a track that was maybe a “ 7 out of 10 , ” Edwards said . “ But in the back of my head I was like , ‘ But the reason you go to Hans Zimmer is for 10 out of 10 , ’ ” he added . Edwards , who ended up using the real , flesh-and-blood human Hans Zimmer for the soundtrack of his movie , said he played the AI-generated track back to the composer . Zimmer , he said , found it amusing . Zimmer wasn ’ t reachable for comment . Edwards ’ s experiment speaks to an issue at the heart of one of the biggest fights facing Hollywood today . Artists and creatives are up in arms over generative AI . Hollywood is currently at a standstill as actors and writers are striking over fairer labor conditions and the use of generative AI in the film industry . There is also fierce pushback from authors and artists who argue that tech companies steal their intellectual property by indiscriminately scraping the web for images and text . Prominent artists such as comedian and author Sarah Silverman have sued AI companies for copyright infringement . It ’ s still early days for music-generating AI , which might explain why Edwards got the results he did , says Henry Ajder , an expert in generative AI . “ From my experience , some quite simple AI music is pretty convincing . It ’ s difficult to tell the difference between an AI-generated composition and a human performed composition , ” he says . But a longer piece in the style of Hans Zimmer is significantly more complex to generate than a simple piano melody with one instrument , he adds . AI systems are limited by what is in their training data , whereas human Zimmer has his imagination and the whole surrounding world to draw inspiration from . Crucially , Edwards said , AI systems lack a fundamentally crucial skill for creating good art : taste . They still don ’ t understand what humans deem good or bad . For that reason , he believes that rather than fearing AI , creatives should use it . “ Everyone ’ s very aware it ’ s coming . It ’ s a tool , ” Edwards said . “ The people that are going to be okay are the people who don ’ t deny this breakthrough is happening , and embrace it and learn it , and try to use it as a tool. ” Greg Rutkowski is a more popular prompt than Picasso . Edwards drew parallels between today ’ s AI boom and the invention of the photo editing software Photoshop . When Photoshop came out , he said , the public discussion was about how the software “ was sacrilegious. ” “ We got over that eventually . Now Photoshop has created so many opportunities for so many people doing art … I wouldn ’ t want to go back , ” he said . Artificial intelligence will seismically shift the industry in the same way the invention of the camera or the digital visual effects in Jurassic Park did , Edwards said : “ It ’ s just another one of those , I hope . ”","['zimmer', 'ai', 'edward', 'director', 'rogue', 'star', 'war', 'story', 'think', 'soundtrack', 'upcoming', 'movie', 'artificial', 'intelligence', 'creator', 'decide', 'try', 'compose', 'ai', 'get', 'pretty', 'damn', 'good', 'result', 'cheeky', 'part', 'think', 'even', 'well', 'tell', 'soundtrack', 'keep', 'secret', 'invent', 'person', 'name', 'actually', 'ai', 'say', 'linkedin', 'live', 'interview', 'mit', 'technology', 'review', 'watch', 'full', 'interview', 'edward', 'ask', 'unspecified', 'ai', 'music', 'company', 'use', 'tech', 'create', 'soundtrack', 'style', 'oscarwinne', 'composer', 'han', 'zimmer', 'system', 'generate', 'track', 'maybe', 'edward', 'say', 'back', 'head', 'reason', 'go', 'zimmer', 'add', 'end', 'use', 'real', 'fleshandblood', 'human', 'han', 'zimmer', 'soundtrack', 'movie', 'say', 'play', 'aigenerated', 'track', 'back', 'composer', 'zimmer', 'say', 'find', 'amusing', 'zimmer', 'reachable', 'comment', 'experiment', 'speak', 'issue', 'heart', 'big', 'fight', 'face', 'hollywood', 'today', 'artist', 'creative', 'arm', 'generative', 'ai', 'hollywood', 'currently', 'standstill', 'actor', 'writer', 'strike', 'fair', 'labor', 'condition', 'use', 'generative', 'ai', 'film', 'industry', 'also', 'fierce', 'pushback', 'author', 'artist', 'argue', 'tech', 'company', 'steal', 'intellectual', 'property', 'indiscriminately', 'scrape', 'web', 'image', 'text', 'prominent', 'artist', 'comedian', 'author', 'sue', 'ai', 'company', 'copyright', 'infringement', 'still', 'early', 'day', 'musicgenerate', 'ai', 'explain', 'edward', 'get', 'result', 'say', 'expert', 'generative', 'ai', 'experience', 'quite', 'simple', 'music', 'pretty', 'convincing', 'difficult', 'tell', 'difference', 'aigenerated', 'composition', 'human', 'perform', 'composition', 'say', 'long', 'piece', 'style', 'zimmer', 'significantly', 'complex', 'generate', 'simple', 'piano', 'melody', 'instrument', 'add', 'system', 'limit', 'training', 'datum', 'human', 'zimmer', 'imagination', 'whole', 'surround', 'world', 'draw', 'inspiration', 'crucially', 'say', 'ai', 'system', 'lack', 'fundamentally', 'crucial', 'skill', 'create', 'good', 'art', 'taste', 'still', 'understand', 'human', 'deem', 'good', 'bad', 'reason', 'believe', 'rather', 'fear', 'creative', 'use', 'aware', 'come', 'tool', 'say', 'people', 'go', 'okay', 'people', 'deny', 'breakthrough', 'happen', 'embrace', 'learn', 'try', 'use', 'tool', 'popular', 'prompt', 'picasso', 'draw', 'parallel', 'today', 'ai', 'boom', 'invention', 'photo', 'editing', 'software', 'photoshop', 'come', 'say', 'public', 'discussion', 'software', 'sacrilegious', 'get', 'eventually', 'create', 'many', 'opportunity', 'many', 'people', 'art', 'want', 'go', 'back', 'say', 'artificial', 'intelligence', 'seismically', 'shift', 'industry', 'way', 'invention', 'camera', 'digital', 'visual', 'effect', 'park', 'say', 'hope']","<p>AI generated a “7 out of 10” track. “But the reason you go into Hans Zimmer is for 10 out of 10,” says Gareth Edwards. </p>
"
Meet the next generation of AI superstars,https://www.technologyreview.com/2023/09/19/1079846/meet-the-next-generation-of-ai-superstars/,2023-09-19,"This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here. So smart! So talented! This week I’m pleased to introduce you to a new crop of bright minds working on some of the most challenging problems in AI and beyond. You can read MIT Technology Review’s full list of 35 Innovators Under 35 here.  We’ve previously highlighted some of the most promising people in tech before they became household names. In 2002, the list included two young innovators named Larry Page and Sergey Brin of Google. A 23-year-old Mark Zuckerberg was on the list in 2007. In 2008 we featured Andrew Ng, who wrote an excellent essay for us this year sharing his tips for aspiring innovators on trying, failing, and the future of AI.  This year we’ve seen tech companies racing to release their hottest new AI systems, and often neglecting safety and ethics. The AI scientists on this year's innovators list are more aware than ever of the harm the technology can pose, and are determined to fix it. To do that, they’re pioneering new methods that are helping to shift the way the AI industry thinks about safety.  Sharon Li, pictured above and our Innovator of the Year, is an assistant professor at the University of Wisconsin, Madison. She created a remarkable AI safety feature called out-of-distribution detection. This feature helps AI models determine if they should abstain from action when faced with something they weren’t trained on. This is crucial as AI systems are rolled out from the lab and encounter new situations in the messy real world. Irene Solaiman, global public policy director at Hugging Face, developed an approach that calls for tech companies to release new models in phases, allowing more time to test them for failures and build in guardrails.   Many of our innovators are working to fight climate change. I was delighted to see so many people on the list using their skills in AI to tackle the biggest problem facing humanity, either by helping the AI community track and lower its emissions or by using AI to mitigate emissions in highly polluting industries. Sasha Luccioni, an AI researcher at startup Hugging Face, has developed a better way for tech companies to estimate and measure the carbon footprint of AI language models.  Catherine De Wolf of ETH Zurich is using AI to help reduce emissions and the waste of materials in the construction industry.  Alhussein Fawzi of DeepMind developed game-playing AI to speed up fundamental computations, which helps to cut costs and save energy on devices.  This year’s innovators are also working on practical applications of AI that illustrate how the technology could become more and more useful. They’re coming up with exciting new ways to use it to boost scientific research and build helpful tools in other fields. Lerrel Pinto of New York University is using AI to help robots learn from their mistakes. This, he hopes, will lead to robots in the home that do a lot more than vacuum—and could become more integral to our daily lives.  Connor Coley of MIT developed open-source software that uses artificial intelligence to help discover and synthesize new molecules.  Pranav Rajpurkar of Harvard Medical School has developed a way for AI to teach itself to accurately interpret medical images without any help from humans.  Richard Zhang, a senior research scientist at Adobe, invented the visual similarity algorithms underlying image-generating AI models like Stable Diffusion and StyleGAN. Without his work, we would not have the image-generating AI that has captivated the world.  That’s not all! This year’s list is brimming with inspiring people and ideas for the next big thing in robotics, computing, biotechnology, and climate and energy. Read the full list of this year’s young innovators here. And finally, if you work in AI and you think you’ve got some exciting, cutting-edge stuff to share, get in touch! We’re always interested in hearing from people doing interesting work. DeepMind’s cofounder: Generative AI is just a phase. What’s next is interactive AI. DeepMind cofounder Mustafa Suleyman wants to build a chatbot that does a whole lot more than chat. Here’s Suleyman’s pitch: In the future, we’ll have what he calls interactive AI, meaning bots that can carry out tasks you set for them by calling on other software and other people to get stuff done. He’s founded a new billion-dollar company, Inflection, to build it.  Come again? Suleyman, who left DeepMind in 2022, has some … interesting … thoughts about the success of online regulation, which border on naïveté. (“It’s pretty difficult to find radicalization content or terrorist material online. It’s pretty difficult to buy weapons and drugs online.”) Despite that, he remains earnest and evangelical in his convictions, and he is in a position to make big moves in the industry. He sat down with MIT Technology Review’s senior editor for AI, Will Douglas Heaven, to chat about his plans and the need for robust AI regulation. Read more here. AI just beat a human test for creativity. What does that even mean?A new study found that AI chatbots achieved higher average scores than humans in a test commonly used to assess human creativity. The findings do not necessarily indicate that AIs are developing an ability to do something uniquely human. However, they might give us a better understanding of how humans and machines approach creative tasks. (MIT Technology Review)  This driverless-car company is using chatbots to make its vehicles smarterSelf-driving-car startup Wayve can now interrogate its vehicles, asking them questions about their driving decisions—and getting answers back. The idea is to use the same tech behind ChatGPT to help train driverless cars. (MIT Technology Review)  How Silicon Valley doomers are shaping Rishi Sunak’s AI plansThe UK’s prime minister, Rishi Sunak, is keen to boost his country’s AI industry. But in a short span of time, something has shifted in the UK’s approach. The country seems to be becoming a cheerleader for the AI doom narrative, thanks to heavy lobbying from the effective altruism movement. (Politico)  Silicon Valley’s AI religionA thought-provoking piece about something I too have observed in the tech space: technologists are increasingly weaving a narrative around AI and artificial general intelligence that isn’t that dissimilar from religious narratives. This story connects the dots. (Vox)  How generative AI worksA great and helpful visual explainer that’s essential reading for anyone AI-curious. (The Financial Times)  ","This story originally appeared in The Algorithm , our weekly newsletter on AI . To get stories like this in your inbox first , sign up here . So smart ! So talented ! This week I ’ m pleased to introduce you to a new crop of bright minds working on some of the most challenging problems in AI and beyond . You can read MIT Technology Review ’ s full list of 35 Innovators Under 35 here . We ’ ve previously highlighted some of the most promising people in tech before they became household names . In 2002 , the list included two young innovators named Larry Page and Sergey Brin of Google . A 23-year-old Mark Zuckerberg was on the list in 2007 . In 2008 we featured Andrew Ng , who wrote an excellent essay for us this year sharing his tips for aspiring innovators on trying , failing , and the future of AI . This year we ’ ve seen tech companies racing to release their hottest new AI systems , and often neglecting safety and ethics . The AI scientists on this year 's innovators list are more aware than ever of the harm the technology can pose , and are determined to fix it . To do that , they ’ re pioneering new methods that are helping to shift the way the AI industry thinks about safety . Sharon Li , pictured above and our Innovator of the Year , is an assistant professor at the University of Wisconsin , Madison . She created a remarkable AI safety feature called out-of-distribution detection . This feature helps AI models determine if they should abstain from action when faced with something they weren ’ t trained on . This is crucial as AI systems are rolled out from the lab and encounter new situations in the messy real world . Irene Solaiman , global public policy director at Hugging Face , developed an approach that calls for tech companies to release new models in phases , allowing more time to test them for failures and build in guardrails . Many of our innovators are working to fight climate change . I was delighted to see so many people on the list using their skills in AI to tackle the biggest problem facing humanity , either by helping the AI community track and lower its emissions or by using AI to mitigate emissions in highly polluting industries . Sasha Luccioni , an AI researcher at startup Hugging Face , has developed a better way for tech companies to estimate and measure the carbon footprint of AI language models . Catherine De Wolf of ETH Zurich is using AI to help reduce emissions and the waste of materials in the construction industry . Alhussein Fawzi of DeepMind developed game-playing AI to speed up fundamental computations , which helps to cut costs and save energy on devices . This year ’ s innovators are also working on practical applications of AI that illustrate how the technology could become more and more useful . They ’ re coming up with exciting new ways to use it to boost scientific research and build helpful tools in other fields . Lerrel Pinto of New York University is using AI to help robots learn from their mistakes . This , he hopes , will lead to robots in the home that do a lot more than vacuum—and could become more integral to our daily lives . Connor Coley of MIT developed open-source software that uses artificial intelligence to help discover and synthesize new molecules . Pranav Rajpurkar of Harvard Medical School has developed a way for AI to teach itself to accurately interpret medical images without any help from humans . Richard Zhang , a senior research scientist at Adobe , invented the visual similarity algorithms underlying image-generating AI models like Stable Diffusion and StyleGAN . Without his work , we would not have the image-generating AI that has captivated the world . That ’ s not all ! This year ’ s list is brimming with inspiring people and ideas for the next big thing in robotics , computing , biotechnology , and climate and energy . Read the full list of this year ’ s young innovators here . And finally , if you work in AI and you think you ’ ve got some exciting , cutting-edge stuff to share , get in touch ! We ’ re always interested in hearing from people doing interesting work . DeepMind ’ s cofounder : Generative AI is just a phase . What ’ s next is interactive AI . DeepMind cofounder Mustafa Suleyman wants to build a chatbot that does a whole lot more than chat . Here ’ s Suleyman ’ s pitch : In the future , we ’ ll have what he calls interactive AI , meaning bots that can carry out tasks you set for them by calling on other software and other people to get stuff done . He ’ s founded a new billion-dollar company , Inflection , to build it . Come again ? Suleyman , who left DeepMind in 2022 , has some … interesting … thoughts about the success of online regulation , which border on naïveté . ( “ It ’ s pretty difficult to find radicalization content or terrorist material online . It ’ s pretty difficult to buy weapons and drugs online. ” ) Despite that , he remains earnest and evangelical in his convictions , and he is in a position to make big moves in the industry . He sat down with MIT Technology Review ’ s senior editor for AI , Will Douglas Heaven , to chat about his plans and the need for robust AI regulation . Read more here . AI just beat a human test for creativity . What does that even mean ? A new study found that AI chatbots achieved higher average scores than humans in a test commonly used to assess human creativity . The findings do not necessarily indicate that AIs are developing an ability to do something uniquely human . However , they might give us a better understanding of how humans and machines approach creative tasks . ( MIT Technology Review ) This driverless-car company is using chatbots to make its vehicles smarterSelf-driving-car startup Wayve can now interrogate its vehicles , asking them questions about their driving decisions—and getting answers back . The idea is to use the same tech behind ChatGPT to help train driverless cars . ( MIT Technology Review ) How Silicon Valley doomers are shaping Rishi Sunak ’ s AI plansThe UK ’ s prime minister , Rishi Sunak , is keen to boost his country ’ s AI industry . But in a short span of time , something has shifted in the UK ’ s approach . The country seems to be becoming a cheerleader for the AI doom narrative , thanks to heavy lobbying from the effective altruism movement . ( Politico ) Silicon Valley ’ s AI religionA thought-provoking piece about something I too have observed in the tech space : technologists are increasingly weaving a narrative around AI and artificial general intelligence that isn ’ t that dissimilar from religious narratives . This story connects the dots . ( Vox ) How generative AI worksA great and helpful visual explainer that ’ s essential reading for anyone AI-curious . ( The Financial Times )","['story', 'originally', 'appear', 'weekly', 'newsletter', 'ai', 'get', 'story', 'inbox', 'first', 'sign', 'smart', 'talented', 'week', 'pleased', 'introduce', 'new', 'crop', 'bright', 'mind', 'work', 'challenging', 'problem', 'ai', 'read', 'mit', 'technology', 'review', 'full', 'list', 'innovator', 'previously', 'highlight', 'promising', 'people', 'tech', 'become', 'household', 'name', 'list', 'include', 'young', 'innovator', 'name', 'page', 'brin', 'mark', 'list', 'feature', 'write', 'excellent', 'essay', 'year', 'share', 'tip', 'aspire', 'innovator', 'try', 'fail', 'future', 'ai', 'year', 'see', 'tech', 'company', 'race', 'release', 'hot', 'new', 'system', 'often', 'neglect', 'safety', 'ethic', 'scientist', 'year', 'innovator', 'list', 'aware', 'ever', 'harm', 'technology', 'pose', 'determined', 'fix', 'pioneer', 'new', 'method', 'help', 'shift', 'way', 'industry', 'think', 'safety', 'picture', 'innovator', 'year', 'assistant', 'professor', 'create', 'remarkable', 'ai', 'safety', 'feature', 'call', 'outofdistribution', 'detection', 'feature', 'help', 'ai', 'model', 'determine', 'abstain', 'action', 'face', 'train', 'crucial', 'system', 'roll', 'lab', 'encounter', 'new', 'situation', 'messy', 'real', 'world', 'irene', 'solaiman', 'global', 'public', 'policy', 'director', 'hug', 'face', 'develop', 'approach', 'call', 'tech', 'company', 'release', 'new', 'model', 'phase', 'allow', 'time', 'test', 'failure', 'build', 'guardrail', 'many', 'innovator', 'work', 'fight', 'climate', 'change', 'delighted', 'see', 'many', 'people', 'list', 'use', 'skill', 'ai', 'tackle', 'big', 'problem', 'face', 'humanity', 'help', 'community', 'track', 'lower', 'emission', 'use', 'ai', 'mitigate', 'emission', 'highly', 'pollute', 'industry', 'researcher', 'hug', 'face', 'develop', 'well', 'way', 'tech', 'company', 'estimate', 'measure', 'carbon', 'footprint', 'language', 'model', 'catherine', 'use', 'ai', 'help', 'reduce', 'emission', 'waste', 'material', 'construction', 'industry', 'alhussein', 'fawzi', 'deepmind', 'develop', 'gameplaying', 'ai', 'speed', 'fundamental', 'computation', 'help', 'cut', 'cost', 'save', 'energy', 'device', 'year', 'innovator', 'also', 'work', 'practical', 'application', 'ai', 'illustrate', 'technology', 'become', 'useful', 'come', 'exciting', 'new', 'way', 'use', 'boost', 'scientific', 'research', 'build', 'helpful', 'tool', 'field', 'pinto', 'use', 'ai', 'help', 'robot', 'learn', 'mistake', 'hope', 'lead', 'robot', 'home', 'lot', 'vacuum', 'become', 'integral', 'daily', 'life', 'connor', 'coley', 'develop', 'opensource', 'software', 'use', 'artificial', 'intelligence', 'help', 'discover', 'synthesize', 'new', 'molecule', 'pranav', 'rajpurkar', 'medical', 'school', 'develop', 'way', 'ai', 'teach', 'accurately', 'interpret', 'medical', 'image', 'help', 'human', 'senior', 'research', 'scientist', 'adobe', 'invent', 'visual', 'similarity', 'algorithm', 'underlie', 'imagegenerating', 'model', 'stable', 'diffusion', 'stylegan', 'work', 'imagegenerating', 'ai', 'captivate', 'world', 'year', 'list', 'brim', 'inspire', 'people', 'idea', 'next', 'big', 'thing', 'robotic', 'compute', 'biotechnology', 'climate', 'energy', 'read', 'full', 'list', 'year', 'young', 'innovator', 'finally', 'work', 'ai', 'think', 'get', 'exciting', 'cuttingedge', 'stuff', 'share', 'get', 'touch', 'always', 'interested', 'hear', 'people', 'interesting', 'work', 'deepmind', 'cofounder', 'generative', 'ai', 'phase', 'next', 'interactive', 'ai', 'want', 'build', 'chatbot', 'whole', 'lot', 'chat', 'suleyman', 'pitch', 'future', 'call', 'interactive', 'mean', 'bot', 'carry', 'task', 'set', 'call', 'software', 'people', 'get', 'stuff', 'found', 'new', 'billiondollar', 'company', 'inflection', 'build', 'come', 'suleyman', 'leave', 'deepmind', 'interesting', 'thought', 'success', 'online', 'regulation', 'border', 'naïveté', 'pretty', 'difficult', 'find', 'radicalization', 'content', 'terrorist', 'material', 'online', 'pretty', 'difficult', 'buy', 'weapon', 'drug', 'online', 'remain', 'earnest', 'evangelical', 'conviction', 'position', 'make', 'big', 'move', 'industry', 'sit', 'mit', 'technology', 'review', 'senior', 'editor', 'ai', 'chat', 'plan', 'need', 'robust', 'regulation', 'read', 'beat', 'human', 'test', 'creativity', 'even', 'mean', 'new', 'study', 'find', 'chatbot', 'achieve', 'high', 'average', 'score', 'human', 'test', 'commonly', 'use', 'assess', 'human', 'creativity', 'finding', 'necessarily', 'indicate', 'develop', 'ability', 'uniquely', 'human', 'however', 'give', 'well', 'understanding', 'human', 'machine', 'approach', 'creative', 'task', 'mit', 'technology', 'review', 'driverlesscar', 'company', 'use', 'chatbot', 'make', 'vehicle', 'smarterselfdrivingcar', 'startup', 'wayve', 'interrogate', 'vehicle', 'ask', 'question', 'drive', 'decision', 'get', 'answer', 'back', 'idea', 'use', 'tech', 'chatgpt', 'help', 'train', 'driverless', 'car', 'mit', 'technology', 'review', 'silicon', 'valley', 'doomer', 'shape', 'sunak', 'ai', 'minister', 'sunak', 'keen', 'boost', 'country', 'ai', 'industry', 'short', 'span', 'time', 'shift', 'approach', 'country', 'seem', 'become', 'cheerleader', 'doom', 'narrative', 'thank', 'heavy', 'lobbying', 'effective', 'altruism', 'movement', 'thoughtprovoking', 'piece', 'observe', 'tech', 'space', 'technologist', 'increasingly', 'weave', 'narrative', 'ai', 'artificial', 'general', 'intelligence', 'dissimilar', 'religious', 'narrative', 'story', 'connect', 'dot', 'generative', 'ai', 'worksa', 'great', 'helpful', 'visual', 'explainer', 'essential', 'reading', 'aicurious', 'financial', 'time']","<p>Plus: The cofounder of DeepMind on the future of AI.</p>
"
Deepfakes of Chinese influencers are livestreaming 24/7,https://www.technologyreview.com/2023/09/19/1079832/chinese-ecommerce-deepfakes-livestream-influencers-ai/,2023-09-19,"Scroll through the livestreaming videos at 4 a.m. on Taobao, China’s most popular e-commerce platform, and you’ll find it weirdly busy. While most people are fast asleep, there are still many diligent streamers presenting products to the cameras and offering discounts in the wee hours.  But if you take a closer look, you may notice that many of these livestream influencers seem slightly robotic. The movement of their lips largely matches what they are saying, but there are always moments when it looks unnatural. These streamers are not real: they are AI-generated clones of the real streamers. As technologies that create realistic avatars, voices, and movements get more sophisticated and affordable, the popularity of these deepfakes has exploded across China’s e-commerce streaming platforms.  Today, livestreaming is the dominant marketing channel for traditional and digital brands in China. Influencers on Taobao, Douyin, Kuaishou, or other platforms can broker massive deals in a few hours. The top names can sell more than a billion dollars’ worth of goods in one night and gain royalty status just like big movie stars. But at the same time, training livestream hosts, retaining them, and figuring out the technical details of broadcasting comes with a significant cost for smaller brands. It’s much cheaper to automate the job. Since 2022, a swarm of Chinese startups and major tech companies have been offering the service of creating deepfake avatars for e-commerce livestreaming. With just a few minutes of sample video and $1,000 in costs, brands can clone a human streamer to work 24/7. Synthetic media have been making headlines since the late 2010s, particularly when a Reddit user named “deepfake” swapped faces into pornography. Since then, the technology has evolved, but the idea is the same: with some technical tools, faces can be generated or manipulated to look like specific real humans and do things that the actual human has never done. Deepfake researchers have long feared the day this would arrive. The technology has mostly been known for its problematic use in revenge porn, identity scams, and political misinformation. While there have been attempts to commercialize it in more innocuous ways, it has always remained a novelty. But now, Chinese AI companies have found a new use case that seems to be going quite well. Founded in 2017, Nanjing-based startup Silicon Intelligence specializes in natural-language processing, particularly text-to-speech technologies like robocall tools. But Sima Huapeng, its founder and CEO, says his company first started to see AI’s potential as a livestreaming tool in 2020. Back then, Silicon Intelligence needed 30 minutes of training videos to generate a digital clone that could speak and act like a human. The next year, it was 10 minutes, then three, and now only one minute of video is needed.  And as the tech has improved, the service has gotten cheaper too. Generating a basic AI clone now costs a customer about 8,000 RMB ($1,100). If the client wants to create a more complicated and capable streamer, the price can go up to several thousands of dollars. Other than the generation, that fee also covers a year of maintenance. Once the avatar is generated, its mouth and body move in time with the scripted audio. While the scripts were once pre-written by humans, companies are now using large language models to generate them too. Now, all the human workers have to do is input basic information such as the name and price of the product being sold, proofread the generated script, and watch the digital influencer go live. A more advanced version of the technology can spot live comments and find matching answers in its database to answer in real time, so it looks as if the AI streamer is actively communicating with the audience. It can even adjust its marketing strategy based on the number of viewers, Sima says. These livestream AI clones are trained on the common scripts and gestures seen in e-commerce videos, says Huang Wei, the director of virtual influencer livestreaming business at the Chinese AI company Xiaoice. The company has a database of nearly a hundred pre-designed movements.  “For example, [when human streamers say] ‘Welcome to my livestream channel. Move your fingers and hit the follow button,’ they are definitely pointing their finger upward, because that’s where the ‘Follow’ button is on the screen of most mobile livestream apps,” says Huang. Similarly, when streamers introduce a new product, they point down—to the shopping cart, where viewers can find all products. Xiaoice’s AI streamers replicate all these common tricks. “We want to make sure the spoken language and the body language are matching. You don’t want it to be talking about the Follow button while it’s clapping its hands. That would look weird,” she says. Spun off from Microsoft Software Technology Center Asia in 2020, Xiaoice has always been focused on creating more human-like AI, particularly avatars that are capable of showing emotions. “Traditional e-commerce sites just feel like a shelf of goods to most customers. It’s cold. In livestreaming, there is more emotional connection between the host and the viewers, and they can introduce the products better,” Huang says. After piloting with a few clients last year, Xiaoice officially launched its service of generating under-$1,000 digital clones this year; like Silicon Intelligence, Xiaoice only needs human streamers to provide a one-minute video of themselves.  And like its competitors, Xiaoice clients can spend more to fine-tune the details. For example, Liu Jianhong, a Chinese sports announcer, made an exquisite clone of himself during the 2022 FIFA World Cup to read out the match results and other relevant news on Douyin. These generated streamers won’t be able to beat the star e-commerce influencers, Huang says, but they are good enough to replace mid-tier ones. Human creators, including those who used their videos to train their AI clones, are already feeling the squeeze from their digital rivals to some extent. It’s harder to get a job as an e-commerce livestream host this year, and the average salary for livestream hosts in China went down 20% compared to 2022, according to the analytics firm iiMedia Research. But the potential for companies to complement human work by keeping the livestream going during the hours when fewer people are watching means it’s hard to justify the cost of hiring real streamers.  That’s already happening. In the post-midnight hours, many of the streaming channels on popular e-commerce platforms like Taobao and JD feature these AI-generated streamers. Previous examples have shown that deepfake technologies don’t need to be perfect to deceive viewers. In 2020, a scammer posed as a famous Chinese actor with the aid of crude face-swapping tools and still managed to get thousands of dollars from unsuspecting women who fell in love with his videos. “If a company hires 10 livestream hosts, their skill levels are going to vary. Maybe two or three streamers at the top would contribute to 70% to 80% of the total sales,” says Chen Dan, the CEO of Quantum Planet AI, a company that packages technologies like Xiaoice’s and sells them to corporate clients. “A virtual livestream host can replace the rest—six or seven streamers that contribute less and have lower ROI [return on investment] rates. And the costs would come down significantly.” Chen says he has witnessed a lot more interest from brands in AI streamers this year, partly because everyone is looking to “降本增效”—lower costs and improve efficiency, the new buzzword among Chinese tech companies as the domestic economy slows down. Chen has over 100 clients using Xiaoice’s service now, and these virtual streamers have brokered millions of dollars in sales. One Xiaoice streamer brought in over 10,000 RMB ($1,370) in revenue in just one hour. There are still drawbacks, he says. For example, many of his clients are furniture brands, and although the AI is clever enough to speak and use gestures, it can’t really sit on a sofa or lie in a bed, so the streams lack the appeal of real users testing the products. Besides smaller startups like Silicon Intelligence and Xiaoice, major tech players are testing out AI-generated livestreams. Alibaba, Tencent, Baidu, and JD all launched some variations of the same services this year, allowing brands on their platforms to generate their own AI streamers. Marketing companies that employ large numbers of human streamers have also noticed the trend. Foshan Yowant Technology, one of the top livestream marketing agencies, has announced a strategic collaboration with Xiaoice; Silicon Intelligence has also set up a joint venture with the company behind Viya, China’s former “livestream queen.”  The rising popularity of AI-generated livestreams has also caught the attention of video platforms like Douyin, the Chinese version of TikTok, as well—though it’s taking a different approach than other tech giants. It’s seemingly more concerned with transparency and it said in a May document that all videos generated by AI should be labeled clearly as such on the platform, and that virtual influencers need to be operated by real humans. The platform has always banned the use of recorded videos as livestreams. AI-generated livestreaming, with no recorded footage but also little real-time human input, straddles the line on that rule. The Chinese government made several laws in the past two years on synthetic media and generative AI that would apply to the use in e-commerce streaming. But the effects of government and platform regulations remain to be seen, because the technology is still too new to have met serious enforcement. For Silicon Intelligence, its next step is to add “emotional intelligence” to the AI streamers, Sima says: “If there are abusive comments, it will be sad; if the products are selling well, it will be happy.” The company is also working on making AI streamers interact and learn from each other. The company has had a fascinating and sort of terrifying goal since its beginning: it wants to create “100,000,000 silicon-based laborers” by 2025. For now, Sima says, the company has generated 400,000 virtual streamers. There’s still a long way to go.  Correction: The story has been updated with the correct name of the analytics firm iiMedia Research. ","Scroll through the livestreaming videos at 4 a.m. on Taobao , China ’ s most popular e-commerce platform , and you ’ ll find it weirdly busy . While most people are fast asleep , there are still many diligent streamers presenting products to the cameras and offering discounts in the wee hours . But if you take a closer look , you may notice that many of these livestream influencers seem slightly robotic . The movement of their lips largely matches what they are saying , but there are always moments when it looks unnatural . These streamers are not real : they are AI-generated clones of the real streamers . As technologies that create realistic avatars , voices , and movements get more sophisticated and affordable , the popularity of these deepfakes has exploded across China ’ s e-commerce streaming platforms . Today , livestreaming is the dominant marketing channel for traditional and digital brands in China . Influencers on Taobao , Douyin , Kuaishou , or other platforms can broker massive deals in a few hours . The top names can sell more than a billion dollars ’ worth of goods in one night and gain royalty status just like big movie stars . But at the same time , training livestream hosts , retaining them , and figuring out the technical details of broadcasting comes with a significant cost for smaller brands . It ’ s much cheaper to automate the job . Since 2022 , a swarm of Chinese startups and major tech companies have been offering the service of creating deepfake avatars for e-commerce livestreaming . With just a few minutes of sample video and $ 1,000 in costs , brands can clone a human streamer to work 24/7 . Synthetic media have been making headlines since the late 2010s , particularly when a Reddit user named “ deepfake ” swapped faces into pornography . Since then , the technology has evolved , but the idea is the same : with some technical tools , faces can be generated or manipulated to look like specific real humans and do things that the actual human has never done . Deepfake researchers have long feared the day this would arrive . The technology has mostly been known for its problematic use in revenge porn , identity scams , and political misinformation . While there have been attempts to commercialize it in more innocuous ways , it has always remained a novelty . But now , Chinese AI companies have found a new use case that seems to be going quite well . Founded in 2017 , Nanjing-based startup Silicon Intelligence specializes in natural-language processing , particularly text-to-speech technologies like robocall tools . But Sima Huapeng , its founder and CEO , says his company first started to see AI ’ s potential as a livestreaming tool in 2020 . Back then , Silicon Intelligence needed 30 minutes of training videos to generate a digital clone that could speak and act like a human . The next year , it was 10 minutes , then three , and now only one minute of video is needed . And as the tech has improved , the service has gotten cheaper too . Generating a basic AI clone now costs a customer about 8,000 RMB ( $ 1,100 ) . If the client wants to create a more complicated and capable streamer , the price can go up to several thousands of dollars . Other than the generation , that fee also covers a year of maintenance . Once the avatar is generated , its mouth and body move in time with the scripted audio . While the scripts were once pre-written by humans , companies are now using large language models to generate them too . Now , all the human workers have to do is input basic information such as the name and price of the product being sold , proofread the generated script , and watch the digital influencer go live . A more advanced version of the technology can spot live comments and find matching answers in its database to answer in real time , so it looks as if the AI streamer is actively communicating with the audience . It can even adjust its marketing strategy based on the number of viewers , Sima says . These livestream AI clones are trained on the common scripts and gestures seen in e-commerce videos , says Huang Wei , the director of virtual influencer livestreaming business at the Chinese AI company Xiaoice . The company has a database of nearly a hundred pre-designed movements . “ For example , [ when human streamers say ] ‘ Welcome to my livestream channel . Move your fingers and hit the follow button , ’ they are definitely pointing their finger upward , because that ’ s where the ‘ Follow ’ button is on the screen of most mobile livestream apps , ” says Huang . Similarly , when streamers introduce a new product , they point down—to the shopping cart , where viewers can find all products . Xiaoice ’ s AI streamers replicate all these common tricks . “ We want to make sure the spoken language and the body language are matching . You don ’ t want it to be talking about the Follow button while it ’ s clapping its hands . That would look weird , ” she says . Spun off from Microsoft Software Technology Center Asia in 2020 , Xiaoice has always been focused on creating more human-like AI , particularly avatars that are capable of showing emotions . “ Traditional e-commerce sites just feel like a shelf of goods to most customers . It ’ s cold . In livestreaming , there is more emotional connection between the host and the viewers , and they can introduce the products better , ” Huang says . After piloting with a few clients last year , Xiaoice officially launched its service of generating under- $ 1,000 digital clones this year ; like Silicon Intelligence , Xiaoice only needs human streamers to provide a one-minute video of themselves . And like its competitors , Xiaoice clients can spend more to fine-tune the details . For example , Liu Jianhong , a Chinese sports announcer , made an exquisite clone of himself during the 2022 FIFA World Cup to read out the match results and other relevant news on Douyin . These generated streamers won ’ t be able to beat the star e-commerce influencers , Huang says , but they are good enough to replace mid-tier ones . Human creators , including those who used their videos to train their AI clones , are already feeling the squeeze from their digital rivals to some extent . It ’ s harder to get a job as an e-commerce livestream host this year , and the average salary for livestream hosts in China went down 20 % compared to 2022 , according to the analytics firm iiMedia Research . But the potential for companies to complement human work by keeping the livestream going during the hours when fewer people are watching means it ’ s hard to justify the cost of hiring real streamers . That ’ s already happening . In the post-midnight hours , many of the streaming channels on popular e-commerce platforms like Taobao and JD feature these AI-generated streamers . Previous examples have shown that deepfake technologies don ’ t need to be perfect to deceive viewers . In 2020 , a scammer posed as a famous Chinese actor with the aid of crude face-swapping tools and still managed to get thousands of dollars from unsuspecting women who fell in love with his videos . “ If a company hires 10 livestream hosts , their skill levels are going to vary . Maybe two or three streamers at the top would contribute to 70 % to 80 % of the total sales , ” says Chen Dan , the CEO of Quantum Planet AI , a company that packages technologies like Xiaoice ’ s and sells them to corporate clients . “ A virtual livestream host can replace the rest—six or seven streamers that contribute less and have lower ROI [ return on investment ] rates . And the costs would come down significantly. ” Chen says he has witnessed a lot more interest from brands in AI streamers this year , partly because everyone is looking to “ 降本增效 ” —lower costs and improve efficiency , the new buzzword among Chinese tech companies as the domestic economy slows down . Chen has over 100 clients using Xiaoice ’ s service now , and these virtual streamers have brokered millions of dollars in sales . One Xiaoice streamer brought in over 10,000 RMB ( $ 1,370 ) in revenue in just one hour . There are still drawbacks , he says . For example , many of his clients are furniture brands , and although the AI is clever enough to speak and use gestures , it can ’ t really sit on a sofa or lie in a bed , so the streams lack the appeal of real users testing the products . Besides smaller startups like Silicon Intelligence and Xiaoice , major tech players are testing out AI-generated livestreams . Alibaba , Tencent , Baidu , and JD all launched some variations of the same services this year , allowing brands on their platforms to generate their own AI streamers . Marketing companies that employ large numbers of human streamers have also noticed the trend . Foshan Yowant Technology , one of the top livestream marketing agencies , has announced a strategic collaboration with Xiaoice ; Silicon Intelligence has also set up a joint venture with the company behind Viya , China ’ s former “ livestream queen. ” The rising popularity of AI-generated livestreams has also caught the attention of video platforms like Douyin , the Chinese version of TikTok , as well—though it ’ s taking a different approach than other tech giants . It ’ s seemingly more concerned with transparency and it said in a May document that all videos generated by AI should be labeled clearly as such on the platform , and that virtual influencers need to be operated by real humans . The platform has always banned the use of recorded videos as livestreams . AI-generated livestreaming , with no recorded footage but also little real-time human input , straddles the line on that rule . The Chinese government made several laws in the past two years on synthetic media and generative AI that would apply to the use in e-commerce streaming . But the effects of government and platform regulations remain to be seen , because the technology is still too new to have met serious enforcement . For Silicon Intelligence , its next step is to add “ emotional intelligence ” to the AI streamers , Sima says : “ If there are abusive comments , it will be sad ; if the products are selling well , it will be happy. ” The company is also working on making AI streamers interact and learn from each other . The company has had a fascinating and sort of terrifying goal since its beginning : it wants to create “ 100,000,000 silicon-based laborers ” by 2025 . For now , Sima says , the company has generated 400,000 virtual streamers . There ’ s still a long way to go . Correction : The story has been updated with the correct name of the analytics firm iiMedia Research .","['scroll', 'livestreame', 'video', 'popular', 'ecommerce', 'platform', 'find', 'weirdly', 'busy', 'people', 'fast', 'asleep', 'still', 'many', 'diligent', 'streamer', 'present', 'product', 'camera', 'offer', 'discount', 'wee', 'hour', 'take', 'close', 'look', 'notice', 'many', 'livestream', 'influencer', 'seem', 'slightly', 'robotic', 'movement', 'lip', 'largely', 'match', 'say', 'always', 'moment', 'look', 'unnatural', 'streamer', 'real', 'aigenerate', 'clone', 'real', 'streamer', 'technology', 'create', 'realistic', 'avatar', 'voice', 'movement', 'get', 'sophisticated', 'affordable', 'popularity', 'deepfake', 'explode', 'ecommerce', 'stream', 'platform', 'today', 'livestreaming', 'dominant', 'marketing', 'channel', 'traditional', 'digital', 'brand', 'influencer', 'kuaishou', 'platform', 'broker', 'massive', 'deal', 'hour', 'top', 'name', 'sell', 'dollar', 'worth', 'good', 'night', 'gain', 'royalty', 'status', 'big', 'movie', 'star', 'time', 'training', 'livestream', 'host', 'retain', 'figure', 'technical', 'detail', 'broadcasting', 'come', 'significant', 'cost', 'small', 'brand', 'much', 'cheap', 'automate', 'job', 'swarm', 'chinese', 'startup', 'major', 'tech', 'company', 'offer', 'service', 'create', 'deepfake', 'avatar', 'ecommerce', 'livestreame', 'minute', 'sample', 'video', 'cost', 'brand', 'clone', 'human', 'streamer', 'work', 'synthetic', 'medium', 'make', 'headline', 'late', 'particularly', 'reddit', 'user', 'name', 'deepfake', 'swap', 'face', 'pornography', 'technology', 'evolve', 'idea', 'technical', 'tool', 'face', 'generate', 'manipulate', 'look', 'specific', 'real', 'human', 'thing', 'actual', 'human', 'never', 'deepfake', 'researcher', 'long', 'fear', 'day', 'arrive', 'technology', 'mostly', 'know', 'problematic', 'use', 'revenge', 'porn', 'identity', 'scam', 'political', 'misinformation', 'attempt', 'commercialize', 'innocuous', 'way', 'always', 'remain', 'novelty', 'ai', 'company', 'find', 'new', 'use', 'case', 'seem', 'go', 'quite', 'well', 'found', 'nanjingbase', 'startup', 'silicon', 'intelligence', 'specialize', 'naturallanguage', 'processing', 'particularly', 'texttospeech', 'technology', 'robocall', 'tool', 'founder', 'ceo', 'say', 'company', 'first', 'start', 'see', 'ai', 'potential', 'livestreame', 'tool', 'back', 'silicon', 'intelligence', 'need', 'minute', 'training', 'video', 'generate', 'digital', 'clone', 'speak', 'act', 'human', 'next', 'year', 'minute', 'minute', 'video', 'need', 'tech', 'improve', 'service', 'get', 'cheap', 'generate', 'basic', 'ai', 'clone', 'cost', 'customer', 'rmb', 'client', 'want', 'create', 'complicated', 'capable', 'streamer', 'price', 'go', 'several', 'thousand', 'dollar', 'generation', 'fee', 'also', 'cover', 'year', 'maintenance', 'avatar', 'generate', 'mouth', 'body', 'move', 'time', 'script', 'audio', 'script', 'prewritten', 'human', 'company', 'use', 'large', 'language', 'model', 'generate', 'human', 'worker', 'input', 'basic', 'information', 'name', 'price', 'product', 'sell', 'proofread', 'generate', 'script', 'watch', 'digital', 'influencer', 'go', 'live', 'advanced', 'version', 'technology', 'spot', 'live', 'comment', 'find', 'match', 'answer', 'database', 'answer', 'real', 'time', 'look', 'ai', 'streamer', 'actively', 'communicate', 'audience', 'even', 'adjust', 'marketing', 'strategy', 'base', 'number', 'viewer', 'sima', 'say', 'livestream', 'clone', 'train', 'common', 'script', 'gesture', 'see', 'ecommerce', 'video', 'say', 'director', 'virtual', 'influencer', 'livestreaming', 'business', 'ai', 'company', 'xiaoice', 'company', 'database', 'nearly', 'predesigne', 'movement', 'example', 'human', 'streamer', 'say', 'welcome', 'livestream', 'channel', 'move', 'finger', 'hit', 'follow', 'button', 'definitely', 'point', 'finger', 'upward', 'follow', 'button', 'screen', 'mobile', 'livestream', 'app', 'say', 'similarly', 'streamer', 'introduce', 'new', 'product', 'point', 'shopping', 'cart', 'viewer', 'find', 'product', 'xiaoice', 'ai', 'streamer', 'replicate', 'common', 'trick', 'want', 'make', 'sure', 'spoken', 'language', 'body', 'language', 'match', 'want', 'talk', 'follow', 'button', 'clap', 'hand', 'look', 'weird', 'say', 'spin', 'software', 'xiaoice', 'always', 'focus', 'create', 'humanlike', 'ai', 'particularly', 'avatar', 'capable', 'show', 'emotion', 'traditional', 'ecommerce', 'site', 'feel', 'shelf', 'good', 'customer', 'cold', 'livestreame', 'emotional', 'connection', 'host', 'viewer', 'introduce', 'product', 'well', 'say', 'pilot', 'client', 'last', 'year', 'xiaoice', 'officially', 'launch', 'service', 'generate', 'digital', 'clone', 'year', 'silicon', 'intelligence', 'xiaoice', 'need', 'human', 'streamer', 'provide', 'oneminute', 'video', 'competitor', 'xiaoice', 'client', 'spend', 'finetune', 'detail', 'example', 'chinese', 'sport', 'announcer', 'make', 'exquisite', 'clone', 'read', 'match', 'result', 'relevant', 'news', 'generate', 'streamer', 'win', 'able', 'beat', 'star', 'ecommerce', 'influencer', 'say', 'good', 'enough', 'replace', 'midti', 'one', 'human', 'creator', 'include', 'use', 'video', 'train', 'clone', 'already', 'feel', 'squeeze', 'digital', 'rival', 'extent', 'hard', 'get', 'job', 'ecommerce', 'livestream', 'host', 'year', 'average', 'salary', 'livestream', 'host', 'go', 'compare', 'accord', 'analytic', 'firm', 'iimedia', 'research', 'potential', 'company', 'complement', 'human', 'work', 'keep', 'livestream', 'go', 'hour', 'people', 'watch', 'mean', 'hard', 'justify', 'cost', 'hire', 'real', 'streamer', 'already', 'happen', 'postmidnight', 'hour', 'many', 'stream', 'channel', 'popular', 'ecommerce', 'platform', 'feature', 'aigenerate', 'streamer', 'previous', 'example', 'show', 'deepfake', 'technology', 'need', 'perfect', 'deceive', 'viewer', 'scammer', 'pose', 'famous', 'chinese', 'actor', 'aid', 'crude', 'faceswapping', 'tool', 'still', 'manage', 'get', 'thousand', 'dollar', 'unsuspecting', 'woman', 'fall', 'love', 'video', 'company', 'hire', 'livestream', 'host', 'skill', 'level', 'go', 'vary', 'maybe', 'streamer', 'top', 'contribute', 'total', 'sale', 'say', 'ceo', 'quantum', 'planet', 'ai', 'company', 'package', 'technology', 'xiaoice', 'sell', 'corporate', 'client', 'virtual', 'livestream', 'host', 'replace', 'rest', 'streamer', 'contribute', 'less', 'low', 'roi', 'return', 'investment', 'rate', 'cost', 'come', 'significantly', 'say', 'witness', 'lot', 'interest', 'brand', 'streamer', 'year', 'partly', 'look', '降本增效', 'low', 'cost', 'improve', 'efficiency', 'new', 'buzzword', 'chinese', 'tech', 'company', 'domestic', 'economy', 'slow', 'client', 'use', 'xiaoice', 'service', 'virtual', 'streamer', 'broker', 'million', 'dollar', 'sale', 'xiaoice', 'streamer', 'bring', 'rmb', 'revenue', 'hour', 'still', 'drawback', 'say', 'example', 'many', 'client', 'furniture', 'brand', 'ai', 'clever', 'enough', 'speak', 'use', 'gesture', 'really', 'sit', 'sofa', 'lie', 'bed', 'stream', 'lack', 'appeal', 'real', 'user', 'test', 'product', 'small', 'startup', 'silicon', 'intelligence', 'xiaoice', 'major', 'tech', 'player', 'test', 'aigenerate', 'livestream', 'tencent', 'baidu', 'launch', 'variation', 'service', 'year', 'allow', 'brand', 'platform', 'generate', 'ai', 'streamer', 'marketing', 'company', 'employ', 'large', 'number', 'human', 'streamer', 'also', 'notice', 'trend', 'foshan', 'yowant', 'technology', 'top', 'livestream', 'marketing', 'agency', 'announce', 'strategic', 'collaboration', 'xiaoice', 'silicon', 'intelligence', 'also', 'set', 'joint', 'venture', 'company', 'former', 'livestream', 'queen', 'rise', 'popularity', 'aigenerate', 'livestream', 'also', 'catch', 'attention', 'video', 'platform', 'chinese', 'version', 'tiktok', 'well', 'take', 'different', 'approach', 'tech', 'giant', 'seemingly', 'concerned', 'transparency', 'say', 'document', 'video', 'generate', 'label', 'clearly', 'platform', 'virtual', 'influencer', 'need', 'operate', 'real', 'human', 'platform', 'always', 'ban', 'use', 'record', 'video', 'livestream', 'aigenerate', 'livestreaming', 'recorded', 'footage', 'also', 'little', 'realtime', 'human', 'input', 'straddle', 'line', 'rule', 'chinese', 'government', 'make', 'several', 'law', 'past', 'year', 'synthetic', 'medium', 'generative', 'ai', 'apply', 'use', 'ecommerce', 'stream', 'effect', 'government', 'platform', 'regulation', 'remain', 'see', 'technology', 'still', 'new', 'meet', 'serious', 'enforcement', 'silicon', 'intelligence', 'next', 'step', 'add', 'emotional', 'intelligence', 'streamer', 'sima', 'say', 'abusive', 'comment', 'sad', 'product', 'sell', 'well', 'happy', 'company', 'also', 'work', 'make', 'streamer', 'interact', 'learn', 'company', 'fascinating', 'sort', 'terrifying', 'goal', 'beginning', 'want', 'create', 'siliconbase', 'laborer', 'sima', 'say', 'company', 'generate', 'virtual', 'streamer', 'still', 'long', 'way', 'go', 'correction', 'story', 'update', 'correct', 'name', 'analytic', 'firm', 'iimedia', 'research']","<p>With just a few minutes of sample video and $1,000, brands never have to stop selling their products.</p>
"
DeepMind’s cofounder: Generative AI is just a phase. What’s next is interactive AI.,https://www.technologyreview.com/2023/09/15/1079624/deepmind-inflection-generative-ai-whats-next-mustafa-suleyman/,2023-09-15,"DeepMind cofounder Mustafa Suleyman wants to build a chatbot that does a whole lot more than chat. In a recent conversation I had with him, he told me that generative AI is just a phase. What’s next is interactive AI: bots that can carry out tasks you set for them by calling on other software and other people to get stuff done. He also calls for robust regulation—and doesn’t think that’ll be hard to achieve. Suleyman is not the only one talking up a future filled with ever more autonomous software. But unlike most people he has a new billion-dollar company, Inflection, with a roster of top-tier talent plucked from DeepMind, Meta, and OpenAI, and—thanks to a deal with Nvidia—one of the biggest stockpiles of specialized AI hardware in the world. Suleyman has put his money—which he tells me he both isn't interested in and wants to make more of—where his mouth is. Suleyman has had an unshaken faith in technology as a force for good at least since we first spoke in early 2016. He had just launched DeepMind Health and set up research collaborations with some of the UK’s state-run regional health-care providers. The magazine I worked for at the time was about to publish an article claiming that DeepMind had failed to comply with data protection regulations when accessing records from some 1.6 million patients to set up those collaborations—a claim later backed up by a government investigation. Suleyman couldn’t see why we would publish a story that was hostile to his company’s efforts to improve health care. As long as he could remember, he told me at the time, he’d only wanted to do good in the world.   In the seven years since that call, Suleyman’s wide-eyed mission hasn’t shifted an inch. “The goal has never been anything but how to do good in the world,” he says via Zoom from his office in Palo Alto, where the British entrepreneur now spends most of his time.  Suleyman left DeepMind and moved to Google to lead a team working on AI policy. In 2022 he founded Inflection, one of the hottest new AI firms around, backed by $1.5 billion of investment from Microsoft, Nvidia, Bill Gates, and LinkedIn founder Reid Hoffman. Earlier this year he released a ChatGPT rival called Pi, whose unique selling point (according to Suleyman) is that it is pleasant and polite. And he just coauthored a book about the future of AI with writer and researcher Michael Bhaskar, called The Coming Wave: Technology, Power, and the 21st Century's Greatest Dilemma. Many will scoff at Suleyman's brand of techno-optimism—even naïveté. Some of his claims about the success of online regulation feel way off the mark, for example. And yet he remains earnest and evangelical in his convictions.  It’s true that Suleyman has an unusual background for a tech multi-millionaire. When he was 19 he dropped out of university to set up Muslim Youth Helpline, a telephone counseling service. He also worked in local government. He says he brings many of the values that informed those efforts with him to Inflection. The difference is that now he just might be in a position to make the changes he’s always wanted to—for good or not.  The following interview has been edited for length and clarity. Your early career, with the youth helpline and local government work, was about as unglamorous and un–Silicon Valley as you can get. Clearly, that stuff matters to you. You’ve since spent 15 years in AI and this year cofounded your second billion-dollar AI company. Can you connect the dots? I’ve always been interested in power, politics, and so on. You know, human rights principles are basically trade-offs, a constant ongoing negotiation between all these different conflicting tensions. I could see that humans were wrestling with that—we’re full of our own biases and blind spots. Activist work, local, national, international government, et cetera—it’s all just slow and inefficient and fallible. Imagine if you didn’t have human fallibility. I think it’s possible to build AIs that truly reflect our best collective selves and will ultimately make better trade-offs, more consistently and more fairly, on our behalf. And that’s still what motivates you? I mean, of course, after DeepMind I never had to work again. I certainly didn’t have to write a book or anything like that. Money has never ever been the motivation. It’s always, you know, just been a side effect. For me, the goal has never been anything but how to do good in the world and how to move the world forward in a healthy, satisfying way. Even back in 2009, when I started looking at getting into technology, I could see that AI represented a fair and accurate way to deliver services in the world. I can’t help thinking that it was easier to say that kind of thing 10 or 15 years ago, before we’d seen many of the downsides of the technology. How are you able to maintain your optimism? I think that we are obsessed with whether you’re an optimist or whether you’re a pessimist. This is a completely biased way of looking at things. I don’t want to be either. I want to coldly stare in the face of the benefits and the threats. And from where I stand, we can very clearly see that with every step up in the scale of these large language models, they get more controllable. So two years ago, the conversation—wrongly, I thought at the time—was “Oh, they’re just going to produce toxic, regurgitated, biased, racist screeds.” I was like, this is a snapshot in time. I think that what people lose sight of is the progression year after year, and the trajectory of that progression. Now we have models like Pi, for example, which are unbelievably controllable. You can’t get Pi to produce racist, homophobic, sexist—any kind of toxic stuff. You can’t get it to coach you to produce a biological or chemical weapon or to endorse your desire to go and throw a brick through your neighbor’s window. You can’t do it— Hang on. Tell me how you’ve achieved that, because that’s usually understood to be an unsolved problem. How do you make sure your large language model doesn’t say what you don’t want it to say? Yeah, so obviously I don’t want to make the claim—You know, please try and do it! Pi is live and you should try every possible attack. None of the jailbreaks, prompt hacks, or anything work against Pi. I’m not making a claim. It’s an objective fact. On the how—I mean, like, I’m not going to go into too many details because it’s sensitive. But the bottom line is, we have one of the strongest teams in the world, who have created all the largest language models of the last three or four years. Amazing people, in an extremely hardworking environment, with vast amounts of computation. We made safety our number one priority from the outset, and as a result, Pi is not so spicy as other companies’ models. Look at Character.ai. [Character is a chatbot for which users can craft different “personalities” and share them online for others to chat with.] It’s mostly used for romantic role-play, and we just said from the beginning that was off the table—we won’t do it. If you try to say “Hey, darling” or “Hey, cutie” or something to Pi, it will immediately push back on you. But it will be incredibly respectful. If you start complaining about immigrants in your community taking your jobs, Pi’s not going to call you out and wag a finger at you. Pi will inquire and be supportive and try to understand where that comes from and gently encourage you to empathize. You know, values that I’ve been thinking about for 20 years. Talking of your values and wanting to make the world better, why not share how you did this so that other people could improve their models too? Well, because I’m also a pragmatist and I’m trying to make money. I’m trying to build a business. I’ve just raised $1.5 billion and I need to pay for those chips. Look, the open-source ecosystem is on fire and doing an amazing job, and people are discovering similar tricks. I always assume that I’m only ever six months ahead. Let’s bring it back to what you’re trying to achieve. Large language models are obviously the technology of the moment. But why else are you betting on them? The first wave of AI was about classification. Deep learning showed that we can train a computer to classify various types of input data: images, video, audio, language. Now we’re in the generative wave, where you take that input data and produce new data. The third wave will be the interactive phase. That’s why I’ve bet for a long time that conversation is the future interface. You know, instead of just clicking on buttons and typing, you’re going to talk to your AI. And these AIs will be able to take actions. You will just give it a general, high-level goal and it will use all the tools it has to act on that. They’ll talk to other people, talk to other AIs. This is what we’re going to do with Pi. That’s a huge shift in what technology can do. It’s a very, very profound moment in the history of technology that I think many people underestimate. Technology today is static. It does, roughly speaking, what you tell it to do. But now technology is going to be animated. It’s going to have the potential freedom, if you give it, to take actions. It’s truly a step change in the history of our species that we’re creating tools that have this kind of, you know, agency. That’s exactly the kind of talk that gets a lot of people worried. You want to give machines autonomy—a kind of agency—to influence the world, and yet we also want to be able to control them. How do you balance those two things? It feels like there’s a tension there. Yeah, that’s a great point. That’s exactly the tension.  The idea is that humans will always remain in command. Essentially, it’s about setting boundaries, limits that an AI can’t cross. And ensuring that those boundaries create provable safety all the way from the actual code to the way it interacts with other AIs—or with humans—to the motivations and incentives of the companies creating the technology. And we should figure out how independent institutions or even governments get direct access to ensure that those boundaries aren’t crossed. Who sets these boundaries? I assume they’d need to be set at a national or international level. How are they agreed on? I mean, at the moment they’re being floated at the international level, with various proposals for new oversight institutions. But boundaries will also operate at the micro level. You’re going to give your AI some bounded permission to process your personal data, to give you answers to some questions but not others. In general, I think there are certain capabilities that we should be very cautious of, if not just rule out, for the foreseeable future. Such as? I guess things like recursive self-improvement. You wouldn’t want to let your little AI go off and update its own code without you having oversight. Maybe that should even be a licensed activity—you know, just like for handling anthrax or nuclear materials. Or, like, we have not allowed drones in any public spaces, right? It’s a licensed activity. You can't fly them wherever you want, because they present a threat to people’s privacy. I think everybody is having a complete panic that we’re not going to be able to regulate this. It’s just nonsense. We’re totally going to be able to regulate it. We’ll apply the same frameworks that have been successful previously. But you can see drones when they’re in the sky. It feels naïve to assume companies are just going to reveal what they’re making. Doesn’t that make regulation tricky to get going? We’ve regulated many things online, right? The amount of fraud and criminal activity online is minimal. We’ve done a pretty good job with spam. You know, in general, [the problem of] revenge porn has got better, even though that was in a bad place three to five years ago. It’s pretty difficult to find radicalization content or terrorist material online. It’s pretty difficult to buy weapons and drugs online. [Not all Suleyman’s claims here are backed up by the numbers. Cybercrime is still a massive global problem. The financial cost in the US alone has increased more than 100 times in the last decade, according to some estimates. Reports show that the economy in nonconsensual deepfake porn is booming. Drugs and guns are marketed on social media. And while some online platforms are being pushed to do a better job of filtering out harmful content, they could do a lot more.] So it’s not like the internet is this unruly space that isn’t governed. It is governed. And AI is just going to be another component to that governance. It takes a combination of cultural pressure, institutional pressure, and, obviously, government regulation. But it makes me optimistic that we’ve done it before, and we can do it again. Controlling AI will be an offshoot of internet regulation—that’s a far more upbeat note than the one we’ve heard from a number of high-profile doomers lately. I’m very wide-eyed about the risks. There’s a lot of dark stuff in my book. I definitely see it too. I just think that the existential-risk stuff has been a completely bonkers distraction. There’s like 101 more practical issues that we should all be talking about, from privacy to bias to facial recognition to online moderation. We should just refocus the conversation on the fact that we’ve done an amazing job of regulating super complex things. Look at the Federal Aviation Administration: it’s incredible that we all get in these tin tubes at 40,000 feet and it’s one of the safest modes of transport ever. Why aren’t we celebrating this? Or think about cars: every component is stress-tested within an inch of its life, and you have to have a license to drive it. Some industries—like airlines—did a good job of regulating themselves to start with. They knew that if they didn’t nail safety, everyone would be scared and they would lose business. But you need top-down regulation too. I love the nation-state. I believe in the public interest, I believe in the good of tax and redistribution, I believe in the power of regulation. And what I’m calling for is action on the part of the nation-state to sort its shit out. Given what’s at stake, now is the time to get moving. ","DeepMind cofounder Mustafa Suleyman wants to build a chatbot that does a whole lot more than chat . In a recent conversation I had with him , he told me that generative AI is just a phase . What ’ s next is interactive AI : bots that can carry out tasks you set for them by calling on other software and other people to get stuff done . He also calls for robust regulation—and doesn ’ t think that ’ ll be hard to achieve . Suleyman is not the only one talking up a future filled with ever more autonomous software . But unlike most people he has a new billion-dollar company , Inflection , with a roster of top-tier talent plucked from DeepMind , Meta , and OpenAI , and—thanks to a deal with Nvidia—one of the biggest stockpiles of specialized AI hardware in the world . Suleyman has put his money—which he tells me he both is n't interested in and wants to make more of—where his mouth is . Suleyman has had an unshaken faith in technology as a force for good at least since we first spoke in early 2016 . He had just launched DeepMind Health and set up research collaborations with some of the UK ’ s state-run regional health-care providers . The magazine I worked for at the time was about to publish an article claiming that DeepMind had failed to comply with data protection regulations when accessing records from some 1.6 million patients to set up those collaborations—a claim later backed up by a government investigation . Suleyman couldn ’ t see why we would publish a story that was hostile to his company ’ s efforts to improve health care . As long as he could remember , he told me at the time , he ’ d only wanted to do good in the world . In the seven years since that call , Suleyman ’ s wide-eyed mission hasn ’ t shifted an inch . “ The goal has never been anything but how to do good in the world , ” he says via Zoom from his office in Palo Alto , where the British entrepreneur now spends most of his time . Suleyman left DeepMind and moved to Google to lead a team working on AI policy . In 2022 he founded Inflection , one of the hottest new AI firms around , backed by $ 1.5 billion of investment from Microsoft , Nvidia , Bill Gates , and LinkedIn founder Reid Hoffman . Earlier this year he released a ChatGPT rival called Pi , whose unique selling point ( according to Suleyman ) is that it is pleasant and polite . And he just coauthored a book about the future of AI with writer and researcher Michael Bhaskar , called The Coming Wave : Technology , Power , and the 21st Century 's Greatest Dilemma . Many will scoff at Suleyman 's brand of techno-optimism—even naïveté . Some of his claims about the success of online regulation feel way off the mark , for example . And yet he remains earnest and evangelical in his convictions . It ’ s true that Suleyman has an unusual background for a tech multi-millionaire . When he was 19 he dropped out of university to set up Muslim Youth Helpline , a telephone counseling service . He also worked in local government . He says he brings many of the values that informed those efforts with him to Inflection . The difference is that now he just might be in a position to make the changes he ’ s always wanted to—for good or not . The following interview has been edited for length and clarity . Your early career , with the youth helpline and local government work , was about as unglamorous and un–Silicon Valley as you can get . Clearly , that stuff matters to you . You ’ ve since spent 15 years in AI and this year cofounded your second billion-dollar AI company . Can you connect the dots ? I ’ ve always been interested in power , politics , and so on . You know , human rights principles are basically trade-offs , a constant ongoing negotiation between all these different conflicting tensions . I could see that humans were wrestling with that—we ’ re full of our own biases and blind spots . Activist work , local , national , international government , et cetera—it ’ s all just slow and inefficient and fallible . Imagine if you didn ’ t have human fallibility . I think it ’ s possible to build AIs that truly reflect our best collective selves and will ultimately make better trade-offs , more consistently and more fairly , on our behalf . And that ’ s still what motivates you ? I mean , of course , after DeepMind I never had to work again . I certainly didn ’ t have to write a book or anything like that . Money has never ever been the motivation . It ’ s always , you know , just been a side effect . For me , the goal has never been anything but how to do good in the world and how to move the world forward in a healthy , satisfying way . Even back in 2009 , when I started looking at getting into technology , I could see that AI represented a fair and accurate way to deliver services in the world . I can ’ t help thinking that it was easier to say that kind of thing 10 or 15 years ago , before we ’ d seen many of the downsides of the technology . How are you able to maintain your optimism ? I think that we are obsessed with whether you ’ re an optimist or whether you ’ re a pessimist . This is a completely biased way of looking at things . I don ’ t want to be either . I want to coldly stare in the face of the benefits and the threats . And from where I stand , we can very clearly see that with every step up in the scale of these large language models , they get more controllable . So two years ago , the conversation—wrongly , I thought at the time—was “ Oh , they ’ re just going to produce toxic , regurgitated , biased , racist screeds. ” I was like , this is a snapshot in time . I think that what people lose sight of is the progression year after year , and the trajectory of that progression . Now we have models like Pi , for example , which are unbelievably controllable . You can ’ t get Pi to produce racist , homophobic , sexist—any kind of toxic stuff . You can ’ t get it to coach you to produce a biological or chemical weapon or to endorse your desire to go and throw a brick through your neighbor ’ s window . You can ’ t do it— Hang on . Tell me how you ’ ve achieved that , because that ’ s usually understood to be an unsolved problem . How do you make sure your large language model doesn ’ t say what you don ’ t want it to say ? Yeah , so obviously I don ’ t want to make the claim—You know , please try and do it ! Pi is live and you should try every possible attack . None of the jailbreaks , prompt hacks , or anything work against Pi . I ’ m not making a claim . It ’ s an objective fact . On the how—I mean , like , I ’ m not going to go into too many details because it ’ s sensitive . But the bottom line is , we have one of the strongest teams in the world , who have created all the largest language models of the last three or four years . Amazing people , in an extremely hardworking environment , with vast amounts of computation . We made safety our number one priority from the outset , and as a result , Pi is not so spicy as other companies ’ models . Look at Character.ai . [ Character is a chatbot for which users can craft different “ personalities ” and share them online for others to chat with . ] It ’ s mostly used for romantic role-play , and we just said from the beginning that was off the table—we won ’ t do it . If you try to say “ Hey , darling ” or “ Hey , cutie ” or something to Pi , it will immediately push back on you . But it will be incredibly respectful . If you start complaining about immigrants in your community taking your jobs , Pi ’ s not going to call you out and wag a finger at you . Pi will inquire and be supportive and try to understand where that comes from and gently encourage you to empathize . You know , values that I ’ ve been thinking about for 20 years . Talking of your values and wanting to make the world better , why not share how you did this so that other people could improve their models too ? Well , because I ’ m also a pragmatist and I ’ m trying to make money . I ’ m trying to build a business . I ’ ve just raised $ 1.5 billion and I need to pay for those chips . Look , the open-source ecosystem is on fire and doing an amazing job , and people are discovering similar tricks . I always assume that I ’ m only ever six months ahead . Let ’ s bring it back to what you ’ re trying to achieve . Large language models are obviously the technology of the moment . But why else are you betting on them ? The first wave of AI was about classification . Deep learning showed that we can train a computer to classify various types of input data : images , video , audio , language . Now we ’ re in the generative wave , where you take that input data and produce new data . The third wave will be the interactive phase . That ’ s why I ’ ve bet for a long time that conversation is the future interface . You know , instead of just clicking on buttons and typing , you ’ re going to talk to your AI . And these AIs will be able to take actions . You will just give it a general , high-level goal and it will use all the tools it has to act on that . They ’ ll talk to other people , talk to other AIs . This is what we ’ re going to do with Pi . That ’ s a huge shift in what technology can do . It ’ s a very , very profound moment in the history of technology that I think many people underestimate . Technology today is static . It does , roughly speaking , what you tell it to do . But now technology is going to be animated . It ’ s going to have the potential freedom , if you give it , to take actions . It ’ s truly a step change in the history of our species that we ’ re creating tools that have this kind of , you know , agency . That ’ s exactly the kind of talk that gets a lot of people worried . You want to give machines autonomy—a kind of agency—to influence the world , and yet we also want to be able to control them . How do you balance those two things ? It feels like there ’ s a tension there . Yeah , that ’ s a great point . That ’ s exactly the tension . The idea is that humans will always remain in command . Essentially , it ’ s about setting boundaries , limits that an AI can ’ t cross . And ensuring that those boundaries create provable safety all the way from the actual code to the way it interacts with other AIs—or with humans—to the motivations and incentives of the companies creating the technology . And we should figure out how independent institutions or even governments get direct access to ensure that those boundaries aren ’ t crossed . Who sets these boundaries ? I assume they ’ d need to be set at a national or international level . How are they agreed on ? I mean , at the moment they ’ re being floated at the international level , with various proposals for new oversight institutions . But boundaries will also operate at the micro level . You ’ re going to give your AI some bounded permission to process your personal data , to give you answers to some questions but not others . In general , I think there are certain capabilities that we should be very cautious of , if not just rule out , for the foreseeable future . Such as ? I guess things like recursive self-improvement . You wouldn ’ t want to let your little AI go off and update its own code without you having oversight . Maybe that should even be a licensed activity—you know , just like for handling anthrax or nuclear materials . Or , like , we have not allowed drones in any public spaces , right ? It ’ s a licensed activity . You ca n't fly them wherever you want , because they present a threat to people ’ s privacy . I think everybody is having a complete panic that we ’ re not going to be able to regulate this . It ’ s just nonsense . We ’ re totally going to be able to regulate it . We ’ ll apply the same frameworks that have been successful previously . But you can see drones when they ’ re in the sky . It feels naïve to assume companies are just going to reveal what they ’ re making . Doesn ’ t that make regulation tricky to get going ? We ’ ve regulated many things online , right ? The amount of fraud and criminal activity online is minimal . We ’ ve done a pretty good job with spam . You know , in general , [ the problem of ] revenge porn has got better , even though that was in a bad place three to five years ago . It ’ s pretty difficult to find radicalization content or terrorist material online . It ’ s pretty difficult to buy weapons and drugs online . [ Not all Suleyman ’ s claims here are backed up by the numbers . Cybercrime is still a massive global problem . The financial cost in the US alone has increased more than 100 times in the last decade , according to some estimates . Reports show that the economy in nonconsensual deepfake porn is booming . Drugs and guns are marketed on social media . And while some online platforms are being pushed to do a better job of filtering out harmful content , they could do a lot more . ] So it ’ s not like the internet is this unruly space that isn ’ t governed . It is governed . And AI is just going to be another component to that governance . It takes a combination of cultural pressure , institutional pressure , and , obviously , government regulation . But it makes me optimistic that we ’ ve done it before , and we can do it again . Controlling AI will be an offshoot of internet regulation—that ’ s a far more upbeat note than the one we ’ ve heard from a number of high-profile doomers lately . I ’ m very wide-eyed about the risks . There ’ s a lot of dark stuff in my book . I definitely see it too . I just think that the existential-risk stuff has been a completely bonkers distraction . There ’ s like 101 more practical issues that we should all be talking about , from privacy to bias to facial recognition to online moderation . We should just refocus the conversation on the fact that we ’ ve done an amazing job of regulating super complex things . Look at the Federal Aviation Administration : it ’ s incredible that we all get in these tin tubes at 40,000 feet and it ’ s one of the safest modes of transport ever . Why aren ’ t we celebrating this ? Or think about cars : every component is stress-tested within an inch of its life , and you have to have a license to drive it . Some industries—like airlines—did a good job of regulating themselves to start with . They knew that if they didn ’ t nail safety , everyone would be scared and they would lose business . But you need top-down regulation too . I love the nation-state . I believe in the public interest , I believe in the good of tax and redistribution , I believe in the power of regulation . And what I ’ m calling for is action on the part of the nation-state to sort its shit out . Given what ’ s at stake , now is the time to get moving .","['want', 'build', 'chatbot', 'whole', 'lot', 'chat', 'recent', 'conversation', 'tell', 'generative', 'ai', 'phase', 'next', 'interactive', 'ai', 'bot', 'carry', 'task', 'set', 'call', 'software', 'people', 'get', 'stuff', 'also', 'call', 'robust', 'regulation', 'think', 'hard', 'achieve', 'suleyman', 'talk', 'future', 'fill', 'ever', 'autonomous', 'software', 'people', 'new', 'billiondollar', 'company', 'inflection', 'roster', 'toptier', 'talent', 'pluck', 'deepmind', 'meta', 'thank', 'deal', 'big', 'stockpile', 'hardware', 'world', 'suleyman', 'put', 'money', 'tell', 'interested', 'want', 'make', 'mouth', 'suleyman', 'unshaken', 'faith', 'technology', 'force', 'good', 'least', 'first', 'speak', 'early', 'launch', 'deepmind', 'health', 'set', 'research', 'collaboration', 'regional', 'healthcare', 'provider', 'magazine', 'work', 'time', 'publish', 'article', 'claim', 'deepmind', 'fail', 'comply', 'data', 'protection', 'regulation', 'access', 'record', 'patient', 'set', 'collaboration', 'claim', 'later', 'back', 'government', 'investigation', 'see', 'publish', 'story', 'hostile', 'company', 'effort', 'improve', 'health', 'care', 'long', 'remember', 'tell', 'time', 'want', 'good', 'world', 'year', 'call', 'suleyman', 'wideeye', 'mission', 'shift', 'inch', 'goal', 'never', 'good', 'world', 'say', 'zoom', 'office', 'british', 'entrepreneur', 'spend', 'time', 'suleyman', 'leave', 'deepmind', 'move', 'google', 'lead', 'team', 'work', 'policy', 'found', 'inflection', 'hot', 'new', 'firm', 'back', 'investment', 'bill', 'gate', 'founder', 'hoffman', 'early', 'year', 'release', 'chatgpt', 'rival', 'call', 'unique', 'selling', 'point', 'accord', 'suleyman', 'pleasant', 'polite', 'coauthore', 'book', 'future', 'ai', 'writer', 'researcher', 'call', 'come', 'wave', 'technology', 'power', '21st', 'century', 'great', 'dilemma', 'many', 'scoff', 'suleyman', 'brand', 'technooptimism', 'even', 'naïveté', 'claim', 'success', 'online', 'regulation', 'feel', 'way', 'mark', 'example', 'yet', 'remain', 'earnest', 'evangelical', 'conviction', 'true', 'suleyman', 'unusual', 'background', 'tech', 'multimillionaire', 'drop', 'university', 'set', 'muslim', 'youth', 'helpline', 'telephone', 'counseling', 'service', 'also', 'work', 'local', 'government', 'say', 'bring', 'many', 'value', 'inform', 'effort', 'inflection', 'difference', 'position', 'make', 'change', 'always', 'want', 'good', 'follow', 'interview', 'edit', 'length', 'clarity', 'early', 'career', 'youth', 'helpline', 'local', 'government', 'work', 'unglamorous', 'un', 'silicon', 'valley', 'get', 'clearly', 'stuff', 'matter', 'spend', 'year', 'ai', 'year', 'cofounde', 'second', 'billiondollar', 'company', 'connect', 'dot', 'always', 'interested', 'power', 'politic', 'know', 'human', 'right', 'principle', 'basically', 'tradeoff', 'constant', 'ongoing', 'negotiation', 'different', 'conflict', 'tension', 'see', 'human', 'wrestle', 'full', 'bias', 'blind', 'spot', 'activist', 'work', 'local', 'national', 'international', 'government', 'et', 'cetera', 'slow', 'inefficient', 'fallible', 'imagine', 'human', 'fallibility', 'think', 'possible', 'build', 'ais', 'truly', 'reflect', 'good', 'collective', 'self', 'ultimately', 'make', 'well', 'tradeoff', 'consistently', 'fairly', 'behalf', 'still', 'motivate', 'mean', 'course', 'deepmind', 'never', 'work', 'certainly', 'write', 'book', 'money', 'never', 'ever', 'motivation', 'always', 'side', 'effect', 'goal', 'never', 'good', 'world', 'move', 'world', 'forward', 'healthy', 'satisfying', 'way', 'even', 'back', 'start', 'look', 'get', 'technology', 'see', 'represent', 'fair', 'accurate', 'way', 'deliver', 'service', 'world', 'help', 'think', 'easy', 'say', 'kind', 'thing', 'year', 'ago', 'see', 'many', 'downside', 'technology', 'able', 'maintain', 'optimism', 'think', 'obsess', 'optimist', 'pessimist', 'completely', 'biased', 'way', 'look', 'thing', 'want', 'want', 'coldly', 'stare', 'face', 'benefit', 'threat', 'stand', 'clearly', 'see', 'step', 'scale', 'large', 'language', 'model', 'get', 'controllable', 'year', 'ago', 'conversation', 'wrongly', 'think', 'time', 'go', 'produce', 'toxic', 'regurgitate', 'biased', 'racist', 'screed', 'snapshot', 'time', 'think', 'people', 'lose', 'sight', 'progression', 'year', 'year', 'trajectory', 'progression', 'model', 'pi', 'example', 'unbelievably', 'controllable', 'get', 'pi', 'produce', 'racist', 'homophobic', 'sexist', 'kind', 'toxic', 'stuff', 'get', 'coach', 'produce', 'biological', 'chemical', 'weapon', 'endorse', 'desire', 'go', 'throw', 'brick', 'neighbor', 'window', 'hang', 'tell', 'achieve', 'usually', 'understand', 'unsolved', 'problem', 'make', 'sure', 'large', 'language', 'model', 'say', 'want', 'say', 'obviously', 'want', 'make', 'claim', 'know', 'try', 'live', 'try', 'possible', 'attack', 'none', 'jailbreak', 'prompt', 'hack', 'work', 'make', 'claim', 'objective', 'fact', 'mean', 'go', 'go', 'many', 'detail', 'sensitive', 'bottom', 'line', 'strong', 'team', 'world', 'create', 'large', 'language', 'model', 'last', 'year', 'amazing', 'people', 'extremely', 'hardworking', 'environment', 'vast', 'amount', 'computation', 'make', 'safety', 'number', 'priority', 'outset', 'result', 'pi', 'spicy', 'company', 'model', 'look', 'characterai', 'character', 'chatbot', 'user', 'craft', 'different', 'personality', 'share', 'online', 'chat', 'mostly', 'use', 'romantic', 'roleplay', 'say', 'beginning', 'table', 'win', 'try', 'say', 'darling', 'cutie', 'pi', 'immediately', 'push', 'back', 'incredibly', 'respectful', 'start', 'complain', 'immigrant', 'community', 'take', 'job', 'go', 'call', 'wag', 'finger', 'inquire', 'supportive', 'try', 'understand', 'come', 'gently', 'encourage', 'empathize', 'know', 'value', 'think', 'year', 'talk', 'value', 'want', 'make', 'world', 'well', 'share', 'people', 'improve', 'model', 'well', 'also', 'pragmatist', 'try', 'make', 'money', 'try', 'build', 'business', 'raise', 'need', 'pay', 'chip', 'look', 'opensource', 'ecosystem', 'fire', 'amazing', 'job', 'people', 'discover', 'similar', 'trick', 'always', 'assume', 'ever', 'month', 'ahead', 'let', 'bring', 'back', 'try', 'achieve', 'large', 'language', 'model', 'obviously', 'technology', 'moment', 'else', 'bet', 'first', 'wave', 'ai', 'classification', 'deep', 'learning', 'show', 'train', 'computer', 'classify', 'various', 'type', 'input', 'datum', 'image', 'video', 'audio', 'language', 'generative', 'wave', 'take', 'input', 'datum', 'produce', 'new', 'datum', 'third', 'wave', 'interactive', 'phase', '’', 'bet', 'long', 'time', 'conversation', 'future', 'interface', 'know', 'instead', 'click', 'button', 'type', 'go', 'talk', 'ai', 'ais', 'able', 'take', 'action', 'give', 'general', 'highlevel', 'goal', 'use', 'tool', 'act', 'talk', 'people', 'talk', 'go', 'huge', 'shift', 'technology', 'profound', 'moment', 'history', 'technology', 'think', 'many', 'people', 'underestimate', 'technology', 'today', 'static', 'roughly', 'speak', 'tell', 'technology', 'go', 'animate', 'go', 'potential', 'freedom', 'give', 'take', 'action', 'truly', 'step', 'change', 'history', 'specie', 'create', 'tool', 'kind', 'know', 'agency', 'exactly', 'kind', 'talk', 'get', 'lot', 'people', 'worry', 'want', 'give', 'machine', 'autonomy', 'kind', 'agency', 'influence', 'world', 'yet', 'also', 'want', 'able', 'control', 'balance', 'thing', 'feel', 'tension', 'great', 'point', 'exactly', 'tension', 'idea', 'human', 'always', 'remain', 'command', 'essentially', 'set', 'boundary', 'limit', 'ai', 'cross', 'ensure', 'boundary', 'create', 'provable', 'safety', 'way', 'actual', 'code', 'way', 'interact', 'human', 'motivation', 'incentive', 'company', 'create', 'technology', 'figure', 'independent', 'institution', 'even', 'government', 'get', 'direct', 'access', 'ensure', 'boundary', 'cross', 'set', 'boundary', 'assume', 'need', 'set', 'national', 'international', 'level', 'agree', 'mean', 'moment', 'float', 'international', 'level', 'various', 'proposal', 'new', 'oversight', 'institution', 'boundary', 'also', 'operate', 'micro', 'level', 'go', 'give', 'ai', 'bound', 'permission', 'process', 'personal', 'datum', 'give', 'answer', 'question', 'general', 'think', 'certain', 'capability', 'cautious', 'rule', 'foreseeable', 'future', 'guess', 'thing', 'recursive', 'selfimprovement', 'want', 'let', 'little', 'go', 'update', 'code', 'oversight', 'maybe', 'even', 'licensed', 'activity', 'know', 'handle', 'anthrax', 'nuclear', 'material', 'allow', 'drone', 'public', 'space', 'right', 'licensed', 'activity', 'fly', 'want', 'present', 'threat', 'people', 'privacy', 'think', 'complete', 'panic', 'go', 'able', 'regulate', 'nonsense', 'totally', 'go', 'able', 'regulate', 'apply', 'framework', 'successful', 'previously', 'see', 'drone', 'sky', 'feel', 'assume', 'company', 'go', 'reveal', 'make', 'make', 'regulation', 'tricky', 'get', 'go', 'regulate', 'many', 'thing', 'online', 'right', 'amount', 'fraud', 'criminal', 'activity', 'online', 'minimal', 'pretty', 'good', 'job', 'spam', 'know', 'general', 'problem', 'revenge', 'porn', 'get', 'well', 'even', 'bad', 'place', 'year', 'ago', 'pretty', 'difficult', 'find', 'radicalization', 'content', 'terrorist', 'material', 'online', 'pretty', 'difficult', 'buy', 'weapon', 'drug', 'online', 'suleyman', 'claim', 'back', 'number', 'cybercrime', 'still', 'massive', 'global', 'problem', 'financial', 'cost', 'alone', 'increase', 'time', 'last', 'decade', 'accord', 'estimate', 'report', 'show', 'economy', 'nonconsensual', 'deepfake', 'porn', 'boom', 'drug', 'gun', 'market', 'social', 'medium', 'online', 'platform', 'push', 'well', 'job', 'filter', 'harmful', 'content', 'lot', 'internet', 'unruly', 'space', 'govern', 'govern', 'ai', 'go', 'component', 'governance', 'take', 'combination', 'cultural', 'pressure', 'institutional', 'pressure', 'obviously', 'government', 'regulation', 'make', 'optimistic', 'control', 'ai', 'offshoot', 'internet', 'regulation', 'far', 'upbeat', 'note', 'one', 'hear', 'number', 'highprofile', 'doomer', 'lately', 'wideeye', 'risk', 'lot', 'dark', 'stuff', 'book', 'definitely', 'see', 'think', 'existentialrisk', 'stuff', 'completely', 'bonker', 'distraction', 'practical', 'issue', 'talk', 'privacy', 'bias', 'facial', 'recognition', 'online', 'moderation', 'refocus', 'conversation', 'fact', 'amazing', 'job', 'regulate', 'super', 'complex', 'thing', 'look', 'federal', 'administration', 'incredible', 'get', 'tin', 'tube', 'foot', 'safe', 'mode', 'transport', 'ever', 'celebrate', 'think', 'car', 'component', 'stressteste', 'inch', 'life', 'license', 'drive', 'industry', 'airline', 'good', 'job', 'regulate', 'start', 'know', 'nail', 'safety', 'scared', 'lose', 'business', 'need', 'topdown', 'regulation', 'love', 'nationstate', 'believe', 'public', 'interest', 'believe', 'good', 'tax', 'redistribution', 'believe', 'power', 'regulation', 'call', 'action', 'part', 'nationstate', 'sort', 'shit', 'give', 'stake', 'time', 'move']","<p>“This is a profound moment in the history of technology,” says Mustafa Suleyman.</p>
"
This driverless car company is using chatbots to make its vehicles smarter,https://www.technologyreview.com/2023/09/14/1079458/this-driverless-car-company-is-using-chatbots-to-make-its-vehicles-smarter/,2023-09-14,"Self-driving car startup Wayve can now interrogate its vehicles, asking them questions about their driving decisions—and getting answers back. The idea is to use the same tech behind ChatGPT to help train driverless cars. The company combined its existing self-driving software with a large language model, creating a hybrid model it calls LINGO-1. LINGO-1 synchs up video data and driving data (the actions that the cars take second by second) with natural-language descriptions that capture what the car sees and what it does.  The mainstream approach to driverless cars is slow and difficult. These startups think going all-in on AI will get there faster. The UK-based firm has had a string of breakthroughs in the last few years. In 2021 it showed that it could take AI trained on the streets of London and use it to drive cars in four other cities across the UK, a challenge that typically requires significant reengineering. Last year it used that same AI to drive more than one kind of vehicle, another industry first. And now it can chat to its cars. In a demo the company gave me this week, CEO Alex Kendall played footage taken from the camera on one of its Jaguar I-PACE vehicles, jumped to a random spot in the video, and started typing questions: “What’s the weather like?” The weather is cloudy. “What hazards do you see?” There is a school on the left. “Why did you stop?” Because the traffic light is red. “We saw some remarkable things come up in the last couple of weeks,” said Kendall. “I never would have thought to ask something like this, but look—” He typed: “How many stories is the building on the right?” Three stories. “Look at that!” he said, sounding like a proud dad. “We never trained it to do that. It’s really amazed us. We see this as a breakthrough in AI safety.” “I’m impressed with LINGO-1’s capabilities,” says Pieter Abbeel, a robotics researcher at the University of California, Berkeley, and cofounder of the robotics company Covariant, who has played with a demo of the tech. Abbeel asked LINGO-1 what-if questions like “What would you do if the light were green?” “Almost every time it gave a very precise answer,” he says. By quizzing the self-driving software every step of the way, Wayve hopes to understand exactly why and how its cars make certain decisions. Most of the time the cars drive fine. When they don’t, it’s a problem—as industry frontrunners like Cruise and Waymo have found. Both those firms have rolled out small fleets of robotaxis on the streets of a few US cities. But the technology is far from perfect. Cruise and Waymo’s cars have been involved in multiple minor collisions (Waymo is reported to have killed a dog) and block traffic when they get stuck. San Francisco officials have claimed that in August two Cruise vehicles got in the way of an ambulance carrying an injured person, who later died in hospital. Cruise denies the officials’ account.    Wayve hopes that asking its own cars to explain themselves when they do something wrong will uncover flaws faster than poring over video playbacks or scrolling through error reports alone. AI could help robots learn new skills and adapt to the real world quickly. “The most important challenge in self-driving is safety,” says Abbeel. “With a system like LINGO-1, I think you get a much better idea of how well it understands driving in the world.” This makes it easier to identify the weak spots, he says. The next step is to use language to teach the cars, says Kendall. To train LINGO-1, Wayve got its team of expert drivers—some of them former driving instructors—to talk out loud while driving, explaining what they were doing and why: why they sped up, why they slowed down, what hazards they were aware of. The company uses this data to fine-tune the model, giving it driving tips much as an instructor might coach a human learner. Telling a car how to do something rather than just showing it speeds up the training a lot, says Kendall. Wayve is not the first to use large language models in robotics. Other companies, including Google and Abbeel’s firm Covariant, are using natural language to quiz or instruct domestic or industrial robots. The hybrid tech even has a name: visual-language-action models (VLAMs). But Wayve is the first to use VLAMs for self-driving. “People often say an image is worth a thousand words, but in machine learning it’s the opposite,” says Kendall. “A few words can be worth a thousand images.” An image contains a lot of data that’s redundant. “When you’re driving, you don’t care about the sky, or the color of the car in front, or stuff like this,” he says. “Words can focus on the information that matters.” “Wayve’s approach is definitely interesting and unique,” says Lerrel Pinto, a robotics researcher at New York University. In particular, he likes the way LINGO-1 explains its actions. But he’s curious about what happens when the model makes stuff up. “I don’t trust large language models to be factual,” he says. “I’m not sure if I can trust them to run my car.” Upol Ehsan, a researcher at the Georgia Institute of Technology who works on ways to get AI to explain its decision-making to humans, has similar reservations. “Large language models are, to use the technical phrase, great bullshitters,” says Ehsan. “We need to apply a bright yellow ‘caution’ tape and make sure the language generated isn’t hallucinated.” Wayve is well aware of these limitations and is working to make LINGO-1 as accurate as possible. “We see the same challenges that you see in any large language model,” says Kendall. “It’s certainly not perfect.” One advantage LINGO-1 has over non-hybrid models is that its responses are grounded by the accompanying video data. In theory, this should make LINGO-1 more truthful.   This is about more than just cars, says Kendall. “There’s a reason why you and I have evolved language: it’s the most efficient way that we know of to communicate complex topics. And I think the same will be true with intelligent machines. The way that we’ll interact with robots in the future will be through language.” Abbeel agrees. “Zooming out, I think we are about to see a revolution in robotics,” he says. ","Self-driving car startup Wayve can now interrogate its vehicles , asking them questions about their driving decisions—and getting answers back . The idea is to use the same tech behind ChatGPT to help train driverless cars . The company combined its existing self-driving software with a large language model , creating a hybrid model it calls LINGO-1 . LINGO-1 synchs up video data and driving data ( the actions that the cars take second by second ) with natural-language descriptions that capture what the car sees and what it does . The mainstream approach to driverless cars is slow and difficult . These startups think going all-in on AI will get there faster . The UK-based firm has had a string of breakthroughs in the last few years . In 2021 it showed that it could take AI trained on the streets of London and use it to drive cars in four other cities across the UK , a challenge that typically requires significant reengineering . Last year it used that same AI to drive more than one kind of vehicle , another industry first . And now it can chat to its cars . In a demo the company gave me this week , CEO Alex Kendall played footage taken from the camera on one of its Jaguar I-PACE vehicles , jumped to a random spot in the video , and started typing questions : “ What ’ s the weather like ? ” The weather is cloudy . “ What hazards do you see ? ” There is a school on the left . “ Why did you stop ? ” Because the traffic light is red . “ We saw some remarkable things come up in the last couple of weeks , ” said Kendall . “ I never would have thought to ask something like this , but look— ” He typed : “ How many stories is the building on the right ? ” Three stories . “ Look at that ! ” he said , sounding like a proud dad . “ We never trained it to do that . It ’ s really amazed us . We see this as a breakthrough in AI safety. ” “ I ’ m impressed with LINGO-1 ’ s capabilities , ” says Pieter Abbeel , a robotics researcher at the University of California , Berkeley , and cofounder of the robotics company Covariant , who has played with a demo of the tech . Abbeel asked LINGO-1 what-if questions like “ What would you do if the light were green ? ” “ Almost every time it gave a very precise answer , ” he says . By quizzing the self-driving software every step of the way , Wayve hopes to understand exactly why and how its cars make certain decisions . Most of the time the cars drive fine . When they don ’ t , it ’ s a problem—as industry frontrunners like Cruise and Waymo have found . Both those firms have rolled out small fleets of robotaxis on the streets of a few US cities . But the technology is far from perfect . Cruise and Waymo ’ s cars have been involved in multiple minor collisions ( Waymo is reported to have killed a dog ) and block traffic when they get stuck . San Francisco officials have claimed that in August two Cruise vehicles got in the way of an ambulance carrying an injured person , who later died in hospital . Cruise denies the officials ’ account . Wayve hopes that asking its own cars to explain themselves when they do something wrong will uncover flaws faster than poring over video playbacks or scrolling through error reports alone . AI could help robots learn new skills and adapt to the real world quickly . “ The most important challenge in self-driving is safety , ” says Abbeel . “ With a system like LINGO-1 , I think you get a much better idea of how well it understands driving in the world. ” This makes it easier to identify the weak spots , he says . The next step is to use language to teach the cars , says Kendall . To train LINGO-1 , Wayve got its team of expert drivers—some of them former driving instructors—to talk out loud while driving , explaining what they were doing and why : why they sped up , why they slowed down , what hazards they were aware of . The company uses this data to fine-tune the model , giving it driving tips much as an instructor might coach a human learner . Telling a car how to do something rather than just showing it speeds up the training a lot , says Kendall . Wayve is not the first to use large language models in robotics . Other companies , including Google and Abbeel ’ s firm Covariant , are using natural language to quiz or instruct domestic or industrial robots . The hybrid tech even has a name : visual-language-action models ( VLAMs ) . But Wayve is the first to use VLAMs for self-driving . “ People often say an image is worth a thousand words , but in machine learning it ’ s the opposite , ” says Kendall . “ A few words can be worth a thousand images. ” An image contains a lot of data that ’ s redundant . “ When you ’ re driving , you don ’ t care about the sky , or the color of the car in front , or stuff like this , ” he says . “ Words can focus on the information that matters. ” “ Wayve ’ s approach is definitely interesting and unique , ” says Lerrel Pinto , a robotics researcher at New York University . In particular , he likes the way LINGO-1 explains its actions . But he ’ s curious about what happens when the model makes stuff up . “ I don ’ t trust large language models to be factual , ” he says . “ I ’ m not sure if I can trust them to run my car. ” Upol Ehsan , a researcher at the Georgia Institute of Technology who works on ways to get AI to explain its decision-making to humans , has similar reservations . “ Large language models are , to use the technical phrase , great bullshitters , ” says Ehsan . “ We need to apply a bright yellow ‘ caution ’ tape and make sure the language generated isn ’ t hallucinated. ” Wayve is well aware of these limitations and is working to make LINGO-1 as accurate as possible . “ We see the same challenges that you see in any large language model , ” says Kendall . “ It ’ s certainly not perfect. ” One advantage LINGO-1 has over non-hybrid models is that its responses are grounded by the accompanying video data . In theory , this should make LINGO-1 more truthful . This is about more than just cars , says Kendall . “ There ’ s a reason why you and I have evolved language : it ’ s the most efficient way that we know of to communicate complex topics . And I think the same will be true with intelligent machines . The way that we ’ ll interact with robots in the future will be through language. ” Abbeel agrees . “ Zooming out , I think we are about to see a revolution in robotics , ” he says .","['selfdrive', 'car', 'startup', 'wayve', 'interrogate', 'vehicle', 'ask', 'question', 'drive', 'decision', 'get', 'answer', 'back', 'idea', 'use', 'tech', 'chatgpt', 'help', 'train', 'driverless', 'car', 'company', 'combine', 'exist', 'selfdriving', 'software', 'large', 'language', 'model', 'create', 'hybrid', 'model', 'call', 'lingo1', 'synch', 'video', 'datum', 'drive', 'datum', 'action', 'car', 'take', 'second', 'second', 'naturallanguage', 'description', 'capture', 'car', 'see', 'mainstream', 'approach', 'driverless', 'car', 'slow', 'difficult', 'startup', 'think', 'go', 'get', 'fast', 'ukbased', 'firm', 'string', 'breakthrough', 'last', 'year', 'show', 'take', 'train', 'street', 'use', 'drive', 'car', 'city', 'challenge', 'typically', 'require', 'significant', 'reengineering', 'last', 'year', 'use', 'ai', 'drive', 'kind', 'vehicle', 'industry', 'first', 'chat', 'car', 'demo', 'company', 'give', 'week', 'ceo', 'play', 'footage', 'take', 'camera', 'jaguar', 'ipace', 'vehicle', 'jump', 'random', 'spot', 'video', 'start', 'type', 'question', 'weather', 'weather', 'cloudy', 'hazard', 'see', 'school', 'left', 'stop', 'traffic', 'light', 'red', 'see', 'remarkable', 'thing', 'come', 'last', 'couple', 'week', 'say', 'kendall', 'never', 'think', 'ask', 'look', 'type', 'many', 'story', 'building', 'right', 'story', 'look', 'say', 'sound', 'proud', 'dad', 'never', 'train', 'really', 'amazed', 'see', 'breakthrough', 'safety', 'impressed', 'capability', 'say', 'pieter', 'abbeel', 'robotic', 'researcher', 'cofounder', 'robotic', 'company', 'covariant', 'play', 'demo', 'tech', 'abbeel', 'ask', 'question', 'light', 'green', 'almost', 'time', 'give', 'precise', 'answer', 'say', 'quiz', 'selfdriving', 'software', 'step', 'way', 'wayve', 'hope', 'understand', 'exactly', 'car', 'make', 'certain', 'decision', 'time', 'car', 'drive', 'fine', 'problem', 'industry', 'frontrunner', 'cruise', 'waymo', 'find', 'firm', 'roll', 'small', 'fleet', 'robotaxis', 'street', 'city', 'technology', 'far', 'perfect', 'cruise', 'waymo', 'car', 'involve', 'multiple', 'minor', 'collision', 'waymo', 'report', 'kill', 'dog', 'block', 'traffic', 'stick', 'official', 'claim', 'cruise', 'vehicle', 'get', 'way', 'ambulance', 'carry', 'injure', 'person', 'later', 'die', 'hospital', 'cruise', 'deny', 'official', 'account', 'wayve', 'hope', 'ask', 'car', 'explain', 'wrong', 'uncover', 'flaw', 'fast', 'pore', 'video', 'playback', 'scroll', 'error', 'report', 'alone', 'help', 'robot', 'learn', 'new', 'skill', 'adapt', 'real', 'world', 'quickly', 'important', 'challenge', 'selfdriving', 'safety', 'say', 'abbeel', 'system', 'lingo1', 'think', 'get', 'much', 'well', 'idea', 'well', 'understand', 'drive', 'world', 'make', 'easy', 'identify', 'weak', 'spot', 'say', 'next', 'step', 'use', 'language', 'teach', 'car', 'say', 'kendall', 'train', 'wayve', 'get', 'team', 'expert', 'driver', 'former', 'drive', 'instructor', 'talk', 'loud', 'drive', 'explain', 'speed', 'slow', 'hazard', 'aware', 'company', 'use', 'datum', 'finetune', 'model', 'give', 'drive', 'tip', 'much', 'instructor', 'coach', 'human', 'learner', 'tell', 'car', 'rather', 'show', 'speed', 'training', 'lot', 'say', 'kendall', 'wayve', 'first', 'use', 'large', 'language', 'model', 'robotic', 'company', 'include', 'firm', 'covariant', 'use', 'natural', 'language', 'quiz', 'instruct', 'domestic', 'industrial', 'robot', 'hybrid', 'tech', 'even', 'name', 'visuallanguageaction', 'model', 'vlam', 'wayve', 'first', 'use', 'vlam', 'selfdrive', 'people', 'often', 'say', 'image', 'worth', 'word', 'machine', 'learn', 'opposite', 'say', 'kendall', 'word', 'worth', 'image', 'image', 'contain', 'lot', 'datum', 'redundant', 'drive', 'care', 'sky', 'color', 'car', 'front', 'stuff', 'say', 'word', 'focus', 'information', 'matter', 'wayve', 'approach', 'definitely', 'interesting', 'unique', 'say', 'robotic', 'researcher', 'particular', 'like', 'way', 'explain', 'action', 'curious', 'happen', 'model', 'make', 'stuff', 'trust', 'large', 'language', 'model', 'factual', 'say', 'sure', 'trust', 'run', 'car', 'researcher', 'technology', 'work', 'way', 'get', 'ai', 'explain', 'decisionmaking', 'human', 'similar', 'reservation', 'large', 'language', 'model', 'use', 'technical', 'phrase', 'great', 'bullshitter', 'say', 'need', 'apply', 'bright', 'yellow', 'caution', 'tape', 'make', 'sure', 'language', 'generate', 'hallucinate', 'wayve', 'well', 'aware', 'limitation', 'work', 'make', 'lingo1', 'accurate', 'possible', 'see', 'challenge', 'see', 'large', 'language', 'model', 'say', 'kendall', 'certainly', 'perfect', 'advantage', 'lingo1', 'nonhybrid', 'model', 'response', 'ground', 'accompany', 'video', 'datum', 'theory', 'make', 'lingo1', 'truthful', 'car', 'say', 'kendall', 'reason', 'evolve', 'language', 'efficient', 'way', 'know', 'communicate', 'complex', 'topic', 'think', 'true', 'intelligent', 'machine', 'way', 'interact', 'robot', 'future', 'language', 'abbeel', 'agree', 'zoom', 'think', 'see', 'revolution', 'robotic', 'say']","<p>Large language models are the next big thing for robotics, making cars and other robots quicker to train and easier to control (if you trust them).</p>
"
AI just beat a human test for creativity. What does that even mean?,https://www.technologyreview.com/2023/09/14/1079465/ai-just-beat-a-human-test-for-creativity-what-does-that-even-mean/,2023-09-14,"AI is getting better at passing tests designed to measure human creativity. In a study published in Scientific Reports today, AI chatbots achieved higher average scores than humans in the Alternate Uses Task, a test commonly used to assess this ability.  This study will add fuel to an ongoing debate among AI researchers about what it even means for a computer to pass tests devised for humans. The findings do not necessarily indicate that AIs are developing an ability to do something uniquely human. It could just be that AIs can pass creativity tests, not that they’re actually creative in the way we understand. However, research like this might give us a better understanding of how humans and machines approach creative tasks. Researchers started by asking three AI chatbots—OpenAI’s ChatGPT and GPT-4 as well as Copy.Ai, which is built on GPT-3—to come up with as many uses for a rope, a box, a pencil, and a candle as possible within just 30 seconds.  Their prompts instructed the large language models to come up with original and creative uses for each of the items, explaining that the quality of the ideas was more important than the quantity. Each chatbot was tested 11 times for each of the four objects. The researchers also gave 256 human participants the same instructions. The researchers used two methods to assess both AI and human responses. The first was an algorithm that rated how closely the suggested use for the object was to the object’s original purpose. The second involved asking six human assessors (who were unaware that some of the answers had been generated by AI systems) to evaluate each response on a scale of 1 to 5 in terms of how creative and original it was—1 being not at all, and 5 being very. Average scores for both humans and AIs were then calculated.  Although the chatbots’ responses were rated as better than the humans’ on average, the best-scoring human responses were higher. With hopes and fears about the technology running wild, it's time to agree on what it can and can't do. While the purpose of the study was not to prove that AI systems are capable of replacing humans in creative roles, it raises philosophical questions about the characteristics that are unique to humans, says Simone Grassini, an associate professor of psychology at the University of Bergen, Norway, who co-led the research. “We’ve shown that in the past few years, technology has taken a very big leap forward when we talk about imitating human behavior,” he says. “These models are continuously evolving.”  Proving that machines can perform well in tasks designed for measuring creativity in humans doesn’t demonstrate that they’re capable of anything approaching original thought, says Ryan Burnell, a senior research associate at the Alan Turing Institute, who was not involved with the research. The chatbots that were tested are “black boxes,” meaning that we don’t know exactly what data they were trained on, or how they generate their responses, he says. “What’s very plausibly happening here is that a model wasn’t coming up with new creative ideas—it was just drawing on things it’s seen in its training data, which could include this exact Alternate Uses Task,” he explains. “In that case, we’re not measuring creativity. We’re measuring the model’s past knowledge of this kind of task.” That doesn’t mean that it’s not still useful to compare how machines and humans approach certain problems, says Anna Ivanova, an MIT postdoctoral researcher studying language models, who did not work on the project.  However, we should bear in mind that although chatbots are very good at completing specific requests, slight tweaks like rephrasing a prompt can be enough to stop them from performing as well, she says. Ivanova believes that these kinds of studies should prompt us to examine the link between the task we’re asking AI models to complete and the cognitive capacity we’re trying to measure. “We shouldn’t assume that people and models solve problems in the same way,” she says. ","AI is getting better at passing tests designed to measure human creativity . In a study published in Scientific Reports today , AI chatbots achieved higher average scores than humans in the Alternate Uses Task , a test commonly used to assess this ability . This study will add fuel to an ongoing debate among AI researchers about what it even means for a computer to pass tests devised for humans . The findings do not necessarily indicate that AIs are developing an ability to do something uniquely human . It could just be that AIs can pass creativity tests , not that they ’ re actually creative in the way we understand . However , research like this might give us a better understanding of how humans and machines approach creative tasks . Researchers started by asking three AI chatbots—OpenAI ’ s ChatGPT and GPT-4 as well as Copy.Ai , which is built on GPT-3—to come up with as many uses for a rope , a box , a pencil , and a candle as possible within just 30 seconds . Their prompts instructed the large language models to come up with original and creative uses for each of the items , explaining that the quality of the ideas was more important than the quantity . Each chatbot was tested 11 times for each of the four objects . The researchers also gave 256 human participants the same instructions . The researchers used two methods to assess both AI and human responses . The first was an algorithm that rated how closely the suggested use for the object was to the object ’ s original purpose . The second involved asking six human assessors ( who were unaware that some of the answers had been generated by AI systems ) to evaluate each response on a scale of 1 to 5 in terms of how creative and original it was—1 being not at all , and 5 being very . Average scores for both humans and AIs were then calculated . Although the chatbots ’ responses were rated as better than the humans ’ on average , the best-scoring human responses were higher . With hopes and fears about the technology running wild , it 's time to agree on what it can and ca n't do . While the purpose of the study was not to prove that AI systems are capable of replacing humans in creative roles , it raises philosophical questions about the characteristics that are unique to humans , says Simone Grassini , an associate professor of psychology at the University of Bergen , Norway , who co-led the research . “ We ’ ve shown that in the past few years , technology has taken a very big leap forward when we talk about imitating human behavior , ” he says . “ These models are continuously evolving. ” Proving that machines can perform well in tasks designed for measuring creativity in humans doesn ’ t demonstrate that they ’ re capable of anything approaching original thought , says Ryan Burnell , a senior research associate at the Alan Turing Institute , who was not involved with the research . The chatbots that were tested are “ black boxes , ” meaning that we don ’ t know exactly what data they were trained on , or how they generate their responses , he says . “ What ’ s very plausibly happening here is that a model wasn ’ t coming up with new creative ideas—it was just drawing on things it ’ s seen in its training data , which could include this exact Alternate Uses Task , ” he explains . “ In that case , we ’ re not measuring creativity . We ’ re measuring the model ’ s past knowledge of this kind of task. ” That doesn ’ t mean that it ’ s not still useful to compare how machines and humans approach certain problems , says Anna Ivanova , an MIT postdoctoral researcher studying language models , who did not work on the project . However , we should bear in mind that although chatbots are very good at completing specific requests , slight tweaks like rephrasing a prompt can be enough to stop them from performing as well , she says . Ivanova believes that these kinds of studies should prompt us to examine the link between the task we ’ re asking AI models to complete and the cognitive capacity we ’ re trying to measure . “ We shouldn ’ t assume that people and models solve problems in the same way , ” she says .","['get', 'well', 'pass', 'test', 'design', 'measure', 'human', 'creativity', 'study', 'publish', 'scientific', 'report', 'today', 'ai', 'chatbot', 'achieve', 'high', 'average', 'score', 'human', 'alternate', 'use', 'task', 'test', 'commonly', 'use', 'assess', 'ability', 'study', 'add', 'fuel', 'ongoing', 'debate', 'ai', 'researcher', 'even', 'mean', 'computer', 'pass', 'test', 'devise', 'human', 'finding', 'necessarily', 'indicate', 'develop', 'ability', 'uniquely', 'human', 'pass', 'creativity', 'test', 'actually', 'creative', 'way', 'understand', 'however', 'research', 'give', 'well', 'understanding', 'human', 'machine', 'approach', 'creative', 'task', 'researcher', 'start', 'ask', 'chatbot', 'chatgpt', 'gpt4', 'well', 'build', 'come', 'many', 'use', 'rope', 'box', 'pencil', 'candle', 'possible', 'second', 'prompt', 'instruct', 'large', 'language', 'model', 'come', 'original', 'creative', 'use', 'item', 'explain', 'quality', 'idea', 'important', 'quantity', 'chatbot', 'test', 'time', 'object', 'researcher', 'also', 'give', 'human', 'participant', 'instruction', 'researcher', 'use', 'method', 'assess', 'ai', 'human', 'response', 'first', 'rate', 'closely', 'suggest', 'use', 'object', 'object', 'original', 'purpose', 'second', 'involved', 'ask', 'human', 'assessor', 'unaware', 'answer', 'generate', 'system', 'evaluate', 'response', 'scale', 'term', 'creative', 'original', 'was—1', 'average', 'score', 'human', 'calculate', 'chatbot', 'response', 'rate', 'well', 'human', 'average', 'bestscore', 'human', 'response', 'high', 'hope', 'fear', 'technology', 'run', 'wild', 'time', 'agree', 'purpose', 'study', 'prove', 'system', 'capable', 'replace', 'human', 'creative', 'role', 'raise', 'philosophical', 'question', 'characteristic', 'unique', 'human', 'say', 'simone', 'grassini', 'associate', 'professor', 'psychology', 'norway', 'cole', 'research', 'show', 'past', 'year', 'technology', 'take', 'big', 'leap', 'forward', 'talk', 'imitate', 'human', 'behavior', 'say', 'model', 'continuously', 'evolve', 'prove', 'machine', 'perform', 'well', 'task', 'design', 'measure', 'creativity', 'human', 'demonstrate', 'capable', 'approach', 'original', 'thought', 'say', 'senior', 'research', 'associate', 'ture', 'involve', 'research', 'chatbot', 'test', 'black', 'box', 'mean', 'know', 'exactly', 'datum', 'train', 'generate', 'response', 'say', 'plausibly', 'happen', 'model', 'come', 'new', 'creative', 'idea', 'draw', 'thing', 'see', 'training', 'datum', 'include', 'exact', 'alternate', 'use', 'task', 'explain', 'case', 'measure', 'creativity', 'measure', 'model', 'past', 'knowledge', 'kind', 'task', 'mean', 'still', 'useful', 'compare', 'machine', 'human', 'approach', 'certain', 'problem', 'say', 'mit', 'postdoctoral', 'researcher', 'study', 'language', 'model', 'work', 'project', 'however', 'bear', 'mind', 'chatbot', 'good', 'complete', 'specific', 'request', 'slight', 'tweak', 'rephrase', 'prompt', 'enough', 'stop', 'perform', 'well', 'say', 'believe', 'kind', 'study', 'prompt', 'examine', 'link', 'task', 'ask', 'model', 'complete', 'cognitive', 'capacity', 'try', 'measure', 'assume', 'people', 'model', 'solve', 'problem', 'way', 'say']","<p>Large language models are getting better at mimicking human creativity. That doesn’t mean they’re actually being creative, though.</p>
"
"2023 Innovator of the Year: As AI models are released into the wild, Sharon Li wants to ensure they’re safe",https://www.technologyreview.com/2023/09/12/1078351/sharon-li-ai-innovation/,2023-09-12,"Sharon Li is MIT Technology Review’s 2023 Innovator of the Year. Meet the rest of this year's Innovators Under 35.  As we launch AI systems from the lab into the real world, we need to be prepared for these systems to break in surprising and catastrophic ways. It’s already happening. Last year, for example, a chess-playing robot arm in Moscow fractured the finger of a seven-year-old boy. The robot grabbed the boy’s finger as he was moving a chess piece and let go only after nearby adults managed to pry open its claws.  This did not happen because the robot was programmed to do harm. It was because the robot was overly confident that the boy’s finger was a chess piece.   The incident is a classic example of something Sharon Li, 32, wants to prevent. Li, an assistant professor at the University of Wisconsin, Madison, is a pioneer in an AI safety feature called out-of-distribution (OOD) detection. This feature, she says, helps AI models determine when they should abstain from action if faced with something they weren’t trained on.  Li developed one of the first algorithms on out-of-distribution detection for deep neural networks. Google has since set up a dedicated team to integrate OOD detection into its products. Last year, Li’s theoretical analysis of OOD detection was chosen from over 10,000 submissions as an outstanding paper by NeurIPS, one of the most prestigious AI conferences. We’re currently in an AI gold rush, and tech companies are racing to release their AI models. But most of today’s models are trained to identify specific things and often fail when they encounter the unfamiliar scenarios typical of the messy, unpredictable real world. Their inability to reliably understand what they “know” and what they don’t “know” is the weakness behind many AI disasters.  Li’s work calls on the AI community to rethink its approach to training. “A lot of the classic approaches that have been in place over the last 50 years are actually safety unaware,” she says.  Her approach embraces uncertainty by using machine learning to detect unknown data out in the world and design AI models to adjust to it on the fly. Out-of-distribution detection could help prevent accidents when autonomous cars run into unfamiliar objects on the road, or make medical AI systems more useful in finding a new disease.  “In all those situations, what we really need [is a] safety-aware machine learning model that’s able to identify what it doesn’t know,” says Li.  This approach could also aid today’s buzziest AI technology, large language models such as ChatGPT. These models are often confident liars, presenting falsehoods as facts. This is where OOD detection could help. Say a person asks a chatbot a question it doesn’t have an answer to in its training data. Instead of making something up, an AI model using OOD detection would decline to answer.  Tips for aspiring innovators on trying, failing, and the future of AI. Li’s research tackles one of the most fundamental questions in machine learning, says John Hopcroft, a professor at Cornell University, who was her PhD advisor.  Her work has also seen a surge of interest from other researchers. “What she is doing is getting other researchers to work,” says Hopcroft, who adds that she’s “basically created one of the subfields” of AI safety research. Now, Li is seeking a deeper understanding of the safety risks relating to large AI models, which are powering all kinds of new online applications and products. She hopes that by making the models underlying these products safer, we’ll be better able to mitigate AI’s risks.  “The ultimate goal is to ensure trustworthy, safe machine learning,” she says.  Sharon Li is one of MIT Technology Review’s 2023 Innovators Under 35. Meet the rest of this year’s honorees. ","Sharon Li is MIT Technology Review ’ s 2023 Innovator of the Year . Meet the rest of this year 's Innovators Under 35 . As we launch AI systems from the lab into the real world , we need to be prepared for these systems to break in surprising and catastrophic ways . It ’ s already happening . Last year , for example , a chess-playing robot arm in Moscow fractured the finger of a seven-year-old boy . The robot grabbed the boy ’ s finger as he was moving a chess piece and let go only after nearby adults managed to pry open its claws . This did not happen because the robot was programmed to do harm . It was because the robot was overly confident that the boy ’ s finger was a chess piece . The incident is a classic example of something Sharon Li , 32 , wants to prevent . Li , an assistant professor at the University of Wisconsin , Madison , is a pioneer in an AI safety feature called out-of-distribution ( OOD ) detection . This feature , she says , helps AI models determine when they should abstain from action if faced with something they weren ’ t trained on . Li developed one of the first algorithms on out-of-distribution detection for deep neural networks . Google has since set up a dedicated team to integrate OOD detection into its products . Last year , Li ’ s theoretical analysis of OOD detection was chosen from over 10,000 submissions as an outstanding paper by NeurIPS , one of the most prestigious AI conferences . We ’ re currently in an AI gold rush , and tech companies are racing to release their AI models . But most of today ’ s models are trained to identify specific things and often fail when they encounter the unfamiliar scenarios typical of the messy , unpredictable real world . Their inability to reliably understand what they “ know ” and what they don ’ t “ know ” is the weakness behind many AI disasters . Li ’ s work calls on the AI community to rethink its approach to training . “ A lot of the classic approaches that have been in place over the last 50 years are actually safety unaware , ” she says . Her approach embraces uncertainty by using machine learning to detect unknown data out in the world and design AI models to adjust to it on the fly . Out-of-distribution detection could help prevent accidents when autonomous cars run into unfamiliar objects on the road , or make medical AI systems more useful in finding a new disease . “ In all those situations , what we really need [ is a ] safety-aware machine learning model that ’ s able to identify what it doesn ’ t know , ” says Li . This approach could also aid today ’ s buzziest AI technology , large language models such as ChatGPT . These models are often confident liars , presenting falsehoods as facts . This is where OOD detection could help . Say a person asks a chatbot a question it doesn ’ t have an answer to in its training data . Instead of making something up , an AI model using OOD detection would decline to answer . Tips for aspiring innovators on trying , failing , and the future of AI . Li ’ s research tackles one of the most fundamental questions in machine learning , says John Hopcroft , a professor at Cornell University , who was her PhD advisor . Her work has also seen a surge of interest from other researchers . “ What she is doing is getting other researchers to work , ” says Hopcroft , who adds that she ’ s “ basically created one of the subfields ” of AI safety research . Now , Li is seeking a deeper understanding of the safety risks relating to large AI models , which are powering all kinds of new online applications and products . She hopes that by making the models underlying these products safer , we ’ ll be better able to mitigate AI ’ s risks . “ The ultimate goal is to ensure trustworthy , safe machine learning , ” she says . Sharon Li is one of MIT Technology Review ’ s 2023 Innovators Under 35 . Meet the rest of this year ’ s honorees .","['mit', 'technology', 'review', 'innovator', 'year', 'meet', 'rest', 'year', 'innovator', 'launch', 'system', 'lab', 'real', 'world', 'need', 'prepare', 'system', 'break', 'surprising', 'catastrophic', 'way', 'already', 'happen', 'last', 'year', 'example', 'chessplaye', 'robot', 'arm', 'fracture', 'finger', 'sevenyearold', 'boy', 'robot', 'grab', 'boy', 'finger', 'move', 'chess', 'piece', 'let', 'go', 'nearby', 'adult', 'manage', 'open', 'claw', 'happen', 'robot', 'program', 'harm', 'robot', 'overly', 'confident', 'boy', 'finger', 'chess', 'piece', 'incident', 'classic', 'example', 'want', 'prevent', 'assistant', 'professor', 'pioneer', 'safety', 'feature', 'call', 'outofdistribution', 'ood', 'detection', 'feature', 'say', 'help', 'model', 'determine', 'abstain', 'action', 'face', 'train', 'develop', 'first', 'algorithm', 'outofdistribution', 'detection', 'deep', 'neural', 'network', 'set', 'dedicated', 'team', 'integrate', 'ood', 'detection', 'product', 'last', 'year', 'theoretical', 'analysis', 'ood', 'detection', 'choose', 'submission', 'outstanding', 'paper', 'prestigious', 'ai', 'conference', 'currently', 'ai', 'gold', 'rush', 'tech', 'company', 'race', 'release', 'model', 'today', 'model', 'train', 'identify', 'specific', 'thing', 'often', 'fail', 'encounter', 'unfamiliar', 'scenario', 'typical', 'messy', 'unpredictable', 'real', 'world', 'inability', 'reliably', 'understand', 'know', 'know', 'weakness', 'many', 'ai', 'disaster', 'work', 'call', 'community', 'rethink', 'approach', 'train', 'lot', 'classic', 'approach', 'place', 'last', 'year', 'actually', 'safety', 'unaware', 'say', 'approach', 'embrace', 'uncertainty', 'use', 'machine', 'learning', 'detect', 'unknown', 'datum', 'world', 'design', 'ai', 'model', 'adjust', 'fly', 'outofdistribution', 'detection', 'help', 'prevent', 'accident', 'autonomous', 'car', 'run', 'unfamiliar', 'object', 'road', 'make', 'medical', 'ai', 'system', 'useful', 'find', 'new', 'disease', 'situation', 'really', 'need', 'safetyaware', 'machine', 'learning', 'model', 'able', 'identify', 'say', 'approach', 'also', 'aid', 'today', 'buzzi', 'ai', 'technology', 'large', 'language', 'model', 'chatgpt', 'model', 'often', 'confident', 'liar', 'present', 'falsehood', 'fact', 'detection', 'help', 'say', 'person', 'ask', 'chatbot', 'question', 'answer', 'training', 'datum', 'instead', 'make', 'model', 'use', 'ood', 'detection', 'decline', 'answer', 'tip', 'aspire', 'innovator', 'try', 'fail', 'future', 'ai', 'research', 'tackle', 'fundamental', 'question', 'machine', 'learning', 'say', 'professor', 'phd', 'advisor', 'work', 'also', 'see', 'surge', 'interest', 'researcher', 'get', 'researcher', 'work', 'say', 'add', 'basically', 'create', 'subfield', 'safety', 'research', 'seek', 'deep', 'understanding', 'safety', 'risk', 'relate', 'large', 'ai', 'model', 'power', 'kind', 'new', 'online', 'application', 'product', 'hope', 'make', 'model', 'underlie', 'product', 'safe', 'well', 'able', 'mitigate', 'ai', 'risk', 'ultimate', 'goal', 'ensure', 'trustworthy', 'safe', 'machine', 'learn', 'say', 'mit', 'technology', 'review', 'innovator', 'meet', 'rest', 'year', 'honoree']","<p>Li’s research could prevent AI models from failing catastrophically when they encounter unfamiliar scenarios.</p>
"
Andrew Ng: How to be an innovator,https://www.technologyreview.com/2023/09/12/1078367/andrew-ng-innovator-ai/,2023-09-12,"This essay is part of MIT Technology Review’s 2023 Innovators Under 35 package. Meet this year’s honorees. Innovation is a powerful engine for uplifting society and fueling economic growth. Antibiotics, electric lights, refrigerators, airplanes, smartphones—we have these things because innovators created something that didn’t exist before. MIT Technology Review’s Innovators Under 35 list celebrates individuals who have accomplished a lot early in their careers and are likely to accomplish much more still.  Having spent many years working on AI research and building AI products, I’m fortunate to have participated in a few innovations that made an impact, like using reinforcement learning to fly helicopter drones at Stanford, starting and leading Google Brain to drive large-scale deep learning, and creating online courses that led to the founding of Coursera. I’d like to share some thoughts about how to do it well, sidestep some of the pitfalls, and avoid building things that lead to serious harm along the way. As I have said before, I believe AI is the new electricity. Electricity revolutionized all industries and changed our way of life, and AI is doing the same. It’s reaching into every industry and discipline, and it’s yielding advances that help multitudes of people. AI—like electricity—is a general-­purpose technology. Many innovations, such as a medical treatment, space rocket, or battery design, are fit for one purpose. In contrast, AI is useful for generating art, serving web pages that are relevant to a search query, optimizing shipping routes to save fuel, helping cars avoid collisions, and much more.  The advance of AI creates opportunities for everyone in all corners of the economy to explore whether or how it applies to their area. Thus, learning about AI creates disproportionately many opportunities to do something that no one else has ever done before. For instance, at AI Fund, a venture studio that I lead, I’ve been privileged to participate in projects that apply AI to maritime shipping, relationship coaching, talent management, education, and other areas. Because many AI technologies are new, their application to most domains has not yet been explored. In this way, knowing how to take advantage of AI gives you numerous opportunities to collaborate with others.  Looking ahead, a few developments are especially exciting. These areas offer rich opportunities for innovators. Moreover, many of them are within reach of broadly tech-savvy people, not just people already in AI. Online courses, open-source software, software as a service, and online research papers give everyone tools to learn and start innovating. But even if these technologies aren’t yet within your grasp, many other paths to innovation are wide open. That said, a lot of ideas that initially seem promising turn out to be duds. Duds are unavoidable if you take innovation seriously. Here are some projects of mine that you probably haven’t heard of, because they were duds:  It was painful when these projects didn’t succeed, but the lessons I learned turned out to be instrumental for other projects that fared better. Through my failed attempt at V-shape flying, I learned to plan projects much better and front-load risks. The effort to unload dishwashers failed, but it led my team to build the Robot Operating System (ROS), which became a popular open-source framework that’s now in robots from self-driving cars to mechanical dogs. Even though my initial focus on unsupervised learning was a poor choice, the steps we took turned out to be critical in scaling up deep learning at Google Brain. Society has a deep interest in the fruits of innovation. And that is a good reason to approach innovation with optimism. Innovation has never been easy. When you do something new, there will be skeptics. In my younger days, I faced a lot of skepticism when starting most of the projects that ultimately proved to be successful. But this is not to say the skeptics are always wrong. I faced skepticism for most of the unsuccessful projects as well. As I became more experienced, I found that more and more people would agree with whatever I said, and that was even more worrying. I had to actively seek out people who would challenge me and tell me the truth. Luckily, these days I am surrounded by people who will tell me when they think I’m doing something dumb!  While skepticism is healthy and even necessary, society has a deep interest in the fruits of innovation. And that is a good reason to approach innovation with optimism. I’d rather side with the optimist who wants to give it a shot and might fail than the pessimist who doubts what’s possible.  As we focus on AI as a driver of valuable innovation throughout society, social responsibility is more important than ever. People both inside and outside the field see a wide range of possible harms AI may cause. These include both short-term issues, such as bias and harmful applications of the technology, and long-term risks, such as concentration of power and potentially catastrophic applications. It’s important to have open and intellectually rigorous conversations about them. In that way, we can come to an agreement on what the real risks are and how to reduce them. Over the past millennium, successive waves of innovation have reduced infant mortality, improved nutrition, boosted literacy, raised standards of living worldwide, and fostered civil rights including protections for women, minorities, and other marginalized groups. Yet innovations have also contributed to climate change, spurred rising inequality, polarized society, and increased loneliness.  Li’s research could prevent AI models from failing catastrophically when they encounter unfamiliar scenarios. Clearly, the benefits of innovation come with risks, and we have not always managed them wisely. AI is the next wave, and we have an obligation to learn lessons from the past to maximize future benefits for everyone and minimize harm. This will require commitment from both individuals and society at large.  At the social level, governments are moving to regulate AI. To some innovators, regulation may look like an unnecessary restraint on progress. I see it differently. Regulation helps us avoid mistakes and enables new benefits as we move into an uncertain future. I welcome regulation that calls for more transparency into the opaque workings of large tech companies; this will help us understand their impact and steer them toward achieving broader societal benefits. Moreover, new regulations are needed because many existing ones were written for a pre-AI world. The new regulations should specify the outcomes we want in important areas like health care and finance—and those we do not want.  But avoiding harm shouldn’t be just a priority for society. It also needs to be a priority for each innovator. As technologists, we have a responsibility to understand the implications of our research and innovate in ways that are beneficial. Traditionally, many technologists adopted the attitude that the shape technology takes is inevitable and there’s nothing we can do about it, so we might as well innovate freely. But we know that’s not true.  Avoiding harm shouldn’t be just a priority for society. It also needs to be a priority for each innovator.  When innovators choose to work on differential privacy (which allows AI to learn from data without exposing personally identifying information), they make a powerful statement that privacy matters. That statement helps shape the social norms adopted by public and private institutions. Conversely, when innovators create Web3 cryptographic protocols to launder money, that too creates a powerful statement—in my view, a harmful one—that governments should not be able to trace how funds are transferred and spent.  If you see something unethical being done, I hope you’ll raise it with your colleagues and supervisors and engage them in constructive conversations. And if you are asked to work on something that you don’t think helps humanity, I hope you’ll actively work to put a stop to it. If you are unable to do so, then consider walking away. At AI Fund, I have killed projects that I assessed to be financially sound but ethically unsound. I urge you to do the same.  Now, go forth and innovate! If you’re already in the innovation game, keep at it. There’s no telling what great accomplishment lies in your future. If your ideas are in the daydream stage, share them with others and get help to shape them into something practical and successful. Start executing, and find ways to use the power of innovation for good.  This essay is part of MIT Technology Review’s 2023 Innovators Under 35 package. Meet this year’s honorees. Andrew Ng is a renowned global AI innovator. He leads AI Fund, DeepLearning.AI, and Landing AI. ","This essay is part of MIT Technology Review ’ s 2023 Innovators Under 35 package . Meet this year ’ s honorees . Innovation is a powerful engine for uplifting society and fueling economic growth . Antibiotics , electric lights , refrigerators , airplanes , smartphones—we have these things because innovators created something that didn ’ t exist before . MIT Technology Review ’ s Innovators Under 35 list celebrates individuals who have accomplished a lot early in their careers and are likely to accomplish much more still . Having spent many years working on AI research and building AI products , I ’ m fortunate to have participated in a few innovations that made an impact , like using reinforcement learning to fly helicopter drones at Stanford , starting and leading Google Brain to drive large-scale deep learning , and creating online courses that led to the founding of Coursera . I ’ d like to share some thoughts about how to do it well , sidestep some of the pitfalls , and avoid building things that lead to serious harm along the way . As I have said before , I believe AI is the new electricity . Electricity revolutionized all industries and changed our way of life , and AI is doing the same . It ’ s reaching into every industry and discipline , and it ’ s yielding advances that help multitudes of people . AI—like electricity—is a general-­purpose technology . Many innovations , such as a medical treatment , space rocket , or battery design , are fit for one purpose . In contrast , AI is useful for generating art , serving web pages that are relevant to a search query , optimizing shipping routes to save fuel , helping cars avoid collisions , and much more . The advance of AI creates opportunities for everyone in all corners of the economy to explore whether or how it applies to their area . Thus , learning about AI creates disproportionately many opportunities to do something that no one else has ever done before . For instance , at AI Fund , a venture studio that I lead , I ’ ve been privileged to participate in projects that apply AI to maritime shipping , relationship coaching , talent management , education , and other areas . Because many AI technologies are new , their application to most domains has not yet been explored . In this way , knowing how to take advantage of AI gives you numerous opportunities to collaborate with others . Looking ahead , a few developments are especially exciting . These areas offer rich opportunities for innovators . Moreover , many of them are within reach of broadly tech-savvy people , not just people already in AI . Online courses , open-source software , software as a service , and online research papers give everyone tools to learn and start innovating . But even if these technologies aren ’ t yet within your grasp , many other paths to innovation are wide open . That said , a lot of ideas that initially seem promising turn out to be duds . Duds are unavoidable if you take innovation seriously . Here are some projects of mine that you probably haven ’ t heard of , because they were duds : It was painful when these projects didn ’ t succeed , but the lessons I learned turned out to be instrumental for other projects that fared better . Through my failed attempt at V-shape flying , I learned to plan projects much better and front-load risks . The effort to unload dishwashers failed , but it led my team to build the Robot Operating System ( ROS ) , which became a popular open-source framework that ’ s now in robots from self-driving cars to mechanical dogs . Even though my initial focus on unsupervised learning was a poor choice , the steps we took turned out to be critical in scaling up deep learning at Google Brain . Society has a deep interest in the fruits of innovation . And that is a good reason to approach innovation with optimism . Innovation has never been easy . When you do something new , there will be skeptics . In my younger days , I faced a lot of skepticism when starting most of the projects that ultimately proved to be successful . But this is not to say the skeptics are always wrong . I faced skepticism for most of the unsuccessful projects as well . As I became more experienced , I found that more and more people would agree with whatever I said , and that was even more worrying . I had to actively seek out people who would challenge me and tell me the truth . Luckily , these days I am surrounded by people who will tell me when they think I ’ m doing something dumb ! While skepticism is healthy and even necessary , society has a deep interest in the fruits of innovation . And that is a good reason to approach innovation with optimism . I ’ d rather side with the optimist who wants to give it a shot and might fail than the pessimist who doubts what ’ s possible . As we focus on AI as a driver of valuable innovation throughout society , social responsibility is more important than ever . People both inside and outside the field see a wide range of possible harms AI may cause . These include both short-term issues , such as bias and harmful applications of the technology , and long-term risks , such as concentration of power and potentially catastrophic applications . It ’ s important to have open and intellectually rigorous conversations about them . In that way , we can come to an agreement on what the real risks are and how to reduce them . Over the past millennium , successive waves of innovation have reduced infant mortality , improved nutrition , boosted literacy , raised standards of living worldwide , and fostered civil rights including protections for women , minorities , and other marginalized groups . Yet innovations have also contributed to climate change , spurred rising inequality , polarized society , and increased loneliness . Li ’ s research could prevent AI models from failing catastrophically when they encounter unfamiliar scenarios . Clearly , the benefits of innovation come with risks , and we have not always managed them wisely . AI is the next wave , and we have an obligation to learn lessons from the past to maximize future benefits for everyone and minimize harm . This will require commitment from both individuals and society at large . At the social level , governments are moving to regulate AI . To some innovators , regulation may look like an unnecessary restraint on progress . I see it differently . Regulation helps us avoid mistakes and enables new benefits as we move into an uncertain future . I welcome regulation that calls for more transparency into the opaque workings of large tech companies ; this will help us understand their impact and steer them toward achieving broader societal benefits . Moreover , new regulations are needed because many existing ones were written for a pre-AI world . The new regulations should specify the outcomes we want in important areas like health care and finance—and those we do not want . But avoiding harm shouldn ’ t be just a priority for society . It also needs to be a priority for each innovator . As technologists , we have a responsibility to understand the implications of our research and innovate in ways that are beneficial . Traditionally , many technologists adopted the attitude that the shape technology takes is inevitable and there ’ s nothing we can do about it , so we might as well innovate freely . But we know that ’ s not true . Avoiding harm shouldn ’ t be just a priority for society . It also needs to be a priority for each innovator . When innovators choose to work on differential privacy ( which allows AI to learn from data without exposing personally identifying information ) , they make a powerful statement that privacy matters . That statement helps shape the social norms adopted by public and private institutions . Conversely , when innovators create Web3 cryptographic protocols to launder money , that too creates a powerful statement—in my view , a harmful one—that governments should not be able to trace how funds are transferred and spent . If you see something unethical being done , I hope you ’ ll raise it with your colleagues and supervisors and engage them in constructive conversations . And if you are asked to work on something that you don ’ t think helps humanity , I hope you ’ ll actively work to put a stop to it . If you are unable to do so , then consider walking away . At AI Fund , I have killed projects that I assessed to be financially sound but ethically unsound . I urge you to do the same . Now , go forth and innovate ! If you ’ re already in the innovation game , keep at it . There ’ s no telling what great accomplishment lies in your future . If your ideas are in the daydream stage , share them with others and get help to shape them into something practical and successful . Start executing , and find ways to use the power of innovation for good . This essay is part of MIT Technology Review ’ s 2023 Innovators Under 35 package . Meet this year ’ s honorees . Andrew Ng is a renowned global AI innovator . He leads AI Fund , DeepLearning.AI , and Landing AI .","['essay', 'part', 'mit', 'technology', 'review', 'innovator', 'package', 'meet', 'year', 'honoree', 'innovation', 'powerful', 'engine', 'uplift', 'society', 'fuel', 'economic', 'growth', 'antibiotic', 'electric', 'light', 'refrigerator', 'airplane', 'smartphone', 'thing', 'innovator', 'create', 'exist', 'mit', 'technology', 'review', 'innovator', 'list', 'celebrate', 'individual', 'accomplish', 'lot', 'early', 'career', 'likely', 'accomplish', 'much', 'still', 'spend', 'many', 'year', 'work', 'research', 'building', 'ai', 'product', 'fortunate', 'participate', 'innovation', 'make', 'impact', 'use', 'reinforcement', 'learning', 'fly', 'helicopter', 'drone', 'start', 'lead', 'brain', 'drive', 'largescale', 'deep', 'learning', 'create', 'online', 'course', 'lead', 'founding', 'coursera', 'like', 'share', 'thought', 'well', 'sidestep', 'pitfall', 'avoid', 'build', 'thing', 'lead', 'serious', 'harm', 'way', 'say', 'believe', 'ai', 'new', 'electricity', 'electricity', 'revolutionize', 'industry', 'change', 'way', 'life', 'reach', 'industry', 'discipline', 'yield', 'advance', 'help', 'multitude', 'people', 'ai', 'electricity', 'general\xadpurpose', 'technology', 'many', 'innovation', 'medical', 'treatment', 'space', 'rocket', 'battery', 'design', 'fit', 'purpose', 'contrast', 'ai', 'useful', 'generate', 'art', 'serve', 'web', 'page', 'relevant', 'search', 'query', 'optimize', 'shipping', 'route', 'save', 'fuel', 'help', 'car', 'avoid', 'collision', 'much', 'advance', 'create', 'opportunity', 'corner', 'economy', 'explore', 'apply', 'area', 'thus', 'learn', 'ai', 'create', 'disproportionately', 'many', 'opportunity', 'one', 'else', 'ever', 'instance', 'fund', 'venture', 'studio', 'lead', 'privilege', 'participate', 'project', 'apply', 'ai', 'maritime', 'shipping', 'relationship', 'coach', 'talent', 'management', 'education', 'area', 'many', 'ai', 'technology', 'new', 'application', 'domain', 'yet', 'explore', 'way', 'know', 'take', 'advantage', 'ai', 'give', 'numerous', 'opportunity', 'collaborate', 'look', 'ahead', 'development', 'especially', 'exciting', 'area', 'offer', 'rich', 'opportunity', 'innovator', 'moreover', 'many', 'reach', 'broadly', 'techsavvy', 'people', 'people', 'already', 'ai', 'online', 'course', 'opensource', 'software', 'software', 'service', 'online', 'research', 'paper', 'give', 'tool', 'learn', 'start', 'innovate', 'even', 'technology', 'yet', 'grasp', 'many', 'path', 'innovation', 'wide', 'open', 'say', 'lot', 'idea', 'initially', 'seem', 'promising', 'turn', 'dud', 'dud', 'unavoidable', 'take', 'innovation', 'seriously', 'project', 'mine', 'probably', 'hear', 'duds', 'painful', 'project', 'succeed', 'lesson', 'learn', 'turn', 'instrumental', 'project', 'fare', 'well', 'fail', 'attempt', 'vshape', 'fly', 'learn', 'plan', 'project', 'much', 'well', 'frontload', 'risk', 'effort', 'unload', 'dishwasher', 'fail', 'lead', 'team', 'build', 'robot', 'operating', 'system', 'ro', 'become', 'popular', 'opensource', 'framework', 'robot', 'selfdrive', 'car', 'mechanical', 'dog', 'even', 'initial', 'focus', 'unsupervised', 'learning', 'poor', 'choice', 'step', 'take', 'turn', 'critical', 'scale', 'deep', 'learning', 'society', 'deep', 'interest', 'fruit', 'innovation', 'good', 'reason', 'approach', 'innovation', 'optimism', 'innovation', 'never', 'easy', 'new', 'skeptic', 'young', 'day', 'face', 'lot', 'skepticism', 'start', 'project', 'ultimately', 'prove', 'successful', 'say', 'skeptic', 'always', 'wrong', 'face', 'skepticism', 'unsuccessful', 'project', 'well', 'become', 'experienced', 'find', 'people', 'agree', 'say', 'even', 'worrying', 'actively', 'seek', 'people', 'challenge', 'tell', 'truth', 'luckily', 'day', 'surround', 'people', 'tell', 'think', 'dumb', 'skepticism', 'healthy', 'even', 'necessary', 'society', 'deep', 'interest', 'fruit', 'innovation', 'good', 'reason', 'approach', 'innovation', 'optimism', 'rather', 'side', 'optimist', 'want', 'give', 'shot', 'fail', 'pessimist', 'doubt', 'possible', 'focus', 'ai', 'driver', 'valuable', 'innovation', 'society', 'social', 'responsibility', 'important', 'ever', 'people', 'field', 'see', 'wide', 'range', 'possible', 'harm', 'cause', 'include', 'shortterm', 'issue', 'bias', 'harmful', 'application', 'technology', 'longterm', 'risk', 'concentration', 'power', 'potentially', 'catastrophic', 'application', 'important', 'open', 'intellectually', 'rigorous', 'conversation', 'way', 'come', 'agreement', 'real', 'risk', 'reduce', 'past', 'millennium', 'successive', 'wave', 'innovation', 'reduce', 'infant', 'mortality', 'improve', 'nutrition', 'boost', 'literacy', 'raise', 'standard', 'live', 'worldwide', 'foster', 'civil', 'right', 'include', 'protection', 'woman', 'minority', 'marginalize', 'group', 'yet', 'innovation', 'also', 'contribute', 'climate', 'change', 'spur', 'rise', 'inequality', 'polarize', 'society', 'increase', 'loneliness', 'research', 'prevent', 'ai', 'model', 'fail', 'catastrophically', 'encounter', 'unfamiliar', 'scenario', 'clearly', 'benefit', 'innovation', 'come', 'risk', 'always', 'manage', 'wisely', 'ai', 'next', 'wave', 'obligation', 'learn', 'lesson', 'past', 'maximize', 'future', 'benefit', 'minimize', 'harm', 'require', 'commitment', 'individual', 'society', 'large', 'social', 'level', 'government', 'move', 'regulate', 'ai', 'innovator', 'regulation', 'look', 'unnecessary', 'restraint', 'progress', 'see', 'differently', 'regulation', 'help', 'avoid', 'mistake', 'enable', 'new', 'benefit', 'move', 'uncertain', 'future', 'welcome', 'regulation', 'call', 'transparency', 'opaque', 'working', 'large', 'tech', 'company', 'help', 'understand', 'impact', 'steer', 'achieve', 'broad', 'societal', 'benefit', 'moreover', 'new', 'regulation', 'need', 'many', 'exist', 'one', 'write', 'preai', 'world', 'new', 'regulation', 'specify', 'outcome', 'want', 'important', 'area', 'health', 'care', 'finance', 'want', 'avoid', 'harm', 'priority', 'society', 'also', 'need', 'priority', 'innovator', 'technologist', 'responsibility', 'understand', 'implication', 'research', 'innovate', 'way', 'beneficial', 'traditionally', 'many', 'technologist', 'adopt', 'attitude', 'shape', 'technology', 'take', 'inevitable', 'well', 'innovate', 'freely', 'know', 'true', 'avoid', 'harm', 'priority', 'society', 'also', 'need', 'priority', 'innovator', 'innovator', 'choose', 'work', 'differential', 'privacy', 'allow', 'ai', 'learn', 'datum', 'expose', 'personally', 'identify', 'information', 'make', 'powerful', 'statement', 'privacy', 'matter', 'statement', 'help', 'shape', 'social', 'norm', 'adopt', 'public', 'private', 'institution', 'conversely', 'innovator', 'create', 'web3', 'cryptographic', 'protocol', 'launder', 'money', 'create', 'powerful', 'statement', 'view', 'harmful', 'government', 'able', 'trace', 'fund', 'transfer', 'spend', 'see', 'unethical', 'hope', 'raise', 'colleague', 'supervisor', 'engage', 'constructive', 'conversation', 'ask', 'work', 'think', 'help', 'humanity', 'hope', 'actively', 'work', 'put', 'stop', 'unable', 'consider', 'walk', 'away', 'fund', 'kill', 'project', 'assess', 'financially', 'sound', 'ethically', 'urge', 'go', 'forth', 'innovate', 'already', 'innovation', 'game', 'keep', 'tell', 'great', 'accomplishment', 'lie', 'future', 'idea', 'daydream', 'stage', 'share', 'get', 'help', 'shape', 'practical', 'successful', 'start', 'execute', 'find', 'way', 'use', 'power', 'innovation', 'good', 'essay', 'part', 'mit', 'technology', 'review', 'innovator', 'package', 'meet', 'year', 'renowned', 'lead', 'fund', 'deeplearningai', 'land', 'ai']","<p>Tips for aspiring innovators on trying, failing, and the future of AI.</p>
"
Moving data through the supply chain with unprecedented speed,https://www.technologyreview.com/2023/09/20/1079519/moving-data-through-the-supply-chain-with-unprecedented-speed/,2023-09-20,"In partnership withGS1 US Product information is a powerful commodity in today’s digital economy. Making it accessible can let consumers know if an item contains allergens, help retailers respond swiftly to product recalls, and enable suppliers to track real-time inventory levels. But data can become siloed and inaccessible if organizations fail to make it easy to connect with. This means shifting away from legacy processes and using a “phygital” approach, which brings together data from physical objects and connected digital sources. “The phygital creates a link between the actual physical good and its digital representation, which can unlock vast volumes of information for consumers—data they haven’t been able to access in the past because it has been tied up in proprietary systems,” says Carrie Wilkie, senior vice president at GS1 US, a member of GS1, a global not-for-profit supply chain standards organization. Driving adoption of this phygital connection are technological enablement and standards for interoperability. Standards define a common language between technologies and can make data more technology agnostic. These standards, along with evolving data carriers such as two-dimensional (2D) barcodes and Radio Frequency Identification (RFID), are boosting supply chain visibility in an era of uncertainty, and are transforming how consumers select and interact with products. Among the best-known global standards for classifying products are Global Trade Item Numbers (GTINs), which are used for identifying products, and Global Location Numbers (GLNs) for location. These unique identifiers, when embedded in a data carrier such as a barcode, are examples of standards that provide a way for varying technologies and trading partners across the globe to interpret the data in the same way, enabling them to find products anywhere in their supply chain. Today, a simple scan can connect permissioned data between points in the supply chain. Unlocking the full potential of data in a more robust data carrier can elevate that simple scan to connect any product data to digital information that flows seamlessly across trading partners. The Universal Product Code (UPC), the one-dimensional machine-readable identifier in North America, and the European Article Number (EAN) barcode for the rest of the world, are the longest-established and most widely used of all barcodes. These common barcodes—and the data behind them—can shed new light on supply chain data. However, a new generation of barcodes is emerging that promises to provide consumers with greater transparency, helping them to make smarter decisions about what they buy and use, while simultaneously improving supply chain safety and resiliency for all stakeholders. While UPC and EAN barcodes carry GTIN data and can be found on consumer products all over the world, they fail to “create a link between the physical and the digital,” says Wilkie, “We need more information about products at our fingertips in a machine-readable, interoperable way than we’ll ever be able to fit on product packaging.” Advanced data carriers and emerging standards are capturing unprecedented amounts of data for businesses, regulators, consumers, and patients alike, offering much more than just links to static webpages. Rather, two-dimensional (2D) barcodes and Radio Frequency Identification (RFID) technology can support phygital connections to tell a richer story about a product, including where it comes from, if it contains allergens, is organic, even how it can be recycled for sustainability purposes. Better yet, 2D barcodes and RFID technology allow brands to communicate directly with consumers to offer more timely, accurate, and authoritative information. This is a step beyond consumers using their cell phones to look up product data while browsing in a physical store, which nearly four out of 10 consumers currently do, according to 2020 research by PwC Global. Another advantage of today’s more advanced data carriers: One-dimensional barcodes can contain about 20 characters of information, but 2D barcodes, such as QR codes (quick-response codes), can hold more than 7,000 characters of data, and can provide access to more detailed information such as features, ingredients, expiration date, care instructions, and marketing. Innovative use cases for QR codes are expanding rapidly, as this matrix code can be read with a line-of-sight device like a hand-held scanner or personal device like a cell phone. “By using 2D barcodes, we’re able to start unlocking more information for consumers, patients, and regulators, and create more of a phygital experience at every point in the supply chain,” says Wilkie. For example, a grocery store chain can use a QR code containing batch and expiration data to support traceability, waste management, and consumer safety around the world. Another application of QR codes is on-demand discounting. According to Wilkie, a bakery can rely on a QR code and electronic store shelf tags “to determine which racks of bread will expire in the next few days,” and can easily mark down products about to expire without the intervention of a store associate. The result is a win-win scenario. Consumers benefit by receiving product discounts, while the retailer saves on both product waste and manual labor costs. RFID technology is another advanced data carrier that is already delivering significant advantages. RFID uses electronic tags that respond to radio waves to automatically transmit data. They are affixed to products or pallets, enabling strategically positioned readers to capture and share huge amounts of information in real time. Since data is transmitted via radio waves, unlike barcodes, line of sight is not needed. As a wireless system of tags and readers, RFID can deliver enormous benefits. For starters, RFID technology drives a more precise understanding of physical inventory across the supply chain, and in physical stores, with accuracy levels near 99%. By minimizing inventory errors and notifying organizations when it’s time to restock, RFID not only drives supply chain efficiencies but enhances the experiences of customers who want assurances that the products they order are readily available. RFID can also enhance in-store consumer experiences when used in applications like smart shelves that can detect when products are removed, dynamic-pricing displays, and frictionless check-out where an RFID reader can read and check out an entire basket of tagged goods almost instantly. With real-time visibility into stock levels, RFID can also ensure better on-shelf availability of products. In fact, a study by research and consulting company Spherical Insights says the global market for electronic shelving technology, sized at $1.02 billion in 2022, will grow to $3.43 billion by 2032. For data carriers to deliver on their promises of greater supply chain visibility and enhanced customer experiences, global standards need widespread adoption. Standards and technology innovations are extending the power and flexibility of unique identifiers, providing a gateway to unprecedented volumes of important information. For example, the GS1 Digital Link standard with a QR code, says Wilkie, “allows organizations to take the GTIN and GS1 identification, and encode it in a URL in a standardized way, unlocking value for the consumer by allowing them to go to a website and access more information than would ever fit on a product package.” Products still go beep at the point of sale; it’s just that consumers are now able to access more information than ever before in ways that not only facilitate a more phygital interaction at every point in the supply chain but promise to transform the way in which product data is shared with consumers and suppliers. “Supply chain standards are table stakes,” says Wilkie. “Using standards to ensure interoperability is critical in making sure that the supply chain is efficient.” This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.  ","In partnership withGS1 US Product information is a powerful commodity in today ’ s digital economy . Making it accessible can let consumers know if an item contains allergens , help retailers respond swiftly to product recalls , and enable suppliers to track real-time inventory levels . But data can become siloed and inaccessible if organizations fail to make it easy to connect with . This means shifting away from legacy processes and using a “ phygital ” approach , which brings together data from physical objects and connected digital sources . “ The phygital creates a link between the actual physical good and its digital representation , which can unlock vast volumes of information for consumers—data they haven ’ t been able to access in the past because it has been tied up in proprietary systems , ” says Carrie Wilkie , senior vice president at GS1 US , a member of GS1 , a global not-for-profit supply chain standards organization . Driving adoption of this phygital connection are technological enablement and standards for interoperability . Standards define a common language between technologies and can make data more technology agnostic . These standards , along with evolving data carriers such as two-dimensional ( 2D ) barcodes and Radio Frequency Identification ( RFID ) , are boosting supply chain visibility in an era of uncertainty , and are transforming how consumers select and interact with products . Among the best-known global standards for classifying products are Global Trade Item Numbers ( GTINs ) , which are used for identifying products , and Global Location Numbers ( GLNs ) for location . These unique identifiers , when embedded in a data carrier such as a barcode , are examples of standards that provide a way for varying technologies and trading partners across the globe to interpret the data in the same way , enabling them to find products anywhere in their supply chain . Today , a simple scan can connect permissioned data between points in the supply chain . Unlocking the full potential of data in a more robust data carrier can elevate that simple scan to connect any product data to digital information that flows seamlessly across trading partners . The Universal Product Code ( UPC ) , the one-dimensional machine-readable identifier in North America , and the European Article Number ( EAN ) barcode for the rest of the world , are the longest-established and most widely used of all barcodes . These common barcodes—and the data behind them—can shed new light on supply chain data . However , a new generation of barcodes is emerging that promises to provide consumers with greater transparency , helping them to make smarter decisions about what they buy and use , while simultaneously improving supply chain safety and resiliency for all stakeholders . While UPC and EAN barcodes carry GTIN data and can be found on consumer products all over the world , they fail to “ create a link between the physical and the digital , ” says Wilkie , “ We need more information about products at our fingertips in a machine-readable , interoperable way than we ’ ll ever be able to fit on product packaging. ” Advanced data carriers and emerging standards are capturing unprecedented amounts of data for businesses , regulators , consumers , and patients alike , offering much more than just links to static webpages . Rather , two-dimensional ( 2D ) barcodes and Radio Frequency Identification ( RFID ) technology can support phygital connections to tell a richer story about a product , including where it comes from , if it contains allergens , is organic , even how it can be recycled for sustainability purposes . Better yet , 2D barcodes and RFID technology allow brands to communicate directly with consumers to offer more timely , accurate , and authoritative information . This is a step beyond consumers using their cell phones to look up product data while browsing in a physical store , which nearly four out of 10 consumers currently do , according to 2020 research by PwC Global . Another advantage of today ’ s more advanced data carriers : One-dimensional barcodes can contain about 20 characters of information , but 2D barcodes , such as QR codes ( quick-response codes ) , can hold more than 7,000 characters of data , and can provide access to more detailed information such as features , ingredients , expiration date , care instructions , and marketing . Innovative use cases for QR codes are expanding rapidly , as this matrix code can be read with a line-of-sight device like a hand-held scanner or personal device like a cell phone . “ By using 2D barcodes , we ’ re able to start unlocking more information for consumers , patients , and regulators , and create more of a phygital experience at every point in the supply chain , ” says Wilkie . For example , a grocery store chain can use a QR code containing batch and expiration data to support traceability , waste management , and consumer safety around the world . Another application of QR codes is on-demand discounting . According to Wilkie , a bakery can rely on a QR code and electronic store shelf tags “ to determine which racks of bread will expire in the next few days , ” and can easily mark down products about to expire without the intervention of a store associate . The result is a win-win scenario . Consumers benefit by receiving product discounts , while the retailer saves on both product waste and manual labor costs . RFID technology is another advanced data carrier that is already delivering significant advantages . RFID uses electronic tags that respond to radio waves to automatically transmit data . They are affixed to products or pallets , enabling strategically positioned readers to capture and share huge amounts of information in real time . Since data is transmitted via radio waves , unlike barcodes , line of sight is not needed . As a wireless system of tags and readers , RFID can deliver enormous benefits . For starters , RFID technology drives a more precise understanding of physical inventory across the supply chain , and in physical stores , with accuracy levels near 99 % . By minimizing inventory errors and notifying organizations when it ’ s time to restock , RFID not only drives supply chain efficiencies but enhances the experiences of customers who want assurances that the products they order are readily available . RFID can also enhance in-store consumer experiences when used in applications like smart shelves that can detect when products are removed , dynamic-pricing displays , and frictionless check-out where an RFID reader can read and check out an entire basket of tagged goods almost instantly . With real-time visibility into stock levels , RFID can also ensure better on-shelf availability of products . In fact , a study by research and consulting company Spherical Insights says the global market for electronic shelving technology , sized at $ 1.02 billion in 2022 , will grow to $ 3.43 billion by 2032 . For data carriers to deliver on their promises of greater supply chain visibility and enhanced customer experiences , global standards need widespread adoption . Standards and technology innovations are extending the power and flexibility of unique identifiers , providing a gateway to unprecedented volumes of important information . For example , the GS1 Digital Link standard with a QR code , says Wilkie , “ allows organizations to take the GTIN and GS1 identification , and encode it in a URL in a standardized way , unlocking value for the consumer by allowing them to go to a website and access more information than would ever fit on a product package. ” Products still go beep at the point of sale ; it ’ s just that consumers are now able to access more information than ever before in ways that not only facilitate a more phygital interaction at every point in the supply chain but promise to transform the way in which product data is shared with consumers and suppliers . “ Supply chain standards are table stakes , ” says Wilkie . “ Using standards to ensure interoperability is critical in making sure that the supply chain is efficient. ” This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['partnership', 'withgs1', 'product', 'information', 'powerful', 'commodity', 'today', 'digital', 'economy', 'make', 'accessible', 'let', 'consumer', 'know', 'item', 'contain', 'help', 'retailer', 'respond', 'swiftly', 'product', 'recall', 'enable', 'supplier', 'track', 'realtime', 'inventory', 'level', 'datum', 'become', 'siloe', 'inaccessible', 'organization', 'fail', 'make', 'easy', 'connect', 'mean', 'shift', 'away', 'legacy', 'process', 'use', 'phygital', 'approach', 'bring', 'datum', 'physical', 'object', 'connect', 'digital', 'source', 'phygital', 'create', 'link', 'actual', 'physical', 'good', 'digital', 'representation', 'unlock', 'vast', 'volume', 'information', 'consumer', 'datum', 'able', 'access', 'past', 'tie', 'proprietary', 'system', 'say', 'senior', 'vice', 'president', 'gs1', 'member', 'gs1', 'global', 'notforprofit', 'supply', 'chain', 'standard', 'organization', 'drive', 'adoption', 'phygital', 'connection', 'technological', 'enablement', 'standard', 'interoperability', 'standard', 'define', 'common', 'language', 'technology', 'make', 'datum', 'technology', 'agnostic', 'standard', 'evolve', 'data', 'carrier', 'twodimensional', 'barcode', 'radio', 'frequency', 'identification', 'rfid', 'boost', 'supply', 'chain', 'visibility', 'era', 'uncertainty', 'transform', 'consumer', 'select', 'interact', 'product', 'bestknown', 'global', 'standard', 'classify', 'product', 'global', 'trade', 'item', 'number', 'gtin', 'use', 'identify', 'product', 'global', 'location', 'number', 'gln', 'location', 'unique', 'identifier', 'embed', 'data', 'carrier', 'barcode', 'example', 'standard', 'provide', 'way', 'vary', 'technology', 'trading', 'partner', 'globe', 'interpret', 'datum', 'way', 'enable', 'find', 'product', 'anywhere', 'supply', 'chain', 'today', 'simple', 'scan', 'connect', 'permissioned', 'datum', 'point', 'supply', 'chain', 'unlock', 'full', 'potential', 'datum', 'robust', 'datum', 'carrier', 'elevate', 'simple', 'connect', 'product', 'datum', 'digital', 'information', 'flow', 'seamlessly', 'trading', 'partner', 'universal', 'product', 'code', 'upc', 'onedimensional', 'machinereadable', 'identifier', 'european', 'article', 'number', 'ean', 'barcode', 'rest', 'world', 'longestestablished', 'widely', 'use', 'barcode', 'common', 'barcode', 'datum', 'shed', 'new', 'light', 'supply', 'chain', 'datum', 'however', 'new', 'generation', 'barcode', 'emerge', 'promise', 'provide', 'consumer', 'great', 'transparency', 'help', 'make', 'smart', 'decision', 'buy', 'use', 'simultaneously', 'improve', 'supply', 'chain', 'safety', 'resiliency', 'stakeholder', 'upc', 'ean', 'barcode', 'carry', 'gtin', 'datum', 'find', 'consumer', 'product', 'world', 'fail', 'create', 'link', 'physical', 'digital', 'say', 'wilkie', 'need', 'information', 'product', 'fingertip', 'machinereadable', 'interoperable', 'way', 'ever', 'able', 'fit', 'product', 'packaging', 'advanced', 'datum', 'carrier', 'emerge', 'standard', 'capture', 'unprecedented', 'amount', 'datum', 'business', 'regulator', 'consumer', 'patient', 'alike', 'offer', 'much', 'link', 'static', 'webpage', 'rather', 'twodimensional', 'barcode', 'radio', 'frequency', 'identification', 'rfid', 'technology', 'support', 'phygital', 'connection', 'tell', 'rich', 'story', 'product', 'include', 'come', 'contain', 'allergen', 'organic', 'even', 'recycle', 'sustainability', 'purpose', 'well', 'yet', 'barcode', 'rfid', 'technology', 'allow', 'brand', 'communicate', 'directly', 'consumer', 'offer', 'timely', 'accurate', 'authoritative', 'information', 'step', 'consumer', 'use', 'cell', 'phone', 'look', 'product', 'datum', 'browse', 'physical', 'store', 'nearly', 'consumer', 'currently', 'accord', 'research', 'advantage', 'today', 'advanced', 'data', 'carrier', 'onedimensional', 'barcode', 'contain', 'character', 'information', 'barcode', 'code', 'quickresponse', 'code', 'hold', 'character', 'datum', 'provide', 'access', 'detailed', 'information', 'feature', 'ingredient', 'expiration', 'date', 'care', 'instruction', 'marketing', 'innovative', 'use', 'case', 'qr', 'code', 'expand', 'rapidly', 'matrix', 'code', 'read', 'lineofsight', 'device', 'handheld', 'scanner', 'personal', 'device', 'cell', 'phone', 'use', 'barcode', 'able', 'start', 'unlock', 'information', 'consumer', 'patient', 'regulator', 'create', 'phygital', 'experience', 'point', 'supply', 'chain', 'say', 'wilkie', 'example', 'grocery', 'store', 'chain', 'use', 'qr', 'code', 'contain', 'batch', 'expiration', 'datum', 'support', 'traceability', 'waste', 'management', 'consumer', 'safety', 'world', 'application', 'qr', 'code', 'ondemand', 'discount', 'accord', 'wilkie', 'bakery', 'rely', 'qr', 'code', 'electronic', 'store', 'shelf', 'tag', 'determine', 'rack', 'bread', 'expire', 'next', 'day', 'easily', 'mark', 'product', 'expire', 'intervention', 'store', 'associate', 'result', 'scenario', 'consumer', 'benefit', 'receive', 'product', 'discount', 'retailer', 'save', 'product', 'waste', 'manual', 'labor', 'cost', 'rfid', 'technology', 'advanced', 'data', 'carrier', 'already', 'deliver', 'significant', 'advantage', 'use', 'electronic', 'tag', 'respond', 'radio', 'wave', 'automatically', 'transmit', 'datum', 'affix', 'product', 'pallet', 'enable', 'strategically', 'position', 'reader', 'capture', 'share', 'huge', 'amount', 'information', 'real', 'time', 'datum', 'transmit', 'radio', 'wave', 'barcode', 'line', 'sight', 'need', 'wireless', 'system', 'tag', 'reader', 'rfid', 'deliver', 'enormous', 'benefit', 'starter', 'rfid', 'technology', 'drive', 'precise', 'understanding', 'physical', 'inventory', 'supply', 'chain', 'physical', 'store', 'accuracy', 'level', 'minimize', 'inventory', 'error', 'notify', 'organization', 'time', 'restock', 'rfid', 'drive', 'supply', 'chain', 'efficiency', 'enhance', 'experience', 'customer', 'want', 'assurance', 'product', 'order', 'readily', 'available', 'rfid', 'also', 'enhance', 'instore', 'consumer', 'experience', 'use', 'application', 'smart', 'shelf', 'detect', 'product', 'remove', 'dynamicpricing', 'display', 'frictionless', 'checkout', 'rfid', 'reader', 'read', 'check', 'entire', 'basket', 'tag', 'good', 'almost', 'instantly', 'realtime', 'visibility', 'stock', 'level', 'rfid', 'also', 'ensure', 'well', 'onshelf', 'availability', 'product', 'fact', 'study', 'research', 'consult', 'company', 'spherical', 'insight', 'say', 'global', 'market', 'electronic', 'shelving', 'technology', 'size', 'grow', 'datum', 'carrier', 'deliver', 'promise', 'great', 'supply', 'chain', 'visibility', 'enhanced', 'customer', 'experience', 'global', 'standard', 'need', 'widespread', 'adoption', 'standard', 'technology', 'innovation', 'extend', 'power', 'flexibility', 'unique', 'identifier', 'provide', 'gateway', 'unprecedented', 'volume', 'important', 'information', 'example', 'gs1', 'standard', 'qr', 'code', 'say', 'wilkie', 'allow', 'organization', 'take', 'gtin', 'gs1', 'identification', 'encode', 'url', 'standardized', 'way', 'unlock', 'value', 'consumer', 'allow', 'go', 'website', 'access', 'information', 'ever', 'fit', 'product', 'package', 'product', 'still', 'go', 'beep', 'point', 'sale', 'consumer', 'able', 'access', 'information', 'ever', 'way', 'facilitate', 'phygital', 'interaction', 'point', 'supply', 'chain', 'promise', 'transform', 'way', 'product', 'datum', 'share', 'consumer', 'supplier', 'supply', 'chain', 'standard', 'table', 'stake', 'say', 'wilkie', 'use', 'standard', 'ensure', 'interoperability', 'critical', 'make', 'sure', 'supply', 'chain', 'efficient', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","Product information is a powerful commodity in today’s digital economy. Making it accessible can let consumers know if an item contains allergens, help retailers respond swiftly to product recalls, and enable suppliers to track real-time inventory levels. But data can become siloed and inaccessible if organizations fail to make it easy to connect with. This…"
Investing in holistic innovation,https://www.technologyreview.com/2023/09/13/1079032/investing-in-holistic-innovation/,2023-09-13,"In association withJPMorgan Chase & Co. For many organizations, innovation is focused on a few strategically prioritized initiatives and often is incremental by design. Change and the surprises it brings can be a grudgingly accepted necessity. Savvy companies, however, acknowledge that innovation must also be part of a firm’s strategy and deployed through every department. The value of most companies in the market, for example, is based on the value of their future cash flows, says Hugo Dubourg, co-head of ESG and sustainability research for JPMorgan Chase’s Europe, Middle East, and Africa equity research group. However, he continues, “While markets are pricing companies and assets every fraction of a second, the future economic value is what markets are theoretically trying to assess,” Dubourg says. From this perspective, the longevity of a company depends on how it keeps up with  innovation. Enterprises need to constantly look for ways to improve and expand what they offer to the marketplace. For example, Sameena Shah, managing director of AI research at JPMorgan Chase, says the company’s bankers have been looking for new ways to study early-stage startups looking to raise capital. The challenge was, she says, “finding good prospects in a domain that is fundamentally very opaque and has a lot of variability.” The solution for JPMorgan Chase was a new digital platform, built off an algorithm that continually seeks out data, and learns to find prospects by triaging its data into standardized representations to describe startups and likely investors. For users, the platform also offers the context of its output, to help them understand the recommendations. “Many bankers told us that they had not known about some of the contexts or data points. That’s the power of machines,” Shah says. Forward-thinking financial services can also help investors that are looking beyond just the enterprise’s bottom line. Dubourg says new investments draw on a growing pool of external data to move into new investing contexts. “We’re moving from a world of unconstrained economics to a world with physical, environmental limits,” Dubourg says. Doing so, he says, means internalizing novel external data; expanding from traditional financial analysis to a model increasingly defined by nonfinancial factors such as climate change and environmental, social, and governance (ESG) goals. Given the breadth of potentially relevant data in these cases, even specialist investors and companies are unlikely to have access to all the knowledge necessary to make fully informed decisions. JPMorgan Chase’s own solution, ESG Discovery, draws single-source ESG data from relevant businesses and sectors, providing thematic deep-dives and company-specific views. Dubourg says the platform makes sure investors have “every relevant piece of ESG information accessible in one, single spot.” Innovation is meant to improve how companies work, which does not necessarily involve new technologies or devices: sometimes it is a matter of rethinking processes. For this, talent is essential. An expansive approach to talent can give companies richer choices to support their work. Gill Haus, CIO of consumer and community banking at JPMorgan Chase, says developing the technology at the center of the firm is not just about finding a group of brilliant individuals, it’s about organizing around products and customers. “What really makes a technology organization,” Haus says, “is the way you hire teams and the way you coach them.” One way JPMorgan Chase nurtures innovation is its Tech for Social Good program, focused on engaging community members, especially students and nonprofit workers. This community-based initiative is focused on developing new thinking from inside and outside the company. It has three main goals: innovate for the social sector, build the workforce of the future, and develop skills within the company. “What’s so exciting here is we have so many complex problems to solve, so many incredible people that are looking for assistance, that you just have an environment where people can grow their careers really quickly,” says Haus. Driving innovation at JPMorgan Chase focuses on finding ways to improve how cutting-edge tools are applied, such as AI and ML. To ensure responsible AI, for example, the company’s ML designs go beyond standard software development controls, or even focusing on explainability, responsibility, and training, as most companies do, says David Castillo, managing director and product line general manager for AI-ML at JPMorgan Chase. This “fairly unique” process ensures responsible AI is in place at a higher level, so that even lines of business at different maturity levels for AI and ML operate at the same standard as any other, he says. “We’re addressing the entire machine learning development life cycle,” Castillo says. Instead of restricting innovation, this approach “creates a very interesting, streamlined opportunity for machine learning from end-to-end. We’re being responsible across the entire spectrum,” he says. “We want to be able to make sure that that every piece of data that’s being used for model training has lineage that we can trace back to its origin,” he says. It’s important that new iterations of a model feature carry forward its lineage, he says. “We’ve scrubbed that data for personally identifying information [PII], we’ve taken out proxies to PII, we’ve identified all of these landmines.” Businesses and creators need a methodical process for protecting innovations. Controlling intellectual property (IP) rights is part of building business models for the future, so ideas can be built on by others. Daryl Wooldridge, managing director and global head of IP at JPMorgan Chase says the firm actively protects innovation. A company’s IP portfolio of patents, trademarks, copyrights, and trade secrets protects innovative products and technology, he says. Patent protection can be costly and time consuming (it takes one to five years for a U.S. Patent Office review), but also creates a valuable asset for the company, providing a sustained competitive advantage and prestige for the inventor, Wooldridge says. It is “validating that this was so innovative, that you were the first person to secure this particular solution in this space. That's really prestigious, and it should be recognized that way,” he says. IP protection is vital to companies that rely on the trust of customers and partners, he says. Patents and open source, for example, are not mutually exclusive, but complimentary. “Now, the community has confidence that what I'm providing I actually own and have the ability to provide,” he says. To Wooldridge, it’s important to ensure that IP protection is part of the innovation process and doesn’t slow down technological development. “By being part of the entire innovation process, we are able to go from idea, to initial diagrams, to the steps necessary to build the solution, while getting that solution protected,” he says.  This article is for informational purposes only and it is not intended as legal, tax, financial, investment, accounting or regulatory advice. Opinions expressed herein are the personal views of the individual(s) and do not represent the views of JPMorgan Chase & Co. The accuracy of any statements, linked resources, reported findings or quotations are not the responsibility of JPMorgan Chase & Co. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. ","In association withJPMorgan Chase & Co. For many organizations , innovation is focused on a few strategically prioritized initiatives and often is incremental by design . Change and the surprises it brings can be a grudgingly accepted necessity . Savvy companies , however , acknowledge that innovation must also be part of a firm ’ s strategy and deployed through every department . The value of most companies in the market , for example , is based on the value of their future cash flows , says Hugo Dubourg , co-head of ESG and sustainability research for JPMorgan Chase ’ s Europe , Middle East , and Africa equity research group . However , he continues , “ While markets are pricing companies and assets every fraction of a second , the future economic value is what markets are theoretically trying to assess , ” Dubourg says . From this perspective , the longevity of a company depends on how it keeps up with innovation . Enterprises need to constantly look for ways to improve and expand what they offer to the marketplace . For example , Sameena Shah , managing director of AI research at JPMorgan Chase , says the company ’ s bankers have been looking for new ways to study early-stage startups looking to raise capital . The challenge was , she says , “ finding good prospects in a domain that is fundamentally very opaque and has a lot of variability. ” The solution for JPMorgan Chase was a new digital platform , built off an algorithm that continually seeks out data , and learns to find prospects by triaging its data into standardized representations to describe startups and likely investors . For users , the platform also offers the context of its output , to help them understand the recommendations . “ Many bankers told us that they had not known about some of the contexts or data points . That ’ s the power of machines , ” Shah says . Forward-thinking financial services can also help investors that are looking beyond just the enterprise ’ s bottom line . Dubourg says new investments draw on a growing pool of external data to move into new investing contexts . “ We ’ re moving from a world of unconstrained economics to a world with physical , environmental limits , ” Dubourg says . Doing so , he says , means internalizing novel external data ; expanding from traditional financial analysis to a model increasingly defined by nonfinancial factors such as climate change and environmental , social , and governance ( ESG ) goals . Given the breadth of potentially relevant data in these cases , even specialist investors and companies are unlikely to have access to all the knowledge necessary to make fully informed decisions . JPMorgan Chase ’ s own solution , ESG Discovery , draws single-source ESG data from relevant businesses and sectors , providing thematic deep-dives and company-specific views . Dubourg says the platform makes sure investors have “ every relevant piece of ESG information accessible in one , single spot. ” Innovation is meant to improve how companies work , which does not necessarily involve new technologies or devices : sometimes it is a matter of rethinking processes . For this , talent is essential . An expansive approach to talent can give companies richer choices to support their work . Gill Haus , CIO of consumer and community banking at JPMorgan Chase , says developing the technology at the center of the firm is not just about finding a group of brilliant individuals , it ’ s about organizing around products and customers . “ What really makes a technology organization , ” Haus says , “ is the way you hire teams and the way you coach them. ” One way JPMorgan Chase nurtures innovation is its Tech for Social Good program , focused on engaging community members , especially students and nonprofit workers . This community-based initiative is focused on developing new thinking from inside and outside the company . It has three main goals : innovate for the social sector , build the workforce of the future , and develop skills within the company . “ What ’ s so exciting here is we have so many complex problems to solve , so many incredible people that are looking for assistance , that you just have an environment where people can grow their careers really quickly , ” says Haus . Driving innovation at JPMorgan Chase focuses on finding ways to improve how cutting-edge tools are applied , such as AI and ML . To ensure responsible AI , for example , the company ’ s ML designs go beyond standard software development controls , or even focusing on explainability , responsibility , and training , as most companies do , says David Castillo , managing director and product line general manager for AI-ML at JPMorgan Chase . This “ fairly unique ” process ensures responsible AI is in place at a higher level , so that even lines of business at different maturity levels for AI and ML operate at the same standard as any other , he says . “ We ’ re addressing the entire machine learning development life cycle , ” Castillo says . Instead of restricting innovation , this approach “ creates a very interesting , streamlined opportunity for machine learning from end-to-end . We ’ re being responsible across the entire spectrum , ” he says . “ We want to be able to make sure that that every piece of data that ’ s being used for model training has lineage that we can trace back to its origin , ” he says . It ’ s important that new iterations of a model feature carry forward its lineage , he says . “ We ’ ve scrubbed that data for personally identifying information [ PII ] , we ’ ve taken out proxies to PII , we ’ ve identified all of these landmines. ” Businesses and creators need a methodical process for protecting innovations . Controlling intellectual property ( IP ) rights is part of building business models for the future , so ideas can be built on by others . Daryl Wooldridge , managing director and global head of IP at JPMorgan Chase says the firm actively protects innovation . A company ’ s IP portfolio of patents , trademarks , copyrights , and trade secrets protects innovative products and technology , he says . Patent protection can be costly and time consuming ( it takes one to five years for a U.S. Patent Office review ) , but also creates a valuable asset for the company , providing a sustained competitive advantage and prestige for the inventor , Wooldridge says . It is “ validating that this was so innovative , that you were the first person to secure this particular solution in this space . That 's really prestigious , and it should be recognized that way , ” he says . IP protection is vital to companies that rely on the trust of customers and partners , he says . Patents and open source , for example , are not mutually exclusive , but complimentary . “ Now , the community has confidence that what I 'm providing I actually own and have the ability to provide , ” he says . To Wooldridge , it ’ s important to ensure that IP protection is part of the innovation process and doesn ’ t slow down technological development . “ By being part of the entire innovation process , we are able to go from idea , to initial diagrams , to the steps necessary to build the solution , while getting that solution protected , ” he says . This article is for informational purposes only and it is not intended as legal , tax , financial , investment , accounting or regulatory advice . Opinions expressed herein are the personal views of the individual ( s ) and do not represent the views of JPMorgan Chase & Co . The accuracy of any statements , linked resources , reported findings or quotations are not the responsibility of JPMorgan Chase & Co . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['association', 'many', 'organization', 'innovation', 'focus', 'strategically', 'prioritized', 'initiative', 'often', 'incremental', 'design', 'change', 'surprise', 'bring', 'grudgingly', 'accept', 'necessity', 'savvy', 'company', 'however', 'acknowledge', 'innovation', 'also', 'part', 'firm', 'strategy', 'deploy', 'department', 'value', 'company', 'market', 'example', 'base', 'value', 'future', 'cash', 'flow', 'say', 'cohead', 'esg', 'sustainability', 'research', 'group', 'however', 'continue', 'market', 'price', 'company', 'asset', 'fraction', 'second', 'future', 'economic', 'value', 'market', 'theoretically', 'try', 'assess', 'say', 'perspective', 'longevity', 'company', 'depend', 'keep', 'innovation', 'enterprise', 'need', 'constantly', 'look', 'way', 'improve', 'expand', 'offer', 'marketplace', 'example', 'manage', 'director', 'research', 'say', 'company', 'banker', 'look', 'new', 'way', 'study', 'earlystage', 'startup', 'look', 'raise', 'capital', 'challenge', 'say', 'find', 'good', 'prospect', 'domain', 'fundamentally', 'opaque', 'lot', 'variability', 'solution', 'new', 'digital', 'platform', 'build', 'continually', 'seek', 'datum', 'learn', 'find', 'prospect', 'triage', 'datum', 'standardized', 'representation', 'describe', 'startup', 'likely', 'investor', 'user', 'platform', 'also', 'offer', 'context', 'output', 'help', 'understand', 'recommendation', 'many', 'banker', 'tell', 'know', 'context', 'datum', 'point', 'power', 'machine', 'shah', 'forwardthinke', 'financial', 'service', 'also', 'help', 'investor', 'look', 'enterprise', 'bottom', 'line', 'say', 'new', 'investment', 'draw', 'grow', 'pool', 'external', 'datum', 'move', 'new', 'investing', 'context', 'move', 'world', 'unconstrained', 'economic', 'world', 'physical', 'environmental', 'limit', 'say', 'say', 'mean', 'internalize', 'novel', 'external', 'datum', 'expand', 'traditional', 'financial', 'analysis', 'model', 'increasingly', 'define', 'nonfinancial', 'factor', 'climate', 'change', 'environmental', 'social', 'governance', 'esg', 'goal', 'give', 'breadth', 'potentially', 'relevant', 'datum', 'case', 'even', 'specialist', 'investor', 'company', 'unlikely', 'access', 'knowledge', 'necessary', 'make', 'fully', 'inform', 'decision', 'solution', 'discovery', 'draw', 'singlesource', 'esg', 'datum', 'relevant', 'business', 'sector', 'provide', 'thematic', 'deepdive', 'companyspecific', 'view', 'say', 'platform', 'make', 'sure', 'investor', 'relevant', 'piece', 'esg', 'information', 'accessible', 'single', 'spot', 'innovation', 'mean', 'improve', 'company', 'work', 'necessarily', 'involve', 'new', 'technology', 'device', 'sometimes', 'matter', 'rethink', 'process', 'talent', 'essential', 'expansive', 'approach', 'talent', 'give', 'company', 'rich', 'choice', 'support', 'work', 'consumer', 'community', 'banking', 'say', 'develop', 'technology', 'center', 'firm', 'find', 'group', 'brilliant', 'individual', 'organize', 'product', 'customer', 'really', 'make', 'technology', 'organization', 'say', 'way', 'hire', 'team', 'way', 'coach', 'way', 'nurture', 'innovation', 'tech', 'social', 'good', 'program', 'focus', 'engage', 'community', 'member', 'especially', 'student', 'nonprofit', 'worker', 'communitybase', 'initiative', 'focus', 'develop', 'new', 'thinking', 'company', 'main', 'goal', 'innovate', 'social', 'sector', 'build', 'workforce', 'future', 'develop', 'skill', 'company', 'exciting', 'many', 'complex', 'problem', 'solve', 'many', 'incredible', 'people', 'look', 'assistance', 'environment', 'people', 'grow', 'career', 'really', 'quickly', 'say', 'haus', 'drive', 'innovation', 'focus', 'find', 'way', 'improve', 'cuttingedge', 'tool', 'apply', 'ensure', 'responsible', 'ai', 'example', 'company', 'ml', 'design', 'go', 'standard', 'software', 'development', 'control', 'even', 'focus', 'explainability', 'responsibility', 'training', 'company', 'say', 'manage', 'director', 'product', 'line', 'general', 'manager', 'aiml', 'fairly', 'unique', 'process', 'ensure', 'responsible', 'ai', 'place', 'high', 'level', 'even', 'line', 'business', 'different', 'maturity', 'level', 'ai', 'operate', 'standard', 'say', 'address', 'entire', 'machine', 'learn', 'development', 'life', 'cycle', 'say', 'instead', 'restrict', 'innovation', 'approach', 'create', 'interesting', 'streamlined', 'opportunity', 'machine', 'learn', 'endtoend', 'responsible', 'entire', 'spectrum', 'say', 'want', 'able', 'make', 'sure', 'piece', 'datum', 'use', 'model', 'training', 'lineage', 'trace', 'back', 'origin', 'say', 'important', 'new', 'iteration', 'model', 'feature', 'carry', 'forward', 'lineage', 'say', 'scrub', 'datum', 'personally', 'identify', 'information', 'pii', 'take', 'proxy', 'pii', 'identify', 'landmine', 'business', 'creator', 'need', 'methodical', 'process', 'protect', 'innovation', 'control', 'intellectual', 'property', 'ip', 'right', 'part', 'build', 'business', 'model', 'future', 'idea', 'build', 'wooldridge', 'manage', 'director', 'global', 'head', 'ip', 'say', 'firm', 'actively', 'protect', 'innovation', 'company', 'ip', 'portfolio', 'patent', 'trademark', 'copyright', 'trade', 'secret', 'protect', 'innovative', 'product', 'technology', 'say', 'patent', 'protection', 'costly', 'time', 'consume', 'take', 'year', 'patent', 'office', 'review', 'also', 'create', 'valuable', 'asset', 'company', 'provide', 'sustained', 'competitive', 'advantage', 'prestige', 'inventor', 'wooldridge', 'say', 'validate', 'innovative', 'first', 'person', 'secure', 'particular', 'solution', 'space', 'really', 'prestigious', 'recognize', 'way', 'say', 'ip', 'protection', 'vital', 'company', 'rely', 'trust', 'customer', 'partner', 'say', 'patent', 'open', 'source', 'example', 'mutually', 'exclusive', 'complimentary', 'community', 'confidence', 'provide', 'actually', 'ability', 'provide', 'say', 'wooldridge', 'important', 'ensure', 'ip', 'protection', 'part', 'innovation', 'process', 'slow', 'technological', 'development', 'part', 'entire', 'innovation', 'process', 'able', 'go', 'idea', 'initial', 'diagram', 'step', 'necessary', 'build', 'solution', 'get', 'solution', 'protect', 'say', 'article', 'informational', 'purpose', 'intend', 'legal', 'tax', 'financial', 'investment', 'accounting', 'regulatory', 'advice', 'opinion', 'express', 'herein', 'personal', 'view', 'individual', 'represent', 'view', 'co', 'accuracy', 'statement', 'link', 'resource', 'report', 'finding', 'quotation', 'responsibility', 'co', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","For many organizations, innovation is focused on a few strategically prioritized initiatives and often is incremental by design. Change and the surprises it brings can be a grudgingly accepted necessity. Savvy companies, however, acknowledge that innovation must also be part of a firm’s strategy and deployed through every department. The value of most companies in…"
Unlocking the value of supply chain data across industries,https://www.technologyreview.com/2023/08/29/1078245/unlocking-the-value-of-supply-chain-data-across-industries/,2023-08-29,"In partnership withGS1 US The product shortages and supply-chain delays of the global covid-19 pandemic are still fresh memories. Consumers and industry are concerned that the next geopolitical climate event may have a similar impact. Against a backdrop of evolving regulations, these conditions mean manufacturers want to be prepared against short supplies, concerned customers, and weakened margins. For supply chain professionals, achieving a “phygital” information flow—the blending of physical and digital data—is key to unlocking resilience and efficiency. As physical objects travel through supply chains, they generate a rich flow of data about the item and its journey—from its raw materials, its manufacturing conditions, even its expiration date—bringing new visibility and pinpointing bottlenecks. This phygital information flow offers significant advantages, enhancing the ability to create rich customer experiences to satisfying environmental, social, and corporate governance (ESG) goals. In a 2022 EY global survey of executives, 70% of respondents agreed that a sustainable supply chain will increase their company’s revenue. For disparate parties to exchange product information effectively, they require a common framework and universally understood language. Among supply chain players, data standards create a shared foundation. Standards help uniquely identify, accurately capture, and automatically share critical information about products, locations, and assets across trading communities. The push for digital standards Supply chain data’s power lies in consistency, accuracy, and seamless sharing to fuel analytics and generate insight about operations. Standards can help precisely describe the physical and digital objects that make up a supply chain, and track what happens to them from production to delivery. This increased visibility is under sharp focus: according to a 2022 survey of supply chain leaders by McKinsey and Company, more than 90% of respondents from nearly every sector invested in digital supply chain technologies during the previous year. These standards rely on number and attribution systems—which can be encoded into data carriers and attached to products—to uniquely identify assets at every level. When data is captured, it provides digital access to information about products and their movement through the supply chain. Numbering and attribution systems such as the Global Trade Item Number (GTIN) identify traded items and products; likewise, Serial Shipping Container Codes (SSCCs) identify logistic units. Global Location Numbers (GLNs) identify business data including an invoice address or a delivery location. Global Product Classification (GPC) codes are a global standard that use a hierarchical system to classify items by characteristics. Data carriers include Universal Product Code (UPC) barcodes, one-dimensional (1D) barcodes familiar to consumers, commonly scanned at the point of sale in North America. Outside the U.S. and Canada, the European Article Number (EAN) barcode is used. These barcodes encode GTIN identifier data. In recent years, more complex and robust data carriers have become common, including radio-frequency identification (RFID) tags and two-dimensional (2D) barcodes like QR codes (quick-response codes). These codes contain vastly more data than simple 1D barcodes. These identification and data capture standards work alongside others for information sharing, including master data, business transaction data, physical event data, and communication standards for sharing information among applications and partners. Phygital information must meet a wide range of needs, including regulatory compliance, consumer and patient engagement protections, and supply chain and trading partner requirements, such as procurement, production, marketing, and ESG reporting. Regulation is an important industry driver: chain of custody and authentication of products and trading partners are vital for safe, secure supply chains. “Governments and regulatory agencies have leveraged the pervasiveness of standards adoption to further global goals of food, product, and consumer safety,” says Siobhan O’Bara, senior vice president of community engagement for GS1 US, a member of GS1, a global not-for-profit supply chain standards organization. Global standards and unique identifiers are not only driving today’s supply chain evolution, but they also allow for robust use cases across a wide variety of industries. Here are a few examples to consider. Healthcare: Today’s healthcare organizations are under pressure to improve patient outcomes, prevent errors, and control costs. Identification systems can help by empowering patients with information that help them follow medical protocols. “We know in healthcare that a critical part of our world is not only whether people have access to healthcare but whether they follow their clinical instructions,” says O’Bara. O’Bara offers the example of a home nebulizer, a device used to deliver medicine to improve respiratory symptoms. By equipping a nebulizer with an RFID chip, she says, “a patient can keep track of whether they are following the prescribed treatment. For instance, if there’s a filter with that nebulizer, when it gets locked into the device, the chip sends a signal, and the nebulizer can display for the patient at the correct time that the filter has been consumed. This mechanism can also convey to healthcare practitioners whether the patient is following the protocol properly.” The result is not only a lower risk of patient miscommunication but improved patient care. Retail: Data about an item’s origins can prevent business losses and enhance public safety. For example, a grocery store that has a product recall on spinach due to a bacterial outbreak must be able to trace the origin of batches, or must destroy its entire inventory. A unique identifier can improve the speed, accuracy, and traceability of recalls for public safety, precision, and cost effectiveness. Consumer goods: A 2D barcode on a bottle of hand lotion can reveal a vast amount of data for consumers, including its origin, ingredients, organic certification, and packaging materials. For industry, unique identifications can tell warehouse workers where a product is located, inform distributors whether a product contains potentially dangerous ingredients, and warn retailers if a product has age restrictions. “Data delivers value in all directions of the supply chain,” says O’Bara. “Data standards are the only way to accurately and consistently—with confidence—obtain and rely on these data points to complete your business operations,” she says. Manufacturing: Achieving ESG compliance hinges on an organization’s supply chain visibility, says O’Bara. “You always have to have data to support your ESG claims, and the only way to get that data is by tracking it through a consistent and calculated method, no matter where it’s consumed.” Standards provide access to structured sustainability information that can be measured to ensure compliance with ESG regulations, and shared with supply chain partners. Standards empower organizations to identify, capture, and share information seamlessly, creating a common language that can support business processes. Savvy organizations are going a step further, providing customers with direct access to supply chain and other valuable data. According to 2023 research by Gartner, customers who are “enabled” with visibility into the supply chain are twice as likely to return; however, only 23% of supply chains currently enable customers this way. O’Bara points to digital labeling as a perfect example of the supply chain future. Digital labels accessed through 2D barcodes by smart devices could provide consumers with information about hundreds of product attributes, such as nutrition, as well as facts that go beyond the label such as environmental, lifestyle, and sustainability factors. This future-forward approach to an increasingly phygital world could drive long-term consumer engagement, and open the door for increased business growth. “Once you have unlocked value from unique identifiers, there are so many more ways that you can think creatively and cross-functionally about how unifying standards along a supply chain can enable commercial functions and consumer engagement with potential to drive substantial top- and bottom-line revenue,” says O’Bara. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. ","In partnership withGS1 US The product shortages and supply-chain delays of the global covid-19 pandemic are still fresh memories . Consumers and industry are concerned that the next geopolitical climate event may have a similar impact . Against a backdrop of evolving regulations , these conditions mean manufacturers want to be prepared against short supplies , concerned customers , and weakened margins . For supply chain professionals , achieving a “ phygital ” information flow—the blending of physical and digital data—is key to unlocking resilience and efficiency . As physical objects travel through supply chains , they generate a rich flow of data about the item and its journey—from its raw materials , its manufacturing conditions , even its expiration date—bringing new visibility and pinpointing bottlenecks . This phygital information flow offers significant advantages , enhancing the ability to create rich customer experiences to satisfying environmental , social , and corporate governance ( ESG ) goals . In a 2022 EY global survey of executives , 70 % of respondents agreed that a sustainable supply chain will increase their company ’ s revenue . For disparate parties to exchange product information effectively , they require a common framework and universally understood language . Among supply chain players , data standards create a shared foundation . Standards help uniquely identify , accurately capture , and automatically share critical information about products , locations , and assets across trading communities . The push for digital standards Supply chain data ’ s power lies in consistency , accuracy , and seamless sharing to fuel analytics and generate insight about operations . Standards can help precisely describe the physical and digital objects that make up a supply chain , and track what happens to them from production to delivery . This increased visibility is under sharp focus : according to a 2022 survey of supply chain leaders by McKinsey and Company , more than 90 % of respondents from nearly every sector invested in digital supply chain technologies during the previous year . These standards rely on number and attribution systems—which can be encoded into data carriers and attached to products—to uniquely identify assets at every level . When data is captured , it provides digital access to information about products and their movement through the supply chain . Numbering and attribution systems such as the Global Trade Item Number ( GTIN ) identify traded items and products ; likewise , Serial Shipping Container Codes ( SSCCs ) identify logistic units . Global Location Numbers ( GLNs ) identify business data including an invoice address or a delivery location . Global Product Classification ( GPC ) codes are a global standard that use a hierarchical system to classify items by characteristics . Data carriers include Universal Product Code ( UPC ) barcodes , one-dimensional ( 1D ) barcodes familiar to consumers , commonly scanned at the point of sale in North America . Outside the U.S. and Canada , the European Article Number ( EAN ) barcode is used . These barcodes encode GTIN identifier data . In recent years , more complex and robust data carriers have become common , including radio-frequency identification ( RFID ) tags and two-dimensional ( 2D ) barcodes like QR codes ( quick-response codes ) . These codes contain vastly more data than simple 1D barcodes . These identification and data capture standards work alongside others for information sharing , including master data , business transaction data , physical event data , and communication standards for sharing information among applications and partners . Phygital information must meet a wide range of needs , including regulatory compliance , consumer and patient engagement protections , and supply chain and trading partner requirements , such as procurement , production , marketing , and ESG reporting . Regulation is an important industry driver : chain of custody and authentication of products and trading partners are vital for safe , secure supply chains . “ Governments and regulatory agencies have leveraged the pervasiveness of standards adoption to further global goals of food , product , and consumer safety , ” says Siobhan O ’ Bara , senior vice president of community engagement for GS1 US , a member of GS1 , a global not-for-profit supply chain standards organization . Global standards and unique identifiers are not only driving today ’ s supply chain evolution , but they also allow for robust use cases across a wide variety of industries . Here are a few examples to consider . Healthcare : Today ’ s healthcare organizations are under pressure to improve patient outcomes , prevent errors , and control costs . Identification systems can help by empowering patients with information that help them follow medical protocols . “ We know in healthcare that a critical part of our world is not only whether people have access to healthcare but whether they follow their clinical instructions , ” says O ’ Bara . O ’ Bara offers the example of a home nebulizer , a device used to deliver medicine to improve respiratory symptoms . By equipping a nebulizer with an RFID chip , she says , “ a patient can keep track of whether they are following the prescribed treatment . For instance , if there ’ s a filter with that nebulizer , when it gets locked into the device , the chip sends a signal , and the nebulizer can display for the patient at the correct time that the filter has been consumed . This mechanism can also convey to healthcare practitioners whether the patient is following the protocol properly. ” The result is not only a lower risk of patient miscommunication but improved patient care . Retail : Data about an item ’ s origins can prevent business losses and enhance public safety . For example , a grocery store that has a product recall on spinach due to a bacterial outbreak must be able to trace the origin of batches , or must destroy its entire inventory . A unique identifier can improve the speed , accuracy , and traceability of recalls for public safety , precision , and cost effectiveness . Consumer goods : A 2D barcode on a bottle of hand lotion can reveal a vast amount of data for consumers , including its origin , ingredients , organic certification , and packaging materials . For industry , unique identifications can tell warehouse workers where a product is located , inform distributors whether a product contains potentially dangerous ingredients , and warn retailers if a product has age restrictions . “ Data delivers value in all directions of the supply chain , ” says O ’ Bara . “ Data standards are the only way to accurately and consistently—with confidence—obtain and rely on these data points to complete your business operations , ” she says . Manufacturing : Achieving ESG compliance hinges on an organization ’ s supply chain visibility , says O ’ Bara . “ You always have to have data to support your ESG claims , and the only way to get that data is by tracking it through a consistent and calculated method , no matter where it ’ s consumed. ” Standards provide access to structured sustainability information that can be measured to ensure compliance with ESG regulations , and shared with supply chain partners . Standards empower organizations to identify , capture , and share information seamlessly , creating a common language that can support business processes . Savvy organizations are going a step further , providing customers with direct access to supply chain and other valuable data . According to 2023 research by Gartner , customers who are “ enabled ” with visibility into the supply chain are twice as likely to return ; however , only 23 % of supply chains currently enable customers this way . O ’ Bara points to digital labeling as a perfect example of the supply chain future . Digital labels accessed through 2D barcodes by smart devices could provide consumers with information about hundreds of product attributes , such as nutrition , as well as facts that go beyond the label such as environmental , lifestyle , and sustainability factors . This future-forward approach to an increasingly phygital world could drive long-term consumer engagement , and open the door for increased business growth . “ Once you have unlocked value from unique identifiers , there are so many more ways that you can think creatively and cross-functionally about how unifying standards along a supply chain can enable commercial functions and consumer engagement with potential to drive substantial top- and bottom-line revenue , ” says O ’ Bara . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['partnership', 'withgs1', 'product', 'shortage', 'supplychain', 'delay', 'global', 'covid19', 'pandemic', 'still', 'fresh', 'memory', 'consumer', 'industry', 'concerned', 'next', 'geopolitical', 'climate', 'event', 'similar', 'impact', 'backdrop', 'evolve', 'regulation', 'condition', 'mean', 'manufacturer', 'want', 'prepare', 'short', 'supply', 'concerned', 'customer', 'weaken', 'margin', 'supply', 'chain', 'professional', 'achieve', 'phygital', 'information', 'flow', 'blending', 'physical', 'digital', 'datum', 'key', 'unlock', 'resilience', 'efficiency', 'physical', 'object', 'travel', 'supply', 'chain', 'generate', 'rich', 'flow', 'datum', 'item', 'journey', 'raw', 'material', 'manufacturing', 'condition', 'even', 'expiration', 'date', 'bring', 'new', 'visibility', 'pinpointing', 'bottleneck', 'phygital', 'information', 'flow', 'offer', 'significant', 'advantage', 'enhance', 'ability', 'create', 'rich', 'customer', 'experience', 'satisfy', 'environmental', 'social', 'corporate', 'governance', 'esg', 'goal', 'global', 'survey', 'executive', 'respondent', 'agree', 'sustainable', 'supply', 'chain', 'increase', 'company', 'revenue', 'disparate', 'party', 'exchange', 'product', 'information', 'effectively', 'require', 'common', 'framework', 'universally', 'understand', 'language', 'supply', 'chain', 'player', 'datum', 'standard', 'create', 'share', 'foundation', 'standard', 'uniquely', 'identify', 'accurately', 'capture', 'automatically', 'share', 'critical', 'information', 'product', 'location', 'asset', 'trading', 'community', 'push', 'digital', 'standard', 'supply', 'chain', 'datum', 'power', 'lie', 'consistency', 'accuracy', 'seamless', 'sharing', 'fuel', 'analytic', 'generate', 'insight', 'operation', 'standard', 'help', 'precisely', 'describe', 'physical', 'digital', 'object', 'make', 'supply', 'chain', 'track', 'happen', 'production', 'delivery', 'increase', 'visibility', 'sharp', 'focus', 'accord', 'survey', 'supply', 'chain', 'leader', 'mckinsey', 'company', 'respondent', 'nearly', 'sector', 'invest', 'digital', 'supply', 'chain', 'technology', 'previous', 'year', 'standard', 'rely', 'number', 'attribution', 'system', 'encode', 'datum', 'carrier', 'attach', 'product', 'uniquely', 'identify', 'asset', 'level', 'datum', 'capture', 'provide', 'digital', 'access', 'information', 'product', 'movement', 'supply', 'chain', 'numbering', 'attribution', 'system', 'global', 'trade', 'item', 'number', 'gtin', 'identify', 'trade', 'item', 'product', 'likewise', 'serial', 'shipping', 'container', 'code', 'ssccs', 'identify', 'logistic', 'unit', 'global', 'location', 'number', 'gln', 'identify', 'business', 'datum', 'include', 'invoice', 'address', 'delivery', 'location', 'global', 'product', 'classification', 'gpc', 'code', 'global', 'standard', 'use', 'hierarchical', 'system', 'classify', 'item', 'characteristic', 'datum', 'carrier', 'include', 'universal', 'product', 'code', 'barcode', 'onedimensional', '1d', 'barcode', 'familiar', 'consumer', 'commonly', 'scan', 'point', 'sale', 'european', 'article', 'number', 'ean', 'barcode', 'use', 'barcode', 'encode', 'gtin', 'identifi', 'datum', 'recent', 'year', 'complex', 'robust', 'datum', 'carrier', 'become', 'common', 'include', 'radiofrequency', 'identification', 'rfid', 'tag', 'twodimensional', 'barcode', 'qr', 'code', 'quickresponse', 'code', 'code', 'contain', 'vastly', 'datum', 'simple', '1d', 'barcode', 'identification', 'datum', 'capture', 'standard', 'work', 'information', 'sharing', 'include', 'master', 'datum', 'business', 'transaction', 'datum', 'physical', 'event', 'datum', 'communication', 'standard', 'share', 'information', 'application', 'partner', 'phygital', 'information', 'meet', 'wide', 'range', 'need', 'include', 'regulatory', 'compliance', 'consumer', 'patient', 'engagement', 'protection', 'supply', 'chain', 'trading', 'partner', 'requirement', 'procurement', 'production', 'marketing', 'esg', 'report', 'regulation', 'important', 'industry', 'driver', 'chain', 'custody', 'authentication', 'product', 'trading', 'partner', 'vital', 'safe', 'secure', 'supply', 'chain', 'government', 'regulatory', 'agency', 'leverage', 'pervasiveness', 'standard', 'adoption', 'global', 'goal', 'food', 'product', 'consumer', 'safety', 'say', 'bara', 'senior', 'vice', 'president', 'community', 'engagement', 'gs1', 'member', 'gs1', 'global', 'notforprofit', 'supply', 'chain', 'standard', 'organization', 'global', 'standard', 'unique', 'identifier', 'drive', 'today', 'supply', 'chain', 'evolution', 'also', 'allow', 'robust', 'use', 'case', 'wide', 'variety', 'industry', 'example', 'consider', 'healthcare', 'today', 'healthcare', 'organization', 'pressure', 'improve', 'patient', 'outcome', 'prevent', 'error', 'control', 'cost', 'identification', 'system', 'help', 'empower', 'patient', 'information', 'help', 'follow', 'medical', 'protocol', 'know', 'healthcare', 'critical', 'part', 'world', 'people', 'access', 'healthcare', 'follow', 'clinical', 'instruction', 'say', 'bara', 'offer', 'example', 'home', 'nebulizer', 'device', 'use', 'deliver', 'medicine', 'improve', 'respiratory', 'symptom', 'equip', 'nebulizer', 'rfid', 'chip', 'say', 'patient', 'keep', 'track', 'follow', 'prescribed', 'treatment', 'instance', 'filter', 'nebulizer', 'lock', 'device', 'chip', 'send', 'signal', 'nebulizer', 'display', 'patient', 'correct', 'time', 'filter', 'consume', 'mechanism', 'also', 'convey', 'healthcare', 'practitioner', 'patient', 'follow', 'protocol', 'properly', 'result', 'low', 'risk', 'patient', 'miscommunication', 'improve', 'patient', 'care', 'retail', 'datum', 'item', 'origin', 'prevent', 'business', 'loss', 'enhance', 'public', 'safety', 'example', 'grocery', 'store', 'product', 'recall', 'spinach', 'bacterial', 'outbreak', 'able', 'trace', 'origin', 'batch', 'destroy', 'entire', 'inventory', 'unique', 'identifier', 'improve', 'speed', 'accuracy', 'traceability', 'recall', 'public', 'safety', 'precision', 'cost', 'effectiveness', 'consumer', 'good', 'barcode', 'bottle', 'hand', 'lotion', 'reveal', 'vast', 'amount', 'datum', 'consumer', 'include', 'origin', 'ingredient', 'organic', 'certification', 'packaging', 'material', 'industry', 'unique', 'identification', 'tell', 'warehouse', 'worker', 'product', 'locate', 'inform', 'distributor', 'product', 'contain', 'potentially', 'dangerous', 'ingredient', 'warn', 'retailer', 'product', 'age', 'restriction', 'datum', 'deliver', 'value', 'direction', 'supply', 'chain', 'say', 'bara', 'datum', 'standard', 'way', 'accurately', 'consistently', 'confidence', 'obtain', 'rely', 'datum', 'point', 'complete', 'business', 'operation', 'say', 'manufacture', 'achieve', 'esg', 'compliance', 'hinge', 'organization', 'supply', 'chain', 'visibility', 'say', 'bara', 'always', 'datum', 'support', 'esg', 'claim', 'way', 'get', 'datum', 'track', 'consistent', 'calculated', 'method', 'matter', 'consume', 'standard', 'provide', 'access', 'structured', 'sustainability', 'information', 'measure', 'ensure', 'compliance', 'esg', 'regulation', 'share', 'supply', 'chain', 'partner', 'standard', 'empower', 'organization', 'identify', 'capture', 'share', 'information', 'seamlessly', 'create', 'common', 'language', 'support', 'business', 'process', 'savvy', 'organization', 'go', 'step', 'far', 'provide', 'customer', 'direct', 'access', 'supply', 'chain', 'valuable', 'datum', 'accord', 'research', 'customer', 'enable', 'visibility', 'supply', 'chain', 'twice', 'likely', 'return', 'however', 'supply', 'chain', 'currently', 'enable', 'customer', 'way', 'bara', 'point', 'digital', 'labeling', 'perfect', 'example', 'supply', 'chain', 'future', 'digital', 'label', 'access', '2d', 'barcode', 'smart', 'device', 'provide', 'consumer', 'information', 'hundred', 'product', 'attribute', 'nutrition', 'well', 'fact', 'go', 'label', 'environmental', 'lifestyle', 'sustainability', 'factor', 'futureforward', 'approach', 'increasingly', 'phygital', 'world', 'drive', 'longterm', 'consumer', 'engagement', 'open', 'door', 'increase', 'business', 'growth', 'unlocked', 'value', 'unique', 'identifier', 'many', 'way', 'think', 'creatively', 'crossfunctionally', 'unifying', 'standard', 'supply', 'chain', 'enable', 'commercial', 'function', 'consumer', 'engagement', 'potential', 'drive', 'substantial', 'top', 'bottomline', 'revenue', 'say', 'bara', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","The product shortages and supply-chain delays of the global covid-19 pandemic are still fresh memories. Consumers and industry are concerned that the next geopolitical climate event may have a similar impact. Against a backdrop of evolving regulations, these conditions mean manufacturers want to be prepared against short supplies, concerned customers, and weakened margins. For supply…"
The rise of the tech ethics congregation,https://www.technologyreview.com/2023/08/15/1077369/tech-ethics-congregation/,2023-08-15,"Just before Christmas last year, a pastor preached a gospel of morals over money to several hundred members of his flock. Wearing a sport coat, angular glasses, and wired earbuds, he spoke animatedly into his laptop from his tiny glass office inside a co-working space, surrounded by six whiteboards filled with his feverish brainstorming. Sharing a scriptural parable familiar to many in his online audience—a group assembled from across 48 countries, many in the Global South—he explained why his congregation was undergoing dramatic growth in an age when the life of the spirit often struggles to compete with cold, hard, capitalism. “People have different sources of motivation [for getting involved in a community],” he sermonized. “It’s not only money. People actually have a deeper purpose in life.”  Many of the thousands of people who’d been joining his community were taking the time and energy to do so “because they care about the human condition, and they care about the future of our democracy,” he argued. “That is not academic,” he continued. “That is not theoretical. That is talking about future generations, that’s talking about your happiness, that’s talking about how you see the world. This is big … a paradigm shift.” The leader in question was not an ordained minister, nor even a religious man. His increasingly popular community is not—technically—a church, synagogue, or temple. And the scripture he referenced wasn’t from the Bible. It was Microsoft Encarta vs. Wikipedia—the story of how a movement of self-­motivated volunteers defeated an army of corporate-funded professionals in a crusade to provide information, back in the bygone days of 2009. “If you’re young,” said the preacher, named David Ryan Polgar, “you’ll need to google it.” Polgar, 44, is the founder of All Tech Is Human, a nonprofit organization devoted to promoting ethics and responsibility in tech. Founded in 2018, ATIH is based in Manhattan but hosts a growing range of in-person programming—social mixers, mentoring opportunities, career fairs, and job-seeking resources—in several other cities across the US and beyond, reaching thousands. Such numbers would delight most churches.  Like other kinds of congregations, ATIH focuses on relationship-­building: the staff invests much of its time, for example, in activities like curating its “Responsible Tech Organization” list, which names over 500 companies in which community members can get involved, and growing its responsible-tech talent pool, a list of nearly 1,400 individuals interested in careers in the field. Such programs, ATIH says, bring together many excellent but often disconnected initiatives, all in line with the ATIH mission “to tackle wicked tech & society issues and co-create a tech future aligned with the public interest.” The organization itself doesn’t often get explicitly political with op-eds or policy advocacy. Rather, All Tech Is Human’s underlying strategy is to quickly expand the “responsible-tech ecosystem.” In other words, its leaders believe there are large numbers of individuals in and around the technology world, often from marginalized backgrounds, who wish tech focused less on profits and more on being a force for ethics and justice. These people will be a powerful force, Polgar believes, if—as the counterculture icon Timothy Leary famously exhorted—they can “find the others.” If that sounds like reluctance to take sides on hot-button issues in tech policy, or to push for change directly, Polgar calls it an “agnostic” business model. And such a model has real strengths, including the ability to bring tech culture’s opposing tribes together under one big tent.  Companies say they want ethical AI. But those working in the field say that ambition comes at their expense. But as we’ll see, attempts to stay above the fray can cause more problems than they solve. Meanwhile, All Tech Is Human is growing so fast, with over 5,000 members on its Slack channel as of this writing, that if it were a church, it would soon deserve the prefix “mega.” The group has also consistently impressed me with its inclusiveness: the volunteer and professional leadership of women and people of color is a point of major emphasis, and speaker lineups are among the most heterogeneous I’ve seen in any tech-related endeavor. Crowds, too, are full of young professionals from diverse backgrounds who participate in programs out of passion and curiosity, not hope of financial gain. Well, at least attendees don’t go to ATIH for direct financial gain; as is true with many successful religious congregations, the organization serves as an intentional incubator for professional networking.  Still, having interviewed several dozen attendees, I’m convinced that many are hungry for communal support as they navigate a world in which tech has become a transcendent force, for better or worse. Growth has brought things to a turning point. ATIH now stands to receive millions of dollars—including funds from large foundations and tech philanthropist demigods who once ignored it. And Polgar now finds himself in a networking stratosphere with people like Canadian prime minister Justin Trudeau, among other prominent politicos. Will the once-humble community remain dedicated to centering people on the margins of tech culture? Or will monied interests make it harder to fight for the people Christian theologians might call “the least of these”? Techno-solutionism and related ideas can function as a kind of theology, justifying harm in the here and now with the promise of a sweet technological hereafter. I first started looking into ATIH in late 2021, while researching my forthcoming book Tech Agnostic: How Technology Became the World’s Most Powerful Religion, and Why It Desperately Needs a Reformation (MIT Press, 2024). The book project began because I’d been coming across a striking number of similarities between modern technological culture and religion, and the parallels felt important, given my background. I am a longtime (nonreligious) chaplain at both Harvard and MIT. After two decades immersed in the world of faith, back in 2018 I gave up on what had been my dream: to build a nonprofit “godless congregation” for the growing population of atheists, agnostics, and the religiously unaffiliated. Having started that work just before social media mavens like Mark Zuckerberg began to speak of “connecting the world,” I ultimately lost faith in the notion of building community around either religion or secularism when I realized that technology had overtaken both. Indeed, tech seems to be the dominant force in our economy, politics, and culture, not to mention a daily obsession that can increasingly look like an addiction from which some might plausibly seek the help of a higher power to recover. Tech culture has long been known for its prophets (Jobs, Gates, Musk, et al.), and tech as a whole is even increasingly oriented around moral and ethical messages, such as Google’s infamous “Don’t be evil.”  The tech-as-religion comparison I’ve found myself drawing is often unflattering to tech leaders and institutions. Techno-solutionism and related ideas can function as a kind of theology, justifying harm in the here and now with the promise of a sweet technological hereafter; powerful CEOs and investors can form the center of a kind of priestly hierarchy, if not an outright caste system; high-tech weapons and surveillance systems seem to threaten an apocalypse of biblical proportions.  When I discovered ATIH, I was pleasantly surprised to find a potentially positiveexample of the sort of dynamic I was describing. I am the sort of atheist who admits that certain features of religion can offer people real benefits. And ATIH seemed to be succeeding precisely because it genuinely operated like a secular, tech-­ethics-focused version of a religious congregation. “It does work that way,” Polgar acknowledged in February 2022, in the first of our several conversations on the topic. Since then, I’ve continued to admire ATIH’s communal and ethical spirit, while wondering whether communities devoted explicitly to tech ethics might just help bring about a reformation that saves tech from itself. Along with admiration, I’ve also sought to determine whether ATIH is worthy of our faith. I discovered ATIH’s events in late 2021, first through the online Responsible Tech University Summit, a day-long program dedicated to exploring the intersections of tech ethics and campus life. (One of ATIH’s signature programs is its Responsible Tech University Network, which involves, among other things, a growing group of over 80 student “university ambassadors” who represent the organization on their campuses.) All the organization’s programs are organized around typical tech ethics themes, like “the business case for AI ethics,” but participants attend as much for the community as for the topic at hand.  Sarah Husain, who’d worked on Twitter’s Trust and Safety team until it was eliminated by Elon Musk, told me at a May 2022 event that several colleagues in her field had spoken highly of ATIH, recommending she attend. Chana Deitsch, an undergraduate business student who participates in ATIH’s mentorship program, says it not only helps with job leads and reference letters but provides a sense of confidence and belonging. Alex Sarkissian, formerly a Deloitte consultant and now a Buddhist chaplaincy student, feels that the organization has potential “to be a kind of spiritual community for me in addition to my sangha [Buddhist congregation].” I’ve encountered mainly earnest and insightful members like these, people who come together for serious mutual support and ethical reflection and—non-trivially—funaround a cause I’ve come to hold dear. Granted, few ATIH participants, in my observation, hold C-level tech positions, which could undermine the organization’s claims that it has the ability to unite stakeholders toward effectual action … or perhaps it simply signifies a populism that could eventually put sympathizers in high places? Despite my skepticism toward both theology and technology, ATIH has often given me the feeling that I’ve found my own tech tribe. Polgar is a nerdily charismatic former lawyer who has been developing the ideas and networks from which the organization sprouted for over a decade. As a young professor of business law at a couple of small, under-resourced colleges in Connecticut in the early 2010s, he began pondering the ethics of technologies that had recently emerged as dominant and ubiquitous forces across society and culture. Adopting the title “tech ethicist,” he began to write a series of missives on digital health and the idea of “co-creating a better tech future.” His 2017 Medium post “All Tech Is Human,” about how technology design should be informed by more than robotic rationality or utility, generated enthusiastic response and led to the formal founding of the organization a year later. The ATIH concept took a while to catch on, Polgar told me. He worked unpaid for three years and came “close to quitting.” But his background inspired perseverance. Born in 1979 in Cooperstown, New York, Polgar was a philosophical kid who admired Nikola Tesla and wanted to be an inventor. “Why can’t I start something big,” he remembers thinking back then, “even from a little place like this?” Despite their growing influence, Polgar and the organization continue to emphasize their outsider status. ATIH, he argues, is building its following in significant part with people who, for their interest in ethical approaches to technology, feel as unjustly ignored as he and many of his upstate peers felt in the shadow of New York City. ATIH’s model, says the organization’s head of partnerships, Sandra Khalil, is to offer not a “sage on the stage” but, rather, a “guide on the side.” Khalil, a veteran of the US Departments of State and Homeland Security, also came to the organization with an outsider’s pugnacity, feeling “severely underutilized” in previous roles as a non-lawyer intent on “challenging the status quo.” Polgar, however, hardly shrinks from opportunities to influence tech discourse, whether through media interviews with outlets like the BBC World News or by joining advisory boards like TikTok’s content advisory council. ATIH admits, in its “Ten Principles,” that it draws both from grassroots models, which it says “have ideas but often lack power,” and from “top-down” ones, which can “lack a diversity of ideas” but “have power.” The organization does not ask for or accept membership fees from participants, relying instead on major donations solicited by Polgar and his team, who control decision-making. There hasn’t seemed to be a significant call for more democracy—yet. Part of why I’m insisting ATIH is a congregation is that the group assembled around Polgar demonstrates a religious zeal for organizing and relationship-building as tools for advancing positive moral values. Case in point: Rebekah Tweed, ATIH’s associate director, once worked in an actual church, as a youth pastor; now she applies a skill set my field calls “pastoral care” to creating mutually supportive space for ethically minded techies. In 2020, Tweed volunteered on ATIH’s first major public project, the Responsible Tech Guide, a crowdsourced document that highlighted the hundreds of people and institutions working in the field. After she formally joined the organization, it landed its first big-time donation: $300,000 over two years from the Ford Foundation, to pay her salary as well as Polgar’s. They were its first full-time employees.  Polgar was repeatedly rebuffed in early attempts to recruit large gifts, but of late, the growing ATIH team has received significant support from sources including Melinda French Gates’s Pivotal Ventures and about half a million dollars each from Schmidt Futures (the philanthropic fund of former Google CEO Eric Schmidt) and the Patrick J. McGovern Foundation (yet another tech billionaire’s fortune). Can an organization that serves a truly inclusive audience afford to get in bed with Fortune 500 companies and/or multibillionaires who will inevitably be motivated by a desire to seem ethical? The question is: Can an organization that serves a truly inclusive audience, emphasizing humanity and ethics in its own name, afford to get in bed with Fortune 500 companies like Google and Microsoft and/or multibillionaires who will inevitably be motivated by a desire to seem ethical and responsible, even when they decidedly are not? Or rather, can it afford not to do so, when growth means the organization’s staff can grow (and earn a living wage)? And could such tensions someday cause a full-blown schism in the ATIH community?  The potential challenges first came to light for me at a May 2022 summit in New York. For the first time in several large ATIH events I had personally observed, the meeting featured an invited speaker employed by one of the world’s largest tech companies: Harsha Bhatlapenumarthy, a governance manager at Meta and also a volunteer leader in a professional association called Trust and Safety.  Bhatlapenumarthy—whose panel was called “Tech Policy & Social Media: Where are we headed?”—avoided addressing any of her employer’s recent controversies. Instead of offering any meaningful comment in response to Meta’s troubles over its handling of things from pro-anorexia content to election misinformation, she spoke only vaguely about its ethical responsibilities. The company, she said, was focused on “setting the content moderator up for success.” Which is an interesting way to describe a situation in which Meta had, for example, recently been sued for union busting and human trafficking by content moderators in Kenya. Several attendees were taken aback that Bhatlapenumarthy’s advocacy for her powerful employer went essentially unchallenged during the panel. Among them was Yael Eisenstat, Facebook’s former global head of election integrity operations for political advertising and the summit’s closing speaker. In a fireside chat immediately following the panel in which Bhatlapenumarthy participated, Eisenstat, who’d been a whistleblower against her former employer, eloquently dismissed Bhatlapenumarthy’s non-remarks. “I believe [Meta] doesn’t want this on their platform,” she said, referring to violent and deceptive content, “but they will not touch their business model.” Eisenstat added that she would feel “more encouraged” if companies would stop “holding up the founder as a god.”  Eisenstat added to me later, by private message, that “sending a more junior-level employee to speak one-directionally about Meta’s vision of responsible tech is somewhat disingenuous.” In inviting such a speaker, couldn’t ATIH reasonably be understood to be implicated in the offense? If Bhatlapenumarthy’s presence as a seeming mouthpiece for Big Tech talking points had been an isolated incident, I might have ignored it. But a few months later, I found myself wondering if a concerning pattern was emerging. In September 2022, I attended Building a Better Tech Future for Children, an ATIH event cohosted with the Joan Ganz Cooney Center at Sesame Workshop, a nonprofit research and innovation lab associated with the legendary children’s TV show Sesame Street. This struck me as a shrewd partnership for ATIH: every congregation needs a Sunday school. A community organization aspiring to the advancement of humanity and the betterment of the world will inevitably turn its thoughts to educating the next generation according to its values.  After a keynote from Elizabeth Milovidov, senior manager for digital child safety at the Lego Group, on designing digital experiences with children’s well-being in mind came a panel featuring speakers from influential players such as the Omidyar Network and TikTok, as well as young activists. The group discussed the risks and harms facing young people online, and the general tone was optimistic that various efforts to protect them would be successful, particularly if built upon one another. “Digital spaces can be a positive source in the lives of young people,” said the moderator, Mina Aslan. Technology is changing how we read—and that means we need to rethink how we teach. Also on the panel was Harvard Medical School professor Michael Rich, a self-proclaimed “mediatrician”—a portmanteau of “media’’ and “pediatrician.” Rich made good points—for example, stressing the importance of asking kids what they’re hoping for from tech, not just talking about the risks they confront. But one comment triggered my spider-sense: when he said that today’s tech is like his generation’s cigarettes, in that you can’t just tell kids “Don’t do it.”  The analogy between tobacco and social media is at best a bizarre one to draw. Millions of young people became smokers not just through peer pressure, but because for decades, Big Tobacco’s whole business model was built on undue corporate influence and even outright lying, including paying influential doctors and scientists to downplay the death they dealt. Surely ATIH’s leadership would want to avoid any hint that such practices would be acceptable in tech? Tobacco eventually became among the most heavily regulated industries in history, with results including, famously, the US surgeon general’s warnings on tobacco ads and packages. Now the current surgeon general, Vivek Murthy, has warned there is “growing evidence” that social media is “associated with harm to young people’s mental health.” But on the panel (and in his commentary elsewhere), Rich only briefly acknowledged such potential harms, forgoing talk of regulating social media for the idea of cultivating “resilience” in the industry’s millions of young customers. To be clear, I agree with Rich that it is a losing strategy to expect young people to completely abstain from social media. But I fear that tech and our broader society alike are not taking nearly enough ethical responsibility for protecting children from what can be powerful engines of harm. And I was disappointed to see Rich’s relatively sanguine views not only expressed but centered at an ATIH meeting. How much responsibility should a “responsible tech” organization like ATIH take—or not—for inviting speakers with corporate ties, especially when it is not fully open with its audience about such ties? How obligated is ATIH to publicly interrogate the conclusions of such speakers?  Rich’s response to questions I’d asked after his panel was, essentially, that parents ought to channel their energies into making “better choices” around tech, which—conveniently for some of the doctor’s corporate sponsors—lays the responsibility for children’s safety on the parents instead of the tech industry. His lab, I later learned, raised nearly $6 million in 2022, at least partly through grants from Meta, TikTok, and Amazon. When TikTok CEO Shou Chew testified before the US Congress in March 2023, he cited Rich’s lab—and only Rich’s lab—as an example of how TikTok used science and medicine to protect minors. Does this represent a conflict of interest—and therefore a serious ethical failing on the part of both Rich and ATIH for platforming him? I don’t know. I do worry, though, that there’s something inhumane in Rich’s emphasis on building kids’ “resilience” rather than interrogating why they should have to be so resilient against tech in the first place. What kind of institution does ATIH want to be? One that pushes back against the powerful, or one that upholds a corporate-friendly version of diversity, allowing its wealthy sponsors to remain comfortable at (almost) all times? As the Gospel of Matthew says, no man (or organization of “humans”) can serve two masters. Asking around ATIH’s network about my concerns, I found ambivalence. “I do believe it is possible to do research sponsored by companies ethically,” said Justin Hendrix, an occasional ATIH participant and editor of Tech Policy Press, a wonky journal in which academics and others tend to critique established tech narratives. “But it is right to scrutinize it for signs of impropriety.” “I see your concern,” Polgar later told me when I asked him about my apprehensions. Raising his brow with a look of surprise when I wondered aloud whether Rich’s funding sources might have affected the commentary he offered for ATIH’s audience, Polgar made clear he did not agree with all the doctor’s views. He also admitted it is his “worst fear” that his organization might be co-opted by funding opportunities that make it harder “to be a speaker of truth.” “Don’t become a parody of yourself,” he said, seeming to turn the focus of his homily inward. Several months after the Sesame Workshop event, I attended a crowded mixer at ATIH’s now-regular monthly venue, the Midtown Manhattan offices of the VC firm Betaworks, with a very different kind of speaker: the tech critic Douglas Rushkoff, a freethinker who has often spoken of the need for a kind of secular faith in our common humanity in the face of tech capitalism’s quasi-religious extremism. Polgar is a longtime admirer of his work. “All tech bros are human,” Rushkoff cracked, launching into an enthusiastically received talk. Fresh off a publicity tour for a book about tech billionaires buying luxury bunkers to escape a potential doomsday of their own making, Rushkoff provided a starkly antiauthoritarian contrast to the speakers I’d taken issue with at the earlier events. Ultimately, I don’t know whether ATIH will succeed in its attempts to serve what Rushkoff would call “team human” rather than becoming an accessory to the overwhelming wealth tech can generate by seeming to make humanity commodifiable and, ultimately, redundant. I do, however, continue to believe that building a more humane tech future will require communal support, because none of us can do it alone.  I chose the theme of tech agnosticism for my book in part because I am often reminded that I truly don’t know—and neither do you—when or where tech’s enormous powers might actually do the good they purport to do. But I suspect we’re going to need a lot more of what Neil Postman’s 1992 book Technopoly, an early exploration of the theme of tech-as-­religion and a precursor to the techlash, called “loving resistance fighters.” While I lack prophetic abilities to know whether Polgar and co. will help spark such a resistance, the potential is genuinely there. In a participatory congregation, one can always worry about co-­option, as even Polgar himself admits he does; but isn’t it also the responsibility of each of us to actively help keep our communities accountable to their own ethical values? Let’s maintain our skepticism, while hoping the ethical tech congregation gives us continued reason to keep the faith.  Greg M. Epstein serves as the humanist chaplain at Harvard University and MIT and as the convener for ethical life at MIT’s Office of Religious, Spiritual, and Ethical Life. ","Just before Christmas last year , a pastor preached a gospel of morals over money to several hundred members of his flock . Wearing a sport coat , angular glasses , and wired earbuds , he spoke animatedly into his laptop from his tiny glass office inside a co-working space , surrounded by six whiteboards filled with his feverish brainstorming . Sharing a scriptural parable familiar to many in his online audience—a group assembled from across 48 countries , many in the Global South—he explained why his congregation was undergoing dramatic growth in an age when the life of the spirit often struggles to compete with cold , hard , capitalism . “ People have different sources of motivation [ for getting involved in a community ] , ” he sermonized . “ It ’ s not only money . People actually have a deeper purpose in life. ” Many of the thousands of people who ’ d been joining his community were taking the time and energy to do so “ because they care about the human condition , and they care about the future of our democracy , ” he argued . “ That is not academic , ” he continued . “ That is not theoretical . That is talking about future generations , that ’ s talking about your happiness , that ’ s talking about how you see the world . This is big … a paradigm shift. ” The leader in question was not an ordained minister , nor even a religious man . His increasingly popular community is not—technically—a church , synagogue , or temple . And the scripture he referenced wasn ’ t from the Bible . It was Microsoft Encarta vs. Wikipedia—the story of how a movement of self-­motivated volunteers defeated an army of corporate-funded professionals in a crusade to provide information , back in the bygone days of 2009 . “ If you ’ re young , ” said the preacher , named David Ryan Polgar , “ you ’ ll need to google it. ” Polgar , 44 , is the founder of All Tech Is Human , a nonprofit organization devoted to promoting ethics and responsibility in tech . Founded in 2018 , ATIH is based in Manhattan but hosts a growing range of in-person programming—social mixers , mentoring opportunities , career fairs , and job-seeking resources—in several other cities across the US and beyond , reaching thousands . Such numbers would delight most churches . Like other kinds of congregations , ATIH focuses on relationship-­building : the staff invests much of its time , for example , in activities like curating its “ Responsible Tech Organization ” list , which names over 500 companies in which community members can get involved , and growing its responsible-tech talent pool , a list of nearly 1,400 individuals interested in careers in the field . Such programs , ATIH says , bring together many excellent but often disconnected initiatives , all in line with the ATIH mission “ to tackle wicked tech & society issues and co-create a tech future aligned with the public interest. ” The organization itself doesn ’ t often get explicitly political with op-eds or policy advocacy . Rather , All Tech Is Human ’ s underlying strategy is to quickly expand the “ responsible-tech ecosystem. ” In other words , its leaders believe there are large numbers of individuals in and around the technology world , often from marginalized backgrounds , who wish tech focused less on profits and more on being a force for ethics and justice . These people will be a powerful force , Polgar believes , if—as the counterculture icon Timothy Leary famously exhorted—they can “ find the others. ” If that sounds like reluctance to take sides on hot-button issues in tech policy , or to push for change directly , Polgar calls it an “ agnostic ” business model . And such a model has real strengths , including the ability to bring tech culture ’ s opposing tribes together under one big tent . Companies say they want ethical AI . But those working in the field say that ambition comes at their expense . But as we ’ ll see , attempts to stay above the fray can cause more problems than they solve . Meanwhile , All Tech Is Human is growing so fast , with over 5,000 members on its Slack channel as of this writing , that if it were a church , it would soon deserve the prefix “ mega. ” The group has also consistently impressed me with its inclusiveness : the volunteer and professional leadership of women and people of color is a point of major emphasis , and speaker lineups are among the most heterogeneous I ’ ve seen in any tech-related endeavor . Crowds , too , are full of young professionals from diverse backgrounds who participate in programs out of passion and curiosity , not hope of financial gain . Well , at least attendees don ’ t go to ATIH for direct financial gain ; as is true with many successful religious congregations , the organization serves as an intentional incubator for professional networking . Still , having interviewed several dozen attendees , I ’ m convinced that many are hungry for communal support as they navigate a world in which tech has become a transcendent force , for better or worse . Growth has brought things to a turning point . ATIH now stands to receive millions of dollars—including funds from large foundations and tech philanthropist demigods who once ignored it . And Polgar now finds himself in a networking stratosphere with people like Canadian prime minister Justin Trudeau , among other prominent politicos . Will the once-humble community remain dedicated to centering people on the margins of tech culture ? Or will monied interests make it harder to fight for the people Christian theologians might call “ the least of these ” ? Techno-solutionism and related ideas can function as a kind of theology , justifying harm in the here and now with the promise of a sweet technological hereafter . I first started looking into ATIH in late 2021 , while researching my forthcoming book Tech Agnostic : How Technology Became the World ’ s Most Powerful Religion , and Why It Desperately Needs a Reformation ( MIT Press , 2024 ) . The book project began because I ’ d been coming across a striking number of similarities between modern technological culture and religion , and the parallels felt important , given my background . I am a longtime ( nonreligious ) chaplain at both Harvard and MIT . After two decades immersed in the world of faith , back in 2018 I gave up on what had been my dream : to build a nonprofit “ godless congregation ” for the growing population of atheists , agnostics , and the religiously unaffiliated . Having started that work just before social media mavens like Mark Zuckerberg began to speak of “ connecting the world , ” I ultimately lost faith in the notion of building community around either religion or secularism when I realized that technology had overtaken both . Indeed , tech seems to be the dominant force in our economy , politics , and culture , not to mention a daily obsession that can increasingly look like an addiction from which some might plausibly seek the help of a higher power to recover . Tech culture has long been known for its prophets ( Jobs , Gates , Musk , et al . ) , and tech as a whole is even increasingly oriented around moral and ethical messages , such as Google ’ s infamous “ Don ’ t be evil. ” The tech-as-religion comparison I ’ ve found myself drawing is often unflattering to tech leaders and institutions . Techno-solutionism and related ideas can function as a kind of theology , justifying harm in the here and now with the promise of a sweet technological hereafter ; powerful CEOs and investors can form the center of a kind of priestly hierarchy , if not an outright caste system ; high-tech weapons and surveillance systems seem to threaten an apocalypse of biblical proportions . When I discovered ATIH , I was pleasantly surprised to find a potentially positiveexample of the sort of dynamic I was describing . I am the sort of atheist who admits that certain features of religion can offer people real benefits . And ATIH seemed to be succeeding precisely because it genuinely operated like a secular , tech-­ethics-focused version of a religious congregation . “ It does work that way , ” Polgar acknowledged in February 2022 , in the first of our several conversations on the topic . Since then , I ’ ve continued to admire ATIH ’ s communal and ethical spirit , while wondering whether communities devoted explicitly to tech ethics might just help bring about a reformation that saves tech from itself . Along with admiration , I ’ ve also sought to determine whether ATIH is worthy of our faith . I discovered ATIH ’ s events in late 2021 , first through the online Responsible Tech University Summit , a day-long program dedicated to exploring the intersections of tech ethics and campus life . ( One of ATIH ’ s signature programs is its Responsible Tech University Network , which involves , among other things , a growing group of over 80 student “ university ambassadors ” who represent the organization on their campuses . ) All the organization ’ s programs are organized around typical tech ethics themes , like “ the business case for AI ethics , ” but participants attend as much for the community as for the topic at hand . Sarah Husain , who ’ d worked on Twitter ’ s Trust and Safety team until it was eliminated by Elon Musk , told me at a May 2022 event that several colleagues in her field had spoken highly of ATIH , recommending she attend . Chana Deitsch , an undergraduate business student who participates in ATIH ’ s mentorship program , says it not only helps with job leads and reference letters but provides a sense of confidence and belonging . Alex Sarkissian , formerly a Deloitte consultant and now a Buddhist chaplaincy student , feels that the organization has potential “ to be a kind of spiritual community for me in addition to my sangha [ Buddhist congregation ] . ” I ’ ve encountered mainly earnest and insightful members like these , people who come together for serious mutual support and ethical reflection and—non-trivially—funaround a cause I ’ ve come to hold dear . Granted , few ATIH participants , in my observation , hold C-level tech positions , which could undermine the organization ’ s claims that it has the ability to unite stakeholders toward effectual action … or perhaps it simply signifies a populism that could eventually put sympathizers in high places ? Despite my skepticism toward both theology and technology , ATIH has often given me the feeling that I ’ ve found my own tech tribe . Polgar is a nerdily charismatic former lawyer who has been developing the ideas and networks from which the organization sprouted for over a decade . As a young professor of business law at a couple of small , under-resourced colleges in Connecticut in the early 2010s , he began pondering the ethics of technologies that had recently emerged as dominant and ubiquitous forces across society and culture . Adopting the title “ tech ethicist , ” he began to write a series of missives on digital health and the idea of “ co-creating a better tech future. ” His 2017 Medium post “ All Tech Is Human , ” about how technology design should be informed by more than robotic rationality or utility , generated enthusiastic response and led to the formal founding of the organization a year later . The ATIH concept took a while to catch on , Polgar told me . He worked unpaid for three years and came “ close to quitting. ” But his background inspired perseverance . Born in 1979 in Cooperstown , New York , Polgar was a philosophical kid who admired Nikola Tesla and wanted to be an inventor . “ Why can ’ t I start something big , ” he remembers thinking back then , “ even from a little place like this ? ” Despite their growing influence , Polgar and the organization continue to emphasize their outsider status . ATIH , he argues , is building its following in significant part with people who , for their interest in ethical approaches to technology , feel as unjustly ignored as he and many of his upstate peers felt in the shadow of New York City . ATIH ’ s model , says the organization ’ s head of partnerships , Sandra Khalil , is to offer not a “ sage on the stage ” but , rather , a “ guide on the side. ” Khalil , a veteran of the US Departments of State and Homeland Security , also came to the organization with an outsider ’ s pugnacity , feeling “ severely underutilized ” in previous roles as a non-lawyer intent on “ challenging the status quo. ” Polgar , however , hardly shrinks from opportunities to influence tech discourse , whether through media interviews with outlets like the BBC World News or by joining advisory boards like TikTok ’ s content advisory council . ATIH admits , in its “ Ten Principles , ” that it draws both from grassroots models , which it says “ have ideas but often lack power , ” and from “ top-down ” ones , which can “ lack a diversity of ideas ” but “ have power. ” The organization does not ask for or accept membership fees from participants , relying instead on major donations solicited by Polgar and his team , who control decision-making . There hasn ’ t seemed to be a significant call for more democracy—yet . Part of why I ’ m insisting ATIH is a congregation is that the group assembled around Polgar demonstrates a religious zeal for organizing and relationship-building as tools for advancing positive moral values . Case in point : Rebekah Tweed , ATIH ’ s associate director , once worked in an actual church , as a youth pastor ; now she applies a skill set my field calls “ pastoral care ” to creating mutually supportive space for ethically minded techies . In 2020 , Tweed volunteered on ATIH ’ s first major public project , the Responsible Tech Guide , a crowdsourced document that highlighted the hundreds of people and institutions working in the field . After she formally joined the organization , it landed its first big-time donation : $ 300,000 over two years from the Ford Foundation , to pay her salary as well as Polgar ’ s . They were its first full-time employees . Polgar was repeatedly rebuffed in early attempts to recruit large gifts , but of late , the growing ATIH team has received significant support from sources including Melinda French Gates ’ s Pivotal Ventures and about half a million dollars each from Schmidt Futures ( the philanthropic fund of former Google CEO Eric Schmidt ) and the Patrick J. McGovern Foundation ( yet another tech billionaire ’ s fortune ) . Can an organization that serves a truly inclusive audience afford to get in bed with Fortune 500 companies and/or multibillionaires who will inevitably be motivated by a desire to seem ethical ? The question is : Can an organization that serves a truly inclusive audience , emphasizing humanity and ethics in its own name , afford to get in bed with Fortune 500 companies like Google and Microsoft and/or multibillionaires who will inevitably be motivated by a desire to seem ethical and responsible , even when they decidedly are not ? Or rather , can it afford not to do so , when growth means the organization ’ s staff can grow ( and earn a living wage ) ? And could such tensions someday cause a full-blown schism in the ATIH community ? The potential challenges first came to light for me at a May 2022 summit in New York . For the first time in several large ATIH events I had personally observed , the meeting featured an invited speaker employed by one of the world ’ s largest tech companies : Harsha Bhatlapenumarthy , a governance manager at Meta and also a volunteer leader in a professional association called Trust and Safety . Bhatlapenumarthy—whose panel was called “ Tech Policy & Social Media : Where are we headed ? ” —avoided addressing any of her employer ’ s recent controversies . Instead of offering any meaningful comment in response to Meta ’ s troubles over its handling of things from pro-anorexia content to election misinformation , she spoke only vaguely about its ethical responsibilities . The company , she said , was focused on “ setting the content moderator up for success. ” Which is an interesting way to describe a situation in which Meta had , for example , recently been sued for union busting and human trafficking by content moderators in Kenya . Several attendees were taken aback that Bhatlapenumarthy ’ s advocacy for her powerful employer went essentially unchallenged during the panel . Among them was Yael Eisenstat , Facebook ’ s former global head of election integrity operations for political advertising and the summit ’ s closing speaker . In a fireside chat immediately following the panel in which Bhatlapenumarthy participated , Eisenstat , who ’ d been a whistleblower against her former employer , eloquently dismissed Bhatlapenumarthy ’ s non-remarks . “ I believe [ Meta ] doesn ’ t want this on their platform , ” she said , referring to violent and deceptive content , “ but they will not touch their business model. ” Eisenstat added that she would feel “ more encouraged ” if companies would stop “ holding up the founder as a god. ” Eisenstat added to me later , by private message , that “ sending a more junior-level employee to speak one-directionally about Meta ’ s vision of responsible tech is somewhat disingenuous. ” In inviting such a speaker , couldn ’ t ATIH reasonably be understood to be implicated in the offense ? If Bhatlapenumarthy ’ s presence as a seeming mouthpiece for Big Tech talking points had been an isolated incident , I might have ignored it . But a few months later , I found myself wondering if a concerning pattern was emerging . In September 2022 , I attended Building a Better Tech Future for Children , an ATIH event cohosted with the Joan Ganz Cooney Center at Sesame Workshop , a nonprofit research and innovation lab associated with the legendary children ’ s TV show Sesame Street . This struck me as a shrewd partnership for ATIH : every congregation needs a Sunday school . A community organization aspiring to the advancement of humanity and the betterment of the world will inevitably turn its thoughts to educating the next generation according to its values . After a keynote from Elizabeth Milovidov , senior manager for digital child safety at the Lego Group , on designing digital experiences with children ’ s well-being in mind came a panel featuring speakers from influential players such as the Omidyar Network and TikTok , as well as young activists . The group discussed the risks and harms facing young people online , and the general tone was optimistic that various efforts to protect them would be successful , particularly if built upon one another . “ Digital spaces can be a positive source in the lives of young people , ” said the moderator , Mina Aslan . Technology is changing how we read—and that means we need to rethink how we teach . Also on the panel was Harvard Medical School professor Michael Rich , a self-proclaimed “ mediatrician ” —a portmanteau of “ media ’ ’ and “ pediatrician. ” Rich made good points—for example , stressing the importance of asking kids what they ’ re hoping for from tech , not just talking about the risks they confront . But one comment triggered my spider-sense : when he said that today ’ s tech is like his generation ’ s cigarettes , in that you can ’ t just tell kids “ Don ’ t do it. ” The analogy between tobacco and social media is at best a bizarre one to draw . Millions of young people became smokers not just through peer pressure , but because for decades , Big Tobacco ’ s whole business model was built on undue corporate influence and even outright lying , including paying influential doctors and scientists to downplay the death they dealt . Surely ATIH ’ s leadership would want to avoid any hint that such practices would be acceptable in tech ? Tobacco eventually became among the most heavily regulated industries in history , with results including , famously , the US surgeon general ’ s warnings on tobacco ads and packages . Now the current surgeon general , Vivek Murthy , has warned there is “ growing evidence ” that social media is “ associated with harm to young people ’ s mental health. ” But on the panel ( and in his commentary elsewhere ) , Rich only briefly acknowledged such potential harms , forgoing talk of regulating social media for the idea of cultivating “ resilience ” in the industry ’ s millions of young customers . To be clear , I agree with Rich that it is a losing strategy to expect young people to completely abstain from social media . But I fear that tech and our broader society alike are not taking nearly enough ethical responsibility for protecting children from what can be powerful engines of harm . And I was disappointed to see Rich ’ s relatively sanguine views not only expressed but centered at an ATIH meeting . How much responsibility should a “ responsible tech ” organization like ATIH take—or not—for inviting speakers with corporate ties , especially when it is not fully open with its audience about such ties ? How obligated is ATIH to publicly interrogate the conclusions of such speakers ? Rich ’ s response to questions I ’ d asked after his panel was , essentially , that parents ought to channel their energies into making “ better choices ” around tech , which—conveniently for some of the doctor ’ s corporate sponsors—lays the responsibility for children ’ s safety on the parents instead of the tech industry . His lab , I later learned , raised nearly $ 6 million in 2022 , at least partly through grants from Meta , TikTok , and Amazon . When TikTok CEO Shou Chew testified before the US Congress in March 2023 , he cited Rich ’ s lab—and only Rich ’ s lab—as an example of how TikTok used science and medicine to protect minors . Does this represent a conflict of interest—and therefore a serious ethical failing on the part of both Rich and ATIH for platforming him ? I don ’ t know . I do worry , though , that there ’ s something inhumane in Rich ’ s emphasis on building kids ’ “ resilience ” rather than interrogating why they should have to be so resilient against tech in the first place . What kind of institution does ATIH want to be ? One that pushes back against the powerful , or one that upholds a corporate-friendly version of diversity , allowing its wealthy sponsors to remain comfortable at ( almost ) all times ? As the Gospel of Matthew says , no man ( or organization of “ humans ” ) can serve two masters . Asking around ATIH ’ s network about my concerns , I found ambivalence . “ I do believe it is possible to do research sponsored by companies ethically , ” said Justin Hendrix , an occasional ATIH participant and editor of Tech Policy Press , a wonky journal in which academics and others tend to critique established tech narratives . “ But it is right to scrutinize it for signs of impropriety. ” “ I see your concern , ” Polgar later told me when I asked him about my apprehensions . Raising his brow with a look of surprise when I wondered aloud whether Rich ’ s funding sources might have affected the commentary he offered for ATIH ’ s audience , Polgar made clear he did not agree with all the doctor ’ s views . He also admitted it is his “ worst fear ” that his organization might be co-opted by funding opportunities that make it harder “ to be a speaker of truth. ” “ Don ’ t become a parody of yourself , ” he said , seeming to turn the focus of his homily inward . Several months after the Sesame Workshop event , I attended a crowded mixer at ATIH ’ s now-regular monthly venue , the Midtown Manhattan offices of the VC firm Betaworks , with a very different kind of speaker : the tech critic Douglas Rushkoff , a freethinker who has often spoken of the need for a kind of secular faith in our common humanity in the face of tech capitalism ’ s quasi-religious extremism . Polgar is a longtime admirer of his work . “ All tech bros are human , ” Rushkoff cracked , launching into an enthusiastically received talk . Fresh off a publicity tour for a book about tech billionaires buying luxury bunkers to escape a potential doomsday of their own making , Rushkoff provided a starkly antiauthoritarian contrast to the speakers I ’ d taken issue with at the earlier events . Ultimately , I don ’ t know whether ATIH will succeed in its attempts to serve what Rushkoff would call “ team human ” rather than becoming an accessory to the overwhelming wealth tech can generate by seeming to make humanity commodifiable and , ultimately , redundant . I do , however , continue to believe that building a more humane tech future will require communal support , because none of us can do it alone . I chose the theme of tech agnosticism for my book in part because I am often reminded that I truly don ’ t know—and neither do you—when or where tech ’ s enormous powers might actually do the good they purport to do . But I suspect we ’ re going to need a lot more of what Neil Postman ’ s 1992 book Technopoly , an early exploration of the theme of tech-as-­religion and a precursor to the techlash , called “ loving resistance fighters. ” While I lack prophetic abilities to know whether Polgar and co. will help spark such a resistance , the potential is genuinely there . In a participatory congregation , one can always worry about co-­option , as even Polgar himself admits he does ; but isn ’ t it also the responsibility of each of us to actively help keep our communities accountable to their own ethical values ? Let ’ s maintain our skepticism , while hoping the ethical tech congregation gives us continued reason to keep the faith . Greg M. Epstein serves as the humanist chaplain at Harvard University and MIT and as the convener for ethical life at MIT ’ s Office of Religious , Spiritual , and Ethical Life .","['last', 'year', 'pastor', 'preach', 'gospel', 'moral', 'money', 'several', 'member', 'flock', 'wear', 'sport', 'coat', 'angular', 'glass', 'wire', 'earbud', 'speak', 'animatedly', 'laptop', 'tiny', 'glass', 'office', 'coworke', 'space', 'surround', 'whiteboard', 'fill', 'feverish', 'brainstorming', 'share', 'scriptural', 'parable', 'familiar', 'many', 'online', 'audience', 'group', 'assemble', 'country', 'many', 'global', 'south', 'explain', 'congregation', 'undergo', 'dramatic', 'growth', 'age', 'life', 'spirit', 'often', 'struggle', 'compete', 'cold', 'hard', 'capitalism', 'people', 'different', 'source', 'motivation', 'involve', 'community', 'sermonize', 'money', 'people', 'actually', 'deep', 'purpose', 'life', 'many', 'thousand', 'people', 'join', 'community', 'take', 'time', 'energy', 'care', 'human', 'condition', 'care', 'future', 'democracy', 'argue', 'academic', 'continue', 'theoretical', 'talk', 'future', 'generation', 'talk', 'happiness', 'talk', 'see', 'world', 'big', 'paradigm', 'shift', 'leader', 'question', 'ordain', 'minister', 'even', 'religious', 'man', 'increasingly', 'popular', 'community', 'technically', 'church', 'synagogue', 'scripture', 'reference', 'story', 'movement', 'self\xadmotivated', 'volunteer', 'defeat', 'army', 'corporatefunded', 'professional', 'crusade', 'provide', 'information', 'back', 'bygone', 'day', 'young', 'say', 'preacher', 'name', 'need', 'google', 'polgar', 'founder', 'tech', 'human', 'nonprofit', 'organization', 'devote', 'promote', 'ethic', 'responsibility', 'tech', 'found', 'atih', 'base', 'host', 'grow', 'range', 'programming', 'social', 'mixer', 'mentor', 'opportunity', 'career', 'fair', 'jobseeke', 'resource', 'several', 'city', 'reach', 'thousand', 'number', 'delight', 'church', 'kind', 'congregation', 'focus', 'relationship\xadbuilde', 'staff', 'invest', 'much', 'time', 'example', 'activity', 'curate', 'responsible', 'tech', 'organization', 'list', 'name', 'company', 'community', 'member', 'involve', 'grow', 'talent', 'pool', 'list', 'nearly', 'individual', 'interest', 'career', 'field', 'program', 'say', 'bring', 'together', 'many', 'excellent', 'often', 'disconnect', 'initiative', 'line', 'mission', 'tackle', 'wicked', 'tech', 'society', 'issue', 'cocreate', 'tech', 'future', 'align', 'public', 'interest', 'organization', 'often', 'get', 'explicitly', 'political', 'oped', 'policy', 'advocacy', 'rather', 'tech', 'human', 'underlie', 'strategy', 'quickly', 'expand', 'responsibletech', 'ecosystem', 'word', 'leader', 'believe', 'large', 'number', 'individual', 'technology', 'world', 'often', 'marginalize', 'background', 'wish', 'tech', 'focus', 'less', 'profit', 'force', 'ethic', 'justice', 'people', 'powerful', 'force', 'polgar', 'believe', 'counterculture', 'icon', 'famously', 'exhort', 'find', 'sound', 'reluctance', 'take', 'side', 'hotbutton', 'issue', 'tech', 'policy', 'push', 'change', 'directly', 'polgar', 'call', 'agnostic', 'business', 'model', 'model', 'real', 'strength', 'include', 'ability', 'bring', 'tech', 'culture', 'oppose', 'tribe', 'together', 'big', 'tent', 'company', 'say', 'want', 'ethical', 'ai', 'work', 'field', 'say', 'ambition', 'come', 'expense', 'see', 'attempt', 'stay', 'fray', 'cause', 'problem', 'solve', 'meanwhile', 'tech', 'human', 'grow', 'fast', 'member', 'slack', 'channel', 'writing', 'church', 'soon', 'deserve', 'prefix', 'mega', 'group', 'also', 'consistently', 'impress', 'inclusiveness', 'volunteer', 'professional', 'leadership', 'woman', 'people', 'color', 'point', 'major', 'emphasis', 'speaker', 'lineup', 'heterogeneous', 'see', 'techrelate', 'endeavor', 'crowd', 'full', 'young', 'professional', 'diverse', 'background', 'participate', 'program', 'passion', 'curiosity', 'hope', 'financial', 'gain', 'well', 'least', 'go', 'atih', 'direct', 'financial', 'gain', 'true', 'many', 'successful', 'religious', 'congregation', 'organization', 'serve', 'intentional', 'incubator', 'professional', 'networking', 'still', 'interview', 'several', 'dozen', 'attendee', 'convinced', 'many', 'hungry', 'communal', 'support', 'navigate', 'world', 'tech', 'become', 'transcendent', 'force', 'well', 'bad', 'growth', 'bring', 'thing', 'turning', 'point', 'stand', 'receive', 'million', 'dollar', 'include', 'fund', 'large', 'foundation', 'tech', 'philanthropist', 'demigod', 'ignore', 'find', 'networking', 'stratosphere', 'people', 'canadian', 'prominent', 'politico', 'oncehumble', 'community', 'remain', 'dedicated', 'center', 'people', 'margin', 'tech', 'culture', 'monie', 'interest', 'make', 'hard', 'fight', 'people', 'christian', 'theologian', 'call', 'least', 'technosolutionism', 'related', 'idea', 'function', 'kind', 'theology', 'justify', 'harm', 'promise', 'sweet', 'technological', 'hereafter', 'first', 'start', 'look', 'atih', 'late', 'research', 'forthcoming', 'book', 'tech', 'agnostic', 'technology', 'become', 'world', 'powerful', 'religion', 'desperately', 'need', 'reformation', 'mit', 'press', 'book', 'project', 'begin', 'come', 'strike', 'number', 'similarity', 'modern', 'technological', 'culture', 'religion', 'parallel', 'feel', 'important', 'give', 'background', 'longtime', 'nonreligious', 'chaplain', 'mit', 'decade', 'immerse', 'world', 'faith', 'back', 'give', 'dream', 'build', 'nonprofit', 'godless', 'congregation', 'grow', 'population', 'atheist', 'agnostic', 'religiously', 'unaffiliated', 'start', 'work', 'social', 'medium', 'maven', 'begin', 'speak', 'connect', 'world', 'ultimately', 'lose', 'faith', 'notion', 'build', 'community', 'religion', 'secularism', 'realize', 'technology', 'overtake', 'indeed', 'tech', 'seem', 'dominant', 'force', 'economy', 'politic', 'culture', 'mention', 'daily', 'obsession', 'increasingly', 'look', 'addiction', 'plausibly', 'seek', 'help', 'high', 'power', 'recover', 'tech', 'culture', 'long', 'know', 'prophet', 'job', 'gate', 'musk', 'et', 'tech', 'whole', 'even', 'increasingly', 'orient', 'moral', 'ethical', 'message', 'infamous', 'evil', 'techasreligion', 'comparison', 'find', 'draw', 'often', 'unflattering', 'tech', 'leader', 'institution', 'technosolutionism', 'related', 'idea', 'function', 'kind', 'theology', 'justify', 'harm', 'promise', 'sweet', 'technological', 'hereafter', 'powerful', 'ceo', 'investor', 'form', 'center', 'kind', 'priestly', 'hierarchy', 'outright', 'caste', 'system', 'hightech', 'weapon', 'surveillance', 'system', 'seem', 'threaten', 'apocalypse', 'biblical', 'proportion', 'discover', 'atih', 'pleasantly', 'surprised', 'find', 'potentially', 'positiveexample', 'sort', 'dynamic', 'describe', 'sort', 'atheist', 'admit', 'certain', 'feature', 'religion', 'offer', 'people', 'real', 'benefit', 'seem', 'succeed', 'precisely', 'genuinely', 'operate', 'secular', 'tech\xadethicsfocused', 'version', 'religious', 'congregation', 'work', 'way', 'polgar', 'acknowledge', 'first', 'several', 'conversation', 'topic', 'continue', 'admire', 'communal', 'ethical', 'spirit', 'wonder', 'community', 'devote', 'explicitly', 'tech', 'ethic', 'help', 'bring', 'reformation', 'save', 'tech', 'admiration', 'also', 'seek', 'determine', 'atih', 'worthy', 'faith', 'discover', 'atih', 'event', 'late', 'first', 'online', 'responsible', 'tech', 'university', 'summit', 'daylong', 'program', 'dedicate', 'explore', 'intersection', 'tech', 'ethic', 'campus', 'life', 'signature', 'program', 'responsible', 'tech', 'university', 'network', 'involve', 'thing', 'grow', 'group', 'student', 'university', 'ambassador', 'represent', 'organization', 'campus', 'organization', 'program', 'organize', 'typical', 'tech', 'ethic', 'theme', 'business', 'case', 'ethic', 'participant', 'attend', 'much', 'community', 'topic', 'hand', 'sarah', 'husain', 'work', 'twitter', 'trust', 'safety', 'team', 'eliminate', 'tell', 'event', 'several', 'colleague', 'field', 'speak', 'highly', 'atih', 'recommend', 'attend', 'undergraduate', 'business', 'student', 'participate', 'mentorship', 'program', 'say', 'help', 'job', 'lead', 'reference', 'letter', 'provide', 'sense', 'confidence', 'belong', 'formerly', 'deloitte', 'consultant', 'buddhist', 'chaplaincy', 'student', 'feel', 'organization', 'potential', 'kind', 'spiritual', 'community', 'addition', 'sangha', 'buddhist', 'congregation', 'encounter', 'mainly', 'earnest', 'insightful', 'member', 'people', 'come', 'together', 'serious', 'mutual', 'support', 'ethical', 'reflection', 'nontrivially', 'funaround', 'cause', 'come', 'hold', 'dear', 'grant', 'atih', 'participant', 'observation', 'hold', 'clevel', 'tech', 'position', 'undermine', 'organization', 'claim', 'ability', 'unite', 'stakeholder', 'effectual', 'action', 'perhaps', 'simply', 'signify', 'populism', 'eventually', 'put', 'sympathizer', 'high', 'place', 'skepticism', 'theology', 'technology', 'often', 'give', 'feeling', 'find', 'tech', 'tribe', 'polgar', 'nerdily', 'charismatic', 'former', 'lawyer', 'develop', 'idea', 'network', 'organization', 'sprout', 'decade', 'young', 'professor', 'business', 'law', 'couple', 'small', 'underresourced', 'college', 'connecticut', 'early', 'begin', 'ponder', 'ethic', 'technology', 'recently', 'emerge', 'dominant', 'ubiquitous', 'force', 'society', 'culture', 'adopt', 'title', 'tech', 'ethicist', 'begin', 'write', 'series', 'missive', 'digital', 'health', 'idea', 'cocreate', 'well', 'tech', 'future', 'medium', 'post', 'tech', 'human', 'technology', 'design', 'inform', 'robotic', 'rationality', 'utility', 'generate', 'enthusiastic', 'response', 'lead', 'formal', 'founding', 'organization', 'year', 'later', 'atih', 'concept', 'take', 'catch', 'polgar', 'tell', 'work', 'unpaid', 'year', 'come', 'close', 'quit', 'background', 'inspire', 'perseverance', 'bear', 'cooperstown', 'polgar', 'philosophical', 'kid', 'admire', 'want', 'inventor', 'start', 'big', 'remember', 'think', 'back', 'even', 'little', 'place', 'grow', 'influence', 'polgar', 'organization', 'continue', 'emphasize', 'outsider', 'status', 'argue', 'build', 'following', 'significant', 'part', 'people', 'interest', 'ethical', 'approach', 'technology', 'feel', 'unjustly', 'ignore', 'many', 'upstate', 'peer', 'feel', 'shadow', 'model', 'say', 'organization', 'head', 'partnership', 'offer', 'sage', 'stage', 'rather', 'guide', 'side', 'khalil', 'veteran', 'department', 'state', 'homeland', 'security', 'also', 'come', 'organization', 'outsider', 'pugnacity', 'severely', 'underutilize', 'previous', 'role', 'nonlawyer', 'intent', 'challenge', 'status', 'quo', 'polgar', 'however', 'hardly', 'shrink', 'opportunity', 'influence', 'tech', 'discourse', 'medium', 'interview', 'outlet', 'join', 'advisory', 'board', 'tiktok', 'advisory', 'admit', 'principle', 'draw', 'grassroots', 'model', 'say', 'idea', 'often', 'lack', 'power', 'topdown', 'one', 'lack', 'diversity', 'idea', 'power', 'organization', 'ask', 'accept', 'membership', 'fee', 'participant', 'rely', 'instead', 'major', 'donation', 'solicit', 'polgar', 'team', 'control', 'decisionmake', 'seem', 'significant', 'call', 'democracy', 'part', 'insist', 'congregation', 'group', 'assemble', 'polgar', 'demonstrate', 'religious', 'zeal', 'organize', 'relationshipbuilde', 'tool', 'advance', 'positive', 'moral', 'value', 'case', 'point', 'rebekah', 'tweed', 'associate', 'director', 'work', 'actual', 'church', 'youth', 'pastor', 'apply', 'skill', 'set', 'field', 'call', 'pastoral', 'care', 'create', 'mutually', 'supportive', 'space', 'ethically', 'minded', 'techie', 'tweed', 'volunteer', 'first', 'major', 'public', 'project', 'responsible', 'tech', 'guide', 'crowdsource', 'document', 'highlight', 'hundred', 'people', 'institution', 'work', 'field', 'formally', 'join', 'organization', 'land', 'first', 'bigtime', 'donation', 'year', 'pay', 'salary', 'well', 'polgar', 'first', 'fulltime', 'employee', 'polgar', 'repeatedly', 'rebuff', 'early', 'attempt', 'recruit', 'large', 'gift', 'late', 'grow', 'atih', 'team', 'receive', 'significant', 'support', 'source', 'include', 'gate', 'pivotal', 'venture', 'dollar', 'schmidt', 'future', 'philanthropic', 'fund', 'former', 'yet', 'tech', 'billionaire', 'fortune', 'organization', 'serve', 'truly', 'inclusive', 'audience', 'afford', 'get', 'bed', 'fortune', 'company', 'andor', 'multibillionaire', 'inevitably', 'motivate', 'desire', 'seem', 'ethical', 'question', 'organization', 'serve', 'truly', 'inclusive', 'audience', 'emphasize', 'humanity', 'ethic', 'name', 'afford', 'get', 'bed', 'fortune', 'company', 'multibillionaire', 'inevitably', 'motivate', 'desire', 'seem', 'ethical', 'responsible', 'even', 'decidedly', 'rather', 'afford', 'growth', 'mean', 'organization', 'staff', 'grow', 'earn', 'living', 'wage', 'tension', 'someday', 'cause', 'fullblown', 'schism', 'community', 'potential', 'challenge', 'first', 'come', 'light', 'summit', 'first', 'time', 'several', 'large', 'atih', 'event', 'personally', 'observe', 'meeting', 'feature', 'invite', 'speaker', 'employ', 'world', 'large', 'tech', 'company', 'governance', 'manager', 'also', 'volunteer', 'leader', 'professional', 'association', 'call', 'trust', 'safety', 'bhatlapenumarthy', 'panel', 'call', 'tech', 'policy', 'social', 'medium', 'head', 'avoid', 'address', 'employer', 'recent', 'controversy', 'instead', 'offer', 'meaningful', 'comment', 'response', 'meta', 'trouble', 'handling', 'thing', 'content', 'election', 'misinformation', 'speak', 'vaguely', 'ethical', 'responsibility', 'company', 'say', 'focus', 'set', 'content', 'moderator', 'success', 'interesting', 'way', 'describe', 'situation', 'meta', 'example', 'recently', 'sue', 'union', 'busting', 'human', 'trafficking', 'content', 'moderator', 'several', 'attendee', 'take', 'aback', 'bhatlapenumarthy', 'advocacy', 'powerful', 'employer', 'go', 'essentially', 'unchallenged', 'panel', 'former', 'global', 'head', 'election', 'integrity', 'operation', 'political', 'advertising', 'summit', 'closing', 'speaker', 'fireside', 'chat', 'immediately', 'follow', 'panel', 'participate', 'eisenstat', 'whistleblower', 'former', 'employer', 'eloquently', 'dismiss', 'bhatlapenumarthy', 'nonremark', 'believe', 'meta', 'want', 'platform', 'say', 'refer', 'violent', 'deceptive', 'content', 'touch', 'business', 'model', 'eisenstat', 'add', 'feel', 'encouraged', 'company', 'stop', 'hold', 'founder', 'eisenstat', 'add', 'later', 'private', 'message', 'send', 'juniorlevel', 'employee', 'speak', 'onedirectionally', 'meta', 'vision', 'responsible', 'tech', 'somewhat', 'disingenuous', 'invite', 'speaker', 'atih', 'reasonably', 'understand', 'implicate', 'offense', 'bhatlapenumarthy', 'presence', 'seem', 'mouthpiece', 'big', 'tech', 'talking', 'point', 'isolated', 'incident', 'ignore', 'month', 'later', 'find', 'wonder', 'concern', 'pattern', 'emerge', 'attend', 'build', 'well', 'tech', 'future', 'child', 'atih', 'event', 'cohoste', 'sesame', 'workshop', 'nonprofit', 'research', 'innovation', 'lab', 'associate', 'legendary', 'child', 'tv', 'show', 'strike', 'shrewd', 'partnership', 'atih', 'congregation', 'need', 'school', 'community', 'organization', 'aspire', 'advancement', 'humanity', 'betterment', 'world', 'inevitably', 'turn', 'thought', 'educate', 'next', 'generation', 'accord', 'value', 'keynote', 'senior', 'manager', 'digital', 'child', 'safety', 'group', 'design', 'digital', 'experience', 'child', 'wellbee', 'mind', 'come', 'panel', 'feature', 'speaker', 'influential', 'player', 'omidyar', 'network', 'tiktok', 'well', 'young', 'activist', 'group', 'discuss', 'risk', 'harm', 'face', 'young', 'people', 'online', 'general', 'tone', 'optimistic', 'various', 'effort', 'protect', 'successful', 'particularly', 'build', 'digital', 'space', 'positive', 'source', 'life', 'young', 'people', 'say', 'moderator', 'technology', 'change', 'read', 'mean', 'need', 'rethink', 'teach', 'also', 'panel', 'medical', 'school', 'professor', 'selfproclaime', 'mediatrician', 'portmanteau', 'medium', 'pediatrician', 'rich', 'make', 'good', 'point', 'example', 'stress', 'importance', 'ask', 'kid', 'hope', 'tech', 'talk', 'risk', 'confront', 'comment', 'trigger', 'spidersense', 'say', 'today', 'tech', 'generation', 'cigarette', 'tell', 'kid', 'analogy', 'tobacco', 'social', 'medium', 'good', 'bizarre', 'one', 'draw', 'million', 'young', 'people', 'become', 'smoker', 'peer', 'pressure', 'decade', 'big', 'tobacco', 'whole', 'business', 'model', 'build', 'undue', 'corporate', 'influence', 'even', 'outright', 'lie', 'include', 'pay', 'influential', 'doctor', 'scientist', 'downplay', 'death', 'deal', 'surely', 'leadership', 'want', 'avoid', 'hint', 'practice', 'acceptable', 'tech', 'tobacco', 'eventually', 'become', 'heavily', 'regulate', 'industry', 'history', 'result', 'include', 'famously', 'general', 'warning', 'tobacco', 'ad', 'package', 'current', 'surgeon', 'general', 'warn', 'grow', 'evidence', 'social', 'medium', 'associate', 'harm', 'young', 'people', 'mental', 'health', 'panel', 'commentary', 'elsewhere', 'rich', 'briefly', 'acknowledge', 'potential', 'harm', 'forgo', 'talk', 'regulate', 'social', 'medium', 'idea', 'cultivate', 'resilience', 'industry', 'million', 'young', 'customer', 'clear', 'agree', 'rich', 'lose', 'strategy', 'expect', 'young', 'people', 'completely', 'abstain', 'social', 'medium', 'fear', 'tech', 'broad', 'society', 'alike', 'take', 'nearly', 'enough', 'ethical', 'responsibility', 'protect', 'child', 'powerful', 'engine', 'harm', 'disappointed', 'see', 'rich', 'relatively', 'sanguine', 'view', 'express', 'center', 'atih', 'meet', 'much', 'responsibility', 'responsible', 'tech', 'organization', 'take', 'invite', 'speaker', 'corporate', 'tie', 'especially', 'fully', 'open', 'audience', 'tie', 'obligated', 'atih', 'publicly', 'interrogate', 'conclusion', 'speaker', 'rich', 'response', 'question', 'ask', 'panel', 'essentially', 'parent', 'channel', 'energy', 'make', 'well', 'choice', 'tech', 'conveniently', 'doctor', 'corporate', 'sponsor', 'lay', 'responsibility', 'child', 'safety', 'parent', 'instead', 'tech', 'industry', 'lab', 'later', 'learn', 'raise', 'nearly', 'least', 'partly', 'grant', 'meta', 'tiktok', 'tiktok', 'ceo', 'testify', 'cite', 'rich', 'lab', 'rich', 'lab', 'example', 'tiktok', 'use', 'science', 'medicine', 'protect', 'minor', 'represent', 'conflict', 'interest', 'therefore', 'serious', 'ethical', 'failing', 'part', 'rich', 'atih', 'platforme', 'know', 'worry', 'inhumane', 'rich', 'emphasis', 'build', 'kid', 'resilience', 'rather', 'interrogate', 'resilient', 'tech', 'first', 'place', 'kind', 'institution', 'atih', 'want', 'one', 'push', 'back', 'powerful', 'one', 'uphold', 'corporatefriendly', 'version', 'diversity', 'allow', 'wealthy', 'sponsor', 'remain', 'comfortable', 'almost', 'time', 'gospel', 'say', 'man', 'organization', 'human', 'serve', 'master', 'ask', 'network', 'concern', 'find', 'ambivalence', 'believe', 'possible', 'research', 'sponsor', 'company', 'ethically', 'say', 'occasional', 'atih', 'participant', 'editor', 'tech', 'policy', 'press', 'wonky', 'journal', 'academic', 'tend', 'critique', 'establish', 'tech', 'narrative', 'right', 'scrutinize', 'sign', 'impropriety', 'see', 'concern', 'polgar', 'later', 'tell', 'ask', 'apprehension', 'raise', 'brow', 'look', 'surprise', 'wonder', 'aloud', 'rich', 'funding', 'source', 'affect', 'commentary', 'offer', 'audience', 'polgar', 'make', 'clear', 'agree', 'doctor', 'view', 'also', 'admit', 'bad', 'fear', 'organization', 'coopte', 'fund', 'opportunity', 'make', 'hard', 'speaker', 'truth', 'become', 'parody', 'say', 'seem', 'turn', 'focus', 'homily', 'inward', 'several', 'month', 'sesame', 'workshop', 'event', 'attend', 'crowded', 'mixer', 'nowregular', 'monthly', 'venue', 'office', 'betawork', 'different', 'kind', 'speaker', 'tech', 'critic', 'dougla', 'freethinker', 'often', 'speak', 'need', 'kind', 'secular', 'faith', 'common', 'humanity', 'face', 'tech', 'capitalism', 'quasireligious', 'extremism', 'polgar', 'longtime', 'admirer', 'work', 'tech', 'bro', 'human', 'rushkoff', 'crack', 'launch', 'enthusiastically', 'receive', 'talk', 'fresh', 'publicity', 'tour', 'book', 'tech', 'billionaire', 'buy', 'luxury', 'bunker', 'escape', 'potential', 'doomsday', 'making', 'rushkoff', 'provide', 'starkly', 'antiauthoritarian', 'contrast', 'speaker', 'take', 'issue', 'early', 'event', 'ultimately', 'know', 'atih', 'succeed', 'attempt', 'serve', 'rushkoff', 'call', 'team', 'rather', 'become', 'accessory', 'overwhelming', 'wealth', 'tech', 'generate', 'seem', 'make', 'humanity', 'commodifiable', 'ultimately', 'redundant', 'however', 'continue', 'believe', 'build', 'humane', 'tech', 'future', 'require', 'communal', 'support', 'none', 'alone', 'choose', 'theme', 'tech', 'agnosticism', 'book', 'part', 'often', 'remind', 'truly', 'know', 'tech', 'enormous', 'power', 'actually', 'good', 'purport', 'suspect', 'go', 'need', 'lot', 'book', 'technopoly', 'early', 'exploration', 'theme', 'techas\xadreligion', 'precursor', 'techlash', 'call', 'love', 'resistance', 'fighter', 'lack', 'prophetic', 'ability', 'know', 'polgar', 'co', 'help', 'spark', 'resistance', 'potential', 'genuinely', 'participatory', 'congregation', 'always', 'worry', 'co\xadoption', 'even', 'polgar', 'admit', 'also', 'responsibility', 'actively', 'help', 'keep', 'community', 'accountable', 'ethical', 'value', 'let', 'maintain', 'skepticism', 'hope', 'ethical', 'tech', 'congregation', 'give', 'continue', 'reason', 'keep', 'faith', 'epstein', 'serve', 'humanist', 'chaplain', 'mit', 'convener', 'ethical', 'life', 'office', 'religious', 'spiritual', 'ethical', 'life']","<p>In a world where tech has become a transcendent force, people are always looking for guidance.</p>
"
Human-plus-AI solutions mitigate security threats,https://www.technologyreview.com/2023/08/10/1077088/human-plus-ai-solutions-mitigate-security-threats/,2023-08-10,"In association withTeleperformance Fifty years ago, the average business transaction was pretty straightforward. Shoppers handed purchases directly to cashiers, business partners shook hands in person, and people brought malfunctioning machines to a repair shop across the street. The proximity of all participating parties meant that both customers and businesses could verify authority and authenticity with their own eyes. But the internet has changed the very nature of how we transact, and more recently the rise of remote work has added yet more complexity to the mix. Today, a customer in Texas can call a business in Prague for product support and reach a technician ten thousand miles away in a coworking space in India—all while using a communication platform on the cloud. In other words, there are many more technology layers and much greater distances involved in even basic business interactions today. As such, authentication and verification have become much more challenging. This greatly expanded attack surface can spell bad news for companies that aren’t properly equipped to defend themselves against cybersecurity threats. Globally, the average data breach costs $4.35 million. In the U.S., the figure is more than double that—around $9.44 million. And such breaches are all-too-common occurrences, with more than 1,800 data compromises reported in the U.S. in 2022. But in the same way that business has evolved for the modern era, protective cybersecurity measures are also becoming more advanced. Today, digital solutions that integrate emerging technologies like AI into human-centric workflows are helping mitigate myriad threats. What’s more, intelligent digital solutions can protect sensitive business data while simultaneously simplifying and streamlining business operations. Download the full report. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. ","In association withTeleperformance Fifty years ago , the average business transaction was pretty straightforward . Shoppers handed purchases directly to cashiers , business partners shook hands in person , and people brought malfunctioning machines to a repair shop across the street . The proximity of all participating parties meant that both customers and businesses could verify authority and authenticity with their own eyes . But the internet has changed the very nature of how we transact , and more recently the rise of remote work has added yet more complexity to the mix . Today , a customer in Texas can call a business in Prague for product support and reach a technician ten thousand miles away in a coworking space in India—all while using a communication platform on the cloud . In other words , there are many more technology layers and much greater distances involved in even basic business interactions today . As such , authentication and verification have become much more challenging . This greatly expanded attack surface can spell bad news for companies that aren ’ t properly equipped to defend themselves against cybersecurity threats . Globally , the average data breach costs $ 4.35 million . In the U.S. , the figure is more than double that—around $ 9.44 million . And such breaches are all-too-common occurrences , with more than 1,800 data compromises reported in the U.S. in 2022 . But in the same way that business has evolved for the modern era , protective cybersecurity measures are also becoming more advanced . Today , digital solutions that integrate emerging technologies like AI into human-centric workflows are helping mitigate myriad threats . What ’ s more , intelligent digital solutions can protect sensitive business data while simultaneously simplifying and streamlining business operations . Download the full report . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['association', 'withteleperformance', 'year', 'ago', 'average', 'business', 'transaction', 'pretty', 'straightforward', 'shopper', 'hand', 'purchase', 'directly', 'cashier', 'business', 'partner', 'shake', 'hand', 'person', 'people', 'bring', 'malfunction', 'machine', 'repair', 'shop', 'street', 'proximity', 'participate', 'party', 'mean', 'customer', 'business', 'verify', 'authority', 'authenticity', 'eye', 'internet', 'change', 'nature', 'transact', 'recently', 'rise', 'remote', 'work', 'add', 'yet', 'complexity', 'mix', 'today', 'customer', 'call', 'business', 'prague', 'product', 'support', 'reach', 'technician', 'mile', 'away', 'coworke', 'space', 'use', 'communication', 'platform', 'cloud', 'word', 'many', 'technology', 'layer', 'much', 'great', 'distance', 'involve', 'even', 'basic', 'business', 'interaction', 'today', 'authentication', 'verification', 'become', 'much', 'challenging', 'greatly', 'expand', 'attack', 'surface', 'spell', 'bad', 'news', 'company', 'properly', 'equip', 'defend', 'cybersecurity', 'threat', 'globally', 'average', 'datum', 'breach', 'cost', 'figure', 'double', 'breach', 'alltoocommon', 'occurrence', 'datum', 'compromise', 'report', 'way', 'business', 'evolve', 'modern', 'era', 'protective', 'cybersecurity', 'measure', 'also', 'become', 'advanced', 'today', 'digital', 'solution', 'integrate', 'emerge', 'technology', 'ai', 'humancentric', 'workflow', 'help', 'mitigate', 'myriad', 'threat', 'intelligent', 'digital', 'solution', 'protect', 'sensitive', 'business', 'datum', 'simultaneously', 'simplify', 'streamline', 'business', 'operation', 'download', 'full', 'report', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","Fifty years ago, the average business transaction was pretty straightforward. Shoppers handed purchases directly to cashiers, business partners shook hands in person, and people brought malfunctioning machines to a repair shop across the street. The proximity of all participating parties meant that both customers and businesses could verify authority and authenticity with their own eyes.…"
Merging physical and digital tools to build resilient supply chains,https://www.technologyreview.com/2023/08/09/1077042/merging-physical-and-digital-tools-to-build-resilient-supply-chains/,2023-08-09,"In partnership withGS1 US Organizations are building resilient supply chains with a “phygital” approach, a blend of digital and physical tools. In recent years, the global supply chain has been disrupted due to the covid-19 pandemic, geopolitical volatility, overwhelmed legacy systems, and labor shortages. The National Association of Manufacturers (NAM), an industrial advocacy group, warns the disruption isn’t over—NAM’s spring 2023 survey found 90% of respondents saw significant (52.5%) or partial (39%) supply chain disruption during the past two years. Just 0.5% of respondents reported no disruption at all. Digitization presents an opportunity to overcome supply chain disruption by making data flow more efficiently, using technology and data standards to break barriers between disparate systems. “Phygital merges two worlds together, where standards provide an interoperable system of defined data structures,” says Melanie Nuce-Hilton, senior vice president of innovation and partnerships at GS1 US, a member of GS1, a global not-for-profit supply chain standards organization. “The approach is intended to deliver multiple benefits—improved supply chain visibility for traceability and inventory management, better customer experiences across online and offline interactions, and the potential for better circularity and waste reduction by maintaining linkages between products and their data throughout their lifecycle,” she says. Phygital systems blend digital tools and data standards with physical data carriers, such as barcodes. These tie products, assets, logistics units, and locations within a supply chain to digital information for enhanced accuracy and consistency. This capability, especially with more advanced data carriers, can help automate data flows and boost supply-chain visibility. Newer barcode iterations such as the increasingly common QR codes (quick-response codes) or Data Matrix barcodes (codes with a black-and-white grid pattern), store more information—up to 7,000 characters, compared to about 20 characters for conventional bar codes. The technology is growing alongside its use in the supply chain. Grand View Research data measured the global barcode reader market at $7.3 billion in 2022, and projects it will maintain a 7% CAGR from 2023 to 2030. By uniquely identifying products and tracking their supply chain journey with universal standards, Nuce-Hilton says, organizations can unlock extended value for the whole enterprise. This can lead to raising operational efficiencies, improving safety, attracting consumers, advancing energy efficiency, and decreasing waste. “Supply chain resilience isn’t just about the supply chain,” she says. “It’s about the whole enterprise coming together from a data, product, and execution point of view to create an immersive experience.” Several industries have explored phygital connections to enhance user experience or speed up processes. There are multiple ways to connect physical objects to technology and standardized data; all can help make data accessible, sharable, and useful. These phygital connections of product data, financial facts, and information to real-world activity can lead to a more resilient supply chain, Nuce-Hilton says. For example, retailer Pacsun launched a phygital venture—Pacsun Los Angeles Tycoon—in early 2023 with platform provider Roblox. This metaverse experience uses avatars so participants can connect and play games while viewing Pacsun’s 2023 summer clothing collection, bridging physical and virtual experiences. Nike also used phygital tools in 2022 in its Cryptokicks digital sneaker campaign with Roblox. Avid sneaker collectors can buy virtual sneakers as non-fungible tokens (NFT). Each unique digital pair is one of 20,000 customizable NFTs, some of which trade for hundreds—or thousands—of dollars. Healthcare companies have invested in phygital track-and-trace technology like barcodes and RFID tags for patient safety: Global healthcare company Fresenius Kabi relies on GS1 DataMatrix, a two-dimensional barcode carrying drug information, for its product portfolio. German consultancy Roland Berger said in its 2021 Future of Health 3 study that such digital health care technologies are reaching maturity, pointing to not just tracking but digital patient monitoring, early detection devices, and using data for AI for diagnostics and therapies. Phygital technology also helps the food industry keep food safer, while ensuring trading partners and consumers can get the products they want. Imagine, Nuce-Hilton says, a frozen pizza manufacturer whose products, with various expiration dates included in 2D barcodes, ship to hundreds of retailers from dozens of plants. With machine-readable expiration data, the manufacturer and retailer can know which products will expire and when, avoiding delays, inventory gaps, and empty shelves. A 2023 Zebra Technologies annual survey found nearly half of retail shoppers who left stores emptyhanded did so because their item was out of stock, an experience increasingly commonplace during the past two years, having increased by 26% since the 2019 survey. Phygital tools, with data standards and technology, deliver broad benefits to the enterprise, Nuce-Hilton says. These are some of the ways businesses can benefit. Supply-chain traceability: Produce grower Ocean Mist Farms encodes traceability data, such as which crew picked the produce, the farm location, and packaging methods, in barcodes on case labels to enhance inventory management and order optimization. The grower calculates that barcodes, digital tools, and data standards help them achieve up to 35% in time savings compared to their former system. Safety and quality assurance: Fast-food restauranteur Subway relies on barcodes for product data, which identify product expiration dates, best-before guidelines, and sell-by data. Traceability means faster and more accurate inventory management, and cuts down on human error. Data standards and technology empower them to quickly apply safety practices to protect consumers. Improved sustainability posture: Consumers and investors increasingly want to see environmental, social, and governance (ESG) data. Companies can build trust by increasing availability of ESG data, providing accountability. Phygital ESG data can include such things has product origin, ingredients, biodegradability, production processes, and energy use. More connected consumers: When customers scan object identifiers, they establish a phygital connection. This can provide customers with information such as how an item is made, ingredients, or geographical origin. Customers are interested: McKinsey 2022 data says customers who buy using omnichannel methods (a combination of physical, digital, and other experiences) shop 1.7 times more than consumers who don’t—and they also spend more. Organizations can benefit—using standards, technology, and data—by putting their data to work more broadly, says Nuce-Hilton. She suggests the following: Deploy the right technology tools: Advanced data carriers hold a large amount of information, so it’s critical to use appropriate analytics tools and IT resources to analyze and convert data into business insights. Throughout the supply chain, these tools can enhance inventory management, streamline logistics, and support traceability and sustainability. Look to AI for speed: As phygital systems make it easier to collect product data, innovative technologies such as generative AI promise to up the ante by accelerating tasks, such as creating code and analyzing supply chain data to detect anomalies and recommend corrections. Shape behavior around standards and collaboration: Increasingly complex supply chain ecosystems make collaboration and communication critical. “You can deploy any technology you want and call it what you want, but until the behaviors associated between trading partners change, you won’t be successful,” says Nuce-Hilton. Processes and underlying data structures are a common language for supply-chain partners.  Supply chain resilience is needed to meet fluctuating consumer demands, respond to unanticipated roadblocks, and satisfy ESG goals. Today’s business environment makes that a challenge, says Nuce-Hilton. “But we could change that, if we empower organizations with the data to make better decisions further upstream in the supply chain,” she says. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. ","In partnership withGS1 US Organizations are building resilient supply chains with a “ phygital ” approach , a blend of digital and physical tools . In recent years , the global supply chain has been disrupted due to the covid-19 pandemic , geopolitical volatility , overwhelmed legacy systems , and labor shortages . The National Association of Manufacturers ( NAM ) , an industrial advocacy group , warns the disruption isn ’ t over—NAM ’ s spring 2023 survey found 90 % of respondents saw significant ( 52.5 % ) or partial ( 39 % ) supply chain disruption during the past two years . Just 0.5 % of respondents reported no disruption at all . Digitization presents an opportunity to overcome supply chain disruption by making data flow more efficiently , using technology and data standards to break barriers between disparate systems . “ Phygital merges two worlds together , where standards provide an interoperable system of defined data structures , ” says Melanie Nuce-Hilton , senior vice president of innovation and partnerships at GS1 US , a member of GS1 , a global not-for-profit supply chain standards organization . “ The approach is intended to deliver multiple benefits—improved supply chain visibility for traceability and inventory management , better customer experiences across online and offline interactions , and the potential for better circularity and waste reduction by maintaining linkages between products and their data throughout their lifecycle , ” she says . Phygital systems blend digital tools and data standards with physical data carriers , such as barcodes . These tie products , assets , logistics units , and locations within a supply chain to digital information for enhanced accuracy and consistency . This capability , especially with more advanced data carriers , can help automate data flows and boost supply-chain visibility . Newer barcode iterations such as the increasingly common QR codes ( quick-response codes ) or Data Matrix barcodes ( codes with a black-and-white grid pattern ) , store more information—up to 7,000 characters , compared to about 20 characters for conventional bar codes . The technology is growing alongside its use in the supply chain . Grand View Research data measured the global barcode reader market at $ 7.3 billion in 2022 , and projects it will maintain a 7 % CAGR from 2023 to 2030 . By uniquely identifying products and tracking their supply chain journey with universal standards , Nuce-Hilton says , organizations can unlock extended value for the whole enterprise . This can lead to raising operational efficiencies , improving safety , attracting consumers , advancing energy efficiency , and decreasing waste . “ Supply chain resilience isn ’ t just about the supply chain , ” she says . “ It ’ s about the whole enterprise coming together from a data , product , and execution point of view to create an immersive experience. ” Several industries have explored phygital connections to enhance user experience or speed up processes . There are multiple ways to connect physical objects to technology and standardized data ; all can help make data accessible , sharable , and useful . These phygital connections of product data , financial facts , and information to real-world activity can lead to a more resilient supply chain , Nuce-Hilton says . For example , retailer Pacsun launched a phygital venture—Pacsun Los Angeles Tycoon—in early 2023 with platform provider Roblox . This metaverse experience uses avatars so participants can connect and play games while viewing Pacsun ’ s 2023 summer clothing collection , bridging physical and virtual experiences . Nike also used phygital tools in 2022 in its Cryptokicks digital sneaker campaign with Roblox . Avid sneaker collectors can buy virtual sneakers as non-fungible tokens ( NFT ) . Each unique digital pair is one of 20,000 customizable NFTs , some of which trade for hundreds—or thousands—of dollars . Healthcare companies have invested in phygital track-and-trace technology like barcodes and RFID tags for patient safety : Global healthcare company Fresenius Kabi relies on GS1 DataMatrix , a two-dimensional barcode carrying drug information , for its product portfolio . German consultancy Roland Berger said in its 2021 Future of Health 3 study that such digital health care technologies are reaching maturity , pointing to not just tracking but digital patient monitoring , early detection devices , and using data for AI for diagnostics and therapies . Phygital technology also helps the food industry keep food safer , while ensuring trading partners and consumers can get the products they want . Imagine , Nuce-Hilton says , a frozen pizza manufacturer whose products , with various expiration dates included in 2D barcodes , ship to hundreds of retailers from dozens of plants . With machine-readable expiration data , the manufacturer and retailer can know which products will expire and when , avoiding delays , inventory gaps , and empty shelves . A 2023 Zebra Technologies annual survey found nearly half of retail shoppers who left stores emptyhanded did so because their item was out of stock , an experience increasingly commonplace during the past two years , having increased by 26 % since the 2019 survey . Phygital tools , with data standards and technology , deliver broad benefits to the enterprise , Nuce-Hilton says . These are some of the ways businesses can benefit . Supply-chain traceability : Produce grower Ocean Mist Farms encodes traceability data , such as which crew picked the produce , the farm location , and packaging methods , in barcodes on case labels to enhance inventory management and order optimization . The grower calculates that barcodes , digital tools , and data standards help them achieve up to 35 % in time savings compared to their former system . Safety and quality assurance : Fast-food restauranteur Subway relies on barcodes for product data , which identify product expiration dates , best-before guidelines , and sell-by data . Traceability means faster and more accurate inventory management , and cuts down on human error . Data standards and technology empower them to quickly apply safety practices to protect consumers . Improved sustainability posture : Consumers and investors increasingly want to see environmental , social , and governance ( ESG ) data . Companies can build trust by increasing availability of ESG data , providing accountability . Phygital ESG data can include such things has product origin , ingredients , biodegradability , production processes , and energy use . More connected consumers : When customers scan object identifiers , they establish a phygital connection . This can provide customers with information such as how an item is made , ingredients , or geographical origin . Customers are interested : McKinsey 2022 data says customers who buy using omnichannel methods ( a combination of physical , digital , and other experiences ) shop 1.7 times more than consumers who don ’ t—and they also spend more . Organizations can benefit—using standards , technology , and data—by putting their data to work more broadly , says Nuce-Hilton . She suggests the following : Deploy the right technology tools : Advanced data carriers hold a large amount of information , so it ’ s critical to use appropriate analytics tools and IT resources to analyze and convert data into business insights . Throughout the supply chain , these tools can enhance inventory management , streamline logistics , and support traceability and sustainability . Look to AI for speed : As phygital systems make it easier to collect product data , innovative technologies such as generative AI promise to up the ante by accelerating tasks , such as creating code and analyzing supply chain data to detect anomalies and recommend corrections . Shape behavior around standards and collaboration : Increasingly complex supply chain ecosystems make collaboration and communication critical . “ You can deploy any technology you want and call it what you want , but until the behaviors associated between trading partners change , you won ’ t be successful , ” says Nuce-Hilton . Processes and underlying data structures are a common language for supply-chain partners . Supply chain resilience is needed to meet fluctuating consumer demands , respond to unanticipated roadblocks , and satisfy ESG goals . Today ’ s business environment makes that a challenge , says Nuce-Hilton . “ But we could change that , if we empower organizations with the data to make better decisions further upstream in the supply chain , ” she says . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['partnership', 'withgs1', 'organization', 'build', 'resilient', 'supply', 'chain', 'phygital', 'approach', 'blend', 'digital', 'physical', 'tool', 'recent', 'year', 'global', 'supply', 'chain', 'disrupt', 'covid19', 'pandemic', 'geopolitical', 'volatility', 'overwhelm', 'legacy', 'system', 'labor', 'shortage', 'manufacturer', 'industrial', 'advocacy', 'group', 'warn', 'disruption', 'spring', 'survey', 'find', 'respondent', 'see', 'significant', 'partial', 'supply', 'chain', 'disruption', 'past', 'year', 'respondent', 'report', 'disruption', 'digitization', 'present', 'opportunity', 'overcome', 'supply', 'chain', 'disruption', 'make', 'datum', 'flow', 'efficiently', 'use', 'technology', 'datum', 'standard', 'break', 'barrier', 'disparate', 'system', 'phygital', 'merge', 'world', 'together', 'standard', 'provide', 'interoperable', 'system', 'define', 'data', 'structure', 'say', 'senior', 'vice', 'president', 'innovation', 'partnership', 'gs1', 'member', 'gs1', 'global', 'notforprofit', 'supply', 'chain', 'standard', 'organization', 'approach', 'intend', 'deliver', 'multiple', 'benefit', 'improve', 'supply', 'chain', 'visibility', 'traceability', 'inventory', 'management', 'well', 'customer', 'experience', 'online', 'offline', 'interaction', 'potential', 'well', 'circularity', 'waste', 'reduction', 'maintain', 'linkage', 'product', 'datum', 'lifecycle', 'say', 'phygital', 'system', 'blend', 'digital', 'tool', 'datum', 'standard', 'physical', 'datum', 'carrier', 'barcode', 'tie', 'product', 'asset', 'logistic', 'unit', 'location', 'supply', 'chain', 'digital', 'information', 'enhance', 'accuracy', 'consistency', 'capability', 'especially', 'advanced', 'datum', 'carrier', 'help', 'automate', 'datum', 'flow', 'boost', 'supplychain', 'visibility', 'new', 'barcode', 'iteration', 'increasingly', 'common', 'qr', 'code', 'quickresponse', 'code', 'datum', 'matrix', 'barcode', 'code', 'blackandwhite', 'grid', 'pattern', 'store', 'information', 'character', 'compare', 'character', 'conventional', 'bar', 'code', 'technology', 'grow', 'use', 'supply', 'chain', 'grand', 'view', 'research', 'datum', 'measure', 'global', 'barcode', 'reader', 'market', 'project', 'maintain', 'cagr', 'uniquely', 'identify', 'product', 'track', 'supply', 'chain', 'journey', 'universal', 'standard', 'say', 'organization', 'unlock', 'extend', 'value', 'whole', 'enterprise', 'lead', 'raise', 'operational', 'efficiency', 'improve', 'safety', 'attract', 'consumer', 'advance', 'energy', 'efficiency', 'decrease', 'waste', 'supply', 'chain', 'resilience', 'supply', 'chain', 'say', 'whole', 'enterprise', 'come', 'together', 'data', 'product', 'execution', 'point', 'view', 'create', 'immersive', 'experience', 'several', 'industry', 'explore', 'phygital', 'connection', 'enhance', 'user', 'experience', 'speed', 'process', 'multiple', 'way', 'connect', 'physical', 'object', 'technology', 'standardized', 'datum', 'help', 'make', 'datum', 'accessible', 'sharable', 'useful', 'phygital', 'connection', 'product', 'datum', 'financial', 'fact', 'information', 'realworld', 'activity', 'lead', 'resilient', 'supply', 'chain', 'say', 'example', 'retailer', 'launch', 'phygital', 'venture', 'early', 'platform', 'provider', 'roblox', 'metaverse', 'experience', 'use', 'avatar', 'participant', 'connect', 'play', 'game', 'view', 'summer', 'clothing', 'collection', 'bridge', 'physical', 'virtual', 'experience', 'also', 'use', 'phygital', 'tool', 'cryptokick', 'digital', 'sneaker', 'campaign', 'roblox', 'sneaker', 'collector', 'buy', 'virtual', 'sneaker', 'nonfungible', 'token', 'unique', 'digital', 'pair', 'customizable', 'nft', 'trade', 'hundred', 'thousand', 'dollar', 'healthcare', 'company', 'invest', 'phygital', 'trackandtrace', 'technology', 'barcode', 'rfid', 'tag', 'patient', 'safety', 'global', 'rely', 'gs1', 'datamatrix', 'twodimensional', 'barcode', 'carry', 'drug', 'information', 'product', 'portfolio', 'german', 'say', 'future', 'health', 'study', 'digital', 'health', 'care', 'technology', 'reach', 'maturity', 'point', 'track', 'digital', 'patient', 'monitoring', 'early', 'detection', 'device', 'use', 'datum', 'ai', 'diagnostic', 'therapie', 'phygital', 'technology', 'also', 'help', 'food', 'industry', 'keep', 'food', 'safe', 'ensure', 'trading', 'partner', 'consumer', 'get', 'product', 'want', 'imagine', 'say', 'frozen', 'pizza', 'manufacturer', 'product', 'various', 'expiration', 'date', 'include', 'barcode', 'ship', 'hundred', 'retailer', 'dozen', 'plant', 'machinereadable', 'expiration', 'datum', 'manufacturer', 'retailer', 'know', 'product', 'expire', 'avoid', 'delay', 'inventory', 'gap', 'empty', 'shelf', 'zebra', 'technology', 'annual', 'survey', 'find', 'nearly', 'half', 'retail', 'shopper', 'leave', 'store', 'emptyhande', 'item', 'stock', 'experience', 'increasingly', 'commonplace', 'past', 'year', 'increase', 'survey', 'phygital', 'tool', 'datum', 'standard', 'technology', 'deliver', 'broad', 'benefit', 'enterprise', 'say', 'way', 'business', 'benefit', 'supplychain', 'traceability', 'produce', 'grower', 'mist', 'farm', 'encode', 'traceability', 'datum', 'crew', 'pick', 'produce', 'farm', 'location', 'packaging', 'method', 'barcode', 'case', 'label', 'enhance', 'inventory', 'management', 'order', 'optimization', 'grower', 'calculate', 'barcode', 'digital', 'tool', 'datum', 'standard', 'help', 'achieve', 'time', 'saving', 'compare', 'former', 'system', 'safety', 'quality', 'assurance', 'fastfood', 'restauranteur', 'subway', 'rely', 'barcode', 'product', 'datum', 'identify', 'product', 'expiration', 'date', 'bestbefore', 'guideline', 'datum', 'traceability', 'mean', 'fast', 'accurate', 'inventory', 'management', 'cut', 'human', 'error', 'data', 'standard', 'technology', 'empower', 'quickly', 'apply', 'safety', 'practice', 'protect', 'consumer', 'improve', 'sustainability', 'posture', 'consumer', 'investor', 'increasingly', 'want', 'see', 'environmental', 'social', 'governance', 'esg', 'datum', 'company', 'build', 'trust', 'increase', 'availability', 'esg', 'data', 'provide', 'accountability', 'phygital', 'esg', 'datum', 'include', 'thing', 'product', 'origin', 'ingredient', 'biodegradability', 'production', 'process', 'energy', 'use', 'connected', 'consumer', 'customer', 'scan', 'object', 'identifier', 'establish', 'phygital', 'connection', 'provide', 'customer', 'information', 'item', 'make', 'ingredient', 'geographical', 'origin', 'customer', 'interested', 'mckinsey', 'datum', 'say', 'customer', 'buy', 'use', 'omnichannel', 'method', 'combination', 'physical', 'digital', 'experience', 'shop', 'time', 'consumer', 'also', 'spend', 'organization', 'benefit', 'use', 'standard', 'technology', 'datum', 'put', 'datum', 'work', 'broadly', 'say', 'suggest', 'following', 'deploy', 'right', 'technology', 'tool', 'advanced', 'datum', 'carrier', 'hold', 'large', 'amount', 'information', 'critical', 'use', 'appropriate', 'analytic', 'tool', 'resource', 'analyze', 'convert', 'datum', 'business', 'insight', 'supply', 'chain', 'tool', 'enhance', 'inventory', 'management', 'streamline', 'logistic', 'support', 'traceability', 'sustainability', 'look', 'ai', 'speed', 'phygital', 'system', 'make', 'easy', 'collect', 'product', 'datum', 'innovative', 'technology', 'generative', 'promise', 'ante', 'accelerate', 'task', 'create', 'code', 'analyze', 'supply', 'chain', 'datum', 'detect', 'anomaly', 'recommend', 'correction', 'shape', 'behavior', 'standard', 'collaboration', 'increasingly', 'complex', 'supply', 'chain', 'ecosystem', 'make', 'collaboration', 'communication', 'critical', 'deploy', 'technology', 'want', 'call', 'want', 'behavior', 'associate', 'trading', 'partner', 'change', 'win', 'successful', 'say', 'process', 'underlying', 'data', 'structure', 'common', 'language', 'supplychain', 'partner', 'supply', 'chain', 'resilience', 'need', 'meet', 'fluctuate', 'consumer', 'demand', 'respond', 'unanticipated', 'roadblock', 'satisfy', 'esg', 'goal', 'today', 'business', 'environment', 'make', 'challenge', 'say', 'change', 'empower', 'organization', 'datum', 'make', 'well', 'decision', 'far', 'upstream', 'supply', 'chain', 'say', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","Organizations are building resilient supply chains with a “phygital” approach, a blend of digital and physical tools. In recent years, the global supply chain has been disrupted due to the covid-19 pandemic, geopolitical volatility, overwhelmed legacy systems, and labor shortages. The National Association of Manufacturers (NAM), an industrial advocacy group, warns the disruption isn’t over—NAM’s…"
Transformation requires companywide engagement,https://www.technologyreview.com/2023/08/02/1077096/transformation-requires-companywide-engagement/,2023-08-02,"In association withOracle + KPMG Enterprises are shifting operations from on-premises to the cloud, and industry momentum for digital transformation continues to push forward. Gartner estimates that 80% of CEOs are increasing investment in digital technologies in 2023 to achieve greater efficiency and productivity. Cloud and digitization are becoming a necessity across industries to ensure competitiveness. But as companies make these shifts, employees feel the change in atmosphere. And according to the American Psychological Association, people often reflexively resist organizational change, especially when they don’t understand the reasons for the change. Transformation is not just about onboarding technology: it means changes in people’s responsibilities, daily routines, and workstyles. The key to taking full advantage of these powerful digital tools is understanding how they affect employees at all levels. It can be hard for leaders to avoid rushing ahead, because digital transformation is a chance to look at the whole operation, says Jennifer Chilton, principal of advisory and enterprise solutions at KPMG. It’s exciting to ponder an all-encompassing view of efficiency that can automate manual processes, she says, not only for a smoother workflow, but for faster information flow around the business: “Improve the controls, improve the speed.” A successful transition requires an equally expansive view of the one component on which it all hinges: people. Michelle Kent, principal at KPMG’s people and change practice, has a blunt message for executives who give their staff little notice before major changes, scant information about the future state of the company, and negligible involvement in the planning: “That’s not how people work.” Download the full report. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. ","In association withOracle + KPMG Enterprises are shifting operations from on-premises to the cloud , and industry momentum for digital transformation continues to push forward . Gartner estimates that 80 % of CEOs are increasing investment in digital technologies in 2023 to achieve greater efficiency and productivity . Cloud and digitization are becoming a necessity across industries to ensure competitiveness . But as companies make these shifts , employees feel the change in atmosphere . And according to the American Psychological Association , people often reflexively resist organizational change , especially when they don ’ t understand the reasons for the change . Transformation is not just about onboarding technology : it means changes in people ’ s responsibilities , daily routines , and workstyles . The key to taking full advantage of these powerful digital tools is understanding how they affect employees at all levels . It can be hard for leaders to avoid rushing ahead , because digital transformation is a chance to look at the whole operation , says Jennifer Chilton , principal of advisory and enterprise solutions at KPMG . It ’ s exciting to ponder an all-encompassing view of efficiency that can automate manual processes , she says , not only for a smoother workflow , but for faster information flow around the business : “ Improve the controls , improve the speed. ” A successful transition requires an equally expansive view of the one component on which it all hinges : people . Michelle Kent , principal at KPMG ’ s people and change practice , has a blunt message for executives who give their staff little notice before major changes , scant information about the future state of the company , and negligible involvement in the planning : “ That ’ s not how people work. ” Download the full report . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['association', 'withoracle', 'enterprise', 'shift', 'operation', 'onpremise', 'cloud', 'industry', 'momentum', 'digital', 'transformation', 'continue', 'push', 'forward', 'estimate', 'ceo', 'increase', 'investment', 'digital', 'technology', 'achieve', 'great', 'efficiency', 'productivity', 'cloud', 'digitization', 'become', 'necessity', 'industry', 'ensure', 'competitiveness', 'company', 'make', 'shift', 'employee', 'feel', 'change', 'atmosphere', 'accord', 'people', 'often', 'reflexively', 'resist', 'organizational', 'change', 'especially', 'understand', 'reason', 'change', 'transformation', 'onboarde', 'technology', 'mean', 'change', 'people', 'responsibility', 'daily', 'routine', 'workstyle', 'key', 'take', 'full', 'advantage', 'powerful', 'digital', 'tool', 'understand', 'affect', 'employee', 'level', 'hard', 'leader', 'avoid', 'rush', 'ahead', 'digital', 'transformation', 'chance', 'look', 'whole', 'operation', 'say', 'principal', 'advisory', 'enterprise', 'solution', 'exciting', 'ponder', 'allencompassing', 'view', 'efficiency', 'automate', 'manual', 'process', 'say', 'smoother', 'workflow', 'fast', 'information', 'flow', 'business', 'improve', 'control', 'improve', 'speed', 'successful', 'transition', 'require', 'equally', 'expansive', 'view', 'component', 'hinge', 'people', 'principal', 'people', 'change', 'practice', 'blunt', 'message', 'executive', 'give', 'staff', 'little', 'notice', 'major', 'change', 'scant', 'information', 'future', 'state', 'company', 'negligible', 'involvement', 'planning', 'people', 'work', 'download', 'full', 'report', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","Enterprises are shifting operations from on-premises to the cloud, and industry momentum for digital transformation continues to push forward. Gartner estimates that 80% of CEOs are increasing investment in digital technologies in 2023 to achieve greater efficiency and productivity. Cloud and digitization are becoming a necessity across industries to ensure competitiveness. But as companies make…"
Accelerating retail personalization at scale,https://www.technologyreview.com/2023/07/18/1076380/accelerating-retail-personalization-at-scale/,2023-07-18,"In association withOracle + Deloitte Today’s retailers are faced with a clear opportunity for transformation. Consumer expectations are constantly evolving, challenging retailers to keep pace. A blend of online and in-person shopping forged during the pandemic persists, forcing retailers to deliver a highly personalized omnichannel experience. And retailers’ values are becoming as important to consumers as their products and services. “As consumers, we are more sophisticated shoppers. We have so much buying power with the mobile technology at our fingertips and high expectations,” says Mike Webster, senior vice president and general manager at Oracle Retail. “And despite the grand promises of retail technology, the shopping experience may leave us underwhelmed due to a poor execution.”  This is a clear call for many retailers to create customer-centric shopping experiences. Forget about a laser-like focus on product development and delivery. Rather, savvy retailers are creating holistic, personalized shopping experiences that engage and fulfill customer needs throughout the customer journey. Consumers want this personal touch: 66% say they want brands to reach out to them, with personalized messages such as discounts and offers on items they’ve purchased before (44%) or predictions about products they may like (32%), according to a 2022 consumer research report by Oracle Retail. “In a world where the consumer is getting more and more diverse, more and more segmented, and more and more individualistic, it’s critical that retailers reimagine how to put the customer at the heart of their processes,” says Daniel Edsall, principal and global grocery leader at Deloitte Consulting LLP. But while shifting focus from traditional merchandising to a fully customer-centered view is imperative, retailers must overcome some significant obstacles to succeed. Many are burdened by legacy technology that is expensive to maintain and difficult to reconfigure. Labor shortages continue to hamper retailers’ efforts to embark on new endeavors. And pandemic-induced shockwaves can still be felt in the form of supply chain disruptions and delivery delays. The good news is that there are ways to embrace a more customer-centric business model while addressing modern-day labor and technology challenges. One key: cloud-based technology platforms that enable technology innovation and empower retailers to shift from siloed product categories and departments to a holistic view of the customer, inventory, and operations. Download the full report. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. ","In association withOracle + Deloitte Today ’ s retailers are faced with a clear opportunity for transformation . Consumer expectations are constantly evolving , challenging retailers to keep pace . A blend of online and in-person shopping forged during the pandemic persists , forcing retailers to deliver a highly personalized omnichannel experience . And retailers ’ values are becoming as important to consumers as their products and services . “ As consumers , we are more sophisticated shoppers . We have so much buying power with the mobile technology at our fingertips and high expectations , ” says Mike Webster , senior vice president and general manager at Oracle Retail . “ And despite the grand promises of retail technology , the shopping experience may leave us underwhelmed due to a poor execution. ” This is a clear call for many retailers to create customer-centric shopping experiences . Forget about a laser-like focus on product development and delivery . Rather , savvy retailers are creating holistic , personalized shopping experiences that engage and fulfill customer needs throughout the customer journey . Consumers want this personal touch : 66 % say they want brands to reach out to them , with personalized messages such as discounts and offers on items they ’ ve purchased before ( 44 % ) or predictions about products they may like ( 32 % ) , according to a 2022 consumer research report by Oracle Retail . “ In a world where the consumer is getting more and more diverse , more and more segmented , and more and more individualistic , it ’ s critical that retailers reimagine how to put the customer at the heart of their processes , ” says Daniel Edsall , principal and global grocery leader at Deloitte Consulting LLP . But while shifting focus from traditional merchandising to a fully customer-centered view is imperative , retailers must overcome some significant obstacles to succeed . Many are burdened by legacy technology that is expensive to maintain and difficult to reconfigure . Labor shortages continue to hamper retailers ’ efforts to embark on new endeavors . And pandemic-induced shockwaves can still be felt in the form of supply chain disruptions and delivery delays . The good news is that there are ways to embrace a more customer-centric business model while addressing modern-day labor and technology challenges . One key : cloud-based technology platforms that enable technology innovation and empower retailers to shift from siloed product categories and departments to a holistic view of the customer , inventory , and operations . Download the full report . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['association', 'withoracle', 'deloitte', 'today', 'retailer', 'face', 'clear', 'opportunity', 'transformation', 'consumer', 'expectation', 'constantly', 'evolve', 'challenge', 'retailer', 'keep', 'pace', 'blend', 'shopping', 'forge', 'pandemic', 'persist', 'force', 'retailer', 'deliver', 'highly', 'personalize', 'omnichannel', 'experience', 'retailer', 'value', 'become', 'important', 'consumer', 'product', 'service', 'consumer', 'sophisticated', 'shopper', 'much', 'buying', 'power', 'mobile', 'technology', 'fingertip', 'high', 'expectation', 'say', 'senior', 'vice', 'president', 'general', 'manager', 'retail', 'grand', 'promise', 'retail', 'technology', 'shopping', 'experience', 'leave', 'underwhelme', 'poor', 'execution', 'clear', 'call', 'many', 'retailer', 'create', 'shopping', 'experience', 'forget', 'laserlike', 'focus', 'product', 'development', 'delivery', 'rather', 'savvy', 'retailer', 'create', 'holistic', 'personalize', 'shopping', 'experience', 'engage', 'fulfill', 'customer', 'need', 'customer', 'journey', 'consumer', 'want', 'personal', 'touch', 'say', 'want', 'brand', 'reach', 'personalize', 'message', 'discount', 'offer', 'item', 'purchase', 'prediction', 'product', 'like', 'accord', 'consumer', 'research', 'report', 'oracle', 'retail', 'world', 'consumer', 'get', 'diverse', 'segmented', 'individualistic', 'critical', 'retailer', 'reimagine', 'put', 'customer', 'heart', 'process', 'say', 'edsall', 'principal', 'global', 'grocery', 'leader', 'consulting', 'shift', 'focus', 'traditional', 'merchandising', 'fully', 'customercentered', 'view', 'imperative', 'retailer', 'overcome', 'significant', 'obstacle', 'succeed', 'many', 'burden', 'legacy', 'technology', 'expensive', 'maintain', 'difficult', 'reconfigure', 'labor', 'shortage', 'continue', 'hamper', 'retailer', 'effort', 'embark', 'new', 'endeavor', 'pandemicinduce', 'shockwave', 'still', 'feel', 'form', 'supply', 'chain', 'disruption', 'delivery', 'delay', 'news', 'way', 'embrace', 'customercentric', 'business', 'model', 'address', 'modernday', 'labor', 'technology', 'challenge', 'key', 'cloudbase', 'technology', 'platform', 'enable', 'technology', 'innovation', 'empower', 'retailer', 'shift', 'siloe', 'product', 'category', 'department', 'holistic', 'view', 'customer', 'inventory', 'operation', 'download', 'full', 'report', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","Today’s retailers are faced with a clear opportunity for transformation. Consumer expectations are constantly evolving, challenging retailers to keep pace. A blend of online and in-person shopping forged during the pandemic persists, forcing retailers to deliver a highly personalized omnichannel experience. And retailers’ values are becoming as important to consumers as their products and services.…"
Transforming business begins with IT,https://www.technologyreview.com/2023/07/14/1073208/transforming-business-begins-with-it/,2023-07-14,"In partnership withInfosys Cobalt From securing a hybrid workforce to building pipelines for ever-increasing data streams and keeping multiple mission-critical systems up and running, the modern IT department faces numerous pressures. As senior director of IT for the packaged food company Conagra, Amit Khot is optimistic about the ways modern technology solutions and infrastructure can enable businesses to thrive and innovate. Khot describes the power of advanced data analytics to both improve a company’s understanding of its customers and to optimize its operations. The ability to combine internal company data with data collected from social media and at point of sale will enable savvy companies to recognize new patterns. These advanced analytics, he says, will go beyond answering standard questions about financials and historical performance to provide insight into more complex questions about customers’ thoughts and changing preferences. Meanwhile, these same data tools can also be used to fine-tune daily business operations, pinpointing issues with order fulfillment, improving long-range supply-and-demand forecasting, and digitizing manufacturing plant processes. Khot explains, “Planning is looking into the future, depending upon your past historical data, as to what your future demand and supply should look like. We have gone through a journey to modernize our planning platforms.” A modern enterprise resource planning (ERP) system is also a must for a distributed organization like Conagra. A single connected ERP system can manage and provide visibility into business processes that involve multiple divisions or departments. By doing so, a modern ERP can also ease highly complex processes, such as the technology integration of a newly acquired company. Says Khot, “having a single view of finance, having a single view of the supply chain as early and as fast as possible, is one of the most important things that can help us get synergies out of the business as fast as possible.” This episode of Business Lab is produced in partnership with Infosys Cobalt. FULL TRANSCRIPT Laurel Ruma: From MIT Technology Review, I’m Laurel Ruma and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace. Our topic today is technological evolution. Companies, whether they’re regional or global, startup or legacy, need to be able to quickly deploy technologies as markets and supply chains shift and change. While many worries may keep executives up at night, building modern systems, and adopting the right technologies to better understand data will help those executives and companies gain efficiencies and provide an excellent customer experience. Two words for you: meeting demand. My guest is Amit Khot, Senior Director of IT for Conagra. Welcome, Amit.This episode of Business Lab is produced in partnership with Infosys Cobalt. Amit Khot: Hello, nice to meet you. Laurel: Great to have you here. So just a little background in case folks aren’t familiar with Conagra as a consumer packaged goods company that has been around for more than 100 years. Conagra produces products like Birds Eye, Healthy Choice, and Slim Jim; various foods that you can find in supermarkets and in restaurants around the world. You have been with Conagra now for 23 years. How has your role evolved as a company and technology has transformed? Amit: Absolutely. I started with Conagra, as you said, 23 years ago and I started as a program analyst with the company. Again, 23 years ago is a long time. Program analyst starting from that point and then I evolved into implementing our SAP ecosystem. That is what I started doing in the early 2000s. As time evolved, in the early 2010s, we started with a lot of mergers and acquisitions of businesses similar to ours. And as we started doing that, I played a role in doing due diligence for those businesses from an IT perspective. In addition to that, I also then helped integrate those companies within Conagra or Conagra Enterprise. In 2015 or so, we then did some divestitures and spins during that time, and I played a role in doing our program rating, an entire spinoff that we did for one of our major potato businesses, and I played the role of program director for that. During the same time, what we did is we went through an SG&A [selling, general, and administrative expense] reduction program, and I worked with some of our consulting businesses to come up with an analysis to say how much we spend on our production support. And that is the time when we actually contracted with Infosys to help us do the production support as part of aligning with the rest of the industry, where most of the industries were getting production support done by an outsourcing partner. I helped with that and right after that happened I had an opportunity to lead our SAP and integration platform. I did that and finally I ended up being in a role that I’m in right now, where I do enterprise architecture for applications on various value streams. That includes supply chain, manufacturing, finance, our global business systems, as well as platforms and integration. That’s my role currently, and that has been my journey for the last 23 years. A long time. Laurel: Well, certainly a long history of the company and how it evolved as well. But more recently, what kind of digital transformation has Conagra gone through in these recent years, and how do you approach these shifts and changes from an IT perspective? Amit: I think that’s a great question. Digital transformation has many meanings. I mean [sometimes], you do something which is really transformative. In other cases, you keep up with modernizing your technologies. One of the major initiatives that I helped design and lead initially was our S4 implementation. I helped with coming up with the design for our S4, which is what we call our ERP [enterprise resource planning] modernization program. And now I help that program with providing subject matter expertise across the various aspects of how a modern ERP should look. That is one of the programs that we are going through right now. One of the other transformation journeys that, as a company, we have gone through is planning transformation. Planning is looking into the future, depending upon your past historical data, as to what your future demand and supply should look like. We have gone through a journey to modernize our planning platforms. It’s one of the other things that we have done. In addition to that, we are currently marching on a journey now to modernize and digitize our manufacturing. A large initiative. You might know that we have multiple plants and manufacturing locations and co-packers. Digitizing can ensure that we get the most efficiencies out of our platforms. So that is underway. And last but not the least, I will say that we have started getting pretty good maturity and understanding on various cloud services or cloud platforms in general. And as such, we have started maturing in cloud platforms like Azure services or SAP’s BTP [Business Technology Platform] and such. Those are some of the key initiatives that we have gone through to digitally transform our business and there are a lot of things that we plan on doing in the future. Laurel: I think it was particularly important that you mentioned that your role encompasses so many different parts of the company, because supply chain is certainly one of those important ones, and you have to think of systems from end to end. So how did the covid-19 pandemic affect Conagra as people worked from home and started to shop online and do that more and more? Did this shift intensify adoption of specific technologies within the company? Amit: What we didn’t do is we didn’t create a different strategy just to attack the covid pandemic. We had a lot of strategies built. I think one of the most important things that we had to do during the covid pandemic was keeping the system stable. Keeping the system stable is not a trivial task. I mean, if you look at our application and platform portfolios, it’s pretty large. Keeping everything up and running so that we can actually fulfill the customer’s demand is a big deal, and to keep them going, I think that was one of the most important things that we did during the pandemic. The next thing I would say is that we were premature in using our collaborative platform and collaboration technologies, like Office 365 and Webex, even before covid hit us. I would say that with the pandemic and everybody going remote, one of the most important things that we did is that we added more resiliency to some of those platforms. And our usage of that platform spiked to such an extent that that was almost a call that we failed. I mean, how much collaboration people did during that time using the technology. There were a lot of times before that, where people used to be in the rooms and in conference rooms doing whiteboarding and such, and delivering projects, being in one place, but with covid, I think people were leveraging a lot of these collaborative technologies to be able to get there. I would say the third thing that we did is, covid opened our eyes to how we changed our way of working. Before, we used to be delivering a lot of our solutions using waterfall methodologies. They used to be very long and they used to take up a lot of time, and we would not be able to figure out until the end whether we are going to be succeeding with some of the projects or not. We then adopted continuous delivery as a way to deliver work. And that spiked up the use of tools like JIRA quite a bit. But that was started during the covid time and we continue to use that more and more. And lastly, I would say that we had to do analysis on our data to figure out how we can, as I said before, how do we keep our system stable. But then also analyzing how do we fulfill the demand, and, as such, what are some of our pain points? And we used some of the cloud platforms and cloud services to do some quick analysis to be able to fulfill our shipments. Those are the few things that we did and learned and adopted during the covid-19 pandemic. Laurel: And that’s certainly important to be able to actually see that data in real-time to help your customers. How do you think adopting cloud and using more data technologies will help your customer experience improve? Amit: I think one of the most important things in our business is to have a 360-degree view of customers. I mean, it’s pretty vital, right? As you might know, our business is a very customer-focused business. For us, large retailers are typically our customers. Our consumer is one step removed from us. What the technological advancement helps us now do, is, today, most of the information that we create as we do the business, resides within our four walls. As the social media platforms and such have become prominent, what is really important to us is being able to have the data that is inside of our four walls, mash that up with the data that is coming from the social media platforms, plus the point of sale data, all those things. When you mash all these things together, I think it provides us pretty decent consumer insights. These consumer insights, ultimately, lead us to a lot of product innovation. You might have seen our CEO talk about that. We have created a pretty decent new innovation pipeline during the last few years. And I think the digital technology and the technology that exists out there has definitely provided us with, I would say, a lot of capabilities to be able to innovate faster. The next thing I would say is the innovation side of the business is one side of the world, but the other side is then being able to fulfill the shipments on time and in full. If you look at it, there are a lot of customers of ours who want most of our shipments to be on time and in full. And if that doesn’t happen, we end up paying fines. Some of these digital technologies help us pinpoint where our issues are and what we should be doing differently to be able to fulfill our shipments on time and in full. That is another thing that we have gotten better at, and it’s just based upon the improvement of technology. Better planning. Better planning is equal to being able to predict the demands of our consumers and customers and how that then leads to us to be able to plan out some of the long-term horizons of supply. How do we do that from the long-term to the medium-term to the short-term. Those are the things that we have been able to do as a part of delivering some of our planning projects. That is based upon some of the modern technology that exists out there. And lastly, I would say the shop floor agility in general. With us investing in digital manufacturing, I would say that technology has definitely enabled us to be able to deliver digitization within manufacturing that has increased the shop floor agility. And I would say that that is going to be a long journey for us, but we are marching towards the results where we will be a lot more agile on the shop floor than we have ever been before. Laurel: And that’s so important when there are just more challenges thrown your way and also all these opportunities with such fantastic technology as well. And you mentioned this earlier, but why does Conagra need an enterprise resource planning system, and how does it partner with companies like Infosys to stay on that cutting edge of technology to make sure you can answer all of these challenges? Amit: Yeah, that’s a great question. I mean, as I mentioned before, we have numerous plants, numerous customers, and numerous business partners that we work with. Once you have a lot of these, the impact and the business processes that cross doing these businesses, that crosses HR, that crosses supply chain, that crosses manufacturing. There are customer-facing business processes that exist. And if you look at all these processes that exist within any business like we have, which is basically a consumer foods business, what ends up happening is that if you do not have a combined view of your business at one place, it becomes an extremely hard proposition. Just doing simple business can become really hard. So it is really important for us to have a connected system, a connected view of business processes, and to enable something like that. ERPs play a very important role. As I said during your first question, when you asked me, “what was your journey?” We started our journey of implementing SAP as an ERP in the early 2000s. One of the prime reasons for doing that is exactly to solve the problem that I just talked about: how do we get a consolidated view of our entire business cycle? And that is what ERP helps us deliver. Now, ERP doesn’t just give you the stack of your business, but it also then gives you an ability to do analysis of the data that is in your system, and then create transformations that you wouldn’t do before, if all the systems in this business process were independent and isolated. So that is one of the big reasons why ERPs play such an important role in business like ours, and I would say that that is the case with most of the industry that we are in. Now when it comes to help from partners like Infosys to create the innovation, I would say it is a two-part answer. One of the first things is that ERPs have become so important in just running our business that having a stable system is one of the most important things that typically many of the IT functions deliver. To keep ourselves stable, partners like Infosys that help us manage our production and production support, they play an important aspect and role in making sure that the systems are stable and current from a technology perspective. That’s one aspect of it. Another thing that Infosys, and partners like Infosys, are helping us just do the production support. That frees up capacity of our subject matter experts to be able to then look at different solutions to solve the new business problems that pop up for us. That frees up the capacity for them to be able to do different things. That’s number two. Number three is that the companies like Infosys, and other business partners that we have, have a lot of customers just like us and even customers that are not in the same industry as us. What they hear, the business problems they hear from these other businesses and other customers that they have, that gives them an advantage in insights that we as Conagra by ourselves won’t be able to get. Because everybody has a different problem that they’re trying to solve. And if Infosys has that insight, they can provide us a great external point of view to be able to then solve some of the business problems that we have, which could be similar to what somebody else might have seen. And that just helps us solve these business problems faster. And this is an external point of view from a customer-centric perspective, but at the same time, with the scale and the number of partners that Infosys deals with, especially from the supplier side of the world, there are technologies that Infosys has reached which we do not have reach of. Partners like Infosys can even bring some of these advanced technologies that exist out there and provide us and guide us in. I believe that there lies a huge opportunity for companies like that to help us bring these new technologies and platforms to be able to help us solve some of the business problems that we have today—and probably solve and provide us insights into some of the business problems that might be coming to us that we have not thought about. Laurel: That’s a great point about the partnership with Infosys, and in general, how you actually bring the data and predictive analytics to your capabilities because you do have so much data coming in from fifty different brands, countless vendors, all those customers. How can this be maximized to gain those insights? Amit: Yeah, that’s a great question. And just as you said, many brands and countless business partners and customers. We generate terabytes of data every year, and that data typically lies in our four walls. I mean, just in our ERPs and our business warehouse systems. And based upon that data, I think most of the industries like us have gotten really good at doing traditional analytics. Traditional analytics is equal to, how are our financials looking? What is the performance of a certain brand depending upon the historical data? And so on and so forth. I mean, that is the traditional analytics that we have gotten really good at. What becomes important now that you have gotten good traditional analytics is, what do you not know yet? What are those gems within your existing data that you have not taken advantage of? Some of these newer technologies and platforms, what they have started helping us do, and probably they’ll keep on helping us do, is being able to glean into our data and start pointing to what is it that we are not looking at. I mean, what we know is always great, but those unknowns that we have not actually gleaned into is what some of these technologies that are coming forward are going to be able to help us look at. That’s one aspect of the world. Now, the second aspect of the world is, as I said, the data exists just within our four walls. But as I said before, that social media data, that point of sale data, the data that doesn’t exist within our four walls, I think that has a different kind of insight and power. Now, think about the fact that you are able to mash up the data which is from these external sources and the data that you have inside, and then think about some of the data that you generate just because you have consumers that are calling into your consumer affairs division. You take all this data mashed up together, and I think you can create analytics that we were never able to produce before. And I think that is a power of what we get from just mashing all this data, and matching all this data together, and we can maximize a lot of insights. And then once you have that mashup happen, I think the predictions are different. In the sense that many times our existing forecasting solutions typically are very much dependent upon historical data to be able to do predictions on our supply and demand. They’re doing predictions like that. However, with the external data being mastered, I think it goes beyond that. I think it also starts giving us an insight into what the consumers are thinking, what the customers are thinking, how their tastes and choices are changing. I think that is the next forefront for us from a predictability perspective. And I think that the new technologies and platforms are going to help us do that yet better. Laurel: So this is a good point. We have this data and you need to make some really great decisions from it, but you also need to really assess those analytics, make predictions in the future, but also make sure your entire systems are running correctly end to end. How, then, can cloud applications coupled with this need and progress of your digital transformation journey help with a tactic like mergers and acquisitions that you mentioned earlier was part of your career? How has that specifically been one of those things that helps the company actually create efficiencies and really see technology as a partner? Amit: Yeah, absolutely. That’s a great question. One of the key reasons for acquisitions is that we can actually take advantage of the synergies that we can get. This is almost one plus one equal to three. That’s number one. Number two is, then on top of the synergies, the innovation pipeline, let’s say, the acquired company has and the experience that we have. When you combine those two together, I think we can create innovation at scale. That is two of the key reasons why we can go on and acquire a company. And when we do that, I think one of the most important aspects of that is then to take that acquired company and then basically integrate that company within our business processes. I would say that is a key activity that you have to partake in when you acquire a company. As we have gone through some of these digitalization journeys, as I said, we are pretty experienced with integrating some of these acquired companies into our enterprise, our systems, as well as in business processes. But that journey typically is not trivial. I mean, it takes a long time to integrate and acquire a company into our business processes. As we go through that journey, many times, being able to gain the insights of the business as quickly as possible is one of the key aspects of it because that starts getting you the returns on an acquisition much faster. To be able to do that, I think having a single view of finance, having a single view of the supply chain as early and as fast as possible, is one of the most important things. Having a technology—or I would say a single pane of glass—that sits right on top of our platforms and also on top of the acquired business’ platform and us being able to look at a consolidated data view of both the data sets together is one of the most important things that can help us get synergies out of this business as fast as possible.That’s one aspect of it. The second aspect is with us having invested in some of the SaaS [software-as-a-service] solutions or SaaS applications, what ends up happening when you have the SaaS applications is that we end up not customizing these applications in a way that the industry looks at them. As a matter of fact, when you have an HR application, it is very standard and industry standard. Now when you acquire a business, if our business processes are pretty similar to each other, and if you have a SaaS solution and if they also use a SaaS solution, to integrate that certain business process onto our business processes becomes a lot easier. There is another aspect of why the new technology and the cloud platforms can be really helpful. And last but not least is, the moment you acquire a company, you also get a lot of business systems and applications that the acquired company had been using to run their businesses. As we integrate the acquired company onto us, what is important is to reduce that technical debt as fast as possible. Because the technical debt that we acquired has license costs, it has legal costs to it, it has data costs to it, and it has IT costs to it. If you look at them, the faster we get out of them, the better off we are. I mean, our aspect becomes simpler. And what we end up doing many times is we archive the data from the systems onto some of these cloud platforms and cloud services, and then are able to look at that from a historical perspective that helps us decommission this technical debt as fast as possible. Laurel: Well, we’ve certainly covered quite a bit of the current state of how you’re looking at technology. What are you thinking about for the future? How are you seeing technology innovation really helping in the next three to five years? Amit: That’s a great question. I will say that AI, even though it’s a buzzword, I think that it is a technology that does seem like it has a pretty great future even for us. Let me give you an example. As I said before, we journal terabytes of data within our four walls, just based upon doing business as usual. Now, there are so many things, as I said, gems that exist within our data set today. As humans, sometimes it is very hard to glean out what those gems are. I truly feel that the technology that exists and that is going to be coming out can look inside our data sets and be able to provide insights as to what are the data sets that we probably have not thought about can be leveraged further to, as I said, find the gems. That’s one side of the world. And the second side of the world is the unknowns, the predicting demand of the customers based upon the changing tastes and the demographics of our consumers, and then combining that data with the data that already exists within our system. I think that humans are going to take a long time to be able to get some insights, and AI definitely is going to be one of the key technologies that can help us get there faster. That’s number one. Number two is training using augmented reality. While I think “meta” seems like one of the other buzzwords that is out there, I think AR can definitely be of huge benefit to us. Typically, people have different ways of learning. Let’s say that you put somebody in a plant, a new employee. As you know, retention is very hard nowadays. If you have new employees coming in all the time, to train them on our processes and our machinery, our methods of working, I think it is generally pretty hard. But now think about if you are able to train this new team member that we have coming in with the means of some kind of augmentation. I think that is going to be the next generation of training and I feel that that can be something really cool that can happen in the future. Last but not the least, I would say machine learning, again, is used as a buzzword a lot, but in my mind, machine learning and putting the machine learning on some very small computers and then putting these computers in our manufacturing location where people are doing some of these mundane tasks day in and day out. The classic feedback control systems are not working efficiently, and there’s a human interaction needed. Where these tasks are performed, these mundane tasks are performed using humans. [But what if] we were able to introduce machine learning to be able to get rid of these mundane tasks that humans do and let them focus on more important things? I think machine learning in a box is going to be one of the other technologies that excites me. Laurel: Excellent. Those are great insights Amit. Thank you very much for joining us today on the Business Lab. Amit: Absolutely. Thank you for taking the time to talk to me. Laurel: That was Amit Khot, Senior Director of IT for Conagra Brands, who I spoke with from Cambridge, Massachusetts, the home of MIT and MIT Technology Review, overlooking the Charles River. That’s it for this episode of Business Lab. I’m your host, Laurel Ruma. I’m the Global Director of Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology, and you can find us in print, on the web, and at events each year around the world. For more information about us and the show, please check out our website at TechnologyReview.com. This show is available wherever you get your podcasts. If you enjoyed this episode, we hope you’ll take a moment to rate and review us. Business Lab is a production of MIT Technology Review. This episode was produced by Giro Studios. Thanks for listening. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.","In partnership withInfosys Cobalt From securing a hybrid workforce to building pipelines for ever-increasing data streams and keeping multiple mission-critical systems up and running , the modern IT department faces numerous pressures . As senior director of IT for the packaged food company Conagra , Amit Khot is optimistic about the ways modern technology solutions and infrastructure can enable businesses to thrive and innovate . Khot describes the power of advanced data analytics to both improve a company ’ s understanding of its customers and to optimize its operations . The ability to combine internal company data with data collected from social media and at point of sale will enable savvy companies to recognize new patterns . These advanced analytics , he says , will go beyond answering standard questions about financials and historical performance to provide insight into more complex questions about customers ’ thoughts and changing preferences . Meanwhile , these same data tools can also be used to fine-tune daily business operations , pinpointing issues with order fulfillment , improving long-range supply-and-demand forecasting , and digitizing manufacturing plant processes . Khot explains , “ Planning is looking into the future , depending upon your past historical data , as to what your future demand and supply should look like . We have gone through a journey to modernize our planning platforms. ” A modern enterprise resource planning ( ERP ) system is also a must for a distributed organization like Conagra . A single connected ERP system can manage and provide visibility into business processes that involve multiple divisions or departments . By doing so , a modern ERP can also ease highly complex processes , such as the technology integration of a newly acquired company . Says Khot , “ having a single view of finance , having a single view of the supply chain as early and as fast as possible , is one of the most important things that can help us get synergies out of the business as fast as possible. ” This episode of Business Lab is produced in partnership with Infosys Cobalt . FULL TRANSCRIPT Laurel Ruma : From MIT Technology Review , I ’ m Laurel Ruma and this is Business Lab , the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace . Our topic today is technological evolution . Companies , whether they ’ re regional or global , startup or legacy , need to be able to quickly deploy technologies as markets and supply chains shift and change . While many worries may keep executives up at night , building modern systems , and adopting the right technologies to better understand data will help those executives and companies gain efficiencies and provide an excellent customer experience . Two words for you : meeting demand . My guest is Amit Khot , Senior Director of IT for Conagra . Welcome , Amit.This episode of Business Lab is produced in partnership with Infosys Cobalt . Amit Khot : Hello , nice to meet you . Laurel : Great to have you here . So just a little background in case folks aren ’ t familiar with Conagra as a consumer packaged goods company that has been around for more than 100 years . Conagra produces products like Birds Eye , Healthy Choice , and Slim Jim ; various foods that you can find in supermarkets and in restaurants around the world . You have been with Conagra now for 23 years . How has your role evolved as a company and technology has transformed ? Amit : Absolutely . I started with Conagra , as you said , 23 years ago and I started as a program analyst with the company . Again , 23 years ago is a long time . Program analyst starting from that point and then I evolved into implementing our SAP ecosystem . That is what I started doing in the early 2000s . As time evolved , in the early 2010s , we started with a lot of mergers and acquisitions of businesses similar to ours . And as we started doing that , I played a role in doing due diligence for those businesses from an IT perspective . In addition to that , I also then helped integrate those companies within Conagra or Conagra Enterprise . In 2015 or so , we then did some divestitures and spins during that time , and I played a role in doing our program rating , an entire spinoff that we did for one of our major potato businesses , and I played the role of program director for that . During the same time , what we did is we went through an SG & A [ selling , general , and administrative expense ] reduction program , and I worked with some of our consulting businesses to come up with an analysis to say how much we spend on our production support . And that is the time when we actually contracted with Infosys to help us do the production support as part of aligning with the rest of the industry , where most of the industries were getting production support done by an outsourcing partner . I helped with that and right after that happened I had an opportunity to lead our SAP and integration platform . I did that and finally I ended up being in a role that I ’ m in right now , where I do enterprise architecture for applications on various value streams . That includes supply chain , manufacturing , finance , our global business systems , as well as platforms and integration . That ’ s my role currently , and that has been my journey for the last 23 years . A long time . Laurel : Well , certainly a long history of the company and how it evolved as well . But more recently , what kind of digital transformation has Conagra gone through in these recent years , and how do you approach these shifts and changes from an IT perspective ? Amit : I think that ’ s a great question . Digital transformation has many meanings . I mean [ sometimes ] , you do something which is really transformative . In other cases , you keep up with modernizing your technologies . One of the major initiatives that I helped design and lead initially was our S4 implementation . I helped with coming up with the design for our S4 , which is what we call our ERP [ enterprise resource planning ] modernization program . And now I help that program with providing subject matter expertise across the various aspects of how a modern ERP should look . That is one of the programs that we are going through right now . One of the other transformation journeys that , as a company , we have gone through is planning transformation . Planning is looking into the future , depending upon your past historical data , as to what your future demand and supply should look like . We have gone through a journey to modernize our planning platforms . It ’ s one of the other things that we have done . In addition to that , we are currently marching on a journey now to modernize and digitize our manufacturing . A large initiative . You might know that we have multiple plants and manufacturing locations and co-packers . Digitizing can ensure that we get the most efficiencies out of our platforms . So that is underway . And last but not the least , I will say that we have started getting pretty good maturity and understanding on various cloud services or cloud platforms in general . And as such , we have started maturing in cloud platforms like Azure services or SAP ’ s BTP [ Business Technology Platform ] and such . Those are some of the key initiatives that we have gone through to digitally transform our business and there are a lot of things that we plan on doing in the future . Laurel : I think it was particularly important that you mentioned that your role encompasses so many different parts of the company , because supply chain is certainly one of those important ones , and you have to think of systems from end to end . So how did the covid-19 pandemic affect Conagra as people worked from home and started to shop online and do that more and more ? Did this shift intensify adoption of specific technologies within the company ? Amit : What we didn ’ t do is we didn ’ t create a different strategy just to attack the covid pandemic . We had a lot of strategies built . I think one of the most important things that we had to do during the covid pandemic was keeping the system stable . Keeping the system stable is not a trivial task . I mean , if you look at our application and platform portfolios , it ’ s pretty large . Keeping everything up and running so that we can actually fulfill the customer ’ s demand is a big deal , and to keep them going , I think that was one of the most important things that we did during the pandemic . The next thing I would say is that we were premature in using our collaborative platform and collaboration technologies , like Office 365 and Webex , even before covid hit us . I would say that with the pandemic and everybody going remote , one of the most important things that we did is that we added more resiliency to some of those platforms . And our usage of that platform spiked to such an extent that that was almost a call that we failed . I mean , how much collaboration people did during that time using the technology . There were a lot of times before that , where people used to be in the rooms and in conference rooms doing whiteboarding and such , and delivering projects , being in one place , but with covid , I think people were leveraging a lot of these collaborative technologies to be able to get there . I would say the third thing that we did is , covid opened our eyes to how we changed our way of working . Before , we used to be delivering a lot of our solutions using waterfall methodologies . They used to be very long and they used to take up a lot of time , and we would not be able to figure out until the end whether we are going to be succeeding with some of the projects or not . We then adopted continuous delivery as a way to deliver work . And that spiked up the use of tools like JIRA quite a bit . But that was started during the covid time and we continue to use that more and more . And lastly , I would say that we had to do analysis on our data to figure out how we can , as I said before , how do we keep our system stable . But then also analyzing how do we fulfill the demand , and , as such , what are some of our pain points ? And we used some of the cloud platforms and cloud services to do some quick analysis to be able to fulfill our shipments . Those are the few things that we did and learned and adopted during the covid-19 pandemic . Laurel : And that ’ s certainly important to be able to actually see that data in real-time to help your customers . How do you think adopting cloud and using more data technologies will help your customer experience improve ? Amit : I think one of the most important things in our business is to have a 360-degree view of customers . I mean , it ’ s pretty vital , right ? As you might know , our business is a very customer-focused business . For us , large retailers are typically our customers . Our consumer is one step removed from us . What the technological advancement helps us now do , is , today , most of the information that we create as we do the business , resides within our four walls . As the social media platforms and such have become prominent , what is really important to us is being able to have the data that is inside of our four walls , mash that up with the data that is coming from the social media platforms , plus the point of sale data , all those things . When you mash all these things together , I think it provides us pretty decent consumer insights . These consumer insights , ultimately , lead us to a lot of product innovation . You might have seen our CEO talk about that . We have created a pretty decent new innovation pipeline during the last few years . And I think the digital technology and the technology that exists out there has definitely provided us with , I would say , a lot of capabilities to be able to innovate faster . The next thing I would say is the innovation side of the business is one side of the world , but the other side is then being able to fulfill the shipments on time and in full . If you look at it , there are a lot of customers of ours who want most of our shipments to be on time and in full . And if that doesn ’ t happen , we end up paying fines . Some of these digital technologies help us pinpoint where our issues are and what we should be doing differently to be able to fulfill our shipments on time and in full . That is another thing that we have gotten better at , and it ’ s just based upon the improvement of technology . Better planning . Better planning is equal to being able to predict the demands of our consumers and customers and how that then leads to us to be able to plan out some of the long-term horizons of supply . How do we do that from the long-term to the medium-term to the short-term . Those are the things that we have been able to do as a part of delivering some of our planning projects . That is based upon some of the modern technology that exists out there . And lastly , I would say the shop floor agility in general . With us investing in digital manufacturing , I would say that technology has definitely enabled us to be able to deliver digitization within manufacturing that has increased the shop floor agility . And I would say that that is going to be a long journey for us , but we are marching towards the results where we will be a lot more agile on the shop floor than we have ever been before . Laurel : And that ’ s so important when there are just more challenges thrown your way and also all these opportunities with such fantastic technology as well . And you mentioned this earlier , but why does Conagra need an enterprise resource planning system , and how does it partner with companies like Infosys to stay on that cutting edge of technology to make sure you can answer all of these challenges ? Amit : Yeah , that ’ s a great question . I mean , as I mentioned before , we have numerous plants , numerous customers , and numerous business partners that we work with . Once you have a lot of these , the impact and the business processes that cross doing these businesses , that crosses HR , that crosses supply chain , that crosses manufacturing . There are customer-facing business processes that exist . And if you look at all these processes that exist within any business like we have , which is basically a consumer foods business , what ends up happening is that if you do not have a combined view of your business at one place , it becomes an extremely hard proposition . Just doing simple business can become really hard . So it is really important for us to have a connected system , a connected view of business processes , and to enable something like that . ERPs play a very important role . As I said during your first question , when you asked me , “ what was your journey ? ” We started our journey of implementing SAP as an ERP in the early 2000s . One of the prime reasons for doing that is exactly to solve the problem that I just talked about : how do we get a consolidated view of our entire business cycle ? And that is what ERP helps us deliver . Now , ERP doesn ’ t just give you the stack of your business , but it also then gives you an ability to do analysis of the data that is in your system , and then create transformations that you wouldn ’ t do before , if all the systems in this business process were independent and isolated . So that is one of the big reasons why ERPs play such an important role in business like ours , and I would say that that is the case with most of the industry that we are in . Now when it comes to help from partners like Infosys to create the innovation , I would say it is a two-part answer . One of the first things is that ERPs have become so important in just running our business that having a stable system is one of the most important things that typically many of the IT functions deliver . To keep ourselves stable , partners like Infosys that help us manage our production and production support , they play an important aspect and role in making sure that the systems are stable and current from a technology perspective . That ’ s one aspect of it . Another thing that Infosys , and partners like Infosys , are helping us just do the production support . That frees up capacity of our subject matter experts to be able to then look at different solutions to solve the new business problems that pop up for us . That frees up the capacity for them to be able to do different things . That ’ s number two . Number three is that the companies like Infosys , and other business partners that we have , have a lot of customers just like us and even customers that are not in the same industry as us . What they hear , the business problems they hear from these other businesses and other customers that they have , that gives them an advantage in insights that we as Conagra by ourselves won ’ t be able to get . Because everybody has a different problem that they ’ re trying to solve . And if Infosys has that insight , they can provide us a great external point of view to be able to then solve some of the business problems that we have , which could be similar to what somebody else might have seen . And that just helps us solve these business problems faster . And this is an external point of view from a customer-centric perspective , but at the same time , with the scale and the number of partners that Infosys deals with , especially from the supplier side of the world , there are technologies that Infosys has reached which we do not have reach of . Partners like Infosys can even bring some of these advanced technologies that exist out there and provide us and guide us in . I believe that there lies a huge opportunity for companies like that to help us bring these new technologies and platforms to be able to help us solve some of the business problems that we have today—and probably solve and provide us insights into some of the business problems that might be coming to us that we have not thought about . Laurel : That ’ s a great point about the partnership with Infosys , and in general , how you actually bring the data and predictive analytics to your capabilities because you do have so much data coming in from fifty different brands , countless vendors , all those customers . How can this be maximized to gain those insights ? Amit : Yeah , that ’ s a great question . And just as you said , many brands and countless business partners and customers . We generate terabytes of data every year , and that data typically lies in our four walls . I mean , just in our ERPs and our business warehouse systems . And based upon that data , I think most of the industries like us have gotten really good at doing traditional analytics . Traditional analytics is equal to , how are our financials looking ? What is the performance of a certain brand depending upon the historical data ? And so on and so forth . I mean , that is the traditional analytics that we have gotten really good at . What becomes important now that you have gotten good traditional analytics is , what do you not know yet ? What are those gems within your existing data that you have not taken advantage of ? Some of these newer technologies and platforms , what they have started helping us do , and probably they ’ ll keep on helping us do , is being able to glean into our data and start pointing to what is it that we are not looking at . I mean , what we know is always great , but those unknowns that we have not actually gleaned into is what some of these technologies that are coming forward are going to be able to help us look at . That ’ s one aspect of the world . Now , the second aspect of the world is , as I said , the data exists just within our four walls . But as I said before , that social media data , that point of sale data , the data that doesn ’ t exist within our four walls , I think that has a different kind of insight and power . Now , think about the fact that you are able to mash up the data which is from these external sources and the data that you have inside , and then think about some of the data that you generate just because you have consumers that are calling into your consumer affairs division . You take all this data mashed up together , and I think you can create analytics that we were never able to produce before . And I think that is a power of what we get from just mashing all this data , and matching all this data together , and we can maximize a lot of insights . And then once you have that mashup happen , I think the predictions are different . In the sense that many times our existing forecasting solutions typically are very much dependent upon historical data to be able to do predictions on our supply and demand . They ’ re doing predictions like that . However , with the external data being mastered , I think it goes beyond that . I think it also starts giving us an insight into what the consumers are thinking , what the customers are thinking , how their tastes and choices are changing . I think that is the next forefront for us from a predictability perspective . And I think that the new technologies and platforms are going to help us do that yet better . Laurel : So this is a good point . We have this data and you need to make some really great decisions from it , but you also need to really assess those analytics , make predictions in the future , but also make sure your entire systems are running correctly end to end . How , then , can cloud applications coupled with this need and progress of your digital transformation journey help with a tactic like mergers and acquisitions that you mentioned earlier was part of your career ? How has that specifically been one of those things that helps the company actually create efficiencies and really see technology as a partner ? Amit : Yeah , absolutely . That ’ s a great question . One of the key reasons for acquisitions is that we can actually take advantage of the synergies that we can get . This is almost one plus one equal to three . That ’ s number one . Number two is , then on top of the synergies , the innovation pipeline , let ’ s say , the acquired company has and the experience that we have . When you combine those two together , I think we can create innovation at scale . That is two of the key reasons why we can go on and acquire a company . And when we do that , I think one of the most important aspects of that is then to take that acquired company and then basically integrate that company within our business processes . I would say that is a key activity that you have to partake in when you acquire a company . As we have gone through some of these digitalization journeys , as I said , we are pretty experienced with integrating some of these acquired companies into our enterprise , our systems , as well as in business processes . But that journey typically is not trivial . I mean , it takes a long time to integrate and acquire a company into our business processes . As we go through that journey , many times , being able to gain the insights of the business as quickly as possible is one of the key aspects of it because that starts getting you the returns on an acquisition much faster . To be able to do that , I think having a single view of finance , having a single view of the supply chain as early and as fast as possible , is one of the most important things . Having a technology—or I would say a single pane of glass—that sits right on top of our platforms and also on top of the acquired business ’ platform and us being able to look at a consolidated data view of both the data sets together is one of the most important things that can help us get synergies out of this business as fast as possible.That ’ s one aspect of it . The second aspect is with us having invested in some of the SaaS [ software-as-a-service ] solutions or SaaS applications , what ends up happening when you have the SaaS applications is that we end up not customizing these applications in a way that the industry looks at them . As a matter of fact , when you have an HR application , it is very standard and industry standard . Now when you acquire a business , if our business processes are pretty similar to each other , and if you have a SaaS solution and if they also use a SaaS solution , to integrate that certain business process onto our business processes becomes a lot easier . There is another aspect of why the new technology and the cloud platforms can be really helpful . And last but not least is , the moment you acquire a company , you also get a lot of business systems and applications that the acquired company had been using to run their businesses . As we integrate the acquired company onto us , what is important is to reduce that technical debt as fast as possible . Because the technical debt that we acquired has license costs , it has legal costs to it , it has data costs to it , and it has IT costs to it . If you look at them , the faster we get out of them , the better off we are . I mean , our aspect becomes simpler . And what we end up doing many times is we archive the data from the systems onto some of these cloud platforms and cloud services , and then are able to look at that from a historical perspective that helps us decommission this technical debt as fast as possible . Laurel : Well , we ’ ve certainly covered quite a bit of the current state of how you ’ re looking at technology . What are you thinking about for the future ? How are you seeing technology innovation really helping in the next three to five years ? Amit : That ’ s a great question . I will say that AI , even though it ’ s a buzzword , I think that it is a technology that does seem like it has a pretty great future even for us . Let me give you an example . As I said before , we journal terabytes of data within our four walls , just based upon doing business as usual . Now , there are so many things , as I said , gems that exist within our data set today . As humans , sometimes it is very hard to glean out what those gems are . I truly feel that the technology that exists and that is going to be coming out can look inside our data sets and be able to provide insights as to what are the data sets that we probably have not thought about can be leveraged further to , as I said , find the gems . That ’ s one side of the world . And the second side of the world is the unknowns , the predicting demand of the customers based upon the changing tastes and the demographics of our consumers , and then combining that data with the data that already exists within our system . I think that humans are going to take a long time to be able to get some insights , and AI definitely is going to be one of the key technologies that can help us get there faster . That ’ s number one . Number two is training using augmented reality . While I think “ meta ” seems like one of the other buzzwords that is out there , I think AR can definitely be of huge benefit to us . Typically , people have different ways of learning . Let ’ s say that you put somebody in a plant , a new employee . As you know , retention is very hard nowadays . If you have new employees coming in all the time , to train them on our processes and our machinery , our methods of working , I think it is generally pretty hard . But now think about if you are able to train this new team member that we have coming in with the means of some kind of augmentation . I think that is going to be the next generation of training and I feel that that can be something really cool that can happen in the future . Last but not the least , I would say machine learning , again , is used as a buzzword a lot , but in my mind , machine learning and putting the machine learning on some very small computers and then putting these computers in our manufacturing location where people are doing some of these mundane tasks day in and day out . The classic feedback control systems are not working efficiently , and there ’ s a human interaction needed . Where these tasks are performed , these mundane tasks are performed using humans . [ But what if ] we were able to introduce machine learning to be able to get rid of these mundane tasks that humans do and let them focus on more important things ? I think machine learning in a box is going to be one of the other technologies that excites me . Laurel : Excellent . Those are great insights Amit . Thank you very much for joining us today on the Business Lab . Amit : Absolutely . Thank you for taking the time to talk to me . Laurel : That was Amit Khot , Senior Director of IT for Conagra Brands , who I spoke with from Cambridge , Massachusetts , the home of MIT and MIT Technology Review , overlooking the Charles River . That ’ s it for this episode of Business Lab . I ’ m your host , Laurel Ruma . I ’ m the Global Director of Insights , the custom publishing division of MIT Technology Review . We were founded in 1899 at the Massachusetts Institute of Technology , and you can find us in print , on the web , and at events each year around the world . For more information about us and the show , please check out our website at TechnologyReview.com . This show is available wherever you get your podcasts . If you enjoyed this episode , we hope you ’ ll take a moment to rate and review us . Business Lab is a production of MIT Technology Review . This episode was produced by Giro Studios . Thanks for listening . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['partnership', 'secure', 'hybrid', 'workforce', 'build', 'pipeline', 'everincrease', 'datum', 'stream', 'keep', 'multiple', 'missioncritical', 'system', 'run', 'modern', 'department', 'face', 'numerous', 'pressure', 'senior', 'director', 'package', 'food', 'company', 'conagra', 'amit', 'khot', 'optimistic', 'way', 'modern', 'technology', 'solution', 'infrastructure', 'enable', 'business', 'thrive', 'innovate', 'khot', 'describe', 'power', 'advanced', 'datum', 'analytic', 'improve', 'company', 'understanding', 'customer', 'optimize', 'operation', 'ability', 'combine', 'internal', 'company', 'datum', 'datum', 'collect', 'social', 'medium', 'point', 'sale', 'enable', 'savvy', 'company', 'recognize', 'new', 'pattern', 'advanced', 'analytic', 'say', 'go', 'answer', 'standard', 'question', 'financial', 'historical', 'performance', 'provide', 'insight', 'complex', 'question', 'customer', 'thought', 'change', 'preference', 'meanwhile', 'data', 'tool', 'also', 'use', 'finetune', 'daily', 'business', 'operation', 'pinpointing', 'issue', 'order', 'fulfillment', 'improve', 'longrange', 'supplyanddemand', 'forecasting', 'digitize', 'manufacturing', 'plant', 'process', 'explain', 'planning', 'look', 'future', 'depend', 'past', 'historical', 'datum', 'future', 'demand', 'supply', 'look', 'go', 'journey', 'modernize', 'planning', 'platform', 'modern', 'enterprise', 'resource', 'planning', 'erp', 'system', 'also', 'must', 'distribute', 'organization', 'single', 'connect', 'erp', 'system', 'manage', 'provide', 'visibility', 'business', 'process', 'involve', 'multiple', 'division', 'department', 'modern', 'erp', 'also', 'ease', 'highly', 'complex', 'process', 'technology', 'integration', 'newly', 'acquire', 'company', 'say', 'khot', 'single', 'view', 'finance', 'single', 'view', 'supply', 'chain', 'early', 'fast', 'possible', 'important', 'thing', 'help', 'get', 'synergy', 'business', 'fast', 'possible', 'episode', 'business', 'lab', 'produce', 'partnership', 'infosy', 'cobalt', 'full', 'transcript', 'laurel', 'mit', 'technology', 'review', 'business', 'lab', 'show', 'help', 'business', 'leader', 'make', 'sense', 'new', 'technology', 'come', 'lab', 'marketplace', 'topic', 'today', 'technological', 'evolution', 'company', 'regional', 'global', 'startup', 'legacy', 'need', 'able', 'quickly', 'deploy', 'technology', 'market', 'supply', 'chain', 'shift', 'change', 'many', 'worry', 'keep', 'executive', 'night', 'build', 'modern', 'system', 'adopt', 'right', 'technology', 'well', 'understand', 'datum', 'help', 'executive', 'company', 'gain', 'efficiency', 'provide', 'excellent', 'customer', 'experience', 'word', 'meet', 'demand', 'guest', 'amit', 'khot', 'senior', 'director', 'conagra', 'welcome', 'episode', 'business', 'lab', 'produce', 'partnership', 'infosys', 'cobalt', 'amit', 'khot', 'nice', 'meet', 'great', 'little', 'background', 'case', 'folk', 'familiar', 'conagra', 'consumer', 'package', 'good', 'company', 'around', 'year', 'conagra', 'produce', 'product', 'bird', 'eye', 'healthy', 'choice', 'slim', 'various', 'food', 'find', 'supermarket', 'restaurant', 'world', 'conagra', 'year', 'role', 'evolve', 'company', 'technology', 'transform', 'amit', 'absolutely', 'start', 'conagra', 'say', 'year', 'ago', 'start', 'program', 'analyst', 'company', 'year', 'ago', 'long', 'time', 'program', 'analyst', 'start', 'point', 'evolve', 'implement', 'ecosystem', 'start', 'early', 'time', 'evolve', 'early', 'start', 'lot', 'merger', 'acquisition', 'business', 'similar', 'start', 'play', 'role', 'due', 'diligence', 'business', 'perspective', 'addition', 'also', 'integrate', 'company', 'conagra', 'conagra', 'enterprise', 'divestiture', 'spin', 'time', 'play', 'role', 'program', 'rating', 'entire', 'spinoff', 'major', 'potato', 'business', 'play', 'role', 'program', 'director', 'time', 'go', 'sell', 'general', 'administrative', 'expense', 'reduction', 'program', 'work', 'consult', 'business', 'come', 'analysis', 'say', 'much', 'spend', 'production', 'support', 'time', 'actually', 'contract', 'infosy', 'help', 'production', 'support', 'part', 'align', 'rest', 'industry', 'industry', 'get', 'production', 'support', 'outsourcing', 'partner', 'help', 'right', 'happen', 'opportunity', 'lead', 'sap', 'integration', 'platform', 'finally', 'end', 'role', 'right', 'enterprise', 'architecture', 'application', 'various', 'value', 'stream', 'include', 'supply', 'chain', 'manufacturing', 'finance', 'global', 'business', 'system', 'well', 'platform', 'integration', 'role', 'currently', 'journey', 'last', 'year', 'long', 'time', 'laurel', 'well', 'certainly', 'long', 'history', 'company', 'evolve', 'well', 'recently', 'kind', 'digital', 'transformation', 'conagra', 'go', 'recent', 'year', 'approach', 'shift', 'change', 'perspective', 'amit', 'think', 'great', 'question', 'digital', 'transformation', 'many', 'meaning', 'mean', 'sometimes', 'really', 'transformative', 'case', 'keep', 'modernize', 'technology', 'major', 'initiative', 'help', 'design', 'lead', 'initially', 'implementation', 'help', 'come', 'design', 'call', 'erp', 'enterprise', 'resource', 'planning', 'modernization', 'program', 'help', 'program', 'provide', 'subject', 'matter', 'expertise', 'various', 'aspect', 'modern', 'erp', 'look', 'program', 'go', 'right', 'transformation', 'journey', 'company', 'go', 'plan', 'transformation', 'planning', 'look', 'future', 'depend', 'past', 'historical', 'datum', 'future', 'demand', 'supply', 'look', 'go', 'journey', 'modernize', 'planning', 'platform', 'thing', 'addition', 'currently', 'march', 'journey', 'modernize', 'digitize', 'manufacturing', 'large', 'initiative', 'know', 'multiple', 'plant', 'manufacturing', 'location', 'copacker', 'digitizing', 'ensure', 'get', 'efficiency', 'platform', 'underway', 'last', 'least', 'say', 'start', 'get', 'pretty', 'good', 'maturity', 'understanding', 'various', 'cloud', 'service', 'cloud', 'platform', 'general', 'start', 'mature', 'cloud', 'platform', 'azure', 'service', 'business', 'technology', 'platform', 'key', 'initiative', 'go', 'digitally', 'transform', 'business', 'lot', 'thing', 'plan', 'future', 'laurel', 'think', 'particularly', 'important', 'mention', 'role', 'encompass', 'many', 'different', 'part', 'company', 'supply', 'chain', 'certainly', 'important', 'one', 'think', 'system', 'end', 'end', 'covid19', 'pandemic', 'affect', 'conagra', 'people', 'work', 'home', 'start', 'shop', 'online', 'shift', 'intensify', 'adoption', 'specific', 'technology', 'company', 'amit', 'create', 'different', 'strategy', 'attack', 'covid', 'pandemic', 'lot', 'strategy', 'build', 'think', 'important', 'thing', 'covid', 'pandemic', 'keep', 'system', 'stable', 'keep', 'system', 'stable', 'trivial', 'task', 'mean', 'look', 'application', 'platform', 'portfolio', 'pretty', 'large', 'keep', 'run', 'actually', 'fulfill', 'customer', 'demand', 'big', 'deal', 'keep', 'go', 'think', 'important', 'thing', 'pandemic', 'next', 'thing', 'say', 'premature', 'use', 'collaborative', 'platform', 'collaboration', 'technology', 'office', 'webex', 'even', 'covid', 'hit', 'say', 'pandemic', 'go', 'remote', 'important', 'thing', 'add', 'resiliency', 'platform', 'usage', 'platform', 'spike', 'extent', 'almost', 'call', 'fail', 'mean', 'much', 'collaboration', 'people', 'time', 'use', 'technology', 'lot', 'time', 'people', 'use', 'room', 'conference', 'room', 'whiteboarding', 'deliver', 'project', 'place', 'think', 'people', 'leverage', 'lot', 'collaborative', 'technology', 'able', 'get', 'say', 'third', 'thing', 'covid', 'open', 'eye', 'change', 'way', 'work', 'use', 'deliver', 'lot', 'solution', 'use', 'waterfall', 'methodology', 'use', 'long', 'use', 'take', 'lot', 'time', 'able', 'figure', 'end', 'go', 'succeed', 'project', 'adopt', 'continuous', 'delivery', 'way', 'deliver', 'work', 'spike', 'use', 'tool', 'bit', 'start', 'covid', 'time', 'continue', 'use', 'lastly', 'say', 'analysis', 'datum', 'figure', 'say', 'keep', 'system', 'stable', 'also', 'analyze', 'fulfill', 'demand', 'pain', 'point', 'use', 'cloud', 'platform', 'cloud', 'service', 'quick', 'analysis', 'able', 'fulfill', 'shipment', 'thing', 'learn', 'adopt', 'covid19', 'pandemic', 'laurel', 'certainly', 'important', 'able', 'actually', 'see', 'datum', 'realtime', 'help', 'customer', 'think', 'adopt', 'cloud', 'use', 'datum', 'technology', 'help', 'customer', 'experience', 'improve', 'amit', 'think', 'important', 'thing', 'business', 'view', 'customer', 'mean', 'pretty', 'vital', 'right', 'know', 'business', 'customerfocused', 'business', 'large', 'retailer', 'typically', 'customer', 'consumer', 'step', 'remove', 'technological', 'advancement', 'help', 'today', 'information', 'create', 'business', 'reside', 'wall', 'social', 'medium', 'platform', 'become', 'prominent', 'really', 'important', 'able', 'datum', 'inside', 'wall', 'mash', 'datum', 'come', 'social', 'medium', 'platform', 'point', 'sale', 'datum', 'thing', 'mash', 'thing', 'together', 'think', 'provide', 'pretty', 'decent', 'consumer', 'insight', 'consumer', 'insight', 'ultimately', 'lead', 'lot', 'product', 'innovation', 'see', 'ceo', 'talk', 'create', 'pretty', 'decent', 'new', 'innovation', 'pipeline', 'last', 'year', 'think', 'digital', 'technology', 'technology', 'exist', 'definitely', 'provide', 'say', 'lot', 'capability', 'able', 'innovate', 'fast', 'next', 'thing', 'say', 'innovation', 'side', 'business', 'side', 'world', 'side', 'able', 'fulfill', 'shipment', 'time', 'full', 'look', 'lot', 'customer', 'want', 'shipment', 'time', 'full', 'happen', 'end', 'pay', 'fine', 'digital', 'technology', 'help', 'pinpoint', 'issue', 'differently', 'able', 'fulfill', 'shipment', 'time', 'full', 'thing', 'get', 'well', 'base', 'improvement', 'technology', 'well', 'plan', 'well', 'planning', 'equal', 'able', 'predict', 'demand', 'consumer', 'customer', 'lead', 'able', 'plan', 'longterm', 'horizon', 'supply', 'longterm', 'mediumterm', 'shortterm', 'thing', 'able', 'part', 'deliver', 'planning', 'project', 'base', 'modern', 'technology', 'exist', 'lastly', 'say', 'shop', 'floor', 'agility', 'general', 'invest', 'digital', 'manufacturing', 'say', 'technology', 'definitely', 'enable', 'able', 'deliver', 'digitization', 'manufacturing', 'increase', 'shop', 'floor', 'agility', 'say', 'go', 'long', 'journey', 'march', 'result', 'lot', 'agile', 'shop', 'floor', 'ever', 'laurel', 'important', 'challenge', 'throw', 'way', 'also', 'opportunity', 'fantastic', 'technology', 'well', 'mention', 'early', 'conagra', 'need', 'enterprise', 'resource', 'planning', 'system', 'partner', 'company', 'infosy', 'stay', 'cut', 'edge', 'technology', 'make', 'sure', 'answer', 'challenge', 'amit', 'great', 'question', 'mean', 'mention', 'numerous', 'plant', 'numerous', 'customer', 'numerous', 'business', 'partner', 'work', 'lot', 'impact', 'business', 'process', 'cross', 'business', 'cross', 'hr', 'cross', 'supply', 'chain', 'crosse', 'manufacture', 'customerface', 'business', 'process', 'exist', 'look', 'process', 'exist', 'business', 'basically', 'consumer', 'food', 'business', 'end', 'happen', 'combine', 'view', 'business', 'place', 'become', 'extremely', 'hard', 'proposition', 'simple', 'business', 'become', 'really', 'hard', 'really', 'important', 'connected', 'system', 'connect', 'view', 'business', 'process', 'enable', 'erps', 'play', 'important', 'role', 'say', 'first', 'question', 'ask', 'journey', 'start', 'journey', 'implement', 'erp', 'early', 'prime', 'reason', 'exactly', 'solve', 'problem', 'talk', 'get', 'consolidated', 'view', 'entire', 'business', 'cycle', 'erp', 'help', 'deliver', 'erp', 'give', 'stack', 'business', 'also', 'give', 'ability', 'analysis', 'datum', 'system', 'create', 'transformation', 'system', 'business', 'process', 'independent', 'isolated', 'big', 'reason', 'erp', 'play', 'important', 'role', 'business', 'say', 'case', 'industry', 'come', 'help', 'partner', 'infosy', 'create', 'innovation', 'say', 'twopart', 'answer', 'first', 'thing', 'erp', 'become', 'important', 'run', 'business', 'stable', 'system', 'important', 'thing', 'typically', 'many', 'function', 'deliver', 'keep', 'stable', 'partner', 'infosy', 'help', 'manage', 'production', 'production', 'support', 'play', 'important', 'aspect', 'role', 'make', 'sure', 'system', 'stable', 'current', 'technology', 'perspective', 'aspect', 'thing', 'infosys', 'partner', 'infosy', 'help', 'production', 'support', 'free', 'capacity', 'subject', 'matter', 'expert', 'able', 'look', 'different', 'solution', 'solve', 'new', 'business', 'problem', 'pop', 'free', 'capacity', 'able', 'different', 'thing', 'number', 'number', 'company', 'infosy', 'business', 'partner', 'lot', 'customer', 'even', 'customer', 'industry', 'hear', 'business', 'problem', 'hear', 'business', 'customer', 'give', 'advantage', 'insight', 'conagra', 'win', 'able', 'get', 'different', 'problem', 'try', 'solve', 'infosy', 'insight', 'provide', 'great', 'external', 'point', 'view', 'able', 'solve', 'business', 'problem', 'similar', 'else', 'see', 'help', 'solve', 'business', 'problem', 'fast', 'external', 'point', 'view', 'customercentric', 'perspective', 'time', 'scale', 'number', 'partner', 'infosys', 'deal', 'especially', 'supplier', 'side', 'world', 'technology', 'infosy', 'reach', 'reach', 'partner', 'infosy', 'even', 'bring', 'advanced', 'technology', 'exist', 'provide', 'guide', 'believe', 'lie', 'huge', 'opportunity', 'company', 'help', 'bring', 'new', 'technology', 'platform', 'able', 'help', 'solve', 'business', 'problem', 'today', 'probably', 'solve', 'provide', 'insight', 'business', 'problem', 'come', 'think', 'laurel', 'great', 'point', 'partnership', 'infosy', 'general', 'actually', 'bring', 'datum', 'predictive', 'analytic', 'capability', 'much', 'datum', 'come', 'different', 'brand', 'countless', 'vendor', 'customer', 'maximize', 'gain', 'insight', 'amit', 'great', 'question', 'say', 'many', 'brand', 'countless', 'business', 'partner', 'customer', 'generate', 'terabyte', 'datum', 'year', 'datum', 'typically', 'lie', 'wall', 'mean', 'erps', 'business', 'warehouse', 'system', 'base', 'datum', 'think', 'industry', 'get', 'really', 'good', 'traditional', 'analytic', 'traditional', 'analytic', 'equal', 'financial', 'look', 'performance', 'certain', 'brand', 'depend', 'historical', 'datum', 'forth', 'mean', 'traditional', 'analytic', 'get', 'really', 'good', 'become', 'important', 'get', 'good', 'traditional', 'analytic', 'know', 'yet', 'gem', 'exist', 'datum', 'take', 'advantage', 'new', 'technology', 'platform', 'start', 'help', 'probably', 'keep', 'help', 'able', 'glean', 'datum', 'start', 'point', 'look', 'mean', 'know', 'always', 'great', 'unknown', 'actually', 'glean', 'technology', 'come', 'forward', 'go', 'able', 'help', 'look', 'aspect', 'world', 'second', 'aspect', 'world', 'say', 'datum', 'exist', 'wall', 'say', 'social', 'medium', 'datum', 'point', 'sale', 'datum', 'datum', 'exist', 'wall', 'think', 'different', 'kind', 'insight', 'power', 'think', 'fact', 'able', 'mash', 'datum', 'external', 'source', 'datum', 'inside', 'think', 'datum', 'generate', 'consumer', 'call', 'consumer', 'affair', 'division', 'take', 'datum', 'mash', 'together', 'think', 'create', 'analytic', 'never', 'able', 'produce', 'think', 'power', 'get', 'mash', 'datum', 'match', 'datum', 'together', 'maximize', 'lot', 'insight', 'happen', 'think', 'prediction', 'different', 'sense', 'many', 'time', 'exist', 'forecasting', 'solution', 'typically', 'much', 'dependent', 'historical', 'datum', 'able', 'prediction', 'supply', 'demand', 'prediction', 'however', 'external', 'datum', 'master', 'think', 'go', 'think', 'also', 'start', 'give', 'insight', 'consumer', 'think', 'customer', 'think', 'taste', 'choice', 'change', 'think', 'next', 'forefront', 'predictability', 'perspective', 'think', 'new', 'technology', 'platform', 'go', 'help', 'yet', 'well', 'laurel', 'good', 'point', 'datum', 'need', 'make', 'really', 'great', 'decision', 'also', 'need', 'really', 'assess', 'analytic', 'make', 'prediction', 'future', 'also', 'make', 'sure', 'entire', 'system', 'run', 'correctly', 'end', 'end', 'cloud', 'application', 'couple', 'need', 'progress', 'digital', 'transformation', 'journey', 'help', 'tactic', 'merger', 'acquisition', 'mention', 'early', 'part', 'career', 'specifically', 'thing', 'help', 'company', 'actually', 'create', 'efficiency', 'really', 'see', 'technology', 'partner', 'amit', 'absolutely', 'great', 'question', 'key', 'reason', 'acquisition', 'actually', 'take', 'advantage', 'synergy', 'get', 'almost', 'equal', 'number', 'number', 'top', 'synergy', 'innovation', 'pipeline', 'let', 'say', 'acquire', 'company', 'experience', 'combine', 'together', 'think', 'create', 'innovation', 'scale', 'key', 'reason', 'go', 'acquire', 'company', 'think', 'important', 'aspect', 'take', 'acquire', 'company', 'basically', 'integrate', 'company', 'business', 'process', 'say', 'key', 'activity', 'partake', 'acquire', 'company', 'go', 'digitalization', 'journey', 'say', 'pretty', 'experienced', 'integrate', 'acquire', 'company', 'enterprise', 'system', 'well', 'business', 'process', 'journey', 'typically', 'trivial', 'mean', 'take', 'long', 'time', 'integrate', 'acquire', 'company', 'business', 'process', 'go', 'journey', 'many', 'time', 'able', 'gain', 'insight', 'business', 'quickly', 'possible', 'key', 'aspect', 'start', 'get', 'return', 'acquisition', 'much', 'fast', 'able', 'think', 'single', 'view', 'finance', 'single', 'view', 'supply', 'chain', 'early', 'fast', 'possible', 'important', 'thing', 'technology', 'say', 'single', 'pane', 'glass', 'sit', 'right', 'top', 'platform', 'also', 'top', 'acquire', 'business', 'platform', 'able', 'look', 'consolidated', 'data', 'view', 'datum', 'set', 'together', 'important', 'thing', 'help', 'get', 'synergy', 'business', 'fast', 'aspect', 'second', 'aspect', 'invest', 'saas', 'softwareasaservice', 'solution', 'saas', 'application', 'end', 'happen', 'saas', 'application', 'end', 'customize', 'application', 'way', 'industry', 'look', 'matter', 'fact', 'hr', 'application', 'standard', 'industry', 'standard', 'acquire', 'business', 'business', 'process', 'pretty', 'similar', 'saas', 'solution', 'also', 'use', 'saas', 'solution', 'integrate', 'certain', 'business', 'process', 'business', 'process', 'become', 'lot', 'easy', 'aspect', 'new', 'technology', 'cloud', 'platform', 'really', 'helpful', 'last', 'least', 'moment', 'acquire', 'company', 'also', 'get', 'lot', 'business', 'system', 'application', 'acquire', 'company', 'use', 'run', 'business', 'integrate', 'acquire', 'company', 'important', 'reduce', 'technical', 'debt', 'fast', 'possible', 'technical', 'debt', 'acquire', 'license', 'cost', 'legal', 'cost', 'datum', 'cost', 'cost', 'look', 'fast', 'get', 'well', 'mean', 'aspect', 'become', 'simple', 'end', 'many', 'time', 'archive', 'datum', 'system', 'cloud', 'platform', 'cloud', 'service', 'able', 'look', 'historical', 'perspective', 'help', 'decommission', 'technical', 'debt', 'fast', 'possible', 'laurel', 'well', 'certainly', 'cover', 'bit', 'current', 'state', 'look', 'technology', 'think', 'future', 'see', 'technology', 'innovation', 'really', 'help', 'next', 'year', 'amit', 'great', 'question', 'say', 'ai', 'even', 'buzzword', 'think', 'technology', 'seem', 'pretty', 'great', 'future', 'even', 'let', 'give', 'example', 'say', 'journal', 'terabyte', 'datum', 'wall', 'base', 'business', 'usual', 'many', 'thing', 'say', 'gem', 'exist', 'datum', 'set', 'today', 'human', 'sometimes', 'hard', 'glean', 'gem', 'truly', 'feel', 'technology', 'exist', 'go', 'come', 'look', 'data', 'set', 'able', 'provide', 'insight', 'datum', 'set', 'probably', 'think', 'leverage', 'far', 'find', 'gem', 'side', 'world', 'second', 'side', 'world', 'unknown', 'predict', 'demand', 'customer', 'base', 'change', 'taste', 'demographic', 'consumer', 'combine', 'datum', 'datum', 'already', 'exist', 'system', 'think', 'human', 'go', 'take', 'long', 'time', 'able', 'get', 'insight', 'definitely', 'go', 'key', 'technology', 'help', 'get', 'fast', 'number', 'number', 'training', 'use', 'augmented', 'reality', 'think', 'meta', 'seem', 'buzzword', 'think', 'ar', 'definitely', 'huge', 'benefit', 'typically', 'people', 'different', 'way', 'learn', 'let', 'say', 'put', 'plant', 'new', 'employee', 'know', 'retention', 'hard', 'nowadays', 'new', 'employee', 'come', 'time', 'train', 'process', 'machinery', 'method', 'work', 'think', 'generally', 'pretty', 'hard', 'think', 'able', 'train', 'new', 'team', 'member', 'come', 'mean', 'kind', 'augmentation', 'think', 'go', 'next', 'generation', 'training', 'feel', 'really', 'cool', 'happen', 'future', 'last', 'least', 'say', 'machine', 'learn', 'use', 'buzzword', 'lot', 'mind', 'machine', 'learning', 'put', 'machine', 'learn', 'small', 'computer', 'put', 'computer', 'manufacturing', 'location', 'people', 'mundane', 'task', 'day', 'day', 'classic', 'feedback', 'control', 'system', 'work', 'efficiently', 'human', 'interaction', 'need', 'task', 'perform', 'mundane', 'task', 'perform', 'use', 'human', 'able', 'introduce', 'machine', 'learning', 'able', 'get', 'rid', 'mundane', 'task', 'human', 'let', 'focus', 'important', 'thing', 'think', 'machine', 'learn', 'box', 'go', 'technology', 'excite', 'laurel', 'excellent', 'great', 'insight', 'amit', 'thank', 'much', 'join', 'today', 'business', 'lab', 'amit', 'absolutely', 'thank', 'take', 'time', 'talk', 'laurel', 'amit', 'khot', 'senior', 'director', 'conagra', 'brand', 'speak', 'home', 'mit', 'mit', 'technology', 'review', 'overlook', 'episode', 'business', 'lab', 'host', 'global', 'director', 'insight', 'custom', 'publishing', 'division', 'mit', 'technology', 'review', 'found', 'find', 'print', 'web', 'event', 'year', 'world', 'information', 'show', 'check', 'website', 'show', 'available', 'get', 'podcast', 'enjoy', 'episode', 'hope', 'take', 'moment', 'rate', 'review', 'business', 'lab', 'production', 'mit', 'technology', 'review', 'episode', 'produce', 'studio', 'thank', 'listen', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","From securing a hybrid workforce to building pipelines for ever-increasing data streams and keeping multiple mission-critical systems up and running, the modern IT department faces numerous pressures. As senior director of IT for the packaged food company Conagra, Amit Khot is optimistic about the ways modern technology solutions and infrastructure can enable businesses to thrive…"
ChatGPT can turn bad writers into better ones,https://www.technologyreview.com/2023/07/13/1076199/chatgpt-can-turn-bad-writers-into-better-ones/,2023-07-13,"People have been using ChatGPT to help them to do their jobs since it was released in November of last year, with enthusiastic adopters using it to help them write everything from marketing materials to emails to reports. Now we have the first indication of its effect in the workplace. A new study by two MIT economics graduate students, published today in Science, suggests it could help reduce gaps in writing ability between employees. They found that it could enable less experienced workers who lack writing skills to produce work similar in quality to that of more skilled colleagues. Shakked Noy and Whitney Zhang recruited 453 marketers, data analysts, and college-educated professionals and got each of them to complete two kinds of tasks they’d normally undertake as part of their jobs, such as writing press releases, short reports, or analysis plans. Half were given the option of using ChatGPT to help them complete the second of the two tasks. A group of other professionals then quality-checked the results, grading the writing on a scale of 1 to 7, with 7 the best. Each piece of work was evaluated by three people working in the same professions, hired through the research platform Prolific.  New large language models will transform many jobs. Whether they will lead to widespread prosperity or not is up to us. The writers who chose to use ChatGPT took 40% less time to complete their tasks, and produced work that the assessors scored 18% higher in quality than that of the participants who didn’t use it. The writers who were already skilled at writing were able to reduce the amount of time they spent on their work, while those who were assessed as being weaker writers produced higher-quality work once they gained access to the chatbot. “ChatGPT is just very good at producing this kind of written content, and so using it to automate parts of the writing process seems likely to save a lot of time,” says Noy, lead author of the research. “One thing that’s clear is that this is very useful for white-collar work—a lot of people will be using it, and it’s going to have a pretty big effect on how white-collar work is structured,” he adds. However, the output of ChatGPT and other generative AI models is far from reliable. ChatGPT is very good at presenting false information as factually correct, meaning that although workers may be able to leverage it to help them produce more work, they also run the risk of introducing errors.  Depending on the nature of a person’s job, those kinds of inaccuracies could have serious implications. Lawyer Steven Schwartz was fined $5,000 by a judge last month for using ChatGPT to produce a legal brief that contained false judicial opinions and legal citations.“Technological advances are commonplace and there is nothing inherently improper about using a reliable artificial intelligence tool for assistance,” the judge, Kevin Castel, wrote. “But existing rules impose a gatekeeping role on attorneys to ensure the accuracy of their filings.” The research hints at how AI could be helpful in the workplace by acting as a sort of virtual assistant, says Riku Arakawa, a researcher at Carnegie Mellon University who studies workers’ use of large language models, and was not involved with the research.  ""I think this is a really interesting result that demonstrates how human-AI cooperation works really well in this kind of task. When a human leverages AI to refine their output, they can produce better content,"" he adds. ","People have been using ChatGPT to help them to do their jobs since it was released in November of last year , with enthusiastic adopters using it to help them write everything from marketing materials to emails to reports . Now we have the first indication of its effect in the workplace . A new study by two MIT economics graduate students , published today in Science , suggests it could help reduce gaps in writing ability between employees . They found that it could enable less experienced workers who lack writing skills to produce work similar in quality to that of more skilled colleagues . Shakked Noy and Whitney Zhang recruited 453 marketers , data analysts , and college-educated professionals and got each of them to complete two kinds of tasks they ’ d normally undertake as part of their jobs , such as writing press releases , short reports , or analysis plans . Half were given the option of using ChatGPT to help them complete the second of the two tasks . A group of other professionals then quality-checked the results , grading the writing on a scale of 1 to 7 , with 7 the best . Each piece of work was evaluated by three people working in the same professions , hired through the research platform Prolific . New large language models will transform many jobs . Whether they will lead to widespread prosperity or not is up to us . The writers who chose to use ChatGPT took 40 % less time to complete their tasks , and produced work that the assessors scored 18 % higher in quality than that of the participants who didn ’ t use it . The writers who were already skilled at writing were able to reduce the amount of time they spent on their work , while those who were assessed as being weaker writers produced higher-quality work once they gained access to the chatbot . “ ChatGPT is just very good at producing this kind of written content , and so using it to automate parts of the writing process seems likely to save a lot of time , ” says Noy , lead author of the research . “ One thing that ’ s clear is that this is very useful for white-collar work—a lot of people will be using it , and it ’ s going to have a pretty big effect on how white-collar work is structured , ” he adds . However , the output of ChatGPT and other generative AI models is far from reliable . ChatGPT is very good at presenting false information as factually correct , meaning that although workers may be able to leverage it to help them produce more work , they also run the risk of introducing errors . Depending on the nature of a person ’ s job , those kinds of inaccuracies could have serious implications . Lawyer Steven Schwartz was fined $ 5,000 by a judge last month for using ChatGPT to produce a legal brief that contained false judicial opinions and legal citations. “ Technological advances are commonplace and there is nothing inherently improper about using a reliable artificial intelligence tool for assistance , ” the judge , Kevin Castel , wrote . “ But existing rules impose a gatekeeping role on attorneys to ensure the accuracy of their filings. ” The research hints at how AI could be helpful in the workplace by acting as a sort of virtual assistant , says Riku Arakawa , a researcher at Carnegie Mellon University who studies workers ’ use of large language models , and was not involved with the research . `` I think this is a really interesting result that demonstrates how human-AI cooperation works really well in this kind of task . When a human leverages AI to refine their output , they can produce better content , '' he adds .","['people', 'use', 'chatgpt', 'help', 'job', 'release', 'last', 'year', 'enthusiastic', 'adopter', 'use', 'help', 'write', 'marketing', 'material', 'email', 'report', 'first', 'indication', 'effect', 'workplace', 'new', 'study', 'mit', 'economic', 'graduate', 'student', 'publish', 'today', 'science', 'suggest', 'help', 'reduce', 'gap', 'write', 'ability', 'employee', 'find', 'enable', 'less', 'experienced', 'worker', 'lack', 'write', 'skill', 'produce', 'work', 'similar', 'quality', 'skilled', 'colleague', 'shakke', 'noy', 'recruit', 'marketer', 'datum', 'analyst', 'collegeeducate', 'professional', 'get', 'complete', 'kind', 'task', 'normally', 'undertake', 'part', 'job', 'write', 'press', 'release', 'short', 'report', 'analysis', 'plan', 'half', 'give', 'option', 'use', 'chatgpt', 'help', 'complete', 'second', 'task', 'group', 'professional', 'qualitychecke', 'result', 'grade', 'writing', 'scale', 'good', 'piece', 'work', 'evaluate', 'people', 'work', 'profession', 'hire', 'research', 'platform', 'prolific', 'new', 'large', 'language', 'model', 'transform', 'many', 'job', 'lead', 'widespread', 'prosperity', 'writer', 'choose', 'use', 'chatgpt', 'take', 'less', 'time', 'complete', 'task', 'produce', 'work', 'assessor', 'score', 'high', 'quality', 'participant', 'use', 'writer', 'already', 'skilled', 'writing', 'able', 'reduce', 'amount', 'time', 'spend', 'work', 'assess', 'weak', 'writer', 'produce', 'higherquality', 'work', 'gain', 'access', 'chatbot', 'chatgpt', 'good', 'produce', 'kind', 'write', 'content', 'use', 'automate', 'part', 'writing', 'process', 'seem', 'likely', 'save', 'lot', 'time', 'say', 'noy', 'lead', 'author', 'research', 'thing', 'clear', 'useful', 'whitecollar', 'work', 'lot', 'people', 'use', 'go', 'pretty', 'big', 'effect', 'whitecollar', 'work', 'structure', 'add', 'however', 'output', 'chatgpt', 'generative', 'model', 'far', 'reliable', 'chatgpt', 'good', 'present', 'false', 'information', 'factually', 'correct', 'mean', 'worker', 'able', 'leverage', 'help', 'produce', 'work', 'also', 'run', 'risk', 'introduce', 'error', 'depend', 'nature', 'person', 'job', 'kind', 'inaccuracy', 'serious', 'implication', 'lawyer', 'fine', 'judge', 'last', 'month', 'use', 'chatgpt', 'produce', 'legal', 'brief', 'contain', 'false', 'judicial', 'opinion', 'legal', 'citation', 'technological', 'advance', 'commonplace', 'inherently', 'improper', 'use', 'reliable', 'artificial', 'intelligence', 'tool', 'assistance', 'judge', 'write', 'exist', 'rule', 'impose', 'gatekeepe', 'role', 'attorney', 'ensure', 'accuracy', 'filing', 'research', 'hint', 'helpful', 'workplace', 'act', 'sort', 'virtual', 'assistant', 'say', 'researcher', 'university', 'study', 'worker', 'use', 'large', 'language', 'model', 'involve', 'research', 'think', 'really', 'interesting', 'result', 'demonstrate', 'humanai', 'cooperation', 'work', 'really', 'well', 'kind', 'task', 'human', 'leverage', 'ai', 'refine', 'output', 'produce', 'well', 'content', 'add']","<p>People who use ChatGPT to help with writing tasks are more productive and produce higher-quality work than those who don’t, a study found.</p>
"
Migrating to the cloud transforms business,https://www.technologyreview.com/2023/09/15/1078111/migrating-to-the-cloud-transforms-business/,2023-09-15,"In partnership withInfosys Cobalt In 2017, BP took on a cloud-first approach that committed to building any new hardware or system builds on the cloud. Just a year prior, only 2% of BP applications lived on the cloud. At the close of 2022, 90% of BP applications had migrated to cloud environments, changing product and service integration and BP’s overall digital operating model. Cloud transformations like BP’s can help enterprises improve operational resiliency, accelerate technology adoption, and reduce data center carbon emissions, says vice president of Digital Foundations at BP, Keisha Garcia. Migrating to the cloud at that scale, however, is challenging and extensive, especially when dealing with a legacy IT estate. “Utilizing cloud platforms provides the necessary computational power and tools to implement advanced analytics, predictive modeling, as well as simulation techniques, which also enables us to continuously improve our sustainability performance,” says Garcia. To successfully migrate to the cloud and subsequently collaborate and deploy cloud technologies, Garcia stresses the importance of clear communication among employees as well as stakeholders. “Involve application teams, service owners, end users early in the development and delivery of the strategy. Again, just bringing everyone along for the journey, I cannot overstate how important that is,” says Garcia. A hybrid approach to transformation that combines cloud migration with the retention of some applications, dedicated data centers, and intermediary migration environments can ensure cost effective and secure operations. With enterprise-wide communication underpinning any successful transformation, Garcia outlines having a strong and flexible governance framework, collaborating with external digital partners, and adapting to agile ways of working as best practices for complex cloud migrations. Looking to the cloud-enabled future, Garcia identifies the convergence of AI and edge computing, mounting progress in quantum computing, and the proliferation of IoT connected devices as transformative technologies that will drive forward better business outcomes.   “I think that the convergence of edge computing and AI presents an exciting opportunity for the real-time data, a real-time low latency processing and decision making at the network edge, which is extremely critical for us, given all of the platforms, rigs that we have out across the globe,” says Garcia. This episode of Business Lab is produced in partnership with Infosys Cobalt.  Laurel Ruma: From MIT Technology Review, I'm Laurel Ruma, and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.Our topic today is building a better cloud ecosystem. From partners to internal stakeholders, enterprises are meeting challenges by deploying and innovating with cloud computing solutions. The key is to work as a team and build talent resources to confidently adopt emerging technologies.Two words for you: optimizing cloud.My guest is Keisha Garcia. Keisha is the vice president of Digital Foundations at BP.This episode of Business Lab is sponsored by Infosys Cobalt.Welcome, Keisha. Keisha Garcia: Thank you, Laurel. I'm happy to be here. Laurel: Well, let's start off. So, what has BP's move to the cloud been like? From your perspectives, what are the major benefits and challenges with cloud transformation? Keisha: Yeah, so our journey from my perspective has been exciting, it's been complex, it's been a learning journey all the while. And it's been long. It's been pretty long. Our journey started in 2013 and we were experimenting on cloud computing for email services and HR learning management systems. And then you fast-forward to 2016, and we were about 2% of our BP applications were on the cloud. As the company, we were conducting proof of concepts and determining what was the best approach and how do we do this at scale, large scale. In 2017, we adopted a cloud-first approach, meaning that anything that was any new hardware, any new system builds were going to be on the cloud, no more adding to our existing, at the time, eight mega data centers and over 107 different data centers throughout our regions throughout the world. We had decided that we were no longer going to add anything, unless it was to the cloud. Or if it had to be on-prem, it had to be by exception only. And so that kind of motivated us to push along and got everybody along for the journey. But again, just getting all of our business and everyone else sold into the business or sold into cloud and cloud concepts and all of those things, given the fact that there were a lot of unknowns at the time, so working with just different vendor partners and trying to find as knowledgeable people as possible. So again, that just all fed to the complexity. By the close of 2022, we had gotten into our stride pretty well and had done quite a few, or a large part of our state, over 90% of our state, to be exact, we have migrated to our cloud environments, which enabled our faster product and service introduction and changing the BP digital operating model, which we've moved now to a product-led organization. So our cloud migrations, I think some of the biggest benefits was it helped us optimize BP's technology stack. Of course, it increased our operational resilience. It introduced new network and data architectures, accelerated our technology adoption, helped to push the modernizing of our state and keeping those evergreen, it also assisted in the reduction of our CO2 emissions from our data centers. However, migrating to the cloud at the scale that we've had to, as large as our landscape is, again, as I said, was challenging, complex, and extensive because we had extensive legacy IT estate. And as I said earlier on, just hosted in the eight large mega data centers throughout the US, as well as in Europe, and then also just the myriad of data centers that we have across our regions. So the challenge, I cannot overstate, it has been that, but the gains have been great. Laurel: So that's a great look at the past and the journey that BP has been on. So, what are some of the major cloud trends you're seeing today? Keisha: So some of the major trends that we're seeing today from a platforms perspective, see increasing numbers of organizations looking to consolidate their business applications on the cloud-based platforms, be more cloud native, have robust data and analytics platforms as well that will allow both real-time and on-demand access to key business information. Again, as we said, we've moved to a product-led organization, so we're seeing that, of course, there's several companies that are doing the same. Digital teams are aligning to product-led operating models to ensure customer centricity and customer focus. And then also just putting that at the forefront. And then product development and enabling business and business-led prioritization and product delivery, which helps, again, with us aligning more to our business strategy. And given where we are going with moving to an integrated energy company and our transition with re:Invent, that has been huge for us. There's lots of markets that we're tapping into, lots of things that we're doing, and we have to get on board with the business to be able to be dynamic and be able to shift and be able to move, and to be able to provide a faster time to market with solutions. And so from that standpoint, being on a cloud platform, having all of the technology that's available to us to do that at pace and align with our business has been awesome for us. And I'm seeing a lot more companies wanting to just share experiences and knowledge because they're trying to do the same. Also, just the cloud native piece of that and cloud native enterprise is an organization that has aligned business and technology teams to help, again, modernize the estate, but we have to build more cloud native capability so that things can be more plug and play versus the huge build outs. And then again, having to do a lot of the upgrading and all of the things that would come along with not building and being on top, or being with more in the cloud native state. Also, just again, part of our reinvention journey, this also enables climate action. We're seeing a lot of folks that are moving towards doing the things that align with the Paris Agreement, as well as all of the things that we're doing along re:Invent. So decarbonizing digital as assets directly impacts about 2% of the global energy consumption. So therefore, it helps. Every bit helps. And so therefore, those are the things that we're seeing. And we're also, of course, moving there in that space to also assist with us getting to net zero. There's also just being able to be more of a connected world. So 2023, this year and beyond, promises, opportunities for large scale industrial 5G, broadband based IoT usage and catapult connections for remote regions. And so we've really started to build off that as well with building digital twins and all of the different things that we're doing at our refineries, and then also on our rigs and platforms that will capitalize on just the cloud-based technology. So, there's quite a few things that we're seeing that are trending, but things that we're already in the works with and moving towards. And the last one is just the evolution of the CIO that we're seeing. The CIO seems to have gone away. I don't see a lot of CIO titles anymore that are out there. And we definitely have moved away from that, as well as the way our organization is structured. And as I said earlier, aligning a lot more with product led organizations and making sure that we have technology leaders that are elevating their financial acumen, along with business prioritizations and outcomes, and bringing that business value and finding where those value streams are within your business strategies and aligning to those, and then evaluating and bringing about the technology that will be the catalyst and a differentiator for most businesses, and definitely ours. Laurel: That's quite a bit. And you mentioned this a little bit earlier, but how do you actually bring together a company to maintain and manage and optimize all of those business practices in the cloud? What are some of those best practices companies should be thinking about in order to collaborate and deploy cloud technologies? Keisha: I think the biggest thing that we are seeing, or that I saw that were best practices and lessons learned and things, is just providing clear stakeholder communications. If you don't have your business on board and understand what it is that you're doing and why you're doing it and what's in it for them, it's going to be hard. It's going to be really, really hard to do a mass migration as we have, an adoption of cloud, the way that we have. And I hate that covid-19 happened, but it definitely forced the business to really see the benefit because we were pretty much about, I want to say 50% on the cloud by the time, probably a little less than 50% on the cloud, by the time covid hit, but we were on the cloud for our major things. And our business really didn't skip a beat really with being able to connect from anywhere in the world. And so they saw the benefit of that. They saw some of the things that we had talked about. But just having that clear outline of communication around what you will get, what you don't get, where you will be in each part of the journey, I cannot express how important that it was to do that. Involve application teams, service owners, end users early in the development and delivery of the strategy. Again, just bringing everyone along for the journey, I cannot overstate how important that is. Modify your operating model, your digital operating model specifically to align so that you're working more seamlessly together across different areas and allowing for the breakdown of expertise in particular areas and having that focus on that expertise and continuing to develop that and evolve that. Because technology, as always, but definitely in this space, changes extremely quickly. And so therefore, you have got to ensure that your people are getting as educated, updated with the skill sets as possible. And building on the benefits, a realization plan, was also key. So those are some of the softer ones that I would think that people might overlook. Other ones, just the hybrid approach that we had, the hybrid approach to transformation. We recognized early on the need for a hybrid approach, combining our cloud migration with the retention of certain applications, dedicated data centers, intermediary migration environments, allowing for cost effective and secure operations. Those are some of the best practices as far as just how we were going to transform and what that looks like, and not thinking that it's a one size fits all and being able to assess your estate, what's best if you have a large in the service of life estate, a legacy estate, as we did where we were legacy with operating model—the code base on our applications across our entire landscape, it was huge. I think when we first started this journey, a good amount of over 60% of our estate was in the end of the service of a life due to one of one or the other of the things that I just mentioned. And so instead of trying to tackle those separately, we decided, what's the best way for us to leverage and bring that all together? So being flexible and looking at the art of the possible across your state and what you have to do to address multiple things was also really a great way to look at this as well. Because you and I probably know from experience, just any time that you say that you're going to go back and do something, nine times out of 10, you don't. You do the first tactical thing and it stays that way forever. So it was, for me, a good thing for us to take best practice, to, if we only have to do something once or only have to open up the box once, then let's just open it up once and figure out how much of transformation can we do in one time to keep us at ease, but also to cover as much modernization as we can before we hand them back over to our ops teams. I think I've already touched on just CIO buy-in and business buy-in. Those are best practices, some of those things that were softer that I mentioned earlier. Having a governance framework, delivery model restructured for effectiveness, and how do you get things approved and establish those things, again, upfront, having that delivery model to ensure smooth cloud migrations while also ensuring business service continuity and accommodating evolving business requirements? Because as you know, again, with some of the trends that we talked about or that I mentioned earlier, those trends are things that when you start to go and implement those things, the business change is enormous. And so being able to be flexible to accommodate those, but not being beared down by who needs to approve this, who's making this decision. If you establish those things upfront with a good governance framework and a delivery model that allows for that flexibility and effectiveness, then that was also key and golden. The collaboration with digital delivery partners. I can't express enough finding great delivery partners. There's no way that knowledge is known by everyone in your organization. And like I said, given the pace at which technology changes and things are being rolled out, you always need people that are also keeping their fingers on the pulse from learnings and different experiences. And so you can only get that sometimes if you also worked externally with external partners. And we had a couple of them, quite a few of them actually, that proved to be very, very great partners. And we all learned together with several of the others, but Emphasis has been a major partner of ours. We had eight vendor partners to supplement in-house capabilities, and it was great. Adapting modern ways of working, agility. And agility in its simplest forms, but also just being agile and utilizing agile practices, which will help you move much faster, setting up your squads, those types of things. And then of course, I'll say it again, last one, but communication, communication, communication, and training for sustainability and just continuing to build your knowledge base to be able to continue to support the platforms and the new technology that's coming on board. So those are some of the things that we saw, or that's lessons learned, best practices in general. Laurel: You mentioned this a little bit earlier, but how critical is talent to that kind of cloud transformation? And what are some of techniques–communication clearly–for recruiting and refining talent for adopting cloud technologies? Keisha: Yeah. Given the fact that, as we talked, there's so many people that are going along this journey, some at the very beginning, some middle, some almost nearing the end. But because of that, the market out there is extremely competitive to get great talent. And then also, just upscaling your talent that you have in the door already. Your existing staff is also critical. So, offering a competitive compensation package, as well as providing training and certification opportunities. Because again, it's keeping your employees motivated and keeping them focused on being a hundred percent all in and passionate around what we're doing and why we're doing it, but also recognizing that people have to enjoy what they do. And the compensation package has to be great. And also, the learning opportunities and promoting a learning culture has to be there because that's what people are looking for. As we see, as people are moving from place to place, in order to retain great talent, in order to attract great talent, all of those offerings need to be there. They're important. They're important for the success of any transformation program that you're doing for some of the reasons that I touched on earlier, the pace at which technology moves, as well as the fact that everybody is out doing all of these things to test the waters, and to also create a more sustainable environment, to also create, to be able to get to market faster, to create all the different trends that are happening with people working in different spaces and places across the globe. All of those things, the offerings have to be there to attract that talent. But most of all, also building a diverse and inclusive workforce. And in order to do that, the offering has to be there across the board for people that want to work from home, people that want to work in an office building, people that are doing different things or at different stages and points in their lives. Having that flexibility to offer your employees to retain that great talent is absolutely key and critical for the purposes of the success of your transformation. Laurel: And then you did mention the importance of working with partners, especially when you're trying to build this collaborative ecosystem. So, what is that like working with partners in this large-scale cloud transformation? Keisha: Again, you can't know everything, and you're not always going to get all of those things. So that's where you have the extension of, I would say an extension of additional brain power, extension of learning and those things. Leveraging partnerships with educational institutions, collaborating with universities and colleges to establish internships, co-op programs, and recruitment pipelines for cloud related roles. Because again, as you see in universities, technology is key and they're learning new things. And students coming out of universities, they're more conscious of all of the things that are going on in the environment, and they're wanting to work with people that are moving towards making the environment better and the sustainability of that. And the low carbon initiatives that are going on and getting to net zero, believe it or not, those are all things that are known. As soon as I go into recruiting at a university, it's the first thing that they ask. What's really going on with re:Invent? What do you see and how do you utilize technology to help leverage that? So I think building those partnerships with educational institutions are great, as well as those partnerships with our third-party vendors that we've done as well, because they're doing some of the same things with getting students, and as well as keeping up with the trends, keeping up their skillsets and capabilities and being able to have that flex staff to flex up and down as necessary. As you go through the ebb and flows of your journey of transformation, things start, stop, and/or increase, and sometimes you need to move at pace, or just the complexities that comes with marrying things with moving off of your legacy estate and still trying to keep that BAU [business as usual] and no downtime for your business. So therefore, all of those things, you cannot know within one organization. You have to look to research and development. And again, the partnerships with the universities, the partnerships with third-party vendors are absolutely critical and key. Laurel: You've mentioned sustainability a couple times. So how moving to the cloud and adopting these kinds of emerging technologies actually help BP as a company address the sustainability goals that it may have? Keisha: We have the reduced environmental impact paired with efficient resource utilization. So, moving to the cloud allowed us to reduce our carbon footprint by transitioning from on-premise data centers to a more energy efficient cloud infrastructure. We are dual cloud company. We use both AWS and Microsoft Azure. And so definitely working with both of them for what they're doing around the energy efficient cloud infrastructure that they're pushing and doing, and working with them on all hands of how to measure that. Also what's the projection of what we're going to contribute as we continue to move forward and get to nearing the end of our cloud journey. Also enabling us to optimize our energy consumption, like I said, by scaling resources up and down based on demand, driving efficient energy usage, reducing waste, and contributing to our wider BP sustainability goals as well. There was a time, again, when we were on-prem and we would have large amounts of servers running. And some of those servers were literally less than 50% utilized. But yet they're still on. They're still utilizing energy as well. So this moving to cloud allows us, again, from the optimization perspective of what we consume. Also, low carbon emissions, data-driven sustainability, and enhanced operational efficiency. Moving to the cloud supports and drives our low carbon emission by enabling our company to utilize renewable power sources, so by adopting emerging technologies, such as AI and machine learning. The transformation to cloud allows for us to analyze vast amounts of data, driving our innovation and decision-making power for BP's sustainable initiatives. And again, this has been huge for us. And being data-driven helps identify opportunities for resource optimization, emissions reduction, as well as environmental impact mitigation. So in the data space, large opportunities there. And then also, there's just a continuous improvement in innovation, and having or utilizing cloud platforms provides the necessary computational power and tools to implement advanced analytics, predictive modeling, as well as simulation techniques, which also enables us to continuously improve our sustainability performance. And it also allows for new solutions to be provided, as well as to contribute to all of the industry-wide sustainability advancements, things that when I get around other CIO tables or other tables with other people that are leading their transformations, we share ideas, we talk about the things that we're doing, how we're measuring that. And sharing that across the table is really good because, again, you get to also hear some of the things that they're doing, which gives you some of the ideas of how they're using technology to continue with the sustainability goals. So from that standpoint, that's how we've helped to leverage our cloud transformation to help with our sustainability aspirations of getting to net zero. Laurel: Yeah, that's quite significant. You've outlined major cloud trends like going cloud native that you're seeing today. So, what are some of the cloud-enabled technologies or use cases that you're really excited to see emerge in the next three to five years? Keisha: So, I would say that I'm really excited about...there's quite a few, so I'll try and limit it. So edge AI. I think that the convergence of edge computing and AI presents an exciting opportunity for the real-time data, a real-time low latency processing and decision making at the network edge, which is extremely critical for us, given all of the platforms, rigs, that we have out across the globe. That is absolutely key. And I'm excited about that because this technology helps to enable and develop our innovative applications in our industry to optimize the energy consumption of smart grids and enhance predictive maintenance and our operations. So for me, edge AI is really one that I'm excited about. Also, quantum computing. It has the potential to solve complex problems and perform computations that are currently infeasible for classical computers. So in the next three to five years, I'd expect to see significant progress in quantum computing technology, which has the potential to revolutionize the way we approach computational challenges and drive innovation across multiple sectors of our business, but multiple sectors of the energy industry in general. And then I'll probably think of a couple more. I would say things have progressed quite far along in this space, but IoT and integration and analytics in that space, the proliferation of IoT devices continues to generate massive volumes of data. So cloud platforms will play a crucial role in processing, analyzing, and extracting meaningful insights from this data. In the next few years, and currently even today, I think we see further advancements in cloud based IoT integration analytics, as well as enabling US or other organizations to harness the full potential of IoT data, which will drive smarter decision making and predictive maintenance, as well as asset optimization and automation, or optimization rather. So I just think from an IoT perspective, again, big driver. We've been doing digital twins, we've been doing quite a few things within our platforms and just within our production business. Those are some of the three that really excite me. And then of course, there's augmented reality, but I won't go into that. But there's a few things that are coming along that really excite us and will be driving our business forward. Laurel: Fantastic. Keisha, thank you so much for joining us today on the Business Lab. Keisha: Thank you for having me. Laurel: That was Keisha Garcia, the vice president of Digital Foundations at BP, who I spoke with from Cambridge, Massachusetts, the home of MIT and MIT Technology Review, overlooking the Charles River. That's it for this episode of Business Lab. I'm your host, Laurel Ruma. I'm the director of Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology. And you can also find us in print on the web and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com. This show is available wherever you get your podcasts. If you enjoyed this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT Technology Review. This episode was produced by Giro Studios. Thanks for listening. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.","In partnership withInfosys Cobalt In 2017 , BP took on a cloud-first approach that committed to building any new hardware or system builds on the cloud . Just a year prior , only 2 % of BP applications lived on the cloud . At the close of 2022 , 90 % of BP applications had migrated to cloud environments , changing product and service integration and BP ’ s overall digital operating model . Cloud transformations like BP ’ s can help enterprises improve operational resiliency , accelerate technology adoption , and reduce data center carbon emissions , says vice president of Digital Foundations at BP , Keisha Garcia . Migrating to the cloud at that scale , however , is challenging and extensive , especially when dealing with a legacy IT estate . “ Utilizing cloud platforms provides the necessary computational power and tools to implement advanced analytics , predictive modeling , as well as simulation techniques , which also enables us to continuously improve our sustainability performance , ” says Garcia . To successfully migrate to the cloud and subsequently collaborate and deploy cloud technologies , Garcia stresses the importance of clear communication among employees as well as stakeholders . “ Involve application teams , service owners , end users early in the development and delivery of the strategy . Again , just bringing everyone along for the journey , I can not overstate how important that is , ” says Garcia . A hybrid approach to transformation that combines cloud migration with the retention of some applications , dedicated data centers , and intermediary migration environments can ensure cost effective and secure operations . With enterprise-wide communication underpinning any successful transformation , Garcia outlines having a strong and flexible governance framework , collaborating with external digital partners , and adapting to agile ways of working as best practices for complex cloud migrations . Looking to the cloud-enabled future , Garcia identifies the convergence of AI and edge computing , mounting progress in quantum computing , and the proliferation of IoT connected devices as transformative technologies that will drive forward better business outcomes . “ I think that the convergence of edge computing and AI presents an exciting opportunity for the real-time data , a real-time low latency processing and decision making at the network edge , which is extremely critical for us , given all of the platforms , rigs that we have out across the globe , ” says Garcia . This episode of Business Lab is produced in partnership with Infosys Cobalt . Laurel Ruma : From MIT Technology Review , I 'm Laurel Ruma , and this is Business Lab , the show that helps business leaders make sense of new technologies coming out of the lab and into the marketplace.Our topic today is building a better cloud ecosystem . From partners to internal stakeholders , enterprises are meeting challenges by deploying and innovating with cloud computing solutions . The key is to work as a team and build talent resources to confidently adopt emerging technologies.Two words for you : optimizing cloud.My guest is Keisha Garcia . Keisha is the vice president of Digital Foundations at BP.This episode of Business Lab is sponsored by Infosys Cobalt.Welcome , Keisha . Keisha Garcia : Thank you , Laurel . I 'm happy to be here . Laurel : Well , let 's start off . So , what has BP 's move to the cloud been like ? From your perspectives , what are the major benefits and challenges with cloud transformation ? Keisha : Yeah , so our journey from my perspective has been exciting , it 's been complex , it 's been a learning journey all the while . And it 's been long . It 's been pretty long . Our journey started in 2013 and we were experimenting on cloud computing for email services and HR learning management systems . And then you fast-forward to 2016 , and we were about 2 % of our BP applications were on the cloud . As the company , we were conducting proof of concepts and determining what was the best approach and how do we do this at scale , large scale . In 2017 , we adopted a cloud-first approach , meaning that anything that was any new hardware , any new system builds were going to be on the cloud , no more adding to our existing , at the time , eight mega data centers and over 107 different data centers throughout our regions throughout the world . We had decided that we were no longer going to add anything , unless it was to the cloud . Or if it had to be on-prem , it had to be by exception only . And so that kind of motivated us to push along and got everybody along for the journey . But again , just getting all of our business and everyone else sold into the business or sold into cloud and cloud concepts and all of those things , given the fact that there were a lot of unknowns at the time , so working with just different vendor partners and trying to find as knowledgeable people as possible . So again , that just all fed to the complexity . By the close of 2022 , we had gotten into our stride pretty well and had done quite a few , or a large part of our state , over 90 % of our state , to be exact , we have migrated to our cloud environments , which enabled our faster product and service introduction and changing the BP digital operating model , which we 've moved now to a product-led organization . So our cloud migrations , I think some of the biggest benefits was it helped us optimize BP 's technology stack . Of course , it increased our operational resilience . It introduced new network and data architectures , accelerated our technology adoption , helped to push the modernizing of our state and keeping those evergreen , it also assisted in the reduction of our CO2 emissions from our data centers . However , migrating to the cloud at the scale that we 've had to , as large as our landscape is , again , as I said , was challenging , complex , and extensive because we had extensive legacy IT estate . And as I said earlier on , just hosted in the eight large mega data centers throughout the US , as well as in Europe , and then also just the myriad of data centers that we have across our regions . So the challenge , I can not overstate , it has been that , but the gains have been great . Laurel : So that 's a great look at the past and the journey that BP has been on . So , what are some of the major cloud trends you 're seeing today ? Keisha : So some of the major trends that we 're seeing today from a platforms perspective , see increasing numbers of organizations looking to consolidate their business applications on the cloud-based platforms , be more cloud native , have robust data and analytics platforms as well that will allow both real-time and on-demand access to key business information . Again , as we said , we 've moved to a product-led organization , so we 're seeing that , of course , there 's several companies that are doing the same . Digital teams are aligning to product-led operating models to ensure customer centricity and customer focus . And then also just putting that at the forefront . And then product development and enabling business and business-led prioritization and product delivery , which helps , again , with us aligning more to our business strategy . And given where we are going with moving to an integrated energy company and our transition with re : Invent , that has been huge for us . There 's lots of markets that we 're tapping into , lots of things that we 're doing , and we have to get on board with the business to be able to be dynamic and be able to shift and be able to move , and to be able to provide a faster time to market with solutions . And so from that standpoint , being on a cloud platform , having all of the technology that 's available to us to do that at pace and align with our business has been awesome for us . And I 'm seeing a lot more companies wanting to just share experiences and knowledge because they 're trying to do the same . Also , just the cloud native piece of that and cloud native enterprise is an organization that has aligned business and technology teams to help , again , modernize the estate , but we have to build more cloud native capability so that things can be more plug and play versus the huge build outs . And then again , having to do a lot of the upgrading and all of the things that would come along with not building and being on top , or being with more in the cloud native state . Also , just again , part of our reinvention journey , this also enables climate action . We 're seeing a lot of folks that are moving towards doing the things that align with the Paris Agreement , as well as all of the things that we 're doing along re : Invent . So decarbonizing digital as assets directly impacts about 2 % of the global energy consumption . So therefore , it helps . Every bit helps . And so therefore , those are the things that we 're seeing . And we 're also , of course , moving there in that space to also assist with us getting to net zero . There 's also just being able to be more of a connected world . So 2023 , this year and beyond , promises , opportunities for large scale industrial 5G , broadband based IoT usage and catapult connections for remote regions . And so we 've really started to build off that as well with building digital twins and all of the different things that we 're doing at our refineries , and then also on our rigs and platforms that will capitalize on just the cloud-based technology . So , there 's quite a few things that we 're seeing that are trending , but things that we 're already in the works with and moving towards . And the last one is just the evolution of the CIO that we 're seeing . The CIO seems to have gone away . I do n't see a lot of CIO titles anymore that are out there . And we definitely have moved away from that , as well as the way our organization is structured . And as I said earlier , aligning a lot more with product led organizations and making sure that we have technology leaders that are elevating their financial acumen , along with business prioritizations and outcomes , and bringing that business value and finding where those value streams are within your business strategies and aligning to those , and then evaluating and bringing about the technology that will be the catalyst and a differentiator for most businesses , and definitely ours . Laurel : That 's quite a bit . And you mentioned this a little bit earlier , but how do you actually bring together a company to maintain and manage and optimize all of those business practices in the cloud ? What are some of those best practices companies should be thinking about in order to collaborate and deploy cloud technologies ? Keisha : I think the biggest thing that we are seeing , or that I saw that were best practices and lessons learned and things , is just providing clear stakeholder communications . If you do n't have your business on board and understand what it is that you 're doing and why you 're doing it and what 's in it for them , it 's going to be hard . It 's going to be really , really hard to do a mass migration as we have , an adoption of cloud , the way that we have . And I hate that covid-19 happened , but it definitely forced the business to really see the benefit because we were pretty much about , I want to say 50 % on the cloud by the time , probably a little less than 50 % on the cloud , by the time covid hit , but we were on the cloud for our major things . And our business really did n't skip a beat really with being able to connect from anywhere in the world . And so they saw the benefit of that . They saw some of the things that we had talked about . But just having that clear outline of communication around what you will get , what you do n't get , where you will be in each part of the journey , I can not express how important that it was to do that . Involve application teams , service owners , end users early in the development and delivery of the strategy . Again , just bringing everyone along for the journey , I can not overstate how important that is . Modify your operating model , your digital operating model specifically to align so that you 're working more seamlessly together across different areas and allowing for the breakdown of expertise in particular areas and having that focus on that expertise and continuing to develop that and evolve that . Because technology , as always , but definitely in this space , changes extremely quickly . And so therefore , you have got to ensure that your people are getting as educated , updated with the skill sets as possible . And building on the benefits , a realization plan , was also key . So those are some of the softer ones that I would think that people might overlook . Other ones , just the hybrid approach that we had , the hybrid approach to transformation . We recognized early on the need for a hybrid approach , combining our cloud migration with the retention of certain applications , dedicated data centers , intermediary migration environments , allowing for cost effective and secure operations . Those are some of the best practices as far as just how we were going to transform and what that looks like , and not thinking that it 's a one size fits all and being able to assess your estate , what 's best if you have a large in the service of life estate , a legacy estate , as we did where we were legacy with operating model—the code base on our applications across our entire landscape , it was huge . I think when we first started this journey , a good amount of over 60 % of our estate was in the end of the service of a life due to one of one or the other of the things that I just mentioned . And so instead of trying to tackle those separately , we decided , what 's the best way for us to leverage and bring that all together ? So being flexible and looking at the art of the possible across your state and what you have to do to address multiple things was also really a great way to look at this as well . Because you and I probably know from experience , just any time that you say that you 're going to go back and do something , nine times out of 10 , you do n't . You do the first tactical thing and it stays that way forever . So it was , for me , a good thing for us to take best practice , to , if we only have to do something once or only have to open up the box once , then let 's just open it up once and figure out how much of transformation can we do in one time to keep us at ease , but also to cover as much modernization as we can before we hand them back over to our ops teams . I think I 've already touched on just CIO buy-in and business buy-in . Those are best practices , some of those things that were softer that I mentioned earlier . Having a governance framework , delivery model restructured for effectiveness , and how do you get things approved and establish those things , again , upfront , having that delivery model to ensure smooth cloud migrations while also ensuring business service continuity and accommodating evolving business requirements ? Because as you know , again , with some of the trends that we talked about or that I mentioned earlier , those trends are things that when you start to go and implement those things , the business change is enormous . And so being able to be flexible to accommodate those , but not being beared down by who needs to approve this , who 's making this decision . If you establish those things upfront with a good governance framework and a delivery model that allows for that flexibility and effectiveness , then that was also key and golden . The collaboration with digital delivery partners . I ca n't express enough finding great delivery partners . There 's no way that knowledge is known by everyone in your organization . And like I said , given the pace at which technology changes and things are being rolled out , you always need people that are also keeping their fingers on the pulse from learnings and different experiences . And so you can only get that sometimes if you also worked externally with external partners . And we had a couple of them , quite a few of them actually , that proved to be very , very great partners . And we all learned together with several of the others , but Emphasis has been a major partner of ours . We had eight vendor partners to supplement in-house capabilities , and it was great . Adapting modern ways of working , agility . And agility in its simplest forms , but also just being agile and utilizing agile practices , which will help you move much faster , setting up your squads , those types of things . And then of course , I 'll say it again , last one , but communication , communication , communication , and training for sustainability and just continuing to build your knowledge base to be able to continue to support the platforms and the new technology that 's coming on board . So those are some of the things that we saw , or that 's lessons learned , best practices in general . Laurel : You mentioned this a little bit earlier , but how critical is talent to that kind of cloud transformation ? And what are some of techniques–communication clearly–for recruiting and refining talent for adopting cloud technologies ? Keisha : Yeah . Given the fact that , as we talked , there 's so many people that are going along this journey , some at the very beginning , some middle , some almost nearing the end . But because of that , the market out there is extremely competitive to get great talent . And then also , just upscaling your talent that you have in the door already . Your existing staff is also critical . So , offering a competitive compensation package , as well as providing training and certification opportunities . Because again , it 's keeping your employees motivated and keeping them focused on being a hundred percent all in and passionate around what we 're doing and why we 're doing it , but also recognizing that people have to enjoy what they do . And the compensation package has to be great . And also , the learning opportunities and promoting a learning culture has to be there because that 's what people are looking for . As we see , as people are moving from place to place , in order to retain great talent , in order to attract great talent , all of those offerings need to be there . They 're important . They 're important for the success of any transformation program that you 're doing for some of the reasons that I touched on earlier , the pace at which technology moves , as well as the fact that everybody is out doing all of these things to test the waters , and to also create a more sustainable environment , to also create , to be able to get to market faster , to create all the different trends that are happening with people working in different spaces and places across the globe . All of those things , the offerings have to be there to attract that talent . But most of all , also building a diverse and inclusive workforce . And in order to do that , the offering has to be there across the board for people that want to work from home , people that want to work in an office building , people that are doing different things or at different stages and points in their lives . Having that flexibility to offer your employees to retain that great talent is absolutely key and critical for the purposes of the success of your transformation . Laurel : And then you did mention the importance of working with partners , especially when you 're trying to build this collaborative ecosystem . So , what is that like working with partners in this large-scale cloud transformation ? Keisha : Again , you ca n't know everything , and you 're not always going to get all of those things . So that 's where you have the extension of , I would say an extension of additional brain power , extension of learning and those things . Leveraging partnerships with educational institutions , collaborating with universities and colleges to establish internships , co-op programs , and recruitment pipelines for cloud related roles . Because again , as you see in universities , technology is key and they 're learning new things . And students coming out of universities , they 're more conscious of all of the things that are going on in the environment , and they 're wanting to work with people that are moving towards making the environment better and the sustainability of that . And the low carbon initiatives that are going on and getting to net zero , believe it or not , those are all things that are known . As soon as I go into recruiting at a university , it 's the first thing that they ask . What 's really going on with re : Invent ? What do you see and how do you utilize technology to help leverage that ? So I think building those partnerships with educational institutions are great , as well as those partnerships with our third-party vendors that we 've done as well , because they 're doing some of the same things with getting students , and as well as keeping up with the trends , keeping up their skillsets and capabilities and being able to have that flex staff to flex up and down as necessary . As you go through the ebb and flows of your journey of transformation , things start , stop , and/or increase , and sometimes you need to move at pace , or just the complexities that comes with marrying things with moving off of your legacy estate and still trying to keep that BAU [ business as usual ] and no downtime for your business . So therefore , all of those things , you can not know within one organization . You have to look to research and development . And again , the partnerships with the universities , the partnerships with third-party vendors are absolutely critical and key . Laurel : You 've mentioned sustainability a couple times . So how moving to the cloud and adopting these kinds of emerging technologies actually help BP as a company address the sustainability goals that it may have ? Keisha : We have the reduced environmental impact paired with efficient resource utilization . So , moving to the cloud allowed us to reduce our carbon footprint by transitioning from on-premise data centers to a more energy efficient cloud infrastructure . We are dual cloud company . We use both AWS and Microsoft Azure . And so definitely working with both of them for what they 're doing around the energy efficient cloud infrastructure that they 're pushing and doing , and working with them on all hands of how to measure that . Also what 's the projection of what we 're going to contribute as we continue to move forward and get to nearing the end of our cloud journey . Also enabling us to optimize our energy consumption , like I said , by scaling resources up and down based on demand , driving efficient energy usage , reducing waste , and contributing to our wider BP sustainability goals as well . There was a time , again , when we were on-prem and we would have large amounts of servers running . And some of those servers were literally less than 50 % utilized . But yet they 're still on . They 're still utilizing energy as well . So this moving to cloud allows us , again , from the optimization perspective of what we consume . Also , low carbon emissions , data-driven sustainability , and enhanced operational efficiency . Moving to the cloud supports and drives our low carbon emission by enabling our company to utilize renewable power sources , so by adopting emerging technologies , such as AI and machine learning . The transformation to cloud allows for us to analyze vast amounts of data , driving our innovation and decision-making power for BP 's sustainable initiatives . And again , this has been huge for us . And being data-driven helps identify opportunities for resource optimization , emissions reduction , as well as environmental impact mitigation . So in the data space , large opportunities there . And then also , there 's just a continuous improvement in innovation , and having or utilizing cloud platforms provides the necessary computational power and tools to implement advanced analytics , predictive modeling , as well as simulation techniques , which also enables us to continuously improve our sustainability performance . And it also allows for new solutions to be provided , as well as to contribute to all of the industry-wide sustainability advancements , things that when I get around other CIO tables or other tables with other people that are leading their transformations , we share ideas , we talk about the things that we 're doing , how we 're measuring that . And sharing that across the table is really good because , again , you get to also hear some of the things that they 're doing , which gives you some of the ideas of how they 're using technology to continue with the sustainability goals . So from that standpoint , that 's how we 've helped to leverage our cloud transformation to help with our sustainability aspirations of getting to net zero . Laurel : Yeah , that 's quite significant . You 've outlined major cloud trends like going cloud native that you 're seeing today . So , what are some of the cloud-enabled technologies or use cases that you 're really excited to see emerge in the next three to five years ? Keisha : So , I would say that I 'm really excited about ... there 's quite a few , so I 'll try and limit it . So edge AI . I think that the convergence of edge computing and AI presents an exciting opportunity for the real-time data , a real-time low latency processing and decision making at the network edge , which is extremely critical for us , given all of the platforms , rigs , that we have out across the globe . That is absolutely key . And I 'm excited about that because this technology helps to enable and develop our innovative applications in our industry to optimize the energy consumption of smart grids and enhance predictive maintenance and our operations . So for me , edge AI is really one that I 'm excited about . Also , quantum computing . It has the potential to solve complex problems and perform computations that are currently infeasible for classical computers . So in the next three to five years , I 'd expect to see significant progress in quantum computing technology , which has the potential to revolutionize the way we approach computational challenges and drive innovation across multiple sectors of our business , but multiple sectors of the energy industry in general . And then I 'll probably think of a couple more . I would say things have progressed quite far along in this space , but IoT and integration and analytics in that space , the proliferation of IoT devices continues to generate massive volumes of data . So cloud platforms will play a crucial role in processing , analyzing , and extracting meaningful insights from this data . In the next few years , and currently even today , I think we see further advancements in cloud based IoT integration analytics , as well as enabling US or other organizations to harness the full potential of IoT data , which will drive smarter decision making and predictive maintenance , as well as asset optimization and automation , or optimization rather . So I just think from an IoT perspective , again , big driver . We 've been doing digital twins , we 've been doing quite a few things within our platforms and just within our production business . Those are some of the three that really excite me . And then of course , there 's augmented reality , but I wo n't go into that . But there 's a few things that are coming along that really excite us and will be driving our business forward . Laurel : Fantastic . Keisha , thank you so much for joining us today on the Business Lab . Keisha : Thank you for having me . Laurel : That was Keisha Garcia , the vice president of Digital Foundations at BP , who I spoke with from Cambridge , Massachusetts , the home of MIT and MIT Technology Review , overlooking the Charles River . That 's it for this episode of Business Lab . I 'm your host , Laurel Ruma . I 'm the director of Insights , the custom publishing division of MIT Technology Review . We were founded in 1899 at the Massachusetts Institute of Technology . And you can also find us in print on the web and at events each year around the world . For more information about us and the show , please check out our website at technologyreview.com . This show is available wherever you get your podcasts . If you enjoyed this episode , we hope you 'll take a moment to rate and review us . Business Lab is a production of MIT Technology Review . This episode was produced by Giro Studios . Thanks for listening . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['partnership', 'cobalt', 'take', 'cloudfirst', 'approach', 'commit', 'build', 'new', 'hardware', 'system', 'build', 'cloud', 'year', 'prior', 'application', 'live', 'cloud', 'close', 'application', 'migrate', 'cloud', 'environment', 'change', 'product', 'service', 'integration', 'overall', 'digital', 'operating', 'model', 'cloud', 'transformation', 'help', 'enterprise', 'improve', 'operational', 'resiliency', 'accelerate', 'technology', 'adoption', 'reduce', 'datum', 'center', 'carbon', 'emission', 'say', 'vice', 'president', 'digital', 'foundation', 'migrate', 'cloud', 'scale', 'however', 'challenging', 'extensive', 'especially', 'deal', 'legacy', 'estate', 'utilize', 'cloud', 'platform', 'provide', 'necessary', 'computational', 'power', 'tool', 'implement', 'advanced', 'analytic', 'predictive', 'modeling', 'well', 'simulation', 'technique', 'also', 'enable', 'continuously', 'improve', 'sustainability', 'performance', 'say', 'successfully', 'migrate', 'cloud', 'subsequently', 'collaborate', 'deploy', 'cloud', 'technology', 'stress', 'importance', 'clear', 'communication', 'employee', 'well', 'stakeholder', 'involve', 'application', 'team', 'service', 'owner', 'end', 'user', 'early', 'development', 'delivery', 'strategy', 'bring', 'journey', 'overstate', 'important', 'say', 'hybrid', 'approach', 'transformation', 'combine', 'cloud', 'migration', 'retention', 'application', 'dedicated', 'data', 'center', 'intermediary', 'migration', 'environment', 'ensure', 'cost', 'effective', 'secure', 'operation', 'enterprisewide', 'communication', 'underpin', 'successful', 'transformation', 'outline', 'strong', 'flexible', 'governance', 'framework', 'collaborate', 'external', 'digital', 'partner', 'adapt', 'agile', 'way', 'work', 'good', 'practice', 'complex', 'cloud', 'migration', 'look', 'cloudenable', 'future', 'identify', 'convergence', 'ai', 'edge', 'compute', 'mount', 'progress', 'quantum', 'computing', 'proliferation', 'iot', 'connect', 'device', 'transformative', 'technology', 'drive', 'forward', 'well', 'business', 'outcome', 'think', 'convergence', 'edge', 'computing', 'present', 'exciting', 'opportunity', 'realtime', 'datum', 'realtime', 'low', 'latency', 'processing', 'decision', 'making', 'network', 'edge', 'extremely', 'critical', 'give', 'platform', 'rig', 'globe', 'say', 'episode', 'business', 'lab', 'produce', 'partnership', 'infosy', 'cobalt', 'mit', 'technology', 'review', 'business', 'lab', 'show', 'help', 'business', 'leader', 'make', 'sense', 'new', 'technology', 'come', 'lab', 'marketplaceour', 'topic', 'today', 'build', 'well', 'cloud', 'ecosystem', 'partner', 'internal', 'stakeholder', 'enterprise', 'meet', 'challenge', 'deploy', 'innovate', 'computing', 'solution', 'key', 'work', 'team', 'build', 'talent', 'resource', 'confidently', 'adopt', 'emerge', 'technologiestwo', 'word', 'optimize', 'guest', 'vice', 'president', 'digital', 'foundation', 'episode', 'business', 'lab', 'sponsor', 'thank', 'happy', 'laurel', 'well', 'let', 'start', 'move', 'cloud', 'perspective', 'major', 'benefit', 'challenge', 'cloud', 'journey', 'perspective', 'exciting', 'complex', 'learning', 'journey', 'long', 'pretty', 'long', 'journey', 'start', 'experiment', 'cloud', 'computing', 'email', 'service', 'learn', 'management', 'system', 'fastforward', 'application', 'cloud', 'company', 'conduct', 'proof', 'concept', 'determine', 'good', 'approach', 'scale', 'large', 'scale', 'adopt', 'cloudfirst', 'approach', 'mean', 'new', 'hardware', 'new', 'system', 'build', 'go', 'cloud', 'add', 'exist', 'time', 'mega', 'datum', 'center', 'different', 'datum', 'center', 'region', 'world', 'decide', 'long', 'go', 'add', 'cloud', 'onprem', 'exception', 'kind', 'motivate', 'push', 'along', 'get', 'journey', 'get', 'business', 'else', 'sell', 'business', 'sell', 'cloud', 'cloud', 'concept', 'thing', 'give', 'fact', 'lot', 'unknown', 'time', 'work', 'different', 'vendor', 'partner', 'try', 'find', 'knowledgeable', 'people', 'possible', 'feed', 'complexity', 'close', 'get', 'stride', 'pretty', 'well', 'large', 'part', 'state', 'state', 'exact', 'migrate', 'cloud', 'environment', 'enable', 'fast', 'product', 'service', 'introduction', 'change', 'digital', 'operating', 'model', 'move', 'productle', 'organization', 'cloud', 'migration', 'think', 'big', 'benefit', 'help', 'optimize', 'technology', 'stack', 'course', 'increase', 'operational', 'resilience', 'introduce', 'new', 'network', 'data', 'architecture', 'accelerate', 'technology', 'adoption', 'help', 'push', 'modernizing', 'state', 'keep', 'evergreen', 'also', 'assist', 'reduction', 'co2', 'emission', 'data', 'center', 'however', 'migrate', 'cloud', 'scale', 'large', 'landscape', 'challenge', 'complex', 'extensive', 'extensive', 'legacy', 'estate', 'say', 'early', 'host', 'large', 'mega', 'datum', 'center', 'well', 'also', 'myriad', 'datum', 'center', 'region', 'challenge', 'overstate', 'gain', 'great', 'laurel', 'great', 'look', 'past', 'journey', 'major', 'cloud', 'trend', 'see', 'today', 'major', 'trend', 'see', 'today', 'platform', 'perspective', 'see', 'increase', 'number', 'organization', 'look', 'consolidate', 'business', 'application', 'cloudbase', 'platform', 'cloud', 'native', 'robust', 'datum', 'analytic', 'platform', 'well', 'allow', 'realtime', 'ondemand', 'access', 'key', 'business', 'information', 'say', 'move', 'productle', 'organization', 'see', 'course', 'several', 'company', 'digital', 'team', 'align', 'productle', 'operating', 'model', 'ensure', 'customer', 'centricity', 'customer', 'focus', 'also', 'put', 'forefront', 'product', 'development', 'enable', 'business', 'businessle', 'prioritization', 'product', 'delivery', 'help', 'align', 'business', 'strategy', 'give', 'go', 'move', 'integrate', 'energy', 'company', 'transition', 'invent', 'huge', 'lot', 'market', 'tap', 'lot', 'thing', 'get', 'board', 'business', 'able', 'dynamic', 'able', 'shift', 'able', 'move', 'able', 'provide', 'fast', 'time', 'market', 'solution', 'standpoint', 'cloud', 'platform', 'technology', 'available', 'pace', 'align', 'business', 'awesome', 'see', 'lot', 'company', 'want', 'share', 'experience', 'knowledge', 'try', 'also', 'cloud', 'native', 'piece', 'cloud', 'native', 'enterprise', 'organization', 'align', 'business', 'technology', 'team', 'help', 'modernize', 'estate', 'build', 'cloud', 'native', 'capability', 'thing', 'plug', 'play', 'huge', 'build', 'lot', 'upgrading', 'thing', 'come', 'build', 'top', 'cloud', 'native', 'state', 'also', 'part', 'reinvention', 'journey', 'also', 'enable', 'climate', 'action', 'see', 'lot', 'folk', 'move', 'thing', 'align', 'agreement', 'well', 'thing', 'along', 'invent', 'decarbonize', 'digital', 'asset', 'directly', 'impact', 'global', 'energy', 'consumption', 'therefore', 'help', 'bit', 'help', 'therefore', 'thing', 'see', 'also', 'course', 'move', 'space', 'also', 'assist', 'get', 'net', 'also', 'able', 'connected', 'world', 'year', 'promise', 'opportunity', 'large', 'scale', 'industrial', 'g', 'broadband', 'base', 'usage', 'catapult', 'connection', 'remote', 'region', 'really', 'start', 'build', 'well', 'build', 'digital', 'twin', 'different', 'thing', 'refinery', 'also', 'rig', 'platform', 'capitalize', 'cloudbased', 'technology', 'thing', 'see', 'trend', 'thing', 'already', 'work', 'move', 'last', 'one', 'evolution', 'see', 'seem', 'go', 'away', 'see', 'lot', 'title', 'anymore', 'definitely', 'move', 'away', 'well', 'way', 'organization', 'structure', 'say', 'early', 'align', 'lot', 'product', 'lead', 'organization', 'make', 'sure', 'technology', 'leader', 'elevate', 'financial', 'acuman', 'business', 'prioritization', 'outcome', 'bring', 'business', 'value', 'find', 'value', 'stream', 'business', 'strategy', 'align', 'evaluate', 'bring', 'technology', 'catalyst', 'differentiator', 'business', 'definitely', 'laurel', 'bit', 'mention', 'little', 'bit', 'early', 'actually', 'bring', 'together', 'company', 'maintain', 'manage', 'optimize', 'business', 'practice', 'cloud', 'good', 'practice', 'company', 'think', 'order', 'collaborate', 'deploy', 'cloud', 'technology', 'think', 'big', 'thing', 'see', 'see', 'good', 'practice', 'lesson', 'learn', 'thing', 'provide', 'clear', 'stakeholder', 'communication', 'business', 'board', 'understand', 'go', 'hard', 'go', 'really', 'really', 'hard', 'mass', 'migration', 'adoption', 'cloud', 'way', 'hate', 'covid19', 'happen', 'definitely', 'force', 'business', 'really', 'see', 'benefit', 'pretty', 'much', 'want', 'say', 'cloud', 'time', 'probably', 'little', 'less', 'cloud', 'time', 'covid', 'hit', 'cloud', 'major', 'thing', 'business', 'really', 'skip', 'beat', 'really', 'able', 'connect', 'anywhere', 'world', 'see', 'benefit', 'see', 'thing', 'talk', 'clear', 'outline', 'communication', 'get', 'get', 'part', 'journey', 'express', 'important', 'involve', 'application', 'team', 'service', 'owner', 'end', 'user', 'early', 'development', 'delivery', 'strategy', 'bring', 'journey', 'overstate', 'important', 'modify', 'operating', 'model', 'digital', 'operating', 'model', 'specifically', 'align', 'work', 'seamlessly', 'together', 'different', 'area', 'allow', 'breakdown', 'expertise', 'particular', 'area', 'focus', 'expertise', 'continue', 'develop', 'evolve', 'technology', 'always', 'definitely', 'space', 'change', 'extremely', 'quickly', 'therefore', 'get', 'ensure', 'people', 'get', 'educate', 'update', 'skill', 'set', 'possible', 'build', 'benefit', 'realization', 'plan', 'also', 'key', 'soft', 'one', 'think', 'people', 'overlook', 'one', 'hybrid', 'approach', 'hybrid', 'approach', 'transformation', 'recognize', 'early', 'need', 'hybrid', 'approach', 'combine', 'cloud', 'migration', 'retention', 'certain', 'application', 'dedicated', 'data', 'center', 'intermediary', 'migration', 'environment', 'allow', 'cost', 'effective', 'secure', 'operation', 'good', 'practice', 'far', 'go', 'transform', 'look', 'think', 'size', 'fit', 'able', 'assess', 'estate', 'good', 'large', 'service', 'life', 'estate', 'legacy', 'estate', 'legacy', 'operating', 'model', 'code', 'base', 'application', 'entire', 'landscape', 'huge', 'think', 'first', 'start', 'journey', 'good', 'amount', 'estate', 'end', 'service', 'life', 'thing', 'mention', 'instead', 'try', 'tackle', 'separately', 'decide', 'good', 'way', 'leverage', 'bring', 'together', 'flexible', 'look', 'art', 'possible', 'state', 'address', 'multiple', 'thing', 'also', 'really', 'great', 'way', 'look', 'well', 'probably', 'know', 'experience', 'time', 'say', 'go', 'go', 'back', 'time', 'first', 'tactical', 'thing', 'stay', 'way', 'forever', 'good', 'thing', 'take', 'good', 'practice', 'open', 'box', 'let', 'open', 'figure', 'much', 'transformation', 'time', 'keep', 'ease', 'also', 'cover', 'much', 'modernization', 'hand', 'back', 'op', 'team', 'think', 'already', 'touch', 'good', 'practice', 'thing', 'soft', 'mention', 'early', 'governance', 'framework', 'delivery', 'model', 'restructure', 'effectiveness', 'get', 'thing', 'approve', 'establish', 'thing', 'upfront', 'delivery', 'model', 'ensure', 'smooth', 'cloud', 'migration', 'also', 'ensure', 'business', 'service', 'continuity', 'accommodate', 'evolve', 'business', 'requirement', 'know', 'trend', 'talk', 'mention', 'early', 'trend', 'thing', 'start', 'go', 'implement', 'thing', 'business', 'change', 'enormous', 'able', 'flexible', 'accommodate', 'bear', 'need', 'approve', 'make', 'decision', 'establish', 'thing', 'upfront', 'good', 'governance', 'framework', 'delivery', 'model', 'allow', 'flexibility', 'effectiveness', 'also', 'key', 'golden', 'collaboration', 'digital', 'delivery', 'partner', 'express', 'enough', 'find', 'great', 'delivery', 'partner', 'way', 'knowledge', 'know', 'organization', 'say', 'give', 'pace', 'technology', 'change', 'thing', 'roll', 'always', 'need', 'people', 'also', 'keep', 'finger', 'pulse', 'learning', 'different', 'experience', 'get', 'sometimes', 'also', 'work', 'externally', 'external', 'partner', 'couple', 'actually', 'prove', 'great', 'partner', 'learn', 'together', 'several', 'emphasis', 'major', 'partner', 'vendor', 'partner', 'supplement', 'inhouse', 'capability', 'great', 'adapt', 'modern', 'way', 'work', 'agility', 'agility', 'simple', 'form', 'also', 'agile', 'utilize', 'agile', 'practice', 'help', 'move', 'much', 'fast', 'set', 'squad', 'type', 'thing', 'course', 'say', 'last', 'communication', 'communication', 'communication', 'training', 'sustainability', 'continue', 'build', 'knowledge', 'base', 'able', 'continue', 'support', 'platform', 'new', 'technology', 'come', 'board', 'thing', 'see', 'lesson', 'learn', 'good', 'practice', 'general', 'laurel', 'mention', 'little', 'bit', 'early', 'critical', 'talent', 'kind', 'cloud', 'transformation', 'technique', 'communication', 'clearly', 'recruit', 'refining', 'talent', 'adopt', 'cloud', 'technology', 'give', 'fact', 'talk', 'many', 'people', 'go', 'journey', 'beginning', 'middle', 'almost', 'near', 'end', 'market', 'extremely', 'competitive', 'get', 'great', 'talent', 'also', 'upscale', 'talent', 'door', 'already', 'exist', 'staff', 'also', 'critical', 'offer', 'competitive', 'compensation', 'package', 'well', 'provide', 'training', 'certification', 'opportunity', 'keep', 'employee', 'motivate', 'keep', 'focus', 'percent', 'passionate', 'also', 'recognize', 'people', 'enjoy', 'compensation', 'package', 'great', 'also', 'learn', 'opportunity', 'promote', 'learning', 'culture', 'people', 'look', 'see', 'people', 'move', 'place', 'place', 'order', 'retain', 'great', 'talent', 'order', 'attract', 'great', 'talent', 'offering', 'need', 'important', 'important', 'success', 'transformation', 'program', 'reason', 'touch', 'early', 'pace', 'technology', 'move', 'well', 'fact', 'thing', 'test', 'water', 'also', 'create', 'sustainable', 'environment', 'also', 'create', 'able', 'get', 'market', 'fast', 'create', 'different', 'trend', 'happen', 'people', 'work', 'different', 'space', 'place', 'globe', 'thing', 'offering', 'attract', 'talent', 'also', 'build', 'diverse', 'inclusive', 'workforce', 'order', 'offering', 'board', 'people', 'want', 'work', 'home', 'people', 'want', 'work', 'office', 'building', 'people', 'different', 'thing', 'different', 'stage', 'point', 'life', 'flexibility', 'offer', 'employee', 'retain', 'great', 'talent', 'absolutely', 'key', 'critical', 'purpose', 'success', 'transformation', 'laurel', 'mention', 'importance', 'work', 'partner', 'especially', 'try', 'build', 'collaborative', 'ecosystem', 'work', 'partner', 'largescale', 'know', 'always', 'go', 'get', 'thing', 'extension', 'say', 'extension', 'additional', 'brain', 'power', 'extension', 'learning', 'thing', 'leverage', 'partnership', 'educational', 'institution', 'collaborate', 'university', 'college', 'establish', 'internship', 'coop', 'program', 'recruitment', 'pipeline', 'cloud', 'relate', 'role', 'see', 'university', 'technology', 'key', 'learn', 'new', 'thing', 'student', 'come', 'university', 'conscious', 'thing', 'go', 'environment', 'want', 'work', 'people', 'move', 'make', 'environment', 'well', 'sustainability', 'low', 'carbon', 'initiative', 'go', 'get', 'net', 'believe', 'thing', 'know', 'soon', 'go', 'recruit', 'university', 'first', 'thing', 'ask', 'really', 'go', 'invent', 'see', 'utilize', 'technology', 'help', 'leverage', 'think', 'build', 'partnership', 'educational', 'institution', 'great', 'well', 'partnership', 'thirdparty', 'vendor', 'well', 'thing', 'get', 'student', 'well', 'keep', 'trend', 'keep', 'skillset', 'capability', 'able', 'flex', 'staff', 'flex', 'necessary', 'go', 'ebb', 'flow', 'journey', 'transformation', 'thing', 'start', 'stop', 'andor', 'increase', 'sometimes', 'need', 'move', 'pace', 'complexity', 'come', 'marry', 'thing', 'move', 'legacy', 'estate', 'still', 'try', 'keep', 'bau', 'business', 'usual', 'downtime', 'business', 'therefore', 'thing', 'know', 'organization', 'look', 'research', 'development', 'partnership', 'university', 'partnership', 'vendor', 'absolutely', 'critical', 'key', 'laurel', 'mention', 'sustainability', 'couple', 'time', 'move', 'cloud', 'adopt', 'kind', 'emerge', 'technology', 'actually', 'help', 'company', 'address', 'sustainability', 'goal', 'reduce', 'environmental', 'impact', 'pair', 'efficient', 'resource', 'utilization', 'move', 'cloud', 'allow', 'reduce', 'carbon', 'footprint', 'transition', 'onpremise', 'datum', 'center', 'energy', 'efficient', 'cloud', 'infrastructure', 'dual', 'cloud', 'company', 'use', 'aw', 'azure', 'definitely', 'work', 'energy', 'efficient', 'cloud', 'infrastructure', 'push', 'work', 'hand', 'measure', 'also', 'projection', 'go', 'contribute', 'continue', 'move', 'forward', 'get', 'near', 'end', 'cloud', 'journey', 'also', 'enable', 'optimize', 'energy', 'consumption', 'say', 'scale', 'resource', 'base', 'demand', 'drive', 'efficient', 'energy', 'usage', 'reduce', 'waste', 'contribute', 'wide', 'sustainability', 'goal', 'well', 'time', 'onprem', 'large', 'amount', 'server', 'run', 'server', 'literally', 'less', 'utilize', 'yet', 'still', 'still', 'utilize', 'energy', 'well', 'move', 'cloud', 'allow', 'optimization', 'perspective', 'consume', 'also', 'low', 'carbon', 'emission', 'datadriven', 'sustainability', 'enhance', 'operational', 'efficiency', 'move', 'cloud', 'support', 'drive', 'low', 'carbon', 'emission', 'enable', 'company', 'utilize', 'renewable', 'power', 'source', 'adopt', 'emerge', 'technology', 'ai', 'machine', 'learn', 'transformation', 'cloud', 'allow', 'analyze', 'vast', 'amount', 'datum', 'drive', 'innovation', 'decisionmake', 'power', 'sustainable', 'initiative', 'huge', 'datadriven', 'help', 'identify', 'opportunity', 'resource', 'optimization', 'emission', 'reduction', 'well', 'environmental', 'impact', 'mitigation', 'data', 'space', 'large', 'opportunity', 'also', 'continuous', 'improvement', 'innovation', 'utilize', 'cloud', 'platform', 'provide', 'necessary', 'computational', 'power', 'tool', 'implement', 'advanced', 'analytic', 'predictive', 'modeling', 'well', 'simulation', 'technique', 'also', 'enable', 'continuously', 'improve', 'sustainability', 'performance', 'also', 'allow', 'new', 'solution', 'provide', 'well', 'contribute', 'industrywide', 'sustainability', 'advancement', 'thing', 'get', 'cio', 'table', 'table', 'people', 'lead', 'transformation', 'share', 'idea', 'talk', 'thing', 'measure', 'share', 'table', 'really', 'good', 'get', 'also', 'hear', 'thing', 'give', 'idea', 'use', 'technology', 'continue', 'sustainability', 'goal', 'standpoint', 'help', 'leverage', 'cloud', 'transformation', 'help', 'sustainability', 'aspiration', 'get', 'net', 'laurel', 'quite', 'significant', 'outline', 'major', 'cloud', 'trend', 'go', 'cloud', 'native', 'see', 'today', 'cloudenable', 'technology', 'use', 'case', 'really', 'excited', 'see', 'emerge', 'next', 'year', 'say', 'really', 'excited', 'try', 'limit', 'edge', 'ai', 'think', 'convergence', 'edge', 'computing', 'present', 'exciting', 'opportunity', 'realtime', 'datum', 'realtime', 'low', 'latency', 'processing', 'decision', 'making', 'network', 'edge', 'extremely', 'critical', 'give', 'platform', 'rig', 'globe', 'absolutely', 'key', 'excited', 'technology', 'help', 'enable', 'develop', 'innovative', 'application', 'industry', 'optimize', 'energy', 'consumption', 'smart', 'grid', 'enhance', 'predictive', 'maintenance', 'operation', 'edge', 'ai', 'really', 'excited', 'also', 'computing', 'potential', 'solve', 'complex', 'problem', 'perform', 'computation', 'currently', 'infeasible', 'classical', 'computer', 'next', 'year', 'expect', 'see', 'significant', 'progress', 'quantum', 'computing', 'technology', 'potential', 'revolutionize', 'way', 'approach', 'computational', 'challenge', 'drive', 'innovation', 'multiple', 'sector', 'business', 'multiple', 'sector', 'energy', 'industry', 'general', 'probably', 'think', 'couple', 'say', 'thing', 'progress', 'quite', 'far', 'along', 'space', 'iot', 'integration', 'analytic', 'space', 'proliferation', 'device', 'continue', 'generate', 'massive', 'volume', 'datum', 'cloud', 'platform', 'play', 'crucial', 'role', 'processing', 'analyze', 'extract', 'meaningful', 'insight', 'datum', 'next', 'year', 'currently', 'even', 'today', 'think', 'see', 'advancement', 'base', 'integration', 'analytic', 'well', 'enable', 'organization', 'harness', 'full', 'potential', 'iot', 'datum', 'drive', 'smart', 'decision', 'making', 'predictive', 'maintenance', 'well', 'asset', 'optimization', 'automation', 'optimization', 'rather', 'think', 'perspective', 'big', 'driver', 'digital', 'twin', 'thing', 'platform', 'production', 'business', 'really', 'excite', 'course', 'augment', 'reality', 'go', 'thing', 'come', 'really', 'excite', 'drive', 'business', 'forward', 'laurel', 'thank', 'much', 'join', 'today', 'business', 'lab', 'thank', 'laurel', 'vice', 'president', 'digital', 'foundation', 'speak', 'home', 'mit', 'mit', 'technology', 'review', 'overlook', 'episode', 'business', 'lab', 'host', 'director', 'insight', 'custom', 'publishing', 'division', 'mit', 'technology', 'review', 'found', 'also', 'find', 'print', 'web', 'event', 'year', 'world', 'information', 'show', 'check', 'website', 'show', 'available', 'get', 'podcast', 'enjoy', 'episode', 'hope', 'take', 'moment', 'rate', 'review', 'business', 'lab', 'production', 'mit', 'technology', 'review', 'episode', 'produce', 'studio', 'thank', 'listen', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","In 2017, BP took on a cloud-first approach that committed to building any new hardware or system builds on the cloud. Just a year prior, only 2% of BP applications lived on the cloud. At the close of 2022, 90% of BP applications had migrated to cloud environments, changing product and service integration and BP’s…"
How software that tracks covid variants could protect us against future outbreaks,https://www.technologyreview.com/2023/09/12/1078357/yatish-turakhia-covid-variants-tracing/,2023-09-12,"Yatish Turakhia is one of MIT Technology Review’s 2023 Innovators Under 35.  When covid-19 started spreading in early 2020, scientists quickly realized that tracking how the virus was mutating would be essential for public health as new strains emerged that put people at greater risk. Yatish Turakhia, then a postdoc at UC Santa Cruz’s Genomics Institute, helped develop a software tool called UShER to track these covid variants by placing them, within minutes of each new sample’s submission, on a family tree of all known SARS-CoV-2 genomes.  The tool, which has been accessible online since 2021, now records more than 15 million viral sequences, and scientists add to it daily. It helps them and public health officials discover new strains, assign them names, and track their evolution. It also allows them to surveil the virus in real time on a global scale with a high degree of precision. More recently, the team built another software tool, called RIPPLES, which examines UShER’s extensive family tree structure and investigates whether specific “branches” of variants may be recombinants—genetically distinct hybrid variants. A recombinant could, for example, take one part of its genome from the delta variant and another part from omicron. Because they essentially have two “parents,” recombinants are both rarer and tougher to identify.  Before the development of RIPPLES, scientists’ only method of identifying potential recombinants was by remembering mutations they’d spotted in other variants. RIPPLES automates that process, allowing health experts to reconstruct the virus’s evolutionary history. It also helps them work out whether a previously unseen sequence is a truly independent mutation or a combination of existing variants.  Tips for aspiring innovators on trying, failing, and the future of AI. “Our global understanding of how covid spreads would have been severely compromised without Yatish’s work,” says David Haussler, scientific director of the UCSC Genomics Institute, who worked with Turakhia on the project. “The product of his algorithm, which nobody else could make, is a global picture of how the virus spread in full genetic detail around the entire globe.” Since the tool’s release in 2022, RIPPLES has helped reveal hundreds of new SARS-CoV-2 recombinants. “I just started working on it during the pandemic solely because I wanted to be useful,” says Turakhia, 31. “Now, when we get a new sequence, people have already used UShER to train models that can predict whether that new variant will be more transmissible and more immune-invasive than omicron or not, just based on this big data that is available.” While both tools were born from the need to track covid, they could also help scientists handle outbreaks of other pathogens. Turakhia and his team are already using it to track respiratory syncytial virus, also known as RSV, and monkeypox. Soon they plan to add tuberculosis and flu. Both infections are tough to track. Their strains are more genetically diverse than those of most diseases, and tuberculosis also has a much larger genome because it’s a bacterium. But the team is already seeing promising results, says Turakhia. “In the future, we are going to create tools which are basically generalized to any pathogen out there—that’s basically our goal,” he adds. “We want to manage the pathogens better, and to develop vaccines that are more reactive to how the pathogens are evolving, and thereby save lives.”  Yatish Turakhia is one of MIT Technology Review’s 2023 Innovators Under 35. Meet the rest of this year’s honorees. ","Yatish Turakhia is one of MIT Technology Review ’ s 2023 Innovators Under 35 . When covid-19 started spreading in early 2020 , scientists quickly realized that tracking how the virus was mutating would be essential for public health as new strains emerged that put people at greater risk . Yatish Turakhia , then a postdoc at UC Santa Cruz ’ s Genomics Institute , helped develop a software tool called UShER to track these covid variants by placing them , within minutes of each new sample ’ s submission , on a family tree of all known SARS-CoV-2 genomes . The tool , which has been accessible online since 2021 , now records more than 15 million viral sequences , and scientists add to it daily . It helps them and public health officials discover new strains , assign them names , and track their evolution . It also allows them to surveil the virus in real time on a global scale with a high degree of precision . More recently , the team built another software tool , called RIPPLES , which examines UShER ’ s extensive family tree structure and investigates whether specific “ branches ” of variants may be recombinants—genetically distinct hybrid variants . A recombinant could , for example , take one part of its genome from the delta variant and another part from omicron . Because they essentially have two “ parents , ” recombinants are both rarer and tougher to identify . Before the development of RIPPLES , scientists ’ only method of identifying potential recombinants was by remembering mutations they ’ d spotted in other variants . RIPPLES automates that process , allowing health experts to reconstruct the virus ’ s evolutionary history . It also helps them work out whether a previously unseen sequence is a truly independent mutation or a combination of existing variants . Tips for aspiring innovators on trying , failing , and the future of AI . “ Our global understanding of how covid spreads would have been severely compromised without Yatish ’ s work , ” says David Haussler , scientific director of the UCSC Genomics Institute , who worked with Turakhia on the project . “ The product of his algorithm , which nobody else could make , is a global picture of how the virus spread in full genetic detail around the entire globe. ” Since the tool ’ s release in 2022 , RIPPLES has helped reveal hundreds of new SARS-CoV-2 recombinants . “ I just started working on it during the pandemic solely because I wanted to be useful , ” says Turakhia , 31 . “ Now , when we get a new sequence , people have already used UShER to train models that can predict whether that new variant will be more transmissible and more immune-invasive than omicron or not , just based on this big data that is available. ” While both tools were born from the need to track covid , they could also help scientists handle outbreaks of other pathogens . Turakhia and his team are already using it to track respiratory syncytial virus , also known as RSV , and monkeypox . Soon they plan to add tuberculosis and flu . Both infections are tough to track . Their strains are more genetically diverse than those of most diseases , and tuberculosis also has a much larger genome because it ’ s a bacterium . But the team is already seeing promising results , says Turakhia . “ In the future , we are going to create tools which are basically generalized to any pathogen out there—that ’ s basically our goal , ” he adds . “ We want to manage the pathogens better , and to develop vaccines that are more reactive to how the pathogens are evolving , and thereby save lives. ” Yatish Turakhia is one of MIT Technology Review ’ s 2023 Innovators Under 35 . Meet the rest of this year ’ s honorees .","['mit', 'technology', 'review', 'innovator', 'covid19', 'start', 'spread', 'early', 'scientist', 'quickly', 'realize', 'track', 'virus', 'mutate', 'essential', 'public', 'health', 'new', 'strain', 'emerge', 'put', 'people', 'great', 'risk', 'institute', 'help', 'develop', 'software', 'tool', 'call', 'usher', 'track', 'covid', 'variant', 'place', 'minute', 'new', 'sample', 'submission', 'family', 'tree', 'know', 'sarscov2', 'genome', 'tool', 'accessible', 'online', 'record', 'viral', 'sequence', 'scientist', 'add', 'daily', 'help', 'public', 'health', 'official', 'discover', 'new', 'strain', 'assign', 'name', 'track', 'evolution', 'also', 'allow', 'surveil', 'virus', 'real', 'time', 'global', 'scale', 'high', 'degree', 'precision', 'recently', 'team', 'build', 'software', 'tool', 'call', 'ripple', 'examine', 'usher', 'extensive', 'family', 'tree', 'structure', 'investigate', 'specific', 'branch', 'variant', 'recombinant', 'genetically', 'distinct', 'hybrid', 'variant', 'recombinant', 'example', 'take', 'part', 'genome', 'delta', 'variant', 'part', 'essentially', 'parent', 'recombinant', 'rarer', 'tough', 'identify', 'development', 'ripple', 'scientist', 'method', 'identify', 'potential', 'recombinant', 'remember', 'mutation', 'spot', 'variant', 'ripple', 'automate', 'process', 'allow', 'health', 'expert', 'reconstruct', 'virus', 'evolutionary', 'history', 'also', 'help', 'work', 'previously', 'unseen', 'sequence', 'truly', 'independent', 'mutation', 'combination', 'exist', 'variant', 'tip', 'aspire', 'innovator', 'try', 'fail', 'future', 'ai', 'global', 'understanding', 'covid', 'spread', 'severely', 'compromise', 'work', 'say', 'scientific', 'director', 'work', 'turakhia', 'project', 'product', 'else', 'make', 'global', 'picture', 'virus', 'spread', 'full', 'genetic', 'detail', 'entire', 'globe', 'tool', 'release', 'ripple', 'help', 'reveal', 'hundred', 'new', 'sarscov2', 'recombinant', 'start', 'work', 'pandemic', 'solely', 'want', 'useful', 'say', 'get', 'new', 'sequence', 'people', 'already', 'use', 'usher', 'train', 'model', 'predict', 'new', 'variant', 'transmissible', 'immuneinvasive', 'base', 'big', 'datum', 'available', 'tool', 'bear', 'need', 'track', 'covid', 'also', 'help', 'scientist', 'handle', 'outbreak', 'pathogen', 'turakhia', 'team', 'already', 'use', 'track', 'respiratory', 'syncytial', 'virus', 'also', 'know', 'monkeypox', 'soon', 'plan', 'add', 'tuberculosis', 'flu', 'infection', 'tough', 'track', 'strain', 'genetically', 'diverse', 'disease', 'tuberculosis', 'also', 'much', 'large', 'genome', 'bacterium', 'team', 'already', 'see', 'promising', 'result', 'say', 'turakhia', 'future', 'go', 'create', 'tool', 'basically', 'generalize', 'pathogen', 'basically', 'goal', 'add', 'want', 'manage', 'pathogen', 'well', 'develop', 'vaccine', 'reactive', 'pathogen', 'evolve', 'thereby', 'save', 'life', 'turakhia', 'mit', 'technology', 'review', 'innovator', 'meet', 'rest', 'year', 'honoree']","<p>During the pandemic, Yatish Turakhia developed software tools to trace the evolution of new covid variants. Now he’s applying his techniques to other diseases.</p>
"
The beautiful complexity of the US radio spectrum,https://www.technologyreview.com/2023/08/23/1077686/radio-spectrum-visualized/,2023-08-23,"Somewhere above you right now, a plane is broadcasting its coordinates on 1090 megahertz. A satellite high above Earth is transmitting weather maps on 1694.1 MHz. On top of all that, every single phone and Wi-Fi router near you blasts internet traffic through the air over radio waves. A carefully regulated radio spectrum is what makes it possible for these signals to get to the right place intact. The Federal Communication Commis-sion and the National Telecommunications and Information Administration share  the task of managing radio frequencies for US airwaves. The NTIA manages all federal radio uses (including military use), while the FCC manages everything else. It is an incredibly complex system, and to help with the job of explaining the importance of managing this invisible natural resource, the NTIA  publishes the United States Frequency Allocation Chart (which you can order as a wall chart for $6). The US government lays claim to a large chunk of spectrum for military use, communications, and transportation. FM radio operates between 88 and 108.0 MHz, and AM radio operates between 540 and 1700 kilohertz. Using licenses, amateur radio operators are granted slices where they can communicate safely, as are businesses and other institutions. Civil aviation, maritime navigation, satellite communications, radio astronomy, cellular voice, and data all lay claim to colorful plots on this chart. The chart uses 33 color-coded categories to visualize the information in a crazy quilt of blocks (some wide, some narrow), spread from 9 kHz (very low frequency) all the way to 300 GHz (extremely high frequency). It does suffer from scale distortions, not unlike a map of Earth. Eric Rosenberg, a telecommunications specialist at NTIA, says a lot of the choices about what service goes where come down to physics and the environment where the service will be used: “You can’t just pick up a block and say, okay, we’re gonna move these radars over here.” The chart is always extremely popular, Rosenberg says; fans include lawmakers in Congress. Last updated in 2016, it is due for another revision. “We’re getting to the point where we really feel that we need to redo it,” he says. “Again, it’s a very large project.” ν A version of this story appeared on Beautiful Public Data (beautifulpublicdata.com), a newsletter by Jon Keegan (KE3GAN). ","Somewhere above you right now , a plane is broadcasting its coordinates on 1090 megahertz . A satellite high above Earth is transmitting weather maps on 1694.1 MHz . On top of all that , every single phone and Wi-Fi router near you blasts internet traffic through the air over radio waves . A carefully regulated radio spectrum is what makes it possible for these signals to get to the right place intact . The Federal Communication Commis-sion and the National Telecommunications and Information Administration share the task of managing radio frequencies for US airwaves . The NTIA manages all federal radio uses ( including military use ) , while the FCC manages everything else . It is an incredibly complex system , and to help with the job of explaining the importance of managing this invisible natural resource , the NTIA publishes the United States Frequency Allocation Chart ( which you can order as a wall chart for $ 6 ) . The US government lays claim to a large chunk of spectrum for military use , communications , and transportation . FM radio operates between 88 and 108.0 MHz , and AM radio operates between 540 and 1700 kilohertz . Using licenses , amateur radio operators are granted slices where they can communicate safely , as are businesses and other institutions . Civil aviation , maritime navigation , satellite communications , radio astronomy , cellular voice , and data all lay claim to colorful plots on this chart . The chart uses 33 color-coded categories to visualize the information in a crazy quilt of blocks ( some wide , some narrow ) , spread from 9 kHz ( very low frequency ) all the way to 300 GHz ( extremely high frequency ) . It does suffer from scale distortions , not unlike a map of Earth . Eric Rosenberg , a telecommunications specialist at NTIA , says a lot of the choices about what service goes where come down to physics and the environment where the service will be used : “ You can ’ t just pick up a block and say , okay , we ’ re gon na move these radars over here. ” The chart is always extremely popular , Rosenberg says ; fans include lawmakers in Congress . Last updated in 2016 , it is due for another revision . “ We ’ re getting to the point where we really feel that we need to redo it , ” he says . “ Again , it ’ s a very large project. ” ν A version of this story appeared on Beautiful Public Data ( beautifulpublicdata.com ) , a newsletter by Jon Keegan ( KE3GAN ) .","['somewhere', 'right', 'plane', 'broadcast', 'coordinate', 'megahertz', 'satellite', 'high', 'earth', 'transmit', 'weather', 'map', 'mhz', 'top', 'single', 'phone', 'wifi', 'router', 'blast', 'internet', 'traffic', 'air', 'radio', 'wave', 'carefully', 'regulate', 'radio', 'spectrum', 'make', 'possible', 'signal', 'get', 'right', 'place', 'intact', 'federal', 'communication', 'commission', 'national', 'information', 'administration', 'share', 'task', 'manage', 'radio', 'frequency', 'airwave', 'manage', 'federal', 'radio', 'use', 'include', 'military', 'use', 'manage', 'else', 'incredibly', 'complex', 'system', 'help', 'job', 'explain', 'importance', 'manage', 'invisible', 'natural', 'resource', 'publish', 'allocation', 'chart', 'order', 'wall', 'chart', 'government', 'lay', 'claim', 'large', 'chunk', 'spectrum', 'military', 'use', 'communication', 'transportation', 'radio', 'operate', 'mhz', 'radio', 'operate', 'kilohertz', 'use', 'license', 'amateur', 'radio', 'operator', 'grant', 'slice', 'communicate', 'safely', 'business', 'institution', 'civil', 'aviation', 'maritime', 'navigation', 'satellite', 'communication', 'radio', 'astronomy', 'cellular', 'voice', 'datum', 'lay', 'claim', 'colorful', 'plot', 'chart', 'chart', 'use', 'colorcoded', 'category', 'visualize', 'information', 'crazy', 'quilt', 'block', 'wide', 'narrow', 'spread', 'khz', 'low', 'frequency', 'way', 'ghz', 'extremely', 'high', 'frequency', 'suffer', 'scale', 'distortion', 'map', 'earth', 'telecommunications', 'specialist', 'say', 'lot', 'choice', 'service', 'go', 'come', 'physics', 'environment', 'service', 'use', 'pick', 'block', 'say', 'go', 'move', 'radar', 'chart', 'always', 'extremely', 'popular', 'say', 'fan', 'include', 'lawmaker', 'last', 'update', 'due', 'revision', 'get', 'point', 'really', 'feel', 'need', 'redo', 'say', 'large', 'project', 'version', 'story', 'appear', 'beautiful', 'public', 'data', 'newsletter']","<p>The United States Frequency Allocation Chart shows how the nation’s precious radio frequencies are carefully shared.</p>
"
"How culture drives foul play on the internet, and how new “upcode” can protect us",https://www.technologyreview.com/2023/08/23/1077693/crypto-foul-play/,2023-08-23,"The world of online misdeeds is an eerie biome, crawling with Bored Apes, Fancy Bears, Shiba Inu coins, self-­replicating viruses, and whales. But the behavior driving fraud, hacks, and scams on the internet has always been familiar and very human. New technologies change little about the fact that illegal operations exist because some people are willing to act illegally and others fall for the stories they tell.  The crypto industry is investing heavily in getting more people to buy in. That doesn't mean you have to. To wit: Crypto speculation looks a lot like online sports betting, which looks like offline sports betting; cyber hacking resembles classic espionage; spear phishers recall flesh-and-blood con artists. The perpetrators of these crimes lure victims with well-worn appeals to faith and promises of financial reward. In Fancy Bear Goes Phishing, Yale law professor Scott Shapiro argues that technological solutions can’t solve the problem because they can’t force people to play nice online. The best ways to protect ourselves from online tricks are social—public policies, legal and business incentives, and cultural shifts.  Shapiro’s book arrives just in time for the last gasp of the latest crypto wave, as major players find themselves trapped in the nets of human institutions. In early June, the US Securities and Exchange Commission went after Binance and Coinbase, the two largest cryptocurrency exchanges in the world, a few months after charging the infamous Sam Bankman-Fried, founder of the massive crypto exchange FTX, with fraud. While Shapiro mentions crypto only as the main means of payment in online crime, the industry’s wild ride through finance and culture deserves its own hefty chapter in the narrative of internet fraud.  It may be too early for deep analysis, but we do have first-person perspectives on crypto from actor Ben McKenzie (former star of the teen drama The O.C.) and streetwear designer and influencer Bobby Hundreds, the authors of—respectively—Easy Money and NFTs Are a Scam/NFTs Are the Future. (More heavily reported books on the crypto era from tech reporter Zeke Faux and Big Short author Michael Lewis are in the works.)  “If we are committing serious crimes like fraud, it is crucially important that we find ways to justify our behavior to others, and crucially, to ourselves.” McKenzie testified at the Senate Banking Committee’s hearing on FTX that he believes the cryptocurrency industry “represents the largest Ponzi scheme in history,” and Easy Money traces his own journey from bored pandemic dabbler to committed crypto critic alongside the industry’s rise and fall. Hundreds also writes a chronological account of his time in crypto—specifically in nonfungible tokens, or NFTs, digital representational objects that he has bought, sold, and “dropped” on his own and through The Hundreds, a “community-based streetwear brand and media company.” For Hundreds, NFTs have value as cultural artifacts, and he’s not convinced that their time should be over (although he acknowledges that between 2019 and the writing of his book, more than $100 million worth of NFTs have been stolen, mostly through phishing scams). “Whether or not NFTs are a scam poses a philosophical question that wanders into moral judgments and cultural practices around free enterprise, mercantilism, and materialism,” he writes.  For all their differences (a lawyer, an actor, and a designer walk into a bar …), Shapiro, McKenzie, and Hundreds all explore characters, motivations, and social dynamics much more than they do technical innovations. Online crime is a human story, these books collectively argue, and explanations of why it happens, why it works, and how we can stay safe are human too. To articulate how internet crime comes to be, Shapiro offers a new paradigm for the relationship between humanity and technology. He relabels technical computer code “downcode” and calls everything human surrounding and driving it “upcode.” From “the inner operations of the human brain” to “the outer social, political, and institutional forces that define the world,” upcode is the teeming ecosystem of humans and human systems behind the curtain of technology. Shapiro argues that upcode is responsible for all of technology’s impacts—positive and negative—and downcode is only its product. Technical tools like the blockchain, firewalls, or two-factor authentication may be implemented as efforts to ensure safety online, but they cannot address the root causes upstream. For any technologist or crypto enthusiast who believes computer code to be law and sees human error as an annoying hiccup, this idea may be disconcerting. But crime begins and ends with humans, Shapiro argues, so upcode is where we must focus both our blame for the problem and our efforts to improve online safety. McKenzie and Hundreds deal with crypto and NFTS almost entirely at the upcode level: neither has training in computer science, and both examine the industry through personal lenses. For McKenzie, it’s the financial realm, where friends encouraged him to invest in tokens to compensate for being out of work during the pandemic. For Hundreds, it’s the art world, which has historically been inaccessible to most and inhospitable for many—and is what led him to gravitate toward streetwear as a creative outlet in the first place. Hundreds saw NFTs as a signal of a larger positive shift toward Web3, a nebulous vision of a more democratized form of the internet where creative individuals could get paid for their work and build communities of fans and artists without relying on tech companies. The appeal of Web3 and NFTs is based in cultural and economic realities; likewise, online scams happen because buggy upcode—like social injustice, runaway capitalism, and corporate monopolies—creates the conditions.   Constructing downcode guardrails to allow in only “good” intentions won’t solve online crime because bad acts are not so easily dismissed as the work of bad actors. The people who perpetrate scams, fraud, and hacks—or even participate in the systems around it, like speculative markets—often subscribe to a moral rubric as they act illegally. In Fancy Bear, Shapiro cites the seminal research of Sarah Gordon, the first to investigate the psychology of people who wrote computer viruses when this malware first popped up in the 1990s. Of the 64 respondents to her global survey, all but one had developmentally appropriate moral reasoning based on ethics, according to a framework created by the psychologist Lawrence Kohlberg: that is, these virus writers made decisions based on a sense of right and wrong. More recent research from Alice Hutchings, the director of the University of Cambridge’s Cybercrime Centre, also found hackers as a group to be “moral agents, possessing a sense of justice, purpose, and identity.” Many hackers find community in their work; others, like Edward Snowden, who leaked classified information from the US National Security Agency in 2013, cross legal boundaries for what they believe to be expressly moral reasons. Bitcoin, meanwhile, may be a frequent agent of crime but was in fact created to offer a “trustless” way to avoid relying on banks after the housing crisis and government bailouts of the 2000s left many wondering if traditional financial institutions could be trusted with consumer interests. The definition of crime is also upcode, shaped by social contracts as well as legal ones. In NFTs Are a Scam/NFTs Are the Future, Hundreds interviews the renowned tech investor and public speaker Gary Vaynerchuk, or “Gary Vee,” a figure he calls the “face of NFTs.” It was Vee’s “zeal and belief” that convinced Hundreds to create his own NFT collection, Adam Bomb Squad. Vee tells Hundreds that critics “may be right” when they call NFTs a scam. But while some projects may be opportunistic rackets, he hopes the work he makes is the variety that endures. Vee might be lying here, but at face value, he professes a belief in a greater good that he and everyone he recruits (including the thousands of attendees at his NFT convention) can help build—even if there’s harm along the way.  McKenzie spends much of two chapters in Easy Money describing his personal encounters with FTX’s Bankman-Fried, who was widely called the “King of Crypto” before his fall. Bankman-Fried professes to believe in crypto’s positive potential; indeed, he has claimed on the record many times that he wanted to do good with his work, despite knowing at points that it was potentially fraudulent. McKenzie struggles to understand this point of view. “If we are committing serious crimes like fraud,” he speculates, “it is crucially important that we find ways to justify our behavior to others and crucially, to ourselves.” While this rationalization certainly doesn’t excuse any crimes, it explains how people can perpetrate eye-boggling fraud again and again, even inventing new ways to scam. The human upcode that makes each of us see ourselves as the protagonist of our story is powerful, even and maybe especially when billions of dollars are at stake.  Technological innovation does not change our fundamental behavior as humans, but technology has brought speed and spread to the gambling table. A single perpetrator can reach more victims faster now that the global world is connected. Despite his research, McKenzie did gamble on crypto—he shorted tokens on a specific, and incorrect, timeline. He doesn’t disclose how much he lost, but it was an amount that “provokes an uncomfortable conversation with your spouse.” He’s hardly the only savvy individual in history to fall for a risky pitch; our brains make it painfully easy to get scammed, another reason why solutions that rely entirely on computer code don’t work. “The human mind is riddled with upcode that causes us to make biased predictions and irrational choices,” Shapiro writes. Take the “representativeness heuristic,” which leads us to judge something by how much it resembles an existing mental image—even if that may lead us to overlook crucial information. If an animal looks like a duck and quacks like a duck, the representativeness heuristic tells us it can swim. Phishing scams rely on this rush to pattern matching. For example, Fancy Bear, the titular Russian hacking group of Shapiro’s book, used a visually and tonally convincing message to attempt to hack into Hillary Clinton campaign staffers’ email accounts in 2016. It worked.  Also coming into play for scams, fraud, and hacks are the “availability heuristic,” which leads us to remember sensational events regardless of their frequency, and the “affect heuristic,” which leads us to emphasize our feelings about a decision over the facts, inflating “our expectations about outcomes we like”—such as winning a huge payout on a gamble. When Hundreds was concerned about whether NFTs were a good investment, he reached out to a friend whose belief was steadfast and found himself calmed. “It was that sense of conviction that separated the losers from the winners,” he writes, even when the facts might have supported stepping back.  The marketing pitch of communal faith and reward, the enticement to join a winning team, feeds a human social instinct—especially as more offline modes of connection are faltering. It’s telling that after the SEC brought charges against Coinbase, the company responded by issuing a pro-crypto NFT, imploring its community to offer support for the struggling industry by minting it. (Coinbase and the minting platform Zora promise to donate the mint fees they’ll receive from consumers to pro-crypto advocacy.) The crypto industry rose to power on this kind of faith-based relationship, and it continues to appeal to some: more than 135,000 of the Coinbase tokens have been minted since the SEC suit was announced. Beyond money, “we’re just as motivated by identity and community (or its upside-down cousin, tribalism),” writes Hundreds, “and the most fervent contemporary movements and trends masterfully meld them all together. The only thing that feels as good as getting rich is doing so by rallying around an impassioned cause with a band of like-minded friends.” Technological innovation does not change our fundamental behavior as humans, but technology has brought speed and spread to the gambling table. A single perpetrator can reach more victims faster now that the global world is connected. The risks are higher now, as clearly demonstrated by the headline-exploding results of the 2016 Clinton email hack, the billions lost by investors in the volatile crypto industry, and billions more lost through crypto hacks and scams. Shapiro argues that the efforts of the antivirus and antihacking industry to code guardrails into our online systems have failed. Fraud goes on. Instead, we must reexamine the upcode that has fostered and supported online crimes: “our settled moral and political convictions on what we owe one another and how we should respect security and privacy.” For Shapiro, effectively addressing online fraud, hacks, and scams requires political, economic, and social shifts such as creating incentives for businesses to protect customers and penalties for data breaches, supporting potential hackers in finding community outside of crime, and developing government and legal policies to prevent illicit payment through mechanisms like cryptocurrencies.  Shapiro admits that shifting upcode this way will likely take generations, but the work has already started. The SEC’s recent moves against crypto exchanges are promising steps, as are the FTC’s public warnings against scammy AI claims and generative AI fraud. Growing public awareness about the importance of data privacy and security will help too. But while some humans are working on evolving our social systems, others will continue to hunt online for other people’s money. In our lifetimes, fraud, hacks, and scams will likely always find a home on the internet. But being aware of the upcode all around us may help us find safer paths through the online jungle.  Rebecca Ackermann is a writer and artist in San Francisco. ","The world of online misdeeds is an eerie biome , crawling with Bored Apes , Fancy Bears , Shiba Inu coins , self-­replicating viruses , and whales . But the behavior driving fraud , hacks , and scams on the internet has always been familiar and very human . New technologies change little about the fact that illegal operations exist because some people are willing to act illegally and others fall for the stories they tell . The crypto industry is investing heavily in getting more people to buy in . That does n't mean you have to . To wit : Crypto speculation looks a lot like online sports betting , which looks like offline sports betting ; cyber hacking resembles classic espionage ; spear phishers recall flesh-and-blood con artists . The perpetrators of these crimes lure victims with well-worn appeals to faith and promises of financial reward . In Fancy Bear Goes Phishing , Yale law professor Scott Shapiro argues that technological solutions can ’ t solve the problem because they can ’ t force people to play nice online . The best ways to protect ourselves from online tricks are social—public policies , legal and business incentives , and cultural shifts . Shapiro ’ s book arrives just in time for the last gasp of the latest crypto wave , as major players find themselves trapped in the nets of human institutions . In early June , the US Securities and Exchange Commission went after Binance and Coinbase , the two largest cryptocurrency exchanges in the world , a few months after charging the infamous Sam Bankman-Fried , founder of the massive crypto exchange FTX , with fraud . While Shapiro mentions crypto only as the main means of payment in online crime , the industry ’ s wild ride through finance and culture deserves its own hefty chapter in the narrative of internet fraud . It may be too early for deep analysis , but we do have first-person perspectives on crypto from actor Ben McKenzie ( former star of the teen drama The O.C . ) and streetwear designer and influencer Bobby Hundreds , the authors of—respectively—Easy Money and NFTs Are a Scam/NFTs Are the Future . ( More heavily reported books on the crypto era from tech reporter Zeke Faux and Big Short author Michael Lewis are in the works . ) “ If we are committing serious crimes like fraud , it is crucially important that we find ways to justify our behavior to others , and crucially , to ourselves. ” McKenzie testified at the Senate Banking Committee ’ s hearing on FTX that he believes the cryptocurrency industry “ represents the largest Ponzi scheme in history , ” and Easy Money traces his own journey from bored pandemic dabbler to committed crypto critic alongside the industry ’ s rise and fall . Hundreds also writes a chronological account of his time in crypto—specifically in nonfungible tokens , or NFTs , digital representational objects that he has bought , sold , and “ dropped ” on his own and through The Hundreds , a “ community-based streetwear brand and media company. ” For Hundreds , NFTs have value as cultural artifacts , and he ’ s not convinced that their time should be over ( although he acknowledges that between 2019 and the writing of his book , more than $ 100 million worth of NFTs have been stolen , mostly through phishing scams ) . “ Whether or not NFTs are a scam poses a philosophical question that wanders into moral judgments and cultural practices around free enterprise , mercantilism , and materialism , ” he writes . For all their differences ( a lawyer , an actor , and a designer walk into a bar … ) , Shapiro , McKenzie , and Hundreds all explore characters , motivations , and social dynamics much more than they do technical innovations . Online crime is a human story , these books collectively argue , and explanations of why it happens , why it works , and how we can stay safe are human too . To articulate how internet crime comes to be , Shapiro offers a new paradigm for the relationship between humanity and technology . He relabels technical computer code “ downcode ” and calls everything human surrounding and driving it “ upcode. ” From “ the inner operations of the human brain ” to “ the outer social , political , and institutional forces that define the world , ” upcode is the teeming ecosystem of humans and human systems behind the curtain of technology . Shapiro argues that upcode is responsible for all of technology ’ s impacts—positive and negative—and downcode is only its product . Technical tools like the blockchain , firewalls , or two-factor authentication may be implemented as efforts to ensure safety online , but they can not address the root causes upstream . For any technologist or crypto enthusiast who believes computer code to be law and sees human error as an annoying hiccup , this idea may be disconcerting . But crime begins and ends with humans , Shapiro argues , so upcode is where we must focus both our blame for the problem and our efforts to improve online safety . McKenzie and Hundreds deal with crypto and NFTS almost entirely at the upcode level : neither has training in computer science , and both examine the industry through personal lenses . For McKenzie , it ’ s the financial realm , where friends encouraged him to invest in tokens to compensate for being out of work during the pandemic . For Hundreds , it ’ s the art world , which has historically been inaccessible to most and inhospitable for many—and is what led him to gravitate toward streetwear as a creative outlet in the first place . Hundreds saw NFTs as a signal of a larger positive shift toward Web3 , a nebulous vision of a more democratized form of the internet where creative individuals could get paid for their work and build communities of fans and artists without relying on tech companies . The appeal of Web3 and NFTs is based in cultural and economic realities ; likewise , online scams happen because buggy upcode—like social injustice , runaway capitalism , and corporate monopolies—creates the conditions . Constructing downcode guardrails to allow in only “ good ” intentions won ’ t solve online crime because bad acts are not so easily dismissed as the work of bad actors . The people who perpetrate scams , fraud , and hacks—or even participate in the systems around it , like speculative markets—often subscribe to a moral rubric as they act illegally . In Fancy Bear , Shapiro cites the seminal research of Sarah Gordon , the first to investigate the psychology of people who wrote computer viruses when this malware first popped up in the 1990s . Of the 64 respondents to her global survey , all but one had developmentally appropriate moral reasoning based on ethics , according to a framework created by the psychologist Lawrence Kohlberg : that is , these virus writers made decisions based on a sense of right and wrong . More recent research from Alice Hutchings , the director of the University of Cambridge ’ s Cybercrime Centre , also found hackers as a group to be “ moral agents , possessing a sense of justice , purpose , and identity. ” Many hackers find community in their work ; others , like Edward Snowden , who leaked classified information from the US National Security Agency in 2013 , cross legal boundaries for what they believe to be expressly moral reasons . Bitcoin , meanwhile , may be a frequent agent of crime but was in fact created to offer a “ trustless ” way to avoid relying on banks after the housing crisis and government bailouts of the 2000s left many wondering if traditional financial institutions could be trusted with consumer interests . The definition of crime is also upcode , shaped by social contracts as well as legal ones . In NFTs Are a Scam/NFTs Are the Future , Hundreds interviews the renowned tech investor and public speaker Gary Vaynerchuk , or “ Gary Vee , ” a figure he calls the “ face of NFTs. ” It was Vee ’ s “ zeal and belief ” that convinced Hundreds to create his own NFT collection , Adam Bomb Squad . Vee tells Hundreds that critics “ may be right ” when they call NFTs a scam . But while some projects may be opportunistic rackets , he hopes the work he makes is the variety that endures . Vee might be lying here , but at face value , he professes a belief in a greater good that he and everyone he recruits ( including the thousands of attendees at his NFT convention ) can help build—even if there ’ s harm along the way . McKenzie spends much of two chapters in Easy Money describing his personal encounters with FTX ’ s Bankman-Fried , who was widely called the “ King of Crypto ” before his fall . Bankman-Fried professes to believe in crypto ’ s positive potential ; indeed , he has claimed on the record many times that he wanted to do good with his work , despite knowing at points that it was potentially fraudulent . McKenzie struggles to understand this point of view . “ If we are committing serious crimes like fraud , ” he speculates , “ it is crucially important that we find ways to justify our behavior to others and crucially , to ourselves. ” While this rationalization certainly doesn ’ t excuse any crimes , it explains how people can perpetrate eye-boggling fraud again and again , even inventing new ways to scam . The human upcode that makes each of us see ourselves as the protagonist of our story is powerful , even and maybe especially when billions of dollars are at stake . Technological innovation does not change our fundamental behavior as humans , but technology has brought speed and spread to the gambling table . A single perpetrator can reach more victims faster now that the global world is connected . Despite his research , McKenzie did gamble on crypto—he shorted tokens on a specific , and incorrect , timeline . He doesn ’ t disclose how much he lost , but it was an amount that “ provokes an uncomfortable conversation with your spouse. ” He ’ s hardly the only savvy individual in history to fall for a risky pitch ; our brains make it painfully easy to get scammed , another reason why solutions that rely entirely on computer code don ’ t work . “ The human mind is riddled with upcode that causes us to make biased predictions and irrational choices , ” Shapiro writes . Take the “ representativeness heuristic , ” which leads us to judge something by how much it resembles an existing mental image—even if that may lead us to overlook crucial information . If an animal looks like a duck and quacks like a duck , the representativeness heuristic tells us it can swim . Phishing scams rely on this rush to pattern matching . For example , Fancy Bear , the titular Russian hacking group of Shapiro ’ s book , used a visually and tonally convincing message to attempt to hack into Hillary Clinton campaign staffers ’ email accounts in 2016 . It worked . Also coming into play for scams , fraud , and hacks are the “ availability heuristic , ” which leads us to remember sensational events regardless of their frequency , and the “ affect heuristic , ” which leads us to emphasize our feelings about a decision over the facts , inflating “ our expectations about outcomes we like ” —such as winning a huge payout on a gamble . When Hundreds was concerned about whether NFTs were a good investment , he reached out to a friend whose belief was steadfast and found himself calmed . “ It was that sense of conviction that separated the losers from the winners , ” he writes , even when the facts might have supported stepping back . The marketing pitch of communal faith and reward , the enticement to join a winning team , feeds a human social instinct—especially as more offline modes of connection are faltering . It ’ s telling that after the SEC brought charges against Coinbase , the company responded by issuing a pro-crypto NFT , imploring its community to offer support for the struggling industry by minting it . ( Coinbase and the minting platform Zora promise to donate the mint fees they ’ ll receive from consumers to pro-crypto advocacy . ) The crypto industry rose to power on this kind of faith-based relationship , and it continues to appeal to some : more than 135,000 of the Coinbase tokens have been minted since the SEC suit was announced . Beyond money , “ we ’ re just as motivated by identity and community ( or its upside-down cousin , tribalism ) , ” writes Hundreds , “ and the most fervent contemporary movements and trends masterfully meld them all together . The only thing that feels as good as getting rich is doing so by rallying around an impassioned cause with a band of like-minded friends. ” Technological innovation does not change our fundamental behavior as humans , but technology has brought speed and spread to the gambling table . A single perpetrator can reach more victims faster now that the global world is connected . The risks are higher now , as clearly demonstrated by the headline-exploding results of the 2016 Clinton email hack , the billions lost by investors in the volatile crypto industry , and billions more lost through crypto hacks and scams . Shapiro argues that the efforts of the antivirus and antihacking industry to code guardrails into our online systems have failed . Fraud goes on . Instead , we must reexamine the upcode that has fostered and supported online crimes : “ our settled moral and political convictions on what we owe one another and how we should respect security and privacy. ” For Shapiro , effectively addressing online fraud , hacks , and scams requires political , economic , and social shifts such as creating incentives for businesses to protect customers and penalties for data breaches , supporting potential hackers in finding community outside of crime , and developing government and legal policies to prevent illicit payment through mechanisms like cryptocurrencies . Shapiro admits that shifting upcode this way will likely take generations , but the work has already started . The SEC ’ s recent moves against crypto exchanges are promising steps , as are the FTC ’ s public warnings against scammy AI claims and generative AI fraud . Growing public awareness about the importance of data privacy and security will help too . But while some humans are working on evolving our social systems , others will continue to hunt online for other people ’ s money . In our lifetimes , fraud , hacks , and scams will likely always find a home on the internet . But being aware of the upcode all around us may help us find safer paths through the online jungle . Rebecca Ackermann is a writer and artist in San Francisco .","['world', 'online', 'misdeed', 'eerie', 'biome', 'crawl', 'bored', 'ape', 'fancy', 'bear', 'inu', 'coin', 'self\xadreplicate', 'virus', 'whale', 'behavior', 'drive', 'fraud', 'hack', 'scam', 'internet', 'always', 'familiar', 'human', 'new', 'technology', 'change', 'little', 'fact', 'illegal', 'operation', 'exist', 'people', 'willing', 'act', 'illegally', 'fall', 'story', 'tell', 'crypto', 'industry', 'invest', 'heavily', 'get', 'people', 'buy', 'mean', 'wit', 'crypto', 'speculation', 'look', 'lot', 'online', 'sport', 'betting', 'look', 'offline', 'sport', 'bet', 'cyber', 'hack', 'resemble', 'classic', 'espionage', 'phisher', 'recall', 'artist', 'perpetrator', 'crime', 'lure', 'victim', 'wellworn', 'appeal', 'faith', 'promise', 'financial', 'reward', 'fancy', 'bear', 'phishe', 'law', 'professor', 'argue', 'technological', 'solution', 'solve', 'problem', 'force', 'people', 'play', 'nice', 'online', 'good', 'way', 'protect', 'online', 'trick', 'social', 'public', 'policy', 'legal', 'business', 'incentive', 'cultural', 'shift', 'book', 'arrive', 'time', 'last', 'gasp', 'late', 'crypto', 'wave', 'major', 'player', 'find', 'trap', 'net', 'human', 'institution', 'early', 'commission', 'go', 'binance', 'coinbase', 'large', 'cryptocurrency', 'exchange', 'world', 'month', 'charge', 'infamous', 'bankmanfrie', 'founder', 'massive', 'crypto', 'exchange', 'ftx', 'fraud', 'mention', 'crypto', 'main', 'mean', 'payment', 'online', 'crime', 'industry', 'wild', 'ride', 'finance', 'culture', 'deserve', 'hefty', 'chapter', 'narrative', 'internet', 'fraud', 'early', 'deep', 'analysis', 'firstperson', 'perspective', 'crypto', 'actor', 'former', 'star', 'teen', 'drama', 'oc', 'streetwear', 'designer', 'influencer', 'bobby', 'hundred', 'author', 'respectively', 'easy', 'money', 'nft', 'scamnft', 'future', 'heavily', 'report', 'book', 'crypto', 'era', 'tech', 'reporter', 'big', 'short', 'author', 'work', 'commit', 'serious', 'crime', 'fraud', 'crucially', 'important', 'find', 'way', 'justify', 'behavior', 'crucially', 'testify', 'hear', 'ftx', 'believe', 'cryptocurrency', 'industry', 'represent', 'large', 'ponzi', 'scheme', 'history', 'easy', 'money', 'trace', 'journey', 'bored', 'pandemic', 'dabbler', 'commit', 'crypto', 'critic', 'industry', 'rise', 'fall', 'hundred', 'also', 'write', 'chronological', 'account', 'time', 'crypto', 'specifically', 'nonfungible', 'token', 'nfts', 'digital', 'representational', 'object', 'sell', 'drop', 'hundred', 'communitybase', 'streetwear', 'brand', 'medium', 'company', 'hundred', 'nft', 'value', 'cultural', 'artifact', 'convinced', 'time', 'acknowledge', 'writing', 'book', 'worth', 'nft', 'steal', 'mostly', 'phishe', 'scam', 'nft', 'scam', 'pose', 'philosophical', 'question', 'wander', 'moral', 'judgment', 'cultural', 'practice', 'free', 'enterprise', 'mercantilism', 'materialism', 'write', 'difference', 'lawyer', 'actor', 'designer', 'walk', 'bar', 'mckenzie', 'hundred', 'explore', 'character', 'motivation', 'social', 'dynamic', 'much', 'technical', 'innovation', 'online', 'crime', 'human', 'story', 'book', 'collectively', 'argue', 'explanation', 'happen', 'work', 'stay', 'safe', 'human', 'articulate', 'internet', 'crime', 'come', 'offer', 'new', 'paradigm', 'relationship', 'humanity', 'technology', 'relabel', 'technical', 'computer', 'code', 'downcode', 'call', 'human', 'surround', 'drive', 'upcode', 'inner', 'operation', 'human', 'brain', 'outer', 'social', 'political', 'institutional', 'force', 'define', 'world', 'upcode', 'teem', 'ecosystem', 'human', 'human', 'system', 'curtain', 'technology', 'argue', 'upcode', 'responsible', 'technology', 'impact', 'positive', 'negative', 'downcode', 'product', 'technical', 'tool', 'blockchain', 'firewall', 'twofactor', 'authentication', 'implement', 'effort', 'ensure', 'safety', 'online', 'address', 'root', 'cause', 'upstream', 'technologist', 'crypto', 'enthusiast', 'believe', 'computer', 'code', 'law', 'see', 'human', 'error', 'annoying', 'hiccup', 'idea', 'disconcert', 'crime', 'begin', 'end', 'human', 'argue', 'upcode', 'focus', 'blame', 'problem', 'effort', 'improve', 'online', 'safety', 'mckenzie', 'hundred', 'deal', 'crypto', 'nft', 'almost', 'entirely', 'upcode', 'level', 'training', 'computer', 'science', 'examine', 'industry', 'personal', 'lense', 'financial', 'realm', 'friend', 'encourage', 'invest', 'token', 'compensate', 'work', 'pandemic', 'hundred', 'art', 'world', 'historically', 'inaccessible', 'inhospitable', 'many', 'lead', 'gravitate', 'streetwear', 'creative', 'outlet', 'first', 'place', 'hundred', 'see', 'nft', 'signal', 'large', 'positive', 'shift', 'web3', 'nebulous', 'vision', 'democratized', 'form', 'internet', 'creative', 'individual', 'pay', 'work', 'build', 'community', 'fan', 'artist', 'rely', 'tech', 'company', 'appeal', 'web3', 'nft', 'base', 'cultural', 'economic', 'reality', 'likewise', 'scam', 'happen', 'buggy', 'upcode', 'social', 'injustice', 'runaway', 'capitalism', 'corporate', 'monopoly', 'create', 'condition', 'construct', 'downcode', 'guardrail', 'allow', 'good', 'intention', 'win', 'solve', 'online', 'crime', 'bad', 'act', 'easily', 'dismiss', 'work', 'bad', 'actor', 'people', 'perpetrate', 'scam', 'fraud', 'hack', 'even', 'participate', 'system', 'speculative', 'market', 'often', 'subscribe', 'moral', 'rubric', 'act', 'illegally', 'fancy', 'cite', 'seminal', 'research', 'first', 'investigate', 'psychology', 'people', 'write', 'computer', 'virus', 'malware', 'first', 'pop', '1990', 'respondent', 'global', 'survey', 'developmentally', 'appropriate', 'moral', 'reasoning', 'base', 'ethic', 'accord', 'framework', 'create', 'psychologist', 'virus', 'writer', 'make', 'decision', 'base', 'sense', 'right', 'wrong', 'recent', 'research', 'alice', 'hutching', 'director', 'also', 'find', 'hacker', 'group', 'moral', 'agent', 'possess', 'sense', 'justice', 'purpose', 'identity', 'many', 'hacker', 'find', 'community', 'work', 'leak', 'classified', 'information', 'security', 'agency', 'cross', 'legal', 'boundary', 'believe', 'expressly', 'moral', 'reason', 'bitcoin', 'meanwhile', 'frequent', 'agent', 'crime', 'fact', 'create', 'offer', 'trustless', 'way', 'avoid', 'rely', 'bank', 'housing', 'crisis', 'government', 'bailout', '2000', 'leave', 'many', 'wonder', 'traditional', 'financial', 'institution', 'trust', 'consumer', 'interest', 'definition', 'crime', 'also', 'shape', 'social', 'contract', 'well', 'legal', 'one', 'nft', 'scamnft', 'future', 'hundred', 'interview', 'renowned', 'tech', 'investor', 'public', 'speaker', 'figure', 'call', 'face', 'nft', 'vee', 'zeal', 'belief', 'convince', 'hundred', 'create', 'tell', 'hundred', 'critic', 'right', 'call', 'nft', 'scam', 'project', 'opportunistic', 'racket', 'hope', 'work', 'make', 'variety', 'endure', 'lie', 'face', 'value', 'profess', 'belief', 'great', 'good', 'recruit', 'include', 'thousand', 'attendee', 'convention', 'help', 'build', 'even', 'harm', 'way', 'mckenzie', 'spend', 'much', 'chapter', 'easy', 'money', 'describe', 'personal', 'encounter', 'bankmanfrie', 'widely', 'call', 'king', 'crypto', 'fall', 'bankmanfrie', 'professe', 'believe', 'positive', 'potential', 'indeed', 'claim', 'record', 'many', 'time', 'want', 'good', 'work', 'know', 'point', 'potentially', 'fraudulent', 'mckenzie', 'struggle', 'understand', 'point', 'view', 'commit', 'serious', 'crime', 'fraud', 'speculate', 'crucially', 'important', 'find', 'way', 'justify', 'behavior', 'crucially', 'rationalization', 'certainly', 'excuse', 'crime', 'explain', 'people', 'perpetrate', 'eyeboggling', 'fraud', 'even', 'invent', 'new', 'way', 'scam', 'human', 'upcode', 'make', 'see', 'protagonist', 'story', 'powerful', 'even', 'maybe', 'especially', 'billion', 'dollar', 'stake', 'technological', 'innovation', 'change', 'fundamental', 'behavior', 'human', 'technology', 'bring', 'speed', 'spread', 'gambling', 'table', 'single', 'perpetrator', 'reach', 'victim', 'fast', 'global', 'world', 'connect', 'research', 'mckenzie', 'gamble', 'crypto', 'short', 'token', 'specific', 'incorrect', 'timeline', 'disclose', 'much', 'lose', 'amount', 'provoke', 'uncomfortable', 'conversation', 'spouse', 'hardly', 'savvy', 'individual', 'history', 'fall', 'risky', 'pitch', 'brain', 'make', 'painfully', 'easy', 'scamme', 'reason', 'solution', 'rely', 'entirely', 'computer', 'work', 'human', 'mind', 'riddle', 'upcode', 'cause', 'make', 'biased', 'prediction', 'irrational', 'choice', 'write', 'take', 'heuristic', 'lead', 'judge', 'much', 'resemble', 'exist', 'mental', 'image', 'even', 'lead', 'overlook', 'crucial', 'information', 'animal', 'look', 'duck', 'quack', 'duck', 'heuristic', 'tell', 'swim', 'phishe', 'scam', 'rely', 'rush', 'pattern', 'match', 'example', 'fancy', 'bear', 'titular', 'russian', 'hack', 'group', 'book', 'use', 'visually', 'tonally', 'convincing', 'message', 'attempt', 'hack', 'campaign', 'staffer', 'email', 'account', 'work', 'also', 'come', 'play', 'scam', 'fraud', 'hack', 'availability', 'heuristic', 'lead', 'remember', 'sensational', 'event', 'regardless', 'frequency', 'affect', 'heuristic', 'lead', 'emphasize', 'feeling', 'decision', 'fact', 'inflate', 'expectation', 'outcome', 'like', 'win', 'huge', 'payout', 'gamble', 'hundred', 'concerned', 'nft', 'good', 'investment', 'reach', 'friend', 'belief', 'steadfast', 'find', 'calm', 'sense', 'conviction', 'separate', 'loser', 'winner', 'write', 'even', 'fact', 'support', 'step', 'back', 'marketing', 'pitch', 'communal', 'faith', 'reward', 'enticement', 'join', 'win', 'team', 'feed', 'human', 'social', 'instinct', 'especially', 'offline', 'mode', 'connection', 'falter', 'tell', 'bring', 'charge', 'coinbase', 'company', 'respond', 'issue', 'procrypto', 'implore', 'community', 'offer', 'support', 'struggle', 'industry', 'mint', 'coinbase', 'minting', 'platform', 'zora', 'promise', 'donate', 'mint', 'fee', 'receive', 'consumer', 'procrypto', 'advocacy', 'crypto', 'industry', 'rise', 'power', 'kind', 'faithbase', 'relationship', 'continue', 'appeal', 'coinbase', 'token', 'mint', 'suit', 'announce', 'money', 'motivate', 'identity', 'community', 'upsidedown', 'cousin', 'tribalism', 'write', 'hundred', 'fervent', 'contemporary', 'movement', 'trend', 'masterfully', 'meld', 'together', 'thing', 'feel', 'good', 'get', 'rich', 'rally', 'impassioned', 'cause', 'band', 'likeminded', 'friend', 'technological', 'innovation', 'change', 'fundamental', 'behavior', 'human', 'technology', 'bring', 'speed', 'spread', 'gambling', 'table', 'single', 'perpetrator', 'reach', 'victim', 'fast', 'global', 'world', 'connect', 'risk', 'high', 'clearly', 'demonstrate', 'headlineexploding', 'result', 'email', 'hack', 'billion', 'lose', 'investor', 'volatile', 'crypto', 'industry', 'billion', 'lose', 'crypto', 'hack', 'argue', 'effort', 'antivirus', 'antihacke', 'industry', 'code', 'guardrail', 'online', 'system', 'fail', 'fraud', 'go', 'instead', 'reexamine', 'upcode', 'foster', 'support', 'online', 'crime', 'settle', 'moral', 'political', 'conviction', 'owe', 'respect', 'security', 'privacy', 'effectively', 'address', 'online', 'fraud', 'hack', 'scam', 'require', 'political', 'economic', 'social', 'shift', 'create', 'incentive', 'business', 'protect', 'customer', 'penalty', 'datum', 'breach', 'support', 'potential', 'hacker', 'find', 'community', 'outside', 'crime', 'develop', 'government', 'legal', 'policy', 'prevent', 'illicit', 'payment', 'mechanism', 'cryptocurrencie', 'admit', 'shift', 'upcode', 'way', 'likely', 'take', 'generation', 'work', 'already', 'start', 'recent', 'move', 'crypto', 'exchange', 'promise', 'step', 'public', 'warning', 'ai', 'claim', 'generative', 'ai', 'fraud', 'grow', 'public', 'awareness', 'importance', 'datum', 'privacy', 'security', 'help', 'human', 'work', 'evolve', 'social', 'system', 'continue', 'hunt', 'online', 'people', 'money', 'lifetime', 'fraud', 'hack', 'scam', 'likely', 'always', 'find', 'home', 'internet', 'aware', 'upcode', 'help', 'find', 'safe', 'path', 'online', 'jungle', 'rebecca', 'writer', 'artist']","<p>Three new books explore the various scams, frauds, and hacks that plague online life.</p>
"
How ubiquitous keyboard software puts hundreds of millions of Chinese users at risk,https://www.technologyreview.com/2023/08/21/1078207/sogou-keyboard-app-security-loophole/,2023-08-21,"For millions of Chinese people, the first software they download on a new laptop or smartphone is always the same: a keyboard app. Yet few of them are aware that it may make everything they type vulnerable to spying eyes. Since dozens of Chinese characters can share the same latinized phonetic spelling, the ordinary QWERTY keyboard alone is incredibly inefficient. A smart, localized keyboard app can save a lot of time and frustration by predicting the characters and words a user wants to type. Today, over 800 million Chinese people use third-party keyboard apps on their PCs, laptops, and mobile phones.  But a recent report by the Citizen Lab, a University of Toronto–affiliated research group focused on technology and security, revealed that Sogou, one of the most popular Chinese keyboard apps, had a massive security loophole. “This is an app that handles very sensitive information—specifically, every single thing that you type,” says Jeffrey Knockel, a senior research associate at the Citizen Lab and coauthor of the report. “So we wanted to look into that in greater detail and see if this app is properly encrypting this very sensitive data that it’s sending over the network—or, as we found, is it improperly doing it in a way that eavesdroppers could decipher?”  Indeed, what he and his colleagues found was that Sogou’s encryption system could be exploited to intercept and decrypt exactly what people were typing, as they were typing it.  Sogou, which was acquired by the tech giant Tencent in 2021, quickly fixed this loophole after the Citizen Lab researchers disclosed it to the company.  “User privacy is fundamental to our business,” a Sogou spokesperson told MIT Technology Review. “We have addressed the issues identified by the Citizen Lab and will continue to work so that user data remains safe and secure. We transparently disclose our data processing activities in our privacy policy and do not otherwise share user data.” But there’s no guarantee that this was the only vulnerability in the app, and the researchers did not examine other popular keyboard apps in the Chinese market—meaning the ubiquitous software will continue to be a security risk for hundreds of millions of people. And, alarmingly, the potential for such makes otherwise encrypted communications by Chinese users—in apps like Signal, for example—vulnerable to systems of state surveillance. Officially called input method editors (IMEs), keyboard apps are necessary for typing in languages that have more characters than a common Latin-alphabet keyboard allows, like those with Japanese, Korean, or Indic characters. For Chinese users, having an IME is almost a necessity.  “There’s a lot more ambiguity to resolve when typing Chinese characters using a Latin alphabet,” says Mona Wang, an Open Technology Fund fellow at the Citizen Lab and another coauthor of the report. Because the same phonetic spelling can be matched to dozens or even hundreds of Chinese characters, and these characters also can be paired in different ways to become different words, a keyboard app that has been fine-tuned to the Chinese language can perform much better than the default keyboard. More than 40 years ago, designers drew and edited thousands of characters by hand to make it possible to type and print in Chinese. Starting in the PC era, Chinese software developers proposed all kinds of IME products to expedite typing, some even ditching phonetic spelling and allowing users to draw or choose the components of a Chinese character. As a result, downloading third-party keyboard software became standard practice for everyone in China. Released in 2006, Sogou Input Method quickly became the most popular keyboard app in the country. It was more capable than any competitor in predicting which character or word the user actually wanted to type, and it did that by scraping text from the internet and maintaining an extensive library of Chinese words. The cloud-based library was updated frequently to include newly coined words, trending expressions, or names of people in the news. In 2007, when Google launched its Chinese keyboard, it even copied Sogou’s word library (and later had to apologize). In 2014, when the iPhone finally enabled third-party IMEs for the first time, Chinese users rushed to download Sogou’s keyboard app, leaving 3,000 reviews in just one day. At one point, over 90% of Chinese PC users were using Sogou. Over the years, its market dominance has waned; as of last year, Baidu Input Method was the top keyboard app in China, with 607 million users and 46.4% of the market share. But Sogou still had 561 million users, according to iiMedia, an analytics firm.  A keyboard app can access a wide variety of user information. For example, once Sogou is downloaded and added to the iPhone keyboard options, the app will ask for “full access.” If it’s granted, anything the user types can be sent to Sogou’s cloud-based server.  Connecting to the cloud is what makes most IMEs successful, allowing them to improve text prediction and enable other miscellaneous features, like the ability to search for GIFs and memes. But this also adds risk since content can, at least in theory, be intercepted during transmission.  It becomes the apps’ responsibility to properly encrypt the data and prevent that from happening. Sogou’s privacy policy says it has “adopted industry-standard security technology measures … to maximize the prevention of leak, destruction, misuse, unauthorized access, unauthorized disclosure, or alteration” of users’ personal information. “People generally had suspicions [about the security of keyboard apps] because they’re advertising [their] cloud service,” says Wang. “Almost certainly they’re sending some amount of keystrokes over the internet.”  Nevertheless, users have continued to grant the apps full access.  When the Citizen Lab researchers started looking at the Sogou Input Method on Windows, Android, and iOS platforms, they found that it used EncryptWall, an encryption system it developed itself, instead of Transport Layer Security (TLS), the standard international cryptographic protocol that has been in use since 1999. (Sogou is also used on other platforms like MacOS and Linux, but the researchers haven’t looked into them.) One critical difference between the two encryption systems, the Citizen Lab found, is that Sogou’s EncryptWall is still vulnerable to an exploit that was revealed in 2002 and can turn encrypted data back into plain text. TLS was updated to protect against this in 2003. But when they used that exploit method on Sogou, the researchers managed to decrypt the exact keystrokes they’d typed.  The existence of this loophole meant that users were vulnerable to all kinds of hacks. The typed content could be intercepted when it went through VPN software, home Wi-Fi routers, and telecom providers.  Not every word is transmitted to the cloud, the researchers found. “If you type in nihao [‘hello’ in Chinese] or something like that, [the app] can answer that without having to use the cloud database,” says Knockel. “But if it’s more complicated and, frankly, more interesting things that you’re typing in, it has to reach out to that cloud database.”  Along with the content being typed, Knockel and his Citizen Lab colleagues also obtained other information like technical identifiers of the user’s device, the app that the typing occurred in, and even a list of apps installed on the device. A lot of malicious actors would be interested in exploiting a loophole like this and eavesdropping on keystrokes, the researchers note—from cybercriminals after private information (like street addresses and bank account numbers) to government hackers.  (In a written response to the Citizen Lab, Sogou said the transmission of typed text is required to access more accurate and extensive vocabularies on the cloud and enable a built-in search engine, and the uses are stated in the privacy agreement.) This particular loophole was closed when Tencent updated the Sogou software across platforms in late July. The Citizen Lab researchers found that the latest version effectively fixed the problem by adopting the TLS encryption protocol.  Around the world, people who are at high risk of being surveilled by state authorities have turned to apps that offer end-to-end encryption. But if keyboard apps are vulnerable, then otherwise encrypted communication apps like Signal or WhatsApp are now also unsafe. What’s more, once a keyboard app is compromised, even an otherwise offline app, like the built-in notebook app, can be a security risk too.  (Signal and WhatsApp did not respond to MIT Technology Review’s requests for comment. A spokesperson from Baidu said, “Baidu Input Method consistently adheres to established security practice standards. As of now, there are no vulnerabilities related to [the encryption exploit Sogou was vulnerable to] within Baidu Input Method’s products.”) As early as 2019, Naomi Wu, a Shenzhen-based tech blogger known as SexyCyborg online, had sounded the alarm about the risk of using Chinese keyboard apps alongside Signal. “The Signal ‘fix’ is ‘Incognito Mode’ aka for the app to say ‘Pretty please don't read everything I type’ to the virtual keyboard and count on Google/random app makers to listen to the flag, and not be under court order to do otherwise,” she wrote in a 2019 Twitter thread. Since keyboard apps have no obligation to honor Signal’s request, “basically all hardware here is self-compromised 5 minutes out of the box,” she added.  Wu suspects that the use of Signal was the reason some Chinese student activists talking to foreign media were detained by the police in 2018.  In January 2021, Signal itself tried to clarify that its Incognito Keyboard feature (which only works for users on Android systems, which are more vulnerable than iOS) was not a foolproof privacy solution: “Keyboards and IME’s can ignore Android’s Incognito Keyboard flag. This Android system flag is a best effort, not a guarantee. It’s important to use a keyboard or IME that you trust. Signal cannot detect or prevent malware on your device,” the company added to its article on keyboard security. The recent Citizen Lab findings lend further support to Wu’s theory.  Lessons learned from China’s White Paper Protests. The security risk is particularly acute for users in China, since they are more likely to use keyboard apps and are under strict surveillance by their government. (Wu herself has disappeared from social media since the end of June, following a visit from police that was reportedly related to her online discussions of Signal and keyboard apps.)  Still, other governments seem to have been paying attention to vulnerabilities with encrypted data transmission as well. A 2012 document leaked by Edward Snowden, for instance, shows that the Five Eyes intelligence alliance—comprising Canada, the US, Britain, Australia, and New Zealand—had been discreetly exploiting a similar loophole in UC Browser, a popular Chinese program, to intercept certain transmissions.  Beyond being targeted by state actors, there are other ways keystroke information acquired via keyboard apps can be sold, leaked, or hacked. In 2021, it was reported that advertisers were able to access personal information through Sogou, as well as Baidu’s keyboard and similar apps, and use it to push customized ads. And in 2013, a loophole was found that made multimedia files that users uploaded and shared through Sogou searchable on Bing.  These security problems are not unique to Chinese apps. In 2016, users of SwiftKey, an IME that was acquired by Microsoft that year, found that the app was auto-filling other people’s email addresses and personal information, as a result of a bug with its cloud sync system. The following year, a virtual keyboard app accidentally leaked 31 million users’ personal data. Even though the specific loophole identified by the Citizen Lab was fixed quickly, given all these breaches, it feels somewhat inevitable that another security flaw in a keyboard app will be revealed soon.  As Knockel notes, using Sogou and similar apps always poses security risks, particularly in China, since all Chinese apps are legally required to surrender data if asked by the government.  “If that’s something that’s concerning to you,” he says, “you might also just reconsider using Sogou, period.” ","For millions of Chinese people , the first software they download on a new laptop or smartphone is always the same : a keyboard app . Yet few of them are aware that it may make everything they type vulnerable to spying eyes . Since dozens of Chinese characters can share the same latinized phonetic spelling , the ordinary QWERTY keyboard alone is incredibly inefficient . A smart , localized keyboard app can save a lot of time and frustration by predicting the characters and words a user wants to type . Today , over 800 million Chinese people use third-party keyboard apps on their PCs , laptops , and mobile phones . But a recent report by the Citizen Lab , a University of Toronto–affiliated research group focused on technology and security , revealed that Sogou , one of the most popular Chinese keyboard apps , had a massive security loophole . “ This is an app that handles very sensitive information—specifically , every single thing that you type , ” says Jeffrey Knockel , a senior research associate at the Citizen Lab and coauthor of the report . “ So we wanted to look into that in greater detail and see if this app is properly encrypting this very sensitive data that it ’ s sending over the network—or , as we found , is it improperly doing it in a way that eavesdroppers could decipher ? ” Indeed , what he and his colleagues found was that Sogou ’ s encryption system could be exploited to intercept and decrypt exactly what people were typing , as they were typing it . Sogou , which was acquired by the tech giant Tencent in 2021 , quickly fixed this loophole after the Citizen Lab researchers disclosed it to the company . “ User privacy is fundamental to our business , ” a Sogou spokesperson told MIT Technology Review . “ We have addressed the issues identified by the Citizen Lab and will continue to work so that user data remains safe and secure . We transparently disclose our data processing activities in our privacy policy and do not otherwise share user data. ” But there ’ s no guarantee that this was the only vulnerability in the app , and the researchers did not examine other popular keyboard apps in the Chinese market—meaning the ubiquitous software will continue to be a security risk for hundreds of millions of people . And , alarmingly , the potential for such makes otherwise encrypted communications by Chinese users—in apps like Signal , for example—vulnerable to systems of state surveillance . Officially called input method editors ( IMEs ) , keyboard apps are necessary for typing in languages that have more characters than a common Latin-alphabet keyboard allows , like those with Japanese , Korean , or Indic characters . For Chinese users , having an IME is almost a necessity . “ There ’ s a lot more ambiguity to resolve when typing Chinese characters using a Latin alphabet , ” says Mona Wang , an Open Technology Fund fellow at the Citizen Lab and another coauthor of the report . Because the same phonetic spelling can be matched to dozens or even hundreds of Chinese characters , and these characters also can be paired in different ways to become different words , a keyboard app that has been fine-tuned to the Chinese language can perform much better than the default keyboard . More than 40 years ago , designers drew and edited thousands of characters by hand to make it possible to type and print in Chinese . Starting in the PC era , Chinese software developers proposed all kinds of IME products to expedite typing , some even ditching phonetic spelling and allowing users to draw or choose the components of a Chinese character . As a result , downloading third-party keyboard software became standard practice for everyone in China . Released in 2006 , Sogou Input Method quickly became the most popular keyboard app in the country . It was more capable than any competitor in predicting which character or word the user actually wanted to type , and it did that by scraping text from the internet and maintaining an extensive library of Chinese words . The cloud-based library was updated frequently to include newly coined words , trending expressions , or names of people in the news . In 2007 , when Google launched its Chinese keyboard , it even copied Sogou ’ s word library ( and later had to apologize ) . In 2014 , when the iPhone finally enabled third-party IMEs for the first time , Chinese users rushed to download Sogou ’ s keyboard app , leaving 3,000 reviews in just one day . At one point , over 90 % of Chinese PC users were using Sogou . Over the years , its market dominance has waned ; as of last year , Baidu Input Method was the top keyboard app in China , with 607 million users and 46.4 % of the market share . But Sogou still had 561 million users , according to iiMedia , an analytics firm . A keyboard app can access a wide variety of user information . For example , once Sogou is downloaded and added to the iPhone keyboard options , the app will ask for “ full access. ” If it ’ s granted , anything the user types can be sent to Sogou ’ s cloud-based server . Connecting to the cloud is what makes most IMEs successful , allowing them to improve text prediction and enable other miscellaneous features , like the ability to search for GIFs and memes . But this also adds risk since content can , at least in theory , be intercepted during transmission . It becomes the apps ’ responsibility to properly encrypt the data and prevent that from happening . Sogou ’ s privacy policy says it has “ adopted industry-standard security technology measures … to maximize the prevention of leak , destruction , misuse , unauthorized access , unauthorized disclosure , or alteration ” of users ’ personal information . “ People generally had suspicions [ about the security of keyboard apps ] because they ’ re advertising [ their ] cloud service , ” says Wang . “ Almost certainly they ’ re sending some amount of keystrokes over the internet. ” Nevertheless , users have continued to grant the apps full access . When the Citizen Lab researchers started looking at the Sogou Input Method on Windows , Android , and iOS platforms , they found that it used EncryptWall , an encryption system it developed itself , instead of Transport Layer Security ( TLS ) , the standard international cryptographic protocol that has been in use since 1999 . ( Sogou is also used on other platforms like MacOS and Linux , but the researchers haven ’ t looked into them . ) One critical difference between the two encryption systems , the Citizen Lab found , is that Sogou ’ s EncryptWall is still vulnerable to an exploit that was revealed in 2002 and can turn encrypted data back into plain text . TLS was updated to protect against this in 2003 . But when they used that exploit method on Sogou , the researchers managed to decrypt the exact keystrokes they ’ d typed . The existence of this loophole meant that users were vulnerable to all kinds of hacks . The typed content could be intercepted when it went through VPN software , home Wi-Fi routers , and telecom providers . Not every word is transmitted to the cloud , the researchers found . “ If you type in nihao [ ‘ hello ’ in Chinese ] or something like that , [ the app ] can answer that without having to use the cloud database , ” says Knockel . “ But if it ’ s more complicated and , frankly , more interesting things that you ’ re typing in , it has to reach out to that cloud database. ” Along with the content being typed , Knockel and his Citizen Lab colleagues also obtained other information like technical identifiers of the user ’ s device , the app that the typing occurred in , and even a list of apps installed on the device . A lot of malicious actors would be interested in exploiting a loophole like this and eavesdropping on keystrokes , the researchers note—from cybercriminals after private information ( like street addresses and bank account numbers ) to government hackers . ( In a written response to the Citizen Lab , Sogou said the transmission of typed text is required to access more accurate and extensive vocabularies on the cloud and enable a built-in search engine , and the uses are stated in the privacy agreement . ) This particular loophole was closed when Tencent updated the Sogou software across platforms in late July . The Citizen Lab researchers found that the latest version effectively fixed the problem by adopting the TLS encryption protocol . Around the world , people who are at high risk of being surveilled by state authorities have turned to apps that offer end-to-end encryption . But if keyboard apps are vulnerable , then otherwise encrypted communication apps like Signal or WhatsApp are now also unsafe . What ’ s more , once a keyboard app is compromised , even an otherwise offline app , like the built-in notebook app , can be a security risk too . ( Signal and WhatsApp did not respond to MIT Technology Review ’ s requests for comment . A spokesperson from Baidu said , “ Baidu Input Method consistently adheres to established security practice standards . As of now , there are no vulnerabilities related to [ the encryption exploit Sogou was vulnerable to ] within Baidu Input Method ’ s products. ” ) As early as 2019 , Naomi Wu , a Shenzhen-based tech blogger known as SexyCyborg online , had sounded the alarm about the risk of using Chinese keyboard apps alongside Signal . “ The Signal ‘ fix ’ is ‘ Incognito Mode ’ aka for the app to say ‘ Pretty please do n't read everything I type ’ to the virtual keyboard and count on Google/random app makers to listen to the flag , and not be under court order to do otherwise , ” she wrote in a 2019 Twitter thread . Since keyboard apps have no obligation to honor Signal ’ s request , “ basically all hardware here is self-compromised 5 minutes out of the box , ” she added . Wu suspects that the use of Signal was the reason some Chinese student activists talking to foreign media were detained by the police in 2018 . In January 2021 , Signal itself tried to clarify that its Incognito Keyboard feature ( which only works for users on Android systems , which are more vulnerable than iOS ) was not a foolproof privacy solution : “ Keyboards and IME ’ s can ignore Android ’ s Incognito Keyboard flag . This Android system flag is a best effort , not a guarantee . It ’ s important to use a keyboard or IME that you trust . Signal can not detect or prevent malware on your device , ” the company added to its article on keyboard security . The recent Citizen Lab findings lend further support to Wu ’ s theory . Lessons learned from China ’ s White Paper Protests . The security risk is particularly acute for users in China , since they are more likely to use keyboard apps and are under strict surveillance by their government . ( Wu herself has disappeared from social media since the end of June , following a visit from police that was reportedly related to her online discussions of Signal and keyboard apps . ) Still , other governments seem to have been paying attention to vulnerabilities with encrypted data transmission as well . A 2012 document leaked by Edward Snowden , for instance , shows that the Five Eyes intelligence alliance—comprising Canada , the US , Britain , Australia , and New Zealand—had been discreetly exploiting a similar loophole in UC Browser , a popular Chinese program , to intercept certain transmissions . Beyond being targeted by state actors , there are other ways keystroke information acquired via keyboard apps can be sold , leaked , or hacked . In 2021 , it was reported that advertisers were able to access personal information through Sogou , as well as Baidu ’ s keyboard and similar apps , and use it to push customized ads . And in 2013 , a loophole was found that made multimedia files that users uploaded and shared through Sogou searchable on Bing . These security problems are not unique to Chinese apps . In 2016 , users of SwiftKey , an IME that was acquired by Microsoft that year , found that the app was auto-filling other people ’ s email addresses and personal information , as a result of a bug with its cloud sync system . The following year , a virtual keyboard app accidentally leaked 31 million users ’ personal data . Even though the specific loophole identified by the Citizen Lab was fixed quickly , given all these breaches , it feels somewhat inevitable that another security flaw in a keyboard app will be revealed soon . As Knockel notes , using Sogou and similar apps always poses security risks , particularly in China , since all Chinese apps are legally required to surrender data if asked by the government . “ If that ’ s something that ’ s concerning to you , ” he says , “ you might also just reconsider using Sogou , period . ”","['million', 'chinese', 'people', 'first', 'software', 'download', 'new', 'laptop', 'smartphone', 'always', 'keyboard', 'app', 'yet', 'aware', 'make', 'type', 'vulnerable', 'spy', 'eye', 'dozen', 'chinese', 'character', 'share', 'latinize', 'phonetic', 'spelling', 'ordinary', 'qwerty', 'alone', 'incredibly', 'inefficient', 'smart', 'localize', 'keyboard', 'app', 'save', 'lot', 'time', 'frustration', 'predict', 'character', 'word', 'user', 'want', 'type', 'today', 'chinese', 'people', 'use', 'thirdparty', 'keyboard', 'app', 'pc', 'laptop', 'mobile', 'phone', 'recent', 'report', 'citizen', 'affiliate', 'research', 'group', 'focus', 'technology', 'security', 'reveal', 'sogou', 'popular', 'chinese', 'keyboard', 'app', 'massive', 'security', 'loophole', 'app', 'handle', 'sensitive', 'information', 'specifically', 'single', 'thing', 'type', 'say', 'senior', 'research', 'associate', 'citizen', 'lab', 'coauthor', 'report', 'want', 'look', 'great', 'detail', 'see', 'app', 'properly', 'encrypt', 'sensitive', 'datum', 'send', 'network', 'find', 'improperly', 'way', 'eavesdropper', 'decipher', 'indeed', 'colleague', 'find', 'encryption', 'system', 'exploit', 'intercept', 'exactly', 'people', 'type', 'type', 'acquire', 'tech', 'giant', 'tencent', 'quickly', 'fix', 'loophole', 'citizen', 'lab', 'researcher', 'disclose', 'company', 'user', 'privacy', 'fundamental', 'business', 'sogou', 'spokesperson', 'tell', 'mit', 'technology', 'review', 'address', 'issue', 'identify', 'citizen', 'lab', 'continue', 'work', 'user', 'datum', 'remain', 'safe', 'secure', 'transparently', 'disclose', 'datum', 'processing', 'activity', 'privacy', 'policy', 'otherwise', 'share', 'user', 'datum', 'guarantee', 'vulnerability', 'app', 'researcher', 'examine', 'popular', 'keyboard', 'app', 'chinese', 'market', 'mean', 'ubiquitous', 'software', 'continue', 'security', 'risk', 'hundred', 'million', 'people', 'alarmingly', 'potential', 'make', 'otherwise', 'encrypt', 'communication', 'chinese', 'user', 'app', 'signal', 'example', 'vulnerable', 'system', 'state', 'surveillance', 'officially', 'call', 'input', 'method', 'editor', 'ime', 'keyboard', 'app', 'necessary', 'type', 'language', 'character', 'common', 'latinalphabet', 'keyboard', 'allow', 'japanese', 'korean', 'indic', 'character', 'chinese', 'user', 'ime', 'almost', 'necessity', 'lot', 'ambiguity', 'resolve', 'type', 'chinese', 'character', 'use', 'latin', 'alphabet', 'say', 'open', 'technology', 'fund', 'fellow', 'citizen', 'lab', 'coauthor', 'report', 'phonetic', 'spelling', 'match', 'dozen', 'even', 'hundred', 'chinese', 'character', 'character', 'also', 'pair', 'different', 'way', 'become', 'different', 'word', 'keyboard', 'app', 'finetune', 'chinese', 'language', 'perform', 'much', 'well', 'default', 'keyboard', 'year', 'ago', 'designer', 'draw', 'edit', 'thousand', 'character', 'hand', 'make', 'possible', 'type', 'print', 'start', 'pc', 'era', 'chinese', 'software', 'developer', 'propose', 'kind', 'ime', 'product', 'expedite', 'type', 'even', 'ditch', 'phonetic', 'spelling', 'allow', 'user', 'draw', 'choose', 'component', 'chinese', 'character', 'result', 'download', 'keyboard', 'software', 'become', 'standard', 'practice', 'release', 'input', 'method', 'quickly', 'become', 'popular', 'keyboard', 'app', 'country', 'capable', 'competitor', 'predict', 'character', 'word', 'user', 'actually', 'want', 'type', 'scrape', 'text', 'internet', 'maintain', 'extensive', 'library', 'chinese', 'word', 'cloudbased', 'library', 'update', 'frequently', 'include', 'newly', 'coin', 'word', 'trend', 'expression', 'name', 'people', 'news', 'launch', 'chinese', 'keyboard', 'even', 'copy', 'sogou', 'word', 'library', 'later', 'apologize', 'iphone', 'finally', 'enable', 'thirdparty', 'ime', 'first', 'time', 'chinese', 'user', 'rush', 'download', 'leave', 'review', 'day', 'point', 'chinese', 'pc', 'user', 'use', 'sogou', 'year', 'market', 'dominance', 'wane', 'last', 'year', 'baidu', 'input', 'method', 'top', 'keyboard', 'app', 'user', 'market', 'share', 'still', 'user', 'accord', 'iimedia', 'analytic', 'firm', 'keyboard', 'app', 'access', 'wide', 'variety', 'user', 'information', 'example', 'sogou', 'download', 'add', 'iphone', 'keyboard', 'option', 'app', 'ask', 'full', 'access', 'grant', 'user', 'type', 'send', 'cloudbase', 'server', 'connect', 'cloud', 'make', 'ime', 'successful', 'allow', 'improve', 'text', 'prediction', 'enable', 'miscellaneous', 'feature', 'ability', 'search', 'gif', 'meme', 'also', 'add', 'risk', 'content', 'least', 'theory', 'intercept', 'transmission', 'become', 'app', 'responsibility', 'properly', 'encrypt', 'datum', 'prevent', 'happen', 'sogou', 'privacy', 'policy', 'say', 'adopt', 'industrystandard', 'security', 'technology', 'measure', 'maximize', 'prevention', 'leak', 'destruction', 'misuse', 'unauthorized', 'access', 'unauthorized', 'disclosure', 'alteration', 'user', 'personal', 'information', 'people', 'generally', 'suspicion', 'security', 'keyboard', 'app', 'advertise', 'cloud', 'service', 'say', 'almost', 'certainly', 'send', 'amount', 'keystroke', 'internet', 'nevertheless', 'user', 'continue', 'grant', 'app', 'full', 'access', 'citizen', 'lab', 'researcher', 'start', 'look', 'sogou', 'input', 'method', 'window', 'android', 'io', 'platform', 'find', 'use', 'encryptwall', 'encryption', 'system', 'develop', 'instead', 'transport', 'layer', 'security', 'tls', 'standard', 'international', 'cryptographic', 'protocol', 'use', 'sogou', 'also', 'use', 'platform', 'researcher', 'look', 'critical', 'difference', 'encryption', 'system', 'citizen', 'lab', 'find', 'encryptwall', 'still', 'vulnerable', 'exploit', 'reveal', 'turn', 'encrypt', 'datum', 'back', 'plain', 'text', 'tls', 'update', 'protect', 'use', 'exploit', 'method', 'sogou', 'researcher', 'manage', 'decrypt', 'exact', 'keystroke', 'type', 'existence', 'loophole', 'mean', 'user', 'vulnerable', 'kind', 'hack', 'type', 'content', 'intercept', 'go', 'vpn', 'software', 'home', 'wifi', 'router', 'telecom', 'provider', 'word', 'transmit', 'cloud', 'researcher', 'find', 'type', 'chinese', 'app', 'answer', 'use', 'cloud', 'database', 'say', 'knockel', 'complicated', 'frankly', 'interesting', 'thing', 'type', 'reach', 'cloud', 'database', 'content', 'type', 'knockel', 'citizen', 'lab', 'colleague', 'also', 'obtain', 'information', 'technical', 'identifier', 'user', 'device', 'app', 'typing', 'occur', 'even', 'list', 'app', 'instal', 'device', 'lot', 'malicious', 'actor', 'interested', 'exploit', 'loophole', 'eavesdropping', 'keystroke', 'researcher', 'note', 'cybercriminal', 'private', 'information', 'street', 'address', 'bank', 'account', 'number', 'government', 'hacker', 'write', 'response', 'citizen', 'sogou', 'say', 'transmission', 'type', 'text', 'require', 'access', 'accurate', 'extensive', 'vocabulary', 'cloud', 'enable', 'builtin', 'search', 'engine', 'use', 'state', 'privacy', 'agreement', 'particular', 'loophole', 'close', 'tencent', 'update', 'sogou', 'software', 'platform', 'late', 'citizen', 'lab', 'researcher', 'find', 'late', 'version', 'effectively', 'fix', 'problem', 'adopt', 'tls', 'encryption', 'protocol', 'world', 'people', 'high', 'risk', 'surveille', 'state', 'authority', 'turn', 'app', 'offer', 'endtoend', 'encryption', 'keyboard', 'app', 'vulnerable', 'otherwise', 'encrypt', 'communication', 'app', 'signal', 'whatsapp', 'also', 'unsafe', 'keyboard', 'app', 'compromise', 'even', 'otherwise', 'offline', 'app', 'builtin', 'notebook', 'app', 'security', 'risk', 'signal', 'whatsapp', 'respond', 'mit', 'technology', 'review', 'request', 'comment', 'spokesperson', 'say', 'baidu', 'input', 'method', 'consistently', 'adhere', 'establish', 'security', 'practice', 'standard', 'vulnerability', 'relate', 'encryption', 'exploit', 'vulnerable', 'baidu', 'input', 'method', 'product', 'early', 'shenzhenbase', 'tech', 'blogger', 'know', 'sexycyborg', 'online', 'sound', 'alarm', 'risk', 'use', 'chinese', 'keyboard', 'app', 'signal', 'signal', 'fix', 'mode', 'aka', 'app', 'say', 'pretty', 'read', 'type', 'virtual', 'keyboard', 'count', 'googlerandom', 'app', 'maker', 'listen', 'flag', 'court', 'order', 'otherwise', 'write', 'twitter', 'thread', 'keyboard', 'app', 'obligation', 'honor', 'signal', 'request', 'basically', 'hardware', 'selfcompromise', 'minute', 'box', 'add', 'suspect', 'use', 'signal', 'reason', 'chinese', 'student', 'activist', 'talk', 'foreign', 'medium', 'detain', 'police', 'signal', 'try', 'clarify', 'feature', 'work', 'user', 'android', 'system', 'vulnerable', 'io', 'foolproof', 'privacy', 'solution', 'keyboard', 'ignore', 'android', 'flag', 'android', 'system', 'flag', 'good', 'effort', 'guarantee', 'important', 'use', 'keyboard', 'ime', 'trust', 'signal', 'detect', 'prevent', 'malware', 'device', 'company', 'add', 'article', 'keyboard', 'security', 'recent', 'citizen', 'lab', 'finding', 'lend', 'support', 'theory', 'lesson', 'learn', 'paper', 'protest', 'security', 'risk', 'particularly', 'acute', 'user', 'likely', 'use', 'keyboard', 'app', 'strict', 'surveillance', 'government', 'disappear', 'social', 'medium', 'end', 'follow', 'visit', 'police', 'reportedly', 'relate', 'online', 'discussion', 'signal', 'keyboard', 'app', 'still', 'government', 'seem', 'pay', 'attention', 'vulnerability', 'encrypt', 'datum', 'transmission', 'well', 'document', 'leak', 'instance', 'show', 'eye', 'intelligence', 'alliance', 'comprise', 'discreetly', 'exploit', 'similar', 'loophole', 'popular', 'chinese', 'program', 'intercept', 'certain', 'transmission', 'target', 'state', 'actor', 'way', 'information', 'acquire', 'keyboard', 'app', 'sell', 'leak', 'hack', 'report', 'advertiser', 'able', 'access', 'personal', 'information', 'sogou', 'well', 'baidu', 'keyboard', 'similar', 'app', 'use', 'push', 'customized', 'ad', 'loophole', 'find', 'make', 'multimedia', 'file', 'user', 'upload', 'share', 'searchable', 'bing', 'security', 'problem', 'unique', 'chinese', 'app', 'user', 'swiftkey', 'ime', 'acquire', 'year', 'find', 'app', 'autofille', 'people', 'email', 'address', 'personal', 'information', 'result', 'bug', 'cloud', 'sync', 'system', 'following', 'year', 'virtual', 'keyboard', 'app', 'accidentally', 'leak', 'user', 'personal', 'datum', 'even', 'specific', 'loophole', 'identify', 'citizen', 'lab', 'fix', 'quickly', 'give', 'breach', 'feel', 'somewhat', 'inevitable', 'security', 'flaw', 'keyboard', 'app', 'reveal', 'soon', 'knockel', 'note', 'use', 'sogou', 'similar', 'app', 'always', 'pose', 'security', 'risk', 'particularly', 'chinese', 'app', 'legally', 'require', 'surrender', 'datum', 'ask', 'government', 'concern', 'say', 'also', 'reconsider', 'use', 'sogou', 'period']","<p>Third-party keyboard apps make typing in Chinese more efficient, but they can also be a privacy nightmare.</p>
"
The future of open source is still very much in flux,https://www.technologyreview.com/2023/08/17/1077498/future-open-source/,2023-08-17,"When Xerox donated a new laser printer to the MIT Artificial Intelligence Lab in 1980, the company couldn’t have known that the machine would ignite a revolution. The printer jammed. And according to the 2002 book Free as in Freedom, Richard M. Stallman, then a 27-year-old programmer at MIT, tried to dig into the code to fix it. He expected to be able to: he’d done it with previous printers. The early decades of software development generally ran on a culture of open access and free exchange, where engineers could dive into each other’s code across time zones and institutions to make it their own or squash a few bugs. But this new printer ran on inaccessible proprietary software. Stallman was locked out—and enraged that Xerox had violated the open code-sharing system he’d come to rely on.  A few years later, in September 1983, Stallman released GNU, an operating system designed to be a free alternative to one of the dominant operating systems at the time: Unix. Stallman envisioned GNU as a means to fight back against the proprietary mechanisms, like copyright, that were beginning to flood the tech industry. The free-software movement was born from one frustrated engineer’s simple, rigid philosophy: for the good of the world, all code should be open, without restriction or commercial intervention.  Forty years later, tech companies are making billions on proprietary software, and much of the technology around us—from ChatGPT to smart thermostats—is inscrutable to everyday consumers. In this environment, Stallman’s movement may look like a failed values experiment crushed under the weight of commercial reality. But in 2023, the free and open-source software movement is not only alive and well; it has become a keystone of the tech industry.  Today, 96% of all code bases incorporate open-source software. GitHub, the biggest platform for the open-source community, is used by more than 100 million developers worldwide. The Biden administration’s Securing Open Source Software Act of 2022 publicly recognized open-source software as critical economic and security infrastructure. Even AWS, Amazon’s money-making cloud arm, supports the development and maintenance of open-source software; it committed its portfolio of patents to an open use community in December of last year. Over the last two years, while public trust in private technology companies has plummeted, organizations including Google, Spotify, the Ford Foundation, Bloomberg, and NASA have established new funding for open-source projects and their counterparts in open science efforts—an extension of the same values applied to scientific research. The fact that open-source software is now so essential means that long-standing leadership and diversity issues in the movement have become everyone’s problems. Many open-source projects began with “benevolent dictator for life” (BDFL) models of governance, where original founders hang on to leadership for years—and not always responsibly. Stallman and some other BDFLs have been criticized by their own communities for misogynistic or even abusive behavior. Stallman stepped down as president of the Free Software Foundation in 2019 (although he returned to the board two years later). Overall, open-source participants are still overwhelmingly white, male, and located in the Global North. Projects can be overly influenced by corporate interests. Meanwhile, the people doing the hard work of keeping critical code healthy are not consistently funded. In fact, many major open-source projects still operate almost completely on volunteer steam. Greater access to the code behind generative models is fueling innovation. But if top companies get spooked, they could close up shop. Challenges notwithstanding, there’s plenty to celebrate in 2023, the year of GNU’s 40th birthday. The modern open-source movement persists as a collaborative haven for transparent ways of working within a highly fragmented and competitive industry. Selena Deckelmann, chief product and technology officer at the Wikimedia Foundation, says the power of open source lies in its “idea that people anywhere can collaborate together on software, but also on many [more] things.” She points out that tools to put this philosophy into action, like mailing lists, online chat, and open version control systems, were pioneered in open-source communities and have been adopted as standard practice by the wider tech industry. “We found a way for people from all over the world, regardless of background, to find a common cause to collaborate with each other,” says Kelsey Hightower, an early contributor to Kubernetes, an open-source system for automating app deployment and management, who recently retired from his role as a distinguished engineer at Google Cloud. “I think that is pretty unique to the world of open source.”  The 2010s backlash against tech’s unfettered growth, and the recent AI boom, have focused a spotlight on the open-source movement’s ideas about who has the right to use other people’s information online and who benefits from technology. Clement Delangue, CEO of the open-source AI company Hugging Face, which was recently valued at $4 billion, testified before Congress in June of 2023 that “ethical openness” in AI development could help make organizations more compliant and transparent, while allowing researchers beyond a few large tech companies access to technology and progress. “We’re in a unique cultural moment,” says Danielle Robinson, executive director of Code for Science and Society, a nonprofit that provides funding and support for public-interest technology. “People are more aware than ever of how capitalism has been influencing what technologies get built, and whether you have a choice to interact with it.” Once again, free and open-source software have become a natural home for the debate about how technology should be. The early days of the free-software movement were fraught with arguments about the meaning of “free.” Stallman and the Free Software Foundation (FSF), founded in 1985, held firm to the idea of four freedoms: people should be allowed to run a program for any purpose, study how it works from the source code and change it to meet their needs, redistribute copies, and distribute modified versions too. Stallman saw free software as an essential right: “Free as in free speech, not free beer,” as his apocryphal slogan goes. He created the GNU General Public License, what’s known as a “copyleft” license, to ensure that the four freedoms were protected in code built with GNU. Linus Torvalds, the Finnish engineer who in 1991 created the now ubiquitous Unix alternative Linux, didn’t buy into this dogma. Torvalds and others, including Microsoft’s Bill Gates, believed that the culture of open exchange among engineers could coexist with commerce, and that more-restrictive licenses could forge a path toward both financial sustainability and protections for software creators and users. It was during a 1998 strategic meeting of free-software advocates—which notably did not include Stallman—that this pragmatic approach became known as “open source.” (The term was coined and introduced to the group not by an engineer, but by the futurist and nano­technology scholar Christine Peterson.)  Karen Sandler, executive director of the Software Freedom Conservancy, a nonprofit that advocates for free and open-source software, saw firsthand how the culture shifted from orthodoxy to a big-tent approach with room for for-profit entities when she worked as general counsel at the Software Freedom Law Center in the early 2000s. “The people who were ideological—some of them stayed quite ideological. But many of them realized, oh, wait a minute, we can get jobs doing this. We can do well by doing good,” Sandler remembers. By leveraging the jobs and support that early tech companies were offering, open-source contributors could sustain their efforts and even make a living doing what they believed in. In that manner, companies using and contributing to free and open software could expand the community beyond volunteer enthusiasts and improve the work itself. “How could we ever make it better if it’s just a few radical people?” Sandler says.  As the tech industry grew around private companies like Sun Microsystems, IBM, Microsoft, and Apple in the late ’90s and early ’00s, new open-source projects sprang up, and established ones grew roots. Apache emerged as an open-source web server in 1995. Red Hat, a company offering enterprise companies support for open-source software like Linux, went public in 1999. GitHub, a platform originally created to support version control for open-source projects, launched in 2008, the same year that Google released Android, the first open-source phone operating system. The more pragmatic definition of the concept came to dominate the field. Meanwhile, Stallman’s original philosophy persisted among dedicated groups of believers—where it still lives today through nonprofits like FSF, which only uses and advocates for software that protects the four freedoms.  “If a company only ends up just sharing, and nothing more, I think that should be celebrated.” As open-source software spread, a bifurcation of the tech stack became standard practice, with open-source code as the support structure for proprietary work. Free and open-source software often served in the underlying foundation or back-end architecture of a product, while companies vigorously pursued and defended copyrights on the user-facing layers. Some estimate that Amazon’s 1999 patent on its one-click buying process was worth $2.4 billion per year to the company until it expired. It relied on Java, an open-source programming language, and other open-source software and tooling to build and maintain it. Today, corporations not only depend on open-source software but play an enormous role in funding and developing open-source projects: Kubernetes (initially launched and maintained at Google) and Meta’s React are both robust sets of software that began as internal solutions freely shared with the larger technology community. But some people, like the Software Freedom Conservancy’s Karen Sandler, identify an ongoing conflict between profit-­driven corporations and the public interest. “Companies have become so savvy and educated with respect to open-source software that they use a ton of it. That’s good,” says Sandler. At the same time, they profit from their proprietary work—which they sometimes attempt to pass off as open too, a practice the scholar and organizer Michelle Thorne dubbed “openwashing” in 2009. For Sandler, if companies don’t also make efforts to support user and creator rights, they’re not pushing forward the free and open-source ethos. And she says for the most part, that’s indeed not happening: “They’re not interested in giving the public any appreciable rights to their software.”  Others, including Kelsey Hightower, are more sanguine about corporate involvement. “If a company only ends up just sharing, and nothing more, I think that should be celebrated,” he says. “Then if for the next two years you allow your paid employees to work on it, maintaining the bugs and issues, but then down the road it’s no longer a priority and you choose to step back, I think we should thank [the company] for those years of contributions.”  In stark contrast, FSF, now in its 38th year, holds firm to its original ideals and opposes any product or company that does not support the ability for users to view, modify, and redistribute code. The group today runs public action campaigns like “End Software Patents,” publishing articles and submitting amicus briefs advocating the end of patents on software. The foundation’s executive director, Zoë Kooyman, hopes to continue pushing the conversation toward freedom rather than commercial concerns. “Every belief system or form of advocacy needs a far end,” she says. “That’s the only way to be able to drive the needle. [At FSF], we are that far end of the spectrum, and we take that role very seriously.”  Forty years on from the release of GNU, there is no singular open-source community, “any more than there is an ‘urban community,’” as researcher and engineer Nadia Asparouhova (formerly Eghbal) writes in her 2020 book Working in Public: The Making and Maintenance of Open Source Software. There’s no singular definition, either. The Open Source Initiative (OSI) was founded in 1998 to steward the meaning of the phrase, but not all modern open-source projects adhere to the 10 specific criteria OSI laid out, and other definitions appear across communities. Scale, technology, social norms, and funding also range widely from project to project and community to community. For example, Kubernetes has a robust, organized community of tens of thousands of contributors and years of Google investment. Salmon is a niche open-source bioinformatics research tool with fewer than 50 contributors, supported by grants. OpenSSL, which encrypts an estimated 66% of the web, is currently maintained by 18 engineers compensated through donations and elective corporate contracts. The major discussions now are more about people than technology: What does healthy and diverse collaboration look like? How can those who support the code get what they need to continue the work? “How do you include a voice for all the people affected by the technology you build?” asks James Vasile, an open-source consultant and strategist who sits on the board of the Electronic Frontier Foundation. “These are big questions. We’ve never grappled with them before. No one was working on this 20 years ago, because that just wasn’t part of the scene. Now it is, and we [in the open-source community] have the chance to consider these questions.” “We need designers, ethnographers, social and cultural experts. We need everyone to be playing a role in open source.” “Free as in puppy,” a phrase that can be traced back to 2006, has emerged as a valuable definition of “free” for modern open-source projects—one that speaks to the responsibilities of creators and users to each other and the software, in addition to their rights. Puppies need food and care to survive; open-source code needs funding and “maintainers,” individuals who consistently respond to requests and feedback from a community, fix bugs, and manage the growth and scope of a project. Many open-source projects have become too big, complicated, or important to be governed by one person or even a small group of like-minded individuals. And open-source contributors have their own needs and concerns, too. A person who’s good at building may not be good at maintaining; someone who creates a project may not want to or be able to run it indefinitely. In 2018, for instance, Guido van Rossum, the creator of the open-source programming language Python, stepped down from leadership after almost 30 years, exhausted from the demands of the mostly uncompensated role. “I’m tired,” he wrote in his resignation message to the community,  “and need a very long break.”  Supporting the people who create, maintain, and use free and open-source software requires new roles and perspectives. Whereas the movement in its early days was populated almost exclusively by engineers communicating across message boards and through code, today’s open-source projects invite participation from new disciplines to handle logistical work like growth and advocacy, as well as efforts toward greater inclusion and belonging. “We’ve shifted from open source being about just the technical stuff to the broader set of expertise and perspectives that are required to make effective open-source projects,” says Michael Brennan, senior program officer with the Technology and Society program at the Ford Foundation, which funds research into open internet issues. “We need designers, ethnographers, social and cultural experts. We need everyone to be playing a role in open source if it’s going to be effective and meet the needs of the people around the world.”  One powerful source of support arrived in 2008 with the launch of GitHub. While it began as a version control tool, it has grown into a suite of services, standards, and systems that is now the “highway system” for most open-source development, as Asparouhova puts it in Working in Public. GitHub helped lower the barrier to entry, drawing wider contribution and spreading best practices such as community codes of conduct. But its success has also given a single platform vast influence over communities dedicated to decentralized collaboration.  Demetris Cheatham, until recently GitHub’s senior director for diversity and inclusion strategy, took that responsibility very seriously. To find out where things stood, the company partnered with the Linux Foundation in 2021 on a survey and resulting report on diversity and inclusion within open source. The data showed that despite a pervasive ethos of collaboration and openness (more than 80% of the respondents reported feeling welcome), communities are dominated by contributors who are straight, white, male, and from the Global North. In response, Cheatham, who is now the company’s chief of staff, focused on ways to broaden access and promote a sense of belonging. GitHub launched All In for Students, a mentorship and education program with 30 students drawn primarily from historically Black colleges and universities. In its second year, the program expanded to more than 400 students.  Representation has not been the only stumbling block to a more equitable open-source ecosystem. The Linux Foundation report showed that only 14% of open-source contributors surveyed were getting paid for their work. While this volunteer spirit aligns with the original vision of free software as a commerce-free exchange of ideas, free labor presents a major access issue. Additionally, 30% of respondents in the survey did not trust that codes of conduct would be enforced—suggesting they did not feel they could count on a respectful working environment. “We’re at another inflection point now where codes of conduct are great, but they’re only a tool,” says Code for Science and Society’s Danielle Robinson. “I’m starting to see larger cultural shifts toward rethinking extractive processes that have been a part of open source for a long time.” Getting maintainers paid and connecting contributors with support are now key to opening up open source to a more diverse group of participants. With that in mind, this year GitHub established resources specifically for maintainers, including workshops and a hub of DEI tools. And in May, the platform launched a new project to connect large, well-resourced open-source communities with smaller ones that need help. Cheatham says it’s crucial to the success of any of these programs that they be shared for free with the broader community. “We’re not inventing anything new at all. We’re just applying open-source principles to diversity, equity, and inclusion,” she says.  GitHub’s influence over open source may be large, but it is not the only group working to get maintainers paid and expand open-source participation. The Software Freedom Conservancy’s Outreachy diversity initiative offers paid internships; as of 2019, 92% of past Outreachy interns have identified as women and 64% as people of color. Open-source fundraising platforms like Open Collective and Tidelift have also emerged to help maintainers tap into resources.  The philanthropic world is stepping up too. The Ford Foundation, the Sloan Foundation, Omidyar Network, and the Chan Zuckerberg Initiative, as well as smaller organizations like Code for Science and Society, have all recently begun or expanded their efforts to support open-source research, contributors, and projects—including specific efforts promoting inclusion and diversity. Govind Shivkumar from Omidyar Network told MIT Technology Review that philanthropy is well positioned to establish funding architecture that could help prove out open-source projects, making them less risky prospects for future governmental funding. In fact, research supported by the Ford Foundation’s Digital Infrastructure Fund contributed to Germany’s recent creation of a national fund for open digital infrastructure. Momentum has also been building in the US. In 2016 the White House began requiring at least 20% of government-­developed software to be open source. Last year’s Securing Open Source Software Act passed with bipartisan support, establishing a framework for attention and investment at the federal level toward making open-source software stronger and more secure. Open source contributes valuable practices and tools, but it may also offer a competitive advantage over proprietary efforts. A document leaked in May from Google argued that open-source communities had pushed, tested, integrated, and expanded the capabilities of large language models more thoroughly than private efforts could’ve accomplished on their own: “Many of the new ideas [in AI development] are from ordinary people. The barrier to entry for training and experimentation has dropped from the total output of a major research organization to one person, an evening, and a beefy laptop.” The recently articulated concept of Time till Open Source Alternative (TTOSA)—the time between the release of a proprietary product and an open-source equivalent—also speaks to this advantage. One researcher estimated the average TTOSA to be seven years but noted that the process has been speeding up thanks to easy-to-use services like GitHub.  At the same time, much of our modern world now relies on underfunded and rapidly expanding digital infrastructure. There has long been an assumption within open source that bugs can be identified and solved quickly by the “many eyes” of a wide community—and indeed this can be true. But when open-source software affects millions of users and its maintenance is handled by handfuls of underpaid individuals, the weight can be too much for the system to bear. In 2021, a security vulnerability in a popular open-source Apache library exposed an estimated hundreds of millions of devices to hacking attacks. Major players across the industry were affected, and large parts of the internet went down. The vulnerability’s lasting impact is hard to quantify even now. Volunteer-run projects like Log4J keep the internet running. The result is unsustainable burnout, and a national security risk when they go wrong. Other risks emerge from open-source development without the support of ethical guardrails. Proprietary efforts like Google’s Bard and OpenAI’s ChatGPT have demonstrated that AI can perpetuate existing biases and may even cause harm—while also not providing the transparency that could help a larger community audit the technology, improve it, and learn from its mistakes. But allowing anyone to use, modify, and distribute AI models and technology could accelerate their misuse. One week after Meta began granting access to its AI model LLaMA, the package leaked onto 4chan, a platform known for spreading misinformation. LLaMA 2, a new model released in July, is fully open to the public, but the company has not disclosed its training data as is typical in open-source projects—putting it somewhere in between open and closed by some definitions, but decidedly not open by OSI’s. (OpenAI is reportedly working on an open-source model as well but has not made a formal announcement.) “There are always trade-offs in the decisions you make in technology,” says Margaret Mitchell, chief ethics scientist at Hugging Face. “I can’t just be wholeheartedly supportive of open source in all cases without any nuances or caveats.” Mitchell and her team have been working on open-source tools to help communities safeguard their work, such as gating mechanisms to allow collaboration only at the project owner’s discretion, and “model cards” that detail a model’s potential biases and social impacts—information researchers and the public can take into consideration when choosing which models to work with.  Open-source software has come a long way since its rebellious roots. But carrying it forward and making it into a movement that fully reflects the values of openness, reciprocity, and access will require careful consideration, financial and community investment, and the movement’s characteristic process of self-improvement through collaboration. As the modern world becomes more dispersed and diverse, the skill sets required to work asynchronously with different groups of people and technologies toward a common goal are only growing more essential. At this rate, 40 years from now technology might look more open than ever—and the world may be better for it.  Rebecca Ackermann is a writer, designer, and artist based in San Francisco. ","When Xerox donated a new laser printer to the MIT Artificial Intelligence Lab in 1980 , the company couldn ’ t have known that the machine would ignite a revolution . The printer jammed . And according to the 2002 book Free as in Freedom , Richard M. Stallman , then a 27-year-old programmer at MIT , tried to dig into the code to fix it . He expected to be able to : he ’ d done it with previous printers . The early decades of software development generally ran on a culture of open access and free exchange , where engineers could dive into each other ’ s code across time zones and institutions to make it their own or squash a few bugs . But this new printer ran on inaccessible proprietary software . Stallman was locked out—and enraged that Xerox had violated the open code-sharing system he ’ d come to rely on . A few years later , in September 1983 , Stallman released GNU , an operating system designed to be a free alternative to one of the dominant operating systems at the time : Unix . Stallman envisioned GNU as a means to fight back against the proprietary mechanisms , like copyright , that were beginning to flood the tech industry . The free-software movement was born from one frustrated engineer ’ s simple , rigid philosophy : for the good of the world , all code should be open , without restriction or commercial intervention . Forty years later , tech companies are making billions on proprietary software , and much of the technology around us—from ChatGPT to smart thermostats—is inscrutable to everyday consumers . In this environment , Stallman ’ s movement may look like a failed values experiment crushed under the weight of commercial reality . But in 2023 , the free and open-source software movement is not only alive and well ; it has become a keystone of the tech industry . Today , 96 % of all code bases incorporate open-source software . GitHub , the biggest platform for the open-source community , is used by more than 100 million developers worldwide . The Biden administration ’ s Securing Open Source Software Act of 2022 publicly recognized open-source software as critical economic and security infrastructure . Even AWS , Amazon ’ s money-making cloud arm , supports the development and maintenance of open-source software ; it committed its portfolio of patents to an open use community in December of last year . Over the last two years , while public trust in private technology companies has plummeted , organizations including Google , Spotify , the Ford Foundation , Bloomberg , and NASA have established new funding for open-source projects and their counterparts in open science efforts—an extension of the same values applied to scientific research . The fact that open-source software is now so essential means that long-standing leadership and diversity issues in the movement have become everyone ’ s problems . Many open-source projects began with “ benevolent dictator for life ” ( BDFL ) models of governance , where original founders hang on to leadership for years—and not always responsibly . Stallman and some other BDFLs have been criticized by their own communities for misogynistic or even abusive behavior . Stallman stepped down as president of the Free Software Foundation in 2019 ( although he returned to the board two years later ) . Overall , open-source participants are still overwhelmingly white , male , and located in the Global North . Projects can be overly influenced by corporate interests . Meanwhile , the people doing the hard work of keeping critical code healthy are not consistently funded . In fact , many major open-source projects still operate almost completely on volunteer steam . Greater access to the code behind generative models is fueling innovation . But if top companies get spooked , they could close up shop . Challenges notwithstanding , there ’ s plenty to celebrate in 2023 , the year of GNU ’ s 40th birthday . The modern open-source movement persists as a collaborative haven for transparent ways of working within a highly fragmented and competitive industry . Selena Deckelmann , chief product and technology officer at the Wikimedia Foundation , says the power of open source lies in its “ idea that people anywhere can collaborate together on software , but also on many [ more ] things. ” She points out that tools to put this philosophy into action , like mailing lists , online chat , and open version control systems , were pioneered in open-source communities and have been adopted as standard practice by the wider tech industry . “ We found a way for people from all over the world , regardless of background , to find a common cause to collaborate with each other , ” says Kelsey Hightower , an early contributor to Kubernetes , an open-source system for automating app deployment and management , who recently retired from his role as a distinguished engineer at Google Cloud . “ I think that is pretty unique to the world of open source. ” The 2010s backlash against tech ’ s unfettered growth , and the recent AI boom , have focused a spotlight on the open-source movement ’ s ideas about who has the right to use other people ’ s information online and who benefits from technology . Clement Delangue , CEO of the open-source AI company Hugging Face , which was recently valued at $ 4 billion , testified before Congress in June of 2023 that “ ethical openness ” in AI development could help make organizations more compliant and transparent , while allowing researchers beyond a few large tech companies access to technology and progress . “ We ’ re in a unique cultural moment , ” says Danielle Robinson , executive director of Code for Science and Society , a nonprofit that provides funding and support for public-interest technology . “ People are more aware than ever of how capitalism has been influencing what technologies get built , and whether you have a choice to interact with it. ” Once again , free and open-source software have become a natural home for the debate about how technology should be . The early days of the free-software movement were fraught with arguments about the meaning of “ free. ” Stallman and the Free Software Foundation ( FSF ) , founded in 1985 , held firm to the idea of four freedoms : people should be allowed to run a program for any purpose , study how it works from the source code and change it to meet their needs , redistribute copies , and distribute modified versions too . Stallman saw free software as an essential right : “ Free as in free speech , not free beer , ” as his apocryphal slogan goes . He created the GNU General Public License , what ’ s known as a “ copyleft ” license , to ensure that the four freedoms were protected in code built with GNU . Linus Torvalds , the Finnish engineer who in 1991 created the now ubiquitous Unix alternative Linux , didn ’ t buy into this dogma . Torvalds and others , including Microsoft ’ s Bill Gates , believed that the culture of open exchange among engineers could coexist with commerce , and that more-restrictive licenses could forge a path toward both financial sustainability and protections for software creators and users . It was during a 1998 strategic meeting of free-software advocates—which notably did not include Stallman—that this pragmatic approach became known as “ open source. ” ( The term was coined and introduced to the group not by an engineer , but by the futurist and nano­technology scholar Christine Peterson . ) Karen Sandler , executive director of the Software Freedom Conservancy , a nonprofit that advocates for free and open-source software , saw firsthand how the culture shifted from orthodoxy to a big-tent approach with room for for-profit entities when she worked as general counsel at the Software Freedom Law Center in the early 2000s . “ The people who were ideological—some of them stayed quite ideological . But many of them realized , oh , wait a minute , we can get jobs doing this . We can do well by doing good , ” Sandler remembers . By leveraging the jobs and support that early tech companies were offering , open-source contributors could sustain their efforts and even make a living doing what they believed in . In that manner , companies using and contributing to free and open software could expand the community beyond volunteer enthusiasts and improve the work itself . “ How could we ever make it better if it ’ s just a few radical people ? ” Sandler says . As the tech industry grew around private companies like Sun Microsystems , IBM , Microsoft , and Apple in the late ’ 90s and early ’ 00s , new open-source projects sprang up , and established ones grew roots . Apache emerged as an open-source web server in 1995 . Red Hat , a company offering enterprise companies support for open-source software like Linux , went public in 1999 . GitHub , a platform originally created to support version control for open-source projects , launched in 2008 , the same year that Google released Android , the first open-source phone operating system . The more pragmatic definition of the concept came to dominate the field . Meanwhile , Stallman ’ s original philosophy persisted among dedicated groups of believers—where it still lives today through nonprofits like FSF , which only uses and advocates for software that protects the four freedoms . “ If a company only ends up just sharing , and nothing more , I think that should be celebrated. ” As open-source software spread , a bifurcation of the tech stack became standard practice , with open-source code as the support structure for proprietary work . Free and open-source software often served in the underlying foundation or back-end architecture of a product , while companies vigorously pursued and defended copyrights on the user-facing layers . Some estimate that Amazon ’ s 1999 patent on its one-click buying process was worth $ 2.4 billion per year to the company until it expired . It relied on Java , an open-source programming language , and other open-source software and tooling to build and maintain it . Today , corporations not only depend on open-source software but play an enormous role in funding and developing open-source projects : Kubernetes ( initially launched and maintained at Google ) and Meta ’ s React are both robust sets of software that began as internal solutions freely shared with the larger technology community . But some people , like the Software Freedom Conservancy ’ s Karen Sandler , identify an ongoing conflict between profit-­driven corporations and the public interest . “ Companies have become so savvy and educated with respect to open-source software that they use a ton of it . That ’ s good , ” says Sandler . At the same time , they profit from their proprietary work—which they sometimes attempt to pass off as open too , a practice the scholar and organizer Michelle Thorne dubbed “ openwashing ” in 2009 . For Sandler , if companies don ’ t also make efforts to support user and creator rights , they ’ re not pushing forward the free and open-source ethos . And she says for the most part , that ’ s indeed not happening : “ They ’ re not interested in giving the public any appreciable rights to their software. ” Others , including Kelsey Hightower , are more sanguine about corporate involvement . “ If a company only ends up just sharing , and nothing more , I think that should be celebrated , ” he says . “ Then if for the next two years you allow your paid employees to work on it , maintaining the bugs and issues , but then down the road it ’ s no longer a priority and you choose to step back , I think we should thank [ the company ] for those years of contributions. ” In stark contrast , FSF , now in its 38th year , holds firm to its original ideals and opposes any product or company that does not support the ability for users to view , modify , and redistribute code . The group today runs public action campaigns like “ End Software Patents , ” publishing articles and submitting amicus briefs advocating the end of patents on software . The foundation ’ s executive director , Zoë Kooyman , hopes to continue pushing the conversation toward freedom rather than commercial concerns . “ Every belief system or form of advocacy needs a far end , ” she says . “ That ’ s the only way to be able to drive the needle . [ At FSF ] , we are that far end of the spectrum , and we take that role very seriously. ” Forty years on from the release of GNU , there is no singular open-source community , “ any more than there is an ‘ urban community , ’ ” as researcher and engineer Nadia Asparouhova ( formerly Eghbal ) writes in her 2020 book Working in Public : The Making and Maintenance of Open Source Software . There ’ s no singular definition , either . The Open Source Initiative ( OSI ) was founded in 1998 to steward the meaning of the phrase , but not all modern open-source projects adhere to the 10 specific criteria OSI laid out , and other definitions appear across communities . Scale , technology , social norms , and funding also range widely from project to project and community to community . For example , Kubernetes has a robust , organized community of tens of thousands of contributors and years of Google investment . Salmon is a niche open-source bioinformatics research tool with fewer than 50 contributors , supported by grants . OpenSSL , which encrypts an estimated 66 % of the web , is currently maintained by 18 engineers compensated through donations and elective corporate contracts . The major discussions now are more about people than technology : What does healthy and diverse collaboration look like ? How can those who support the code get what they need to continue the work ? “ How do you include a voice for all the people affected by the technology you build ? ” asks James Vasile , an open-source consultant and strategist who sits on the board of the Electronic Frontier Foundation . “ These are big questions . We ’ ve never grappled with them before . No one was working on this 20 years ago , because that just wasn ’ t part of the scene . Now it is , and we [ in the open-source community ] have the chance to consider these questions. ” “ We need designers , ethnographers , social and cultural experts . We need everyone to be playing a role in open source. ” “ Free as in puppy , ” a phrase that can be traced back to 2006 , has emerged as a valuable definition of “ free ” for modern open-source projects—one that speaks to the responsibilities of creators and users to each other and the software , in addition to their rights . Puppies need food and care to survive ; open-source code needs funding and “ maintainers , ” individuals who consistently respond to requests and feedback from a community , fix bugs , and manage the growth and scope of a project . Many open-source projects have become too big , complicated , or important to be governed by one person or even a small group of like-minded individuals . And open-source contributors have their own needs and concerns , too . A person who ’ s good at building may not be good at maintaining ; someone who creates a project may not want to or be able to run it indefinitely . In 2018 , for instance , Guido van Rossum , the creator of the open-source programming language Python , stepped down from leadership after almost 30 years , exhausted from the demands of the mostly uncompensated role . “ I ’ m tired , ” he wrote in his resignation message to the community , “ and need a very long break. ” Supporting the people who create , maintain , and use free and open-source software requires new roles and perspectives . Whereas the movement in its early days was populated almost exclusively by engineers communicating across message boards and through code , today ’ s open-source projects invite participation from new disciplines to handle logistical work like growth and advocacy , as well as efforts toward greater inclusion and belonging . “ We ’ ve shifted from open source being about just the technical stuff to the broader set of expertise and perspectives that are required to make effective open-source projects , ” says Michael Brennan , senior program officer with the Technology and Society program at the Ford Foundation , which funds research into open internet issues . “ We need designers , ethnographers , social and cultural experts . We need everyone to be playing a role in open source if it ’ s going to be effective and meet the needs of the people around the world. ” One powerful source of support arrived in 2008 with the launch of GitHub . While it began as a version control tool , it has grown into a suite of services , standards , and systems that is now the “ highway system ” for most open-source development , as Asparouhova puts it in Working in Public . GitHub helped lower the barrier to entry , drawing wider contribution and spreading best practices such as community codes of conduct . But its success has also given a single platform vast influence over communities dedicated to decentralized collaboration . Demetris Cheatham , until recently GitHub ’ s senior director for diversity and inclusion strategy , took that responsibility very seriously . To find out where things stood , the company partnered with the Linux Foundation in 2021 on a survey and resulting report on diversity and inclusion within open source . The data showed that despite a pervasive ethos of collaboration and openness ( more than 80 % of the respondents reported feeling welcome ) , communities are dominated by contributors who are straight , white , male , and from the Global North . In response , Cheatham , who is now the company ’ s chief of staff , focused on ways to broaden access and promote a sense of belonging . GitHub launched All In for Students , a mentorship and education program with 30 students drawn primarily from historically Black colleges and universities . In its second year , the program expanded to more than 400 students . Representation has not been the only stumbling block to a more equitable open-source ecosystem . The Linux Foundation report showed that only 14 % of open-source contributors surveyed were getting paid for their work . While this volunteer spirit aligns with the original vision of free software as a commerce-free exchange of ideas , free labor presents a major access issue . Additionally , 30 % of respondents in the survey did not trust that codes of conduct would be enforced—suggesting they did not feel they could count on a respectful working environment . “ We ’ re at another inflection point now where codes of conduct are great , but they ’ re only a tool , ” says Code for Science and Society ’ s Danielle Robinson . “ I ’ m starting to see larger cultural shifts toward rethinking extractive processes that have been a part of open source for a long time. ” Getting maintainers paid and connecting contributors with support are now key to opening up open source to a more diverse group of participants . With that in mind , this year GitHub established resources specifically for maintainers , including workshops and a hub of DEI tools . And in May , the platform launched a new project to connect large , well-resourced open-source communities with smaller ones that need help . Cheatham says it ’ s crucial to the success of any of these programs that they be shared for free with the broader community . “ We ’ re not inventing anything new at all . We ’ re just applying open-source principles to diversity , equity , and inclusion , ” she says . GitHub ’ s influence over open source may be large , but it is not the only group working to get maintainers paid and expand open-source participation . The Software Freedom Conservancy ’ s Outreachy diversity initiative offers paid internships ; as of 2019 , 92 % of past Outreachy interns have identified as women and 64 % as people of color . Open-source fundraising platforms like Open Collective and Tidelift have also emerged to help maintainers tap into resources . The philanthropic world is stepping up too . The Ford Foundation , the Sloan Foundation , Omidyar Network , and the Chan Zuckerberg Initiative , as well as smaller organizations like Code for Science and Society , have all recently begun or expanded their efforts to support open-source research , contributors , and projects—including specific efforts promoting inclusion and diversity . Govind Shivkumar from Omidyar Network told MIT Technology Review that philanthropy is well positioned to establish funding architecture that could help prove out open-source projects , making them less risky prospects for future governmental funding . In fact , research supported by the Ford Foundation ’ s Digital Infrastructure Fund contributed to Germany ’ s recent creation of a national fund for open digital infrastructure . Momentum has also been building in the US . In 2016 the White House began requiring at least 20 % of government-­developed software to be open source . Last year ’ s Securing Open Source Software Act passed with bipartisan support , establishing a framework for attention and investment at the federal level toward making open-source software stronger and more secure . Open source contributes valuable practices and tools , but it may also offer a competitive advantage over proprietary efforts . A document leaked in May from Google argued that open-source communities had pushed , tested , integrated , and expanded the capabilities of large language models more thoroughly than private efforts could ’ ve accomplished on their own : “ Many of the new ideas [ in AI development ] are from ordinary people . The barrier to entry for training and experimentation has dropped from the total output of a major research organization to one person , an evening , and a beefy laptop. ” The recently articulated concept of Time till Open Source Alternative ( TTOSA ) —the time between the release of a proprietary product and an open-source equivalent—also speaks to this advantage . One researcher estimated the average TTOSA to be seven years but noted that the process has been speeding up thanks to easy-to-use services like GitHub . At the same time , much of our modern world now relies on underfunded and rapidly expanding digital infrastructure . There has long been an assumption within open source that bugs can be identified and solved quickly by the “ many eyes ” of a wide community—and indeed this can be true . But when open-source software affects millions of users and its maintenance is handled by handfuls of underpaid individuals , the weight can be too much for the system to bear . In 2021 , a security vulnerability in a popular open-source Apache library exposed an estimated hundreds of millions of devices to hacking attacks . Major players across the industry were affected , and large parts of the internet went down . The vulnerability ’ s lasting impact is hard to quantify even now . Volunteer-run projects like Log4J keep the internet running . The result is unsustainable burnout , and a national security risk when they go wrong . Other risks emerge from open-source development without the support of ethical guardrails . Proprietary efforts like Google ’ s Bard and OpenAI ’ s ChatGPT have demonstrated that AI can perpetuate existing biases and may even cause harm—while also not providing the transparency that could help a larger community audit the technology , improve it , and learn from its mistakes . But allowing anyone to use , modify , and distribute AI models and technology could accelerate their misuse . One week after Meta began granting access to its AI model LLaMA , the package leaked onto 4chan , a platform known for spreading misinformation . LLaMA 2 , a new model released in July , is fully open to the public , but the company has not disclosed its training data as is typical in open-source projects—putting it somewhere in between open and closed by some definitions , but decidedly not open by OSI ’ s . ( OpenAI is reportedly working on an open-source model as well but has not made a formal announcement . ) “ There are always trade-offs in the decisions you make in technology , ” says Margaret Mitchell , chief ethics scientist at Hugging Face . “ I can ’ t just be wholeheartedly supportive of open source in all cases without any nuances or caveats. ” Mitchell and her team have been working on open-source tools to help communities safeguard their work , such as gating mechanisms to allow collaboration only at the project owner ’ s discretion , and “ model cards ” that detail a model ’ s potential biases and social impacts—information researchers and the public can take into consideration when choosing which models to work with . Open-source software has come a long way since its rebellious roots . But carrying it forward and making it into a movement that fully reflects the values of openness , reciprocity , and access will require careful consideration , financial and community investment , and the movement ’ s characteristic process of self-improvement through collaboration . As the modern world becomes more dispersed and diverse , the skill sets required to work asynchronously with different groups of people and technologies toward a common goal are only growing more essential . At this rate , 40 years from now technology might look more open than ever—and the world may be better for it . Rebecca Ackermann is a writer , designer , and artist based in San Francisco .","['donate', 'new', 'laser', 'printer', 'artificial', 'intelligence', 'lab', 'company', 'know', 'machine', 'ignite', 'revolution', 'printer', 'jam', 'accord', 'book', 'free', 'freedom', 'programmer', 'mit', 'try', 'dig', 'code', 'fix', 'expect', 'able', 'previous', 'printer', 'early', 'decade', 'software', 'development', 'generally', 'run', 'culture', 'open', 'access', 'free', 'exchange', 'engineer', 'dive', 'code', 'time', 'zone', 'institution', 'make', 'squash', 'bug', 'new', 'printer', 'run', 'inaccessible', 'proprietary', 'software', 'stallman', 'lock', 'enrage', 'violate', 'open', 'codesharing', 'system', 'come', 'rely', 'year', 'later', 'stallman', 'release', 'operating', 'system', 'design', 'free', 'alternative', 'dominant', 'operating', 'system', 'time', 'stallman', 'envision', 'means', 'fight', 'back', 'proprietary', 'mechanism', 'copyright', 'begin', 'flood', 'tech', 'industry', 'freesoftware', 'movement', 'bear', 'frustrated', 'engineer', 'simple', 'rigid', 'philosophy', 'good', 'world', 'code', 'open', 'restriction', 'commercial', 'intervention', 'year', 'later', 'tech', 'company', 'make', 'billion', 'proprietary', 'software', 'much', 'technology', 'chatgpt', 'smart', 'thermostat', 'inscrutable', 'everyday', 'consumer', 'environment', 'stallman', 'movement', 'look', 'fail', 'value', 'experiment', 'crush', 'weight', 'commercial', 'reality', 'free', 'opensource', 'software', 'movement', 'alive', 'well', 'become', 'keystone', 'tech', 'industry', 'today', 'code', 'basis', 'incorporate', 'opensource', 'software', 'big', 'platform', 'opensource', 'community', 'use', 'developer', 'worldwide', 'secure', 'open', 'source', 'software', 'act', 'publicly', 'recognize', 'opensource', 'software', 'critical', 'economic', 'security', 'infrastructure', 'even', 'moneymake', 'cloud', 'arm', 'support', 'development', 'maintenance', 'opensource', 'software', 'commit', 'portfolio', 'patent', 'open', 'use', 'community', 'last', 'year', 'last', 'year', 'public', 'trust', 'private', 'technology', 'company', 'plummet', 'organization', 'include', 'spotify', 'establish', 'new', 'funding', 'opensource', 'project', 'counterpart', 'open', 'science', 'effort', 'extension', 'value', 'apply', 'scientific', 'research', 'fact', 'opensource', 'software', 'essential', 'mean', 'longstanding', 'leadership', 'diversity', 'issue', 'movement', 'become', 'problem', 'many', 'opensource', 'project', 'begin', 'benevolent', 'dictator', 'life', 'bdfl', 'model', 'governance', 'original', 'founder', 'hang', 'leadership', 'year', 'always', 'responsibly', 'stallman', 'criticize', 'community', 'misogynistic', 'even', 'abusive', 'behavior', 'stallman', 'step', 'president', 'free', 'software', 'foundation', 'return', 'board', 'year', 'later', 'overall', 'opensource', 'participant', 'still', 'overwhelmingly', 'white', 'male', 'locate', 'global', 'north', 'project', 'overly', 'influence', 'corporate', 'interest', 'meanwhile', 'people', 'hard', 'work', 'keep', 'critical', 'code', 'healthy', 'consistently', 'fund', 'fact', 'many', 'major', 'opensource', 'project', 'still', 'operate', 'almost', 'completely', 'volunteer', 'steam', 'great', 'access', 'code', 'generative', 'model', 'fuel', 'innovation', 'top', 'company', 'spook', 'close', 'shop', 'challenge', 'plenty', 'celebrate', 'year', '40th', 'birthday', 'modern', 'opensource', 'movement', 'persist', 'collaborative', 'transparent', 'way', 'work', 'highly', 'fragmented', 'competitive', 'industry', 'selena', 'deckelmann', 'chief', 'product', 'technology', 'officer', 'say', 'power', 'open', 'source', 'lie', 'idea', 'people', 'anywhere', 'collaborate', 'together', 'software', 'also', 'many', 'thing', 'point', 'tool', 'put', 'philosophy', 'action', 'mailing', 'list', 'online', 'chat', 'open', 'version', 'control', 'system', 'pioneer', 'opensource', 'community', 'adopt', 'standard', 'practice', 'wide', 'tech', 'industry', 'find', 'way', 'people', 'world', 'regardless', 'background', 'find', 'common', 'cause', 'collaborate', 'say', 'early', 'contributor', 'kubernete', 'opensource', 'system', 'automate', 'app', 'deployment', 'management', 'recently', 'retire', 'role', 'distinguished', 'engineer', 'think', 'pretty', 'unique', 'world', 'open', 'source', 'backlash', 'tech', 'unfettered', 'growth', 'recent', 'ai', 'boom', 'focus', 'spotlight', 'idea', 'right', 'use', 'people', 'information', 'online', 'benefit', 'technology', 'clement', 'delangue', 'ceo', 'opensource', 'ai', 'company', 'hug', 'face', 'recently', 'value', 'testify', 'ethical', 'openness', 'development', 'help', 'make', 'organization', 'compliant', 'transparent', 'allow', 'researcher', 'large', 'tech', 'company', 'access', 'technology', 'progress', 'unique', 'cultural', 'moment', 'say', 'executive', 'director', 'code', 'science', 'society', 'nonprofit', 'provide', 'funding', 'support', 'publicinterest', 'technology', 'people', 'aware', 'ever', 'capitalism', 'influence', 'technology', 'build', 'choice', 'interact', 'free', 'opensource', 'software', 'become', 'natural', 'home', 'debate', 'technology', 'early', 'day', 'freesoftware', 'movement', 'fraught', 'argument', 'meaning', 'free', 'stallman', 'free', 'software', 'foundation', 'found', 'hold', 'firm', 'idea', 'freedom', 'people', 'allow', 'run', 'program', 'purpose', 'study', 'work', 'source', 'code', 'change', 'meet', 'need', 'redistribute', 'copy', 'distribute', 'modify', 'version', 'stallman', 'see', 'free', 'software', 'essential', 'right', 'free', 'free', 'speech', 'free', 'beer', 'apocryphal', 'slogan', 'go', 'create', 'general', 'public', 'license', 'know', 'copyleft', 'license', 'ensure', 'freedom', 'protect', 'code', 'build', 'linus', 'torvald', 'finnish', 'engineer', 'create', 'ubiquitous', 'unix', 'alternative', 'buy', 'dogma', 'torvald', 'include', 'bill', 'gate', 'believe', 'culture', 'open', 'exchange', 'engineer', 'coexist', 'commerce', 'morerestrictive', 'license', 'forge', 'path', 'financial', 'sustainability', 'protection', 'software', 'creator', 'user', 'strategic', 'meeting', 'freesoftware', 'advocate', 'notably', 'include', 'stallman', 'pragmatic', 'approach', 'know', 'open', 'source', 'term', 'coin', 'introduce', 'group', 'engineer', 'futurist', 'nano\xadtechnology', 'scholar', 'executive', 'director', 'software', 'freedom', 'conservancy', 'nonprofit', 'advocate', 'free', 'opensource', 'software', 'see', 'firsthand', 'culture', 'shift', 'bigtent', 'approach', 'room', 'forprofit', 'entity', 'work', 'general', 'counsel', 'software', 'freedom', 'law', 'center', 'early', 'people', 'ideological', 'stay', 'quite', 'ideological', 'many', 'realize', 'wait', 'minute', 'get', 'job', 'well', 'good', 'sandler', 'remember', 'leverage', 'job', 'support', 'early', 'tech', 'company', 'offer', 'opensource', 'contributor', 'sustain', 'effort', 'even', 'make', 'living', 'believe', 'manner', 'company', 'use', 'contribute', 'free', 'open', 'software', 'expand', 'community', 'volunteer', 'enthusiast', 'improve', 'work', 'ever', 'make', 'well', 'radical', 'people', 'sandler', 'say', 'tech', 'industry', 'grow', 'private', 'company', 'apple', 'late', '90', 'early', '00', 'new', 'opensource', 'project', 'spring', 'establish', 'one', 'grow', 'root', 'apache', 'emerge', 'opensource', 'web', 'server', 'red', 'hat', 'company', 'offer', 'enterprise', 'company', 'support', 'opensource', 'software', 'go', 'public', 'platform', 'originally', 'create', 'support', 'version', 'control', 'opensource', 'project', 'launch', 'year', 'release', 'first', 'opensource', 'phone', 'operating', 'system', 'pragmatic', 'definition', 'concept', 'come', 'dominate', 'field', 'meanwhile', 'stallman', 'original', 'philosophy', 'persist', 'dedicated', 'group', 'believer', 'still', 'live', 'today', 'nonprofit', 'use', 'advocate', 'software', 'protect', 'freedom', 'company', 'end', 'share', 'think', 'celebrate', 'opensource', 'software', 'spread', 'bifurcation', 'tech', 'stack', 'become', 'standard', 'practice', 'opensource', 'code', 'support', 'structure', 'proprietary', 'work', 'free', 'opensource', 'software', 'often', 'serve', 'underlie', 'foundation', 'backend', 'architecture', 'product', 'company', 'vigorously', 'pursue', 'defend', 'copyright', 'userfacing', 'layer', 'estimate', 'patent', 'oneclick', 'buying', 'process', 'worth', 'year', 'company', 'expire', 'rely', 'java', 'opensource', 'programming', 'language', 'opensource', 'software', 'tool', 'build', 'maintain', 'today', 'corporation', 'depend', 'opensource', 'software', 'play', 'enormous', 'role', 'funding', 'develop', 'opensource', 'project', 'kubernete', 'initially', 'launch', 'maintain', 'meta', 'react', 'robust', 'set', 'software', 'begin', 'internal', 'solution', 'freely', 'share', 'large', 'technology', 'community', 'people', 'software', 'freedom', 'conservancy', 'identify', 'ongoing', 'conflict', 'profit\xaddriven', 'corporation', 'public', 'interest', 'company', 'become', 'savvy', 'educate', 'respect', 'opensource', 'software', 'use', 'ton', 'good', 'say', 'sandler', 'time', 'profit', 'proprietary', 'work', 'sometimes', 'attempt', 'pass', 'open', 'practice', 'scholar', 'organizer', 'michelle', 'thorne', 'dub', 'openwashe', 'sandler', 'company', 'also', 'make', 'effort', 'support', 'user', 'creator', 'right', 'push', 'free', 'opensource', 'ethos', 'say', 'part', 'indeed', 'happen', 'interested', 'give', 'public', 'appreciable', 'right', 'software', 'include', 'sanguine', 'corporate', 'involvement', 'company', 'end', 'share', 'think', 'celebrate', 'say', 'next', 'year', 'allow', 'pay', 'employee', 'work', 'maintain', 'bug', 'issue', 'road', 'long', 'priority', 'choose', 'step', 'back', 'think', 'thank', 'company', 'year', 'contribution', 'stark', 'contrast', '38th', 'year', 'hold', 'firm', 'original', 'ideal', 'oppose', 'product', 'company', 'support', 'ability', 'user', 'view', 'modify', 'redistribute', 'code', 'group', 'today', 'run', 'public', 'action', 'campaign', 'end', 'software', 'patent', 'publish', 'article', 'submit', 'amicus', 'brief', 'advocate', 'end', 'patent', 'software', 'foundation', 'executive', 'director', 'kooyman', 'hope', 'continue', 'push', 'conversation', 'freedom', 'rather', 'commercial', 'concern', 'belief', 'system', 'form', 'advocacy', 'need', 'far', 'end', 'say', 'way', 'able', 'drive', 'needle', 'far', 'end', 'spectrum', 'take', 'role', 'seriously', 'year', 'release', 'singular', 'opensource', 'community', 'urban', 'community', 'researcher', 'engineer', 'nadia', 'formerly', 'eghbal', 'write', 'book', 'work', 'public', 'making', 'maintenance', 'open', 'source', 'software', 'singular', 'definition', 'open', 'source', 'initiative', 'osi', 'found', 'steward', 'meaning', 'phrase', 'modern', 'opensource', 'project', 'adhere', 'specific', 'criterion', 'osi', 'lay', 'definition', 'appear', 'community', 'scale', 'technology', 'social', 'norm', 'funding', 'also', 'range', 'widely', 'project', 'project', 'community', 'community', 'example', 'kubernete', 'robust', 'organize', 'community', 'ten', 'thousand', 'contributor', 'year', 'investment', 'salmon', 'niche', 'opensource', 'bioinformatic', 'research', 'tool', 'contributor', 'support', 'grant', 'openssl', 'encrypt', 'estimated', 'web', 'currently', 'maintain', 'engineer', 'compensate', 'donation', 'elective', 'corporate', 'contract', 'major', 'discussion', 'people', 'technology', 'healthy', 'diverse', 'collaboration', 'look', 'support', 'code', 'get', 'need', 'continue', 'work', 'include', 'voice', 'people', 'affect', 'technology', 'build', 'ask', 'opensource', 'consultant', 'strategist', 'sit', 'board', 'electronic', 'frontier', 'foundation', 'big', 'question', 'never', 'grapple', 'one', 'work', 'year', 'ago', 'part', 'scene', 'opensource', 'community', 'chance', 'consider', 'question', 'need', 'designer', 'ethnographer', 'social', 'cultural', 'expert', 'need', 'play', 'role', 'open', 'source', 'free', 'puppy', 'phrase', 'trace', 'back', 'emerge', 'valuable', 'definition', 'free', 'modern', 'opensource', 'project', 'one', 'speak', 'responsibility', 'creator', 'user', 'software', 'addition', 'right', 'puppy', 'need', 'food', 'care', 'survive', 'opensource', 'code', 'need', 'funding', 'maintainer', 'individual', 'consistently', 'respond', 'request', 'feedback', 'community', 'fix', 'bug', 'manage', 'growth', 'scope', 'project', 'many', 'opensource', 'project', 'become', 'big', 'complicated', 'important', 'govern', 'person', 'even', 'small', 'group', 'likeminded', 'individual', 'opensource', 'contributor', 'need', 'concern', 'person', 'good', 'building', 'good', 'maintain', 'create', 'project', 'want', 'able', 'run', 'indefinitely', 'instance', 'creator', 'opensource', 'programming', 'language', 'python', 'step', 'leadership', 'almost', 'year', 'exhausted', 'demand', 'mostly', 'uncompensated', 'role', 'tired', 'write', 'resignation', 'message', 'community', 'need', 'long', 'break', 'support', 'people', 'create', 'maintain', 'use', 'free', 'opensource', 'software', 'require', 'new', 'role', 'perspective', 'movement', 'early', 'day', 'populate', 'almost', 'exclusively', 'engineer', 'communicate', 'message', 'board', 'code', 'today', 'opensource', 'project', 'invite', 'participation', 'new', 'discipline', 'handle', 'logistical', 'work', 'growth', 'advocacy', 'well', 'effort', 'great', 'inclusion', 'belong', 'shift', 'open', 'source', 'technical', 'stuff', 'broad', 'set', 'expertise', 'perspective', 'require', 'make', 'effective', 'opensource', 'project', 'say', 'senior', 'program', 'officer', 'technology', 'society', 'program', 'fund', 'research', 'open', 'internet', 'issue', 'need', 'designer', 'ethnographer', 'social', 'cultural', 'expert', 'need', 'play', 'role', 'open', 'source', 'go', 'effective', 'meet', 'need', 'people', 'world', 'powerful', 'source', 'support', 'arrive', 'launch', 'begin', 'version', 'control', 'tool', 'grow', 'suite', 'service', 'standard', 'system', 'highway', 'system', 'opensource', 'development', 'put', 'work', 'public', 'help', 'lower', 'barrier', 'entry', 'draw', 'wide', 'contribution', 'spread', 'good', 'practice', 'community', 'code', 'conduct', 'success', 'also', 'give', 'single', 'platform', 'vast', 'influence', 'community', 'dedicate', 'decentralized', 'collaboration', 'demetris', 'recently', 'senior', 'director', 'diversity', 'inclusion', 'strategy', 'take', 'responsibility', 'seriously', 'find', 'thing', 'stand', 'company', 'partner', 'survey', 'result', 'report', 'diversity', 'inclusion', 'open', 'source', 'datum', 'show', 'pervasive', 'ethos', 'collaboration', 'openness', 'respondent', 'report', 'feel', 'welcome', 'community', 'dominate', 'contributor', 'straight', 'white', 'male', 'global', 'north', 'response', 'company', 'chief', 'staff', 'focus', 'way', 'broaden', 'access', 'promote', 'sense', 'belong', 'launch', 'student', 'mentorship', 'education', 'program', 'student', 'draw', 'primarily', 'historically', 'black', 'college', 'university', 'second', 'year', 'program', 'expand', 'student', 'representation', 'stumble', 'block', 'equitable', 'opensource', 'ecosystem', 'report', 'show', 'opensource', 'contributor', 'survey', 'pay', 'work', 'volunteer', 'spirit', 'align', 'original', 'vision', 'free', 'software', 'commercefree', 'exchange', 'idea', 'free', 'labor', 'present', 'major', 'access', 'issue', 'additionally', 'respondent', 'survey', 'trust', 'code', 'conduct', 'enforce', 'suggest', 'feel', 'count', 'respectful', 'work', 'environment', 'inflection', 'point', 'code', 'conduct', 'great', 'tool', 'say', 'code', 'science', 'society', 'start', 'see', 'large', 'cultural', 'shift', 'rethink', 'extractive', 'process', 'part', 'open', 'source', 'long', 'time', 'get', 'maintainer', 'pay', 'connect', 'contributor', 'support', 'key', 'open', 'open', 'source', 'diverse', 'group', 'participant', 'mind', 'year', 'establish', 'resource', 'specifically', 'maintainer', 'include', 'workshop', 'hub', 'dei', 'tool', 'platform', 'launch', 'new', 'project', 'connect', 'large', 'wellresourced', 'opensource', 'community', 'small', 'one', 'need', 'help', 'say', 'crucial', 'success', 'program', 'share', 'free', 'broad', 'community', 'invent', 'new', 'apply', 'opensource', 'principle', 'diversity', 'equity', 'inclusion', 'say', 'influence', 'open', 'source', 'large', 'group', 'work', 'get', 'maintainer', 'pay', 'expand', 'opensource', 'participation', 'software', 'freedom', 'conservancy', 'outreachy', 'diversity', 'initiative', 'offer', 'pay', 'internship', 'past', 'outreachy', 'intern', 'identify', 'woman', 'people', 'color', 'opensource', 'fundraising', 'platform', 'open', 'collective', 'tidelift', 'also', 'emerge', 'help', 'maintainer', 'tap', 'resource', 'philanthropic', 'world', 'step', 'foundation', 'network', 'initiative', 'well', 'small', 'organization', 'code', 'science', 'society', 'recently', 'begin', 'expand', 'effort', 'support', 'opensource', 'research', 'contributor', 'project', 'include', 'specific', 'effort', 'promote', 'inclusion', 'diversity', 'govind', 'tell', 'technology', 'review', 'philanthropy', 'well', 'positioned', 'establish', 'funding', 'architecture', 'help', 'prove', 'opensource', 'project', 'make', 'less', 'risky', 'prospect', 'future', 'governmental', 'funding', 'fact', 'research', 'support', 'digital', 'infrastructure', 'fund', 'contribute', 'recent', 'creation', 'national', 'fund', 'open', 'digital', 'infrastructure', 'momentum', 'also', 'build', 'begin', 'require', 'least', 'government\xaddeveloped', 'software', 'open', 'source', 'last', 'year', 'secure', 'open', 'source', 'software', 'act', 'pass', 'bipartisan', 'support', 'establish', 'framework', 'attention', 'investment', 'federal', 'level', 'make', 'opensource', 'software', 'strong', 'secure', 'open', 'source', 'contribute', 'valuable', 'practice', 'tool', 'also', 'offer', 'competitive', 'advantage', 'proprietary', 'effort', 'document', 'leak', 'argue', 'opensource', 'community', 'test', 'integrate', 'expand', 'capability', 'large', 'language', 'model', 'thoroughly', 'private', 'effort', 'accomplish', 'many', 'new', 'idea', 'development', 'ordinary', 'people', 'barrier', 'entry', 'training', 'experimentation', 'drop', 'total', 'output', 'major', 'research', 'organization', 'person', 'evening', 'beefy', 'laptop', 'recently', 'articulate', 'concept', 'time', 'open', 'source', 'alternative', 'time', 'release', 'proprietary', 'product', 'opensource', 'equivalent', 'also', 'speak', 'advantage', 'researcher', 'estimate', 'average', 'year', 'note', 'process', 'speed', 'thank', 'easytouse', 'service', 'time', 'much', 'modern', 'world', 'rely', 'underfunded', 'rapidly', 'expand', 'digital', 'infrastructure', 'long', 'assumption', 'open', 'source', 'bug', 'identify', 'solve', 'quickly', 'many', 'eye', 'wide', 'community', 'indeed', 'true', 'opensource', 'software', 'affect', 'million', 'user', 'maintenance', 'handle', 'handful', 'underpaid', 'individual', 'weight', 'much', 'system', 'bear', 'security', 'vulnerability', 'popular', 'opensource', 'apache', 'library', 'expose', 'estimate', 'hundred', 'million', 'device', 'hack', 'attack', 'major', 'player', 'industry', 'affect', 'large', 'part', 'internet', 'go', 'vulnerability', 'last', 'impact', 'hard', 'quantify', 'even', 'volunteerrun', 'project', 'log4j', 'keep', 'internet', 'run', 'result', 'unsustainable', 'burnout', 'national', 'security', 'risk', 'go', 'wrong', 'risk', 'emerge', 'opensource', 'development', 'support', 'ethical', 'guardrail', 'proprietary', 'effort', 'chatgpt', 'demonstrate', 'ai', 'perpetuate', 'exist', 'bias', 'even', 'cause', 'harm', 'also', 'provide', 'transparency', 'help', 'large', 'community', 'audit', 'technology', 'improve', 'learn', 'mistake', 'allow', 'use', 'modify', 'distribute', 'model', 'technology', 'accelerate', 'misuse', 'week', 'begin', 'grant', 'access', 'model', 'package', 'leak', 'platform', 'know', 'spread', 'misinformation', 'new', 'model', 'release', 'fully', 'open', 'public', 'company', 'disclose', 'training', 'datum', 'typical', 'opensource', 'project', 'put', 'somewhere', 'open', 'close', 'definition', 'decidedly', 'open', 'osi', 'reportedly', 'work', 'opensource', 'model', 'well', 'make', 'formal', 'announcement', 'always', 'tradeoff', 'decision', 'make', 'technology', 'say', 'chief', 'ethic', 'scientist', 'hug', 'face', 'wholeheartedly', 'supportive', 'open', 'source', 'case', 'nuance', 'caveat', 'mitchell', 'team', 'work', 'opensource', 'tool', 'help', 'community', 'safeguard', 'work', 'gate', 'mechanism', 'allow', 'collaboration', 'project', 'owner', 'discretion', 'model', 'card', 'detail', 'model', 'potential', 'bias', 'social', 'impact', 'information', 'researcher', 'public', 'take', 'consideration', 'choose', 'model', 'work', 'opensource', 'software', 'come', 'long', 'way', 'rebellious', 'root', 'carry', 'forward', 'make', 'movement', 'fully', 'reflect', 'value', 'openness', 'reciprocity', 'access', 'require', 'careful', 'consideration', 'financial', 'community', 'investment', 'movement', 'characteristic', 'process', 'selfimprovement', 'collaboration', 'modern', 'world', 'become', 'dispersed', 'diverse', 'skill', 'set', 'require', 'work', 'asynchronously', 'different', 'group', 'people', 'technology', 'common', 'goal', 'grow', 'essential', 'rate', 'year', 'technology', 'look', 'open', 'ever', 'world', 'well', 'rebecca', 'writer', 'designer', 'artist', 'base']","<p>Free and open software have transformed the tech industry. But we still have a lot to work out to make them healthy, equitable enterprises.</p>
"
The US-China chip war is still escalating,https://www.technologyreview.com/2023/07/12/1076156/us-china-tech-war-escalating/,2023-07-12,"This story first appeared in China Report, MIT Technology Review’s newsletter about technology developments in China. Sign up to receive it in your inbox every Tuesday. The temperature of the US-China tech conflict just keeps rising. Last week, the Chinese Ministry of Commerce announced a new export license system for gallium and germanium, two elements that are used to make computer chips, fiber optics, solar cells, and other tech devices. Most experts see the move as China’s most significant retaliation against the West’s semiconductor tech blockade, which expanded dramatically last October when the US limited the export to China of the most cutting-edge chips and the equipment capable of making them.  Earlier this year, China responded by putting Raytheon and Lockheed Martin on a list of unreliable entities and banned domestic companies from buying chips from the American company Micron. Yet none of these moves could rival the global impact of the gallium/germanium export control. By putting a chokehold on these two raw materials, China is signaling that it, in turn, can cause pain for the Western tech system and push other countries to rethink the curbs they put on China. The country aims to restrict the supply of gallium and germanium, two materials used in computer chips and other products. But experts say it won’t have the desired impact. But as I reported yesterday, China’s new export controls may not have much long-term impact. “Export control is not as effective if the technologies are available in other markets,” Sarah Bauerle Danzman, an associate professor of international studies at Indiana University Bloomington, told me. Since the technology to produce gallium and germanium is very mature, it won’t be too hard for mines in other countries to ramp up their production, although it will take time, investment, policy incentives, and maybe technological improvement to make the process more environmentally friendly. So what happens now? Half of 2023 is now behind us, and even though there have been a few diplomatic events showing the US-China relationship warming up, like trips to China made by US officials Antony Blinken and Janet Yellen, the tensions on the technological front are only getting worse. When the US instituted its chip-related export restrictions in October, it wasn’t clear how much of an impact they would have, because the US doesn’t control the entirety of the semiconductor supply chain. Analysts said one of the biggest outstanding questions was the extent to which the US could persuade its allies to join the blockade.  Now the US has managed to get the key players on board. In May, Japan announced that it is limiting the export of 23 types of equipment used in a variety of chipmaking processes. It even went further than the original US rules. The US limited the export of tools for making the most cutting-edge chips—those of the 14-nanometer generation and under. Japan’s restrictions extend to older, less-advanced chip generations (all the way to the 45-nanometer level), which has the Chinese semiconductor industry worried that production of basic chips used in everyday products, like cars, will also be affected. At the end of June, the Netherlands followed suit and announced that it will limit the export to China of deep ultraviolet (DUV) lithography machines used to pattern chips. That’s also an escalation of the previous rules, which since 2019 had only limited export of the most advanced extreme ultraviolet (EUV) lithography machines. These expanding restrictions likely prompted China to take a page from its enemies’ playbook by instituting the controls on gallium and germanium.  Yellen’s visit last week shows that this back-and-forth retaliation between China and the US-led bloc is not ending anytime soon. Both Yellen and the Chinese leaders expressed their concern at the meeting about the other side’s export controls, yet neither said anything about backing down.  If more aggressive actions are taken soon, we may see the tech war expand out of the semiconductor field to involve things like battery technologies. As I explained in my piece on Monday, that’s where China would have a larger advantage.  Do you believe the technological tensions between the US and China will worsen from here? Let me know your thoughts at zeyi@technologyreview.com. 1. Tesla is laying off some battery manufacturing workers in China as a result of the cutthroat electric-vehicle price competition in the country. (Bloomberg $) 2. China’s top EV maker, BYD, is building three new factories in Brazil to make batteries, EVs, and hybrid cars. They will be built at the location of an old Ford plant. (Quartz) 3. Shenzhen, the city often seen as the Silicon Valley of China, is facing population decline for the first time in decades. (Nikkei Asia $) 4. Five people were arrested by the Hong Kong police for involvement in creating an online shopping app to map out local businesses that support the pro-democracy movement. (Hong Kong Free Press) 5. There’s now an official app for learning how to do journalism in China—with online courses taught about the Marxist view of journalism, why the party needs to control the press, and how to be an “influencer-style journalist.” (China Media Project) 6. During her visit, Yellen sat down for dinner with six female Chinese economists. Then they were called traitors online. (Bloomberg $) 7. A new study says a rapidly growing number of scientists of Chinese descent have left the US since 2018, the year the US Department of Justice launched its “China Initiative.” (Inside Higher Ed). An investigation of the initiative by MIT Technology Review published in late 2021 showed it had shifted its focus from economic espionage to “research integrity.” The initiative was officially shut down in 2022. 8. Threads, the new Twitter competitor released by Meta, hit the top five on Apple’s China app store even though Chinese users have to access the platform with a VPN. (TechCrunch) On July 5, the famous Hong Kong singer CoCo Lee died by suicide after having battled depression for several years. The tragic incident again highlighted the importance of depression treatment, which is often inaccessible in China. As the Chinese publication Xin Kuai Bao reported, fewer than 10% of patients diagnosed with depression in China have received any kind of medical treatment.  But in recent years, as several patents for popular Western brand-name depression drugs have expired, Chinese pharmaceutical companies have ramped up their production of local generic alternatives. There’s also a fierce race to invent home-grown treatments. Last November, the first domestically designed depression drug was approved for sale in China, marking a new era for the industry. There are 17 more domestic treatments in trials right now. Every time high-profile US visitors come to China, Chinese social media always fixates on one thing: what they ate. Apparently, Janet Yellen is a fan of the wild mushrooms from China’s southwest border, which her group ordered four times in one dinner. The specific mushroom, called Jian Shou Qing in China, is also known for having psychedelic effects if not cooked properly. Now the restaurant is cashing in by offering Yellen’s dinner choices as a set, branded the “God of Money” menu, according to Quartz. ","This story first appeared in China Report , MIT Technology Review ’ s newsletter about technology developments in China . Sign up to receive it in your inbox every Tuesday . The temperature of the US-China tech conflict just keeps rising . Last week , the Chinese Ministry of Commerce announced a new export license system for gallium and germanium , two elements that are used to make computer chips , fiber optics , solar cells , and other tech devices . Most experts see the move as China ’ s most significant retaliation against the West ’ s semiconductor tech blockade , which expanded dramatically last October when the US limited the export to China of the most cutting-edge chips and the equipment capable of making them . Earlier this year , China responded by putting Raytheon and Lockheed Martin on a list of unreliable entities and banned domestic companies from buying chips from the American company Micron . Yet none of these moves could rival the global impact of the gallium/germanium export control . By putting a chokehold on these two raw materials , China is signaling that it , in turn , can cause pain for the Western tech system and push other countries to rethink the curbs they put on China . The country aims to restrict the supply of gallium and germanium , two materials used in computer chips and other products . But experts say it won ’ t have the desired impact . But as I reported yesterday , China ’ s new export controls may not have much long-term impact . “ Export control is not as effective if the technologies are available in other markets , ” Sarah Bauerle Danzman , an associate professor of international studies at Indiana University Bloomington , told me . Since the technology to produce gallium and germanium is very mature , it won ’ t be too hard for mines in other countries to ramp up their production , although it will take time , investment , policy incentives , and maybe technological improvement to make the process more environmentally friendly . So what happens now ? Half of 2023 is now behind us , and even though there have been a few diplomatic events showing the US-China relationship warming up , like trips to China made by US officials Antony Blinken and Janet Yellen , the tensions on the technological front are only getting worse . When the US instituted its chip-related export restrictions in October , it wasn ’ t clear how much of an impact they would have , because the US doesn ’ t control the entirety of the semiconductor supply chain . Analysts said one of the biggest outstanding questions was the extent to which the US could persuade its allies to join the blockade . Now the US has managed to get the key players on board . In May , Japan announced that it is limiting the export of 23 types of equipment used in a variety of chipmaking processes . It even went further than the original US rules . The US limited the export of tools for making the most cutting-edge chips—those of the 14-nanometer generation and under . Japan ’ s restrictions extend to older , less-advanced chip generations ( all the way to the 45-nanometer level ) , which has the Chinese semiconductor industry worried that production of basic chips used in everyday products , like cars , will also be affected . At the end of June , the Netherlands followed suit and announced that it will limit the export to China of deep ultraviolet ( DUV ) lithography machines used to pattern chips . That ’ s also an escalation of the previous rules , which since 2019 had only limited export of the most advanced extreme ultraviolet ( EUV ) lithography machines . These expanding restrictions likely prompted China to take a page from its enemies ’ playbook by instituting the controls on gallium and germanium . Yellen ’ s visit last week shows that this back-and-forth retaliation between China and the US-led bloc is not ending anytime soon . Both Yellen and the Chinese leaders expressed their concern at the meeting about the other side ’ s export controls , yet neither said anything about backing down . If more aggressive actions are taken soon , we may see the tech war expand out of the semiconductor field to involve things like battery technologies . As I explained in my piece on Monday , that ’ s where China would have a larger advantage . Do you believe the technological tensions between the US and China will worsen from here ? Let me know your thoughts at zeyi @ technologyreview.com . 1 . Tesla is laying off some battery manufacturing workers in China as a result of the cutthroat electric-vehicle price competition in the country . ( Bloomberg $ ) 2 . China ’ s top EV maker , BYD , is building three new factories in Brazil to make batteries , EVs , and hybrid cars . They will be built at the location of an old Ford plant . ( Quartz ) 3 . Shenzhen , the city often seen as the Silicon Valley of China , is facing population decline for the first time in decades . ( Nikkei Asia $ ) 4 . Five people were arrested by the Hong Kong police for involvement in creating an online shopping app to map out local businesses that support the pro-democracy movement . ( Hong Kong Free Press ) 5 . There ’ s now an official app for learning how to do journalism in China—with online courses taught about the Marxist view of journalism , why the party needs to control the press , and how to be an “ influencer-style journalist. ” ( China Media Project ) 6 . During her visit , Yellen sat down for dinner with six female Chinese economists . Then they were called traitors online . ( Bloomberg $ ) 7 . A new study says a rapidly growing number of scientists of Chinese descent have left the US since 2018 , the year the US Department of Justice launched its “ China Initiative. ” ( Inside Higher Ed ) . An investigation of the initiative by MIT Technology Review published in late 2021 showed it had shifted its focus from economic espionage to “ research integrity. ” The initiative was officially shut down in 2022 . 8 . Threads , the new Twitter competitor released by Meta , hit the top five on Apple ’ s China app store even though Chinese users have to access the platform with a VPN . ( TechCrunch ) On July 5 , the famous Hong Kong singer CoCo Lee died by suicide after having battled depression for several years . The tragic incident again highlighted the importance of depression treatment , which is often inaccessible in China . As the Chinese publication Xin Kuai Bao reported , fewer than 10 % of patients diagnosed with depression in China have received any kind of medical treatment . But in recent years , as several patents for popular Western brand-name depression drugs have expired , Chinese pharmaceutical companies have ramped up their production of local generic alternatives . There ’ s also a fierce race to invent home-grown treatments . Last November , the first domestically designed depression drug was approved for sale in China , marking a new era for the industry . There are 17 more domestic treatments in trials right now . Every time high-profile US visitors come to China , Chinese social media always fixates on one thing : what they ate . Apparently , Janet Yellen is a fan of the wild mushrooms from China ’ s southwest border , which her group ordered four times in one dinner . The specific mushroom , called Jian Shou Qing in China , is also known for having psychedelic effects if not cooked properly . Now the restaurant is cashing in by offering Yellen ’ s dinner choices as a set , branded the “ God of Money ” menu , according to Quartz .","['story', 'first', 'appear', 'mit', 'technology', 'review', 'newsletter', 'technology', 'development', 'sign', 'receive', 'inbox', 'temperature', 'conflict', 'keep', 'rise', 'last', 'week', 'announce', 'new', 'export', 'license', 'system', 'gallium', 'germanium', 'element', 'use', 'make', 'computer', 'chip', 'fiber', 'optic', 'solar', 'cell', 'tech', 'device', 'expert', 'see', 'move', 'significant', 'retaliation', 'semiconductor', 'tech', 'blockade', 'expand', 'dramatically', 'last', 'limit', 'export', 'cuttingedge', 'chip', 'equipment', 'capable', 'make', 'early', 'year', 'respond', 'put', 'raytheon', 'lockheed', 'list', 'unreliable', 'entity', 'ban', 'domestic', 'company', 'buy', 'chip', 'american', 'yet', 'none', 'move', 'rival', 'global', 'impact', 'galliumgermanium', 'export', 'control', 'put', 'chokehold', 'raw', 'material', 'signal', 'turn', 'cause', 'pain', 'western', 'tech', 'system', 'push', 'country', 'rethink', 'curb', 'put', 'country', 'aim', 'restrict', 'supply', 'gallium', 'germanium', 'material', 'use', 'computer', 'chip', 'product', 'expert', 'say', 'win', 'desire', 'impact', 'report', 'yesterday', 'new', 'export', 'control', 'much', 'longterm', 'impact', 'export', 'control', 'effective', 'technology', 'available', 'market', 'danzman', 'associate', 'professor', 'international', 'study', 'tell', 'technology', 'produce', 'gallium', 'germanium', 'mature', 'win', 'hard', 'mine', 'country', 'ramp', 'production', 'take', 'time', 'investment', 'policy', 'incentive', 'maybe', 'technological', 'improvement', 'make', 'process', 'environmentally', 'friendly', 'happen', 'half', 'even', 'diplomatic', 'event', 'show', 'relationship', 'warm', 'trip', 'make', 'official', 'yellen', 'tension', 'technological', 'front', 'get', 'bad', 'institute', 'chiprelated', 'export', 'restriction', 'clear', 'much', 'impact', 'control', 'entirety', 'semiconductor', 'supply', 'chain', 'analyst', 'say', 'big', 'outstanding', 'question', 'extent', 'persuade', 'ally', 'join', 'blockade', 'manage', 'get', 'key', 'player', 'board', 'announce', 'limit', 'export', 'type', 'equipment', 'use', 'variety', 'chipmake', 'process', 'even', 'go', 'far', 'original', 'rule', 'limit', 'export', 'tool', 'make', 'cuttingedge', 'chip', 'generation', 'restriction', 'extend', 'old', 'lessadvance', 'chip', 'generation', 'way', 'level', 'chinese', 'semiconductor', 'industry', 'worry', 'production', 'basic', 'chip', 'use', 'everyday', 'product', 'car', 'also', 'affect', 'end', 'follow', 'suit', 'announce', 'limit', 'export', 'deep', 'ultraviolet', 'lithography', 'machine', 'use', 'pattern', 'chip', 'also', 'escalation', 'previous', 'rule', 'limited', 'export', 'advanced', 'extreme', 'ultraviolet', 'lithography', 'machine', 'expand', 'restriction', 'likely', 'prompt', 'take', 'page', 'enemy', 'playbook', 'institute', 'control', 'gallium', 'germanium', 'yellen', 'visit', 'last', 'week', 'show', 'backandforth', 'retaliation', 'usled', 'bloc', 'end', 'anytime', 'soon', 'chinese', 'leader', 'express', 'concern', 'meeting', 'side', 'export', 'control', 'yet', 'say', 'back', 'aggressive', 'action', 'take', 'soon', 'see', 'tech', 'war', 'expand', 'semiconductor', 'field', 'involve', 'thing', 'battery', 'technology', 'explain', 'piece', 'large', 'advantage', 'believe', 'technological', 'tension', 'worsen', 'let', 'know', 'thought', 'tesla', 'lay', 'battery', 'manufacture', 'worker', 'result', 'cutthroat', 'electricvehicle', 'price', 'competition', 'country', 'top', 'ev', 'maker', 'build', 'new', 'factory', 'make', 'battery', 'evs', 'hybrid', 'car', 'build', 'location', 'old', 'ford', 'plant', 'quartz', 'shenzhen', 'city', 'often', 'see', 'silicon', 'face', 'population', 'decline', 'first', 'time', 'decade', 'people', 'arrest', 'police', 'involvement', 'create', 'online', 'shopping', 'app', 'map', 'local', 'business', 'support', 'prodemocracy', 'movement', 'official', 'app', 'learn', 'journalism', 'online', 'course', 'teach', 'marxist', 'view', 'journalism', 'party', 'need', 'control', 'press', 'influencerstyle', 'journalist', 'project', 'visit', 'sit', 'dinner', 'female', 'chinese', 'economist', 'call', 'traitor', 'online', 'new', 'study', 'say', 'rapidly', 'grow', 'number', 'scientist', 'chinese', 'descent', 'leave', 'year', 'justice', 'launch', 'inside', 'high', 'ed', 'investigation', 'initiative', 'mit', 'technology', 'review', 'publish', 'late', 'show', 'shift', 'focus', 'economic', 'espionage', 'research', 'integrity', 'initiative', 'officially', 'shut', 'thread', 'new', 'twitter', 'competitor', 'release', 'meta', 'hit', 'top', 'apple', 'store', 'even', 'chinese', 'user', 'access', 'platform', 'vpn', 'techcrunch', 'famous', 'singer', 'die', 'suicide', 'battle', 'depression', 'several', 'year', 'tragic', 'incident', 'highlight', 'importance', 'depression', 'treatment', 'often', 'inaccessible', 'chinese', 'publication', 'report', 'patient', 'diagnose', 'depression', 'receive', 'kind', 'medical', 'treatment', 'recent', 'year', 'several', 'patent', 'popular', 'western', 'brandname', 'depression', 'drug', 'expire', 'chinese', 'pharmaceutical', 'company', 'ramp', 'production', 'local', 'generic', 'alternative', 'also', 'fierce', 'race', 'invent', 'homegrown', 'treatment', 'last', 'first', 'domestically', 'design', 'depression', 'drug', 'approve', 'sale', 'mark', 'new', 'era', 'industry', 'domestic', 'treatment', 'trial', 'right', 'time', 'highprofile', 'visitor', 'come', 'chinese', 'social', 'medium', 'always', 'fixate', 'thing', 'eat', 'apparently', 'janet', 'fan', 'wild', 'mushroom', 'border', 'group', 'order', 'time', 'dinner', 'specific', 'mushroom', 'call', 'qe', 'also', 'know', 'psychedelic', 'effect', 'cook', 'properly', 'restaurant', 'cash', 'offer', 'dinner', 'choice', 'set', 'brand', 'money', 'menu', 'accord', 'quartz']","<p>China starts to push back against ever-tightening export restrictions around semiconductors.</p>
"
Building a world-class platform for software engineers,https://www.technologyreview.com/2023/07/10/1075764/building-a-world-class-platform-for-software-engineers/,2023-07-10,"In association withJPMorgan Chase & Co. Software engineers and their ability to deliver are critical to a business’ success. They help organizations keep pace with innovation and respond to disruptive forces. Even companies in industries that are not traditionally considered tech, such as agriculture or financial services, recognize the need for software engineers and are actively seeking to hire talented individuals. Software engineers are responsible for an ever-growing list of demands through the software development cycle. Their working environment is more complex due to proliferation of complex multicloud infrastructure, tooling, and applications. While they command good salaries and professional esteem, the job takes its toll. Additionally, ever-evolving cyber threats and the constant need to stay ahead in terms of continual scanning and early detection add to the complexity.  A 2021 poll conducted by Haystack Analytics found 83% of developers were suffering burnout, driven by increasing demands on their time and inefficient processes. A 2022 survey by LaunchDarkly showed continued burn-out and retention challenges for software engineers, with cumbersome processes a key frustration. Challenges are more acute in organizations saddled with technical debt, heritage applications, and legacy infrastructure. “The engineer’s job has become extremely hard, but with one of the largest tech footprints and investments, JPMorgan Chase has a unique opportunity and responsibility to lead the industry in a paradigm shift toward minimizing the cognitive load for engineers and multiplying their productivity to accelerate the value we deliver to our customers and clients,” says Sandhya Sridharan, global head of Engineering Platforms and Experience at JPMorgan Chase. As part of its modernization journey, JPMorgan Chase is building in a highly integrated self-service engineer platform designed to empower and enable the company’s 43,000+ person engineering community, with the goal of amplifying experience, engagement, and productivity. The firm’s approach is driven by four strategic imperatives. First is a unified interface. This is a personalized, data-driven experience that gives engineers ownership and has a self-service dynamic, which is a change from business-as-usual. “An engineer platform must simplify an engineer’s day-to-day tasks by providing the right level of contextual abstraction along with the appropriate tooling and resources,” explains Sridharan. “This needs to happen within the context of an integrated development environment where engineers spend most of their time providing complete visualization of their build and deployment pipelines.” The second imperative is to be cloud focused. The public cloud offers scalability, which improves speed, agility, and cost. The majority of software developer tooling is primarily available in, and built for, public cloud platforms, which can be more reliable and resilient than on-premise infrastructures. Engineers can quickly take advantage of best-of-breed capabilities, including observability tools, while adopting strategies like canary deployment (releasing first to a small subset of users) that help accelerate time to market. “If we were on-premises, we would not have the flexibility for elastic scale and it wouldn’t be cost effective,” notes Sridharan. The third imperative is to be data-driven, which is core to an industry as complex and fast-moving as financial services. The platform equips engineers with the right data, insights, and recommendations to enable real-time detection and resolution, and to track progress. It also provides telemetry, which can help personalize the engineer experience to individual needs. “Data will power everything we do and inform our decision making as we continue to evolve and improve the platform to best support the needs of our engineers,” Sridharan elaborates. This platform also offers a more robust system for governance and security. Software failures are inevitable, but what matters is whether a platform provides the capability to quickly detect failures and recover. JPMorgan Chase’s platform includes observability tooling that can detect problems and auto-remediate or rollback the change that caused it, reducing outage time for end users. Observability and automation are especially important in heavily regulated sectors, like finance, in terms of audit evidence. “We need to have full traceability of every transaction and changes that go into production,” notes Sridharan. “This not only equips our engineers with detailed insights and trends, but it also saves them several days and weeks of effort anytime we are audited, as the platform provides a full audit report with the click of a button.” JPMorgan Chase’s upgrade of its engineering platform improves productivity, efficiency, and security. Just as important is helping the company compete for engineering talent by offering a vastly more efficient working environment than software engineers might find elsewhere. The firm’s goal is to be the most attractive engineering destination, and given the consistent competition for good talent, it’s more important than ever to offer engineers a world-class working environment with minimal friction. “Engineering excellence and a highly intuitive platform are critical for us to not only retain our top talent, but also continue to attract the best talent in the industry,” says Sridharan. By abstracting both the controls and the infrastructure complexities, JPMorgan Chase’s platform allows engineers to focus on high-value productivity, rather than being mired in coordination or laborious processes. It abstracts controls by automatically factoring in the relevant workflows and processes, such as differing regulatory environments, which the platform incorporates through its policy and rules engine. This allows engineers to focus on what they do best: build high value applications which are scalable, robust, and resilient. As many companies move to multicloud or hybrid cloud environments, some workloads are best conducted via one cloud hyperscaler platform or another. However, choosing which one shouldn’t tax an engineer’s time. The platform directs workflows into the right infrastructure based on factors like regulation and capacity. Abstracting all of this complexity, while providing context where needed, means developers can focus on building the right application to generate customer value.  “To make JPMorgan Chase the most attractive engineering destination, we need to be able to not only remove friction every step of the way, but we need to build a solid foundation that evolves and simplifies the day-to-day life of a software engineer, which will help accelerate our business outcomes,” concludes Sridharan. This article is for informational purposes only and it is not intended as legal, tax, financial, investment, accounting or regulatory advice. Opinions expressed herein are the personal views of the individual(s) and do not represent the views of JPMorgan Chase & Co. The accuracy of any statements, linked resources, reported findings or quotations are not the responsibility of JPMorgan Chase & Co. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. ","In association withJPMorgan Chase & Co. Software engineers and their ability to deliver are critical to a business ’ success . They help organizations keep pace with innovation and respond to disruptive forces . Even companies in industries that are not traditionally considered tech , such as agriculture or financial services , recognize the need for software engineers and are actively seeking to hire talented individuals . Software engineers are responsible for an ever-growing list of demands through the software development cycle . Their working environment is more complex due to proliferation of complex multicloud infrastructure , tooling , and applications . While they command good salaries and professional esteem , the job takes its toll . Additionally , ever-evolving cyber threats and the constant need to stay ahead in terms of continual scanning and early detection add to the complexity . A 2021 poll conducted by Haystack Analytics found 83 % of developers were suffering burnout , driven by increasing demands on their time and inefficient processes . A 2022 survey by LaunchDarkly showed continued burn-out and retention challenges for software engineers , with cumbersome processes a key frustration . Challenges are more acute in organizations saddled with technical debt , heritage applications , and legacy infrastructure . “ The engineer ’ s job has become extremely hard , but with one of the largest tech footprints and investments , JPMorgan Chase has a unique opportunity and responsibility to lead the industry in a paradigm shift toward minimizing the cognitive load for engineers and multiplying their productivity to accelerate the value we deliver to our customers and clients , ” says Sandhya Sridharan , global head of Engineering Platforms and Experience at JPMorgan Chase . As part of its modernization journey , JPMorgan Chase is building in a highly integrated self-service engineer platform designed to empower and enable the company ’ s 43,000+ person engineering community , with the goal of amplifying experience , engagement , and productivity . The firm ’ s approach is driven by four strategic imperatives . First is a unified interface . This is a personalized , data-driven experience that gives engineers ownership and has a self-service dynamic , which is a change from business-as-usual . “ An engineer platform must simplify an engineer ’ s day-to-day tasks by providing the right level of contextual abstraction along with the appropriate tooling and resources , ” explains Sridharan . “ This needs to happen within the context of an integrated development environment where engineers spend most of their time providing complete visualization of their build and deployment pipelines. ” The second imperative is to be cloud focused . The public cloud offers scalability , which improves speed , agility , and cost . The majority of software developer tooling is primarily available in , and built for , public cloud platforms , which can be more reliable and resilient than on-premise infrastructures . Engineers can quickly take advantage of best-of-breed capabilities , including observability tools , while adopting strategies like canary deployment ( releasing first to a small subset of users ) that help accelerate time to market . “ If we were on-premises , we would not have the flexibility for elastic scale and it wouldn ’ t be cost effective , ” notes Sridharan . The third imperative is to be data-driven , which is core to an industry as complex and fast-moving as financial services . The platform equips engineers with the right data , insights , and recommendations to enable real-time detection and resolution , and to track progress . It also provides telemetry , which can help personalize the engineer experience to individual needs . “ Data will power everything we do and inform our decision making as we continue to evolve and improve the platform to best support the needs of our engineers , ” Sridharan elaborates . This platform also offers a more robust system for governance and security . Software failures are inevitable , but what matters is whether a platform provides the capability to quickly detect failures and recover . JPMorgan Chase ’ s platform includes observability tooling that can detect problems and auto-remediate or rollback the change that caused it , reducing outage time for end users . Observability and automation are especially important in heavily regulated sectors , like finance , in terms of audit evidence . “ We need to have full traceability of every transaction and changes that go into production , ” notes Sridharan . “ This not only equips our engineers with detailed insights and trends , but it also saves them several days and weeks of effort anytime we are audited , as the platform provides a full audit report with the click of a button. ” JPMorgan Chase ’ s upgrade of its engineering platform improves productivity , efficiency , and security . Just as important is helping the company compete for engineering talent by offering a vastly more efficient working environment than software engineers might find elsewhere . The firm ’ s goal is to be the most attractive engineering destination , and given the consistent competition for good talent , it ’ s more important than ever to offer engineers a world-class working environment with minimal friction . “ Engineering excellence and a highly intuitive platform are critical for us to not only retain our top talent , but also continue to attract the best talent in the industry , ” says Sridharan . By abstracting both the controls and the infrastructure complexities , JPMorgan Chase ’ s platform allows engineers to focus on high-value productivity , rather than being mired in coordination or laborious processes . It abstracts controls by automatically factoring in the relevant workflows and processes , such as differing regulatory environments , which the platform incorporates through its policy and rules engine . This allows engineers to focus on what they do best : build high value applications which are scalable , robust , and resilient . As many companies move to multicloud or hybrid cloud environments , some workloads are best conducted via one cloud hyperscaler platform or another . However , choosing which one shouldn ’ t tax an engineer ’ s time . The platform directs workflows into the right infrastructure based on factors like regulation and capacity . Abstracting all of this complexity , while providing context where needed , means developers can focus on building the right application to generate customer value . “ To make JPMorgan Chase the most attractive engineering destination , we need to be able to not only remove friction every step of the way , but we need to build a solid foundation that evolves and simplifies the day-to-day life of a software engineer , which will help accelerate our business outcomes , ” concludes Sridharan . This article is for informational purposes only and it is not intended as legal , tax , financial , investment , accounting or regulatory advice . Opinions expressed herein are the personal views of the individual ( s ) and do not represent the views of JPMorgan Chase & Co . The accuracy of any statements , linked resources , reported findings or quotations are not the responsibility of JPMorgan Chase & Co . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['association', 'software', 'engineer', 'ability', 'deliver', 'critical', 'business', 'success', 'help', 'organization', 'keep', 'pace', 'innovation', 'respond', 'disruptive', 'force', 'even', 'company', 'industry', 'traditionally', 'consider', 'tech', 'agriculture', 'financial', 'service', 'recognize', 'need', 'software', 'engineer', 'actively', 'seek', 'hire', 'talented', 'individual', 'software', 'engineer', 'responsible', 'evergrowing', 'list', 'demand', 'software', 'development', 'cycle', 'working', 'environment', 'complex', 'proliferation', 'complex', 'multicloud', 'infrastructure', 'tooling', 'application', 'command', 'good', 'salary', 'professional', 'esteem', 'job', 'take', 'toll', 'additionally', 'everevolve', 'cyber', 'threat', 'constant', 'need', 'stay', 'ahead', 'term', 'continual', 'scanning', 'early', 'detection', 'add', 'complexity', 'poll', 'conduct', 'haystack', 'analytic', 'find', 'developer', 'suffer', 'burnout', 'drive', 'increase', 'demand', 'time', 'inefficient', 'process', 'survey', 'launchdarkly', 'show', 'continue', 'burnout', 'retention', 'challenge', 'software', 'engineer', 'cumbersome', 'process', 'key', 'frustration', 'challenge', 'acute', 'organization', 'saddle', 'technical', 'debt', 'heritage', 'application', 'legacy', 'infrastructure', 'engineer', 'job', 'become', 'extremely', 'hard', 'large', 'tech', 'footprint', 'investment', 'unique', 'opportunity', 'responsibility', 'lead', 'industry', 'paradigm', 'shift', 'minimize', 'cognitive', 'load', 'engineer', 'multiply', 'productivity', 'accelerate', 'value', 'deliver', 'customer', 'client', 'say', 'sridharan', 'global', 'head', 'engineering', 'platform', 'experience', 'part', 'modernization', 'journey', 'build', 'highly', 'integrate', 'selfservice', 'engineer', 'platform', 'design', 'empower', 'enable', 'company', 'person', 'engineering', 'community', 'goal', 'amplify', 'experience', 'engagement', 'productivity', 'firm', 'approach', 'drive', 'strategic', 'imperative', 'first', 'unified', 'interface', 'personalized', 'datadriven', 'experience', 'give', 'engineer', 'ownership', 'selfservice', 'dynamic', 'change', 'businessasusual', 'engineer', 'platform', 'simplify', 'engineer', 'daytoday', 'task', 'provide', 'right', 'level', 'contextual', 'abstraction', 'appropriate', 'tooling', 'resource', 'explain', 'sridharan', 'need', 'happen', 'context', 'integrate', 'development', 'environment', 'engineer', 'spend', 'time', 'provide', 'complete', 'visualization', 'build', 'deployment', 'pipeline', 'second', 'imperative', 'focus', 'public', 'cloud', 'offer', 'scalability', 'improve', 'speed', 'agility', 'cost', 'majority', 'software', 'developer', 'tooling', 'primarily', 'available', 'build', 'public', 'cloud', 'platform', 'reliable', 'resilient', 'onpremise', 'infrastructure', 'engineer', 'quickly', 'take', 'advantage', 'bestofbreed', 'capability', 'include', 'observability', 'tool', 'adopt', 'strategy', 'canary', 'deployment', 'release', 'first', 'small', 'subset', 'user', 'accelerate', 'time', 'market', 'onpremise', 'flexibility', 'elastic', 'scale', 'cost', 'effective', 'note', 'sridharan', 'third', 'imperative', 'datadriven', 'core', 'industry', 'complex', 'fastmoving', 'financial', 'service', 'platform', 'equip', 'engineer', 'right', 'datum', 'insight', 'recommendation', 'enable', 'realtime', 'detection', 'resolution', 'track', 'progress', 'also', 'provide', 'telemetry', 'help', 'personalize', 'engineer', 'experience', 'individual', 'need', 'datum', 'power', 'inform', 'decision', 'making', 'continue', 'evolve', 'improve', 'platform', 'good', 'support', 'need', 'engineer', 'elaborate', 'platform', 'also', 'offer', 'robust', 'system', 'governance', 'security', 'software', 'failure', 'inevitable', 'matter', 'platform', 'provide', 'capability', 'quickly', 'detect', 'failure', 'recover', 'platform', 'include', 'observability', 'tooling', 'detect', 'problem', 'autoremediate', 'rollback', 'change', 'cause', 'reduce', 'outage', 'time', 'end', 'user', 'observability', 'automation', 'especially', 'important', 'heavily', 'regulate', 'sector', 'finance', 'term', 'audit', 'evidence', 'need', 'full', 'traceability', 'transaction', 'change', 'go', 'production', 'note', 'sridharan', 'equip', 'engineer', 'detailed', 'insight', 'trend', 'also', 'save', 'several', 'day', 'week', 'effort', 'anytime', 'audit', 'platform', 'provide', 'full', 'audit', 'report', 'click', 'button', 'upgrade', 'engineering', 'platform', 'improve', 'productivity', 'efficiency', 'security', 'important', 'help', 'company', 'compete', 'engineering', 'talent', 'offer', 'vastly', 'efficient', 'working', 'environment', 'software', 'engineer', 'find', 'elsewhere', 'firm', 'goal', 'attractive', 'engineering', 'destination', 'give', 'consistent', 'competition', 'good', 'talent', 'important', 'ever', 'offer', 'engineer', 'worldclass', 'working', 'environment', 'minimal', 'friction', 'engineering', 'excellence', 'highly', 'intuitive', 'platform', 'critical', 'retain', 'top', 'talent', 'also', 'continue', 'attract', 'good', 'talent', 'industry', 'say', 'abstract', 'control', 'infrastructure', 'complexity', 'platform', 'allow', 'engineer', 'focus', 'highvalue', 'productivity', 'rather', 'mire', 'coordination', 'laborious', 'process', 'abstract', 'control', 'automatically', 'factor', 'relevant', 'workflow', 'process', 'differ', 'regulatory', 'environment', 'platform', 'incorporate', 'policy', 'rule', 'engine', 'allow', 'engineer', 'focus', 'well', 'build', 'high', 'value', 'application', 'scalable', 'robust', 'resilient', 'many', 'company', 'move', 'multicloud', 'hybrid', 'cloud', 'environment', 'workload', 'well', 'conduct', 'cloud', 'hyperscaler', 'platform', 'however', 'choose', 'tax', 'engineer', 'time', 'platform', 'direct', 'workflow', 'right', 'infrastructure', 'base', 'factor', 'regulation', 'capacity', 'abstract', 'complexity', 'provide', 'context', 'need', 'mean', 'developer', 'focus', 'build', 'right', 'application', 'generate', 'customer', 'value', 'make', 'chase', 'attractive', 'engineering', 'destination', 'need', 'able', 'remove', 'friction', 'step', 'way', 'need', 'build', 'solid', 'foundation', 'evolve', 'simplifie', 'daytoday', 'life', 'software', 'engineer', 'help', 'accelerate', 'business', 'outcome', 'conclude', 'sridharan', 'article', 'informational', 'purpose', 'intend', 'legal', 'tax', 'financial', 'investment', 'accounting', 'regulatory', 'advice', 'opinion', 'express', 'herein', 'personal', 'view', 'individual', 'represent', 'view', 'co', 'accuracy', 'statement', 'link', 'resource', 'report', 'finding', 'quotation', 'responsibility', 'co', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","Software engineers and their ability to deliver are critical to a business’ success. They help organizations keep pace with innovation and respond to disruptive forces. Even companies in industries that are not traditionally considered tech, such as agriculture or financial services, recognize the need for software engineers and are actively seeking to hire talented individuals.…"
Open finance heralds a new era,https://www.technologyreview.com/2023/06/28/1075305/open-finance-heralds-a-new-era/,2023-06-28,"In partnership withPlaid Open banking, which allows consumers to securely share their banking data with third-party providers (TPPs), continues to transform financial services. A new generation of financial technology (fintech) companies—peer-to-peer payment services, mobile banking apps, and trading platforms—offer consumers powerful tools to manage their money and extend their banking capabilities. According to the online data platform Statista, the worldwide number of consumers using open banking services is projected to reach 132.2 million by 2024. With open finance, the next iteration of open banking, consumers can control and share their financial information securely with TPPs or financial institutions, accessing an array of financial services including digital banking, lending, payments, renting property, and investing. For consumers the services promise fairness, new products, safety, transparency, convenience, and choice—the ability to control and manage their own data for their own benefit. To deliver, open finance requires strong guardrails for cybersecurity, regulations, and education to ensure consumers can make sound financial decisions. For many, access to financial services like credit is essential for autonomy and economic freedom. However, the credit system’s rigid categories and controls leave out some consumers. According to a TransUnion 2022 global study, 8.1 million people in the U.S. (3% of adults) are considered credit unserved, and another 37 million are considered credit underserved (14% of adults); in Canada, 9.6 million people are either credit unserved or underserved, more than 30% of adults. Systems that evaluate consumer data can be fraught with biases. In the UK, the Financial Conduct Authority (FCA) found individual credit information is significantly different across its three large credit reference agencies (CRAs). It reports relatively low understanding about credit among consumers, and found it is difficult to access credit information, or to raise disputes. Similarly, a 2015 U.S. Consumer Financial Protection Bureau (CFPB) study looked at “credit invisibles” —individuals with no credit or a limited credit history who are less likely to be approved for loans based on the data provided by the three nationwide credit reporting agencies (NCRAs). The CFPB concludes this bias disproportionately affects Black and Hispanic financial consumers. Immigrants encounter bias in credit score calculations as well. Raja Chakravorti, universal access lead at financial services company Plaid, says Fair Isaac Corp-oration (FICO) score calculations do not consistently pull data properly across borders, something he hopes open finance can help fix. “One of the hopes and advantages of open banking or open finance is that we are creating more of that positive trajectory, and over time, financial services would become more inclusive,” he says. Chakravorti says traditional credit bureau data is not the only way to find out whether a consumer is a good financial risk. Alternate data—such as investments, loans, utility payments, and subscription payments—can show a person’s financial behavior and also indicate financial health, he says. How a consumer manages personal finances, like paying bills on time and consistently, can show how likely they are to meet financial responsibilities, he says. Download the full report. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. ","In partnership withPlaid Open banking , which allows consumers to securely share their banking data with third-party providers ( TPPs ) , continues to transform financial services . A new generation of financial technology ( fintech ) companies—peer-to-peer payment services , mobile banking apps , and trading platforms—offer consumers powerful tools to manage their money and extend their banking capabilities . According to the online data platform Statista , the worldwide number of consumers using open banking services is projected to reach 132.2 million by 2024 . With open finance , the next iteration of open banking , consumers can control and share their financial information securely with TPPs or financial institutions , accessing an array of financial services including digital banking , lending , payments , renting property , and investing . For consumers the services promise fairness , new products , safety , transparency , convenience , and choice—the ability to control and manage their own data for their own benefit . To deliver , open finance requires strong guardrails for cybersecurity , regulations , and education to ensure consumers can make sound financial decisions . For many , access to financial services like credit is essential for autonomy and economic freedom . However , the credit system ’ s rigid categories and controls leave out some consumers . According to a TransUnion 2022 global study , 8.1 million people in the U.S. ( 3 % of adults ) are considered credit unserved , and another 37 million are considered credit underserved ( 14 % of adults ) ; in Canada , 9.6 million people are either credit unserved or underserved , more than 30 % of adults . Systems that evaluate consumer data can be fraught with biases . In the UK , the Financial Conduct Authority ( FCA ) found individual credit information is significantly different across its three large credit reference agencies ( CRAs ) . It reports relatively low understanding about credit among consumers , and found it is difficult to access credit information , or to raise disputes . Similarly , a 2015 U.S. Consumer Financial Protection Bureau ( CFPB ) study looked at “ credit invisibles ” —individuals with no credit or a limited credit history who are less likely to be approved for loans based on the data provided by the three nationwide credit reporting agencies ( NCRAs ) . The CFPB concludes this bias disproportionately affects Black and Hispanic financial consumers . Immigrants encounter bias in credit score calculations as well . Raja Chakravorti , universal access lead at financial services company Plaid , says Fair Isaac Corp-oration ( FICO ) score calculations do not consistently pull data properly across borders , something he hopes open finance can help fix . “ One of the hopes and advantages of open banking or open finance is that we are creating more of that positive trajectory , and over time , financial services would become more inclusive , ” he says . Chakravorti says traditional credit bureau data is not the only way to find out whether a consumer is a good financial risk . Alternate data—such as investments , loans , utility payments , and subscription payments—can show a person ’ s financial behavior and also indicate financial health , he says . How a consumer manages personal finances , like paying bills on time and consistently , can show how likely they are to meet financial responsibilities , he says . Download the full report . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['partnership', 'withplaid', 'open', 'banking', 'allow', 'consumer', 'securely', 'share', 'banking', 'datum', 'thirdparty', 'provider', 'tpps', 'continue', 'transform', 'financial', 'service', 'new', 'generation', 'financial', 'technology', 'fintech', 'company', 'peertopeer', 'payment', 'service', 'mobile', 'banking', 'app', 'trading', 'platform', 'offer', 'consumer', 'powerful', 'tool', 'manage', 'money', 'extend', 'banking', 'capability', 'accord', 'online', 'data', 'platform', 'worldwide', 'number', 'consumer', 'use', 'open', 'banking', 'service', 'project', 'reach', 'open', 'finance', 'next', 'iteration', 'open', 'banking', 'consumer', 'control', 'share', 'financial', 'information', 'securely', 'tpps', 'financial', 'institution', 'access', 'array', 'financial', 'service', 'include', 'digital', 'banking', 'lending', 'payment', 'rent', 'property', 'invest', 'consumer', 'service', 'promise', 'fairness', 'new', 'product', 'safety', 'transparency', 'convenience', 'choice', 'ability', 'control', 'manage', 'datum', 'benefit', 'deliver', 'open', 'finance', 'require', 'strong', 'guardrail', 'cybersecurity', 'regulation', 'education', 'ensure', 'consumer', 'make', 'sound', 'financial', 'decision', 'many', 'access', 'financial', 'service', 'credit', 'essential', 'autonomy', 'economic', 'freedom', 'however', 'credit', 'system', 'rigid', 'category', 'control', 'leave', 'consumer', 'accord', 'transunion', 'global', 'study', 'people', 'adult', 'consider', 'credit', 'unserve', 'consider', 'credit', 'underserve', 'adult', 'people', 'credit', 'unserve', 'underserve', 'adult', 'system', 'evaluate', 'consumer', 'datum', 'fraught', 'bias', 'financial', 'conduct', 'authority', 'find', 'individual', 'credit', 'information', 'significantly', 'different', 'large', 'credit', 'reference', 'agency', 'cra', 'report', 'relatively', 'low', 'understanding', 'credit', 'consumer', 'find', 'difficult', 'access', 'credit', 'information', 'raise', 'dispute', 'similarly', 'consumer', 'study', 'look', 'credit', 'invisible', 'individual', 'credit', 'limited', 'credit', 'history', 'less', 'likely', 'approve', 'loan', 'base', 'datum', 'provide', 'nationwide', 'credit', 'reporting', 'agency', 'ncra', 'cfpb', 'conclude', 'bias', 'disproportionately', 'affect', 'black', 'hispanic', 'financial', 'consumer', 'immigrant', 'encounter', 'bias', 'credit', 'score', 'calculation', 'well', 'universal', 'access', 'lead', 'financial', 'service', 'company', 'plaid', 'say', 'score', 'calculation', 'consistently', 'pull', 'datum', 'properly', 'border', 'hope', 'open', 'finance', 'help', 'fix', 'hope', 'advantage', 'open', 'banking', 'open', 'finance', 'create', 'positive', 'trajectory', 'time', 'financial', 'service', 'become', 'inclusive', 'say', 'say', 'traditional', 'credit', 'bureau', 'datum', 'way', 'find', 'consumer', 'good', 'financial', 'risk', 'alternate', 'datum', 'investment', 'loan', 'utility', 'payment', 'subscription', 'payment', 'show', 'person', 'financial', 'behavior', 'also', 'indicate', 'financial', 'health', 'say', 'consumer', 'manage', 'personal', 'finance', 'pay', 'bill', 'time', 'consistently', 'show', 'likely', 'meet', 'financial', 'responsibility', 'say', 'download', 'full', 'report', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","Open banking, which allows consumers to securely share their banking data with third-party providers (TPPs), continues to transform financial services. A new generation of financial technology (fintech) companies—peer-to-peer payment services, mobile banking apps, and trading platforms—offer consumers powerful tools to manage their money and extend their banking capabilities. According to the online data platform Statista,…"
AI gains momentum in core financial services functions,https://www.technologyreview.com/2023/06/28/1075552/ai-gains-momentum-in-core-financial-services-functions/,2023-06-28,"In partnership withDataiku With 2.5 billion payment cards used in more than 200 countries and territories across the globe, Mastercard faces a gigantic fraud risk. Credit card fraud accounted for more than $32 billion in losses—or about 6.6 cents per $100 in transactions—in 2021, with more than one-third occurring the U.S., according to a December 2022 Nilson Report study. Fraud risks of this scale require real-time authentication of individual transactions, which opens the door for errors—potentially allowing a fraudulent transaction or denying a legitimate purchase. However, real-time verification also allows cardholders—and Mastercard—to benefit from the global marketplace. To tackle this challenge, Mastercard marries fraud-detection technologies with AI, using the data it collects over its network. Hitting the right balance between denying potentially legitimate transactions and allowing questionable transactions is a challenge, says Rohit Chauhan, executive vice president of AI at Mastercard. “It’s a really tricky kind of model where you want to decline every possible fraudulent transaction, but at the same time, let the legitimate transactions pass through without any friction,” he says. “On an average day, we see over a billion transactions, and since data is what fuels AI, we were definitely one of the early adopters.” Yet, the benefits of AI adoption surpass improved fraud detection. As such, the application of AI throughout Mastercard has become a priority, Chauhan said.  “The use of AI is about future-proofing Mastercard,” Chauhan says. “If it’s the new electricity, we want electricity to be flowing through every division within Mastercard, and every business unit should be benefiting from it, rather than just the places where it naturally incubates.” Mastercard is not alone. Financial services and banking see AI as an opportunity to automate massively at scale, keep up with accelerating customer expectations, stay competitive in an evolving marketplace, and prepare for disruptions. As AI use cases grow beyond fraud detection and searching unstructured data (data that isn’t organized or in the correct format for an application), businesses will increasingly put AI-powered functionality into the hands of non-technical workers and business operations, allowing innovation to happen across the organization.  With training, retooling, and increased exposure to data, business users can play a central role in building analytics workflows, more efficiently handle regulatory requests, and ensure the quality of data.  Whether the systems are called AI, machine-learning (ML) models, or automated data analytics, data is taking on a more significant role inside companies, says John McCambridge, global solutions director for financial services and insurance at AI and machine learning firm Dataiku. “People are using advanced analytics to solve various problems inside of their businesses and generate lots of return on investment,” he says. “It’s absolutely a potential source of value and can solve very specific problems.” Download the full report. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. ","In partnership withDataiku With 2.5 billion payment cards used in more than 200 countries and territories across the globe , Mastercard faces a gigantic fraud risk . Credit card fraud accounted for more than $ 32 billion in losses—or about 6.6 cents per $ 100 in transactions—in 2021 , with more than one-third occurring the U.S. , according to a December 2022 Nilson Report study . Fraud risks of this scale require real-time authentication of individual transactions , which opens the door for errors—potentially allowing a fraudulent transaction or denying a legitimate purchase . However , real-time verification also allows cardholders—and Mastercard—to benefit from the global marketplace . To tackle this challenge , Mastercard marries fraud-detection technologies with AI , using the data it collects over its network . Hitting the right balance between denying potentially legitimate transactions and allowing questionable transactions is a challenge , says Rohit Chauhan , executive vice president of AI at Mastercard . “ It ’ s a really tricky kind of model where you want to decline every possible fraudulent transaction , but at the same time , let the legitimate transactions pass through without any friction , ” he says . “ On an average day , we see over a billion transactions , and since data is what fuels AI , we were definitely one of the early adopters. ” Yet , the benefits of AI adoption surpass improved fraud detection . As such , the application of AI throughout Mastercard has become a priority , Chauhan said . “ The use of AI is about future-proofing Mastercard , ” Chauhan says . “ If it ’ s the new electricity , we want electricity to be flowing through every division within Mastercard , and every business unit should be benefiting from it , rather than just the places where it naturally incubates. ” Mastercard is not alone . Financial services and banking see AI as an opportunity to automate massively at scale , keep up with accelerating customer expectations , stay competitive in an evolving marketplace , and prepare for disruptions . As AI use cases grow beyond fraud detection and searching unstructured data ( data that isn ’ t organized or in the correct format for an application ) , businesses will increasingly put AI-powered functionality into the hands of non-technical workers and business operations , allowing innovation to happen across the organization . With training , retooling , and increased exposure to data , business users can play a central role in building analytics workflows , more efficiently handle regulatory requests , and ensure the quality of data . Whether the systems are called AI , machine-learning ( ML ) models , or automated data analytics , data is taking on a more significant role inside companies , says John McCambridge , global solutions director for financial services and insurance at AI and machine learning firm Dataiku . “ People are using advanced analytics to solve various problems inside of their businesses and generate lots of return on investment , ” he says . “ It ’ s absolutely a potential source of value and can solve very specific problems. ” Download the full report . This content was produced by Insights , the custom content arm of MIT Technology Review . It was not written by MIT Technology Review ’ s editorial staff .","['partnership', 'withdataiku', 'payment', 'card', 'use', 'country', 'territory', 'globe', 'mastercard', 'face', 'gigantic', 'fraud', 'risk', 'credit', 'card', 'fraud', 'account', 'loss', 'cent', 'transaction', 'onethird', 'occur', 'accord', 'report', 'study', 'fraud', 'risk', 'scale', 'require', 'realtime', 'authentication', 'individual', 'transaction', 'open', 'door', 'error', 'potentially', 'allow', 'fraudulent', 'transaction', 'deny', 'legitimate', 'purchase', 'however', 'realtime', 'verification', 'also', 'allow', 'cardholder', 'mastercard', 'benefit', 'global', 'marketplace', 'tackle', 'challenge', 'marry', 'frauddetection', 'technology', 'use', 'datum', 'collect', 'network', 'hit', 'right', 'balance', 'deny', 'potentially', 'legitimate', 'transaction', 'allow', 'questionable', 'transaction', 'challenge', 'say', 'executive', 'vice', 'president', 'really', 'tricky', 'kind', 'model', 'want', 'decline', 'possible', 'fraudulent', 'transaction', 'time', 'let', 'legitimate', 'transaction', 'pass', 'friction', 'say', 'average', 'day', 'see', 'transaction', 'datum', 'fuel', 'ai', 'definitely', 'early', 'adopter', 'yet', 'benefit', 'adoption', 'surpass', 'improve', 'fraud', 'detection', 'application', 'ai', 'mastercard', 'become', 'priority', 'chauhan', 'say', 'use', 'ai', 'futureproofe', 'say', 'new', 'electricity', 'want', 'electricity', 'flow', 'division', 'business', 'unit', 'benefit', 'rather', 'place', 'naturally', 'incubate', 'alone', 'financial', 'service', 'banking', 'see', 'ai', 'opportunity', 'automate', 'massively', 'scale', 'keep', 'accelerate', 'customer', 'expectation', 'stay', 'competitive', 'evolve', 'marketplace', 'prepare', 'disruption', 'use', 'case', 'grow', 'fraud', 'detection', 'search', 'unstructured', 'data', 'datum', 'organize', 'correct', 'format', 'application', 'business', 'increasingly', 'put', 'aipowere', 'functionality', 'hand', 'nontechnical', 'worker', 'business', 'operation', 'allow', 'innovation', 'happen', 'organization', 'training', 'retooling', 'increase', 'exposure', 'data', 'business', 'user', 'play', 'central', 'role', 'building', 'analytic', 'workflow', 'efficiently', 'handle', 'regulatory', 'request', 'ensure', 'quality', 'datum', 'system', 'call', 'machinelearne', 'ml', 'model', 'automate', 'data', 'analytic', 'datum', 'take', 'significant', 'role', 'company', 'say', 'global', 'solution', 'director', 'financial', 'service', 'insurance', 'machine', 'learn', 'firm', 'dataiku', 'people', 'use', 'advanced', 'analytic', 'solve', 'various', 'problem', 'business', 'generate', 'lot', 'return', 'investment', 'say', 'absolutely', 'potential', 'source', 'value', 'solve', 'specific', 'problem', 'download', 'full', 'report', 'content', 'produce', 'insight', 'custom', 'content', 'arm', 'mit', 'technology', 'review', 'write', 'mit', 'technology', 'review', 'editorial', 'staff']","With 2.5 billion payment cards used in more than 200 countries and territories across the globe, Mastercard faces a gigantic fraud risk. Credit card fraud accounted for more than $32 billion in losses—or about 6.6 cents per $100 in transactions—in 2021, with more than one-third occurring the U.S., according to a December 2022 Nilson Report…"
ORL-AUDITOR: Dataset Auditing in Offline Deep Reinforcement Learning,"[{'title': 'doi', 'href': 'http://dx.doi.org/10.14722/ndss.2024.23184', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2309.03081v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2309.03081v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-09-06 15:28:43,"Field evaluation of a mobile app for assisting blind and visually 
impaired travelers to find bus stops  

Shrinivas Pundlik 1, Prerana Shivshanker 1, Tim Traut-Savino 2, Gang Luo 1  

1. Schepens Eye Research Institute of Mass Eye & Ear, Harvard Medical School Boston, MA 

2. The Carroll Center for the Blind, MA 

Corresponding Author: 
Gang Luo (gang.luo@schepens.harvard.edu) 

Keywords: vision aid, navigation, mobility, accessibility, deep learning, computer vision, public 
transportation   

Commercial Relationships: The authors declared no potential conflicts of interest with respect to 
the research, authorship, and/or publication of this article. The All_Aboard app evaluated in this 
study is released to public for free. There is no revenue from app sale or in-app advertisements. 

 
 
 
 
 
 
 
Abstract 

Purpose: It is reported that there can be considerable gaps due to GPS inaccuracy and 

mapping errors if blind and visually impaired (BVI) travelers rely on digital maps to go to their 

desired bus stops. We evaluated the ability of a mobile app, All_Aboard, to guide BVI travelers 

precisely to the bus-stops. 

Methods: The All_Aboard app detected bus-stop signs in real-time via smartphone camera 

using a neural network model, and provided distance coded audio feedback to help localize the 

detected sign. BVI individuals used the All_Aboard and Google Maps app to localize 10 bus-

stop locations in Boston downtown and another 10 in a sub-urban area. For each bus stop, the 

subjects used the apps to navigate as close as possible to the physical bus-stop sign, starting 

from 30 to 50 meters away. The outcome measures were success rate and gap distance 

between the app-indicated location and the actual physical location of the bus stop. 

Results: The study was conducted with 24 legally blind participants (mean age [SD]: 51[14] 

years; 11 (46%) Female). The success rate of the All_Aboard app (91%) was significantly 

higher than the Google Maps (52%, p<0.001). The gap distance when using the All_Aboard app 

was significantly lower (mean [95%CI]: 1.8 [1.2-2.3] meters) compared to the Google Maps (7 

[6.5-7.5] meters; p<0.001).  

Conclusion: The All_Aboard app localizes bus stops more accurately and reliably than GPS-

based smartphone navigation options in real-world environments.  

Translational Relevance: The All_Aboard mobile app navigation aid can be potentially help BVI 

individuals independently travel via public transportation.  

 
Introduction 

Blind and visually impaired (BVI) people often rely on public transportation, such as buses and 

subways, to travel for employment, leisure or for other needs.1 Geolocation and transportation 

information accessed through smartphones has greatly facilitated macro-navigation for BVI 

people. For example, one can plan a route and get detailed instructions on mobile devices for 

point-to-point navigation using public transits. Navigation apps make up one of the major groups 

of vision assistance mobile apps available in App store and Play store. 2 On the other hand, 

micro-navigation – navigating precisely to the desired destination at any stage of the journey, 

remains a largely unsolved issue for BVI individuals. To comply with the Americans with 

Disabilities Act (1990), regional transit agencies are required to comply with regulations 

regarding the accessibility of transit infrastructures.3 In the context of vision disabilities, the 

requirements include placing of large-print signage at bus stops, providing braille and tactile 

information within transit stations, and making stop announcements inside transit vehicles at 

main points, among others. However, poor interface and lack of cues accessible from distance, 

for example transit stop signage, are one of the main barriers to equal access to public 

transportation.4-9  

Systemic inaccuracies in the GPS based location services is the underlying problem that leads 

to micro-navigation challenges faced by people with BVI. This is also referred to as the last 30 

feet or last 10 m problem in wayfinding. For example, when navigating to a bus stop, a blind 

person following any GPS-based navigation app, may arrive at the app indicated location that 

has a considerable gap (typically 10 m or 30 feet) from the actual bus stop due to the 

localization error in the GPS service. For perspective, this 10 m gap could be almost equal to an 

entire bus length in certain regions. According to the feedback from blind travelers, sometimes 

even a small gap can be large enough for them to miss the bus because the bus drivers 

misunderstand their intention and not stop for them.10-13 In the worst-case scenario, especially in 

crowded cities, the GPS localization may be off by more than a block, making the macro 

navigation apps essentially useless in pedestrian mode.14  

Weather and location (density of tall buildings in downtown areas for example) can further affect 

GPS-based localization. In addition to localization error, there is a possibility of mapping errors 

(sometimes large) in the stop locations made publicly available by the transit agencies. In our 

survey of 174 bus stop locations in Boston metro area, about 23% were mapped more than 2 

bus lengths away. 15 Mapping and localization errors together contribute towards making purely 

location-based services unreliable for micro-navigation tasks, such as finding bus stops. Making 

matters worse, bus stop signs can be one of many signs on a typical urban street (among 

traffic/parking signs and street signs), and thus finding it becomes a visual search task, in 

addition to a plain geolocation task. Since visual search performance is known to be significantly 

degraded in people with low vision,16,17 it is evident that a navigation aid is needed for micro-

navigation with visual search capabilities. 

One of the conventional wayfinding solutions is the use of Bluetooth beacons to provide micro-

location information with high accuracy on nearby landmarks.18-22 This approach’s scalability 

and applicability in outdoor environments are restricted due to the high cost for infrastructure 

modification and maintenance. On the other hand, smartphones could allow rapid scaling of 

accessibility. A few smartphone apps have been developed, tested, or released to help blind 

and visually impaired people access public transportation specifically, or to navigate to 

destinations in general.23-25 These apps are primarily GPS-based, and therefore still subject to 

the limitations of GPS-based navigation systems detailed above. In order to achieve localization 

more accurately, some apps combine location information together with landmark 

recognition.12,13,26,27 However, landmark maps around the various locations have to be built, 

maintained, and made widely available prior to use. Combing signage information extracted 

from GTFS with optical character recognition could provide a viable micro-navigation solution.28 

For the purpose of bus stop navigation, a purely visual approach can work well if combined with 

a typical macro-navigation apps. 

We have developed a mobile app, All_Aboard that recognizes bus stop signs to help the users 

navigate within a very short range of the physical location of the sign.29 When using the app, 

users can first use commonly available macro-navigation tools, such as Google Maps, to arrive 

within the vicinity of the bus stop location, and then they can scan the surroundings with their 

smartphone camera. The All_Aboard app detects the bus stop signs in real-time in the 

smartphone camera imagery, and can guide the users to approach bus stops through auditory 

cues, with the auditory pitch coding the distance to the target. Our preliminary testing of the 

All_Aboard app indicated its superior performance compared to Google Maps app.14  

The goal of this study was to evaluate All_Aboard in real-world conditions with BVI transit users 

and compare its localization ability with navigation via Google Maps app. Our primary 

hypothesis was that the localization based on All_Aboard app was significantly better than just 

using a conventional navigation app (Google Maps) in terms of distance to the desired bus-stop 

location and rate of successful localizations. Given that GPS-based localization typically suffers 

in densely built downtown areas, we further hypothesized that All_Aboard might be more 

effective in these locations compared to more sparsely populated sub-urban areas.   

Methods 

All_Aboard App 

One of the underlying ideas behind the All Aboard app is that the bus stop signage is unique 

(different than other road signs), uniform in appearance (typically, but not always), and 

standardized across the entire area of a transit agency (Figure 1A). Moreover, since the bus 

stop signs for a given transit agency have the same known physical size, one can estimate the 

approximate distance based on the image size (width) of the detected bus stop sign (the farther 

the distance, the smaller the image, and vice versa). Therefore, it is feasible for computer vision 

algorithms to learn the appearance of the bus stop signs, detect them in the images captured by 

smartphone cameras, and estimate the distance to the actual sign. Bus stop detection in 

All_Aboard is performed in real-time using a MobileNetv2 deep learning neural network,30 

trained on about 10000 images of bus stops collected for a given city/region. Images of bus stop 

signs were collected from the Google Street View imagery based on the publicly available bus 

stop coordinates via General Transit Feed Specification (GTFS) standard. The stop signs were 

manually labeled by placing a bounding box in the collected images. The trained model runs 

natively on the smartphone device (no cloud processing).  

Another key idea behind the operation of All_Aboard is that it supplements a macro-navigation 

app and thus only need to be operational in the general vicinity of the bus stop. In a typical 

usage scenario (Figure 1B), the user launches All_Aboard when a macro-navigation app 

(Google Maps etc.) indicates that the user is near the desired bus stop location. The All_Aboard 

app senses phone orientation and searches only when the device is held in an upright position. 

The user can then scan with the phone camera in an arc to first determine the angular 

orientation of the sign. A positive detection leads to a beeping sound and a true positive 

detection is typically indicated by a series of continuous auditory tones. Once locked in the 

direction of the sign, the auditory tones change in frequency as the user get near the bus stop 

sign (similar to a homing signal). Thus, the app can help users gauge relative distance to the 

sign and adjust their approach. There were four levels of audio tones, with the highest 

frequency indicated that the detected sign was within 2 meters (≈ 6 feet). After it is launched, 

the apps works without needing any active intervention from the user, as long as the device is 

held in an upright manner.  

The All_Aboard is available in App store and Play store for free, and it is capable of recognizing 

bus stops in 10 major cities/regions around the world. Once installed, the users can download 

the available trained neural network models for the corresponding transit agencies. In this study, 

the All_Aboard app was loaded with the model trained to detect bus stop signs of the 

Massachusetts Bay Transportation Authority (MBTA) that operates public transit in Boston 

metro area.   

B 

A 
Figure 1: (A) A typical MBTA bus stop sign at one of the trial location in downtown Boston. This is the 
most recent version of the signage, however, older versions with slightly different appearance but similar 
shapes persist throughout the coverage area. Very few bus stops in the area covered by the transit 
agency are sheltered, and the distinctive sign is often the only visual identification of the bus stop. (B) 
Operation of the All_Aboard app in the general vicinity of a bus stop. The lower inset shows the app 
detecting the bus stop sign (the bounding box is drawn around the detected sign in the camera view 
displayed on the smartphone screen. The upper inset shows a successful detection at night in low light 
conditions. A demonstration video of All_Aboard app in action can be found at 
https://www.youtube.com/watch?v=VUVpqEw1_2k.  

Participants 

The inclusion criteria were: vision status of legal blindness, independent mobility without 

assistance from a sighted guide, physical ability to walk over a distance of about 1 mile at a 

given time, and familiarity with smartphone/mobile devices. Participants for this study was 

recruited via referrals from the Carroll Center for the Blind Newton Massachusetts, practitioners 

at vision rehabilitation clinics, and via a pool of volunteers who had participated in prior studies. 

The study protocol was approved by the Institutional Review Board at Mass Eye & Ear. The 

study followed the tenets of the Declaration of Helsinki and written informed consent was 

 
 
obtained from all the participants. The participants were reimbursed for travel to the study sites 

and for their time. 

Study Design 

The study involved 2 visits at 2 separate study sites: downtown Boston (City) and near the 

campus of the Carroll Center for the Blind in Newton, Massachusetts (Suburb). Each study site 

involved navigating to 10 bus stops following a specific route (Figure 2). For each bus stop, 

performance with All_Aboard and Google Maps apps was evaluated with both the apps running 

simultaneously. During the study, the participants were accompanied by a certified orientation 

and mobility instructor (O&M) who provided directions along the route and ensured the safety of 

the participants during the study. Members of the study team also accompanied the participant 

and the O&M instructor, who administered the study and recorded measurements. For all trials, 

a preconfigured Android smartphone was provided to the participants. 

Before starting the study, each participant was provided oral instructions and hands-on training 

with using the All_Aboard app at a practice location. At the start of the trial at each bus stop 

location, the participant was guided by O&M instructor to a location that was about 30 to 50 

meters (≈100 to 150 feet) away from the bus stop sign along the direction of travel in 

approximately straight ahead direction. The starting distance from the stop sign was varied at 

each stop location to dissuade participants from guessing the stop location based on step 

counting. The path from the starting location to bus stop sign did not involve crossing streets, 

except in the case of one bus stop location in Newton, where the stop sign was affixed very 

close to the intersection. At the starting point for each trial, Google Maps app (operating in the 

pedestrian directions mode) was launched by the experimenter, StreetView calibration was 

done (this was one of the features of the Google Maps app that uses live camera imagery to 

geo-locate more accurately), and the mapped location of the said bus stop was set as the 

destination. Then, the All_Aboard app was launched such that both apps were running 

simultaneously, with Google Maps navigation window in the inset at the bottom of the screen 

(Figure 3A).  

Figure 2: Routes at the two study sites: downtown Boston (Left) and in Newton (Right).Each site had 
10 bus stop locations (indicated by numbers). The route is indicated by dashed black line. The bus 
stops in Newton were on the opposite side of the street such that the route was a loop that was 
traversed in the direction shown by the arrows.   

Then, the smartphone device was handed over to the participants. From this starting location, 

the participants was instructed to navigate as close to the bus stop sign as possible. They were 

also instructed to hold the smartphone upright with its rear camera pointing straight ahead 

(Figure 3B), and scan side-to-side to determine the relative orientation of the bus stop sign from 

their walking trajectory. After this point, the participants walked on their own, relying on their 

habitual mobility aid and their residual vision if present, along with the auditory feedback from 

the All_Aboard app. Meanwhile, the Google Maps app provided intermittent voice instructions, 

including distance to the destination in feet (which the participants soon learned were often 

unreliable). 

At the end of the trial at a bus stop location, the participants stopped and notified the 

experimenters when they thought they were closest to the bus stop sign as per their judgment. 

This was primarily based on the audio feedback by the All_Aboard app – when the audio tone 

 
frequency and pitch were at the highest levels indicating the detected sign was very close. A 

few participants could use their residual vision from this point onward to get even closer. 

Distance from where they stopped to the actual stop sign was measured with a tape measure. 

This was the localization distance for All_Aboard app. Google Maps app also indicated via 

auditory feedback when it determined that the participant arrived at the destination (Figure 3C). 

The distance between the bus stop sign and the arrival point according to Google Maps was 

also measured with the tape measure. 

At the time of their first study visit, the participants answered a brief survey that collected basic 

demographic information, vision status, use of vision aids, and their preferred transit options 

(public transit, rideshare, or private vehicle –family member driving). The level of vision was 

recorded either in terms of self-reported visual acuity in Snellen, or as light perception, or no 

light perception (in case of completely blind individuals).  

Figure 3: Running All_Aboard and Google Maps simultaneously to find bus stops. (A) 
Screenshot of the device at the start point. Both All_Aboard and Google Maps (inset) launched 
and run simultaneously. (B) An user holds the phone upright with the rear camera facing straight 
ahead. (C) Screenshot of the device when Google Maps indicate arrival at the destination. The 
All_Aboard app indicates the physical bus stop sign is still some distance ahead.     

 Outcome Measures 

The two main outcome measures, separately obtained for each app (All_Aboard and Google 

Maps), were: the localization error (gap distance) in meters and the rate of successful 

localizations (success rate). As mentioned above, the gap distance was obtained via direct 

measurement of the distance between app indicated/subject determined location of the bus stop 

and the physical location of the bus stop sign. When the All_Aboard guided the participant close 

enough for them to spot (via their residual vision) or identify the bus stop sign or touch the pole 

or post, the gap distance was marked as 0. If the app indicated location on the ground was 

beyond the physical bus stop sign with respect to the direction of travel, then the measured gap 

distance was recorded as negative. A trial instance was deemed as a failure if a reasonable 

 
measurable distance was not obtained. Success rate for each app was defined as % of 

locations for which a valid measurable distance along the travel path was available. 

At any given bus stop location, trial failures could occur because of various reasons.  In case of 

Google Maps app, incorrect mapping of the bus stops was one of the reasons. Such failures 

were predictable and repetitive across all the subjects because the location of the bus stop in 

the map was fundamentally incorrect. One cause of trial failure was the mapped location being 

more than 100 feet away from the physical bus stop. Another cause of trial failure with Google 

Maps app was catastrophic inaccuracies in geolocation and consequent navigation directions, 

for example, when the app directed the users to cross streets, double-back, or go around a 

corner, which would lead them completely miss the bus stop. These failures were more 

prevalent in the downtown Boston area with tall buildings and/or on overcast/rainy days.  

In the case of All_Aboard app, trial failure could occur because of detection failures due to false 

negatives or deficient technique by the subjects while using the app. Shadows and occlusions 

could lead to the app to fail to recognize a bus stop sign. Signage largely slanting away from the 

side walk direction can cause the app fail to detect. On other occasions, the subjects did not 

scan sufficiently while walking towards the bus stop sign and lost the audio signal. If the bus 

stop sign was initially detected but then went outside the field of view of the camera as the 

subject approached, the continuous audio signal suddenly stopped. This was an indicator to the 

subjects that they either passed the sign or are too close to it. They were allowed to retrace their 

steps and try again once to zero-in or confirm the presence of the stop sign in the near vicinity. 

If major intervention by O&M or the experimenter was needed to reorient the subject after initial 

failure to detect, then the trail was considered as a failure for the All_Aboard app for the given 

location, even if the sign was successfully detected in the subsequent tries. 

Statistical Analysis 

Potential factors of interest affecting the outcome measures were app used (All_Aboard or 

Google Maps), the study location (City vs. Suburb), and the vision status (with or without 

residual vision). Completely blind subjects without light perception were categorized as without 

residual vision, while the rest were with residual vision. Vision in the better eye was used for this 

categorization. Association of gap distance with these above potential variables was analyzed 

within-subject via a linear model in repeated measures framework. The success rate was 

analyzed using a binary logistic regression. In addition to the main effects, the interaction 

between the above 3 factors were also examined. Estimated marginal means with their 95% 

confidence intervals and contrasts are reported for gap distance. Estimated mean marginal 

probability of success and the 95% confidence interval is reported from the logistic regression 

model for success rate. P values <0.05 were considered statistically significant. Statistical 

analysis was performed using statistical packages in R (ver. 4.0.4).31-36         

Results 

Total 25 subjects were recruited, of which, data for both study sites was available for 24 

subjects (see Table 1 for summary of subject characteristics). One subject was dropped from 

the trial after first visit due to the concerns about overall physical fitness required to complete 

the study. Eleven participants (46%) were female. A variety of conditions affected the vision of 

the participants. While all were legally blind in the US, their vision ranged from completely blind 

to 20/200 vision. The majority walked with long cane and all except one used an iPhone in their 

daily lives. Public transit was the most preferred transit option, followed by rideshare and private 

vehicle.   

N 

24 

Table 1: Study participant characteristics. 

Age (years) 

Median: 55, IQR: 41 – 61, Min.: 20, Max.: 71 

Gender  

Female: 11 (46%) 

Vision 

Vision disorders 

Available VA measure – N: 11 (46%), range: [20/200 – 20/1200] 
Light Perception – N: 6 (25%) 
No Light Perception – N: 7 (29%) 

RoP: 5, retinitis pigmentosa: 4, retinal detachment: 3, glaucoma: 2;  
One case each of: age-related macular degeneration, optic atrophy, 
aniridia, cone dystrophy, retinal artery occlusion, charge syndrome, 
monochromacy, diabetic retinopathy, Norrie syndrome 

Duration of vision 
loss 

At birth: 14 (58%) 
Acquired: 8, Median duration: 18 years, range: [3 – 41 years]  
Data not available: 2 

Mobility aids used 

Long cane: 16 (67%) 
Guide dog: 6 (25%) 
None: 2 (8%) 

Most preferred 
transit option 

Habitual 
smartphone 

Public Transit - 1st: 11, 2nd: 7, 3rd: 3, NA: 3 
Rideshare -  1st: 9, 2nd: 11, 3rd: 2, NA: 2 
Private car -  1st: 4, 2nd: 5, 3rd: 8, NA: 7 

iPhone 23 (96%) 

Table 2 shows the trial instances and other data for both apps and at each study site. Across 24 

subjects, there were supposed to be trials at 480 designated bus stops. However, over the 

course of the study, some bus stops were skipped due to construction or missing bus stop 

signs, resulting in a total of 48 instances with missing data. Therefore, each app was evaluated 

in a total of 432 instances. Overall success rate and gap distance measures were substantially 

better with All_Aboard app than Google Maps. 

Table 2: Cumulative statistics for trial data. 

Both Apps  Google Maps  All_Aboard 

Bus stop 
instances 
with 
available 
data 

Skipped 
instances 

Both sites 

City 

Suburb 

Both sites 

City 

864 

458 

406 

96 

22 

432 

229 

203 

48 

11 

432 

229 

203 

48 

11 

 
(missing 
data) 

Number of 
successful 
instances 

Success 
rate (%) 

Average 
[SD] gap 
distance 
(m) 

Suburb 

Both sites 

City 

Suburb 

Both sites 

City 

Suburb 

74 

626 

329 

297 

72 

72 

73 

37 

225 

112 

113 

52 

49 

56 

37 

401 

217 

184 

93 

95 

91 

Both sites 

3.36 [3.65] 

6.62 [4.15] 

1.54 [1.36] 

City 

3.04 [3.82] 

6.26 [4.89] 

1.38 [1.34] 

Suburb 

3.72 [3.41] 

6.97 [3.24] 

1.72 [1.36] 

 In Table 2, successful instances with each app are listed independently of each other. When 

compared pairwise at each bus stop instance (Table 3), there were only a handful of instances 

where both apps failed (18 out of 432 or about 4%). There were 13 (3%) instances where 

All_Aboard failed but Google Maps succeeded, and there were 189 (44%) instances where 

Google Maps failed but All_Aboard succeeded. For the former cases, the average [SD] gap 

distance was 9.3[5] meters, and for the latter cases, the average [SD] gap distance with 

All_Aboard was 1.6[1.4] meters. From 225 successful instances with Google Maps, the arrival 

location was mapped past the bus stop sign along the direction of travel in 60 instances (about 

27%), with an average gap distance of 7.2[2.9] meters.  

Table 3: 2x2 tables showing joint successes or failures of the All_Aboard and Google Maps over all 432 
bus stop instances.  

Overall 

City 

Suburb 

Google 
Maps 
Success 

Google 
Maps 
Failure 

Google 
Maps 
Success 

Google 
Maps 
Failure 

Google 
Maps 
Success 

Google 
Maps 
Failure 

All_Aboard 
Success 

All_Aboard 
Failure 

212 

189 

106 

116 

106 

13 

18 

6 

6 

7 

78 

12 

 
There was no significant effect of subject age, gender, or the kind of mobility aid used on the 

gap distance or on the success rate.  The results and discussion is mostly related to 3 key 

factors: the app used, study site, and subject group based on their vision status. 

Gap distance (in meters), averaged over vision status and study sites, was significantly smaller 

with All_Aboard (mean: 1.8, 95% CI: 1.3-2.3) compared to Google Maps (mean: 7.0, 95% CI: 

6.5-7.5; p<0.001). Gap distance with All_Aboard was significantly smaller than Google Maps in 

City and Suburb, as well as in subjects with or without residual vision (Figure 4A). The gap 

distance was significantly larger in completely blind group (mean: 8.4, 95% CI: 7.3-9.5) 

compared to those with residual vision (mean: 5.4, 95% CI: 4.7-6.1; p<0.001) in the City with 

Google Maps. No significant effect of vision status on the gap distance with All_Aboard was 

observed. A significant effect of study site was seen only in the case of Google Maps in subjects 

with residual vision, where gap distance in the Suburb (mean: 6.8, 95% CI: 6.1-7.5) was 

significantly larger than that observed in the City (mean: 5.4, 95% CI: 4.7-6.1; p<0.022). Again, 

no significant effect of study site was observed for gap distance resulting from the All_Aboard 

app.  

A 

B 

Figure 4: Outcome measures by app, location, and vision status. (A) The gap distance with All_Aboard 
was significantly smaller than Google Maps in both sites and in both subject groups. Those with 
residual vision achieved significantly smaller gap distance compared to completely blind with Google 
Maps in City. Gap distance in City was significantly lower than Suburb in the case of subjects with 
residual vision with Google Maps. (B)  The success rate with All_Aboard was significantly higher in both 
sites and in subjects with and without residual vision. Completely blind individuals when using 
All_Aboard in the Suburb had significantly lower success rate compared to those with residual vision. 
For all panels: error bars show 95% confidence interval of the mean; significance levels:  *** : p<0.001, 
** : p=0.001 – 0.01, and * : p=0.01 – 0.05; P value adjustment: BH method for 4 tests. 

The success rate with All_Aboard was significantly higher than Google Maps across both study 

sites and both the subject groups (Figure 4B). The overall success rate with All_Aboard (mean: 

0.91, 95% CI: 0.87-0.94), averaged over study sites and subject group factors, was about 75% 

higher than Google Maps (mean: 0.52, 95% CI: 0.47-0.58; p<0.001). When using All_Aboard in 

the Suburban location, the success rate for completely blind subjects (mean: 0.8, 95% CI: 0.69-

0.90; p<0.001) was slightly but statistically significantly lower compared to those with residual 

vision (mean: 0.95, 95% CI: 0.91-0.98; p<0.001). Otherwise, there was no significant difference 

in success rates for any other conditions.  

Discussion 

 
 
When people (normally sighted or BVI) take buses in areas like Boston metro region, where 

most bus stops are indicated just by a sign on a post, standing even a short distance away from 

the sign may cause the buses do not stop for them. This accessibility challenge may diminish 

independence, compromise adoption of affordable transportation for BVI travelers.1 This is just 

one of the last-10-meter navigation assistance needs of BVI individuals that is unmet. In this 

study, we evaluated the ability of All_Aboard app relative to the Google Maps navigation app in 

guiding BVI travelers accurately to bus stop locations in urban and suburban settings. The rate 

of successful localization of bus stops was substantially higher and the gap distance was much 

smaller when using All_Aboard app compared to Google Maps navigation. On average, the 

All_Aboard app was able to guide the subjects within about 2 meters (6 feet) of the bus stop 

sign, whereas with Google Maps they were likely to be about 7 meters (23 feet) away. The large 

effect size of All_Aboard app in terms of success rate of localization and the gap distance was 

observed irrespective of the location of testing, the vision status of the subjects, other 

demographic characteristics, and the kind of mobility aids they used. Thus, our findings 

demonstrate that All_Aboard app could provide a reliable benefit in navigation by accurately 

detecting the bus stop sings and guiding the users close enough to the designated stop that 

makes it less likely that a bus will pass by them for standing too far from the bus stops. 

Importantly, this study validates that computer vision-based object recognition capabilities can 

be used in a complementary way and provide added benefit to purely location-based navigation 

services in real-world settings. 

During the study design, we were expecting some difference in performance in a city vs. 

suburban location based on our previous preliminary study,14 due to the well-known limitations 

of location-based services in areas with tall structures. Therefore, we did not expect a significant 

effect of location on All_Aboard app, and the findings were more or less consistent with this 

expectation. In case of Google Maps, City vs. Suburb setting did not have any significant effect 

on the success rate, and its effect on gap distance relatively modest (slight but statistically 

significant difference was seen only in subjects with residual vision). The primary reason for this 

lack of a location effect was that the localization error in the City was somewhat balanced or 

counteracted by large mapping errors in the Suburb. Thus, despite better localization accuracy 

of Google Maps in the Suburb, large mapping errors meant that almost the same proportion of 

trials were unsuccessful as in the City.      

While we enrolled participants with a wide range of visual abilities – from completely blind up to 

visual acuity of 20/200, we analyzed only the effect residual vision presence on the performance 

with the navigation apps. If the All_Aboard app could successfully guide low vision travelers 

close to the bus stop sign, it was possible that they could use their residual vision (even if it was 

only restricted to shape or perform perception) from there on to navigate further close to the 

sign. We indeed observed this behavior in a few participants. However, as a whole, gap 

distance was not significantly different between those with or without residual vision – indicating 

that the app already guided the subjects close enough to the bus stop sign, such that any 

further change due to residual vision was not large in terms of distance. However, trial success 

rate was affected by residual vision presence in the Suburb, as completely blind subjects 

experienced significantly more failures with All_Aboard app compared to those with residual 

vision. A possible reason for higher failure rate in the Suburb could be because some of the bus 

stop signs on that site were not properly placed. Some were occluded by trees, slanting towards 

the street instead of the sidewalk, or not at the edge of street curb. In these situations, scanning 

sufficiently wide is crucial. However, completely blind subjects tended to scan across a narrow 

range or not scan at all due to complete loss of visual input to help with orientation. Therefore, 

they were more likely to miss signs that are slightly more difficult to find. More training and 

practicing on scanning skills might help improve their success rate in these situations. 

Acknowledgments 

The All_aboard app development was funded in part by Microsoft AI4A award. The authors 

would like thank Nick Corbett and Dinna Rosenbaum from the Carroll Center for the Blind for 

their help with participant recruitment and coordination.           

References 

1 

2 

3 

4 

5 

6 

7 

8 

9 

Crudden, A., McDonnall, M. C. & Hierholzer, A. Transportation: An Electronic Survey of 
Persons who Are Blind or Have Low Vision. Journal of Visual Impairment & Blindness 
109, 445-456, doi:10.1177/0145482x1510900603 (2015). 

Pundlik, S., Shivshanker, P. & Luo, G. Impact of Apps as Assistive Devices for Visually 
Impaired Persons. Annual Review of Vision Science 9, null, doi:10.1146/annurev-vision-
111022-123837 (2023). 

US Department of Justice Civil Rights Division.     (2010). 

Marston, J. R. & Golledge, R. G. The Hidden Demand for Participation in Activities and 
Travel by Persons who are Visually Impaired. Journal of Visual Impairment & Blindness 
97, 475-488, doi:10.1177/0145482x0309700803 (2003). 

Park, J. & Chowdhury, S. Investigating the barriers in a typical journey by public 
transport users with disabilities. Journal of Transport & Health 10, 361-368, 
doi:https://doi.org/10.1016/j.jth.2018.05.008 (2018). 

Visnes Øksenholt, K. & Aarhaug, J. Public transport and people with impairments – 
exploring non-use of public transport through the case of Oslo, Norway. Disability & 
Society 33, 1280-1302, doi:10.1080/09687599.2018.1481015 (2018). 

Wong, S. Traveling with blindness: A qualitative space-time approach to understanding 
visual impairment and urban mobility. Health & Place 49, 85-92, 
doi:https://doi.org/10.1016/j.healthplace.2017.11.009 (2018). 

Low, W.-Y., Cao, M., De Vos, J. & Hickman, R. The journey experience of visually 
impaired people on public transport in London. Transport Policy 97, 137-148, 
doi:https://doi.org/10.1016/j.tranpol.2020.07.018 (2020). 

Jonnalagedda, A. et al. Enhancing the Safety of Visually Impaired Travelers in and 
around Transit Stations. (The Robotics Institute Carnegie Mellon University, 2014). 

10 

11 

12 

13 

14 

15 

16 

17 

18 

Golledge, R. G., Marston, J. R. & Costanzo, C. M. Attitudes of Visually Impaired Persons 
toward the Use of Public Transportation. Journal of Visual Impairment & Blindness 91, 
446-459, doi:10.1177/0145482x9709100505 (1997). 

Azenkot, S. et al. in Proceedings of the SIGCHI Conference on Human Factors in 
Computing Systems    3247–3256 (Association for Computing Machinery, Vancouver, 
BC, Canada, 2011). 

Hara, K. et al. Improving Public Transit Accessibility for Blind Riders by Crowdsourcing 
Bus Stop Landmark Locations with Google Street View: An Extended Analysis. ACM 
Trans. Access. Comput. 6, Article 5, doi:10.1145/2717513 (2015). 

Perkins School for the Blind. BlindWays: a crowdsourced bus stop location app 
https://www.perkins.org/resource/blindways-crowdsourced-bus-stop-location-app/, (last 
accessed Aug. 2023)). 

Jiang, E. et al. Field testing of All Aboard, an AI app for helping blind individuals to find 
bus stops. Investigative Ophthalmology & Visual Science 62, 3529-3529 (2021). 

Luo, G. & Pundlik, S. Widespread Errors in Bus Stop Location Mapping is an 
Accessibility Barrier for Passengers Who are Blind or Have Low Vision. Journal of Visual 
Impairment & Blindness, doi:10.1177/0145482X231201807 (2023). 

Kuyk, T. K., Liu, L. & Fuhr, P. S. Feature Search in Persons with Severe Visual 
Impairment. Vision Research 45, 3224–3234 (2005). 

Luo, G., Satgunam, P. & Peli, E. Visual Search Performance of Patients with Vision 
Impairment: Effect of Jpeg Image Enhancement. Ophthalmic Physiol Opt 32, 421–428 
(2012). 

Sáez, Y., Muñoz, J., Canto, F., García, A. & Montes, H. Assisting Visually Impaired 
People in the Public Transport System through RF-Communication and Embedded 
Systems. Sensors 19, 1282 (2019). 

19 

Alvarado, A. et al. in Transportation Research Board. 

20 

Chen, H.-E., Lin, Y.-Y., Chen, C.-H. & Wang, I.-F. in Proceedings of the 33rd Annual 
ACM Conference Extended Abstracts on Human Factors in Computing Systems    19–
24 (Association for Computing Machinery, Seoul, Republic of Korea, 2015). 

21 

22 

23 

Parker, A. T. et al. Wayfinding Tools for People With Visual Impairments in Real-World 
Settings: A Literature Review of Recent Studies. Frontiers in Education 6, 
doi:10.3389/feduc.2021.723816 (2021). 

El-taher, F. E.-z., Taha, A., Courtney, J. & Mckeever, S. A Systematic Review of Urban 
Navigation Systems for Visually Impaired People. Sensors 21, 3103 (2021). 

Campbell, M., Bennett, C., Bonnar, C. & Borning, A. in Proceedings of the 16th 
international ACM SIGACCESS conference on Computers & accessibility    11–18 
(Association for Computing Machinery, Rochester, New York, USA, 2014). 

24 

BlindSquare. https://www.blindsquare.com/about/, accessed Aug. 2023). 

25 

Lazarillo. Inclusive navigation and digital maps, accessed Aug. 2023). 

26 

Lock, J. C., Cielniak, G. & Bellotto, N. in AAAI Spring Symposia. 

27 

28 

29 

30 

31 

32 

Saha, M., Fiannaca, A. J., Kneisel, M., Cutrell, E. & Morris, M. R. in Proceedings of the 
21st International ACM SIGACCESS Conference on Computers and Accessibility    
222–235 (Association for Computing Machinery, Pittsburgh, PA, USA, 2019). 

Feng, J. et al. Commute Booster: A Mobile Application for First/Last Mile and Middle Mile 
Navigation Support for People with Blindness and Low Vision. IEEE Journal of 
Translational Engineering in Health and Medicine, 1-1, 
doi:10.1109/JTEHM.2023.3293450 (2023). 

Massachusetts Eye & Ear Infirmary. All_Aboard. Find bus stops for the blind 
https://apps.apple.com/us/app/all-aboard/id1580638469, accessed Aug. 2023). 

Sandler, M. & Howard, A. MobileNetV2: The Next Generation of On-Device Computer 
Vision Networks https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-
on.html, 2018). 

Bates, D., Mächler, M., Bolker, B. & Walker, S. Fitting Linear Mixed-Effects Models 
Using lme4. Journal of Statistical Software 67, 1 - 48, doi:10.18637/jss.v067.i01 (2015). 

Brooks, M. E. et al. glmmTMB Balances Speed and Flexibility Among Packages for 
Zero-inflated Generalized Linear Mixed Modeling. The R Journal 9, 378–400, 
doi:10.32614/RJ-2017-066 (2017). 

33 

34 

35 

emmeans: Estimated Marginal Means, aka Least-Squares Means. R package version 
1.7.2. https://CRAN.R-project.org/package=emmeans (2022). 

Lüdecke, D., Ben-Shachar, M., Patil, I., Waggoner, P. & Makowski, D. performance: An 
R Package for Assessment, Comparison and Testing of Statistical Models. Journal of 
Open Source Software 6, 3139, doi:10.21105/joss.03139 (2021). 

DHARMa: Residual Diagnostics for Hierarchical (Multi-Level / Mixed) Regression 
Models. R package version 0.4.1. https://CRAN.R-project.org/package=DHARMa 
(2021). 

36  Wickham, H. ggplot2: Elegant Graphics for Data Analysis Vol. 
https://ggplot2.tidyverse.org. (Springer-Verlag, 2016). 

 
","Field evaluation of a mobile app for assisting blind and visually impaired travelers to find bus stops Shrinivas Pundlik 1 , Prerana Shivshanker 1 , Tim Traut-Savino 2 , Gang Luo 1 1 . Schepens Eye Research Institute of Mass Eye & Ear , Harvard Medical School Boston , MA 2 . The Carroll Center for the Blind , MA Corresponding Author : Gang Luo ( gang.luo @ schepens.harvard.edu ) Keywords : vision aid , navigation , mobility , accessibility , deep learning , computer vision , public transportation Commercial Relationships : The authors declared no potential conflicts of interest with respect to the research , authorship , and/or publication of this article . The All_Aboard app evaluated in this study is released to public for free . There is no revenue from app sale or in-app advertisements . Abstract Purpose : It is reported that there can be considerable gaps due to GPS inaccuracy and mapping errors if blind and visually impaired ( BVI ) travelers rely on digital maps to go to their desired bus stops . We evaluated the ability of a mobile app , All_Aboard , to guide BVI travelers precisely to the bus-stops . Methods : The All_Aboard app detected bus-stop signs in real-time via smartphone camera using a neural network model , and provided distance coded audio feedback to help localize the detected sign . BVI individuals used the All_Aboard and Google Maps app to localize 10 bus- stop locations in Boston downtown and another 10 in a sub-urban area . For each bus stop , the subjects used the apps to navigate as close as possible to the physical bus-stop sign , starting from 30 to 50 meters away . The outcome measures were success rate and gap distance between the app-indicated location and the actual physical location of the bus stop . Results : The study was conducted with 24 legally blind participants ( mean age [ SD ] : 51 [ 14 ] years ; 11 ( 46 % ) Female ) . The success rate of the All_Aboard app ( 91 % ) was significantly higher than the Google Maps ( 52 % , p < 0.001 ) . The gap distance when using the All_Aboard app was significantly lower ( mean [ 95 % CI ] : 1.8 [ 1.2-2.3 ] meters ) compared to the Google Maps ( 7 [ 6.5-7.5 ] meters ; p < 0.001 ) . Conclusion : The All_Aboard app localizes bus stops more accurately and reliably than GPS- based smartphone navigation options in real-world environments . Translational Relevance : The All_Aboard mobile app navigation aid can be potentially help BVI individuals independently travel via public transportation . Introduction Blind and visually impaired ( BVI ) people often rely on public transportation , such as buses and subways , to travel for employment , leisure or for other needs.1 Geolocation and transportation information accessed through smartphones has greatly facilitated macro-navigation for BVI people . For example , one can plan a route and get detailed instructions on mobile devices for point-to-point navigation using public transits . Navigation apps make up one of the major groups of vision assistance mobile apps available in App store and Play store . 2 On the other hand , micro-navigation – navigating precisely to the desired destination at any stage of the journey , remains a largely unsolved issue for BVI individuals . To comply with the Americans with Disabilities Act ( 1990 ) , regional transit agencies are required to comply with regulations regarding the accessibility of transit infrastructures.3 In the context of vision disabilities , the requirements include placing of large-print signage at bus stops , providing braille and tactile information within transit stations , and making stop announcements inside transit vehicles at main points , among others . However , poor interface and lack of cues accessible from distance , for example transit stop signage , are one of the main barriers to equal access to public transportation.4-9 Systemic inaccuracies in the GPS based location services is the underlying problem that leads to micro-navigation challenges faced by people with BVI . This is also referred to as the last 30 feet or last 10 m problem in wayfinding . For example , when navigating to a bus stop , a blind person following any GPS-based navigation app , may arrive at the app indicated location that has a considerable gap ( typically 10 m or 30 feet ) from the actual bus stop due to the localization error in the GPS service . For perspective , this 10 m gap could be almost equal to an entire bus length in certain regions . According to the feedback from blind travelers , sometimes even a small gap can be large enough for them to miss the bus because the bus drivers misunderstand their intention and not stop for them.10-13 In the worst-case scenario , especially in crowded cities , the GPS localization may be off by more than a block , making the macro navigation apps essentially useless in pedestrian mode.14 Weather and location ( density of tall buildings in downtown areas for example ) can further affect GPS-based localization . In addition to localization error , there is a possibility of mapping errors ( sometimes large ) in the stop locations made publicly available by the transit agencies . In our survey of 174 bus stop locations in Boston metro area , about 23 % were mapped more than 2 bus lengths away . 15 Mapping and localization errors together contribute towards making purely location-based services unreliable for micro-navigation tasks , such as finding bus stops . Making matters worse , bus stop signs can be one of many signs on a typical urban street ( among traffic/parking signs and street signs ) , and thus finding it becomes a visual search task , in addition to a plain geolocation task . Since visual search performance is known to be significantly degraded in people with low vision,16,17 it is evident that a navigation aid is needed for micro- navigation with visual search capabilities . One of the conventional wayfinding solutions is the use of Bluetooth beacons to provide micro- location information with high accuracy on nearby landmarks.18-22 This approach ’ s scalability and applicability in outdoor environments are restricted due to the high cost for infrastructure modification and maintenance . On the other hand , smartphones could allow rapid scaling of accessibility . A few smartphone apps have been developed , tested , or released to help blind and visually impaired people access public transportation specifically , or to navigate to destinations in general.23-25 These apps are primarily GPS-based , and therefore still subject to the limitations of GPS-based navigation systems detailed above . In order to achieve localization more accurately , some apps combine location information together with landmark recognition.12,13,26,27 However , landmark maps around the various locations have to be built , maintained , and made widely available prior to use . Combing signage information extracted from GTFS with optical character recognition could provide a viable micro-navigation solution.28 For the purpose of bus stop navigation , a purely visual approach can work well if combined with a typical macro-navigation apps . We have developed a mobile app , All_Aboard that recognizes bus stop signs to help the users navigate within a very short range of the physical location of the sign.29 When using the app , users can first use commonly available macro-navigation tools , such as Google Maps , to arrive within the vicinity of the bus stop location , and then they can scan the surroundings with their smartphone camera . The All_Aboard app detects the bus stop signs in real-time in the smartphone camera imagery , and can guide the users to approach bus stops through auditory cues , with the auditory pitch coding the distance to the target . Our preliminary testing of the All_Aboard app indicated its superior performance compared to Google Maps app.14 The goal of this study was to evaluate All_Aboard in real-world conditions with BVI transit users and compare its localization ability with navigation via Google Maps app . Our primary hypothesis was that the localization based on All_Aboard app was significantly better than just using a conventional navigation app ( Google Maps ) in terms of distance to the desired bus-stop location and rate of successful localizations . Given that GPS-based localization typically suffers in densely built downtown areas , we further hypothesized that All_Aboard might be more effective in these locations compared to more sparsely populated sub-urban areas . Methods All_Aboard App One of the underlying ideas behind the All Aboard app is that the bus stop signage is unique ( different than other road signs ) , uniform in appearance ( typically , but not always ) , and standardized across the entire area of a transit agency ( Figure 1A ) . Moreover , since the bus stop signs for a given transit agency have the same known physical size , one can estimate the approximate distance based on the image size ( width ) of the detected bus stop sign ( the farther the distance , the smaller the image , and vice versa ) . Therefore , it is feasible for computer vision algorithms to learn the appearance of the bus stop signs , detect them in the images captured by smartphone cameras , and estimate the distance to the actual sign . Bus stop detection in All_Aboard is performed in real-time using a MobileNetv2 deep learning neural network,30 trained on about 10000 images of bus stops collected for a given city/region . Images of bus stop signs were collected from the Google Street View imagery based on the publicly available bus stop coordinates via General Transit Feed Specification ( GTFS ) standard . The stop signs were manually labeled by placing a bounding box in the collected images . The trained model runs natively on the smartphone device ( no cloud processing ) . Another key idea behind the operation of All_Aboard is that it supplements a macro-navigation app and thus only need to be operational in the general vicinity of the bus stop . In a typical usage scenario ( Figure 1B ) , the user launches All_Aboard when a macro-navigation app ( Google Maps etc . ) indicates that the user is near the desired bus stop location . The All_Aboard app senses phone orientation and searches only when the device is held in an upright position . The user can then scan with the phone camera in an arc to first determine the angular orientation of the sign . A positive detection leads to a beeping sound and a true positive detection is typically indicated by a series of continuous auditory tones . Once locked in the direction of the sign , the auditory tones change in frequency as the user get near the bus stop sign ( similar to a homing signal ) . Thus , the app can help users gauge relative distance to the sign and adjust their approach . There were four levels of audio tones , with the highest frequency indicated that the detected sign was within 2 meters ( ≈ 6 feet ) . After it is launched , the apps works without needing any active intervention from the user , as long as the device is held in an upright manner . The All_Aboard is available in App store and Play store for free , and it is capable of recognizing bus stops in 10 major cities/regions around the world . Once installed , the users can download the available trained neural network models for the corresponding transit agencies . In this study , the All_Aboard app was loaded with the model trained to detect bus stop signs of the Massachusetts Bay Transportation Authority ( MBTA ) that operates public transit in Boston metro area . B A Figure 1 : ( A ) A typical MBTA bus stop sign at one of the trial location in downtown Boston . This is the most recent version of the signage , however , older versions with slightly different appearance but similar shapes persist throughout the coverage area . Very few bus stops in the area covered by the transit agency are sheltered , and the distinctive sign is often the only visual identification of the bus stop . ( B ) Operation of the All_Aboard app in the general vicinity of a bus stop . The lower inset shows the app detecting the bus stop sign ( the bounding box is drawn around the detected sign in the camera view displayed on the smartphone screen . The upper inset shows a successful detection at night in low light conditions . A demonstration video of All_Aboard app in action can be found at https : //www.youtube.com/watch ? v=VUVpqEw1_2k . Participants The inclusion criteria were : vision status of legal blindness , independent mobility without assistance from a sighted guide , physical ability to walk over a distance of about 1 mile at a given time , and familiarity with smartphone/mobile devices . Participants for this study was recruited via referrals from the Carroll Center for the Blind Newton Massachusetts , practitioners at vision rehabilitation clinics , and via a pool of volunteers who had participated in prior studies . The study protocol was approved by the Institutional Review Board at Mass Eye & Ear . The study followed the tenets of the Declaration of Helsinki and written informed consent was obtained from all the participants . The participants were reimbursed for travel to the study sites and for their time . Study Design The study involved 2 visits at 2 separate study sites : downtown Boston ( City ) and near the campus of the Carroll Center for the Blind in Newton , Massachusetts ( Suburb ) . Each study site involved navigating to 10 bus stops following a specific route ( Figure 2 ) . For each bus stop , performance with All_Aboard and Google Maps apps was evaluated with both the apps running simultaneously . During the study , the participants were accompanied by a certified orientation and mobility instructor ( O & M ) who provided directions along the route and ensured the safety of the participants during the study . Members of the study team also accompanied the participant and the O & M instructor , who administered the study and recorded measurements . For all trials , a preconfigured Android smartphone was provided to the participants . Before starting the study , each participant was provided oral instructions and hands-on training with using the All_Aboard app at a practice location . At the start of the trial at each bus stop location , the participant was guided by O & M instructor to a location that was about 30 to 50 meters ( ≈100 to 150 feet ) away from the bus stop sign along the direction of travel in approximately straight ahead direction . The starting distance from the stop sign was varied at each stop location to dissuade participants from guessing the stop location based on step counting . The path from the starting location to bus stop sign did not involve crossing streets , except in the case of one bus stop location in Newton , where the stop sign was affixed very close to the intersection . At the starting point for each trial , Google Maps app ( operating in the pedestrian directions mode ) was launched by the experimenter , StreetView calibration was done ( this was one of the features of the Google Maps app that uses live camera imagery to geo-locate more accurately ) , and the mapped location of the said bus stop was set as the destination . Then , the All_Aboard app was launched such that both apps were running simultaneously , with Google Maps navigation window in the inset at the bottom of the screen ( Figure 3A ) . Figure 2 : Routes at the two study sites : downtown Boston ( Left ) and in Newton ( Right ) .Each site had 10 bus stop locations ( indicated by numbers ) . The route is indicated by dashed black line . The bus stops in Newton were on the opposite side of the street such that the route was a loop that was traversed in the direction shown by the arrows . Then , the smartphone device was handed over to the participants . From this starting location , the participants was instructed to navigate as close to the bus stop sign as possible . They were also instructed to hold the smartphone upright with its rear camera pointing straight ahead ( Figure 3B ) , and scan side-to-side to determine the relative orientation of the bus stop sign from their walking trajectory . After this point , the participants walked on their own , relying on their habitual mobility aid and their residual vision if present , along with the auditory feedback from the All_Aboard app . Meanwhile , the Google Maps app provided intermittent voice instructions , including distance to the destination in feet ( which the participants soon learned were often unreliable ) . At the end of the trial at a bus stop location , the participants stopped and notified the experimenters when they thought they were closest to the bus stop sign as per their judgment . This was primarily based on the audio feedback by the All_Aboard app – when the audio tone frequency and pitch were at the highest levels indicating the detected sign was very close . A few participants could use their residual vision from this point onward to get even closer . Distance from where they stopped to the actual stop sign was measured with a tape measure . This was the localization distance for All_Aboard app . Google Maps app also indicated via auditory feedback when it determined that the participant arrived at the destination ( Figure 3C ) . The distance between the bus stop sign and the arrival point according to Google Maps was also measured with the tape measure . At the time of their first study visit , the participants answered a brief survey that collected basic demographic information , vision status , use of vision aids , and their preferred transit options ( public transit , rideshare , or private vehicle –family member driving ) . The level of vision was recorded either in terms of self-reported visual acuity in Snellen , or as light perception , or no light perception ( in case of completely blind individuals ) . Figure 3 : Running All_Aboard and Google Maps simultaneously to find bus stops . ( A ) Screenshot of the device at the start point . Both All_Aboard and Google Maps ( inset ) launched and run simultaneously . ( B ) An user holds the phone upright with the rear camera facing straight ahead . ( C ) Screenshot of the device when Google Maps indicate arrival at the destination . The All_Aboard app indicates the physical bus stop sign is still some distance ahead . Outcome Measures The two main outcome measures , separately obtained for each app ( All_Aboard and Google Maps ) , were : the localization error ( gap distance ) in meters and the rate of successful localizations ( success rate ) . As mentioned above , the gap distance was obtained via direct measurement of the distance between app indicated/subject determined location of the bus stop and the physical location of the bus stop sign . When the All_Aboard guided the participant close enough for them to spot ( via their residual vision ) or identify the bus stop sign or touch the pole or post , the gap distance was marked as 0 . If the app indicated location on the ground was beyond the physical bus stop sign with respect to the direction of travel , then the measured gap distance was recorded as negative . A trial instance was deemed as a failure if a reasonable measurable distance was not obtained . Success rate for each app was defined as % of locations for which a valid measurable distance along the travel path was available . At any given bus stop location , trial failures could occur because of various reasons . In case of Google Maps app , incorrect mapping of the bus stops was one of the reasons . Such failures were predictable and repetitive across all the subjects because the location of the bus stop in the map was fundamentally incorrect . One cause of trial failure was the mapped location being more than 100 feet away from the physical bus stop . Another cause of trial failure with Google Maps app was catastrophic inaccuracies in geolocation and consequent navigation directions , for example , when the app directed the users to cross streets , double-back , or go around a corner , which would lead them completely miss the bus stop . These failures were more prevalent in the downtown Boston area with tall buildings and/or on overcast/rainy days . In the case of All_Aboard app , trial failure could occur because of detection failures due to false negatives or deficient technique by the subjects while using the app . Shadows and occlusions could lead to the app to fail to recognize a bus stop sign . Signage largely slanting away from the side walk direction can cause the app fail to detect . On other occasions , the subjects did not scan sufficiently while walking towards the bus stop sign and lost the audio signal . If the bus stop sign was initially detected but then went outside the field of view of the camera as the subject approached , the continuous audio signal suddenly stopped . This was an indicator to the subjects that they either passed the sign or are too close to it . They were allowed to retrace their steps and try again once to zero-in or confirm the presence of the stop sign in the near vicinity . If major intervention by O & M or the experimenter was needed to reorient the subject after initial failure to detect , then the trail was considered as a failure for the All_Aboard app for the given location , even if the sign was successfully detected in the subsequent tries . Statistical Analysis Potential factors of interest affecting the outcome measures were app used ( All_Aboard or Google Maps ) , the study location ( City vs . Suburb ) , and the vision status ( with or without residual vision ) . Completely blind subjects without light perception were categorized as without residual vision , while the rest were with residual vision . Vision in the better eye was used for this categorization . Association of gap distance with these above potential variables was analyzed within-subject via a linear model in repeated measures framework . The success rate was analyzed using a binary logistic regression . In addition to the main effects , the interaction between the above 3 factors were also examined . Estimated marginal means with their 95 % confidence intervals and contrasts are reported for gap distance . Estimated mean marginal probability of success and the 95 % confidence interval is reported from the logistic regression model for success rate . P values < 0.05 were considered statistically significant . Statistical analysis was performed using statistical packages in R ( ver . 4.0.4 ) .31-36 Results Total 25 subjects were recruited , of which , data for both study sites was available for 24 subjects ( see Table 1 for summary of subject characteristics ) . One subject was dropped from the trial after first visit due to the concerns about overall physical fitness required to complete the study . Eleven participants ( 46 % ) were female . A variety of conditions affected the vision of the participants . While all were legally blind in the US , their vision ranged from completely blind to 20/200 vision . The majority walked with long cane and all except one used an iPhone in their daily lives . Public transit was the most preferred transit option , followed by rideshare and private vehicle . N 24 Table 1 : Study participant characteristics . Age ( years ) Median : 55 , IQR : 41 – 61 , Min . : 20 , Max . : 71 Gender Female : 11 ( 46 % ) Vision Vision disorders Available VA measure – N : 11 ( 46 % ) , range : [ 20/200 – 20/1200 ] Light Perception – N : 6 ( 25 % ) No Light Perception – N : 7 ( 29 % ) RoP : 5 , retinitis pigmentosa : 4 , retinal detachment : 3 , glaucoma : 2 ; One case each of : age-related macular degeneration , optic atrophy , aniridia , cone dystrophy , retinal artery occlusion , charge syndrome , monochromacy , diabetic retinopathy , Norrie syndrome Duration of vision loss At birth : 14 ( 58 % ) Acquired : 8 , Median duration : 18 years , range : [ 3 – 41 years ] Data not available : 2 Mobility aids used Long cane : 16 ( 67 % ) Guide dog : 6 ( 25 % ) None : 2 ( 8 % ) Most preferred transit option Habitual smartphone Public Transit - 1st : 11 , 2nd : 7 , 3rd : 3 , NA : 3 Rideshare - 1st : 9 , 2nd : 11 , 3rd : 2 , NA : 2 Private car - 1st : 4 , 2nd : 5 , 3rd : 8 , NA : 7 iPhone 23 ( 96 % ) Table 2 shows the trial instances and other data for both apps and at each study site . Across 24 subjects , there were supposed to be trials at 480 designated bus stops . However , over the course of the study , some bus stops were skipped due to construction or missing bus stop signs , resulting in a total of 48 instances with missing data . Therefore , each app was evaluated in a total of 432 instances . Overall success rate and gap distance measures were substantially better with All_Aboard app than Google Maps . Table 2 : Cumulative statistics for trial data . Both Apps Google Maps All_Aboard Bus stop instances with available data Skipped instances Both sites City Suburb Both sites City 864 458 406 96 22 432 229 203 48 11 432 229 203 48 11 ( missing data ) Number of successful instances Success rate ( % ) Average [ SD ] gap distance ( m ) Suburb Both sites City Suburb Both sites City Suburb 74 626 329 297 72 72 73 37 225 112 113 52 49 56 37 401 217 184 93 95 91 Both sites 3.36 [ 3.65 ] 6.62 [ 4.15 ] 1.54 [ 1.36 ] City 3.04 [ 3.82 ] 6.26 [ 4.89 ] 1.38 [ 1.34 ] Suburb 3.72 [ 3.41 ] 6.97 [ 3.24 ] 1.72 [ 1.36 ] In Table 2 , successful instances with each app are listed independently of each other . When compared pairwise at each bus stop instance ( Table 3 ) , there were only a handful of instances where both apps failed ( 18 out of 432 or about 4 % ) . There were 13 ( 3 % ) instances where All_Aboard failed but Google Maps succeeded , and there were 189 ( 44 % ) instances where Google Maps failed but All_Aboard succeeded . For the former cases , the average [ SD ] gap distance was 9.3 [ 5 ] meters , and for the latter cases , the average [ SD ] gap distance with All_Aboard was 1.6 [ 1.4 ] meters . From 225 successful instances with Google Maps , the arrival location was mapped past the bus stop sign along the direction of travel in 60 instances ( about 27 % ) , with an average gap distance of 7.2 [ 2.9 ] meters . Table 3 : 2x2 tables showing joint successes or failures of the All_Aboard and Google Maps over all 432 bus stop instances . Overall City Suburb Google Maps Success Google Maps Failure Google Maps Success Google Maps Failure Google Maps Success Google Maps Failure All_Aboard Success All_Aboard Failure 212 189 106 116 106 13 18 6 6 7 78 12 There was no significant effect of subject age , gender , or the kind of mobility aid used on the gap distance or on the success rate . The results and discussion is mostly related to 3 key factors : the app used , study site , and subject group based on their vision status . Gap distance ( in meters ) , averaged over vision status and study sites , was significantly smaller with All_Aboard ( mean : 1.8 , 95 % CI : 1.3-2.3 ) compared to Google Maps ( mean : 7.0 , 95 % CI : 6.5-7.5 ; p < 0.001 ) . Gap distance with All_Aboard was significantly smaller than Google Maps in City and Suburb , as well as in subjects with or without residual vision ( Figure 4A ) . The gap distance was significantly larger in completely blind group ( mean : 8.4 , 95 % CI : 7.3-9.5 ) compared to those with residual vision ( mean : 5.4 , 95 % CI : 4.7-6.1 ; p < 0.001 ) in the City with Google Maps . No significant effect of vision status on the gap distance with All_Aboard was observed . A significant effect of study site was seen only in the case of Google Maps in subjects with residual vision , where gap distance in the Suburb ( mean : 6.8 , 95 % CI : 6.1-7.5 ) was significantly larger than that observed in the City ( mean : 5.4 , 95 % CI : 4.7-6.1 ; p < 0.022 ) . Again , no significant effect of study site was observed for gap distance resulting from the All_Aboard app . A B Figure 4 : Outcome measures by app , location , and vision status . ( A ) The gap distance with All_Aboard was significantly smaller than Google Maps in both sites and in both subject groups . Those with residual vision achieved significantly smaller gap distance compared to completely blind with Google Maps in City . Gap distance in City was significantly lower than Suburb in the case of subjects with residual vision with Google Maps . ( B ) The success rate with All_Aboard was significantly higher in both sites and in subjects with and without residual vision . Completely blind individuals when using All_Aboard in the Suburb had significantly lower success rate compared to those with residual vision . For all panels : error bars show 95 % confidence interval of the mean ; significance levels : * * * : p < 0.001 , * * : p=0.001 – 0.01 , and * : p=0.01 – 0.05 ; P value adjustment : BH method for 4 tests . The success rate with All_Aboard was significantly higher than Google Maps across both study sites and both the subject groups ( Figure 4B ) . The overall success rate with All_Aboard ( mean : 0.91 , 95 % CI : 0.87-0.94 ) , averaged over study sites and subject group factors , was about 75 % higher than Google Maps ( mean : 0.52 , 95 % CI : 0.47-0.58 ; p < 0.001 ) . When using All_Aboard in the Suburban location , the success rate for completely blind subjects ( mean : 0.8 , 95 % CI : 0.69- 0.90 ; p < 0.001 ) was slightly but statistically significantly lower compared to those with residual vision ( mean : 0.95 , 95 % CI : 0.91-0.98 ; p < 0.001 ) . Otherwise , there was no significant difference in success rates for any other conditions . Discussion When people ( normally sighted or BVI ) take buses in areas like Boston metro region , where most bus stops are indicated just by a sign on a post , standing even a short distance away from the sign may cause the buses do not stop for them . This accessibility challenge may diminish independence , compromise adoption of affordable transportation for BVI travelers.1 This is just one of the last-10-meter navigation assistance needs of BVI individuals that is unmet . In this study , we evaluated the ability of All_Aboard app relative to the Google Maps navigation app in guiding BVI travelers accurately to bus stop locations in urban and suburban settings . The rate of successful localization of bus stops was substantially higher and the gap distance was much smaller when using All_Aboard app compared to Google Maps navigation . On average , the All_Aboard app was able to guide the subjects within about 2 meters ( 6 feet ) of the bus stop sign , whereas with Google Maps they were likely to be about 7 meters ( 23 feet ) away . The large effect size of All_Aboard app in terms of success rate of localization and the gap distance was observed irrespective of the location of testing , the vision status of the subjects , other demographic characteristics , and the kind of mobility aids they used . Thus , our findings demonstrate that All_Aboard app could provide a reliable benefit in navigation by accurately detecting the bus stop sings and guiding the users close enough to the designated stop that makes it less likely that a bus will pass by them for standing too far from the bus stops . Importantly , this study validates that computer vision-based object recognition capabilities can be used in a complementary way and provide added benefit to purely location-based navigation services in real-world settings . During the study design , we were expecting some difference in performance in a city vs. suburban location based on our previous preliminary study,14 due to the well-known limitations of location-based services in areas with tall structures . Therefore , we did not expect a significant effect of location on All_Aboard app , and the findings were more or less consistent with this expectation . In case of Google Maps , City vs . Suburb setting did not have any significant effect on the success rate , and its effect on gap distance relatively modest ( slight but statistically significant difference was seen only in subjects with residual vision ) . The primary reason for this lack of a location effect was that the localization error in the City was somewhat balanced or counteracted by large mapping errors in the Suburb . Thus , despite better localization accuracy of Google Maps in the Suburb , large mapping errors meant that almost the same proportion of trials were unsuccessful as in the City . While we enrolled participants with a wide range of visual abilities – from completely blind up to visual acuity of 20/200 , we analyzed only the effect residual vision presence on the performance with the navigation apps . If the All_Aboard app could successfully guide low vision travelers close to the bus stop sign , it was possible that they could use their residual vision ( even if it was only restricted to shape or perform perception ) from there on to navigate further close to the sign . We indeed observed this behavior in a few participants . However , as a whole , gap distance was not significantly different between those with or without residual vision – indicating that the app already guided the subjects close enough to the bus stop sign , such that any further change due to residual vision was not large in terms of distance . However , trial success rate was affected by residual vision presence in the Suburb , as completely blind subjects experienced significantly more failures with All_Aboard app compared to those with residual vision . A possible reason for higher failure rate in the Suburb could be because some of the bus stop signs on that site were not properly placed . Some were occluded by trees , slanting towards the street instead of the sidewalk , or not at the edge of street curb . In these situations , scanning sufficiently wide is crucial . However , completely blind subjects tended to scan across a narrow range or not scan at all due to complete loss of visual input to help with orientation . Therefore , they were more likely to miss signs that are slightly more difficult to find . More training and practicing on scanning skills might help improve their success rate in these situations . Acknowledgments The All_aboard app development was funded in part by Microsoft AI4A award . The authors would like thank Nick Corbett and Dinna Rosenbaum from the Carroll Center for the Blind for their help with participant recruitment and coordination . References 1 2 3 4 5 6 7 8 9 Crudden , A. , McDonnall , M. C. & Hierholzer , A . Transportation : An Electronic Survey of Persons who Are Blind or Have Low Vision . Journal of Visual Impairment & Blindness 109 , 445-456 , doi:10.1177/0145482x1510900603 ( 2015 ) . Pundlik , S. , Shivshanker , P. & Luo , G. Impact of Apps as Assistive Devices for Visually Impaired Persons . Annual Review of Vision Science 9 , null , doi:10.1146/annurev-vision- 111022-123837 ( 2023 ) . US Department of Justice Civil Rights Division . ( 2010 ) . Marston , J. R. & Golledge , R. G. The Hidden Demand for Participation in Activities and Travel by Persons who are Visually Impaired . Journal of Visual Impairment & Blindness 97 , 475-488 , doi:10.1177/0145482x0309700803 ( 2003 ) . Park , J . & Chowdhury , S. Investigating the barriers in a typical journey by public transport users with disabilities . Journal of Transport & Health 10 , 361-368 , doi : https : ( 2018 ) . Visnes Øksenholt , K. & Aarhaug , J . Public transport and people with impairments – exploring non-use of public transport through the case of Oslo , Norway . Disability & Society 33 , 1280-1302 , doi:10.1080/09687599.2018.1481015 ( 2018 ) . Wong , S. Traveling with blindness : A qualitative space-time approach to understanding visual impairment and urban mobility . Health & Place 49 , 85-92 , doi : https : ( 2018 ) . Low , W.-Y. , Cao , M. , De Vos , J . & Hickman , R. The journey experience of visually impaired people on public transport in London . Transport Policy 97 , 137-148 , doi : https : ( 2020 ) . Jonnalagedda , A. et al . Enhancing the Safety of Visually Impaired Travelers in and around Transit Stations . ( The Robotics Institute Carnegie Mellon University , 2014 ) . 10 11 12 13 14 15 16 17 18 Golledge , R. G. , Marston , J. R. & Costanzo , C. M. Attitudes of Visually Impaired Persons toward the Use of Public Transportation . Journal of Visual Impairment & Blindness 91 , 446-459 , doi:10.1177/0145482x9709100505 ( 1997 ) . Azenkot , S. et al . in Proceedings of the SIGCHI Conference on Human Factors in Computing Systems 3247–3256 ( Association for Computing Machinery , Vancouver , BC , Canada , 2011 ) . Hara , K. et al . Improving Public Transit Accessibility for Blind Riders by Crowdsourcing Bus Stop Landmark Locations with Google Street View : An Extended Analysis . ACM Trans . Access . Comput . 6 , Article 5 , doi:10.1145/2717513 ( 2015 ) . Perkins School for the Blind . BlindWays : a crowdsourced bus stop location app https : , ( last accessed Aug. 2023 ) ) . Jiang , E. et al . Field testing of All Aboard , an AI app for helping blind individuals to find bus stops . Investigative Ophthalmology & Visual Science 62 , 3529-3529 ( 2021 ) . Luo , G. & Pundlik , S. Widespread Errors in Bus Stop Location Mapping is an Accessibility Barrier for Passengers Who are Blind or Have Low Vision . Journal of Visual Impairment & Blindness , doi:10.1177/0145482X231201807 ( 2023 ) . Kuyk , T. K. , Liu , L. & Fuhr , P. S. Feature Search in Persons with Severe Visual Impairment . Vision Research 45 , 3224–3234 ( 2005 ) . Luo , G. , Satgunam , P. & Peli , E. Visual Search Performance of Patients with Vision Impairment : Effect of Jpeg Image Enhancement . Ophthalmic Physiol Opt 32 , 421–428 ( 2012 ) . Sáez , Y. , Muñoz , J. , Canto , F. , García , A . & Montes , H. Assisting Visually Impaired People in the Public Transport System through RF-Communication and Embedded Systems . Sensors 19 , 1282 ( 2019 ) . 19 Alvarado , A. et al . in Transportation Research Board . 20 Chen , H.-E. , Lin , Y.-Y. , Chen , C.-H. & Wang , I.-F. in Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems 19– 24 ( Association for Computing Machinery , Seoul , Republic of Korea , 2015 ) . 21 22 23 Parker , A. T. et al . Wayfinding Tools for People With Visual Impairments in Real-World Settings : A Literature Review of Recent Studies . Frontiers in Education 6 , doi:10.3389/feduc.2021.723816 ( 2021 ) . El-taher , F . E.-z. , Taha , A. , Courtney , J . & Mckeever , S. A Systematic Review of Urban Navigation Systems for Visually Impaired People . Sensors 21 , 3103 ( 2021 ) . Campbell , M. , Bennett , C. , Bonnar , C. & Borning , A. in Proceedings of the 16th international ACM SIGACCESS conference on Computers & accessibility 11–18 ( Association for Computing Machinery , Rochester , New York , USA , 2014 ) . 24 BlindSquare . https : //www.blindsquare.com/about/ , accessed Aug. 2023 ) . 25 Lazarillo . Inclusive navigation and digital maps , accessed Aug. 2023 ) . 26 Lock , J. C. , Cielniak , G. & Bellotto , N. in AAAI Spring Symposia . 27 28 29 30 31 32 Saha , M. , Fiannaca , A. J. , Kneisel , M. , Cutrell , E. & Morris , M. R. in Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility 222–235 ( Association for Computing Machinery , Pittsburgh , PA , USA , 2019 ) . Feng , J. et al . Commute Booster : A Mobile Application for First/Last Mile and Middle Mile Navigation Support for People with Blindness and Low Vision . IEEE Journal of Translational Engineering in Health and Medicine , 1-1 , doi:10.1109/JTEHM.2023.3293450 ( 2023 ) . Massachusetts Eye & Ear Infirmary . All_Aboard . Find bus stops for the blind https : , accessed Aug. 2023 ) . Sandler , M. & Howard , A. MobileNetV2 : The Next Generation of On-Device Computer Vision Networks https : on.html , 2018 ) . Bates , D. , Mächler , M. , Bolker , B . & Walker , S. Fitting Linear Mixed-Effects Models Using lme4 . Journal of Statistical Software 67 , 1 - 48 , doi:10.18637/jss.v067.i01 ( 2015 ) . Brooks , M. E. et al . glmmTMB Balances Speed and Flexibility Among Packages for Zero-inflated Generalized Linear Mixed Modeling . The R Journal 9 , 378–400 , doi:10.32614/RJ-2017-066 ( 2017 ) . 33 34 35 emmeans : Estimated Marginal Means , aka Least-Squares Means . R package version 1.7.2. https : ( 2022 ) . Lüdecke , D. , Ben-Shachar , M. , Patil , I. , Waggoner , P. & Makowski , D. performance : An R Package for Assessment , Comparison and Testing of Statistical Models . Journal of Open Source Software 6 , 3139 , doi:10.21105/joss.03139 ( 2021 ) . DHARMa : Residual Diagnostics for Hierarchical ( Multi-Level / Mixed ) Regression Models . R package version 0.4.1. https : ( 2021 ) . 36 Wickham , H. ggplot2 : Elegant Graphics for Data Analysis Vol . https : //ggplot2.tidyverse.org . ( Springer-Verlag , 2016 ) .","['field', 'evaluation', 'mobile', 'app', 'assist', 'blind', 'visually', 'impair', 'traveler', 'find', 'bus', 'stop', 'shivshanker', 'trautsavino', 'gang', 'schepen', 'eye', 'ear', 'medical', 'school', 'carroll', 'center', 'blind', 'corresponding', 'author', 'keyword', 'vision', 'aid', 'navigation', 'mobility', 'accessibility', 'deep', 'learning', 'computer', 'vision', 'public', 'transportation', 'commercial', 'relationship', 'author', 'declare', 'potential', 'conflict', 'interest', 'respect', 'research', 'authorship', 'andor', 'publication', 'article', 'allaboard', 'app', 'evaluate', 'study', 'release', 'public', 'free', 'revenue', 'app', 'sale', 'inapp', 'advertisement', 'abstract', 'purpose', 'report', 'considerable', 'gap', 'gps', 'inaccuracy', 'mapping', 'error', 'blind', 'visually', 'impair', 'bvi', 'traveler', 'rely', 'digital', 'map', 'go', 'desire', 'bus', 'stop', 'evaluate', 'ability', 'mobile', 'app', 'allaboard', 'guide', 'bvi', 'traveler', 'precisely', 'busstop', 'method', 'allaboard', 'app', 'detect', 'busstop', 'sign', 'realtime', 'camera', 'use', 'neural', 'network', 'model', 'provide', 'distance', 'code', 'audio', 'feedback', 'help', 'localize', 'detect', 'sign', 'bvi', 'individual', 'use', 'allaboard', 'localize', 'bus', 'stop', 'location', 'suburban', 'area', 'bus', 'stop', 'subject', 'use', 'app', 'navigate', 'close', 'possible', 'physical', 'busstop', 'sign', 'start', 'meter', 'away', 'outcome', 'measure', 'success', 'rate', 'gap', 'distance', 'appindicate', 'location', 'actual', 'physical', 'location', 'bus', 'stop', 'result', 'study', 'conduct', 'legally', 'blind', 'participant', 'mean', 'age', 'sd', 'year', 'female', 'success', 'rate', 'allaboard', 'app', 'significantly', 'high', 'map', 'p', 'gap', 'distance', 'use', 'allaboard', 'app', 'significantly', 'low', 'mean', 'ci', 'meter', 'compare', 'map', 'meter', 'p', 'conclusion', 'localize', 'bus', 'stop', 'accurately', 'reliably', 'base', 'navigation', 'option', 'realworld', 'environment', 'translational', 'relevance', 'navigation', 'aid', 'potentially', 'help', 'bvi', 'individual', 'independently', 'travel', 'public', 'transportation', 'introduction', 'blind', 'visually', 'impair', 'bvi', 'people', 'often', 'rely', 'public', 'transportation', 'bus', 'subway', 'travel', 'employment', 'leisure', 'needs1', 'geolocation', 'transportation', 'information', 'access', 'smartphone', 'greatly', 'facilitated', 'macronavigation', 'bvi', 'people', 'example', 'plan', 'route', 'get', 'detailed', 'instruction', 'mobile', 'device', 'pointtopoint', 'navigation', 'use', 'public', 'transit', 'navigation', 'app', 'make', 'major', 'group', 'vision', 'assistance', 'mobile', 'app', 'available', 'app', 'store', 'play', 'store', 'hand', 'micronavigation', 'navigate', 'precisely', 'desire', 'destination', 'stage', 'journey', 'remain', 'largely', 'unsolved', 'issue', 'bvi', 'individual', 'comply', 'disability', 'regional', 'transit', 'agency', 'require', 'comply', 'regulation', 'regard', 'accessibility', 'transit', 'context', 'vision', 'disability', 'requirement', 'include', 'place', 'largeprint', 'signage', 'bus', 'stop', 'provide', 'braille', 'tactile', 'information', 'transit', 'station', 'make', 'stop', 'announcement', 'transit', 'vehicle', 'main', 'point', 'however', 'poor', 'interface', 'lack', 'cue', 'accessible', 'distance', 'example', 'transit', 'stop', 'signage', 'main', 'barrier', 'equal', 'access', 'public', 'transportation49', 'systemic', 'inaccuracy', 'gps', 'base', 'location', 'service', 'underlie', 'problem', 'lead', 'micronavigation', 'challenge', 'face', 'people', 'bvi', 'also', 'refer', 'last', 'foot', 'last', 'problem', 'wayfinde', 'example', 'navigate', 'bus', 'stop', 'blind', 'person', 'follow', 'gpsbased', 'navigation', 'app', 'arrive', 'app', 'indicate', 'location', 'considerable', 'gap', 'typically', 'foot', 'actual', 'bus', 'stop', 'localization', 'error', 'gps', 'service', 'perspective', 'gap', 'almost', 'equal', 'entire', 'bus', 'length', 'certain', 'region', 'accord', 'feedback', 'blind', 'traveler', 'sometimes', 'even', 'small', 'gap', 'large', 'enough', 'miss', 'bus', 'bus', 'driver', 'misunderstand', 'intention', 'stop', 'worstcase', 'scenario', 'especially', 'crowd', 'city', 'localization', 'block', 'make', 'macro', 'navigation', 'app', 'essentially', 'useless', 'pedestrian', 'mode14', 'weather', 'location', 'density', 'tall', 'building', 'downtown', 'area', 'example', 'far', 'affect', 'gpsbased', 'localization', 'addition', 'localization', 'error', 'possibility', 'mapping', 'error', 'sometimes', 'large', 'stop', 'location', 'make', 'publicly', 'available', 'transit', 'agency', 'survey', 'bus', 'stop', 'location', 'area', 'map', 'bus', 'length', 'away', 'mapping', 'localization', 'error', 'together', 'contribute', 'make', 'purely', 'locationbase', 'service', 'unreliable', 'micronavigation', 'task', 'find', 'bus', 'stop', 'make', 'matter', 'bad', 'bus', 'stop', 'sign', 'many', 'sign', 'typical', 'urban', 'street', 'trafficparke', 'sign', 'street', 'sign', 'thus', 'find', 'become', 'visual', 'search', 'task', 'addition', 'plain', 'geolocation', 'task', 'visual', 'search', 'performance', 'know', 'significantly', 'degrade', 'people', 'low', 'vision1617', 'evident', 'navigation', 'aid', 'need', 'navigation', 'visual', 'search', 'capability', 'conventional', 'wayfinde', 'solution', 'use', 'beacon', 'provide', 'micro', 'location', 'information', 'high', 'accuracy', 'nearby', 'landmarks1822', 'approach', 'scalability', 'applicability', 'outdoor', 'environment', 'restrict', 'high', 'cost', 'infrastructure', 'modification', 'maintenance', 'hand', 'smartphone', 'allow', 'rapid', 'scaling', 'accessibility', 'smartphone', 'app', 'develop', 'test', 'release', 'help', 'blind', 'visually', 'impair', 'people', 'access', 'public', 'transportation', 'specifically', 'navigate', 'destination', 'app', 'primarily', 'gpsbase', 'therefore', 'still', 'subject', 'limitation', 'gpsbased', 'navigation', 'system', 'detail', 'order', 'achieve', 'localization', 'accurately', 'app', 'combine', 'location', 'information', 'together', 'however', 'landmark', 'map', 'various', 'location', 'build', 'maintain', 'make', 'widely', 'available', 'prior', 'use', 'comb', 'signage', 'information', 'extract', 'gtfs', 'optical', 'character', 'recognition', 'provide', 'viable', 'micronavigation', 'solution28', 'purpose', 'bus', 'stop', 'navigation', 'purely', 'visual', 'approach', 'work', 'well', 'combine', 'typical', 'macronavigation', 'app', 'develop', 'mobile', 'app', 'allaboard', 'recognize', 'bus', 'stop', 'sign', 'help', 'user', 'navigate', 'short', 'range', 'physical', 'location', 'sign29', 'use', 'app', 'user', 'first', 'use', 'commonly', 'available', 'macronavigation', 'tool', 'map', 'arrive', 'vicinity', 'bus', 'stop', 'location', 'scan', 'surrounding', 'smartphone', 'camera', 'allaboard', 'app', 'detect', 'bus', 'stop', 'sign', 'realtime', 'smartphone', 'camera', 'imagery', 'guide', 'user', 'approach', 'bus', 'stop', 'auditory', 'cue', 'auditory', 'pitch', 'code', 'distance', 'target', 'preliminary', 'testing', 'allaboard', 'app', 'indicate', 'superior', 'performance', 'compare', 'map', 'app14', 'goal', 'study', 'evaluate', 'allaboard', 'realworld', 'condition', 'bvi', 'transit', 'user', 'compare', 'localization', 'ability', 'navigation', 'primary', 'hypothesis', 'localization', 'base', 'app', 'significantly', 'well', 'use', 'conventional', 'navigation', 'app', 'map', 'term', 'distance', 'desire', 'busstop', 'location', 'rate', 'successful', 'localization', 'give', 'gpsbase', 'localization', 'typically', 'suffer', 'densely', 'build', 'downtown', 'area', 'far', 'hypothesize', 'allaboard', 'effective', 'location', 'compare', 'sparsely', 'populated', 'suburban', 'area', 'method', 'underlie', 'idea', 'app', 'bus', 'stop', 'signage', 'unique', 'different', 'road', 'sign', 'uniform', 'appearance', 'typically', 'always', 'standardize', 'entire', 'area', 'transit', 'agency', 'figure', '1a', 'moreover', 'bus', 'stop', 'sign', 'give', 'transit', 'agency', 'know', 'physical', 'size', 'estimate', 'approximate', 'distance', 'base', 'image', 'size', 'width', 'detect', 'bus', 'stop', 'sign', 'far', 'distance', 'small', 'image', 'vice', 'versa', 'therefore', 'feasible', 'computer', 'vision', 'algorithm', 'learn', 'appearance', 'bus', 'stop', 'sign', 'detect', 'image', 'capture', 'smartphone', 'camera', 'estimate', 'distance', 'actual', 'sign', 'bus', 'stop', 'detection', 'allaboard', 'perform', 'realtime', 'use', 'mobilenetv2', 'deep', 'learn', 'neural', 'network30', 'train', 'image', 'bus', 'stop', 'collect', 'give', 'cityregion', 'image', 'bus', 'stop', 'sign', 'collect', 'view', 'imagery', 'base', 'publicly', 'available', 'bus', 'stop', 'coordinate', 'general', 'transit', 'feed', 'specification', 'gtfs', 'standard', 'stop', 'sign', 'manually', 'label', 'place', 'bounding', 'box', 'collect', 'image', 'train', 'model', 'run', 'natively', 'smartphone', 'device', 'cloud', 'process', 'key', 'idea', 'operation', 'allaboard', 'supplement', 'macronavigation', 'app', 'thus', 'need', 'operational', 'general', 'vicinity', 'bus', 'stop', 'typical', 'usage', 'scenario', 'figure', 'user', 'launch', 'allaboard', 'macronavigation', 'app', 'map', 'indicate', 'user', 'desire', 'bus', 'stop', 'location', 'allaboard', 'app', 'sense', 'phone', 'orientation', 'search', 'device', 'hold', 'upright', 'position', 'user', 'scan', 'phone', 'camera', 'arc', 'first', 'determine', 'angular', 'orientation', 'sign', 'positive', 'detection', 'lead', 'beep', 'sound', 'true', 'positive', 'detection', 'typically', 'indicate', 'series', 'continuous', 'auditory', 'tone', 'lock', 'direction', 'sign', 'auditory', 'tone', 'change', 'frequency', 'user', 'get', 'bus', 'stop', 'sign', 'similar', 'home', 'signal', 'thus', 'app', 'help', 'user', 'gauge', 'relative', 'distance', 'sign', 'adjust', 'approach', 'level', 'audio', 'tone', 'high', 'frequency', 'indicate', 'detect', 'sign', 'meter', 'foot', 'launch', 'app', 'work', 'need', 'active', 'intervention', 'user', 'long', 'device', 'hold', 'upright', 'manner', 'allaboard', 'available', 'app', 'store', 'play', 'store', 'free', 'capable', 'recognize', 'bus', 'stop', 'major', 'citiesregion', 'world', 'instal', 'user', 'download', 'available', 'train', 'neural', 'network', 'model', 'correspond', 'transit', 'agency', 'study', 'allaboard', 'app', 'load', 'model', 'train', 'detect', 'bus', 'stop', 'sign', 'authority', 'operate', 'public', 'transit', 'figure', 'typical', 'mbta', 'bus', 'stop', 'sign', 'trial', 'location', 'downtown', 'recent', 'version', 'signage', 'however', 'old', 'version', 'slightly', 'different', 'appearance', 'similar', 'shape', 'persist', 'coverage', 'area', 'bus', 'stop', 'area', 'cover', 'transit', 'agency', 'shelter', 'distinctive', 'sign', 'often', 'visual', 'identification', 'bus', 'stop', 'b', 'operation', 'allaboard', 'app', 'general', 'vicinity', 'bus', 'stop', 'lower', 'inset', 'show', 'app', 'detect', 'bus', 'stop', 'sign', 'draw', 'detect', 'sign', 'camera', 'view', 'display', 'smartphone', 'screen', 'upper', 'inset', 'show', 'successful', 'detection', 'night', 'low', 'light', 'condition', 'demonstration', 'video', 'app', 'action', 'find', 'https', 'wwwyoutubecomwatch', 'participant', 'inclusion', 'criterion', 'vision', 'status', 'legal', 'blindness', 'independent', 'mobility', 'assistance', 'sighted', 'guide', 'physical', 'ability', 'walk', 'distance', 'mile', 'give', 'time', 'familiarity', 'smartphonemobile', 'device', 'participant', 'study', 'recruit', 'referral', 'carroll', 'center', 'practitioner', 'vision', 'rehabilitation', 'clinic', 'pool', 'volunteer', 'participate', 'prior', 'study', 'study', 'protocol', 'approve', 'institutional', 'review', 'board', 'eye', 'ear', 'study', 'follow', 'tenet', 'declaration', 'write', 'inform', 'consent', 'obtain', 'participant', 'participant', 'reimburse', 'travel', 'study', 'site', 'time', 'study', 'design', 'study', 'involve', 'visit', 'separate', 'study', 'site', 'downtown', 'city', 'campus', 'carroll', 'center', 'blind', 'suburb', 'study', 'site', 'involve', 'navigate', 'bus', 'stop', 'follow', 'specific', 'route', 'figure', 'bus', 'stop', 'performance', 'allaboard', 'map', 'app', 'evaluate', 'app', 'run', 'simultaneously', 'study', 'participant', 'accompany', 'certify', 'orientation', 'mobility', 'instructor', 'provide', 'direction', 'route', 'ensure', 'safety', 'participant', 'study', 'member', 'study', 'team', 'also', 'accompany', 'participant', 'instructor', 'administer', 'study', 'recorded', 'measurement', 'trial', 'preconfigure', 'android', 'smartphone', 'provide', 'participant', 'start', 'study', 'participant', 'provide', 'oral', 'instruction', 'handson', 'training', 'use', 'allaboard', 'app', 'practice', 'location', 'start', 'trial', 'bus', 'stop', 'location', 'participant', 'guide', 'instructor', 'location', 'meter', 'foot', 'away', 'bus', 'stop', 'sign', 'direction', 'travel', 'approximately', 'straight', 'ahead', 'direction', 'starting', 'distance', 'stop', 'sign', 'varied', 'stop', 'location', 'dissuade', 'participant', 'guess', 'stop', 'location', 'base', 'step', 'count', 'path', 'start', 'location', 'bus', 'stop', 'sign', 'involve', 'cross', 'street', 'case', 'bus', 'stop', 'location', 'stop', 'sign', 'affix', 'close', 'intersection', 'starting', 'point', 'trial', 'operate', 'pedestrian', 'direction', 'mode', 'launch', 'experimenter', 'streetview', 'calibration', 'feature', 'use', 'live', 'camera', 'imagery', 'geolocate', 'accurately', 'map', 'location', 'say', 'bus', 'stop', 'set', 'destination', 'allaboard', 'app', 'launch', 'app', 'run', 'simultaneously', 'navigation', 'window', 'inset', 'bottom', 'screen', 'figure', 'figure', 'route', 'study', 'site', 'downtown', 'boston', 'leave', 'right', 'site', 'bus', 'stop', 'location', 'indicate', 'number', 'route', 'indicate', 'dash', 'black', 'line', 'bus', 'stop', 'opposite', 'side', 'street', 'route', 'loop', 'traverse', 'direction', 'show', 'arrow', 'smartphone', 'device', 'hand', 'participant', 'start', 'location', 'participant', 'instruct', 'navigate', 'close', 'bus', 'stop', 'sign', 'possible', 'also', 'instruct', 'hold', 'smartphone', 'upright', 'rear', 'camera', 'pointing', 'straight', 'ahead', 'figure', '3b', 'scan', 'sidetoside', 'determine', 'relative', 'orientation', 'bus', 'stop', 'sign', 'walk', 'trajectory', 'point', 'participant', 'walk', 'rely', 'habitual', 'mobility', 'aid', 'residual', 'vision', 'present', 'auditory', 'feedback', 'allaboard', 'app', 'meanwhile', 'provide', 'intermittent', 'voice', 'instruction', 'include', 'distance', 'destination', 'foot', 'participant', 'soon', 'learn', 'often', 'unreliable', 'end', 'trial', 'bus', 'stop', 'location', 'participant', 'stop', 'notify', 'experimenter', 'think', 'close', 'bus', 'stop', 'sign', 'judgment', 'primarily', 'base', 'audio', 'feedback', 'allaboard', 'app', 'audio', 'tone', 'frequency', 'pitch', 'high', 'level', 'indicate', 'detect', 'sign', 'close', 'participant', 'use', 'residual', 'vision', 'point', 'onward', 'get', 'even', 'close', 'distance', 'stop', 'actual', 'stop', 'sign', 'measure', 'tape', 'measure', 'localization', 'distance', 'also', 'indicate', 'auditory', 'feedback', 'determine', 'participant', 'arrive', 'destination', 'figure', 'distance', 'bus', 'stop', 'sign', 'arrival', 'point', 'accord', 'map', 'also', 'measure', 'tape', 'measure', 'time', 'first', 'study', 'visit', 'participant', 'answer', 'brief', 'survey', 'collect', 'basic', 'demographic', 'information', 'vision', 'status', 'use', 'vision', 'aid', 'preferred', 'transit', 'option', 'public', 'transit', 'rideshare', 'private', 'vehicle', 'family', 'member', 'drive', 'level', 'vision', 'record', 'term', 'selfreporte', 'visual', 'acuity', 'light', 'perception', 'light', 'perception', 'case', 'completely', 'blind', 'individual', 'figure', 'run', 'allaboard', 'map', 'simultaneously', 'find', 'bus', 'stop', 'screenshot', 'device', 'start', 'point', 'allaboard', 'map', 'inset', 'launch', 'run', 'simultaneously', 'b', 'user', 'hold', 'phone', 'upright', 'rear', 'camera', 'face', 'straight', 'ahead', 'c', 'screenshot', 'device', 'map', 'indicate', 'arrival', 'destination', 'app', 'indicate', 'physical', 'bus', 'stop', 'sign', 'still', 'distance', 'ahead', 'outcome', 'measure', 'main', 'outcome', 'measure', 'separately', 'obtain', 'app', 'allaboard', 'map', 'localization', 'error', 'gap', 'distance', 'meter', 'rate', 'successful', 'localization', 'success', 'rate', 'mention', 'gap', 'distance', 'obtain', 'direct', 'measurement', 'distance', 'indicatedsubject', 'determined', 'location', 'bus', 'stop', 'physical', 'location', 'bus', 'stop', 'sign', 'allaboard', 'guide', 'participant', 'close', 'enough', 'spot', 'residual', 'vision', 'identify', 'bus', 'stop', 'sign', 'touch', 'pole', 'post', 'gap', 'distance', 'mark', 'app', 'indicate', 'location', 'ground', 'physical', 'bus', 'stop', 'sign', 'respect', 'direction', 'travel', 'measured', 'gap', 'distance', 'record', 'negative', 'trial', 'instance', 'deem', 'failure', 'reasonable', 'measurable', 'distance', 'obtain', 'success', 'rate', 'app', 'define', 'location', 'valid', 'measurable', 'distance', 'travel', 'path', 'available', 'give', 'bus', 'stop', 'location', 'trial', 'failure', 'occur', 'various', 'reason', 'case', 'incorrect', 'mapping', 'bus', 'stop', 'reason', 'failure', 'predictable', 'repetitive', 'subject', 'location', 'bus', 'stop', 'map', 'fundamentally', 'incorrect', 'cause', 'trial', 'failure', 'mapped', 'location', 'foot', 'away', 'physical', 'bus', 'stop', 'cause', 'trial', 'failure', 'catastrophic', 'inaccuracy', 'geolocation', 'consequent', 'navigation', 'direction', 'example', 'app', 'direct', 'user', 'cross', 'street', 'doubleback', 'go', 'corner', 'lead', 'completely', 'miss', 'bus', 'stop', 'failure', 'prevalent', 'downtown', 'area', 'tall', 'building', 'andor', 'overcastrainy', 'day', 'case', 'trial', 'failure', 'occur', 'detection', 'failure', 'false', 'negative', 'deficient', 'technique', 'subject', 'use', 'app', 'shadow', 'occlusion', 'lead', 'app', 'fail', 'recognize', 'bus', 'stop', 'sign', 'signage', 'largely', 'slant', 'away', 'side', 'walk', 'direction', 'cause', 'app', 'fail', 'detect', 'occasion', 'subject', 'scan', 'sufficiently', 'walk', 'bus', 'stop', 'sign', 'lose', 'audio', 'signal', 'bus', 'stop', 'sign', 'initially', 'detect', 'go', 'field', 'view', 'camera', 'subject', 'approach', 'continuous', 'audio', 'signal', 'suddenly', 'stop', 'indicator', 'subject', 'pass', 'sign', 'close', 'allow', 'retrace', 'step', 'try', 'zeroin', 'confirm', 'presence', 'stop', 'sign', 'near', 'vicinity', 'major', 'intervention', 'experimenter', 'need', 'reorient', 'subject', 'initial', 'failure', 'detect', 'trail', 'consider', 'failure', 'allaboard', 'app', 'give', 'location', 'even', 'sign', 'successfully', 'detect', 'subsequent', 'try', 'statistical', 'analysis', 'potential', 'factor', 'interest', 'affect', 'outcome', 'measure', 'app', 'use', 'allaboard', 'map', 'study', 'location', 'city', 'suburb', 'vision', 'status', 'residual', 'vision', 'completely', 'blind', 'subject', 'light', 'perception', 'categorize', 'residual', 'vision', 'rest', 'residual', 'vision', 'vision', 'well', 'eye', 'use', 'categorization', 'association', 'gap', 'distance', 'potential', 'variable', 'analyze', 'withinsubject', 'linear', 'model', 'repeat', 'measure', 'framework', 'success', 'rate', 'analyze', 'use', 'binary', 'logistic', 'regression', 'addition', 'main', 'effect', 'interaction', 'factor', 'also', 'examine', 'estimate', 'marginal', 'mean', 'confidence', 'interval', 'contrast', 'report', 'gap', 'distance', 'estimate', 'mean', 'marginal', 'probability', 'success', 'confidence', 'interval', 'report', 'logistic', 'regression', 'model', 'success', 'rate', 'p', 'value', 'consider', 'statistically', 'significant', 'statistical', 'analysis', 'perform', 'use', 'statistical', 'package', 'r', 'ver', 'result', 'total', 'subject', 'recruit', 'datum', 'study', 'site', 'available', 'subject', 'see', 'table', 'summary', 'subject', 'characteristic', 'subject', 'drop', 'trial', 'first', 'visit', 'concern', 'overall', 'physical', 'fitness', 'require', 'complete', 'study', 'participant', 'female', 'variety', 'condition', 'affect', 'vision', 'participant', 'legally', 'blind', 'vision', 'range', 'completely', 'blind', 'vision', 'majority', 'walk', 'long', 'cane', 'use', 'iphone', 'daily', 'life', 'public', 'transit', 'preferred', 'transit', 'option', 'follow', 'rideshare', 'private', 'vehicle', 'table', 'study', 'participant', 'characteristic', 'age', 'year', 'median', 'iqr', 'min', 'max', 'gender', 'female', 'vision', 'vision', 'disorder', 'available', 'measure', 'range', 'light', 'perception', 'light', 'perception', 'rop', 'retinitis', 'retinal', 'detachment', 'glaucoma', 'case', 'agerelate', 'macular', 'degeneration', 'optic', 'atrophy', 'aniridia', 'cone', 'dystrophy', 'retinal', 'artery', 'occlusion', 'charge', 'syndrome', 'monochromacy', 'duration', 'vision', 'loss', 'birth', 'acquire', 'median', 'duration', 'year', 'range', 'year', 'datum', 'available', 'mobility', 'aid', 'use', 'long', 'cane', 'guide', 'dog', 'none', 'preferred', 'transit', 'option', 'habitual', 'smartphone', 'public', 'transit', '1st', '2nd', '3rd', 'rideshare', '1st', '2nd', '3rd', 'private', 'car', '1st', '2nd', '3rd', 'iphone', 'table', 'show', 'trial', 'instance', 'datum', 'app', 'study', 'site', 'subject', 'suppose', 'trial', 'designate', 'bus', 'stop', 'however', 'course', 'study', 'bus', 'stop', 'skip', 'construction', 'missing', 'bus', 'stop', 'sign', 'result', 'total', 'instance', 'miss', 'datum', 'therefore', 'app', 'evaluate', 'total', 'instance', 'overall', 'success', 'rate', 'gap', 'distance', 'measure', 'substantially', 'well', 'app', 'map', 'table', 'cumulative', 'statistic', 'trial', 'datum', 'app', 'bus', 'stop', 'instance', 'available', 'datum', 'skip', 'instance', 'site', 'city', 'suburb', 'site', 'city', 'miss', 'data', 'number', 'successful', 'instance', 'success', 'rate', 'average', 'sd', 'gap', 'distance', 'suburb', 'site', 'city', 'suburb', 'site', 'city', 'suburb', 'site', 'city', 'suburb', 'table', 'successful', 'instance', 'app', 'list', 'independently', 'compare', 'pairwise', 'bus', 'stop', 'instance', 'table', 'handful', 'instance', 'app', 'fail', 'instance', 'fail', 'map', 'succeed', 'instance', 'map', 'fail', 'succeed', 'former', 'case', 'average', 'sd', 'gap', 'distance', 'meter', 'latter', 'case', 'average', 'sd', 'gap', 'distance', 'allaboard', 'meter', 'successful', 'instance', 'map', 'arrival', 'location', 'map', 'bus', 'stop', 'sign', 'direction', 'travel', 'instance', 'average', 'gap', 'distance', 'meter', 'table', 'table', 'show', 'joint', 'success', 'failure', 'allaboard', 'map', 'bus', 'stop', 'instance', 'overall', 'city', 'suburb', 'map', 'failure', 'map', 'map', 'failure', 'map', 'map', 'failure', 'allaboard', 'success', 'allaboard', 'failure', 'significant', 'effect', 'subject', 'age', 'gender', 'kind', 'mobility', 'aid', 'use', 'gap', 'distance', 'success', 'rate', 'result', 'discussion', 'mostly', 'relate', 'key', 'factor', 'app', 'use', 'study', 'site', 'subject', 'group', 'base', 'vision', 'status', 'gap', 'distance', 'meter', 'average', 'vision', 'status', 'study', 'site', 'significantly', 'small', 'allaboard', 'mean', 'ci', 'compare', 'map', 'mean', 'ci', 'p', 'gap', 'distance', 'allaboard', 'significantly', 'small', 'map', 'city', 'suburb', 'well', 'subject', 'residual', 'vision', 'figure', '4a', 'gap', 'distance', 'significantly', 'large', 'completely', 'blind', 'group', 'mean', 'compare', 'residual', 'vision', 'mean', 'ci', 'p', 'city', 'map', 'significant', 'effect', 'vision', 'status', 'gap', 'distance', 'allaboard', 'observe', 'significant', 'effect', 'study', 'site', 'see', 'case', 'map', 'subject', 'residual', 'vision', 'gap', 'distance', 'suburb', 'mean', 'ci', 'significantly', 'large', 'observe', 'city', 'mean', 'ci', 'p', '0022', 'significant', 'effect', 'study', 'site', 'observe', 'gap', 'distance', 'result', 'allaboard', 'app', 'b', 'figure', 'outcome', 'measure', 'app', 'location', 'vision', 'status', 'gap', 'distance', 'allaboard', 'significantly', 'small', 'map', 'site', 'subject', 'group', 'residual', 'vision', 'achieve', 'significantly', 'small', 'gap', 'distance', 'compare', 'completely', 'blind', 'map', 'city', 'gap', 'distance', 'city', 'significantly', 'low', 'suburb', 'case', 'subject', 'residual', 'vision', 'success', 'rate', 'allaboard', 'significantly', 'high', 'site', 'subject', 'residual', 'vision', 'completely', 'blind', 'individual', 'use', 'allaboard', 'suburb', 'significantly', 'low', 'success', 'rate', 'compare', 'residual', 'vision', 'panel', 'error', 'bar', 'show', 'confidence', 'interval', 'mean', 'significance', 'level', 'p', 'value', 'adjustment', 'bh', 'method', 'test', 'success', 'rate', 'allaboard', 'significantly', 'high', 'map', 'study', 'site', 'subject', 'group', 'figure', 'overall', 'success', 'rate', 'allaboard', 'mean', 'ci', 'average', 'study', 'site', 'subject', 'group', 'factor', 'high', 'map', 'mean', 'p', 'use', 'allaboard', 'suburban', 'location', 'success', 'rate', 'completely', 'blind', 'subject', 'mean', 'p', 'slightly', 'statistically', 'significantly', 'low', 'compare', 'residual', 'vision', 'mean', 'p', 'otherwise', 'significant', 'difference', 'success', 'rate', 'condition', 'discussion', 'people', 'normally', 'sight', 'bvi', 'take', 'bus', 'area', 'bus', 'stop', 'indicate', 'sign', 'post', 'stand', 'even', 'short', 'distance', 'away', 'sign', 'cause', 'bus', 'stop', 'accessibility', 'challenge', 'diminish', 'independence', 'compromise', 'adoption', 'affordable', 'transportation', 'last10met', 'navigation', 'assistance', 'need', 'bvi', 'individual', 'unmet', 'study', 'evaluate', 'ability', 'app', 'relative', 'app', 'guide', 'bvi', 'traveler', 'accurately', 'bus', 'stop', 'location', 'urban', 'suburban', 'setting', 'rate', 'successful', 'localization', 'bus', 'stop', 'substantially', 'high', 'gap', 'distance', 'much', 'small', 'use', 'allaboard', 'app', 'compare', 'map', 'navigation', 'average', 'allaboard', 'app', 'able', 'guide', 'subject', 'meter', 'foot', 'bus', 'stop', 'sign', 'map', 'likely', 'meter', 'foot', 'away', 'large', 'effect', 'size', 'app', 'term', 'success', 'rate', 'localization', 'gap', 'distance', 'observe', 'irrespective', 'location', 'test', 'vision', 'status', 'subject', 'demographic', 'characteristic', 'kind', 'mobility', 'aid', 'use', 'thus', 'finding', 'demonstrate', 'app', 'provide', 'reliable', 'benefit', 'navigation', 'accurately', 'detect', 'bus', 'stop', 'sing', 'guide', 'user', 'close', 'enough', 'designate', 'stop', 'make', 'less', 'likely', 'bus', 'pass', 'stand', 'far', 'bus', 'stop', 'importantly', 'study', 'validate', 'computer', 'visionbase', 'object', 'recognition', 'capability', 'use', 'complementary', 'way', 'provide', 'add', 'benefit', 'purely', 'locationbase', 'navigation', 'service', 'realworld', 'setting', 'study', 'design', 'expect', 'difference', 'performance', 'city', 'suburban', 'location', 'base', 'previous', 'preliminary', 'study14', 'wellknown', 'limitation', 'locationbase', 'service', 'area', 'tall', 'structure', 'therefore', 'expect', 'significant', 'effect', 'location', 'app', 'finding', 'less', 'consistent', 'expectation', 'case', 'city', 'suburb', 'setting', 'significant', 'effect', 'success', 'rate', 'effect', 'gap', 'distance', 'relatively', 'modest', 'slight', 'statistically', 'significant', 'difference', 'see', 'subject', 'residual', 'vision', 'primary', 'reason', 'lack', 'location', 'effect', 'localization', 'error', 'city', 'somewhat', 'balanced', 'counteract', 'large', 'mapping', 'error', 'suburb', 'thus', 'well', 'localization', 'accuracy', 'map', 'suburb', 'large', 'mapping', 'error', 'mean', 'almost', 'proportion', 'trial', 'unsuccessful', 'city', 'enrol', 'participant', 'wide', 'range', 'visual', 'ability', 'completely', 'blind', 'visual', 'acuity', 'analyze', 'effect', 'residual', 'vision', 'presence', 'performance', 'navigation', 'app', 'allaboard', 'app', 'successfully', 'guide', 'low', 'vision', 'traveler', 'close', 'bus', 'stop', 'sign', 'possible', 'use', 'residual', 'vision', 'even', 'restrict', 'shape', 'perform', 'perception', 'navigate', 'far', 'close', 'sign', 'indeed', 'observe', 'behavior', 'participant', 'however', 'whole', 'gap', 'distance', 'significantly', 'different', 'residual', 'vision', 'indicate', 'app', 'already', 'guide', 'subject', 'close', 'enough', 'bus', 'stop', 'sign', 'change', 'residual', 'vision', 'large', 'term', 'distance', 'however', 'trial', 'success', 'rate', 'affect', 'residual', 'vision', 'presence', 'suburb', 'completely', 'blind', 'subject', 'experience', 'significantly', 'failure', 'app', 'compare', 'residual', 'vision', 'possible', 'reason', 'high', 'failure', 'rate', 'suburb', 'bus', 'stop', 'sign', 'site', 'properly', 'place', 'occlude', 'tree', 'slant', 'street', 'instead', 'sidewalk', 'edge', 'street', 'curb', 'situation', 'scan', 'sufficiently', 'wide', 'crucial', 'however', 'completely', 'blind', 'subject', 'tend', 'scan', 'narrow', 'range', 'scan', 'due', 'complete', 'loss', 'visual', 'input', 'help', 'orientation', 'therefore', 'likely', 'miss', 'sign', 'slightly', 'difficult', 'find', 'training', 'practice', 'scan', 'skill', 'help', 'improve', 'success', 'rate', 'situation', 'acknowledgment', 'allaboard', 'development', 'fund', 'part', 'award', 'author', 'like', 'rosenbaum', 'carroll', 'center', 'blind', 'help', 'participant', 'recruitment', 'coordination', 'reference', 'crudden', 'mcdonnall', 'c', 'hierholzer', 'transportation', 'electronic', 'survey', 'person', 'blind', 'low', 'vision', 'journal', 'visual', 'impairment', 'blindness', 'shivshanker', 'p', 'impact', 'app', 'assistive', 'device', 'visually', 'impair', 'person', 'annual', 'review', 'vision', 'null', 'doi101146annurevvision', 'civil', 'right', 'division', 'r', 'golledge', 'r', 'g', 'hidden', 'demand', 'participation', 'activity', 'travel', 'person', 'visually', 'impair', 'journal', 'visual', 'impairment', 'blindness', 'doi1011770145482x0309700803', 'park', 'j', 'chowdhury', 'investigate', 'barrier', 'typical', 'journey', 'public', 'transport', 'user', 'journal', 'transport', 'health', 'https', 'visne', 'øksenholt', 'aarhaug', 'public', 'transport', 'people', 'impairment', 'explore', 'nonuse', 'public', 'transport', 'case', 'norway', 'disability', 'society', 'travel', 'blindness', 'qualitative', 'spacetime', 'approach', 'understand', 'visual', 'impairment', 'urban', 'mobility', 'health', 'place', 'low', 'wy', 'hickman', 'r', 'journey', 'experience', 'visually', 'impair', 'people', 'public', 'transport', 'transport', 'policy', 'https', 'et', 'enhance', 'safety', 'visually', 'impair', 'traveler', 'transit', 'station', 'robotic', 'golledge', 'r', 'g', 'r', 'costanzo', 'attitude', 'visually', 'impair', 'person', 'use', 'public', 'transportation', 'journal', 'visual', 'impairment', 'blindness', 'doi1011770145482x9709100505', 'azenkot', 'et', 'proceeding', 'human', 'factor', 'computing', 'system', 'association', 'compute', 'machinery', 'hara', 'improve', 'public', 'transit', 'accessibility', 'blind', 'rider', 'crowdsource', 'bus', 'stop', 'landmark', 'location', 'view', 'extended', 'analysis', 'acm', 'tran', 'access', 'comput', 'article', 'doi1011452717513', 'perkin', 'school', 'blind', 'blindway', 'crowdsource', 'bus', 'stop', 'location', 'app', 'https', 'last', 'access', 'field', 'testing', 'app', 'help', 'blind', 'individual', 'find', 'bus', 'stop', 'investigative', 'ophthalmology', 'visual', 'science', 'widespread', 'error', 'bus', 'stop', 'location', 'mapping', 'accessibility', 'barrier', 'passenger', 'blind', 'low', 'vision', 'journal', 'visual', 'impairment', 'blindness', 'kuyk', 'fuhr', 'p', 'feature', 'search', 'person', 'severe', 'visual', 'impairment', 'vision', 'satgunam', 'p', 'peli', 'e', 'visual', 'search', 'performance', 'patient', 'vision', 'impairment', 'effect', 'image', 'enhancement', 'ophthalmic', 'opt', 'sáez', 'garcía', 'monte', 'h', 'assist', 'visually', 'impair', 'people', 'public', 'transport', 'system', 'rfcommunication', 'embed', 'system', 'sensor', 'et', 'transportation', 'research', 'board', 'ch', 'proceeding', '33rd', 'annual', 'acm', 'conference', 'extend', 'abstract', 'human', 'factor', 'compute', 'system', 'association', 'compute', 'machinery', 'parker', 'et', 'wayfinde', 'tool', 'people', 'visual', 'impairment', 'realworld', 'setting', 'literature', 'review', 'recent', 'study', 'frontier', 'education', 'eltaher', 'taha', 'courtney', 'systematic', 'review', 'urban', 'navigation', 'system', 'visually', 'impair', 'people', 'sensor', 'campbell', 'c', 'bonnar', 'borne', 'proceeding', '16th', 'international', 'acm', 'sigaccess', 'conference', 'computer', 'accessibility', 'association', 'compute', 'machinery', 'blindsquare', 'https', 'wwwblindsquarecomabout', 'access', 'lazarillo', 'inclusive', 'navigation', 'digital', 'map', 'access', 'lock', 'j', 'cielniak', 'bellotto', 'saha', 'fiannaca', 'j', 'kneisel', 'e', 'r', 'proceeding', '21st', 'international', 'acm', 'sigaccess', 'conference', 'computer', 'accessibility', 'association', 'compute', 'machinery', 'commute', 'booster', 'mobile', 'application', 'firstlast', 'mile', 'middle', 'mile', 'navigation', 'support', 'people', 'blindness', 'low', 'vision', 'ieee', 'translational', 'engineering', 'health', 'medicine', 'doi101109jtehm20233293450', 'eye', 'ear', 'infirmary', 'allaboard', 'find', 'bus', 'stop', 'blind', 'https', 'access', 'sandler', 'howard', 'mobilenetv2', 'next', 'generation', 'ondevice', 'computer', 'vision', 'network', 'https', 'bate', 'mächler', 'walker', 'fitting', 'linear', 'mixedeffect', 'model', 'use', 'journal', 'statistical', 'software', 'doi1018637jssv067i01', 'balance', 'speed', 'flexibility', 'package', 'zeroinflate', 'generalize', 'linear', 'mixed', 'modeling', 'r', 'journal', 'doi1032614rj2017066', 'emmean', 'estimate', 'marginal', 'mean', 'leastsquare', 'mean', 'r', 'package', 'version', 'http', 'lüdecke', 'benshachar', 'waggoner', 'p', 'makowski', 'performance', 'r', 'package', 'assessment', 'comparison', 'testing', 'statistical', 'model', 'journal', 'open', 'source', 'software', 'dharma', 'residual', 'diagnostic', 'hierarchical', 'multilevel', 'mixed', 'regression', 'model', 'r', 'package', 'version', 'https', 'ggplot2', 'elegant', 'graphic', 'datum', 'analysis', 'vol', 'https']",
"Go Beyond Imagination: Maximizing Episodic Reachability with World
  Models","[{'href': 'http://arxiv.org/abs/2308.13661v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2308.13661v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-08-25 20:30:20,"ORL-AUDITOR: Dataset Auditing in Offline Deep
Reinforcement Learning

3
2
0
2

p
e
S
6

]

R
C
.
s
c
[

1
v
1
8
0
3
0
.
9
0
3
2
:
v
i
X
r
a

Linkang Du†∗, Min Chen‡∗, Mingyang Sun†, Shouling Ji†, Peng Cheng†, Jiming Chen†, Zhikun Zhang†‡§¶
† Zhejiang University, Hangzhou 310017, China
Email: linkangd@gmail.com, sji@zju.edu.cn, saodiseng@gmail.com, cjm@zju.edu.cn
‡CISPA Helmholtz Center for Information Security, Saarbr¨ucken 66123, Germany
Email: min.chen@cispa.de
§Stanford University, Stanford, California 94305, USA
Email: zhikun@stanford.edu

Abstract—Data is a critical asset in AI, as high-quality datasets
can significantly improve the performance of machine learning
models. In safety-critical domains such as autonomous vehicles,
offline deep reinforcement learning (offline DRL) is frequently
used to train models on pre-collected datasets, as opposed to train-
ing these models by interacting with the real-world environment
as the online DRL. To support the development of these models,
many institutions make datasets publicly available with open-
source licenses, but these datasets are at risk of potential misuse
or infringement. Injecting watermarks to the dataset may protect
the intellectual property of the data, but it cannot handle datasets
that have already been published and is infeasible to be altered
afterward. Other existing solutions, such as dataset inference
and membership inference, do not work well in the offline DRL
scenario due to the diverse model behavior characteristics and
offline setting constraints.

In this paper, we advocate a new paradigm by leveraging the
fact that cumulative rewards can act as a unique identifier that
distinguishes DRL models trained on a specific dataset. To this
end, we propose ORL-AUDITOR, which is the first trajectory-
level dataset auditing mechanism for offline RL scenarios. Our
experiments on multiple offline DRL models and tasks reveal
the efficacy of ORL-AUDITOR, with auditing accuracy over
95% and false positive rates less than 2.88%. We also provide
valuable insights into the practical
implementation of ORL-
AUDITOR by studying various parameter settings. Furthermore,
we demonstrate the auditing capability of ORL-AUDITOR on
open-source datasets from Google and DeepMind, highlighting
its effectiveness in auditing published datasets. ORL-AUDITOR
is open-sourced at https://github.com/link-zju/ORL-Auditor.

I.

INTRODUCTION

Deep reinforcement learning (DRL) has been successfully
applied to many complex decision-making tasks, such as
autopilot [17], robot control [3], [51], power systems [70],
intrusions detection [42], [67]. However, for safety-critical
domains, such as robot control, directly interacting with the
environment is unsafe since the partially trained policy may

∗The first two authors made equal contribution.
¶Zhikun Zhang is the corresponding author.

Network and Distributed System Security (NDSS) Symposium 2024
26 February - 1 March 2024, San Diego, CA, USA
ISBN 1-891562-93-2
https://dx.doi.org/10.14722/ndss.2024.23184
www.ndss-symposium.org

risk damage to robot hardware or surrounding objects [55].
To address this issue, researchers propose the offline deep
reinforcement
learning (Offline DRL) [37] paradigm, also
known as full batch DRL [36]. The general idea is learning
from pre-collected data generated by the expert, handcrafted
controller, or even random strategy respecting the system’s
constraints.

[4], Berkeley Artificial

To facilitate the research of offline DRL, several high-
quality datasets are published by third parties such as Deep-
Mind [26],
Intelligence Research
(BAIR) [18], Polixir Technologies [52], TensorFlow [1], and
Max Planck Institute [27]. These datasets are published with
strict open-source licenses, such as GNU General Public
License [4], Apache License [26], [18], [1], [52], and BSD 3-
Clause License [27], to protect the intellectual property (IP) of
the data owner. The licenses typically encompass two essential
terms. 1) Attribution requires you (the users) to appropriately
acknowledge the source, provide a link to the license, and
indicate any modifications made. 2) ShareAlike stipulates that
if you remix,
transform, or build upon the material, you
must distribute your contributions under the same license as
the original. Furthermore, some datasets are accompanied by
additional patent grants aimed at safeguarding the rights of
data publishers, e.g. StarData [40]. Additionally, closed-form
datasets have the potential to face misuse from insider attacks
or intellectual property infringement (e.g., ex-employees steal-
ing data). Biscom’s 2021 survey finds that 25% of respondents
admitted to taking the valuable data when leaving their job,
with 95% citing a lack of policies or technologies to prevent
data theft [5]. Tessian reports that 40% of US employees
take their generated data or trained models when leaving their
job [61]. The defense against the above threats comes to the
question of how a data owner can prove that a suspect model
was derived from its dataset.

Existing Solutions. Recent mainstream solutions for dataset
copyright protection can be classified into three categories:
Watermarking, dataset inference, and membership inference.
The watermarking approach aims to inject samples from
a specific distribution prior to publishing the dataset [39],
the auditor needs a post-event mechanism
[38]. However,
for open-source data since they are already published in the
real world. In contrast to watermarking techniques, dataset
inference strategies [43], [16] do not require the injection of
explicit watermarks [6] into the datasets or trained models.

 
 
 
 
 
 
To implement the auditing, we first train a critic model
to predict the cumulative rewards of the state-action pairs in
the dataset to be audited, i.e., the target dataset. A straightfor-
ward strategy to derive the auditing result is to compare the
cumulative reward of the state-action pairs from the suspect
model to that of the target dataset through a preset judgment
threshold of the similarity. However, designing the threshold
value is challenging, as it depends on the distributions of pre-
collected datasets, which can vary due to different task settings,
collection procedures, and data post-processing methods. To
address this issue, we recognize that the cumulative rewards
embedded in the state-action pairs of the models are the esti-
mated cumulative rewards of the target dataset, as the offline
DRL models fit the cumulative reward of the dataset during
training. Thus, we train multiple models on the target dataset
with varying initializations and optimization, i.e., the shadow
models, and collect the cumulative rewards of their state-action
pairs. Finally, by comparing the cumulative rewards from the
suspect model and the shadow models, we make the audit
decision through hypothesis testing.

Evaluation. The experimental results show that the auditing
accuracy of ORL-AUDITOR exceeds 95% with false positive
rates less than 2.88% across multiple DRL models and tasks.
By visualizing the cumulative rewards from the shadow models
trained on different datasets, we demonstrate that the cumula-
tive reward is a distinguishable feature for the dataset audit. We
further evaluate three influential factors for the practical adop-
tion of ORL-AUDITOR, i.e., the number of shadow models, the
significance level in hypothesis testing, and the trajectory size.
First, more shadow models improve the audit accuracy, and
ORL-AUDITOR demonstrates exceptional performance with
an audit accuracy exceeding 90%, utilizing a mere 9 shadow
models as illustrated in Table VIII. Second, the minimum sig-
nificance level α of ORL-AUDITOR is about 0.001, meaning
that the auditor outputs a single result with 99.9% confidence.
Third, ORL-AUDITOR tends to obtain higher accuracy with a
larger trajectory size, yet we also notice that a small trajectory
size achieves better results under some tasks [46]. We further
implement ORL-AUDITOR to audit the open-source datasets
from Google [18] and DeepMind [26], and the experimental
results again demonstrate the effectiveness of ORL-AUDITOR
in practice.

Robustness. To evaluate the robustness of ORL-AUDITOR,
we have implemented two defense strategies to prevent the
auditing. The first strategy involves using state-of-the-art mem-
bership inference defense techniques, such as the ensemble
architecture proposed by Tang et al. [60] and Jarin et al. [31].
Despite these defense mechanisms,
the audit accuracy of
ORL-AUDITOR is still over 85%. In addition to the ensem-
ble architecture, the suspect models may distort actions to
hide their training dataset. The offline DRL models for real-
world decision-making tasks (i.e., self-driving cars) often use
Gaussian noise to model natural distortions [2]. Thus, adding
Gaussian noise to the actions is stealthy to avoid the auditor’s
detection, and Gaussian noise is convenient for mathematical
manipulation. To simulate strong and weak action distortion,
we normalize all dimensions of the action space to [−1, 1]
and use Gaussian noise with (µ = 0, σ = 0.1) and (µ =
0, σ = 0.01), respectively. Our experiments show that ORL-
AUDITOR is only slightly affected by Gaussian noise with

Fig. 1: Intuitive explanation of ORL-AUDITOR. The middle
surface is the cumulative rewards of the state-action pairs from
a dataset. The auditor outputs a positive result if the cumulative
rewards of a suspect model’s state-action pairs are between the
two outer surfaces.

Maini et al. [43] and Dziedzic et al. [16] have separately
proposed dataset inference methods for supervised learning
and self-supervised learning models, enabling the model owner
to provide a convincing statistical argument that a particular
model is trained on their private data. However, the dataset
inference with labels [43] needs distances between data and
decision boundaries, which is not possible to obtain in RL with
continuous outputs. The dataset inference without labels [16]
uses the similarity of model behaviors to detect unauthorized
dataset usage. It requires a public dataset to generate some
surrogate models, and forms the auditing basis by comparing
the behavioral difference between the surrogate models and
the models trained on their private data. In offline RL scenes,
since the distributions of the collected datasets depend on both
environment and operator [18], it is difficult to determine a
suitable public dataset to train the surrogate model, making
the audit basis hard to establish. The third category adopts the
notion of membership inference [47], [24], [23]. By collecting
the RL models’ behaviors on the trained examples (members)
and the untrained examples (non-members), a classifier is
constructed to determine whether a data sample is used in the
model’s learning process. However, unlike online scenarios in
[47], [24], [23], the auditor cannot collect additional data from
the environment as the non-member examples in offline cases,
where the auditor does not have access to the environment.

Our Proposal.
In this paper, we propose the first practical
dataset auditing paradigm for the offline RL model (ORL-
AUDITOR). Concretely, we are inspired by the fact that the
cumulative reward, i.e., the sum of all rewards received over a
period of time starting from a given state-action pair, guiding
the RL model to learn the behavior policy. Thus, the cumu-
lative reward is an intrinsic feature of the datasets, making
it suitable as an audit basis. Figure 1 provides a schematic
diagram of ORL-AUDITOR, where the state, the action, and
the cumulative reward compose a three-dimensional space. The
middle surface illustrates the exact cumulative reward of the
dataset, and the other two surfaces show possible offsets of the
exact cumulative reward learned by the offline DRL models
due to the randomness in the initialization and the learning
processes. For a suspect model, the auditor outputs a positive
result, i.e., the data is used to train this model, if the cumulative
reward from its state-action pair falls between the two surfaces;
otherwise, a negative outcome.

2

Cumulative Reward

𝑄𝑄 𝑠𝑠, 𝑎𝑎 + ∆

𝑄𝑄(𝑠𝑠, 𝑎𝑎)

𝑄𝑄 𝑠𝑠, 𝑎𝑎 − ∆

State:

𝑠𝑠

Action:

𝑎𝑎

Positive
Negative

Share

Help/Contact

 
 
 
 
 
 
(µ = 0, σ = 0.01). For σ = 0.1, the TPR values of ORL-
AUDITOR decline, yet the strong distortion also impacts the
performance of the suspect model, especially in complex tasks.

Contributions. Our contributions are three-fold:

• To our knowledge, ORL-AUDITOR is the first dataset audit-
ing method for the offline DRL models, using the cumulative
reward as an intrinsic and stable fingerprint of the dataset.
• We demonstrate the effectiveness of ORL-AUDITOR on four
offline DRL models and three tasks. We also systematically
analyze various experimental factors, i.e., the hyperparam-
eter settings and the robustness of ORL-AUDITOR, and
summarize some important guidelines for adopting ORL-
AUDITOR in practice.

• By implementing ORL-AUDITOR on the open-source
datasets from DeepMind [26] and Google [18], we show
that ORL-AUDITOR can serve as a potent audit solution in
real-world offline DRL scenarios.

II. BACKGROUND

A. Offline RL Problem

The offline reinforcement learning (offline RL) model aims
to learn an optimal (or nearly optimal) policy from a pre-
collected dataset D without an interactive environment. We use
S and A to represent the RL models’ input and output space,
formally called state and action in RL scenes. rt ∈ R is the
temporal reward for each time step, where R is the real number
set. A unit in a pre-collected dataset called transition is a four-
element set: {st, at, rt, st+1}, where st ∈ S, at ∈ A, and
st+1 ∈ S is the successive state of st. And a set of transitions
in chronological order forms a trajectory in dataset D. Based
on the transitions, the offline RL model learns the Markov
Decision Process underneath the datasets and forms a policy
πθ(a | s) to maximize J(π).

J(π) = Est∼dβ (s, a), at∼πθ(a|s)

(cid:35)

γtrt

,

(cid:34) H
(cid:88)

t=0

where we use dβ to denote the distribution over states and
actions in dataset D, and the actions are sampled according to
the behavior policy at ∼ πθ(a | s). The discount factor γ is
applied to discount future rewards in the accumulated reward.
H is the terminal time step of one trajectory.

Example. Figure 2 shows an example based on the “CartPole”
task. 1 In the data collection process, the dataset is generated
from the operation logs between the operator and the envi-
ronment, which contains the position and velocity of the cart
and the pole (i.e., state), the operator’s force direction (i.e.,
action), and the corresponding rewards. Then, in the training
and evaluation process, the offline RL model learns how to
play the “Cartpole” task from only the pre-collected dataset
generated through the data collection process. Finally, we
deploy the well-trained offline RL model in the environment
to perform the task.

1https://www.gymlibrary.dev/environments/classic control/cart pole/

Fig. 2: A running example of the offline DRL models.

B. Offline RL Models

In this section, we first introduce two offline RL algo-
rithms [21], [19], [35] separately representing two basic ideas
of the offline RL models, i.e., the policy constraints strategy
and the value function regularization strategy [50]. Many state-
of-the-art model-free offline RL methods [68], [32], [20], [35]
have been modified from these two approaches. We further
present a state-of-the-art algorithm [20] which is minimalistic
with light computation and hyperparameter setting overhead.
In addition, we briefly describe the behavior clone method
(BC) [49], which learns the state-action distribution over the
dataset via a supervised learning approach. Though BC is not a
typical reinforcement learning method, it can solve the offline
RL problem and usually serves as the baseline method in the
offline RL evaluation.

Behavior Clone (BC) [49]. BC separately takes the pairwise
state s and action a in the datasets as input and label, then it
optimizes the policy through the following function.

θ∗ = arg min

θ

E(s,a)∼D [L (πθ(s), a)] ,

where D is the pre-collected dataset and L is the loss function.
Since BC only imitates action distributions, the performance is
close to the mean of the dataset, even though BC works better
than online RL algorithms in most cases.

Batch-Constrained Q-learning (BCQ) [21], [19]. BCQ is
the first practical data-driven offline RL algorithm. The key
idea of BCQ is to integrate a generative model to achieve
the notion of batch-constrained, i.e., minimizing the deviation
between the candidate actions with the action records of the
dataset. To maintain the diversity of action, BCQ builds a
perturbation model to perturb each selected action. Then it
chooses the highest-valued action through a Q-network, that
learns to estimate the expected cumulative reward of a given
state and action pair. Thus, the objective function of BCQ can
be defined as the following.

π(s) = argmax

Qθ (s, ai + ξϕ (s, ai, Φ))

ai+ξϕ(s,ai,Φ)
{ai ∼ Gω(s)}n

i=1 ,

where Gω(s) is a conditional variational auto-encoder (VAE)-
based [33] generative model that can be used to generate
candidate actions. The value function Qθ is used to score
the n candidate actions and finds the action with the highest
value. ξϕ (s, ai, Φ) is the perturbation model, which outputs

3

Dataset

… , 𝒔𝒔𝒕𝒕, 𝒂𝒂𝒕𝒕, 𝒓𝒓𝒕𝒕, 𝒔𝒔𝒕𝒕""𝟏𝟏 , …

State, Reward

Move Left, R = 1

Move Right, R = 1

Move Left, R = 1

Action

Environment

Operator

Offline DRL Model

Environment

Action

Offline DRL Model

State

Data Collection

Training and Evaluation

Deployment

an adjustment to an action a in the range [−Φ, Φ]. Then,
the perturbation model can be optimized by the deterministic
policy gradient algorithm [58] as follows.

ϕ ← argmax

(cid:88)

ϕ

(s,a)∈B

Qθ (s, a + ξϕ(s, a, Φ)) ,

where B represents a mini-batch state-action pair in the dataset.
To penalize rare states, BCQ takes a convex combination of
the values from two Q-networks and sets a new target value
y to update both Q-networks.
(cid:20)

y = r+γ max

ai

λ min
j=1,2

Qθ′

j

(s′, ai) + (1 − λ) max
j=1,2

Qθ′

j

(cid:21)
(s′, ai)

where ai corresponds to the perturbed actions, sampled from
the generative model Gω(s).

Implicit Q-Learning (IQL) [35]. Compared to the batch-
constrained idea of BCQ [21], [19], IQL strictly avoids query-
ing values of the actions, which are not in the pre-collected
dataset. IQL first constructs a model to evaluate the expected
returns of state-action pairs. The objective function is defined
as shown in Equation 1.

L(θ) = ED

(cid:2)Lτ

2

(cid:0)r(s, a) + γQˆθ (s′, a′) − Qθ(s, a)(cid:1)(cid:3) ,

(1)

2 (u) = |τ − 1(u < 0)|u2, and s′ and a′ represent
where Lτ
the successor state and action of s and a. Both Qθ(s, a)
and Qˆθ are used to assess the expected returns of state-
action pairs. The parameters of Qθ(s, a) are adjusted in each
optimization round, while the parameters of Qˆθ are updated
periodically based on Qθ(s, a) to reduce parameter fluctuations
during model updates. Equation 1 involves the dynamics of the
environment, where the environment state s transitions to the
next environment state s′, potentially introducing interference
in the evaluation of expected returns for state-action pairs. IQL
addresses this issue by introducing a new state value model,
splitting Equation 1 into two objective functions. Equation 2
shows the objective function of the state value model Vψ.

LV (ψ) = ED

(cid:2)Lτ

2

(cid:0)Qˆθ(s, a) − Vψ(s)(cid:1)(cid:3) .

(2)

Then, IQL utilizes Vψ(s) to construct Equation 3 for

updating the parameters of the state-action value model Qθ.

LQ(θ) = ED

(cid:104)

(r(s, a) + γVψ (s′) − Qθ(s, a))2(cid:105)

.

(3)

Finally, IQL considers using the state-action value model
to construct a behavior policy for deployment. This behavior
policy also needs to avoid actions that are outside the dataset
distribution. Thus, IQL employs advantage-weighted regres-
sion to update the policy model.
Lπ(ϕ) = ED [exp (β (Qθ(s, a) − Vψ(s))) log πϕ(a | s)] , (4)

where β ∈ [0, ∞) represents the inverse temperature. For
smaller values of β, IQL is similar to behavior clone, tending
to mimic the data collection policy. For larger values of β,
IQL is more inclined to select actions corresponding to the
highest expected returns according to the state-action value
model. Throughout the entire training process, IQL alternates
between optimizing the parameters θ and ψ, and then updates
ϕ while keeping θ and ψ fixed.

TD3PlusBC [20]. The former methods [21], [19], [35] limit or
regularize action selection such that the learned policy is easier
to evaluate with the given dataset. However, they introduce new
hyperparameters and often leverage secondary components,
such as generative models, while adjusting the underlying RL
algorithm. TD3PlusBC is a minimalist and highly effective
offline RL algorithm based Twin Delayed Deep Deterministic
Policy Gradient (TD3) [22] with BC regularization term, which
pushes the policy towards favoring actions contained in the
dataset D:

π = argmax

E(s,a)∼D

,

π

(cid:2)λQ(s, π(s)) − (π(s) − a)2(cid:3) ,

α

1
N

(cid:80)

where λ =
(s,a)|Q(s,a)| for the dataset of N transitions
(s, a). To facilitate the policy training, TD3PlusBC normalizes
each state in the given dataset by si = si−µ
σ+ϵ , where µ and σ
are the mean and standard deviation respectively.

The model architectures vary significantly regarding objec-
tive function and basic model structure. 1) Objective Function:
BCQ [21], [19] and TD3PlusBC [20] use a policy constraints
strategy to maintain the learned policy similar to the one
used for collecting the dataset. In contrast, IQL [35] adopts
a regularization strategy to improve the stochasticity of the
learned policy or obtain more accurate Q-value estimations.
2) Basic Model Structures: BCQ [21], [19] and IQL [35]
are based on the Q-learning model, while TD3PlusBC [20]
builds upon TD3 [22]. In Section V, our experiments are
mainly conducted on the above four algorithms. However,
ORL-AUDITOR can also be applied to any type of offline
DRL model as long as the auditor has black-box access to
the suspect model.

III. PROBLEM STATEMENT AND EXISTING SOLUTIONS

A. System and Threat Model

the dataset

Application Scenarios.
Figure 3 illustrates a typical ap-
plication scenario where the data providers collect and then
publish or sell
to the customers. A malicious
customer (adversary) with access to the datasets makes a piracy
distribution or illegally builds a Model-as-a-Service (MaaS)
platform. Institution 1 suspects the models are generated by
its dataset, and thus hires an auditor to determine whether the
model trainers pirate the trajectories of the dataset D1.

Auditor’s Background Knowledge and Capability. The
auditor has full knowledge of the target dataset, such as the
number of trajectories and the spaces of state and action. In
offline RL settings, the auditor is prohibited from interacting
with the online environment to collect more data, meaning the
entire auditing only depends on the target dataset. We consider
the auditor has black-box access to the suspect RL model.
Note that this is the most general and challenging scenario
for the auditor. A typical application scenario is that an adver-
sary receives the model settings from customers, such as the
selected offline RL framework, the model’s hyperparameter,
and the desired training episodes. Then, the adversary trains
an offline RL model and provides a service interface to the
customers. The auditor utilizes the states of the dataset (inputs)
to query the suspect model and obtain the corresponding
actions (outputs).

4

explicit watermarks [6] to the datasets or the trained models.
Existing methods [43], [16] can be divided into two categories
according to whether they have explicit classification labels.
With the explicit classification labels, [43] rely on computing
the distances between data points and decision boundaries.
Without
the explicit classification labels, [16] utilizes the
similarity of the models’ behaviors to detect the unauthorized
usage of the dataset, which requires the assumption of an
additional public dataset with a similar distribution to form
the auditing basis.

However, the above methods cannot directly be applied
to reinforcement learning cases due to two reasons. First, the
label-based dataset inference [43] cannot be implemented in
the RL models since their outputs are usually continuous, and
they are guided by the rough reward signals instead of the exact
labels. Second, the distribution of the offline RL dataset not
only depends on the environment but also relies on the strategy
of interacting with the environment [18]. Thus, it is challenging
to find a proper public dataset in offline RL scenarios. As we
delve into Appendix A, it becomes evident that the behavior
similarity of the DRL models varies across different public
training data. Furthermore,
the behavior similarity is also
influenced by various offline DRL frameworks.

Membership Inference Attack against RL [47], [24], [23].
Several membership inference attacks exist against DRL,
which seem to address the problem studied in this paper. Most
of them are targeted at the online RL scenes, assuming that
the attacker owns the environment. Thus, they can utilize the
environment to collect more data and even manipulate some
adversarial states to facilitate the inference.

However, in this paper, we aim at the offline RL cases,
which are more challenging since the only thing the auditor can
use is the pre-collected dataset. That is, in offline RL scenarios,
the existing MIA against RL cannot rely on the environment
to generate non-member data.

IV. ORL-AUDITOR

We instantiate Q of Figure 1 with the cumulative reward,
which is an intrinsic feature of the dataset and suitable for
auditing. ∆ is determined by the shadow models trained on the
datasets instead of a preset threshold to adapt the distribution of
different datasets. Thus, the well-designed Q and ∆ guarantee
the adaptiveness and effectiveness of ORL-AUDITOR.

A. Workflow

For ease of understanding, we refer to the target dataset as
the dataset to be audited and the actual dataset as the dataset
used by the suspect model. If the suspect model is trained
on the target dataset, the actual dataset is the same as the
target dataset, i.e., positive audit result for the suspect model;
otherwise, the suspect model does not use the target dataset,
i.e., negative audit result for the suspect model. Figure 4
illustrates the workflow of ORL-AUDITOR.

Fig. 3: An example of the application scenario. The auditor can
obtain all information about dataset D1 but has no knowledge
about the datasets from other institutions.

Discussion. Compared to the sample-level and dataset-level
data in DNN scenes, RL has trajectory-level data, which is
the minimum record unit of sequential interactions between
the operator and environment. Since a single trajectory can
guide the model from the initial state to the terminal, the
trajectory-level data is regarded as the value unit of the dataset.
Thus, ORL-AUDITOR is designed to audit the dataset from
the trajectory level, where the auditor tries to decide whether
the suspect model uses a specific trajectory in the dataset. In
addition, the auditor can easily extend ORL-AUDITOR to the
dataset-level data by setting a piracy alarm threshold. If the
ratio of misappropriation using trajectories exceeds the preset
threshold, the auditor can claim the dataset-level pirate.

B. Existing Solutions

Watermarking [39], [38]. Watermarking-based dataset copy-
right protection methods inject samples of a specific distribu-
tion before publishing the target dataset. One of its kind is
implemented with backdoor attacks against the ML model. Li
et al. [39] proposed to modify a dataset by adding a trigger,
such as a local patch, to innocent samples in order to make
them appear as a pre-defined target class. To verify the integrity
of the dataset after the attack,
they use a hypothesis test
approach based on posterior probabilities generated by a third-
party model. Inspired by this idea, the auditor can employ the
backdoor attack against the DRL model [34], [64], [66] to
generate a watermark for the offline RL dataset.

However, since the open-source datasets are already pub-
lished, the auditor needs a post-event mechanism that does
not require injecting manipulated samples before publishing
the dataset. Watermarking, on the other hand, is a pre-event
mechanism that involves injecting manipulated samples into
the dataset before publishing. Additionally, it is difficult for
the auditor to guarantee that one effective watermarking has
a consistent distribution with the original dataset, which in-
evitably disturbs the model’s normal behavior.

Dataset Inferences [43], [16]. The core idea of dataset
inference is empowering the model owner to make a com-
pelling statistical argument that a particular model is a copied
version of their own model by demonstrating that it is based
on their private training data. It does not require injecting

Step 1: Model Preparation (MP).
In the left box of
Figure 4, the auditor prepares the critic model and the shadow
DRL models based on the target dataset, which contains m
trajectories T with the length of ni (i ∈ {1, 2, . . . , m}). The
critic model is optimized to estimate the cumulative reward

5

𝟏𝟏
𝒔𝒔𝟏𝟏

𝟏𝟏
, 𝒂𝒂𝟏𝟏

𝟏𝟏
, 𝒓𝒓𝟏𝟏

𝟏𝟏
, 𝒔𝒔𝟐𝟐

𝟏𝟏
𝒔𝒔𝟐𝟐

𝟏𝟏
, 𝒂𝒂𝟐𝟐

𝟏𝟏
, 𝒓𝒓𝟐𝟐

𝟏𝟏
, 𝒔𝒔𝟑𝟑

𝒋𝒋
𝒔𝒔𝟏𝟏
𝒎𝒎
𝒔𝒔𝟏𝟏

𝒋𝒋
, 𝒂𝒂𝟏𝟏
𝒎𝒎
, 𝒂𝒂𝟏𝟏

𝒋𝒋
𝟐𝟐
, 𝒔𝒔𝟐𝟐
, 𝒓𝒓𝟏𝟏
𝒎𝒎
𝒎𝒎
, 𝒔𝒔𝟐𝟐
, 𝒓𝒓𝟏𝟏

𝒋𝒋
𝒋𝒋
, 𝒂𝒂𝟐𝟐
𝒔𝒔𝟐𝟐
𝒎𝒎
𝒔𝒔𝟐𝟐

𝒋𝒋
, 𝒓𝒓𝟐𝟐
𝒎𝒎
, 𝒂𝒂𝟐𝟐

𝒋𝒋
, 𝒔𝒔𝟑𝟑
𝒎𝒎
, 𝒓𝒓𝟐𝟐

𝒎𝒎
, 𝒔𝒔𝟑𝟑

…

…

…

𝟏𝟏
𝒔𝒔𝒏𝒏𝟏𝟏

𝟏𝟏
, 𝒂𝒂𝒏𝒏𝟏𝟏

𝟏𝟏
, 𝒓𝒓𝒏𝒏𝟏𝟏

𝟏𝟏
, 𝒔𝒔𝒆𝒆𝒏𝒏𝒆𝒆

𝒋𝒋
𝒋𝒋
𝒋𝒋
, 𝒂𝒂𝒏𝒏𝒋𝒋
𝒔𝒔𝒏𝒏𝒋𝒋
, 𝒓𝒓𝒏𝒏𝒋𝒋
𝒎𝒎
𝒎𝒎
, 𝒂𝒂𝒏𝒏𝒎𝒎
𝒔𝒔𝒏𝒏𝒎𝒎

𝒋𝒋
, 𝒔𝒔𝒆𝒆𝒏𝒏𝒆𝒆
𝒎𝒎
𝒎𝒎
, 𝒓𝒓𝒏𝒏𝒎𝒎
, 𝒔𝒔𝒆𝒆𝒏𝒏𝒆𝒆

Transition

Trajectory

Institution 1

Dataset

Institution 2

Dataset

Institution 3

Dataset

𝐷𝐷""

𝐷𝐷#

𝐷𝐷$

A
d
v
e
r
s
a
r
y

Model

𝜋𝜋!Model

𝜋𝜋""Model

𝜋𝜋""

Auditor for 
Institution 1 

The models do (or do not)
pirate the
-th Trajectory
of Dataset

.

𝒋𝒋

𝑫𝑫𝟏𝟏

Fig. 4: The workflow of ORL-AUDITOR contains three steps, i.e., model preparation, cumulative reward collection, and audit
process. ORL-AUDITOR first trains a set of shadow DRL models and a critic model on the target dataset, then collects the
cumulative rewards from the state-action pairs of the shadow models and the suspect model. Finally, ORL-AUDITOR audits
every trajectory based on hypothesis testing.

of each state-action pair. For each trajectory in the dataset,
a series of predictions for its state-action pairs compose the
exclusive feature for auditing. There are two ways to optimize
the critic model, i.e., the Monte-Carlo-based (MC-based) and
the temporal-difference-based (TD-based) strategies. We adopt
the TD-based learning method and explain the reasons in
Section IV-B. In addition, the auditor trains a set of shadow
models following the model’s objective function introduced in
Section II with different model initializations.

Step 2: Cumulative Reward Collection (CRC).
In the
middle box,
the shadow models observe the states of the
dataset and take actions. For i-th trajectory in the dataset,
the auditor records the state si
t and the action ai
t of each
shadow model, where ai
t represents the shadow model’s action
at the t-th step of trajectory Ti. After finishing the action
collection, the auditor obtains the k sets of state-action pairs
from the shadow models, representing the learned policies
with different initialization and training processes on the target
dataset. Using the critic model in Step 1, the auditor calculates
the estimations for all state-action records, i.e., the estimated
cumulative rewards, which are the samplings of the exact
cumulative rewards of the corresponding state-action pairs in
the dataset. Similarly, the auditor queries the suspect model
with state si
t. The state-action pairs
are then put into the critic model and to obtain the estimations
for the suspect model.

t and observes the action ai

Step 3: Audit Process (AP). After the above two steps,
the auditor obtains the estimated cumulative rewards from
the shadow models and the suspect model and then con-
ducts the audit process. For j-th (j ∈ {1, 2, . . . , m}) tra-
jectory of the dataset, the auditor collects k series of the
estimated cumulative rewards from the shadow models, i.e.,
{Qi
j | i ∈ {1, 2, . . . , k}}, and one from the suspect model,
i.e., Qs
j. ORL-AUDITOR conducts hypothesis testing based
j from ¯Qj. The auditor can
on the distances of Qi
j, ¯Qj) is out of the distribution of
rule out suspicion if d(Qs
j, ¯Qj) | i ∈ {1, 2, . . . , k}}. Otherwise, the auditor will
{d(Qi
conclude a positive decision, i.e., the suspect model is trained
using this trajectory. The auditor repeatedly implements the
above processes for other trajectories of the dataset and obtains

j and Qs

the final audit report with judgment for all trajectories. We
discuss more details of the distance metric and the hypothesis
testing in Section IV-C.

B. The Selection of Critic Model

The auditor can use either Monte Carlo (MC) based or
Temporal-Difference (TD) based algorithms to train a critic
model from the trajectories of the dataset. The main distinction
between the two methods lies in their learning targets, which
leads to differences in their objective functions. In the case
of MC-based methods, the learning target G is the empirical
cumulative rewards from the dataset.

G(st, at) = rt + γrt+1 + . . . + γH−1rH ,

where G(st, at) represents the exact cumulative reward from
(st, at) to the terminal time step H of one trajectory. The
discount factor γ is applied to discount future rewards. The
critic model is trained by minimizing the following objective.

E(st,at,rt+1,st+1)∼D

(cid:104)

(G (st, at) − Qθ (st, at))2(cid:105)

.

For TD-based methods, the learning target changes to the
expected cumulative reward in a heuristic form, i.e., rt +
γQ (st+1, at+1). Thus, the critic model is trained by mini-
mizing the following loss function.

E

(st,at,rt+1,st+1)∼D

(cid:2)(rt+1 + γQθ′ (st+1, at+1) − Qθ (st, at))2(cid:3) ,

where the critic model starts with arbitrary initialization θ.
Then, it repeatedly evaluates Qθ (st, at), obtains a reward rt+1,
and updates the weights. The θ′ is a snapshot of θ and copies
from θ every few updates of θ. The MC-based method utilizes
the exact cumulative rewards from the dataset to train the critic
model, resulting in an unbiased prediction. It also has strong
convergence properties due to the stationary of Gt. However,
it cannot be applied to situations where the collected data is
truncated, and all trajectories in the dataset must be completed.
In practice, many sequential decision-making tasks usually
have long or infinite time steps. Thus, the dataset provider
segments the interaction record into trajectories by a preset
maximum length. The TD-based method tackles the limitation
of the MC-based algorithm and can learn from incomplete
sequences. Nevertheless, due to the heuristic learning process,

6

Step 1: Model Preparation

Step 2: Cumulative Reward Collection

Dataset

1, 𝑎𝑡
𝑠𝑡

1, 𝑟𝑡

1
1, 𝑠𝑡+1

2, 𝑎𝑡
𝑠𝑡

2, 𝑟𝑡

2
2, 𝑠𝑡+1

|𝑡 = 1 … 𝑛1

|𝑡 = 1 … 𝑛2
…

𝑚, 𝑎𝑡
𝑠𝑡

𝑚, 𝑟𝑡

𝑚, 𝑠𝑡+1

𝑚 |𝑡 = 1 … 𝑛𝑚

Training

𝑇1:

𝑇2:

𝑇𝑚:

1

2

3

4

…

𝑘

Shadow DRL Models

Critic Model 𝑄(𝑠, 𝑎)

1

2

3

4

…

𝑘

Shadow 
DRL Models

𝑆1: 𝑠𝑡

1|𝑡 = 1 … 𝑛1

𝑆2: 𝑠𝑡

2|𝑡 = 1 … 𝑛2

…

𝑆𝑚: 𝑠𝑡

𝑚|𝑡 = 1 … 𝑛𝑚

States

Model 1
1)|𝑡 = 1 … 𝑛1

1, 𝑎𝑡
𝑠𝑡

(𝑠𝑡

2, 𝑎𝑡

2)|𝑡 = 1 … 𝑛2

…

(𝑠𝑡

𝑚, 𝑎𝑡

𝑚)|𝑡 = 1 … 𝑛𝑚

Model 𝑘
1)|𝑡 = 1 … 𝑛1

(𝑠𝑡

1, 𝑎𝑡

Model 𝑖: 1~𝑘

ℚ1

𝑖 : 𝑄 𝑠𝑡

1, 𝑎𝑡

1 |𝑡 = 1 … 𝑛1

…

(𝑠𝑡

2, 𝑎𝑡

2)|𝑡 = 1 … 𝑛2

…

ℚ2

𝑖 : 𝑄 𝑠𝑡

2, 𝑎𝑡
2 |𝑡 = 1 … 𝑛2
…

(𝑠𝑡

𝑚, 𝑎𝑡

𝑚)|𝑡 = 1 … 𝑛𝑚

ℚ𝑚

𝑖 : 𝑄 𝑠𝑡

𝑚, 𝑎𝑡

𝑚 |𝑡 = 1 … 𝑛𝑚

States & Actions

Critic Model Q(𝑠, 𝑎)

Cumulative Rewards

(𝑠𝑡

1, 𝑎𝑡

1)|𝑡 = 1 … 𝑛1

(𝑠𝑡

2, 𝑎𝑡

2)|𝑡 = 1 … 𝑛2

…

(𝑠𝑡

𝑚, 𝑎𝑡

𝑚)|𝑡 = 1 … 𝑛𝑚

ℚ1

𝑆: 𝑄 𝑠𝑡

1, 𝑎𝑡

1 |𝑡 = 1 … 𝑛1

ℚ2

𝑆: 𝑄 𝑠𝑡

2, 𝑎𝑡
2 |𝑡 = 1 … 𝑛2
…

ℚ𝑚

𝑆 : 𝑄 𝑠𝑡

𝑚, 𝑎𝑡

𝑚 |𝑡 = 1 … 𝑛𝑚

Suspect
DRL Model

States & Actions

Critic Model Q(𝑠, 𝑎)

Cumulative Rewards

𝑘
ℚ𝑗

Audit Metric

Step3 Audit Process
For 𝒋-th trajectory of the dataset
(𝑗 ∈ {1,2, … , 𝑚} )

1:
ℚ𝑗
2:
ℚ𝑗

𝑘:
ℚ𝑗

𝑠:
ℚ𝑗

0.7  1.2  1.3  0.6  …  1.1
0.7  0.9  0.8  0.9  …  0.9

⋯

1.3  0.9  0.7  0.5  …  1.3

0.6  1.3  1.1  1.4  …  1.1

Length of Trajectory

1, ഥℚ) 𝑑(ℚ𝑗

2, ഥℚ)
𝑑(ℚ𝑗
ഥℚ is element-wise mean of {ℚ𝑗

𝑑(ℚ𝑗

⋯

𝑘, ഥℚ) 𝑑(ℚ𝑗

𝑠, ഥℚ)
𝑖 |𝑖: 1~𝑘}.

2
ℚ𝑗

1
ℚ𝑗

4
ℚ𝑗

ഥℚ

𝑠
ℚ𝑗

3
ℚ𝑗
∆

5
ℚ𝑗

Positive

𝑠
ℚ𝑗

Negative

Hypothesis testing

If  𝝁

𝑄𝑆 >

, Negative for 𝑇

Algorithm 1 Workflow of ORL-AUDITOR
Input: Dataset D, suspect model πs, number of shadow

models k, significance level α

Output: The trajectory-level audit report

1: // Step 1: Model Preparation
2: Train shadow models {πi | i = 1, . . . , k} and critic model
3: // Step 2: Data Preparation
4: for each model π in {πi | i = 1, . . . , k} ∪ {πs} do
Query π by states s ∈ D and obtain the actions.
5:
Evaluate each (s, a) pair based on the critic model Q.
6:
Record the cumulative reward in sequential form {Qj |
7:

j = 1, . . . , m}.

8: end for
9: // Step 3: Audit Process
10: audit report = []
11: for each trajectory in {Tj | j = 1, . . . , m} do
12:

Calculate the element-wise mean ¯Qj of {Qi

j | i =

1, . . . , k}

13:
14:
15:

Measure the d(Qj, ¯Qj) of each Qi
// Hypothesis testing
From {di | i = 1, . . . , k} and ds, decide whether the

j from ¯Qj.

j and Qs

suspect model Ms pirates Tj with significance level α.

audit report.append(j-th audit result)

16:
17: end for
18: Return audit report

the TD-based method has some bias and is more sensitive to
model initialization. Therefore, we choose the element-wise
mean of the shadow models’ cumulative rewards ¯Q as the
auditing directrix in Section IV-A instead of relying solely on
the critic model’s predictions to compensate for the shortages
of TD-based methods.

C. The Details of Audit Process

In the audit process, the choice of distance metric and
the hypothesis testing method play a critical role in ORL-
AUDITOR’s performance. A proper metric is sensitive to the
deviations between the estimated cumulative rewards, which
can facilitate the hypothesis testing. A suitable hypothesis test-
ing method can provide precise results with high confidence.

Distance Metric. We consider three types of distance metrics,
i.e., ℓp norm, Cosine distance, and Wasserstein distance. ℓp
norm is a popular method of measuring the distance between
vectors, i.e., the sum of the absolute difference of the compo-
nents of the vectors. In the RL scene, the states and actions are
sequential data, meaning the distance metric should measure
both the value and the position deviation of the cumulative
rewards. However, ℓp norm may fail to reflect the difference
from the sequence aspect of the same set of values. Cosine
distance is a derivative of Cosine similarity, defined as the
cosine of the angle between two vectors. Cosine distance
embodies the difference from both the value and position
aspects of the vectors. However, Cosine distance normalizes
the inner product using the two vectors’ norm, which weakens
the numerical differences between the cumulative rewards. The
Wasserstein distance, a.k.a. earth mover’s distance (EMD), is a
metric of the difference between two probability distributions

over a region [54]. It can be defined as follows.

l1(u, v) = inf

π∈Γ(u,v)

(cid:90)

R×R

|x − y|dπ(x, y),

where Γ(u, v) is the set of distributions on R × R whose
marginals are u and v on the first and second factors respec-
tively. Wasserstein distance fits well with audit requirements,
reflecting numerical and positional deviations of the cumula-
tive rewards. Thus, we set Wasserstein distance by default and
compare different distance metrics in Section V.

Hypothesis Testing. After the selection of the distance metric,
the auditor proceeds to hypothesis testing with the distances
of Qi

j and Qs

j from ¯Qj.
H0 : d(Qs
H1 : d(Qs

j, ¯Qj)is not an outlier.
j, ¯Qj)is an outlier.

An intuitive method is to leverage the 3σ principle, i.e.,
the normal samples should be distributed within the range
of three times the standard deviation σd from the mean µd.
The 3σ principle is an efficient hypothesis testing method,
yet the mean µd is easily misled by outliers. Compared to
the 3σ principle, Grubbs’ test [25] is a more robust hypothe-
sis testing method for detecting single outliers in univariate
j, ¯Qj) exceeds
datasets. If the Grubbs’ test statistic of d(Qs
the threshold derived on the significance level, the auditor
j, ¯Qj) deviate from the mean value, i.e., reject
can claim d(Qs
H0 and output negative audit result. For a set of samples
{di | i = 1, 2, . . . , n}, Grubbs’ Test locates the outlier by the
procedures.

1) Calculate the mean µd and standard deviation σd.
2) Calculate the Grubbs’ test statistic by G =
(cid:114) t2

|d(Qs

σd

j , ¯Qj )−µd|

.

3) If G > n−1√
n

α/(n),n−2

n−2+t2

α/(n),n−2

, H0 is invalid,

i.e.,

the

suspect model is not trained by this trajectory. In the above
inequation, t2
α/(n),n−2 represents the upper critical value in
the t-distribution when the degree of freedom is n − 2, and
the significance level is α
n .

Both hypothesis testing methods are based on the assump-
tion that the distance values follow Gaussian distribution. Thus,
ORL-AUDITOR needs to pre-check that the distance values of
the shadow models satisfy the Gaussian distribution. We adopts
Anderson-Darling test [59] since it fits the scenarios where
the auditor has a small number of samplings, and the actual
distribution is unknown. In the evaluation, all the distance
values of the shadow models can pass the Anderson-Darling
test due to the randomness of the models’ initialization and
training. After that, ORL-AUDITOR conducts the hypothesis
testing.

V. EVALUATION

We first introduce the tasks and the experimental setup
in Section V-A. We validate the effectiveness of ORL-
AUDITOR on Behavior Clone and three offline DRL models,
i.e., Batch-Constrained Q-learning (BCQ) [21], Implicit Q-
Learning (IQL) [35], and TD3PlusBC [20] in Section V-B.
Then, we visualize the cumulative rewards by t-SNE [62] to
demonstrate that the cumulative rewards are intrinsic and stable

7

TABLE I: The Overview of Tasks. The “continuous” and
“discrete” illustrate the data type of the state and action with
the corresponding number of dimensions in parentheses.

Task Name

Lunar Lander
(Continuous)

State Shape

Action Shape

Continuous(6-dim)
Discrete(2-dim)

Continuous(2-dim)

Bipedal Walker

Continuous(24-dim)

Continuous(4-dim)

Ant

Continuous(111-dim)

Continuous(8-dim)

features for dataset auditing in Section V-C. After that, we
further evaluate the impact of three factors on ORL-AUDITOR,
i.e., the number of shadow models, the significance level in
hypothesis testing, and the trajectory size in Section V-D.
Finally, we utilize ORL-AUDITOR to audit the open-source
datasets from Google [18] and DeepMind [26] in Section V-E.

A. Experimental Setup

Tasks. We adopt Lunar Lander, Bipedal Walker, and Ant tasks
in Gym [7], which are widely used in the prior works [9],
[30], [48]. The tasks stem from distinct real-world problems,
each with numerical vectors containing different physical in-
formation, e.g., position, velocity, and acceleration. These tasks
involve both discrete and continuous variables in observation
and action spaces, with the dimension ranging from low (2-
dim) to high (111-dim). We give an overview in Table I and
put their details in Appendix B.

Dataset Generation and Offline Model Preparation. To
obtain the datasets for tasks in Table I, we adopt the same
idea as the existing dataset publishers [26], [18], [52], [1],
i.e., training the online RL models in the interactive envi-
ronment and recording the interactions as the datasets. The
datasets consist of numerical vectors. In Lunar Lander, each
transition includes state, next state (6-dimensional continuous
and 2-dimensional discrete variables), action (2-dimensional
continuous variables), and reward (scalar). Therefore, each
transition is a 19-dimensional numerical vector. Similarly, the
data types of Bipedal Walker and Ant are 53-dimensional and
231-dimensional numerical vectors, respectively. The number
of transitions for each task is 5 × 105 (Lunar Lander), 106
(Bipedal Walker), and 2 × 106 (Ant).

The offline RL models learn from the datasets. Table II
summarizes the whole process. For each task, we use five
global random seeds to train five online models separately.
We collect the datasets from five online models with random
seed 0, where every online model only generates one dataset.
For ease of reading, the datasets share the same name with
their online models. We train thirty offline DRL models for
every dataset with distinct global random seeds in initialization
and optimization processes. All the online and offline models
are implemented by open-source RL libraries [53], [56] with
default hyperparameter settings.

Critic Model. We adopt the fully connected neural network
as the critic model, which has four hidden layers with 1024
neurons on each layer. We optimize the critic model following
the TD-based method in Section IV-B by Adam optimizer with
a learning rate of 0.001 and a mini-batch size of 4096. The

TABLE II: The main steps in dataset generation and offline
model preparation with the details of the input and output.

For each combination of task and
offline RL model in the experiment
↓
Train with 5 random seeds: {0, 1, . . . , 4}
↓
5 online RL models detailed in Table XIV
↓
Collect with 1 random seed: {0}
↓
5x1 offline Datasets detailed in Table XV
↓
Train with 30 random seeds: {42, 43, . . . , 72}
↓
5x1x30 offline RL models detailed in
Table XVI, Table XVII, Table XVIII, and Table XIX

entire training takes 150 epochs, and the learning rate decays
to half every 50 epochs.

Evaluation Metrics. Recalling ORL-AUDITOR’s application
scenario in Figure 3, for a single suspect model, the audit
accuracy can well characterize the performance of ORL-
AUDITOR, i.e., the ratio of the number of correctly auditing
trajectory to the total auditing trajectory. In our experiment, the
positive models (trained on the target dataset) and the negative
models (trained on other datasets) are randomly mixed, where
the majority may dominate the accuracy. Thus, we provide the
true positive rate (TPR) and the true negative rate (TNR).

Methods. We provide the audit performance of 3σ principle
and Grubbs’ test with four distance metrics, i.e., ℓ1 norm, ℓ2
norm, Cosine distance, and Wasserstein distance.

Competitors. Recalling Section III-B, existing methods [47],
[24], [23] are designed for the online reinforcement learning
scenes, assuming that the auditor can continuously interact
with the environment to obtain new data as the non-member
example. Based on the behavioral difference of the model
between the member examples and the non-member examples,
they build the member inference method to detect whether
an example is used to train the suspect model. In the offline
scenarios, without access to the environment, the auditor only
has the pre-collected target dataset. Thus, we randomly divide
the target dataset into two parts and train offline RL models
on the subsets separately. Either subset is regarded as the set
of non-member examples for the offline RL models trained on
the other subset. We adopt the same data augmentation, attack
classifier architecture, and hyperparameter settings with [23].

Implementation. We use stable-baselines [53] and d3rlpy [56]
to implement online and offline DRL models separately. All
audit methods are realized with Python 3.8 on a server with 8
NVIDIA GeForce RTX 3090 and 512GB memory.

B. Overall Audit Performance

We assess the effectiveness of ORL-AUDITOR across
twelve combinations of three tasks and four models. Fur-
thermore, we present an evaluation of the efficacy of the
competitors on offline DRL models.

Setup. From Table II, we train 30 offline RL models for
each dataset and obtain 150 offline DRL models for every

8

TABLE III: The performance of existing membership inference
attack against offline DRL models.

Task
Name

Lunar
Lander

Bipedal
Walker

Ant

Offline
Model
BC
BCQ
IQL
TD3PlusBC
BC
BCQ
IQL
TD3PlusBC
BC
BCQ
IQL
TD3PlusBC

Accuracy

Training
50.09±0.68
49.84±1.39
49.88±0.76
50.08±0.92
50.00±0.63
49.97±0.69
50.17±0.95
49.87±0.94
50.44±0.64
50.22±0.52
50.33±0.35
50.13±0.67

Test
48.41±1.87
47.69±1.45
47.34±1.83
48.27±1.81
46.27±2.42
47.38±2.41
47.19±1.90
45.48±1.46
46.74±2.37
45.38±2.16
45.89±1.90
45.03±1.55

experimental setting. We audit the 5 datasets separately, where
the auditor randomly selects 15 models from the target dataset
as the shadow models, and the remaining 15 models along with
the 120 models from other datasets are the positive and the
negative suspect models. For the target dataset, we randomly
select fifty auditing trajectories to audit. Since the unbalanced
amount of the positive and the negative models, we report the
aggregated mean with a standard deviation of both TPR and
TNR for each setting in Table IV and provide the audit results
between every two datasets in Figure 11 (Lunar Lander),
Figure 12 (Bipedal Walker), and Figure 13 (Ant). Each pair
of TPR and TNR in Table IV is derived from the diagonal
and non-diagonal values of the corresponding heatmap. As a
supplementary of [13], we also show the audit result by 3σ
principle in Table VII. The competitors’ performance is shown
in Table III, where the values of mean and standard variation
are calculated by repeating experiment ten times.

Observations. We have the following observations from
Table IV, Table VII, and Table III. 1) Most TPR and TNR
values are higher than 95%, meaning that ORL-AUDITOR is
a valid solution to audit the learned dataset of the offline DRL
models. For instance, all results for ORL-AUDITOR with ℓ1
norm are beyond 94% across the experiment settings.

2) ORL-AUDITOR obtains different audit accuracy over
four distance metrics. The audit effectiveness with ℓ1 norm and
Wasserstein distance is better than that of ℓ2 norm and Cosine
distance. In Table IV and Table VII, ORL-AUDITOR with
Wasserstein distance always performs the best or the second
place. And results of ℓ2 norm are usually behind the other three
distance metrics. Recalling Section IV-C, Wasserstein distance
characterizes both the numerical and the positional deviations
of the cumulative rewards, which is more sensitive. Since
the numerical differences between the cumulative rewards are
slight, e.g., from 0.01 to 0.1 in our experiment, ℓ2 norm may
undercut these small but potential differences.

3) The accuracy of the audit as determined by Grubbs’ test
outperforms that of the 3σ principle. The 3σ principle is an
empirical method, which is easily misled by the outlier cumu-
lative rewards of the shadow models. Recalling Section IV-C,
Grubbs’ test first calculates the statistic G and compares G
with an adaptive threshold, where the number of samples is
also considered in the hypothesis testing.

4) Without the new data from the environment, the ef-
fectiveness of the existing membership inference methods is

attenuated. From one perspective, the similarity between sub-
datasets splited from the same dataset can result in the trained
RL models exhibiting undifferentiated behavior, making it
difficult to effectively distinguish between members and non-
members. On the other hand, when considering the results
presented in Figure 10, we conclude that the actions of RL
models should not be directly utilized as the foundation for
membership inference.

C. Visualization of Cumulative Rewards

To further explain the audit results in Section V-B, we
analyze the cumulative rewards from the shadow models and
j and Qs
the suspect models, i.e., Qi

j, by using t-SNE [62].

j (positive) or Qs

Setup. The caption of each plot in Figure 5 indicates the used
task and offline DRL model. Each point in the plots shows the
visualization of a single Qi
j (negative). In
a single plot, we demonstrate the results of three trajectories
from each tasks’ first datasets. For instance, the target dataset
of the plot titled “Lunar Lander, BC” is dataset “1171” in
Table XV. The thirty positive points for each trajectory are
collected from the shadow models trained on dataset “1171”,
while the thirty negative points are randomly sampled from
the shadow models from the other four datasets.

Observations. From Figure 5, we have the following obser-
vations. 1) For a trajectory of the target dataset, the cumulative
rewards from the shadow models and the suspect models are
clearly divided into different groups, meaning that the critic
model well reflects the differences in the models’ actions.
Thus, the cumulative reward generated by the critic model is
a qualified post-event fingerprint for trajectory-level auditing.

2) The distribution of points varies on the different trajec-
tories. For example, trajectory 1 from the Lunar Lander dataset
is harder to cluster than the other two trajectories. We speculate
that this is because trajectory 1 represents a basic policy, e.g., a
local optimum policy to fire the lander’s thrusters all the way,
and similar trajectories exist in the other four datasets. Due to
the non-uniqueness of the optimal strategy in RL problems and
the impact of randomness in the model training process, the
collected trajectories have unique characteristics. Thus, other
trajectories’ cumulative rewards are clearly divided.

D. Hyperparameter Study

impact

We extend our assessment to scrutinize three pivotal de-
terminants that
the pragmatic integration of ORL-
AUDITOR. Specifically, we consider the amount of shadow
models, the level of significance in hypothesis testing, and the
magnitude of the trajectory size. Due to space limitations, we
only give brief conclusions in this section. Please refer to the
specific analysis in Appendix C, Appendix D, and Appendix E.

Impact of Shadow Models’ Amount. We change the shadow
models’ amount to 9 and 21 with the other settings the same
as Section V-B. Figure 6 shows the value change of TPR and
TNR compared with that of 15 shadow models. Each figure’s
title illustrates the settings of the model and the task, the x-axis
indicates the four metrics, and the y-axis is the absolute value
change. As a supplementary of [13], we provide the detailed
results in Table VIII (9 Shadow Models) and Table IX (21
Shadow Models).

9

TABLE IV: The TPR and TNR results based on Grubbs’ test. The mean and standard deviation of TPR and TNR in each row
represent the audit results for one combination of task and model by four distance metrics. Bold indicates the highest sum of
TPR and TNR, i.e., accuracy, in a row. Each pair of TPR and TNR is derived from the diagonal and non-diagonal values of the
corresponding heatmap in Figure 11, Figure 12 and Figure 13, which are supplementary to [13].

Task
Name

Offline
Model

Lunar
Lander

Bipedal
Walker

Ant

BC
BCQ
IQL
TD3PlusBC
BC
BCQ
IQL
TD3PlusBC
BC
BCQ
IQL
TD3PlusBC

L1 Norm

L2 Norm

Cosine
Distance

Wasserstein
Distance

TPR
99.01±0.46
98.29±1.14
98.61±1.51
98.29±2.04
99.20±1.47
99.52±0.77
95.10±7.41
99.36±1.28
97.42±1.66
97.17±2.96
97.20±2.33
98.53±1.80

TNR
100.00±0.00
100.00±0.00
99.91±0.32
99.48±0.79
100.00±0.00
100.00±0.00
100.00±0.00
94.77±19.42
99.94±0.11
99.80±0.43
99.66±0.73
99.18±1.72

TPR
96.96±0.73
96.03±1.15
97.52±2.51
96.35±3.01
98.40±2.70
98.16±2.89
95.04±5.45
97.15±5.71
96.48±1.66
95.68±2.54
96.61±2.50
97.17±1.79

TNR
100.00±0.00
100.00±0.00
99.97±0.12
99.89±0.22
100.00±0.00
100.00±0.00
100.00±0.00
93.36±21.46
99.90±0.36
99.84±0.43
99.69±0.59
99.35±1.74

TPR
96.93±0.77
95.97±1.07
97.49±2.56
96.27±3.16
98.56±2.68
99.87±0.15
99.84±0.32
96.96±5.82
99.20±1.08
99.66±0.43
99.57±0.79
99.72±0.40

TNR
100.00±0.00
99.99±0.04
99.92±0.19
99.91±0.23
100.00±0.00
100.00±0.00
100.00±0.00
91.98±21.75
85.66±28.23
86.70±26.89
86.25±27.90
87.79±26.43

TPR
98.40±0.74
97.57±1.17
98.32±1.79
98.53±1.25
99.31±1.32
99.89±0.13
95.01±6.72
98.08±3.84
98.00±1.19
98.67±1.65
99.36±0.42
99.25±1.24

TNR
99.94±0.16
99.91±0.14
97.10±5.66
95.59±3.77
100.00±0.00
100.00±0.00
100.00±0.00
88.26±25.34
99.92±0.14
99.79±0.46
99.63±0.78
99.14±1.81

From Figure 6, we have the following observations. 1)
The audit accuracy increases with a larger amount of shadow
models. 2) There exists a saturation point for audit accuracy
with the expansion of shadow models.

Impact of Significance Level. The significance level rep-
resents the auditor’s confidence in the auditing results. In
the significance level α = 0.01,
Section V-B, we adopt
meaning that the auditor has 99% confidence in the judg-
ments. Generally speaking, the significance level represents
the maximum audit capacity of ORL-AUDITOR instead of
a hyperparameter setting since it is an audit requirement by
the dataset owner. We demand the auditor to output a more
confident
judgment, where the error possibility should be
limited to 1‰ and 0.1‰, i.e., α = 0.001 and α = 0.0001.
Figure 7 shows the value change of TPR and TNR compared
with that when α = 0.01. As a supplementary of [13], the
detailed results between every two datasets are in Table X
(α = 0.001) and Table XI (α = 0.0001).

From Figure 7, we have the following observations. 1)
For a complicated task, we recommend the auditor select a
large significance level for ORL-AUDITOR. 2) For the suspect
models with low performance, ORL-AUDITOR should adopt
a large significance level to guarantee audit accuracy. 3) In
general, α = 0.01 is a safe bound of ORL-AUDITOR, and a
lower α may break through the capability boundary of ORL-
AUDITOR, inducing the auditor to misclassify the negative
model to the positive set.

Impact of Trajectory Size. We investigate the relationship
between the trajectory size and audit accuracy. In Section V-B,
we adopt the full-length trajectory, meaning that the auditor
utilizes all states of each trajectory to query the suspect model
and obtains the corresponding actions to conduct the dataset
auditing. We change the trajectory size to 25% and 50% of
the full length with the other settings the same as Section V-B.
Figure 8 shows the value change of TPR and TNR compared
with that of the full-length trajectory. As a supplementary
of [13], we also provide the detailed results in Table XII (25%)
and Table XIII (50%).

From Figure 8, we have the following observations. 1)
ORL-AUDITOR tends to achieve higher accuracy with a larger
trajectory size. 2) A small trajectory size achieves better results
under some tasks since the front states of each trajectory are
able to reflect more behavioral information of the model [46].

E. Real-world Application

In this section, we apply ORL-AUDITOR to audit the open-
source datasets from DeepMind [26] and Google [18]. We
choose the “halfcheetah” task published by both, where the
operator controls a 2-dimensional cheetah robot consisting of
9 links and 8 joints connecting them (including two paws) to
make the cheetah run forward (right) as fast as possible. The
details of the halfcheetah dataset and the offline DRL models
are in Table XX and Table XXI. All experimental settings are
consistent with these in Section V-B.

Observations.
From Table XXII, we have the following
observations. 1) ORL-AUDITOR can be effective in real-world
applications. The TPR and TNR of ORL-AUDITOR exceed
95% with ℓ1 norm and Wasserstein distance, meaning that
ORL-AUDITOR remains valid for the existing open-source
datasets. 2) Wasserstein distance has stable performance on the
experimental and the real-world datasets. The overall accuracy
of ORL-AUDITOR with Wasserstein distance are all higher
than the other three metrics.

VI. ROBUSTNESS

A. Ensemble Architecture

To hinder the audit of a dataset, an adversary may uti-
lize state-of-the-art membership inference defense strategies
proposed in recent research works [60], [31]. These defense
strategies aim to mitigate the influence of a member example
on the behavior of a machine learning model. Based on the
idea of model ensemble, in particular, [60], [31], [11] proposed
to split the training set into several subsets and train sub-
models on each of these subsets. Then, when an auditor uses
an example from the target dataset to query a suspect model,

10

Fig. 5: Visualization of cumulative rewards by t-SNE. The caption of each plot demonstrates the offline DRL model’s type and
task. In a single plot, we randomly select three trajectories from the first dataset for the task, i.e., Lunar Lander dataset 1171,
Bipedal Walker dataset 0841, and Ant dataset 2232 in Table XV, and then show the cumulative rewards from 30 positive models
and 30 negative models for each trajectory.

Fig. 6: Impact of shadow models’ amount. The change value of TPR and TNR when the number of shadow models varies to
9 and 21 compared to the default 15 shadow models. The caption of each plot demonstrates the offline DRL model’s type and
task. The x labels display the four distance metrics. The y labels show the absolute fluctuating values of TPR and TNR.

the adversary aggregates the outputs of the sub-models that
have not been trained on this example.

Setup. The number of divided subsets, denoted by K, repre-
sents a crucial hyperparameter for ensemble-based methods, as
discussed in [60], [31]. Considering the analysis conducted in
these studies, as well as the size of the offline RL datasets, we
have established K = 5 for the present investigation. All other
experimental settings remain unchanged from those described
in Section V-B, and the corresponding audit outcomes are
presented in Table V. As a supplementary of [13], the results
between every two datasets are in Figure 14 (Lunar Lander),
Figure 15 (Bipedal Walker), Figure 16 (Ant), and Figure 17

(Half Cheetah).

Observations. We conclude the following observations based
on the above results. 1) Even when faced with ensemble
architecture, ORL-AUDITOR maintains a high level of audit
accuracy. As shown in Table V, both TPR and TNR con-
sistently exceed 80%. As described in Section IV-A, ORL-
AUDITOR uses predicted cumulative rewards from the critic
model as the basis for auditing. During training, the critic
model captures the overall features of the dataset distribution,
instead of memorizing features from individual samples. Since
the ensemble model is trained on the target dataset, its behavior
embeds the distribution characteristics of the dataset, which

11

Lunar Lander, BC

Lunar Lander, BCQ

Lunar Lander, IQL

Lunar Lander, TD3PlusBC

Trajectory
0
1
2
Audit Result
Positive
Negative

Trajectory
0
1
2
Audit Result
Positive
Negative

Trajectory
0
1
2
Audit Result
Positive
Negative

Trajectory
0
1
2
Audit Result
Positive
Negative

Bipedal Walker, BC

Bipedal Walker, BCQ

Bipedal Walker, IQL

Bipedal Walker, TD3PlusBC

Trajectory
0
1
2
Audit Result
Positive
Negative

Trajectory
0
1
2
Audit Result
Positive
Negative

Trajectory
0
1
2
Audit Result
Positive
Negative

Trajectory
0
1
2
Audit Result
Positive
Negative

Ant, BC

Ant, BCQ

Ant, IQL

Ant, TD3PlusBC

Trajectory
0
1
2
Audit Result
Positive
Negative

Trajectory
0
1
2
Audit Result
Positive
Negative

Trajectory
0
1
2
Audit Result
Positive
Negative

Trajectory
0
1
2
Audit Result
Positive
Negative

Lunar Lander, BC

Bipedal Walker, BC

Ant, BC

5.0

0

-5.0

-10.0

l

e
u
a
V
∆

5.0

0

-5.0

-10.0

l

e
u
a
V
∆

5.0

0

-5.0

-10.0

l

e
u
a
V
∆

Lunar Lander, BCQ

Bipedal Walker, BCQ

Ant, BCQ

5.0

0

-5.0

-10.0

5.0

0

-5.0

-10.0

5.0

0

-5.0

-10.0

5.0

0

-5.0

-10.0

5.0

0

-5.0

-10.0

5.0

0

-5.0

-10.0

Lunar Lander, IQL

Bipedal Walker, IQL

Ant, IQL

Lunar Lander, TD3PlusBC

Bipedal Walker, TD3PlusBC

Ant, TD3PlusBC

5.0

0

-5.0

-10.0

5.0

0

-5.0

-10.0

5.0

0

-5.0

-10.0

L

1

L

1

L

2

L

2

T

T

T

T

P

N

P

N

R

R

R

R

o

s
.

o

s
.

T

T

a

s

s
.

a

s

s
.

P

N

T

T

R

R

P

N

R

R

L

1

L

1

L

2

L

2

T

T

T

T

P

N

P

N

R

R

R

R

o

s
.

o

s
.

T

T

a

s

s
.

a

s

s
.

P

N

T

T

R

R

P

N

R

R

L

1

L

1

L

2

L

2

T

T

T

T

P

N

P

N

R

R

R

R

o

s
.

o

s
.

T

T

a

s

s
.

a

s

s
.

P

N

T

T

R

R

P

N

R

R

L

1

L

1

L

2

L

2

T

T

T

T

P

N

P

N

R

R

R

R

o

s
.

o

s
.

T

T

a

s

s
.

a

s

s
.

P

N

T

T

R

R

P

N

R

R

C

C

W

W

C

C

W

W

C

C

W

W

C

C

W

W

Distance Metric

Distance Metric

Distance Metric

Distance Metric

9 Shadow Models 

21 Shadow Models 

Fig. 7: Impact of the significance level. The change value of TPR and TNR when the significance level varies to 0.001 and
0.0001 compared to the default 0.01. The caption of each plot demonstrates the offline DRL model’s type and task. The x labels
display the four distance metrics. The y labels show the absolute fluctuating values of TPR and TNR.

Fig. 8: Impact of the trajectory size. The change value of TPR and TNR when the trajectory size varies to 25% and 50%
compared to the entire trajectories (100%). The caption of each plot demonstrates the offline DRL model’s type and task. The
x labels display the four distance metrics. The y labels show the absolute fluctuating values of TPR and TNR.

Fig. 9: Robustness against action distortion. The change value of TPR and TNR when the suspect model adds Gaussian noise
parameterized with (µ = 0, σ = 0.01) and (µ = 0, σ = 0.1) to its output. The caption of each plot demonstrates the offline DRL
model’s type and task. The x labels display the four distance metrics. The y labels show the absolute fluctuating values.

ORL-AUDITOR can detect.

2) The use of ensemble architecture may result in a de-
crease in model performance for certain tasks. Our experimen-
tal results, as shown in column “Model Performance (Model
Ensemble)” of Tables Table XVI, Table XVII, Table XVIII,
and Table XIX, demonstrate a decline in the performance of
offline RL models when utilizing ensemble architecture. For

instance, when BCQ models learn from the Ant dataset “3569”,
the mean values of cumulative reward decrease significantly.
Furthermore, due to the sub-models being trained on subsets of
data, they only fit a partial dataset’s distribution. Consequently,
when applying the model ensemble to practical scenarios, the
standard deviations of the model’s performance are large.

12

Lunar Lander, BC

Bipedal Walker, BC

Ant, BC

l

e
u
a
V
∆

l

e
u
a
V
∆

l

e
u
a
V
∆

5.0
0.0
-5.0
-10.0
-15.0
-20.0

5.0
0.0
-5.0
-10.0
-15.0
-20.0

5.0
0.0
-5.0
-10.0
-15.0
-20.0

Lunar Lander, BCQ

Bipedal Walker, BCQ

Ant, BCQ

5.0
0.0
-5.0
-10.0
-15.0
-20.0

5.0
0.0
-5.0
-10.0
-15.0
-20.0

5.0
0.0
-5.0
-10.0
-15.0
-20.0

5.0
0.0
-5.0
-10.0
-15.0
-20.0

5.0
0.0
-5.0
-10.0
-15.0
-20.0

5.0
0.0
-5.0
-10.0
-15.0
-20.0

Lunar Lander, IQL

Bipedal Walker, IQL

Ant, IQL

Lunar Lander, TD3PlusBC

Bipedal Walker, TD3PlusBC

Ant, TD3PlusBC

5.0
0.0
-5.0
-10.0
-15.0
-20.0

5.0
0.0
-5.0
-10.0
-15.0
-20.0

5.0
0.0
-5.0
-10.0
-15.0
-20.0

L

1

L

1

L

2

L

2

T

T

T

T

P

N

P

N

R

R

R

R

o

s
.

o

s
.

T

T

a

s

s
.

a

s

s
.

P

N

T

T

R

R

P

N

R

R

L

1

L

1

L

2

L

2

T

T

T

T

P

N

P

N

R

R

R

R

o

s
.

o

s
.

T

T

a

s

s
.

a

s

s
.

P

N

T

T

R

R

P

N

R

R

L

1

L

1

L

2

L

2

T

T

T

T

P

N

P

N

R

R

R

R

o

s
.

o

s
.

T

T

a

s

s
.

a

s

s
.

P

N

T

T

R

R

P

N

R

R

L

1

L

1

L

2

L

2

T

T

T

T

P

N

P

N

R

R

R

R

o

s
.

o

s
.

T

T

a

s

s
.

a

s

s
.

P

N

T

T

R

R

P

N

R

R

C

C

W

W

C

C

W

W

C

C

W

W

C

C

W

W

Distance Metric

Distance Metric

Distance Metric

Distance Metric

0.001

0.0001

Lunar Lander, BC

Lunar Lander, BCQ

Lunar Lander,

IQL

Lunar Lander, TD3PlusBC

l

e
u
a
V
∆

10.0
5.0
0.0
-5.0
-10.0

l

e
u
a
V
∆

10.0
5.0
0.0
-5.0
-10.0

l

e
u
a
V
∆

10.0
5.0
0.0
-5.0
-10.0

Bipedal Walker, BC

Ant, BC

10.0
5.0
0.0
-5.0
-10.0

10.0
5.0
0.0
-5.0
-10.0

10.0
5.0
0.0
-5.0
-10.0

Bipedal Walker, BCQ

Ant, BCQ

10.0
5.0
0.0
-5.0
-10.0

10.0
5.0
0.0
-5.0
-10.0

10.0
5.0
0.0
-5.0
-10.0

Bipedal Walker,

IQL

Ant,

IQL

10.0
5.0
0.0
-5.0
-10.0

10.0
5.0
0.0
-5.0
-10.0

10.0
5.0
0.0
-5.0
-10.0

Bipedal Walker, TD3PlusBC

Ant, TD3PlusBC

L

1

L

1

L

2

L

2

T

T

T

T

P

N

P

N

R

R

R

R

o

s
.

o

s
.

T

T

a

s

s
.

a

s

s
.

P

N

T

T

R

R

P

N

R

R

L

1

L

1

L

2

L

2

T

T

T

T

P

N

P

N

R

R

R

R

o

s
.

o

s
.

T

T

a

s

s
.

a

s

s
.

P

N

T

T

R

R

P

N

R

R

L

1

L

1

L

2

L

2

T

T

T

T

P

N

P

N

R

R

R

R

o

s
.

o

s
.

T

T

a

s

s
.

a

s

s
.

P

N

T

T

R

R

P

N

R

R

L

1

L

1

L

2

L

2

T

T

T

T

P

N

P

N

R

R

R

R

o

s
.

o

s
.

T

T

a

s

s
.

a

s

s
.

P

N

T

T

R

R

P

N

R

R

C

C

W

W

C

C

W

W

C

C

W

W

C

C

W

W

Distance Metric

Distance Metric

Distance Metric

Distance Metric

25% of full trajectory

50% of full trajectory

Lunar Lander, BC

Lunar Lander, BCQ

Lunar Lander, IQL

Lunar Lander, TD3PlusBC

l

e
u
a
V
∆

0
-20.0
-40.0
-60.0
-80.0

l

e
u
a
V
∆

0
-20.0
-40.0
-60.0
-80.0

l

e
u
a
V
∆

0
-20.0
-40.0
-60.0
-80.0

Bipedal Walker, BC

Ant, BC

0
-20.0
-40.0
-60.0
-80.0

0
-20.0
-40.0
-60.0
-80.0

0
-20.0
-40.0
-60.0
-80.0

Bipedal Walker, BCQ

Ant, BCQ

0
-20.0
-40.0
-60.0
-80.0

0
-20.0
-40.0
-60.0
-80.0

0
-20.0
-40.0
-60.0
-80.0

Bipedal Walker, IQL

Ant, IQL

0
-20.0
-40.0
-60.0
-80.0

0
-20.0
-40.0
-60.0
-80.0

0
-20.0
-40.0
-60.0
-80.0

Bipedal Walker, TD3PlusBC

Ant, TD3PlusBC

L

1

L

1

L

2

L

2

T

T

T

T

P

N

P

N

R

R

R

R

o

s
.

o

s
.

T

T

a

a

s

s
.

s

s
.

P

N

T

T

R

R

P

N

R

R

L

1

L

1

L

2

L

2

T

T

T

T

P

N

P

N

R

R

R

R

o

s
.

o

s
.

T

T

a

a

s

s
.

s

s
.

P

N

T

T

R

R

P

N

R

R

L

1

L

1

L

2

L

2

T

T

T

T

P

N

P

N

R

R

R

R

o

s
.

o

s
.

T

T

a

a

s

s
.

s

s
.

P

N

T

T

R

R

P

N

R

R

L

1

L

1

L

2

L

2

T

T

T

T

P

N

P

N

R

R

R

R

o

s
.

o

s
.

T

T

a

a

s

s
.

s

s
.

P

N

T

T

R

R

P

N

R

R

C

C

W

W

C

C

W

W

C

C

W

W

C

C

W

W

Distance Metric

Distance Metric

Distance Metric

Distance Metric

sigma=0.01

sigma=0.1

TABLE V: The TPR and TNR results of ORL-AUDITOR against model ensemble (K = 5). The mean and standard deviation
of TPR and TNR in each row represent the audit results for one combination of task and model by four distance metrics. Each
pair of TPR and TNR is derived from the diagonal and non-diagonal values of the corresponding heatmap in Figure 14 (Lunar
Lander), Figure 15 (Bipedal Walker), Figure 16 (Ant), and Figure 17 (Half Cheetah), which are supplementary to [13].

Task
Name

Lunar
Lander

Bipedal
Walker

Ant

Half
Cheetah

Offline
Model

BC
BCQ
IQL

BC
BCQ
IQL

TPR
100.00±0.00
99.60±0.80
100.00±0.00
TD3PlusBC 100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
TD3PlusBC 100.00±0.00
99.60±0.80
100.00±0.00
100.00±0.00
TD3PlusBC 99.60±0.80
85.00±25.98
91.00±15.59
90.00±12.81
TD3PlusBC 61.50±20.32

BC
BCQ
IQL

BC
BCQ
IQL

L1 Norm

L2 Norm

Cosine
Distance

Wasserstein
Distance

TNR
100.00±0.00
100.00±0.00
99.90±0.44
99.30±0.95
100.00±0.00
100.00±0.00
100.00±0.00
94.90±19.07
100.00±0.00
99.70±0.71
99.80±0.60
99.30±1.82
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00

TPR
99.20±0.98
98.00±2.19
99.20±0.98
99.60±0.80
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
99.60±0.80
99.60±0.80
99.20±0.98
100.00±0.00
84.50±25.71
89.00±16.76
86.50±16.70
77.00±19.42

TNR
100.00±0.00
100.00±0.00
100.00±0.00
99.90±0.44
100.00±0.00
100.00±0.00
100.00±0.00
93.80±21.63
99.90±0.44
99.80±0.60
99.70±0.71
99.40±2.20
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00

TPR
99.20±0.98
98.00±2.19
99.60±0.80
99.60±0.80
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
99.60±0.80
100.00±0.00
99.20±0.98
100.00±0.00
94.00±10.39
95.00±8.66
94.50±9.53
95.00±8.66

TNR
100.00±0.00
100.00±0.00
99.90±0.44
99.80±0.60
100.00±0.00
100.00±0.00
100.00±0.00
92.70±21.62
83.20±31.99
85.70±28.31
86.80±28.32
87.80±25.87
67.50±43.20
67.17±42.30
71.00±41.37
65.67±41.28

TPR
99.60±0.80
99.60±0.80
99.60±0.80
99.60±0.80
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00
99.20±1.60
100.00±0.00
100.00±0.00
99.60±0.80
87.00±21.38
93.00±12.12
91.50±12.52
52.00±33.26

TNR
99.90±0.44
100.00±0.00
97.60±4.27
95.80±3.57
100.00±0.00
100.00±0.00
100.00±0.00
89.20±23.94
100.00±0.00
99.70±0.71
99.80±0.60
98.50±3.79
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00

B. Action Distortion

The suspect models may perturb the actions, i.e., changing
the original models’ outputs, to conceal its training dataset in
practice. The action distortion mechanism should be stealthy
and cannot be detected by the auditor easily. Considering that
the DRL models are usually applied to real-world decision-
making tasks, such as self-driving cars and industry automa-
the natural distortion is often modeled as
tion [44], [28],
Gaussian noise. For example, thermal noise, which is caused
by the random motion of electrons in a conductor, can be
modeled as a Gaussian noise with a constant power spec-
trum [2]. In addition, Gaussian noise is easy to manipulate
mathematically. For ease of evaluating the effects of different
distortion intensities, all dimensions of the models’ action
space are normalized into [−1, 1]. Then, we utilize Gaussian
noise with mean (µ = 0) and standard deviation (σ = 0.1)
and (σ = 0.01) to represent the two levels of distortion.

Setup. Figure 9 depicts the impact of “with” or “without”
the action distortion. The information about the used offline
DRL model and task is shown in each figure’s title. The x-
axis indicates the four metrics, and the y-axis is the absolute
value change. As a supplementary of [13], the detailed results
between every two datasets are in Figure 18, Figure 19 (Lunar
Lander), Figure 20, Figure 21 (Bipedal Walker), Figure 22,
and Figure 23 (Ant).

Observations. We conclude the following observations based
on the above results. 1) ORL-AUDITOR is able to resist the
potential action distortion from the suspect model, especially
with the Cosine metric. From Figure 9, the TPR and TNR
vary slightly across most of the settings with weak noise,
where the maximum accuracy attenuation is within 3% for
Cosine distance. We speculate that Cosine distance has a
noise suppression ability when calculating the inner product of

two series of cumulative rewards. Also, the weak noise may
facilitate the dataset auditing since it will move the negative
samples farther away from the positive set.

2) ORL-AUDITOR with a single distance metric faces
limitations for heavy distortion. The TPR of ORL-AUDITOR
suffers an obvious decline with strong noise. Since the strong
distortion thoroughly changes the distribution of the models’
actions, the cumulative rewards of the suspect model trained
on the target dataset are different from those of the auditor’s
shadow models. In this case, the auditor cannot identify the
positive models from the negative just by a single kind of
distance metric. From Figure 23, Cosine distance is good at
discriminating the positive models (results in the diagonal),
and Wasserstein distance is proper for the negative models
(results in the non-diagonal). Thus, for strong distortion, the
combination of multiple distance metrics can enhance the
auditing robustness of ORL-AUDITOR. In addition, we should
note that the models’ normal behavior is also destroyed by
the strong distortion. For example, in Table XVIII, the noise
induces the model performance of IQL to decrease up to 25%,
and the better the model’s quality, the more pronounced the
performance drop.

VII. RELATED WORK

Membership and Dataset Inferences. To infer whether an
individual data record was used to train the target model,
Shokri et al. [57] proposed the first practical membership
inference strategy by training a number of shadow classifiers to
distinguish the target model’s outputs on members versus non-
members of its training dataset. Since then, researchers have
investigated membership inference in various systems, such as
machine unlearning [10], facial recognition systems [12], and
neural architecture search [29]. Liu et al. [41] presenting a
first-of-its-kind holistic risk assessment of different inference

13

attacks against machine learning models. Maini et al. [43]
introduced the definition of dataset inference and designed the
first mechanism to identify whether a suspect model copy has
private knowledge from the dataset.

Compared with the existing works, ORL-AUDITOR is a
well-designed solution built for the offline DRL scenes, which
overcomes several new challenges. First, ORL-AUDITOR is
a post-event mechanism that can be directly applied to the
existing open-source datasets. Second, ORL-AUDITOR does
not use any auxiliary datasets.

Knowledge Extraction Against DRL. The DRL models
learn from the interaction with the environment, which can
be valuable information in some cases, e.g.,
indoor robot
navigation. Pan et al. [47] demonstrated such knowledge
extraction vulnerabilities in DRL under various settings and
proposed algorithms to infer floor plans from some trained
Grid World navigation DRL models with LiDAR perception.
For exacting the model functionality, Chen et al. [9] proposed
the first method to acquire the approximation model from the
victim DRL. They built a classifier to reveal the targeted black-
box DRL model’s training algorithm family based only on its
predicted actions and then leveraged state-of-the-art imitation
learning techniques to replicate the model from the identi-
fied algorithm family. Ono et al. [45] integrated differential
privacy [72], [69], [63] into the distributed RL algorithm to
defend the extraction. The local models report noisy gradients
designed to satisfy local differential privacy [14], [15], [65],
[71], i.e., keeping the local information from being exploited
by adversarial reverse engineering. Chen et al. [8] proposed a
novel testing framework for deep learning copyright protection,
which can be adjusted to detect
the knowledge extraction
against DRL.

VIII. DISSCUSION

Highlights of ORL-AUDITOR. 1) ORL-AUDITOR is the first
approach to conduct trajectory-level dataset auditing for offline
DRL models. 2) By conducting a comprehensive analysis of
ORL-AUDITOR under different experimental settings, such
as the shadow model’s amount, the significance level in hy-
pothesis testing, the trajectory size, and the robustness against
ensemble architecture and action distortion, we conclude some
useful observations for adopting ORL-AUDITOR. 3) We apply
ORL-AUDITOR to audit the models trained on the open-source
datasets from Google and DeepMind. All TPR and TNR results
are superior than 95%, demonstrating ORL-AUDITOR is an
effective and efficient strategy for the published datasets.

Limitations and Future Work. Below, we discuss the
limitations of ORL-AUDITOR and promising directions for
further improvements. 1) From Appendix D, the accuracy of
ORL-AUDITOR decreases when the significance level downs
to 0.001. Thus, it is interesting to enhance ORL-AUDITOR
to satisfy stricter auditing demands in the future. 2) ORL-
AUDITOR based on a single distance metric may not be suffi-
ciently robust to strong distortion. Based on the observations
in Section VI-B, integrating more distance metrics in the audit
process may be a further promising direction.

IX. CONCLUSION

In this work, we propose a novel trajectory-level dataset
auditing method for offline DRL models relying on the insight
that cumulative rewards can serve as the dataset’s intrinsic
fingerprint and exist in all models trained on the target dataset.
Both the true positive rate and the true negative rate of
ORL-AUDITOR exceed 90% on four offline DRL models and
three task combinations. We show that ORL-AUDITOR is an
effective and efficient solution to protect the IP of the dataset
owners through multiple experiments. By studying parameter
settings about the number of shadow models, the significance
level in hypothesis testing, and the trajectory size, we conclude
several important observations for adopting ORL-AUDITOR in
practice. The robustness evaluation demonstrates that ORL-
AUDITOR can resist
the defenses of the model ensemble
and the action distortion of the suspect model. Integrating
multiple distance metrics to improve the robustness of ORL-
AUDITOR against action distortion is a promising direction for
future work. Finally, we utilize the open-source datasets from
Google [18] and DeepMind [26] to examine the practicality
of ORL-AUDITOR, and show that ORL-AUDITOR behaves
excellently on existing published datasets.

ACKNOWLEDGMENT

We would like to thank the anonymous reviewers for
their constructive comments. We also thank Yanchao Sun for
sharing her expertise in reinforcement learning. This work was
partly supported by the National Key Research and Develop-
ment Program of China under No. 2022YFB3102100, NSFC
under Grants 62088101, 61833015, 62103371, U20A20159,
and the Fundamental Research Funds for the Central Univer-
sities 226-2022-00107, 226-2023-00111. Min Chen was partly
sponsored by the Helmholtz Association within the project
“Trustworthy Federated Data Analytics” (TFDA) (No. ZT-I-
OO1 4). Zhikun Zhang was supported by the CISPA-Stanford
Center for Cybersecurity (FKZ:13N1S0762).

REFERENCES

[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.
Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow,
A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur,
J. Levenberg, D. Man´e, R. Monga, S. Moore, D. Murray, C. Olah,
M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker,
V. Vanhoucke, V. Vasudevan, F. Vi´egas, O. Vinyals, P. Warden, M. Wat-
tenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-Scale Ma-
chine Learning on Heterogeneous Systems. https://www.tensorflow.org/,
2015.

[2] N. AlHinai. Introduction to Biomedical Signal Processing and Artificial
Intelligence. In Biomedical Signal Processing and Artificial Intelligence
in Healthcare, Developments in Biomedical Engineering and Bioelec-
tronics, pages 1–28. Elsevier, 2020.

[3] S. Amarjyoti. Deep Reinforcement Learning for Robotic Manipulation

- The State of the Art. CoRR abs/1701.08878, 2017.

[4] C. Beattie, J. Z. Leibo, D. Teplyashin, T. Ward, M. Wainwright,
H. K¨uttler, A. Lefrancq, S. Green, V. Vald´es, A. Sadik, J. Schrittwieser,
K. Anderson, S. York, M. Cant, A. Cain, A. Bolton, S. Gaffney,
H. King, D. Hassabis, S. Legg, and S. Petersen. DeepMind Lab. CoRR,
abs/1612.03801, 2016.

[5] Biscom.

Employee Departure Creates Gaping Security Hole.
https://www.biscom.com/employee-departure-creates-gaping-security-
hole-says-new-data, 2021.

[6] F. Boenisch. A Systematic Review on Model Watermarking for Neural

Networks. Frontiers Big Data, 4:729663, 2021.

14

[7] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman,
J. Tang, and W. Zaremba. OpenAI Gym. CoRR, abs/1606.01540, 2016.
J. Chen, J. Wang, T. Peng, Y. Sun, P. Cheng, S. Ji, X. Ma, B. Li, and
D. Song. Copy, Right? a Testing Framework for Copyright Protection
of Deep Learning Models. In IEEE S&P, pages 824–841, 2022.

[8]

Stealing Deep
[9] K. Chen, S. Guo, T. Zhang, X. Xie, and Y. Liu.
In ACM Asia
Reinforcement Learning Models for Fun and Profit.
Conference on Computer and Communications Security (ASIACCS),
pages 307–319, 2021.

[30]

[31]

I. Ilahi, M. Usama, J. Qadir, M. U. Janjua, A. I. Al-Fuqaha, D. T.
Hoang, and D. Niyato. Challenges and Countermeasures for Adversarial
Attacks on Deep Reinforcement Learning. CoRR abs/2001.09684, 2020.
I. Jarin and B. Eshete. MIAShield: Defending Membership Inference
Attacks via Preemptive Exclusion of Members. In Privacy Enhancing
Technologies Symposium, pages 400–416, 2023.

[32] R. Kidambi, A. Rajeswaran, P. Netrapalli, and T. Joachims. MOReL:
Model-Based Offline Reinforcement Learning. In NeurIPS, 2020.
[33] D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. In

[10] M. Chen, Z. Zhang, T. Wang, M. Backes, M. Humbert, and Y. Zhang.

ICLR, 2014.

When Machine Unlearning Jeopardize Privacy. In ACM CCS, 2021.

[11] M. Chen, Z. Zhang, T. Wang, M. Backes, M. Humbert, and Y. Zhang.

Graph Unlearning. In ACM CCS, 2022.

[12] M. Chen, Z. Zhang, T. Wang, M. Backes, and Y. Zhang. FACE-
AUDITOR: Data Auditing in Facial Recognition Systems. In USENIX
Security, 2023.

[13] L. Du, M. Chen, M. Sun, S. Ji, P. Cheng, J. Chen, and Z. Zhang. ORL-
Auditor: Dataset Auditing in Offline Deep Reinforcement Learning. In
Network and Distributed System Security Symposium (NDSS). Internet
Society, 2024.

[14] L. Du, Z. Zhang, S. Bai, C. Liu, S. Ji, P. Cheng, and J. Chen. AHEAD:
Adaptive Hierarchical Decomposition for Range Query under Local
Differential Privacy. In ACM CCS, 2021.

[15] Y. Du, Y. Hu, Z. Zhang, Z. Fang, L. Chen, B. Zheng, and Y. Gao.
LDPTrace: Locally Differentially Private Trajectory Synthesis.
In
VLDB, 2023.

[16] A. Dziedzic, H. Duan, M. A. Kaleem, N. Dhawan, J. Guan, Y. Cattan,
F. Boenisch, and N. Papernot. Dataset Inference for Self-Supervised
Models. CoRR, abs/2209.09024, 2022.

[17] A. R. Fayjie, S. Hossain, D. Oualid, and D. Lee. Driverless Car:
Autonomous Driving Using Deep Reinforcement Learning in Urban
Environment. In International Conference on Ubiquitous Robots (UR),
pages 896–901, 2018.

[18]

J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine. D4RL: Datasets
for Deep Data-Driven Reinforcement Learning. CoRR, abs/2004.07219,
2020.

[19] S. Fujimoto, E. Conti, M. Ghavamzadeh, and J. Pineau. Bench-
marking Batch Deep Reinforcement Learning Algorithms. CoRR,
abs/1910.01708, 2019.

[20] S. Fujimoto and S. S. Gu. A Minimalist Approach to Offline Rein-

forcement Learning. In NeurIPS, pages 20132–20145, 2021.

[21] S. Fujimoto, D. Meger, and D. Precup. Off-Policy Deep Reinforcement
Learning without Exploration. In ICML, pages 2052–2062, 2019.
[22] S. Fujimoto, H. van Hoof, and D. Meger. Addressing Function
Approximation Error in Actor-Critic Methods. In ICML, pages 1582–
1591, 2018.

[23] M. Gomrokchi, S. Amin, H. Aboutalebi, A. Wong, and D. Precup.
Where Did You Learn That From? Surprising Effectiveness of Mem-
bership Inference Attacks Against Temporally Correlated Data in Deep
Reinforcement Learning. CoRR, abs/2109.03975, 2021.

[24] M. Gomrokchi, S. Main, S. Amin, and D. Precup. PrivAttack: A
Membership Inference Attack Framework Against Deep Reinforcement
Learning Agents. In NeurlPS Workshop, 2020.

[25] F. E. Grubbs. Sample Criteria for Testing Outlying Observations. The

Annals of Mathematical Statistics, pages 27–58, 1950.

[26] C¸ . G¨ulc¸ehre, Z. Wang, A. Novikov, T. Paine, S. G. Colmenarejo,
K. Zolna, R. Agarwal, J. Merel, D. J. Mankowitz, C. Paduraru,
G. Dulac-Arnold, J. Li, M. Norouzi, M. Hoffman, N. Heess, and
N. de Freitas. RL Unplugged: A Collection of Benchmarks for Offline
Reinforcement Learning. In NeurIPS 2020, 2020.

[27] N. G¨urtler, S. Blaes, P. Kolev, F. Widmaier, M. Wuthrich, S. Bauer,
B. Sch¨olkopf, and G. Martius. Benchmarking Offline Reinforcement
Learning on Real-Robot Hardware. In ICLR, 2023.

[28] S. He, K. Shi, C. Liu, B. Guo, J. Chen, and Z. Shi. Collaborative Sensing
in Internet of Things: A Comprehensive Survey. IEEE Communications
Surveys & Tutorials, 2022.

[29] H. Huang, Z. Zhang, Y. Shen, M. Backes, Q. Li, and Y. Zhang. On the
Privacy Risks of Cell-Based NAS Architectures. In ACM CCS, 2022.

[34] P. Kiourti, K. Wardega, J. Susmit, and W. Li. TrojDRL: Evaluation of

[35]

Backdoor Attacks on Deep Reinforcement Learning. In DAC, 2020.
I. Kostrikov, A. Nair, and S. Levine. Offline Reinforcement Learning
with Implicit Q-Learning. In ICLR, 2022.

[36] S. Lange, T. Gabel, and M. A. Riedmiller. Batch Reinforcement
In Reinforcement Learning, volume 12 of Adaptation,

Learning.
Learning, and Optimization, pages 45–73. Springer, 2012.

[37] S. Levine, A. Kumar, G. Tucker, and J. Fu. Offline Reinforcement
Learning: Tutorial, Review, and Perspectives on Open Problems. CoRR,
abs/2005.01643, 2020.

[38] Y. Li, Y. Bai, Y. Jiang, Y. Yang, S. Xia, and B. Li. Untargeted Backdoor
Watermark: Towards Harmless and Stealthy Dataset Copyright Protec-
tion. CoRR, abs/2210.00875, 2022.

[39] Y. Li, Z. Zhang, J. Bai, B. Wu, Y. Jiang, and S. Xia. Open-sourced
Dataset Protection via Backdoor Watermarking. CoRR, abs/2010.05821,
2020.

[40] Z. Lin, J. Gehring, V. Khalidov, and G. Synnaeve. STARDATA: A
StarCraft AI Research Dataset. CoRR, abs/1708.02139, 2017.
[41] Y. Liu, R. Wen, X. He, A. Salem, Z. Zhang, M. Backes, E. D. Cristofaro,
M. Fritz, and Y. Zhang. ML-Doctor: Holistic Risk Assessment of
In USENIX
Inference Attacks Against Machine Learning Models.
Security, 2022.

[42] M. Lopez-Martin, B. Carro, and A. Sanchez-Esguevillas. Application
of Deep Reinforcement Learning to Intrusion Detection for Supervised
Problems. Expert Systems with Applications, 141:112963, 2020.
[43] P. Maini, M. Yaghini, and N. Papernot. Dataset inference: Ownership

resolution in machine learning. In ICLR, 2021.

[44] D. Mwiti. 10 Real-Life Applications of Reinforcement Learning. https:

//neptune.ai/blog/reinforcement-learning-applications, 2021.

[45] H. Ono and T. Takahashi. Locally Private Distributed Reinforcement

Learning. CoRR abs/2001.11718, 2020.

[46] T. L. Paine, C. Paduraru, A. Michi, C¸ . G¨ulc¸ehre, K. Zolna, A. Novikov,
Z. Wang, and N. de Freitas. Hyperparameter Selection for Offline
Reinforcement Learning. CoRR, abs/2007.09055, 2020.

[47] X. Pan, W. Wang, X. Zhang, B. Li, J. Yi, and D. Song. How You Act
Tells a Lot: Privacy-Leaking Attack on Deep Reinforcement Learning.
In AAMAS, pages 368–376, 2019.

[48] A. Pattanaik, Z. Tang, S. Liu, G. Bommannan, and G. Chowdhary.
Robust Deep Reinforcement Learning with Adversarial Attacks.
In
AAMAS, pages 2040–2042, 2018.

[49] D. Pomerleau. ALVINN: An Autonomous Land Vehicle in a Neural

Network. In NeurIPS, pages 305–313, 1988.

[50] R. F. Prudencio, M. R. O. A. M´aximo, and E. L. Colombini. A
Survey on Offline Reinforcement Learning: Taxonomy, Review, and
Open Problems. CoRR, abs/2203.01387, 2022.

[51] H. Pu, L. He, P. Cheng, M. Sun, and J. Chen. Security of Industrial
Robots: Vulnerabilities, Attacks, and Mitigations. IEEE Network, 2022.
[52] R. Qin, S. Gao, X. Zhang, Z. Xu, S. Huang, Z. Li, W. Zhang, and Y. Yu.
NeoRL: A Near Real-World Benchmark for Offline Reinforcement
Learning. CoRR, abs/2102.00714, 2021.

[53] A. Raffin, A. Hill, A. Gleave, A. Kanervisto, M. Ernestus, and
Stable-Baselines3: Reliable Reinforcement Learning
N. Dormann.
Implementations. Journal of Machine Learning Research, 22(268):1–8,
2021.

[54] Y. Rubner, C. Tomasi, and L. J. Guibas. A Metric for Distributions

with Applications to Image Databases. In ICCV, pages 59–66, 1998.

[55] T. Rupprecht and Y. Wang. A Survey for Deep Reinforcement

15

Learning in Markovian Cyber-physical Systems: Common Problems
and Solutions. Neural Networks, 153:13–36, 2022.

[56] T. Seno and M. Imai. d3rlpy: An Offline Deep Reinforcement Learning
Library. Journal of Machine Learning Research, 23(315):1–20, 2022.
[57] R. Shokri, M. Stronati, C. Song, and V. Shmatikov. Membership
Inference Attacks Against Machine Learning Models. In IEEE S&P,
pages 3–18, 2017.

[58] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. A.
Riedmiller. Deterministic Policy Gradient Algorithms. In ICML, pages
387–395, 2014.

[59] M. A. Stephens. EDF Statistics for Goodness of Fit and Some Compar-
isons. Journal of the American statistical Association, 69(347):730–737,
1974.

[60] X. Tang, S. Mahloujifar, L. Song, V. Shejwalkar, M. Nasr,
A. Houmansadr, and P. Mittal. Mitigating Membership Inference
Attacks by Self-Distillation Through a Novel Ensemble Architecture.
In USENIX Security, pages 1433–1450, 2022.

[61] Tessian. How the Great Resignation is Creating More Security Chal-
lenges. https://www.tessian.com/blog/how-the-great-resignation-is-crea
ting-more-security-challenges/, 2021.

[62] L. Van der Maaten and G. Hinton. Visualizing Data Using t-SNE.

Journal of machine learning research, 9(11), 2008.

[63] H. Wang, Z. Zhang, T. Wang, S. He, M. Backes, J. Chen, and
Y. Zhang. PrivTrace: Differentially Private Trajectory Synthesis by
Adaptive Markov Model. In USENIX Security, 2023.

[64] L. Wang, Z. Javed, X. Wu, W. Guo, X. Xing, and D. Song. BACK-
DOORL: Backdoor Attack against Competitive Reinforcement Learn-
ing. In IJCAI, pages 3699–3705, 2021.

[65] T. Wang, J. Q. Chen, Z. Zhang, D. Su, Y. Cheng, Z. Li, N. Li, and
S. Jha. Continuous Release of Data Streams under both Centralized
and Local Differential Privacy. In ACM CCS, 2021.

[66] Y. Wang, E. Sarkar, W. Li, M. Maniatakos, and S. E. Jabari. Stop-
and-Go: Exploring Backdoor Attacks on Deep Reinforcement Learning-
Based Traffic Congestion Control Systems. IEEE TIFS, 16:4772–4787,
2021.

[67] Z. Yang, L. He, H. Yu, C. Zhao, P. Cheng, and J. Chen. Detecting
PLC Intrusions using Control Invariants. IEEE IoT J, 9(12):9934–9947,
2022.

[68] T. Yu, A. Kumar, R. Rafailov, A. Rajeswaran, S. Levine, and C. Finn.
COMBO: Conservative Offline Model-Based Policy Optimization. In
NeurIPS, pages 28954–28967, 2021.

[69] Q. Yuan, Z. Zhang, L. Du, M. Chen, P. Cheng, and M. Sun. PrivGraph:
Differentially Private Graph Data Publication by Exploiting Community
Information. In USENIX Security, 2023.

[70] L. Zeng, M. Sun, X. Wan, Z. Zhang, R. Deng, and Y. Xu. Physics-
constrained Vulnerability Assessment of Deep Reinforcement Learning-
based SCOPF. IEEE TPS, 2022.

[71] Z. Zhang, T. Wang, N. Li, S. He, and J. Chen. CALM: Consistent
Adaptive Local Marginal for Marginal Release under Local Differential
Privacy. In ACM CCS, 2018.

[72] Z. Zhang, T. Wang, N. Li, J. Honorio, M. Backes, S. He, J. Chen, and
Y. Zhang. PrivSyn: Differentially Private Data Synthesis. In USENIX
Security, 2021.

A. The Behavior Similarity of Models

APPENDIX

In Figure 10, we provide the behavior similarity of the
offline RL models trained on the datasets in Table XV. Taking
the Bipedal Walker task as an example, the dataset “0841”
is regarded as the target dataset, and the other four are the
public datasets. We observe that the behavior similarity of the
RL models waves heavily among the different public training
data. If the auditor adopts the dataset “1203” as the public
training data, the auditor likely misclassifies the RL models
trained on the other three public datasets into the bootleg
models. In addition, the behavior similarity is also affected

by different offline RL frameworks, i.e., BC [49], BCQ [21],
[19], IQL [35], and TD3PlusBC [20] (detailed in Section II-B).

B. The Details of Tasks

Lunar Lander (continuous version). The LunarLander task
is to smoothly land a spaceship between two flags on the
target pad. The landing pad is always at coordinates (0,0).
The ship has three throttles; one throttle points downward (the
main engine) and the other two points in the left and right
direction (the left and right engines). The observation is an
8-dimensional vector: the coordinates of the lander in the x-
axis and y-axis, its linear velocities in the x-axis and y-axis,
its angle, its angular velocity, and two booleans that represent
whether each leg is in contact with the ground or not. The
action is two real values ranging in [−1, 1]. The first dimension
controls the main engine, where the engine is off when the
value is in [−1, 0) and increases from 50% to 100% throttle
when the value rises from 0 to 1. The other two points are
controlled by the second value, where the spaceship fires the
left engine if the value in [−1.0, −0.5), fires the right engine
if the value in [0.5, 1), and shuts down both engines if the
value in [−0.5, 0.5]. The reward for moving from the top of
the screen to the landing pad and zero speed is about 140
points. Landing outside the landing pad is possible. Thus, the
player loses the terminal reward if the lander moves away from
the landing pad. The player gets 10 additional points for each
leg touching the ground. Firing the main engine is -0.3 points
in each frame. The episode finishes if the lander crashes or
lands smoothly, receiving -100 or 100 points.

Bipedal Walker. The Bipedal Walker task is to operate a
4-joint walker robot
to move forward as fast as possible.
The robot is made of a hull and two legs. Each leg has 2
joints at both the hip and knee. The observation of the task
includes eight continuous physical variables, i.e., hull angle
speed, angular velocity, horizontal speed, vertical speed, the
position of joints and joints angular speed, legs contact with
ground, and 10 lidar rangefinder measurements. Actions are
motor speed values in the [-1, 1] range for each of the 4 joints
at both hips and knees. The walker starts standing at the left
end of the terrain with the hull horizontal, and both legs in the
same position with a slight knee angle. The reward is given
for moving forward, totaling 300+ points up to the far end.
If the robot falls, it gets -100. Applying motor torque costs
a small amount of points. A more optimal model will get a
better score. The episode will terminate if the hull gets in
contact with the ground or the walker exceeds the right end of
the terrain length.

Ant.
In this task, the player manipulates a 3D robot (ant),
which consists of one torso (free rotational body) with four legs
attached to it, with each leg having two links, to move in the
forward (right) direction. The observation contains positional
values of different body parts of the ant, followed by the
velocities of those individual parts (their derivatives), with all
the positions ordered before all the velocities. By default, an
observation is a vector with shape (111,) where the elements
correspond to the following: position (1-dim), angles (12-
dim), velocities(14-dim), and the information about the contact
forces (84-dim). The player can apply torques on the eight
hinges connecting the two links of each leg and the torso

16

Fig. 10: Models’ behavior similarity measured by ℓ1 Norm, ℓ2 Norm, Cosine Distance, and Wasserstein Distance. From Table XV,
we use the first dataset of each task as the private training data and the remaining four datasets are the public training data.
For each plot, the x-axis displays the four public training data, and the y-axis shows the absolute fluctuating values of the
behavior similarity between the models trained on the private dataset and the public datasets. BC, BCQ, IQL, and TD3PlusBC
are abbreviations for different offline RL frameworks.

(nine parts and eight hinges). Thus, the action space is an
8-dim continuous vector representing the torques applied at
the hinge joints. The reward of the “Ant” task consists of four
parts: healthy reward, forward reward, control cost, and contact
cost. The total reward returned is reward = healthy reward +
forward reward - control cost - contact cost. The task ends
when either the ant state is unhealthy, or the episode duration
reaches 1000 timesteps.

C. Impact of Shadow Models’ Amount

We investigate the relationship between the number of

shadow models and the audit accuracy.

Setup. We change the shadow models’ amount to 9 and 21
with the other settings the same as Section V-B. Figure 6 shows
the value change of TPR and TNR compared with that of 15
shadow models. Each figure’s title illustrates the settings of the
model and the task, the x-axis indicates the four metrics, and
the y-axis is the absolute value change. Also, we provide the
detailed results in Table VIII (9 Shadow Models) and Table IX
(21 Shadow Models).

Observations. From Figure 6, we have the following obser-
vations. 1) The audit accuracy increases with a larger amount
of shadow models. Since the values of shadow models are
the multi-sampling of the true value Q (s, a) of the dataset,
the mean and standard deviation will be more precise with
more shadow models. For example, ORL-AUDITOR suffers an
obvious TPR decline (more than 30%) with 9 shadow models.
Since the insufficient knowledge about the diversity of models

trained on the target dataset, the auditor easily misclassifies
the positive models to the negative group.

2) There exists a saturation point for audit accuracy with
the expansion of shadow models. When the shadow models’
amount rises from 15 to 21, the TPR usually increases since the
auditor observes more possible cumulative rewards originating
from the model trained on the target dataset. We should note
that the value changes slightly in most plots, meaning that
similar cumulative rewards appear in the shadow model set,
and the diversity does not increase significantly compared to
that of 15 shadow models. Therefore, excessive shadow models
are unnecessary, and the auditor needs to burden more training
overhead.

D. Impact of Significance Level

The significance level represents the auditor’s confidence
in the audit results. In Section V-B, we adopt the significance
level α = 0.01, meaning that the auditor has 99% confidence in
the judgments made. Generally speaking, the significance level
represents the maximum audit capacity of ORL-AUDITOR
instead of a hyperparameter setting since it is an audit re-
quirement by the dataset owner.

Setup. We demand the auditor to output a more confident
judgment, where the error possibility should be limited to
1‰ and 0.1‰, i.e., α = 0.001 and α = 0.0001. Figure 7
shows the value change of TPR and TNR compared with that
when significance level α = 0.01. The used offline DRL model
and task is shown in each figure’s title. The x-axis indicates
the four metrics and the y-axis is the absolute value change.

17

l

e
u
a
V

l

e
u
a
V

1400.0
1200.0
1000.0
800.0
600.0
400.0
200.0
0.0

4000.0
3500.0
3000.0
2500.0
2000.0
1500.0
1000.0
500.0
0.0

4000.0

3000.0

2000.0

l

e
u
a
V

1000.0

0.0

l

e
u
a
V

5000.0

4000.0

3000.0

2000.0

1000.0

0.0

Lunar Lander, L1 Norm

Lunar Lander, L2 Norm

Lunar Lander, Cosine Distance

BC
BCQ
IQL
TD3PlusBC

2094

4496

6518

9906

Bipedal Walker, L1 Norm

BC
BCQ
IQL
TD3PlusBC

1203

2110
3813
Ant, L1 Norm

6558

BC
BCQ
IQL
TD3PlusBC

3569

4603

5766

7490

Half Cheetah, L1 Norm

BC
BCQ
IQL
TD3PlusBC

medium

random
Dataset Name

rluply

1400.0

1200.0

1000.0

800.0

600.0

400.0

200.0

0.0

5000.0

4000.0

3000.0

2000.0

1000.0

0.0

4000.0
3500.0
3000.0
2500.0
2000.0
1500.0
1000.0
500.0
0.0

8000.0
7000.0
6000.0
5000.0
4000.0
3000.0
2000.0
1000.0
0.0

BC
BCQ
IQL
TD3PlusBC

2094

4496

6518

9906

Bipedal Walker, L2 Norm

BC
BCQ
IQL
TD3PlusBC

1203

2110
3813
Ant, L2 Norm

6558

BC
BCQ
IQL
TD3PlusBC

3569

4603

5766

7490

Half Cheetah, L2 Norm

BC
BCQ
IQL
TD3PlusBC

medium

random
Dataset Name

rluply

1.2

1.0

0.8

0.6

0.4

0.2

0.0

1.2

1.0

0.8

0.6

0.4

0.2

0.0

0.8

0.6

0.4

0.2

0.0

1.2

1.0

0.8

0.6

0.4

0.2

0.0

BC
BCQ
IQL
TD3PlusBC

2094

4496

6518

9906

Bipedal Walker, Cosine Distance

BC
BCQ
IQL
TD3PlusBC

1203

2110

3813

6558

Ant, Cosine Distance

BC
BCQ
IQL
TD3PlusBC

3569

4603

5766

7490

Half Cheetah, Cosine Distance

BC
BCQ
IQL
TD3PlusBC

medium

random
Dataset Name

rluply

0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0.0

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0

0.2
0.18
0.15
0.12
0.1
0.08
0.05
0.02
0.0

0.6

0.5

0.4

0.3

0.2

0.1

0.0

Lunar Lander, Wasserstein Distance

BC
BCQ
IQL
TD3PlusBC

2094
9906
Bipedal Walker, Wasserstein Distance

4496

6518

BC
BCQ
IQL
TD3PlusBC

1203

2110

3813

6558

Ant, Wasserstein Distance

BC
BCQ
IQL
TD3PlusBC

3569
7490
Half Cheetah, Wasserstein Distance

4603

5766

BC
BCQ
IQL
TD3PlusBC

medium

random
Dataset Name

rluply

2) It should be noticed that a small trajectory size achieves
better results under some tasks. For the Ant
task, ORL-
AUDITOR auditing with 25% of the full length obtains at most
7% promotion on the TNR results. Based on the analysis of
[46], the front states of each trajectory are able to reflect more
behavioral information of the model. Thus, in this case, a
shorter trajectory truncates the rear state-action pairs, which
might be unimportant or even weaken the significance of
the hypothesis testing. Exploring effective data auditing with
shorter trajectory sizes or even using only the first state of each
trajectory would be an interesting future direction.

F. Additional Results

As a supplementary of [13], we provide additional results
about ORL-AUDITOR. For ease of reading, we summarize the
main figures and tables in Table VI.

The detailed results between every two datasets are in Table X
(α = 0.001) and Table XI (α = 0.0001).

Observations. From Figure 7, we have the following obser-
vations. 1) For a complicated task, we recommend the auditor
to select a large significance level for ORL-AUDITOR. The
task’s complexity affects the minimum significance level of
ORL-AUDITOR. For example, TPR and TNP change a little
on the Lunar Lander task when the significance level reduces
to 0.001, while they highly shrink on the Ant task. From
Table I, Ant’s state and action space are larger than that of
Lunar Lander. When the auditor leverages the critic model to
compress each model’s state and action pair into a scalar, the
deviation between Qi
j (recalling Figure 4) on the Ant
task is more imperceptible.

j and Qs

2) For the suspect models with low performance, ORL-
AUDITOR should adopt a large significance level to guarantee
audit accuracy. For instance, in the figure titled with “Bipedal
Walker, TD3PlusBC”, all TNR results from four distance
metrics decrease when α reduces to 0.001 and 0.0001. From
Table XIX, most of the TD3PlusBC models’ performance on
the Bipedal Walker task is around -100, meaning that the
TD3PlusBC models do not fully master the knowledge of the
dataset. Thus, the dataset features reflected in their behavior
are ambiguous, which weakens the difference between positive
and negative samples. Meanwhile, the confidence interval, i.e.,
∆ in Figure 1, expands with a lower significance level. For the
above two reasons, the TNR results of the TD3PlusBC models
on the Bipedal Walker task drop more than 10% compared with
these when α = 0.01.

From the above analysis, α = 0.01 is a safe bound
of ORL-AUDITOR, and a lower α may break through the
capability boundary of ORL-AUDITOR, inducing the auditor
to misclassify the negative model to the positive set.

E. Impact of Trajectory Size

We investigate the relationship between the trajectory size
and audit accuracy. In Section V-B, we adopt the full-length
trajectory, meaning that the auditor utilizes all states of each
trajectory to query the suspect model and obtains the corre-
sponding actions to conduct the dataset audit.

Setup. We change the trajectory size to 25% and 50% of the
full length with the other settings the same as Section V-B.
Figure 8 shows the value change of TPR and TNR compared
with that of the full-length trajectory. Each figure’s title illus-
trates the settings of the model and the task, the x-axis indicates
the four metrics, and the y-axis is the absolute value change.
Also, we provide the detailed results in Table XII (25%) and
Table XIII (50%).

Observation. From Figure 8, we have the following observa-
tions. 1) ORL-AUDITOR tends to achieve higher accuracy with
a larger trajectory size. Since the predicted cumulative rewards
of state-action pairs from the critic model are the audit basis, a
longer trajectory collects more actions from the suspect model
to enhance the significance of hypothesis testing. For example,
the TNP results decrease at most 13% when ORL-AUDITOR
only leverages 25% of the trajectory.

18

TABLE VI: The roadmap of the main figures and tables.

Information
Overview of tasks
Online DRL models

Involved Content
Section V-A
Section V-A

Name
Table I
Table XIV

Offline Datasets

Section V-A

Table XV

Offline DRL models

Section V

Overall audit performance

Section V-B

Impact of shadow models’ amount

Appendix C

Impact of significance level

Appendix D

Impact of trajectory size

Appendix E

Real-world application

Section V-E

Robustness: ensemble architecture

Section VI-A

Robustness: perturbing models output

Section VI-B

Table XVI
Table XVII
Table XVIII
Table XIX

Table IV
Table VII
Figure 11
Figure 12
Figure 13

Figure 6
Table VIII
Table IX

Figure 7
Table X
Table XI

Figure 8
Table XII
Table XIII

Table XXII
Table XX
Table XXI

Table V
Figure 14
Figure 16
Figure 17

Figure 9
Figure 18
Figure 19
Figure 20
Figure 21
Figure 22
Figure 23

Description
The state shape and the action shape of each task.
The performance of the used online models for collecting the offline
datasets.
The name, the number of trajectories, and the length of trajectory for
each offline dataset.
The offline models’ performance with or without defense against ORL-
AUDITOR: normal performance (without defense), defended by model
ensemble, and defended by perturbing models’ output.

The true positive rate (TPR) and true negative rate (TNR) results based
on Grubbs’ test and 3σ principle.

The change values of TPR and TNR when the number of shadow
models varies to 9 and 21 compared to the default 15 shadow models.

The change value of TPR and TNR when the significance level (σ)
varies to 0.001 and 0.0001 compared to the default 0.01.

The change value of TPR and TNR when the trajectory size varies to
25% and 50% compared to the default 100% (full length).

The TPR and TNR results on the Half Cheetah datasets, which are
published by DeepMind and Google separately.

The TPR and TNR results of ORL-AUDITOR against model ensemble
(K = 5).

The TPR and TNR results of ORL-AUDITOR against models’ action
distortion.

TABLE VII: As a supplementary of [13], we provide the TPR and TNR results of ORL-AUDITOR based on 3σ principle. The
mean and standard deviation of TPR and TNR in each row represent the audit results for one combination of task and model
by four distance metrics. Bold indicates the highest sum of TPR and TNR, i.e., accuracy, in a row.

Task
Name

Offline
Model

Lunar
Lander

Bipedal
Walker

Ant

BC
BCQ
IQL
TD3PlusBC
BC
BCQ
IQL
TD3PlusBC
BC
BCQ
IQL
TD3PlusBC

L1 Norm

L2 Norm

Cosine
Distance

Wasserstein
Distance

TPR
96.53±1.36
96.13±3.01
97.20±3.24
95.60±4.39
96.56±4.27
96.67±4.20
94.33±7.45
97.00±4.46
90.67±5.30
90.40±8.68
90.67±6.93
95.62±5.19

TNR
100.00±0.00
100.00±0.00
99.97±0.28
99.54±2.53
100.00±0.00
100.00±0.00
100.00±0.00
99.90±1.16
100.00±0.00
99.96±0.42
100.00±0.00
99.74±1.79

TPR
95.47±2.81
94.80±3.18
96.27±2.44
92.80±5.07
95.78±4.58
94.78±7.43
93.78±7.25
94.11±8.63
93.33±4.62
94.13±3.83
89.60±3.99
94.12±5.03

TNR
100.00±0.00
100.00±0.00
100.00±0.00
99.91±0.47
100.00±0.00
100.00±0.00
100.00±0.00
97.80±12.09
100.00±0.00
99.94±0.56
100.00±0.00
99.35±2.58

TPR
95.73±2.58
94.67±2.92
96.53±2.40
93.33±5.40
98.33±2.50
98.67±1.63
98.89±2.17
95.33±6.66
99.20±0.88
98.00±2.00
97.20±3.89
99.08±1.54

TNR
100.00±0.00
100.00±0.00
100.00±0.00
99.93±0.40
100.00±0.00
100.00±0.00
100.00±0.00
97.78±12.19
88.00±27.55
88.47±26.83
88.30±27.38
88.52±26.25

TPR
96.13±2.02
96.40±2.92
96.53±4.14
96.67±2.88
97.22±4.05
97.11±3.69
94.00±9.45
96.44±5.95
95.20±2.99
93.47±6.81
91.20±9.03
97.74±2.66

TNR
100.00±0.00
99.95±0.36
98.90±3.33
96.86±7.24
100.00±0.00
100.00±0.00
100.00±0.00
93.87±19.73
99.99±0.07
99.95±0.49
100.00±0.00
99.60±2.13

19

Fig. 11: The audit accuracy between every two Lunar Lander datasets. The caption of each plot demonstrates the offline DRL
model’s type, the task, and the distance metric. The x labels are the names of datasets to be audited, i.e., the target datasets. The
y labels are the names of datasets the suspect models learned, i.e., the actual datasets. Thus, the diagonal values show the audit
accuracy when the actual dataset is the same as the target dataset, i.e., TPR, and the non-diagonal values are the TNR results.
The positions without value mean 100% accuracy.

Fig. 12: The audit accuracy between every two Bipedal Walker datasets. The caption of each plot demonstrates the offline DRL
model’s type, the task, and the distance metric. The x labels are the names of datasets to be audited, i.e., the target datasets. The
y labels are the names of datasets the suspect models learned, i.e., the actual datasets. Thus, the diagonal values show the audit
accuracy when the actual dataset is the same as the target dataset, i.e., TPR, and the non-diagonal values are the TNR results.
The positions without value mean 100% accuracy.

20

Lunar Lander, BC, L1 Norm

Lunar Lander, BC, L2 Norm

Lunar Lander, BC, Cosine Distance

98.5

98.8

99.9

2094

4496
Lunar Lander, BCQ, L1 Norm

6518

96.9

1171

95.6

98.8
9906

95.9

97.2

96.8

95.7

97.2

98.1

98.1

2094

4496
Lunar Lander, BCQ, L2 Norm

6518

1171

2094

4496
Lunar Lander, BCQ, Cosine Distance

6518

96.8
9906

95.5

99.8

Lunar Lander, BC, Wasserstein Distance
99.5

99.8

99.9

98.1

99.9

99.3

97.2

98.7

99.9
2094

98.5
9906
4496
1171
Lunar Lander, BCQ, Wasserstein Distance
99.9
99.7
97.7

6518

1171

99.1

2094

4496

6518

9906

1171

1171

98.5

1171

2094

4496

6518

9906

1171

2094

4496

6518

9906

96.4

99.6

94.3

96.9

97.7

95.7

2094

4496
Lunar Lander, IQL, L1 Norm

6518

99.2
9906

1171

2094

4496
Lunar Lander, IQL, L2 Norm

6518

99.5

99.7

99.1

99.9

96.1

99.6

94.4

96.7
9906

97.6
9906

99.5

98.5
2094

1171

4496
Lunar Lander, TD3PlusBC, L1 Norm

99.9
6518

97.6
9906

1171

2094

4496
Lunar Lander, TD3PlusBC, L2 Norm

6518

94.5
9906

1171

97.7

2094

4496

6518

9906

99.9

98.1

94.5

98.9
1171

98.5
2094

99.7
4496

98.1

99.5

98.0
6518

98.3

99.9
9906

92.8

99.9

98.9

99.9

92.5

99.1
1171

99.9
2094

99.7
4496

99.9

98.7
9906

98.8

99.5
6518

92.4

99.9

98.8

99.5

99.1
1171

2094

94.4

96.8

95.7

96.4

99.9

99.9

99.9

99.2

99.5

99.6

96.1

1171

2094

4496
Lunar Lander, IQL, Cosine Distance

6518

97.5
9906

99.1

99.6

99.6

99.4

99.4

99.9

94.0

94.8
1171
9906
4496
Lunar Lander, TD3PlusBC, Cosine Distance

6518

2094

99.8
2094

98.4
9906
4496
1171
Lunar Lander, IQL, Wasserstein Distance
89.4
74.8

6518

99.6

97.6

98.9

98.5

99.1

99.2

99.5

98.4

98.9

99.7

94.9

95.5

99.9

96.9

95.7
2094

98.9
1171

99.9
4496
Lunar Lander, TD3PlusBC, Wasserstein Distance
87.1

99.9
6518

98.3
9906

97.9

99.5

93.4

98.7

92.4

99.7
4496

98.7

6518

99.1
9906

98.9

94.7

93.9
1171

99.6

97.3

99.1

93.3
2094

98.9

96.4

98.6

94.5
4496

94.4

94.1

99.1

96.7
6518

85.6

96.5

96.7

99.7
9906

Bipedal Walker, BC, L1 Norm

Bipedal Walker, BC, L2 Norm

Bipedal Walker, BC, Cosine Distance

Bipedal Walker, BC, Wasserstein Distance

96.3

98.9

99.9

99.7

96.7

1203

2110
Bipedal Walker, BCQ, L1 Norm

3813

99.7
6558

0841

1203

2110
Bipedal Walker, BCQ, L2 Norm

3813

93.1
6558

0841

1203

2110
Bipedal Walker, BCQ, Cosine Distance

3813

93.2
6558

99.9
6558
2110
0841
Bipedal Walker, BCQ, Wasserstein Distance

1203

3813

0841

1203

2110

3813

6558

0841

0841

99.9

1203

2110

3813

6558

0841

1203

2110

3813

6558

0841

1203

2110

98.0

92.5

99.9

99.7

99.7

98.3

99.9

99.7

0841

1203

2110
Bipedal Walker, IQL, L1 Norm

3813

6558

0841

1203

2110
Bipedal Walker, IQL, L2 Norm

3813

6558

0841

1203

2110
Bipedal Walker, IQL, Cosine Distance

3813

99.6
6558

1203

6558
2110
0841
Bipedal Walker, IQL, Wasserstein Distance

3813

99.5

80.7

93.7

85.2

95.5

96.4

99.2

95.5

82.0

97.6

0841

1203

2110
Bipedal Walker, TD3PlusBC, L1 Norm

3813

99.9
6558

10.4

0841

1203

2110
Bipedal Walker, TD3PlusBC, L2 Norm
89.1

3813

0.7

99.9
6558

0841

1203

2110
Bipedal Walker, TD3PlusBC, Cosine Distance
87.9

6558

3813

99.7

0.1

0841

2110
Bipedal Walker, TD3PlusBC, Wasserstein Distance

3813

1203

6558

1.9

48.7

3813

96.0

98.0

94.9

6558

0841

1203

2110

3813

96.0

96.8
6558

94.0

96.0

96.5

0841

1203

2110

3813

99.5

92.0

94.0

94.4

0841

1203

2110

3813

97.1

94.0

85.7
6558

79.6

92.0

85.3
6558

92.0

94.2

98.0

94.5

43.8

0841

1203

2110

3813

92.0

90.4
6558

Fig. 13: The audit accuracy between every two Ant datasets. The caption of each plot demonstrates the offline DRL model’s
type, the task, and the distance metric. The x labels are the names of datasets to be audited, i.e., the target datasets. The y
labels are the names of datasets the suspect models learned, i.e., the actual datasets. Thus, the diagonal values show the audit
accuracy when the actual dataset is the same as the target dataset, i.e., TPR, and the non-diagonal values are the TNR results.
The positions without value mean 100% accuracy.

TABLE VIII: The impact of shadow models’ amount. The TPR and TNR results of ORL-AUDITOR with 9 shadow models.

Task
Name

Lunar
Lander

Bipedal
Walker

Ant

Offline
Model

BC
BCQ
IQL

TPR
97.09±1.09
96.97±1.65
96.89±1.96
TD3PlusBC 97.24±2.17
95.14±3.54
93.90±5.98
88.55±10.61
TD3PlusBC 97.39±5.22
90.61±6.99
92.65±3.46
97.05±1.06
TD3PlusBC 93.57±7.04

BC
BCQ
IQL

BC
BCQ
IQL

L1 Norm

L2 Norm

Cosine
Distance

Wasserstein
Distance

TNR
100.00±0.00
100.00±0.00
99.90±0.37
99.32±0.84
100.00±0.00
100.00±0.00
100.00±0.00
94.30±20.23
99.93±0.16
99.78±0.50
99.58±0.92
99.35±1.27

TPR
94.97±1.31
94.53±1.20
95.22±2.85
93.77±3.64
89.68±10.06
95.47±3.37
87.79±8.56
96.57±6.86
92.25±4.98
90.00±5.47
94.44±2.40
93.15±4.39

TNR
100.00±0.00
100.00±0.00
99.98±0.07
99.82±0.45
100.00±0.00
100.00±0.00
100.00±0.00
90.88±22.63
99.95±0.17
99.88±0.27
99.63±0.72
99.59±1.05

TPR
95.09±1.41
94.48±1.09
95.03±3.13
93.81±3.71
97.70±3.59
98.69±0.93
98.80±1.27
97.30±4.95
98.91±0.99
98.02±1.01
99.12±0.43
99.35±0.75

TNR
100.00±0.00
99.98±0.10
99.91±0.22
99.78±0.49
100.00±0.00
100.00±0.00
100.00±0.00
88.39±24.21
85.17±28.30
85.88±28.10
85.16±28.62
87.99±26.18

TPR
96.55±1.98
97.03±1.51
96.82±2.14
97.54±1.16
94.80±3.69
95.35±4.04
90.68±8.87
96.08±7.85
96.23±3.90
98.11±1.42
98.50±1.30
97.86±1.32

TNR
99.93±0.26
99.78±0.38
96.85±6.45
95.45±3.71
100.00±0.00
100.00±0.00
100.00±0.00
84.40±30.65
99.92±0.16
99.76±0.52
99.57±0.93
99.30±1.36

21

Ant, BC, L1 Norm

Ant, BC, L2 Norm

2232

3569

4603

5766

7490

2232

3569

4603

5766

7490

2232

3569

4603

5766

7490

97.5

99.7

99.9

2232

96.4

98.9

2232

93.7

98.5

99.8

2232

99.9

98.0

99.6

99.1

99.9

99.8

94.3

3569

4603
Ant, BCQ, L1 Norm

5766

99.9

97.9

98.7

98.7

99.9

91.9

3569

4603
Ant, IQL, L1 Norm

5766

99.9

99.6

97.5

97.1

97.9

99.9

99.9

3569

4603
Ant, TD3PlusBC, L1 Norm

5766

99.9

98.3
7490

99.9

99.7
7490

99.6

99.9

95.7
7490

2232

3569

99.1

4603

99.8

96.4

5766

97.5

99.5

7490

98.3

96.2

92.5

98.0

98.8

95.9

99.9

2232

94.0

99.1

2232

91.9

98.9

99.9

2232

97.9

99.5

98.3

96.3

99.9

99.1

94.0

3569

4603
Ant, BCQ, L2 Norm

5766

98.3

94.9

99.5

98.4

92.3

3569

4603
Ant, IQL, L2 Norm

5766

99.9

97.2
7490

99.5

99.9

9.0
2232

99.9

99.5

98.8
7490

10.7
2232

Ant, BC, Cosine Distance
99.9

98.0

56.1

97.1

99.9

93.8

99.6

88.6
3569

99.9
4603
Ant, BCQ, Cosine Distance

56.1
5766

12.0

99.9

99.9
7490

Ant, BC, Wasserstein Distance

99.9

97.9

99.5

99.7

99.9

99.5

99.1

99.9

99.7

99.8

97.5

2232

3569

4603
Ant, BCQ, Wasserstein Distance

5766

96.1
7490

98.1

98.8

65.9

14.7

99.5

99.9

92.9

99.9

99.9

97.1

98.9

98.6

99.9

98.6

99.9

96.3

90.9
3569

99.9
4603
Ant, IQL, Cosine Distance

62.0
5766

99.9
7490

11.9

2232

99.6

98.5

99.8

3569

4603
Ant, IQL, Wasserstein Distance

5766

99.9

99.1

97.5

99.3

97.6

99.9

99.9

7490

99.6

99.8

98.7

97.3

98.1

96.7

98.5

99.9

98.3

99.8

99.3

98.7

98.0

99.1

74.7

99.7

92.4

99.9

3569

4603
Ant, TD3PlusBC, L2 Norm

5766

98.9
7490

8.8
2232

90.5
3569

99.6
4603
Ant, TD3PlusBC, Cosine Distance

50.3
5766

7490

2232

3569

4603
Ant, TD3PlusBC, Wasserstein Distance

5766

98.8
7490

96.1

99.0

94.6

92.0

98.2

99.7

99.0

99.5

99.9

99.6

99.9

98.9

98.2

7.1
2232

99.7

91.5
3569

97.2
7490

75.2

99.1

91.3

99.9

89.3
5766

99.2

99.9

92.2

100.0
4603

14.1

99.5

99.7

99.5

99.1

97.5

99.5

98.3

96.8

92.0

97.8

98.8

7490

2232

3569

4603

5766

7490

2232

3569

4603

5766

7490

2232

3569

4603

5766

TABLE IX: The impact of shadow models’ amount. The TPR and TNR results of ORL-AUDITOR with 21 shadow models.

Task
Name

Lunar
Lander

Bipedal
Walker

Ant

Offline
Model

BC
BCQ
IQL

TPR
99.25±0.97
99.56±0.32
97.95±2.45
TD3PlusBC 97.87±3.45
97.07±3.60
100.00±0.00
95.91±4.93
TD3PlusBC 99.91±0.18
98.13±1.55
97.16±2.73
95.91±3.87
TD3PlusBC 99.44±0.60

BC
BCQ
IQL

BC
BCQ
IQL

L1 Norm

L2 Norm

Cosine
Distance

Wasserstein
Distance

TNR
100.00±0.00
100.00±0.00
99.96±0.17
99.45±0.84
100.00±0.00
100.00±0.00
100.00±0.00
95.05±19.14
99.91±0.18
99.81±0.42
99.64±0.77
99.23±1.53

TPR
98.13±1.88
98.40±0.68
97.11±3.12
96.09±3.91
97.24±4.49
99.56±0.69
96.36±4.57
99.87±0.27
97.73±1.19
96.67±2.18
96.49±3.89
98.27±1.03

TNR
100.00±0.00
100.00±0.00
99.98±0.09
99.73±0.58
100.00±0.00
100.00±0.00
100.00±0.00
93.96±20.97
99.86±0.41
99.84±0.42
99.68±0.63
99.36±1.64

TPR
98.00±1.84
98.27±0.47
96.85±3.40
95.78±4.29
97.69±4.51
99.07±1.65
99.51±0.26
99.82±0.36
99.73±0.53
99.69±0.41
99.51±0.67
99.76±0.33

TNR
100.00±0.00
99.99±0.04
99.92±0.19
99.95±0.14
100.00±0.00
100.00±0.00
100.00±0.00
92.79±21.27
86.82±26.97
87.53±26.74
86.53±27.80
88.42±25.93

TPR
99.11±0.93
98.80±0.78
97.11±3.65
98.00±2.60
98.36±2.67
99.96±0.09
96.44±4.54
99.91±0.18
97.55±1.57
98.58±1.69
97.65±2.19
99.79±0.27

TNR
99.96±0.10
99.75±0.41
97.47±5.34
96.27±3.43
100.00±0.00
100.00±0.00
100.00±0.00
91.78±21.28
99.90±0.21
99.80±0.43
99.64±0.78
99.18±1.65

TABLE X: The impact of significance level. The TPR and TNR results of ORL-AUDITOR with σ = 0.001.

Task
Name

Lunar
Lander

Bipedal
Walker

Ant

Offline
Model

BC
BCQ
IQL

TPR
99.63±0.19
99.15±0.67
99.31±0.90
TD3PlusBC 99.20±1.10
99.97±0.05
99.95±0.06
97.04±5.47
TD3PlusBC 99.92±0.16
99.52±0.50
98.91±1.68
98.88±1.27
TD3PlusBC 99.62±0.46

BC
BCQ
IQL

BC
BCQ
IQL

L1 Norm

L2 Norm

Cosine
Distance

Wasserstein
Distance

TNR
100.00±0.00
100.00±0.00
99.83±0.40
99.22±0.89
100.00±0.00
100.00±0.00
100.00±0.00
89.69±22.99
99.86±0.25
99.71±0.60
99.53±0.97
98.92±2.08

TPR
98.21±0.55
97.60±1.13
98.56±1.51
97.47±2.33
98.67±2.67
99.73±0.34
95.81±5.06
97.17±5.65
98.48±0.80
97.97±1.43
98.42±1.80
98.73±0.97

TNR
100.00±0.00
100.00±0.00
99.96±0.16
99.61±0.55
100.00±0.00
100.00±0.00
100.00±0.00
85.59±27.05
99.88±0.40
99.81±0.47
99.60±0.73
99.24±1.91

TPR
98.21±0.63
97.63±1.04
98.51±1.59
97.55±2.22
98.64±2.66
99.95±0.06
99.87±0.27
97.20±5.60
99.55±0.66
99.87±0.15
99.71±0.52
99.79±0.36

TNR
100.00±0.00
99.97±0.13
99.79±0.51
99.75±0.51
100.00±0.00
100.00±0.00
100.00±0.00
82.52±30.48
80.58±33.24
81.88±31.60
80.90±32.60
84.36±28.07

TPR
99.31±0.38
98.59±0.95
99.04±1.21
99.49±0.56
100.00±0.00
99.97±0.05
97.68±4.51
99.68±0.64
99.36±0.49
99.52±0.64
99.92±0.06
99.71±0.58

TNR
99.84±0.38
99.61±0.53
94.88±8.21
92.45±5.32
100.00±0.00
100.00±0.00
100.00±0.00
80.18±35.21
99.85±0.25
99.69±0.63
99.49±1.05
98.78±2.15

TABLE XI: The impact of significance level. The TPR and TNR results of ORL-AUDITOR with σ = 0.0001.

Task
Name

Lunar
Lander

Bipedal
Walker

Ant

Offline
Model

BC
BCQ
IQL

TPR
99.87±0.12
99.49±0.46
99.71±0.52
TD3PlusBC 99.55±0.56
100.00±0.00
100.00±0.00
98.53±2.87
TD3PlusBC 100.00±0.00
99.95±0.11
99.33±1.21
99.73±0.29
TD3PlusBC 99.96±0.05

BC
BCQ
IQL

BC
BCQ
IQL

L1 Norm

L2 Norm

Cosine
Distance

Wasserstein
Distance

TNR
100.00±0.00
100.00±0.00
99.66±0.55
98.80±1.27
100.00±0.00
100.00±0.00
100.00±0.00
79.08±38.26
99.80±0.41
99.56±0.76
99.35±1.25
98.65±2.42

TPR
98.93±0.34
98.48±0.79
98.99±1.04
98.48±1.48
98.67±2.67
99.81±0.26
96.27±4.74
97.23±5.55
99.15±0.67
98.75±1.11
98.91±1.45
99.35±0.50

TNR
100.00±0.00
100.00±0.00
99.91±0.21
98.96±1.24
100.00±0.00
100.00±0.00
100.00±0.00
75.43±39.10
99.85±0.43
99.80±0.48
99.56±0.80
99.11±2.07

TPR
99.04±0.35
98.48±0.82
98.91±1.17
98.56±1.49
98.80±2.40
100.00±0.00
99.87±0.27
97.33±5.33
99.73±0.41
99.92±0.11
99.89±0.21
99.92±0.11

TNR
100.00±0.00
99.95±0.20
99.49±1.15
99.38±0.87
100.00±0.00
100.00±0.00
100.00±0.00
74.60±39.23
77.36±35.68
78.87±33.94
77.78±34.93
81.33±29.74

TPR
99.79±0.22
99.33±0.54
99.52±0.70
99.84±0.16
100.00±0.00
100.00±0.00
98.67±2.67
99.97±0.05
99.73±0.22
99.81±0.23
100.00±0.00
99.90±0.19

TNR
99.56±0.79
98.88±1.54
91.46±10.53
88.16±6.84
100.00±0.00
100.00±0.00
100.00±0.00
73.98±38.98
99.79±0.42
99.52±0.79
99.32±1.31
98.06±3.18

22

TABLE XII: The impact of trajectory size. The TPR and TNR results of ORL-AUDITOR with 25% trajectory size.

Task
Name

Lunar
Lander

Bipedal
Walker

Ant

Offline
Model

BC
BCQ
IQL

TPR
98.13±1.05
98.45±0.51
98.11±1.65
TD3PlusBC 98.00±2.42
99.20±0.97
98.59±2.63
96.80±5.37
TD3PlusBC 97.55±4.91
98.85±0.67
98.11±1.40
98.45±1.00
TD3PlusBC 98.80±1.33

BC
BCQ
IQL

BC
BCQ
IQL

L1 Norm

L2 Norm

Cosine
Distance

Wasserstein
Distance

TNR
99.53±1.30
99.28±1.19
95.42±5.71
95.44±4.57
100.00±0.00
100.00±0.00
100.00±0.00
94.08±21.64
99.90±0.35
99.85±0.34
99.90±0.30
99.76±0.43

TPR
96.27±2.00
97.33±0.76
96.72±2.57
96.43±3.13
97.47±3.12
97.68±2.95
95.73±5.42
97.20±5.60
97.04±1.24
97.36±1.61
96.45±1.47
96.92±1.60

TNR
99.64±1.09
99.59±0.71
97.10±3.96
96.86±3.24
100.00±0.00
100.00±0.00
100.00±0.00
89.72±24.95
99.84±0.47
99.78±0.46
99.85±0.40
99.70±0.67

TPR
96.29±1.97
96.91±1.06
96.80±2.25
95.95±2.56
98.61±2.71
99.68±0.27
99.41±0.45
96.93±5.74
99.49±0.88
99.49±0.76
99.68±0.51
99.22±1.18

TNR
99.13±2.41
99.01±1.33
92.42±5.78
92.95±3.92
100.00±0.00
100.00±0.00
100.00±0.00
90.65±23.69
92.58±19.10
92.64±19.34
92.56±20.01
93.58±17.31

TPR
98.10±0.92
98.56±1.10
98.19±1.55
98.45±1.43
99.36±0.90
98.61±2.45
97.01±5.45
97.41±5.17
98.96±0.81
99.12±0.59
99.04±0.55
99.28±1.07

TNR
97.74±2.30
94.02±4.44
84.64±9.09
81.51±9.13
100.00±0.00
100.00±0.00
100.00±0.00
84.06±33.72
99.90±0.35
99.84±0.34
99.80±0.49
99.74±0.45

TABLE XIII: The impact of trajectory size. The TPR and TNR results of ORL-AUDITOR with 50% trajectory size.

Task
Name

Lunar
Lander

Bipedal
Walker

Ant

Offline
Model

BC
BCQ
IQL

TPR
98.37±0.68
98.16±0.55
98.03±2.25
TD3PlusBC 98.03±2.33
99.44±0.75
98.75±2.38
95.68±6.60
TD3PlusBC 98.35±3.31
98.21±0.98
97.76±2.05
97.71±1.81
TD3PlusBC 98.52±1.81

BC
BCQ
IQL

BC
BCQ
IQL

L1 Norm

L2 Norm

Cosine
Distance

Wasserstein
Distance

TNR
100.00±0.00
99.96±0.19
98.93±1.35
98.28±2.03
100.00±0.00
100.00±0.00
100.00±0.00
94.29±21.47
99.92±0.24
99.85±0.36
99.73±0.61
99.61±0.77

TPR
97.07±0.90
96.11±0.83
96.80±2.84
96.37±3.41
97.81±2.83
97.92±2.81
95.47±5.34
97.20±5.60
97.04±1.33
96.72±1.50
96.53±1.65
96.99±1.57

TNR
100.00±0.00
99.95±0.20
99.29±1.06
99.27±0.97
100.00±0.00
100.00±0.00
100.00±0.00
91.75±22.51
99.85±0.43
99.81±0.44
99.82±0.40
99.74±0.64

TPR
97.25±0.72
96.40±0.84
97.25±2.18
96.51±2.98
98.67±2.67
99.89±0.10
99.81±0.31
96.96±6.02
99.49±0.83
99.60±0.60
99.79±0.30
99.82±0.25

TNR
100.00±0.02
99.58±0.80
96.38±2.48
95.94±4.14
100.00±0.00
100.00±0.00
100.00±0.00
90.88±23.07
88.52±25.25
89.27±24.10
88.67±25.63
90.62±24.30

TPR
98.58±0.50
97.57±1.64
98.27±2.29
98.24±1.79
99.41±0.86
99.55±0.72
96.40±6.11
97.41±5.17
98.59±0.81
98.88±1.30
98.99±0.65
99.13±1.29

TNR
98.50±1.91
96.28±3.72
86.89±10.96
84.30±8.08
100.00±0.00
100.00±0.00
100.00±0.00
89.04±27.90
99.92±0.24
99.84±0.36
99.70±0.66
99.58±0.79

TABLE XIV: The details of the online models for generating the offline datasets. The model performance shows the cumulative
reward for 10 separate evaluations.

Task Name Online Model Train Step Model Name Model Performance

Lunar Lander

SAC

1e6

Bipedal Walker

PPO

1e6

Ant

SAC

2e6

1171
2094
4496
6518
9906
0841
1203
2110
3813
6558
2232
3569
4603
5766
7490

275.47±14.38
50.79±65.95
195.02±143.15
246.40±33.91
209.33±91.73
285.55±60.84
286.94±53.46
283.58±47.35
235.88±103.83
285.16±65.92
5377.70±1653.17
1924.58±1180.96
5531.45±844.10
3025.89±547.36
5897.37±477.34

23

Fig. 14: The audit accuracy against model ensemble for Lunar Lander. The caption of each plot demonstrates the offline DRL
model’s type, the task, the distance metric, and the hyperparameter K of the model ensemble. The x labels are the names of
datasets to be audited, i.e., the target datasets. The y labels are the names of datasets the suspect models learned, i.e., the actual
datasets. Thus, the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset, i.e., TPR,
and the non-diagonal values are the TNR results. The positions without value mean 100% accuracy.

TABLE XV: The details of the offline DRL datasets

Task Name

Number of Transitions Dataset Name Number of Trajectories Length of trajectory

Lunar Lander

5e5

Bipedal Walker

1e6

Ant

2e6

2175
578
1252
1878
1566
1019
1027
877
887
1041
2093
3497
2096
2217
2103

229.83±83.51
864.19±231.88
399.30±240.88
266.13±99.65
319.21±231.06
981.03±190.79
973.07±118.42
1139.55±151.10
1126.63±379.05
959.77±146.13
955.46±177.72
571.66±375.40
954.01±175.82
901.84±236.93
951.02±187.93

1171
2094
4496
6518
9906
0841
1203
2110
3813
6558
2232
3569
4603
5766
7490

24

Lunar Lander, BC, L1 Norm, K = 5

Lunar Lander, BC, L2 Norm, K = 5

Lunar Lander, BC, Cosine, K = 5

Lunar Lander, BC, Wasserstein, K = 5

98.0

98.0

98.0

98.0

98.0

98.0

1171

2094

4496

6518

9906

1171

2094

4496
Lunar Lander, BCQ, L1 Norm, K = 5

6518

9906

1171

2094

4496
Lunar Lander, BCQ, L2 Norm, K = 5

6518

9906

1171

2094

4496
Lunar Lander, BCQ, Cosine, K = 5

6518

9906

1171

98.0

98.0

98.0

2094

4496

6518

9906

1171

2094

4496

6518

9906

1171

2094

4496

6518

9906

1171

2094

4496
Lunar Lander, IQL, L1 Norm, K = 5

6518

9906

1171

2094

4496
Lunar Lander, IQL, L2 Norm, K = 5

6518

94.0
9906

1171

2094

4496
Lunar Lander, IQL, Cosine, K = 5

6518

94.0
9906

98.0

98.0

98.0

98.0

98.0

98.0

94.0

98.0

98.0
2094

9906
4496
1171
Lunar Lander, TD3PlusBC, L1 Norm, K = 5

6518

98.0
9906
4496
1171
Lunar Lander, TD3PlusBC, L2 Norm, K = 5

2094

6518

2094

9906
4496
1171
Lunar Lander, TD3PlusBC, Cosine, K = 5

6518

96.0
1171

96.0
2094

4496
Lunar Lander, TD3PlusBC, Wasserstein, K = 5
92.0

6518

98.0

92.0

98.0
9906

98.0

98.0

98.0

98.0

98.0
1171

98.0
2094

98.0
4496

98.0
6518

9906

98.0
1171

2094

4496

6518

9906

98.0
1171

2094

98.0

98.0
4496

6518

9906

96.0

96.0

98.0

96.0
1171

92.0
2094

94.0

94.0

98.0
6518

86.0

98.0

96.0

9906

98.0

98.0

92.0
4496

2094

1171
9906
4496
Lunar Lander, BCQ, Wasserstein, K = 5

6518

1171

2094

4496
Lunar Lander, IQL, Wasserstein, K = 5
88.0
84.0

6518

98.0

98.0
9906

Fig. 15: The audit accuracy against model ensemble for Bipedal Walker. The caption of each plot demonstrates the offline DRL
model’s type, the task, the distance metric, and the hyperparameter K of the model ensemble. The x labels are the names of
datasets to be audited, i.e., the target datasets. The y labels are the names of datasets the suspect models learned, i.e., the actual
datasets. Thus, the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset, i.e., TPR,
and the non-diagonal values are the TNR results. The positions without value mean 100% accuracy.

TABLE XVI: As a supplementary of [13], we provide more details of the BC offline models. The model performance shows
the cumulative reward for 10 separate evaluations.

Offline
Model

Task
Name

Lunar
Lander

BC

Bipedal
Walker

Ant

Dataset
Name
1171
2094
4496
6518
9906
0841
1203
2110
3813
6558
2232
3569
4603
5766
7490

Model Performance
(No Defense)
272.06±5.14
39.04±34.07
173.62±58.93
211.85±43.52
225.45±32.91
264.92±29.11
288.85±15.39
276.80±26.03
164.56±46.14
281.02±48.65
5479.72±354.79
1493.77±413.96
5424.74±422.83
2806.80±286.57
5514.17±441.78

Model Performance
(Trajectory Splitting)
269.17±11.28
46.65±37.73
183.34±47.00
215.13±57.35
213.70±40.65
277.42±18.00
287.12±16.88
277.15±24.63
156.62±47.94
277.24±54.87
5427.47±609.23
1523.73±473.18
5463.20±511.58
2863.00±291.50
5410.28±467.33

Model Performance
(Model Ensemble)
266.20±13.81
45.74±116.66
189.41±102.32
199.44±118.66
234.34±67.54
241.77±117.88
298.62±1.29
265.78±98.38
66.65±97.36
308.01±0.87
5933.60±98.05
1695.64±1255.83
5269.72±1692.57
2951.89±728.14
5785.87±630.97

Model Performance
(Gauss. 0.01)
270.84±6.38
55.03±34.09
161.95±54.27
223.84±41.63
215.19±35.72
257.99±26.73
287.64±16.36
283.05±20.17
160.48±56.24
284.69±22.77
5324.99±441.27
1460.97±436.37
5470.37±473.25
2899.11±313.43
5451.01±430.96

Model Performance
(Gauss. 0.1)
269.01±11.12
53.99±30.89
177.23±35.35
219.14±44.10
215.76±33.81
268.83±25.55
285.91±15.71
286.19±14.48
182.20±55.79
268.39±39.12
4332.23±589.30
1412.06±391.99
4679.76±496.30
2458.92±272.67
4417.19±687.25

25

Bipedal Walker, BC, L1 Norm, K = 5

Bipedal Walker, BC, L2 Norm, K = 5

Bipedal Walker, BC, Cosine, K = 5

Bipedal Walker, BC, Wasserstein, K = 5

0841

1203

2110

3813

6558

0841

1203

2110

3813

6558

0841

1203

2110

3813

6558

0841

1203

2110

0841

1203

2110
Bipedal Walker, BCQ, L1 Norm, K = 5

3813

6558

0841

1203

2110
Bipedal Walker, BCQ, L2 Norm, K = 5

3813

6558

0841

1203

2110
Bipedal Walker, IQL, L1 Norm, K = 5

3813

6558

0841

1203

2110
Bipedal Walker, IQL, L2 Norm, K = 5

3813

6558

0841

1203

2110
Bipedal Walker, BCQ, Cosine, K = 5

3813

6558

0841

1203

2110
Bipedal Walker, IQL, Cosine, K = 5

3813

6558

0841

2110
Bipedal Walker, TD3PlusBC, L1 Norm, K = 5

6558

1203

3813

12.0

0841

1203

2110
Bipedal Walker, TD3PlusBC, L2 Norm, K = 5
94.0

3813

6558

0.0

1203

6558
2110
0841
Bipedal Walker, TD3PlusBC, Cosine, K = 5
94.0

3813

0.0

1203

0841
6558
2110
Bipedal Walker, BCQ, Wasserstein, K = 5

3813

1203

0841
6558
2110
Bipedal Walker, IQL, Wasserstein, K = 5

3813

0841

2110
Bipedal Walker, TD3PlusBC, Wasserstein, K = 5

3813

1203

6558

4.0

52.0

54.0

98.0

96.0

92.0

86.0

92.0

94.0

94.0

3813

96.0

98.0

96.0

96.0

94.0

96.0

98.0

94.0

92.0

94.0

96.0

6558

0841

1203

2110

3813

6558

0841

1203

2110

3813

6558

0841

1203

2110

3813

6558

0841

1203

2110

3813

6558

Fig. 16: The audit accuracy against model ensemble for Ant. The caption of each plot demonstrates the offline DRL model’s
type, the task, the distance metric, and the hyperparameter K of the model ensemble. The x labels are the names of datasets to
be audited, i.e., the target datasets. The y labels are the names of datasets the suspect models learned, i.e., the actual datasets.
Thus, the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset, i.e., TPR, and the
non-diagonal values are the TNR results. The positions without value mean 100% accuracy.

TABLE XVII: As a supplementary of [13], we provide more details of the BCQ offline models. The model performance shows
the cumulative reward for 10 separate evaluations.

Offline
Model

Task
Name

Lunar
Lander

BCQ

Bipedal
Walker

Ant

Dataset
Name
1171
2094
4496
6518
9906
0841
1203
2110
3813
6558
2232
3569
4603
5766
7490

Model Performance
(No Defense)
270.69±8.51
52.67±26.38
166.13±57.37
234.99±30.93
243.74±24.43
228.05±43.06
269.87±28.93
281.34±18.56
166.87±55.81
271.52±57.34
3844.45±875.84
1032.55±327.13
4554.06±676.32
2583.27±268.12
3653.48±1108.85

Model Performance
(Trajectory Splitting)
270.45±12.51
64.70±22.08
195.16±37.67
227.41±36.30
236.93±23.49
235.75±39.17
276.35±23.98
282.23±20.88
181.39±45.03
271.09±75.34
3651.94±943.58
951.30±312.96
4562.26±828.88
2502.33±323.71
3755.22±1159.16

Model Performance
(Model Ensemble)
278.43±9.57
30.79±81.96
88.06±182.38
233.08±45.90
236.40±41.93
229.03±117.30
243.15±112.91
264.16±97.68
131.04±165.52
306.09±4.03
4295.42±2225.70
435.72±420.34
3980.17±2203.92
2603.11±1075.03
4012.54±2267.61

Model Performance
(Gauss. 0.01)
268.41±13.91
57.80±30.64
188.89±38.33
236.13±33.25
237.51±31.45
247.17±37.65
276.78±21.02
270.97±24.22
177.90±52.03
275.55±30.70
3587.01±816.81
1013.29±283.76
4480.04±639.38
2640.93±323.35
3552.11±1115.43

Model Performance
(Gauss. 0.1)
270.25±13.45
55.73±28.36
191.19±46.30
235.19±25.16
233.42±34.97
249.94±28.21
281.59±17.69
270.78±26.49
185.86±45.97
262.56±43.95
2514.55±772.19
942.25±266.46
3412.76±804.53
2031.98±293.49
2432.82±892.02

26

Ant, BC, L1 Norm, K = 5

2232

98.0

Ant, BC, L2 Norm, K = 5
98.0

98.0

Ant, BC, Cosine, K = 5
98.0
28.0

98.0

10.0

96.0

Ant, BC, Wasserstein, K = 5

2232

3569

4603

5766

7490

2232

3569

4603

5766

7490

3569

4603
Ant, BCQ, L1 Norm, K = 5

5766

7490

2232

3569

4603
Ant, BCQ, L2 Norm, K = 5

5766

98.0

98.0

98.0

98.0

98.0

98.0

2232

3569
5766
4603
Ant, IQL, L1 Norm, K = 5

7490

2232

3569
5766
4603
Ant, IQL, L2 Norm, K = 5

7490

2232

3569

98.0

98.0

98.0

98.0

98.0

98.0

98.0

92.0

7490

10.0
2232

32.0
94.0
3569
5766
4603
Ant, BCQ, Cosine, K = 5
44.0
98.0

7490

2232

18.0

3569

4603
Ant, BCQ, Wasserstein, K = 5

5766

98.0

98.0

98.0

98.0

54.0
94.0
3569
5766
4603
Ant, IQL, Cosine, K = 5
82.0
98.0

7490

2232

14.0

3569

4603
Ant, IQL, Wasserstein, K = 5

5766

98.0

98.0

98.0

98.0

7490

98.0

7490

2232

3569

4603
Ant, TD3PlusBC, L1 Norm, K = 5

5766

7490

2232

3569

4603
Ant, TD3PlusBC, L2 Norm, K = 5

5766

7490

4603
Ant, TD3PlusBC, Cosine, K = 5

98.0

98.0

98.0

98.0

92.0

98.0

90.0

2232

3569

4603

5766

7490

2232

3569

4603

5766

7490

74.0

98.0

90.0

84.0
5766

8.0
2232

94.0
3569

90.0

4603

7490

18.0

2232

3569

4603
Ant, TD3PlusBC, Wasserstein, K = 5

5766

7490

98.0

98.0

98.0

84.0

92.0

98.0

7490

2232

3569

4603

5766

7490

94.0
3569

46.0
5766

4603

5766

7490

2232

3569

4603

5766

7490

10.0
2232

98.0

98.0

6.0
2232

Fig. 17: The audit accuracy against model ensemble for Half Cheetah. The caption of each plot demonstrates the offline DRL
model’s type, the task, the distance metric, and the hyperparameter K of the model ensemble. The x labels are the names of
datasets to be audited, i.e., the target datasets. The y labels are the names of datasets the suspect models learned, i.e., the actual
datasets. Thus, the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset, i.e., TPR,
and the non-diagonal values are the TNR results. The positions without value mean 100% accuracy.

TABLE XVIII: As a supplementary of [13], we provide more details of the IQL offline models. The model performance shows
the cumulative reward for 10 separate evaluations.

Offline
Model

Task
Name

Lunar
Lander

IQL

Bipedal
Walker

Ant

Dataset
Name
1171
2094
4496
6518
9906
0841
1203
2110
3813
6558
2232
3569
4603
5766
7490

Model Performance
(No Defense)
268.55±10.81
57.92±31.14
181.48±40.09
226.85±43.37
237.33±29.23
261.23±33.91
284.63±17.38
285.24±28.87
169.20±38.88
279.97±33.62
4577.36±865.63
1406.45±447.39
5248.48±477.42
2846.64±295.47
4814.81±556.16

Model Performance
(Trajectory Splitting)
271.91±5.10
39.17±27.55
190.44±51.16
240.15±27.03
238.88±20.10
254.33±34.94
291.47±14.52
288.77±21.97
155.45±57.20
285.89±23.26
4437.55±766.02
1415.31±336.28
5148.72±476.71
2779.50±233.82
4715.59±628.54

Model Performance
(Model Ensemble)
275.03±21.12
47.20±86.05
138.19±219.58
237.64±32.00
221.57±104.60
272.50±54.24
271.86±54.77
299.45±4.43
172.28±123.79
159.33±182.92
4968.65±1337.85
1563.86±1225.73
5822.61±164.84
2680.32±1019.01
3367.15±2159.49

Model Performance
(Gauss. 0.01)
266.55±13.58
49.96±28.60
194.79±43.22
218.41±45.31
245.07±20.08
254.18±35.26
285.79±17.42
287.20±19.29
172.41±43.21
284.75±21.41
4678.37±804.33
1421.81±459.03
5232.36±536.94
2879.16±262.72
4877.90±707.65

Model Performance
(Gauss. 0.1)
265.35±14.22
46.82±29.83
181.89±38.97
245.00±20.38
231.25±25.67
264.38±34.52
285.38±14.02
281.40±23.24
163.51±55.19
268.64±40.07
3420.01±912.08
1239.03±328.98
4135.40±708.39
2338.67±263.68
3461.92±694.92

27

Half Cheetah, BC, L1 Norm, K = 5

Half Cheetah, BC, L2 Norm, K = 5

Half Cheetah, BC, Cosine, K = 5

Half Cheetah, BC, Wasserstein, K = 5

expert

medium

random

rluply

expert

medium

random

rluply

98.0

2.0

4.0

22.0

90.0

40.0

40.0

0.0

92.0

76.0

expert

medium random
Half Cheetah, BCQ, L1 Norm, K = 5

rluply

expert

medium random
Half Cheetah, BCQ, L2 Norm, K = 5

rluply

expert

medium random
Half Cheetah, BCQ, Cosine, K = 5

rluply

98.0

98.0

96.0

2.0

2.0

30.0

64.0

60.0

2.0

74.0

80.0

expert

medium random
Half Cheetah, IQL, L1 Norm, K = 5

rluply

expert

medium random
Half Cheetah, IQL, L2 Norm, K = 5

rluply

expert

medium random
Half Cheetah, IQL, Cosine, K = 5

rluply

expert

96.0

96.0

medium

random

rluply

96.0

68.0

92.0

58.0

expert

medium random
Half Cheetah, TD3PlusBC, L1 Norm, K = 5

rluply

expert

medium random
Half Cheetah, TD3PlusBC, L2 Norm, K = 5

rluply

8.0

0.0

50.0

92.0

2.0

78.0

rluply
medium random
expert
Half Cheetah, TD3PlusBC, Cosine, K = 5

98.0

expert
rluply
medium random
Half Cheetah, BCQ, Wasserstein, K = 5

50.0

expert
rluply
medium random
Half Cheetah, IQL, Wasserstein, K = 5

72.0

96.0

70.0

expert

medium random
Half Cheetah, TD3PlusBC, Wasserstein, K = 5

rluply

expert

74.0

84.0

38.0

88.0

86.0

80.0

94.0

medium

random

rluply

expert

medium random

46.0

rluply

expert

medium random

44.0

rluply

expert

medium random

2.0

2.0

34.0

2.0

68.0

80.0

rluply

54.0

4.0

98.0

expert

medium random

52.0

rluply

Fig. 18: The audit accuracy with Gaussian noise (µ = 0, σ = 0.01) on the suspect models’ action for Lunar Lander. The caption
of each plot demonstrates the offline DRL model’s type, the task, the distance metric, and the noise strength. The x labels are the
names of datasets to be audited, i.e., the target datasets. The y labels are the names of datasets the suspect models learned, i.e.,
the actual datasets. Thus, the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset,
i.e., TPR, and the non-diagonal values are the TNR results. The positions without value mean 100% accuracy.

TABLE XIX: As a supplementary of [13], we provide more details of the TD3PlusBC offline models. The model performance
shows the cumulative reward for 10 separate evaluations.

Offline
Model

Task
Name

Lunar
Lander

TD3PlusBC

Bipedal
Walker

Ant

Dataset
Name
1171
2094
4496
6518
9906
0841
1203
2110
3813
6558
2232
3569
4603
5766
7490

Model Performance
(No Defense)
263.65±21.47
99.72±47.21
201.58±42.49
242.58±21.54
235.98±25.48
-102.88±56.03
-86.65±22.94
-101.94±22.86
-114.96±14.97
154.70±148.81
259.94±116.75
549.14±192.30
374.17±199.78
396.13±130.69
314.48±222.06

Model Performance
(Trajectory Splitting)
266.03±13.80
95.78±34.82
207.92±33.77
229.34±29.64
241.78±21.68
-100.63±59.89
-87.17±22.47
-100.43±26.01
-115.04±14.37
138.26±165.87
216.71±118.76
563.13±156.03
370.58±217.99
368.56±115.13
326.59±153.02

Model Performance
(Model Ensemble)
263.34±16.97
71.96±111.26
159.76±135.46
248.09±30.91
206.93±113.68
-108.40±0.22
-95.59±17.13
-80.32±14.02
-126.18±2.36
303.03±2.30
258.28±297.07
566.88±655.52
151.13±112.93
369.73±275.57
689.21±637.77

Model Performance
(Gauss. 0.01)
267.15±12.96
100.67±34.95
207.07±28.13
238.51±21.01
230.41±34.05
-101.93±54.70
-87.64±22.35
-101.02±23.63
-113.97±12.93
165.64±136.38
243.30±121.39
579.86±213.98
372.37±194.00
334.74±172.22
365.24±212.09

Model Performance
(Gauss. 0.1)
265.12±10.36
90.74±37.66
194.69±42.59
243.96±16.41
229.55±36.81
-97.05±73.10
-86.95±25.50
-98.78±26.75
-119.21±10.67
168.47±68.50
222.48±139.45
495.19±160.81
367.64±279.00
361.40±117.68
275.81±130.30

TABLE XX: The details of the HalfCheetah dataset

Task Name

Half Cheetah

Number of Transitions
1e6
1e6
1e6
3.003e5

Dataset Name
D4RL Expert
D4RL Medium
D4RL Random
RL Unplugged

Number of Trajectories
1001
1001
1001
300

Length of trajectory
998.00 ±0.06
997.90 ±3.13
998.00±0.00
1001.00±0.00

28

Lunar Lander, BC, L1 Norm, 0.01

Lunar Lander, BC, L2 Norm, 0.01

Lunar Lander, BC, Cosine, 0.01

Lunar Lander, BC, Wasserstein, 0.01

1171

98.9

96.7

96.5

2094

4496

6518

9906

98.4

98.5

95.7

96.5

95.1

96.4

99.7

98.1

97.7

1171

2094

4496
Lunar Lander, BCQ, L1 Norm, 0.01

6518

98.1
9906

1171

2094

4496
Lunar Lander, BCQ, L2 Norm, 0.01

6518

96.1
9906

1171

2094

4496
Lunar Lander, BCQ, Cosine, 0.01

6518

96.3
9906

1171

98.4

95.6

95.3

99.8

99.5

99.8

99.9

98.0

97.1

99.2

99.9

99.9

98.8

99.9
2094

1171

4496
Lunar Lander, BCQ, Wasserstein, 0.01
99.6

6518

97.7

99.9

98.7
9906

2094

4496

6518

9906

1171

2094

4496

6518

9906

1171

2094

4496

6518

9906

95.3

99.5

93.2

96.8

93.9

96.7

97.5

95.6

95.7

99.9

99.9

96.3

99.9

99.2

99.6

99.6

95.7

1171

2094

4496
Lunar Lander, IQL, L1 Norm, 0.01

6518

98.9
9906

1171

2094

4496
Lunar Lander, IQL, L2 Norm, 0.01

6518

96.9
9906

1171

2094

4496
Lunar Lander, IQL, Cosine, 0.01

6518

96.8
9906

99.9

99.9

99.3

99.7

98.9

99.5

99.7

96.0

99.6

94.3

99.1

99.6

99.6

99.4

99.4

99.9

94.0

98.6
2094

97.6
1171
9906
4496
Lunar Lander, TD3PlusBC, L1 Norm, 0.01
97.6

99.9
6518

2094

94.3
1171
9906
4496
Lunar Lander, TD3PlusBC, L2 Norm, 0.01
92.4

6518

2094

94.3
1171
9906
4496
Lunar Lander, TD3PlusBC, Cosine, 0.01
92.1

6518

99.9
1171

99.7
2094

4496
Lunar Lander, IQL, Wasserstein, 0.01
74.7

6518

99.6

97.7

89.4

98.7
9906

98.9

98.5

99.1

99.2

99.5

98.4

98.9

99.7

95.1

95.5

99.9

96.9

98.7
1171

95.7
2094

99.9
4496
Lunar Lander, TD3PlusBC, Wasserstein, 0.01
87.1

99.9
6518

98.3
9906

98.0

99.5

98.8

93.4

99.9

98.1

94.3

98.9
1171

98.5
2094

99.7
4496

98.1

99.5

98.0
6518

98.3

99.9
9906

99.9

98.9

99.9

92.3

99.1
1171

99.9
2094

99.7
4496

99.9

98.5
9906

98.7

99.5
6518

99.9

98.8

99.5

99.1
1171

2094

92.4

99.7
4496

98.7

6518

98.9
9906

98.9

94.6

93.9
1171

99.6

97.4

99.1

93.3
2094

98.9

96.4

98.6

94.5
4496

94.5

94.1

99.1

96.7
6518

85.6

96.4

96.7

99.7
9906

Fig. 19: The audit accuracy with Gaussian noise (µ = 0, σ = 0.1) on the suspect models’ action for Lunar Lander. The caption
of each plot demonstrates the offline DRL model’s type, the task, the distance metric, and the noise strength. The x labels are the
names of datasets to be audited, i.e., the target datasets. The y labels are the names of datasets the suspect models learned, i.e.,
the actual datasets. Thus, the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset,
i.e., TPR, and the non-diagonal values are the TNR results. The positions without value mean 100% accuracy.

TABLE XXI: As a supplementary of [13], we provide more details of the models trained on the HalfCheetah dataset. The model
performance shows the cumulative reward for 10 separate evaluations.

Task Name

Offline Model

Dataset Name

Half Cheetah

BC

BCQ

IQL

TD3PlusBC

D4RL Expert
D4RL Medium
D4RL Random
RL Unplugged
D4RL Expert
D4RL Medium
D4RL Random
RL Unplugged
D4RL Expert
D4RL Medium
D4RL Random
RL Unplugged
D4RL Expert
D4RL Medium
D4RL Random
RL Unplugged

Model Performance
(No Defense)
12620.94±307.84
4223.77±134.67
-0.33±0.24
-427.50±113.42
10974.19±842.10
4765.24±98.75
-1.13±0.43
-421.91±212.36
10163.20±1106.70
4808.11±46.99
1649.55±518.47
-378.74±151.65
12712.69±383.33
4969.74±56.31
1046.23±226.61
-181.50±205.29

Model Performance
(Trajectory Splitting)
12624.61±333.32
4265.82±96.17
-0.33±0.22
-431.01±110.15
10735.35±1345.57
4746.03±108.99
-1.15±0.54
-419.59±219.29
9920.53±879.89
4800.87±59.75
1644.31±551.32
-367.87±156.81
12752.25±274.38
4964.48±57.44
1050.03±214.80
-175.49±225.09

Model Performance
(Model Ensemble)
12868.22±180.39
4293.35±75.67
-0.37±0.62
-427.06±56.30
12334.59±539.99
4512.03±99.46
-0.54±0.78
-378.28±64.55
11268.02±2640.57
4671.25±99.09
1822.31±31.63
-311.62±16.31
11468.00±872.43
4871.85±82.15
1128.32±3.15
-385.80±54.32

TABLE XXII: The TPR and TNR results on the Half Cheetah task. The mean and standard deviation of TPR and TNR in each
row represent the audit results for one combination of task and model by four distance metrics. Bold indicates the highest sum
of TPR and TNR, i.e., accuracy, in a row. Each pair of TPR and TNR is derived from the diagonal and non-diagonal values of
the corresponding heatmap in Figure 24.

Task
Name

Half
Cheetah

Offline
Model

TPR
96.07±3.15
95.37±0.55
95.47±0.77
TD3PlusBC 95.00±2.87

BC
BCQ
IQL

L1 Norm

L2 Norm

Cosine
Distance

Wasserstein
Distance

TNR
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00

TPR
96.07±2.34
95.83±1.20
95.68±1.02
95.50±1.99

TNR
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00

TPR
99.80±0.35
99.57±0.47
99.78±0.23
99.87±0.16

TNR
68.62±42.47
70.14±41.14
71.38±41.05
70.57±40.85

TPR
98.47±1.13
97.47±1.35
97.12±2.70
98.27±1.09

TNR
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00

29

1171

2094

4496

6518

9906

1171

2094

4496

6518

9906

Lunar Lander, BC, L1 Norm, 0.1

Lunar Lander, BC, L2 Norm, 0.1

Lunar Lander, BC, Cosine, 0.1

2.0

3.2

2.0

4.4

0.0

4.0

0.1

4.0

0.1

0.5

3.7

2.0

Lunar Lander, BC, Wasserstein, 0.1
99.9

71.9

99.9

85.6

99.9

99.5

57.1

63.6

1171

2094

4496
Lunar Lander, BCQ, L1 Norm, 0.1

6518

2.0
9906

1171

2094

4496
Lunar Lander, BCQ, L2 Norm, 0.1

6518

2.0
9906

1171

2094

4496
Lunar Lander, BCQ, Cosine, 0.1

6518

2.0
9906

1171

2094

4496
Lunar Lander, BCQ, Wasserstein, 0.1
99.4

6518

99.9

87.2

64.4
9906

5.9

11.7

15.3

0.4

8.5

9.9

4.8

3.7

8.5

10.0

4.9

3.3

77.2

99.9

99.6

99.9

96.1

99.7

99.8

73.9

1171

2094

4496
Lunar Lander, IQL, L1 Norm, 0.1

6518

13.3
9906

1171

2094

4496
Lunar Lander, IQL, L2 Norm, 0.1

6518

13.1
9906

1171

56.7

42.1

2094

4496

6518

9906

1171

2094

4496

6518

9906

25.7

99.9

22.8

99.7

40.5

46.1

45.1

41.9

2094

19.7
1171
9906
4496
Lunar Lander, TD3PlusBC, L1 Norm, 0.1
46.7

6518

2094

12.0
1171
9906
4496
Lunar Lander, TD3PlusBC, L2 Norm, 0.1
32.5

6518

96.3

98.1

53.9

99.3
1171

98.6
2094

99.9
4496

98.1

31.7

98.0
6518

98.3

82.5
9906

87.7

47.7

99.7
1171

2094

4496

99.9

72.0
9906

40.8

99.6
6518

1171

43.7

2094

4496
Lunar Lander, IQL, Cosine, 0.1

6518

22.4

99.8

45.6

99.9

38.3

13.1
9906

99.9

99.6

1171

2094

4496
Lunar Lander, TD3PlusBC, Cosine, 0.1

6518

11.2
9906

99.9
1171

99.7
2094

4496
Lunar Lander, IQL, Wasserstein, 0.1
73.4

6518

99.7

98.7

90.4

85.2
9906

95.7

98.5

99.3

99.5

96.7

98.9

98.7

99.7

91.2

95.5

96.5

98.3
1171

96.7
2094

4496
Lunar Lander, TD3PlusBC, Wasserstein, 0.1
87.9

99.4

96.3

94.0

99.1

99.9
6518

95.7
9906

86.0

99.8

46.0

37.7

2094

4496

6518

72.3
9906

99.0

94.7

93.9
1171

99.6

97.6

99.1

93.5
2094

98.9

95.2

98.7

94.5
4496

95.1

94.3

96.4

97.0
6518

85.1

96.5

96.7

99.6
9906

33.1

99.9

99.7
1171

Fig. 20: The audit accuracy with Gaussian noise (µ = 0, σ = 0.01) on the suspect models’ action for Bipedal Walker. The
caption of each plot demonstrates the offline DRL model’s type, the task, the distance metric, and the noise strength. The x
labels are the names of datasets to be audited, i.e., the target datasets. The y labels are the names of datasets the suspect models
learned, i.e., the actual datasets. Thus, the diagonal values show the audit accuracy when the actual dataset is the same as the
target dataset, i.e., TPR, and the non-diagonal values are the TNR results. The positions without value mean 100% accuracy.

TABLE XXIII: The TPR and TNR results of ORL-AUDITOR when splitting each trajectory into shorter ones (S = 5). The
mean and standard deviation of TPR and TNR in each row represent the audit results for one combination of task and model by
four distance metrics. Each pair of TPR and TNR is derived from the diagonal and non-diagonal values of the corresponding
heatmap in Figure 25 (Lunar Lander), Figure 26 (Bipedal Walker), Figure 27 (Ant), and Figure 28 (Half Cheetah).

Task
Name

Offline
Model

Lunar
Lander

Bipedal
Walker

Ant

Half
Cheetah

BC
BCQ
IQL
TD3PlusBC
BC
BCQ
IQL
TD3PlusBC
BC
BCQ
IQL
TD3PlusBC
BC
BCQ
IQL
TD3PlusBC

L1 Norm

L2 Norm

Cosine
Distance

Wasserstein
Distance

TPR
99.01±0.46
98.29±1.10
98.59±1.55
98.29±2.04
99.65±0.57
99.55±0.71
95.17±7.39
99.39±1.23
98.03±1.38
97.47±2.93
97.68±2.08
98.71±1.63
98.50±1.50
96.83±1.54
97.00±2.00
97.53±1.30

TNR
100.00±0.00
100.00±0.00
99.91±0.31
99.48±0.79
100.00±0.00
100.00±0.00
100.00±0.00
94.77±19.42
99.93±0.12
99.80±0.44
99.65±0.73
99.18±1.71
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00

TPR
96.96±0.73
95.87±1.12
97.52±2.51
96.35±3.01
98.45±2.71
98.19±2.84
95.01±5.49
97.15±5.71
96.77±1.49
95.89±2.32
96.77±2.50
97.20±1.79
96.87±2.14
96.27±1.36
96.25±1.13
96.53±1.20

TNR
100.00±0.00
100.00±0.00
99.97±0.12
99.89±0.22
100.00±0.00
100.00±0.00
100.00±0.00
93.37±21.46
99.90±0.36
99.84±0.41
99.69±0.59
99.35±1.72
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00

TPR
96.96±0.73
95.87±1.06
97.49±2.56
96.27±3.16
98.64±2.66
99.68±0.45
99.81±0.31
96.93±6.00
99.39±0.91
99.65±0.63
99.63±0.62
99.81±0.31
99.74±0.27
99.93±0.12
99.56±0.20
99.56±0.61

TNR
100.00±0.00
99.99±0.03
99.92±0.19
99.91±0.23
100.00±0.00
100.00±0.00
100.00±0.00
91.98±21.75
86.07±27.76
86.86±27.33
85.74±28.43
88.35±25.99
69.56±41.70
68.58±41.88
72.63±40.22
71.21±40.66

TPR
98.43±0.73
97.60±1.14
98.32±1.79
98.53±1.25
99.79±0.43
99.89±0.10
95.33±7.01
98.13±3.73
98.05±1.43
98.83±1.55
99.31±0.49
99.22±1.31
98.60±0.92
97.43±1.38
97.06±2.73
98.37±1.09

TNR
99.94±0.18
99.91±0.15
97.10±5.66
95.59±3.77
100.00±0.00
100.00±0.00
100.00±0.00
88.23±25.40
99.91±0.15
99.79±0.47
99.63±0.78
99.14±1.81
100.00±0.00
100.00±0.00
100.00±0.00
100.00±0.00

30

0841

99.9

1203

2110

3813

6558

0841

99.9

0841

1203

2110

3813

6558

1203

2110

3813

6558

0841

1203

2110

Bipedal Walker, BC, L1 Norm, 0.01

Bipedal Walker, BC, L2 Norm, 0.01

Bipedal Walker, BC, Cosine, 0.01

Bipedal Walker, BC, Wasserstein, 0.01

7.5

3.9

99.2

78.9

45.1

68.7

99.9

99.9

39.2

4.1

66.1

0841

1203

2110
Bipedal Walker, BCQ, L1 Norm, 0.01

3813

93.9
6558

0841

1203

2110
Bipedal Walker, BCQ, L2 Norm, 0.01

3813

93.2
6558

0841

1203

2110
Bipedal Walker, BCQ, Cosine, 0.01

3813

93.1
6558

1203

96.9
0841
6558
2110
Bipedal Walker, BCQ, Wasserstein, 0.01
99.9

3813

96.8

92.1

98.5

95.3

99.9

99.7

99.6

99.2

0841

1203

2110
Bipedal Walker, IQL, L1 Norm, 0.01

3813

6558

0841

1203

2110
Bipedal Walker, IQL, L2 Norm, 0.01

3813

6558

0841

1203

2110
Bipedal Walker, IQL, Cosine, 0.01

3813

98.8
6558

0841

1203

2110
Bipedal Walker, IQL, Wasserstein, 0.01

3813

6558

86.8

43.2

91.5

99.3

89.6

77.1

89.7

99.9

98.8

85.1

68.4

94.5

97.7
0841
6558
2110
Bipedal Walker, TD3PlusBC, L1 Norm, 0.01

1203

3813

10.4

98.4
0841
6558
2110
Bipedal Walker, TD3PlusBC, L2 Norm, 0.01
88.8

1203

3813

0.7

1203

0841
6558
2110
Bipedal Walker, TD3PlusBC, Cosine, 0.01
87.9

3813

99.7

0.1

0841

2110
Bipedal Walker, TD3PlusBC, Wasserstein, 0.01

1203

3813

98.4
6558

1.9

48.9

3813

96.0

98.0

94.9

6558

0841

1203

2110

3813

96.0

95.2
6558

94.0

96.0

96.5

0841

1203

2110

3813

99.6

92.0

94.0

94.4

0841

1203

2110

3813

96.9

94.0

84.3
6558

79.5

92.0

86.0
6558

92.2

94.1

98.0

94.5

43.7

0841

1203

2110

3813

92.0

89.6
6558

Fig. 21: The audit accuracy with Gaussian noise (µ = 0, σ = 0.1) on the suspect models’ action for Bipedal Walker. The caption
of each plot demonstrates the offline DRL model’s type, the task, the distance metric, and the noise strength. The x labels are the
names of datasets to be audited, i.e., the target datasets. The y labels are the names of datasets the suspect models learned, i.e.,
the actual datasets. Thus, the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset,
i.e., TPR, and the non-diagonal values are the TNR results. The positions without value mean 100% accuracy.

31

0841

1203

2110

3813

6558

0841

1203

2110

3813

6558

0841

1203

2110

3813

6558

0841

1203

2110

Bipedal Walker, BC, L1 Norm, 0.1

Bipedal Walker, BC, L2 Norm, 0.1

Bipedal Walker, BC, Cosine, 0.1

Bipedal Walker, BC, Wasserstein, 0.1

0.0

0.0

0.0

0.0

5.6

93.2

0.0

0.0

0.0

0.0

0.0

0.0

24.9

30.7

0.0

0.0

0841

1203

2110
Bipedal Walker, BCQ, L1 Norm, 0.1

3813

0.0
6558

0841

1203

2110
Bipedal Walker, BCQ, L2 Norm, 0.1

3813

19.1
6558

0841

1203

2110
Bipedal Walker, BCQ, Cosine, 0.1

3813

35.5
6558

0.0

0.0

1.1

0.0

8.8

93.9

0.0

0.0

0.0

0.0

17.2

28.1

0841

1203

2110
Bipedal Walker, IQL, L1 Norm, 0.1

3813

0.0
6558

0841

1203

2110
Bipedal Walker, IQL, L2 Norm, 0.1

3813

39.6
6558

0841

1203

2110
Bipedal Walker, IQL, Cosine, 0.1

3813

38.0
6558

0.0

0.0

0.0

0.0

7.1

92.1

0.0

0.0

0.0

0.0

19.9

29.3

0.0
0841
6558
2110
Bipedal Walker, TD3PlusBC, L1 Norm, 0.1

1203

3813

11.6

1203

0.0
0841
6558
2110
Bipedal Walker, TD3PlusBC, L2 Norm, 0.1
99.9
91.1

3813

0.7

1203

19.3
0841
6558
2110
Bipedal Walker, TD3PlusBC, Cosine, 0.1
99.5
90.3

3813

99.9

0.1

0841

1203

2110
Bipedal Walker, BCQ, Wasserstein, 0.1
2.5

3813

0.0
6558

0.4

0.0

0.0

0841

1203

2110
Bipedal Walker, IQL, Wasserstein, 0.1
0.0

3813

0.3
6558

0.0

0.0

0.0

0841

2110
Bipedal Walker, TD3PlusBC, Wasserstein, 0.1

3813

1203

0.0
6558

2.3

50.9

3813

96.0

98.0

94.9

6558

0841

1203

2110

3813

96.0

0.0
6558

91.1

91.9

94.1

96.0

96.8

99.7

98.4

0841

1203

2110

3813

97.8

94.0

3.7
6558

92.0

94.0

95.0

99.8

97.6

0841

1203

2110

3813

83.7

92.0

13.5
6558

92.3

94.4

98.0

94.4

45.4

0841

1203

2110

3813

92.0

7.7
6558

Fig. 22: The audit accuracy with Gaussian noise (µ = 0, σ = 0.01) on the suspect models’ action for Ant. The caption of each
plot demonstrates the offline DRL model’s type, the task, the distance metric, and the noise strength. The x labels are the names
of datasets to be audited, i.e., the target datasets. The y labels are the names of datasets the suspect models learned, i.e., the
actual datasets. Thus, the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset, i.e.,
TPR, and the non-diagonal values are the TNR results. The positions without value mean 100% accuracy.

32

Ant, BC, L1 Norm, 0.01
99.9

97.5

99.6

96.9

99.8

84.4

5766
4603
3569
Ant, BCQ, L1 Norm, 0.01

99.9

97.2

98.7

99.9

98.7

99.9

87.1

3569
5766
4603
Ant, IQL, L1 Norm, 0.01
99.9

99.3

97.5

93.2

97.9

99.9

99.7

99.9

96.1
7490

99.9

99.6
7490

99.6

99.9

95.1

99.9

2232

93.5

99.1

2232

91.3

98.9

99.9

Ant, BC, L2 Norm, 0.01
98.3

95.2

99.9

98.8

89.7

5766
4603
3569
Ant, BCQ, L2 Norm, 0.01

98.3

94.0

99.5

97.9

88.7

3569
5766
4603
Ant, IQL, L2 Norm, 0.01
98.7

97.5

98.2

96.1

98.5

99.9

96.7

99.9

96.7
7490

99.9

99.5

99.9

8.8
2232

99.9

99.5

Ant, BC, Cosine, 0.01
97.9
57.0

97.1

99.9

11.7

99.9

94.3

99.9

57.3
99.7
88.2
5766
4603
3569
Ant, BCQ, Cosine, 0.01
98.2
62.9

98.3

99.7

99.9

94.3

99.7

Ant, BC, Wasserstein, 0.01

99.9

97.7

99.2

99.7

99.9

99.5

99.3

99.8

99.8

97.1

7490

2232

3569

4603
Ant, BCQ, Wasserstein, 0.01

5766

15.4

99.9

96.9

98.9

98.6

99.9

99.9

98.6

99.9

96.3

98.9
7490

10.1
2232

65.7
92.4
3569
5766
4603
Ant, IQL, Cosine, 0.01
73.5
98.7

99.9
7490

12.3

99.8

99.5

98.4

99.2

99.7

92.9

99.9

2232

99.7

98.5

99.8

3569

4603
Ant, IQL, Wasserstein, 0.01

5766

99.9

98.9

97.5

98.7

97.6

99.9

99.9

96.5
7490

99.9

7490

99.6

99.9

94.5

99.7

99.9

2232

95.5

98.9

2232

93.1

98.5

99.8

2232

3569

4603

5766

7490

2232

3569

4603

5766

7490

2232

3569

4603

5766

7490

2232

3569

4603
Ant, TD3PlusBC, L1 Norm, 0.01

5766

94.9
7490

2232

3569

4603
Ant, TD3PlusBC, L2 Norm, 0.01

5766

98.7
7490

2232

3569

99.1

4603

99.8

95.2

5766

97.5

99.6

7490

98.3

93.6

92.5

98.0

98.8

98.0

99.5

95.9

98.2

99.7

99.0

92.9

92.1

99.0

99.5

2232

3569

4603

5766

99.5
7490

2232

3569

4603

5766

95.7
7490

8.1
2232

99.9

99.6

98.2

7.0
2232

90.9
3569

99.5
4603
Ant, TD3PlusBC, Cosine, 0.01

51.3
5766

99.5

99.7

92.1
3569

99.2

99.9

92.2

4603

75.4

99.1

91.9

87.3
5766

7490

14.7

99.5

2232

3569

4603
Ant, TD3PlusBC, Wasserstein, 0.01

5766

98.1
7490

99.8

99.1

99.1

97.5

99.5

98.3

96.6

92.0

97.8

98.8

7490

2232

3569

4603

5766

7490

Fig. 23: The audit accuracy with Gaussian noise (µ = 0, σ = 0.1) on the suspect models’ action for Ant. The caption of each
plot demonstrates the offline DRL model’s type, the task, the distance metric, and the noise strength. The x labels are the names
of datasets to be audited, i.e., the target datasets. The y labels are the names of datasets the suspect models learned, i.e., the
actual datasets. Thus, the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset, i.e.,
TPR, and the non-diagonal values are the TNR results. The positions without value mean 100% accuracy.

33

8.7

2232

3569

4603

99.9

5766

7490

2232

3569

4603

5766

7490

2232

3569

4603

5766

7490

2232

10.0

98.9

2232

9.7

98.9

99.9

2232

Ant, BC, L1 Norm, 0.1
99.9

13.6

99.9

2.0

2.3

5766
4603
3569
Ant, BCQ, L1 Norm, 0.1
99.9

24.4

99.0

2.0

99.3

99.9

9.5

3569
5766
4603
Ant, IQL, L1 Norm, 0.1
99.9

25.3

98.5

2.0

98.1

99.9

8.4

3569

4603
Ant, TD3PlusBC, L1 Norm, 0.1

5766

3.6
7490

99.9

99.9

3.9
7490

99.9

3.2
7490

Ant, BC, Cosine, 0.1

Ant, BC, Wasserstein, 0.1

59.7

13.3

12.9

Ant, BC, L2 Norm, 0.1
98.7

22.5

6.0

2.3

5766
4603
3569
Ant, BCQ, L2 Norm, 0.1
98.3

27.6

99.9

5.6

5.1

3569
5766
4603
Ant, IQL, L2 Norm, 0.1
98.7

40.0

99.3

5.6

98.9

4.5

3569

4603
Ant, TD3PlusBC, L2 Norm, 0.1

5766

4.4
7490

3.3
7490

8.8

99.9

2232

9.9

99.6

2232

9.2

99.9

2232

8.5

99.6

98.7

98.1

74.8

99.1

99.9

95.1

84.7

2.7
7490

10.8
2232

99.9

99.9

62.2
99.8
88.9
5766
4603
3569
Ant, BCQ, Cosine, 0.1
98.2
68.7

99.9
7490

16.7

77.5

99.9

96.5

84.4

10.8
2232

99.9

9.1
2232

99.6

98.8

7.7
2232

94.1
3569

69.3
5766

99.9
4603
Ant, IQL, Cosine, 0.1
78.1
98.9

81.6

99.9

95.7

84.8

91.8
3569

99.6
4603
Ant, TD3PlusBC, Cosine, 0.1

54.9
5766

99.9

80.8

93.5
3569

99.8

99.4

93.7

99.9
4603

78.5

99.4

94.3

99.1

91.3
5766

99.7
7490

13.1

99.7
7490

17.3

99.7
7490

99.9

29.9

99.9

5.6

3.7

3569

4603
Ant, BCQ, Wasserstein, 0.1

5766

99.9

42.5

98.9

7.3

99.3

99.9

14.1

3569

4603
Ant, IQL, Wasserstein, 0.1

5766

99.9

49.1

98.4

4.0

98.1

99.9

10.1

2.9
7490

99.9

99.9

43.1
7490

99.9

99.9

2232

62.3

98.9

2232

12.5

98.9

99.9

2232

3569

4603
Ant, TD3PlusBC, Wasserstein, 0.1

5766

25.7
7490

93.1

99.4

51.3

97.9

99.7

98.8

3.4

93.0

97.9

99.3

2232

3569

4603

5766

65.9
7490

2232

8.3

3569

99.4

27.9

4603

5766

98.1

99.7

7490

98.8

2.0

93.4

98.1

99.3

99.8

59.7

99.5

5.4

93.4

99.4

99.9

94.5

2232

3569

4603

5766

2.3
7490

2232

3569

4603

5766

2.0
7490

Fig. 24: The audit accuracy between every two Half Cheetah datasets. The caption of each plot demonstrates the offline DRL
model’s type, the task, and the distance metric. The x labels are the names of datasets to be audited, i.e., the target datasets. The
y labels are the names of datasets the suspect models learned, i.e., the actual datasets. Thus, the diagonal values show the audit
accuracy when the actual dataset is the same as the target dataset, i.e., TPR, and the non-diagonal values are the TNR results.
The positions without value mean 100% accuracy.

34

Half Cheetah, BC, L1 Norm

Half Cheetah, BC, L2 Norm

Half Cheetah, BC, Cosine Distance

Half Cheetah, BC, Wasserstein Distance

expert

91.2

94.0

medium

random

rluply

96.7

94.7

90.0

96.4

95.6

5.4

3.1

26.9

1.6

96.3

99.2

expert

medium random
Half Cheetah, BCQ, L1 Norm

rluply

expert

medium random
Half Cheetah, BCQ, L2 Norm

rluply

expert

94.8

96.0

medium

random

rluply

95.1

96.3

97.7

94.9

expert

medium random
Half Cheetah, IQL, L1 Norm

95.3

rluply

expert

medium random
Half Cheetah, IQL, L2 Norm

94.7

rluply

expert

95.5

95.2

96.7

94.5

97.3

95.6

95.2

94.6

99.7

expert

medium random
Half Cheetah, BCQ, Cosine Distance

rluply

11.1

5.3

30.2

99.9

98.8

95.2

99.9

3.9

96.1

99.6

expert

medium random
Half Cheetah, IQL, Cosine Distance

rluply

5.9

99.9

92.1

2.9

55.5

99.9

1.4

99.0

99.4

99.1

97.6

medium random
expert
Half Cheetah, BCQ, Wasserstein Distance

rluply

97.2

99.7

96.7

97.2

96.3

expert
rluply
medium random
Half Cheetah, IQL, Wasserstein Distance

99.5

92.5

98.5

98.0

expert

medium random
Half Cheetah, TD3PlusBC, L1 Norm

rluply

expert

medium random
Half Cheetah, TD3PlusBC, L2 Norm

rluply

expert

medium random
Half Cheetah, TD3PlusBC, Cosine Distance

rluply

expert

medium random
Half Cheetah, TD3PlusBC, Wasserstein Distance

rluply

expert

90.3

92.8

95.6

96.1

94.8

96.1

6.9

99.9

4.8

45.1

89.5

expert

medium random

98.0

rluply

expert

medium random

98.3

rluply

expert

medium random

2.2

98.3

99.6

rluply

98.9

96.4

99.1

expert

medium random

98.7

rluply

medium

random

rluply

medium

random

rluply

Fig. 25: The audit accuracy of ORL-AUDITOR on Lunar Lander when splitting each trajectory into shorter ones (S = 5). The
caption of each plot demonstrates the offline DRL model’s type, the task, the distance metric, and the hyperparameter S of
trajectory splitting. The x labels are the names of datasets to be audited, i.e., the target datasets. The y labels are the names of
datasets the suspect models learned, i.e., the actual datasets. Thus, the diagonal values show the audit accuracy when the actual
dataset is the same as the target dataset, i.e., TPR, and the non-diagonal values are the TNR results. The positions without value
mean 100% accuracy.

35

Lunar Lander, BC, L1 Norm, split 5

Lunar Lander, BC, L2 Norm, split 5

Lunar Lander, BC, Cosine, split 5

Lunar Lander, BC, Wasserstein, split 5

96.9

95.9

96.8

95.9

97.2

98.1

97.2

98.1

99.5

99.8

99.9

98.3

99.9

99.2

97.2

98.7

1171

99.1

2094

4496

6518

9906

98.5

98.8

99.9

1171

2094

4496
Lunar Lander, BCQ, L1 Norm, split 5

6518

98.8
9906

1171

2094

4496
Lunar Lander, BCQ, L2 Norm, split 5

6518

96.7
9906

1171

2094

4496
Lunar Lander, BCQ, Cosine, split 5

6518

96.8
9906

1171

98.5

96.4

99.5

97.9

95.5

94.1

95.2

94.4

99.9

96.8

95.6

96.9

95.6

1171

2094

4496
Lunar Lander, IQL, L1 Norm, split 5

6518

99.2
9906

1171

2094

4496
Lunar Lander, IQL, L2 Norm, split 5

6518

97.3
9906

1171

2094

4496
Lunar Lander, IQL, Cosine, split 5

6518

97.2
9906

99.5

99.7

99.1

99.5

99.9

96.0

99.6

94.4

99.1

99.6

99.6

99.4

99.4

99.9

94.0

98.6
2094

97.6
1171
9906
4496
Lunar Lander, TD3PlusBC, L1 Norm, split 5
97.7

99.9
6518

94.5
1171
9906
4496
Lunar Lander, TD3PlusBC, L2 Norm, split 5
92.8

2094

6518

2094

94.8
1171
9906
4496
Lunar Lander, TD3PlusBC, Cosine, split 5
92.4

6518

99.9
2094

98.5
9906
4496
1171
Lunar Lander, BCQ, Wasserstein, split 5
97.7
99.9
99.6

6518

96.5

99.9

99.9

99.9

99.2

99.5

99.6

96.1

99.8
2094

1171

4496
Lunar Lander, IQL, Wasserstein, split 5
74.8

6518

99.6

97.6

89.4

98.4
9906

98.9

98.5

99.1

99.2

99.5

98.4

98.9

99.7

94.9

95.5

99.9

96.9

98.9
1171

95.7
2094

99.9
4496
Lunar Lander, TD3PlusBC, Wasserstein, split 5
87.1

99.9
6518

98.3
9906

99.5

97.9

98.7

93.4

99.9

98.1

94.5

98.9
1171

98.5
2094

99.7
4496

98.1

99.5

98.0
6518

98.3

99.9
9906

99.9

98.9

99.9

92.5

99.1
1171

99.9
2094

99.7
4496

99.9

98.7
9906

98.8

99.5
6518

99.9

98.8

99.5

99.1
1171

2094

92.4

99.7
4496

98.7

6518

99.1
9906

98.9

94.7

93.9
1171

99.6

97.3

99.1

93.3
2094

98.9

96.4

98.6

94.5
4496

94.4

94.1

99.1

96.7
6518

85.6

96.5

96.7

99.7
9906

2094

4496

6518

9906

1171

2094

4496

6518

9906

1171

2094

4496

6518

9906

Fig. 26: The audit accuracy of ORL-AUDITOR on Bipedal Walker when splitting each trajectory into shorter ones (S = 5).
The caption of each plot demonstrates the offline DRL model’s type, the task, the distance metric, and the hyperparameter S of
trajectory splitting. The x labels are the names of datasets to be audited, i.e., the target datasets. The y labels are the names of
datasets the suspect models learned, i.e., the actual datasets. Thus, the diagonal values show the audit accuracy when the actual
dataset is the same as the target dataset, i.e., TPR, and the non-diagonal values are the TNR results. The positions without value
mean 100% accuracy.

36

0841

1203

2110

3813

6558

1203

2110

3813

6558

0841

1203

2110

3813

6558

0841

1203

2110

Bipedal Walker, BC, L1 Norm, split 5

Bipedal Walker, BC, L2 Norm, split 5

Bipedal Walker, BC, Cosine, split 5

Bipedal Walker, BC, Wasserstein, split 5

98.5

98.9

99.2

99.9

0841

1203

2110
Bipedal Walker, BCQ, L1 Norm, split 5

3813

99.7
6558

0841

1203

2110
Bipedal Walker, BCQ, L2 Norm, split 5

3813

93.1
6558

0841

1203

2110
Bipedal Walker, BCQ, Cosine, split 5

3813

93.3
6558

0841

99.9

1203

6558
2110
0841
Bipedal Walker, BCQ, Wasserstein, split 5
99.9

3813

98.1

92.7

99.7

98.3

99.7

99.9

99.7

99.9

0841

1203

2110
Bipedal Walker, IQL, L1 Norm, split 5

3813

6558

0841

1203

2110
Bipedal Walker, IQL, L2 Norm, split 5

3813

6558

0841

1203

2110
Bipedal Walker, IQL, Cosine, split 5

3813

98.8
6558

1203

0841
6558
2110
Bipedal Walker, IQL, Wasserstein, split 5

3813

99.7

80.8

93.7

85.1

99.9

97.6

95.5

96.4

99.2

81.5

97.6

0841

2110
Bipedal Walker, TD3PlusBC, L1 Norm, split 5

3813

1203

99.9
6558

10.4

1203

0841

2110
Bipedal Walker, TD3PlusBC, L2 Norm, split 5
89.3

3813

0.7

99.9
6558

1203

0841
6558
2110
Bipedal Walker, TD3PlusBC, Cosine, split 5
87.9

3813

99.7

0.1

0841

2110
Bipedal Walker, TD3PlusBC, Wasserstein, split 5

3813

6558

1203

1.9

48.0

3813

96.0

98.0

94.9

6558

0841

1203

2110

3813

96.0

96.9
6558

94.0

96.0

96.4

0841

1203

2110

3813

99.7

92.0

94.0

94.4

0841

1203

2110

3813

97.1

94.0

85.7
6558

79.6

92.0

84.9
6558

92.1

94.2

98.0

94.5

43.9

0841

1203

2110

3813

92.0

90.7
6558

Fig. 27: The audit accuracy of ORL-AUDITOR on Ant when splitting each trajectory into shorter ones (S = 5). The caption
of each plot demonstrates the offline DRL model’s type, the task, the distance metric, and the hyperparameter S of trajectory
splitting. The x labels are the names of datasets to be audited, i.e., the target datasets. The y labels are the names of datasets
the suspect models learned, i.e., the actual datasets. Thus, the diagonal values show the audit accuracy when the actual dataset
is the same as the target dataset, i.e., TPR, and the non-diagonal values are the TNR results. The positions without value mean
100% accuracy.

37

Ant, BC, Cosine, split 5
98.0
57.6

97.6

99.9

12.0

99.9

Ant, BC, Wasserstein, split 5

99.9

98.1

99.5

99.7

99.9

99.5

99.5

99.7

99.8

97.6

Ant, BC, L1 Norm, split 5
99.9

98.0

99.6

99.9

99.9

99.8

95.6

3569

4603
Ant, BCQ, L1 Norm, split 5

5766

99.9

97.7

98.7

98.7

99.9

92.0

99.9

98.4
7490

96.8

99.9

2232

94.4

99.1

Ant, BC, L2 Norm, split 5
98.3

96.0

99.9

99.2

94.7

3569

4603
Ant, BCQ, L2 Norm, split 5

5766

98.3

95.2

99.5

98.0

99.9

92.8

3569
5766
4603
Ant, IQL, L1 Norm, split 5

7490

2232

3569
5766
4603
Ant, IQL, L2 Norm, split 5

99.9

99.6

97.5

98.1

97.9

99.9

99.6

99.9

98.7

97.3

92.0

98.9

99.9

98.1

96.9

98.5

99.9

98.7

98.3

99.7

99.9

2232

97.6

98.9

2232

94.5

98.5

99.8

2232

3569

4603

5766

7490

2232

3569

4603

5766

7490

2232

3569

4603

5766

7490

2232

3569

4603
Ant, TD3PlusBC, L1 Norm, split 5

5766

96.1
7490

2232

3569

4603
Ant, TD3PlusBC, L2 Norm, split 5

5766

98.9
7490

2232

3569

99.1

99.8

96.1

4603

5766

97.5

99.6

7490

98.3

97.4

92.5

98.0

98.8

98.3

99.5

96.1

98.2

99.7

99.0

94.8

92.2

99.0

99.5

99.9

97.2
7490

99.9

99.6

99.9

11.0
2232

99.9

99.5

99.1
7490

8.9
2232

95.9

99.7

59.1
99.6
88.5
5766
4603
3569
Ant, BCQ, Cosine, split 5
98.1
64.9

98.4

99.5

95.2

65.1
93.0
3569
5766
4603
Ant, IQL, Cosine, split 5
67.0
99.9
98.7

11.6

99.9
7490

15.5

99.5

99.8

99.3

98.4

99.1

99.7

92.3

99.9

8.1
2232

99.9

99.6

98.2

7.9
2232

91.8
3569

99.7
4603
Ant, TD3PlusBC, Cosine, split 5

47.5
5766

99.2

99.2

99.7

92.7
3569

92.2

4603

82.6

99.1

89.2

91.7
5766

96.8
7490

99.9

95.6
7490

99.9

7490

99.6

99.9

7490

2232

3569

4603
Ant, BCQ, Wasserstein, split 5

5766

13.1

99.9

98.0

98.9

98.5

99.9

98.6

99.9

96.1

7490

2232

3569

4603
Ant, IQL, Wasserstein, split 5

5766

99.9

98.9

99.7

98.5

99.8

97.5

99.2

97.6

99.9

2232

3569

4603
Ant, TD3PlusBC, Wasserstein, split 5

5766

98.7
7490

99.8

99.5

99.1

97.5

99.5

98.3

96.6

92.0

97.8

98.7

2232

3569

4603

5766

7490

2232

3569

4603

5766

7490

2232

3569

4603

5766

7490

Fig. 28: The audit accuracy of ORL-AUDITOR on Half Cheetah when splitting each trajectory into shorter ones (S = 5). The
caption of each plot demonstrates the offline DRL model’s type, the task, the distance metric, and the hyperparameter S of
trajectory splitting. The x labels are the names of datasets to be audited, i.e., the target datasets. The y labels are the names of
datasets the suspect models learned, i.e., the actual datasets. Thus, the diagonal values show the audit accuracy when the actual
dataset is the same as the target dataset, i.e., TPR, and the non-diagonal values are the TNR results. The positions without value
mean 100% accuracy.

38

Half Cheetah, BC, L1 Norm, split 5

Half Cheetah, BC, L2 Norm, split 5

Half Cheetah, BC, Cosine, split 5

Half Cheetah, BC, Wasserstein, split 5

expert

99.7

medium

random

rluply

98.0

97.6

94.4

99.5

91.7

7.7

3.8

30.6

96.3

95.5

99.8

3.5

97.7

99.5

expert

medium random
Half Cheetah, BCQ, L1 Norm, split 5

rluply

expert

medium random
Half Cheetah, BCQ, L2 Norm, split 5

rluply

expert

medium random
Half Cheetah, BCQ, Cosine, split 5

rluply

expert

99.2

97.3

medium

random

rluply

95.3

97.9

90.5

97.2

95.6

95.2

94.7

99.9

7.6

2.7

26.9

3.1

92.3

99.7

expert

medium random
Half Cheetah, IQL, L1 Norm, split 5

rluply

expert

medium random
Half Cheetah, IQL, L2 Norm, split 5

rluply

expert

medium random
Half Cheetah, IQL, Cosine, split 5

rluply

expert

99.9

medium

random

rluply

97.9

96.9

97.6

99.3

94.2

7.9

99.9

94.9

95.3

95.9

94.6

5.3

61.9

99.6

2.7

99.6

99.5

98.7

98.3

rluply
medium random
expert
Half Cheetah, BCQ, Wasserstein, split 5

97.5

99.7

96.7

97.2

96.1

expert
rluply
medium random
Half Cheetah, IQL, Wasserstein, split 5

99.3

92.4

98.5

98.0

expert

medium random
Half Cheetah, TD3PlusBC, L1 Norm, split 5

rluply

expert

medium random
Half Cheetah, TD3PlusBC, L2 Norm, split 5

rluply

expert
rluply
medium random
Half Cheetah, TD3PlusBC, Cosine, split 5

expert

medium random
Half Cheetah, TD3PlusBC, Wasserstein, split 5

rluply

expert

99.1

96.9

95.6

97.2

94.8

96.3

medium

random

rluply

expert

medium random

98.3

rluply

expert

medium random

98.1

rluply

expert

medium random

99.7

90.9

6.9

4.7

52.3

1.6

98.1

98.5

rluply

98.9

96.5

99.3

expert

medium random

98.7

rluply

","ORL-AUDITOR : Dataset Auditing in Offline Deep Reinforcement Learning 3 2 0 2 p e S 6 ] R C . s c [ 1 v 1 8 0 3 0 . 9 0 3 2 : v i X r a Linkang Du†∗ , Min Chen‡∗ , Mingyang Sun† , Shouling Ji† , Peng Cheng† , Jiming Chen† , Zhikun Zhang†‡§¶ † Zhejiang University , Hangzhou 310017 , China Email : linkangd @ gmail.com , sji @ zju.edu.cn , saodiseng @ gmail.com , cjm @ zju.edu.cn ‡CISPA Helmholtz Center for Information Security , Saarbr¨ucken 66123 , Germany Email : min.chen @ cispa.de §Stanford University , Stanford , California 94305 , USA Email : zhikun @ stanford.edu Abstract—Data is a critical asset in AI , as high-quality datasets can significantly improve the performance of machine learning models . In safety-critical domains such as autonomous vehicles , offline deep reinforcement learning ( offline DRL ) is frequently used to train models on pre-collected datasets , as opposed to train- ing these models by interacting with the real-world environment as the online DRL . To support the development of these models , many institutions make datasets publicly available with open- source licenses , but these datasets are at risk of potential misuse or infringement . Injecting watermarks to the dataset may protect the intellectual property of the data , but it can not handle datasets that have already been published and is infeasible to be altered afterward . Other existing solutions , such as dataset inference and membership inference , do not work well in the offline DRL scenario due to the diverse model behavior characteristics and offline setting constraints . In this paper , we advocate a new paradigm by leveraging the fact that cumulative rewards can act as a unique identifier that distinguishes DRL models trained on a specific dataset . To this end , we propose ORL-AUDITOR , which is the first trajectory- level dataset auditing mechanism for offline RL scenarios . Our experiments on multiple offline DRL models and tasks reveal the efficacy of ORL-AUDITOR , with auditing accuracy over 95 % and false positive rates less than 2.88 % . We also provide valuable insights into the practical implementation of ORL- AUDITOR by studying various parameter settings . Furthermore , we demonstrate the auditing capability of ORL-AUDITOR on open-source datasets from Google and DeepMind , highlighting its effectiveness in auditing published datasets . ORL-AUDITOR is open-sourced at https : //github.com/link-zju/ORL-Auditor . I . INTRODUCTION Deep reinforcement learning ( DRL ) has been successfully applied to many complex decision-making tasks , such as autopilot [ 17 ] , robot control [ 3 ] , [ 51 ] , power systems [ 70 ] , intrusions detection [ 42 ] , [ 67 ] . However , for safety-critical domains , such as robot control , directly interacting with the environment is unsafe since the partially trained policy may ∗The first two authors made equal contribution . ¶Zhikun Zhang is the corresponding author . Network and Distributed System Security ( NDSS ) Symposium 2024 26 February - 1 March 2024 , San Diego , CA , USA ISBN 1-891562-93-2 https : www.ndss-symposium.org risk damage to robot hardware or surrounding objects [ 55 ] . To address this issue , researchers propose the offline deep reinforcement learning ( Offline DRL ) [ 37 ] paradigm , also known as full batch DRL [ 36 ] . The general idea is learning from pre-collected data generated by the expert , handcrafted controller , or even random strategy respecting the system ’ s constraints . [ 4 ] , Berkeley Artificial To facilitate the research of offline DRL , several high- quality datasets are published by third parties such as Deep- Mind [ 26 ] , Intelligence Research ( BAIR ) [ 18 ] , Polixir Technologies [ 52 ] , TensorFlow [ 1 ] , and Max Planck Institute [ 27 ] . These datasets are published with strict open-source licenses , such as GNU General Public License [ 4 ] , Apache License [ 26 ] , [ 18 ] , [ 1 ] , [ 52 ] , and BSD 3- Clause License [ 27 ] , to protect the intellectual property ( IP ) of the data owner . The licenses typically encompass two essential terms . 1 ) Attribution requires you ( the users ) to appropriately acknowledge the source , provide a link to the license , and indicate any modifications made . 2 ) ShareAlike stipulates that if you remix , transform , or build upon the material , you must distribute your contributions under the same license as the original . Furthermore , some datasets are accompanied by additional patent grants aimed at safeguarding the rights of data publishers , e.g . StarData [ 40 ] . Additionally , closed-form datasets have the potential to face misuse from insider attacks or intellectual property infringement ( e.g. , ex-employees steal- ing data ) . Biscom ’ s 2021 survey finds that 25 % of respondents admitted to taking the valuable data when leaving their job , with 95 % citing a lack of policies or technologies to prevent data theft [ 5 ] . Tessian reports that 40 % of US employees take their generated data or trained models when leaving their job [ 61 ] . The defense against the above threats comes to the question of how a data owner can prove that a suspect model was derived from its dataset . Existing Solutions . Recent mainstream solutions for dataset copyright protection can be classified into three categories : Watermarking , dataset inference , and membership inference . The watermarking approach aims to inject samples from a specific distribution prior to publishing the dataset [ 39 ] , the auditor needs a post-event mechanism [ 38 ] . However , for open-source data since they are already published in the real world . In contrast to watermarking techniques , dataset inference strategies [ 43 ] , [ 16 ] do not require the injection of explicit watermarks [ 6 ] into the datasets or trained models . To implement the auditing , we first train a critic model to predict the cumulative rewards of the state-action pairs in the dataset to be audited , i.e. , the target dataset . A straightfor- ward strategy to derive the auditing result is to compare the cumulative reward of the state-action pairs from the suspect model to that of the target dataset through a preset judgment threshold of the similarity . However , designing the threshold value is challenging , as it depends on the distributions of pre- collected datasets , which can vary due to different task settings , collection procedures , and data post-processing methods . To address this issue , we recognize that the cumulative rewards embedded in the state-action pairs of the models are the esti- mated cumulative rewards of the target dataset , as the offline DRL models fit the cumulative reward of the dataset during training . Thus , we train multiple models on the target dataset with varying initializations and optimization , i.e. , the shadow models , and collect the cumulative rewards of their state-action pairs . Finally , by comparing the cumulative rewards from the suspect model and the shadow models , we make the audit decision through hypothesis testing . Evaluation . The experimental results show that the auditing accuracy of ORL-AUDITOR exceeds 95 % with false positive rates less than 2.88 % across multiple DRL models and tasks . By visualizing the cumulative rewards from the shadow models trained on different datasets , we demonstrate that the cumula- tive reward is a distinguishable feature for the dataset audit . We further evaluate three influential factors for the practical adop- tion of ORL-AUDITOR , i.e. , the number of shadow models , the significance level in hypothesis testing , and the trajectory size . First , more shadow models improve the audit accuracy , and ORL-AUDITOR demonstrates exceptional performance with an audit accuracy exceeding 90 % , utilizing a mere 9 shadow models as illustrated in Table VIII . Second , the minimum sig- nificance level α of ORL-AUDITOR is about 0.001 , meaning that the auditor outputs a single result with 99.9 % confidence . Third , ORL-AUDITOR tends to obtain higher accuracy with a larger trajectory size , yet we also notice that a small trajectory size achieves better results under some tasks [ 46 ] . We further implement ORL-AUDITOR to audit the open-source datasets from Google [ 18 ] and DeepMind [ 26 ] , and the experimental results again demonstrate the effectiveness of ORL-AUDITOR in practice . Robustness . To evaluate the robustness of ORL-AUDITOR , we have implemented two defense strategies to prevent the auditing . The first strategy involves using state-of-the-art mem- bership inference defense techniques , such as the ensemble architecture proposed by Tang et al . [ 60 ] and Jarin et al . [ 31 ] . Despite these defense mechanisms , the audit accuracy of ORL-AUDITOR is still over 85 % . In addition to the ensem- ble architecture , the suspect models may distort actions to hide their training dataset . The offline DRL models for real- world decision-making tasks ( i.e. , self-driving cars ) often use Gaussian noise to model natural distortions [ 2 ] . Thus , adding Gaussian noise to the actions is stealthy to avoid the auditor ’ s detection , and Gaussian noise is convenient for mathematical manipulation . To simulate strong and weak action distortion , we normalize all dimensions of the action space to [ −1 , 1 ] and use Gaussian noise with ( µ = 0 , σ = 0.1 ) and ( µ = 0 , σ = 0.01 ) , respectively . Our experiments show that ORL- AUDITOR is only slightly affected by Gaussian noise with Fig . 1 : Intuitive explanation of ORL-AUDITOR . The middle surface is the cumulative rewards of the state-action pairs from a dataset . The auditor outputs a positive result if the cumulative rewards of a suspect model ’ s state-action pairs are between the two outer surfaces . Maini et al . [ 43 ] and Dziedzic et al . [ 16 ] have separately proposed dataset inference methods for supervised learning and self-supervised learning models , enabling the model owner to provide a convincing statistical argument that a particular model is trained on their private data . However , the dataset inference with labels [ 43 ] needs distances between data and decision boundaries , which is not possible to obtain in RL with continuous outputs . The dataset inference without labels [ 16 ] uses the similarity of model behaviors to detect unauthorized dataset usage . It requires a public dataset to generate some surrogate models , and forms the auditing basis by comparing the behavioral difference between the surrogate models and the models trained on their private data . In offline RL scenes , since the distributions of the collected datasets depend on both environment and operator [ 18 ] , it is difficult to determine a suitable public dataset to train the surrogate model , making the audit basis hard to establish . The third category adopts the notion of membership inference [ 47 ] , [ 24 ] , [ 23 ] . By collecting the RL models ’ behaviors on the trained examples ( members ) and the untrained examples ( non-members ) , a classifier is constructed to determine whether a data sample is used in the model ’ s learning process . However , unlike online scenarios in [ 47 ] , [ 24 ] , [ 23 ] , the auditor can not collect additional data from the environment as the non-member examples in offline cases , where the auditor does not have access to the environment . Our Proposal . In this paper , we propose the first practical dataset auditing paradigm for the offline RL model ( ORL- AUDITOR ) . Concretely , we are inspired by the fact that the cumulative reward , i.e. , the sum of all rewards received over a period of time starting from a given state-action pair , guiding the RL model to learn the behavior policy . Thus , the cumu- lative reward is an intrinsic feature of the datasets , making it suitable as an audit basis . Figure 1 provides a schematic diagram of ORL-AUDITOR , where the state , the action , and the cumulative reward compose a three-dimensional space . The middle surface illustrates the exact cumulative reward of the dataset , and the other two surfaces show possible offsets of the exact cumulative reward learned by the offline DRL models due to the randomness in the initialization and the learning processes . For a suspect model , the auditor outputs a positive result , i.e. , the data is used to train this model , if the cumulative reward from its state-action pair falls between the two surfaces ; otherwise , a negative outcome . 2 Cumulative Reward 𝑄𝑄 𝑠𝑠 , 𝑎𝑎 + ∆ 𝑄𝑄 ( 𝑠𝑠 , 𝑎𝑎 ) 𝑄𝑄 𝑠𝑠 , 𝑎𝑎 − ∆ State : 𝑠𝑠 Action : 𝑎𝑎 Positive Negative Share Help/Contact ( µ = 0 , σ = 0.01 ) . For σ = 0.1 , the TPR values of ORL- AUDITOR decline , yet the strong distortion also impacts the performance of the suspect model , especially in complex tasks . Contributions . Our contributions are three-fold : • To our knowledge , ORL-AUDITOR is the first dataset audit- ing method for the offline DRL models , using the cumulative reward as an intrinsic and stable fingerprint of the dataset . • We demonstrate the effectiveness of ORL-AUDITOR on four offline DRL models and three tasks . We also systematically analyze various experimental factors , i.e. , the hyperparam- eter settings and the robustness of ORL-AUDITOR , and summarize some important guidelines for adopting ORL- AUDITOR in practice . • By implementing ORL-AUDITOR on the open-source datasets from DeepMind [ 26 ] and Google [ 18 ] , we show that ORL-AUDITOR can serve as a potent audit solution in real-world offline DRL scenarios . II . BACKGROUND A. Offline RL Problem The offline reinforcement learning ( offline RL ) model aims to learn an optimal ( or nearly optimal ) policy from a pre- collected dataset D without an interactive environment . We use S and A to represent the RL models ’ input and output space , formally called state and action in RL scenes . rt ∈ R is the temporal reward for each time step , where R is the real number set . A unit in a pre-collected dataset called transition is a four- element set : { st , at , rt , st+1 } , where st ∈ S , at ∈ A , and st+1 ∈ S is the successive state of st. And a set of transitions in chronological order forms a trajectory in dataset D. Based on the transitions , the offline RL model learns the Markov Decision Process underneath the datasets and forms a policy πθ ( a | s ) to maximize J ( π ) . J ( π ) = Est∼dβ ( s , a ) , at∼πθ ( a|s ) ( cid:35 ) γtrt , ( cid:34 ) H ( cid:88 ) t=0 where we use dβ to denote the distribution over states and actions in dataset D , and the actions are sampled according to the behavior policy at ∼ πθ ( a | s ) . The discount factor γ is applied to discount future rewards in the accumulated reward . H is the terminal time step of one trajectory . Example . Figure 2 shows an example based on the “ CartPole ” task . 1 In the data collection process , the dataset is generated from the operation logs between the operator and the envi- ronment , which contains the position and velocity of the cart and the pole ( i.e. , state ) , the operator ’ s force direction ( i.e. , action ) , and the corresponding rewards . Then , in the training and evaluation process , the offline RL model learns how to play the “ Cartpole ” task from only the pre-collected dataset generated through the data collection process . Finally , we deploy the well-trained offline RL model in the environment to perform the task . 1https : control/cart pole/ Fig . 2 : A running example of the offline DRL models . B. Offline RL Models In this section , we first introduce two offline RL algo- rithms [ 21 ] , [ 19 ] , [ 35 ] separately representing two basic ideas of the offline RL models , i.e. , the policy constraints strategy and the value function regularization strategy [ 50 ] . Many state- of-the-art model-free offline RL methods [ 68 ] , [ 32 ] , [ 20 ] , [ 35 ] have been modified from these two approaches . We further present a state-of-the-art algorithm [ 20 ] which is minimalistic with light computation and hyperparameter setting overhead . In addition , we briefly describe the behavior clone method ( BC ) [ 49 ] , which learns the state-action distribution over the dataset via a supervised learning approach . Though BC is not a typical reinforcement learning method , it can solve the offline RL problem and usually serves as the baseline method in the offline RL evaluation . Behavior Clone ( BC ) [ 49 ] . BC separately takes the pairwise state s and action a in the datasets as input and label , then it optimizes the policy through the following function . θ∗ = arg min θ E ( s , a ) ∼D [ L ( πθ ( s ) , a ) ] , where D is the pre-collected dataset and L is the loss function . Since BC only imitates action distributions , the performance is close to the mean of the dataset , even though BC works better than online RL algorithms in most cases . Batch-Constrained Q-learning ( BCQ ) [ 21 ] , [ 19 ] . BCQ is the first practical data-driven offline RL algorithm . The key idea of BCQ is to integrate a generative model to achieve the notion of batch-constrained , i.e. , minimizing the deviation between the candidate actions with the action records of the dataset . To maintain the diversity of action , BCQ builds a perturbation model to perturb each selected action . Then it chooses the highest-valued action through a Q-network , that learns to estimate the expected cumulative reward of a given state and action pair . Thus , the objective function of BCQ can be defined as the following . π ( s ) = argmax Qθ ( s , ai + ξϕ ( s , ai , Φ ) ) ai+ξϕ ( s , ai , Φ ) { ai ∼ Gω ( s ) } n i=1 , where Gω ( s ) is a conditional variational auto-encoder ( VAE ) - based [ 33 ] generative model that can be used to generate candidate actions . The value function Qθ is used to score the n candidate actions and finds the action with the highest value . ξϕ ( s , ai , Φ ) is the perturbation model , which outputs 3 Dataset … , 𝒔𝒔𝒕𝒕 , 𝒂𝒂𝒕𝒕 , 𝒓𝒓𝒕𝒕 , 𝒔𝒔𝒕𝒕 '' 𝟏𝟏 , … State , Reward Move Left , R = 1 Move Right , R = 1 Move Left , R = 1 Action Environment Operator Offline DRL Model Environment Action Offline DRL Model State Data Collection Training and Evaluation Deployment an adjustment to an action a in the range [ −Φ , Φ ] . Then , the perturbation model can be optimized by the deterministic policy gradient algorithm [ 58 ] as follows . ϕ ← argmax ( cid:88 ) ϕ ( s , a ) ∈B Qθ ( s , a + ξϕ ( s , a , Φ ) ) , where B represents a mini-batch state-action pair in the dataset . To penalize rare states , BCQ takes a convex combination of the values from two Q-networks and sets a new target value y to update both Q-networks . ( cid:20 ) y = r+γ max ai λ min j=1,2 Qθ′ j ( s′ , ai ) + ( 1 − λ ) max j=1,2 Qθ′ j ( cid:21 ) ( s′ , ai ) where ai corresponds to the perturbed actions , sampled from the generative model Gω ( s ) . Implicit Q-Learning ( IQL ) [ 35 ] . Compared to the batch- constrained idea of BCQ [ 21 ] , [ 19 ] , IQL strictly avoids query- ing values of the actions , which are not in the pre-collected dataset . IQL first constructs a model to evaluate the expected returns of state-action pairs . The objective function is defined as shown in Equation 1 . L ( θ ) = ED ( cid:2 ) Lτ 2 ( cid:0 ) r ( s , a ) + γQˆθ ( s′ , a′ ) − Qθ ( s , a ) ( cid:1 ) ( cid:3 ) , ( 1 ) 2 ( u ) = |τ − 1 ( u < 0 ) |u2 , and s′ and a′ represent where Lτ the successor state and action of s and a . Both Qθ ( s , a ) and Qˆθ are used to assess the expected returns of state- action pairs . The parameters of Qθ ( s , a ) are adjusted in each optimization round , while the parameters of Qˆθ are updated periodically based on Qθ ( s , a ) to reduce parameter fluctuations during model updates . Equation 1 involves the dynamics of the environment , where the environment state s transitions to the next environment state s′ , potentially introducing interference in the evaluation of expected returns for state-action pairs . IQL addresses this issue by introducing a new state value model , splitting Equation 1 into two objective functions . Equation 2 shows the objective function of the state value model Vψ . LV ( ψ ) = ED ( cid:2 ) Lτ 2 ( cid:0 ) Qˆθ ( s , a ) − Vψ ( s ) ( cid:1 ) ( cid:3 ) . ( 2 ) Then , IQL utilizes Vψ ( s ) to construct Equation 3 for updating the parameters of the state-action value model Qθ . LQ ( θ ) = ED ( cid:104 ) ( r ( s , a ) + γVψ ( s′ ) − Qθ ( s , a ) ) 2 ( cid:105 ) . ( 3 ) Finally , IQL considers using the state-action value model to construct a behavior policy for deployment . This behavior policy also needs to avoid actions that are outside the dataset distribution . Thus , IQL employs advantage-weighted regres- sion to update the policy model . Lπ ( ϕ ) = ED [ exp ( β ( Qθ ( s , a ) − Vψ ( s ) ) ) log πϕ ( a | s ) ] , ( 4 ) where β ∈ [ 0 , ∞ ) represents the inverse temperature . For smaller values of β , IQL is similar to behavior clone , tending to mimic the data collection policy . For larger values of β , IQL is more inclined to select actions corresponding to the highest expected returns according to the state-action value model . Throughout the entire training process , IQL alternates between optimizing the parameters θ and ψ , and then updates ϕ while keeping θ and ψ fixed . TD3PlusBC [ 20 ] . The former methods [ 21 ] , [ 19 ] , [ 35 ] limit or regularize action selection such that the learned policy is easier to evaluate with the given dataset . However , they introduce new hyperparameters and often leverage secondary components , such as generative models , while adjusting the underlying RL algorithm . TD3PlusBC is a minimalist and highly effective offline RL algorithm based Twin Delayed Deep Deterministic Policy Gradient ( TD3 ) [ 22 ] with BC regularization term , which pushes the policy towards favoring actions contained in the dataset D : π = argmax E ( s , a ) ∼D , π ( cid:2 ) λQ ( s , π ( s ) ) − ( π ( s ) − a ) 2 ( cid:3 ) , α 1 N ( cid:80 ) where λ = ( s , a ) |Q ( s , a ) | for the dataset of N transitions ( s , a ) . To facilitate the policy training , TD3PlusBC normalizes each state in the given dataset by si = si−µ σ+ϵ , where µ and σ are the mean and standard deviation respectively . The model architectures vary significantly regarding objec- tive function and basic model structure . 1 ) Objective Function : BCQ [ 21 ] , [ 19 ] and TD3PlusBC [ 20 ] use a policy constraints strategy to maintain the learned policy similar to the one used for collecting the dataset . In contrast , IQL [ 35 ] adopts a regularization strategy to improve the stochasticity of the learned policy or obtain more accurate Q-value estimations . 2 ) Basic Model Structures : BCQ [ 21 ] , [ 19 ] and IQL [ 35 ] are based on the Q-learning model , while TD3PlusBC [ 20 ] builds upon TD3 [ 22 ] . In Section V , our experiments are mainly conducted on the above four algorithms . However , ORL-AUDITOR can also be applied to any type of offline DRL model as long as the auditor has black-box access to the suspect model . III . PROBLEM STATEMENT AND EXISTING SOLUTIONS A . System and Threat Model the dataset Application Scenarios . Figure 3 illustrates a typical ap- plication scenario where the data providers collect and then publish or sell to the customers . A malicious customer ( adversary ) with access to the datasets makes a piracy distribution or illegally builds a Model-as-a-Service ( MaaS ) platform . Institution 1 suspects the models are generated by its dataset , and thus hires an auditor to determine whether the model trainers pirate the trajectories of the dataset D1 . Auditor ’ s Background Knowledge and Capability . The auditor has full knowledge of the target dataset , such as the number of trajectories and the spaces of state and action . In offline RL settings , the auditor is prohibited from interacting with the online environment to collect more data , meaning the entire auditing only depends on the target dataset . We consider the auditor has black-box access to the suspect RL model . Note that this is the most general and challenging scenario for the auditor . A typical application scenario is that an adver- sary receives the model settings from customers , such as the selected offline RL framework , the model ’ s hyperparameter , and the desired training episodes . Then , the adversary trains an offline RL model and provides a service interface to the customers . The auditor utilizes the states of the dataset ( inputs ) to query the suspect model and obtain the corresponding actions ( outputs ) . 4 explicit watermarks [ 6 ] to the datasets or the trained models . Existing methods [ 43 ] , [ 16 ] can be divided into two categories according to whether they have explicit classification labels . With the explicit classification labels , [ 43 ] rely on computing the distances between data points and decision boundaries . Without the explicit classification labels , [ 16 ] utilizes the similarity of the models ’ behaviors to detect the unauthorized usage of the dataset , which requires the assumption of an additional public dataset with a similar distribution to form the auditing basis . However , the above methods can not directly be applied to reinforcement learning cases due to two reasons . First , the label-based dataset inference [ 43 ] can not be implemented in the RL models since their outputs are usually continuous , and they are guided by the rough reward signals instead of the exact labels . Second , the distribution of the offline RL dataset not only depends on the environment but also relies on the strategy of interacting with the environment [ 18 ] . Thus , it is challenging to find a proper public dataset in offline RL scenarios . As we delve into Appendix A , it becomes evident that the behavior similarity of the DRL models varies across different public training data . Furthermore , the behavior similarity is also influenced by various offline DRL frameworks . Membership Inference Attack against RL [ 47 ] , [ 24 ] , [ 23 ] . Several membership inference attacks exist against DRL , which seem to address the problem studied in this paper . Most of them are targeted at the online RL scenes , assuming that the attacker owns the environment . Thus , they can utilize the environment to collect more data and even manipulate some adversarial states to facilitate the inference . However , in this paper , we aim at the offline RL cases , which are more challenging since the only thing the auditor can use is the pre-collected dataset . That is , in offline RL scenarios , the existing MIA against RL can not rely on the environment to generate non-member data . IV . ORL-AUDITOR We instantiate Q of Figure 1 with the cumulative reward , which is an intrinsic feature of the dataset and suitable for auditing . ∆ is determined by the shadow models trained on the datasets instead of a preset threshold to adapt the distribution of different datasets . Thus , the well-designed Q and ∆ guarantee the adaptiveness and effectiveness of ORL-AUDITOR . A. Workflow For ease of understanding , we refer to the target dataset as the dataset to be audited and the actual dataset as the dataset used by the suspect model . If the suspect model is trained on the target dataset , the actual dataset is the same as the target dataset , i.e. , positive audit result for the suspect model ; otherwise , the suspect model does not use the target dataset , i.e. , negative audit result for the suspect model . Figure 4 illustrates the workflow of ORL-AUDITOR . Fig . 3 : An example of the application scenario . The auditor can obtain all information about dataset D1 but has no knowledge about the datasets from other institutions . Discussion . Compared to the sample-level and dataset-level data in DNN scenes , RL has trajectory-level data , which is the minimum record unit of sequential interactions between the operator and environment . Since a single trajectory can guide the model from the initial state to the terminal , the trajectory-level data is regarded as the value unit of the dataset . Thus , ORL-AUDITOR is designed to audit the dataset from the trajectory level , where the auditor tries to decide whether the suspect model uses a specific trajectory in the dataset . In addition , the auditor can easily extend ORL-AUDITOR to the dataset-level data by setting a piracy alarm threshold . If the ratio of misappropriation using trajectories exceeds the preset threshold , the auditor can claim the dataset-level pirate . B . Existing Solutions Watermarking [ 39 ] , [ 38 ] . Watermarking-based dataset copy- right protection methods inject samples of a specific distribu- tion before publishing the target dataset . One of its kind is implemented with backdoor attacks against the ML model . Li et al . [ 39 ] proposed to modify a dataset by adding a trigger , such as a local patch , to innocent samples in order to make them appear as a pre-defined target class . To verify the integrity of the dataset after the attack , they use a hypothesis test approach based on posterior probabilities generated by a third- party model . Inspired by this idea , the auditor can employ the backdoor attack against the DRL model [ 34 ] , [ 64 ] , [ 66 ] to generate a watermark for the offline RL dataset . However , since the open-source datasets are already pub- lished , the auditor needs a post-event mechanism that does not require injecting manipulated samples before publishing the dataset . Watermarking , on the other hand , is a pre-event mechanism that involves injecting manipulated samples into the dataset before publishing . Additionally , it is difficult for the auditor to guarantee that one effective watermarking has a consistent distribution with the original dataset , which in- evitably disturbs the model ’ s normal behavior . Dataset Inferences [ 43 ] , [ 16 ] . The core idea of dataset inference is empowering the model owner to make a com- pelling statistical argument that a particular model is a copied version of their own model by demonstrating that it is based on their private training data . It does not require injecting Step 1 : Model Preparation ( MP ) . In the left box of Figure 4 , the auditor prepares the critic model and the shadow DRL models based on the target dataset , which contains m trajectories T with the length of ni ( i ∈ { 1 , 2 , . . . , m } ) . The critic model is optimized to estimate the cumulative reward 5 𝟏𝟏 𝒔𝒔𝟏𝟏 𝟏𝟏 , 𝒂𝒂𝟏𝟏 𝟏𝟏 , 𝒓𝒓𝟏𝟏 𝟏𝟏 , 𝒔𝒔𝟐𝟐 𝟏𝟏 𝒔𝒔𝟐𝟐 𝟏𝟏 , 𝒂𝒂𝟐𝟐 𝟏𝟏 , 𝒓𝒓𝟐𝟐 𝟏𝟏 , 𝒔𝒔𝟑𝟑 𝒋𝒋 𝒔𝒔𝟏𝟏 𝒎𝒎 𝒔𝒔𝟏𝟏 𝒋𝒋 , 𝒂𝒂𝟏𝟏 𝒎𝒎 , 𝒂𝒂𝟏𝟏 𝒋𝒋 𝟐𝟐 , 𝒔𝒔𝟐𝟐 , 𝒓𝒓𝟏𝟏 𝒎𝒎 𝒎𝒎 , 𝒔𝒔𝟐𝟐 , 𝒓𝒓𝟏𝟏 𝒋𝒋 𝒋𝒋 , 𝒂𝒂𝟐𝟐 𝒔𝒔𝟐𝟐 𝒎𝒎 𝒔𝒔𝟐𝟐 𝒋𝒋 , 𝒓𝒓𝟐𝟐 𝒎𝒎 , 𝒂𝒂𝟐𝟐 𝒋𝒋 , 𝒔𝒔𝟑𝟑 𝒎𝒎 , 𝒓𝒓𝟐𝟐 𝒎𝒎 , 𝒔𝒔𝟑𝟑 … … … 𝟏𝟏 𝒔𝒔𝒏𝒏𝟏𝟏 𝟏𝟏 , 𝒂𝒂𝒏𝒏𝟏𝟏 𝟏𝟏 , 𝒓𝒓𝒏𝒏𝟏𝟏 𝟏𝟏 , 𝒔𝒔𝒆𝒆𝒏𝒏𝒆𝒆 𝒋𝒋 𝒋𝒋 𝒋𝒋 , 𝒂𝒂𝒏𝒏𝒋𝒋 𝒔𝒔𝒏𝒏𝒋𝒋 , 𝒓𝒓𝒏𝒏𝒋𝒋 𝒎𝒎 𝒎𝒎 , 𝒂𝒂𝒏𝒏𝒎𝒎 𝒔𝒔𝒏𝒏𝒎𝒎 𝒋𝒋 , 𝒔𝒔𝒆𝒆𝒏𝒏𝒆𝒆 𝒎𝒎 𝒎𝒎 , 𝒓𝒓𝒏𝒏𝒎𝒎 , 𝒔𝒔𝒆𝒆𝒏𝒏𝒆𝒆 Transition Trajectory Institution 1 Dataset Institution 2 Dataset Institution 3 Dataset 𝐷𝐷 '' 𝐷𝐷 # 𝐷𝐷 $ A d v e r s a r y Model 𝜋𝜋 ! Model 𝜋𝜋 '' Model 𝜋𝜋 '' Auditor for Institution 1 The models do ( or do not ) pirate the -th Trajectory of Dataset . 𝒋𝒋 𝑫𝑫𝟏𝟏 Fig . 4 : The workflow of ORL-AUDITOR contains three steps , i.e. , model preparation , cumulative reward collection , and audit process . ORL-AUDITOR first trains a set of shadow DRL models and a critic model on the target dataset , then collects the cumulative rewards from the state-action pairs of the shadow models and the suspect model . Finally , ORL-AUDITOR audits every trajectory based on hypothesis testing . of each state-action pair . For each trajectory in the dataset , a series of predictions for its state-action pairs compose the exclusive feature for auditing . There are two ways to optimize the critic model , i.e. , the Monte-Carlo-based ( MC-based ) and the temporal-difference-based ( TD-based ) strategies . We adopt the TD-based learning method and explain the reasons in Section IV-B . In addition , the auditor trains a set of shadow models following the model ’ s objective function introduced in Section II with different model initializations . Step 2 : Cumulative Reward Collection ( CRC ) . In the middle box , the shadow models observe the states of the dataset and take actions . For i-th trajectory in the dataset , the auditor records the state si t and the action ai t of each shadow model , where ai t represents the shadow model ’ s action at the t-th step of trajectory Ti . After finishing the action collection , the auditor obtains the k sets of state-action pairs from the shadow models , representing the learned policies with different initialization and training processes on the target dataset . Using the critic model in Step 1 , the auditor calculates the estimations for all state-action records , i.e. , the estimated cumulative rewards , which are the samplings of the exact cumulative rewards of the corresponding state-action pairs in the dataset . Similarly , the auditor queries the suspect model with state si t. The state-action pairs are then put into the critic model and to obtain the estimations for the suspect model . t and observes the action ai Step 3 : Audit Process ( AP ) . After the above two steps , the auditor obtains the estimated cumulative rewards from the shadow models and the suspect model and then con- ducts the audit process . For j-th ( j ∈ { 1 , 2 , . . . , m } ) tra- jectory of the dataset , the auditor collects k series of the estimated cumulative rewards from the shadow models , i.e. , { Qi j | i ∈ { 1 , 2 , . . . , k } } , and one from the suspect model , i.e. , Qs j. ORL-AUDITOR conducts hypothesis testing based j from ¯Qj . The auditor can on the distances of Qi j , ¯Qj ) is out of the distribution of rule out suspicion if d ( Qs j , ¯Qj ) | i ∈ { 1 , 2 , . . . , k } } . Otherwise , the auditor will { d ( Qi conclude a positive decision , i.e. , the suspect model is trained using this trajectory . The auditor repeatedly implements the above processes for other trajectories of the dataset and obtains j and Qs the final audit report with judgment for all trajectories . We discuss more details of the distance metric and the hypothesis testing in Section IV-C. B . The Selection of Critic Model The auditor can use either Monte Carlo ( MC ) based or Temporal-Difference ( TD ) based algorithms to train a critic model from the trajectories of the dataset . The main distinction between the two methods lies in their learning targets , which leads to differences in their objective functions . In the case of MC-based methods , the learning target G is the empirical cumulative rewards from the dataset . G ( st , at ) = rt + γrt+1 + . . . + γH−1rH , where G ( st , at ) represents the exact cumulative reward from ( st , at ) to the terminal time step H of one trajectory . The discount factor γ is applied to discount future rewards . The critic model is trained by minimizing the following objective . E ( st , at , rt+1 , st+1 ) ∼D ( cid:104 ) ( G ( st , at ) − Qθ ( st , at ) ) 2 ( cid:105 ) . For TD-based methods , the learning target changes to the expected cumulative reward in a heuristic form , i.e. , rt + γQ ( st+1 , at+1 ) . Thus , the critic model is trained by mini- mizing the following loss function . E ( st , at , rt+1 , st+1 ) ∼D ( cid:2 ) ( rt+1 + γQθ′ ( st+1 , at+1 ) − Qθ ( st , at ) ) 2 ( cid:3 ) , where the critic model starts with arbitrary initialization θ . Then , it repeatedly evaluates Qθ ( st , at ) , obtains a reward rt+1 , and updates the weights . The θ′ is a snapshot of θ and copies from θ every few updates of θ . The MC-based method utilizes the exact cumulative rewards from the dataset to train the critic model , resulting in an unbiased prediction . It also has strong convergence properties due to the stationary of Gt . However , it can not be applied to situations where the collected data is truncated , and all trajectories in the dataset must be completed . In practice , many sequential decision-making tasks usually have long or infinite time steps . Thus , the dataset provider segments the interaction record into trajectories by a preset maximum length . The TD-based method tackles the limitation of the MC-based algorithm and can learn from incomplete sequences . Nevertheless , due to the heuristic learning process , 6 Step 1 : Model Preparation Step 2 : Cumulative Reward Collection Dataset 1 , 𝑎𝑡 𝑠𝑡 1 , 𝑟𝑡 1 1 , 𝑠𝑡+1 2 , 𝑎𝑡 𝑠𝑡 2 , 𝑟𝑡 2 2 , 𝑠𝑡+1 |𝑡 = 1 … 𝑛1 |𝑡 = 1 … 𝑛2 … 𝑚 , 𝑎𝑡 𝑠𝑡 𝑚 , 𝑟𝑡 𝑚 , 𝑠𝑡+1 𝑚 |𝑡 = 1 … 𝑛𝑚 Training 𝑇1 : 𝑇2 : 𝑇𝑚 : 1 2 3 4 … 𝑘 Shadow DRL Models Critic Model 𝑄 ( 𝑠 , 𝑎 ) 1 2 3 4 … 𝑘 Shadow DRL Models 𝑆1 : 𝑠𝑡 1|𝑡 = 1 … 𝑛1 𝑆2 : 𝑠𝑡 2|𝑡 = 1 … 𝑛2 … 𝑆𝑚 : 𝑠𝑡 𝑚|𝑡 = 1 … 𝑛𝑚 States Model 1 1 ) |𝑡 = 1 … 𝑛1 1 , 𝑎𝑡 𝑠𝑡 ( 𝑠𝑡 2 , 𝑎𝑡 2 ) |𝑡 = 1 … 𝑛2 … ( 𝑠𝑡 𝑚 , 𝑎𝑡 𝑚 ) |𝑡 = 1 … 𝑛𝑚 Model 𝑘 1 ) |𝑡 = 1 … 𝑛1 ( 𝑠𝑡 1 , 𝑎𝑡 Model 𝑖 : 1~𝑘 ℚ1 𝑖 : 𝑄 𝑠𝑡 1 , 𝑎𝑡 1 |𝑡 = 1 … 𝑛1 … ( 𝑠𝑡 2 , 𝑎𝑡 2 ) |𝑡 = 1 … 𝑛2 … ℚ2 𝑖 : 𝑄 𝑠𝑡 2 , 𝑎𝑡 2 |𝑡 = 1 … 𝑛2 … ( 𝑠𝑡 𝑚 , 𝑎𝑡 𝑚 ) |𝑡 = 1 … 𝑛𝑚 ℚ𝑚 𝑖 : 𝑄 𝑠𝑡 𝑚 , 𝑎𝑡 𝑚 |𝑡 = 1 … 𝑛𝑚 States & Actions Critic Model Q ( 𝑠 , 𝑎 ) Cumulative Rewards ( 𝑠𝑡 1 , 𝑎𝑡 1 ) |𝑡 = 1 … 𝑛1 ( 𝑠𝑡 2 , 𝑎𝑡 2 ) |𝑡 = 1 … 𝑛2 … ( 𝑠𝑡 𝑚 , 𝑎𝑡 𝑚 ) |𝑡 = 1 … 𝑛𝑚 ℚ1 𝑆 : 𝑄 𝑠𝑡 1 , 𝑎𝑡 1 |𝑡 = 1 … 𝑛1 ℚ2 𝑆 : 𝑄 𝑠𝑡 2 , 𝑎𝑡 2 |𝑡 = 1 … 𝑛2 … ℚ𝑚 𝑆 : 𝑄 𝑠𝑡 𝑚 , 𝑎𝑡 𝑚 |𝑡 = 1 … 𝑛𝑚 Suspect DRL Model States & Actions Critic Model Q ( 𝑠 , 𝑎 ) Cumulative Rewards 𝑘 ℚ𝑗 Audit Metric Step3 Audit Process For 𝒋-th trajectory of the dataset ( 𝑗 ∈ { 1,2 , … , 𝑚 } ) 1 : ℚ𝑗 2 : ℚ𝑗 𝑘 : ℚ𝑗 𝑠 : ℚ𝑗 0.7 1.2 1.3 0.6 … 1.1 0.7 0.9 0.8 0.9 … 0.9 ⋯ 1.3 0.9 0.7 0.5 … 1.3 0.6 1.3 1.1 1.4 … 1.1 Length of Trajectory 1 , ഥℚ ) 𝑑 ( ℚ𝑗 2 , ഥℚ ) 𝑑 ( ℚ𝑗 ഥℚ is element-wise mean of { ℚ𝑗 𝑑 ( ℚ𝑗 ⋯ 𝑘 , ഥℚ ) 𝑑 ( ℚ𝑗 𝑠 , ഥℚ ) 𝑖 |𝑖 : 1~𝑘 } . 2 ℚ𝑗 1 ℚ𝑗 4 ℚ𝑗 ഥℚ 𝑠 ℚ𝑗 3 ℚ𝑗 ∆ 5 ℚ𝑗 Positive 𝑠 ℚ𝑗 Negative Hypothesis testing If 𝝁 𝑄𝑆 > , Negative for 𝑇 Algorithm 1 Workflow of ORL-AUDITOR Input : Dataset D , suspect model πs , number of shadow models k , significance level α Output : The trajectory-level audit report 1 : // Step 1 : Model Preparation 2 : Train shadow models { πi | i = 1 , . . . , k } and critic model 3 : // Step 2 : Data Preparation 4 : for each model π in { πi | i = 1 , . . . , k } ∪ { πs } do Query π by states s ∈ D and obtain the actions . 5 : Evaluate each ( s , a ) pair based on the critic model Q . 6 : Record the cumulative reward in sequential form { Qj | 7 : j = 1 , . . . , m } . 8 : end for 9 : // Step 3 : Audit Process 10 : audit report = [ ] 11 : for each trajectory in { Tj | j = 1 , . . . , m } do 12 : Calculate the element-wise mean ¯Qj of { Qi j | i = 1 , . . . , k } 13 : 14 : 15 : Measure the d ( Qj , ¯Qj ) of each Qi // Hypothesis testing From { di | i = 1 , . . . , k } and ds , decide whether the j from ¯Qj . j and Qs suspect model Ms pirates Tj with significance level α. audit report.append ( j-th audit result ) 16 : 17 : end for 18 : Return audit report the TD-based method has some bias and is more sensitive to model initialization . Therefore , we choose the element-wise mean of the shadow models ’ cumulative rewards ¯Q as the auditing directrix in Section IV-A instead of relying solely on the critic model ’ s predictions to compensate for the shortages of TD-based methods . C. The Details of Audit Process In the audit process , the choice of distance metric and the hypothesis testing method play a critical role in ORL- AUDITOR ’ s performance . A proper metric is sensitive to the deviations between the estimated cumulative rewards , which can facilitate the hypothesis testing . A suitable hypothesis test- ing method can provide precise results with high confidence . Distance Metric . We consider three types of distance metrics , i.e. , ℓp norm , Cosine distance , and Wasserstein distance . ℓp norm is a popular method of measuring the distance between vectors , i.e. , the sum of the absolute difference of the compo- nents of the vectors . In the RL scene , the states and actions are sequential data , meaning the distance metric should measure both the value and the position deviation of the cumulative rewards . However , ℓp norm may fail to reflect the difference from the sequence aspect of the same set of values . Cosine distance is a derivative of Cosine similarity , defined as the cosine of the angle between two vectors . Cosine distance embodies the difference from both the value and position aspects of the vectors . However , Cosine distance normalizes the inner product using the two vectors ’ norm , which weakens the numerical differences between the cumulative rewards . The Wasserstein distance , a.k.a . earth mover ’ s distance ( EMD ) , is a metric of the difference between two probability distributions over a region [ 54 ] . It can be defined as follows . l1 ( u , v ) = inf π∈Γ ( u , v ) ( cid:90 ) R×R |x − y|dπ ( x , y ) , where Γ ( u , v ) is the set of distributions on R × R whose marginals are u and v on the first and second factors respec- tively . Wasserstein distance fits well with audit requirements , reflecting numerical and positional deviations of the cumula- tive rewards . Thus , we set Wasserstein distance by default and compare different distance metrics in Section V. Hypothesis Testing . After the selection of the distance metric , the auditor proceeds to hypothesis testing with the distances of Qi j and Qs j from ¯Qj . H0 : d ( Qs H1 : d ( Qs j , ¯Qj ) is not an outlier . j , ¯Qj ) is an outlier . An intuitive method is to leverage the 3σ principle , i.e. , the normal samples should be distributed within the range of three times the standard deviation σd from the mean µd . The 3σ principle is an efficient hypothesis testing method , yet the mean µd is easily misled by outliers . Compared to the 3σ principle , Grubbs ’ test [ 25 ] is a more robust hypothe- sis testing method for detecting single outliers in univariate j , ¯Qj ) exceeds datasets . If the Grubbs ’ test statistic of d ( Qs the threshold derived on the significance level , the auditor j , ¯Qj ) deviate from the mean value , i.e. , reject can claim d ( Qs H0 and output negative audit result . For a set of samples { di | i = 1 , 2 , . . . , n } , Grubbs ’ Test locates the outlier by the procedures . 1 ) Calculate the mean µd and standard deviation σd . 2 ) Calculate the Grubbs ’ test statistic by G = ( cid:114 ) t2 |d ( Qs σd j , ¯Qj ) −µd| . 3 ) If G > n−1√ n α/ ( n ) , n−2 n−2+t2 α/ ( n ) , n−2 , H0 is invalid , i.e. , the suspect model is not trained by this trajectory . In the above inequation , t2 α/ ( n ) , n−2 represents the upper critical value in the t-distribution when the degree of freedom is n − 2 , and the significance level is α n . Both hypothesis testing methods are based on the assump- tion that the distance values follow Gaussian distribution . Thus , ORL-AUDITOR needs to pre-check that the distance values of the shadow models satisfy the Gaussian distribution . We adopts Anderson-Darling test [ 59 ] since it fits the scenarios where the auditor has a small number of samplings , and the actual distribution is unknown . In the evaluation , all the distance values of the shadow models can pass the Anderson-Darling test due to the randomness of the models ’ initialization and training . After that , ORL-AUDITOR conducts the hypothesis testing . V. EVALUATION We first introduce the tasks and the experimental setup in Section V-A . We validate the effectiveness of ORL- AUDITOR on Behavior Clone and three offline DRL models , i.e. , Batch-Constrained Q-learning ( BCQ ) [ 21 ] , Implicit Q- Learning ( IQL ) [ 35 ] , and TD3PlusBC [ 20 ] in Section V-B . Then , we visualize the cumulative rewards by t-SNE [ 62 ] to demonstrate that the cumulative rewards are intrinsic and stable 7 TABLE I : The Overview of Tasks . The “ continuous ” and “ discrete ” illustrate the data type of the state and action with the corresponding number of dimensions in parentheses . Task Name Lunar Lander ( Continuous ) State Shape Action Shape Continuous ( 6-dim ) Discrete ( 2-dim ) Continuous ( 2-dim ) Bipedal Walker Continuous ( 24-dim ) Continuous ( 4-dim ) Ant Continuous ( 111-dim ) Continuous ( 8-dim ) features for dataset auditing in Section V-C. After that , we further evaluate the impact of three factors on ORL-AUDITOR , i.e. , the number of shadow models , the significance level in hypothesis testing , and the trajectory size in Section V-D . Finally , we utilize ORL-AUDITOR to audit the open-source datasets from Google [ 18 ] and DeepMind [ 26 ] in Section V-E. A . Experimental Setup Tasks . We adopt Lunar Lander , Bipedal Walker , and Ant tasks in Gym [ 7 ] , which are widely used in the prior works [ 9 ] , [ 30 ] , [ 48 ] . The tasks stem from distinct real-world problems , each with numerical vectors containing different physical in- formation , e.g. , position , velocity , and acceleration . These tasks involve both discrete and continuous variables in observation and action spaces , with the dimension ranging from low ( 2- dim ) to high ( 111-dim ) . We give an overview in Table I and put their details in Appendix B. Dataset Generation and Offline Model Preparation . To obtain the datasets for tasks in Table I , we adopt the same idea as the existing dataset publishers [ 26 ] , [ 18 ] , [ 52 ] , [ 1 ] , i.e. , training the online RL models in the interactive envi- ronment and recording the interactions as the datasets . The datasets consist of numerical vectors . In Lunar Lander , each transition includes state , next state ( 6-dimensional continuous and 2-dimensional discrete variables ) , action ( 2-dimensional continuous variables ) , and reward ( scalar ) . Therefore , each transition is a 19-dimensional numerical vector . Similarly , the data types of Bipedal Walker and Ant are 53-dimensional and 231-dimensional numerical vectors , respectively . The number of transitions for each task is 5 × 105 ( Lunar Lander ) , 106 ( Bipedal Walker ) , and 2 × 106 ( Ant ) . The offline RL models learn from the datasets . Table II summarizes the whole process . For each task , we use five global random seeds to train five online models separately . We collect the datasets from five online models with random seed 0 , where every online model only generates one dataset . For ease of reading , the datasets share the same name with their online models . We train thirty offline DRL models for every dataset with distinct global random seeds in initialization and optimization processes . All the online and offline models are implemented by open-source RL libraries [ 53 ] , [ 56 ] with default hyperparameter settings . Critic Model . We adopt the fully connected neural network as the critic model , which has four hidden layers with 1024 neurons on each layer . We optimize the critic model following the TD-based method in Section IV-B by Adam optimizer with a learning rate of 0.001 and a mini-batch size of 4096 . The TABLE II : The main steps in dataset generation and offline model preparation with the details of the input and output . For each combination of task and offline RL model in the experiment ↓ Train with 5 random seeds : { 0 , 1 , . . . , 4 } ↓ 5 online RL models detailed in Table XIV ↓ Collect with 1 random seed : { 0 } ↓ 5x1 offline Datasets detailed in Table XV ↓ Train with 30 random seeds : { 42 , 43 , . . . , 72 } ↓ 5x1x30 offline RL models detailed in Table XVI , Table XVII , Table XVIII , and Table XIX entire training takes 150 epochs , and the learning rate decays to half every 50 epochs . Evaluation Metrics . Recalling ORL-AUDITOR ’ s application scenario in Figure 3 , for a single suspect model , the audit accuracy can well characterize the performance of ORL- AUDITOR , i.e. , the ratio of the number of correctly auditing trajectory to the total auditing trajectory . In our experiment , the positive models ( trained on the target dataset ) and the negative models ( trained on other datasets ) are randomly mixed , where the majority may dominate the accuracy . Thus , we provide the true positive rate ( TPR ) and the true negative rate ( TNR ) . Methods . We provide the audit performance of 3σ principle and Grubbs ’ test with four distance metrics , i.e. , ℓ1 norm , ℓ2 norm , Cosine distance , and Wasserstein distance . Competitors . Recalling Section III-B , existing methods [ 47 ] , [ 24 ] , [ 23 ] are designed for the online reinforcement learning scenes , assuming that the auditor can continuously interact with the environment to obtain new data as the non-member example . Based on the behavioral difference of the model between the member examples and the non-member examples , they build the member inference method to detect whether an example is used to train the suspect model . In the offline scenarios , without access to the environment , the auditor only has the pre-collected target dataset . Thus , we randomly divide the target dataset into two parts and train offline RL models on the subsets separately . Either subset is regarded as the set of non-member examples for the offline RL models trained on the other subset . We adopt the same data augmentation , attack classifier architecture , and hyperparameter settings with [ 23 ] . Implementation . We use stable-baselines [ 53 ] and d3rlpy [ 56 ] to implement online and offline DRL models separately . All audit methods are realized with Python 3.8 on a server with 8 NVIDIA GeForce RTX 3090 and 512GB memory . B . Overall Audit Performance We assess the effectiveness of ORL-AUDITOR across twelve combinations of three tasks and four models . Fur- thermore , we present an evaluation of the efficacy of the competitors on offline DRL models . Setup . From Table II , we train 30 offline RL models for each dataset and obtain 150 offline DRL models for every 8 TABLE III : The performance of existing membership inference attack against offline DRL models . Task Name Lunar Lander Bipedal Walker Ant Offline Model BC BCQ IQL TD3PlusBC BC BCQ IQL TD3PlusBC BC BCQ IQL TD3PlusBC Accuracy Training 50.09±0.68 49.84±1.39 49.88±0.76 50.08±0.92 50.00±0.63 49.97±0.69 50.17±0.95 49.87±0.94 50.44±0.64 50.22±0.52 50.33±0.35 50.13±0.67 Test 48.41±1.87 47.69±1.45 47.34±1.83 48.27±1.81 46.27±2.42 47.38±2.41 47.19±1.90 45.48±1.46 46.74±2.37 45.38±2.16 45.89±1.90 45.03±1.55 experimental setting . We audit the 5 datasets separately , where the auditor randomly selects 15 models from the target dataset as the shadow models , and the remaining 15 models along with the 120 models from other datasets are the positive and the negative suspect models . For the target dataset , we randomly select fifty auditing trajectories to audit . Since the unbalanced amount of the positive and the negative models , we report the aggregated mean with a standard deviation of both TPR and TNR for each setting in Table IV and provide the audit results between every two datasets in Figure 11 ( Lunar Lander ) , Figure 12 ( Bipedal Walker ) , and Figure 13 ( Ant ) . Each pair of TPR and TNR in Table IV is derived from the diagonal and non-diagonal values of the corresponding heatmap . As a supplementary of [ 13 ] , we also show the audit result by 3σ principle in Table VII . The competitors ’ performance is shown in Table III , where the values of mean and standard variation are calculated by repeating experiment ten times . Observations . We have the following observations from Table IV , Table VII , and Table III . 1 ) Most TPR and TNR values are higher than 95 % , meaning that ORL-AUDITOR is a valid solution to audit the learned dataset of the offline DRL models . For instance , all results for ORL-AUDITOR with ℓ1 norm are beyond 94 % across the experiment settings . 2 ) ORL-AUDITOR obtains different audit accuracy over four distance metrics . The audit effectiveness with ℓ1 norm and Wasserstein distance is better than that of ℓ2 norm and Cosine distance . In Table IV and Table VII , ORL-AUDITOR with Wasserstein distance always performs the best or the second place . And results of ℓ2 norm are usually behind the other three distance metrics . Recalling Section IV-C , Wasserstein distance characterizes both the numerical and the positional deviations of the cumulative rewards , which is more sensitive . Since the numerical differences between the cumulative rewards are slight , e.g. , from 0.01 to 0.1 in our experiment , ℓ2 norm may undercut these small but potential differences . 3 ) The accuracy of the audit as determined by Grubbs ’ test outperforms that of the 3σ principle . The 3σ principle is an empirical method , which is easily misled by the outlier cumu- lative rewards of the shadow models . Recalling Section IV-C , Grubbs ’ test first calculates the statistic G and compares G with an adaptive threshold , where the number of samples is also considered in the hypothesis testing . 4 ) Without the new data from the environment , the ef- fectiveness of the existing membership inference methods is attenuated . From one perspective , the similarity between sub- datasets splited from the same dataset can result in the trained RL models exhibiting undifferentiated behavior , making it difficult to effectively distinguish between members and non- members . On the other hand , when considering the results presented in Figure 10 , we conclude that the actions of RL models should not be directly utilized as the foundation for membership inference . C. Visualization of Cumulative Rewards To further explain the audit results in Section V-B , we analyze the cumulative rewards from the shadow models and j and Qs the suspect models , i.e. , Qi j , by using t-SNE [ 62 ] . j ( positive ) or Qs Setup . The caption of each plot in Figure 5 indicates the used task and offline DRL model . Each point in the plots shows the visualization of a single Qi j ( negative ) . In a single plot , we demonstrate the results of three trajectories from each tasks ’ first datasets . For instance , the target dataset of the plot titled “ Lunar Lander , BC ” is dataset “ 1171 ” in Table XV . The thirty positive points for each trajectory are collected from the shadow models trained on dataset “ 1171 ” , while the thirty negative points are randomly sampled from the shadow models from the other four datasets . Observations . From Figure 5 , we have the following obser- vations . 1 ) For a trajectory of the target dataset , the cumulative rewards from the shadow models and the suspect models are clearly divided into different groups , meaning that the critic model well reflects the differences in the models ’ actions . Thus , the cumulative reward generated by the critic model is a qualified post-event fingerprint for trajectory-level auditing . 2 ) The distribution of points varies on the different trajec- tories . For example , trajectory 1 from the Lunar Lander dataset is harder to cluster than the other two trajectories . We speculate that this is because trajectory 1 represents a basic policy , e.g. , a local optimum policy to fire the lander ’ s thrusters all the way , and similar trajectories exist in the other four datasets . Due to the non-uniqueness of the optimal strategy in RL problems and the impact of randomness in the model training process , the collected trajectories have unique characteristics . Thus , other trajectories ’ cumulative rewards are clearly divided . D. Hyperparameter Study impact We extend our assessment to scrutinize three pivotal de- terminants that the pragmatic integration of ORL- AUDITOR . Specifically , we consider the amount of shadow models , the level of significance in hypothesis testing , and the magnitude of the trajectory size . Due to space limitations , we only give brief conclusions in this section . Please refer to the specific analysis in Appendix C , Appendix D , and Appendix E. Impact of Shadow Models ’ Amount . We change the shadow models ’ amount to 9 and 21 with the other settings the same as Section V-B . Figure 6 shows the value change of TPR and TNR compared with that of 15 shadow models . Each figure ’ s title illustrates the settings of the model and the task , the x-axis indicates the four metrics , and the y-axis is the absolute value change . As a supplementary of [ 13 ] , we provide the detailed results in Table VIII ( 9 Shadow Models ) and Table IX ( 21 Shadow Models ) . 9 TABLE IV : The TPR and TNR results based on Grubbs ’ test . The mean and standard deviation of TPR and TNR in each row represent the audit results for one combination of task and model by four distance metrics . Bold indicates the highest sum of TPR and TNR , i.e. , accuracy , in a row . Each pair of TPR and TNR is derived from the diagonal and non-diagonal values of the corresponding heatmap in Figure 11 , Figure 12 and Figure 13 , which are supplementary to [ 13 ] . Task Name Offline Model Lunar Lander Bipedal Walker Ant BC BCQ IQL TD3PlusBC BC BCQ IQL TD3PlusBC BC BCQ IQL TD3PlusBC L1 Norm L2 Norm Cosine Distance Wasserstein Distance TPR 99.01±0.46 98.29±1.14 98.61±1.51 98.29±2.04 99.20±1.47 99.52±0.77 95.10±7.41 99.36±1.28 97.42±1.66 97.17±2.96 97.20±2.33 98.53±1.80 TNR 100.00±0.00 100.00±0.00 99.91±0.32 99.48±0.79 100.00±0.00 100.00±0.00 100.00±0.00 94.77±19.42 99.94±0.11 99.80±0.43 99.66±0.73 99.18±1.72 TPR 96.96±0.73 96.03±1.15 97.52±2.51 96.35±3.01 98.40±2.70 98.16±2.89 95.04±5.45 97.15±5.71 96.48±1.66 95.68±2.54 96.61±2.50 97.17±1.79 TNR 100.00±0.00 100.00±0.00 99.97±0.12 99.89±0.22 100.00±0.00 100.00±0.00 100.00±0.00 93.36±21.46 99.90±0.36 99.84±0.43 99.69±0.59 99.35±1.74 TPR 96.93±0.77 95.97±1.07 97.49±2.56 96.27±3.16 98.56±2.68 99.87±0.15 99.84±0.32 96.96±5.82 99.20±1.08 99.66±0.43 99.57±0.79 99.72±0.40 TNR 100.00±0.00 99.99±0.04 99.92±0.19 99.91±0.23 100.00±0.00 100.00±0.00 100.00±0.00 91.98±21.75 85.66±28.23 86.70±26.89 86.25±27.90 87.79±26.43 TPR 98.40±0.74 97.57±1.17 98.32±1.79 98.53±1.25 99.31±1.32 99.89±0.13 95.01±6.72 98.08±3.84 98.00±1.19 98.67±1.65 99.36±0.42 99.25±1.24 TNR 99.94±0.16 99.91±0.14 97.10±5.66 95.59±3.77 100.00±0.00 100.00±0.00 100.00±0.00 88.26±25.34 99.92±0.14 99.79±0.46 99.63±0.78 99.14±1.81 From Figure 6 , we have the following observations . 1 ) The audit accuracy increases with a larger amount of shadow models . 2 ) There exists a saturation point for audit accuracy with the expansion of shadow models . Impact of Significance Level . The significance level rep- resents the auditor ’ s confidence in the auditing results . In the significance level α = 0.01 , Section V-B , we adopt meaning that the auditor has 99 % confidence in the judg- ments . Generally speaking , the significance level represents the maximum audit capacity of ORL-AUDITOR instead of a hyperparameter setting since it is an audit requirement by the dataset owner . We demand the auditor to output a more confident judgment , where the error possibility should be limited to 1‰ and 0.1‰ , i.e. , α = 0.001 and α = 0.0001 . Figure 7 shows the value change of TPR and TNR compared with that when α = 0.01 . As a supplementary of [ 13 ] , the detailed results between every two datasets are in Table X ( α = 0.001 ) and Table XI ( α = 0.0001 ) . From Figure 7 , we have the following observations . 1 ) For a complicated task , we recommend the auditor select a large significance level for ORL-AUDITOR . 2 ) For the suspect models with low performance , ORL-AUDITOR should adopt a large significance level to guarantee audit accuracy . 3 ) In general , α = 0.01 is a safe bound of ORL-AUDITOR , and a lower α may break through the capability boundary of ORL- AUDITOR , inducing the auditor to misclassify the negative model to the positive set . Impact of Trajectory Size . We investigate the relationship between the trajectory size and audit accuracy . In Section V-B , we adopt the full-length trajectory , meaning that the auditor utilizes all states of each trajectory to query the suspect model and obtains the corresponding actions to conduct the dataset auditing . We change the trajectory size to 25 % and 50 % of the full length with the other settings the same as Section V-B . Figure 8 shows the value change of TPR and TNR compared with that of the full-length trajectory . As a supplementary of [ 13 ] , we also provide the detailed results in Table XII ( 25 % ) and Table XIII ( 50 % ) . From Figure 8 , we have the following observations . 1 ) ORL-AUDITOR tends to achieve higher accuracy with a larger trajectory size . 2 ) A small trajectory size achieves better results under some tasks since the front states of each trajectory are able to reflect more behavioral information of the model [ 46 ] . E. Real-world Application In this section , we apply ORL-AUDITOR to audit the open- source datasets from DeepMind [ 26 ] and Google [ 18 ] . We choose the “ halfcheetah ” task published by both , where the operator controls a 2-dimensional cheetah robot consisting of 9 links and 8 joints connecting them ( including two paws ) to make the cheetah run forward ( right ) as fast as possible . The details of the halfcheetah dataset and the offline DRL models are in Table XX and Table XXI . All experimental settings are consistent with these in Section V-B . Observations . From Table XXII , we have the following observations . 1 ) ORL-AUDITOR can be effective in real-world applications . The TPR and TNR of ORL-AUDITOR exceed 95 % with ℓ1 norm and Wasserstein distance , meaning that ORL-AUDITOR remains valid for the existing open-source datasets . 2 ) Wasserstein distance has stable performance on the experimental and the real-world datasets . The overall accuracy of ORL-AUDITOR with Wasserstein distance are all higher than the other three metrics . VI . ROBUSTNESS A . Ensemble Architecture To hinder the audit of a dataset , an adversary may uti- lize state-of-the-art membership inference defense strategies proposed in recent research works [ 60 ] , [ 31 ] . These defense strategies aim to mitigate the influence of a member example on the behavior of a machine learning model . Based on the idea of model ensemble , in particular , [ 60 ] , [ 31 ] , [ 11 ] proposed to split the training set into several subsets and train sub- models on each of these subsets . Then , when an auditor uses an example from the target dataset to query a suspect model , 10 Fig . 5 : Visualization of cumulative rewards by t-SNE . The caption of each plot demonstrates the offline DRL model ’ s type and task . In a single plot , we randomly select three trajectories from the first dataset for the task , i.e. , Lunar Lander dataset 1171 , Bipedal Walker dataset 0841 , and Ant dataset 2232 in Table XV , and then show the cumulative rewards from 30 positive models and 30 negative models for each trajectory . Fig . 6 : Impact of shadow models ’ amount . The change value of TPR and TNR when the number of shadow models varies to 9 and 21 compared to the default 15 shadow models . The caption of each plot demonstrates the offline DRL model ’ s type and task . The x labels display the four distance metrics . The y labels show the absolute fluctuating values of TPR and TNR . the adversary aggregates the outputs of the sub-models that have not been trained on this example . Setup . The number of divided subsets , denoted by K , repre- sents a crucial hyperparameter for ensemble-based methods , as discussed in [ 60 ] , [ 31 ] . Considering the analysis conducted in these studies , as well as the size of the offline RL datasets , we have established K = 5 for the present investigation . All other experimental settings remain unchanged from those described in Section V-B , and the corresponding audit outcomes are presented in Table V. As a supplementary of [ 13 ] , the results between every two datasets are in Figure 14 ( Lunar Lander ) , Figure 15 ( Bipedal Walker ) , Figure 16 ( Ant ) , and Figure 17 ( Half Cheetah ) . Observations . We conclude the following observations based on the above results . 1 ) Even when faced with ensemble architecture , ORL-AUDITOR maintains a high level of audit accuracy . As shown in Table V , both TPR and TNR con- sistently exceed 80 % . As described in Section IV-A , ORL- AUDITOR uses predicted cumulative rewards from the critic model as the basis for auditing . During training , the critic model captures the overall features of the dataset distribution , instead of memorizing features from individual samples . Since the ensemble model is trained on the target dataset , its behavior embeds the distribution characteristics of the dataset , which 11 Lunar Lander , BC Lunar Lander , BCQ Lunar Lander , IQL Lunar Lander , TD3PlusBC Trajectory 0 1 2 Audit Result Positive Negative Trajectory 0 1 2 Audit Result Positive Negative Trajectory 0 1 2 Audit Result Positive Negative Trajectory 0 1 2 Audit Result Positive Negative Bipedal Walker , BC Bipedal Walker , BCQ Bipedal Walker , IQL Bipedal Walker , TD3PlusBC Trajectory 0 1 2 Audit Result Positive Negative Trajectory 0 1 2 Audit Result Positive Negative Trajectory 0 1 2 Audit Result Positive Negative Trajectory 0 1 2 Audit Result Positive Negative Ant , BC Ant , BCQ Ant , IQL Ant , TD3PlusBC Trajectory 0 1 2 Audit Result Positive Negative Trajectory 0 1 2 Audit Result Positive Negative Trajectory 0 1 2 Audit Result Positive Negative Trajectory 0 1 2 Audit Result Positive Negative Lunar Lander , BC Bipedal Walker , BC Ant , BC 5.0 0 -5.0 -10.0 l e u a V ∆ 5.0 0 -5.0 -10.0 l e u a V ∆ 5.0 0 -5.0 -10.0 l e u a V ∆ Lunar Lander , BCQ Bipedal Walker , BCQ Ant , BCQ 5.0 0 -5.0 -10.0 5.0 0 -5.0 -10.0 5.0 0 -5.0 -10.0 5.0 0 -5.0 -10.0 5.0 0 -5.0 -10.0 5.0 0 -5.0 -10.0 Lunar Lander , IQL Bipedal Walker , IQL Ant , IQL Lunar Lander , TD3PlusBC Bipedal Walker , TD3PlusBC Ant , TD3PlusBC 5.0 0 -5.0 -10.0 5.0 0 -5.0 -10.0 5.0 0 -5.0 -10.0 L 1 L 1 L 2 L 2 T T T T P N P N R R R R o s . o s . T T a s s . a s s . P N T T R R P N R R L 1 L 1 L 2 L 2 T T T T P N P N R R R R o s . o s . T T a s s . a s s . P N T T R R P N R R L 1 L 1 L 2 L 2 T T T T P N P N R R R R o s . o s . T T a s s . a s s . P N T T R R P N R R L 1 L 1 L 2 L 2 T T T T P N P N R R R R o s . o s . T T a s s . a s s . P N T T R R P N R R C C W W C C W W C C W W C C W W Distance Metric Distance Metric Distance Metric Distance Metric 9 Shadow Models 21 Shadow Models Fig . 7 : Impact of the significance level . The change value of TPR and TNR when the significance level varies to 0.001 and 0.0001 compared to the default 0.01 . The caption of each plot demonstrates the offline DRL model ’ s type and task . The x labels display the four distance metrics . The y labels show the absolute fluctuating values of TPR and TNR . Fig . 8 : Impact of the trajectory size . The change value of TPR and TNR when the trajectory size varies to 25 % and 50 % compared to the entire trajectories ( 100 % ) . The caption of each plot demonstrates the offline DRL model ’ s type and task . The x labels display the four distance metrics . The y labels show the absolute fluctuating values of TPR and TNR . Fig . 9 : Robustness against action distortion . The change value of TPR and TNR when the suspect model adds Gaussian noise parameterized with ( µ = 0 , σ = 0.01 ) and ( µ = 0 , σ = 0.1 ) to its output . The caption of each plot demonstrates the offline DRL model ’ s type and task . The x labels display the four distance metrics . The y labels show the absolute fluctuating values . ORL-AUDITOR can detect . 2 ) The use of ensemble architecture may result in a de- crease in model performance for certain tasks . Our experimen- tal results , as shown in column “ Model Performance ( Model Ensemble ) ” of Tables Table XVI , Table XVII , Table XVIII , and Table XIX , demonstrate a decline in the performance of offline RL models when utilizing ensemble architecture . For instance , when BCQ models learn from the Ant dataset “ 3569 ” , the mean values of cumulative reward decrease significantly . Furthermore , due to the sub-models being trained on subsets of data , they only fit a partial dataset ’ s distribution . Consequently , when applying the model ensemble to practical scenarios , the standard deviations of the model ’ s performance are large . 12 Lunar Lander , BC Bipedal Walker , BC Ant , BC l e u a V ∆ l e u a V ∆ l e u a V ∆ 5.0 0.0 -5.0 -10.0 -15.0 -20.0 5.0 0.0 -5.0 -10.0 -15.0 -20.0 5.0 0.0 -5.0 -10.0 -15.0 -20.0 Lunar Lander , BCQ Bipedal Walker , BCQ Ant , BCQ 5.0 0.0 -5.0 -10.0 -15.0 -20.0 5.0 0.0 -5.0 -10.0 -15.0 -20.0 5.0 0.0 -5.0 -10.0 -15.0 -20.0 5.0 0.0 -5.0 -10.0 -15.0 -20.0 5.0 0.0 -5.0 -10.0 -15.0 -20.0 5.0 0.0 -5.0 -10.0 -15.0 -20.0 Lunar Lander , IQL Bipedal Walker , IQL Ant , IQL Lunar Lander , TD3PlusBC Bipedal Walker , TD3PlusBC Ant , TD3PlusBC 5.0 0.0 -5.0 -10.0 -15.0 -20.0 5.0 0.0 -5.0 -10.0 -15.0 -20.0 5.0 0.0 -5.0 -10.0 -15.0 -20.0 L 1 L 1 L 2 L 2 T T T T P N P N R R R R o s . o s . T T a s s . a s s . P N T T R R P N R R L 1 L 1 L 2 L 2 T T T T P N P N R R R R o s . o s . T T a s s . a s s . P N T T R R P N R R L 1 L 1 L 2 L 2 T T T T P N P N R R R R o s . o s . T T a s s . a s s . P N T T R R P N R R L 1 L 1 L 2 L 2 T T T T P N P N R R R R o s . o s . T T a s s . a s s . P N T T R R P N R R C C W W C C W W C C W W C C W W Distance Metric Distance Metric Distance Metric Distance Metric 0.001 0.0001 Lunar Lander , BC Lunar Lander , BCQ Lunar Lander , IQL Lunar Lander , TD3PlusBC l e u a V ∆ 10.0 5.0 0.0 -5.0 -10.0 l e u a V ∆ 10.0 5.0 0.0 -5.0 -10.0 l e u a V ∆ 10.0 5.0 0.0 -5.0 -10.0 Bipedal Walker , BC Ant , BC 10.0 5.0 0.0 -5.0 -10.0 10.0 5.0 0.0 -5.0 -10.0 10.0 5.0 0.0 -5.0 -10.0 Bipedal Walker , BCQ Ant , BCQ 10.0 5.0 0.0 -5.0 -10.0 10.0 5.0 0.0 -5.0 -10.0 10.0 5.0 0.0 -5.0 -10.0 Bipedal Walker , IQL Ant , IQL 10.0 5.0 0.0 -5.0 -10.0 10.0 5.0 0.0 -5.0 -10.0 10.0 5.0 0.0 -5.0 -10.0 Bipedal Walker , TD3PlusBC Ant , TD3PlusBC L 1 L 1 L 2 L 2 T T T T P N P N R R R R o s . o s . T T a s s . a s s . P N T T R R P N R R L 1 L 1 L 2 L 2 T T T T P N P N R R R R o s . o s . T T a s s . a s s . P N T T R R P N R R L 1 L 1 L 2 L 2 T T T T P N P N R R R R o s . o s . T T a s s . a s s . P N T T R R P N R R L 1 L 1 L 2 L 2 T T T T P N P N R R R R o s . o s . T T a s s . a s s . P N T T R R P N R R C C W W C C W W C C W W C C W W Distance Metric Distance Metric Distance Metric Distance Metric 25 % of full trajectory 50 % of full trajectory Lunar Lander , BC Lunar Lander , BCQ Lunar Lander , IQL Lunar Lander , TD3PlusBC l e u a V ∆ 0 -20.0 -40.0 -60.0 -80.0 l e u a V ∆ 0 -20.0 -40.0 -60.0 -80.0 l e u a V ∆ 0 -20.0 -40.0 -60.0 -80.0 Bipedal Walker , BC Ant , BC 0 -20.0 -40.0 -60.0 -80.0 0 -20.0 -40.0 -60.0 -80.0 0 -20.0 -40.0 -60.0 -80.0 Bipedal Walker , BCQ Ant , BCQ 0 -20.0 -40.0 -60.0 -80.0 0 -20.0 -40.0 -60.0 -80.0 0 -20.0 -40.0 -60.0 -80.0 Bipedal Walker , IQL Ant , IQL 0 -20.0 -40.0 -60.0 -80.0 0 -20.0 -40.0 -60.0 -80.0 0 -20.0 -40.0 -60.0 -80.0 Bipedal Walker , TD3PlusBC Ant , TD3PlusBC L 1 L 1 L 2 L 2 T T T T P N P N R R R R o s . o s . T T a a s s . s s . P N T T R R P N R R L 1 L 1 L 2 L 2 T T T T P N P N R R R R o s . o s . T T a a s s . s s . P N T T R R P N R R L 1 L 1 L 2 L 2 T T T T P N P N R R R R o s . o s . T T a a s s . s s . P N T T R R P N R R L 1 L 1 L 2 L 2 T T T T P N P N R R R R o s . o s . T T a a s s . s s . P N T T R R P N R R C C W W C C W W C C W W C C W W Distance Metric Distance Metric Distance Metric Distance Metric sigma=0.01 sigma=0.1 TABLE V : The TPR and TNR results of ORL-AUDITOR against model ensemble ( K = 5 ) . The mean and standard deviation of TPR and TNR in each row represent the audit results for one combination of task and model by four distance metrics . Each pair of TPR and TNR is derived from the diagonal and non-diagonal values of the corresponding heatmap in Figure 14 ( Lunar Lander ) , Figure 15 ( Bipedal Walker ) , Figure 16 ( Ant ) , and Figure 17 ( Half Cheetah ) , which are supplementary to [ 13 ] . Task Name Lunar Lander Bipedal Walker Ant Half Cheetah Offline Model BC BCQ IQL BC BCQ IQL TPR 100.00±0.00 99.60±0.80 100.00±0.00 TD3PlusBC 100.00±0.00 100.00±0.00 100.00±0.00 100.00±0.00 TD3PlusBC 100.00±0.00 99.60±0.80 100.00±0.00 100.00±0.00 TD3PlusBC 99.60±0.80 85.00±25.98 91.00±15.59 90.00±12.81 TD3PlusBC 61.50±20.32 BC BCQ IQL BC BCQ IQL L1 Norm L2 Norm Cosine Distance Wasserstein Distance TNR 100.00±0.00 100.00±0.00 99.90±0.44 99.30±0.95 100.00±0.00 100.00±0.00 100.00±0.00 94.90±19.07 100.00±0.00 99.70±0.71 99.80±0.60 99.30±1.82 100.00±0.00 100.00±0.00 100.00±0.00 100.00±0.00 TPR 99.20±0.98 98.00±2.19 99.20±0.98 99.60±0.80 100.00±0.00 100.00±0.00 100.00±0.00 100.00±0.00 99.60±0.80 99.60±0.80 99.20±0.98 100.00±0.00 84.50±25.71 89.00±16.76 86.50±16.70 77.00±19.42 TNR 100.00±0.00 100.00±0.00 100.00±0.00 99.90±0.44 100.00±0.00 100.00±0.00 100.00±0.00 93.80±21.63 99.90±0.44 99.80±0.60 99.70±0.71 99.40±2.20 100.00±0.00 100.00±0.00 100.00±0.00 100.00±0.00 TPR 99.20±0.98 98.00±2.19 99.60±0.80 99.60±0.80 100.00±0.00 100.00±0.00 100.00±0.00 100.00±0.00 99.60±0.80 100.00±0.00 99.20±0.98 100.00±0.00 94.00±10.39 95.00±8.66 94.50±9.53 95.00±8.66 TNR 100.00±0.00 100.00±0.00 99.90±0.44 99.80±0.60 100.00±0.00 100.00±0.00 100.00±0.00 92.70±21.62 83.20±31.99 85.70±28.31 86.80±28.32 87.80±25.87 67.50±43.20 67.17±42.30 71.00±41.37 65.67±41.28 TPR 99.60±0.80 99.60±0.80 99.60±0.80 99.60±0.80 100.00±0.00 100.00±0.00 100.00±0.00 100.00±0.00 99.20±1.60 100.00±0.00 100.00±0.00 99.60±0.80 87.00±21.38 93.00±12.12 91.50±12.52 52.00±33.26 TNR 99.90±0.44 100.00±0.00 97.60±4.27 95.80±3.57 100.00±0.00 100.00±0.00 100.00±0.00 89.20±23.94 100.00±0.00 99.70±0.71 99.80±0.60 98.50±3.79 100.00±0.00 100.00±0.00 100.00±0.00 100.00±0.00 B . Action Distortion The suspect models may perturb the actions , i.e. , changing the original models ’ outputs , to conceal its training dataset in practice . The action distortion mechanism should be stealthy and can not be detected by the auditor easily . Considering that the DRL models are usually applied to real-world decision- making tasks , such as self-driving cars and industry automa- the natural distortion is often modeled as tion [ 44 ] , [ 28 ] , Gaussian noise . For example , thermal noise , which is caused by the random motion of electrons in a conductor , can be modeled as a Gaussian noise with a constant power spec- trum [ 2 ] . In addition , Gaussian noise is easy to manipulate mathematically . For ease of evaluating the effects of different distortion intensities , all dimensions of the models ’ action space are normalized into [ −1 , 1 ] . Then , we utilize Gaussian noise with mean ( µ = 0 ) and standard deviation ( σ = 0.1 ) and ( σ = 0.01 ) to represent the two levels of distortion . Setup . Figure 9 depicts the impact of “ with ” or “ without ” the action distortion . The information about the used offline DRL model and task is shown in each figure ’ s title . The x- axis indicates the four metrics , and the y-axis is the absolute value change . As a supplementary of [ 13 ] , the detailed results between every two datasets are in Figure 18 , Figure 19 ( Lunar Lander ) , Figure 20 , Figure 21 ( Bipedal Walker ) , Figure 22 , and Figure 23 ( Ant ) . Observations . We conclude the following observations based on the above results . 1 ) ORL-AUDITOR is able to resist the potential action distortion from the suspect model , especially with the Cosine metric . From Figure 9 , the TPR and TNR vary slightly across most of the settings with weak noise , where the maximum accuracy attenuation is within 3 % for Cosine distance . We speculate that Cosine distance has a noise suppression ability when calculating the inner product of two series of cumulative rewards . Also , the weak noise may facilitate the dataset auditing since it will move the negative samples farther away from the positive set . 2 ) ORL-AUDITOR with a single distance metric faces limitations for heavy distortion . The TPR of ORL-AUDITOR suffers an obvious decline with strong noise . Since the strong distortion thoroughly changes the distribution of the models ’ actions , the cumulative rewards of the suspect model trained on the target dataset are different from those of the auditor ’ s shadow models . In this case , the auditor can not identify the positive models from the negative just by a single kind of distance metric . From Figure 23 , Cosine distance is good at discriminating the positive models ( results in the diagonal ) , and Wasserstein distance is proper for the negative models ( results in the non-diagonal ) . Thus , for strong distortion , the combination of multiple distance metrics can enhance the auditing robustness of ORL-AUDITOR . In addition , we should note that the models ’ normal behavior is also destroyed by the strong distortion . For example , in Table XVIII , the noise induces the model performance of IQL to decrease up to 25 % , and the better the model ’ s quality , the more pronounced the performance drop . VII . RELATED WORK Membership and Dataset Inferences . To infer whether an individual data record was used to train the target model , Shokri et al . [ 57 ] proposed the first practical membership inference strategy by training a number of shadow classifiers to distinguish the target model ’ s outputs on members versus non- members of its training dataset . Since then , researchers have investigated membership inference in various systems , such as machine unlearning [ 10 ] , facial recognition systems [ 12 ] , and neural architecture search [ 29 ] . Liu et al . [ 41 ] presenting a first-of-its-kind holistic risk assessment of different inference 13 attacks against machine learning models . Maini et al . [ 43 ] introduced the definition of dataset inference and designed the first mechanism to identify whether a suspect model copy has private knowledge from the dataset . Compared with the existing works , ORL-AUDITOR is a well-designed solution built for the offline DRL scenes , which overcomes several new challenges . First , ORL-AUDITOR is a post-event mechanism that can be directly applied to the existing open-source datasets . Second , ORL-AUDITOR does not use any auxiliary datasets . Knowledge Extraction Against DRL . The DRL models learn from the interaction with the environment , which can be valuable information in some cases , e.g. , indoor robot navigation . Pan et al . [ 47 ] demonstrated such knowledge extraction vulnerabilities in DRL under various settings and proposed algorithms to infer floor plans from some trained Grid World navigation DRL models with LiDAR perception . For exacting the model functionality , Chen et al . [ 9 ] proposed the first method to acquire the approximation model from the victim DRL . They built a classifier to reveal the targeted black- box DRL model ’ s training algorithm family based only on its predicted actions and then leveraged state-of-the-art imitation learning techniques to replicate the model from the identi- fied algorithm family . Ono et al . [ 45 ] integrated differential privacy [ 72 ] , [ 69 ] , [ 63 ] into the distributed RL algorithm to defend the extraction . The local models report noisy gradients designed to satisfy local differential privacy [ 14 ] , [ 15 ] , [ 65 ] , [ 71 ] , i.e. , keeping the local information from being exploited by adversarial reverse engineering . Chen et al . [ 8 ] proposed a novel testing framework for deep learning copyright protection , which can be adjusted to detect the knowledge extraction against DRL . VIII . DISSCUSION Highlights of ORL-AUDITOR . 1 ) ORL-AUDITOR is the first approach to conduct trajectory-level dataset auditing for offline DRL models . 2 ) By conducting a comprehensive analysis of ORL-AUDITOR under different experimental settings , such as the shadow model ’ s amount , the significance level in hy- pothesis testing , the trajectory size , and the robustness against ensemble architecture and action distortion , we conclude some useful observations for adopting ORL-AUDITOR . 3 ) We apply ORL-AUDITOR to audit the models trained on the open-source datasets from Google and DeepMind . All TPR and TNR results are superior than 95 % , demonstrating ORL-AUDITOR is an effective and efficient strategy for the published datasets . Limitations and Future Work . Below , we discuss the limitations of ORL-AUDITOR and promising directions for further improvements . 1 ) From Appendix D , the accuracy of ORL-AUDITOR decreases when the significance level downs to 0.001 . Thus , it is interesting to enhance ORL-AUDITOR to satisfy stricter auditing demands in the future . 2 ) ORL- AUDITOR based on a single distance metric may not be suffi- ciently robust to strong distortion . Based on the observations in Section VI-B , integrating more distance metrics in the audit process may be a further promising direction . IX . CONCLUSION In this work , we propose a novel trajectory-level dataset auditing method for offline DRL models relying on the insight that cumulative rewards can serve as the dataset ’ s intrinsic fingerprint and exist in all models trained on the target dataset . Both the true positive rate and the true negative rate of ORL-AUDITOR exceed 90 % on four offline DRL models and three task combinations . We show that ORL-AUDITOR is an effective and efficient solution to protect the IP of the dataset owners through multiple experiments . By studying parameter settings about the number of shadow models , the significance level in hypothesis testing , and the trajectory size , we conclude several important observations for adopting ORL-AUDITOR in practice . The robustness evaluation demonstrates that ORL- AUDITOR can resist the defenses of the model ensemble and the action distortion of the suspect model . Integrating multiple distance metrics to improve the robustness of ORL- AUDITOR against action distortion is a promising direction for future work . Finally , we utilize the open-source datasets from Google [ 18 ] and DeepMind [ 26 ] to examine the practicality of ORL-AUDITOR , and show that ORL-AUDITOR behaves excellently on existing published datasets . ACKNOWLEDGMENT We would like to thank the anonymous reviewers for their constructive comments . We also thank Yanchao Sun for sharing her expertise in reinforcement learning . This work was partly supported by the National Key Research and Develop- ment Program of China under No . 2022YFB3102100 , NSFC under Grants 62088101 , 61833015 , 62103371 , U20A20159 , and the Fundamental Research Funds for the Central Univer- sities 226-2022-00107 , 226-2023-00111 . Min Chen was partly sponsored by the Helmholtz Association within the project “ Trustworthy Federated Data Analytics ” ( TFDA ) ( No . ZT-I- OO1 4 ) . Zhikun Zhang was supported by the CISPA-Stanford Center for Cybersecurity ( FKZ:13N1S0762 ) . REFERENCES [ 1 ] M. Abadi , A. Agarwal , P. Barham , E. Brevdo , Z. Chen , C. Citro , G. S. Corrado , A. Davis , J . Dean , M. Devin , S. Ghemawat , I. Goodfellow , A. Harp , G. Irving , M. Isard , Y. Jia , R. Jozefowicz , L. Kaiser , M. Kudlur , J. Levenberg , D. Man´e , R. Monga , S. Moore , D. Murray , C. Olah , M. Schuster , J. Shlens , B. Steiner , I. Sutskever , K. Talwar , P. Tucker , V. Vanhoucke , V. Vasudevan , F. Vi´egas , O. Vinyals , P. Warden , M. Wat- tenberg , M. Wicke , Y. Yu , and X. Zheng . TensorFlow : Large-Scale Ma- chine Learning on Heterogeneous Systems . https : //www.tensorflow.org/ , 2015 . [ 2 ] N. AlHinai . Introduction to Biomedical Signal Processing and Artificial Intelligence . In Biomedical Signal Processing and Artificial Intelligence in Healthcare , Developments in Biomedical Engineering and Bioelec- tronics , pages 1–28 . Elsevier , 2020 . [ 3 ] S. Amarjyoti . Deep Reinforcement Learning for Robotic Manipulation - The State of the Art . CoRR abs/1701.08878 , 2017 . [ 4 ] C. Beattie , J . Z. Leibo , D. Teplyashin , T. Ward , M. Wainwright , H. K¨uttler , A. Lefrancq , S. Green , V. Vald´es , A. Sadik , J. Schrittwieser , K. Anderson , S. York , M. Cant , A. Cain , A. Bolton , S. Gaffney , H. King , D. Hassabis , S. Legg , and S. Petersen . DeepMind Lab . CoRR , abs/1612.03801 , 2016 . [ 5 ] Biscom . Employee Departure Creates Gaping Security Hole . https : hole-says-new-data , 2021 . [ 6 ] F. Boenisch . A Systematic Review on Model Watermarking for Neural Networks . Frontiers Big Data , 4:729663 , 2021 . 14 [ 7 ] G. Brockman , V. Cheung , L. Pettersson , J. Schneider , J. Schulman , J. Tang , and W. Zaremba . OpenAI Gym . CoRR , abs/1606.01540 , 2016 . J. Chen , J. Wang , T. Peng , Y . Sun , P. Cheng , S. Ji , X. Ma , B. Li , and D. Song . Copy , Right ? a Testing Framework for Copyright Protection of Deep Learning Models . In IEEE S & P , pages 824–841 , 2022 . [ 8 ] Stealing Deep [ 9 ] K. Chen , S. Guo , T. Zhang , X. Xie , and Y. Liu . In ACM Asia Reinforcement Learning Models for Fun and Profit . Conference on Computer and Communications Security ( ASIACCS ) , pages 307–319 , 2021 . [ 30 ] [ 31 ] I. Ilahi , M. Usama , J. Qadir , M. U. Janjua , A. I. Al-Fuqaha , D. T. Hoang , and D. Niyato . Challenges and Countermeasures for Adversarial Attacks on Deep Reinforcement Learning . CoRR abs/2001.09684 , 2020 . I. Jarin and B. Eshete . MIAShield : Defending Membership Inference Attacks via Preemptive Exclusion of Members . In Privacy Enhancing Technologies Symposium , pages 400–416 , 2023 . [ 32 ] R. Kidambi , A. Rajeswaran , P. Netrapalli , and T. Joachims . MOReL : Model-Based Offline Reinforcement Learning . In NeurIPS , 2020 . [ 33 ] D. P. Kingma and M. Welling . Auto-Encoding Variational Bayes . In [ 10 ] M. Chen , Z. Zhang , T. Wang , M. Backes , M. Humbert , and Y. Zhang . ICLR , 2014 . When Machine Unlearning Jeopardize Privacy . In ACM CCS , 2021 . [ 11 ] M. Chen , Z. Zhang , T. Wang , M. Backes , M. Humbert , and Y. Zhang . Graph Unlearning . In ACM CCS , 2022 . [ 12 ] M. Chen , Z. Zhang , T. Wang , M. Backes , and Y. Zhang . FACE- AUDITOR : Data Auditing in Facial Recognition Systems . In USENIX Security , 2023 . [ 13 ] L. Du , M. Chen , M. Sun , S. Ji , P. Cheng , J. Chen , and Z. Zhang . ORL- Auditor : Dataset Auditing in Offline Deep Reinforcement Learning . In Network and Distributed System Security Symposium ( NDSS ) . Internet Society , 2024 . [ 14 ] L. Du , Z. Zhang , S. Bai , C. Liu , S. Ji , P. Cheng , and J. Chen . AHEAD : Adaptive Hierarchical Decomposition for Range Query under Local Differential Privacy . In ACM CCS , 2021 . [ 15 ] Y . Du , Y. Hu , Z. Zhang , Z. Fang , L. Chen , B. Zheng , and Y. Gao . LDPTrace : Locally Differentially Private Trajectory Synthesis . In VLDB , 2023 . [ 16 ] A. Dziedzic , H. Duan , M. A. Kaleem , N. Dhawan , J. Guan , Y. Cattan , F. Boenisch , and N. Papernot . Dataset Inference for Self-Supervised Models . CoRR , abs/2209.09024 , 2022 . [ 17 ] A. R. Fayjie , S. Hossain , D. Oualid , and D. Lee . Driverless Car : Autonomous Driving Using Deep Reinforcement Learning in Urban Environment . In International Conference on Ubiquitous Robots ( UR ) , pages 896–901 , 2018 . [ 18 ] J. Fu , A. Kumar , O. Nachum , G. Tucker , and S. Levine . D4RL : Datasets for Deep Data-Driven Reinforcement Learning . CoRR , abs/2004.07219 , 2020 . [ 19 ] S. Fujimoto , E. Conti , M. Ghavamzadeh , and J. Pineau . Bench- marking Batch Deep Reinforcement Learning Algorithms . CoRR , abs/1910.01708 , 2019 . [ 20 ] S. Fujimoto and S. S. Gu . A Minimalist Approach to Offline Rein- forcement Learning . In NeurIPS , pages 20132–20145 , 2021 . [ 21 ] S. Fujimoto , D. Meger , and D. Precup . Off-Policy Deep Reinforcement Learning without Exploration . In ICML , pages 2052–2062 , 2019 . [ 22 ] S. Fujimoto , H. van Hoof , and D. Meger . Addressing Function Approximation Error in Actor-Critic Methods . In ICML , pages 1582– 1591 , 2018 . [ 23 ] M. Gomrokchi , S. Amin , H. Aboutalebi , A. Wong , and D. Precup . Where Did You Learn That From ? Surprising Effectiveness of Mem- bership Inference Attacks Against Temporally Correlated Data in Deep Reinforcement Learning . CoRR , abs/2109.03975 , 2021 . [ 24 ] M. Gomrokchi , S. Main , S. Amin , and D. Precup . PrivAttack : A Membership Inference Attack Framework Against Deep Reinforcement Learning Agents . In NeurlPS Workshop , 2020 . [ 25 ] F. E. Grubbs . Sample Criteria for Testing Outlying Observations . The Annals of Mathematical Statistics , pages 27–58 , 1950 . [ 26 ] C¸ . G¨ulc¸ehre , Z. Wang , A. Novikov , T. Paine , S. G. Colmenarejo , K. Zolna , R. Agarwal , J. Merel , D. J. Mankowitz , C. Paduraru , G. Dulac-Arnold , J. Li , M. Norouzi , M. Hoffman , N. Heess , and N. de Freitas . RL Unplugged : A Collection of Benchmarks for Offline Reinforcement Learning . In NeurIPS 2020 , 2020 . [ 27 ] N. G¨urtler , S. Blaes , P. Kolev , F. Widmaier , M. Wuthrich , S. Bauer , B. Sch¨olkopf , and G. Martius . Benchmarking Offline Reinforcement Learning on Real-Robot Hardware . In ICLR , 2023 . [ 28 ] S. He , K. Shi , C. Liu , B. Guo , J. Chen , and Z. Shi . Collaborative Sensing in Internet of Things : A Comprehensive Survey . IEEE Communications Surveys & Tutorials , 2022 . [ 29 ] H. Huang , Z. Zhang , Y. Shen , M. Backes , Q. Li , and Y. Zhang . On the Privacy Risks of Cell-Based NAS Architectures . In ACM CCS , 2022 . [ 34 ] P. Kiourti , K. Wardega , J. Susmit , and W. Li . TrojDRL : Evaluation of [ 35 ] Backdoor Attacks on Deep Reinforcement Learning . In DAC , 2020 . I. Kostrikov , A. Nair , and S. Levine . Offline Reinforcement Learning with Implicit Q-Learning . In ICLR , 2022 . [ 36 ] S. Lange , T. Gabel , and M. A. Riedmiller . Batch Reinforcement In Reinforcement Learning , volume 12 of Adaptation , Learning . Learning , and Optimization , pages 45–73 . Springer , 2012 . [ 37 ] S. Levine , A. Kumar , G. Tucker , and J. Fu . Offline Reinforcement Learning : Tutorial , Review , and Perspectives on Open Problems . CoRR , abs/2005.01643 , 2020 . [ 38 ] Y. Li , Y. Bai , Y. Jiang , Y. Yang , S. Xia , and B. Li . Untargeted Backdoor Watermark : Towards Harmless and Stealthy Dataset Copyright Protec- tion . CoRR , abs/2210.00875 , 2022 . [ 39 ] Y. Li , Z. Zhang , J. Bai , B. Wu , Y. Jiang , and S. Xia . Open-sourced Dataset Protection via Backdoor Watermarking . CoRR , abs/2010.05821 , 2020 . [ 40 ] Z. Lin , J. Gehring , V. Khalidov , and G. Synnaeve . STARDATA : A StarCraft AI Research Dataset . CoRR , abs/1708.02139 , 2017 . [ 41 ] Y. Liu , R. Wen , X . He , A. Salem , Z. Zhang , M. Backes , E. D. Cristofaro , M. Fritz , and Y. Zhang . ML-Doctor : Holistic Risk Assessment of In USENIX Inference Attacks Against Machine Learning Models . Security , 2022 . [ 42 ] M. Lopez-Martin , B. Carro , and A. Sanchez-Esguevillas . Application of Deep Reinforcement Learning to Intrusion Detection for Supervised Problems . Expert Systems with Applications , 141:112963 , 2020 . [ 43 ] P. Maini , M. Yaghini , and N. Papernot . Dataset inference : Ownership resolution in machine learning . In ICLR , 2021 . [ 44 ] D. Mwiti . 10 Real-Life Applications of Reinforcement Learning . https : , 2021 . [ 45 ] H. Ono and T. Takahashi . Locally Private Distributed Reinforcement Learning . CoRR abs/2001.11718 , 2020 . [ 46 ] T. L. Paine , C. Paduraru , A. Michi , C¸ . G¨ulc¸ehre , K. Zolna , A. Novikov , Z. Wang , and N. de Freitas . Hyperparameter Selection for Offline Reinforcement Learning . CoRR , abs/2007.09055 , 2020 . [ 47 ] X. Pan , W. Wang , X. Zhang , B. Li , J. Yi , and D. Song . How You Act Tells a Lot : Privacy-Leaking Attack on Deep Reinforcement Learning . In AAMAS , pages 368–376 , 2019 . [ 48 ] A. Pattanaik , Z. Tang , S. Liu , G. Bommannan , and G. Chowdhary . Robust Deep Reinforcement Learning with Adversarial Attacks . In AAMAS , pages 2040–2042 , 2018 . [ 49 ] D. Pomerleau . ALVINN : An Autonomous Land Vehicle in a Neural Network . In NeurIPS , pages 305–313 , 1988 . [ 50 ] R. F. Prudencio , M. R. O . A. M´aximo , and E. L. Colombini . A Survey on Offline Reinforcement Learning : Taxonomy , Review , and Open Problems . CoRR , abs/2203.01387 , 2022 . [ 51 ] H. Pu , L. He , P. Cheng , M. Sun , and J. Chen . Security of Industrial Robots : Vulnerabilities , Attacks , and Mitigations . IEEE Network , 2022 . [ 52 ] R. Qin , S. Gao , X. Zhang , Z. Xu , S. Huang , Z. Li , W. Zhang , and Y. Yu . NeoRL : A Near Real-World Benchmark for Offline Reinforcement Learning . CoRR , abs/2102.00714 , 2021 . [ 53 ] A. Raffin , A. Hill , A. Gleave , A. Kanervisto , M. Ernestus , and Stable-Baselines3 : Reliable Reinforcement Learning N. Dormann . Implementations . Journal of Machine Learning Research , 22 ( 268 ) :1–8 , 2021 . [ 54 ] Y. Rubner , C. Tomasi , and L. J. Guibas . A Metric for Distributions with Applications to Image Databases . In ICCV , pages 59–66 , 1998 . [ 55 ] T. Rupprecht and Y. Wang . A Survey for Deep Reinforcement 15 Learning in Markovian Cyber-physical Systems : Common Problems and Solutions . Neural Networks , 153:13–36 , 2022 . [ 56 ] T. Seno and M. Imai . d3rlpy : An Offline Deep Reinforcement Learning Library . Journal of Machine Learning Research , 23 ( 315 ) :1–20 , 2022 . [ 57 ] R. Shokri , M. Stronati , C. Song , and V. Shmatikov . Membership Inference Attacks Against Machine Learning Models . In IEEE S & P , pages 3–18 , 2017 . [ 58 ] D. Silver , G. Lever , N. Heess , T. Degris , D. Wierstra , and M. A. Riedmiller . Deterministic Policy Gradient Algorithms . In ICML , pages 387–395 , 2014 . [ 59 ] M. A. Stephens . EDF Statistics for Goodness of Fit and Some Compar- isons . Journal of the American statistical Association , 69 ( 347 ) :730–737 , 1974 . [ 60 ] X. Tang , S. Mahloujifar , L. Song , V. Shejwalkar , M. Nasr , A. Houmansadr , and P. Mittal . Mitigating Membership Inference Attacks by Self-Distillation Through a Novel Ensemble Architecture . In USENIX Security , pages 1433–1450 , 2022 . [ 61 ] Tessian . How the Great Resignation is Creating More Security Chal- lenges . https : ting-more-security-challenges/ , 2021 . [ 62 ] L. Van der Maaten and G. Hinton . Visualizing Data Using t-SNE . Journal of machine learning research , 9 ( 11 ) , 2008 . [ 63 ] H. Wang , Z. Zhang , T. Wang , S. He , M. Backes , J. Chen , and Y. Zhang . PrivTrace : Differentially Private Trajectory Synthesis by Adaptive Markov Model . In USENIX Security , 2023 . [ 64 ] L. Wang , Z. Javed , X. Wu , W. Guo , X. Xing , and D. Song . BACK- DOORL : Backdoor Attack against Competitive Reinforcement Learn- ing . In IJCAI , pages 3699–3705 , 2021 . [ 65 ] T. Wang , J. Q. Chen , Z. Zhang , D. Su , Y. Cheng , Z. Li , N. Li , and S. Jha . Continuous Release of Data Streams under both Centralized and Local Differential Privacy . In ACM CCS , 2021 . [ 66 ] Y. Wang , E. Sarkar , W. Li , M. Maniatakos , and S. E. Jabari . Stop- and-Go : Exploring Backdoor Attacks on Deep Reinforcement Learning- Based Traffic Congestion Control Systems . IEEE TIFS , 16:4772–4787 , 2021 . [ 67 ] Z. Yang , L. He , H. Yu , C. Zhao , P. Cheng , and J. Chen . Detecting PLC Intrusions using Control Invariants . IEEE IoT J , 9 ( 12 ) :9934–9947 , 2022 . [ 68 ] T. Yu , A. Kumar , R. Rafailov , A. Rajeswaran , S. Levine , and C. Finn . COMBO : Conservative Offline Model-Based Policy Optimization . In NeurIPS , pages 28954–28967 , 2021 . [ 69 ] Q. Yuan , Z. Zhang , L. Du , M. Chen , P. Cheng , and M. Sun . PrivGraph : Differentially Private Graph Data Publication by Exploiting Community Information . In USENIX Security , 2023 . [ 70 ] L. Zeng , M. Sun , X. Wan , Z. Zhang , R. Deng , and Y. Xu . Physics- constrained Vulnerability Assessment of Deep Reinforcement Learning- based SCOPF . IEEE TPS , 2022 . [ 71 ] Z. Zhang , T. Wang , N. Li , S. He , and J. Chen . CALM : Consistent Adaptive Local Marginal for Marginal Release under Local Differential Privacy . In ACM CCS , 2018 . [ 72 ] Z. Zhang , T. Wang , N. Li , J. Honorio , M. Backes , S. He , J. Chen , and Y. Zhang . PrivSyn : Differentially Private Data Synthesis . In USENIX Security , 2021 . A . The Behavior Similarity of Models APPENDIX In Figure 10 , we provide the behavior similarity of the offline RL models trained on the datasets in Table XV . Taking the Bipedal Walker task as an example , the dataset “ 0841 ” is regarded as the target dataset , and the other four are the public datasets . We observe that the behavior similarity of the RL models waves heavily among the different public training data . If the auditor adopts the dataset “ 1203 ” as the public training data , the auditor likely misclassifies the RL models trained on the other three public datasets into the bootleg models . In addition , the behavior similarity is also affected by different offline RL frameworks , i.e. , BC [ 49 ] , BCQ [ 21 ] , [ 19 ] , IQL [ 35 ] , and TD3PlusBC [ 20 ] ( detailed in Section II-B ) . B . The Details of Tasks Lunar Lander ( continuous version ) . The LunarLander task is to smoothly land a spaceship between two flags on the target pad . The landing pad is always at coordinates ( 0,0 ) . The ship has three throttles ; one throttle points downward ( the main engine ) and the other two points in the left and right direction ( the left and right engines ) . The observation is an 8-dimensional vector : the coordinates of the lander in the x- axis and y-axis , its linear velocities in the x-axis and y-axis , its angle , its angular velocity , and two booleans that represent whether each leg is in contact with the ground or not . The action is two real values ranging in [ −1 , 1 ] . The first dimension controls the main engine , where the engine is off when the value is in [ −1 , 0 ) and increases from 50 % to 100 % throttle when the value rises from 0 to 1 . The other two points are controlled by the second value , where the spaceship fires the left engine if the value in [ −1.0 , −0.5 ) , fires the right engine if the value in [ 0.5 , 1 ) , and shuts down both engines if the value in [ −0.5 , 0.5 ] . The reward for moving from the top of the screen to the landing pad and zero speed is about 140 points . Landing outside the landing pad is possible . Thus , the player loses the terminal reward if the lander moves away from the landing pad . The player gets 10 additional points for each leg touching the ground . Firing the main engine is -0.3 points in each frame . The episode finishes if the lander crashes or lands smoothly , receiving -100 or 100 points . Bipedal Walker . The Bipedal Walker task is to operate a 4-joint walker robot to move forward as fast as possible . The robot is made of a hull and two legs . Each leg has 2 joints at both the hip and knee . The observation of the task includes eight continuous physical variables , i.e. , hull angle speed , angular velocity , horizontal speed , vertical speed , the position of joints and joints angular speed , legs contact with ground , and 10 lidar rangefinder measurements . Actions are motor speed values in the [ -1 , 1 ] range for each of the 4 joints at both hips and knees . The walker starts standing at the left end of the terrain with the hull horizontal , and both legs in the same position with a slight knee angle . The reward is given for moving forward , totaling 300+ points up to the far end . If the robot falls , it gets -100 . Applying motor torque costs a small amount of points . A more optimal model will get a better score . The episode will terminate if the hull gets in contact with the ground or the walker exceeds the right end of the terrain length . Ant . In this task , the player manipulates a 3D robot ( ant ) , which consists of one torso ( free rotational body ) with four legs attached to it , with each leg having two links , to move in the forward ( right ) direction . The observation contains positional values of different body parts of the ant , followed by the velocities of those individual parts ( their derivatives ) , with all the positions ordered before all the velocities . By default , an observation is a vector with shape ( 111 , ) where the elements correspond to the following : position ( 1-dim ) , angles ( 12- dim ) , velocities ( 14-dim ) , and the information about the contact forces ( 84-dim ) . The player can apply torques on the eight hinges connecting the two links of each leg and the torso 16 Fig . 10 : Models ’ behavior similarity measured by ℓ1 Norm , ℓ2 Norm , Cosine Distance , and Wasserstein Distance . From Table XV , we use the first dataset of each task as the private training data and the remaining four datasets are the public training data . For each plot , the x-axis displays the four public training data , and the y-axis shows the absolute fluctuating values of the behavior similarity between the models trained on the private dataset and the public datasets . BC , BCQ , IQL , and TD3PlusBC are abbreviations for different offline RL frameworks . ( nine parts and eight hinges ) . Thus , the action space is an 8-dim continuous vector representing the torques applied at the hinge joints . The reward of the “ Ant ” task consists of four parts : healthy reward , forward reward , control cost , and contact cost . The total reward returned is reward = healthy reward + forward reward - control cost - contact cost . The task ends when either the ant state is unhealthy , or the episode duration reaches 1000 timesteps . C. Impact of Shadow Models ’ Amount We investigate the relationship between the number of shadow models and the audit accuracy . Setup . We change the shadow models ’ amount to 9 and 21 with the other settings the same as Section V-B . Figure 6 shows the value change of TPR and TNR compared with that of 15 shadow models . Each figure ’ s title illustrates the settings of the model and the task , the x-axis indicates the four metrics , and the y-axis is the absolute value change . Also , we provide the detailed results in Table VIII ( 9 Shadow Models ) and Table IX ( 21 Shadow Models ) . Observations . From Figure 6 , we have the following obser- vations . 1 ) The audit accuracy increases with a larger amount of shadow models . Since the values of shadow models are the multi-sampling of the true value Q ( s , a ) of the dataset , the mean and standard deviation will be more precise with more shadow models . For example , ORL-AUDITOR suffers an obvious TPR decline ( more than 30 % ) with 9 shadow models . Since the insufficient knowledge about the diversity of models trained on the target dataset , the auditor easily misclassifies the positive models to the negative group . 2 ) There exists a saturation point for audit accuracy with the expansion of shadow models . When the shadow models ’ amount rises from 15 to 21 , the TPR usually increases since the auditor observes more possible cumulative rewards originating from the model trained on the target dataset . We should note that the value changes slightly in most plots , meaning that similar cumulative rewards appear in the shadow model set , and the diversity does not increase significantly compared to that of 15 shadow models . Therefore , excessive shadow models are unnecessary , and the auditor needs to burden more training overhead . D. Impact of Significance Level The significance level represents the auditor ’ s confidence in the audit results . In Section V-B , we adopt the significance level α = 0.01 , meaning that the auditor has 99 % confidence in the judgments made . Generally speaking , the significance level represents the maximum audit capacity of ORL-AUDITOR instead of a hyperparameter setting since it is an audit re- quirement by the dataset owner . Setup . We demand the auditor to output a more confident judgment , where the error possibility should be limited to 1‰ and 0.1‰ , i.e. , α = 0.001 and α = 0.0001 . Figure 7 shows the value change of TPR and TNR compared with that when significance level α = 0.01 . The used offline DRL model and task is shown in each figure ’ s title . The x-axis indicates the four metrics and the y-axis is the absolute value change . 17 l e u a V l e u a V 1400.0 1200.0 1000.0 800.0 600.0 400.0 200.0 0.0 4000.0 3500.0 3000.0 2500.0 2000.0 1500.0 1000.0 500.0 0.0 4000.0 3000.0 2000.0 l e u a V 1000.0 0.0 l e u a V 5000.0 4000.0 3000.0 2000.0 1000.0 0.0 Lunar Lander , L1 Norm Lunar Lander , L2 Norm Lunar Lander , Cosine Distance BC BCQ IQL TD3PlusBC 2094 4496 6518 9906 Bipedal Walker , L1 Norm BC BCQ IQL TD3PlusBC 1203 2110 3813 Ant , L1 Norm 6558 BC BCQ IQL TD3PlusBC 3569 4603 5766 7490 Half Cheetah , L1 Norm BC BCQ IQL TD3PlusBC medium random Dataset Name rluply 1400.0 1200.0 1000.0 800.0 600.0 400.0 200.0 0.0 5000.0 4000.0 3000.0 2000.0 1000.0 0.0 4000.0 3500.0 3000.0 2500.0 2000.0 1500.0 1000.0 500.0 0.0 8000.0 7000.0 6000.0 5000.0 4000.0 3000.0 2000.0 1000.0 0.0 BC BCQ IQL TD3PlusBC 2094 4496 6518 9906 Bipedal Walker , L2 Norm BC BCQ IQL TD3PlusBC 1203 2110 3813 Ant , L2 Norm 6558 BC BCQ IQL TD3PlusBC 3569 4603 5766 7490 Half Cheetah , L2 Norm BC BCQ IQL TD3PlusBC medium random Dataset Name rluply 1.2 1.0 0.8 0.6 0.4 0.2 0.0 1.2 1.0 0.8 0.6 0.4 0.2 0.0 0.8 0.6 0.4 0.2 0.0 1.2 1.0 0.8 0.6 0.4 0.2 0.0 BC BCQ IQL TD3PlusBC 2094 4496 6518 9906 Bipedal Walker , Cosine Distance BC BCQ IQL TD3PlusBC 1203 2110 3813 6558 Ant , Cosine Distance BC BCQ IQL TD3PlusBC 3569 4603 5766 7490 Half Cheetah , Cosine Distance BC BCQ IQL TD3PlusBC medium random Dataset Name rluply 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0.0 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 0.2 0.18 0.15 0.12 0.1 0.08 0.05 0.02 0.0 0.6 0.5 0.4 0.3 0.2 0.1 0.0 Lunar Lander , Wasserstein Distance BC BCQ IQL TD3PlusBC 2094 9906 Bipedal Walker , Wasserstein Distance 4496 6518 BC BCQ IQL TD3PlusBC 1203 2110 3813 6558 Ant , Wasserstein Distance BC BCQ IQL TD3PlusBC 3569 7490 Half Cheetah , Wasserstein Distance 4603 5766 BC BCQ IQL TD3PlusBC medium random Dataset Name rluply 2 ) It should be noticed that a small trajectory size achieves better results under some tasks . For the Ant task , ORL- AUDITOR auditing with 25 % of the full length obtains at most 7 % promotion on the TNR results . Based on the analysis of [ 46 ] , the front states of each trajectory are able to reflect more behavioral information of the model . Thus , in this case , a shorter trajectory truncates the rear state-action pairs , which might be unimportant or even weaken the significance of the hypothesis testing . Exploring effective data auditing with shorter trajectory sizes or even using only the first state of each trajectory would be an interesting future direction . F. Additional Results As a supplementary of [ 13 ] , we provide additional results about ORL-AUDITOR . For ease of reading , we summarize the main figures and tables in Table VI . The detailed results between every two datasets are in Table X ( α = 0.001 ) and Table XI ( α = 0.0001 ) . Observations . From Figure 7 , we have the following obser- vations . 1 ) For a complicated task , we recommend the auditor to select a large significance level for ORL-AUDITOR . The task ’ s complexity affects the minimum significance level of ORL-AUDITOR . For example , TPR and TNP change a little on the Lunar Lander task when the significance level reduces to 0.001 , while they highly shrink on the Ant task . From Table I , Ant ’ s state and action space are larger than that of Lunar Lander . When the auditor leverages the critic model to compress each model ’ s state and action pair into a scalar , the deviation between Qi j ( recalling Figure 4 ) on the Ant task is more imperceptible . j and Qs 2 ) For the suspect models with low performance , ORL- AUDITOR should adopt a large significance level to guarantee audit accuracy . For instance , in the figure titled with “ Bipedal Walker , TD3PlusBC ” , all TNR results from four distance metrics decrease when α reduces to 0.001 and 0.0001 . From Table XIX , most of the TD3PlusBC models ’ performance on the Bipedal Walker task is around -100 , meaning that the TD3PlusBC models do not fully master the knowledge of the dataset . Thus , the dataset features reflected in their behavior are ambiguous , which weakens the difference between positive and negative samples . Meanwhile , the confidence interval , i.e. , ∆ in Figure 1 , expands with a lower significance level . For the above two reasons , the TNR results of the TD3PlusBC models on the Bipedal Walker task drop more than 10 % compared with these when α = 0.01 . From the above analysis , α = 0.01 is a safe bound of ORL-AUDITOR , and a lower α may break through the capability boundary of ORL-AUDITOR , inducing the auditor to misclassify the negative model to the positive set . E. Impact of Trajectory Size We investigate the relationship between the trajectory size and audit accuracy . In Section V-B , we adopt the full-length trajectory , meaning that the auditor utilizes all states of each trajectory to query the suspect model and obtains the corre- sponding actions to conduct the dataset audit . Setup . We change the trajectory size to 25 % and 50 % of the full length with the other settings the same as Section V-B . Figure 8 shows the value change of TPR and TNR compared with that of the full-length trajectory . Each figure ’ s title illus- trates the settings of the model and the task , the x-axis indicates the four metrics , and the y-axis is the absolute value change . Also , we provide the detailed results in Table XII ( 25 % ) and Table XIII ( 50 % ) . Observation . From Figure 8 , we have the following observa- tions . 1 ) ORL-AUDITOR tends to achieve higher accuracy with a larger trajectory size . Since the predicted cumulative rewards of state-action pairs from the critic model are the audit basis , a longer trajectory collects more actions from the suspect model to enhance the significance of hypothesis testing . For example , the TNP results decrease at most 13 % when ORL-AUDITOR only leverages 25 % of the trajectory . 18 TABLE VI : The roadmap of the main figures and tables . Information Overview of tasks Online DRL models Involved Content Section V-A Section V-A Name Table I Table XIV Offline Datasets Section V-A Table XV Offline DRL models Section V Overall audit performance Section V-B Impact of shadow models ’ amount Appendix C Impact of significance level Appendix D Impact of trajectory size Appendix E Real-world application Section V-E Robustness : ensemble architecture Section VI-A Robustness : perturbing models output Section VI-B Table XVI Table XVII Table XVIII Table XIX Table IV Table VII Figure 11 Figure 12 Figure 13 Figure 6 Table VIII Table IX Figure 7 Table X Table XI Figure 8 Table XII Table XIII Table XXII Table XX Table XXI Table V Figure 14 Figure 16 Figure 17 Figure 9 Figure 18 Figure 19 Figure 20 Figure 21 Figure 22 Figure 23 Description The state shape and the action shape of each task . The performance of the used online models for collecting the offline datasets . The name , the number of trajectories , and the length of trajectory for each offline dataset . The offline models ’ performance with or without defense against ORL- AUDITOR : normal performance ( without defense ) , defended by model ensemble , and defended by perturbing models ’ output . The true positive rate ( TPR ) and true negative rate ( TNR ) results based on Grubbs ’ test and 3σ principle . The change values of TPR and TNR when the number of shadow models varies to 9 and 21 compared to the default 15 shadow models . The change value of TPR and TNR when the significance level ( σ ) varies to 0.001 and 0.0001 compared to the default 0.01 . The change value of TPR and TNR when the trajectory size varies to 25 % and 50 % compared to the default 100 % ( full length ) . The TPR and TNR results on the Half Cheetah datasets , which are published by DeepMind and Google separately . The TPR and TNR results of ORL-AUDITOR against model ensemble ( K = 5 ) . The TPR and TNR results of ORL-AUDITOR against models ’ action distortion . TABLE VII : As a supplementary of [ 13 ] , we provide the TPR and TNR results of ORL-AUDITOR based on 3σ principle . The mean and standard deviation of TPR and TNR in each row represent the audit results for one combination of task and model by four distance metrics . Bold indicates the highest sum of TPR and TNR , i.e. , accuracy , in a row . Task Name Offline Model Lunar Lander Bipedal Walker Ant BC BCQ IQL TD3PlusBC BC BCQ IQL TD3PlusBC BC BCQ IQL TD3PlusBC L1 Norm L2 Norm Cosine Distance Wasserstein Distance TPR 96.53±1.36 96.13±3.01 97.20±3.24 95.60±4.39 96.56±4.27 96.67±4.20 94.33±7.45 97.00±4.46 90.67±5.30 90.40±8.68 90.67±6.93 95.62±5.19 TNR 100.00±0.00 100.00±0.00 99.97±0.28 99.54±2.53 100.00±0.00 100.00±0.00 100.00±0.00 99.90±1.16 100.00±0.00 99.96±0.42 100.00±0.00 99.74±1.79 TPR 95.47±2.81 94.80±3.18 96.27±2.44 92.80±5.07 95.78±4.58 94.78±7.43 93.78±7.25 94.11±8.63 93.33±4.62 94.13±3.83 89.60±3.99 94.12±5.03 TNR 100.00±0.00 100.00±0.00 100.00±0.00 99.91±0.47 100.00±0.00 100.00±0.00 100.00±0.00 97.80±12.09 100.00±0.00 99.94±0.56 100.00±0.00 99.35±2.58 TPR 95.73±2.58 94.67±2.92 96.53±2.40 93.33±5.40 98.33±2.50 98.67±1.63 98.89±2.17 95.33±6.66 99.20±0.88 98.00±2.00 97.20±3.89 99.08±1.54 TNR 100.00±0.00 100.00±0.00 100.00±0.00 99.93±0.40 100.00±0.00 100.00±0.00 100.00±0.00 97.78±12.19 88.00±27.55 88.47±26.83 88.30±27.38 88.52±26.25 TPR 96.13±2.02 96.40±2.92 96.53±4.14 96.67±2.88 97.22±4.05 97.11±3.69 94.00±9.45 96.44±5.95 95.20±2.99 93.47±6.81 91.20±9.03 97.74±2.66 TNR 100.00±0.00 99.95±0.36 98.90±3.33 96.86±7.24 100.00±0.00 100.00±0.00 100.00±0.00 93.87±19.73 99.99±0.07 99.95±0.49 100.00±0.00 99.60±2.13 19 Fig . 11 : The audit accuracy between every two Lunar Lander datasets . The caption of each plot demonstrates the offline DRL model ’ s type , the task , and the distance metric . The x labels are the names of datasets to be audited , i.e. , the target datasets . The y labels are the names of datasets the suspect models learned , i.e. , the actual datasets . Thus , the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset , i.e. , TPR , and the non-diagonal values are the TNR results . The positions without value mean 100 % accuracy . Fig . 12 : The audit accuracy between every two Bipedal Walker datasets . The caption of each plot demonstrates the offline DRL model ’ s type , the task , and the distance metric . The x labels are the names of datasets to be audited , i.e. , the target datasets . The y labels are the names of datasets the suspect models learned , i.e. , the actual datasets . Thus , the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset , i.e. , TPR , and the non-diagonal values are the TNR results . The positions without value mean 100 % accuracy . 20 Lunar Lander , BC , L1 Norm Lunar Lander , BC , L2 Norm Lunar Lander , BC , Cosine Distance 98.5 98.8 99.9 2094 4496 Lunar Lander , BCQ , L1 Norm 6518 96.9 1171 95.6 98.8 9906 95.9 97.2 96.8 95.7 97.2 98.1 98.1 2094 4496 Lunar Lander , BCQ , L2 Norm 6518 1171 2094 4496 Lunar Lander , BCQ , Cosine Distance 6518 96.8 9906 95.5 99.8 Lunar Lander , BC , Wasserstein Distance 99.5 99.8 99.9 98.1 99.9 99.3 97.2 98.7 99.9 2094 98.5 9906 4496 1171 Lunar Lander , BCQ , Wasserstein Distance 99.9 99.7 97.7 6518 1171 99.1 2094 4496 6518 9906 1171 1171 98.5 1171 2094 4496 6518 9906 1171 2094 4496 6518 9906 96.4 99.6 94.3 96.9 97.7 95.7 2094 4496 Lunar Lander , IQL , L1 Norm 6518 99.2 9906 1171 2094 4496 Lunar Lander , IQL , L2 Norm 6518 99.5 99.7 99.1 99.9 96.1 99.6 94.4 96.7 9906 97.6 9906 99.5 98.5 2094 1171 4496 Lunar Lander , TD3PlusBC , L1 Norm 99.9 6518 97.6 9906 1171 2094 4496 Lunar Lander , TD3PlusBC , L2 Norm 6518 94.5 9906 1171 97.7 2094 4496 6518 9906 99.9 98.1 94.5 98.9 1171 98.5 2094 99.7 4496 98.1 99.5 98.0 6518 98.3 99.9 9906 92.8 99.9 98.9 99.9 92.5 99.1 1171 99.9 2094 99.7 4496 99.9 98.7 9906 98.8 99.5 6518 92.4 99.9 98.8 99.5 99.1 1171 2094 94.4 96.8 95.7 96.4 99.9 99.9 99.9 99.2 99.5 99.6 96.1 1171 2094 4496 Lunar Lander , IQL , Cosine Distance 6518 97.5 9906 99.1 99.6 99.6 99.4 99.4 99.9 94.0 94.8 1171 9906 4496 Lunar Lander , TD3PlusBC , Cosine Distance 6518 2094 99.8 2094 98.4 9906 4496 1171 Lunar Lander , IQL , Wasserstein Distance 89.4 74.8 6518 99.6 97.6 98.9 98.5 99.1 99.2 99.5 98.4 98.9 99.7 94.9 95.5 99.9 96.9 95.7 2094 98.9 1171 99.9 4496 Lunar Lander , TD3PlusBC , Wasserstein Distance 87.1 99.9 6518 98.3 9906 97.9 99.5 93.4 98.7 92.4 99.7 4496 98.7 6518 99.1 9906 98.9 94.7 93.9 1171 99.6 97.3 99.1 93.3 2094 98.9 96.4 98.6 94.5 4496 94.4 94.1 99.1 96.7 6518 85.6 96.5 96.7 99.7 9906 Bipedal Walker , BC , L1 Norm Bipedal Walker , BC , L2 Norm Bipedal Walker , BC , Cosine Distance Bipedal Walker , BC , Wasserstein Distance 96.3 98.9 99.9 99.7 96.7 1203 2110 Bipedal Walker , BCQ , L1 Norm 3813 99.7 6558 0841 1203 2110 Bipedal Walker , BCQ , L2 Norm 3813 93.1 6558 0841 1203 2110 Bipedal Walker , BCQ , Cosine Distance 3813 93.2 6558 99.9 6558 2110 0841 Bipedal Walker , BCQ , Wasserstein Distance 1203 3813 0841 1203 2110 3813 6558 0841 0841 99.9 1203 2110 3813 6558 0841 1203 2110 3813 6558 0841 1203 2110 98.0 92.5 99.9 99.7 99.7 98.3 99.9 99.7 0841 1203 2110 Bipedal Walker , IQL , L1 Norm 3813 6558 0841 1203 2110 Bipedal Walker , IQL , L2 Norm 3813 6558 0841 1203 2110 Bipedal Walker , IQL , Cosine Distance 3813 99.6 6558 1203 6558 2110 0841 Bipedal Walker , IQL , Wasserstein Distance 3813 99.5 80.7 93.7 85.2 95.5 96.4 99.2 95.5 82.0 97.6 0841 1203 2110 Bipedal Walker , TD3PlusBC , L1 Norm 3813 99.9 6558 10.4 0841 1203 2110 Bipedal Walker , TD3PlusBC , L2 Norm 89.1 3813 0.7 99.9 6558 0841 1203 2110 Bipedal Walker , TD3PlusBC , Cosine Distance 87.9 6558 3813 99.7 0.1 0841 2110 Bipedal Walker , TD3PlusBC , Wasserstein Distance 3813 1203 6558 1.9 48.7 3813 96.0 98.0 94.9 6558 0841 1203 2110 3813 96.0 96.8 6558 94.0 96.0 96.5 0841 1203 2110 3813 99.5 92.0 94.0 94.4 0841 1203 2110 3813 97.1 94.0 85.7 6558 79.6 92.0 85.3 6558 92.0 94.2 98.0 94.5 43.8 0841 1203 2110 3813 92.0 90.4 6558 Fig . 13 : The audit accuracy between every two Ant datasets . The caption of each plot demonstrates the offline DRL model ’ s type , the task , and the distance metric . The x labels are the names of datasets to be audited , i.e. , the target datasets . The y labels are the names of datasets the suspect models learned , i.e. , the actual datasets . Thus , the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset , i.e. , TPR , and the non-diagonal values are the TNR results . The positions without value mean 100 % accuracy . TABLE VIII : The impact of shadow models ’ amount . The TPR and TNR results of ORL-AUDITOR with 9 shadow models . Task Name Lunar Lander Bipedal Walker Ant Offline Model BC BCQ IQL TPR 97.09±1.09 96.97±1.65 96.89±1.96 TD3PlusBC 97.24±2.17 95.14±3.54 93.90±5.98 88.55±10.61 TD3PlusBC 97.39±5.22 90.61±6.99 92.65±3.46 97.05±1.06 TD3PlusBC 93.57±7.04 BC BCQ IQL BC BCQ IQL L1 Norm L2 Norm Cosine Distance Wasserstein Distance TNR 100.00±0.00 100.00±0.00 99.90±0.37 99.32±0.84 100.00±0.00 100.00±0.00 100.00±0.00 94.30±20.23 99.93±0.16 99.78±0.50 99.58±0.92 99.35±1.27 TPR 94.97±1.31 94.53±1.20 95.22±2.85 93.77±3.64 89.68±10.06 95.47±3.37 87.79±8.56 96.57±6.86 92.25±4.98 90.00±5.47 94.44±2.40 93.15±4.39 TNR 100.00±0.00 100.00±0.00 99.98±0.07 99.82±0.45 100.00±0.00 100.00±0.00 100.00±0.00 90.88±22.63 99.95±0.17 99.88±0.27 99.63±0.72 99.59±1.05 TPR 95.09±1.41 94.48±1.09 95.03±3.13 93.81±3.71 97.70±3.59 98.69±0.93 98.80±1.27 97.30±4.95 98.91±0.99 98.02±1.01 99.12±0.43 99.35±0.75 TNR 100.00±0.00 99.98±0.10 99.91±0.22 99.78±0.49 100.00±0.00 100.00±0.00 100.00±0.00 88.39±24.21 85.17±28.30 85.88±28.10 85.16±28.62 87.99±26.18 TPR 96.55±1.98 97.03±1.51 96.82±2.14 97.54±1.16 94.80±3.69 95.35±4.04 90.68±8.87 96.08±7.85 96.23±3.90 98.11±1.42 98.50±1.30 97.86±1.32 TNR 99.93±0.26 99.78±0.38 96.85±6.45 95.45±3.71 100.00±0.00 100.00±0.00 100.00±0.00 84.40±30.65 99.92±0.16 99.76±0.52 99.57±0.93 99.30±1.36 21 Ant , BC , L1 Norm Ant , BC , L2 Norm 2232 3569 4603 5766 7490 2232 3569 4603 5766 7490 2232 3569 4603 5766 7490 97.5 99.7 99.9 2232 96.4 98.9 2232 93.7 98.5 99.8 2232 99.9 98.0 99.6 99.1 99.9 99.8 94.3 3569 4603 Ant , BCQ , L1 Norm 5766 99.9 97.9 98.7 98.7 99.9 91.9 3569 4603 Ant , IQL , L1 Norm 5766 99.9 99.6 97.5 97.1 97.9 99.9 99.9 3569 4603 Ant , TD3PlusBC , L1 Norm 5766 99.9 98.3 7490 99.9 99.7 7490 99.6 99.9 95.7 7490 2232 3569 99.1 4603 99.8 96.4 5766 97.5 99.5 7490 98.3 96.2 92.5 98.0 98.8 95.9 99.9 2232 94.0 99.1 2232 91.9 98.9 99.9 2232 97.9 99.5 98.3 96.3 99.9 99.1 94.0 3569 4603 Ant , BCQ , L2 Norm 5766 98.3 94.9 99.5 98.4 92.3 3569 4603 Ant , IQL , L2 Norm 5766 99.9 97.2 7490 99.5 99.9 9.0 2232 99.9 99.5 98.8 7490 10.7 2232 Ant , BC , Cosine Distance 99.9 98.0 56.1 97.1 99.9 93.8 99.6 88.6 3569 99.9 4603 Ant , BCQ , Cosine Distance 56.1 5766 12.0 99.9 99.9 7490 Ant , BC , Wasserstein Distance 99.9 97.9 99.5 99.7 99.9 99.5 99.1 99.9 99.7 99.8 97.5 2232 3569 4603 Ant , BCQ , Wasserstein Distance 5766 96.1 7490 98.1 98.8 65.9 14.7 99.5 99.9 92.9 99.9 99.9 97.1 98.9 98.6 99.9 98.6 99.9 96.3 90.9 3569 99.9 4603 Ant , IQL , Cosine Distance 62.0 5766 99.9 7490 11.9 2232 99.6 98.5 99.8 3569 4603 Ant , IQL , Wasserstein Distance 5766 99.9 99.1 97.5 99.3 97.6 99.9 99.9 7490 99.6 99.8 98.7 97.3 98.1 96.7 98.5 99.9 98.3 99.8 99.3 98.7 98.0 99.1 74.7 99.7 92.4 99.9 3569 4603 Ant , TD3PlusBC , L2 Norm 5766 98.9 7490 8.8 2232 90.5 3569 99.6 4603 Ant , TD3PlusBC , Cosine Distance 50.3 5766 7490 2232 3569 4603 Ant , TD3PlusBC , Wasserstein Distance 5766 98.8 7490 96.1 99.0 94.6 92.0 98.2 99.7 99.0 99.5 99.9 99.6 99.9 98.9 98.2 7.1 2232 99.7 91.5 3569 97.2 7490 75.2 99.1 91.3 99.9 89.3 5766 99.2 99.9 92.2 100.0 4603 14.1 99.5 99.7 99.5 99.1 97.5 99.5 98.3 96.8 92.0 97.8 98.8 7490 2232 3569 4603 5766 7490 2232 3569 4603 5766 7490 2232 3569 4603 5766 TABLE IX : The impact of shadow models ’ amount . The TPR and TNR results of ORL-AUDITOR with 21 shadow models . Task Name Lunar Lander Bipedal Walker Ant Offline Model BC BCQ IQL TPR 99.25±0.97 99.56±0.32 97.95±2.45 TD3PlusBC 97.87±3.45 97.07±3.60 100.00±0.00 95.91±4.93 TD3PlusBC 99.91±0.18 98.13±1.55 97.16±2.73 95.91±3.87 TD3PlusBC 99.44±0.60 BC BCQ IQL BC BCQ IQL L1 Norm L2 Norm Cosine Distance Wasserstein Distance TNR 100.00±0.00 100.00±0.00 99.96±0.17 99.45±0.84 100.00±0.00 100.00±0.00 100.00±0.00 95.05±19.14 99.91±0.18 99.81±0.42 99.64±0.77 99.23±1.53 TPR 98.13±1.88 98.40±0.68 97.11±3.12 96.09±3.91 97.24±4.49 99.56±0.69 96.36±4.57 99.87±0.27 97.73±1.19 96.67±2.18 96.49±3.89 98.27±1.03 TNR 100.00±0.00 100.00±0.00 99.98±0.09 99.73±0.58 100.00±0.00 100.00±0.00 100.00±0.00 93.96±20.97 99.86±0.41 99.84±0.42 99.68±0.63 99.36±1.64 TPR 98.00±1.84 98.27±0.47 96.85±3.40 95.78±4.29 97.69±4.51 99.07±1.65 99.51±0.26 99.82±0.36 99.73±0.53 99.69±0.41 99.51±0.67 99.76±0.33 TNR 100.00±0.00 99.99±0.04 99.92±0.19 99.95±0.14 100.00±0.00 100.00±0.00 100.00±0.00 92.79±21.27 86.82±26.97 87.53±26.74 86.53±27.80 88.42±25.93 TPR 99.11±0.93 98.80±0.78 97.11±3.65 98.00±2.60 98.36±2.67 99.96±0.09 96.44±4.54 99.91±0.18 97.55±1.57 98.58±1.69 97.65±2.19 99.79±0.27 TNR 99.96±0.10 99.75±0.41 97.47±5.34 96.27±3.43 100.00±0.00 100.00±0.00 100.00±0.00 91.78±21.28 99.90±0.21 99.80±0.43 99.64±0.78 99.18±1.65 TABLE X : The impact of significance level . The TPR and TNR results of ORL-AUDITOR with σ = 0.001 . Task Name Lunar Lander Bipedal Walker Ant Offline Model BC BCQ IQL TPR 99.63±0.19 99.15±0.67 99.31±0.90 TD3PlusBC 99.20±1.10 99.97±0.05 99.95±0.06 97.04±5.47 TD3PlusBC 99.92±0.16 99.52±0.50 98.91±1.68 98.88±1.27 TD3PlusBC 99.62±0.46 BC BCQ IQL BC BCQ IQL L1 Norm L2 Norm Cosine Distance Wasserstein Distance TNR 100.00±0.00 100.00±0.00 99.83±0.40 99.22±0.89 100.00±0.00 100.00±0.00 100.00±0.00 89.69±22.99 99.86±0.25 99.71±0.60 99.53±0.97 98.92±2.08 TPR 98.21±0.55 97.60±1.13 98.56±1.51 97.47±2.33 98.67±2.67 99.73±0.34 95.81±5.06 97.17±5.65 98.48±0.80 97.97±1.43 98.42±1.80 98.73±0.97 TNR 100.00±0.00 100.00±0.00 99.96±0.16 99.61±0.55 100.00±0.00 100.00±0.00 100.00±0.00 85.59±27.05 99.88±0.40 99.81±0.47 99.60±0.73 99.24±1.91 TPR 98.21±0.63 97.63±1.04 98.51±1.59 97.55±2.22 98.64±2.66 99.95±0.06 99.87±0.27 97.20±5.60 99.55±0.66 99.87±0.15 99.71±0.52 99.79±0.36 TNR 100.00±0.00 99.97±0.13 99.79±0.51 99.75±0.51 100.00±0.00 100.00±0.00 100.00±0.00 82.52±30.48 80.58±33.24 81.88±31.60 80.90±32.60 84.36±28.07 TPR 99.31±0.38 98.59±0.95 99.04±1.21 99.49±0.56 100.00±0.00 99.97±0.05 97.68±4.51 99.68±0.64 99.36±0.49 99.52±0.64 99.92±0.06 99.71±0.58 TNR 99.84±0.38 99.61±0.53 94.88±8.21 92.45±5.32 100.00±0.00 100.00±0.00 100.00±0.00 80.18±35.21 99.85±0.25 99.69±0.63 99.49±1.05 98.78±2.15 TABLE XI : The impact of significance level . The TPR and TNR results of ORL-AUDITOR with σ = 0.0001 . Task Name Lunar Lander Bipedal Walker Ant Offline Model BC BCQ IQL TPR 99.87±0.12 99.49±0.46 99.71±0.52 TD3PlusBC 99.55±0.56 100.00±0.00 100.00±0.00 98.53±2.87 TD3PlusBC 100.00±0.00 99.95±0.11 99.33±1.21 99.73±0.29 TD3PlusBC 99.96±0.05 BC BCQ IQL BC BCQ IQL L1 Norm L2 Norm Cosine Distance Wasserstein Distance TNR 100.00±0.00 100.00±0.00 99.66±0.55 98.80±1.27 100.00±0.00 100.00±0.00 100.00±0.00 79.08±38.26 99.80±0.41 99.56±0.76 99.35±1.25 98.65±2.42 TPR 98.93±0.34 98.48±0.79 98.99±1.04 98.48±1.48 98.67±2.67 99.81±0.26 96.27±4.74 97.23±5.55 99.15±0.67 98.75±1.11 98.91±1.45 99.35±0.50 TNR 100.00±0.00 100.00±0.00 99.91±0.21 98.96±1.24 100.00±0.00 100.00±0.00 100.00±0.00 75.43±39.10 99.85±0.43 99.80±0.48 99.56±0.80 99.11±2.07 TPR 99.04±0.35 98.48±0.82 98.91±1.17 98.56±1.49 98.80±2.40 100.00±0.00 99.87±0.27 97.33±5.33 99.73±0.41 99.92±0.11 99.89±0.21 99.92±0.11 TNR 100.00±0.00 99.95±0.20 99.49±1.15 99.38±0.87 100.00±0.00 100.00±0.00 100.00±0.00 74.60±39.23 77.36±35.68 78.87±33.94 77.78±34.93 81.33±29.74 TPR 99.79±0.22 99.33±0.54 99.52±0.70 99.84±0.16 100.00±0.00 100.00±0.00 98.67±2.67 99.97±0.05 99.73±0.22 99.81±0.23 100.00±0.00 99.90±0.19 TNR 99.56±0.79 98.88±1.54 91.46±10.53 88.16±6.84 100.00±0.00 100.00±0.00 100.00±0.00 73.98±38.98 99.79±0.42 99.52±0.79 99.32±1.31 98.06±3.18 22 TABLE XII : The impact of trajectory size . The TPR and TNR results of ORL-AUDITOR with 25 % trajectory size . Task Name Lunar Lander Bipedal Walker Ant Offline Model BC BCQ IQL TPR 98.13±1.05 98.45±0.51 98.11±1.65 TD3PlusBC 98.00±2.42 99.20±0.97 98.59±2.63 96.80±5.37 TD3PlusBC 97.55±4.91 98.85±0.67 98.11±1.40 98.45±1.00 TD3PlusBC 98.80±1.33 BC BCQ IQL BC BCQ IQL L1 Norm L2 Norm Cosine Distance Wasserstein Distance TNR 99.53±1.30 99.28±1.19 95.42±5.71 95.44±4.57 100.00±0.00 100.00±0.00 100.00±0.00 94.08±21.64 99.90±0.35 99.85±0.34 99.90±0.30 99.76±0.43 TPR 96.27±2.00 97.33±0.76 96.72±2.57 96.43±3.13 97.47±3.12 97.68±2.95 95.73±5.42 97.20±5.60 97.04±1.24 97.36±1.61 96.45±1.47 96.92±1.60 TNR 99.64±1.09 99.59±0.71 97.10±3.96 96.86±3.24 100.00±0.00 100.00±0.00 100.00±0.00 89.72±24.95 99.84±0.47 99.78±0.46 99.85±0.40 99.70±0.67 TPR 96.29±1.97 96.91±1.06 96.80±2.25 95.95±2.56 98.61±2.71 99.68±0.27 99.41±0.45 96.93±5.74 99.49±0.88 99.49±0.76 99.68±0.51 99.22±1.18 TNR 99.13±2.41 99.01±1.33 92.42±5.78 92.95±3.92 100.00±0.00 100.00±0.00 100.00±0.00 90.65±23.69 92.58±19.10 92.64±19.34 92.56±20.01 93.58±17.31 TPR 98.10±0.92 98.56±1.10 98.19±1.55 98.45±1.43 99.36±0.90 98.61±2.45 97.01±5.45 97.41±5.17 98.96±0.81 99.12±0.59 99.04±0.55 99.28±1.07 TNR 97.74±2.30 94.02±4.44 84.64±9.09 81.51±9.13 100.00±0.00 100.00±0.00 100.00±0.00 84.06±33.72 99.90±0.35 99.84±0.34 99.80±0.49 99.74±0.45 TABLE XIII : The impact of trajectory size . The TPR and TNR results of ORL-AUDITOR with 50 % trajectory size . Task Name Lunar Lander Bipedal Walker Ant Offline Model BC BCQ IQL TPR 98.37±0.68 98.16±0.55 98.03±2.25 TD3PlusBC 98.03±2.33 99.44±0.75 98.75±2.38 95.68±6.60 TD3PlusBC 98.35±3.31 98.21±0.98 97.76±2.05 97.71±1.81 TD3PlusBC 98.52±1.81 BC BCQ IQL BC BCQ IQL L1 Norm L2 Norm Cosine Distance Wasserstein Distance TNR 100.00±0.00 99.96±0.19 98.93±1.35 98.28±2.03 100.00±0.00 100.00±0.00 100.00±0.00 94.29±21.47 99.92±0.24 99.85±0.36 99.73±0.61 99.61±0.77 TPR 97.07±0.90 96.11±0.83 96.80±2.84 96.37±3.41 97.81±2.83 97.92±2.81 95.47±5.34 97.20±5.60 97.04±1.33 96.72±1.50 96.53±1.65 96.99±1.57 TNR 100.00±0.00 99.95±0.20 99.29±1.06 99.27±0.97 100.00±0.00 100.00±0.00 100.00±0.00 91.75±22.51 99.85±0.43 99.81±0.44 99.82±0.40 99.74±0.64 TPR 97.25±0.72 96.40±0.84 97.25±2.18 96.51±2.98 98.67±2.67 99.89±0.10 99.81±0.31 96.96±6.02 99.49±0.83 99.60±0.60 99.79±0.30 99.82±0.25 TNR 100.00±0.02 99.58±0.80 96.38±2.48 95.94±4.14 100.00±0.00 100.00±0.00 100.00±0.00 90.88±23.07 88.52±25.25 89.27±24.10 88.67±25.63 90.62±24.30 TPR 98.58±0.50 97.57±1.64 98.27±2.29 98.24±1.79 99.41±0.86 99.55±0.72 96.40±6.11 97.41±5.17 98.59±0.81 98.88±1.30 98.99±0.65 99.13±1.29 TNR 98.50±1.91 96.28±3.72 86.89±10.96 84.30±8.08 100.00±0.00 100.00±0.00 100.00±0.00 89.04±27.90 99.92±0.24 99.84±0.36 99.70±0.66 99.58±0.79 TABLE XIV : The details of the online models for generating the offline datasets . The model performance shows the cumulative reward for 10 separate evaluations . Task Name Online Model Train Step Model Name Model Performance Lunar Lander SAC 1e6 Bipedal Walker PPO 1e6 Ant SAC 2e6 1171 2094 4496 6518 9906 0841 1203 2110 3813 6558 2232 3569 4603 5766 7490 275.47±14.38 50.79±65.95 195.02±143.15 246.40±33.91 209.33±91.73 285.55±60.84 286.94±53.46 283.58±47.35 235.88±103.83 285.16±65.92 5377.70±1653.17 1924.58±1180.96 5531.45±844.10 3025.89±547.36 5897.37±477.34 23 Fig . 14 : The audit accuracy against model ensemble for Lunar Lander . The caption of each plot demonstrates the offline DRL model ’ s type , the task , the distance metric , and the hyperparameter K of the model ensemble . The x labels are the names of datasets to be audited , i.e. , the target datasets . The y labels are the names of datasets the suspect models learned , i.e. , the actual datasets . Thus , the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset , i.e. , TPR , and the non-diagonal values are the TNR results . The positions without value mean 100 % accuracy . TABLE XV : The details of the offline DRL datasets Task Name Number of Transitions Dataset Name Number of Trajectories Length of trajectory Lunar Lander 5e5 Bipedal Walker 1e6 Ant 2e6 2175 578 1252 1878 1566 1019 1027 877 887 1041 2093 3497 2096 2217 2103 229.83±83.51 864.19±231.88 399.30±240.88 266.13±99.65 319.21±231.06 981.03±190.79 973.07±118.42 1139.55±151.10 1126.63±379.05 959.77±146.13 955.46±177.72 571.66±375.40 954.01±175.82 901.84±236.93 951.02±187.93 1171 2094 4496 6518 9906 0841 1203 2110 3813 6558 2232 3569 4603 5766 7490 24 Lunar Lander , BC , L1 Norm , K = 5 Lunar Lander , BC , L2 Norm , K = 5 Lunar Lander , BC , Cosine , K = 5 Lunar Lander , BC , Wasserstein , K = 5 98.0 98.0 98.0 98.0 98.0 98.0 1171 2094 4496 6518 9906 1171 2094 4496 Lunar Lander , BCQ , L1 Norm , K = 5 6518 9906 1171 2094 4496 Lunar Lander , BCQ , L2 Norm , K = 5 6518 9906 1171 2094 4496 Lunar Lander , BCQ , Cosine , K = 5 6518 9906 1171 98.0 98.0 98.0 2094 4496 6518 9906 1171 2094 4496 6518 9906 1171 2094 4496 6518 9906 1171 2094 4496 Lunar Lander , IQL , L1 Norm , K = 5 6518 9906 1171 2094 4496 Lunar Lander , IQL , L2 Norm , K = 5 6518 94.0 9906 1171 2094 4496 Lunar Lander , IQL , Cosine , K = 5 6518 94.0 9906 98.0 98.0 98.0 98.0 98.0 98.0 94.0 98.0 98.0 2094 9906 4496 1171 Lunar Lander , TD3PlusBC , L1 Norm , K = 5 6518 98.0 9906 4496 1171 Lunar Lander , TD3PlusBC , L2 Norm , K = 5 2094 6518 2094 9906 4496 1171 Lunar Lander , TD3PlusBC , Cosine , K = 5 6518 96.0 1171 96.0 2094 4496 Lunar Lander , TD3PlusBC , Wasserstein , K = 5 92.0 6518 98.0 92.0 98.0 9906 98.0 98.0 98.0 98.0 98.0 1171 98.0 2094 98.0 4496 98.0 6518 9906 98.0 1171 2094 4496 6518 9906 98.0 1171 2094 98.0 98.0 4496 6518 9906 96.0 96.0 98.0 96.0 1171 92.0 2094 94.0 94.0 98.0 6518 86.0 98.0 96.0 9906 98.0 98.0 92.0 4496 2094 1171 9906 4496 Lunar Lander , BCQ , Wasserstein , K = 5 6518 1171 2094 4496 Lunar Lander , IQL , Wasserstein , K = 5 88.0 84.0 6518 98.0 98.0 9906 Fig . 15 : The audit accuracy against model ensemble for Bipedal Walker . The caption of each plot demonstrates the offline DRL model ’ s type , the task , the distance metric , and the hyperparameter K of the model ensemble . The x labels are the names of datasets to be audited , i.e. , the target datasets . The y labels are the names of datasets the suspect models learned , i.e. , the actual datasets . Thus , the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset , i.e. , TPR , and the non-diagonal values are the TNR results . The positions without value mean 100 % accuracy . TABLE XVI : As a supplementary of [ 13 ] , we provide more details of the BC offline models . The model performance shows the cumulative reward for 10 separate evaluations . Offline Model Task Name Lunar Lander BC Bipedal Walker Ant Dataset Name 1171 2094 4496 6518 9906 0841 1203 2110 3813 6558 2232 3569 4603 5766 7490 Model Performance ( No Defense ) 272.06±5.14 39.04±34.07 173.62±58.93 211.85±43.52 225.45±32.91 264.92±29.11 288.85±15.39 276.80±26.03 164.56±46.14 281.02±48.65 5479.72±354.79 1493.77±413.96 5424.74±422.83 2806.80±286.57 5514.17±441.78 Model Performance ( Trajectory Splitting ) 269.17±11.28 46.65±37.73 183.34±47.00 215.13±57.35 213.70±40.65 277.42±18.00 287.12±16.88 277.15±24.63 156.62±47.94 277.24±54.87 5427.47±609.23 1523.73±473.18 5463.20±511.58 2863.00±291.50 5410.28±467.33 Model Performance ( Model Ensemble ) 266.20±13.81 45.74±116.66 189.41±102.32 199.44±118.66 234.34±67.54 241.77±117.88 298.62±1.29 265.78±98.38 66.65±97.36 308.01±0.87 5933.60±98.05 1695.64±1255.83 5269.72±1692.57 2951.89±728.14 5785.87±630.97 Model Performance ( Gauss . 0.01 ) 270.84±6.38 55.03±34.09 161.95±54.27 223.84±41.63 215.19±35.72 257.99±26.73 287.64±16.36 283.05±20.17 160.48±56.24 284.69±22.77 5324.99±441.27 1460.97±436.37 5470.37±473.25 2899.11±313.43 5451.01±430.96 Model Performance ( Gauss . 0.1 ) 269.01±11.12 53.99±30.89 177.23±35.35 219.14±44.10 215.76±33.81 268.83±25.55 285.91±15.71 286.19±14.48 182.20±55.79 268.39±39.12 4332.23±589.30 1412.06±391.99 4679.76±496.30 2458.92±272.67 4417.19±687.25 25 Bipedal Walker , BC , L1 Norm , K = 5 Bipedal Walker , BC , L2 Norm , K = 5 Bipedal Walker , BC , Cosine , K = 5 Bipedal Walker , BC , Wasserstein , K = 5 0841 1203 2110 3813 6558 0841 1203 2110 3813 6558 0841 1203 2110 3813 6558 0841 1203 2110 0841 1203 2110 Bipedal Walker , BCQ , L1 Norm , K = 5 3813 6558 0841 1203 2110 Bipedal Walker , BCQ , L2 Norm , K = 5 3813 6558 0841 1203 2110 Bipedal Walker , IQL , L1 Norm , K = 5 3813 6558 0841 1203 2110 Bipedal Walker , IQL , L2 Norm , K = 5 3813 6558 0841 1203 2110 Bipedal Walker , BCQ , Cosine , K = 5 3813 6558 0841 1203 2110 Bipedal Walker , IQL , Cosine , K = 5 3813 6558 0841 2110 Bipedal Walker , TD3PlusBC , L1 Norm , K = 5 6558 1203 3813 12.0 0841 1203 2110 Bipedal Walker , TD3PlusBC , L2 Norm , K = 5 94.0 3813 6558 0.0 1203 6558 2110 0841 Bipedal Walker , TD3PlusBC , Cosine , K = 5 94.0 3813 0.0 1203 0841 6558 2110 Bipedal Walker , BCQ , Wasserstein , K = 5 3813 1203 0841 6558 2110 Bipedal Walker , IQL , Wasserstein , K = 5 3813 0841 2110 Bipedal Walker , TD3PlusBC , Wasserstein , K = 5 3813 1203 6558 4.0 52.0 54.0 98.0 96.0 92.0 86.0 92.0 94.0 94.0 3813 96.0 98.0 96.0 96.0 94.0 96.0 98.0 94.0 92.0 94.0 96.0 6558 0841 1203 2110 3813 6558 0841 1203 2110 3813 6558 0841 1203 2110 3813 6558 0841 1203 2110 3813 6558 Fig . 16 : The audit accuracy against model ensemble for Ant . The caption of each plot demonstrates the offline DRL model ’ s type , the task , the distance metric , and the hyperparameter K of the model ensemble . The x labels are the names of datasets to be audited , i.e. , the target datasets . The y labels are the names of datasets the suspect models learned , i.e. , the actual datasets . Thus , the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset , i.e. , TPR , and the non-diagonal values are the TNR results . The positions without value mean 100 % accuracy . TABLE XVII : As a supplementary of [ 13 ] , we provide more details of the BCQ offline models . The model performance shows the cumulative reward for 10 separate evaluations . Offline Model Task Name Lunar Lander BCQ Bipedal Walker Ant Dataset Name 1171 2094 4496 6518 9906 0841 1203 2110 3813 6558 2232 3569 4603 5766 7490 Model Performance ( No Defense ) 270.69±8.51 52.67±26.38 166.13±57.37 234.99±30.93 243.74±24.43 228.05±43.06 269.87±28.93 281.34±18.56 166.87±55.81 271.52±57.34 3844.45±875.84 1032.55±327.13 4554.06±676.32 2583.27±268.12 3653.48±1108.85 Model Performance ( Trajectory Splitting ) 270.45±12.51 64.70±22.08 195.16±37.67 227.41±36.30 236.93±23.49 235.75±39.17 276.35±23.98 282.23±20.88 181.39±45.03 271.09±75.34 3651.94±943.58 951.30±312.96 4562.26±828.88 2502.33±323.71 3755.22±1159.16 Model Performance ( Model Ensemble ) 278.43±9.57 30.79±81.96 88.06±182.38 233.08±45.90 236.40±41.93 229.03±117.30 243.15±112.91 264.16±97.68 131.04±165.52 306.09±4.03 4295.42±2225.70 435.72±420.34 3980.17±2203.92 2603.11±1075.03 4012.54±2267.61 Model Performance ( Gauss . 0.01 ) 268.41±13.91 57.80±30.64 188.89±38.33 236.13±33.25 237.51±31.45 247.17±37.65 276.78±21.02 270.97±24.22 177.90±52.03 275.55±30.70 3587.01±816.81 1013.29±283.76 4480.04±639.38 2640.93±323.35 3552.11±1115.43 Model Performance ( Gauss . 0.1 ) 270.25±13.45 55.73±28.36 191.19±46.30 235.19±25.16 233.42±34.97 249.94±28.21 281.59±17.69 270.78±26.49 185.86±45.97 262.56±43.95 2514.55±772.19 942.25±266.46 3412.76±804.53 2031.98±293.49 2432.82±892.02 26 Ant , BC , L1 Norm , K = 5 2232 98.0 Ant , BC , L2 Norm , K = 5 98.0 98.0 Ant , BC , Cosine , K = 5 98.0 28.0 98.0 10.0 96.0 Ant , BC , Wasserstein , K = 5 2232 3569 4603 5766 7490 2232 3569 4603 5766 7490 3569 4603 Ant , BCQ , L1 Norm , K = 5 5766 7490 2232 3569 4603 Ant , BCQ , L2 Norm , K = 5 5766 98.0 98.0 98.0 98.0 98.0 98.0 2232 3569 5766 4603 Ant , IQL , L1 Norm , K = 5 7490 2232 3569 5766 4603 Ant , IQL , L2 Norm , K = 5 7490 2232 3569 98.0 98.0 98.0 98.0 98.0 98.0 98.0 92.0 7490 10.0 2232 32.0 94.0 3569 5766 4603 Ant , BCQ , Cosine , K = 5 44.0 98.0 7490 2232 18.0 3569 4603 Ant , BCQ , Wasserstein , K = 5 5766 98.0 98.0 98.0 98.0 54.0 94.0 3569 5766 4603 Ant , IQL , Cosine , K = 5 82.0 98.0 7490 2232 14.0 3569 4603 Ant , IQL , Wasserstein , K = 5 5766 98.0 98.0 98.0 98.0 7490 98.0 7490 2232 3569 4603 Ant , TD3PlusBC , L1 Norm , K = 5 5766 7490 2232 3569 4603 Ant , TD3PlusBC , L2 Norm , K = 5 5766 7490 4603 Ant , TD3PlusBC , Cosine , K = 5 98.0 98.0 98.0 98.0 92.0 98.0 90.0 2232 3569 4603 5766 7490 2232 3569 4603 5766 7490 74.0 98.0 90.0 84.0 5766 8.0 2232 94.0 3569 90.0 4603 7490 18.0 2232 3569 4603 Ant , TD3PlusBC , Wasserstein , K = 5 5766 7490 98.0 98.0 98.0 84.0 92.0 98.0 7490 2232 3569 4603 5766 7490 94.0 3569 46.0 5766 4603 5766 7490 2232 3569 4603 5766 7490 10.0 2232 98.0 98.0 6.0 2232 Fig . 17 : The audit accuracy against model ensemble for Half Cheetah . The caption of each plot demonstrates the offline DRL model ’ s type , the task , the distance metric , and the hyperparameter K of the model ensemble . The x labels are the names of datasets to be audited , i.e. , the target datasets . The y labels are the names of datasets the suspect models learned , i.e. , the actual datasets . Thus , the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset , i.e. , TPR , and the non-diagonal values are the TNR results . The positions without value mean 100 % accuracy . TABLE XVIII : As a supplementary of [ 13 ] , we provide more details of the IQL offline models . The model performance shows the cumulative reward for 10 separate evaluations . Offline Model Task Name Lunar Lander IQL Bipedal Walker Ant Dataset Name 1171 2094 4496 6518 9906 0841 1203 2110 3813 6558 2232 3569 4603 5766 7490 Model Performance ( No Defense ) 268.55±10.81 57.92±31.14 181.48±40.09 226.85±43.37 237.33±29.23 261.23±33.91 284.63±17.38 285.24±28.87 169.20±38.88 279.97±33.62 4577.36±865.63 1406.45±447.39 5248.48±477.42 2846.64±295.47 4814.81±556.16 Model Performance ( Trajectory Splitting ) 271.91±5.10 39.17±27.55 190.44±51.16 240.15±27.03 238.88±20.10 254.33±34.94 291.47±14.52 288.77±21.97 155.45±57.20 285.89±23.26 4437.55±766.02 1415.31±336.28 5148.72±476.71 2779.50±233.82 4715.59±628.54 Model Performance ( Model Ensemble ) 275.03±21.12 47.20±86.05 138.19±219.58 237.64±32.00 221.57±104.60 272.50±54.24 271.86±54.77 299.45±4.43 172.28±123.79 159.33±182.92 4968.65±1337.85 1563.86±1225.73 5822.61±164.84 2680.32±1019.01 3367.15±2159.49 Model Performance ( Gauss . 0.01 ) 266.55±13.58 49.96±28.60 194.79±43.22 218.41±45.31 245.07±20.08 254.18±35.26 285.79±17.42 287.20±19.29 172.41±43.21 284.75±21.41 4678.37±804.33 1421.81±459.03 5232.36±536.94 2879.16±262.72 4877.90±707.65 Model Performance ( Gauss . 0.1 ) 265.35±14.22 46.82±29.83 181.89±38.97 245.00±20.38 231.25±25.67 264.38±34.52 285.38±14.02 281.40±23.24 163.51±55.19 268.64±40.07 3420.01±912.08 1239.03±328.98 4135.40±708.39 2338.67±263.68 3461.92±694.92 27 Half Cheetah , BC , L1 Norm , K = 5 Half Cheetah , BC , L2 Norm , K = 5 Half Cheetah , BC , Cosine , K = 5 Half Cheetah , BC , Wasserstein , K = 5 expert medium random rluply expert medium random rluply 98.0 2.0 4.0 22.0 90.0 40.0 40.0 0.0 92.0 76.0 expert medium random Half Cheetah , BCQ , L1 Norm , K = 5 rluply expert medium random Half Cheetah , BCQ , L2 Norm , K = 5 rluply expert medium random Half Cheetah , BCQ , Cosine , K = 5 rluply 98.0 98.0 96.0 2.0 2.0 30.0 64.0 60.0 2.0 74.0 80.0 expert medium random Half Cheetah , IQL , L1 Norm , K = 5 rluply expert medium random Half Cheetah , IQL , L2 Norm , K = 5 rluply expert medium random Half Cheetah , IQL , Cosine , K = 5 rluply expert 96.0 96.0 medium random rluply 96.0 68.0 92.0 58.0 expert medium random Half Cheetah , TD3PlusBC , L1 Norm , K = 5 rluply expert medium random Half Cheetah , TD3PlusBC , L2 Norm , K = 5 rluply 8.0 0.0 50.0 92.0 2.0 78.0 rluply medium random expert Half Cheetah , TD3PlusBC , Cosine , K = 5 98.0 expert rluply medium random Half Cheetah , BCQ , Wasserstein , K = 5 50.0 expert rluply medium random Half Cheetah , IQL , Wasserstein , K = 5 72.0 96.0 70.0 expert medium random Half Cheetah , TD3PlusBC , Wasserstein , K = 5 rluply expert 74.0 84.0 38.0 88.0 86.0 80.0 94.0 medium random rluply expert medium random 46.0 rluply expert medium random 44.0 rluply expert medium random 2.0 2.0 34.0 2.0 68.0 80.0 rluply 54.0 4.0 98.0 expert medium random 52.0 rluply Fig . 18 : The audit accuracy with Gaussian noise ( µ = 0 , σ = 0.01 ) on the suspect models ’ action for Lunar Lander . The caption of each plot demonstrates the offline DRL model ’ s type , the task , the distance metric , and the noise strength . The x labels are the names of datasets to be audited , i.e. , the target datasets . The y labels are the names of datasets the suspect models learned , i.e. , the actual datasets . Thus , the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset , i.e. , TPR , and the non-diagonal values are the TNR results . The positions without value mean 100 % accuracy . TABLE XIX : As a supplementary of [ 13 ] , we provide more details of the TD3PlusBC offline models . The model performance shows the cumulative reward for 10 separate evaluations . Offline Model Task Name Lunar Lander TD3PlusBC Bipedal Walker Ant Dataset Name 1171 2094 4496 6518 9906 0841 1203 2110 3813 6558 2232 3569 4603 5766 7490 Model Performance ( No Defense ) 263.65±21.47 99.72±47.21 201.58±42.49 242.58±21.54 235.98±25.48 -102.88±56.03 -86.65±22.94 -101.94±22.86 -114.96±14.97 154.70±148.81 259.94±116.75 549.14±192.30 374.17±199.78 396.13±130.69 314.48±222.06 Model Performance ( Trajectory Splitting ) 266.03±13.80 95.78±34.82 207.92±33.77 229.34±29.64 241.78±21.68 -100.63±59.89 -87.17±22.47 -100.43±26.01 -115.04±14.37 138.26±165.87 216.71±118.76 563.13±156.03 370.58±217.99 368.56±115.13 326.59±153.02 Model Performance ( Model Ensemble ) 263.34±16.97 71.96±111.26 159.76±135.46 248.09±30.91 206.93±113.68 -108.40±0.22 -95.59±17.13 -80.32±14.02 -126.18±2.36 303.03±2.30 258.28±297.07 566.88±655.52 151.13±112.93 369.73±275.57 689.21±637.77 Model Performance ( Gauss . 0.01 ) 267.15±12.96 100.67±34.95 207.07±28.13 238.51±21.01 230.41±34.05 -101.93±54.70 -87.64±22.35 -101.02±23.63 -113.97±12.93 165.64±136.38 243.30±121.39 579.86±213.98 372.37±194.00 334.74±172.22 365.24±212.09 Model Performance ( Gauss . 0.1 ) 265.12±10.36 90.74±37.66 194.69±42.59 243.96±16.41 229.55±36.81 -97.05±73.10 -86.95±25.50 -98.78±26.75 -119.21±10.67 168.47±68.50 222.48±139.45 495.19±160.81 367.64±279.00 361.40±117.68 275.81±130.30 TABLE XX : The details of the HalfCheetah dataset Task Name Half Cheetah Number of Transitions 1e6 1e6 1e6 3.003e5 Dataset Name D4RL Expert D4RL Medium D4RL Random RL Unplugged Number of Trajectories 1001 1001 1001 300 Length of trajectory 998.00 ±0.06 997.90 ±3.13 998.00±0.00 1001.00±0.00 28 Lunar Lander , BC , L1 Norm , 0.01 Lunar Lander , BC , L2 Norm , 0.01 Lunar Lander , BC , Cosine , 0.01 Lunar Lander , BC , Wasserstein , 0.01 1171 98.9 96.7 96.5 2094 4496 6518 9906 98.4 98.5 95.7 96.5 95.1 96.4 99.7 98.1 97.7 1171 2094 4496 Lunar Lander , BCQ , L1 Norm , 0.01 6518 98.1 9906 1171 2094 4496 Lunar Lander , BCQ , L2 Norm , 0.01 6518 96.1 9906 1171 2094 4496 Lunar Lander , BCQ , Cosine , 0.01 6518 96.3 9906 1171 98.4 95.6 95.3 99.8 99.5 99.8 99.9 98.0 97.1 99.2 99.9 99.9 98.8 99.9 2094 1171 4496 Lunar Lander , BCQ , Wasserstein , 0.01 99.6 6518 97.7 99.9 98.7 9906 2094 4496 6518 9906 1171 2094 4496 6518 9906 1171 2094 4496 6518 9906 95.3 99.5 93.2 96.8 93.9 96.7 97.5 95.6 95.7 99.9 99.9 96.3 99.9 99.2 99.6 99.6 95.7 1171 2094 4496 Lunar Lander , IQL , L1 Norm , 0.01 6518 98.9 9906 1171 2094 4496 Lunar Lander , IQL , L2 Norm , 0.01 6518 96.9 9906 1171 2094 4496 Lunar Lander , IQL , Cosine , 0.01 6518 96.8 9906 99.9 99.9 99.3 99.7 98.9 99.5 99.7 96.0 99.6 94.3 99.1 99.6 99.6 99.4 99.4 99.9 94.0 98.6 2094 97.6 1171 9906 4496 Lunar Lander , TD3PlusBC , L1 Norm , 0.01 97.6 99.9 6518 2094 94.3 1171 9906 4496 Lunar Lander , TD3PlusBC , L2 Norm , 0.01 92.4 6518 2094 94.3 1171 9906 4496 Lunar Lander , TD3PlusBC , Cosine , 0.01 92.1 6518 99.9 1171 99.7 2094 4496 Lunar Lander , IQL , Wasserstein , 0.01 74.7 6518 99.6 97.7 89.4 98.7 9906 98.9 98.5 99.1 99.2 99.5 98.4 98.9 99.7 95.1 95.5 99.9 96.9 98.7 1171 95.7 2094 99.9 4496 Lunar Lander , TD3PlusBC , Wasserstein , 0.01 87.1 99.9 6518 98.3 9906 98.0 99.5 98.8 93.4 99.9 98.1 94.3 98.9 1171 98.5 2094 99.7 4496 98.1 99.5 98.0 6518 98.3 99.9 9906 99.9 98.9 99.9 92.3 99.1 1171 99.9 2094 99.7 4496 99.9 98.5 9906 98.7 99.5 6518 99.9 98.8 99.5 99.1 1171 2094 92.4 99.7 4496 98.7 6518 98.9 9906 98.9 94.6 93.9 1171 99.6 97.4 99.1 93.3 2094 98.9 96.4 98.6 94.5 4496 94.5 94.1 99.1 96.7 6518 85.6 96.4 96.7 99.7 9906 Fig . 19 : The audit accuracy with Gaussian noise ( µ = 0 , σ = 0.1 ) on the suspect models ’ action for Lunar Lander . The caption of each plot demonstrates the offline DRL model ’ s type , the task , the distance metric , and the noise strength . The x labels are the names of datasets to be audited , i.e. , the target datasets . The y labels are the names of datasets the suspect models learned , i.e. , the actual datasets . Thus , the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset , i.e. , TPR , and the non-diagonal values are the TNR results . The positions without value mean 100 % accuracy . TABLE XXI : As a supplementary of [ 13 ] , we provide more details of the models trained on the HalfCheetah dataset . The model performance shows the cumulative reward for 10 separate evaluations . Task Name Offline Model Dataset Name Half Cheetah BC BCQ IQL TD3PlusBC D4RL Expert D4RL Medium D4RL Random RL Unplugged D4RL Expert D4RL Medium D4RL Random RL Unplugged D4RL Expert D4RL Medium D4RL Random RL Unplugged D4RL Expert D4RL Medium D4RL Random RL Unplugged Model Performance ( No Defense ) 12620.94±307.84 4223.77±134.67 -0.33±0.24 -427.50±113.42 10974.19±842.10 4765.24±98.75 -1.13±0.43 -421.91±212.36 10163.20±1106.70 4808.11±46.99 1649.55±518.47 -378.74±151.65 12712.69±383.33 4969.74±56.31 1046.23±226.61 -181.50±205.29 Model Performance ( Trajectory Splitting ) 12624.61±333.32 4265.82±96.17 -0.33±0.22 -431.01±110.15 10735.35±1345.57 4746.03±108.99 -1.15±0.54 -419.59±219.29 9920.53±879.89 4800.87±59.75 1644.31±551.32 -367.87±156.81 12752.25±274.38 4964.48±57.44 1050.03±214.80 -175.49±225.09 Model Performance ( Model Ensemble ) 12868.22±180.39 4293.35±75.67 -0.37±0.62 -427.06±56.30 12334.59±539.99 4512.03±99.46 -0.54±0.78 -378.28±64.55 11268.02±2640.57 4671.25±99.09 1822.31±31.63 -311.62±16.31 11468.00±872.43 4871.85±82.15 1128.32±3.15 -385.80±54.32 TABLE XXII : The TPR and TNR results on the Half Cheetah task . The mean and standard deviation of TPR and TNR in each row represent the audit results for one combination of task and model by four distance metrics . Bold indicates the highest sum of TPR and TNR , i.e. , accuracy , in a row . Each pair of TPR and TNR is derived from the diagonal and non-diagonal values of the corresponding heatmap in Figure 24 . Task Name Half Cheetah Offline Model TPR 96.07±3.15 95.37±0.55 95.47±0.77 TD3PlusBC 95.00±2.87 BC BCQ IQL L1 Norm L2 Norm Cosine Distance Wasserstein Distance TNR 100.00±0.00 100.00±0.00 100.00±0.00 100.00±0.00 TPR 96.07±2.34 95.83±1.20 95.68±1.02 95.50±1.99 TNR 100.00±0.00 100.00±0.00 100.00±0.00 100.00±0.00 TPR 99.80±0.35 99.57±0.47 99.78±0.23 99.87±0.16 TNR 68.62±42.47 70.14±41.14 71.38±41.05 70.57±40.85 TPR 98.47±1.13 97.47±1.35 97.12±2.70 98.27±1.09 TNR 100.00±0.00 100.00±0.00 100.00±0.00 100.00±0.00 29 1171 2094 4496 6518 9906 1171 2094 4496 6518 9906 Lunar Lander , BC , L1 Norm , 0.1 Lunar Lander , BC , L2 Norm , 0.1 Lunar Lander , BC , Cosine , 0.1 2.0 3.2 2.0 4.4 0.0 4.0 0.1 4.0 0.1 0.5 3.7 2.0 Lunar Lander , BC , Wasserstein , 0.1 99.9 71.9 99.9 85.6 99.9 99.5 57.1 63.6 1171 2094 4496 Lunar Lander , BCQ , L1 Norm , 0.1 6518 2.0 9906 1171 2094 4496 Lunar Lander , BCQ , L2 Norm , 0.1 6518 2.0 9906 1171 2094 4496 Lunar Lander , BCQ , Cosine , 0.1 6518 2.0 9906 1171 2094 4496 Lunar Lander , BCQ , Wasserstein , 0.1 99.4 6518 99.9 87.2 64.4 9906 5.9 11.7 15.3 0.4 8.5 9.9 4.8 3.7 8.5 10.0 4.9 3.3 77.2 99.9 99.6 99.9 96.1 99.7 99.8 73.9 1171 2094 4496 Lunar Lander , IQL , L1 Norm , 0.1 6518 13.3 9906 1171 2094 4496 Lunar Lander , IQL , L2 Norm , 0.1 6518 13.1 9906 1171 56.7 42.1 2094 4496 6518 9906 1171 2094 4496 6518 9906 25.7 99.9 22.8 99.7 40.5 46.1 45.1 41.9 2094 19.7 1171 9906 4496 Lunar Lander , TD3PlusBC , L1 Norm , 0.1 46.7 6518 2094 12.0 1171 9906 4496 Lunar Lander , TD3PlusBC , L2 Norm , 0.1 32.5 6518 96.3 98.1 53.9 99.3 1171 98.6 2094 99.9 4496 98.1 31.7 98.0 6518 98.3 82.5 9906 87.7 47.7 99.7 1171 2094 4496 99.9 72.0 9906 40.8 99.6 6518 1171 43.7 2094 4496 Lunar Lander , IQL , Cosine , 0.1 6518 22.4 99.8 45.6 99.9 38.3 13.1 9906 99.9 99.6 1171 2094 4496 Lunar Lander , TD3PlusBC , Cosine , 0.1 6518 11.2 9906 99.9 1171 99.7 2094 4496 Lunar Lander , IQL , Wasserstein , 0.1 73.4 6518 99.7 98.7 90.4 85.2 9906 95.7 98.5 99.3 99.5 96.7 98.9 98.7 99.7 91.2 95.5 96.5 98.3 1171 96.7 2094 4496 Lunar Lander , TD3PlusBC , Wasserstein , 0.1 87.9 99.4 96.3 94.0 99.1 99.9 6518 95.7 9906 86.0 99.8 46.0 37.7 2094 4496 6518 72.3 9906 99.0 94.7 93.9 1171 99.6 97.6 99.1 93.5 2094 98.9 95.2 98.7 94.5 4496 95.1 94.3 96.4 97.0 6518 85.1 96.5 96.7 99.6 9906 33.1 99.9 99.7 1171 Fig . 20 : The audit accuracy with Gaussian noise ( µ = 0 , σ = 0.01 ) on the suspect models ’ action for Bipedal Walker . The caption of each plot demonstrates the offline DRL model ’ s type , the task , the distance metric , and the noise strength . The x labels are the names of datasets to be audited , i.e. , the target datasets . The y labels are the names of datasets the suspect models learned , i.e. , the actual datasets . Thus , the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset , i.e. , TPR , and the non-diagonal values are the TNR results . The positions without value mean 100 % accuracy . TABLE XXIII : The TPR and TNR results of ORL-AUDITOR when splitting each trajectory into shorter ones ( S = 5 ) . The mean and standard deviation of TPR and TNR in each row represent the audit results for one combination of task and model by four distance metrics . Each pair of TPR and TNR is derived from the diagonal and non-diagonal values of the corresponding heatmap in Figure 25 ( Lunar Lander ) , Figure 26 ( Bipedal Walker ) , Figure 27 ( Ant ) , and Figure 28 ( Half Cheetah ) . Task Name Offline Model Lunar Lander Bipedal Walker Ant Half Cheetah BC BCQ IQL TD3PlusBC BC BCQ IQL TD3PlusBC BC BCQ IQL TD3PlusBC BC BCQ IQL TD3PlusBC L1 Norm L2 Norm Cosine Distance Wasserstein Distance TPR 99.01±0.46 98.29±1.10 98.59±1.55 98.29±2.04 99.65±0.57 99.55±0.71 95.17±7.39 99.39±1.23 98.03±1.38 97.47±2.93 97.68±2.08 98.71±1.63 98.50±1.50 96.83±1.54 97.00±2.00 97.53±1.30 TNR 100.00±0.00 100.00±0.00 99.91±0.31 99.48±0.79 100.00±0.00 100.00±0.00 100.00±0.00 94.77±19.42 99.93±0.12 99.80±0.44 99.65±0.73 99.18±1.71 100.00±0.00 100.00±0.00 100.00±0.00 100.00±0.00 TPR 96.96±0.73 95.87±1.12 97.52±2.51 96.35±3.01 98.45±2.71 98.19±2.84 95.01±5.49 97.15±5.71 96.77±1.49 95.89±2.32 96.77±2.50 97.20±1.79 96.87±2.14 96.27±1.36 96.25±1.13 96.53±1.20 TNR 100.00±0.00 100.00±0.00 99.97±0.12 99.89±0.22 100.00±0.00 100.00±0.00 100.00±0.00 93.37±21.46 99.90±0.36 99.84±0.41 99.69±0.59 99.35±1.72 100.00±0.00 100.00±0.00 100.00±0.00 100.00±0.00 TPR 96.96±0.73 95.87±1.06 97.49±2.56 96.27±3.16 98.64±2.66 99.68±0.45 99.81±0.31 96.93±6.00 99.39±0.91 99.65±0.63 99.63±0.62 99.81±0.31 99.74±0.27 99.93±0.12 99.56±0.20 99.56±0.61 TNR 100.00±0.00 99.99±0.03 99.92±0.19 99.91±0.23 100.00±0.00 100.00±0.00 100.00±0.00 91.98±21.75 86.07±27.76 86.86±27.33 85.74±28.43 88.35±25.99 69.56±41.70 68.58±41.88 72.63±40.22 71.21±40.66 TPR 98.43±0.73 97.60±1.14 98.32±1.79 98.53±1.25 99.79±0.43 99.89±0.10 95.33±7.01 98.13±3.73 98.05±1.43 98.83±1.55 99.31±0.49 99.22±1.31 98.60±0.92 97.43±1.38 97.06±2.73 98.37±1.09 TNR 99.94±0.18 99.91±0.15 97.10±5.66 95.59±3.77 100.00±0.00 100.00±0.00 100.00±0.00 88.23±25.40 99.91±0.15 99.79±0.47 99.63±0.78 99.14±1.81 100.00±0.00 100.00±0.00 100.00±0.00 100.00±0.00 30 0841 99.9 1203 2110 3813 6558 0841 99.9 0841 1203 2110 3813 6558 1203 2110 3813 6558 0841 1203 2110 Bipedal Walker , BC , L1 Norm , 0.01 Bipedal Walker , BC , L2 Norm , 0.01 Bipedal Walker , BC , Cosine , 0.01 Bipedal Walker , BC , Wasserstein , 0.01 7.5 3.9 99.2 78.9 45.1 68.7 99.9 99.9 39.2 4.1 66.1 0841 1203 2110 Bipedal Walker , BCQ , L1 Norm , 0.01 3813 93.9 6558 0841 1203 2110 Bipedal Walker , BCQ , L2 Norm , 0.01 3813 93.2 6558 0841 1203 2110 Bipedal Walker , BCQ , Cosine , 0.01 3813 93.1 6558 1203 96.9 0841 6558 2110 Bipedal Walker , BCQ , Wasserstein , 0.01 99.9 3813 96.8 92.1 98.5 95.3 99.9 99.7 99.6 99.2 0841 1203 2110 Bipedal Walker , IQL , L1 Norm , 0.01 3813 6558 0841 1203 2110 Bipedal Walker , IQL , L2 Norm , 0.01 3813 6558 0841 1203 2110 Bipedal Walker , IQL , Cosine , 0.01 3813 98.8 6558 0841 1203 2110 Bipedal Walker , IQL , Wasserstein , 0.01 3813 6558 86.8 43.2 91.5 99.3 89.6 77.1 89.7 99.9 98.8 85.1 68.4 94.5 97.7 0841 6558 2110 Bipedal Walker , TD3PlusBC , L1 Norm , 0.01 1203 3813 10.4 98.4 0841 6558 2110 Bipedal Walker , TD3PlusBC , L2 Norm , 0.01 88.8 1203 3813 0.7 1203 0841 6558 2110 Bipedal Walker , TD3PlusBC , Cosine , 0.01 87.9 3813 99.7 0.1 0841 2110 Bipedal Walker , TD3PlusBC , Wasserstein , 0.01 1203 3813 98.4 6558 1.9 48.9 3813 96.0 98.0 94.9 6558 0841 1203 2110 3813 96.0 95.2 6558 94.0 96.0 96.5 0841 1203 2110 3813 99.6 92.0 94.0 94.4 0841 1203 2110 3813 96.9 94.0 84.3 6558 79.5 92.0 86.0 6558 92.2 94.1 98.0 94.5 43.7 0841 1203 2110 3813 92.0 89.6 6558 Fig . 21 : The audit accuracy with Gaussian noise ( µ = 0 , σ = 0.1 ) on the suspect models ’ action for Bipedal Walker . The caption of each plot demonstrates the offline DRL model ’ s type , the task , the distance metric , and the noise strength . The x labels are the names of datasets to be audited , i.e. , the target datasets . The y labels are the names of datasets the suspect models learned , i.e. , the actual datasets . Thus , the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset , i.e. , TPR , and the non-diagonal values are the TNR results . The positions without value mean 100 % accuracy . 31 0841 1203 2110 3813 6558 0841 1203 2110 3813 6558 0841 1203 2110 3813 6558 0841 1203 2110 Bipedal Walker , BC , L1 Norm , 0.1 Bipedal Walker , BC , L2 Norm , 0.1 Bipedal Walker , BC , Cosine , 0.1 Bipedal Walker , BC , Wasserstein , 0.1 0.0 0.0 0.0 0.0 5.6 93.2 0.0 0.0 0.0 0.0 0.0 0.0 24.9 30.7 0.0 0.0 0841 1203 2110 Bipedal Walker , BCQ , L1 Norm , 0.1 3813 0.0 6558 0841 1203 2110 Bipedal Walker , BCQ , L2 Norm , 0.1 3813 19.1 6558 0841 1203 2110 Bipedal Walker , BCQ , Cosine , 0.1 3813 35.5 6558 0.0 0.0 1.1 0.0 8.8 93.9 0.0 0.0 0.0 0.0 17.2 28.1 0841 1203 2110 Bipedal Walker , IQL , L1 Norm , 0.1 3813 0.0 6558 0841 1203 2110 Bipedal Walker , IQL , L2 Norm , 0.1 3813 39.6 6558 0841 1203 2110 Bipedal Walker , IQL , Cosine , 0.1 3813 38.0 6558 0.0 0.0 0.0 0.0 7.1 92.1 0.0 0.0 0.0 0.0 19.9 29.3 0.0 0841 6558 2110 Bipedal Walker , TD3PlusBC , L1 Norm , 0.1 1203 3813 11.6 1203 0.0 0841 6558 2110 Bipedal Walker , TD3PlusBC , L2 Norm , 0.1 99.9 91.1 3813 0.7 1203 19.3 0841 6558 2110 Bipedal Walker , TD3PlusBC , Cosine , 0.1 99.5 90.3 3813 99.9 0.1 0841 1203 2110 Bipedal Walker , BCQ , Wasserstein , 0.1 2.5 3813 0.0 6558 0.4 0.0 0.0 0841 1203 2110 Bipedal Walker , IQL , Wasserstein , 0.1 0.0 3813 0.3 6558 0.0 0.0 0.0 0841 2110 Bipedal Walker , TD3PlusBC , Wasserstein , 0.1 3813 1203 0.0 6558 2.3 50.9 3813 96.0 98.0 94.9 6558 0841 1203 2110 3813 96.0 0.0 6558 91.1 91.9 94.1 96.0 96.8 99.7 98.4 0841 1203 2110 3813 97.8 94.0 3.7 6558 92.0 94.0 95.0 99.8 97.6 0841 1203 2110 3813 83.7 92.0 13.5 6558 92.3 94.4 98.0 94.4 45.4 0841 1203 2110 3813 92.0 7.7 6558 Fig . 22 : The audit accuracy with Gaussian noise ( µ = 0 , σ = 0.01 ) on the suspect models ’ action for Ant . The caption of each plot demonstrates the offline DRL model ’ s type , the task , the distance metric , and the noise strength . The x labels are the names of datasets to be audited , i.e. , the target datasets . The y labels are the names of datasets the suspect models learned , i.e. , the actual datasets . Thus , the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset , i.e. , TPR , and the non-diagonal values are the TNR results . The positions without value mean 100 % accuracy . 32 Ant , BC , L1 Norm , 0.01 99.9 97.5 99.6 96.9 99.8 84.4 5766 4603 3569 Ant , BCQ , L1 Norm , 0.01 99.9 97.2 98.7 99.9 98.7 99.9 87.1 3569 5766 4603 Ant , IQL , L1 Norm , 0.01 99.9 99.3 97.5 93.2 97.9 99.9 99.7 99.9 96.1 7490 99.9 99.6 7490 99.6 99.9 95.1 99.9 2232 93.5 99.1 2232 91.3 98.9 99.9 Ant , BC , L2 Norm , 0.01 98.3 95.2 99.9 98.8 89.7 5766 4603 3569 Ant , BCQ , L2 Norm , 0.01 98.3 94.0 99.5 97.9 88.7 3569 5766 4603 Ant , IQL , L2 Norm , 0.01 98.7 97.5 98.2 96.1 98.5 99.9 96.7 99.9 96.7 7490 99.9 99.5 99.9 8.8 2232 99.9 99.5 Ant , BC , Cosine , 0.01 97.9 57.0 97.1 99.9 11.7 99.9 94.3 99.9 57.3 99.7 88.2 5766 4603 3569 Ant , BCQ , Cosine , 0.01 98.2 62.9 98.3 99.7 99.9 94.3 99.7 Ant , BC , Wasserstein , 0.01 99.9 97.7 99.2 99.7 99.9 99.5 99.3 99.8 99.8 97.1 7490 2232 3569 4603 Ant , BCQ , Wasserstein , 0.01 5766 15.4 99.9 96.9 98.9 98.6 99.9 99.9 98.6 99.9 96.3 98.9 7490 10.1 2232 65.7 92.4 3569 5766 4603 Ant , IQL , Cosine , 0.01 73.5 98.7 99.9 7490 12.3 99.8 99.5 98.4 99.2 99.7 92.9 99.9 2232 99.7 98.5 99.8 3569 4603 Ant , IQL , Wasserstein , 0.01 5766 99.9 98.9 97.5 98.7 97.6 99.9 99.9 96.5 7490 99.9 7490 99.6 99.9 94.5 99.7 99.9 2232 95.5 98.9 2232 93.1 98.5 99.8 2232 3569 4603 5766 7490 2232 3569 4603 5766 7490 2232 3569 4603 5766 7490 2232 3569 4603 Ant , TD3PlusBC , L1 Norm , 0.01 5766 94.9 7490 2232 3569 4603 Ant , TD3PlusBC , L2 Norm , 0.01 5766 98.7 7490 2232 3569 99.1 4603 99.8 95.2 5766 97.5 99.6 7490 98.3 93.6 92.5 98.0 98.8 98.0 99.5 95.9 98.2 99.7 99.0 92.9 92.1 99.0 99.5 2232 3569 4603 5766 99.5 7490 2232 3569 4603 5766 95.7 7490 8.1 2232 99.9 99.6 98.2 7.0 2232 90.9 3569 99.5 4603 Ant , TD3PlusBC , Cosine , 0.01 51.3 5766 99.5 99.7 92.1 3569 99.2 99.9 92.2 4603 75.4 99.1 91.9 87.3 5766 7490 14.7 99.5 2232 3569 4603 Ant , TD3PlusBC , Wasserstein , 0.01 5766 98.1 7490 99.8 99.1 99.1 97.5 99.5 98.3 96.6 92.0 97.8 98.8 7490 2232 3569 4603 5766 7490 Fig . 23 : The audit accuracy with Gaussian noise ( µ = 0 , σ = 0.1 ) on the suspect models ’ action for Ant . The caption of each plot demonstrates the offline DRL model ’ s type , the task , the distance metric , and the noise strength . The x labels are the names of datasets to be audited , i.e. , the target datasets . The y labels are the names of datasets the suspect models learned , i.e. , the actual datasets . Thus , the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset , i.e. , TPR , and the non-diagonal values are the TNR results . The positions without value mean 100 % accuracy . 33 8.7 2232 3569 4603 99.9 5766 7490 2232 3569 4603 5766 7490 2232 3569 4603 5766 7490 2232 10.0 98.9 2232 9.7 98.9 99.9 2232 Ant , BC , L1 Norm , 0.1 99.9 13.6 99.9 2.0 2.3 5766 4603 3569 Ant , BCQ , L1 Norm , 0.1 99.9 24.4 99.0 2.0 99.3 99.9 9.5 3569 5766 4603 Ant , IQL , L1 Norm , 0.1 99.9 25.3 98.5 2.0 98.1 99.9 8.4 3569 4603 Ant , TD3PlusBC , L1 Norm , 0.1 5766 3.6 7490 99.9 99.9 3.9 7490 99.9 3.2 7490 Ant , BC , Cosine , 0.1 Ant , BC , Wasserstein , 0.1 59.7 13.3 12.9 Ant , BC , L2 Norm , 0.1 98.7 22.5 6.0 2.3 5766 4603 3569 Ant , BCQ , L2 Norm , 0.1 98.3 27.6 99.9 5.6 5.1 3569 5766 4603 Ant , IQL , L2 Norm , 0.1 98.7 40.0 99.3 5.6 98.9 4.5 3569 4603 Ant , TD3PlusBC , L2 Norm , 0.1 5766 4.4 7490 3.3 7490 8.8 99.9 2232 9.9 99.6 2232 9.2 99.9 2232 8.5 99.6 98.7 98.1 74.8 99.1 99.9 95.1 84.7 2.7 7490 10.8 2232 99.9 99.9 62.2 99.8 88.9 5766 4603 3569 Ant , BCQ , Cosine , 0.1 98.2 68.7 99.9 7490 16.7 77.5 99.9 96.5 84.4 10.8 2232 99.9 9.1 2232 99.6 98.8 7.7 2232 94.1 3569 69.3 5766 99.9 4603 Ant , IQL , Cosine , 0.1 78.1 98.9 81.6 99.9 95.7 84.8 91.8 3569 99.6 4603 Ant , TD3PlusBC , Cosine , 0.1 54.9 5766 99.9 80.8 93.5 3569 99.8 99.4 93.7 99.9 4603 78.5 99.4 94.3 99.1 91.3 5766 99.7 7490 13.1 99.7 7490 17.3 99.7 7490 99.9 29.9 99.9 5.6 3.7 3569 4603 Ant , BCQ , Wasserstein , 0.1 5766 99.9 42.5 98.9 7.3 99.3 99.9 14.1 3569 4603 Ant , IQL , Wasserstein , 0.1 5766 99.9 49.1 98.4 4.0 98.1 99.9 10.1 2.9 7490 99.9 99.9 43.1 7490 99.9 99.9 2232 62.3 98.9 2232 12.5 98.9 99.9 2232 3569 4603 Ant , TD3PlusBC , Wasserstein , 0.1 5766 25.7 7490 93.1 99.4 51.3 97.9 99.7 98.8 3.4 93.0 97.9 99.3 2232 3569 4603 5766 65.9 7490 2232 8.3 3569 99.4 27.9 4603 5766 98.1 99.7 7490 98.8 2.0 93.4 98.1 99.3 99.8 59.7 99.5 5.4 93.4 99.4 99.9 94.5 2232 3569 4603 5766 2.3 7490 2232 3569 4603 5766 2.0 7490 Fig . 24 : The audit accuracy between every two Half Cheetah datasets . The caption of each plot demonstrates the offline DRL model ’ s type , the task , and the distance metric . The x labels are the names of datasets to be audited , i.e. , the target datasets . The y labels are the names of datasets the suspect models learned , i.e. , the actual datasets . Thus , the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset , i.e. , TPR , and the non-diagonal values are the TNR results . The positions without value mean 100 % accuracy . 34 Half Cheetah , BC , L1 Norm Half Cheetah , BC , L2 Norm Half Cheetah , BC , Cosine Distance Half Cheetah , BC , Wasserstein Distance expert 91.2 94.0 medium random rluply 96.7 94.7 90.0 96.4 95.6 5.4 3.1 26.9 1.6 96.3 99.2 expert medium random Half Cheetah , BCQ , L1 Norm rluply expert medium random Half Cheetah , BCQ , L2 Norm rluply expert 94.8 96.0 medium random rluply 95.1 96.3 97.7 94.9 expert medium random Half Cheetah , IQL , L1 Norm 95.3 rluply expert medium random Half Cheetah , IQL , L2 Norm 94.7 rluply expert 95.5 95.2 96.7 94.5 97.3 95.6 95.2 94.6 99.7 expert medium random Half Cheetah , BCQ , Cosine Distance rluply 11.1 5.3 30.2 99.9 98.8 95.2 99.9 3.9 96.1 99.6 expert medium random Half Cheetah , IQL , Cosine Distance rluply 5.9 99.9 92.1 2.9 55.5 99.9 1.4 99.0 99.4 99.1 97.6 medium random expert Half Cheetah , BCQ , Wasserstein Distance rluply 97.2 99.7 96.7 97.2 96.3 expert rluply medium random Half Cheetah , IQL , Wasserstein Distance 99.5 92.5 98.5 98.0 expert medium random Half Cheetah , TD3PlusBC , L1 Norm rluply expert medium random Half Cheetah , TD3PlusBC , L2 Norm rluply expert medium random Half Cheetah , TD3PlusBC , Cosine Distance rluply expert medium random Half Cheetah , TD3PlusBC , Wasserstein Distance rluply expert 90.3 92.8 95.6 96.1 94.8 96.1 6.9 99.9 4.8 45.1 89.5 expert medium random 98.0 rluply expert medium random 98.3 rluply expert medium random 2.2 98.3 99.6 rluply 98.9 96.4 99.1 expert medium random 98.7 rluply medium random rluply medium random rluply Fig . 25 : The audit accuracy of ORL-AUDITOR on Lunar Lander when splitting each trajectory into shorter ones ( S = 5 ) . The caption of each plot demonstrates the offline DRL model ’ s type , the task , the distance metric , and the hyperparameter S of trajectory splitting . The x labels are the names of datasets to be audited , i.e. , the target datasets . The y labels are the names of datasets the suspect models learned , i.e. , the actual datasets . Thus , the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset , i.e. , TPR , and the non-diagonal values are the TNR results . The positions without value mean 100 % accuracy . 35 Lunar Lander , BC , L1 Norm , split 5 Lunar Lander , BC , L2 Norm , split 5 Lunar Lander , BC , Cosine , split 5 Lunar Lander , BC , Wasserstein , split 5 96.9 95.9 96.8 95.9 97.2 98.1 97.2 98.1 99.5 99.8 99.9 98.3 99.9 99.2 97.2 98.7 1171 99.1 2094 4496 6518 9906 98.5 98.8 99.9 1171 2094 4496 Lunar Lander , BCQ , L1 Norm , split 5 6518 98.8 9906 1171 2094 4496 Lunar Lander , BCQ , L2 Norm , split 5 6518 96.7 9906 1171 2094 4496 Lunar Lander , BCQ , Cosine , split 5 6518 96.8 9906 1171 98.5 96.4 99.5 97.9 95.5 94.1 95.2 94.4 99.9 96.8 95.6 96.9 95.6 1171 2094 4496 Lunar Lander , IQL , L1 Norm , split 5 6518 99.2 9906 1171 2094 4496 Lunar Lander , IQL , L2 Norm , split 5 6518 97.3 9906 1171 2094 4496 Lunar Lander , IQL , Cosine , split 5 6518 97.2 9906 99.5 99.7 99.1 99.5 99.9 96.0 99.6 94.4 99.1 99.6 99.6 99.4 99.4 99.9 94.0 98.6 2094 97.6 1171 9906 4496 Lunar Lander , TD3PlusBC , L1 Norm , split 5 97.7 99.9 6518 94.5 1171 9906 4496 Lunar Lander , TD3PlusBC , L2 Norm , split 5 92.8 2094 6518 2094 94.8 1171 9906 4496 Lunar Lander , TD3PlusBC , Cosine , split 5 92.4 6518 99.9 2094 98.5 9906 4496 1171 Lunar Lander , BCQ , Wasserstein , split 5 97.7 99.9 99.6 6518 96.5 99.9 99.9 99.9 99.2 99.5 99.6 96.1 99.8 2094 1171 4496 Lunar Lander , IQL , Wasserstein , split 5 74.8 6518 99.6 97.6 89.4 98.4 9906 98.9 98.5 99.1 99.2 99.5 98.4 98.9 99.7 94.9 95.5 99.9 96.9 98.9 1171 95.7 2094 99.9 4496 Lunar Lander , TD3PlusBC , Wasserstein , split 5 87.1 99.9 6518 98.3 9906 99.5 97.9 98.7 93.4 99.9 98.1 94.5 98.9 1171 98.5 2094 99.7 4496 98.1 99.5 98.0 6518 98.3 99.9 9906 99.9 98.9 99.9 92.5 99.1 1171 99.9 2094 99.7 4496 99.9 98.7 9906 98.8 99.5 6518 99.9 98.8 99.5 99.1 1171 2094 92.4 99.7 4496 98.7 6518 99.1 9906 98.9 94.7 93.9 1171 99.6 97.3 99.1 93.3 2094 98.9 96.4 98.6 94.5 4496 94.4 94.1 99.1 96.7 6518 85.6 96.5 96.7 99.7 9906 2094 4496 6518 9906 1171 2094 4496 6518 9906 1171 2094 4496 6518 9906 Fig . 26 : The audit accuracy of ORL-AUDITOR on Bipedal Walker when splitting each trajectory into shorter ones ( S = 5 ) . The caption of each plot demonstrates the offline DRL model ’ s type , the task , the distance metric , and the hyperparameter S of trajectory splitting . The x labels are the names of datasets to be audited , i.e. , the target datasets . The y labels are the names of datasets the suspect models learned , i.e. , the actual datasets . Thus , the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset , i.e. , TPR , and the non-diagonal values are the TNR results . The positions without value mean 100 % accuracy . 36 0841 1203 2110 3813 6558 1203 2110 3813 6558 0841 1203 2110 3813 6558 0841 1203 2110 Bipedal Walker , BC , L1 Norm , split 5 Bipedal Walker , BC , L2 Norm , split 5 Bipedal Walker , BC , Cosine , split 5 Bipedal Walker , BC , Wasserstein , split 5 98.5 98.9 99.2 99.9 0841 1203 2110 Bipedal Walker , BCQ , L1 Norm , split 5 3813 99.7 6558 0841 1203 2110 Bipedal Walker , BCQ , L2 Norm , split 5 3813 93.1 6558 0841 1203 2110 Bipedal Walker , BCQ , Cosine , split 5 3813 93.3 6558 0841 99.9 1203 6558 2110 0841 Bipedal Walker , BCQ , Wasserstein , split 5 99.9 3813 98.1 92.7 99.7 98.3 99.7 99.9 99.7 99.9 0841 1203 2110 Bipedal Walker , IQL , L1 Norm , split 5 3813 6558 0841 1203 2110 Bipedal Walker , IQL , L2 Norm , split 5 3813 6558 0841 1203 2110 Bipedal Walker , IQL , Cosine , split 5 3813 98.8 6558 1203 0841 6558 2110 Bipedal Walker , IQL , Wasserstein , split 5 3813 99.7 80.8 93.7 85.1 99.9 97.6 95.5 96.4 99.2 81.5 97.6 0841 2110 Bipedal Walker , TD3PlusBC , L1 Norm , split 5 3813 1203 99.9 6558 10.4 1203 0841 2110 Bipedal Walker , TD3PlusBC , L2 Norm , split 5 89.3 3813 0.7 99.9 6558 1203 0841 6558 2110 Bipedal Walker , TD3PlusBC , Cosine , split 5 87.9 3813 99.7 0.1 0841 2110 Bipedal Walker , TD3PlusBC , Wasserstein , split 5 3813 6558 1203 1.9 48.0 3813 96.0 98.0 94.9 6558 0841 1203 2110 3813 96.0 96.9 6558 94.0 96.0 96.4 0841 1203 2110 3813 99.7 92.0 94.0 94.4 0841 1203 2110 3813 97.1 94.0 85.7 6558 79.6 92.0 84.9 6558 92.1 94.2 98.0 94.5 43.9 0841 1203 2110 3813 92.0 90.7 6558 Fig . 27 : The audit accuracy of ORL-AUDITOR on Ant when splitting each trajectory into shorter ones ( S = 5 ) . The caption of each plot demonstrates the offline DRL model ’ s type , the task , the distance metric , and the hyperparameter S of trajectory splitting . The x labels are the names of datasets to be audited , i.e. , the target datasets . The y labels are the names of datasets the suspect models learned , i.e. , the actual datasets . Thus , the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset , i.e. , TPR , and the non-diagonal values are the TNR results . The positions without value mean 100 % accuracy . 37 Ant , BC , Cosine , split 5 98.0 57.6 97.6 99.9 12.0 99.9 Ant , BC , Wasserstein , split 5 99.9 98.1 99.5 99.7 99.9 99.5 99.5 99.7 99.8 97.6 Ant , BC , L1 Norm , split 5 99.9 98.0 99.6 99.9 99.9 99.8 95.6 3569 4603 Ant , BCQ , L1 Norm , split 5 5766 99.9 97.7 98.7 98.7 99.9 92.0 99.9 98.4 7490 96.8 99.9 2232 94.4 99.1 Ant , BC , L2 Norm , split 5 98.3 96.0 99.9 99.2 94.7 3569 4603 Ant , BCQ , L2 Norm , split 5 5766 98.3 95.2 99.5 98.0 99.9 92.8 3569 5766 4603 Ant , IQL , L1 Norm , split 5 7490 2232 3569 5766 4603 Ant , IQL , L2 Norm , split 5 99.9 99.6 97.5 98.1 97.9 99.9 99.6 99.9 98.7 97.3 92.0 98.9 99.9 98.1 96.9 98.5 99.9 98.7 98.3 99.7 99.9 2232 97.6 98.9 2232 94.5 98.5 99.8 2232 3569 4603 5766 7490 2232 3569 4603 5766 7490 2232 3569 4603 5766 7490 2232 3569 4603 Ant , TD3PlusBC , L1 Norm , split 5 5766 96.1 7490 2232 3569 4603 Ant , TD3PlusBC , L2 Norm , split 5 5766 98.9 7490 2232 3569 99.1 99.8 96.1 4603 5766 97.5 99.6 7490 98.3 97.4 92.5 98.0 98.8 98.3 99.5 96.1 98.2 99.7 99.0 94.8 92.2 99.0 99.5 99.9 97.2 7490 99.9 99.6 99.9 11.0 2232 99.9 99.5 99.1 7490 8.9 2232 95.9 99.7 59.1 99.6 88.5 5766 4603 3569 Ant , BCQ , Cosine , split 5 98.1 64.9 98.4 99.5 95.2 65.1 93.0 3569 5766 4603 Ant , IQL , Cosine , split 5 67.0 99.9 98.7 11.6 99.9 7490 15.5 99.5 99.8 99.3 98.4 99.1 99.7 92.3 99.9 8.1 2232 99.9 99.6 98.2 7.9 2232 91.8 3569 99.7 4603 Ant , TD3PlusBC , Cosine , split 5 47.5 5766 99.2 99.2 99.7 92.7 3569 92.2 4603 82.6 99.1 89.2 91.7 5766 96.8 7490 99.9 95.6 7490 99.9 7490 99.6 99.9 7490 2232 3569 4603 Ant , BCQ , Wasserstein , split 5 5766 13.1 99.9 98.0 98.9 98.5 99.9 98.6 99.9 96.1 7490 2232 3569 4603 Ant , IQL , Wasserstein , split 5 5766 99.9 98.9 99.7 98.5 99.8 97.5 99.2 97.6 99.9 2232 3569 4603 Ant , TD3PlusBC , Wasserstein , split 5 5766 98.7 7490 99.8 99.5 99.1 97.5 99.5 98.3 96.6 92.0 97.8 98.7 2232 3569 4603 5766 7490 2232 3569 4603 5766 7490 2232 3569 4603 5766 7490 Fig . 28 : The audit accuracy of ORL-AUDITOR on Half Cheetah when splitting each trajectory into shorter ones ( S = 5 ) . The caption of each plot demonstrates the offline DRL model ’ s type , the task , the distance metric , and the hyperparameter S of trajectory splitting . The x labels are the names of datasets to be audited , i.e. , the target datasets . The y labels are the names of datasets the suspect models learned , i.e. , the actual datasets . Thus , the diagonal values show the audit accuracy when the actual dataset is the same as the target dataset , i.e. , TPR , and the non-diagonal values are the TNR results . The positions without value mean 100 % accuracy . 38 Half Cheetah , BC , L1 Norm , split 5 Half Cheetah , BC , L2 Norm , split 5 Half Cheetah , BC , Cosine , split 5 Half Cheetah , BC , Wasserstein , split 5 expert 99.7 medium random rluply 98.0 97.6 94.4 99.5 91.7 7.7 3.8 30.6 96.3 95.5 99.8 3.5 97.7 99.5 expert medium random Half Cheetah , BCQ , L1 Norm , split 5 rluply expert medium random Half Cheetah , BCQ , L2 Norm , split 5 rluply expert medium random Half Cheetah , BCQ , Cosine , split 5 rluply expert 99.2 97.3 medium random rluply 95.3 97.9 90.5 97.2 95.6 95.2 94.7 99.9 7.6 2.7 26.9 3.1 92.3 99.7 expert medium random Half Cheetah , IQL , L1 Norm , split 5 rluply expert medium random Half Cheetah , IQL , L2 Norm , split 5 rluply expert medium random Half Cheetah , IQL , Cosine , split 5 rluply expert 99.9 medium random rluply 97.9 96.9 97.6 99.3 94.2 7.9 99.9 94.9 95.3 95.9 94.6 5.3 61.9 99.6 2.7 99.6 99.5 98.7 98.3 rluply medium random expert Half Cheetah , BCQ , Wasserstein , split 5 97.5 99.7 96.7 97.2 96.1 expert rluply medium random Half Cheetah , IQL , Wasserstein , split 5 99.3 92.4 98.5 98.0 expert medium random Half Cheetah , TD3PlusBC , L1 Norm , split 5 rluply expert medium random Half Cheetah , TD3PlusBC , L2 Norm , split 5 rluply expert rluply medium random Half Cheetah , TD3PlusBC , Cosine , split 5 expert medium random Half Cheetah , TD3PlusBC , Wasserstein , split 5 rluply expert 99.1 96.9 95.6 97.2 94.8 96.3 medium random rluply expert medium random 98.3 rluply expert medium random 98.1 rluply expert medium random 99.7 90.9 6.9 4.7 52.3 1.6 98.1 98.5 rluply 98.9 96.5 99.3 expert medium random 98.7 rluply","['orlauditor', 'dataset', 'auditing', 'offline', 'deep', 'reinforcement', 'learn', 'p', 'e', 'r', 'c', 'c', 'r', 'shoule', 'jiming', 'sji', 'zjueducn', 'saodiseng', 'cjm', 'helmholtz', 'center', 'information', 'security', 'saarbr¨ucken', 'email', 'minchen', 'cispade', 'abstract', 'datum', 'critical', 'asset', 'ai', 'highquality', 'dataset', 'significantly', 'improve', 'performance', 'machine', 'learning', 'model', 'safetycritical', 'domain', 'autonomous', 'vehicle', 'offline', 'deep', 'reinforcement', 'learn', 'frequently', 'use', 'train', 'model', 'precollecte', 'dataset', 'oppose', 'train', 'ing', 'model', 'interact', 'realworld', 'environment', 'online', 'drl', 'support', 'development', 'model', 'many', 'institution', 'make', 'dataset', 'publicly', 'available', 'open', 'source', 'license', 'dataset', 'risk', 'potential', 'misuse', 'infringement', 'inject', 'watermark', 'dataset', 'protect', 'intellectual', 'property', 'datum', 'handle', 'dataset', 'already', 'publish', 'infeasible', 'alter', 'afterward', 'exist', 'solution', 'dataset', 'inference', 'membership', 'inference', 'work', 'well', 'offline', 'scenario', 'diverse', 'model', 'behavior', 'characteristic', 'offline', 'set', 'constraint', 'paper', 'advocate', 'new', 'paradigm', 'leverage', 'fact', 'cumulative', 'reward', 'act', 'unique', 'identifier', 'distinguish', 'drl', 'model', 'train', 'specific', 'dataset', 'end', 'propose', 'orlauditor', 'first', 'trajectory', 'level', 'dataset', 'auditing', 'mechanism', 'scenario', 'experiment', 'multiple', 'offline', 'drl', 'model', 'task', 'reveal', 'efficacy', 'orlauditor', 'auditing', 'accuracy', 'false', 'positive', 'rate', 'less', 'also', 'provide', 'valuable', 'insight', 'practical', 'implementation', 'auditor', 'study', 'various', 'parameter', 'setting', 'furthermore', 'demonstrate', 'auditing', 'capability', 'orlauditor', 'opensource', 'dataset', 'deepmind', 'highlight', 'effectiveness', 'audit', 'publish', 'dataset', 'orlauditor', 'opensource', 'https', 'githubcomlinkzjuorlauditor', 'introduction', 'deep', 'reinforcement', 'learn', 'successfully', 'apply', 'many', 'complex', 'decisionmake', 'task', 'autopilot', 'robot', 'control', 'power', 'system', 'intrusion', 'detection', 'however', 'safetycritical', 'domain', 'robot', 'control', 'directly', 'interact', 'environment', 'unsafe', 'partially', 'train', 'policy', 'first', 'author', 'make', 'equal', 'contribution', 'correspond', 'author', 'network', 'distribute', 'system', 'security', 'symposium', 'february', 'isbn', 'https', 'risk', 'damage', 'robot', 'hardware', 'surround', 'object', 'address', 'issue', 'researcher', 'propose', 'offline', 'deep', 'reinforcement', 'learn', 'offline', 'paradigm', 'also', 'know', 'full', 'batch', 'general', 'idea', 'learn', 'precollecte', 'datum', 'generate', 'expert', 'handcraft', 'controller', 'even', 'random', 'strategy', 'respect', 'system', 'constraint', 'artificial', 'facilitate', 'research', 'offline', 'drl', 'several', 'high', 'quality', 'dataset', 'publish', 'third', 'party', 'deep', 'mind', 'intelligence', 'research', 'bair', 'polixir', 'technology', 'tensorflow', 'dataset', 'publish', 'strict', 'opensource', 'license', 'general', 'public', 'license', 'apache', 'license', 'bsd', 'clause', 'license', 'protect', 'intellectual', 'property', 'ip', 'datum', 'owner', 'license', 'typically', 'encompas', 'essential', 'term', 'attribution', 'require', 'user', 'appropriately', 'acknowledge', 'source', 'provide', 'link', 'license', 'indicate', 'modification', 'make', 'sharealike', 'stipulate', 'remix', 'transform', 'build', 'material', 'distribute', 'contribution', 'license', 'original', 'furthermore', 'dataset', 'accompany', 'additional', 'patent', 'grant', 'aim', 'safeguard', 'right', 'datum', 'publisher', 'stardata', 'additionally', 'closedform', 'dataset', 'potential', 'face', 'misuse', 'insider', 'attack', 'intellectual', 'property', 'infringement', 'exemployee', 'steal', 'ing', 'datum', 'survey', 'find', 'respondent', 'admit', 'take', 'valuable', 'datum', 'leave', 'job', 'cite', 'lack', 'policy', 'technology', 'prevent', 'datum', 'theft', 'tessian', 'report', 'employee', 'take', 'generate', 'datum', 'train', 'model', 'leave', 'job', 'defense', 'threat', 'come', 'question', 'data', 'owner', 'prove', 'suspect', 'model', 'derive', 'dataset', 'exist', 'solution', 'recent', 'mainstream', 'solution', 'dataset', 'copyright', 'protection', 'classify', 'category', 'watermarke', 'dataset', 'inference', 'membership', 'inference', 'watermarking', 'approach', 'aim', 'inject', 'sample', 'specific', 'distribution', 'prior', 'publish', 'dataset', 'auditor', 'need', 'postevent', 'mechanism', 'however', 'opensource', 'datum', 'already', 'publish', 'real', 'world', 'contrast', 'watermarking', 'technique', 'dataset', 'inference', 'strategy', 'require', 'injection', 'explicit', 'watermark', 'dataset', 'train', 'model', 'implement', 'auditing', 'first', 'train', 'critic', 'model', 'predict', 'cumulative', 'reward', 'stateaction', 'pair', 'dataset', 'audit', 'target', 'dataset', 'straightfor', 'ward', 'strategy', 'derive', 'auditing', 'result', 'compare', 'cumulative', 'reward', 'stateaction', 'pair', 'suspect', 'model', 'target', 'dataset', 'preset', 'judgment', 'threshold', 'similarity', 'however', 'design', 'threshold', 'value', 'challenge', 'depend', 'distribution', 'pre', 'collect', 'dataset', 'vary', 'different', 'task', 'setting', 'collection', 'procedure', 'datum', 'postprocessing', 'method', 'address', 'issue', 'recognize', 'cumulative', 'reward', 'embed', 'stateaction', 'pair', 'model', 'esti', 'mate', 'cumulative', 'reward', 'target', 'dataset', 'offline', 'model', 'fit', 'cumulative', 'reward', 'dataset', 'training', 'thus', 'train', 'multiple', 'model', 'target', 'dataset', 'vary', 'initialization', 'optimization', 'shadow', 'model', 'collect', 'cumulative', 'reward', 'stateaction', 'pair', 'finally', 'compare', 'cumulative', 'reward', 'suspect', 'model', 'shadow', 'model', 'make', 'audit', 'decision', 'hypothesis', 'testing', 'evaluation', 'experimental', 'result', 'show', 'auditing', 'accuracy', 'orlauditor', 'exceed', 'false', 'positive', 'rate', 'less', 'multiple', 'drl', 'model', 'task', 'visualize', 'cumulative', 'reward', 'shadow', 'model', 'train', 'different', 'dataset', 'demonstrate', 'cumula', 'tive', 'reward', 'distinguishable', 'feature', 'dataset', 'audit', 'far', 'evaluate', 'influential', 'factor', 'practical', 'adop', 'tion', 'orlauditor', 'number', 'shadow', 'model', 'significance', 'level', 'hypothesis', 'testing', 'trajectory', 'size', 'first', 'shadow', 'model', 'improve', 'audit', 'accuracy', 'orlauditor', 'demonstrate', 'exceptional', 'performance', 'audit', 'accuracy', 'exceed', 'utilize', 'mere', 'shadow', 'model', 'illustrate', 'table', 'viii', 'second', 'minimum', 'sig', 'nificance', 'level', 'orlauditor', 'mean', 'auditor', 'output', 'single', 'result', 'confidence', 'third', 'orlauditor', 'tend', 'obtain', 'high', 'accuracy', 'large', 'trajectory', 'size', 'yet', 'also', 'notice', 'small', 'trajectory', 'size', 'achieve', 'well', 'result', 'task', 'far', 'implement', 'orlauditor', 'audit', 'opensource', 'dataset', 'deepmind', 'experimental', 'result', 'demonstrate', 'effectiveness', 'orlauditor', 'practice', 'robustness', 'evaluate', 'robustness', 'orlauditor', 'implement', 'defense', 'strategy', 'prevent', 'auditing', 'first', 'strategy', 'involve', 'use', 'stateoftheart', 'bership', 'inference', 'defense', 'technique', 'ensemble', 'architecture', 'propose', 'defense', 'mechanism', 'audit', 'accuracy', 'orlauditor', 'still', 'addition', 'ble', 'architecture', 'suspect', 'model', 'distort', 'action', 'hide', 'training', 'dataset', 'offline', 'drl', 'model', 'real', 'world', 'decisionmake', 'task', 'selfdrive', 'car', 'often', 'use', 'gaussian', 'noise', 'model', 'natural', 'distortion', 'thus', 'add', 'gaussian', 'noise', 'action', 'stealthy', 'avoid', 'auditor', 'detection', 'noise', 'convenient', 'mathematical', 'manipulation', 'simulate', 'strong', 'weak', 'action', 'distortion', 'normalize', 'dimension', 'action', 'space', '−1', 'use', 'gaussian', 'noise', 'µ', 'respectively', 'experiment', 'show', 'auditor', 'slightly', 'affect', 'gaussian', 'noise', 'fig', 'intuitive', 'explanation', 'orlauditor', 'middle', 'surface', 'cumulative', 'reward', 'stateaction', 'pair', 'dataset', 'auditor', 'output', 'positive', 'result', 'cumulative', 'reward', 'suspect', 'model', 'stateaction', 'pair', 'outer', 'surface', 'maini', 'separately', 'propose', 'dataset', 'inference', 'method', 'supervised', 'learning', 'selfsupervise', 'learning', 'model', 'enable', 'model', 'owner', 'provide', 'convincing', 'statistical', 'argument', 'particular', 'model', 'train', 'private', 'datum', 'however', 'dataset', 'inference', 'label', 'need', 'distance', 'datum', 'decision', 'boundary', 'possible', 'obtain', 'rl', 'continuous', 'output', 'dataset', 'inference', 'label', 'use', 'similarity', 'model', 'behavior', 'detect', 'unauthorized', 'dataset', 'usage', 'require', 'public', 'dataset', 'generate', 'surrogate', 'model', 'form', 'auditing', 'basis', 'compare', 'behavioral', 'difference', 'surrogate', 'model', 'model', 'train', 'private', 'datum', 'scene', 'distribution', 'collect', 'dataset', 'depend', 'environment', 'operator', 'difficult', 'determine', 'suitable', 'public', 'dataset', 'train', 'surrogate', 'model', 'make', 'audit', 'basis', 'hard', 'establish', 'third', 'category', 'adopt', 'notion', 'membership', 'inference', 'collect', 'rl', 'model', 'behavior', 'train', 'example', 'member', 'untrained', 'example', 'nonmember', 'classifier', 'construct', 'determine', 'data', 'sample', 'use', 'model', 'learn', 'process', 'however', 'online', 'scenario', 'auditor', 'collect', 'additional', 'datum', 'environment', 'nonmember', 'example', 'offline', 'case', 'auditor', 'access', 'environment', 'proposal', 'paper', 'propose', 'first', 'practical', 'dataset', 'auditing', 'paradigm', 'offline', 'auditor', 'concretely', 'inspire', 'fact', 'cumulative', 'reward', 'sum', 'reward', 'receive', 'period', 'time', 'start', 'give', 'stateaction', 'pair', 'guide', 'rl', 'model', 'learn', 'behavior', 'policy', 'thus', 'reward', 'intrinsic', 'feature', 'dataset', 'make', 'suitable', 'audit', 'basis', 'figure', 'provide', 'schematic', 'diagram', 'orlauditor', 'state', 'action', 'cumulative', 'reward', 'compose', 'threedimensional', 'space', 'surface', 'illustrate', 'exact', 'cumulative', 'reward', 'dataset', 'surface', 'show', 'possible', 'offset', 'exact', 'cumulative', 'reward', 'learn', 'offline', 'drl', 'model', 'randomness', 'initialization', 'learning', 'process', 'suspect', 'model', 'auditor', 'output', 'positive', 'result', 'datum', 'use', 'train', 'model', 'cumulative', 'reward', 'stateaction', 'pair', 'fall', 'surface', 'otherwise', 'negative', 'outcome', 'cumulative', 'reward', '𝑎𝑎', '𝑎𝑎', '𝑠𝑠', '𝑎𝑎', 'state', 'action', '𝑎𝑎', 'positive', 'negative', 'share', 'helpcontact', 'tpr', 'value', 'auditor', 'decline', 'yet', 'strong', 'distortion', 'also', 'impact', 'performance', 'suspect', 'model', 'especially', 'complex', 'task', 'contribution', 'contribution', 'threefold', 'knowledge', 'orlauditor', 'first', 'dataset', 'audit', 'ing', 'method', 'offline', 'drl', 'model', 'use', 'cumulative', 'reward', 'intrinsic', 'stable', 'fingerprint', 'dataset', '•', 'demonstrate', 'effectiveness', 'orlauditor', 'offline', 'drl', 'model', 'task', 'also', 'systematically', 'analyze', 'various', 'experimental', 'factor', 'hyperparam', 'eter', 'setting', 'robustness', 'orlauditor', 'summarize', 'important', 'guideline', 'adopt', 'orl', 'auditor', 'practice', 'implement', 'orlauditor', 'opensource', 'dataset', 'deepmind', 'show', 'orlauditor', 'serve', 'potent', 'audit', 'solution', 'scenario', 'background', 'offline', 'rl', 'problem', 'offline', 'reinforcement', 'learning', 'model', 'aim', 'learn', 'optimal', 'nearly', 'optimal', 'policy', 'pre', 'collect', 'dataset', 'interactive', 'environment', 'use', 'represent', 'rl', 'model', 'input', 'output', 'space', 'formally', 'call', 'state', 'action', 'scene', 'r', 'temporal', 'reward', 'time', 'step', 'r', 'real', 'number', 'set', 'unit', 'precollecte', 'dataset', 'call', 'transition', 'element', 'set', 'st1', 'successive', 'state', 'set', 'transition', 'chronological', 'order', 'form', 'trajectory', 'dataset', 'base', 'transition', 'offline', 'model', 'learn', 'markov', 'decision', 'process', 'dataset', 'form', 'policy', 'πθ', 'maximize', 'j', 'π', 'π', 'est∼dβ', 'h', 'use', 'dβ', 'denote', 'distribution', 'state', 'action', 'dataset', 'action', 'sample', 'accord', 'behavior', 'policy', 'πθ', 'discount', 'factor', 'apply', 'discount', 'future', 'reward', 'accumulate', 'reward', 'h', 'terminal', 'time', 'step', 'trajectory', 'example', 'figure', 'show', 'example', 'base', 'cartpole', 'task', 'datum', 'collection', 'process', 'dataset', 'generate', 'operation', 'log', 'operator', 'envi', 'ronment', 'contain', 'position', 'velocity', 'cart', 'pole', 'state', 'operator', 'force', 'direction', 'action', 'corresponding', 'reward', 'training', 'evaluation', 'process', 'offline', 'model', 'learn', 'play', 'cartpole', 'task', 'precollecte', 'dataset', 'generate', 'datum', 'collection', 'process', 'finally', 'deploy', 'model', 'environment', 'perform', 'task', 'controlcart', 'pole', 'fig', 'running', 'example', 'offline', 'model', 'offline', 'rl', 'model', 'section', 'first', 'introduce', 'offline', 'separately', 'represent', 'basic', 'idea', 'offline', 'model', 'policy', 'constraint', 'strategy', 'value', 'function', 'regularization', 'strategy', 'many', 'state', 'method', 'modify', 'approach', 'far', 'present', 'stateoftheart', 'minimalistic', 'light', 'computation', 'hyperparameter', 'set', 'overhead', 'addition', 'briefly', 'describe', 'behavior', 'clone', 'learn', 'stateaction', 'distribution', 'dataset', 'supervised', 'learning', 'approach', 'typical', 'reinforcement', 'learning', 'method', 'solve', 'offline', 'problem', 'usually', 'serve', 'baseline', 'method', 'offline', 'evaluation', 'behavior', 'clone', 'separately', 'take', 'pairwise', 'state', 'action', 'dataset', 'input', 'label', 'optimize', 'policy', 'follow', 'function', 'θ∗', 'arg', 'l', 'πθ', 'precollecte', 'dataset', 'l', 'loss', 'function', 'imitate', 'action', 'distribution', 'performance', 'close', 'mean', 'dataset', 'even', 'work', 'well', 'online', 'algorithm', 'case', 'batchconstraine', 'qlearne', 'first', 'practical', 'datadriven', 'key', 'idea', 'integrate', 'generative', 'model', 'achieve', 'notion', 'batchconstraine', 'minimize', 'deviation', 'candidate', 'action', 'action', 'record', 'dataset', 'maintain', 'diversity', 'action', 'build', 'perturbation', 'model', 'perturb', 'select', 'action', 'choose', 'highestvalue', 'action', 'qnetwork', 'learn', 'estimate', 'expect', 'cumulative', 'reward', 'give', 'state', 'action', 'pair', 'thus', 'objective', 'function', 'define', 'follow', 'π', 'argmax', 'ai', 'ai', 'φ', 'aiξϕ', 'ai', 'φ', 'ai', 'conditional', 'variational', 'autoencoder', 'vae', 'base', 'generative', 'model', 'use', 'generate', 'candidate', 'action', 'value', 'function', 'qθ', 'use', 'score', 'n', 'candidate', 'action', 'find', 'action', 'high', 'value', 'ai', 'φ', 'perturbation', 'model', 'output', 'dataset', '𝒓𝒓𝒕𝒕', '𝒔𝒔𝒕𝒕', 'state', 'reward', 'move', 'leave', 'r', 'move', 'right', 'r', 'move', 'leave', 'r', 'action', 'environment', 'operator', 'offline', 'model', 'state', 'datum', 'collection', 'training', 'evaluation', 'deployment', 'adjustment', 'action', 'range', '−φ', 'perturbation', 'model', 'optimize', 'deterministic', 'policy', 'gradient', 'follow', 'φ', 'represent', 'minibatch', 'stateaction', 'pair', 'dataset', 'penalize', 'rare', 'state', 'take', 'convex', 'combination', 'value', 'qnetwork', 'set', 'new', 'target', 'value', 'update', 'qnetwork', 'cid20', 'ai', 'ai', 'ai', 'correspond', 'perturb', 'action', 'sample', 'generative', 'model', 'implicit', 'qlearning', 'iql', 'compare', 'batch', 'constrain', 'idea', 'iql', 'strictly', 'avoid', 'query', 'ing', 'value', 'action', 'precollecte', 'dataset', 'iql', 'first', 'construct', 'model', 'evaluate', 'expect', 'return', 'stateaction', 'pair', 'objective', 'function', 'define', 'show', 'equation', 'l', 'θ', 'ed', 'cid2', 'cid0', 'r', 'γqˆθ', 'cid1', 'cid3', 'u', 'τ', 'u', 'represent', 'successor', 'state', 'action', 'qˆθ', 'use', 'assess', 'expect', 'return', 'state', 'action', 'pair', 'parameter', 'adjust', 'optimization', 'round', 'parameter', 'update', 'periodically', 'base', 'reduce', 'parameter', 'fluctuation', 'model', 'update', 'equation', 'involve', 'dynamic', 'environment', 'environment', 'state', 'transition', 'next', 'environment', 'state', 'potentially', 'introduce', 'interference', 'evaluation', 'expect', 'return', 'stateaction', 'pair', 'iql', 'address', 'issue', 'introduce', 'new', 'state', 'value', 'model', 'splitting', 'equation', 'objective', 'function', 'equation', 'show', 'objective', 'function', 'state', 'value', 'model', 'ed', 'cid2', 'cid0', 'cid3', 'iql', 'utilize', 'construct', 'equation', 'update', 'parameter', 'stateaction', 'value', 'model', 'lq', 'θ', 'cid104', 'r', 'γvψ', 'finally', 'iql', 'consider', 'use', 'stateaction', 'value', 'model', 'construct', 'behavior', 'policy', 'deployment', 'behavior', 'policy', 'also', 'need', 'avoid', 'action', 'dataset', 'distribution', 'thus', 'iql', 'employ', 'advantageweighted', 'regre', 'sion', 'update', 'policy', 'model', 'ed', 'exp', 'log', '∞', 'represent', 'inverse', 'temperature', 'small', 'value', 'iql', 'similar', 'behavior', 'clone', 'tend', 'mimic', 'datum', 'collection', 'policy', 'large', 'value', 'iql', 'inclined', 'select', 'action', 'correspond', 'high', 'expect', 'return', 'accord', 'stateaction', 'value', 'model', 'entire', 'training', 'process', 'iql', 'alternate', 'optimize', 'parameter', 'update', 'keep', 'fix', 'former', 'method', 'limit', 'regularize', 'action', 'selection', 'learn', 'policy', 'easy', 'evaluate', 'give', 'dataset', 'however', 'introduce', 'new', 'hyperparameter', 'often', 'leverage', 'secondary', 'component', 'generative', 'model', 'adjust', 'underlie', 'minimalist', 'highly', 'effective', 'offline', 'base', 'delay', 'deep', 'deterministic', 'policy', 'gradient', 'td3', 'regularization', 'term', 'push', 'policy', 'favor', 'action', 'contain', 'dataset', 'π', 'argmax', 'e', 'cid2', 'π', 'π', 'cid3', 'n', 'q', 'dataset', 'n', 'transition', 'facilitate', 'policy', 'training', 'normalize', 'state', 'give', 'dataset', 'si', 'si−µ', 'σϵ', 'σ', 'mean', 'standard', 'deviation', 'respectively', 'model', 'architecture', 'vary', 'significantly', 'regard', 'objec', 'tive', 'function', 'basic', 'model', 'structure', 'objective', 'function', 'use', 'policy', 'constraint', 'strategy', 'maintain', 'learn', 'policy', 'similar', 'one', 'use', 'collect', 'dataset', 'contrast', 'iql', 'adopt', 'regularization', 'strategy', 'improve', 'stochasticity', 'learn', 'policy', 'obtain', 'accurate', 'qvalue', 'estimation', 'basic', 'model', 'structure', 'iql', 'base', 'qlearning', 'model', 'build', 'td3', 'section', 'experiment', 'mainly', 'conduct', 'algorithm', 'however', 'orlauditor', 'also', 'apply', 'type', 'offline', 'drl', 'model', 'long', 'auditor', 'blackbox', 'access', 'suspect', 'model', 'problem', 'statement', 'exist', 'solution', 'system', 'threat', 'model', 'dataset', 'application', 'scenario', 'figure', 'illustrate', 'typical', 'plication', 'scenario', 'datum', 'provider', 'collect', 'publish', 'sell', 'customer', 'malicious', 'customer', 'adversary', 'access', 'dataset', 'make', 'piracy', 'distribution', 'illegally', 'build', 'modelasaservice', 'maas', 'platform', 'institution', 'suspect', 'model', 'generate', 'dataset', 'thus', 'hire', 'auditor', 'determine', 'model', 'trainer', 'pirate', 'trajectory', 'dataset', 'd1', 'auditor', 'background', 'knowledge', 'capability', 'auditor', 'full', 'knowledge', 'target', 'dataset', 'number', 'trajectory', 'space', 'state', 'action', 'offline', 'setting', 'auditor', 'prohibit', 'interact', 'online', 'environment', 'collect', 'datum', 'mean', 'entire', 'auditing', 'depend', 'target', 'dataset', 'consider', 'auditor', 'blackbox', 'access', 'model', 'note', 'general', 'challenging', 'scenario', 'auditor', 'typical', 'application', 'scenario', 'adver', 'sary', 'receive', 'model', 'setting', 'customer', 'select', 'offline', 'model', 'hyperparameter', 'desire', 'training', 'episode', 'adversary', 'train', 'offline', 'rl', 'model', 'provide', 'service', 'interface', 'customer', 'auditor', 'utilize', 'state', 'dataset', 'input', 'query', 'suspect', 'model', 'obtain', 'correspond', 'action', 'output', 'explicit', 'watermark', 'dataset', 'train', 'model', 'exist', 'method', 'divide', 'category', 'accord', 'explicit', 'classification', 'label', 'explicit', 'classification', 'label', 'rely', 'compute', 'distance', 'datum', 'point', 'decision', 'boundary', 'explicit', 'classification', 'label', 'utilize', 'similarity', 'model', 'behavior', 'detect', 'unauthorized', 'usage', 'dataset', 'require', 'assumption', 'additional', 'public', 'dataset', 'similar', 'distribution', 'form', 'auditing', 'basis', 'however', 'method', 'directly', 'apply', 'reinforcement', 'learning', 'case', 'reason', 'first', 'labelbase', 'dataset', 'inference', 'implement', 'rl', 'model', 'output', 'usually', 'continuous', 'guide', 'rough', 'reward', 'signal', 'instead', 'exact', 'label', 'second', 'distribution', 'offline', 'dataset', 'depend', 'environment', 'also', 'rely', 'strategy', 'interact', 'environment', 'thus', 'challenge', 'find', 'proper', 'public', 'dataset', 'scenario', 'delve', 'become', 'evident', 'behavior', 'similarity', 'drl', 'model', 'vary', 'different', 'public', 'training', 'datum', 'furthermore', 'behavior', 'similarity', 'also', 'influence', 'various', 'offline', 'drl', 'framework', 'membership', 'inference', 'attack', 'several', 'membership', 'inference', 'attack', 'exist', 'seem', 'address', 'problem', 'study', 'paper', 'target', 'scene', 'assume', 'attacker', 'environment', 'thus', 'utilize', 'environment', 'collect', 'datum', 'even', 'manipulate', 'adversarial', 'state', 'facilitate', 'inference', 'however', 'paper', 'aim', 'offline', 'case', 'challenging', 'thing', 'auditor', 'use', 'precollecte', 'dataset', 'scenario', 'exist', 'mia', 'rely', 'environment', 'generate', 'nonmember', 'datum', 'orlauditor', 'instantiate', 'q', 'figure', 'cumulative', 'reward', 'intrinsic', 'feature', 'dataset', 'suitable', 'auditing', 'determine', 'shadow', 'model', 'train', 'dataset', 'instead', 'preset', 'threshold', 'adapt', 'distribution', 'different', 'dataset', 'thus', 'welldesigne', 'q', 'guarantee', 'adaptiveness', 'effectiveness', 'orlauditor', 'workflow', 'ease', 'understanding', 'refer', 'target', 'dataset', 'dataset', 'audit', 'actual', 'dataset', 'dataset', 'use', 'suspect', 'model', 'suspect', 'model', 'train', 'target', 'dataset', 'actual', 'dataset', 'target', 'dataset', 'positive', 'audit', 'result', 'suspect', 'model', 'otherwise', 'suspect', 'model', 'use', 'target', 'dataset', 'negative', 'audit', 'result', 'suspect', 'model', 'figure', 'illustrate', 'workflow', 'orlauditor', 'fig', 'example', 'application', 'scenario', 'auditor', 'obtain', 'information', 'dataset', 'd1', 'knowledge', 'dataset', 'institution', 'discussion', 'compare', 'samplelevel', 'datasetlevel', 'datum', 'scene', 'trajectorylevel', 'datum', 'minimum', 'record', 'unit', 'sequential', 'interaction', 'operator', 'environment', 'single', 'trajectory', 'guide', 'model', 'initial', 'state', 'terminal', 'trajectorylevel', 'datum', 'regard', 'value', 'unit', 'dataset', 'thus', 'orlauditor', 'design', 'audit', 'dataset', 'trajectory', 'level', 'auditor', 'try', 'decide', 'suspect', 'model', 'use', 'specific', 'trajectory', 'dataset', 'addition', 'auditor', 'easily', 'extend', 'orlauditor', 'datasetlevel', 'datum', 'set', 'piracy', 'alarm', 'threshold', 'ratio', 'misappropriation', 'use', 'trajectory', 'exceed', 'preset', 'threshold', 'auditor', 'claim', 'datasetlevel', 'pirate', 'b', 'exist', 'solution', 'watermarking', 'watermarkingbase', 'dataset', 'copy', 'right', 'protection', 'method', 'inject', 'sample', 'specific', 'distribu', 'tion', 'publish', 'target', 'dataset', 'kind', 'implement', 'backdoor', 'attack', 'propose', 'modify', 'dataset', 'add', 'trigger', 'local', 'patch', 'innocent', 'sample', 'order', 'make', 'appear', 'predefine', 'target', 'class', 'verify', 'integrity', 'dataset', 'attack', 'use', 'hypothesis', 'test', 'approach', 'base', 'posterior', 'probability', 'generate', 'third', 'party', 'model', 'inspire', 'idea', 'auditor', 'employ', 'backdoor', 'attack', 'drl', 'model', 'generate', 'watermark', 'offline', 'dataset', 'however', 'opensource', 'dataset', 'already', 'pub', 'lishe', 'auditor', 'need', 'postevent', 'mechanism', 'require', 'inject', 'manipulate', 'sample', 'publish', 'dataset', 'watermarking', 'hand', 'preevent', 'mechanism', 'involve', 'inject', 'manipulate', 'sample', 'dataset', 'publish', 'additionally', 'difficult', 'auditor', 'guarantee', 'effective', 'watermarking', 'consistent', 'distribution', 'original', 'dataset', 'evitably', 'disturb', 'model', 'normal', 'behavior', 'dataset', 'inference', 'core', 'idea', 'dataset', 'inference', 'empower', 'model', 'owner', 'make', 'com', 'pelling', 'statistical', 'argument', 'particular', 'model', 'copy', 'version', 'model', 'demonstrate', 'base', 'private', 'training', 'datum', 'require', 'inject', 'step', 'model', 'preparation', 'mp', 'left', 'box', 'figure', 'auditor', 'prepare', 'critic', 'model', 'shadow', 'model', 'base', 'target', 'dataset', 'contain', 'length', 'critic', 'model', 'optimize', 'estimate', 'cumulative', 'reward', '𝒋𝒋', '𝒋𝒋', '𝒋𝒋', '𝒔𝒔𝒏𝒏𝟏𝟏', '𝒂𝒂𝒏𝒏𝟏𝟏', '𝒓𝒓𝒏𝒏𝟏𝟏', '𝒂𝒂𝒏𝒏𝒋𝒋', '𝒋𝒋', '𝒔𝒔𝒆𝒆𝒏𝒏𝒆𝒆', 'transition', 'trajectory', 'institution', 'dataset', 'institution', 'dataset', 'institution', 'dataset', 'v', 'e', 'r', 'r', 'model', 'model', '𝜋𝜋', 'model', 'auditor', 'institution', 'model', 'pirate', 'th', 'trajectory', 'dataset', '𝒋𝒋', 'fig', 'workflow', 'orlauditor', 'contain', 'step', 'ie', 'model', 'preparation', 'cumulative', 'reward', 'collection', 'audit', 'process', 'orlauditor', 'first', 'train', 'set', 'shadow', 'drl', 'model', 'critic', 'model', 'target', 'dataset', 'collect', 'cumulative', 'reward', 'stateaction', 'pair', 'shadow', 'model', 'suspect', 'model', 'finally', 'orlauditor', 'audits', 'trajectory', 'base', 'hypothesis', 'testing', 'stateaction', 'pair', 'trajectory', 'dataset', 'series', 'prediction', 'stateaction', 'pair', 'compose', 'exclusive', 'feature', 'auditing', 'way', 'optimize', 'critic', 'model', 'montecarlobase', 'mcbase', 'temporaldifferencebase', 'tdbase', 'strategy', 'adopt', 'tdbase', 'learning', 'method', 'explain', 'reason', 'section', 'ivb', 'addition', 'auditor', 'train', 'set', 'shadow', 'model', 'follow', 'model', 'objective', 'function', 'introduce', 'section', 'different', 'model', 'initialization', 'step', 'cumulative', 'reward', 'collection', 'crc', 'shadow', 'model', 'observe', 'state', 'dataset', 'take', 'action', 'trajectory', 'dataset', 'auditor', 'record', 'state', 'action', 'ai', 'shadow', 'model', 'represent', 'shadow', 'model', 'action', 'step', 'trajectory', 'ti', 'finish', 'action', 'collection', 'auditor', 'obtain', 'k', 'set', 'stateaction', 'pair', 'shadow', 'model', 'represent', 'learn', 'policy', 'different', 'initialization', 'training', 'process', 'target', 'dataset', 'use', 'critic', 'model', 'step', 'auditor', 'calculate', 'estimation', 'stateaction', 'record', 'estimate', 'cumulative', 'reward', 'sampling', 'exact', 'cumulative', 'reward', 'corresponding', 'stateaction', 'pair', 'dataset', 'similarly', 'auditor', 'query', 'suspect', 'model', 'stateaction', 'pair', 'put', 'critic', 'model', 'obtain', 'estimation', 'suspect', 'model', 'observe', 'action', 'ai', 'step', 'audit', 'process', 'step', 'auditor', 'obtain', 'estimate', 'cumulative', 'reward', 'shadow', 'model', 'suspect', 'model', 'con', 'duct', 'audit', 'process', 'jectory', 'dataset', 'auditor', 'collect', 'series', 'estimate', 'cumulative', 'reward', 'shadow', 'model', 'qi', 'k', 'suspect', 'model', 'qs', 'orlauditor', 'conduct', 'hypothesis', 'testing', 'base', '¯qj', 'auditor', 'distance', 'distribution', 'rule', 'suspicion', 'qs', 'j', 'k', 'otherwise', 'auditor', 'qi', 'conclude', 'positive', 'decision', 'suspect', 'model', 'train', 'use', 'trajectory', 'auditor', 'repeatedly', 'implement', 'process', 'trajectory', 'dataset', 'obtain', 'qs', 'final', 'audit', 'report', 'judgment', 'trajectory', 'discuss', 'detail', 'distance', 'metric', 'hypothesis', 'testing', 'section', 'selection', 'critic', 'model', 'auditor', 'use', 'base', 'temporaldifference', 'base', 'algorithm', 'train', 'critic', 'model', 'trajectory', 'dataset', 'main', 'distinction', 'method', 'lie', 'learning', 'target', 'lead', 'difference', 'objective', 'function', 'case', 'mcbase', 'method', 'learn', 'target', 'empirical', 'cumulative', 'reward', 'dataset', 'γh−1rh', 'represent', 'exact', 'cumulative', 'reward', 'terminal', 'time', 'step', 'h', 'trajectory', 'discount', 'factor', 'apply', 'discount', 'future', 'reward', 'critic', 'model', 'train', 'minimize', 'follow', 'objective', 'e', 'rt1', 'st1', 'cid104', 'cid105', 'tdbase', 'method', 'learn', 'target', 'change', 'expect', 'cumulative', 'reward', 'heuristic', 'form', 'γq', 'st1', 'at1', 'thus', 'critic', 'model', 'train', 'mini', 'mize', 'follow', 'loss', 'function', 'e', 'rt1', 'st1', 'cid2', 'rt1', 'st1', 'at1', 'cid3', 'critic', 'model', 'start', 'arbitrary', 'initialization', 'θ', 'repeatedly', 'evaluate', 'obtain', 'reward', 'rt1', 'update', 'weight', 'θ′', 'snapshot', 'θ', 'copy', 'θ', 'update', 'θ', 'mcbase', 'method', 'utilize', 'exact', 'cumulative', 'reward', 'dataset', 'train', 'critic', 'model', 'result', 'unbiased', 'prediction', 'also', 'strong', 'convergence', 'property', 'stationary', 'however', 'apply', 'situation', 'collect', 'datum', 'truncate', 'trajectory', 'dataset', 'complete', 'practice', 'many', 'sequential', 'decisionmake', 'task', 'usually', 'long', 'infinite', 'time', 'step', 'thus', 'dataset', 'provider', 'segment', 'interaction', 'record', 'trajectory', 'preset', 'maximum', 'length', 'tdbase', 'method', 'tackle', 'limitation', 'mcbase', 'learn', 'incomplete', 'sequence', 'nevertheless', 'heuristic', 'learn', 'process', 'step', 'model', 'preparation', 'step', 'cumulative', 'reward', 'collection', 'dataset', '𝑟𝑡', '𝑠𝑡1', '𝑟𝑡', '𝑠𝑡1', '𝑛2', '𝑛𝑚', 'train', 'shadow', 'model', 'critic', 'model', '𝑎', 'shadow', 'model', '𝑛2', '𝑛𝑚', 'state', 'model', '𝑛2', '𝑛𝑚', 'model', '𝑛1', 'model', '𝑎𝑡', '𝑡', '𝑛2', '𝑡', '𝑛2', '𝑛𝑚', '𝑛𝑚', 'state', 'action', 'critic', 'model', '𝑠', 'cumulative', 'reward', '𝑎𝑡', '𝑛1', '𝑛2', '𝑛𝑚', '𝑎𝑡', '𝑡', '𝑛1', '𝑡', '𝑛2', '𝑛𝑚', 'suspect', 'model', 'state', 'action', 'critic', 'model', '𝑠', 'cumulative', 'reward', 'audit', 'metric', 'step3', 'audit', 'process', '𝒋th', 'trajectory', 'dataset', '𝑚', '⋯', 'length', 'trajectory', '𝑑', '𝑑', 'elementwise', 'mean', '𝑑', '⋯', '𝑑', 'positive', 'negative', 'hypothesis', 'testing', 'negative', 'workflow', 'orlauditor', 'input', 'suspect', 'model', 'number', 'shadow', 'model', 'significance', 'level', 'output', 'trajectorylevel', 'audit', 'report', 'step', 'model', 'preparation', 'train', 'shadow', 'model', 'k', 'critic', 'model', 'step', 'datum', 'preparation', 'model', 'πi', 'k', '∪', 'query', 'state', 'obtain', 'action', 'evaluate', 'pair', 'base', 'critic', 'model', 'record', 'cumulative', 'reward', 'sequential', 'form', 'qj', 'j', 'end', 'step', 'audit', 'process', 'audit', 'report', 'trajectory', 'j', 'calculate', 'elementwise', 'mean', '¯qj', 'k', 'measure', 'qj', 'qi', 'hypothesis', 'testing', 'k', 'decide', 'j', '¯qj', 'j', 'qs', 'suspect', 'model', 'pirate', 'significance', 'level', 'audit', 'reportappend', 'audit', 'result', 'end', 'return', 'audit', 'report', 'tdbase', 'method', 'bias', 'sensitive', 'model', 'initialization', 'therefore', 'choose', 'elementwise', 'mean', 'shadow', 'model', 'cumulative', 'reward', '¯q', 'auditing', 'directrix', 'section', 'iva', 'instead', 'rely', 'solely', 'critic', 'model', 'prediction', 'compensate', 'shortage', 'tdbase', 'method', 'detail', 'audit', 'process', 'audit', 'process', 'choice', 'distance', 'metric', 'hypothesis', 'testing', 'method', 'play', 'critical', 'role', 'auditor', 'performance', 'proper', 'metric', 'sensitive', 'deviation', 'estimate', 'cumulative', 'reward', 'facilitate', 'hypothesis', 'test', 'suitable', 'hypothesis', 'test', 'ing', 'method', 'provide', 'precise', 'result', 'high', 'confidence', 'distance', 'metric', 'consider', 'type', 'distance', 'metric', 'norm', 'cosine', 'distance', 'distance', 'ℓp', 'norm', 'popular', 'method', 'measure', 'distance', 'vector', 'sum', 'absolute', 'difference', 'compo', 'nent', 'vector', 'rl', 'scene', 'state', 'action', 'sequential', 'datum', 'mean', 'distance', 'metric', 'measure', 'value', 'position', 'deviation', 'cumulative', 'reward', 'however', 'ℓp', 'norm', 'fail', 'reflect', 'difference', 'sequence', 'aspect', 'set', 'value', 'cosine', 'distance', 'derivative', 'cosine', 'similarity', 'define', 'cosine', 'angle', 'vector', 'cosine', 'distance', 'embody', 'difference', 'value', 'position', 'aspect', 'vector', 'however', 'cosine', 'distance', 'normalize', 'inner', 'product', 'use', 'vector', 'norm', 'weaken', 'numerical', 'difference', 'cumulative', 'reward', 'wasserstein', 'distance', 'aka', 'earth', 'mover', 'distance', 'metric', 'difference', 'probability', 'distribution', 'region', 'define', 'follow', 'inf', 'u', 'v', 'set', 'distribution', 'r', 'r', 'marginal', 'u', 'v', 'first', 'second', 'factor', 'tively', 'wasserstein', 'distance', 'fit', 'well', 'audit', 'requirement', 'reflect', 'positional', 'deviation', 'cumula', 'tive', 'reward', 'thus', 'set', 'wasserstein', 'distance', 'default', 'compare', 'different', 'distance', 'metric', 'section', 'hypothesis', 'testing', 'selection', 'distance', 'metric', 'auditor', 'proceed', 'hypothesis', 'testing', 'distance', 'qs', 'j', '¯qj', 'h0', 'qs', 'j', 'outlier', 'j', 'outlier', 'intuitive', 'method', 'leverage', 'principle', 'normal', 'sample', 'distribute', 'range', 'time', 'standard', 'deviation', 'mean', 'principle', 'efficient', 'hypothesis', 'testing', 'method', 'yet', 'mean', 'easily', 'mislead', 'outlier', 'compare', 'principle', 'grubb', 'test', 'robust', 'sis', 'testing', 'method', 'detect', 'single', 'outlier', 'univariate', '¯qj', 'exceed', 'dataset', 'grubb', 'test', 'statistic', 'qs', 'threshold', 'derive', 'significance', 'level', 'auditor', 'deviate', 'mean', 'value', 'reject', 'claim', 'h0', 'output', 'negative', 'audit', 'result', 'set', 'sample', 'grubb', 'test', 'locate', 'outlier', 'procedure', 'calculate', 'mean', 'µd', 'standard', 'deviation', 'calculate', 'grubb', 'test', 'statistic', 'qs', 'n', 'n−2t2', 'invalid', 'suspect', 'model', 'train', 'trajectory', 'inequation', 'represent', 'upper', 'critical', 'value', 'tdistribution', 'degree', 'freedom', 'significance', 'level', 'n', 'hypothesis', 'testing', 'method', 'base', 'assump', 'tion', 'distance', 'value', 'follow', 'gaussian', 'distribution', 'thus', 'orlauditor', 'need', 'precheck', 'distance', 'value', 'shadow', 'model', 'satisfy', 'gaussian', 'distribution', 'adopt', 'andersondarle', 'test', 'fit', 'scenario', 'auditor', 'small', 'number', 'sampling', 'actual', 'distribution', 'unknown', 'evaluation', 'distance', 'value', 'shadow', 'model', 'pass', 'andersondarle', 'test', 'randomness', 'model', 'initialization', 'training', 'orlauditor', 'conduct', 'hypothesis', 'test', 'evaluation', 'first', 'introduce', 'task', 'experimental', 'setup', 'section', 'validate', 'effectiveness', 'auditor', 'behavior', 'clone', 'offline', 'drl', 'model', 'batchconstraine', 'qlearne', 'implicit', 'learning', 'iql', 'section', 'visualize', 'cumulative', 'reward', 'tsne', 'demonstrate', 'cumulative', 'reward', 'intrinsic', 'stable', 'table', 'overview', 'task', 'continuous', 'discrete', 'illustrate', 'data', 'type', 'state', 'action', 'correspond', 'number', 'dimension', 'parenthesis', 'task', 'lunar', 'lander', 'continuous', 'state', 'shape', 'action', 'shape', 'continuous', '6dim', 'discrete', 'continuous', 'walker', 'continuous', 'continuous', 'ant', 'continuous', 'continuous', 'feature', 'dataset', 'auditing', 'section', 'far', 'evaluate', 'impact', 'factor', 'orlauditor', 'number', 'shadow', 'model', 'significance', 'level', 'hypothesis', 'testing', 'trajectory', 'size', 'section', 'finally', 'utilize', 'orlauditor', 'audit', 'opensource', 'dataset', 'deepmind', 'section', 'experimental', 'setup', 'task', 'adopt', 'lunar', 'lander', 'bipedal', 'walker', 'ant', 'task', 'gym', 'widely', 'use', 'prior', 'work', 'task', 'stem', 'distinct', 'realworld', 'problem', 'numerical', 'vector', 'contain', 'different', 'physical', 'formation', 'position', 'velocity', 'acceleration', 'task', 'involve', 'discrete', 'continuous', 'variable', 'observation', 'action', 'space', 'dimension', 'range', 'low', 'dim', 'high', 'give', 'overview', 'table', 'put', 'detail', 'b', 'dataset', 'generation', 'offline', 'model', 'preparation', 'obtain', 'dataset', 'task', 'table', 'adopt', 'idea', 'exist', 'dataset', 'publisher', 'train', 'model', 'interactive', 'envi', 'ronment', 'record', 'interaction', 'dataset', 'dataset', 'consist', 'numerical', 'vector', 'lunar', 'lander', 'transition', 'include', 'state', 'next', 'state', '6dimensional', 'continuous', '2dimensional', 'discrete', 'variable', 'action', 'continuous', 'variable', 'reward', 'scalar', 'therefore', 'transition', 'vector', 'similarly', 'datum', 'type', 'bipedal', 'walker', 'ant', '53dimensional', 'numerical', 'vector', 'respectively', 'number', 'transition', 'task', 'lunar', 'lander', 'bipedal', 'walker', '×', 'ant', 'offline', 'rl', 'model', 'learn', 'dataset', 'table', 'summarize', 'whole', 'process', 'task', 'use', 'global', 'random', 'seed', 'train', 'online', 'model', 'separately', 'collect', 'dataset', 'online', 'model', 'random', 'seed', 'online', 'model', 'generate', 'dataset', 'ease', 'read', 'dataset', 'share', 'name', 'online', 'model', 'train', 'offline', 'drl', 'model', 'dataset', 'distinct', 'global', 'random', 'seed', 'initialization', 'optimization', 'process', 'online', 'offline', 'model', 'implement', 'librarie', 'default', 'hyperparameter', 'setting', 'critic', 'model', 'adopt', 'fully', 'connect', 'neural', 'network', 'critic', 'model', 'hidden', 'layer', 'neuron', 'layer', 'optimize', 'critic', 'model', 'follow', 'tdbase', 'method', 'section', 'ivb', 'adam', 'optimizer', 'learning', 'rate', 'minibatch', 'size', 'table', 'main', 'step', 'dataset', 'generation', 'offline', 'model', 'preparation', 'detail', 'input', 'output', 'combination', 'task', 'offline', 'model', 'experiment', '↓', 'train', 'random', 'seed', '↓', 'online', 'rl', 'model', 'detail', 'table', '↓', 'collect', 'random', 'seed', '↓', 'offline', 'dataset', 'detail', 'table', '↓', 'train', 'random', 'seed', '↓', 'offline', 'rl', 'model', 'detail', 'table', 'xvi', 'table', 'xvii', 'table', 'table', 'entire', 'training', 'take', 'epoch', 'learning', 'rate', 'decay', 'epoch', 'evaluation', 'metric', 'recall', 'orlauditor', 'application', 'scenario', 'figure', 'single', 'suspect', 'model', 'audit', 'accuracy', 'well', 'characterize', 'performance', 'auditor', 'ratio', 'number', 'correctly', 'audit', 'trajectory', 'total', 'auditing', 'trajectory', 'experiment', 'positive', 'model', 'train', 'target', 'dataset', 'negative', 'model', 'train', 'dataset', 'randomly', 'mixed', 'majority', 'dominate', 'accuracy', 'thus', 'provide', 'true', 'positive', 'rate', 'tpr', 'true', 'negative', 'rate', 'tnr', 'method', 'provide', 'audit', 'performance', 'principle', 'grubbs', 'test', 'distance', 'metric', 'ℓ1', 'norm', 'norm', 'cosine', 'distance', 'distance', 'competitor', 'recall', 'section', 'iiib', 'exist', 'method', 'design', 'online', 'reinforcement', 'learning', 'scene', 'assume', 'auditor', 'continuously', 'interact', 'environment', 'obtain', 'new', 'datum', 'nonmember', 'example', 'base', 'behavioral', 'difference', 'model', 'member', 'example', 'nonmember', 'example', 'build', 'member', 'inference', 'method', 'detect', 'example', 'use', 'train', 'suspect', 'model', 'offline', 'scenario', 'access', 'environment', 'auditor', 'precollecte', 'target', 'dataset', 'thus', 'randomly', 'divide', 'target', 'dataset', 'part', 'train', 'offline', 'model', 'subset', 'separately', 'subset', 'regard', 'set', 'nonmember', 'example', 'offline', 'rl', 'model', 'train', 'subset', 'adopt', 'data', 'augmentation', 'attack', 'classifier', 'architecture', 'hyperparameter', 'setting', 'implementation', 'use', 'stablebaseline', 'd3rlpy', 'implement', 'online', 'offline', 'drl', 'model', 'separately', 'audit', 'method', 'realize', 'server', 'geforce', 'gb', 'memory', 'b', 'overall', 'audit', 'performance', 'assess', 'effectiveness', 'orlauditor', 'combination', 'task', 'model', 'fur', 'present', 'evaluation', 'efficacy', 'competitor', 'offline', 'drl', 'model', 'setup', 'table', 'train', 'offline', 'rl', 'model', 'dataset', 'obtain', 'offline', 'drl', 'model', 'table', 'performance', 'exist', 'membership', 'inference', 'attack', 'offline', 'model', 'task', 'lunar', 'train', '5009±068', '4984±139', '4988±076', 'test', '4719±190', '4548±146', '4674±237', '4538±216', '4503±155', 'experimental', 'setting', 'audit', 'dataset', 'separately', 'auditor', 'randomly', 'select', 'model', 'target', 'dataset', 'shadow', 'model', 'remain', 'model', 'model', 'dataset', 'positive', 'negative', 'suspect', 'model', 'target', 'dataset', 'randomly', 'select', 'auditing', 'trajectory', 'audit', 'unbalanced', 'amount', 'positive', 'negative', 'model', 'report', 'aggregated', 'mean', 'standard', 'deviation', 'tpr', 'tnr', 'setting', 'table', 'provide', 'audit', 'result', 'dataset', 'figure', 'lunar', 'lander', 'figure', 'bipedal', 'walker', 'figure', 'ant', 'pair', 'tpr', 'tnr', 'table', 'derive', 'diagonal', 'nondiagonal', 'value', 'corresponding', 'heatmap', 'supplementary', 'also', 'show', 'audit', 'result', 'principle', 'table', 'competitor', 'performance', 'show', 'table', 'value', 'mean', 'standard', 'variation', 'calculate', 'repeat', 'experiment', 'time', 'observation', 'follow', 'observation', 'table', 'iv', 'table', 'table', 'tpr', 'tnr', 'value', 'high', 'mean', 'orlauditor', 'valid', 'solution', 'audit', 'learned', 'dataset', 'offline', 'drl', 'model', 'instance', 'result', 'orlauditor', 'ℓ1', 'norm', 'experiment', 'setting', 'orlauditor', 'obtain', 'different', 'audit', 'accuracy', 'distance', 'metric', 'audit', 'effectiveness', 'ℓ1', 'norm', 'wasserstein', 'distance', 'well', 'ℓ2', 'norm', 'cosine', 'distance', 'table', 'table', 'vii', 'orlauditor', 'wasserstein', 'distance', 'always', 'perform', 'good', 'second', 'place', 'result', 'ℓ2', 'norm', 'usually', 'distance', 'metric', 'recall', 'section', 'distance', 'characterize', 'positional', 'deviation', 'cumulative', 'reward', 'sensitive', 'numerical', 'difference', 'cumulative', 'reward', 'slight', 'eg', 'experiment', 'norm', 'undercut', 'small', 'potential', 'difference', 'accuracy', 'audit', 'determine', 'grubb', 'test', 'outperform', 'principle', 'principle', 'empirical', 'method', 'easily', 'mislead', 'outlier', 'cumu', 'lative', 'reward', 'shadow', 'model', 'recall', 'section', 'ivc', 'grubb', 'test', 'first', 'calculate', 'statistic', 'g', 'compare', 'g', 'adaptive', 'threshold', 'number', 'sample', 'also', 'consider', 'hypothesis', 'testing', 'new', 'datum', 'environment', 'fectiveness', 'exist', 'membership', 'inference', 'method', 'attenuate', 'perspective', 'similarity', 'sub', 'dataset', 'split', 'dataset', 'result', 'train', 'rl', 'model', 'exhibit', 'undifferentiated', 'behavior', 'make', 'difficult', 'effectively', 'distinguish', 'member', 'member', 'hand', 'consider', 'result', 'present', 'figure', 'conclude', 'action', 'rl', 'model', 'directly', 'utilize', 'foundation', 'membership', 'inference', 'c', 'visualization', 'cumulative', 'reward', 'far', 'explain', 'audit', 'result', 'section', 'analyze', 'cumulative', 'reward', 'shadow', 'model', 'j', 'qs', 'suspect', 'model', 'j', 'use', 'tsne', 'j', 'positive', 'qs', 'setup', 'caption', 'plot', 'figure', 'indicate', 'used', 'task', 'offline', 'drl', 'model', 'point', 'plot', 'show', 'visualization', 'single', 'qi', 'negative', 'single', 'plot', 'demonstrate', 'result', 'trajectory', 'task', 'first', 'dataset', 'instance', 'target', 'dataset', 'plot', 'title', 'lunar', 'lander', 'dataset', 'table', 'positive', 'point', 'trajectory', 'collect', 'shadow', 'model', 'train', 'dataset', 'negative', 'point', 'randomly', 'sample', 'shadow', 'model', 'dataset', 'observation', 'figure', 'follow', 'obser', 'vation', 'trajectory', 'target', 'dataset', 'cumulative', 'reward', 'shadow', 'model', 'suspect', 'model', 'clearly', 'divide', 'different', 'group', 'mean', 'critic', 'model', 'well', 'reflect', 'difference', 'model', 'action', 'thus', 'cumulative', 'reward', 'generate', 'critic', 'model', 'qualified', 'postevent', 'fingerprint', 'trajectorylevel', 'auditing', 'distribution', 'point', 'vary', 'different', 'trajec', 'tory', 'example', 'trajectory', 'lunar', 'lander', 'dataset', 'hard', 'cluster', 'trajectory', 'speculate', 'trajectory', 'represent', 'basic', 'policy', 'eg', 'local', 'optimum', 'policy', 'fire', 'lander', 'thruster', 'way', 'similar', 'trajectory', 'exist', 'dataset', 'nonuniqueness', 'optimal', 'strategy', 'rl', 'problem', 'impact', 'randomness', 'model', 'training', 'process', 'collect', 'trajectory', 'unique', 'characteristic', 'thus', 'trajectory', 'cumulative', 'reward', 'clearly', 'divide', 'hyperparameter', 'study', 'impact', 'extend', 'assessment', 'scrutinize', 'pivotal', 'de', 'terminant', 'pragmatic', 'integration', 'auditor', 'specifically', 'consider', 'amount', 'shadow', 'model', 'level', 'significance', 'hypothesis', 'testing', 'magnitude', 'trajectory', 'size', 'space', 'limitation', 'give', 'brief', 'conclusion', 'section', 'refer', 'specific', 'analysis', 'c', 'e', 'impact', 'shadow', 'model', 'amount', 'change', 'shadow', 'model', 'amount', 'setting', 'section', 'figure', 'show', 'value', 'change', 'tpr', 'tnr', 'compare', 'shadow', 'model', 'figure', 'title', 'illustrate', 'setting', 'model', 'task', 'indicate', 'metric', 'yaxis', 'absolute', 'value', 'change', 'supplementary', 'provide', 'detailed', 'result', 'table', 'viii', 'shadow', 'model', 'table', 'shadow', 'model', 'table', 'tpr', 'tnr', 'result', 'base', 'grubb', 'test', 'mean', 'standard', 'deviation', 'tpr', 'tnr', 'row', 'represent', 'audit', 'result', 'combination', 'task', 'model', 'distance', 'metric', 'indicate', 'high', 'sum', 'tpr', 'tnr', 'accuracy', 'row', 'pair', 'tpr', 'tnr', 'derive', 'diagonal', 'nondiagonal', 'value', 'corresponding', 'heatmap', 'figure', 'figure', 'figure', 'supplementary', 'task', 'offline', 'model', 'norm', 'norm', 'cosine', 'distance', 'wasserstein', 'distance', 'tpr', '9901±046', '9829±114', '9861±151', '9829±204', '9936±128', '9717±296', '9980±043', 'tpr', '9648±166', '9661±250', '9717±179', '9990±036', '9984±043', 'tpr', '9984±032', '9696±582', '9920±108', '9966±043', '9972±040', 'tpr', '9840±074', '9757±117', '9832±179', '9853±125', '9931±132', '9501±672', '9936±042', '9925±124', 'tnr', '9992±014', '9963±078', 'figure', 'follow', 'observation', 'audit', 'accuracy', 'increase', 'large', 'amount', 'shadow', 'model', 'exist', 'saturation', 'point', 'audit', 'accuracy', 'expansion', 'shadow', 'model', 'impact', 'significance', 'level', 'significance', 'level', 'rep', 'resent', 'auditor', 'confidence', 'auditing', 'result', 'significance', 'level', 'section', 'adopt', 'mean', 'auditor', 'confidence', 'judg', 'ment', 'generally', 'speak', 'significance', 'level', 'represent', 'maximum', 'audit', 'capacity', 'orlauditor', 'instead', 'hyperparameter', 'setting', 'audit', 'requirement', 'dataset', 'owner', 'demand', 'auditor', 'output', 'confident', 'judgment', 'error', 'possibility', 'limit', 'figure', 'show', 'value', 'change', 'tpr', 'tnr', 'compare', 'supplementary', 'detailed', 'result', 'dataset', 'table', 'table', 'figure', 'follow', 'observation', 'complicated', 'task', 'recommend', 'auditor', 'select', 'large', 'significance', 'level', 'orlauditor', 'suspect', 'model', 'low', 'performance', 'orlauditor', 'adopt', 'large', 'significance', 'level', 'guarantee', 'audit', 'accuracy', 'general', 'safe', 'bind', 'orlauditor', 'low', 'break', 'capability', 'boundary', 'auditor', 'induce', 'auditor', 'misclassify', 'negative', 'model', 'positive', 'set', 'impact', 'trajectory', 'size', 'investigate', 'relationship', 'trajectory', 'size', 'audit', 'accuracy', 'section', 'adopt', 'fulllength', 'trajectory', 'mean', 'auditor', 'utilize', 'state', 'trajectory', 'query', 'suspect', 'model', 'obtain', 'correspond', 'action', 'conduct', 'dataset', 'auditing', 'change', 'trajectory', 'size', 'full', 'length', 'setting', 'section', 'figure', 'show', 'value', 'change', 'tpr', 'tnr', 'compare', 'fulllength', 'trajectory', 'supplementary', 'also', 'provide', 'detailed', 'result', 'table', 'table', 'xiii', 'figure', 'follow', 'observation', 'orlauditor', 'tend', 'achieve', 'high', 'accuracy', 'large', 'trajectory', 'size', 'small', 'trajectory', 'size', 'achieve', 'well', 'result', 'task', 'front', 'state', 'trajectory', 'able', 'reflect', 'behavioral', 'information', 'model', 'e', 'realworld', 'application', 'section', 'apply', 'orlauditor', 'audit', 'open', 'source', 'dataset', 'deepmind', 'choose', 'task', 'publish', 'operator', 'control', 'cheetah', 'robot', 'consist', 'link', 'joint', 'connect', 'include', 'paw', 'make', 'run', 'forward', 'right', 'fast', 'possible', 'detail', 'offline', 'drl', 'model', 'table', 'xx', 'table', 'experimental', 'setting', 'consistent', 'section', 'vb', 'observation', 'table', 'follow', 'observation', 'orlauditor', 'effective', 'realworld', 'application', 'tpr', 'orlauditor', 'exceed', 'ℓ1', 'norm', 'wasserstein', 'distance', 'mean', 'orlauditor', 'remain', 'valid', 'exist', 'opensource', 'dataset', 'wasserstein', 'distance', 'stable', 'performance', 'experimental', 'realworld', 'dataset', 'overall', 'accuracy', 'orlauditor', 'wasserstein', 'distance', 'high', 'metric', 'vi', 'robustness', 'ensemble', 'architecture', 'hinder', 'audit', 'dataset', 'adversary', 'lize', 'stateoftheart', 'membership', 'inference', 'defense', 'strategy', 'propose', 'recent', 'research', 'work', 'defense', 'strategy', 'aim', 'mitigate', 'influence', 'member', 'example', 'behavior', 'machine', 'learning', 'model', 'base', 'idea', 'model', 'ensemble', 'particular', 'propose', 'split', 'training', 'set', 'several', 'subset', 'train', 'sub', 'model', 'subset', 'auditor', 'use', 'example', 'target', 'dataset', 'query', 'suspect', 'model', 'fig', 'visualization', 'cumulative', 'reward', 'tsne', 'caption', 'plot', 'demonstrate', 'offline', 'model', 'type', 'task', 'single', 'plot', 'randomly', 'select', 'trajectory', 'first', 'dataset', 'task', 'ie', 'lunar', 'lander', 'dataset', 'bipedal', 'ant', 'dataset', 'table', 'show', 'cumulative', 'reward', 'positive', 'model', 'negative', 'model', 'trajectory', 'fig', 'impact', 'shadow', 'model', 'amount', 'change', 'value', 'tpr', 'tnr', 'number', 'shadow', 'model', 'vary', 'compare', 'default', 'shadow', 'model', 'caption', 'plot', 'demonstrate', 'offline', 'model', 'type', 'task', 'label', 'display', 'distance', 'metric', 'label', 'show', 'absolute', 'fluctuating', 'value', 'tpr', 'tnr', 'adversary', 'aggregate', 'output', 'submodel', 'train', 'example', 'setup', 'number', 'divide', 'subset', 'denote', 'repre', 'sent', 'crucial', 'hyperparameter', 'ensemblebase', 'method', 'discuss', 'consider', 'analysis', 'conduct', 'study', 'well', 'size', 'offline', 'dataset', 'establish', 'present', 'investigation', 'experimental', 'setting', 'remain', 'unchanged', 'describe', 'section', 'correspond', 'audit', 'outcome', 'present', 'table', 'v', 'supplementary', 'result', 'dataset', 'figure', 'lunar', 'lander', 'figure', 'bipedal', 'walker', 'figure', 'ant', 'figure', 'half', 'cheetah', 'observation', 'conclude', 'follow', 'observation', 'base', 'result', 'even', 'face', 'ensemble', 'architecture', 'orlauditor', 'maintain', 'high', 'level', 'audit', 'accuracy', 'show', 'table', 'tpr', 'sistently', 'exceed', 'describe', 'section', 'iva', 'auditor', 'use', 'predict', 'cumulative', 'reward', 'critic', 'model', 'basis', 'auditing', 'train', 'critic', 'model', 'capture', 'overall', 'feature', 'dataset', 'distribution', 'instead', 'memorize', 'feature', 'individual', 'sample', 'ensemble', 'model', 'train', 'target', 'dataset', 'behavior', 'embed', 'distribution', 'characteristic', 'dataset', 'lunar', 'lander', 'lunar', 'lander', 'lunar', 'lander', 'iql', 'lunar', 'lander', 'trajectory', 'audit', 'result', 'positive', 'negative', 'trajectory', 'audit', 'result', 'positive', 'negative', 'trajectory', 'audit', 'result', 'positive', 'negative', 'trajectory', 'audit', 'result', 'positive', 'negative', 'trajectory', 'audit', 'result', 'positive', 'negative', 'trajectory', 'audit', 'result', 'positive', 'negative', 'trajectory', 'audit', 'result', 'positive', 'negative', 'trajectory', 'audit', 'result', 'positive', 'negative', 'ant', 'iql', 'trajectory', 'audit', 'result', 'positive', 'negative', 'trajectory', 'audit', 'result', 'positive', 'negative', 'trajectory', 'audit', 'result', 'positive', 'negative', 'trajectory', 'audit', 'result', 'positive', 'negative', 'lunar', 'lander', 'l', 'e', 'u', 'v', 'l', 'e', 'u', 'v', 'l', 'e', 'u', 'v', 'lunar', 'lander', 'lunar', 'lander', 'iql', 'lunar', 'lander', 'l', 'l', 'l', 'l', 'p', 'r', 'r', 'r', 'r', 'p', 'r', 'r', 'p', 'r', 'r', 'l', 'l', 'l', 'l', 'p', 'r', 'r', 'r', 'r', 'p', 'r', 'r', 'p', 'r', 'r', 'l', 'l', 'l', 'l', 'p', 'r', 'r', 'r', 'r', 'p', 'r', 'r', 'p', 'r', 'r', 'l', 'l', 'l', 'l', 'p', 'r', 'r', 'r', 'r', 'p', 'r', 'r', 'p', 'r', 'r', 'c', 'distance', 'metric', 'distance', 'metric', 'distance', 'metric', 'distance', 'metric', 'shadow', 'model', 'shadow', 'model', 'fig', 'impact', 'significance', 'level', 'change', 'value', 'tpr', 'tnr', 'significance', 'level', 'vary', 'compare', 'default', 'caption', 'plot', 'demonstrate', 'offline', 'model', 'type', 'task', 'label', 'display', 'distance', 'metric', 'label', 'show', 'absolute', 'fluctuating', 'value', 'tpr', 'tnr', 'fig', 'impact', 'trajectory', 'size', 'change', 'value', 'tpr', 'tnr', 'trajectory', 'size', 'vary', 'compare', 'entire', 'trajectory', 'caption', 'plot', 'demonstrate', 'offline', 'model', 'type', 'task', 'label', 'display', 'distance', 'metric', 'label', 'show', 'absolute', 'fluctuating', 'value', 'tpr', 'tnr', 'fig', 'robustness', 'action', 'distortion', 'change', 'value', 'tpr', 'tnr', 'suspect', 'model', 'add', 'µ', 'σ', 'output', 'caption', 'plot', 'demonstrate', 'offline', 'model', 'type', 'task', 'label', 'display', 'distance', 'metric', 'label', 'show', 'absolute', 'fluctuating', 'value', 'orlauditor', 'detect', 'use', 'ensemble', 'architecture', 'result', 'crease', 'model', 'performance', 'certain', 'task', 'experiman', 'tal', 'result', 'show', 'column', 'model', 'performance', 'model', 'ensemble', 'table', 'table', 'xvi', 'table', 'xvii', 'table', 'table', 'demonstrate', 'decline', 'performance', 'offline', 'rl', 'model', 'utilize', 'ensemble', 'architecture', 'instance', 'model', 'learn', 'ant', 'dataset', 'mean', 'value', 'cumulative', 'reward', 'decrease', 'significantly', 'furthermore', 'submodel', 'train', 'subset', 'datum', 'fit', 'partial', 'dataset', 'distribution', 'consequently', 'apply', 'model', 'ensemble', 'practical', 'scenario', 'standard', 'deviation', 'model', 'performance', 'large', 'lunar', 'lander', 'v', 'u', 'v', 'u', 'v', 'lunar', 'lander', 'lunar', 'lander', 'iql', 'lunar', 'lander', 'l', 'l', 'l', 'l', 'p', 'r', 'r', 'r', 'r', 'p', 'r', 'r', 'p', 'r', 'r', 'l', 'l', 'l', 'l', 'p', 'r', 'r', 'r', 'r', 'p', 'r', 'r', 'p', 'r', 'r', 'l', 'l', 'l', 'l', 'p', 'r', 'r', 'r', 'r', 'p', 'r', 'r', 'p', 'r', 'r', 'l', 'l', 'l', 'l', 'p', 'r', 'r', 'r', 'r', 'p', 'r', 'r', 'p', 'r', 'r', 'c', 'distance', 'metric', 'distance', 'metric', 'distance', 'metric', 'distance', 'lunar', 'lander', 'lunar', 'lander', 'lunar', 'lander', 'iql', 'lunar', 'lander', 'v', 'l', 'e', 'u', 'v', 'l', 'e', 'u', 'v', 'bipedal', 'bipedal', 'walker', 'bipedal', 'walker', 'iql', 'bipedal', 'walker', 'l', 'l', 'l', 'p', 'r', 'r', 'r', 'r', 'p', 'r', 'r', 'p', 'r', 'r', 'l', 'l', 'l', 'l', 'p', 'r', 'r', 'r', 'r', 'p', 'r', 'r', 'p', 'r', 'r', 'l', 'l', 'l', 'l', 'p', 'r', 'r', 'r', 'r', 'p', 'r', 'r', 'p', 'r', 'r', 'l', 'l', 'l', 'l', 'p', 'r', 'r', 'r', 'r', 'p', 'r', 'r', 'p', 'r', 'r', 'c', 'distance', 'metric', 'distance', 'metric', 'distance', 'metric', 'distance', 'metric', 'full', 'trajectory', 'full', 'trajectory', 'lunar', 'lander', 'lunar', 'lander', 'lunar', 'lander', 'iql', 'lunar', 'lander', 'v', 'l', 'e', 'u', 'v', 'l', 'e', 'u', 'v', 'bipedal', 'walker', 'bipedal', 'walker', 'bipedal', 'walker', 'iql', 'bipedal', 'walker', 'l', 'l', 'l', 'p', 'r', 'r', 'r', 'r', 'p', 'r', 'r', 'p', 'r', 'r', 'l', 'l', 'l', 'l', 'p', 'r', 'r', 'r', 'r', 'p', 'r', 'r', 'p', 'r', 'r', 'l', 'l', 'l', 'l', 'p', 'r', 'r', 'r', 'r', 'p', 'r', 'r', 'p', 'r', 'r', 'l', 'l', 'l', 'l', 'p', 'r', 'r', 'r', 'r', 'p', 'r', 'r', 'p', 'r', 'r', 'c', 'distance', 'metric', 'distance', 'metric', 'distance', 'metric', 'distance', 'metric', 'sigma01', 'table', 'tpr', 'tnr', 'result', 'orlauditor', 'model', 'ensemble', 'k', 'mean', 'standard', 'deviation', 'tpr', 'tnr', 'row', 'represent', 'audit', 'result', 'combination', 'task', 'model', 'distance', 'metric', 'pair', 'tpr', 'tnr', 'derive', 'diagonal', 'nondiagonal', 'value', 'corresponding', 'heatmap', 'figure', 'lunar', 'lander', 'figure', 'bipedal', 'walker', 'figure', 'ant', 'figure', 'half', 'supplementary', 'task', 'name', 'lunar', 'half', 'norm', 'norm', 'cosine', 'distance', 'wasserstein', 'distance', '8900±1676', '9990±044', '9980±060', '8680±2832', 'tpr', 'tnr', '9990±044', '9980±060', 'action', 'distortion', 'suspect', 'model', 'perturb', 'action', 'change', 'original', 'model', 'output', 'conceal', 'training', 'dataset', 'practice', 'action', 'distortion', 'mechanism', 'stealthy', 'detect', 'auditor', 'easily', 'consider', 'drl', 'model', 'usually', 'apply', 'realworld', 'decision', 'make', 'task', 'selfdrive', 'car', 'industry', 'automa', 'natural', 'distortion', 'often', 'model', 'tion', 'gaussian', 'noise', 'example', 'thermal', 'noise', 'cause', 'random', 'motion', 'electron', 'conductor', 'model', 'gaussian', 'noise', 'constant', 'power', 'spec', 'trum', 'addition', 'noise', 'easy', 'manipulate', 'mathematically', 'ease', 'evaluate', 'effect', 'different', 'distortion', 'intensity', 'dimension', 'model', 'action', 'space', 'normalize', '−1', 'utilize', 'gaussian', 'noise', 'µ', 'standard', 'deviation', 'represent', 'level', 'distortion', 'setup', 'figure', 'depict', 'impact', 'action', 'distortion', 'information', 'used', 'offline', 'drl', 'model', 'task', 'show', 'figure', 'title', 'indicate', 'metric', 'yaxis', 'absolute', 'value', 'change', 'supplementary', 'detailed', 'result', 'dataset', 'figure', 'figure', 'lunar', 'lander', 'figure', 'figure', 'bipedal', 'walker', 'figure', 'figure', 'ant', 'observation', 'conclude', 'follow', 'observation', 'base', 'result', 'orlauditor', 'able', 'resist', 'potential', 'action', 'distortion', 'suspect', 'model', 'especially', 'cosine', 'metric', 'figure', 'tpr', 'tnr', 'vary', 'slightly', 'setting', 'weak', 'noise', 'maximum', 'accuracy', 'attenuation', 'cosine', 'distance', 'speculate', 'cosine', 'distance', 'noise', 'suppression', 'ability', 'calculate', 'inner', 'product', 'series', 'cumulative', 'reward', 'also', 'weak', 'noise', 'facilitate', 'dataset', 'auditing', 'move', 'negative', 'sample', 'far', 'away', 'positive', 'set', 'orlauditor', 'single', 'distance', 'metric', 'face', 'limitation', 'heavy', 'distortion', 'tpr', 'orlauditor', 'suffer', 'obvious', 'decline', 'strong', 'noise', 'strong', 'distortion', 'thoroughly', 'change', 'distribution', 'model', 'action', 'cumulative', 'reward', 'suspect', 'model', 'train', 'target', 'dataset', 'different', 'auditor', 'shadow', 'model', 'case', 'auditor', 'identify', 'positive', 'model', 'negative', 'single', 'kind', 'distance', 'metric', 'figure', 'cosine', 'distance', 'good', 'discriminate', 'positive', 'model', 'result', 'diagonal', 'distance', 'proper', 'negative', 'model', 'result', 'nondiagonal', 'thus', 'strong', 'distortion', 'combination', 'multiple', 'distance', 'metric', 'enhance', 'auditing', 'robustness', 'orlauditor', 'addition', 'note', 'model', 'normal', 'behavior', 'also', 'destroy', 'strong', 'distortion', 'example', 'table', 'noise', 'induce', 'model', 'performance', 'iql', 'decrease', 'well', 'model', 'quality', 'pronounced', 'performance', 'drop', 'vii', 'relate', 'work', 'membership', 'dataset', 'inference', 'infer', 'individual', 'data', 'record', 'use', 'train', 'target', 'model', 'shokri', 'propose', 'first', 'practical', 'membership', 'inference', 'strategy', 'train', 'number', 'shadow', 'classifier', 'distinguish', 'target', 'model', 'output', 'member', 'non', 'member', 'training', 'dataset', 'researcher', 'investigate', 'membership', 'inference', 'various', 'system', 'machine', 'unlearn', 'facial', 'recognition', 'system', 'neural', 'architecture', 'search', 'present', 'firstofitskind', 'holistic', 'risk', 'assessment', 'different', 'inference', 'attack', 'machine', 'learning', 'model', 'maini', 'introduce', 'definition', 'dataset', 'inference', 'design', 'first', 'mechanism', 'identify', 'suspect', 'model', 'copy', 'private', 'knowledge', 'dataset', 'compare', 'exist', 'work', 'orlauditor', 'welldesigne', 'solution', 'build', 'offline', 'drl', 'scene', 'overcome', 'several', 'new', 'challenge', 'first', 'orlauditor', 'postevent', 'mechanism', 'directly', 'apply', 'exist', 'opensource', 'dataset', 'second', 'orlauditor', 'use', 'auxiliary', 'dataset', 'knowledge', 'extraction', 'drl', 'drl', 'model', 'learn', 'interaction', 'environment', 'valuable', 'information', 'case', 'eg', 'indoor', 'robot', 'demonstrate', 'knowledge', 'extraction', 'vulnerability', 'drl', 'various', 'setting', 'propose', 'algorithm', 'infer', 'floor', 'plan', 'train', 'grid', 'world', 'navigation', 'model', 'lidar', 'perception', 'exact', 'model', 'propose', 'first', 'method', 'acquire', 'approximation', 'model', 'victim', 'build', 'classifier', 'reveal', 'targeted', 'black', 'box', 'model', 'train', 'base', 'predict', 'action', 'leverage', 'stateoftheart', 'imitation', 'learn', 'technique', 'replicate', 'model', 'identi', 'integrate', 'differential', 'privacy', 'distribute', 'defend', 'extraction', 'local', 'model', 'report', 'noisy', 'gradient', 'design', 'satisfy', 'local', 'differential', 'privacy', 'keep', 'local', 'information', 'exploit', 'adversarial', 'reverse', 'engineering', 'propose', 'novel', 'testing', 'framework', 'deep', 'learn', 'copyright', 'protection', 'adjust', 'detect', 'knowledge', 'extraction', 'drl', 'viii', 'disscusion', 'highlight', 'orlauditor', 'orlauditor', 'first', 'approach', 'conduct', 'trajectorylevel', 'dataset', 'auditing', 'offline', 'drl', 'model', 'conduct', 'comprehensive', 'analysis', 'orlauditor', 'different', 'experimental', 'setting', 'shadow', 'model', 'amount', 'significance', 'level', 'pothesis', 'test', 'trajectory', 'size', 'robustness', 'ensemble', 'architecture', 'action', 'distortion', 'conclude', 'useful', 'observation', 'adopt', 'orlauditor', 'apply', 'orlauditor', 'audit', 'model', 'train', 'opensource', 'dataset', 'deepmind', 'tpr', 'tnr', 'result', 'superior', 'demonstrating', 'orlauditor', 'effective', 'efficient', 'strategy', 'publish', 'dataset', 'limitation', 'future', 'work', 'discuss', 'limitation', 'orlauditor', 'promise', 'direction', 'improvement', 'accuracy', 'orlauditor', 'decrease', 'significance', 'level', 'thus', 'interesting', 'enhance', 'orlauditor', 'satisfy', 'strict', 'auditing', 'demand', 'future', 'auditor', 'base', 'single', 'distance', 'metric', 'suffi', 'ciently', 'robust', 'strong', 'distortion', 'base', 'observation', 'section', 'integrate', 'distance', 'metric', 'audit', 'process', 'far', 'promising', 'direction', 'conclusion', 'work', 'propose', 'novel', 'trajectorylevel', 'dataset', 'auditing', 'method', 'offline', 'drl', 'model', 'rely', 'insight', 'cumulative', 'reward', 'serve', 'dataset', 'intrinsic', 'fingerprint', 'exist', 'model', 'train', 'target', 'dataset', 'true', 'positive', 'rate', 'true', 'negative', 'rate', 'orlauditor', 'exceed', 'offline', 'drl', 'model', 'task', 'combination', 'show', 'orlauditor', 'effective', 'efficient', 'solution', 'protect', 'ip', 'dataset', 'owner', 'multiple', 'experiment', 'study', 'parameter', 'setting', 'number', 'shadow', 'model', 'significance', 'level', 'hypothesis', 'testing', 'trajectory', 'size', 'conclude', 'several', 'important', 'observation', 'adopt', 'orlauditor', 'practice', 'robustness', 'evaluation', 'demonstrate', 'auditor', 'resist', 'defense', 'model', 'ensemble', 'action', 'distortion', 'suspect', 'model', 'integrate', 'multiple', 'distance', 'metric', 'improve', 'robustness', 'auditor', 'action', 'distortion', 'promising', 'direction', 'future', 'work', 'finally', 'utilize', 'opensource', 'dataset', 'deepmind', 'examine', 'practicality', 'orlauditor', 'show', 'orlauditor', 'behave', 'excellently', 'exist', 'publish', 'dataset', 'acknowledgment', 'like', 'thank', 'anonymous', 'reviewer', 'constructive', 'comment', 'also', 'thank', 'share', 'expertise', 'reinforcement', 'learning', 'work', 'partly', 'support', 'national', 'key', 'research', 'develop', 'ment', 'program', 'nsfc', 'grant', 'fundamental', 'research', 'fund', 'central', 'univer', 'sitie', 'partly', 'sponsor', 'project', 'trustworthy', 'federate', 'data', 'analytic', 'zti', 'oo1', 'support', 'center', 'cybersecurity', 'reference', 'agarwal', 'brevdo', 'ghemawat', 'goodfellow', 'harp', 'jozefowicz', 'man´e', 'r', 'moore', 'shlen', 'sutskever', 'tucker', 'vanhoucke', 'vinyal', 'tensorflow', 'largescale', 'learning', 'heterogeneous', 'system', 'https', 'wwwtensorfloworg', 'alhinai', 'introduction', 'biomedical', 'signal', 'processing', 'artificial', 'intelligence', 'biomedical', 'signal', 'processing', 'artificial', 'intelligence', 'healthcare', 'development', 'biomedical', 'engineering', 'bioelec', 'tronic', 'page', '1–28', 'elsevi', 'deep', 'reinforcement', 'learning', 'robotic', 'manipulation', 'state', 'art', 'c', 'leibo', 'teplyashin', 'ward', 'k¨uttler', 'lefrancq', 'green', 'sadik', 'cain', 'bolton', 'h', 'king', 'legg', 'petersen', 'deepmind', 'lab', 'employee', 'departure', 'create', 'gape', 'security', 'hole', 'https', 'holesaysnewdata', 'systematic', 'review', 'model', 'watermarking', 'neural', 'network', 'frontier', 'big', 'datum', 'pettersson', 'schneider', 'schulman', 'song', 'copy', 'right', 'testing', 'framework', 'copyright', 'protection', 'deep', 'learning', 'model', 'ieee', 'p', 'page', 'steal', 'deep', 'reinforcement', 'learning', 'model', 'fun', 'profit', 'conference', 'computer', 'communication', 'security', 'asiaccs', 'page', 'ilahi', 'niyato', 'challenge', 'countermeasure', 'adversarial', 'attack', 'deep', 'reinforcement', 'learning', 'eshete', 'miashield', 'defend', 'membership', 'inference', 'attack', 'preemptive', 'exclusion', 'member', 'privacy', 'enhance', 'technology', 'symposium', 'page', 'r', 'kidambi', 'rajeswaran', 'morel', 'modelbase', 'offline', 'reinforcement', 'learning', 'neurip', 'p', 'well', 'autoencode', 'variational', 'baye', 'backe', 'iclr', 'machine', 'unlearn', 'jeopardize', 'privacy', 'backe', 'graph', 'unlearn', 'backe', 'face', 'auditor', 'datum', 'auditing', 'facial', 'recognition', 'system', 'usenix', 'security', 'l', 'auditor', 'dataset', 'auditing', 'offline', 'deep', 'reinforcement', 'learning', 'network', 'distribute', 'system', 'security', 'symposium', 'internet', 'society', 'l', 'du', 'bai', 'ahead', 'adaptive', 'hierarchical', 'decomposition', 'range', 'query', 'local', 'differential', 'privacy', 'ldptrace', 'locally', 'differentially', 'private', 'trajectory', 'synthesis', 'vldb', 'dziedzic', 'kaleem', 'papernot', 'dataset', 'inference', 'selfsupervise', 'model', 'corr', 'abs220909024', 'r', 'fayjie', 'hossain', 'oualid', 'driverless', 'car', 'autonomous', 'driving', 'use', 'deep', 'reinforcement', 'learning', 'urban', 'environment', 'international', 'conference', 'ubiquitous', 'robot', 'page', 'nachum', 'tucker', 'levine', 'dataset', 'deep', 'datadriven', 'reinforcement', 'learning', 'abs200407219', 'fujimoto', 'ghavamzadeh', 'bench', 'mark', 'batch', 'deep', 'reinforcement', 'learning', 'algorithm', 'fujimoto', 'gu', 'minimalist', 'approach', 'offline', 'rein', 'forcement', 'learning', 'neurip', 'page', '20132–20145', 'fujimoto', 'meger', 'precup', 'offpolicy', 'deep', 'reinforcement', 'learning', 'exploration', 'icml', 'page', 'fujimoto', 'h', 'hoof', 'address', 'function', 'approximation', 'error', 'actorcritic', 'method', 'icml', 'page', 'amin', 'h', 'aboutalebi', 'wong', 'precup', 'learn', 'surprising', 'effectiveness', 'bership', 'inference', 'attack', 'temporally', 'correlate', 'datum', 'deep', 'reinforcement', 'learning', 'main', 'amin', 'precup', 'privattack', 'membership', 'inference', 'attack', 'framework', 'deep', 'reinforcement', 'learn', 'agent', 'e', 'sample', 'criterion', 'test', 'outlying', 'observation', 'annal', 'mathematical', 'statistic', 'page', 'g¨ulc¸ehre', 'z', 'novikov', 'paine', 'g', 'r', 'agarwal', 'merel', 'c', 'paduraru', 'dulacarnold', 'freitas', 'unplug', 'collection', 'benchmark', 'offline', 'reinforcement', 'learning', 'neurip', 'g¨urtler', 'blaes', 'kolev', 'widmai', 'martius', 'benchmarking', 'offline', 'reinforcement', 'learning', 'realrobot', 'hardware', 'iclr', 'collaborative', 'sensing', 'internet', 'thing', 'comprehensive', 'survey', 'ieee', 'communication', 'survey', 'tutorial', 'h', 'back', 'privacy', 'risk', 'cellbase', 'architecture', 'j', 'susmit', 'trojdrl', 'evaluation', 'backdoor', 'attack', 'deep', 'reinforcement', 'learning', 'kostrikov', 'nair', 'levine', 'offline', 'reinforcement', 'learning', 'implicit', 'qlearning', 'iclr', 'lange', 'riedmiller', 'batch', 'reinforcement', 'reinforcement', 'learning', 'volume', 'adaptation', 'learn', 'learning', 'optimization', 'page', 'springer', 'levine', 'tucker', 'offline', 'reinforcement', 'learning', 'tutorial', 'review', 'perspective', 'open', 'problem', 'bai', 'untargete', 'backdoor', 'watermark', 'harmless', 'stealthy', 'dataset', 'copyright', 'protec', 'tion', 'opensource', 'dataset', 'protection', 'backdoor', 'watermarking', 'corr', 'abs201005821', 'gehre', 'khalidov', 'synnaeve', 'stardata', 'starcraft', 'ai', 'research', 'corr', 'salem', 'backe', 'e', 'cristofaro', 'fritz', 'mldoctor', 'holistic', 'risk', 'assessment', 'usenix', 'inference', 'attack', 'machine', 'learning', 'model', 'security', 'b', 'sanchezesguevillas', 'application', 'deep', 'reinforcement', 'learning', 'intrusion', 'detection', 'supervised', 'problem', 'expert', 'system', 'application', 'p', 'maini', 'yaghini', 'papernot', 'dataset', 'inference', 'ownership', 'resolution', 'machine', 'learn', 'iclr', 'mwiti', 'reallife', 'application', 'reinforcement', 'learning', 'https', 'locally', 'private', 'distribute', 'reinforcement', 'learning', 'l', 'paine', 'c', 'paduraru', 'novikov', 'z', 'freitas', 'hyperparameter', 'selection', 'offline', 'reinforcement', 'learning', 'abs200709055', 'song', 'act', 'tell', 'lot', 'privacyleake', 'attack', 'deep', 'reinforcement', 'learning', 'page', 'pattanaik', 'bommannan', 'chowdhary', 'robust', 'deep', 'reinforcement', 'learning', 'adversarial', 'attack', 'page', 'pomerleau', 'autonomous', 'land', 'vehicle', 'neural', 'network', 'neurip', 'page', 'r', 'r', 'm´aximo', 'l', 'colombini', 'survey', 'offline', 'reinforcement', 'learning', 'taxonomy', 'review', 'open', 'problem', 'abs220301387', 'h', 'l', 'security', 'industrial', 'robot', 'vulnerability', 'attack', 'mitigation', 'ieee', 'network', 'r', 'qin', 'gao', 'near', 'realworld', 'benchmark', 'offline', 'reinforcement', 'learning', 'abs210200714', 'raffin', 'hill', 'gleave', 'kanervisto', 'ernestus', 'reliable', 'reinforcement', 'learning', 'implementation', 'journal', 'machine', 'learn', 'research', 'rubner', 'tomasi', 'metric', 'distribution', 'application', 'image', 'database', 'iccv', 'page', 'rupprecht', 'survey', 'deep', 'reinforcement', 'learning', 'markovian', 'cyberphysical', 'system', 'common', 'problem', 'solution', 'neural', 'network', 'seno', 'imai', 'd3rlpy', 'offline', 'deep', 'reinforcement', 'learning', 'library', 'journal', 'machine', 'learn', 'research', 'r', 'shokri', 'song', 'shmatikov', 'membership', 'inference', 'attack', 'machine', 'learning', 'model', 'ieee', 'p', 'page', 'silver', 'lever', 'wierstra', 'riedmiller', 'deterministic', 'policy', 'gradient', 'algorithm', 'icml', 'page', 'stephen', 'statistic', 'goodness', 'fit', 'compar', 'ison', 'journal', 'american', 'statistical', 'association', 'l', 'song', 'p', 'mittal', 'mitigate', 'membership', 'inference', 'attack', 'selfdistillation', 'novel', 'ensemble', 'architecture', 'usenix', 'security', 'page', 'tessian', 'great', 'resignation', 'create', 'security', 'chal', 'lenge', 'https', 'tingmoresecuritychallenge', 'der', 'maaten', 'visualize', 'datum', 'use', 'tsne', 'journal', 'machine', 'learn', 'research', 'h', 'back', 'privtrace', 'differentially', 'private', 'trajectory', 'synthesis', 'adaptive', 'markov', 'model', 'usenix', 'security', 'l', 'song', 'doorl', 'attack', 'competitive', 'reinforcement', 'learn', 'ijcai', 'page', 'jha', 'continuous', 'release', 'datum', 'stream', 'centralized', 'local', 'differential', 'privacy', 'maniatakos', 'stop', 'andgo', 'explore', 'backdoor', 'attack', 'deep', 'reinforcement', 'learning', 'base', 'traffic', 'congestion', 'control', 'system', 'ieee', 'tif', 'l', 'detect', 'plc', 'intrusion', 'use', 'control', 'invariant', 'ieee', 'r', 'rafailov', 'levine', 'combo', 'conservative', 'offline', 'modelbase', 'policy', 'optimization', 'neurip', 'page', 'yuan', 'privgraph', 'differentially', 'private', 'graph', 'datum', 'publication', 'exploit', 'community', 'information', 'usenix', 'security', 'wan', 'r', 'deng', 'constrain', 'vulnerability', 'assessment', 'deep', 'reinforcement', 'learning', 'base', 'scopf', 'ieee', 'tps', 'calm', 'consistent', 'adaptive', 'local', 'marginal', 'marginal', 'release', 'local', 'differential', 'privacy', 'backe', 'differentially', 'private', 'datum', 'synthesis', 'usenix', 'security', 'behavior', 'similarity', 'model', 'figure', 'provide', 'behavior', 'similarity', 'offline', 'rl', 'model', 'train', 'dataset', 'table', 'take', 'bipedal', 'walker', 'task', 'example', 'dataset', 'regard', 'target', 'dataset', 'public', 'dataset', 'observe', 'behavior', 'similarity', 'rl', 'model', 'wave', 'heavily', 'different', 'public', 'training', 'datum', 'auditor', 'adopt', 'dataset', 'public', 'training', 'datum', 'auditor', 'likely', 'misclassifie', 'rl', 'model', 'train', 'public', 'dataset', 'bootleg', 'model', 'addition', 'behavior', 'similarity', 'also', 'affect', 'different', 'offline', 'rl', 'framework', 'iql', 'detailed', 'section', 'detail', 'task', 'lunar', 'lander', 'continuous', 'version', 'lunarlander', 'task', 'smoothly', 'land', 'spaceship', 'flag', 'target', 'pad', 'landing', 'pad', 'always', 'coordinate', 'ship', 'throttle', 'throttle', 'point', 'downward', 'main', 'engine', 'point', 'left', 'right', 'direction', 'left', 'right', 'engine', 'observation', '8dimensional', 'vector', 'coordinate', 'lander', 'x', 'axis', 'yaxis', 'linear', 'velocity', 'yaxis', 'angle', 'angular', 'velocity', 'boolean', 'represent', 'leg', 'contact', 'ground', 'action', 'real', 'value', 'range', '−1', 'first', 'dimension', 'control', 'main', 'engine', 'engine', 'value', '−1', 'increase', 'throttle', 'value', 'rise', 'point', 'control', 'second', 'value', 'spaceship', 'fire', 'left', 'engine', 'value', '−10', 'fire', 'right', 'engine', 'value', 'shut', 'engine', 'value', 'reward', 'move', 'top', 'screen', 'landing', 'pad', 'speed', 'point', 'landing', 'landing', 'pad', 'possible', 'thus', 'player', 'lose', 'terminal', 'reward', 'lander', 'move', 'away', 'landing', 'pad', 'player', 'get', 'additional', 'point', 'leg', 'touch', 'ground', 'fire', 'main', 'engine', 'point', 'frame', 'episode', 'finish', 'lander', 'crash', 'land', 'smoothly', 'receive', 'point', 'bipedal', 'walker', 'bipedal', 'walker', 'task', 'operate', 'walker', 'robot', 'move', 'forward', 'fast', 'possible', 'robot', 'make', 'hull', 'leg', 'leg', 'joint', 'hip', 'knee', 'observation', 'task', 'include', 'continuous', 'physical', 'variable', 'angle', 'speed', 'angular', 'velocity', 'horizontal', 'speed', 'vertical', 'speed', 'position', 'joint', 'joint', 'angular', 'speed', 'leg', 'contact', 'ground', 'lidar', 'rangefinder', 'measurement', 'action', 'motor', 'speed', 'value', 'range', 'joint', 'hip', 'knee', 'walker', 'start', 'stand', 'left', 'end', 'terrain', 'hull', 'horizontal', 'leg', 'position', 'slight', 'knee', 'angle', 'reward', 'give', 'move', 'forward', 'total', 'point', 'far', 'end', 'robot', 'fall', 'get', 'apply', 'motor', 'torque', 'cost', 'small', 'amount', 'point', 'optimal', 'model', 'get', 'well', 'score', 'episode', 'terminate', 'hull', 'get', 'contact', 'ground', 'walker', 'exceed', 'right', 'end', 'terrain', 'length', 'ant', 'task', 'player', 'manipulate', '3d', 'robot', 'ant', 'consist', 'torso', 'free', 'rotational', 'body', 'leg', 'attach', 'leg', 'link', 'move', 'forward', 'right', 'direction', 'observation', 'contain', 'positional', 'value', 'different', 'body', 'part', 'ant', 'follow', 'velocity', 'individual', 'part', 'derivative', 'position', 'order', 'velocity', 'default', 'observation', 'vector', 'shape', 'element', 'correspond', 'follow', 'position', 'angle', 'dim', 'velocity', '14dim', 'information', 'contact', 'force', '84dim', 'player', 'apply', 'torque', 'hinge', 'connect', 'link', 'leg', 'fig', 'model', 'behavior', 'similarity', 'measure', 'ℓ1', 'norm', 'norm', 'cosine', 'distance', 'distance', 'table', 'use', 'first', 'dataset', 'task', 'private', 'training', 'datum', 'remain', 'dataset', 'public', 'training', 'datum', 'plot', 'xaxis', 'display', 'public', 'training', 'datum', 'yaxis', 'show', 'absolute', 'fluctuating', 'value', 'behavior', 'similarity', 'model', 'train', 'private', 'dataset', 'public', 'dataset', 'iql', 'abbreviation', 'different', 'offline', 'rl', 'framework', 'part', 'hinge', 'thus', 'action', 'space', 'continuous', 'vector', 'represent', 'torque', 'apply', 'hinge', 'joint', 'reward', 'ant', 'task', 'consist', 'part', 'healthy', 'reward', 'forward', 'reward', 'control', 'cost', 'contact', 'cost', 'total', 'reward', 'return', 'reward', 'healthy', 'reward', 'forward', 'reward', 'control', 'cost', 'contact', 'cost', 'task', 'end', 'ant', 'state', 'unhealthy', 'episode', 'duration', 'reach', 'timestep', 'c', 'impact', 'shadow', 'model', 'amount', 'investigate', 'relationship', 'number', 'shadow', 'model', 'audit', 'accuracy', 'setup', 'change', 'shadow', 'model', 'amount', 'setting', 'section', 'figure', 'show', 'value', 'change', 'tpr', 'tnr', 'compare', 'shadow', 'model', 'figure', 'title', 'illustrate', 'setting', 'model', 'task', 'indicate', 'metric', 'yaxis', 'absolute', 'value', 'change', 'also', 'provide', 'detailed', 'result', 'table', 'viii', 'shadow', 'model', 'table', 'shadow', 'model', 'observation', 'figure', 'follow', 'obser', 'vation', 'audit', 'accuracy', 'increase', 'large', 'amount', 'shadow', 'model', 'value', 'shadow', 'model', 'multisampling', 'true', 'value', 'q', 'dataset', 'mean', 'standard', 'deviation', 'precise', 'shadow', 'model', 'example', 'orlauditor', 'suffer', 'obvious', 'tpr', 'decline', 'shadow', 'model', 'insufficient', 'knowledge', 'diversity', 'model', 'train', 'target', 'dataset', 'auditor', 'easily', 'misclassifie', 'positive', 'model', 'negative', 'group', 'exist', 'saturation', 'point', 'audit', 'accuracy', 'expansion', 'shadow', 'model', 'shadow', 'model', 'amount', 'rise', 'tpr', 'usually', 'increase', 'auditor', 'observe', 'possible', 'cumulative', 'reward', 'originate', 'model', 'train', 'target', 'dataset', 'note', 'value', 'change', 'slightly', 'plot', 'mean', 'similar', 'cumulative', 'reward', 'appear', 'shadow', 'model', 'set', 'diversity', 'increase', 'significantly', 'compare', 'shadow', 'model', 'therefore', 'excessive', 'shadow', 'model', 'unnecessary', 'auditor', 'need', 'burden', 'training', 'overhead', 'impact', 'significance', 'level', 'significance', 'level', 'represent', 'auditor', 'confidence', 'audit', 'result', 'section', 'adopt', 'significance', 'level', 'meaning', 'auditor', 'confidence', 'judgment', 'generally', 'speak', 'significance', 'level', 'represent', 'maximum', 'audit', 'capacity', 'orlauditor', 'instead', 'hyperparameter', 'setting', 'audit', 'quirement', 'dataset', 'owner', 'setup', 'demand', 'auditor', 'output', 'confident', 'judgment', 'error', 'possibility', 'limit', 'figure', 'show', 'value', 'change', 'tpr', 'tnr', 'compare', 'significance', 'level', 'used', 'offline', 'drl', 'model', 'task', 'show', 'figure', 'title', 'indicate', 'metric', 'yaxis', 'absolute', 'value', 'change', 'l', 'u', 'l', 'e', 'u', 'v', 'l', 'e', 'u', 'v', 'u', 'v', 'lunar', 'lander', 'norm', 'lunar', 'lander', 'norm', 'lunar', 'lander', 'cosine', 'distance', 'bipedal', 'walker', 'norm', 'ant', 'norm', 'half', 'medium', 'random', 'dataset', 'name', 'rluply', 'bipedal', 'walker', 'norm', 'ant', 'norm', 'half', 'norm', 'medium', 'random', 'dataset', 'name', 'bipedal', 'walker', 'cosine', 'distance', 'ant', 'cosine', 'distance', 'half', 'cosine', 'distance', 'medium', 'random', 'dataset', 'name', 'rluply', 'lunar', 'lander', 'distance', 'bipedal', 'wasserstein', 'distance', 'ant', 'distance', 'half', 'distance', 'medium', 'random', 'dataset', 'name', 'rluply', 'notice', 'small', 'trajectory', 'size', 'achieve', 'well', 'result', 'task', 'ant', 'task', 'auditor', 'auditing', 'full', 'length', 'obtain', 'promotion', 'tnr', 'result', 'base', 'analysis', 'front', 'state', 'trajectory', 'able', 'reflect', 'behavioral', 'information', 'model', 'thus', 'case', 'short', 'trajectory', 'truncate', 'rear', 'stateaction', 'pair', 'unimportant', 'even', 'weaken', 'significance', 'hypothesis', 'testing', 'explore', 'effective', 'datum', 'auditing', 'short', 'trajectory', 'size', 'even', 'use', 'first', 'state', 'trajectory', 'interesting', 'future', 'direction', 'additional', 'result', 'supplementary', 'provide', 'additional', 'result', 'orlauditor', 'ease', 'read', 'summarize', 'main', 'figure', 'table', 'table', 'detailed', 'result', 'dataset', 'table', 'table', 'observation', 'figure', 'follow', 'obser', 'vation', 'complicated', 'task', 'recommend', 'auditor', 'select', 'large', 'significance', 'level', 'orlauditor', 'task', 'complexity', 'affect', 'minimum', 'significance', 'level', 'orlauditor', 'example', 'tpr', 'tnp', 'change', 'little', 'lunar', 'lander', 'task', 'significance', 'level', 'reduce', 'highly', 'shrink', 'ant', 'task', 'table', 'ant', 'state', 'action', 'space', 'large', 'lunar', 'lander', 'auditor', 'leverage', 'critic', 'model', 'compress', 'model', 'state', 'action', 'pair', 'scalar', 'deviation', 'recall', 'figure', 'ant', 'task', 'imperceptible', 'j', 'qs', 'suspect', 'model', 'low', 'performance', 'auditor', 'adopt', 'large', 'significance', 'level', 'guarantee', 'audit', 'accuracy', 'instance', 'figure', 'title', 'bipedal', 'walker', 'tnr', 'result', 'distance', 'metric', 'decrease', 'reduce', 'table', 'model', 'performance', 'bipedal', 'walker', 'task', 'around', 'mean', 'td3plusbc', 'model', 'fully', 'master', 'knowledge', 'dataset', 'thus', 'dataset', 'feature', 'reflect', 'behavior', 'ambiguous', 'weaken', 'difference', 'positive', 'negative', 'sample', 'meanwhile', 'confidence', 'interval', 'figure', 'expand', 'low', 'significance', 'level', 'reason', 'tnr', 'result', 'td3plusbc', 'model', 'bipedal', 'walker', 'task', 'drop', 'compare', 'analysis', 'safe', 'bind', 'orlauditor', 'low', 'break', 'capability', 'boundary', 'orlauditor', 'induce', 'auditor', 'misclassify', 'negative', 'model', 'positive', 'set', 'e', 'impact', 'trajectory', 'size', 'investigate', 'relationship', 'trajectory', 'size', 'audit', 'accuracy', 'section', 'adopt', 'fulllength', 'trajectory', 'mean', 'auditor', 'utilize', 'state', 'trajectory', 'query', 'suspect', 'model', 'obtain', 'corre', 'sponde', 'action', 'conduct', 'dataset', 'audit', 'setup', 'change', 'trajectory', 'size', 'full', 'length', 'setting', 'section', 'figure', 'show', 'value', 'change', 'tpr', 'tnr', 'compare', 'fulllength', 'trajectory', 'figure', 'title', 'illus', 'trate', 'setting', 'model', 'task', 'indicate', 'metric', 'yaxis', 'absolute', 'value', 'change', 'also', 'provide', 'detailed', 'result', 'table', 'table', 'xiii', 'observation', 'figure', 'follow', 'observa', 'tion', 'orlauditor', 'tend', 'achieve', 'high', 'accuracy', 'large', 'trajectory', 'size', 'predict', 'cumulative', 'reward', 'stateaction', 'pair', 'critic', 'model', 'audit', 'basis', 'long', 'trajectory', 'collect', 'action', 'suspect', 'model', 'enhance', 'significance', 'hypothesis', 'testing', 'example', 'tnp', 'result', 'decrease', 'orlauditor', 'leverage', 'trajectory', 'table', 'vi', 'roadmap', 'main', 'figure', 'table', 'information', 'overview', 'task', 'online', 'drl', 'model', 'involve', 'content', 'section', 'name', 'table', 'table', 'table', 'model', 'section', 'overall', 'audit', 'performance', 'section', 'vb', 'impact', 'shadow', 'model', 'amount', 'c', 'impact', 'significance', 'level', 'impact', 'trajectory', 'size', 'realworld', 'application', 'section', 'robustness', 'ensemble', 'architecture', 'section', 'robustness', 'perturb', 'model', 'output', 'section', 'table', 'table', 'table', 'table', 'table', 'table', 'figure', 'figure', 'figure', 'figure', 'table', 'viii', 'table', 'figure', 'table', 'table', 'figure', 'table', 'table', 'table', 'table', 'table', 'figure', 'figure', 'figure', 'figure', 'figure', 'figure', 'figure', 'figure', 'figure', 'figure', 'description', 'state', 'shape', 'action', 'shape', 'task', 'performance', 'used', 'online', 'model', 'collect', 'offline', 'dataset', 'name', 'number', 'trajectory', 'length', 'trajectory', 'offline', 'dataset', 'offline', 'model', 'performance', 'defense', 'auditor', 'normal', 'performance', 'defense', 'defend', 'model', 'ensemble', 'defend', 'perturb', 'model', 'output', 'true', 'positive', 'rate', 'tpr', 'true', 'negative', 'rate', 'tnr', 'result', 'base', 'grubb', 'test', 'principle', 'change', 'value', 'tpr', 'tnr', 'number', 'shadow', 'model', 'vary', 'compare', 'default', 'shadow', 'model', 'change', 'value', 'tpr', 'tnr', 'significance', 'level', 'vary', 'compare', 'default', 'change', 'value', 'tpr', 'tnr', 'trajectory', 'size', 'vary', 'compare', 'default', 'full', 'length', 'tpr', 'tnr', 'result', 'half', 'cheetah', 'dataset', 'publish', 'deepmind', 'separately', 'tpr', 'tnr', 'result', 'orlauditor', 'model', 'ensemble', 'k', 'tpr', 'tnr', 'result', 'orlauditor', 'model', 'action', 'distortion', 'table', 'supplementary', 'provide', 'tpr', 'tnr', 'result', 'orlauditor', 'base', 'principle', 'mean', 'standard', 'deviation', 'tpr', 'tnr', 'row', 'represent', 'audit', 'result', 'combination', 'task', 'model', 'distance', 'metric', 'indicate', 'high', 'sum', 'tpr', 'tnr', 'accuracy', 'row', 'task', 'offline', 'model', 'norm', 'norm', 'cosine', 'distance', 'wasserstein', 'distance', 'tpr', '9653±136', '9613±301', '9720±324', '9560±439', '9656±427', '9667±420', '9433±745', '9700±446', '9067±530', '9067±693', '9562±519', 'tnr', 'tpr', '9547±281', '9480±318', '9627±244', '9280±507', '9578±458', '9478±743', '9411±863', '9333±462', '9413±383', '8960±399', '9412±503', 'tpr', '9573±258', '9653±240', '9889±217', '9720±389', '9908±154', 'tpr', '9653±414', '9667±288', '9995±049', 'fig', 'audit', 'accuracy', 'lunar', 'lander', 'dataset', 'caption', 'plot', 'demonstrate', 'offline', 'model', 'type', 'task', 'distance', 'metric', 'label', 'name', 'dataset', 'audit', 'target', 'dataset', 'label', 'name', 'dataset', 'suspect', 'model', 'learn', 'actual', 'dataset', 'thus', 'diagonal', 'value', 'show', 'audit', 'accuracy', 'actual', 'dataset', 'target', 'dataset', 'tpr', 'nondiagonal', 'value', 'tnr', 'result', 'position', 'value', 'accuracy', 'fig', 'audit', 'accuracy', 'bipedal', 'walker', 'dataset', 'caption', 'plot', 'demonstrate', 'offline', 'model', 'type', 'task', 'distance', 'metric', 'label', 'name', 'dataset', 'audit', 'target', 'dataset', 'label', 'name', 'dataset', 'suspect', 'model', 'learn', 'actual', 'dataset', 'thus', 'diagonal', 'value', 'show', 'audit', 'accuracy', 'actual', 'dataset', 'target', 'dataset', 'tpr', 'nondiagonal', 'value', 'tnr', 'result', 'position', 'value', 'accuracy', 'lunar', 'lander', 'norm', 'lunar', 'lander', 'norm', 'lunar', 'lander', 'cosine', 'distance', 'lunar', 'lander', 'norm', 'lunar', 'lander', 'norm', 'lunar', 'lander', 'cosine', 'distance', 'lunar', 'lander', 'distance', 'lunar', 'lander', 'distance', 'lunar', 'lander', 'iql', 'norm', 'lunar', 'lander', 'iql', 'norm', 'lunar', 'lander', 'norm', 'lunar', 'lander', 'norm', 'lunar', 'lander', 'iql', 'cosine', 'distance', 'lunar', 'lander', 'cosine', 'distance', 'lunar', 'lander', 'iql', 'wasserstein', 'distance', 'lunar', 'lander', 'distance', 'bipedal', 'norm', 'norm', 'cosine', 'distance', 'distance', 'bipedal', 'walker', 'norm', 'walker', 'norm', 'walker', 'cosine', 'distance', 'distance', 'iql', 'norm', 'iql', 'norm', 'iql', 'cosine', 'distance', 'iql', 'distance', 'bipedal', 'walker', 'norm', 'bipedal', 'walker', 'norm', 'bipedal', 'walker', 'cosine', 'distance', 'distance', 'fig', 'audit', 'accuracy', 'ant', 'dataset', 'caption', 'plot', 'demonstrate', 'offline', 'model', 'type', 'task', 'distance', 'metric', 'label', 'name', 'dataset', 'audit', 'target', 'dataset', 'label', 'name', 'dataset', 'suspect', 'model', 'learn', 'actual', 'dataset', 'thus', 'diagonal', 'value', 'show', 'audit', 'accuracy', 'actual', 'dataset', 'target', 'dataset', 'tpr', 'nondiagonal', 'value', 'tnr', 'result', 'position', 'value', 'accuracy', 'table', 'viii', 'impact', 'shadow', 'model', 'amount', 'tpr', 'tnr', 'result', 'orlauditor', 'shadow', 'model', 'task', 'lunar', '9265±346', '9705±106', 'norm', 'norm', 'cosine', 'distance', 'wasserstein', 'distance', '9978±050', 'tpr', '9377±364', '9547±337', '9000±547', '10000±000', '9995±017', '9770±359', '9730±495', '9891±099', '9802±101', '9912±043', '9935±075', '8588±2810', 'tpr', '9655±198', '9703±151', '9754±116', '9811±142', '9850±130', '9685±645', '9992±016', '9976±052', '9957±093', '9930±136', 'ant', 'norm', 'norm', 'ant', 'norm', 'ant', 'iql', 'norm', 'ant', 'norm', 'ant', 'norm', 'ant', 'iql', 'norm', 'cosine', 'distance', 'ant', 'cosine', 'distance', 'distance', 'ant', 'distance', 'ant', 'iql', 'cosine', 'distance', 'ant', 'iql', 'distance', 'ant', 'norm', 'ant', 'cosine', 'distance', 'ant', 'distance', 'table', 'impact', 'shadow', 'model', 'amount', 'tpr', 'tnr', 'result', 'orlauditor', 'shadow', 'model', 'task', 'lunar', 'norm', 'norm', 'cosine', 'distance', 'wasserstein', 'distance', '9996±017', 'tpr', '9813±188', '9840±068', '9636±457', '9987±027', '9827±103', '9986±041', '9968±063', 'tpr', '9800±184', '9685±340', '9578±429', '9769±451', '9907±165', '9951±026', '9982±036', '8682±2697', 'tpr', '9800±260', 'table', 'impact', 'significance', 'level', 'tpr', 'tnr', 'result', 'orlauditor', 'task', 'lunar', '9891±168', 'norm', 'norm', 'cosine', 'distance', 'wasserstein', 'distance', '9983±040', 'tpr', '9821±055', '9760±113', '9856±151', '9747±233', '9973±034', '9581±506', '9988±040', '9981±047', '9960±073', 'tpr', '9821±063', '9755±222', '9987±027', 'tpr', '9859±095', '9768±451', '9971±058', 'tnr', '9985±025', '9969±063', '9949±105', '9878±215', 'table', 'impact', 'significance', 'level', 'tpr', 'tnr', 'result', 'orlauditor', 'task', 'lunar', '9933±121', '9973±029', 'norm', 'norm', 'cosine', 'distance', 'wasserstein', 'distance', '9980±041', '9956±076', 'tpr', '9981±026', '9723±555', '9915±067', '9980±048', '9911±207', '9856±149', '9880±240', '9992±011', '9989±021', '9992±011', '10000±000', 'tpr', '9979±022', '9933±054', '9973±022', '9956±079', '9888±154', '9806±318', 'table', 'impact', 'trajectory', 'size', 'tpr', 'tnr', 'result', 'orlauditor', 'trajectory', 'size', 'task', 'lunar', '9800±242', '9680±537', '9811±140', '9845±100', 'norm', 'norm', 'cosine', 'distance', 'wasserstein', 'distance', '9953±130', '9985±034', '9976±043', 'tpr', '9627±200', '9672±257', '9643±313', '9704±124', '9692±160', '9964±109', '9978±046', '9985±040', 'tpr', '9861±271', '9922±118', 'tnr', 'tpr', '9810±092', '9856±110', '9936±090', '9861±245', '9701±545', '9896±081', '9928±107', 'table', 'impact', 'trajectory', 'size', 'tpr', 'tnr', 'result', 'orlauditor', 'trajectory', 'size', 'task', 'lunar', '9776±205', 'norm', 'norm', 'cosine', 'distance', 'wasserstein', 'distance', '9985±036', '9680±284', '9781±283', '9792±281', '9547±534', '9704±133', '9672±150', '9929±106', '9725±072', '9640±084', '9725±218', '9651±298', '9989±010', '9981±031', '9949±083', '9960±060', '9979±030', '9982±025', 'tpr', '9824±179', '9941±086', '9955±072', '8430±808', '9984±036', '9970±066', 'table', 'detail', 'online', 'model', 'generate', 'offline', 'dataset', 'model', 'performance', 'show', 'cumulative', 'reward', 'separate', 'evaluation', 'task', 'name', 'model', 'train', 'step', 'model', 'name', 'model', 'performance', 'lunar', 'lander', 'sac', 'bipedal', 'walker', 'ppo', 'sac', 'fig', 'audit', 'accuracy', 'model', 'ensemble', 'lunar', 'lander', 'caption', 'plot', 'demonstrate', 'offline', 'model', 'type', 'task', 'distance', 'metric', 'hyperparameter', 'model', 'ensemble', 'label', 'name', 'dataset', 'audit', 'target', 'dataset', 'label', 'name', 'dataset', 'suspect', 'model', 'learn', 'actual', 'dataset', 'thus', 'diagonal', 'value', 'show', 'audit', 'accuracy', 'actual', 'dataset', 'target', 'dataset', 'tpr', 'nondiagonal', 'value', 'tnr', 'result', 'position', 'value', 'accuracy', 'table', 'detail', 'offline', 'dataset', 'task', 'name', 'number', 'transition', 'dataset', 'name', 'number', 'trajectory', 'length', 'trajectory', 'lunar', 'lander', 'bipedal', 'walker', 'ant', '22983±8351', '26613±9965', '98103±19079', '97307±11842', '112663±37905', '95546±17772', '90184±23693', 'lunar', 'lander', 'norm', 'lunar', 'lander', 'norm', 'k', 'lunar', 'lander', 'cosine', 'lunar', 'lander', 'k', 'lunar', 'lander', 'norm', 'k', 'lunar', 'lander', 'norm', 'k', 'lunar', 'lander', 'cosine', 'lunar', 'lander', 'iql', 'norm', 'k', 'lunar', 'lander', 'iql', 'norm', 'k', 'lunar', 'lander', 'iql', 'cosine', 'k', 'lunar', 'lander', 'norm', 'k', 'lunar', 'lander', 'norm', 'k', 'lunar', 'lander', 'cosine', 'lunar', 'lander', 'wasserstein', 'k', 'lunar', 'lander', 'wasserstein', 'k', 'lunar', 'lander', 'iql', 'wasserstein', 'k', 'fig', 'audit', 'accuracy', 'model', 'ensemble', 'bipedal', 'walker', 'caption', 'plot', 'demonstrate', 'offline', 'model', 'type', 'task', 'distance', 'metric', 'hyperparameter', 'model', 'ensemble', 'label', 'name', 'dataset', 'audit', 'target', 'dataset', 'label', 'name', 'dataset', 'suspect', 'model', 'learn', 'actual', 'dataset', 'thus', 'diagonal', 'value', 'show', 'audit', 'accuracy', 'actual', 'dataset', 'target', 'dataset', 'tpr', 'nondiagonal', 'value', 'tnr', 'result', 'position', 'value', 'accuracy', 'table', 'xvi', 'supplementary', 'provide', 'detail', 'model', 'model', 'performance', 'show', 'cumulative', 'reward', 'separate', 'evaluation', 'offline', 'model', 'lunar', 'dataset', 'name', 'model', 'performance', 'defense', '28885±1539', 'model', 'performance', 'trajectory', 'splitting', '18334±4700', '152373±47318', 'model', 'performance', 'model', 'ensemble', '30801±087', 'model', 'performance', 'gauss', '27084±638', '28764±1636', '532499±44127', 'model', 'performance', 'gauss', '26883±2555', 'bipedal', 'walker', 'norm', 'bipedal', 'walker', 'norm', 'k', 'bipedal', 'walker', 'cosine', 'bipedal', 'walker', 'walker', 'norm', 'k', 'walker', 'norm', 'k', 'iql', 'norm', 'k', 'iql', 'norm', 'k', 'walker', 'cosine', 'k', 'iql', 'cosine', 'k', 'norm', 'k', 'bipedal', 'walker', 'norm', 'k', 'bipedal', 'cosine', 'bipedal', 'walker', 'wasserstein', 'k', 'bipedal', 'walker', 'iql', 'wasserstein', 'k', 'wasserstein', 'k', 'fig', 'audit', 'accuracy', 'model', 'ensemble', 'ant', 'caption', 'plot', 'demonstrate', 'offline', 'model', 'type', 'task', 'distance', 'metric', 'hyperparameter', 'model', 'ensemble', 'label', 'name', 'dataset', 'audit', 'target', 'dataset', 'label', 'name', 'dataset', 'suspect', 'model', 'learn', 'actual', 'dataset', 'thus', 'diagonal', 'value', 'show', 'audit', 'accuracy', 'actual', 'dataset', 'target', 'dataset', 'tpr', 'nondiagonal', 'value', 'tnr', 'result', 'position', 'value', 'accuracy', 'table', 'xvii', 'supplementary', 'provide', 'detail', 'offline', 'model', 'model', 'performance', 'show', 'cumulative', 'reward', 'separate', 'evaluation', 'offline', 'model', 'lunar', 'dataset', 'name', 'model', 'performance', 'defense', 'model', 'performance', 'trajectory', 'splitting', 'model', 'performance', 'model', 'ensemble', '27843±957', '26416±9768', '30609±403', 'model', 'performance', 'gauss', 'model', 'performance', 'gauss', '341276±80453', 'ant', 'norm', 'k', 'ant', 'norm', 'k', 'ant', 'cosine', 'k', 'ant', 'k', 'ant', 'norm', 'k', 'ant', 'norm', 'k', 'ant', 'iql', 'norm', 'ant', 'iql', 'norm', 'k', 'ant', 'cosine', 'k', 'ant', 'wasserstein', 'k', 'ant', 'iql', 'cosine', 'ant', 'iql', 'wasserstein', 'k', 'ant', 'norm', 'k', 'ant', 'norm', 'k', 'ant', 'cosine', 'ant', 'wasserstein', 'k', 'fig', 'audit', 'accuracy', 'model', 'ensemble', 'half', 'caption', 'plot', 'demonstrate', 'offline', 'model', 'type', 'task', 'distance', 'metric', 'hyperparameter', 'model', 'ensemble', 'label', 'name', 'dataset', 'audit', 'target', 'dataset', 'label', 'name', 'dataset', 'suspect', 'model', 'learn', 'actual', 'dataset', 'thus', 'diagonal', 'value', 'show', 'audit', 'accuracy', 'actual', 'dataset', 'target', 'dataset', 'tpr', 'nondiagonal', 'value', 'tnr', 'result', 'position', 'value', 'accuracy', 'table', 'supplementary', 'provide', 'detail', 'iql', 'offline', 'model', 'model', 'performance', 'show', 'cumulative', 'reward', 'separate', 'evaluation', 'offline', 'model', 'lunar', 'dataset', 'name', 'model', 'performance', 'defense', '5792±3114', '28463±1738', 'model', 'performance', 'trajectory', 'splitting', '27191±510', 'model', 'performance', 'model', 'ensemble', '496865±133785', '156386±122573', 'model', 'performance', 'gauss', '4996±2860', '28720±1929', '287916±26272', 'model', 'performance', 'gauss', '26438±3452', '123903±32898', 'half', 'norm', 'half', 'norm', 'k', 'half', 'cosine', 'half', 'k', 'expert', 'medium', 'random', 'rluply', 'expert', 'medium', 'random', 'rluply', 'expert', 'medium', 'random', 'half', 'norm', 'rluply', 'expert', 'medium', 'random', 'half', 'norm', 'k', 'rluply', 'expert', 'medium', 'random', 'half', 'cosine', 'k', 'rluply', 'expert', 'medium', 'random', 'half', 'cheetah', 'iql', 'norm', 'rluply', 'expert', 'medium', 'random', 'half', 'cheetah', 'iql', 'norm', 'k', 'rluply', 'expert', 'medium', 'random', 'half', 'cheetah', 'iql', 'cosine', 'rluply', 'expert', 'medium', 'random', 'rluply', 'expert', 'medium', 'random', 'half', 'cheetah', 'norm', 'rluply', 'expert', 'medium', 'random', 'half', 'cheetah', 'norm', 'k', 'rluply', 'rluply', 'medium', 'random', 'expert', 'half', 'cosine', 'expert', 'rluply', 'medium', 'random', 'half', 'wasserstein', 'k', 'expert', 'rluply', 'medium', 'random', 'half', 'cheetah', 'iql', 'wasserstein', 'k', 'expert', 'medium', 'random', 'half', 'cheetah', 'wasserstein', 'k', 'rluply', 'expert', 'medium', 'random', 'rluply', 'expert', 'medium', 'random', 'rluply', 'expert', 'medium', 'random', 'rluply', 'expert', 'medium', 'random', 'rluply', 'expert', 'medium', 'random', 'fig', 'audit', 'accuracy', 'suspect', 'model', 'action', 'lunar', 'lander', 'caption', 'plot', 'demonstrate', 'offline', 'model', 'type', 'task', 'distance', 'metric', 'noise', 'strength', 'label', 'name', 'dataset', 'audit', 'target', 'dataset', 'label', 'name', 'dataset', 'suspect', 'model', 'learn', 'actual', 'dataset', 'thus', 'diagonal', 'value', 'show', 'audit', 'accuracy', 'actual', 'dataset', 'target', 'dataset', 'tpr', 'nondiagonal', 'value', 'tnr', 'result', 'position', 'value', 'accuracy', 'table', 'supplementary', 'provide', 'detail', 'model', 'model', 'performance', 'show', 'cumulative', 'reward', 'separate', 'evaluation', 'offline', 'model', 'lunar', 'dataset', 'name', 'model', 'performance', 'defense', 'model', 'performance', 'trajectory', 'splitting', '24178±2168', '21671±11876', 'model', 'performance', 'model', 'ensemble', '7196±11126', '10840±022', '12618±236', 'model', 'performance', 'gauss', '20707±2813', 'model', 'performance', 'gauss', '19469±4259', 'table', 'detail', 'dataset', 'name', 'half', 'cheetah', 'number', 'transition', 'dataset', 'name', 'expert', 'medium', 'random', 'rl', 'unplugged', 'number', 'trajectory', 'length', 'trajectory', 'lunar', 'lander', 'norm', 'lunar', 'lander', 'norm', 'lunar', 'lander', 'cosine', 'lunar', 'lander', 'lunar', 'lander', 'norm', 'lunar', 'lander', 'norm', 'lunar', 'lander', 'cosine', 'lunar', 'lander', 'wasserstein', 'lunar', 'lander', 'iql', 'norm', 'lunar', 'lander', 'iql', 'norm', 'lunar', 'lander', 'iql', 'cosine', 'lunar', 'lander', 'norm', 'lunar', 'lander', 'norm', 'lunar', 'lander', 'cosine', 'lunar', 'lander', 'iql', 'wasserstein', 'lunar', 'lander', 'fig', 'audit', 'accuracy', 'suspect', 'model', 'action', 'lunar', 'lander', 'caption', 'plot', 'demonstrate', 'offline', 'model', 'type', 'task', 'distance', 'metric', 'noise', 'strength', 'label', 'name', 'dataset', 'audit', 'target', 'dataset', 'label', 'name', 'dataset', 'suspect', 'model', 'learn', 'actual', 'dataset', 'thus', 'diagonal', 'value', 'show', 'audit', 'accuracy', 'actual', 'dataset', 'target', 'dataset', 'tpr', 'nondiagonal', 'value', 'tnr', 'result', 'position', 'value', 'accuracy', 'table', 'supplementary', 'provide', 'detail', 'model', 'train', 'dataset', 'model', 'performance', 'show', 'cumulative', 'reward', 'separate', 'evaluation', 'task', 'offline', 'model', 'half', 'expert', 'medium', 'random', 'rl', 'unplugged', 'expert', 'medium', 'random', 'rl', 'unplugged', 'expert', 'medium', 'random', 'rl', 'unplugged', 'expert', 'medium', 'random', 'unplugged', 'model', 'performance', 'defense', '033±024', '1097419±84210', '1271269±38333', 'model', 'performance', 'trajectory', 'splitting', '033±022', '41959±21929', '1275225±27438', 'model', 'performance', 'model', 'ensemble', '1286822±18039', '429335±7567', '1233459±53999', '054±078', '1126802±264057', 'table', 'tpr', 'tnr', 'result', 'half', 'cheetah', 'task', 'mean', 'standard', 'deviation', 'tpr', 'tnr', 'row', 'represent', 'audit', 'result', 'combination', 'task', 'model', 'distance', 'metric', 'indicate', 'high', 'sum', 'tpr', 'tnr', 'accuracy', 'row', 'pair', 'tpr', 'tnr', 'derive', 'diagonal', 'nondiagonal', 'value', 'corresponding', 'heatmap', 'figure', 'task', 'name', 'half', 'model', 'tpr', 'iql', 'norm', 'norm', 'cosine', 'distance', 'wasserstein', 'distance', '9607±234', '9583±120', 'tpr', '9847±113', '9747±135', 'lunar', 'lander', 'norm', 'lunar', 'lander', 'norm', 'lunar', 'lander', 'cosine', 'lunar', 'lander', 'lunar', 'lander', 'norm', 'lunar', 'lander', 'norm', 'lunar', 'lander', 'cosine', 'lunar', 'lander', 'wasserstein', 'lunar', 'lander', 'iql', 'norm', 'lunar', 'lander', 'iql', 'norm', 'lunar', 'lander', 'norm', 'lunar', 'lander', 'norm', 'lunar', 'lander', 'iql', 'cosine', 'lunar', 'lander', 'cosine', 'lunar', 'lander', 'iql', 'wasserstein', 'lunar', 'lander', 'wasserstein', 'fig', 'audit', 'accuracy', 'suspect', 'model', 'action', 'bipedal', 'walker', 'caption', 'plot', 'demonstrate', 'offline', 'model', 'type', 'task', 'distance', 'metric', 'noise', 'strength', 'label', 'name', 'dataset', 'audit', 'target', 'dataset', 'label', 'name', 'dataset', 'suspect', 'model', 'learn', 'actual', 'dataset', 'thus', 'diagonal', 'value', 'show', 'audit', 'accuracy', 'actual', 'dataset', 'target', 'dataset', 'tpr', 'nondiagonal', 'value', 'tnr', 'result', 'position', 'value', 'accuracy', 'table', 'tpr', 'tnr', 'result', 'orlauditor', 'split', 'trajectory', 'short', 'one', 'mean', 'standard', 'deviation', 'tpr', 'tnr', 'row', 'represent', 'audit', 'result', 'combination', 'task', 'model', 'distance', 'metric', 'pair', 'tpr', 'tnr', 'derive', 'diagonal', 'nondiagonal', 'value', 'corresponding', 'heatmap', 'figure', 'lunar', 'lander', 'figure', 'bipedal', 'walker', 'figure', 'ant', 'figure', 'half', 'cheetah', 'task', 'offline', 'model', 'half', 'norm', 'norm', 'cosine', 'distance', 'wasserstein', 'distance', '9955±071', '9871±163', '9850±150', '9683±154', '9700±200', '9753±130', 'tnr', '9993±012', '9845±271', '9819±284', '9589±232', '9677±250', '9720±179', '9687±214', '9627±136', '9625±113', '9653±120', '9990±036', '9984±041', '9587±106', '9749±256', '9627±316', '9864±266', '9968±045', '9981±031', '9965±063', '9963±062', '9981±031', '9974±027', '9993±012', '9956±020', '9956±061', '7263±4022', 'tpr', '9843±073', '9760±114', '9832±179', '9853±125', '9883±155', '9837±109', 'tnr', '9979±047', 'norm', 'norm', 'cosine', 'walker', 'norm', 'walker', 'norm', 'walker', 'cosine', 'bipedal', 'walker', 'wasserstein', 'iql', 'norm', 'iql', 'norm', 'iql', 'cosine', 'iql', 'bipedal', 'walker', 'norm', 'bipedal', 'walker', 'norm', 'bipedal', 'walker', 'cosine', 'fig', 'audit', 'accuracy', 'suspect', 'model', 'action', 'bipedal', 'walker', 'caption', 'plot', 'demonstrate', 'offline', 'model', 'type', 'task', 'distance', 'metric', 'noise', 'strength', 'label', 'name', 'dataset', 'audit', 'target', 'dataset', 'label', 'name', 'dataset', 'suspect', 'model', 'learn', 'actual', 'dataset', 'thus', 'diagonal', 'value', 'show', 'audit', 'accuracy', 'actual', 'dataset', 'target', 'dataset', 'tpr', 'nondiagonal', 'value', 'tnr', 'result', 'position', 'value', 'accuracy', 'norm', 'bipedal', 'walker', 'norm', 'bipedal', 'walker', 'cosine', 'bipedal', 'walker', 'norm', 'walker', 'norm', 'walker', 'cosine', 'iql', 'norm', 'iql', 'norm', 'iql', 'cosine', 'bipedal', 'walker', 'norm', 'bipedal', 'walker', 'norm', 'bipedal', 'walker', 'cosine', 'walker', 'wasserstein', 'iql', 'wasserstein', 'wasserstein', 'fig', 'audit', 'accuracy', 'suspect', 'model', 'action', 'ant', 'caption', 'plot', 'demonstrate', 'offline', 'model', 'type', 'task', 'distance', 'metric', 'noise', 'strength', 'label', 'name', 'dataset', 'audit', 'target', 'dataset', 'label', 'name', 'dataset', 'suspect', 'model', 'learn', 'actual', 'dataset', 'thus', 'diagonal', 'value', 'show', 'audit', 'accuracy', 'actual', 'dataset', 'target', 'dataset', 'tpr', 'nondiagonal', 'value', 'tnr', 'result', 'position', 'value', 'accuracy', 'ant', 'norm', 'ant', 'norm', 'ant', 'iql', 'norm', 'ant', 'norm', 'ant', 'norm', 'ant', 'iql', 'norm', 'ant', 'cosine', 'ant', 'cosine', 'ant', 'ant', 'wasserstein', 'ant', 'iql', 'cosine', 'ant', 'iql', 'wasserstein', 'ant', 'norm', 'ant', 'norm', 'ant', 'cosine', 'ant', 'fig', 'audit', 'accuracy', 'suspect', 'model', 'action', 'ant', 'caption', 'plot', 'demonstrate', 'offline', 'model', 'type', 'task', 'distance', 'metric', 'noise', 'strength', 'label', 'name', 'dataset', 'audit', 'target', 'dataset', 'label', 'name', 'dataset', 'suspect', 'model', 'learn', 'actual', 'dataset', 'thus', 'diagonal', 'value', 'show', 'audit', 'accuracy', 'actual', 'dataset', 'target', 'dataset', 'tpr', 'nondiagonal', 'value', 'tnr', 'result', 'position', 'value', 'accuracy', 'norm', 'ant', 'norm', 'ant', 'iql', 'norm', 'ant', 'norm', 'ant', 'cosine', 'ant', 'ant', 'norm', 'ant', 'norm', 'ant', 'iql', 'norm', 'ant', 'norm', 'ant', 'cosine', 'ant', 'iql', 'cosine', 'ant', 'cosine', 'ant', 'wasserstein', 'ant', 'iql', 'wasserstein', 'ant', 'wasserstein', 'fig', 'audit', 'accuracy', 'half', 'cheetah', 'dataset', 'caption', 'plot', 'demonstrate', 'offline', 'model', 'type', 'task', 'distance', 'metric', 'label', 'name', 'dataset', 'audit', 'target', 'dataset', 'label', 'name', 'dataset', 'suspect', 'model', 'learn', 'actual', 'dataset', 'thus', 'diagonal', 'value', 'show', 'audit', 'accuracy', 'actual', 'dataset', 'target', 'dataset', 'tpr', 'nondiagonal', 'value', 'tnr', 'result', 'position', 'value', 'accuracy', 'half', 'norm', 'half', 'norm', 'half', 'cosine', 'distance', 'half', 'wasserstein', 'distance', 'expert', 'medium', 'random', 'rluply', 'expert', 'medium', 'random', 'half', 'norm', 'rluply', 'expert', 'medium', 'random', 'half', 'norm', 'rluply', 'medium', 'random', 'rluply', 'expert', 'medium', 'random', 'half', 'cheetah', 'iql', 'norm', 'rluply', 'expert', 'medium', 'random', 'half', 'cheetah', 'iql', 'norm', 'rluply', 'expert', 'expert', 'medium', 'random', 'half', 'cosine', 'distance', 'rluply', 'expert', 'medium', 'random', 'half', 'cheetah', 'iql', 'cosine', 'distance', 'rluply', 'medium', 'random', 'expert', 'half', 'distance', 'expert', 'rluply', 'medium', 'random', 'half', 'cheetah', 'iql', 'distance', 'expert', 'medium', 'random', 'half', 'cheetah', 'norm', 'rluply', 'expert', 'medium', 'random', 'half', 'cheetah', 'norm', 'rluply', 'expert', 'medium', 'random', 'half', 'cheetah', 'cosine', 'distance', 'rluply', 'expert', 'medium', 'random', 'half', 'cheetah', 'distance', 'expert', 'expert', 'medium', 'random', 'rluply', 'expert', 'medium', 'random', 'rluply', 'expert', 'medium', 'random', 'rluply', 'expert', 'medium', 'random', 'rluply', 'medium', 'random', 'rluply', 'medium', 'random', 'fig', 'audit', 'accuracy', 'orlauditor', 'lunar', 'lander', 'split', 'trajectory', 'short', 'one', 'caption', 'plot', 'demonstrate', 'offline', 'model', 'type', 'task', 'distance', 'metric', 'hyperparameter', 'trajectory', 'splitting', 'label', 'name', 'dataset', 'audit', 'target', 'dataset', 'label', 'name', 'dataset', 'suspect', 'model', 'learn', 'actual', 'dataset', 'thus', 'diagonal', 'value', 'show', 'audit', 'accuracy', 'actual', 'dataset', 'target', 'dataset', 'tpr', 'nondiagonal', 'value', 'tnr', 'result', 'position', 'value', 'accuracy', 'lunar', 'lander', 'norm', 'split', 'lunar', 'lander', 'norm', 'split', 'lunar', 'lander', 'cosine', 'split', 'lunar', 'lander', 'wasserstein', 'split', 'lunar', 'lander', 'norm', 'split', 'lunar', 'lander', 'norm', 'split', 'lunar', 'lander', 'cosine', 'split', 'lunar', 'lander', 'iql', 'norm', 'split', 'lunar', 'lander', 'iql', 'norm', 'split', 'lunar', 'lander', 'iql', 'cosine', 'split', 'lunar', 'lander', 'norm', 'split', 'lunar', 'lander', 'norm', 'split', 'lunar', 'lander', 'cosine', 'split', 'lunar', 'lander', 'wasserstein', 'split', 'lunar', 'lander', 'iql', 'wasserstein', 'split', 'lunar', 'lander', 'split', 'fig', 'audit', 'accuracy', 'orlauditor', 'bipedal', 'walker', 'split', 'trajectory', 'short', 'one', 'caption', 'plot', 'demonstrate', 'offline', 'model', 'type', 'task', 'distance', 'metric', 'hyperparameter', 'trajectory', 'splitting', 'label', 'name', 'dataset', 'audit', 'target', 'dataset', 'label', 'name', 'dataset', 'suspect', 'model', 'learn', 'actual', 'dataset', 'thus', 'diagonal', 'value', 'show', 'audit', 'accuracy', 'actual', 'dataset', 'target', 'dataset', 'tpr', 'nondiagonal', 'value', 'tnr', 'result', 'position', 'value', 'accuracy', 'norm', 'split', 'bipedal', 'walker', 'norm', 'split', 'bipedal', 'walker', 'cosine', 'split', 'bipedal', 'walker', 'wasserstein', 'split', 'walker', 'norm', 'split', 'walker', 'norm', 'split', 'walker', 'cosine', 'split', 'wasserstein', 'split', 'norm', 'split', 'iql', 'norm', 'split', 'iql', 'cosine', 'split', 'bipedal', 'walker', 'iql', 'wasserstein', 'split', 'norm', 'split', 'norm', 'split', 'bipedal', 'walker', 'cosine', 'split', 'split', 'fig', 'audit', 'accuracy', 'orlauditor', 'ant', 'split', 'trajectory', 'short', 'one', 'caption', 'plot', 'demonstrate', 'offline', 'model', 'type', 'task', 'distance', 'metric', 'hyperparameter', 'trajectory', 'splitting', 'label', 'name', 'dataset', 'audit', 'target', 'dataset', 'label', 'name', 'dataset', 'suspect', 'model', 'learn', 'actual', 'dataset', 'thus', 'diagonal', 'value', 'show', 'audit', 'accuracy', 'actual', 'dataset', 'target', 'dataset', 'tpr', 'nondiagonal', 'value', 'tnr', 'result', 'position', 'value', 'accuracy', 'ant', 'cosine', 'split', 'ant', 'wasserstein', 'split', 'ant', 'norm', 'split', 'ant', 'norm', 'split', 'ant', 'norm', 'split', 'ant', 'norm', 'split', 'ant', 'iql', 'norm', 'split', 'ant', 'iql', 'norm', 'split', 'ant', 'norm', 'split', 'ant', 'norm', 'split', 'ant', 'cosine', 'split', 'ant', 'iql', 'cosine', 'split', 'ant', 'cosine', 'split', 'ant', 'wasserstein', 'split', 'ant', 'iql', 'wasserstein', 'split', 'ant', 'split', 'fig', 'audit', 'accuracy', 'orlauditor', 'half', 'cheetah', 'split', 'trajectory', 'short', 'one', 'caption', 'plot', 'demonstrate', 'offline', 'model', 'type', 'task', 'distance', 'metric', 'hyperparameter', 'trajectory', 'splitting', 'label', 'name', 'dataset', 'audit', 'target', 'dataset', 'label', 'name', 'dataset', 'suspect', 'model', 'learn', 'actual', 'dataset', 'thus', 'diagonal', 'value', 'show', 'audit', 'accuracy', 'actual', 'dataset', 'target', 'dataset', 'tpr', 'nondiagonal', 'value', 'tnr', 'result', 'position', 'value', 'accuracy', 'half', 'norm', 'split', 'half', 'norm', 'split', 'half', 'cosine', 'split', 'half', 'wasserstein', 'split', 'expert', 'medium', 'random', 'rluply', 'expert', 'medium', 'random', 'half', 'norm', 'split', 'rluply', 'expert', 'medium', 'random', 'half', 'norm', 'split', 'rluply', 'expert', 'medium', 'random', 'half', 'cosine', 'split', 'rluply', 'expert', 'medium', 'random', 'rluply', 'expert', 'medium', 'random', 'half', 'cheetah', 'iql', 'norm', 'split', 'rluply', 'expert', 'medium', 'random', 'half', 'cheetah', 'iql', 'norm', 'split', 'rluply', 'expert', 'medium', 'random', 'half', 'cheetah', 'iql', 'cosine', 'split', 'rluply', 'expert', 'medium', 'random', 'rluply', 'rluply', 'medium', 'random', 'expert', 'half', 'wasserstein', 'split', 'expert', 'rluply', 'medium', 'random', 'half', 'cheetah', 'iql', 'wasserstein', 'split', 'expert', 'medium', 'random', 'half', 'cheetah', 'norm', 'split', 'rluply', 'expert', 'medium', 'random', 'half', 'cheetah', 'norm', 'split', 'rluply', 'expert', 'rluply', 'medium', 'random', 'half', 'cheetah', 'cosine', 'split', 'expert', 'medium', 'random', 'half', 'cheetah', 'split', 'rluply', 'expert', 'medium', 'random', 'rluply', 'expert', 'medium', 'random', 'rluply', 'expert', 'medium', 'random', 'rluply', 'expert', 'medium', 'random', 'rluply', 'expert', 'medium', 'random', 'rluply']",
Why We Don't Have AGI Yet,"[{'href': 'http://arxiv.org/abs/2308.03598v4', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2308.03598v4', 'rel': 'related', 'type': 'application/pdf'}]",2023-08-07 13:59:31,"ElasticNotebook: Enabling Live Migration for
Computational Notebooks (Technical Report)

Zhaoheng Li∗, Pranav Gor∗, Rahul Prabhu∗, Hui Yu∗, Yuzhou Mao+, Yongjoo Park∗
University of Illinois at Urbana-Champaign∗ University of Michigan+
{zl20,gor2,rprabhu5,huiy3,yongjoo}@illinois.edu,yuzhom@umich.edu

3
2
0
2

p
e
S
0
2

]

B
D
.
s
c
[

1
v
3
8
0
1
1
.
9
0
3
2
:
v
i
X
r
a

ABSTRACT
Computational notebooks (e.g., Jupyter, Google Colab) are widely
used for interactive data science and machine learning. In those
frameworks, users can start a session, then execute cells (i.e., a set of
statements) to create variables, train models, visualize results, etc.
Unfortunately, existing notebook systems do not offer live migra-
tion: when a notebook launches on a new machine, it loses its state,
preventing users from continuing their tasks from where they had
left off. This is because, unlike DBMS, the sessions directly rely on
underlying kernels (e.g., Python/R interpreters) without an addi-
tional data management layer. Existing techniques for preserving
states, such as copying all variables or OS-level checkpointing, are
unreliable (often fail), inefficient, and platform-dependent. Also,
re-running code from scratch can be highly time-consuming.

In this paper, we introduce a new notebook system, Elastic-
Notebook, that offers live migration via checkpointing/restoration
using a novel mechanism that is reliable, efficient, and platform-
independent. Specifically, by observing all cell executions via trans-
parent, lightweight monitoring, ElasticNotebook can find a reliable
and efficient way (i.e., replication plan) for reconstructing the origi-
nal session state, considering variable-cell dependencies, observed
runtime, variable sizes, etc. To this end, our new graph-based opti-
mization problem finds how to reconstruct all variables (efficiently)
from a subset of variables that can be transferred across machines.
We show that ElasticNotebook reduces end-to-end migration and
restoration times by 85%-98% and 94%-99%, respectively, on a vari-
ety (i.e., Kaggle, JWST, and Tutorial) of notebooks with negligible
runtime and memory overheads of <2.5% and <10%.

PVLDB Reference Format:
Zhaoheng Li∗, Pranav Gor∗, Rahul Prabhu∗, Hui Yu∗, Yuzhou Mao+,
Yongjoo Park∗. ElasticNotebook: Enabling Live Migration for
Computational Notebooks. PVLDB, 14(1): XXX-XXX, 2020.
doi:XX.XX/XXX.XX

1 INTRODUCTION
Computational notebooks1 (e.g., Jupyter [63, 101], Rstudio [86]) are
widely used in data science and machine learning for interactive tu-
torials [62], data exploration [19, 26, 121], visualization [28], model

This work is licensed under the Creative Commons BY-NC-ND 4.0 International
License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of
this license. For any use beyond those covered by this license, obtain permission by
emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights
licensed to the VLDB Endowment.
Proceedings of the VLDB Endowment, Vol. 14, No. 1 ISSN 2150-8097.
doi:XX.XX/XXX.XX

1In this work, we use the term a “notebook” to mean either a system serving the
notebook or the contents of the notebook, depending on the context.

User Interface

Our Data Layer

Kernel

cell 1
...code...

cell 2
...code...

Technique 1: dynamic exe-
cution history in graph (§4)

Technique 2: optimization
for fast migration (§5)

Python
R
LLVM

Figure 1: Our transparent data layer (in the middle) enables
robust, efficient, and platform-independent live migration.

tuning and selection [9, 114], etc. Cloud providers offer Software-
as-a-Services (e.g., AWS hub [91], Azure ML studio [4], Google
Colab [47], IBM Watson studio [56]) with commonly used libraries
(e.g., Pandas, PyTorch). A notebook workflow begins with a user
starting a computing session. Then, the user can execute a cell (i.e.,
a set of statements), one by one, to load datasets, create variables,
train models, visualize results, etc. The session can be terminated
manually or automatically to save resources and costs.

Limitation: No Live Replication. Unfortunately, existing note-
books do not offer transparent infrastructure scaling (independent
of applications), which are becoming increasingly popular in the
cloud for instant scalability and cost reduction (e.g., auto-scaling
DBMS [85, 112], micro-service orchestration [25, 68]). That is, if
we copy a notebook file to a new VM (e.g., for larger memory) or
suspend a session to save costs, the resumed notebook loses its state
(i.e., a set of variables), having only code and outputs. In other words,
the user cannot resume their task from where they had previously
left off. This is because the notebooks directly rely on underlying
kernels (e.g., Python/R interpreters, C++ REPL) without an addi-
tional data management layer. Accordingly, the variables residing
in processes are erased as they terminate with sessions. To address
this, we can potentially save those variables and restore them on a
new environment. However, existing techniques such as serializing
all variables [37–39] and checkpointing OS processes [3, 18, 43, 61]
may fail, are inefficient, and platform-dependent (discussed shortly).
Finally, re-running code from scratch can be time-consuming.

Our Goal. We propose ElasticNotebook, a notebook system that
offers live state migration via checkpointing/restoration using a
reliable, efficient, and platform-independent state replication mech-
anism. Reliability: It enables correct/successful replication for
(almost) all notebooks. Efficiency: It is significantly more efficient
than others. Platform-independence: It does not rely on platform-
/architecture-specific features. That is, ElasticNotebook enables
live notebook replication for potentially all notebook workloads by
introducing a novel data management layer. For example, if a user
specifies a new machine to run a currently active notebook, the
system transparently replicates the notebook, including all of its
variables, as if the notebook has been running on the new machine.

 
 
 
 
 
 
Zhaoheng Li, Pranav Gor, Rahul Prabhu, Hui Yu, Yuzhou Mao, Yongjoo Park

Table 1: Comparison between our ElasticNotebook and other possible approaches to saving/restoring session states

Approach

Mechanism

Serialization-based tools [35, 38–40, 104]
System-level checkpointing [3, 18, 43, 61, 64]
Notebook Versioning and Replay [11, 76, 98]
Execution environment migration [1, 115]
Ours (ElasticNotebook)

Serializes and stores variables during computing session (fails with unserializable variables)
Saves memory dump of computing session (high network cost and low portability)
Enable re-execution of versioned notebook snapshots for result verification
Migrates installed modules; useful in conjunction with (but orthogonal to) session state replication
Optimally combines copy/recompute for reliability, efficiency, and platform independence

If we can provide this capability with little to no modifications
to existing systems (e.g., Jupyter), we can offer benefits to a large
number of data scientists and educators who use notebooks. To
achieve this, we must overcome the following technical challenges.

Challenge. Creating a reliable, efficient, and platform-independent
replication mechanism is challenging. First, the mechanism must
offer high coverage. That is, for almost all notebooks people create,
we should be able to successfully replicate them across machines.
Second, the mechanism should be significantly faster than straight-
forward approaches—rerunning all the cells exactly as they were
run in the past, or copying, if possible, all the variables with serial-
ization/deserialization. Third, the mechanism should integrate with
existing notebook systems with clean separation for sustainable
development and easier adoption.

Our Approach. Our core idea is that by observing the evolution
of session states via lightweight monitoring, we can address the
three important challenges—reliability, efficiency, and platform-
independence—by combining program language techniques (i.e., on-
the-fly code analyses) and novel algorithmic solutions (i.e., graph-
based mathematical optimization). Specifically, to represent session
state changes, we introduce the application history, a special form of
bipartite graph expressing the dependencies among variables and
cell executions. Using this graph, we take the following approach.
First, we achieve reliability and platform independence by choos-
ing a computational plan (or replication plan) that can safely re-
construct platform-dependent variables (e.g., Python generators,
incompletely defined custom classes) based on the other platform-
independent variables. That is, in the presence of variables that
cannot be serialized for platform-independent replication, Elas-
ticNotebook uses the application history to recompute them dy-
namically on a target machine. In this process, ElasticNotebook
optimizes for the collective cost of recomputing all such variables
while still maintaining their correctness (§4).

Second, for efficiency, ElasticNotebook optimizes its replication
plan to determine (1) the variables that will be copied, and (2) the
variables that will be recomputed based on the copied variables,
to minimize the end-to-end migration (or restoration) time in con-
sideration of serialization costs, recomputation costs, data transfer
costs, etc. For example, even if a variable can be reliably transferred
across machines, the variable may still be dynamically constructed
if doing so results in a lower total cost. To make this decision in a
principled way, we devise a new graph-based optimization problem,
which reduces to a well-established min-cut problem (§5).

Implementation: While our contributions can apply to many dy-
namically analyzable languages (e.g., Python/R, LLVM-based ones),
we implement our prototype (in C and Python) for the Python user
interface, which is widely used for data science, machine learning,

statistical analysis, etc. Specifically, ElasticNotebook provides a
data management layer to Jupyter as a hidden cell magic [103] to
transparently monitor cell executions and offer efficient replication.
Difference from Existing Work. Compared to existing work, we
pursue a significantly different direction. For example, there are
tools that make data serialization more convenient [40, 104]; how-
ever, they fail if a session contains non-serializable variables, and
are inefficient because they do not consider opportunities for dy-
namic recomputation. Alternatively, system-level checkpointing [3,
18, 43, 61] is platform-dependent, limited to checkpointing memory
(e.g., not GPU), less efficient than ours since dynamic recompu-
tation is impossible. Building on top of result reuse [42, 116] and
lineage tracing [54, 83, 93], we introduce deeper (reference-aware)
analyses (§4.2) and novel optimization techniques to incorporate
unique constraints such as inter-variable dependencies (§5) and
also empirically confirm their effectiveness (§7.2). Completely or-
thogonal work includes library migration [1, 115] and scalable data
science [79, 81, 117]. Table 1 summarizes differences.
Contributions. Our contributions are as follows:
• Motivation. We discuss alternative approaches and explain

the advantage of our approach. (§2)

• Architecture. We describe our system architecture for achiev-

ing efficient and robust session replication. (§3)

• Data Model. We introduce a novel data model (Application
History Graph) for expression session history, which enables
efficient and accurate state replication. (§4)

• Optimization Problem and Solution. We formally define
the optimization problem of minimizing state replication cost
through balancing variable copying and recomputation. We
propose an efficient and effective solution. (§5)

• Evaluation. We show ElasticNotebook reduces upscaling, down-
scaling, and restore times by 85%-98%, 84%-99%, and 94%-99%,
respectively. Overheads are negligible (<2.5% runtime). (§7)

2 MOTIVATION
This section describes use cases (§2.1) and requirements (§2.2) for
session replication, and our intuition for higher efficiency (§2.3).

2.1 Why is Live Migration Useful?
A seamless state replication for computational notebooks can al-
low easier infrastructure scaling and frequent session suspension,
without interrupting user workflow, as described below.
Fast Replication for Elastic Computing. The ability to move
a state across machines is useful for scaling resources [21, 64],
allowing us to migrate a live session to the machines with the right
equipment/resources (e.g., GPU [22], specific architectures [119]).
For interruption-free scaling, we can copy data D from a source

ElasticNotebook: Enabling Live Migration for Computational Notebooks

User Interface
...
%%intercept
code
...

def intercept(code):
preprocess(code)
# regular kernel execution
out = execute(code)
postprocess(out, code)

Application History

df_train

Cell 1
3mins

df

Cell 2
1min

Cell 3
20mins

model

Cell 4
10mins

plot

df_test

Figure 2: For every cell run, we can inject custom pre-/post-
processing logic. “%%intercept” is hidden to users.

Variable
Store cost (mins)
Reload cost (mins)
Total cost (mins)

df
8
2
10

df_train df_test
6.4
1.6
8

1.6
0.4
2

model
0.2
0.2
0.4

plot
0.1
0.1
0.2

machine to a target machine in a way that the original session state
can be restored from D. In this process, we want to minimize the
end-to-end time for creating D, transferring D to a target machine,
reconstructing the state from D on the target machine. This is the
first use case we empirically study (§7.3).

Fast Restart for On-demand Computing. Leveraging pay-as-
you-go pricing model offered by many cloud vendors [5, 48], sus-
pending sessions (and VMs) when not in use is an effective way
for reducing charges (e.g., up to 6× [115]). With the ability to cre-
ate data D sufficient for reconstructing the current session state,
we can persist D prior to either manual or automated suspen-
sion [20, 47, 59], to quickly resume, when needed, the session in
the same state. This achieves on-demand, granular computing with
fast session restart times without impacting user experience due to
frequent session suspensions [58, 97]. In this process, we want to
restore the session as quickly as possible by minimizing the time it
takes for downloading D and reconstructing a state from it. This is
the second use case we empirically study (§7.4).

2.2 How to Enable Data Management Layer?
We discuss the pros and cons of several different approaches to
enabling a data management layer.

OS-level Checkpointing. To save the current session state, we can
checkpoint the entire memory space associated with the underlying
Python/R kernels. To make the process more efficient, existing tools
like CRIU patch the Linux kernel to trace dirty pages. However,
as described in §1, this approach is platform-independent, incurs
higher space cost, and is limited to storing the state of primary
memory (not GPU or other devices). We empirically compare our
approach to CRIU to understand reliability and efficiency (§7).

Object wrappers. Watchpoint object wrappers [41, 44] are com-
monly used for debugging purposes [83] and program slicing [54,
93]: they maintain deep copies for objects in the session state, which
are compared to check for changes after each frame execution; how-
ever, they are unsuitable for use during data science workflows due
to the unacceptable ~20× runtime overhead in our preliminary tests.

Monitoring Cell Executions (Ours). In order to trace cell exe-
cutions and their effects on variables, we can add a lightweight
wrapper (i.e., our data management layer) that functions before
and after each cell execution to monitor the cell code, runtime,
and variable changes. This idea is depicted conceptually in Fig 2.
Specifically, our implementation uses cell magics, a Jupyter-native
mechanism that allows arbitrary modification to cell statements
when the cell is executed. With this, we add pre-/post-processing
steps to capture cell code and resulting session state modifications.

Store Vars
N/A
All

Method
Rerun all
Store all
Fast-migrate model,plot
Fast-restore df,model,plot 2

Rerun cells Migration Cost Restore Cost
All
3+1+20+10=33
N/A
1, 2

3+1+20+10=33
10+8+2+.4+.2=20.6 2+1.6+.4+.2+.1=4.3
3+1+.4+.2=4.6
10+1+.4+.2=11.6

3+1+.2+.1=4.3
2+1+.2+.1=3.3

Figure 3: Example app history (top) and different replica-
tion plan costs (bottom). Combining recompute/copy allows
faster migration (Fast-migrate). Alternatively, the optimal
plan changes if the restoration is prioritized (Fast-restore).

2.3 Fast Replication with Application History
This section describes our core idea for devising an efficient repli-
cation strategy by leveraging the ability to monitor cell executions.

Application History. An application history graph (AHG) is a bipar-
tite graph for expressing session states changes with respect to cell
runs. There are two types of nodes: variables and transformations.
A transformation node connects input variables to output variables
(see an example in Fig 3). AHG aims to achieve two properties:
• Completeness: No false negatives. All input/output variable

for each transformation must be captured.

• Minimal: Minimal false positives. The number of variables that
are incorrectly identified as accessed/modified, while variables
are not actually accessed/modified, must be minimized.

These properties are required for correct state reconstruction (§4).

Core Optimization Idea. AHG allows for efficient state replica-
tion with a combination of (1) recompute and (2) copy. Motivating
Example. Suppose a data analyst fitting a regression model (Fig 3).
The notebook contains 4 cell runs: data load (Cell 1), train-test split
(Cell 2), fitting (Cell 3), and evaluation (Cell 4). After fitting, the ana-
lyst decides to move the session to a new machine for GPU. Simply
rerunning the entire notebook incurs 33 minutes. Alternatively,
serializing/copying variables takes 20.6 minutes.

However, there is a more efficient approach. By copying only
model and plot and recomputing others on a new machine (Fast-
migrate), we can complete end-to-end migration in 4.6 minutes.
Or, if we prioritize restoration time (to reduce user-perceived restart
time for on-demand computing), our optimized plan (Fast-restore)
takes 3.3 minutes. This example illustrates significant optimization
opportunities in session replication. Our goal is to have the ability
to find the best replication plan for arbitrarily complex AHGs.

3 SYSTEM OVERVIEW
This section presents ElasticNotebook at a high level by describing
its components (§3.1) and operations (§3.2).

Data Layer (core part of ElasticNotebook)

Table 2: Notations and their meaning

Zhaoheng Li, Pranav Gor, Rahul Prabhu, Hui Yu, Yuzhou Mao, Yongjoo Park

Notebook
User
Interface

x=""hello""

y=""world""

.
.
.

Cell Execution
Interceptor (§4.2)

Optimizer (§5)

ID Graphs &
object hashes

Optimization
algorithm

Application
History
Graph (§4.1)

Cost Model
(§5.2)

Session
Replicator (§4.3)

Writer

Notebook
Replayer

Jupyter
Kernel
Namespace
(user_ns)

KEY VAL
x
y

hello
world
...

Figure 4: ElasticNotebook architecture. Its data layer acts as
a gateway between the user interface and the kernel: cell
executions are intercepted to observe session state changes.

3.1 ElasticNotebook Components
ElasticNotebook introduces a unique data layer that acts as a gate-
way between the user and the kernel (See Fig 4): it monitors every
cell execution, observing code and resulting session state changes.

Cell Execution Interceptor. The Cell Execution Interceptor inter-
cepts cell execution requests and adds pre-/post-processing scripts
before rerouting it into the underlying kernel for regular execu-
tion. The added scripts perform (1) cell code analyses and the AHG
updates, and (2) cell runtime recordings.

Application History Graph (AHG). The AHG is incrementally
built by the Cell Execution Interceptor to record how variables have
been accessed/modified by each cell execution (§4). The AHG is
used by the Optimizer to compute replication plans (§5).

Cost Model. The cost model stores profiled metrics (i.e., cell run-
times, variable sizes, network bandwidth), serving as the hyperpa-
rameters for the Optimizer (§5.2).

Optimizer. The Optimizer uses the AHG and the Cost Model to
determine the most efficient replication plan consisting of (1) vari-
ables to store and (2) cells to re-run. We discuss ElasticNotebook’s
cost model and optimization in detail in §5.

Session Replicator. The Session Replicator replicates a notebook
session according to the Optimizer’s plan. Specifically, the Writer
creates and writes a checkpoint file to storage (e.g., SSD, cloud
storage), while the Notebook Replayer reads the file and restores
the session, both following the replication plan. We discuss Elastic-
Notebook’s session replication in detail in §3.2.

3.2 ElasticNotebook Workflow
This section describes ElasticNotebook’s operations. ElasticNote-
book monitors every cell execution during a session lifecycle, then
performs on-request replication of the session in two steps: check-
pointing (writing to the checkpoint file) and restoration.
Monitoring Cell Executions. Upon each cell execution by the user,
ElasticNotebook performs the following steps:
1. Accessed variables of the cell execution are identified via AST

analysis (described in §4.2).

2. The cell code is executed by the Jupyter kernel.

Definition
Set of Variables
Set of Variable Snapshots (VSs)
Set of Active Variable Snapshots
Set of Cell Executions (CEs)
Set of write dependencies
Set of read dependencies

Symbols
X
V
V𝑎
C (= 𝑐𝑡1, 𝑐𝑡2, . . .)
E𝑤
E𝑟
G := { V ∪ C, E𝑤 ∪ E𝑟 } Application History Graph (AHG)
𝑟𝑒𝑞 : X → 2C
Reconstruction mapping function
𝑤𝑠𝑡𝑜𝑟𝑒 : X → R+
Variable storage cost
𝑤𝑟𝑒𝑟𝑢𝑛 : C → R+
Cell Rerun cost
𝑤M : 2X → R+
Migration cost function
𝑤R : 2X → R+
Recomputation cost function
Pairs of linked variables
L ⊆ X × X
Flow graph
H := { V𝐻 , E𝐻 }
𝑐 : E𝐻 → R+
Flow graph edge capacity function

3. Variable changes (i.e., creation/deletion/modification) are iden-

tified within the global namespace (§4.2).

4. The AHG is updated using (1) the cell code and (2) modified

variables by the cell execution.

5. The Cost Model is updated to record cell runtime.
Initiating Replication. When replication is requested, Elastic-
Notebook creates and writes a checkpoint file to storage, which can
be restored later to exactly and efficiently reconstruct the current
session. ElasticNotebook first completes the Cost Model by pro-
filing variable sizes and network bandwidth to storage; then, the
Optimizer utilizes the AHG and Cost model to compute a replica-
tion plan, according to which the Writer creates the checkpoint file:
it consists of (1) a subset of stored variables from the session state,
(2) cells to rerun, (3) the AHG, and (4) the Cost Model.
Restoring a Session. When requested, ElasticNotebook restores
the notebook session from the checkpoint file according to the
replication plan. The Notebook Replayer reconstructs variables
in the order they appeared in the original session by combining
(1) cell reruns and (2) data deserialization followed by variable re-
declaration (into the kernel). Finally, ElasticNotebook loads the
AHG and Cost Model for future replications.

Accuracy Guarantee: ElasticNotebook’s state reconstructing is
effectively the same as re-running all the cells from scratch exactly
in the order they were run in the past. That is, ElasticNotebook
shortens the end-to-end reconstruction time by loading saved vari-
ables (into the kernel namespace) if doing so achieves time savings.
§4.3 presents formal correctness analysis. §6.1 discusses how we
address external resources, side effects, and deserialization failures.

4 APPLICATION HISTORY GRAPH
This section formally defines the Application History Graph (§4.1),
and describes how we achieve exact state replication (§4.3).

4.1 AHG Formal Definition
The AHG is a directed acyclic graph expressing how a session state
has changed with respect to cell executions. Fig 5 is an example.

Definition 1. A variable is a named entity (e.g., df) referencing
an object (which can be uniquely identified by its object ID).

ElasticNotebook: Enabling Live Migration for Computational Notebooks

A variable can be primitive (e.g., int, string) or complex (e.g., list,
dataframe). Multiple variables may point to the same object. The
set of all variables (i.e., X) defined in the global namespace forms a
session state. Cell executions may modify the values of variables
(or referenced objects) without changes to their names, which we
recognize in AHG using variable snapshot, as follows.

Definition 2. A variable snapshot (VS) is a name-timestamp
pair, (𝑥, 𝑡), representing the variable 𝑥 created/modified at 𝑡. We
denote the set of VSes as V.

Definition 3. A cell execution (CE) 𝑐𝑡 represents a cell execution
that finishes at timestamp 𝑡.

All cell executions are linear; that is, for each session, there is at most
one cell running at a time, and their executions are totally ordered.
We denote the list of CEs by C. Each CE also stores executed cell
code, which can be used for re-runs (§3.2).

Definition 4. A write dependency (𝑐𝑡 → (x, 𝑡)) indicates CE 𝑐𝑡
may have modified/created at time 𝑡 the object(s) reachable from
the variable 𝑥. We denote the set of write dependencies as E𝑤.

In Fig 5, 𝑐𝑡3 modifies x with “x += 1”; hence, (𝑐𝑡3 → (x, 𝑐𝑡3 )).
Definition 5. A read dependency ((x, 𝑠) → 𝑐𝑡 ) indicates CE 𝑐𝑡
may have accessed object(s) reachable from x last created/modified
at time 𝑠. We denote the set of read dependencies by E𝑟 .

In Fig 5, “gen=(i for i in l1)” in 𝐶𝑡4 accesses elements in the
list l1 after its creation in 𝑐𝑡3 ; hence there is ((x → 𝑐𝑡3 ), 𝑐𝑡4 ). Note
that write/read dependencies are allowed to contain false positives;
nevertheless, our replication ensures correctness (§4.3).

Definition 6. The AHG 𝐺 := {V∪C, E𝑤 ∪E𝑟 } is a bipartite graph,
where V is VSes, C is CEs; E𝑤 and E𝑟 are write/read dependencies,
respectively. It models the lineage of the notebook session.

In sum, AHG formalizes variable accesses/modifications with re-
spect to cell executions. at the variable level (not object level), theo-
retically bounding the size of AHG to scale linearly with the number
of defined variables, not the number of underlying objects (which
can be very large for lists, dataframes, and so on). We empirically
verify AHG’s low memory overhead in §7.5.

4.2 Dynamic AHG Construction
We describe how ElasticNotebook constructs the AHG accurately.
Constructing the AHG. The AHG is incrementally built with
accessed/created/modified variables by each cell execution:
• A new CE 𝑐𝑡 is created; 𝑡 is an execution completion time.
• Read dependencies are created from VSes (𝑥1, 𝑡𝑥1 ), ..., (𝑥𝑘 , 𝑡𝑥𝑘 )
to 𝑐𝑡 , where 𝑥1, ..., 𝑥𝑘 are variables possibly accessed by 𝑐𝑡 .
• VSes (𝑦1, 𝑡), ..., (𝑦𝑘 , 𝑡) are created, where 𝑦1, ..., 𝑦𝑘 are variables
possibly modified and created by 𝑐𝑡 . Write dependencies are
added from 𝑐𝑡 to each of the newly created VSes.

Fig 5 (right) shows an example AHG. Identifying access/modified
variables is crucial for its construction, which we describe below.

ID Graph. The ID Graph aims to to detect changes at the reference
level (in addition to values). For instance, conventional equality
checks (e.g., based on serialization) will return True for “[a] ==

Notebook
Cell 1 (𝑐𝑡1 )
x, y = 1
Cell 2 (𝑐𝑡2 )
z = y
if False:
print(x)
Cell 3 (𝑐𝑡3 )
x += 1
l1 = [z, 2, 3]
Cell 4 (𝑐𝑡4 )
gen=(i for i in l1)
2dlist = [l1]
Cell 5 (𝑐𝑡5 )
print(gen)

(x, 𝒕1)

(y, 𝑡1)

𝒄𝒕1

𝑐𝑡2

(z, 𝑡2)

𝒄𝒕3

(x, 𝒕3)

(l1,𝑡3)

𝑐𝑡4

(gen, 𝑡4)

(2dlist, 𝑡4)

𝑐𝑡5

(gen, 𝑡5)

(x, 𝑡1)

(Overwritten/deleted)
Variable Snapshot

𝑐𝑡1

Cell
Execution

(x, 𝑡1)

Active
Variable Snapshot

Figure 5: An example notebook and its corresponding Appli-
cation History Graph. The AHG tells ElasticNotebook how
to recompute variables; for example, rerunning 𝑐𝑡1 and 𝑐𝑡3 is
necessary for recomputing x (red).

[b]” if a and b have the same value (e.g., a = [1] and b = [1]),
whereas we ensure it returns True only if a and b refer to the
same object, i.e., id(a)==id(b), where id is the object’s unique ID.
This is because for correct state replication, shared references (e.g.
aliases) and inter-variable relationships must be captured precisely.

Identifying Accessed Variables. ElasticNotebook identifies both
directly accessed variables (via AST [31] parsing) and indirectly
accessed variables (with ID Graphs), as follows.

Direct Accesses: Cell code is analyzed with AST, stepping also into
user-defined functions (potentially nested) to check for accesses to
variables not explicitly passed in as parameters (e.g., global x).

Indirect Accesses: The object(s) reachable from a variable X may
be accessed indirectly via another variable Y if X and Y reference
common object(s) (e.g., when aliases exist, Fig 6a), which cannot
be identified via parsing only. To recognize indirect accesses, we
check the existence of overlaps between the ID Graphs of X and Y.
Our approach is conservative; that is, it may over-identify vari-
ables by including, for example, ones reachable from control flow
branches that were not taken during cell executions. However, these
false positives do not affect accuracy of state replication (§4.3).

Identifying Modified Variables. Variable modifications are iden-
tified using a combination of (1) object hashes and (2) ID Graphs.
Value Changes: ElasticNotebook identifies value modifications
by comparing hashes (by xxHash [118]) before and after each cell
execution while using deep copy as a fallback. If the deep copy fails
(e.g., unserializable or uncomparable variables), we consider them
to be modified-on-access using results from AST and ID Graph
(§6.1). This may result in false positives; however, as previously
mentioned, these false positives do not affect the accuracy.

Structural Changes: The ID Graph enables detecting structural
changes (Fig 6b). After each cell execution, the current variables’ ID
Graphs are compared to the ones created before to identify reference
swaps. In Fig 6b, while the value of 2dlist1 remains unchanged

Cell 1
func = lambda x:...
obj1.foo = func
obj2.foo = func
Cell 2
obj2.foo(""str"")

&obj1

&obj2

ID Graph

&func

(a) Detecting indirect variable accesses from aliases

Cell 1
list1 = [1, 2, 3]
2dlist1 = [list1]
2dlist2 = [list1]
Cell 2
list2 = [1, 2, 3]
2dlist1[0] = list2

Before Cell 2

After Cell 2

ID Graph

&2dlist1

&2dlist1

≠

&list1

&list2

Value

[[1,2,3]]

=

[[1,2,3]]

(b) Detecting structural variable modifications

Figure 6: Two uses of the ID Graph during AHG construction.

after execution after executing Cell 2, the memory address of its
nested list has been changed, no longer referencing list1.

4.3 State Reconstruction with AHG
This section describes how we reconstruct variable(s). We focus
on reconstructing the latest version of each variable, as defined in
active variable snapshot (VS) in an AHG.

Definition 7. VS (𝑥, 𝑡𝑖 ) is active if 𝑥 is in the system (i.e., not
deleted), and there is no VS (𝑥, 𝑡 𝑗 ) such that 𝑡𝑖 < 𝑡 𝑗 .

An active VS, (𝑥, 𝑡𝑖 ), represents the current version of 𝑥. For example,
even if we checkpoint after 𝑐𝑡5 (in Fig 5), “(x, 𝑡3)” is active since x
was last modified by 𝑐𝑡3 . We denote the set of active VSes as V𝑎.
Reconstruction Algorithm. Our goal is to identify the most effi-
cient computation strategy for reconstructing one or more active
variables. Note that we do not reconstruct non-active variables
since they are not part of the current session state. In achieving
this goal, the AHG allows us to avoid unnecessary cell executions
(e.g., because their outcomes have been overwritten) and to learn
proper execution orders. Moreover, this process can be extended
to reconstruct a set of variables more efficiently than computing
them one by one. while still ensuring correctness.

Specifically, to recompute VS (𝑥, 𝑡), we traverse back to its an-
cestors in the AHG (e.g., using the breadth-first search), collecting
all CEs into a list 𝑟𝑒𝑞(𝑥, 𝑡), until we find a ground variable for every
path, where the ground variable is a variable whose value is avail-
able in the system, i.e., either another active VS or copied variable.
By rerunning all the CEs in 𝑟𝑒𝑞(𝑥, 𝑡) in the order of their completion
times, we can obtain the target VS (𝑥, 𝑡). To extend this algorithm
to multiple VSes, say (𝑥1, 𝑡𝑥1), (𝑥2, 𝑡𝑥2), and (𝑥3, 𝑡𝑥3), we obtain 𝑟𝑒𝑞
for each VS and union them into a merged set (that is, identical CEs
collapse into one). By rerunning all the CEs in the merged set, we
obtain all target VSes. Fig 5 shows an example. To recompute (𝑥, 𝑡3),
we rerun 𝑐𝑡3 which requires the previous version (x, 𝑡1) as input,
which in turn requires 𝑐𝑡1 to be rerun. Notably, it is not necessary
to rerun 𝑐𝑡2 as its output z is available in the namespace. Finally,
§6.1 discusses how this approach can recover even if some ground
variables are unexpectedly unobtainable.

Zhaoheng Li, Pranav Gor, Rahul Prabhu, Hui Yu, Yuzhou Mao, Yongjoo Park

Notebook
Cell 3 (𝑐𝑡3 )
l1 = [z, 2, 3]
Cell 4 (𝑐𝑡4 )
2dlist = [l1]

Replication Plan

l1
Migrate

2dlist
Migrate

Recompute Recompute
Recompute

Migrate

&l1==&2dlist1[0]

True
True
False

Figure 7: Two variables sharing references (in Fig 5). They
must be migrated/recomputed together for the correct repli-
cation, serving as constraints to our opt problem (see §5.3).

Why Only Use Active VSes? Theoretically, it is possible to use
non-active variables as ground variables. That is, by preserving
deleted/overwritten variables (e.g., in a cache), we may be able to
speed up the recomputation of active variables [42, 116]. However,
we don’t consider this approach as many data science workloads
are memory-hungry with large training data and model sizes. Still,
there might be cases where we can speed up recomputation by
storing small overwritten variables, which we leave as future work.

Correctness of Reconstruction. As stated in §2.3, the AHG is
allowed to have false positives, meaning it may indicate a cell ac-
cessed/modified variables that were not actually accessed/modified.
While the false positives have a performance impact, they do not
affect the correctness of identification.

Theorem 4.1. Given the approximate AHG G of ElasticNotebook
with false positives, and the true AHG G∗, there is 𝑟𝑒𝑞∗ (𝑥, 𝑡 ∗) ⊆
𝑟𝑒𝑞(𝑥, 𝑡) for any variable 𝑥 ∈ X, where (𝑥, 𝑡) and (𝑥, 𝑡 ∗), 𝑟𝑒𝑞 and
𝑟𝑒𝑞∗ are the active VSs of 𝑥 and reconstruction mapping functions
defined on G and G∗ respectively.

That is, for any arbitrary variable 𝑥, while 𝑟𝑒𝑞(𝑥, 𝑡) may contain
cell executions unnecessary for recomputing 𝑥, it will never miss
any necessary cell executions (i.e., those in 𝑟𝑒𝑞(𝑥, 𝑡 ∗)). The proof is
presented in Appendix A.2.

5 CORRECT & EFFICIENT REPLICATION
This section covers how ElasticNotebook computes an efficient and
correct plan for state replication with the AHG and profiled metrics.
We describe correctness requirements in §5.1, the cost model in
§5.2, the optimization problem in §5.3, and our solution in §5.4.

5.1 Correctness Requirements
ElasticNotebook aims to correctly replicate session states. which
we define the notion of in this section:

Definition 8. A replication of state X is value-equivalent if
∀𝑥 ∈ X,𝑥 =𝑛𝑒𝑤 (𝑥), where 𝑛𝑒𝑤 (𝑥) is the value of 𝑥 post-replication.

A value-equivalent replication preserves the value of each indi-
vidual variable and is guaranteed by the correct identification of
𝑟𝑒𝑞(𝑥, 𝑡) for each variable 𝑥 (§4.3). However, it is additionally im-
portant that shared references are preserved, as defined below.

Definition 9. A value-equivalent replication of a session state X
is additionally isomorphic if ∀𝑎, 𝑏, 𝑖𝑑 (𝑎) = 𝑖𝑑 (𝑏) → 𝑖𝑑_𝑛𝑒𝑤 (𝑎) =
𝑖𝑑_𝑛𝑒𝑤 (𝑏), where 𝑎, 𝑏 are arbitrary references (e.g., x[0][1], y.foo),
and 𝑖𝑑 (𝑎), 𝑖𝑑_𝑛𝑒𝑤 (𝑎) are the unique IDs (i.e., memory addresses)
of the objects pointed to by 𝑎 before and after replication.

ElasticNotebook: Enabling Live Migration for Computational Notebooks

migration cost

=

capacity

Source

y

z

x

l1

Active VSes

Cell Executions

capacity=∞ ca

p

acity

=

reru

n

cost

Sink

𝑐𝑡1

𝑐𝑡2

𝑐𝑡3

𝑐𝑡4

𝑐𝑡5

capacity=∞

2dlist1

gen

Figure 8: Running min-cut on the flow graph constructed
from the AHG in Fig 5. The partition (red) defined by the
minimum cut (dashed edges) determines the replication plan.

ElasticNotebook defines replication as ’correct’ only if it is isomor-
phic, requiring all shared references to be preserved: two references
pointing to the same object pre-replication will still do so post-
replication. That is, inter-object relations are identical (analogous
to graph isomorphism). We describe how ElasticNotebook ensures
isomorphic replication via its linked variable constraint in §5.3.

5.2 Cost Model
Our model captures the costs associated with (1) serializing vari-
ables, (2) writing byte data into storage (e.g., local SSD, cloud stor-
age) and (3) rerunning cell executions. These costs are computed
using the AHG and profiled system metrics.

Variable Migration Cost. Migrating a variable (from one session
to another) includes serializing it to the checkpoint file, then loading
it into a new session. Given a subset of variables to migrate S ⊆ X,
the migration cost 𝑤𝑀 can be expressed as follows:
∑︁
𝛼 × 𝑤𝑠𝑡𝑜𝑟𝑒 (𝑥) + 𝑤𝑙𝑜𝑎𝑑 (𝑥)

𝑤𝑀 (S) =

(1)

𝑥 ∈ S

Where 𝑤𝑠𝑡𝑜𝑟𝑒 (𝑥) and 𝑤𝑙𝑜𝑎𝑑 (𝑥) are the time costs for serializing
the value of 𝑥 at checkpointing time into a file and unpacking into
the new session, respectively. These times are estimated using the
size of 𝑥 and storage latency/bandwidth from ElasticNotebook’s
Profiler (§3.1). The time costs for unserializable variables are set to
infinity. 𝛼 is a coefficient for adjusting the time cost of storage; for
example, if ElasticNotebook is to be invoked upon auto-suspension,
𝛼 can be set to a low value to discount the user-perceived time of
storing variables prior to completely suspending a session (as the
user is likely away).

Variable Recomputation Cost. The Interceptor records cell run-
times during a session lifecycle (§3.1). Combined with the recon-
struction mapping 𝑟𝑒𝑞() for the AHG (§4.3), the cost 𝑤𝑅 for recom-
puting a subset of variables S ⊆ X can be defined as follows:
𝑟𝑒𝑞(𝑥, 𝑡)

𝑤𝑟𝑒𝑟𝑢𝑛 (𝑐), where 𝑟𝑒𝑞(S) =

𝑤𝑅 (S) =

∑︁

(cid:216)

(2)

𝑐 ∈𝑟𝑒𝑞 ( S)

𝑥 ∈ S

where (𝑥, 𝑡) is the active VS of 𝑥 and 𝑤𝑟𝑒𝑟𝑢𝑛 (𝑐) : C → R+ is the
estimated time to rerun the CE 𝑐 in the new session.

Replication Plan Cost. Using migration and recomputation costs
(i.e., Eqs. (1) and (2)), the total cost 𝑤—with variables to migrate S

and variables to recompute X − S—is expressed as:

𝑤 (S) = 𝑤𝑀 (S) + 𝑤𝑅 (X − S)

(3)

5.3 Optimization Problem for State Replication
The goal is to find the variables to migrate S ⊆ X that minimizes
the cost Eq. (3). To ensure isomoprhic replication in consideration
of variable inter-dependencies, additional constraints are added.

Constraint for Linked Variables. Two variables containing refer-
ences to the same object (which we refer to as linked variables, e.g.,
l1 and 2dlist1 in Fig 7) must be either both migrated or recom-
puted, as migrating one and recomputing the other may result in
their contained shared reference/alias being broken, as illustrated
in Fig 7. Let the set of linked variable pairs be denoted as L, then
the constraint can be formally expressed as follows:

(𝑥1 ∈ S ∧ 𝑥2 ∈ S) ∨ (𝑥1 ∉ S ∧ 𝑥2 ∉ S) ∀(𝑥1, 𝑥2) ∈ L
Problem definition. Using the cost model in Eq. (3) and the con-
straint in Eq. (4), we formally define the state replication problem:

(4)

Problem 1. Optimal State Replication
Input:

1. AHG G = {V ∪ C, E}
2. Migration cost function 𝑤𝑀 : 2X → R+
3. Recompute cost function 𝑤𝑅 : 2X → R+
4. Linked variables L ⊆ X × X
A replication plan of subset of variables S ⊆ X for
which we migrate (and another subset X − S which
we recompute)

Output:

Objective: Minimize replication cost 𝑤𝑀 (S) + 𝑤𝑅 (X − S)
Constraint: Linked variables are either both migrated or recom-

puted: (𝑥1, 𝑥2 ∈ S) ∨ (𝑥1, 𝑥2 ∉ S) ∀(𝑥1, 𝑥2) ∈ L

The next section (§5.4) presents our solution to Prob 1.

5.4 Solving State Replication Opt. Problem
We solve Prob 1 by reducing it to a min-cut problem, with a 𝑠𝑟𝑐-𝑠𝑖𝑛𝑘
flow graph constructed from the AHG such that each 𝑠𝑟𝑐-𝑠𝑖𝑛𝑘 cut
(a subset of edges, which, when removed from the flow graph,
disconnects source 𝑠 and sink 𝑡) corresponds to a replication plan
S, while the cost of the cut is equal to the replication cost 𝑤𝑀 (S) +
𝑤𝑅 (X − S). Therefore, finding the minimum cost 𝑠𝑟𝑐-𝑠𝑖𝑛𝑘 cut is
equivalent to finding the optimal replication plan.

Flow Graph Construction. A flow graph 𝐻 := {V𝐻 , E𝐻 } and its
edge capacity 𝜙 : E𝐻 → R+ are defined as follows:
• V𝐻 = V𝑎 ∪ C ∪ {𝑠𝑟𝑐, 𝑠𝑖𝑛𝑘 }: V𝑎 is active VSes, C is cell execu-
tions, and 𝑠𝑟𝑐 and 𝑠𝑖𝑛𝑘 are dummy source and sink nodes.
• ∀𝑥 ∈ V𝑎, (𝑠𝑟𝑐, (𝑥, 𝑡)) ∈ E𝐻 and 𝜙 (𝑠𝑟𝑐, (𝑥, 𝑡)) = 𝑤𝑀 (𝑥): We add
an edge from the source to each active VS with a capacity equal
to the migration cost of the variable.

• ∀𝑐 ∈ C, (𝑐, 𝑠𝑖𝑛𝑘) ∈ E𝐻 and 𝜙 (𝑐, 𝑠𝑖𝑛𝑘) = 𝑤𝑟𝑒𝑟𝑢𝑛 (𝑐): We add an
edge with capacity from each CE to the sink with a capacity
equal to the rerun cost of the CE.

• ∀𝑐 ∈ C, 𝑐 ∈ 𝑟𝑒𝑞(𝑥, 𝑡) → ((𝑥, 𝑡), 𝑐) ∈ E𝐻 and 𝜙 ((𝑥, 𝑡), 𝑐) = ∞
and (𝑥, 𝑡) ∈ V𝑎: We add an edge with infinite capacity from an
active VS (𝑥, 𝑡) to a CE 𝑐 if (𝑥, 𝑡) must be recomputed.

• ∀(𝑥1, 𝑥2) ∈ L,

((𝑥1, 𝑡1) ↔ (𝑥2, 𝑡2)) ∈ E𝐻 and 𝜙 ((𝑥1, 𝑡1) ↔
(𝑥2, 𝑡2)) = ∞: We add a bi-directional edge with an infinite

capacity between each pair of active VSes corresponding to
linked variables 𝑥1 and 𝑥2, e.g., l1 and 2dlist1.

The flow graph H for the AHG in Fig 5 is depicted in Fig 8.
Solution. We can now solve Prob 1 by running a 𝑠𝑟𝑐—𝑠𝑖𝑛𝑘 min-cut
solving algorithm (i.e., Ford-Fulkerson [30]) on 𝐻 . The set of edges
that form the 𝑠𝑟𝑐—𝑠𝑖𝑛𝑘 min-cut (dashed edges), when removed,
disconnects 𝑠𝑟𝑐 from 𝑠𝑖𝑛𝑘; therefore, it defines a partition (in red)
of the nodes into nodes reachable from 𝑠𝑟𝑐, V𝐻𝑠𝑟𝑐
and nodes un-
. The replication plan can be obtained
reachable from 𝑠𝑟𝑐, V𝐻𝑠𝑖𝑛𝑘
from the partition:
• S = {𝑥 | (𝑥, 𝑡) ∈ V𝐻𝑠𝑖𝑛𝑘 ∩ V𝑎 } are the active variable snapshots
(and thus variables) that we want to migrate; in the example,
these variables are l1, 2dlist1, and gen.

• V𝐻𝑠𝑟𝑐 ∩ C are the CEs which we will rerun post-migration to
recompute X − S. In the example, these CEs are 𝑡1, 𝑡2, and 𝑡3;
when rerun, they recompute y, z, and x.2

By construction of H , the sum of migration and recomputation
costs of this configuration 𝑤𝑀 ({𝑥 | (𝑥, 𝑡) ∈ V𝐻𝑠𝑖𝑛𝑘 ) + 𝑤𝑅 (C𝑎 −
(V𝐻𝑠𝑟𝑐 ∩ C)) is precisely the cost of the found 𝑠𝑟𝑐—𝑠𝑖𝑛𝑘 min-cut.

6 IMPLEMENTATION AND DISCUSSION
This section describes ElasticNotebook’s implementation details
(§6.1) and design considerations (§6.2).

6.1 Implementation
Integrating with Jupyter. For seamless integration, ElasticNote-
book’s data layer is implemented using a magic extension [103],
which is loaded into the kernel upon session initialization. The cell
magic is automatically added to each cell (§2.2) to transparently
intercept user cell executions, perform code analyses, create ID
Graphs and object hashes, and so on.

Serialization Protocol. The Pickle protocol (e.g., __reduce__) is
employed for (1) object serialization and (2) definition of reachable
objects, i.e., an object y is reachable from a variable x if pickle(x)
includes y. As Pickle is the de-facto standard (in Python) observed
by almost all data science libraries (e.g., NumPy, PyTorch [29]),
ElasticNotebook can be used for almost all use cases.
Handling Undeserializable variables. Certain variables can be
serialized but contain errors in its deserialization instructions (which
we refer to as undeserializable variables), and are typically caused
by oversights in incompletely implemented libraries [15, 69]. While
undetectable via serializability checks prior to checkpointing, Elas-
ticNotebook handles them via fallback recomputation: if Elastic-
Notebook encounters an error while deserializing a stored variable
during session restoration, it will trace the AHG to determine and
rerun (only) necessary cell executions to recompute said variable,
which is still faster than recomputing the session from scratch.

6.2 Design Considerations
Definition of Session State. In ElasticNotebook, the session state
is formally defined as the contents of the user namespace dictionary
(user_ns), which contains key-value pairs of variable names to their

2Rerunning 𝑡3 also recomputes l1; however, it will be overwritten with the stored
l1 in the checkpoint file following the procedure in §3.2. This is to preserve the link
between l1 and 2dlist1.

Zhaoheng Li, Pranav Gor, Rahul Prabhu, Hui Yu, Yuzhou Mao, Yongjoo Park

values (i.e., reachable objects). The session state does not include
local/module/hidden variables, which we do not aim to capture.
Unobservable State / External Functions. Although the Pickle
protocol is followed by almost all libraries, there could be lesser-
known ones with incorrect serialization (e.g., ignoring data defined
in a C stack). To address this, ElasticNotebook can be easily ex-
tended to allow users to annotate cells/variables to inform our
system that they must be recomputed for proper reconstruction.
Mathematically, this has the same effect as setting their recomputa-
tion costs to infinity in Eq. (2).
Cell Executions with Side Effects. Certain cell executions may
cause external changes outside a notebook session (e.g., filesystem)
and may not be desirable to rerun (e.g., uploading items to a reposi-
tory). Our prototype currently does not identify these side effects
as our focus is read-oriented data science and analytics workloads.
Nevertheless, our system can be extended at least in two ways to
prevent them. (1: Annotation) We can allow users to add manual
annotations to the cells that may cause side effects; then, our system
will never re-run them during replications3
(2: Sandbook) We can
block external changes by replicating a notebook into a sandbox
with altered file system access (e.g., chroot [73]) and blocked out-
going network (e.g., ufw [27]). The sandbox can then be associated
with regular file/network accesses upon successful restoration.
Non-deterministic Operations. The replication has the same ef-
fect as rerunning the cells in the exact same order as they occurred
in the past; thus, under the existence of nondeterministic operations
(e.g., randint()), the reconstructed variables may have different
values than the original ones. Users can avoid this by using annota-
tions to inform ElasticNotebook to always copy them.
Library Version Compatibility. Accurate replication is ensured
when external resources (e.g., installed modules, database tables)
remain the same before and after the replication. While there are
existing tools (i.e., pip freeze [84]) for reproducing computational
environments on existing data science platforms (i.e., Jupyter Note-
book, Colab) [1, 115], this work does not incorporate such tools.

7 EXPERIMENTAL EVALUATION
In this section, we empirically study the effectiveness of Elastic-
Notebook’s session replication. We make the following claims:
1. Robust Replication: Unlike existing mechanisms, ElasticNote-
book is capable of replicating almost all notebooks. (§7.2)
2. Faster Migration: ElasticNotebook reduces session migration
time to upscaled/downscaled machines by 85%–98%/84%-99%
compared to rerunning all cells and is up to 2.07×/2.00× faster
than the next best alternative, respectively. (§7.3)

3. Faster Resumption: ElasticNotebook reduces session restora-
tion time by 94%–99% compared to rerunning all cells and is up
to 3.92× faster than the next best alternative. (§7.4)

4. Low Runtime Overhead: ElasticNotebook incurs negligible
overhead—amortized runtime and memory overhead of <2.5%
and <10%, respectively. (§7.5)

3Replication may be unfeasible due to annotations, e.g., an unserializable variable
requiring an cell execution annotated ’never-rerun’ to recompute. ElasticNotebook can
detect these cases as they have infinite min-cut cost (§5.4), upon which the user can be
warned to delete the problematic variable to proceed with replicating the remaining
(majority of) variables in the state.

ElasticNotebook: Enabling Live Migration for Computational Notebooks

Table 3: Summary of datasets for evaluation.

Dataset
Notebooks Runtime (s) Input data (MB) Cell count
Kaggle [57]
35
JWST [60]
5
5
Tutorial [107]
HW [13, 46, 66] 15

107–12,560
2–109
1–139
16–439

178-31831
25–323
10–96
9–1203

15–103
21–44
10–48
11–160

5. Low Storage Overhead: ElasticNotebook’s checkpoint sizes

are up to 66% smaller compared to existing tools. (§7.6)

6. Adaptability to System Environments: ElasticNotebook
achieves consistent savings across various environments with
different network speeds and available compute resources. (§7.7)
7. Scalability for Complex Notebooks: ElasticNotebook’s run-
time and memory overheads remain negligible (<150ms, <4MB)
even for complex notebooks with 2000 cells. (§7.8)

7.1 Experiment Setup
Datasets. We select a total of 60 notebooks from 4 datasets:
• Kaggle [57]: We select 35 popular notebooks on the topic of EDA
(exploratory data analysis) + machine learning from Kaggle
created by Grandmaster/Master-level users.

• JWST [60]: We select 5 notebooks on the topic of data pipelining
from the example notebooks provided on investigating data
from the James Webb Space Telescope (JWST).

• Tutorial [107]: We select 5 notebooks from the Cornell Vir-
tual Workshop Tutorial. These notebooks are lightweight and
introduce tools (i.e., clustering, graph analysis) to the user.
• Homework [13, 46, 66]: 15 in-progress notebooks are chosen
from data science exercises. They contain out-of-order cell exe-
cutions, runtime errors, and mistakes (e.g., df_backup=df4).
Table 3 reports our selected notebooks’ dataset sizes and runtimes.

Methods. We evaluate ElasticNotebook against existing tools ca-
pable of performing session replication:
• RerunAll [102]: Save (only) cell code and outputs as an ipynb

file. All cells are rerun to restore the session state.

• CRIU [18]: Performs a system-level memory dump of the pro-
cess hosting the notebook session. The session state is restored
by loading the memory dump and reviving the process.

• %Store [104]: A checkpointing tool that serializes variables one
by one into storage. We use a modified version using Dill [39]
instead of Pickle [38] for robustness.5

• DumpSession [40]: Unlike %Store, DumpSession packs the en-

tire session state into one single file.

Ablation Study. We additionally compare against the following
ablated implementations of ElasticNotebook:
• ElasticNotebook + Helix [116]: We replace our min-cut solution
with Helix, which does not consider linked variables (§5.3).
• EN (No ID graph): This method omits ID Graphs, relying only on
AST analysis and object hashes for detecting variable accesses
and modifications, respectively.

4This creates a shallow copy of df, which does not serve the purpose of backup.
5The original implementation of %store uses Python Pickle [38], and fails on too many
notebooks to give meaningful results.

RerunAll

%Store

EN (No ID graph)
100%
80%
60%
40%
20%
0%

)

%

(

e
t
a
r

s
s
e
c
c
u
S

CRIU (same architecture)
DumpSession

ElasticNotebook (Ours)

CRIU (cross-architecture)
ElasticNotebook + Helix

e
r
u
l
i
a
f
%
0
0
1

Figure 9: Ratio of correct replications. ElasticNotebook
achieves 100% correctness, on par with full rerun (RerunAll).

Table 4: Existing work fails for these cases. Ours works.

Notebook(s)
NFL [88]

All 5 JWST
notebooks [60]

Arxiv [70]
Plant [94]

Type
hashlib [33]

mmap [36]

Description and purpose
Dropdown list in plot

Helps avoid reading large file
into memory

generator [32]

Speedup iterable comprehension
via lazy element generation

We consider these methods regarding replication correctness (§7.2)
to gauge the impact of ignoring (1) the linked constraint and (2)
implicit accesses and structural modifications, respectively.

Environment. We use an Azure Standard D32as v5 VM instance
with 32 vCPUs and 128 GB RAM. For the migration experiment
(§7.3), we migrate sessions from D32as to D64as/D16as with 64/16
vCPUs and 256/64 GB RAM for upscaling/downscaling, respectively.
Input data and checkpoints are read/stored from/to an Azure stor-
age with block blobs configuration (NFS). Its network bandwidth is
274 MB/s with a read latency of 175 𝜇𝑠.

Time measurement. We measure (1) migration time as the time
from starting the checkpointing process to having the state restored
(i.e., all variables declared into the namespace) in the destination
session and (2) restoration time as the time to restore the state from
a checkpoint file. We clear our cache between (1) checkpointing
and restoring a notebook and (2) between subsequent runs.

Reproducibility. Our implementation of ElasticNotebook, experi-
ment notebooks, and scripts can be found in our Github repository.6

7.2 Robust Session Replication
This section compares the robustness of ElasticNotebook’s session
replication to existing methods. We count the number of isomorphic
(thus, correct) replications (§5.1) achieved with each method on the
60 notebooks and report the results in Fig 9.

ElasticNotebook correctly replicates all sessions, on par with
full rerun from checkpoint file (which almost always works). No-
tably, it replicates 19, 25, and 2 notebooks containing unserializable
variables, variable aliases, and undeserializable variables (§6.1), re-
spectively. DumpSession and %Store fail on 19/60 notebooks con-
taining unserializable variables, many of which are used to enhance
data science workflow efficiency (examples in Table 4); ElasticNote-
book successfully replicates them as it can bypass the serialization
of these variables through recomputation. %Store additionally fails
on 21/60 notebooks (total 40/60) without unserializable variables
but contain variable aliases (i.e., Timeseries [89] notebook, Cell 15,

6https://github.com/illinoisdata/ElasticNotebook

RerunAll

CRIU

100%

100%

100%

100%

%Store
49%

DumpSession

ElasticNotebook (Ours)

100%

100%

100%

100%

100%

51%

100%

100%

84%

100%

60%86%

Zhaoheng Li, Pranav Gor, Rahul Prabhu, Hui Yu, Yuzhou Mao, Yongjoo Park

Sklearn [109]

NLP [2]

StoreSales [7] TPS-Mar [113]

Glove [92]

Trading [95] Timeseries [80] Stacking [24] Agriculture [109] LANL [87]

HW-LM [45] HW-ex3 [67]

l
l

A
n
u
r
e
R
f
o
%
e
m
T

i

40%
30%
20%
10%
0%

Figure 10: ElasticNotebook’s session upscaling time (D32as v5 VM→D64as v5 VM) vs. existing tools. Times normalized w.r.t.
RerunAll. ElasticNotebook speeds up migration by 85%-98% and is up to 2.07× faster than the next best alternative.

l
l

A
n
u
r
e
R
f
o
%
e
m
T

i

20%
15%
10%
5%
0%

RerunAll

100%

100%

CRIU

100%

%Store

DumpSession

ElasticNotebook (Ours)

100%

100%

100%

100%

100%

100%

31%

100%

29%

100%

24%

100%

25%

Sklearn [109]

NLP [2]

StoreSales [7] TPS-Mar [113]

Glove [92]

Trading [95] Timeseries [80] Stacking [24] Agriculture [109] LANL [87]

HW-LM [45] HW-ex3 [67]

Figure 11: ElasticNotebook’s session restoration time vs. existing tools. Times normalized w.r.t. RerunAll. ElasticNotebook
speeds up session restore by 94%-99%, and is up to 3.92× faster compared to the next best alternative.

Table 5: Runtime and memory overhead of ElasticNotebook’s workflow monitoring on selected notebooks.

Notebook runtime (s)
Total cell monitoring time (s)
Runtime overhead (%)
User Namespace memory usage (MB) 1021.45
ElasticNotebook memory usage (MB) 19.16
1.88
Memory overhead (%)

Sklearn
58.48
1.26
2.14

NLP
1016.77
4.30
0.42
325.82
4.73
1.45

StoreSales TPS-Mar Glove
696.64
178.42
283.06
6.43
1.34
0.81
0.92
0.78
0.28
347.16
1558.52
6732.17
33.25
1.69
0.14
9.58
0.11
0.002

Trading
687.54
0.46
0.07
1363.32
4.09
0.30

Timeser.
204.10
0.60
0.29
130.27
0.28
0.21

Stacking Agricult. LANL
269.40
788.54
3.08
2.13
1.14
0.27
5026.48
20211.51
0.06
0.33
0.001
0.002

1437.87
0.19
0.01
7641.19
0.14
0.001

HW-LM HW-ex3
22.54
0.50
2.21
31.28
0.99
3.16

27.29
0.09
0.32
19.06
0.47
2.45

linked components of a Matplotlib [105] plot—f,fig,ax) ; it serial-
izes variables into individual files, which breaks object references
and isomorphism. ElasticNotebook’s linked variables constraint
(§5.3) ensures that it does not do so. ElasticNotebook + Helix fails
to correctly replicate 5/60 notebooks containing variable aliases due
to its lacking of the linked variable constraint. EN (No ID graph)
fails to correctly replicate 11/60 sessions due to it missing indirect
accesses and structural modifications causing incorrect construc-
tion of the AHG, which in turn leads it to recompute some variables
value-incorrectly. CRIU fails on one notebook [90] which contains
an invisible file; however, unlike ElasticNotebook’s failures, this
failure is currently a fundamental limitation in CRIU [17].
Robust Migration across System Architectures. We additionally
performed session replication from our D32as VM (x64 architecture)
to a D32pds V5 VM instance (arm64 architecture). The CRIU images
cannot be replicated across machines with different architectures.
In contrast, ElasticNotebook does not have such a limitation.

7.3 Faster Session Migration
This section compares the efficiency of ElasticNotebook’s session
migration to existing methods. We choose 10 notebooks with no
unserializable variables (otherwise, existing methods fail) to com-
pare the end-to-end session migration time achieved by different
methods. We report upscaling and downscaling results in Fig 10
and Fig 16, respectively.

The design goal of ElasticNotebook is to reduce session replica-
tion time through balancing variable storage and recomputation,
which is successfully reflected as follows. ElasticNotebook is able
to reduce session migration time to the upscaled/downscaled VMs
by 85%–98%/84%-99% compared RerunAll. Compared to DumpSes-
sion, %Store, and CRIU, which store all variables in the checkpoint

file, ElasticNotebook upscales/downscales up to 2.07×/2.00× faster
than the best of the three. DumpSession, while being the next best
alternative for upscaling/downscaling on 8/9 notebooks, falls short
in robustness as demonstrated in §7.2. %Store’s individual reading
and writing of each variable results in high overhead from multiple
calls to the NFS for each migration. CRIU is the slowest non-rerun
method for upscaling/downscaling on 6/7 notebooks, due to the
size of its memory dump (higher I/O during migration) being up to
10× larger compared to checkpoint files from native tools (§7.6).

7.4 Faster Session Restoration
In this section, we compare the efficiency of ElasticNotebook’s
session restoration to existing methods. We generate checkpoint
files using each method, then compare the time taken to restore
the session from the checkpoint files on the 10 notebooks from
§7.3. For ElasticNotebook, we set the coefficient 𝛼 to 0.05 (§5.2) to
emphasize session restoration time heavily.

We report the results in Fig 10. ElasticNotebook’s restoration
time is 94%–99% faster compared to full rerun. Compared to the
baselines, ElasticNotebook is 3.92× faster than the next best alter-
native. These fast restoration can be attributed to ElasticNotebook
capable of adapting to the new optimization objective, unlike the
baselines: for example, on the Sklearn [109] notebook, instead of re-
running cell 3 (df = pd.read_csv(...)) to re-read the dataframe
df into the session as in the migration-centric plan, the restoration-
centric plan opts to store df instead. The reasoning is that despite
the sum of serialization and deserialization times of df being greater
than the re-reading time with pd.read_csv (6.19s + 1.17s > 5.5s),
the deserialization time by itself is less than the re-reading time
(1.17s < 5.5s); hence, storing df is the optimal choice.

ElasticNotebook: Enabling Live Migration for Computational Notebooks

RerunAll
DumpSession
1048%

CRIU
ElasticNotebook (Ours)
283%

388%

467%

%Store

1137%

200%
150%
100%
50%

S
D

f
o
%
e
z
i
S

0.5%

0.4%

5.6%

2.6%

0.1%

0.5%

NLP[2]

TPS[113] Trading[95] Timeseries[80]Agriculture[109] HW-LM [45]

Figure 12: ElasticNotebook’s checkpoint file size vs. exist-
ing tools. Times normalized w.r.t. output from DumpSession.
ElasticNotebook’s checkpoint file size is up to 67% smaller
compared to those from existing tools (excluding RerunAll).

7.5 Low Runtime Overhead
This section investigates the overhead of ElasticNotebook’s note-
book workflow monitoring. We measure ElasticNotebook’s total
time spent in pre/post-processing steps before/after each cell execu-
tion for updating the AHG and cell runtimes (Total cell monitoring
time), and total storage space taken to store the AHG, ID Graphs,
and hashes at checkpoint time (ElasticNotebook memory usage).

We report the results in Table 5. ElasticNotebook’s cell monitor-
ing incurs a maximum and median runtime overhead of (only) 2.21%
and 0.6%; thus, ElasticNotebook can be seamlessly integrated into
existing workflow. ElasticNotebook is similarly memory-efficient
as its stored items (AHG, ID Graphs, and hashes) are all metadata
largely independent of the size of items in the session: the median
memory overhead is 0.25%, with the worst case being 9.58%.
Fine-grained Analysis. To study the per-cell time and memory
overheads during experimental notebook usage, we examined three
notebooks from Homework category to confirm the maximum time
and memory overheads were 92ms and 4.9MB, respectively. We
report details in Appendix A.1.

7.6 Lower Storage Overhead
This section measures the storage cost of ElasticNotebook’s check-
point files: we compare the migration-centric checkpoint file sizes
from ElasticNotebook and those from other baseline methods.

We report select results in Fig 12. ElasticNotebook’s AHG al-
lows it to choose between storing and recomputing each variable,
reflected in ElasticNotebook’s checkpoint files being up to 67%
smaller compared to DumpSession’s. For example, on the Agri-
culture [89] notebook, ElasticNotebook recomputes the train-test
splits of the input dataframes X and Y (Cell 5, x_train, x_test,...
= train_test_split(X, Y)) instead of storing them in the check-
point file: this saves considerable storage space (2.5GB) in addition
to speeding up migration. Conversely, CRIU’s checkpoint file sizes
can be 10× larger than ElasticNotebook’s as it additionally dumps
memory occupied by the Python process itself and imported mod-
ules, no matter necessary or not, into the checkpoint file. Output
sizes from RerunAll (i.e., notebook metadata size consisting of cell
code and outputs) are provided for comparison. While metadata
are significantly smaller than checkpoint files, the storage benefit
is offset by significantly slower session recovery times (§7.4).

7.7 Performance Gains Across Environments
This section demonstrates ElasticNotebook’s operation in environ-
ments with varying specifications. We perform a parameter sweep

ElasticNotebook Migrate Time
DumpSession

ElasticNotebook Recompute Time
RerunAll

)
s
(
e
m
T

i

)
s
(
e
m
T

i

1,250
1,000
750
500
250
0

2,000
1,500
1,000
500
0

1600 800 400 200 100 50
Network bandwidth (Mbps)
(a) AI4CODE [53]

1600 800 400 200 100 50
Network bandwidth (Mbps)

)
s
(
e
m
T

i

2,500
2,000
1,500
1,000
500
0

)
s
(
e
m
T

i

500
400
300
200
100
0

1600 800 400 200 100 50
Network bandwidth (Mbps)
(b) Stacking [24]

1600 800 400 200 100 50
Network bandwidth (Mbps)

(c) Agriculture [89]

(d) Asset [95]

Figure 13: ElasticNotebook adapts to different environments
for its replication plan. The lower the network bandwidth,
the more variables are recomputed.
Twitter [110]

Interactive [108]

Sklearn [109]

)
B
M

(
d
a
e
h
r
e
v
O

4
3
2
1
0

)
s

m

(

e
m
T

i

0

500
1500
1000
No. cell executions

2000

(a) AHG size

150

100

50

0

0

500
1500
1000
No. cell executions
(b) Optimization Time

2000

Figure 14: Scalability of ElasticNotebook with cell execution
count. The size of AHG increases linearly. Replication plan
optimization time increases sub-linearly.

on the NFS network bandwidth via rate limiting [10] and compare
the migration time of ElasticNotebook, DumpSession (migrating
all variables), and RerunAll.

We report the results in Fig 13. ElasticNotebook’s balancing of
variables storage and recomputation ensures that it is always at least
as fast as the faster of DumpSession and RerunAll. Notably, Elastic-
Notebook can adapt to the relative availability between network
bandwidth and compute power: as the bandwidth decreases, the
replication plan is changed accordingly to migrate more variables
through recomputation rather than storage. For example, on the
Stacking [24] notebook, at regular bandwidth (>400Mbps), Elastic-
Notebook’s replication plan includes migrating most of the session
state, opting only to recompute certain train/test splits (i.e., Cell 37,
Y_train, Y_validation). At <400 Mbps, ElasticNotebook modifies
its plan to recompute instead of store a computationally expensive
processed dataframe (Cell 39, latest_record). At <100 Mbps, Elas-
ticNotebook modifies its plan again to only store the imported class
and function definitions (i.e., XGBRegressor, mean_squared_error
in Cell 1) while recomputing the rest of the notebook.

7.8 Scaling to Complex Workloads
In this section, we test the scalability of ElasticNotebook’s session
replication on complex notebook sessions with a large number of
cell executions and re-executions. Specifically, we choose 3 tutorial
notebooks, on which we randomly re-execute cells and measure
the (1) size of ElasticNotebook’s AHG and (2) optimization time for
computing the replication plan at up to 2000 cell re-executions7.

7This is twice the length of the longest observed notebook on Kaggle [50].

We report the results in Fig 14. The memory consumption of Elas-
ticNotebook’s AHG exhibits linear scaling vs. the number of cell
executions reaching only <4MB at 2000 cell re-executions, which
is negligible compared to the memory consumption of the note-
book session (>1GB) itself. ElasticNotebook’s optimization time
for computing the replication plan similarly exhibits linear scaling,
reaching a negligible <150ms at 2000 cell re-executions: ElasticNote-
book’s chosen algorithm for solving min-cut, Ford-Fulkerson [30],
has time complexity 𝑂 (𝐸 𝑓 ), where 𝐸 is the number of edges in the
AHG and 𝑓 is the cost of the optimal replication plan: The former
scales linearly while the latter is largely constant.

8 RELATED WORK
Intermediate Result Reuse in Data Science. The storage of in-
termediate results has been explored in various contexts in Data
Science due to the incremental and feed-forward nature of tasks,
which allows outputs from prior operations to be useful for speed-
ing up future operations [42, 55, 65, 111, 116, 117, 122]. Examples
include caching to speed up model training replay for ML model di-
agnosis [42, 111], caching to speed up anticipated future dataframe
operations in notebook workflows [117], and storage of cell out-
puts to facilitate graphical exploration of the notebook’s execu-
tion history for convenient cell re-runs [55, 65]. There are related
works [116, 122] which algorithmically explore the most efficient
way to (re)compute a state given currently stored items; compared
to our work, while Helix [116] similarly features balancing loading
and recomputation, its model lacks the linked variable constraint
which may result in silently incorrect replication if directly applied
to the computational notebook problem setting.

Data-level Session Replication. Session replication on Jupyter-
based platforms can be performed with serialization libraries [34, 35,
38, 39, 78]. There exists a variety of checkpoint tools built on these
serialization libraries: IPython’s %Store [104] is a Pickle-based [38]
interface for saving variables to a key-value store; however, it breaks
object references as linked variables are serialized into separate
files. The Dill-based [39] DumpSession [40] correctly resolves ob-
ject references, yet it still fails if the session contains unserializable
objects. Tensorflow [49] and Pytorch [29] offer periodical check-
pointing during ML model training limited to objects within the
same library. Jupyter’s native checkpointing mechanism [102] only
saves cell metadata and often fails to exactly restore a session due to
the common presence of hidden states. Compared to existing data-
level tools, session replication with ElasticNotebook is both more
efficient and robust: the Application History Graph enables balanc-
ing state storage and recomputation, which achieves considerable
speedup while avoiding failure on unserializable objects.

System-Level Session Replication. Session replication can sim-
ilarly be performed using system-level checkpoint/restart (C/R)
tools, on which there is much existing work [6, 6, 8, 12, 23, 52, 72,
77, 96]. Applicable tools include DMTCP [3] and CRIU [18]; recently,
CRUM [43] and CRAC [61] have explored extending C/R to CUDA
applications. Elsa [64] integrates CRIU with JupyterHub to enable
C/R of JupyterHub servers. Compared to ElasticNotebook, system-
level tools are less efficient and robust due to their large memory
dump sizes and limited cross-platform portability, respectively.

Zhaoheng Li, Pranav Gor, Rahul Prabhu, Hui Yu, Yuzhou Mao, Yongjoo Park

Lineage Tracing. Lineage tracing has seen extensive use in state
management to enable recomputation of data for more efficient
storage of state or fault tolerance [16, 51, 71, 82, 106, 111, 120]
Recently, the usage of data lineage in computational notebooks
has enabled multi-version notebook replay [76], recommending
notebook interactions [75], and creating reproducible notebook
containers [1], and program slicing, i.e., finding the minimal set of
code to run to compute certain variable(s) [51, 54, 65, 83, 93]. This
work adopts lineage tracing techniques to capturing inter-variable
dependencies (the Application History Graph) for optimization; to
the best of our knowledge, existing works on Python programs
focus on capturing value modifications (via equality comparisons);
however, our techniques additionally identifies and captures strucal
changes via the ID graph, which is crucial for preserving variable
aliases and avoiding silent errors during state replication.
Replicating Execution Environment. An identical execution en-
vironment may be necessary for session replication on a different
machine. There is some recent work exploring environment repli-
cation for Jupyter Notebook via containerizing input files and mod-
ules [1, 115]. While useful in conjunction with ElasticNotebook,
we consider these works to be largely orthogonal.
Notebook Parameterization and Scripts. There exists works on
executing notebooks in parameterized form for systematic experi-
mentation (e.g., in the form of a script via [99] or papermill [100]).
While ElasticNotebook is designed for use within interactive note-
book interfaces, it is similarly applicable for the migration of pa-
rameterized notebook execution results.

9 CONCLUSION
In this work, we have proposed ElasticNotebook, a new computa-
tional notebook system that newly offers elastic scaling and check-
pointing/restoration. To achieve this, ElasticNotebook introduces
a transparent data management layer between the user interface
and the underlying kernel, enabling robust, efficient, and platform-
independent state replication for notebook sessions. Its core con-
tributions include (1) low-overhead, on-the-fly application history
construction and (2) a new optimization for combining copying and
re-computation of variables that comprise session states. We have
demonstrated that ElasticNotebook can reduce upscaling, down-
scaling, and restoration times by 85%-98%, 84%-99%, and 94%-99%,
respectively, on real-world data science notebooks with negligible
runtime and memory overheads of <2.5% and <10%, respectively.
In the future, we plan to achieve higher efficiency and usability by
tracing state changes at a finer level. Specifically, we will introduce
micro-cells to capture code blocks inside a cell that repeatedly runs
(e.g., for-loop for machine learning training). Then, the system will
automatically store intermediate models (along with other meta-
data) that will enable live migration and checkpointing/restoration
for long-running cell executions.

ACKNOWLEDGMENTS
The authors are grateful to Chandra Chekuri and Kent Quanrud for
assistance with the derivation of the reduction to min-cut employed
in ElasticNotebook. This work is supported in part by the National
Center for Supercomputing Applications and Microsoft Azure.

ElasticNotebook: Enabling Live Migration for Computational Notebooks

REFERENCES

[1] Raza Ahmad, Naga Nithin Manne, and Tanu Malik. 2022. Reproducible Notebook
Containers using Application Virtualization. In 2022 IEEE 18th International
Conference on e-Science (e-Science). IEEE, 1–10.

[2] AndresHG. 2021. NLP, GloVe, BERT, TF-IDF, LSTM... Explained. https://www.
kaggle.com/code/andreshg/nlp-glove-bert-tf-idf-lstm-explained/notebook.
Jason Ansel, Kapil Arya, and Gene Cooperman. 2009. DMTCP: Transparent
checkpointing for cluster computations and the desktop. In 2009 IEEE Interna-
tional Symposium on Parallel & Distributed Processing. IEEE, 1–12.

[4] Microsoft Azure. 2023. Azure ML Studio. https://learn.microsoft.com/en-

[3]

us/azure/machine-learning/how-to-run-jupyter-notebooks.

[5] Microsoft Azure. 2023. Microsoft Azure pay-as-you-go. https://azure.microsoft.

com/en-us/pricing/purchase-options/pay-as-you-go/.

[6] Anju Bala and Inderveer Chana. 2012. Fault tolerance-challenges, techniques
and implementation in cloud computing. International Journal of Computer
Science Issues (IJCSI) 9, 1 (2012), 288.

[7] Ekrem Bayar. 2022.

Store Sales TS Forecasting - A Comprehensive
Guide. https://www.kaggle.com/code/ekrembayar/store-sales-ts-forecasting-a-
comprehensive-guide/notebook.

[8] Mohammad Riyaz Belgaum, Safeeullah Soomro, Zainab Alansari, and Muham-
mad Alam. 2018. Cloud service ranking using checkpoint-based load balancing
in real-time scheduling of cloud computing. In Progress in advanced computing
and intelligent engineering. Springer, 667–676.
James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter
optimization. Journal of machine learning research 13, 2 (2012).

[9]

[10] Simon Séhier Bert Hubert, Jacco Geul. 2020. WonderShaper. https://github.

com/magnific0/wondershaper.

[11] Michael Brachmann and William Spoth. 2020. Your notebook is not crumby
enough, REPLace it. In Conference on Innovative Data Systems Research (CIDR).
[12] Gang Chen, Hai Jin, Deqing Zou, Bing Bing Zhou, Weizhong Qiang, and Gang
Hu. 2010. Shelp: Automatic self-healing for multiple application instances in a
virtual machine environment. In 2010 IEEE International Conference on Cluster
Computing. IEEE, 97–106.

[13] Chhaya Choudhary. 2023. Machine Learning and Deep learning Notebooks.

https://github.com/chhayac/Machine-Learning-Notebooks.

[14] Chhaya Choudhary. 2023. This project is about customer churn predic-
tion. https://github.com/chhayac/Machine-Learning-Notebooks/blob/master/
customer_churn_prediction.ipynb.

[15] Bokeh Contributors. 2023. Bokeh - Interaction. https://docs.bokeh.org/en/

[16]

latest/docs/user_guide/interaction.html.
Iván Cores, Gabriel Rodríguez, Mará J Martín, Patricia González, and Roberto R
Osorio. 2013. Improving scalability of application-level checkpoint-recovery by
reducing checkpoint sizes. New Generation Computing 31 (2013), 163–185.

[17] CRIU. 2023. CRIU - Invisible file. https://criu.org/Invisible_files.
[18] CRIU. 2023. Linux CRIU. https://criu.org/Main_Page.
[19] Andrew Crotty, Alex Galakatos, Emanuel Zgraggen, Carsten Binnig, and Tim
Kraska. 2015. Vizdom: interactive analytics through pen and touch. Proceedings
of the VLDB Endowment 8, 12 (2015), 2024–2027.
JupyterHub Idle Culler. 2023.
jupyterhub/jupyterhub-idle-culler.

JupyterHub Idle Culler. https://github.com/

[20]

[21] Renato LF Cunha, Lucas C Villa Real, Renan Souza, Bruno Silva, and Marco AS
Netto. 2021. Context-aware Execution Migration Tool for Data Science Jupyter
Notebooks on Hybrid Clouds. In 2021 IEEE 17th International Conference on
eScience (eScience). IEEE, 30–39.

[22] Nvidia Developer. 2023. Nvidia - CUDA. https://developer.nvidia.com/cuda-

toolkit.

[23] Sheng Di, Yves Robert, Frédéric Vivien, Derrick Kondo, Cho-Li Wang, and
Franck Cappello. 2013. Optimization of cloud task processing with checkpoint-
restart mechanism. In Proceedings of the International Conference on High Per-
formance Computing, Networking, Storage and Analysis. 1–12.

[24] DimitreOliveira. 2019. Model stacking, feature engineering and EDA.
https://www.kaggle.com/code/dimitreoliveira/model-stacking-feature-
engineering-and-eda/notebook.

[25] Docker. [n.d.]. Docker documentation - Swarm mode overview. https://docs.

docker.com/engine/swarm/.

[26] Cody Dunne, Nathalie Henry Riche, Bongshin Lee, Ronald Metoyer, and George
Robertson. 2012. GraphTrail: Analyzing large multivariate, heterogeneous
networks while supporting exploration history. In Proceedings of the SIGCHI
conference on human factors in computing systems. 1663–1672.

[27] dwd daniel. 2022.

UncomplicatedFirewall.

https://wiki.ubuntu.com/

UncomplicatedFirewall.

[28] Philipp Eichmann, Emanuel Zgraggen, Carsten Binnig, and Tim Kraska. 2020.
Idebench: A benchmark for interactive data exploration. In Proceedings of the
2020 ACM SIGMOD International Conference on Management of Data. 1555–1569.
https://pytorch-

[29] Lightning AI et al. 2018.

PyTorch ModelCheckpoint.

lightning.readthedocs.io/en/stable/api/pytorch_lightning.callbacks.
ModelCheckpoint.html.

[30] LRDR FORD-FULKERSON. 1962. Flows in Networks.
[31] Python Software Foundation. 2023. Python - AST. https://docs.python.org/3/

library/ast.html.

[32] Python Software Foundation. 2023. Python - Generators. https://wiki.python.

org/moin/Generators.

[33] Python Software Foundation. 2023. Python Hashlib. https://docs.python.org/3/

library/hashlib.html.

[34] Python Software Foundation. 2023. Python JSON. https://docs.python.org/3/

library/json.html.

[35] Python Software Foundation. 2023. Python Marshal. https://docs.python.org/3/

library/marshal.html.

[36] Python Software Foundation. 2023. Python Mmap. https://docs.python.org/3/

library/mmap.html.

[37] Python Software Foundation. 2023. Python Object Reduction. https://docs.

python.org/3/library/pickle.html#object.__reduce__.

[38] Python Software Foundation. 2023. Python Pickle Documentation. https:

//docs.python.org/3/library/pickle.html.

[39] The Uncertainty Quantification Foundation. 2023. Dill - PyPi. https://pypi.org/

project/dill/.

[40] The Uncertainty Quantification Foundation. 2023. Dill dump session. https:

//dill.readthedocs.io/en/latest/dill.html.

[41] Tian Gao. 2020. Python Watchpoints. https://pypi.org/project/watchpoints/.
[42] Rolando Garcia, Eric Liu, Vikram Sreekanti, Bobby Yan, Anusha Dandamudi,
Joseph E Gonzalez, Joseph M Hellerstein, and Koushik Sen. 2020. Hindsight
logging for model training. arXiv preprint arXiv:2006.07357 (2020).

[43] Rohan Garg, Apoorve Mohan, Michael Sullivan, and Gene Cooperman. 2018.
CRUM: Checkpoint-restart support for CUDA’s unified memory. In 2018 IEEE
International Conference on Cluster Computing (CLUSTER). IEEE, 302–313.

[44] GDB. 2022. GDB Watchpoints.

https://sourceware.org/gdb/download/

onlinedocs/gdb/Set-Watchpoints.html.

[45] Aurélien Geron. 2023. Chapter 4 – Training Models. https://github.com/ageron/

handson-ml3/blob/main/04_training_linear_models.ipynb.

[46] Aurélien Geron. 2023. Machine Learning Notebooks, 3rd edition. https://github.

com/ageron/handson-ml3.

[47] Google. 2023. Google Colab. https://colab.research.google.com/.
[48] Google. 2023. Google Colab pay-as-you-go. https://colab.research.google.com/

signup.

[49] Google. 2023. Tensorflow Checkpoint. https://www.tensorflow.org/guide/

checkpoint.

[50] Google and X. 2022. Google AI4Code – Understand Code in Python Notebooks.

https://www.kaggle.com/competitions/AI4Code.

[51] Philip J Guo and Margo I Seltzer. 2012. Burrito: Wrapping your lab notebook in

computational infrastructure. (2012).

[52] HAProxy. 2023. HAProxy. http://www.haproxy.org/.
[53] Sanskar Hasija. 2022. AI4Code Detailed EDA. https://www.kaggle.com/code/

odins0n/ai4code-detailed-eda.

[55]

[54] Andrew Head, Fred Hohman, Titus Barik, Steven M Drucker, and Robert DeLine.
2019. Managing messes in computational notebooks. In Proceedings of the 2019
CHI Conference on Human Factors in Computing Systems. 1–12.
Inc. Hex Technologies. 2023. Hex 2.0: Reactivity, Graphs, and a little bit of
Magic. https://hex.tech/blog/hex-two-point-oh/.
IBM. 2022.
knowledge-accelerators/1.0.0?topic=catalog-jupyter-notebook.

IBM Watson Studio Service. https://www.ibm.com/docs/en/

[56]

[57] Kaggle Inc. 2023. Kaggle. https://www.kaggle.com/.
[58] Kaggle Inc. 2023. Kaggle Forums - Product Feedback. https://www.kaggle.com/

discussions/product-feedback.

[59] Kaggle Inc. 2023. Kaggle Notebook Specifications. https://www.kaggle.com/

docs/notebooks#technical-specifications.

[60] Space Telescope Science Institute. 2023.

JWST Data Analysis Exam-
ple. https://jwst-docs.stsci.edu/jwst-post-pipeline-data-analysis/data-analysis-
example-jupyter-notebooks.

[62]

[61] Twinkle Jain and Gene Cooperman. 2020. Crac: Checkpoint-restart architecture
for cuda with streams and uvm. In SC20: International Conference for High
Performance Computing, Networking, Storage and Analysis. IEEE, 1–15.
Jeremiah W Johnson. 2020. Benefits and pitfalls of jupyter notebooks in the
classroom. In Proceedings of the 21st Annual Conference on Information Technol-
ogy Education. 32–37.

[63] Project Jupyter. 2023. Jupyter Notebook. https://jupyter.org/.
[64] Mario Juric, Steven Stetzler, and Colin T Slater. 2021. Checkpoint, Restore, and
Live Migration for Science Platforms. arXiv preprint arXiv:2101.05782 (2021).
[65] David Koop and Jay Patel. 2017. Dataflow notebooks: encoding and tracking
dependencies of cells. In 9th USENIX Workshop on the Theory and Practice of
Provenance (TaPP 2017).

[66] Martin Krasser. 2023. Machine learning notebooks. https://github.com/

krasserm/machine-learning-notebook.

[67] Martin Krasser. 2023. Multi-class Classification. https://github.com/krasserm/

machine-learning-notebooks/blob/master/ml-ex3.ipynb.

[68] Kubernetes. [n.d.]. Kubernetes. https://kubernetes.io/.

[69] SFU Database System Lab. 2022. Dataprep - Low-Code Data Preparation. https:

[100] Nteract Team. 2023. Welcome to papermill. https://papermill.readthedocs.io/

//dataprep.ai/.

en/latest/.

[70] Colin Lagator. 2020. Arxiv Data Processing. https://www.kaggle.com/code/

[101] The IPython Development Team. 2023. IPython Interactive Computing. https:

colinlagator/arxiv-data-processing.

//ipython.org/.

Zhaoheng Li, Pranav Gor, Rahul Prabhu, Hui Yu, Yuzhou Mao, Yongjoo Park

[71] Haoyuan Li, Ali Ghodsi, Matei Zaharia, Scott Shenker, and Ion Stoica. 2014.
Tachyon: Reliable, memory speed storage for cluster computing frameworks.
In Proceedings of the ACM Symposium on Cloud Computing. 1–15.

[72] Yawei Li and Zhiling Lan. 2010. FREM: A fast restart mechanism for general

checkpoint/restart. IEEE Trans. Comput. 60, 5 (2010), 639–652.
[73] Arch Linux. 2023. chroot. https://wiki.archlinux.org/title/chroot.
[74] Zhicheng Liu and Jeffrey Heer. 2014. The effects of interactive latency on
exploratory visual analysis. IEEE transactions on visualization and computer
graphics 20, 12 (2014), 2122–2131.

[75] Stephen Macke, Hongpu Gong, Doris Jung-Lin Lee, Andrew Head, Doris Xin,
and Aditya Parameswaran. 2020. Fine-grained lineage for safer notebook
interactions. arXiv preprint arXiv:2012.06981 (2020).

[76] Naga Nithin Manne, Shilvi Satpati, Tanu Malik, Amitabha Bagchi, Ashish
Gehani, and Amitabh Chaudhary. 2022. CHEX: Multiversion Replay with
Ordered Checkpoints. arXiv preprint arXiv:2202.08429 (2022).

[77] Anjali D Meshram, AS Sambare, and SD Zade. 2013. Fault tolerance model for
reliable cloud computing. International Journal on Recent and Innovation Trends
in Computing and Communication 1, 7 (2013), 600–603.
Inc. MongoDB. 2023. BSON. https://pymongo.readthedocs.io/en/stable/api/
bson/index.html.

[78]

[79] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard
Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I Jordan,
et al. 2018. Ray: A distributed framework for emerging {AI} applications. In
13th {USENIX} Symposium on Operating Systems Design and Implementation
({OSDI} 18). 561–577.

[80] Rob Mulla. 2020. Time Series forecasting with Prophet. https://www.kaggle.

com/code/robikscube/time-series-forecasting-with-prophet.

[81] Devin Petersohn, Stephen Macke, Doris Xin, William Ma, Doris Lee, Xiangxi
Mo, Joseph E Gonzalez, Joseph M Hellerstein, Anthony D Joseph, and Aditya
Parameswaran. 2020. Towards scalable dataframe systems. arXiv preprint
arXiv:2001.00888 (2020).

[82] Arnab Phani, Benjamin Rath, and Matthias Boehm. 2021. LIMA: Fine-grained
Lineage Tracing and Reuse in Machine Learning Systems. In Proceedings of the
2021 International Conference on Management of Data. 1426–1439.
Joao Felipe Pimentel, Leonardo Murta, Vanessa Braganholo, and Juliana Freire.
2017. noWorkflow: a tool for collecting, analyzing, and managing provenance
from python scripts. Proceedings of the VLDB Endowment 10, 12 (2017).
[84] The pip developers. 2023. Pip Freeze. https://pip.pypa.io/en/stable/cli/pip_

[83]

freeze/.

[85] Olga Poppe, Qun Guo, Willis Lang, Pankaj Arora, Morgan Oslake, Shize Xu,
and Ajay Kalhan. 2022. Moneyball: proactive auto-scaling in Microsoft Azure
SQL database serverless. Proceedings of the VLDB Endowment 15, 6 (2022),
1279–1287.

[86] PBC Posit Software, PBC formerly RStudio. 2023. Posit RStudio. https://posit.

[102] The IPython Development Team. 2023. Jupyter checkpoint. https://jupyter-

server.readthedocs.io/en/latest/developers/contents.html.

[103] The IPython Development Team. 2023. Jupyter Magics Class. https://ipython.

readthedocs.io/en/stable/config/custommagics.html.

[104] The IPython Development Team. 2023. Jupyter store magic. https://ipython.

readthedocs.io/en/stable/config/extensions/storemagic.html.

[105] The Matplotlib Development Team. 2023. Matplotlib. https://matplotlib.org/.
[106] Quoc-Cuong To, Juan Soto, and Volker Markl. 2018. A survey of state manage-
ment in big data processing systems. The VLDB Journal 27, 6 (2018), 847–872.
[107] Cornell University. 2021. Cornell Virtual Workshop Tutorial Notebooks. https:

//github.com/CornellCAC/CVW_PyDataSci2.

[108] Cornell University. 2021. Investigating Tweet Timelines Using Interactive Bokeh
Scatterplots. https://github.com/CornellCAC/CVW_PyDataSci2/blob/master/
code/interactive_visualization_with_bokeh.ipynb.

[109] Cornell University. 2021.

SKLearn Tweet Classification.

https:

//github.com/CornellCAC/CVW_PyDataSci2/blob/master/code/sklearn_
tweet_classification.ipynb.

[110] Cornell University. 2021. Twitter Networks. https://github.com/CornellCAC/

CVW_PyDataSci2/blob/master/code/twitter_networks.ipynb.

[111] Manasi Vartak, Joana M F. da Trindade, Samuel Madden, and Matei Zaharia.
2018. Mistique: A system to store and query model intermediates for model
diagnosis. In Proceedings of the 2018 International Conference on Management of
Data. 1285–1300.

[112] Alexandre Verbitski, Anurag Gupta, Debanjan Saha, Murali Brahmadesam,
Kamal Gupta, Raman Mittal, Sailesh Krishnamurthy, Sandor Maurice, Tengiz
Kharatishvili, and Xiaofeng Bao. 2017. Amazon aurora: Design considerations
for high throughput cloud-native relational databases. In Proceedings of the 2017
ACM International Conference on Management of Data. 1041–1052.

[113] Devlikamov Vlad. 2022.

[TPS-Mar] Fast workflow using scikit-learn-
https://www.kaggle.com/code/lordozvlad/tps-mar-fast-workflow-

intelex.
using-scikit-learn-intelex/notebook.

[114] Eric-Jan Wagenmakers and Simon Farrell. 2004. AIC model selection using
Akaike weights. Psychonomic bulletin & review 11, 1 (2004), 192–196.
[115] Dimuthu Wannipurage, Suresh Marru, and Marlon Pierce. 2022. A Framework
to capture and reproduce the Absolute State of Jupyter Notebooks. arXiv
preprint arXiv:2204.07452 (2022).

[116] Doris Xin, Stephen Macke, Litian Ma, Jialin Liu, Shuchen Song, and Aditya
Parameswaran. 2018. Helix: Holistic optimization for accelerating iterative
machine learning. arXiv preprint arXiv:1812.05762 (2018).

[117] Doris Xin, Devin Petersohn, Dixin Tang, Yifan Wu, Joseph E Gonzalez, Joseph M
Hellerstein, Anthony D Joseph, and Aditya G Parameswaran. 2021. Enhancing
the interactivity of dataframe queries by leveraging think time. arXiv preprint
arXiv:2103.02145 (2021).

[118] xxHash. 2023. xxHash - Extremely fast non-cryptographic hash algorithm.

co/.

https://github.com/Cyan4973/xxHash.

[87] Gabriel Preda. 2019. LANL Earthquake EDA and Prediction. https://www.

[119] Yandex. 2023. CatBoost - open-source gradient boosting library. https://catboost.

kaggle.com/code/gpreda/lanl-earthquake-eda-and-prediction.

ai/.

[88] Kalilur Rahman. 2022.

NFL Data Bowl 2023 - Offensive Plays EDA.
https://www.kaggle.com/code/kalilurrahman/nfl-data-bowl-2023-offensive-
plays-eda/notebook.

[89] DS Rahul. 2020. Agricultural Drought Prediction. https://www.kaggle.com/

code/dsrhul/agricultural-drought-prediction.

[90] Mani Raj. 2022. Amex Dataset. https://www.kaggle.com/code/manirajheerakar/

amex-dataset.

[91] Amazon Web Services. 2023. AWS JupyterHub. https://docs.aws.amazon.com/

emr/latest/ReleaseGuide/emr-jupyterhub.html.

[92] Shahules. 2022. Basic EDA,Cleaning and GloVe. https://www.kaggle.com/code/

shahules/basic-eda-cleaning-and-glove/notebook.

[93] Shreya Shankar, Stephen Macke, Sarah Chasins, Andrew Head, and Aditya
Parameswaran. 2022. Bolt-on, compact, and rapid program slicing for notebooks.
Proceedings of the VLDB Endowment 15, 13 (2022), 4038–4047.

[94] shreyas thorat30. 2023. Plant disease classification SDP. https://www.kaggle.

com/code/shreyasthorat30/plant-disease-classification-sdp.

[95] Andrey Shtrauss. 2022. Building an Asset Trading Strategy. https://www.kaggle.
com/code/shtrausslearning/building-an-asset-trading-strategy/notebook.
[96] Stelios Sidiroglou, Oren Laadan, Carlos Perez, Nicolas Viennot, Jason Nieh,
and Angelos D Keromytis. 2009. Assure: automatic software self-healing using
rescue points. ACM SIGARCH Computer Architecture News 37, 1 (2009), 37–48.
[97] StackOverflow. 2019. Colab Session Timeout. https://stackoverflow.com/

questions/57113226/how-can-i-prevent-google-colab-from-disconnecting.

[98] Stitchfix. 2017. Nodebooks. https://github.com/stitchfix/nodebook.
[99]

Jupyter Development Team. 2023. nbconvert - Jupyter Notebook Conversion.
https://github.com/jupyter/nbconvert.

[120] Matei Zaharia, Mosharaf Chowdhury, Michael J Franklin, Scott Shenker, and
Ion Stoica. 2010. Spark: Cluster computing with working sets. In 2nd USENIX
Workshop on Hot Topics in Cloud Computing (HotCloud 10).

[121] Emanuel Zgraggen, Robert Zeleznik, and Steven M Drucker. 2014. Panoram-
icData: Data analysis through pen & touch. IEEE transactions on visualization
and computer graphics 20, 12 (2014), 2112–2121.

[122] Ce Zhang, Arun Kumar, and Christopher Ré. 2016. Materialization optimizations
for feature selection workloads. ACM Transactions on Database Systems (TODS)
41, 1 (2016), 1–32.

A APPENDIX
A.1 Low Per-cell overhead
We report the results for per-cell time and memory overheads on
3 Homework notebooks in Fig 15. ElasticNotebook’s memory and
per-cell monitoring overhead are consistently under 10% and 1ms,
respectively. There are occasionally ’spikes’ when certain cells
declaring/modifying complex variables are executed; for example,
the 60% and 91ms memory and time overheads of cell 28 in [46]
is attributed to constructing the ID Graph for a complex nested
list. However, even in this worst case, the time overhead is still

ElasticNotebook: Enabling Live Migration for Computational Notebooks

)
B
M

(
d
a
e
h
r
e
v
O

30

20

10

0

0

User namespace memory usage

20
60
40
No. cell executions

80

)
B
M

(
d
a
e
h
r
e
v
O

ElasticNotebook memory usage
40

30

20

10

0

0

5

10

15

No. cell executions

)
B
M

(
d
a
e
h
r
e
v
O

20

15

10

5

0

0

[45]

[67]

[14]

)
s

m

(

e
m
T

i

100

75

50

25

0

25

0

25%
50%
No. cell executions

75% 100%

10

5
20
No. cell executions

15

(a) Mem. overhead, [45]

(b) Mem. overhead, [67]

(c) Mem. overhead, [14]

(d) Per-cell time overhead

Figure 15: Runtime and memory overhead of ElasticNotebook during notebook use on selected homework notebooks. Memory
overhead is consistently low, and per-cell runtime overhead is negligible for most cell executions.

RerunAll

CRIU

%Store

DumpSession

ElasticNotebook (Ours)

100%
40%

50%

100%

100%

100%

100%

100%

100%

100%

100%

51%

100%

100%

84%

100%

55%

93%

30%

20%

10%

0%

Sklearn [109]

NLP [2]

StoreSales [7] TPS-Mar [113]

Glove [92]

Trading [95] Timeseries [80] Stacking [24] Agriculture [109] LANL [87]

HW-LM [45] HW-ex3 [67]

Notebook

l
l

A
n
u
r
e
R
f
o
%
e
m
T

i

Figure 16: ElasticNotebook’s session downscaling time (D32as v5 VM→D16as v5 VM) vs. existing tools. Times normalized w.r.t.
RerunAll. ElasticNotebook speeds up migration by 84%-99% and is up to 2.00× faster than the next best alternative.

be a an arbitrary variable, and (𝑥, 𝑡 G), (𝑥, 𝑡 G∗ ) be its active VSs in G
and G∗ respectively. There is 𝑡 G ≥ 𝑡 G∗ : if 𝑡 G > 𝑡 G∗ (due to falsely
implied non-overwrite modifications, i.e., gen in Fig 17) then there
must be a path from (𝑥, 𝑡 G) to (𝑥, 𝑡 G∗ ): (𝑥, 𝑡 G), 𝑐𝑡G
, (𝑥, 𝑡𝑘1 ), 𝑐𝑡𝑘1
,..., (𝑥, 𝑡𝑘𝑙 ), 𝑐𝑡𝑘𝑙
< 𝑡 G∗ and
, (𝑥, 𝑡 G∗ ), where 𝑡 G < 𝑡𝑘1
< ... < 𝑡𝑘𝑙
all contain false non-overwrite modifications to 𝑥. There-
𝑐𝑡𝑘1
, ..., 𝑐𝑡𝑘𝑙
fore, the subtree rooted at (𝑥, 𝑡 G) in G must be contained the subtree
rooted at (𝑥, 𝑡 G∗ ) in G∗, hence 𝑟𝑒𝑞∗ (𝑥, 𝑡 G∗ ) ⊆ 𝑟𝑒𝑞(𝑥, 𝑡 G).
□

A.3 Handling Large Pandas Dataframes
To avoid hashing large Pandas dataframes after each cell execution,
ElasticNotebook uses the dataframes’ underlying writeable flag as
a dirty bit to detect in-place changes: before each cell execution, the
writeable flag is set to False, and the dataframe is identified as
modified if the flag has been flipped to True after the cell execution.

G

(x, 𝒕1)

𝒄𝒕1

𝒄𝒕2

(z, 𝒕2)
𝑐𝑡3

G∗

(x, 𝑡1)

(y, 𝑡1)

(y, 𝑡1)

𝑐𝑡1

𝒄𝒕2

(z, 𝒕2)
𝑐𝑡3

(x, 𝑡3)

(l1, 𝑡3)

(x, 𝑡3)

(l1, 𝑡3)

(gen, 𝒕4)

𝒄𝒕4

𝒄𝒕5

(2dlist, 𝑡4)

(gen, 𝒕4)

𝒄𝒕4

𝑐𝑡5

(2dlist, 𝑡4)

(gen, 𝒕5)

𝑟𝑒𝑞∗ (𝑥 ) = {𝑐𝑡2 } ⊆ 𝑟𝑒𝑞 (𝑥 ) = {𝑐𝑡1, 𝑐𝑡2 }
𝑟𝑒𝑞∗ (𝑔𝑒𝑛) = {𝑐𝑡4 } ⊆ 𝑟𝑒𝑞 (𝑔𝑒𝑛) = {𝑐𝑡4, 𝑐𝑡5 }

(x, 𝑡1)

(Overwritten/deleted)
Variable Snapshot

𝑐𝑡1

Cell
Execution

(x, 𝑡1)

Active
Variable Snapshot

Figure 17: AHG G may contain false positives compared to
the true AHG G∗. The correctness is still ensured, while the
efficiency may be affected due to extra cells re-running, for
example, when recomputing z (green) and gen (red).

well under the 500ms threshold suggested for interactive data en-
gines [74], while the memory overhead is of a low absolute value
(4MB) compared to the size of the (not yet loaded) datasets, thus
having negligible user impact.

A.2 Proof of Theorem 4.1
An illustration of our proof is provided in Fig 17.

Proof. As there are no false negatives, the true AHG G∗ is con-
tained within the approximate AHG G, i.e., G∗ ⊆ G (Fig 17). Let 𝑥

","ElasticNotebook : Enabling Live Migration for Computational Notebooks ( Technical Report ) Zhaoheng Li∗ , Pranav Gor∗ , Rahul Prabhu∗ , Hui Yu∗ , Yuzhou Mao+ , Yongjoo Park∗ University of Illinois at Urbana-Champaign∗ University of Michigan+ { zl20 , gor2 , rprabhu5 , huiy3 , yongjoo } @ illinois.edu , yuzhom @ umich.edu 3 2 0 2 p e S 0 2 ] B D . s c [ 1 v 3 8 0 1 1 . 9 0 3 2 : v i X r a ABSTRACT Computational notebooks ( e.g. , Jupyter , Google Colab ) are widely used for interactive data science and machine learning . In those frameworks , users can start a session , then execute cells ( i.e. , a set of statements ) to create variables , train models , visualize results , etc . Unfortunately , existing notebook systems do not offer live migra- tion : when a notebook launches on a new machine , it loses its state , preventing users from continuing their tasks from where they had left off . This is because , unlike DBMS , the sessions directly rely on underlying kernels ( e.g. , Python/R interpreters ) without an addi- tional data management layer . Existing techniques for preserving states , such as copying all variables or OS-level checkpointing , are unreliable ( often fail ) , inefficient , and platform-dependent . Also , re-running code from scratch can be highly time-consuming . In this paper , we introduce a new notebook system , Elastic- Notebook , that offers live migration via checkpointing/restoration using a novel mechanism that is reliable , efficient , and platform- independent . Specifically , by observing all cell executions via trans- parent , lightweight monitoring , ElasticNotebook can find a reliable and efficient way ( i.e. , replication plan ) for reconstructing the origi- nal session state , considering variable-cell dependencies , observed runtime , variable sizes , etc . To this end , our new graph-based opti- mization problem finds how to reconstruct all variables ( efficiently ) from a subset of variables that can be transferred across machines . We show that ElasticNotebook reduces end-to-end migration and restoration times by 85 % -98 % and 94 % -99 % , respectively , on a vari- ety ( i.e. , Kaggle , JWST , and Tutorial ) of notebooks with negligible runtime and memory overheads of < 2.5 % and < 10 % . PVLDB Reference Format : Zhaoheng Li∗ , Pranav Gor∗ , Rahul Prabhu∗ , Hui Yu∗ , Yuzhou Mao+ , Yongjoo Park∗ . ElasticNotebook : Enabling Live Migration for Computational Notebooks . PVLDB , 14 ( 1 ) : XXX-XXX , 2020. doi : XX.XX/XXX.XX 1 INTRODUCTION Computational notebooks1 ( e.g. , Jupyter [ 63 , 101 ] , Rstudio [ 86 ] ) are widely used in data science and machine learning for interactive tu- torials [ 62 ] , data exploration [ 19 , 26 , 121 ] , visualization [ 28 ] , model This work is licensed under the Creative Commons BY-NC-ND 4.0 International License . Visit https : to view a copy of this license . For any use beyond those covered by this license , obtain permission by emailing info @ vldb.org . Copyright is held by the owner/author ( s ) . Publication rights licensed to the VLDB Endowment . Proceedings of the VLDB Endowment , Vol . 14 , No . 1 ISSN 2150-8097. doi : XX.XX/XXX.XX 1In this work , we use the term a “ notebook ” to mean either a system serving the notebook or the contents of the notebook , depending on the context . User Interface Our Data Layer Kernel cell 1 ... code ... cell 2 ... code ... Technique 1 : dynamic exe- cution history in graph ( §4 ) Technique 2 : optimization for fast migration ( §5 ) Python R LLVM Figure 1 : Our transparent data layer ( in the middle ) enables robust , efficient , and platform-independent live migration . tuning and selection [ 9 , 114 ] , etc . Cloud providers offer Software- as-a-Services ( e.g. , AWS hub [ 91 ] , Azure ML studio [ 4 ] , Google Colab [ 47 ] , IBM Watson studio [ 56 ] ) with commonly used libraries ( e.g. , Pandas , PyTorch ) . A notebook workflow begins with a user starting a computing session . Then , the user can execute a cell ( i.e. , a set of statements ) , one by one , to load datasets , create variables , train models , visualize results , etc . The session can be terminated manually or automatically to save resources and costs . Limitation : No Live Replication . Unfortunately , existing note- books do not offer transparent infrastructure scaling ( independent of applications ) , which are becoming increasingly popular in the cloud for instant scalability and cost reduction ( e.g. , auto-scaling DBMS [ 85 , 112 ] , micro-service orchestration [ 25 , 68 ] ) . That is , if we copy a notebook file to a new VM ( e.g. , for larger memory ) or suspend a session to save costs , the resumed notebook loses its state ( i.e. , a set of variables ) , having only code and outputs . In other words , the user can not resume their task from where they had previously left off . This is because the notebooks directly rely on underlying kernels ( e.g. , Python/R interpreters , C++ REPL ) without an addi- tional data management layer . Accordingly , the variables residing in processes are erased as they terminate with sessions . To address this , we can potentially save those variables and restore them on a new environment . However , existing techniques such as serializing all variables [ 37–39 ] and checkpointing OS processes [ 3 , 18 , 43 , 61 ] may fail , are inefficient , and platform-dependent ( discussed shortly ) . Finally , re-running code from scratch can be time-consuming . Our Goal . We propose ElasticNotebook , a notebook system that offers live state migration via checkpointing/restoration using a reliable , efficient , and platform-independent state replication mech- anism . Reliability : It enables correct/successful replication for ( almost ) all notebooks . Efficiency : It is significantly more efficient than others . Platform-independence : It does not rely on platform- /architecture-specific features . That is , ElasticNotebook enables live notebook replication for potentially all notebook workloads by introducing a novel data management layer . For example , if a user specifies a new machine to run a currently active notebook , the system transparently replicates the notebook , including all of its variables , as if the notebook has been running on the new machine . Zhaoheng Li , Pranav Gor , Rahul Prabhu , Hui Yu , Yuzhou Mao , Yongjoo Park Table 1 : Comparison between our ElasticNotebook and other possible approaches to saving/restoring session states Approach Mechanism Serialization-based tools [ 35 , 38–40 , 104 ] System-level checkpointing [ 3 , 18 , 43 , 61 , 64 ] Notebook Versioning and Replay [ 11 , 76 , 98 ] Execution environment migration [ 1 , 115 ] Ours ( ElasticNotebook ) Serializes and stores variables during computing session ( fails with unserializable variables ) Saves memory dump of computing session ( high network cost and low portability ) Enable re-execution of versioned notebook snapshots for result verification Migrates installed modules ; useful in conjunction with ( but orthogonal to ) session state replication Optimally combines copy/recompute for reliability , efficiency , and platform independence If we can provide this capability with little to no modifications to existing systems ( e.g. , Jupyter ) , we can offer benefits to a large number of data scientists and educators who use notebooks . To achieve this , we must overcome the following technical challenges . Challenge . Creating a reliable , efficient , and platform-independent replication mechanism is challenging . First , the mechanism must offer high coverage . That is , for almost all notebooks people create , we should be able to successfully replicate them across machines . Second , the mechanism should be significantly faster than straight- forward approaches—rerunning all the cells exactly as they were run in the past , or copying , if possible , all the variables with serial- ization/deserialization . Third , the mechanism should integrate with existing notebook systems with clean separation for sustainable development and easier adoption . Our Approach . Our core idea is that by observing the evolution of session states via lightweight monitoring , we can address the three important challenges—reliability , efficiency , and platform- independence—by combining program language techniques ( i.e. , on- the-fly code analyses ) and novel algorithmic solutions ( i.e. , graph- based mathematical optimization ) . Specifically , to represent session state changes , we introduce the application history , a special form of bipartite graph expressing the dependencies among variables and cell executions . Using this graph , we take the following approach . First , we achieve reliability and platform independence by choos- ing a computational plan ( or replication plan ) that can safely re- construct platform-dependent variables ( e.g. , Python generators , incompletely defined custom classes ) based on the other platform- independent variables . That is , in the presence of variables that can not be serialized for platform-independent replication , Elas- ticNotebook uses the application history to recompute them dy- namically on a target machine . In this process , ElasticNotebook optimizes for the collective cost of recomputing all such variables while still maintaining their correctness ( §4 ) . Second , for efficiency , ElasticNotebook optimizes its replication plan to determine ( 1 ) the variables that will be copied , and ( 2 ) the variables that will be recomputed based on the copied variables , to minimize the end-to-end migration ( or restoration ) time in con- sideration of serialization costs , recomputation costs , data transfer costs , etc . For example , even if a variable can be reliably transferred across machines , the variable may still be dynamically constructed if doing so results in a lower total cost . To make this decision in a principled way , we devise a new graph-based optimization problem , which reduces to a well-established min-cut problem ( §5 ) . Implementation : While our contributions can apply to many dy- namically analyzable languages ( e.g. , Python/R , LLVM-based ones ) , we implement our prototype ( in C and Python ) for the Python user interface , which is widely used for data science , machine learning , statistical analysis , etc . Specifically , ElasticNotebook provides a data management layer to Jupyter as a hidden cell magic [ 103 ] to transparently monitor cell executions and offer efficient replication . Difference from Existing Work . Compared to existing work , we pursue a significantly different direction . For example , there are tools that make data serialization more convenient [ 40 , 104 ] ; how- ever , they fail if a session contains non-serializable variables , and are inefficient because they do not consider opportunities for dy- namic recomputation . Alternatively , system-level checkpointing [ 3 , 18 , 43 , 61 ] is platform-dependent , limited to checkpointing memory ( e.g. , not GPU ) , less efficient than ours since dynamic recompu- tation is impossible . Building on top of result reuse [ 42 , 116 ] and lineage tracing [ 54 , 83 , 93 ] , we introduce deeper ( reference-aware ) analyses ( §4.2 ) and novel optimization techniques to incorporate unique constraints such as inter-variable dependencies ( §5 ) and also empirically confirm their effectiveness ( §7.2 ) . Completely or- thogonal work includes library migration [ 1 , 115 ] and scalable data science [ 79 , 81 , 117 ] . Table 1 summarizes differences . Contributions . Our contributions are as follows : • Motivation . We discuss alternative approaches and explain the advantage of our approach . ( §2 ) • Architecture . We describe our system architecture for achiev- ing efficient and robust session replication . ( §3 ) • Data Model . We introduce a novel data model ( Application History Graph ) for expression session history , which enables efficient and accurate state replication . ( §4 ) • Optimization Problem and Solution . We formally define the optimization problem of minimizing state replication cost through balancing variable copying and recomputation . We propose an efficient and effective solution . ( §5 ) • Evaluation . We show ElasticNotebook reduces upscaling , down- scaling , and restore times by 85 % -98 % , 84 % -99 % , and 94 % -99 % , respectively . Overheads are negligible ( < 2.5 % runtime ) . ( §7 ) 2 MOTIVATION This section describes use cases ( §2.1 ) and requirements ( §2.2 ) for session replication , and our intuition for higher efficiency ( §2.3 ) . 2.1 Why is Live Migration Useful ? A seamless state replication for computational notebooks can al- low easier infrastructure scaling and frequent session suspension , without interrupting user workflow , as described below . Fast Replication for Elastic Computing . The ability to move a state across machines is useful for scaling resources [ 21 , 64 ] , allowing us to migrate a live session to the machines with the right equipment/resources ( e.g. , GPU [ 22 ] , specific architectures [ 119 ] ) . For interruption-free scaling , we can copy data D from a source ElasticNotebook : Enabling Live Migration for Computational Notebooks User Interface ... % % intercept code ... def intercept ( code ) : preprocess ( code ) # regular kernel execution out = execute ( code ) postprocess ( out , code ) Application History df_train Cell 1 3mins df Cell 2 1min Cell 3 20mins model Cell 4 10mins plot df_test Figure 2 : For every cell run , we can inject custom pre-/post- processing logic . “ % % intercept ” is hidden to users . Variable Store cost ( mins ) Reload cost ( mins ) Total cost ( mins ) df 8 2 10 df_train df_test 6.4 1.6 8 1.6 0.4 2 model 0.2 0.2 0.4 plot 0.1 0.1 0.2 machine to a target machine in a way that the original session state can be restored from D. In this process , we want to minimize the end-to-end time for creating D , transferring D to a target machine , reconstructing the state from D on the target machine . This is the first use case we empirically study ( §7.3 ) . Fast Restart for On-demand Computing . Leveraging pay-as- you-go pricing model offered by many cloud vendors [ 5 , 48 ] , sus- pending sessions ( and VMs ) when not in use is an effective way for reducing charges ( e.g. , up to 6× [ 115 ] ) . With the ability to cre- ate data D sufficient for reconstructing the current session state , we can persist D prior to either manual or automated suspen- sion [ 20 , 47 , 59 ] , to quickly resume , when needed , the session in the same state . This achieves on-demand , granular computing with fast session restart times without impacting user experience due to frequent session suspensions [ 58 , 97 ] . In this process , we want to restore the session as quickly as possible by minimizing the time it takes for downloading D and reconstructing a state from it . This is the second use case we empirically study ( §7.4 ) . 2.2 How to Enable Data Management Layer ? We discuss the pros and cons of several different approaches to enabling a data management layer . OS-level Checkpointing . To save the current session state , we can checkpoint the entire memory space associated with the underlying Python/R kernels . To make the process more efficient , existing tools like CRIU patch the Linux kernel to trace dirty pages . However , as described in §1 , this approach is platform-independent , incurs higher space cost , and is limited to storing the state of primary memory ( not GPU or other devices ) . We empirically compare our approach to CRIU to understand reliability and efficiency ( §7 ) . Object wrappers . Watchpoint object wrappers [ 41 , 44 ] are com- monly used for debugging purposes [ 83 ] and program slicing [ 54 , 93 ] : they maintain deep copies for objects in the session state , which are compared to check for changes after each frame execution ; how- ever , they are unsuitable for use during data science workflows due to the unacceptable ~20× runtime overhead in our preliminary tests . Monitoring Cell Executions ( Ours ) . In order to trace cell exe- cutions and their effects on variables , we can add a lightweight wrapper ( i.e. , our data management layer ) that functions before and after each cell execution to monitor the cell code , runtime , and variable changes . This idea is depicted conceptually in Fig 2 . Specifically , our implementation uses cell magics , a Jupyter-native mechanism that allows arbitrary modification to cell statements when the cell is executed . With this , we add pre-/post-processing steps to capture cell code and resulting session state modifications . Store Vars N/A All Method Rerun all Store all Fast-migrate model , plot Fast-restore df , model , plot 2 Rerun cells Migration Cost Restore Cost All 3+1+20+10=33 N/A 1 , 2 3+1+20+10=33 10+8+2+.4+.2=20.6 2+1.6+.4+.2+.1=4.3 3+1+.4+.2=4.6 10+1+.4+.2=11.6 3+1+.2+.1=4.3 2+1+.2+.1=3.3 Figure 3 : Example app history ( top ) and different replica- tion plan costs ( bottom ) . Combining recompute/copy allows faster migration ( Fast-migrate ) . Alternatively , the optimal plan changes if the restoration is prioritized ( Fast-restore ) . 2.3 Fast Replication with Application History This section describes our core idea for devising an efficient repli- cation strategy by leveraging the ability to monitor cell executions . Application History . An application history graph ( AHG ) is a bipar- tite graph for expressing session states changes with respect to cell runs . There are two types of nodes : variables and transformations . A transformation node connects input variables to output variables ( see an example in Fig 3 ) . AHG aims to achieve two properties : • Completeness : No false negatives . All input/output variable for each transformation must be captured . • Minimal : Minimal false positives . The number of variables that are incorrectly identified as accessed/modified , while variables are not actually accessed/modified , must be minimized . These properties are required for correct state reconstruction ( §4 ) . Core Optimization Idea . AHG allows for efficient state replica- tion with a combination of ( 1 ) recompute and ( 2 ) copy . Motivating Example . Suppose a data analyst fitting a regression model ( Fig 3 ) . The notebook contains 4 cell runs : data load ( Cell 1 ) , train-test split ( Cell 2 ) , fitting ( Cell 3 ) , and evaluation ( Cell 4 ) . After fitting , the ana- lyst decides to move the session to a new machine for GPU . Simply rerunning the entire notebook incurs 33 minutes . Alternatively , serializing/copying variables takes 20.6 minutes . However , there is a more efficient approach . By copying only model and plot and recomputing others on a new machine ( Fast- migrate ) , we can complete end-to-end migration in 4.6 minutes . Or , if we prioritize restoration time ( to reduce user-perceived restart time for on-demand computing ) , our optimized plan ( Fast-restore ) takes 3.3 minutes . This example illustrates significant optimization opportunities in session replication . Our goal is to have the ability to find the best replication plan for arbitrarily complex AHGs . 3 SYSTEM OVERVIEW This section presents ElasticNotebook at a high level by describing its components ( §3.1 ) and operations ( §3.2 ) . Data Layer ( core part of ElasticNotebook ) Table 2 : Notations and their meaning Zhaoheng Li , Pranav Gor , Rahul Prabhu , Hui Yu , Yuzhou Mao , Yongjoo Park Notebook User Interface x= '' hello '' y= '' world '' . . . Cell Execution Interceptor ( §4.2 ) Optimizer ( §5 ) ID Graphs & object hashes Optimization algorithm Application History Graph ( §4.1 ) Cost Model ( §5.2 ) Session Replicator ( §4.3 ) Writer Notebook Replayer Jupyter Kernel Namespace ( user_ns ) KEY VAL x y hello world ... Figure 4 : ElasticNotebook architecture . Its data layer acts as a gateway between the user interface and the kernel : cell executions are intercepted to observe session state changes . 3.1 ElasticNotebook Components ElasticNotebook introduces a unique data layer that acts as a gate- way between the user and the kernel ( See Fig 4 ) : it monitors every cell execution , observing code and resulting session state changes . Cell Execution Interceptor . The Cell Execution Interceptor inter- cepts cell execution requests and adds pre-/post-processing scripts before rerouting it into the underlying kernel for regular execu- tion . The added scripts perform ( 1 ) cell code analyses and the AHG updates , and ( 2 ) cell runtime recordings . Application History Graph ( AHG ) . The AHG is incrementally built by the Cell Execution Interceptor to record how variables have been accessed/modified by each cell execution ( §4 ) . The AHG is used by the Optimizer to compute replication plans ( §5 ) . Cost Model . The cost model stores profiled metrics ( i.e. , cell run- times , variable sizes , network bandwidth ) , serving as the hyperpa- rameters for the Optimizer ( §5.2 ) . Optimizer . The Optimizer uses the AHG and the Cost Model to determine the most efficient replication plan consisting of ( 1 ) vari- ables to store and ( 2 ) cells to re-run . We discuss ElasticNotebook ’ s cost model and optimization in detail in §5 . Session Replicator . The Session Replicator replicates a notebook session according to the Optimizer ’ s plan . Specifically , the Writer creates and writes a checkpoint file to storage ( e.g. , SSD , cloud storage ) , while the Notebook Replayer reads the file and restores the session , both following the replication plan . We discuss Elastic- Notebook ’ s session replication in detail in §3.2 . 3.2 ElasticNotebook Workflow This section describes ElasticNotebook ’ s operations . ElasticNote- book monitors every cell execution during a session lifecycle , then performs on-request replication of the session in two steps : check- pointing ( writing to the checkpoint file ) and restoration . Monitoring Cell Executions . Upon each cell execution by the user , ElasticNotebook performs the following steps : 1 . Accessed variables of the cell execution are identified via AST analysis ( described in §4.2 ) . 2 . The cell code is executed by the Jupyter kernel . Definition Set of Variables Set of Variable Snapshots ( VSs ) Set of Active Variable Snapshots Set of Cell Executions ( CEs ) Set of write dependencies Set of read dependencies Symbols X V V𝑎 C ( = 𝑐𝑡1 , 𝑐𝑡2 , . . . ) E𝑤 E𝑟 G : = { V ∪ C , E𝑤 ∪ E𝑟 } Application History Graph ( AHG ) 𝑟𝑒𝑞 : X → 2C Reconstruction mapping function 𝑤𝑠𝑡𝑜𝑟𝑒 : X → R+ Variable storage cost 𝑤𝑟𝑒𝑟𝑢𝑛 : C → R+ Cell Rerun cost 𝑤M : 2X → R+ Migration cost function 𝑤R : 2X → R+ Recomputation cost function Pairs of linked variables L ⊆ X × X Flow graph H : = { V𝐻 , E𝐻 } 𝑐 : E𝐻 → R+ Flow graph edge capacity function 3 . Variable changes ( i.e. , creation/deletion/modification ) are iden- tified within the global namespace ( §4.2 ) . 4 . The AHG is updated using ( 1 ) the cell code and ( 2 ) modified variables by the cell execution . 5 . The Cost Model is updated to record cell runtime . Initiating Replication . When replication is requested , Elastic- Notebook creates and writes a checkpoint file to storage , which can be restored later to exactly and efficiently reconstruct the current session . ElasticNotebook first completes the Cost Model by pro- filing variable sizes and network bandwidth to storage ; then , the Optimizer utilizes the AHG and Cost model to compute a replica- tion plan , according to which the Writer creates the checkpoint file : it consists of ( 1 ) a subset of stored variables from the session state , ( 2 ) cells to rerun , ( 3 ) the AHG , and ( 4 ) the Cost Model . Restoring a Session . When requested , ElasticNotebook restores the notebook session from the checkpoint file according to the replication plan . The Notebook Replayer reconstructs variables in the order they appeared in the original session by combining ( 1 ) cell reruns and ( 2 ) data deserialization followed by variable re- declaration ( into the kernel ) . Finally , ElasticNotebook loads the AHG and Cost Model for future replications . Accuracy Guarantee : ElasticNotebook ’ s state reconstructing is effectively the same as re-running all the cells from scratch exactly in the order they were run in the past . That is , ElasticNotebook shortens the end-to-end reconstruction time by loading saved vari- ables ( into the kernel namespace ) if doing so achieves time savings . §4.3 presents formal correctness analysis . §6.1 discusses how we address external resources , side effects , and deserialization failures . 4 APPLICATION HISTORY GRAPH This section formally defines the Application History Graph ( §4.1 ) , and describes how we achieve exact state replication ( §4.3 ) . 4.1 AHG Formal Definition The AHG is a directed acyclic graph expressing how a session state has changed with respect to cell executions . Fig 5 is an example . Definition 1 . A variable is a named entity ( e.g. , df ) referencing an object ( which can be uniquely identified by its object ID ) . ElasticNotebook : Enabling Live Migration for Computational Notebooks A variable can be primitive ( e.g. , int , string ) or complex ( e.g. , list , dataframe ) . Multiple variables may point to the same object . The set of all variables ( i.e. , X ) defined in the global namespace forms a session state . Cell executions may modify the values of variables ( or referenced objects ) without changes to their names , which we recognize in AHG using variable snapshot , as follows . Definition 2 . A variable snapshot ( VS ) is a name-timestamp pair , ( 𝑥 , 𝑡 ) , representing the variable 𝑥 created/modified at 𝑡 . We denote the set of VSes as V. Definition 3 . A cell execution ( CE ) 𝑐𝑡 represents a cell execution that finishes at timestamp 𝑡 . All cell executions are linear ; that is , for each session , there is at most one cell running at a time , and their executions are totally ordered . We denote the list of CEs by C. Each CE also stores executed cell code , which can be used for re-runs ( §3.2 ) . Definition 4 . A write dependency ( 𝑐𝑡 → ( x , 𝑡 ) ) indicates CE 𝑐𝑡 may have modified/created at time 𝑡 the object ( s ) reachable from the variable 𝑥 . We denote the set of write dependencies as E𝑤 . In Fig 5 , 𝑐𝑡3 modifies x with “ x += 1 ” ; hence , ( 𝑐𝑡3 → ( x , 𝑐𝑡3 ) ) . Definition 5 . A read dependency ( ( x , 𝑠 ) → 𝑐𝑡 ) indicates CE 𝑐𝑡 may have accessed object ( s ) reachable from x last created/modified at time 𝑠 . We denote the set of read dependencies by E𝑟 . In Fig 5 , “ gen= ( i for i in l1 ) ” in 𝐶𝑡4 accesses elements in the list l1 after its creation in 𝑐𝑡3 ; hence there is ( ( x → 𝑐𝑡3 ) , 𝑐𝑡4 ) . Note that write/read dependencies are allowed to contain false positives ; nevertheless , our replication ensures correctness ( §4.3 ) . Definition 6 . The AHG 𝐺 : = { V∪C , E𝑤 ∪E𝑟 } is a bipartite graph , where V is VSes , C is CEs ; E𝑤 and E𝑟 are write/read dependencies , respectively . It models the lineage of the notebook session . In sum , AHG formalizes variable accesses/modifications with re- spect to cell executions . at the variable level ( not object level ) , theo- retically bounding the size of AHG to scale linearly with the number of defined variables , not the number of underlying objects ( which can be very large for lists , dataframes , and so on ) . We empirically verify AHG ’ s low memory overhead in §7.5 . 4.2 Dynamic AHG Construction We describe how ElasticNotebook constructs the AHG accurately . Constructing the AHG . The AHG is incrementally built with accessed/created/modified variables by each cell execution : • A new CE 𝑐𝑡 is created ; 𝑡 is an execution completion time . • Read dependencies are created from VSes ( 𝑥1 , 𝑡𝑥1 ) , ... , ( 𝑥𝑘 , 𝑡𝑥𝑘 ) to 𝑐𝑡 , where 𝑥1 , ... , 𝑥𝑘 are variables possibly accessed by 𝑐𝑡 . • VSes ( 𝑦1 , 𝑡 ) , ... , ( 𝑦𝑘 , 𝑡 ) are created , where 𝑦1 , ... , 𝑦𝑘 are variables possibly modified and created by 𝑐𝑡 . Write dependencies are added from 𝑐𝑡 to each of the newly created VSes . Fig 5 ( right ) shows an example AHG . Identifying access/modified variables is crucial for its construction , which we describe below . ID Graph . The ID Graph aims to to detect changes at the reference level ( in addition to values ) . For instance , conventional equality checks ( e.g. , based on serialization ) will return True for “ [ a ] == Notebook Cell 1 ( 𝑐𝑡1 ) x , y = 1 Cell 2 ( 𝑐𝑡2 ) z = y if False : print ( x ) Cell 3 ( 𝑐𝑡3 ) x += 1 l1 = [ z , 2 , 3 ] Cell 4 ( 𝑐𝑡4 ) gen= ( i for i in l1 ) 2dlist = [ l1 ] Cell 5 ( 𝑐𝑡5 ) print ( gen ) ( x , 𝒕1 ) ( y , 𝑡1 ) 𝒄𝒕1 𝑐𝑡2 ( z , 𝑡2 ) 𝒄𝒕3 ( x , 𝒕3 ) ( l1 , 𝑡3 ) 𝑐𝑡4 ( gen , 𝑡4 ) ( 2dlist , 𝑡4 ) 𝑐𝑡5 ( gen , 𝑡5 ) ( x , 𝑡1 ) ( Overwritten/deleted ) Variable Snapshot 𝑐𝑡1 Cell Execution ( x , 𝑡1 ) Active Variable Snapshot Figure 5 : An example notebook and its corresponding Appli- cation History Graph . The AHG tells ElasticNotebook how to recompute variables ; for example , rerunning 𝑐𝑡1 and 𝑐𝑡3 is necessary for recomputing x ( red ) . [ b ] ” if a and b have the same value ( e.g. , a = [ 1 ] and b = [ 1 ] ) , whereas we ensure it returns True only if a and b refer to the same object , i.e. , id ( a ) ==id ( b ) , where id is the object ’ s unique ID . This is because for correct state replication , shared references ( e.g . aliases ) and inter-variable relationships must be captured precisely . Identifying Accessed Variables . ElasticNotebook identifies both directly accessed variables ( via AST [ 31 ] parsing ) and indirectly accessed variables ( with ID Graphs ) , as follows . Direct Accesses : Cell code is analyzed with AST , stepping also into user-defined functions ( potentially nested ) to check for accesses to variables not explicitly passed in as parameters ( e.g. , global x ) . Indirect Accesses : The object ( s ) reachable from a variable X may be accessed indirectly via another variable Y if X and Y reference common object ( s ) ( e.g. , when aliases exist , Fig 6a ) , which can not be identified via parsing only . To recognize indirect accesses , we check the existence of overlaps between the ID Graphs of X and Y . Our approach is conservative ; that is , it may over-identify vari- ables by including , for example , ones reachable from control flow branches that were not taken during cell executions . However , these false positives do not affect accuracy of state replication ( §4.3 ) . Identifying Modified Variables . Variable modifications are iden- tified using a combination of ( 1 ) object hashes and ( 2 ) ID Graphs . Value Changes : ElasticNotebook identifies value modifications by comparing hashes ( by xxHash [ 118 ] ) before and after each cell execution while using deep copy as a fallback . If the deep copy fails ( e.g. , unserializable or uncomparable variables ) , we consider them to be modified-on-access using results from AST and ID Graph ( §6.1 ) . This may result in false positives ; however , as previously mentioned , these false positives do not affect the accuracy . Structural Changes : The ID Graph enables detecting structural changes ( Fig 6b ) . After each cell execution , the current variables ’ ID Graphs are compared to the ones created before to identify reference swaps . In Fig 6b , while the value of 2dlist1 remains unchanged Cell 1 func = lambda x : ... obj1.foo = func obj2.foo = func Cell 2 obj2.foo ( `` str '' ) & obj1 & obj2 ID Graph & func ( a ) Detecting indirect variable accesses from aliases Cell 1 list1 = [ 1 , 2 , 3 ] 2dlist1 = [ list1 ] 2dlist2 = [ list1 ] Cell 2 list2 = [ 1 , 2 , 3 ] 2dlist1 [ 0 ] = list2 Before Cell 2 After Cell 2 ID Graph & 2dlist1 & 2dlist1 ≠ & list1 & list2 Value [ [ 1,2,3 ] ] = [ [ 1,2,3 ] ] ( b ) Detecting structural variable modifications Figure 6 : Two uses of the ID Graph during AHG construction . after execution after executing Cell 2 , the memory address of its nested list has been changed , no longer referencing list1 . 4.3 State Reconstruction with AHG This section describes how we reconstruct variable ( s ) . We focus on reconstructing the latest version of each variable , as defined in active variable snapshot ( VS ) in an AHG . Definition 7 . VS ( 𝑥 , 𝑡𝑖 ) is active if 𝑥 is in the system ( i.e. , not deleted ) , and there is no VS ( 𝑥 , 𝑡 𝑗 ) such that 𝑡𝑖 < 𝑡 𝑗 . An active VS , ( 𝑥 , 𝑡𝑖 ) , represents the current version of 𝑥 . For example , even if we checkpoint after 𝑐𝑡5 ( in Fig 5 ) , “ ( x , 𝑡3 ) ” is active since x was last modified by 𝑐𝑡3 . We denote the set of active VSes as V𝑎 . Reconstruction Algorithm . Our goal is to identify the most effi- cient computation strategy for reconstructing one or more active variables . Note that we do not reconstruct non-active variables since they are not part of the current session state . In achieving this goal , the AHG allows us to avoid unnecessary cell executions ( e.g. , because their outcomes have been overwritten ) and to learn proper execution orders . Moreover , this process can be extended to reconstruct a set of variables more efficiently than computing them one by one . while still ensuring correctness . Specifically , to recompute VS ( 𝑥 , 𝑡 ) , we traverse back to its an- cestors in the AHG ( e.g. , using the breadth-first search ) , collecting all CEs into a list 𝑟𝑒𝑞 ( 𝑥 , 𝑡 ) , until we find a ground variable for every path , where the ground variable is a variable whose value is avail- able in the system , i.e. , either another active VS or copied variable . By rerunning all the CEs in 𝑟𝑒𝑞 ( 𝑥 , 𝑡 ) in the order of their completion times , we can obtain the target VS ( 𝑥 , 𝑡 ) . To extend this algorithm to multiple VSes , say ( 𝑥1 , 𝑡𝑥1 ) , ( 𝑥2 , 𝑡𝑥2 ) , and ( 𝑥3 , 𝑡𝑥3 ) , we obtain 𝑟𝑒𝑞 for each VS and union them into a merged set ( that is , identical CEs collapse into one ) . By rerunning all the CEs in the merged set , we obtain all target VSes . Fig 5 shows an example . To recompute ( 𝑥 , 𝑡3 ) , we rerun 𝑐𝑡3 which requires the previous version ( x , 𝑡1 ) as input , which in turn requires 𝑐𝑡1 to be rerun . Notably , it is not necessary to rerun 𝑐𝑡2 as its output z is available in the namespace . Finally , §6.1 discusses how this approach can recover even if some ground variables are unexpectedly unobtainable . Zhaoheng Li , Pranav Gor , Rahul Prabhu , Hui Yu , Yuzhou Mao , Yongjoo Park Notebook Cell 3 ( 𝑐𝑡3 ) l1 = [ z , 2 , 3 ] Cell 4 ( 𝑐𝑡4 ) 2dlist = [ l1 ] Replication Plan l1 Migrate 2dlist Migrate Recompute Recompute Recompute Migrate & l1== & 2dlist1 [ 0 ] True True False Figure 7 : Two variables sharing references ( in Fig 5 ) . They must be migrated/recomputed together for the correct repli- cation , serving as constraints to our opt problem ( see §5.3 ) . Why Only Use Active VSes ? Theoretically , it is possible to use non-active variables as ground variables . That is , by preserving deleted/overwritten variables ( e.g. , in a cache ) , we may be able to speed up the recomputation of active variables [ 42 , 116 ] . However , we don ’ t consider this approach as many data science workloads are memory-hungry with large training data and model sizes . Still , there might be cases where we can speed up recomputation by storing small overwritten variables , which we leave as future work . Correctness of Reconstruction . As stated in §2.3 , the AHG is allowed to have false positives , meaning it may indicate a cell ac- cessed/modified variables that were not actually accessed/modified . While the false positives have a performance impact , they do not affect the correctness of identification . Theorem 4.1 . Given the approximate AHG G of ElasticNotebook with false positives , and the true AHG G∗ , there is 𝑟𝑒𝑞∗ ( 𝑥 , 𝑡 ∗ ) ⊆ 𝑟𝑒𝑞 ( 𝑥 , 𝑡 ) for any variable 𝑥 ∈ X , where ( 𝑥 , 𝑡 ) and ( 𝑥 , 𝑡 ∗ ) , 𝑟𝑒𝑞 and 𝑟𝑒𝑞∗ are the active VSs of 𝑥 and reconstruction mapping functions defined on G and G∗ respectively . That is , for any arbitrary variable 𝑥 , while 𝑟𝑒𝑞 ( 𝑥 , 𝑡 ) may contain cell executions unnecessary for recomputing 𝑥 , it will never miss any necessary cell executions ( i.e. , those in 𝑟𝑒𝑞 ( 𝑥 , 𝑡 ∗ ) ) . The proof is presented in Appendix A.2 . 5 CORRECT & EFFICIENT REPLICATION This section covers how ElasticNotebook computes an efficient and correct plan for state replication with the AHG and profiled metrics . We describe correctness requirements in §5.1 , the cost model in §5.2 , the optimization problem in §5.3 , and our solution in §5.4 . 5.1 Correctness Requirements ElasticNotebook aims to correctly replicate session states . which we define the notion of in this section : Definition 8 . A replication of state X is value-equivalent if ∀𝑥 ∈ X , 𝑥 =𝑛𝑒𝑤 ( 𝑥 ) , where 𝑛𝑒𝑤 ( 𝑥 ) is the value of 𝑥 post-replication . A value-equivalent replication preserves the value of each indi- vidual variable and is guaranteed by the correct identification of 𝑟𝑒𝑞 ( 𝑥 , 𝑡 ) for each variable 𝑥 ( §4.3 ) . However , it is additionally im- portant that shared references are preserved , as defined below . Definition 9 . A value-equivalent replication of a session state X is additionally isomorphic if ∀𝑎 , 𝑏 , 𝑖𝑑 ( 𝑎 ) = 𝑖𝑑 ( 𝑏 ) → 𝑖𝑑_𝑛𝑒𝑤 ( 𝑎 ) = 𝑖𝑑_𝑛𝑒𝑤 ( 𝑏 ) , where 𝑎 , 𝑏 are arbitrary references ( e.g. , x [ 0 ] [ 1 ] , y.foo ) , and 𝑖𝑑 ( 𝑎 ) , 𝑖𝑑_𝑛𝑒𝑤 ( 𝑎 ) are the unique IDs ( i.e. , memory addresses ) of the objects pointed to by 𝑎 before and after replication . ElasticNotebook : Enabling Live Migration for Computational Notebooks migration cost = capacity Source y z x l1 Active VSes Cell Executions capacity=∞ ca p acity = reru n cost Sink 𝑐𝑡1 𝑐𝑡2 𝑐𝑡3 𝑐𝑡4 𝑐𝑡5 capacity=∞ 2dlist1 gen Figure 8 : Running min-cut on the flow graph constructed from the AHG in Fig 5 . The partition ( red ) defined by the minimum cut ( dashed edges ) determines the replication plan . ElasticNotebook defines replication as ’ correct ’ only if it is isomor- phic , requiring all shared references to be preserved : two references pointing to the same object pre-replication will still do so post- replication . That is , inter-object relations are identical ( analogous to graph isomorphism ) . We describe how ElasticNotebook ensures isomorphic replication via its linked variable constraint in §5.3 . 5.2 Cost Model Our model captures the costs associated with ( 1 ) serializing vari- ables , ( 2 ) writing byte data into storage ( e.g. , local SSD , cloud stor- age ) and ( 3 ) rerunning cell executions . These costs are computed using the AHG and profiled system metrics . Variable Migration Cost . Migrating a variable ( from one session to another ) includes serializing it to the checkpoint file , then loading it into a new session . Given a subset of variables to migrate S ⊆ X , the migration cost 𝑤𝑀 can be expressed as follows : ∑︁ 𝛼 × 𝑤𝑠𝑡𝑜𝑟𝑒 ( 𝑥 ) + 𝑤𝑙𝑜𝑎𝑑 ( 𝑥 ) 𝑤𝑀 ( S ) = ( 1 ) 𝑥 ∈ S Where 𝑤𝑠𝑡𝑜𝑟𝑒 ( 𝑥 ) and 𝑤𝑙𝑜𝑎𝑑 ( 𝑥 ) are the time costs for serializing the value of 𝑥 at checkpointing time into a file and unpacking into the new session , respectively . These times are estimated using the size of 𝑥 and storage latency/bandwidth from ElasticNotebook ’ s Profiler ( §3.1 ) . The time costs for unserializable variables are set to infinity . 𝛼 is a coefficient for adjusting the time cost of storage ; for example , if ElasticNotebook is to be invoked upon auto-suspension , 𝛼 can be set to a low value to discount the user-perceived time of storing variables prior to completely suspending a session ( as the user is likely away ) . Variable Recomputation Cost . The Interceptor records cell run- times during a session lifecycle ( §3.1 ) . Combined with the recon- struction mapping 𝑟𝑒𝑞 ( ) for the AHG ( §4.3 ) , the cost 𝑤𝑅 for recom- puting a subset of variables S ⊆ X can be defined as follows : 𝑟𝑒𝑞 ( 𝑥 , 𝑡 ) 𝑤𝑟𝑒𝑟𝑢𝑛 ( 𝑐 ) , where 𝑟𝑒𝑞 ( S ) = 𝑤𝑅 ( S ) = ∑︁ ( cid:216 ) ( 2 ) 𝑐 ∈𝑟𝑒𝑞 ( S ) 𝑥 ∈ S where ( 𝑥 , 𝑡 ) is the active VS of 𝑥 and 𝑤𝑟𝑒𝑟𝑢𝑛 ( 𝑐 ) : C → R+ is the estimated time to rerun the CE 𝑐 in the new session . Replication Plan Cost . Using migration and recomputation costs ( i.e. , Eqs . ( 1 ) and ( 2 ) ) , the total cost 𝑤—with variables to migrate S and variables to recompute X − S—is expressed as : 𝑤 ( S ) = 𝑤𝑀 ( S ) + 𝑤𝑅 ( X − S ) ( 3 ) 5.3 Optimization Problem for State Replication The goal is to find the variables to migrate S ⊆ X that minimizes the cost Eq . ( 3 ) . To ensure isomoprhic replication in consideration of variable inter-dependencies , additional constraints are added . Constraint for Linked Variables . Two variables containing refer- ences to the same object ( which we refer to as linked variables , e.g. , l1 and 2dlist1 in Fig 7 ) must be either both migrated or recom- puted , as migrating one and recomputing the other may result in their contained shared reference/alias being broken , as illustrated in Fig 7 . Let the set of linked variable pairs be denoted as L , then the constraint can be formally expressed as follows : ( 𝑥1 ∈ S ∧ 𝑥2 ∈ S ) ∨ ( 𝑥1 ∉ S ∧ 𝑥2 ∉ S ) ∀ ( 𝑥1 , 𝑥2 ) ∈ L Problem definition . Using the cost model in Eq . ( 3 ) and the con- straint in Eq . ( 4 ) , we formally define the state replication problem : ( 4 ) Problem 1 . Optimal State Replication Input : 1 . AHG G = { V ∪ C , E } 2 . Migration cost function 𝑤𝑀 : 2X → R+ 3 . Recompute cost function 𝑤𝑅 : 2X → R+ 4 . Linked variables L ⊆ X × X A replication plan of subset of variables S ⊆ X for which we migrate ( and another subset X − S which we recompute ) Output : Objective : Minimize replication cost 𝑤𝑀 ( S ) + 𝑤𝑅 ( X − S ) Constraint : Linked variables are either both migrated or recom- puted : ( 𝑥1 , 𝑥2 ∈ S ) ∨ ( 𝑥1 , 𝑥2 ∉ S ) ∀ ( 𝑥1 , 𝑥2 ) ∈ L The next section ( §5.4 ) presents our solution to Prob 1 . 5.4 Solving State Replication Opt . Problem We solve Prob 1 by reducing it to a min-cut problem , with a 𝑠𝑟𝑐-𝑠𝑖𝑛𝑘 flow graph constructed from the AHG such that each 𝑠𝑟𝑐-𝑠𝑖𝑛𝑘 cut ( a subset of edges , which , when removed from the flow graph , disconnects source 𝑠 and sink 𝑡 ) corresponds to a replication plan S , while the cost of the cut is equal to the replication cost 𝑤𝑀 ( S ) + 𝑤𝑅 ( X − S ) . Therefore , finding the minimum cost 𝑠𝑟𝑐-𝑠𝑖𝑛𝑘 cut is equivalent to finding the optimal replication plan . Flow Graph Construction . A flow graph 𝐻 : = { V𝐻 , E𝐻 } and its edge capacity 𝜙 : E𝐻 → R+ are defined as follows : • V𝐻 = V𝑎 ∪ C ∪ { 𝑠𝑟𝑐 , 𝑠𝑖𝑛𝑘 } : V𝑎 is active VSes , C is cell execu- tions , and 𝑠𝑟𝑐 and 𝑠𝑖𝑛𝑘 are dummy source and sink nodes . • ∀𝑥 ∈ V𝑎 , ( 𝑠𝑟𝑐 , ( 𝑥 , 𝑡 ) ) ∈ E𝐻 and 𝜙 ( 𝑠𝑟𝑐 , ( 𝑥 , 𝑡 ) ) = 𝑤𝑀 ( 𝑥 ) : We add an edge from the source to each active VS with a capacity equal to the migration cost of the variable . • ∀𝑐 ∈ C , ( 𝑐 , 𝑠𝑖𝑛𝑘 ) ∈ E𝐻 and 𝜙 ( 𝑐 , 𝑠𝑖𝑛𝑘 ) = 𝑤𝑟𝑒𝑟𝑢𝑛 ( 𝑐 ) : We add an edge with capacity from each CE to the sink with a capacity equal to the rerun cost of the CE . • ∀𝑐 ∈ C , 𝑐 ∈ 𝑟𝑒𝑞 ( 𝑥 , 𝑡 ) → ( ( 𝑥 , 𝑡 ) , 𝑐 ) ∈ E𝐻 and 𝜙 ( ( 𝑥 , 𝑡 ) , 𝑐 ) = ∞ and ( 𝑥 , 𝑡 ) ∈ V𝑎 : We add an edge with infinite capacity from an active VS ( 𝑥 , 𝑡 ) to a CE 𝑐 if ( 𝑥 , 𝑡 ) must be recomputed . • ∀ ( 𝑥1 , 𝑥2 ) ∈ L , ( ( 𝑥1 , 𝑡1 ) ↔ ( 𝑥2 , 𝑡2 ) ) ∈ E𝐻 and 𝜙 ( ( 𝑥1 , 𝑡1 ) ↔ ( 𝑥2 , 𝑡2 ) ) = ∞ : We add a bi-directional edge with an infinite capacity between each pair of active VSes corresponding to linked variables 𝑥1 and 𝑥2 , e.g. , l1 and 2dlist1 . The flow graph H for the AHG in Fig 5 is depicted in Fig 8 . Solution . We can now solve Prob 1 by running a 𝑠𝑟𝑐—𝑠𝑖𝑛𝑘 min-cut solving algorithm ( i.e. , Ford-Fulkerson [ 30 ] ) on 𝐻 . The set of edges that form the 𝑠𝑟𝑐—𝑠𝑖𝑛𝑘 min-cut ( dashed edges ) , when removed , disconnects 𝑠𝑟𝑐 from 𝑠𝑖𝑛𝑘 ; therefore , it defines a partition ( in red ) of the nodes into nodes reachable from 𝑠𝑟𝑐 , V𝐻𝑠𝑟𝑐 and nodes un- . The replication plan can be obtained reachable from 𝑠𝑟𝑐 , V𝐻𝑠𝑖𝑛𝑘 from the partition : • S = { 𝑥 | ( 𝑥 , 𝑡 ) ∈ V𝐻𝑠𝑖𝑛𝑘 ∩ V𝑎 } are the active variable snapshots ( and thus variables ) that we want to migrate ; in the example , these variables are l1 , 2dlist1 , and gen. • V𝐻𝑠𝑟𝑐 ∩ C are the CEs which we will rerun post-migration to recompute X − S. In the example , these CEs are 𝑡1 , 𝑡2 , and 𝑡3 ; when rerun , they recompute y , z , and x.2 By construction of H , the sum of migration and recomputation costs of this configuration 𝑤𝑀 ( { 𝑥 | ( 𝑥 , 𝑡 ) ∈ V𝐻𝑠𝑖𝑛𝑘 ) + 𝑤𝑅 ( C𝑎 − ( V𝐻𝑠𝑟𝑐 ∩ C ) ) is precisely the cost of the found 𝑠𝑟𝑐—𝑠𝑖𝑛𝑘 min-cut . 6 IMPLEMENTATION AND DISCUSSION This section describes ElasticNotebook ’ s implementation details ( §6.1 ) and design considerations ( §6.2 ) . 6.1 Implementation Integrating with Jupyter . For seamless integration , ElasticNote- book ’ s data layer is implemented using a magic extension [ 103 ] , which is loaded into the kernel upon session initialization . The cell magic is automatically added to each cell ( §2.2 ) to transparently intercept user cell executions , perform code analyses , create ID Graphs and object hashes , and so on . Serialization Protocol . The Pickle protocol ( e.g. , __reduce__ ) is employed for ( 1 ) object serialization and ( 2 ) definition of reachable objects , i.e. , an object y is reachable from a variable x if pickle ( x ) includes y . As Pickle is the de-facto standard ( in Python ) observed by almost all data science libraries ( e.g. , NumPy , PyTorch [ 29 ] ) , ElasticNotebook can be used for almost all use cases . Handling Undeserializable variables . Certain variables can be serialized but contain errors in its deserialization instructions ( which we refer to as undeserializable variables ) , and are typically caused by oversights in incompletely implemented libraries [ 15 , 69 ] . While undetectable via serializability checks prior to checkpointing , Elas- ticNotebook handles them via fallback recomputation : if Elastic- Notebook encounters an error while deserializing a stored variable during session restoration , it will trace the AHG to determine and rerun ( only ) necessary cell executions to recompute said variable , which is still faster than recomputing the session from scratch . 6.2 Design Considerations Definition of Session State . In ElasticNotebook , the session state is formally defined as the contents of the user namespace dictionary ( user_ns ) , which contains key-value pairs of variable names to their 2Rerunning 𝑡3 also recomputes l1 ; however , it will be overwritten with the stored l1 in the checkpoint file following the procedure in §3.2 . This is to preserve the link between l1 and 2dlist1 . Zhaoheng Li , Pranav Gor , Rahul Prabhu , Hui Yu , Yuzhou Mao , Yongjoo Park values ( i.e. , reachable objects ) . The session state does not include local/module/hidden variables , which we do not aim to capture . Unobservable State / External Functions . Although the Pickle protocol is followed by almost all libraries , there could be lesser- known ones with incorrect serialization ( e.g. , ignoring data defined in a C stack ) . To address this , ElasticNotebook can be easily ex- tended to allow users to annotate cells/variables to inform our system that they must be recomputed for proper reconstruction . Mathematically , this has the same effect as setting their recomputa- tion costs to infinity in Eq . ( 2 ) . Cell Executions with Side Effects . Certain cell executions may cause external changes outside a notebook session ( e.g. , filesystem ) and may not be desirable to rerun ( e.g. , uploading items to a reposi- tory ) . Our prototype currently does not identify these side effects as our focus is read-oriented data science and analytics workloads . Nevertheless , our system can be extended at least in two ways to prevent them . ( 1 : Annotation ) We can allow users to add manual annotations to the cells that may cause side effects ; then , our system will never re-run them during replications3 ( 2 : Sandbook ) We can block external changes by replicating a notebook into a sandbox with altered file system access ( e.g. , chroot [ 73 ] ) and blocked out- going network ( e.g. , ufw [ 27 ] ) . The sandbox can then be associated with regular file/network accesses upon successful restoration . Non-deterministic Operations . The replication has the same ef- fect as rerunning the cells in the exact same order as they occurred in the past ; thus , under the existence of nondeterministic operations ( e.g. , randint ( ) ) , the reconstructed variables may have different values than the original ones . Users can avoid this by using annota- tions to inform ElasticNotebook to always copy them . Library Version Compatibility . Accurate replication is ensured when external resources ( e.g. , installed modules , database tables ) remain the same before and after the replication . While there are existing tools ( i.e. , pip freeze [ 84 ] ) for reproducing computational environments on existing data science platforms ( i.e. , Jupyter Note- book , Colab ) [ 1 , 115 ] , this work does not incorporate such tools . 7 EXPERIMENTAL EVALUATION In this section , we empirically study the effectiveness of Elastic- Notebook ’ s session replication . We make the following claims : 1 . Robust Replication : Unlike existing mechanisms , ElasticNote- book is capable of replicating almost all notebooks . ( §7.2 ) 2 . Faster Migration : ElasticNotebook reduces session migration time to upscaled/downscaled machines by 85 % –98 % /84 % -99 % compared to rerunning all cells and is up to 2.07×/2.00× faster than the next best alternative , respectively . ( §7.3 ) 3 . Faster Resumption : ElasticNotebook reduces session restora- tion time by 94 % –99 % compared to rerunning all cells and is up to 3.92× faster than the next best alternative . ( §7.4 ) 4 . Low Runtime Overhead : ElasticNotebook incurs negligible overhead—amortized runtime and memory overhead of < 2.5 % and < 10 % , respectively . ( §7.5 ) 3Replication may be unfeasible due to annotations , e.g. , an unserializable variable requiring an cell execution annotated ’ never-rerun ’ to recompute . ElasticNotebook can detect these cases as they have infinite min-cut cost ( §5.4 ) , upon which the user can be warned to delete the problematic variable to proceed with replicating the remaining ( majority of ) variables in the state . ElasticNotebook : Enabling Live Migration for Computational Notebooks Table 3 : Summary of datasets for evaluation . Dataset Notebooks Runtime ( s ) Input data ( MB ) Cell count Kaggle [ 57 ] 35 JWST [ 60 ] 5 5 Tutorial [ 107 ] HW [ 13 , 46 , 66 ] 15 107–12,560 2–109 1–139 16–439 178-31831 25–323 10–96 9–1203 15–103 21–44 10–48 11–160 5 . Low Storage Overhead : ElasticNotebook ’ s checkpoint sizes are up to 66 % smaller compared to existing tools . ( §7.6 ) 6 . Adaptability to System Environments : ElasticNotebook achieves consistent savings across various environments with different network speeds and available compute resources . ( §7.7 ) 7 . Scalability for Complex Notebooks : ElasticNotebook ’ s run- time and memory overheads remain negligible ( < 150ms , < 4MB ) even for complex notebooks with 2000 cells . ( §7.8 ) 7.1 Experiment Setup Datasets . We select a total of 60 notebooks from 4 datasets : • Kaggle [ 57 ] : We select 35 popular notebooks on the topic of EDA ( exploratory data analysis ) + machine learning from Kaggle created by Grandmaster/Master-level users . • JWST [ 60 ] : We select 5 notebooks on the topic of data pipelining from the example notebooks provided on investigating data from the James Webb Space Telescope ( JWST ) . • Tutorial [ 107 ] : We select 5 notebooks from the Cornell Vir- tual Workshop Tutorial . These notebooks are lightweight and introduce tools ( i.e. , clustering , graph analysis ) to the user . • Homework [ 13 , 46 , 66 ] : 15 in-progress notebooks are chosen from data science exercises . They contain out-of-order cell exe- cutions , runtime errors , and mistakes ( e.g. , df_backup=df4 ) . Table 3 reports our selected notebooks ’ dataset sizes and runtimes . Methods . We evaluate ElasticNotebook against existing tools ca- pable of performing session replication : • RerunAll [ 102 ] : Save ( only ) cell code and outputs as an ipynb file . All cells are rerun to restore the session state . • CRIU [ 18 ] : Performs a system-level memory dump of the pro- cess hosting the notebook session . The session state is restored by loading the memory dump and reviving the process . • % Store [ 104 ] : A checkpointing tool that serializes variables one by one into storage . We use a modified version using Dill [ 39 ] instead of Pickle [ 38 ] for robustness.5 • DumpSession [ 40 ] : Unlike % Store , DumpSession packs the en- tire session state into one single file . Ablation Study . We additionally compare against the following ablated implementations of ElasticNotebook : • ElasticNotebook + Helix [ 116 ] : We replace our min-cut solution with Helix , which does not consider linked variables ( §5.3 ) . • EN ( No ID graph ) : This method omits ID Graphs , relying only on AST analysis and object hashes for detecting variable accesses and modifications , respectively . 4This creates a shallow copy of df , which does not serve the purpose of backup . 5The original implementation of % store uses Python Pickle [ 38 ] , and fails on too many notebooks to give meaningful results . RerunAll % Store EN ( No ID graph ) 100 % 80 % 60 % 40 % 20 % 0 % ) % ( e t a r s s e c c u S CRIU ( same architecture ) DumpSession ElasticNotebook ( Ours ) CRIU ( cross-architecture ) ElasticNotebook + Helix e r u l i a f % 0 0 1 Figure 9 : Ratio of correct replications . ElasticNotebook achieves 100 % correctness , on par with full rerun ( RerunAll ) . Table 4 : Existing work fails for these cases . Ours works . Notebook ( s ) NFL [ 88 ] All 5 JWST notebooks [ 60 ] Arxiv [ 70 ] Plant [ 94 ] Type hashlib [ 33 ] mmap [ 36 ] Description and purpose Dropdown list in plot Helps avoid reading large file into memory generator [ 32 ] Speedup iterable comprehension via lazy element generation We consider these methods regarding replication correctness ( §7.2 ) to gauge the impact of ignoring ( 1 ) the linked constraint and ( 2 ) implicit accesses and structural modifications , respectively . Environment . We use an Azure Standard D32as v5 VM instance with 32 vCPUs and 128 GB RAM . For the migration experiment ( §7.3 ) , we migrate sessions from D32as to D64as/D16as with 64/16 vCPUs and 256/64 GB RAM for upscaling/downscaling , respectively . Input data and checkpoints are read/stored from/to an Azure stor- age with block blobs configuration ( NFS ) . Its network bandwidth is 274 MB/s with a read latency of 175 𝜇𝑠 . Time measurement . We measure ( 1 ) migration time as the time from starting the checkpointing process to having the state restored ( i.e. , all variables declared into the namespace ) in the destination session and ( 2 ) restoration time as the time to restore the state from a checkpoint file . We clear our cache between ( 1 ) checkpointing and restoring a notebook and ( 2 ) between subsequent runs . Reproducibility . Our implementation of ElasticNotebook , experi- ment notebooks , and scripts can be found in our Github repository.6 7.2 Robust Session Replication This section compares the robustness of ElasticNotebook ’ s session replication to existing methods . We count the number of isomorphic ( thus , correct ) replications ( §5.1 ) achieved with each method on the 60 notebooks and report the results in Fig 9 . ElasticNotebook correctly replicates all sessions , on par with full rerun from checkpoint file ( which almost always works ) . No- tably , it replicates 19 , 25 , and 2 notebooks containing unserializable variables , variable aliases , and undeserializable variables ( §6.1 ) , re- spectively . DumpSession and % Store fail on 19/60 notebooks con- taining unserializable variables , many of which are used to enhance data science workflow efficiency ( examples in Table 4 ) ; ElasticNote- book successfully replicates them as it can bypass the serialization of these variables through recomputation . % Store additionally fails on 21/60 notebooks ( total 40/60 ) without unserializable variables but contain variable aliases ( i.e. , Timeseries [ 89 ] notebook , Cell 15 , 6https : RerunAll CRIU 100 % 100 % 100 % 100 % % Store 49 % DumpSession ElasticNotebook ( Ours ) 100 % 100 % 100 % 100 % 100 % 51 % 100 % 100 % 84 % 100 % 60 % 86 % Zhaoheng Li , Pranav Gor , Rahul Prabhu , Hui Yu , Yuzhou Mao , Yongjoo Park Sklearn [ 109 ] NLP [ 2 ] StoreSales [ 7 ] TPS-Mar [ 113 ] Glove [ 92 ] Trading [ 95 ] Timeseries [ 80 ] Stacking [ 24 ] Agriculture [ 109 ] LANL [ 87 ] HW-LM [ 45 ] HW-ex3 [ 67 ] l l A n u r e R f o % e m T i 40 % 30 % 20 % 10 % 0 % Figure 10 : ElasticNotebook ’ s session upscaling time ( D32as v5 VM→D64as v5 VM ) vs. existing tools . Times normalized w.r.t . RerunAll . ElasticNotebook speeds up migration by 85 % -98 % and is up to 2.07× faster than the next best alternative . l l A n u r e R f o % e m T i 20 % 15 % 10 % 5 % 0 % RerunAll 100 % 100 % CRIU 100 % % Store DumpSession ElasticNotebook ( Ours ) 100 % 100 % 100 % 100 % 100 % 100 % 31 % 100 % 29 % 100 % 24 % 100 % 25 % Sklearn [ 109 ] NLP [ 2 ] StoreSales [ 7 ] TPS-Mar [ 113 ] Glove [ 92 ] Trading [ 95 ] Timeseries [ 80 ] Stacking [ 24 ] Agriculture [ 109 ] LANL [ 87 ] HW-LM [ 45 ] HW-ex3 [ 67 ] Figure 11 : ElasticNotebook ’ s session restoration time vs. existing tools . Times normalized w.r.t . RerunAll . ElasticNotebook speeds up session restore by 94 % -99 % , and is up to 3.92× faster compared to the next best alternative . Table 5 : Runtime and memory overhead of ElasticNotebook ’ s workflow monitoring on selected notebooks . Notebook runtime ( s ) Total cell monitoring time ( s ) Runtime overhead ( % ) User Namespace memory usage ( MB ) 1021.45 ElasticNotebook memory usage ( MB ) 19.16 1.88 Memory overhead ( % ) Sklearn 58.48 1.26 2.14 NLP 1016.77 4.30 0.42 325.82 4.73 1.45 StoreSales TPS-Mar Glove 696.64 178.42 283.06 6.43 1.34 0.81 0.92 0.78 0.28 347.16 1558.52 6732.17 33.25 1.69 0.14 9.58 0.11 0.002 Trading 687.54 0.46 0.07 1363.32 4.09 0.30 Timeser . 204.10 0.60 0.29 130.27 0.28 0.21 Stacking Agricult . LANL 269.40 788.54 3.08 2.13 1.14 0.27 5026.48 20211.51 0.06 0.33 0.001 0.002 1437.87 0.19 0.01 7641.19 0.14 0.001 HW-LM HW-ex3 22.54 0.50 2.21 31.28 0.99 3.16 27.29 0.09 0.32 19.06 0.47 2.45 linked components of a Matplotlib [ 105 ] plot—f , fig , ax ) ; it serial- izes variables into individual files , which breaks object references and isomorphism . ElasticNotebook ’ s linked variables constraint ( §5.3 ) ensures that it does not do so . ElasticNotebook + Helix fails to correctly replicate 5/60 notebooks containing variable aliases due to its lacking of the linked variable constraint . EN ( No ID graph ) fails to correctly replicate 11/60 sessions due to it missing indirect accesses and structural modifications causing incorrect construc- tion of the AHG , which in turn leads it to recompute some variables value-incorrectly . CRIU fails on one notebook [ 90 ] which contains an invisible file ; however , unlike ElasticNotebook ’ s failures , this failure is currently a fundamental limitation in CRIU [ 17 ] . Robust Migration across System Architectures . We additionally performed session replication from our D32as VM ( x64 architecture ) to a D32pds V5 VM instance ( arm64 architecture ) . The CRIU images can not be replicated across machines with different architectures . In contrast , ElasticNotebook does not have such a limitation . 7.3 Faster Session Migration This section compares the efficiency of ElasticNotebook ’ s session migration to existing methods . We choose 10 notebooks with no unserializable variables ( otherwise , existing methods fail ) to com- pare the end-to-end session migration time achieved by different methods . We report upscaling and downscaling results in Fig 10 and Fig 16 , respectively . The design goal of ElasticNotebook is to reduce session replica- tion time through balancing variable storage and recomputation , which is successfully reflected as follows . ElasticNotebook is able to reduce session migration time to the upscaled/downscaled VMs by 85 % –98 % /84 % -99 % compared RerunAll . Compared to DumpSes- sion , % Store , and CRIU , which store all variables in the checkpoint file , ElasticNotebook upscales/downscales up to 2.07×/2.00× faster than the best of the three . DumpSession , while being the next best alternative for upscaling/downscaling on 8/9 notebooks , falls short in robustness as demonstrated in §7.2 . % Store ’ s individual reading and writing of each variable results in high overhead from multiple calls to the NFS for each migration . CRIU is the slowest non-rerun method for upscaling/downscaling on 6/7 notebooks , due to the size of its memory dump ( higher I/O during migration ) being up to 10× larger compared to checkpoint files from native tools ( §7.6 ) . 7.4 Faster Session Restoration In this section , we compare the efficiency of ElasticNotebook ’ s session restoration to existing methods . We generate checkpoint files using each method , then compare the time taken to restore the session from the checkpoint files on the 10 notebooks from §7.3 . For ElasticNotebook , we set the coefficient 𝛼 to 0.05 ( §5.2 ) to emphasize session restoration time heavily . We report the results in Fig 10 . ElasticNotebook ’ s restoration time is 94 % –99 % faster compared to full rerun . Compared to the baselines , ElasticNotebook is 3.92× faster than the next best alter- native . These fast restoration can be attributed to ElasticNotebook capable of adapting to the new optimization objective , unlike the baselines : for example , on the Sklearn [ 109 ] notebook , instead of re- running cell 3 ( df = pd.read_csv ( ... ) ) to re-read the dataframe df into the session as in the migration-centric plan , the restoration- centric plan opts to store df instead . The reasoning is that despite the sum of serialization and deserialization times of df being greater than the re-reading time with pd.read_csv ( 6.19s + 1.17s > 5.5s ) , the deserialization time by itself is less than the re-reading time ( 1.17s < 5.5s ) ; hence , storing df is the optimal choice . ElasticNotebook : Enabling Live Migration for Computational Notebooks RerunAll DumpSession 1048 % CRIU ElasticNotebook ( Ours ) 283 % 388 % 467 % % Store 1137 % 200 % 150 % 100 % 50 % S D f o % e z i S 0.5 % 0.4 % 5.6 % 2.6 % 0.1 % 0.5 % NLP [ 2 ] TPS [ 113 ] Trading [ 95 ] Timeseries [ 80 ] Agriculture [ 109 ] HW-LM [ 45 ] Figure 12 : ElasticNotebook ’ s checkpoint file size vs. exist- ing tools . Times normalized w.r.t . output from DumpSession . ElasticNotebook ’ s checkpoint file size is up to 67 % smaller compared to those from existing tools ( excluding RerunAll ) . 7.5 Low Runtime Overhead This section investigates the overhead of ElasticNotebook ’ s note- book workflow monitoring . We measure ElasticNotebook ’ s total time spent in pre/post-processing steps before/after each cell execu- tion for updating the AHG and cell runtimes ( Total cell monitoring time ) , and total storage space taken to store the AHG , ID Graphs , and hashes at checkpoint time ( ElasticNotebook memory usage ) . We report the results in Table 5 . ElasticNotebook ’ s cell monitor- ing incurs a maximum and median runtime overhead of ( only ) 2.21 % and 0.6 % ; thus , ElasticNotebook can be seamlessly integrated into existing workflow . ElasticNotebook is similarly memory-efficient as its stored items ( AHG , ID Graphs , and hashes ) are all metadata largely independent of the size of items in the session : the median memory overhead is 0.25 % , with the worst case being 9.58 % . Fine-grained Analysis . To study the per-cell time and memory overheads during experimental notebook usage , we examined three notebooks from Homework category to confirm the maximum time and memory overheads were 92ms and 4.9MB , respectively . We report details in Appendix A.1 . 7.6 Lower Storage Overhead This section measures the storage cost of ElasticNotebook ’ s check- point files : we compare the migration-centric checkpoint file sizes from ElasticNotebook and those from other baseline methods . We report select results in Fig 12 . ElasticNotebook ’ s AHG al- lows it to choose between storing and recomputing each variable , reflected in ElasticNotebook ’ s checkpoint files being up to 67 % smaller compared to DumpSession ’ s . For example , on the Agri- culture [ 89 ] notebook , ElasticNotebook recomputes the train-test splits of the input dataframes X and Y ( Cell 5 , x_train , x_test , ... = train_test_split ( X , Y ) ) instead of storing them in the check- point file : this saves considerable storage space ( 2.5GB ) in addition to speeding up migration . Conversely , CRIU ’ s checkpoint file sizes can be 10× larger than ElasticNotebook ’ s as it additionally dumps memory occupied by the Python process itself and imported mod- ules , no matter necessary or not , into the checkpoint file . Output sizes from RerunAll ( i.e. , notebook metadata size consisting of cell code and outputs ) are provided for comparison . While metadata are significantly smaller than checkpoint files , the storage benefit is offset by significantly slower session recovery times ( §7.4 ) . 7.7 Performance Gains Across Environments This section demonstrates ElasticNotebook ’ s operation in environ- ments with varying specifications . We perform a parameter sweep ElasticNotebook Migrate Time DumpSession ElasticNotebook Recompute Time RerunAll ) s ( e m T i ) s ( e m T i 1,250 1,000 750 500 250 0 2,000 1,500 1,000 500 0 1600 800 400 200 100 50 Network bandwidth ( Mbps ) ( a ) AI4CODE [ 53 ] 1600 800 400 200 100 50 Network bandwidth ( Mbps ) ) s ( e m T i 2,500 2,000 1,500 1,000 500 0 ) s ( e m T i 500 400 300 200 100 0 1600 800 400 200 100 50 Network bandwidth ( Mbps ) ( b ) Stacking [ 24 ] 1600 800 400 200 100 50 Network bandwidth ( Mbps ) ( c ) Agriculture [ 89 ] ( d ) Asset [ 95 ] Figure 13 : ElasticNotebook adapts to different environments for its replication plan . The lower the network bandwidth , the more variables are recomputed . Twitter [ 110 ] Interactive [ 108 ] Sklearn [ 109 ] ) B M ( d a e h r e v O 4 3 2 1 0 ) s m ( e m T i 0 500 1500 1000 No . cell executions 2000 ( a ) AHG size 150 100 50 0 0 500 1500 1000 No . cell executions ( b ) Optimization Time 2000 Figure 14 : Scalability of ElasticNotebook with cell execution count . The size of AHG increases linearly . Replication plan optimization time increases sub-linearly . on the NFS network bandwidth via rate limiting [ 10 ] and compare the migration time of ElasticNotebook , DumpSession ( migrating all variables ) , and RerunAll . We report the results in Fig 13 . ElasticNotebook ’ s balancing of variables storage and recomputation ensures that it is always at least as fast as the faster of DumpSession and RerunAll . Notably , Elastic- Notebook can adapt to the relative availability between network bandwidth and compute power : as the bandwidth decreases , the replication plan is changed accordingly to migrate more variables through recomputation rather than storage . For example , on the Stacking [ 24 ] notebook , at regular bandwidth ( > 400Mbps ) , Elastic- Notebook ’ s replication plan includes migrating most of the session state , opting only to recompute certain train/test splits ( i.e. , Cell 37 , Y_train , Y_validation ) . At < 400 Mbps , ElasticNotebook modifies its plan to recompute instead of store a computationally expensive processed dataframe ( Cell 39 , latest_record ) . At < 100 Mbps , Elas- ticNotebook modifies its plan again to only store the imported class and function definitions ( i.e. , XGBRegressor , mean_squared_error in Cell 1 ) while recomputing the rest of the notebook . 7.8 Scaling to Complex Workloads In this section , we test the scalability of ElasticNotebook ’ s session replication on complex notebook sessions with a large number of cell executions and re-executions . Specifically , we choose 3 tutorial notebooks , on which we randomly re-execute cells and measure the ( 1 ) size of ElasticNotebook ’ s AHG and ( 2 ) optimization time for computing the replication plan at up to 2000 cell re-executions7 . 7This is twice the length of the longest observed notebook on Kaggle [ 50 ] . We report the results in Fig 14 . The memory consumption of Elas- ticNotebook ’ s AHG exhibits linear scaling vs. the number of cell executions reaching only < 4MB at 2000 cell re-executions , which is negligible compared to the memory consumption of the note- book session ( > 1GB ) itself . ElasticNotebook ’ s optimization time for computing the replication plan similarly exhibits linear scaling , reaching a negligible < 150ms at 2000 cell re-executions : ElasticNote- book ’ s chosen algorithm for solving min-cut , Ford-Fulkerson [ 30 ] , has time complexity 𝑂 ( 𝐸 𝑓 ) , where 𝐸 is the number of edges in the AHG and 𝑓 is the cost of the optimal replication plan : The former scales linearly while the latter is largely constant . 8 RELATED WORK Intermediate Result Reuse in Data Science . The storage of in- termediate results has been explored in various contexts in Data Science due to the incremental and feed-forward nature of tasks , which allows outputs from prior operations to be useful for speed- ing up future operations [ 42 , 55 , 65 , 111 , 116 , 117 , 122 ] . Examples include caching to speed up model training replay for ML model di- agnosis [ 42 , 111 ] , caching to speed up anticipated future dataframe operations in notebook workflows [ 117 ] , and storage of cell out- puts to facilitate graphical exploration of the notebook ’ s execu- tion history for convenient cell re-runs [ 55 , 65 ] . There are related works [ 116 , 122 ] which algorithmically explore the most efficient way to ( re ) compute a state given currently stored items ; compared to our work , while Helix [ 116 ] similarly features balancing loading and recomputation , its model lacks the linked variable constraint which may result in silently incorrect replication if directly applied to the computational notebook problem setting . Data-level Session Replication . Session replication on Jupyter- based platforms can be performed with serialization libraries [ 34 , 35 , 38 , 39 , 78 ] . There exists a variety of checkpoint tools built on these serialization libraries : IPython ’ s % Store [ 104 ] is a Pickle-based [ 38 ] interface for saving variables to a key-value store ; however , it breaks object references as linked variables are serialized into separate files . The Dill-based [ 39 ] DumpSession [ 40 ] correctly resolves ob- ject references , yet it still fails if the session contains unserializable objects . Tensorflow [ 49 ] and Pytorch [ 29 ] offer periodical check- pointing during ML model training limited to objects within the same library . Jupyter ’ s native checkpointing mechanism [ 102 ] only saves cell metadata and often fails to exactly restore a session due to the common presence of hidden states . Compared to existing data- level tools , session replication with ElasticNotebook is both more efficient and robust : the Application History Graph enables balanc- ing state storage and recomputation , which achieves considerable speedup while avoiding failure on unserializable objects . System-Level Session Replication . Session replication can sim- ilarly be performed using system-level checkpoint/restart ( C/R ) tools , on which there is much existing work [ 6 , 6 , 8 , 12 , 23 , 52 , 72 , 77 , 96 ] . Applicable tools include DMTCP [ 3 ] and CRIU [ 18 ] ; recently , CRUM [ 43 ] and CRAC [ 61 ] have explored extending C/R to CUDA applications . Elsa [ 64 ] integrates CRIU with JupyterHub to enable C/R of JupyterHub servers . Compared to ElasticNotebook , system- level tools are less efficient and robust due to their large memory dump sizes and limited cross-platform portability , respectively . Zhaoheng Li , Pranav Gor , Rahul Prabhu , Hui Yu , Yuzhou Mao , Yongjoo Park Lineage Tracing . Lineage tracing has seen extensive use in state management to enable recomputation of data for more efficient storage of state or fault tolerance [ 16 , 51 , 71 , 82 , 106 , 111 , 120 ] Recently , the usage of data lineage in computational notebooks has enabled multi-version notebook replay [ 76 ] , recommending notebook interactions [ 75 ] , and creating reproducible notebook containers [ 1 ] , and program slicing , i.e. , finding the minimal set of code to run to compute certain variable ( s ) [ 51 , 54 , 65 , 83 , 93 ] . This work adopts lineage tracing techniques to capturing inter-variable dependencies ( the Application History Graph ) for optimization ; to the best of our knowledge , existing works on Python programs focus on capturing value modifications ( via equality comparisons ) ; however , our techniques additionally identifies and captures strucal changes via the ID graph , which is crucial for preserving variable aliases and avoiding silent errors during state replication . Replicating Execution Environment . An identical execution en- vironment may be necessary for session replication on a different machine . There is some recent work exploring environment repli- cation for Jupyter Notebook via containerizing input files and mod- ules [ 1 , 115 ] . While useful in conjunction with ElasticNotebook , we consider these works to be largely orthogonal . Notebook Parameterization and Scripts . There exists works on executing notebooks in parameterized form for systematic experi- mentation ( e.g. , in the form of a script via [ 99 ] or papermill [ 100 ] ) . While ElasticNotebook is designed for use within interactive note- book interfaces , it is similarly applicable for the migration of pa- rameterized notebook execution results . 9 CONCLUSION In this work , we have proposed ElasticNotebook , a new computa- tional notebook system that newly offers elastic scaling and check- pointing/restoration . To achieve this , ElasticNotebook introduces a transparent data management layer between the user interface and the underlying kernel , enabling robust , efficient , and platform- independent state replication for notebook sessions . Its core con- tributions include ( 1 ) low-overhead , on-the-fly application history construction and ( 2 ) a new optimization for combining copying and re-computation of variables that comprise session states . We have demonstrated that ElasticNotebook can reduce upscaling , down- scaling , and restoration times by 85 % -98 % , 84 % -99 % , and 94 % -99 % , respectively , on real-world data science notebooks with negligible runtime and memory overheads of < 2.5 % and < 10 % , respectively . In the future , we plan to achieve higher efficiency and usability by tracing state changes at a finer level . Specifically , we will introduce micro-cells to capture code blocks inside a cell that repeatedly runs ( e.g. , for-loop for machine learning training ) . Then , the system will automatically store intermediate models ( along with other meta- data ) that will enable live migration and checkpointing/restoration for long-running cell executions . ACKNOWLEDGMENTS The authors are grateful to Chandra Chekuri and Kent Quanrud for assistance with the derivation of the reduction to min-cut employed in ElasticNotebook . This work is supported in part by the National Center for Supercomputing Applications and Microsoft Azure . ElasticNotebook : Enabling Live Migration for Computational Notebooks REFERENCES [ 1 ] Raza Ahmad , Naga Nithin Manne , and Tanu Malik . 2022 . Reproducible Notebook Containers using Application Virtualization . In 2022 IEEE 18th International Conference on e-Science ( e-Science ) . IEEE , 1–10 . [ 2 ] AndresHG . 2021 . NLP , GloVe , BERT , TF-IDF , LSTM ... Explained . https : //www . . Jason Ansel , Kapil Arya , and Gene Cooperman . 2009 . DMTCP : Transparent checkpointing for cluster computations and the desktop . In 2009 IEEE Interna- tional Symposium on Parallel & Distributed Processing . IEEE , 1–12 . [ 4 ] Microsoft Azure . 2023 . Azure ML Studio . https : //learn.microsoft.com/en- [ 3 ] . [ 5 ] Microsoft Azure . 2023 . Microsoft Azure pay-as-you-go . https : //azure.microsoft . . [ 6 ] Anju Bala and Inderveer Chana . 2012 . Fault tolerance-challenges , techniques and implementation in cloud computing . International Journal of Computer Science Issues ( IJCSI ) 9 , 1 ( 2012 ) , 288 . [ 7 ] Ekrem Bayar . 2022 . Store Sales TS Forecasting - A Comprehensive Guide . https : comprehensive-guide/notebook . [ 8 ] Mohammad Riyaz Belgaum , Safeeullah Soomro , Zainab Alansari , and Muham- mad Alam . 2018 . Cloud service ranking using checkpoint-based load balancing in real-time scheduling of cloud computing . In Progress in advanced computing and intelligent engineering . Springer , 667–676 . James Bergstra and Yoshua Bengio . 2012 . Random search for hyper-parameter optimization . Journal of machine learning research 13 , 2 ( 2012 ) . [ 9 ] [ 10 ] Simon Séhier Bert Hubert , Jacco Geul . 2020 . WonderShaper . https : //github . com/magnific0/wondershaper . [ 11 ] Michael Brachmann and William Spoth . 2020 . Your notebook is not crumby enough , REPLace it . In Conference on Innovative Data Systems Research ( CIDR ) . [ 12 ] Gang Chen , Hai Jin , Deqing Zou , Bing Bing Zhou , Weizhong Qiang , and Gang Hu . 2010 . Shelp : Automatic self-healing for multiple application instances in a virtual machine environment . In 2010 IEEE International Conference on Cluster Computing . IEEE , 97–106 . [ 13 ] Chhaya Choudhary . 2023 . Machine Learning and Deep learning Notebooks . https : . [ 14 ] Chhaya Choudhary . 2023 . This project is about customer churn predic- tion . https : customer_churn_prediction.ipynb . [ 15 ] Bokeh Contributors . 2023 . Bokeh - Interaction . https : //docs.bokeh.org/en/ [ 16 ] . Iván Cores , Gabriel Rodríguez , Mará J Martín , Patricia González , and Roberto R Osorio . 2013 . Improving scalability of application-level checkpoint-recovery by reducing checkpoint sizes . New Generation Computing 31 ( 2013 ) , 163–185 . [ 17 ] CRIU . 2023 . CRIU - Invisible file . https : //criu.org/Invisible_files . [ 18 ] CRIU . 2023 . Linux CRIU . https : //criu.org/Main_Page . [ 19 ] Andrew Crotty , Alex Galakatos , Emanuel Zgraggen , Carsten Binnig , and Tim Kraska . 2015 . Vizdom : interactive analytics through pen and touch . Proceedings of the VLDB Endowment 8 , 12 ( 2015 ) , 2024–2027 . JupyterHub Idle Culler . 2023. jupyterhub/jupyterhub-idle-culler . JupyterHub Idle Culler . https : //github.com/ [ 20 ] [ 21 ] Renato LF Cunha , Lucas C Villa Real , Renan Souza , Bruno Silva , and Marco AS Netto . 2021 . Context-aware Execution Migration Tool for Data Science Jupyter Notebooks on Hybrid Clouds . In 2021 IEEE 17th International Conference on eScience ( eScience ) . IEEE , 30–39 . [ 22 ] Nvidia Developer . 2023 . Nvidia - CUDA . https : //developer.nvidia.com/cuda- toolkit . [ 23 ] Sheng Di , Yves Robert , Frédéric Vivien , Derrick Kondo , Cho-Li Wang , and Franck Cappello . 2013 . Optimization of cloud task processing with checkpoint- restart mechanism . In Proceedings of the International Conference on High Per- formance Computing , Networking , Storage and Analysis . 1–12 . [ 24 ] DimitreOliveira . 2019 . Model stacking , feature engineering and EDA . https : engineering-and-eda/notebook . [ 25 ] Docker . [ n.d. ] . Docker documentation - Swarm mode overview . https : //docs . docker.com/engine/swarm/ . [ 26 ] Cody Dunne , Nathalie Henry Riche , Bongshin Lee , Ronald Metoyer , and George Robertson . 2012 . GraphTrail : Analyzing large multivariate , heterogeneous networks while supporting exploration history . In Proceedings of the SIGCHI conference on human factors in computing systems . 1663–1672 . [ 27 ] dwd daniel . 2022 . UncomplicatedFirewall . https : //wiki.ubuntu.com/ UncomplicatedFirewall . [ 28 ] Philipp Eichmann , Emanuel Zgraggen , Carsten Binnig , and Tim Kraska . 2020 . Idebench : A benchmark for interactive data exploration . In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data . 1555–1569 . https : //pytorch- [ 29 ] Lightning AI et al . 2018 . PyTorch ModelCheckpoint . . ModelCheckpoint.html . [ 30 ] LRDR FORD-FULKERSON . 1962 . Flows in Networks . [ 31 ] Python Software Foundation . 2023 . Python - AST . https : //docs.python.org/3/ library/ast.html . [ 32 ] Python Software Foundation . 2023 . Python - Generators . https : //wiki.python . org/moin/Generators . [ 33 ] Python Software Foundation . 2023 . Python Hashlib . https : //docs.python.org/3/ library/hashlib.html . [ 34 ] Python Software Foundation . 2023 . Python JSON . https : //docs.python.org/3/ library/json.html . [ 35 ] Python Software Foundation . 2023 . Python Marshal . https : //docs.python.org/3/ library/marshal.html . [ 36 ] Python Software Foundation . 2023 . Python Mmap . https : //docs.python.org/3/ library/mmap.html . [ 37 ] Python Software Foundation . 2023 . Python Object Reduction . https : //docs . python.org/3/library/pickle.html # object.__reduce__ . [ 38 ] Python Software Foundation . 2023 . Python Pickle Documentation . https : . [ 39 ] The Uncertainty Quantification Foundation . 2023 . Dill - PyPi . https : //pypi.org/ project/dill/ . [ 40 ] The Uncertainty Quantification Foundation . 2023 . Dill dump session . https : . [ 41 ] Tian Gao . 2020 . Python Watchpoints . https : //pypi.org/project/watchpoints/ . [ 42 ] Rolando Garcia , Eric Liu , Vikram Sreekanti , Bobby Yan , Anusha Dandamudi , Joseph E Gonzalez , Joseph M Hellerstein , and Koushik Sen. 2020 . Hindsight logging for model training . arXiv preprint arXiv:2006.07357 ( 2020 ) . [ 43 ] Rohan Garg , Apoorve Mohan , Michael Sullivan , and Gene Cooperman . 2018 . CRUM : Checkpoint-restart support for CUDA ’ s unified memory . In 2018 IEEE International Conference on Cluster Computing ( CLUSTER ) . IEEE , 302–313 . [ 44 ] GDB . 2022 . GDB Watchpoints . https : //sourceware.org/gdb/download/ . [ 45 ] Aurélien Geron . 2023 . Chapter 4 – Training Models . https : //github.com/ageron/ . [ 46 ] Aurélien Geron . 2023 . Machine Learning Notebooks , 3rd edition . https : //github . com/ageron/handson-ml3 . [ 47 ] Google . 2023 . Google Colab . https : //colab.research.google.com/ . [ 48 ] Google . 2023 . Google Colab pay-as-you-go . https : //colab.research.google.com/ signup . [ 49 ] Google . 2023 . Tensorflow Checkpoint . https : //www.tensorflow.org/guide/ checkpoint . [ 50 ] Google and X . 2022 . Google AI4Code – Understand Code in Python Notebooks . https : . [ 51 ] Philip J Guo and Margo I Seltzer . 2012 . Burrito : Wrapping your lab notebook in computational infrastructure . ( 2012 ) . [ 52 ] HAProxy . 2023 . HAProxy . http : //www.haproxy.org/ . [ 53 ] Sanskar Hasija . 2022 . AI4Code Detailed EDA . https : //www.kaggle.com/code/ odins0n/ai4code-detailed-eda . [ 55 ] [ 54 ] Andrew Head , Fred Hohman , Titus Barik , Steven M Drucker , and Robert DeLine . 2019 . Managing messes in computational notebooks . In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . 1–12 . Inc. Hex Technologies . 2023 . Hex 2.0 : Reactivity , Graphs , and a little bit of Magic . https : //hex.tech/blog/hex-two-point-oh/ . IBM . 2022. knowledge-accelerators/1.0.0 ? topic=catalog-jupyter-notebook . IBM Watson Studio Service . https : //www.ibm.com/docs/en/ [ 56 ] [ 57 ] Kaggle Inc. 2023 . Kaggle . https : //www.kaggle.com/ . [ 58 ] Kaggle Inc. 2023 . Kaggle Forums - Product Feedback . https : //www.kaggle.com/ discussions/product-feedback . [ 59 ] Kaggle Inc. 2023 . Kaggle Notebook Specifications . https : //www.kaggle.com/ docs/notebooks # technical-specifications . [ 60 ] Space Telescope Science Institute . 2023 . JWST Data Analysis Exam- ple . https : example-jupyter-notebooks . [ 62 ] [ 61 ] Twinkle Jain and Gene Cooperman . 2020 . Crac : Checkpoint-restart architecture for cuda with streams and uvm . In SC20 : International Conference for High Performance Computing , Networking , Storage and Analysis . IEEE , 1–15 . Jeremiah W Johnson . 2020 . Benefits and pitfalls of jupyter notebooks in the classroom . In Proceedings of the 21st Annual Conference on Information Technol- ogy Education . 32–37 . [ 63 ] Project Jupyter . 2023 . Jupyter Notebook . https : //jupyter.org/ . [ 64 ] Mario Juric , Steven Stetzler , and Colin T Slater . 2021 . Checkpoint , Restore , and Live Migration for Science Platforms . arXiv preprint arXiv:2101.05782 ( 2021 ) . [ 65 ] David Koop and Jay Patel . 2017 . Dataflow notebooks : encoding and tracking dependencies of cells . In 9th USENIX Workshop on the Theory and Practice of Provenance ( TaPP 2017 ) . [ 66 ] Martin Krasser . 2023 . Machine learning notebooks . https : //github.com/ krasserm/machine-learning-notebook . [ 67 ] Martin Krasser . 2023 . Multi-class Classification . https : //github.com/krasserm/ . [ 68 ] Kubernetes . [ n.d. ] . Kubernetes . https : //kubernetes.io/ . [ 69 ] SFU Database System Lab . 2022 . Dataprep - Low-Code Data Preparation . https : [ 100 ] Nteract Team . 2023 . Welcome to papermill . https : //papermill.readthedocs.io/ //dataprep.ai/ . en/latest/ . [ 70 ] Colin Lagator . 2020 . Arxiv Data Processing . https : //www.kaggle.com/code/ [ 101 ] The IPython Development Team . 2023 . IPython Interactive Computing . https : colinlagator/arxiv-data-processing . //ipython.org/ . Zhaoheng Li , Pranav Gor , Rahul Prabhu , Hui Yu , Yuzhou Mao , Yongjoo Park [ 71 ] Haoyuan Li , Ali Ghodsi , Matei Zaharia , Scott Shenker , and Ion Stoica . 2014 . Tachyon : Reliable , memory speed storage for cluster computing frameworks . In Proceedings of the ACM Symposium on Cloud Computing . 1–15 . [ 72 ] Yawei Li and Zhiling Lan . 2010 . FREM : A fast restart mechanism for general checkpoint/restart . IEEE Trans . Comput . 60 , 5 ( 2010 ) , 639–652 . [ 73 ] Arch Linux . 2023. chroot . https : //wiki.archlinux.org/title/chroot . [ 74 ] Zhicheng Liu and Jeffrey Heer . 2014 . The effects of interactive latency on exploratory visual analysis . IEEE transactions on visualization and computer graphics 20 , 12 ( 2014 ) , 2122–2131 . [ 75 ] Stephen Macke , Hongpu Gong , Doris Jung-Lin Lee , Andrew Head , Doris Xin , and Aditya Parameswaran . 2020 . Fine-grained lineage for safer notebook interactions . arXiv preprint arXiv:2012.06981 ( 2020 ) . [ 76 ] Naga Nithin Manne , Shilvi Satpati , Tanu Malik , Amitabha Bagchi , Ashish Gehani , and Amitabh Chaudhary . 2022 . CHEX : Multiversion Replay with Ordered Checkpoints . arXiv preprint arXiv:2202.08429 ( 2022 ) . [ 77 ] Anjali D Meshram , AS Sambare , and SD Zade . 2013 . Fault tolerance model for reliable cloud computing . International Journal on Recent and Innovation Trends in Computing and Communication 1 , 7 ( 2013 ) , 600–603 . Inc. MongoDB . 2023 . BSON . https : bson/index.html . [ 78 ] [ 79 ] Philipp Moritz , Robert Nishihara , Stephanie Wang , Alexey Tumanov , Richard Liaw , Eric Liang , Melih Elibol , Zongheng Yang , William Paul , Michael I Jordan , et al . 2018 . Ray : A distributed framework for emerging { AI } applications . In 13th { USENIX } Symposium on Operating Systems Design and Implementation ( { OSDI } 18 ) . 561–577 . [ 80 ] Rob Mulla . 2020 . Time Series forecasting with Prophet . https : //www.kaggle . . [ 81 ] Devin Petersohn , Stephen Macke , Doris Xin , William Ma , Doris Lee , Xiangxi Mo , Joseph E Gonzalez , Joseph M Hellerstein , Anthony D Joseph , and Aditya Parameswaran . 2020 . Towards scalable dataframe systems . arXiv preprint arXiv:2001.00888 ( 2020 ) . [ 82 ] Arnab Phani , Benjamin Rath , and Matthias Boehm . 2021 . LIMA : Fine-grained Lineage Tracing and Reuse in Machine Learning Systems . In Proceedings of the 2021 International Conference on Management of Data . 1426–1439 . Joao Felipe Pimentel , Leonardo Murta , Vanessa Braganholo , and Juliana Freire . 2017. noWorkflow : a tool for collecting , analyzing , and managing provenance from python scripts . Proceedings of the VLDB Endowment 10 , 12 ( 2017 ) . [ 84 ] The pip developers . 2023 . Pip Freeze . https : //pip.pypa.io/en/stable/cli/pip_ [ 83 ] freeze/ . [ 85 ] Olga Poppe , Qun Guo , Willis Lang , Pankaj Arora , Morgan Oslake , Shize Xu , and Ajay Kalhan . 2022 . Moneyball : proactive auto-scaling in Microsoft Azure SQL database serverless . Proceedings of the VLDB Endowment 15 , 6 ( 2022 ) , 1279–1287 . [ 86 ] PBC Posit Software , PBC formerly RStudio . 2023 . Posit RStudio . https : //posit . [ 102 ] The IPython Development Team . 2023 . Jupyter checkpoint . https : //jupyter- . [ 103 ] The IPython Development Team . 2023 . Jupyter Magics Class . https : //ipython . . [ 104 ] The IPython Development Team . 2023 . Jupyter store magic . https : //ipython . . [ 105 ] The Matplotlib Development Team . 2023 . Matplotlib . https : //matplotlib.org/ . [ 106 ] Quoc-Cuong To , Juan Soto , and Volker Markl . 2018 . A survey of state manage- ment in big data processing systems . The VLDB Journal 27 , 6 ( 2018 ) , 847–872 . [ 107 ] Cornell University . 2021 . Cornell Virtual Workshop Tutorial Notebooks . https : . [ 108 ] Cornell University . 2021 . Investigating Tweet Timelines Using Interactive Bokeh Scatterplots . https : . [ 109 ] Cornell University . 2021 . SKLearn Tweet Classification . https : tweet_classification.ipynb . [ 110 ] Cornell University . 2021 . Twitter Networks . https : //github.com/CornellCAC/ . [ 111 ] Manasi Vartak , Joana M F. da Trindade , Samuel Madden , and Matei Zaharia . 2018 . Mistique : A system to store and query model intermediates for model diagnosis . In Proceedings of the 2018 International Conference on Management of Data . 1285–1300 . [ 112 ] Alexandre Verbitski , Anurag Gupta , Debanjan Saha , Murali Brahmadesam , Kamal Gupta , Raman Mittal , Sailesh Krishnamurthy , Sandor Maurice , Tengiz Kharatishvili , and Xiaofeng Bao . 2017 . Amazon aurora : Design considerations for high throughput cloud-native relational databases . In Proceedings of the 2017 ACM International Conference on Management of Data . 1041–1052 . [ 113 ] Devlikamov Vlad . 2022 . [ TPS-Mar ] Fast workflow using scikit-learn- https : intelex . . [ 114 ] Eric-Jan Wagenmakers and Simon Farrell . 2004 . AIC model selection using Akaike weights . Psychonomic bulletin & review 11 , 1 ( 2004 ) , 192–196 . [ 115 ] Dimuthu Wannipurage , Suresh Marru , and Marlon Pierce . 2022 . A Framework to capture and reproduce the Absolute State of Jupyter Notebooks . arXiv preprint arXiv:2204.07452 ( 2022 ) . [ 116 ] Doris Xin , Stephen Macke , Litian Ma , Jialin Liu , Shuchen Song , and Aditya Parameswaran . 2018 . Helix : Holistic optimization for accelerating iterative machine learning . arXiv preprint arXiv:1812.05762 ( 2018 ) . [ 117 ] Doris Xin , Devin Petersohn , Dixin Tang , Yifan Wu , Joseph E Gonzalez , Joseph M Hellerstein , Anthony D Joseph , and Aditya G Parameswaran . 2021 . Enhancing the interactivity of dataframe queries by leveraging think time . arXiv preprint arXiv:2103.02145 ( 2021 ) . [ 118 ] xxHash . 2023. xxHash - Extremely fast non-cryptographic hash algorithm . co/ . https : //github.com/Cyan4973/xxHash . [ 87 ] Gabriel Preda . 2019 . LANL Earthquake EDA and Prediction . https : //www . [ 119 ] Yandex . 2023 . CatBoost - open-source gradient boosting library . https : //catboost . . ai/ . [ 88 ] Kalilur Rahman . 2022 . NFL Data Bowl 2023 - Offensive Plays EDA . https : plays-eda/notebook . [ 89 ] DS Rahul . 2020 . Agricultural Drought Prediction . https : //www.kaggle.com/ . [ 90 ] Mani Raj . 2022 . Amex Dataset . https : amex-dataset . [ 91 ] Amazon Web Services . 2023 . AWS JupyterHub . https : //docs.aws.amazon.com/ . [ 92 ] Shahules . 2022 . Basic EDA , Cleaning and GloVe . https : //www.kaggle.com/code/ . [ 93 ] Shreya Shankar , Stephen Macke , Sarah Chasins , Andrew Head , and Aditya Parameswaran . 2022 . Bolt-on , compact , and rapid program slicing for notebooks . Proceedings of the VLDB Endowment 15 , 13 ( 2022 ) , 4038–4047 . [ 94 ] shreyas thorat30 . 2023 . Plant disease classification SDP . https : //www.kaggle . . [ 95 ] Andrey Shtrauss . 2022 . Building an Asset Trading Strategy . https : //www.kaggle . . [ 96 ] Stelios Sidiroglou , Oren Laadan , Carlos Perez , Nicolas Viennot , Jason Nieh , and Angelos D Keromytis . 2009 . Assure : automatic software self-healing using rescue points . ACM SIGARCH Computer Architecture News 37 , 1 ( 2009 ) , 37–48 . [ 97 ] StackOverflow . 2019 . Colab Session Timeout . https : //stackoverflow.com/ . [ 98 ] Stitchfix . 2017 . Nodebooks . https : //github.com/stitchfix/nodebook . [ 99 ] Jupyter Development Team . 2023. nbconvert - Jupyter Notebook Conversion . https : //github.com/jupyter/nbconvert . [ 120 ] Matei Zaharia , Mosharaf Chowdhury , Michael J Franklin , Scott Shenker , and Ion Stoica . 2010 . Spark : Cluster computing with working sets . In 2nd USENIX Workshop on Hot Topics in Cloud Computing ( HotCloud 10 ) . [ 121 ] Emanuel Zgraggen , Robert Zeleznik , and Steven M Drucker . 2014 . Panoram- icData : Data analysis through pen & touch . IEEE transactions on visualization and computer graphics 20 , 12 ( 2014 ) , 2112–2121 . [ 122 ] Ce Zhang , Arun Kumar , and Christopher Ré . 2016 . Materialization optimizations for feature selection workloads . ACM Transactions on Database Systems ( TODS ) 41 , 1 ( 2016 ) , 1–32 . A APPENDIX A.1 Low Per-cell overhead We report the results for per-cell time and memory overheads on 3 Homework notebooks in Fig 15 . ElasticNotebook ’ s memory and per-cell monitoring overhead are consistently under 10 % and 1ms , respectively . There are occasionally ’ spikes ’ when certain cells declaring/modifying complex variables are executed ; for example , the 60 % and 91ms memory and time overheads of cell 28 in [ 46 ] is attributed to constructing the ID Graph for a complex nested list . However , even in this worst case , the time overhead is still ElasticNotebook : Enabling Live Migration for Computational Notebooks ) B M ( d a e h r e v O 30 20 10 0 0 User namespace memory usage 20 60 40 No . cell executions 80 ) B M ( d a e h r e v O ElasticNotebook memory usage 40 30 20 10 0 0 5 10 15 No . cell executions ) B M ( d a e h r e v O 20 15 10 5 0 0 [ 45 ] [ 67 ] [ 14 ] ) s m ( e m T i 100 75 50 25 0 25 0 25 % 50 % No . cell executions 75 % 100 % 10 5 20 No . cell executions 15 ( a ) Mem . overhead , [ 45 ] ( b ) Mem . overhead , [ 67 ] ( c ) Mem . overhead , [ 14 ] ( d ) Per-cell time overhead Figure 15 : Runtime and memory overhead of ElasticNotebook during notebook use on selected homework notebooks . Memory overhead is consistently low , and per-cell runtime overhead is negligible for most cell executions . RerunAll CRIU % Store DumpSession ElasticNotebook ( Ours ) 100 % 40 % 50 % 100 % 100 % 100 % 100 % 100 % 100 % 100 % 100 % 51 % 100 % 100 % 84 % 100 % 55 % 93 % 30 % 20 % 10 % 0 % Sklearn [ 109 ] NLP [ 2 ] StoreSales [ 7 ] TPS-Mar [ 113 ] Glove [ 92 ] Trading [ 95 ] Timeseries [ 80 ] Stacking [ 24 ] Agriculture [ 109 ] LANL [ 87 ] HW-LM [ 45 ] HW-ex3 [ 67 ] Notebook l l A n u r e R f o % e m T i Figure 16 : ElasticNotebook ’ s session downscaling time ( D32as v5 VM→D16as v5 VM ) vs. existing tools . Times normalized w.r.t . RerunAll . ElasticNotebook speeds up migration by 84 % -99 % and is up to 2.00× faster than the next best alternative . be a an arbitrary variable , and ( 𝑥 , 𝑡 G ) , ( 𝑥 , 𝑡 G∗ ) be its active VSs in G and G∗ respectively . There is 𝑡 G ≥ 𝑡 G∗ : if 𝑡 G > 𝑡 G∗ ( due to falsely implied non-overwrite modifications , i.e. , gen in Fig 17 ) then there must be a path from ( 𝑥 , 𝑡 G ) to ( 𝑥 , 𝑡 G∗ ) : ( 𝑥 , 𝑡 G ) , 𝑐𝑡G , ( 𝑥 , 𝑡𝑘1 ) , 𝑐𝑡𝑘1 , ... , ( 𝑥 , 𝑡𝑘𝑙 ) , 𝑐𝑡𝑘𝑙 < 𝑡 G∗ and , ( 𝑥 , 𝑡 G∗ ) , where 𝑡 G < 𝑡𝑘1 < ... < 𝑡𝑘𝑙 all contain false non-overwrite modifications to 𝑥. There- 𝑐𝑡𝑘1 , ... , 𝑐𝑡𝑘𝑙 fore , the subtree rooted at ( 𝑥 , 𝑡 G ) in G must be contained the subtree rooted at ( 𝑥 , 𝑡 G∗ ) in G∗ , hence 𝑟𝑒𝑞∗ ( 𝑥 , 𝑡 G∗ ) ⊆ 𝑟𝑒𝑞 ( 𝑥 , 𝑡 G ) . □ A.3 Handling Large Pandas Dataframes To avoid hashing large Pandas dataframes after each cell execution , ElasticNotebook uses the dataframes ’ underlying writeable flag as a dirty bit to detect in-place changes : before each cell execution , the writeable flag is set to False , and the dataframe is identified as modified if the flag has been flipped to True after the cell execution . G ( x , 𝒕1 ) 𝒄𝒕1 𝒄𝒕2 ( z , 𝒕2 ) 𝑐𝑡3 G∗ ( x , 𝑡1 ) ( y , 𝑡1 ) ( y , 𝑡1 ) 𝑐𝑡1 𝒄𝒕2 ( z , 𝒕2 ) 𝑐𝑡3 ( x , 𝑡3 ) ( l1 , 𝑡3 ) ( x , 𝑡3 ) ( l1 , 𝑡3 ) ( gen , 𝒕4 ) 𝒄𝒕4 𝒄𝒕5 ( 2dlist , 𝑡4 ) ( gen , 𝒕4 ) 𝒄𝒕4 𝑐𝑡5 ( 2dlist , 𝑡4 ) ( gen , 𝒕5 ) 𝑟𝑒𝑞∗ ( 𝑥 ) = { 𝑐𝑡2 } ⊆ 𝑟𝑒𝑞 ( 𝑥 ) = { 𝑐𝑡1 , 𝑐𝑡2 } 𝑟𝑒𝑞∗ ( 𝑔𝑒𝑛 ) = { 𝑐𝑡4 } ⊆ 𝑟𝑒𝑞 ( 𝑔𝑒𝑛 ) = { 𝑐𝑡4 , 𝑐𝑡5 } ( x , 𝑡1 ) ( Overwritten/deleted ) Variable Snapshot 𝑐𝑡1 Cell Execution ( x , 𝑡1 ) Active Variable Snapshot Figure 17 : AHG G may contain false positives compared to the true AHG G∗ . The correctness is still ensured , while the efficiency may be affected due to extra cells re-running , for example , when recomputing z ( green ) and gen ( red ) . well under the 500ms threshold suggested for interactive data en- gines [ 74 ] , while the memory overhead is of a low absolute value ( 4MB ) compared to the size of the ( not yet loaded ) datasets , thus having negligible user impact . A.2 Proof of Theorem 4.1 An illustration of our proof is provided in Fig 17 . Proof . As there are no false negatives , the true AHG G∗ is con- tained within the approximate AHG G , i.e. , G∗ ⊆ G ( Fig 17 ) . Let 𝑥","['elasticnotebook', 'enable', 'live', 'migration', 'computational', 'technical', 'report', 'pranav', 'gor∗', 'prabhu∗', 'rprabhu5', 'huiy3', 'umichedu', 'p', 'e', 'b', 'c', 'v', 'r', 'abstract', 'computational', 'notebook', 'widely', 'use', 'interactive', 'datum', 'science', 'machine', 'learning', 'framework', 'user', 'start', 'session', 'execute', 'cell', 'set', 'statement', 'create', 'variable', 'train', 'model', 'visualize', 'result', 'unfortunately', 'exist', 'notebook', 'system', 'offer', 'live', 'tion', 'notebook', 'launch', 'new', 'machine', 'lose', 'state', 'prevent', 'user', 'continue', 'task', 'leave', 'dbms', 'session', 'directly', 'rely', 'underlying', 'kernel', 'interpreter', 'addi', 'tional', 'data', 'management', 'layer', 'exist', 'technique', 'preserve', 'state', 'copy', 'variable', 'oslevel', 'checkpointing', 'unreliable', 'often', 'fail', 'inefficient', 'platformdependent', 'also', 'rerun', 'code', 'scratch', 'highly', 'timeconsuming', 'paper', 'introduce', 'new', 'notebook', 'system', 'elastic', 'notebook', 'offer', 'live', 'migration', 'checkpointingrestoration', 'use', 'novel', 'mechanism', 'reliable', 'efficient', 'platform', 'independent', 'specifically', 'observe', 'cell', 'execution', 'parent', 'lightweight', 'monitoring', 'elasticnotebook', 'find', 'reliable', 'efficient', 'way', 'ie', 'replication', 'plan', 'reconstruct', 'origi', 'nal', 'session', 'state', 'consider', 'variablecell', 'dependency', 'observe', 'runtime', 'variable', 'size', 'end', 'new', 'graphbased', 'mization', 'problem', 'find', 'reconstruct', 'variable', 'efficiently', 'subset', 'variable', 'transfer', 'machine', 'show', 'elasticnotebook', 'reduce', 'endtoend', 'migration', 'restoration', 'time', 'respectively', 'ety', 'jwst', 'tutorial', 'notebook', 'negligible', 'runtime', 'memory', 'overhead', 'pvldb', 'reference', 'format', 'pranav', 'gor∗', 'prabhu∗', 'elasticnotebook', 'enable', 'live', 'migration', 'computational', 'notebook', 'xxxxxx', 'doi', 'xxxxxxxxx', 'introduction', 'computational', 'rstudio', 'widely', 'use', 'datum', 'science', 'machine', 'learning', 'interactive', 'torial', 'datum', 'exploration', 'visualization', 'model', 'work', 'license', 'creative', 'common', 'international', 'license', 'visit', 'https', 'view', 'copy', 'license', 'use', 'cover', 'license', 'obtain', 'permission', 'email', 'info', 'vldborg', 'copyright', 'hold', 'ownerauthor', 'publication', 'right', 'license', 'vldb', 'endowment', 'proceeding', 'vldb', 'endowment', 'vol', 'issn', 'doi', 'xxxxxxxxx', 'work', 'use', 'term', 'notebook', 'mean', 'system', 'serve', 'notebook', 'content', 'notebook', 'depend', 'context', 'user', 'interface', 'datum', 'code', 'cell', 'code', 'technique', 'dynamic', 'exe', 'cution', 'history', 'graph', 'technique', 'optimization', 'fast', 'migration', 'python', 'r', 'llvm', 'figure', 'transparent', 'datum', 'layer', 'middle', 'enable', 'robust', 'efficient', 'platformindependent', 'live', 'migration', 'tuning', 'selection', 'cloud', 'provider', 'offer', 'software', 'asaservice', 'hub', 'azure', 'studio', 'studio', 'commonly', 'use', 'library', 'panda', 'pytorch', 'notebook', 'workflow', 'begin', 'user', 'start', 'computing', 'session', 'user', 'execute', 'cell', 'set', 'statement', 'load', 'dataset', 'create', 'variable', 'train', 'model', 'visualize', 'result', 'session', 'terminate', 'manually', 'automatically', 'save', 'resource', 'cost', 'limitation', 'live', 'replication', 'unfortunately', 'exist', 'note', 'book', 'offer', 'transparent', 'infrastructure', 'scale', 'independent', 'application', 'become', 'increasingly', 'popular', 'cloud', 'instant', 'scalability', 'cost', 'reduction', 'eg', 'autoscale', 'dbms', 'microservice', 'orchestration', 'copy', 'notebook', 'file', 'new', 'large', 'memory', 'suspend', 'session', 'save', 'cost', 'resume', 'notebook', 'lose', 'state', 'set', 'variable', 'code', 'output', 'word', 'user', 'resume', 'task', 'previously', 'leave', 'notebook', 'directly', 'rely', 'underlying', 'kernel', 'interpreter', 'c', 'repl', 'addi', 'tional', 'data', 'management', 'layer', 'accordingly', 'variable', 'reside', 'process', 'erase', 'terminate', 'session', 'address', 'potentially', 'save', 'variable', 'restore', 'new', 'environment', 'however', 'exist', 'technique', 'serialize', 'variable', 'checkpointe', 'process', 'fail', 'inefficient', 'platformdependent', 'discuss', 'shortly', 'finally', 'rerun', 'code', 'scratch', 'timeconsume', 'goal', 'propose', 'elasticnotebook', 'notebook', 'system', 'offer', 'live', 'state', 'migration', 'checkpointingrestoration', 'use', 'reliable', 'efficient', 'platformindependent', 'state', 'replication', 'mech', 'anism', 'reliability', 'enable', 'correctsuccessful', 'replication', 'almost', 'notebook', 'efficiency', 'significantly', 'efficient', 'platformindependence', 'rely', 'platform', 'architecturespecific', 'feature', 'elasticnotebook', 'enable', 'live', 'notebook', 'replication', 'potentially', 'notebook', 'workload', 'introduce', 'novel', 'data', 'management', 'layer', 'example', 'user', 'specify', 'new', 'machine', 'run', 'currently', 'active', 'notebook', 'system', 'transparently', 'replicate', 'notebook', 'include', 'variable', 'notebook', 'run', 'new', 'machine', 'pranav', 'rahul', 'park', 'table', 'comparison', 'elasticnotebook', 'possible', 'approach', 'savingrestore', 'session', 'state', 'approach', 'mechanism', 'serializationbase', 'tool', 'systemlevel', 'checkpointe', 'notebook', 'versione', 'replay', 'execution', 'environment', 'elasticnotebook', 'serialize', 'store', 'variable', 'computing', 'session', 'fail', 'unserializable', 'variable', 'save', 'memory', 'dump', 'computing', 'session', 'high', 'network', 'cost', 'low', 'portability', 'enable', 'reexecution', 'versione', 'notebook', 'snapshot', 'result', 'verification', 'migrate', 'instal', 'module', 'useful', 'conjunction', 'orthogonal', 'session', 'state', 'replication', 'optimally', 'combine', 'copyrecompute', 'reliability', 'efficiency', 'platform', 'independence', 'provide', 'capability', 'little', 'modification', 'exist', 'system', 'offer', 'benefit', 'large', 'number', 'datum', 'scientist', 'educator', 'use', 'notebook', 'achieve', 'overcome', 'follow', 'technical', 'challenge', 'challenge', 'create', 'reliable', 'efficient', 'platformindependent', 'replication', 'mechanism', 'challenge', 'first', 'mechanism', 'offer', 'high', 'coverage', 'almost', 'notebook', 'people', 'create', 'able', 'successfully', 'replicate', 'machine', 'second', 'mechanism', 'significantly', 'fast', 'straight', 'forward', 'approach', 'rerun', 'cell', 'exactly', 'run', 'past', 'copy', 'possible', 'variable', 'serial', 'izationdeserialization', 'third', 'mechanism', 'integrate', 'exist', 'notebook', 'system', 'clean', 'separation', 'sustainable', 'development', 'easy', 'adoption', 'approach', 'core', 'idea', 'observe', 'evolution', 'session', 'state', 'lightweight', 'monitoring', 'address', 'important', 'challenge', 'reliability', 'efficiency', 'platform', 'independence', 'combine', 'program', 'language', 'technique', 'thefly', 'code', 'analysis', 'novel', 'algorithmic', 'solution', 'graph', 'base', 'mathematical', 'optimization', 'specifically', 'represent', 'session', 'state', 'change', 'introduce', 'application', 'history', 'special', 'form', 'bipartite', 'graph', 'express', 'dependency', 'variable', 'cell', 'execution', 'use', 'graph', 'take', 'follow', 'approach', 'first', 'achieve', 'reliability', 'platform', 'independence', 'choo', 'e', 'computational', 'plan', 'replication', 'plan', 'safely', 'construct', 'variable', 'generator', 'incompletely', 'define', 'custom', 'class', 'base', 'platform', 'independent', 'variable', 'presence', 'variable', 'serialize', 'platformindependent', 'replication', 'use', 'application', 'history', 'recompute', 'dy', 'namically', 'target', 'machine', 'process', 'elasticnotebook', 'optimize', 'collective', 'cost', 'recompute', 'variable', 'still', 'maintain', 'correctness', 'second', 'efficiency', 'elasticnotebook', 'optimize', 'replication', 'plan', 'determine', 'variable', 'copy', 'variable', 'recompute', 'base', 'copy', 'variable', 'minimize', 'endtoend', 'migration', 'restoration', 'time', 'sideration', 'serialization', 'cost', 'recomputation', 'cost', 'data', 'transfer', 'cost', 'example', 'even', 'variable', 'reliably', 'transfer', 'machine', 'variable', 'still', 'dynamically', 'construct', 'result', 'low', 'total', 'cost', 'make', 'decision', 'principled', 'way', 'devise', 'new', 'graphbased', 'optimization', 'problem', 'reduce', 'wellestablished', 'mincut', 'problem', 'implementation', 'contribution', 'apply', 'many', 'namically', 'analyzable', 'language', 'llvmbase', 'one', 'implement', 'prototype', 'c', 'python', 'user', 'interface', 'widely', 'use', 'datum', 'science', 'machine', 'learn', 'statistical', 'analysis', 'specifically', 'elasticnotebook', 'provide', 'data', 'management', 'layer', 'jupyter', 'hide', 'cell', 'magic', 'transparently', 'monitor', 'cell', 'execution', 'offer', 'efficient', 'replication', 'difference', 'exist', 'work', 'compare', 'exist', 'work', 'pursue', 'significantly', 'different', 'direction', 'example', 'tool', 'make', 'data', 'serialization', 'convenient', 'ever', 'fail', 'session', 'contain', 'nonserializable', 'variable', 'inefficient', 'consider', 'opportunity', 'dy', 'namic', 'recomputation', 'alternatively', 'systemlevel', 'checkpointe', 'platformdependent', 'limited', 'checkpointe', 'memory', 'less', 'efficient', 'dynamic', 'recompu', 'tation', 'impossible', 'building', 'top', 'result', 'reuse', 'lineage', 'trace', 'introduce', 'deep', 'referenceaware', 'analysis', 'novel', 'optimization', 'technique', 'incorporate', 'unique', 'constraint', 'intervariable', 'dependency', 'also', 'empirically', 'confirm', 'effectiveness', 'completely', 'thogonal', 'work', 'include', 'library', 'migration', 'scalable', 'datum', 'science', 'table', 'summarize', 'difference', 'contribution', 'contribution', 'follow', 'motivation', 'discuss', 'alternative', 'approach', 'explain', 'advantage', 'approach', 'architecture', 'describe', 'system', 'architecture', 'efficient', 'robust', 'session', 'replication', 'datum', 'model', 'introduce', 'novel', 'data', 'model', 'application', 'history', 'graph', 'expression', 'session', 'history', 'enable', 'efficient', 'accurate', 'state', 'replication', 'optimization', 'problem', 'solution', 'formally', 'define', 'optimization', 'problem', 'minimize', 'state', 'replication', 'cost', 'balance', 'variable', 'copying', 'recomputation', 'propose', 'efficient', 'effective', 'solution', 'evaluation', 'show', 'elasticnotebook', 'reduce', 'upscale', 'scaling', 'restore', 'time', 'respectively', 'overhead', 'negligible', 'runtime', 'motivation', 'section', 'describe', 'use', 'case', 'requirement', 'session', 'replication', 'intuition', 'high', 'efficiency', 'live', 'migration', 'useful', 'seamless', 'state', 'replication', 'computational', 'notebook', 'low', 'easy', 'infrastructure', 'scaling', 'frequent', 'session', 'suspension', 'interrupt', 'user', 'workflow', 'describe', 'fast', 'replication', 'elastic', 'computing', 'ability', 'move', 'state', 'machine', 'useful', 'scale', 'resource', 'allow', 'migrate', 'live', 'session', 'machine', 'right', 'equipmentresource', 'specific', 'architecture', 'interruptionfree', 'scaling', 'copy', 'datum', 'source', 'elasticnotebook', 'enable', 'live', 'migration', 'computational', 'intercept', 'code', 'def', 'intercept', 'code', 'preprocess', 'code', 'regular', 'kernel', 'execution', 'execute', 'code', 'postprocess', 'code', 'application', 'history', 'cell', 'df', 'cell', '1min', 'cell', 'model', 'cell', 'plot', 'dftest', 'figure', 'cell', 'run', 'inject', 'custom', 'prepost', 'processing', 'logic', 'intercept', 'hide', 'user', 'variable', 'store', 'cost', 'min', 'reload', 'cost', 'min', 'total', 'cost', 'min', 'df', 'dftrain', 'dftest', 'model', 'plot', 'machine', 'target', 'machine', 'way', 'original', 'session', 'state', 'restore', 'process', 'want', 'minimize', 'endtoend', 'time', 'create', 'transfer', 'target', 'machine', 'reconstruct', 'state', 'target', 'machine', 'first', 'use', 'case', 'empirically', 'study', 'fast', 'restart', 'ondemand', 'computing', 'leverage', 'pricing', 'model', 'offer', 'many', 'cloud', 'vendor', 'sus', 'pende', 'session', 'use', 'effective', 'way', 'reduce', 'charge', 'eg', '6×', 'ability', 'cre', 'eat', 'datum', 'sufficient', 'reconstruct', 'current', 'session', 'state', 'persist', 'prior', 'manual', 'automate', 'suspen', 'sion', 'quickly', 'resume', 'need', 'session', 'state', 'achieve', 'ondemand', 'granular', 'computing', 'fast', 'session', 'restart', 'time', 'impact', 'user', 'experience', 'frequent', 'session', 'suspension', 'process', 'want', 'restore', 'session', 'quickly', 'possible', 'minimize', 'time', 'take', 'download', 'reconstruct', 'state', 'second', 'use', 'case', 'empirically', 'study', 'enable', 'data', 'management', 'layer', 'discuss', 'pro', 'con', 'several', 'different', 'approach', 'enable', 'data', 'management', 'layer', 'oslevel', 'checkpointing', 'save', 'current', 'session', 'state', 'checkpoint', 'entire', 'memory', 'space', 'associate', 'underlie', 'kernel', 'make', 'process', 'efficient', 'exist', 'tool', 'patch', 'trace', 'dirty', 'page', 'however', 'describe', 'approach', 'platformindependent', 'incur', 'high', 'space', 'cost', 'limit', 'store', 'state', 'primary', 'memory', 'device', 'empirically', 'compare', 'approach', 'criu', 'understand', 'reliability', 'efficiency', 'object', 'wrapper', 'watchpoint', 'object', 'wrapper', 'com', 'monly', 'use', 'debug', 'purpose', 'program', 'slice', 'maintain', 'deep', 'copy', 'object', 'session', 'state', 'compare', 'check', 'change', 'frame', 'execution', 'ever', 'unsuitable', 'use', 'datum', 'science', 'workflow', 'unacceptable', 'runtime', 'overhead', 'preliminary', 'test', 'monitor', 'cell', 'execution', 'order', 'trace', 'cell', 'exe', 'cution', 'effect', 'variable', 'add', 'lightweight', 'wrapper', 'data', 'management', 'layer', 'function', 'cell', 'execution', 'monitor', 'cell', 'code', 'runtime', 'variable', 'change', 'idea', 'depict', 'conceptually', 'fig', 'specifically', 'implementation', 'use', 'cell', 'magic', 'jupyternative', 'mechanism', 'allow', 'arbitrary', 'modification', 'cell', 'statement', 'cell', 'execute', 'add', 'prepostprocesse', 'step', 'capture', 'cell', 'code', 'result', 'session', 'state', 'modification', 'store', 'var', 'method', 'rerun', 'store', 'fastmigrate', 'model', 'plot', 'fastrestore', 'df', 'model', 'plot', 'rerun', 'cell', 'migration', 'cost', 'restore', 'cost', 'figure', 'example', 'app', 'history', 'top', 'different', 'tion', 'plan', 'cost', 'bottom', 'combine', 'recomputecopy', 'allow', 'fast', 'migration', 'fastmigrate', 'alternatively', 'optimal', 'plan', 'change', 'restoration', 'prioritize', 'fastrestore', 'fast', 'replication', 'application', 'history', 'section', 'describe', 'core', 'idea', 'devise', 'efficient', 'repli', 'cation', 'strategy', 'leverage', 'ability', 'monitor', 'cell', 'execution', 'application', 'history', 'application', 'history', 'graph', 'ahg', 'bipar', 'tite', 'graph', 'express', 'session', 'state', 'change', 'respect', 'cell', 'run', 'type', 'variable', 'transformation', 'transformation', 'node', 'connect', 'input', 'variable', 'output', 'variable', 'see', 'example', 'fig', 'aim', 'achieve', 'property', 'completeness', 'false', 'negative', 'inputoutput', 'variable', 'transformation', 'capture', 'minimal', 'minimal', 'false', 'positive', 'number', 'variable', 'incorrectly', 'identify', 'accessedmodified', 'variable', 'actually', 'accessedmodifie', 'minimize', 'property', 'require', 'correct', 'state', 'reconstruction', 'core', 'optimization', 'idea', 'ahg', 'allow', 'efficient', 'state', 'replica', 'tion', 'combination', 'recompute', 'copy', 'motivate', 'example', 'suppose', 'data', 'analyst', 'fit', 'regression', 'model', 'fig', 'notebook', 'contain', 'cell', 'run', 'datum', 'load', 'cell', 'traint', 'split', 'cell', 'fitting', 'cell', 'evaluation', 'cell', 'fit', 'decide', 'move', 'session', 'new', 'machine', 'simply', 'rerun', 'entire', 'notebook', 'incur', 'minute', 'alternatively', 'serializingcopye', 'variable', 'take', 'minute', 'however', 'efficient', 'approach', 'copy', 'model', 'plot', 'recompute', 'new', 'machine', 'fast', 'migrate', 'complete', 'endtoend', 'migration', 'minute', 'prioritize', 'restoration', 'time', 'reduce', 'userperceived', 'restart', 'time', 'ondemand', 'compute', 'optimize', 'plan', 'fastrestore', 'take', 'minute', 'example', 'illustrate', 'significant', 'optimization', 'opportunity', 'session', 'replication', 'goal', 'ability', 'find', 'good', 'replication', 'plan', 'arbitrarily', 'complex', 'ahgs', 'system', 'overview', 'section', 'present', 'elasticnotebook', 'high', 'level', 'describe', 'component', 'operation', 'datum', 'layer', 'core', 'part', 'elasticnotebook', 'table', 'notation', 'meaning', 'pranav', 'rahul', 'notebook', 'user', 'world', 'cell', 'execution', 'interceptor', 'optimizer', 'graph', 'object', 'hash', 'optimization', 'application', 'history', 'graph', 'cost', 'model', 'session', 'replicator', 'writer', 'notebook', 'usern', 'key', 'val', 'world', 'figure', 'elasticnotebook', 'architecture', 'datum', 'layer', 'act', 'gateway', 'user', 'interface', 'kernel', 'cell', 'execution', 'intercept', 'observe', 'session', 'state', 'change', 'elasticnotebook', 'component', 'elasticnotebook', 'introduce', 'unique', 'data', 'layer', 'act', 'gate', 'way', 'user', 'kernel', 'see', 'fig', 'monitor', 'cell', 'execution', 'observe', 'code', 'result', 'session', 'state', 'change', 'cell', 'execution', 'interceptor', 'cell', 'execution', 'interceptor', 'cept', 'cell', 'execution', 'request', 'add', 'prepostprocesse', 'script', 'reroute', 'underlie', 'kernel', 'regular', 'tion', 'add', 'script', 'perform', 'cell', 'code', 'analysis', 'ahg', 'update', 'cell', 'runtime', 'recording', 'application', 'history', 'graph', 'ahg', 'incrementally', 'build', 'cell', 'execution', 'interceptor', 'record', 'variable', 'accessedmodifie', 'cell', 'execution', 'ahg', 'use', 'optimizer', 'compute', 'replication', 'plan', 'cost', 'model', 'cost', 'model', 'store', 'profile', 'metric', 'cell', 'run', 'time', 'variable', 'size', 'network', 'bandwidth', 'serve', 'hyperpa', 'rameter', 'optimizer', 'optimizer', 'optimizer', 'use', 'ahg', 'cost', 'model', 'determine', 'efficient', 'replication', 'plan', 'consist', 'able', 'store', 'cell', 'rerun', 'discuss', 'elasticnotebook', 'cost', 'model', 'optimization', 'detail', 'session', 'replicator', 'session', 'replicator', 'replicate', 'notebook', 'session', 'accord', 'optimizer', 'plan', 'specifically', 'writer', 'create', 'write', 'checkpoint', 'file', 'storage', 'ssd', 'cloud', 'storage', 'notebook', 'read', 'file', 'restore', 'session', 'follow', 'replication', 'plan', 'discuss', 'elastic', 'notebook', 'session', 'replication', 'detail', 'elasticnotebook', 'workflow', 'section', 'describe', 'operation', 'elasticnote', 'book', 'monitor', 'cell', 'execution', 'session', 'lifecycle', 'perform', 'onrequest', 'replication', 'session', 'step', 'check', 'point', 'writing', 'checkpoint', 'file', 'restoration', 'monitor', 'cell', 'execution', 'cell', 'execution', 'user', 'elasticnotebook', 'perform', 'follow', 'step', 'access', 'variable', 'cell', 'execution', 'identify', 'analysis', 'describe', 'cell', 'code', 'execute', 'definition', 'set', 'variable', 'set', 'variable', 'snapshot', 'vss', 'set', 'active', 'variable', 'snapshot', 'set', 'cell', 'execution', 'ce', 'set', 'write', 'dependency', 'set', 'read', 'dependency', 'symbol', 'v𝑎', 'c', '∪', '∪', 'application', 'history', 'graph', '𝑟𝑒𝑞', 'reconstruction', 'mapping', 'function', '𝑤𝑠𝑡𝑜𝑟𝑒', 'variable', 'storage', 'cost', '𝑤𝑟𝑒𝑟𝑢𝑛', 'c', 'r', 'cell', 'rerun', 'cost', '𝑤m', 'r', 'migration', 'cost', 'function', 'r', 'recomputation', 'cost', 'function', 'pair', 'link', 'variable', 'flow', 'graph', 'h', 'r', 'flow', 'graph', 'edge', 'capacity', 'function', 'variable', 'change', 'ie', 'creationdeletionmodification', 'iden', 'tifie', 'global', 'namespace', 'ahg', 'update', 'use', 'cell', 'code', 'modify', 'variable', 'cell', 'execution', 'cost', 'model', 'update', 'record', 'cell', 'runtime', 'initiate', 'replication', 'replication', 'request', 'elastic', 'notebook', 'create', 'write', 'checkpoint', 'file', 'storage', 'restore', 'later', 'exactly', 'efficiently', 'reconstruct', 'current', 'session', 'elasticnotebook', 'first', 'complete', 'cost', 'model', 'pro', 'filing', 'variable', 'size', 'network', 'bandwidth', 'storage', 'optimizer', 'utilize', 'ahg', 'cost', 'model', 'compute', 'replica', 'tion', 'plan', 'accord', 'writer', 'create', 'checkpoint', 'file', 'consist', 'subset', 'store', 'variable', 'session', 'state', 'cell', 'rerun', 'ahg', 'cost', 'model', 'restore', 'session', 'request', 'elasticnotebook', 'restore', 'notebook', 'session', 'checkpoint', 'file', 'accord', 'replication', 'plan', 'notebook', 'reconstruct', 'variable', 'order', 'appear', 'original', 'session', 'combine', 'cell', 'rerun', 'datum', 'deserialization', 'follow', 'variable', 'declaration', 'kernel', 'finally', 'elasticnotebook', 'load', 'ahg', 'cost', 'model', 'future', 'replication', 'accuracy', 'elasticnotebook', 'state', 'reconstructing', 'effectively', 'rerun', 'cell', 'scratch', 'exactly', 'order', 'run', 'past', 'elasticnotebook', 'shorten', 'endtoend', 'reconstruction', 'time', 'loading', 'save', 'able', 'kernel', 'namespace', 'achieve', 'time', 'saving', 'present', 'formal', 'correctness', 'analysis', 'discuss', 'address', 'external', 'resource', 'side', 'effect', 'deserialization', 'failure', 'application', 'history', 'graph', 'section', 'formally', 'define', 'application', 'history', 'graph', 'describe', 'achieve', 'exact', 'state', 'replication', 'ahg', 'formal', 'definition', 'ahg', 'direct', 'acyclic', 'graph', 'express', 'session', 'state', 'change', 'respect', 'cell', 'execution', 'fig', 'example', 'definition', 'variable', 'name', 'entity', 'df', 'reference', 'object', 'uniquely', 'identify', 'object', 'elasticnotebook', 'enable', 'live', 'migration', 'computational', 'notebook', 'variable', 'primitive', 'eg', 'int', 'string', 'complex', 'list', 'dataframe', 'multiple', 'variable', 'point', 'object', 'set', 'variable', 'define', 'global', 'namespace', 'form', 'session', 'state', 'cell', 'execution', 'modify', 'value', 'variable', 'reference', 'object', 'change', 'name', 'recognize', 'ahg', 'use', 'variable', 'snapshot', 'follow', 'definition', 'variable', 'snapshot', 'nametimestamp', 'pair', 'represent', 'variable', '𝑥', 'createdmodifie', 'denote', 'set', 'vse', 'definition', 'cell', 'execution', 'represent', 'cell', 'execution', 'finish', 'cell', 'execution', 'linear', 'session', 'cell', 'run', 'time', 'execution', 'totally', 'order', 'denote', 'list', 'ce', 'also', 'store', 'execute', 'cell', 'code', 'use', 'rerun', 'definition', 'write', 'dependency', 'indicate', 'modifiedcreate', 'time', 'object', 'reachable', 'variable', '𝑥', 'denote', 'set', 'write', 'dependency', 'fig', '𝑐𝑡3', 'modifie', 'hence', '𝑐𝑡3', '𝑐𝑡3', 'definition', 'read', 'dependency', '𝑠', 'indicate', 'access', 'object', 'reachable', 'createdmodifie', 'time', 'denote', 'set', 'read', 'dependency', 'fig', '𝐶𝑡4', 'access', 'element', 'list', 'creation', '𝑐𝑡3', 'hence', 'x', '𝑐𝑡3', '𝑐𝑡4', 'note', 'writeread', 'dependency', 'allow', 'contain', 'false', 'positive', 'nevertheless', 'replication', 'ensure', 'correctness', 'definition', 'ahg', '𝐺', 'v∪c', 'bipartite', 'graph', 'v', 'vse', 'c', 'writeread', 'dependency', 'respectively', 'model', 'lineage', 'notebook', 'session', 'sum', 'formalize', 'variable', 'accessesmodification', 'spect', 'cell', 'execution', 'variable', 'level', 'object', 'level', 'theo', 'retically', 'bound', 'size', 'ahg', 'scale', 'linearly', 'number', 'define', 'variable', 'number', 'underlying', 'object', 'large', 'list', 'dataframe', 'empirically', 'verify', 'ahg', 'low', 'memory', 'overhead', 'dynamic', 'ahg', 'construction', 'describe', 'elasticnotebook', 'construct', 'ahg', 'accurately', 'construct', 'ahg', 'incrementally', 'build', 'accessedcreatedmodified', 'variable', 'cell', 'execution', '•', 'new', 'create', 'execution', 'completion', 'time', 'read', 'dependency', 'create', 'vse', '𝑥1', '𝑡𝑥1', '𝑥𝑘', '𝑡𝑥𝑘', '𝑥1', '𝑥𝑘', 'variable', 'possibly', 'access', 'vse', 'create', 'variable', 'possibly', 'modify', 'create', 'write', 'dependency', 'add', 'newly', 'create', 'vse', 'fig', 'right', 'show', 'example', 'identify', 'accessmodified', 'variable', 'crucial', 'construction', 'describe', 'graph', 'graph', 'aim', 'detect', 'change', 'reference', 'level', 'addition', 'value', 'instance', 'conventional', 'equality', 'check', 'eg', 'base', 'serialization', 'return', 'true', 'notebook', 'cell', 'cell', '𝑐𝑡2', 'z', 'false', 'print', 'cell', '𝑐𝑡3', 'z', 'cell', '𝑐𝑡4', 'cell', '𝑐𝑡5', 'print', '𝒕1', '𝒄𝒕1', '𝑐𝑡2', 'z', '𝒄𝒕3', '𝑐𝑡4', '𝑐𝑡5', 'overwrittendelete', 'variable', 'snapshot', '𝑐𝑡1', 'cell', 'execution', '𝑡1', 'active', 'variable', 'snapshot', 'figure', 'example', 'notebook', 'correspond', 'appli', 'cation', 'history', 'graph', 'ahg', 'tell', 'elasticnotebook', 'recompute', 'variable', 'example', 'rerun', '𝑐𝑡1', '𝑐𝑡3', 'necessary', 'recompute', 'x', 'b', 'b', 'value', 'ensure', 'return', 'true', 'b', 'refer', 'object', 'object', 'unique', 'correct', 'state', 'replication', 'share', 'reference', 'alias', 'intervariable', 'relationship', 'capture', 'precisely', 'identify', 'access', 'variable', 'elasticnotebook', 'identifie', 'directly', 'access', 'variable', 'parse', 'indirectly', 'access', 'variable', 'graph', 'follow', 'direct', 'access', 'cell', 'code', 'analyze', 'stepping', 'also', 'userdefined', 'function', 'potentially', 'nest', 'check', 'access', 'variable', 'explicitly', 'pass', 'parameter', 'global', 'indirect', 'access', 'object', 'reachable', 'variable', 'access', 'indirectly', 'variable', 'reference', 'common', 'object', 'eg', 'alias', 'exist', 'fig', '6a', 'identify', 'parse', 'recognize', 'indirect', 'access', 'check', 'existence', 'overlap', 'graph', 'approach', 'conservative', 'overidentify', 'vari', 'able', 'include', 'example', 'one', 'reachable', 'control', 'flow', 'branch', 'take', 'cell', 'execution', 'however', 'false', 'positive', 'affect', 'accuracy', 'state', 'replication', 'identify', 'modify', 'variable', 'variable', 'modification', 'iden', 'tifie', 'use', 'combination', 'object', 'hash', 'graphs', 'value', 'change', 'elasticnotebook', 'identifie', 'value', 'modification', 'compare', 'hash', 'cell', 'execution', 'use', 'deep', 'copy', 'fallback', 'deep', 'copy', 'fail', 'eg', 'unserializable', 'uncomparable', 'variable', 'consider', 'modifiedonaccess', 'use', 'result', 'ast', 'graph', 'result', 'false', 'positive', 'however', 'previously', 'mention', 'false', 'positive', 'affect', 'accuracy', 'structural', 'change', 'graph', 'enable', 'detect', 'structural', 'change', 'fig', 'cell', 'execution', 'current', 'variable', 'graph', 'compare', 'one', 'create', 'identify', 'reference', 'swap', 'fig', 'value', 'remain', 'unchanged', 'cell', 'lambda', 'func', 'func', 'cell', 'graph', 'func', 'detect', 'indirect', 'variable', 'access', 'alias', 'cell', 'list1', 'list1', 'list1', 'cell', 'list2', 'list2', 'cell', 'cell', 'graph', 'list1', 'list2', 'value', 'b', 'detect', 'structural', 'variable', 'modification', 'figure', 'use', 'graph', 'ahg', 'construction', 'execution', 'execute', 'cell', 'memory', 'address', 'nest', 'list', 'change', 'long', 'reference', 'list1', 'state', 'reconstruction', 'ahg', 'section', 'describe', 'reconstruct', 'variable', 'focus', 'reconstruct', 'late', 'version', 'variable', 'define', 'active', 'variable', 'snapshot', 'ahg', 'definition', '𝑥', 'active', '𝑥', 'system', 'delete', 'vs', '𝑗', '𝑗', 'active', '𝑥', 'represent', 'current', 'version', '𝑥', 'example', 'even', 'checkpoint', '𝑐𝑡5', 'fig', 'active', 'last', 'modify', '𝑐𝑡3', 'denote', 'set', 'active', 'vse', 'v𝑎', 'reconstruction', 'goal', 'identify', 'cient', 'computation', 'strategy', 'reconstruct', 'active', 'variable', 'note', 'reconstruct', 'nonactive', 'variable', 'part', 'current', 'session', 'state', 'achieve', 'goal', 'ahg', 'allow', 'avoid', 'unnecessary', 'cell', 'execution', 'eg', 'outcome', 'overwrite', 'learn', 'proper', 'execution', 'order', 'moreover', 'process', 'extend', 'reconstruct', 'set', 'variable', 'efficiently', 'compute', 'still', 'ensure', 'correctness', 'specifically', 'recompute', 'traverse', 'back', 'cestor', 'eg', 'use', 'breadthfirst', 'search', 'collect', 'ce', 'list', '𝑟𝑒𝑞', '𝑡', 'find', 'ground', 'variable', 'path', 'ground', 'variable', 'variable', 'value', 'avail', 'able', 'system', 'ie', 'active', 'copy', 'variable', 'rerun', 'ce', 'order', 'completion', 'time', 'obtain', 'target', '𝑡', 'extend', 'multiple', 'vse', 'say', '𝑥1', '𝑡𝑥1', '𝑥2', '𝑡𝑥2', '𝑥3', '𝑡𝑥3', 'obtain', 'vs', 'union', 'merge', 'set', 'identical', 'ce', 'collapse', 'rerun', 'ce', 'merge', 'set', 'obtain', 'target', 'vse', 'fig', 'show', 'example', 'recompute', 'rerun', '𝑐𝑡3', 'require', 'previous', 'version', '𝑡1', 'input', 'turn', 'require', '𝑐𝑡1', 'rerun', 'notably', 'necessary', 'rerun', '𝑐𝑡2', 'output', 'z', 'available', 'namespace', 'finally', 'discuss', 'approach', 'recover', 'even', 'ground', 'variable', 'unexpectedly', 'unobtainable', 'pranav', 'rahul', 'notebook', 'cell', '𝑐𝑡3', 'z', 'cell', '𝑐𝑡4', 'replication', 'plan', 'migrate', 'migrate', 'recompute', 'recompute', 'recompute', 'migrate', 'true', 'true', 'false', 'figure', 'variable', 'share', 'reference', 'fig', 'migratedrecompute', 'together', 'correct', 'repli', 'cation', 'serve', 'constraint', 'opt', 'problem', 'see', 'use', 'active', 'vse', 'theoretically', 'possible', 'use', 'nonactive', 'variable', 'ground', 'variable', 'preserve', 'deletedoverwritten', 'variable', 'cache', 'able', 'speed', 'recomputation', 'active', 'variable', 'however', 'consider', 'approach', 'many', 'datum', 'science', 'workload', 'memoryhungry', 'large', 'training', 'datum', 'model', 'size', 'still', 'case', 'speed', 'recomputation', 'store', 'small', 'overwritten', 'variable', 'leave', 'future', 'work', 'correctness', 'reconstruction', 'state', 'ahg', 'allow', 'false', 'positive', 'mean', 'indicate', 'cell', 'cessedmodifie', 'variable', 'actually', 'accessedmodifie', 'false', 'positive', 'performance', 'impact', 'affect', 'correctness', 'identification', 'theorem', 'give', 'approximate', 'g', 'elasticnotebook', 'false', 'positive', 'true', 'ahg', 'g∗', '∗', '𝑟𝑒𝑞', 'variable', '𝑥', '𝑡', '∗', '𝑟𝑒𝑞', 'active', 'vss', 'reconstruction', 'mapping', 'function', 'define', 'g', 'g∗', 'respectively', 'arbitrary', 'variable', 'contain', 'cell', 'execution', 'unnecessary', 'recompute', '𝑥', 'never', 'miss', 'necessary', 'cell', 'execution', '∗', 'proof', 'present', 'correct', 'efficient', 'replication', 'section', 'cover', 'elasticnotebook', 'compute', 'efficient', 'correct', 'plan', 'state', 'replication', 'ahg', 'profiled', 'metric', 'describe', 'correctness', 'requirement', 'cost', 'model', 'optimization', 'problem', 'solution', 'correctness', 'requirement', 'elasticnotebook', 'aim', 'correctly', 'replicate', 'session', 'state', 'define', 'notion', 'section', 'definition', 'replication', 'state', 'valueequivalent', 'value', 'postreplication', 'valueequivalent', 'replication', 'preserve', 'value', 'vidual', 'variable', 'guarantee', 'correct', 'identification', '𝑟𝑒𝑞', 'variable', 'however', 'additionally', 'portant', 'share', 'reference', 'preserve', 'define', 'definition', 'valueequivalent', 'replication', 'session', 'state', 'additionally', 'isomorphic', '𝑖𝑑', '𝑏', '𝑖𝑑𝑛𝑒𝑤', '𝑖𝑑𝑛𝑒𝑤', '𝑏', 'arbitrary', 'reference', '𝑖𝑑', '𝑖𝑑𝑛𝑒𝑤', 'unique', 'id', 'memory', 'address', 'object', 'point', '𝑎', 'replication', 'elasticnotebook', 'enable', 'live', 'migration', 'computational', 'notebook', 'migration', 'cost', 'capacity', 'source', 'active', 'vse', 'cell', 'execution', 'capacity∞', 'p', 'acity', 'n', 'cost', 'sink', '𝑐𝑡3', '𝑐𝑡4', '𝑐𝑡5', 'figure', 'run', 'mincut', 'flow', 'graph', 'construct', 'ahg', 'fig', 'partition', 'red', 'define', 'minimum', 'cut', 'dash', 'edge', 'determine', 'replication', 'plan', 'elasticnotebook', 'define', 'replication', 'correct', 'isomor', 'phic', 'require', 'share', 'reference', 'preserve', 'reference', 'point', 'object', 'prereplication', 'still', 'post', 'replication', 'interobject', 'relation', 'identical', 'analogous', 'graph', 'isomorphism', 'describe', 'elasticnotebook', 'ensure', 'isomorphic', 'replication', 'link', 'variable', 'constraint', 'cost', 'model', 'model', 'capture', 'cost', 'associate', 'serialize', 'able', 'write', 'byte', 'datum', 'storage', 'local', 'ssd', 'cloud', 'stor', 'age', 'rerun', 'cell', 'execution', 'cost', 'compute', 'use', 'ahg', 'profiled', 'system', 'metric', 'variable', 'migration', 'cost', 'migrate', 'variable', 'session', 'include', 'serialize', 'checkpoint', 'file', 'load', 'new', 'session', 'give', 'subset', 'variable', 'migrate', 'migration', 'cost', '𝑤𝑀', 'express', 'follow', '𝛼', '×', '𝑤𝑠𝑡𝑜𝑟𝑒', '𝑤𝑙𝑜𝑎𝑑', '𝑤𝑀', '𝑤𝑠𝑡𝑜𝑟𝑒', 'time', 'cost', 'serialize', 'value', '𝑥', 'checkpointe', 'time', 'file', 'unpack', 'new', 'session', 'respectively', 'time', 'estimate', 'use', 'size', '𝑥', 'storage', 'latencybandwidth', 'elasticnotebook', 'time', 'cost', 'unserializable', 'variable', 'set', 'infinity', 'coefficient', 'adjust', 'time', 'cost', 'storage', 'example', 'elasticnotebook', 'invoke', 'autosuspension', '𝛼', 'set', 'low', 'value', 'discount', 'userperceived', 'time', 'store', 'variable', 'prior', 'completely', 'suspend', 'session', 'user', 'likely', 'away', 'variable', 'recomputation', 'cost', 'interceptor', 'record', 'cell', 'run', 'time', 'session', 'lifecycle', 'combine', 'recon', 'struction', 'mapping', '𝑟𝑒𝑞', 'ahg', 'cost', 'recom', 'put', 'subset', 'variable', 'define', 'follow', '𝑤𝑟𝑒𝑟𝑢𝑛', '𝑐', '∑︁', 'cid216', '𝑐', '∈𝑟𝑒𝑞', '𝑥', 'active', 'vs', '𝑥', '𝑤𝑟𝑒𝑟𝑢𝑛', 'c', 'r', 'estimate', 'time', 'rerun', 'new', 'session', 'replication', 'plan', 'cost', 'use', 'migration', 'recomputation', 'cost', 'eq', 'total', 'cost', 'variable', 'migrate', 'variable', 'recompute', 'express', '𝑤', '𝑤𝑀', 'optimization', 'problem', 'state', 'replication', 'goal', 'find', 'variable', 'migrate', 'minimize', 'cost', 'ensure', 'isomoprhic', 'replication', 'consideration', 'variable', 'interdependency', 'additional', 'constraint', 'add', 'constraint', 'link', 'variable', 'variable', 'contain', 'refer', 'ence', 'object', 'refer', 'link', 'variable', 'fig', 'migrate', 'recom', 'put', 'migrate', 'recompute', 'result', 'contain', 'share', 'referencealia', 'break', 'illustrate', 'fig', 'let', 'set', 'link', 'variable', 'pair', 'denote', 'l', 'constraint', 'formally', 'express', 'follow', '𝑥1', '∧', '𝑥2', '𝑥1', '∧', '∉', '𝑥1', '𝑥2', 'l', 'problem', 'definition', 'use', 'cost', 'model', 'con', 'straint', 'formally', 'define', 'state', 'replication', 'problem', 'problem', 'optimal', 'state', 'replication', 'input', 'ahg', '∪', 'e', 'migration', 'cost', 'function', '𝑤𝑀', 'r', 'recompute', 'cost', 'function', 'link', 'variable', 'l', 'replication', 'plan', 'subset', 'variable', 'migrate', 'subset', 'recompute', 'output', 'objective', 'minimize', 'replication', 'cost', 'constraint', 'link', 'variable', 'migrate', 'recom', 'put', '𝑥1', '𝑥2', '𝑥1', '∉', '𝑥1', '𝑥2', '∈', 'l', 'next', 'section', 'present', 'solution', 'solve', 'state', 'replication', 'opt', 'problem', 'solve', 'reduce', 'mincut', 'problem', '𝑠𝑟𝑐𝑠𝑖𝑛𝑘', 'flow', 'graph', 'construct', '𝑠𝑟𝑐𝑠𝑖𝑛𝑘', 'cut', 'subset', 'edge', 'remove', 'flow', 'graph', 'disconnect', 'source', 'sink', 'correspond', 'replication', 'plan', 'cost', 'cut', 'equal', 'replication', 'cost', 'therefore', 'find', 'minimum', 'cost', '𝑠𝑟𝑐𝑠𝑖𝑛𝑘', 'cut', 'equivalent', 'find', 'optimal', 'replication', 'plan', 'flow', 'graph', 'construction', 'flow', 'graph', 'edge', 'capacity', 'r', 'define', 'follow', 'v𝑎', '∪', '𝑠𝑟𝑐', 'v𝑎', 'active', 'vse', 'c', 'cell', 'tion', '𝑠𝑟𝑐', '𝑠𝑖𝑛𝑘', 'dummy', 'source', 'sink', 'v𝑎', '𝑠𝑟𝑐', '𝑠𝑟𝑐', '𝑡', 'add', 'edge', 'source', 'active', 'capacity', 'equal', 'migration', 'cost', 'variable', '𝑤𝑟𝑒𝑟𝑢𝑛', 'add', 'edge', 'capacity', 'sink', 'capacity', 'equal', 'rerun', 'cost', '𝑡', '𝑐', '∞', 'v𝑎', 'add', 'edge', 'infinite', 'capacity', 'active', '𝑥', '𝑡', 'ce', 'recompute', '𝑥1', '𝑥2', 'l', '𝑥1', '𝑥2', '𝑥1', '𝑥2', '∞', 'add', 'bidirectional', 'edge', 'infinite', 'capacity', 'pair', 'active', 'vse', 'correspond', 'link', 'variable', '𝑥1', '𝑥2', 'flow', 'graph', 'h', 'fig', 'depict', 'fig', 'solution', 'solve', 'prob', 'run', '𝑠𝑟𝑐—𝑠𝑖𝑛𝑘', 'mincut', 'solve', 'ie', 'fordfulkerson', 'set', 'edge', 'form', 'mincut', 'dash', 'edge', 'remove', 'disconnect', '𝑠𝑟𝑐', '𝑠𝑖𝑛𝑘', 'therefore', 'define', 'partition', 'red', 'node', 'node', 'reachable', '𝑠𝑟𝑐', 'replication', 'plan', 'obtain', 'reachable', '𝑠𝑟𝑐', 'partition', '𝑥', '∩', 'v𝑎', 'active', 'variable', 'snapshot', 'thus', 'variable', 'want', 'migrate', 'example', 'variable', '∩', 'c', 'ce', 'rerun', 'postmigration', 'recompute', 'example', 'ce', '𝑡1', 'rerun', 'recompute', 'z', 'construction', 'h', 'sum', 'migration', 'recomputation', 'cost', 'configuration', '∩', 'c', 'precisely', 'cost', 'find', 'mincut', 'implementation', 'discussion', 'section', 'describe', 'implementation', 'detail', 'design', 'consideration', 'implementation', 'integrate', 'jupyter', 'seamless', 'integration', 'elasticnote', 'book', 'datum', 'layer', 'implement', 'use', 'magic', 'extension', 'load', 'kernel', 'session', 'initialization', 'cell', 'magic', 'automatically', 'add', 'cell', 'transparently', 'intercept', 'user', 'cell', 'execution', 'perform', 'code', 'analysis', 'create', 'graph', 'object', 'hash', 'serialization', 'protocol', 'pickle', 'protocol', 'eg', 'reduce', 'employ', 'object', 'serialization', 'definition', 'reachable', 'object', 'object', 'reachable', 'variable', 'pickle', 'include', 'pickle', 'standard', 'observe', 'almost', 'datum', 'science', 'librarie', 'numpy', 'pytorch', 'elasticnotebook', 'use', 'almost', 'use', 'case', 'handle', 'undeserializable', 'variable', 'certain', 'variable', 'serialize', 'contain', 'error', 'deserialization', 'instruction', 'refer', 'undeserializable', 'variable', 'typically', 'cause', 'oversight', 'incompletely', 'implement', 'library', 'undetectable', 'serializability', 'check', 'prior', 'checkpointe', 'handle', 'fallback', 'recomputation', 'elastic', 'notebook', 'encounter', 'error', 'deserialize', 'store', 'variable', 'session', 'restoration', 'trace', 'ahg', 'determine', 'rerun', 'necessary', 'cell', 'execution', 'recompute', 'say', 'variable', 'still', 'fast', 'recompute', 'session', 'scratch', 'design', 'consideration', 'definition', 'session', 'state', 'elasticnotebook', 'session', 'state', 'formally', 'define', 'content', 'user', 'namespace', 'usern', 'contain', 'keyvalue', 'pair', 'variable', 'name', '𝑡3', 'also', 'recompute', 'however', 'overwrite', 'store', 'checkpoint', 'file', 'follow', 'procedure', 'preserve', 'link', 'pranav', 'rahul', 'park', 'value', 'reachable', 'object', 'session', 'state', 'include', 'localmodulehidden', 'variable', 'aim', 'capture', 'unobservable', 'state', 'external', 'function', 'pickle', 'protocol', 'follow', 'almost', 'library', 'lesser', 'know', 'one', 'incorrect', 'serialization', 'ignore', 'datum', 'define', 'c', 'stack', 'address', 'elasticnotebook', 'easily', 'tend', 'allow', 'user', 'annotate', 'cellsvariable', 'inform', 'system', 'recompute', 'proper', 'reconstruction', 'mathematically', 'effect', 'set', 'recomputa', 'tion', 'cost', 'infinity', 'cell', 'execution', 'side', 'effect', 'certain', 'cell', 'execution', 'cause', 'external', 'change', 'notebook', 'session', 'filesystem', 'desirable', 'rerun', 'eg', 'upload', 'item', 'reposi', 'tory', 'prototype', 'currently', 'identify', 'side', 'effect', 'focus', 'readoriente', 'data', 'science', 'analytic', 'workload', 'nevertheless', 'system', 'extend', 'least', 'way', 'prevent', 'annotation', 'allow', 'user', 'add', 'manual', 'annotation', 'cell', 'cause', 'side', 'effect', 'system', 'never', 'rerun', 'replications3', 'sandbook', 'block', 'external', 'change', 'replicate', 'notebook', 'sandbox', 'altered', 'file', 'system', 'access', 'chroot', 'block', 'go', 'network', 'ufw', 'sandbox', 'associate', 'regular', 'filenetwork', 'access', 'successful', 'restoration', 'nondeterministic', 'operation', 'replication', 'ef', 'fect', 'rerun', 'cell', 'exact', 'order', 'occur', 'past', 'thus', 'existence', 'nondeterministic', 'operation', 'randint', 'reconstructed', 'variable', 'different', 'value', 'original', 'one', 'user', 'avoid', 'use', 'annota', 'tion', 'inform', 'elasticnotebook', 'always', 'copy', 'library', 'version', 'compatibility', 'accurate', 'replication', 'ensure', 'external', 'resource', 'eg', 'instal', 'module', 'database', 'table', 'remain', 'replication', 'exist', 'tool', 'freeze', 'reproduce', 'computational', 'environment', 'exist', 'datum', 'science', 'platform', 'book', 'colab', 'work', 'incorporate', 'tool', 'experimental', 'evaluation', 'section', 'empirically', 'study', 'effectiveness', 'elastic', 'notebook', 'session', 'replication', 'make', 'follow', 'claim', 'robust', 'replication', 'exist', 'mechanism', 'elasticnote', 'book', 'capable', 'replicate', 'almost', 'notebook', 'fast', 'migration', 'reduce', 'session', 'migration', 'time', 'upscaleddownscaled', 'machine', 'compare', 'rerun', 'cell', 'fast', 'next', 'good', 'alternative', 'respectively', 'fast', 'resumption', 'reduce', 'session', 'restora', 'tion', 'time', 'compare', 'rerun', 'cell', 'fast', 'next', 'good', 'alternative', 'low', 'runtime', 'overhead', 'elasticnotebook', 'incur', 'negligible', 'overhead', 'amortize', 'runtime', 'memory', 'overhead', 'respectively', 'unfeasible', 'annotation', 'eg', 'unserializable', 'variable', 'require', 'cell', 'execution', 'annotate', 'neverrerun', 'recompute', 'elasticnotebook', 'detect', 'case', 'infinite', 'mincut', 'cost', 'user', 'warn', 'delete', 'problematic', 'variable', 'proceed', 'replicate', 'remain', 'majority', 'variable', 'state', 'elasticnotebook', 'enable', 'live', 'migration', 'computational', 'notebook', 'table', 'summary', 'dataset', 'evaluation', 'dataset', 'notebook', 'runtime', 'input', 'datum', 'cell', 'jwst', 'tutorial', '16–439', '25–323', 'low', 'storage', 'overhead', 'checkpoint', 'size', 'small', 'compare', 'exist', 'tool', 'adaptability', 'system', 'environment', 'elasticnotebook', 'achieve', 'consistent', 'saving', 'various', 'environment', 'different', 'network', 'speed', 'available', 'compute', 'resource', 'scalability', 'complex', 'notebook', 'run', 'time', 'memory', 'overhead', 'remain', 'negligible', '150ms', 'even', 'complex', 'notebook', 'cell', 'experiment', 'setup', 'dataset', 'select', 'total', 'notebook', 'dataset', 'kaggle', 'select', 'popular', 'notebook', 'topic', 'exploratory', 'datum', 'analysis', 'machine', 'learn', 'kaggle', 'create', 'grandmastermasterlevel', 'user', 'jwst', 'select', 'notebook', 'topic', 'datum', 'pipeline', 'example', 'notebook', 'provide', 'investigate', 'datum', 'telescope', 'jwst', 'tutorial', 'select', 'notebook', 'cornell', 'vir', 'tual', 'workshop', 'tutorial', 'notebook', 'lightweight', 'introduce', 'tool', 'cluster', 'graph', 'analysis', 'user', 'homework', 'inprogress', 'notebook', 'choose', 'data', 'science', 'exercise', 'contain', 'outoforder', 'cell', 'exe', 'cution', 'runtime', 'error', 'mistake', 'dfbackupdf4', 'table', 'report', 'select', 'notebook', 'dataset', 'size', 'runtime', 'method', 'evaluate', 'elasticnotebook', 'exist', 'tool', 'pable', 'perform', 'session', 'replication', 'rerunall', 'save', 'cell', 'code', 'output', 'ipynb', 'file', 'cell', 'rerun', 'restore', 'session', 'state', 'criu', 'perform', 'systemlevel', 'memory', 'dump', 'pro', 'cess', 'host', 'notebook', 'session', 'session', 'state', 'restore', 'load', 'memory', 'dump', 'revive', 'process', 'store', 'checkpointing', 'tool', 'serialize', 'variable', 'storage', 'use', 'modify', 'version', 'use', 'dill', 'instead', 'pickle', 'dumpsession', 'store', 'dumpsession', 'pack', 'tire', 'session', 'state', 'single', 'file', 'ablation', 'study', 'additionally', 'compare', 'follow', 'ablate', 'implementation', 'elasticnotebook', 'elasticnotebook', 'helix', 'replace', 'mincut', 'solution', 'helix', 'consider', 'link', 'variable', '•', 'graph', 'method', 'omit', 'graph', 'rely', 'ast', 'analysis', 'object', 'hash', 'detect', 'variable', 'access', 'modification', 'respectively', 'create', 'shallow', 'copy', 'df', 'serve', 'purpose', 'backup', 'original', 'implementation', 'store', 'use', 'python', 'fail', 'many', 'notebook', 'give', 'meaningful', 'result', 'rerunall', 'store', 'graph', 'e', 'r', 'e', 'criu', 'architecture', 'dumpsession', 'elasticnotebook', 'criu', 'crossarchitecture', 'elasticnotebook', 'helix', 'e', 'figure', 'ratio', 'correct', 'replication', 'elasticnotebook', 'achieve', 'correctness', 'par', 'full', 'rerun', 'rerunall', 'table', 'exist', 'work', 'fail', 'case', 'work', 'notebook', 'nfl', 'jwst', 'notebook', 'arxiv', 'plant', 'type', 'hashlib', 'mmap', 'description', 'purpose', 'dropdown', 'list', 'plot', 'help', 'avoid', 'read', 'large', 'file', 'memory', 'generator', 'speedup', 'iterable', 'comprehension', 'lazy', 'element', 'generation', 'consider', 'method', 'regard', 'replication', 'correctness', 'gauge', 'impact', 'ignore', 'link', 'constraint', 'implicit', 'access', 'structural', 'modification', 'respectively', 'environment', 'use', 'azure', 'standard', 'instance', 'vcpus', 'gb', 'ram', 'migration', 'experiment', 'migrate', 'session', 'd64asd16as', 'vcpus', 'gb', 'ram', 'upscalingdownscale', 'respectively', 'input', 'datum', 'checkpoint', 'readstore', 'fromto', 'azure', 'stor', 'age', 'block', 'configuration', 'nfs', 'network', 'bandwidth', 'mb', 'read', 'latency', 'time', 'measurement', 'measure', 'migration', 'time', 'time', 'start', 'checkpointe', 'process', 'state', 'restore', 'variable', 'declare', 'namespace', 'destination', 'session', 'restoration', 'time', 'time', 'restore', 'state', 'checkpoint', 'file', 'clear', 'cache', 'checkpointing', 'restore', 'notebook', 'subsequent', 'run', 'reproducibility', 'implementation', 'elasticnotebook', 'experi', 'ment', 'notebook', 'script', 'find', 'repository6', 'robust', 'session', 'replication', 'section', 'compare', 'robustness', 'elasticnotebook', 'session', 'replication', 'exist', 'method', 'count', 'number', 'isomorphic', 'thus', 'correct', 'replication', 'achieve', 'method', 'notebook', 'report', 'result', 'fig', 'elasticnotebook', 'correctly', 'replicate', 'session', 'par', 'full', 'rerun', 'checkpoint', 'file', 'almost', 'always', 'work', 'tably', 'replicate', 'notebook', 'contain', 'unserializable', 'variable', 'variable', 'alias', 'undeserializable', 'variable', 'spectively', 'dumpsession', 'store', 'fail', 'notebook', 'con', 'taine', 'unserializable', 'variable', 'many', 'use', 'enhance', 'datum', 'science', 'workflow', 'efficiency', 'example', 'table', 'elasticnote', 'book', 'successfully', 'replicate', 'bypass', 'serialization', 'variable', 'recomputation', 'store', 'additionally', 'fail', 'notebook', 'total', 'unserializable', 'variable', 'contain', 'variable', 'alias', 'timeserie', 'notebook', 'cell', 'rerunall', 'criu', 'store', 'dumpsession', 'elasticnotebook', 'pranav', 'rahul', 'park', 'sklearn', 'nlp', 'storesale', 'glove', 'trading', 'timeserie', 'stack', 'agriculture', 'lanl', 'hwlm', 'hwex3', 'l', 'n', 'r', 'e', 'r', 'e', 'figure', 'elasticnotebook', 'session', 'upscale', 'time', 'd32as', 'exist', 'tool', 'time', 'normalize', 'wrt', 'rerunall', 'elasticnotebook', 'speed', 'migration', 'fast', 'next', 'good', 'alternative', 'l', 'n', 'r', 'e', 'r', 'e', 'rerunall', 'criu', 'store', 'dumpsession', 'elasticnotebook', 'sklearn', 'nlp', 'storesale', 'glove', 'trading', 'timeserie', 'stack', 'agriculture', 'lanl', 'hwlm', 'hwex3', 'figure', 'elasticnotebook', 'session', 'restoration', 'time', 'exist', 'tool', 'time', 'normalize', 'wrt', 'rerunall', 'elasticnotebook', 'speed', 'session', 'restore', 'fast', 'compare', 'next', 'good', 'alternative', 'table', 'runtime', 'memory', 'overhead', 'elasticnotebook', 'workflow', 'monitoring', 'select', 'notebook', 'notebook', 'runtime', 'total', 'cell', 'monitoring', 'time', 'runtime', 'overhead', 'user', 'namespace', 'memory', 'usage', 'elasticnotebook', 'memory', 'usage', 'memory', 'overhead', 'sklearn', 'nlp', 'storesale', 'trading', 'timeser', 'stack', 'agricult', 'lanl', 'hwlm', 'hwex3', 'link', 'component', 'matplotlib', 'plot', 'fig', 'ax', 'serial', 'ize', 'variable', 'individual', 'file', 'break', 'object', 'reference', 'isomorphism', 'elasticnotebook', 'link', 'variable', 'constraint', 'ensure', 'elasticnotebook', 'helix', 'fail', 'correctly', 'replicate', 'notebook', 'contain', 'variable', 'alias', 'lacking', 'link', 'variable', 'constraint', 'graph', 'fail', 'correctly', 'replicate', 'session', 'miss', 'indirect', 'access', 'structural', 'modification', 'cause', 'incorrect', 'construc', 'tion', 'ahg', 'turn', 'lead', 'recompute', 'variable', 'valueincorrectly', 'criu', 'fail', 'notebook', 'contain', 'invisible', 'file', 'however', 'elasticnotebook', 'failure', 'failure', 'currently', 'fundamental', 'limitation', 'criu', 'robust', 'migration', 'system', 'architecture', 'additionally', 'perform', 'session', 'replication', 'x64', 'architecture', 'instance', 'architecture', 'criu', 'image', 'replicate', 'machine', 'different', 'architecture', 'contrast', 'elasticnotebook', 'limitation', 'fast', 'session', 'migration', 'section', 'compare', 'efficiency', 'elasticnotebook', 'session', 'migration', 'exist', 'method', 'choose', 'notebook', 'unserializable', 'variable', 'otherwise', 'exist', 'method', 'fail', 'com', 'pare', 'endtoend', 'session', 'migration', 'time', 'achieve', 'different', 'method', 'report', 'upscale', 'downscale', 'result', 'fig', 'fig', 'respectively', 'design', 'goal', 'elasticnotebook', 'reduce', 'session', 'replica', 'tion', 'time', 'balance', 'variable', 'storage', 'recomputation', 'successfully', 'reflect', 'follow', 'elasticnotebook', 'able', 'reduce', 'session', 'migration', 'time', 'upscaleddownscaled', 'vms', 'compare', 'rerunall', 'compare', 'dumpse', 'sion', 'store', 'criu', 'store', 'variable', 'checkpoint', 'file', 'elasticnotebook', 'upscalesdownscale', 'fast', 'good', 'dumpsession', 'next', 'good', 'alternative', 'upscalingdownscale', 'notebook', 'fall', 'short', 'robustness', 'demonstrate', 'store', 'individual', 'reading', 'writing', 'variable', 'result', 'high', 'overhead', 'multiple', 'call', 'nfs', 'migration', 'criu', 'slow', 'nonrerun', 'method', 'upscalingdownscale', 'notebook', 'size', 'memory', 'dump', 'high', 'io', 'migration', 'large', 'compare', 'checkpoint', 'file', 'native', 'tool', 'fast', 'session', 'restoration', 'section', 'compare', 'efficiency', 'elasticnotebook', 'session', 'restoration', 'exist', 'method', 'generate', 'checkpoint', 'file', 'use', 'method', 'compare', 'time', 'take', 'restore', 'session', 'checkpoint', 'file', 'notebook', 'elasticnotebook', 'set', 'coefficient', 'emphasize', 'session', 'restoration', 'time', 'heavily', 'report', 'result', 'restoration', 'time', 'fast', 'compare', 'full', 'rerun', 'compare', 'baseline', 'elasticnotebook', '392×', 'fast', 'next', 'good', 'alter', 'native', 'fast', 'restoration', 'attribute', 'elasticnotebook', 'capable', 'adapt', 'new', 'optimization', 'objective', 'baseline', 'example', 'notebook', 'instead', 'run', 'cell', 'df', 'reread', 'dataframe', 'df', 'session', 'migrationcentric', 'plan', 'restoration', 'centric', 'plan', 'opt', 'store', 'df', 'instead', 'reasoning', 'sum', 'serialization', 'deserialization', 'time', 'df', 'great', 'reread', 'time', 'pdreadcsv', '117', '55', 'deserialization', 'time', 'less', 'reread', 'time', '117', '55', 'hence', 'store', 'df', 'optimal', 'choice', 'elasticnotebook', 'enable', 'live', 'migration', 'computational', 'notebook', 'rerunall', 'store', 'e', 'z', 'nlp', 'tps', 'trading', 'timeserie', 'agriculture', 'hwlm', 'figure', 'elasticnotebook', 'checkpoint', 'file', 'size', 'exist', 'ing', 'tool', 'time', 'normalize', 'output', 'dumpsession', 'checkpoint', 'file', 'size', 'small', 'compare', 'exist', 'tool', 'exclude', 'rerunall', 'low', 'runtime', 'overhead', 'section', 'investigate', 'overhead', 'elasticnotebook', 'note', 'book', 'workflow', 'monitor', 'measure', 'elasticnotebook', 'total', 'time', 'spend', 'prepostprocesse', 'step', 'beforeafter', 'cell', 'execu', 'tion', 'update', 'ahg', 'cell', 'runtime', 'total', 'cell', 'monitoring', 'time', 'total', 'storage', 'space', 'take', 'store', 'graph', 'hash', 'checkpoint', 'time', 'elasticnotebook', 'memory', 'usage', 'report', 'result', 'table', 'elasticnotebook', 'cell', 'monitor', 'incur', 'maximum', 'median', 'runtime', 'overhead', 'thus', 'elasticnotebook', 'seamlessly', 'integrate', 'exist', 'workflow', 'elasticnotebook', 'similarly', 'memoryefficient', 'store', 'item', 'graph', 'hash', 'largely', 'independent', 'size', 'item', 'session', 'median', 'memory', 'overhead', 'bad', 'case', 'finegraine', 'analysis', 'study', 'percell', 'time', 'memory', 'overhead', 'experimental', 'notebook', 'usage', 'examine', 'notebook', 'homework', 'category', 'confirm', 'maximum', 'time', 'memory', 'overhead', '92ms', 'respectively', 'report', 'detail', 'low', 'storage', 'overhead', 'section', 'measure', 'storage', 'cost', 'elasticnotebook', 'check', 'point', 'file', 'compare', 'migrationcentric', 'checkpoint', 'file', 'size', 'elasticnotebook', 'baseline', 'method', 'report', 'select', 'result', 'fig', 'low', 'choose', 'store', 'recompute', 'variable', 'reflect', 'elasticnotebook', 'checkpoint', 'file', 'small', 'compare', 'dumpsession', 'example', 'agri', 'culture', 'notebook', 'elasticnotebook', 'recompute', 'traintest', 'split', 'input', 'dataframe', 'cell', 'xtrain', 'traintestsplit', 'instead', 'store', 'check', 'point', 'file', 'save', 'considerable', 'storage', 'space', 'gb', 'addition', 'speed', 'migration', 'conversely', 'criu', 'checkpoint', 'file', 'size', 'large', 'elasticnotebook', 'additionally', 'dump', 'memory', 'occupy', 'process', 'import', 'mod', 'ule', 'matter', 'necessary', 'checkpoint', 'file', 'output', 'size', 'rerunall', 'ie', 'notebook', 'metadata', 'size', 'consist', 'cell', 'code', 'output', 'provide', 'comparison', 'significantly', 'small', 'checkpoint', 'file', 'storage', 'benefit', 'offset', 'significantly', 'slow', 'session', 'recovery', 'performance', 'gain', 'environment', 'section', 'demonstrate', 'operation', 'environ', 'ment', 'vary', 'specification', 'perform', 'parameter', 'sweep', 'elasticnotebook', 'migrate', 'time', 'elasticnotebook', 'recompute', 'time', 'rerunall', 'e', 'e', 'network', 'bandwidth', 'network', 'bandwidth', 'e', 'e', 'network', 'bandwidth', 'stack', 'network', 'bandwidth', 'c', 'agriculture', 'asset', 'figure', 'elasticnotebook', 'adapt', 'different', 'environment', 'replication', 'plan', 'low', 'network', 'bandwidth', 'variable', 'recompute', 'twitter', 'interactive', 'sklearn', 'e', 'h', 'e', 'e', 'cell', 'execution', 'ahg', 'size', 'cell', 'execution', 'b', 'optimization', 'time', 'figure', 'scalability', 'elasticnotebook', 'cell', 'execution', 'count', 'size', 'ahg', 'increase', 'linearly', 'replication', 'plan', 'optimization', 'time', 'increase', 'sublinearly', 'network', 'bandwidth', 'rate', 'limit', 'compare', 'migration', 'time', 'elasticnotebook', 'dumpsession', 'migrate', 'variable', 'rerunall', 'report', 'result', 'elasticnotebook', 'balance', 'variable', 'storage', 'recomputation', 'ensure', 'always', 'least', 'fast', 'fast', 'dumpsession', 'rerunall', 'notably', 'elastic', 'notebook', 'adapt', 'relative', 'availability', 'network', 'bandwidth', 'compute', 'power', 'bandwidth', 'decrease', 'replication', 'plan', 'change', 'accordingly', 'migrate', 'variable', 'recomputation', 'rather', 'storage', 'example', 'stacking', 'notebook', 'regular', 'bandwidth', 'elastic', 'notebook', 'replication', 'plan', 'include', 'migrate', 'session', 'state', 'opt', 'recompute', 'certain', 'traintest', 'split', 'ie', 'cell', 'ytrain', 'yvalidation', 'modify', 'plan', 'recompute', 'instead', 'store', 'computationally', 'expensive', 'process', 'dataframe', 'cell', 'latestrecord', 'modify', 'plan', 'store', 'import', 'class', 'function', 'definition', 'xgbregressor', 'cell', 'recompute', 'rest', 'notebook', 'scale', 'complex', 'workload', 'section', 'test', 'scalability', 'session', 'replication', 'complex', 'notebook', 'session', 'large', 'number', 'cell', 'execution', 'reexecution', 'specifically', 'choose', 'tutorial', 'notebook', 'randomly', 'reexecute', 'cell', 'measure', 'size', 'elasticnotebook', 'ahg', 'optimization', 'time', 'compute', 'replication', 'plan', 'cell', 'twice', 'length', 'long', 'observe', 'notebook', 'report', 'result', 'fig', 'memory', 'consumption', 'ahg', 'exhibit', 'linear', 'scaling', 'number', 'cell', 'execution', 'reach', 'mb', 'cell', 'reexecution', 'negligible', 'compare', 'memory', 'consumption', 'note', 'book', 'session', 'gb', 'optimization', 'time', 'compute', 'replication', 'plan', 'similarly', 'exhibit', 'linear', 'scaling', 'reach', 'negligible', '150ms', 'cell', 'reexecution', 'elasticnote', 'book', 'choose', 'solve', 'mincut', 'fordfulkerson', 'time', 'complexity', '𝑂', 'number', 'edge', 'ahg', '𝑓', 'cost', 'optimal', 'replication', 'plan', 'former', 'scale', 'linearly', 'latter', 'largely', 'constant', 'relate', 'work', 'intermediate', 'result', 'reuse', 'datum', 'science', 'storage', 'termediate', 'result', 'explore', 'various', 'context', 'datum', 'science', 'incremental', 'feedforward', 'nature', 'task', 'allow', 'output', 'prior', 'operation', 'useful', 'speed', 'future', 'operation', 'example', 'include', 'cache', 'speed', 'model', 'training', 'replay', 'agnosis', 'cache', 'speed', 'anticipate', 'future', 'dataframe', 'operation', 'notebook', 'workflow', 'storage', 'cell', 'put', 'facilitate', 'graphical', 'exploration', 'notebook', 'tion', 'history', 'convenient', 'cell', 'rerun', 'relate', 'work', 'algorithmically', 'explore', 'efficient', 'way', 'compute', 'state', 'give', 'currently', 'store', 'item', 'compare', 'work', 'helix', 'similarly', 'feature', 'balance', 'loading', 'recomputation', 'model', 'lack', 'link', 'variable', 'constraint', 'result', 'silently', 'incorrect', 'replication', 'directly', 'apply', 'computational', 'notebook', 'problem', 'set', 'datalevel', 'session', 'replication', 'session', 'replication', 'base', 'platform', 'perform', 'serialization', 'librarie', 'exist', 'variety', 'checkpoint', 'tool', 'build', 'serialization', 'librarie', 'store', 'picklebased', 'interface', 'save', 'variable', 'keyvalue', 'store', 'however', 'break', 'object', 'reference', 'link', 'variable', 'serialize', 'separate', 'file', 'dillbase', 'dumpsession', 'correctly', 'resolve', 'reference', 'yet', 'still', 'fail', 'session', 'contain', 'unserializable', 'object', 'tensorflow', 'pytorch', 'offer', 'periodical', 'check', 'point', 'model', 'training', 'limit', 'object', 'library', 'native', 'checkpointing', 'mechanism', 'save', 'cell', 'metadata', 'often', 'fail', 'exactly', 'restore', 'session', 'common', 'presence', 'hide', 'state', 'compare', 'exist', 'datum', 'level', 'tool', 'session', 'replication', 'elasticnotebook', 'efficient', 'robust', 'application', 'history', 'graph', 'enable', 'balanc', 'ing', 'state', 'storage', 'recomputation', 'achieve', 'considerable', 'speedup', 'avoid', 'failure', 'unserializable', 'object', 'systemlevel', 'session', 'replication', 'session', 'replication', 'sim', 'ilarly', 'perform', 'use', 'systemlevel', 'tool', 'much', 'exist', 'work', 'applicable', 'tool', 'include', 'criu', 'recently', 'crum', 'crac', 'explore', 'extend', 'cuda', 'application', 'elsa', 'integrate', 'criu', 'enable', 'server', 'compare', 'elasticnotebook', 'system', 'level', 'tool', 'less', 'efficient', 'robust', 'large', 'memory', 'dump', 'size', 'limited', 'crossplatform', 'portability', 'respectively', 'pranav', 'rahul', 'park', 'lineage', 'trace', 'lineage', 'tracing', 'see', 'extensive', 'use', 'state', 'management', 'enable', 'recomputation', 'datum', 'efficient', 'storage', 'state', 'fault', 'tolerance', 'recently', 'usage', 'datum', 'lineage', 'computational', 'notebook', 'enable', 'multiversion', 'notebook', 'replay', 'recommend', 'notebook', 'interaction', 'create', 'reproducible', 'notebook', 'container', 'program', 'slicing', 'find', 'minimal', 'set', 'code', 'run', 'compute', 'certain', 'variable', 'work', 'adopt', 'lineage', 'trace', 'technique', 'capture', 'intervariable', 'dependency', 'application', 'history', 'graph', 'optimization', 'good', 'knowledge', 'exist', 'work', 'program', 'focus', 'capture', 'value', 'modification', 'equality', 'comparison', 'however', 'technique', 'additionally', 'identify', 'capture', 'strucal', 'change', 'graph', 'crucial', 'preserve', 'variable', 'alias', 'avoid', 'silent', 'error', 'state', 'replication', 'replicate', 'execution', 'environment', 'identical', 'execution', 'vironment', 'necessary', 'session', 'replication', 'different', 'machine', 'recent', 'work', 'explore', 'environment', 'repli', 'cation', 'notebook', 'containerize', 'input', 'file', 'mod', 'ule', 'useful', 'conjunction', 'elasticnotebook', 'consider', 'work', 'largely', 'orthogonal', 'notebook', 'parameterization', 'script', 'exist', 'work', 'execute', 'notebook', 'parameterized', 'form', 'systematic', 'experi', 'mentation', 'form', 'script', 'papermill', 'elasticnotebook', 'design', 'use', 'interactive', 'note', 'book', 'interface', 'similarly', 'applicable', 'migration', 'rameterize', 'notebook', 'execution', 'result', 'conclusion', 'work', 'propose', 'elasticnotebook', 'new', 'computa', 'tional', 'notebook', 'system', 'newly', 'offer', 'elastic', 'scaling', 'check', 'pointingrestoration', 'achieve', 'elasticnotebook', 'introduce', 'transparent', 'data', 'management', 'layer', 'user', 'interface', 'underlie', 'kernel', 'enable', 'robust', 'efficient', 'platform', 'independent', 'state', 'replication', 'notebook', 'session', 'core', 'con', 'tribution', 'include', 'lowoverhead', 'onthefly', 'application', 'history', 'construction', 'new', 'optimization', 'combine', 'copying', 'recomputation', 'variable', 'comprise', 'session', 'state', 'demonstrate', 'elasticnotebook', 'reduce', 'upscale', 'scaling', 'restoration', 'time', 'respectively', 'realworld', 'science', 'notebook', 'negligible', 'runtime', 'memory', 'overhead', 'respectively', 'future', 'plan', 'achieve', 'high', 'efficiency', 'usability', 'trace', 'state', 'change', 'fine', 'level', 'specifically', 'introduce', 'microcell', 'capture', 'code', 'block', 'cell', 'repeatedly', 'run', 'forloop', 'machine', 'learn', 'training', 'system', 'automatically', 'store', 'intermediate', 'model', 'meta', 'datum', 'enable', 'live', 'migration', 'checkpointingrestoration', 'longrunne', 'cell', 'execution', 'acknowledgment', 'author', 'grateful', 'kent', 'quanrud', 'assistance', 'derivation', 'reduction', 'employ', 'elasticnotebook', 'work', 'support', 'part', 'national', 'center', 'supercomputing', 'application', 'azure', 'elasticnotebook', 'enable', 'live', 'migration', 'computational', 'notebook', 'reference', 'naga', 'tanu', 'malik', 'reproducible', 'notebook', 'container', 'use', 'application', 'virtualization', 'ieee', '18th', 'international', 'conference', 'escience', 'escience', 'ieee', 'andreshg', 'nlp', 'glove', 'tfidf', 'lstm', 'explain', 'https', 'gene', 'cooperman', 'dmtcp', 'transparent', 'checkpointing', 'cluster', 'computation', 'desktop', 'ieee', 'tional', 'symposium', 'parallel', 'distribute', 'processing', 'ieee', 'azure', 'azure', 'studio', 'https', 'learnmicrosoftcoman', 'azure', 'azure', 'azuremicrosoft', 'anju', 'fault', 'tolerancechallenge', 'technique', 'implementation', 'international', 'journal', 'computer', 'science', 'issue', 'ijcsi', 'ekrem', 'store', 'sale', 'forecast', 'comprehensive', 'guide', 'https', 'mohammad', 'riyaz', 'belgaum', 'mad', 'cloud', 'service', 'rank', 'use', 'checkpointbase', 'load', 'balance', 'realtime', 'scheduling', 'cloud', 'computing', 'progress', 'advanced', 'computing', 'intelligent', 'engineering', 'springer', 'random', 'search', 'hyperparameter', 'optimization', 'journal', 'machine', 'learn', 'research', 'wondershaper', 'notebook', 'crumby', 'enough', 'replace', 'conference', 'innovative', 'datum', 'system', 'research', 'gang', 'deqe', 'zou', 'bing', 'bing', 'shelp', 'automatic', 'selfheale', 'multiple', 'application', 'instance', 'virtual', 'machine', 'environment', 'ieee', 'international', 'conference', 'cluster', 'computing', 'ieee', 'chhaya', 'choudhary', 'machine', 'learning', 'deep', 'learning', 'notebook', 'https', 'chhaya', 'choudhary', 'project', 'customer', 'churn', 'predic', 'tion', 'bokeh', 'contributor', 'bokeh', 'interaction', 'https', 'docsbokehorgen', 'iván', 'core', 'roberto', 'r', 'osorio', 'improve', 'scalability', 'applicationlevel', 'checkpointrecovery', 'reduce', 'checkpoint', 'size', 'new', 'generation', 'compute', 'criu', 'criu', 'invisible', 'file', 'https', 'criuorginvisiblefile', 'criu', 'https', 'criuorgmainpage', 'emanuel', 'carsten', 'binnig', 'interactive', 'analytic', 'pen', 'touch', 'proceeding', 'vldb', 'endowment', 'jupyterhubjupyterhubidleculler', 'https', 'cunha', 'real', 'silva', 'marco', 'netto', 'contextaware', 'execution', 'migration', 'tool', 'datum', 'notebook', 'hybrid', 'cloud', 'ieee', '17th', 'international', 'conference', 'escience', 'escience', 'ieee', 'developer', 'cuda', 'https', 'developernvidiacomcuda', 'toolkit', 'yve', 'frédéric', 'choli', 'optimization', 'cloud', 'task', 'process', 'checkpoint', 'restart', 'mechanism', 'proceeding', 'international', 'conference', 'high', 'formance', 'compute', 'network', 'storage', 'analysis', 'dimitreoliveira', 'model', 'stack', 'feature', 'engineering', 'https', 'engineeringandedanotebook', 'docker', 'nd', 'docker', 'documentation', 'swarm', 'mode', 'overview', 'https', 'doc', 'dockercomengineswarm', 'cody', 'dunne', 'graphtrail', 'analyze', 'large', 'multivariate', 'heterogeneous', 'network', 'support', 'exploration', 'history', 'proceeding', 'human', 'factor', 'computing', 'system', 'dwd', 'uncomplicatedfirewall', 'https', 'wikiubuntucom', 'uncomplicatedfirewall', 'philipp', 'eichmann', 'emanuel', 'carsten', 'binnig', 'idebench', 'benchmark', 'interactive', 'datum', 'exploration', 'proceeding', 'conference', 'management', 'datum', 'https', 'pytorch', 'lightning', 'ai', 'pytorch', 'modelcheckpoint', 'lrdr', 'fordfulkerson', 'flow', 'network', 'software', 'python', 'https', 'docspythonorg3', 'libraryasthtml', 'software', 'python', 'generator', 'https', 'wikipython', 'orgmoingenerator', 'software', 'https', 'docspythonorg3', 'libraryhashlibhtml', 'software', 'https', 'docspythonorg3', 'libraryjsonhtml', 'software', 'https', 'docspythonorg3', 'librarymarshalhtml', 'software', 'https', 'software', 'foundation', 'python', 'object', 'reduction', 'https', 'doc', 'pythonorg3librarypicklehtml', 'objectreduce', 'software', 'foundation', 'python', 'pickle', 'documentation', 'https', 'uncertainty', 'quantification', 'foundation', 'dill', 'pypi', 'https', 'pypiorg', 'projectdill', 'uncertainty', 'quantification', 'foundation', 'dill', 'dump', 'session', 'https', 'tian', 'watchpoint', 'https', 'pypiorgprojectwatchpoint', 'rolando', 'sreekanti', 'log', 'model', 'training', 'arxiv', 'preprint', 'garg', 'gene', 'crum', 'support', 'cuda', 'unified', 'memory', 'ieee', 'international', 'conference', 'cluster', 'computing', 'cluster', 'ieee', 'aurélien', 'geron', 'chapter', 'training', 'model', 'githubcomageron', 'aurélien', 'geron', 'machine', 'learn', 'notebook', 'edition', 'signup', 'tensorflow', 'checkpoint', 'https', 'wwwtensorfloworgguide', 'checkpoint', 'google', 'x', 'understand', 'code', 'https', 'burrito', 'wrap', 'lab', 'notebook', 'computational', 'infrastructure', 'haproxy', 'http', 'wwwhaproxyorg', 'detailed', 'https', 'wwwkagglecomcode', 'fred', 'hohman', 'druck', 'manage', 'mess', 'computational', 'notebook', 'proceeding', 'conference', 'human', 'factor', 'compute', 'system', 'technology', 'reactivity', 'graph', 'little', 'bit', 'magic', 'https', 'hextechbloghextwopointoh', 'topiccatalogjupyternotebook', 'studio', 'service', 'https', 'wwwibmcomdocsen', 'wwwkagglecom', 'kaggle', 'forum', 'product', 'feedback', 'https', 'wwwkagglecom', 'discussionsproductfeedback', 'notebook', 'specification', 'https', 'wwwkagglecom', 'docsnotebook', 'technicalspecification', 'space', 'telescope', 'jwst', 'datum', 'analysis', 'exam', 'ple', 'https', 'examplejupyternotebook', 'twinkle', 'jain', 'gene', 'cooperman', 'crac', 'architecture', 'cuda', 'stream', 'uvm', 'international', 'conference', 'high', 'performance', 'compute', 'network', 'storage', 'analysis', 'ieee', 'benefit', 'pitfall', 'notebook', 'classroom', 'proceeding', '21st', 'annual', 'conference', 'information', 'technol', 'ogy', 'education', 'project', 'notebook', 'stetzler', 'slater', 'checkpoint', 'restore', 'live', 'migration', 'science', 'platform', 'arxiv', 'preprint', 'dataflow', 'notebook', 'encoding', 'tracking', 'dependency', 'cell', '9th', 'usenix', 'workshop', 'theory', 'practice', 'provenance', 'tapp', 'machine', 'learn', 'notebook', 'multiclass', 'classification', 'https', 'githubcomkrasserm', 'kubernete', 'nd', 'kubernete', 'sfu', 'database', 'system', 'lab', 'dataprep', 'preparation', 'https', 'nteract', 'team', 'welcome', 'papermill', 'https', 'papermillreadthedocsio', 'dataprepai', 'enlat', 'lagator', 'arxiv', 'datum', 'processing', 'https', 'wwwkagglecomcode', 'development', 'team', 'ipython', 'interactive', 'computing', 'https', 'colinlagatorarxivdataprocesse', 'pranav', 'rahul', 'park', 'matei', 'shenker', 'tachyon', 'reliable', 'memory', 'speed', 'storage', 'cluster', 'computing', 'framework', 'proceeding', 'acm', 'symposium', 'zhile', 'frem', 'fast', 'restart', 'mechanism', 'general', 'checkpointrestart', 'ieee', 'comput', 'chroot', 'https', 'wikiarchlinuxorgtitlechroot', 'heer', 'effect', 'interactive', 'latency', 'exploratory', 'visual', 'analysis', 'ieee', 'transaction', 'visualization', 'computer', 'graphic', 'stephen', 'finegraine', 'lineage', 'safe', 'notebook', 'interaction', 'arxiv', 'preprint', 'naga', 'shilvi', 'tanu', 'malik', 'chaudhary', 'chex', 'multiversion', 'replay', 'order', 'checkpoint', 'arxiv', 'preprint', 'arxiv220208429', 'anjali', 'meshram', 'sambare', 'zade', 'fault', 'tolerance', 'model', 'reliable', 'cloud', 'international', 'journal', 'recent', 'innovation', 'trend', 'computing', 'communication', 'mongodb', 'bson', 'alexey', 'elibol', 'ray', 'distribute', 'framework', 'emerge', 'ai', 'application', '13th', 'usenix', 'symposium', 'operating', 'system', 'design', 'implementation', 'osdi', 'time', 'series', 'forecasting', 'prophet', 'https', 'wwwkaggle', 'stephen', 'scalable', 'dataframe', 'system', 'arxiv', 'preprint', 'arnab', 'benjamin', 'rath', 'matthias', 'boehm', 'finegraine', 'lineage', 'tracing', 'reuse', 'machine', 'learning', 'system', 'proceeding', 'international', 'conference', 'management', 'datum', 'braganholo', 'noworkflow', 'tool', 'collect', 'analyze', 'manage', 'provenance', 'script', 'proceeding', 'vldb', 'endowment', 'developer', 'freeze', 'https', 'pippypaioenstableclipip', 'freeze', 'shize', 'moneyball', 'proactive', 'autoscaling', 'sql', 'database', 'serverless', 'proceeding', 'vldb', 'endowment', 'pbc', 'posit', 'software', 'formerly', 'rstudio', 'posit', 'rstudio', 'https', 'posit', 'development', 'team', 'https', 'development', 'team', 'class', 'https', 'ipython', 'development', 'team', 'store', 'magic', 'https', 'ipython', 'development', 'team', 'matplotlib', 'https', 'matplotliborg', 'quoccuong', 'volker', 'survey', 'state', 'manage', 'ment', 'big', 'datum', 'processing', 'system', 'vldb', 'journal', 'university', 'cornell', 'virtual', 'workshop', 'tutorial', 'notebook', 'https', 'university', 'investigate', 'timeline', 'use', 'interactive', 'bokeh', 'scatterplot', 'https', 'university', 'sklearn', 'tweet', 'classification', 'https', 'university', 'twitter', 'network', 'https', 'githubcomcornellcac', 'trindade', 'matei', 'mistique', 'system', 'store', 'query', 'model', 'intermediate', 'model', 'diagnosis', 'proceeding', 'international', 'conference', 'management', 'datum', 'alexandre', 'verbitski', 'anurag', 'gupta', 'murali', 'brahmadesam', 'kamal', 'gupta', 'raman', 'mittal', 'sailesh', 'krishnamurthy', 'maurice', 'design', 'consideration', 'high', 'throughput', 'cloudnative', 'relational', 'database', 'proceeding', 'international', 'conference', 'management', 'datum', 'devlikamov', 'vlad', 'fast', 'workflow', 'use', 'scikitlearn', 'https', 'ericjan', 'wagenmaker', 'model', 'selection', 'use', 'akaike', 'weight', 'psychonomic', 'bulletin', 'review', 'wannipurage', 'suresh', 'marru', 'marlon', 'pierce', 'framework', 'capture', 'reproduce', 'absolute', 'state', 'arxiv', 'preprint', 'arxiv220407452', 'stephen', 'litian', 'song', 'helix', 'holistic', 'optimization', 'accelerate', 'iterative', 'machine', 'learn', 'arxiv', 'preprint', 'enhance', 'interactivity', 'dataframe', 'query', 'leverage', 'think', 'time', 'arxiv', 'preprint', 'arxiv210302145', 'xxhash', 'extremely', 'fast', 'noncryptographic', 'hash', 'co', 'prediction', 'https', 'www', 'yandex', 'opensource', 'gradient', 'boost', 'library', 'ai', 'bowl', 'offensive', 'play', 'rahul', 'agricultural', 'drought', 'prediction', 'https', 'wwwkagglecom', 'raj', 'https', 'amexdataset', 'web', 'service', 'aw', 'docsawsamazoncom', 'shahule', 'basic', 'cleaning', 'glove', 'wwwkagglecomcode', 'stephen', 'macke', 'chasin', 'compact', 'rapid', 'program', 'slicing', 'notebook', 'proceeding', 'vldb', 'endowment', 'shreyas', 'thorat30', 'plant', 'disease', 'classification', 'sdp', 'https', 'wwwkaggle', 'shtrauss', 'build', 'asset', 'trading', 'strategy', 'https', 'wwwkaggle', 'stelio', 'assure', 'automatic', 'software', 'selfheale', 'use', 'rescue', 'point', 'acm', 'sigarch', 'computer', 'architecture', 'news', 'stackoverflow', 'colab', 'session', 'timeout', 'https', 'stitchfix', 'nodebook', 'https', 'githubcomstitchfixnodebook', 'team', 'nbconvert', 'conversion', 'https', 'githubcomjupyternbconvert', 'matei', 'chowdhury', 'shenker', 'spark', 'cluster', 'computing', 'working', 'set', '2nd', 'usenix', 'workshop', 'hot', 'topic', 'hotcloud', 'emanuel', 'druck', 'icdata', 'datum', 'analysis', 'pen', 'touch', 'ieee', 'transaction', 'visualization', 'computer', 'graphic', 'materialization', 'optimization', 'feature', 'selection', 'workload', 'acm', 'transaction', 'database', 'system', 'tod', '1–32', 'a1', 'low', 'percell', 'overhead', 'report', 'result', 'percell', 'time', 'memory', 'overhead', 'homework', 'notebook', 'elasticnotebook', 'memory', 'percell', 'monitoring', 'overhead', 'consistently', '1ms', 'respectively', 'occasionally', 'spike', 'certain', 'cell', 'declaringmodifye', 'complex', 'variable', 'execute', 'example', '91ms', 'memory', 'time', 'overhead', 'cell', 'attribute', 'construct', 'graph', 'complex', 'nest', 'list', 'however', 'even', 'bad', 'case', 'time', 'overhead', 'still', 'elasticnotebook', 'enable', 'live', 'migration', 'computational', 'notebook', 'e', 'h', 'e', 'user', 'namespace', 'memory', 'usage', 'cell', 'execution', 'b', 'e', 'h', 'elasticnotebook', 'memory', 'usage', 'cell', 'execution', 'e', 'h', 'r', 'e', 'e', 'cell', 'execution', 'cell', 'execution', 'mem', 'overhead', 'overhead', 'c', 'overhead', 'percell', 'time', 'overhead', 'figure', 'runtime', 'memory', 'overhead', 'elasticnotebook', 'notebook', 'use', 'select', 'homework', 'notebook', 'memory', 'overhead', 'consistently', 'low', 'percell', 'runtime', 'overhead', 'negligible', 'cell', 'execution', 'rerunall', 'criu', 'store', 'dumpsession', 'elasticnotebook', 'sklearn', 'nlp', 'storesale', 'glove', 'trading', 'timeserie', 'stack', 'agriculture', 'lanl', 'hwlm', 'hwex3', 'notebook', 'n', 'r', 'e', 'r', 'e', 'figure', 'elasticnotebook', 'session', 'downscale', 'time', 'vm→d16a', 'exist', 'tool', 'time', 'normalize', 'wrt', 'rerunall', 'elasticnotebook', 'speed', 'migration', '200×', 'fast', 'next', 'good', 'alternative', 'arbitrary', 'variable', 'g', 'g∗', 'active', 'vss', 'g∗', 'respectively', '≥', 'g∗', 'due', 'falsely', 'imply', 'nonoverwrite', 'modification', 'fig', 'path', 'g', 'g∗', '𝑥', 'g', '𝑐𝑡g', '𝑡𝑘1', '𝑐𝑡𝑘1', '𝑡𝑘𝑙', '𝑐𝑡𝑘𝑙', 'g∗', 'g∗', 'g', '𝑡𝑘1', '𝑡𝑘𝑙', 'contain', 'false', 'nonoverwrite', 'modification', '𝑐𝑡𝑘1', '𝑐𝑡𝑘𝑙', 'fore', 'subtree', 'root', 'g', 'contain', 'subtree', 'root', 'g∗', 'g∗', 'hence', '𝑟𝑒𝑞∗', 'g∗', '𝑟𝑒𝑞', 'g', 'handle', 'large', 'panda', 'dataframe', 'avoid', 'hash', 'large', 'panda', 'dataframe', 'cell', 'execution', 'elasticnotebook', 'use', 'dataframe', 'underlie', 'writeable', 'flag', 'dirty', 'bit', 'detect', 'inplace', 'change', 'cell', 'execution', 'writeable', 'flag', 'set', 'false', 'dataframe', 'identify', 'modified', 'flag', 'flip', 'true', 'cell', 'execution', 'g', '𝒕1', '𝒄𝒕1', '𝒄𝒕2', '𝑐𝑡3', 'g∗', '𝑡1', '𝑐𝑡3', '𝒄𝒕4', '𝒄𝒕5', '𝒄𝒕4', '𝑐𝑡5', '𝑐𝑡2', '𝑟𝑒𝑞', '𝑐𝑡4', '𝑟𝑒𝑞', '𝑐𝑡4', '𝑐𝑡5', 'overwrittendelete', 'variable', 'snapshot', '𝑐𝑡1', 'cell', 'execution', '𝑡1', 'active', 'variable', 'snapshot', 'figure', 'ahg', 'contain', 'false', 'positive', 'compare', 'true', 'ahg', 'g∗', 'correctness', 'still', 'ensure', 'efficiency', 'affect', 'extra', 'cell', 'rerun', 'example', 'recompute', 'z', 'green', 'well', '500ms', 'threshold', 'suggest', 'interactive', 'datum', 'gine', 'memory', 'overhead', 'low', 'absolute', 'value', 'compare', 'size', 'yet', 'load', 'dataset', 'thus', 'negligible', 'user', 'impact', 'a2', 'proof', 'theorem', 'illustration', 'proof', 'provide', 'fig', 'proof', 'false', 'negative', 'true', 'ahg', 'g∗', 'taine', 'approximate', 'g∗', 'g', 'fig', 'let']",
ElasticNotebook: Enabling Live Migration for Computational Notebooks,"[{'href': 'http://arxiv.org/abs/2309.11083v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2309.11083v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-09-20 06:18:07,"Go Beyond Imagination: Maximizing Episodic Reachability with World Models

Yao Fu 1 Run Peng 1 Honglak Lee 1 2

3
2
0
2

g
u
A
5
2

]

G
L
.
s
c
[

1
v
1
6
6
3
1
.
8
0
3
2
:
v
i
X
r
a

Abstract
Efficient exploration is a challenging topic in rein-
forcement learning, especially for sparse reward
tasks. To deal with the reward sparsity, people
commonly apply intrinsic rewards to motivate
agents to explore the state space efficiently. In
this paper, we introduce a new intrinsic reward
design called GoBI - Go Beyond Imagination,
which combines the traditional lifelong novelty
motivation with an episodic intrinsic reward that
is designed to maximize the stepwise reachability
expansion. More specifically, we apply learned
world models to generate predicted future states
with random actions. States with more unique
predictions that are not in episodic memory are as-
signed high intrinsic rewards. Our method greatly
outperforms previous state-of-the-art methods on
12 of the most challenging Minigrid navigation
tasks and improves the sample efficiency on loco-
motion tasks from DeepMind Control Suite.

1. Introduction

Efficient exploration in state space is a fundamental chal-
lenge in reinforcement learning (RL) (Hazan et al., 2019;
Lee et al., 2019), especially when the environment rewards
are sparse (Mnih et al., 2013; 2016; Schulman et al., 2017)
or absent (Liu & Abbeel, 2021; Parisi et al., 2021). Such
reward sparsity makes RL algorithms easy to fail due to the
lack of useful signals for policy update (Riedmiller et al.,
2018; Florensa et al., 2018; Sekar et al., 2020). A com-
mon approach for exploration is to introduce self-motivated
intrinsic rewards such as state visitation counts (Strehl
& Littman, 2008; Kolter & Ng, 2009) and prediction er-
rors (Stadie et al., 2015; Pathak et al., 2017; Burda et al.,
2018). Most of these intrinsic reward designs measure life-
long state novelty and prioritize visiting states that are less

1University

of Michigan

2LG AI. Correspon-
Yao Fu <violetfy@umich.edu>, Run Peng
dence to:
<roihn@umich.edu>, Honglak Lee <honglak@eecs.umich.edu
& honglak@lgresearch.ai>.

Proceedings of the 40 th International Conference on Machine
Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright
2023 by the author(s).

1

visited starting from the beginning of training.

While the above methods achieves great improvement on
hard-exploration tasks like Montezumas Revenge (Burda
et al., 2018), they generally only work well on “single-
ton” environments, where training and evaluation environ-
ments are the same. However, due to the poor generalization
performance of reinforcement learning in unseen environ-
ments (Kirk et al., 2021), nowadays researchers have been
paying more attention on procedurally-generated environ-
ments (Cobbe et al., 2019; 2020; Flet-Berliac et al., 2021),
where the nature of task remains the same but the environ-
ment is randomly constructed for each new episode. For
example, a maze-like environment will have different maze
structures, making it rare for the agent to encounter the
same observations across different episodes. Therefore,
lifelong novelty intrinsic motivations usually fail in hard
procedurally-generated environments of this kind (Raileanu
& Rockt¨aschel, 2020; Zha et al., 2021) because an agent
will be trapped around newly-generated states.

Inspired by human’s frequent use of short-term memory (An-
dersen et al., 2006; Eichenbaum, 2017) to avoid repeatedly
visiting the same space, recent work propose to derive intrin-
sic rewards on episodic level (Savinov et al., 2018; Badia
et al., 2020; Raileanu & Rockt¨aschel, 2020; Zha et al., 2021;
Zhang et al., 2021). The episodic intrinsic rewards gener-
ally give bonus to large episodic-level state space visitation
coverage, therefore encourage visiting as many states as
possible in the same episode. However, does visiting more
states necessarily mean efficient episodic-level exploration?
We notice that some state visitations are unnecessary and
can be avoided if they are predictable from episodic mem-
ory. For example, when navigating through a house to find
a fridge, if you open a door and find an empty room, you
do not need to go into it anymore because you can easily
predict what the states are like in the room (i.e., intuitively
speaking, you would be moving around in an empty room).
With this inspiration, we propose to design the episodic in-
trinsic reward to not only maximize the number of visited
states in an episode, but also consider those states that are
not visited but can be predicted from episodic memory.

More precisely, we maintain an episodic buffer to store all
the visited states as well as states reachable from the visited
states within a few time steps. To get the reachable states,

 
 
 
 
 
 
Go Beyond Imagination: Maximizing Episodic Reachability with World Models

Figure 1. Illustration of how GoBI works on Minigrid. For the environment in the upper-left corner, the red triangle indicates the position
and orientation of the agent. It has a 7 × 7 partially-observable view (highlighted). During pre-training (Stage 1), we collect data using
a random policy to train a forward dynamics model ˆfϕ(panot, at) = ot+1, where panot denotes the panoramic view as is defined in
Section 3.1. For policy training (Stage 2), we apply ˆf to predict observations in the future k time steps with n random actions for each
step. We add the new ones to an episodic buffer M and take the change of size of M as the episodic intrinsic reward repi. The lifelong
intrinsic reward is COUNT-based. Our intrinsic reward GoBI is rlifelong ∗ repi.

we train a world model with forward dynamics function and
apply random actions to the learned dynamics model to pre-
dict future states. The predictions are added to the episodic
buffer if they are not there already. We use the change of
size of this episodic buffer as the episodic intrinsic reward.
Following many previous work, we weight the episodic in-
trinsic reward by a lifelong intrinsic reward (Badia et al.,
2020; Zhang et al., 2021) like the COUNT-based rewards.
With this newly proposed intrinsic reward design GoBI - Go
Beyond Imagination, the agent is expected to both explore
the most of the state space throughout training to discover
extrinsic rewards, and learn to act in an efficient manner
within a single episode to avoid being trapped by seemingly
novel states.

The contributions of this work can be highlighted as fol-
lows: (i) We propose a novel way to combine world models
with episodic memory to formulate an effective episodic
intrinsic reward design. (ii) In sparse-reward procedurally-
generated Minigrid environments (Chevalier-Boisvert et al.,
2023), GoBI greatly improves the training sample efficiency
in comparison with prior state-of-the-art intrinsic reward
functions. (iii) GoBI extends well to DeepMind Control
Suite (Tunyasuvunakool et al., 2020) with high-dimensional
visual inputs and shows promising results on sparse-reward
continuous control tasks. (iv) We analyze the design of
GoBI and present extensive ablations to show the contribu-
tion of each component.

2. Method

We consider reinforcement learning problems framed as
Markov Decision Process (MDP) M = (S, A, T, R, γ),
where S and A denote the state space and action space.
T : S × A × S → [0, 1] is the state transition function.
R : S × A × S → R is the reward function. γ is the
reward discount factor. At each step t, the state of the
environment is denoted as st ∈ S. The agent generates
an action at ∈ A to interact with the environment. The
environment then transits to the next underlying state st+1 ∈
S. Apart from the new state st+1, the environment also
returns an extrinsic reward rext that describes how well the
agent reacts to st. In sparse-reward tasks, rext is usually
0. In this work, we follow the previous work to train RL
algorithms with rext +λ∗rint
is a self-motivated
intrinsic reward and λ is a hyper-parameter that controls the
relative importance between intrinsic and extrinsic rewards.

, where rint

t

t

2.1. Go Beyond Imagination

Reachable States and Episodic Buffer Our intrinsic re-
ward design aims to exploit the information hidden inside
the neighbourhood of states. We define a state A to be k-
step reachable from state B if the agent can reach A from
B within k time steps. During the training process, for each
new episode, we initialize an empty episodic memory buffer
M. At time step t, we hash st as well as all the states
reachable from st. We denote the set containing all the hash

2

Environment

e.g. 🚶

Action Space

1. Left
2. Right
3. Forward
4. Pickup
5. Drop
6. Toggle
7. Done

⬅
➡
🚶
🤏
✋
🛠
✅

Encoder

Decoder

Policy Net

Conv2d

U
L
E
R
+
r
a
e
n
L

i

U
L
E
R
+
r
a
e
n
L

i

Episodic Memory Expansion

= 🚶

next obs             =  

Random 
Actions

k

n

Already in 

New state

Pre-train forward 
dynamics model

Stage 1

X

Count(             )

Episodic Reachability 
Maximization

Stage 2

 
 
 
 
Go Beyond Imagination: Maximizing Episodic Reachability with World Models

Figure 2. An illustration of how our episodic buffer updates. We consider reachable states that are k = 2 time steps away from st. After
the agent moves from s1 to s2, the episodic buffer M (shaded in green) expands by 3 new reachable states (shaded in yellow). More
formally, M ← M ∪ MR(s2), where MR(s2) indicates a set of all the states 2-step reachable from s2. Notice that all the states in the
trajectory, i.e., s0, s1, s2 are also added to the buffer.

codes of st and its reachable states as MR(st). Then we
update M by M ← M ∪ MR(st). Storing the hash codes
instead of directly storing the states may alleviate the poten-
tial memory issue of the buffer. We illustrate this process in
Figure 2. When the agent reaches state s2, we add 3 more
states that are reachable from s2 but not in M.

Forward Dynamics
In real environments, it is common
that we do not have access to the neighbourhood relationship
between states. However, we can learn a world model by
training a forward dynamics model ˆfϕ(st, at) = st+1 to
predict the states reachable from st. This forward dynamics
can be pre-trained using data collected by a random policy or
trained online together with policy training. When training
the policy, for each time step t, we generate k · n random
actions and use the learned dynamics ˆfϕ to predict states in
the future k steps. We hash the current state st as well as the
t+1, ..., ˆsn
predicted future states ˆs1
t+k and
add the hash codes to the episodic buffer M if they are not
in the buffer. Apart from alleviating potential memory issue
as is mentioned in the last paragraph, using a hash function
may also mitigate the noise introduced by ˆf . With a learned
dynamics model, the predictions of reachable states are
usually not perfect. However, in the experiment section we
show that even with imperfect predictions, our method can
improve the training sample efficiency a lot.

t+k, ..., ˆsn

t+1, ..., ˆs1

Episodic Novelty We aim to design an episodic-level nov-
elty reward that guides the agent to extend the frontier of its
predicted reachable space efficiently to discover states not
visited and not predictable within the same episode. More
specifically, we denote the size of the episodic buffer M
as mt at time step t and design a reachability-based bonus
repi = mt+1 − mt that encourages the agent to find unex-
plored regions. For each time step, the agent is expected to
reach the state that is reachable to more new states in the
current episode.

3

Intrinsic Reward Formulation We further weight our
episodic intrinsic reward by a lifelong intrinsic reward to
encourage the agent to explore the regions that are not well
explored in the past. More formally, the proposed intrinsic
reward GoBI is defined as:

t = (mt+1 − mt) ∗ rlifelong
rint

t

(1)

Here, rlifelong
denotes lifelong intrinsic reward. We note
t
that our framework is compatible with any choice of lifelong
intrinsic reward. Specifically, we use the simple COUNT-
based reward 1/(cid:112)N (st+1) for the navigation experiments
on Minigrid environments (Chevalier-Boisvert et al., 2023),
where N denotes the count of st+1 from the start of train-
ing.1 For the experiments on DeepMind Control Suite (Tun-
yasuvunakool et al., 2020) we use the state-of-the-art in-
trinsic reward RE3 (Seo et al., 2021), which estimates state
entropy by a random encoder.

Intrinsic Decay Intrinsic rewards are expected to be
asymptotically consistent so that it will not influence the
policy learning at later stage of training and result in a
sub-optimal policy. To guarantee that the policy learning
focuses more on extrinsic rewards as training proceeds, in
RE3 (Seo et al., 2021), the authors apply exponential decay
schedule for the intrinsic rewards to decrease over time. Al-
though COUNT-based reward theoretically converges to
0 with enough exploration, it decreases quite slowly in
procedurally-generated environments. Therefore, we also
apply intrinsic reward decay when calculating GoBI by de-
creasing the intrinsic reward coefficient λ during training.
We summarize our method in Algorithm 1 and illustrate the
training process on Minigrid navigation tasks in Figure 1.

1For environments that are partially observable (e.g., in Mini-
grid, the agent observes a 7×7 pixel local view of the environment),
we substitute state st with observation ot when calculating the in-
trinsic rewards.

t = 1

t = 2

Action: Move(S1, S2)

Episodic Buffer

Non-reachable Space

New Reachable Space

Reachable States from

Visited State

Reachable State

Non-reachable State

Trajectory

Observed Transition Path

Unreachable Transition Path

Go Beyond Imagination: Maximizing Episodic Reachability with World Models

Algorithm 1 Go Beyond Imagination

Input: Intrinsic Reward Coefficient λ0, Forward Pre-
diction Step k, Number of Random Actions n, Intrinsic
Reward Decay Parameter ρ
Initialize policy πθ, dynamics model ˆfϕ, replay buffer B.
(Optional) Collect episodes with πθ and train ˆfϕ with
prediction loss
for episode e = 1, 2, ... until convergence do

Initialize episodic buffer M.
λ ← λ0 ∗ (1 − ρ)(e−1)∗T
for t = 1 to T do

Execute πθ in the environment to get a transition
pair (st, at, st+1, rext
mt ← size(M)
M ← M ∪ {hash(st)})
for t′ = 1 to k do

).

t

gets a high intrinsic reward if the corresponding similarity
scores are low. Only with low enough similarity scores do
they add st to the buffer. Although their method and ours
are similar at high level, they are different by design. For ex-
ample, for an agent standing in front of an empty blind alley
with dead end, agent trained with GoBI does not benefit in
going deep into the blind alley because everything there can
be predicted as reachable and added to the episodic buffer
already. However, EC encourages going to the very end of
the blind alley to reach the state with low similarity score
and high intrinsic reward, even though going into an empty
blind alley is not beneficial for exploration and wastes time
that can be used to explore other parts of the environment.
In Appendix C, we present the visitation heatmaps of poli-
cies learned by EC and find that it prefers going to the room
corners, which well matches our explanation above.

t′, ..., an
t′

generate n random actions a1
M ← M ∪ {hash( ˆfϕ((cid:98)si

t+t′, ai

end for
t = (size(M) − mt) ∗ rlifelong
rint
B ← B ∪ {(st, at, st+1, rext

t + λ ∗ rint

t )}

t′)|i = 1, ..., n})

end for
update πθ with RL objective
update ˆfϕ with prediction loss

end for

2.2. Conceptual Advantage of GoBI over Prior Works

Previous works including RIDE (Raileanu & Rockt¨aschel,
2020) and NovelD (Zhang et al., 2021) also combine
episodic intrinsic reward with lifelong novelty as we do.
However, most of them focus on episodic-level state vis-
itation. For example, NovelD only assigns non-zero re-
wards to a state when it is visited for the first time in the
episode. However, we notice that not all state visitations
are necessary. The agent’s goal for exploration is to gather
information about the states. Therefore for states that are
easily predictable from episodic memory, visiting them may
not really help to acquire more information about the en-
vironment. In Figure 4, we plot the visitation heatmap of
GoBI and NovelD to demonstrate the different exploration
behaviours of the two methods.

Our method is closely related to another work that mea-
sures episodic curiosity (EC) (Savinov et al., 2018). In EC,
the authors train a reachability network that takes in two
arbitrary states and outputs a similarity score between 0
and 1, where 1 indicates the two states are the same and 0
indicates they are totally different. The network is trained
using collected episodes by marking temporally close states
as positive examples and temporally far ones as negative
samples. Meanwhile, they also maintain an episodic buffer.
A state st is compared with all the states in the buffer and

(a) MiniGrid

(b) Deepmind Control

Figure 3. Rendering of the environments used in this work. Left:
2D grid world navigation tasks that require object interactions.
Right: DeepMind Control tasks with visual observations.

3. Experiments

In this section, we evaluate GoBI in two domains: 2D
procedurally-generated Minigrid environments (Chevalier-
Boisvert et al., 2018) with hard-exploration tasks and lo-
comotion tasks from DeepMind Control Suite (Tunyasu-
vunakool et al., 2020). The experiments are designed to
answer the following research questions: (1) How does
GoBI perform against previous state-of-the-art intrinsic re-
ward designs in terms of training-time sample efficiency on
challenging procedurally-generated environments? (2) Can
GoBI successfully extend to complex continuous domains
with high-dimensional observations, for example control
tasks with visual observations? (3) How does each compo-
nent of our intrinsic reward contribute to the performance?
(4) What is the influence of the accuracy of the learned
world models to our method?

3.1. Minigrid Navigation Tasks

Minigrid Environments MiniGrid (Chevalier-Boisvert
et al., 2018) is a set of partially-observable procedurally-

4

Go Beyond Imagination: Maximizing Episodic Reachability with World Models

Figure 4. Visitation heatmaps on KeyCorridorS5R3 at different training stages. This figure compares the policy behaviour of GoBI and
NovelD. A dark red color means plentiful visitations, white means the agent has seen the space but did not step on it, and black means
space that are not discovered. It is worth noticing that early in the training (3.5M and 7M time steps), our policy already learns not to go
into an empty room, likely because states in an empty room are easily predictable. On the contrary, even after 11M steps, an agent trained
with NovelD still goes into an empty room (bottom-right corner) for more state visitations.

generated grid world navigation tasks. The agent is expected
to interact with objects such as keys, balls, doors, and boxes
to navigate through rooms and find the goal that is randomly
placed in one of the rooms. The tasks only provide one
sparse reward at the end of each episode, which indicates if
the agent successfully finds the goal or not and how many
steps it takes to reach the goal. In this work, we consider
3 types of tasks including MultiRoom, KeyCorridor, and
ObstructedMaze. Some environments that we experiment
on in this paper are shown in Figure 3a. The upper-right is
a KeyCorridor-S4R3 environment, where the agent should
learn to open the doors to find a key, use it to open the locked
blue door, and pick up the green ball. The bottom-left fig-
ure shows an ObstructedMaze-Full environment, which is
similar to KeyCorridor but more challenging. The rooms
are larger, the doors are blocked by balls, and the keys are
hidden in boxes. The upper-left and bottom-right environ-
ments are MultiRoom environments, in which the agent has
to navigate through connected rooms to reach the goal in
the last room.

Baselines We compare with state-of-the-art intrinsic re-
ward designs that work well on Minigrid including Nov-
elD (Zhang et al., 2021), RIDE (Raileanu & Rockt¨aschel,
2020), and RND (Burda et al., 2018). For a fair compari-
son, we follow the same basic RL algorithm and network
architectures used in the official codebase of NovelD and
only change the intrinsic rewards rint for all the methods.
We also compare our method with EC (Savinov et al., 2018)
because of the similarity of the high-level idea between
the two methods. However, the original paper of EC does
not include experiments on Minigrid. Therefore, we im-
plement our own version to adapt to Minigrid. We follow
their implementation suggestions in the paper and tune the
hyper-parameters such as novelty threshold by grid search.

Dynamics Model Training For each experiment on Min-
igrid, we first run a random policy for 1e5 steps to collect
data and use them to train a forward dynamics model as the
world model. Among the pairs collected, there are about
5e4 different transition pairs. During our experiments, we
observe that fine-tuning the pre-trained dynamics model
during policy training has no significant influence on the
performance. Similar to (Parisi et al., 2021), we use the
360◦ panoramic views as the input to predict the future
observations. This is a rotation-invariant representation of
the observed state. We consider this still a fair compari-
son with the previous state-of-the-arts because both NovelD
and RIDE rely on using the state information instead of
observations for the episodic count calculation.

Due to the limited field of view of the agent, we only forward
the learned dynamics by k = 1 step when predicting. We
predict the next observations produced by all 7 discrete
actions in the Minigrid tasks including turn left, turn right,
forward, toggle, pick up, drop, and done. We directly apply
the default Python hashing function to hash the observations
and predicted future observations. We do not expect the
hashing function to mitigate the prediction error on Minigrid,
but only use it to reduce the dimension of observations and
predictions.

Training Performance on Minigrid Figure 5 shows
the learning curves of GoBI and state-of-the-art explo-
ration baselines NovelD, RIDE, RND, and EC on 12 most
challenging Minigrid navigation tasks, including Multi-
Room, KeyCorridor, and ObstructedMaze. Our curves
are shifted towards right by the number of random explo-
ration environment steps used to train the world model.
In all 12 environments, GoBI significantly outperforms
previous methods in terms of sample efficiency. For in-

5

GoBI(Ours)

One episode of Training Environment

same

s
t
n
u
o
C
d
e
t
i
s
V

i

Unlock

3.5M

3.5M

7M

7M

11M

11M

Training Steps

21M

21M

Pick

Minigrid-KeyCorridor-S5R3-v0

NovelD

Unseen

Seen

 
Go Beyond Imagination: Maximizing Episodic Reachability with World Models

Figure 5. Training performance of GoBI and the baselines on 12 MiniGrid environments. The x-axis shows the number of environment
steps. We shift the training curves towards right by the number of environment steps we use to pre-train the dynamics model, i.e. 1e5 time
steps. Results are averaged across 4 seeds.

stance, on ObstructedMaze-2Dlhb, GoBI is about three
times more sample efficient than NovelD. On the hard-
est ObstructedMaze-Full environment, GoBI achieves near-
optimal performance within 70M steps. Lastly, although we
try to tune the hyper-parameters of EC, our implementation
of EC still does not learn well on the Minigrid environments.

Qualitative Results To clearly present the exploration be-
havior learned by GoBI, we show the visitation heatmaps
of GoBI and NovelD on a KeyCorridorS5R3 environment
in Figure 4. Not only does our method converge to an opti-
mal policy faster, the exploration behaviour is very different
from NovelD. GoBI quickly learns not to visit easily pre-
dictable states like an empty room, making it more efficient
to explore interesting parts of the environment, for example,
the room with a key in it.

Dynamics Model Training We follow the world model
structure in Dreamer (Hafner et al., 2019) and directly apply
their encoder, transition model, and observation model to
predict future observations. However, compared to Mini-
grid, it requires way more data to train a decent dynamics
model on DeepMind Control to generate visually-reasonable
predictions. Therefore, different from the experiments on
Minigrid, we do not pre-train the dynamics models. Instead
we train the dynamics model together with the policy as is
shown in Algorithm 1. We find that the number of sampled
random actions n = 5 works well across all 4 environments.
For the number of forward prediction steps k, we set it to be
3 for Pendulum Swingup and 1 for the other 3 environments.

For the hashing function, we find that a simple SimHash as is
suggested in (Tang et al., 2017) works well in capturing the
similarities between similar observations. We use SimHash
to hash the image observations to 50 bits.

3.2. Experiments on Control Tasks

We further test GoBI on DeepMind Control Suite, which
are a set of image-based continuous control tasks. These
tasks are more challenging than Minigrid because of its high-
dimensional observations and stochastic transitions. Notice
that these environments are not procedurally-generated. The
experiments in this section are to show the generality of
our method by experimentally showing that GoBI extends
well to sparse-reward tasks with continuous action space
and high-dimensional observation space.

Training Performance on DeepMind Control We com-
pare with the state-of-the-art intrinsic motivation on Deepm-
Mind Control tasks - RE3 (Seo et al., 2021), which applies a
k-nearest neighbor entropy estimator in the low-dimensional
representation space of a randomly initialized encoder to
maximize state entropy. RE3 is also what we use for the life-
long intrinsic reward part rlifelong of GoBI in Eq 1. Another
two intrinsic reward baselines we consider are ICM (Pathak
et al., 2017) and RND (Burda et al., 2018). For a fair com-
parison, all the experiments use the same basic RL algorithm

6

MultiRoom-N6

MultiRoom-N7-S8

MultiRoom-N12-S10

KeyCorridorS3R3

0.6

0.4

0.2

0.0

0.6

0.4

0.2

0.0

0.8

0.6

0.4

0.2

0.0

0.0

0.2

0.3

0.5

0.6

0.8

0.0

0.1

0.3

0.5

0.6

0.8

0.0

0.2

0.4

0.6

0.8

0.9

0.0

0.2

0.5

0.7

1.0

1.2

KeyCorridorS4R3

KeyCorridorS5R3

KeyCorridorS6R3

ObstructedMaze-2Dlh

1e7

0.8

0.6

0.4

0.2

0.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

0.0

0.2

0.5

0.7

1.0

1.2

0.0

0.3

0.6

0.9

1.2

1.5

0.0

0.4

0.8

1.2

1.6

2.0

0.0

0.4

0.8

1.3

1.7

2.1

ObstructedMaze-2Dlhb

ObstructedMaze-1Q

ObstructedMaze-2Q

ObstructedMaze-Full

1e7

0.8

0.6

0.4

0.2

0.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

n
r
u

t

e
R
e
g
a
r
e
v
A

0.6

0.4

0.2

0.0

n
r
u

t

e
R
e
g
a
r
e
v
A

0.8

0.6

0.4

0.2

0.0

n
r
u

t

e
R
e
g
a
r
e
v
A

0.8

0.6

0.4

0.2

0.0

0.0

0.8

1.6

2.4

3.2

4.0

0.0

0.6

1.3

1.9

2.6

3.2

0.0

0.9

1.7

2.6

3.4

4.3

0.0

2.0

4.0

6.0

8.0

10.0

NovelD

RIDE

EC

RND

Ours (GoBI)

1e7

 
 
 
Go Beyond Imagination: Maximizing Episodic Reachability with World Models

episodes than MultiRoom (all generated rooms are squares
with fixed sizes), therefore the COUNT-based rewards con-
tribute more in such environments than in MultiRoom. At
the same time, R2 performs way worse than GoBI. Agents
trained with R2 prefer actions that only increase the size of
the episodic buffer a bit therefore getting positive score more
often. We provide an illustrative example in Appendix F
to explain why R2 does not work well compared to GoBI.
Using only lifelong intrinsic reward R3 performs the worst
and struggles to learn efficiently on large Multiroom, Key
Corridor, and Obstructed Maze environments.

Real Dynamics vs Learned Dynamics A learned dynam-
ics model is generally not perfect, especially for partially-
observable environments like Minigrid. In many cases the
predictions can never be accurate. For example, when the
agent first opens the door of a new room, usually it will
not accurately predict everything behind the door. Figure 8
shows the training curves between using the real dynamics
model vs a learned dynamics model. Not surprisingly, with
the same intrinsic reward function, using the real dynamics
converges faster to a near-optimal policy. However, even
with imperfect dynamics model, our method still greatly
surpasses previous state-of-the-arts.

Figure 8. Comparison between using the real dynamics model of
the environments vs using a learned one on Minigrid environments.
In both MultiRoom and KeyCorridor, using a real dynamics model
to derive intrinsic reward makes the policy converge faster, espe-
cially on KeyCorridor.

Multi-Step Predictions Figure 9 shows the learning per-
formance of GoBI on Minigrid with a varying choices of
the number of future steps to do predictions k = 1, 2, 3. For
k > 1, our dynamics model outputs panot+1 instead of ot+1
and we hash and store the observations from panoramas in
each future time step. With a real forward dynamics model,
a larger k generally accelerates exploration more, because it
prioritizes actions that lead to the states that are reachable
to more states in the long run. However, due to the limited
field of view of the agent and the model inaccuracy, this is
not the case if we use a learned model. Forwarding 2 steps
is still faster than only 1 step, but more steps than that does
not really make exploration faster.

Figure 6. Training curves of GoBI and the baselines on DeepMind
Control Suite. The curves are averaged across 5 seeds.

RAD (Laskin et al., 2020). The results are shown in Figure
6. The additional episodic-level intrinsic reward term im-
proves the sample efficiency a lot compared to only using
lifelong intrinsic reward, especially on Hopper Hop and
Walker Run Sparse.

3.3. Ablation Study

GoBI Variations
In this section, we analyze how each
component of our intrinsic reward contributes to the final
performance. We ablate each component of GoBI and run
experiments on Minigrid environments with the following:

• R1: only episodic intrinsic reward mt+1 − mt
• R2: indicator of whether new states are added to the
episodic buffer (1{mt+1 − mt > 0})/(cid:112)N (ot+1)

• R3: only lifelong intrinsic reward 1/(cid:112)N (ot+1)

Figure 7. Training performance comparison among GoBI, R1, R2,
and R3 on 3 Minigrid environments.

Training performance of GoBI as well as R1, R2, and R3 are
shown in Figure 7. Although R1 works on MultiRoom, it
suffers on Obstructed Maze and large KeyCorridor environ-
ments. The underlying reason may be that in Key Corridor
and Obstructed Maze the room structures change less across

7

n
r
u
t
e
R
e
d
o
s
p
E

i

800

600

400

200

0

n
r
u
t
e
R
e
d
o
s
p
E

i

250

200

150

100

50

0

Pendulum Swingup

Cartpole Swingup Sparse

800

600

400

200

0

0.0

0.2

0.4

0.7

0.9

1.1

0.0

0.5

1.0

1.6

2.1

2.6

Hopper Hop

Walker Run Sparse

1e5

1e5

400

300

200

100

0

0.0

1.0

2.0

3.0

4.0

5.0

0.0

1.0

2.0

3.0

4.0

5.0

RAD+ICM

RAD+RND

RAD+RE3

RAD

1e5

1e5

Ours

 
 
MultiRoom-N12-S10

KeyCorridorS6R3

ObstructedMaze-2Dlh

n
r
u
t
e
R
e
g
a
r
e
v
A

0.6

0.5

0.4

0.3

0.2

0.1

0.0

0.8

0.6

0.4

0.2

0.0

0.8

0.6

0.4

0.2

0.0

0.0

0.2

0.3

0.5

0.6

0.8

0.0

0.4

0.7

1.1

1.4

1.8

0.0

0.2

0.4

0.6

0.8

1.0

R1

R2

R3

Ours

1e7

 
MultiRoom-N7-S8

KeyCorridorS3R3

t

n
r
u
e
R
e
g
a
r
e
v
A

0.6

0.4

0.2

0.0

0.8

0.6

0.4

0.2

0.0

0.0

0.1

0.2

0.4

0.5

0.6

0.0

0.1

0.2

0.3

0.4

0.5

real dynamics

learned dynamics

1e7

 
Go Beyond Imagination: Maximizing Episodic Reachability with World Models

Rockt¨aschel, 2020) and NovelD (Zhang et al., 2021) both
count the episodic state visitations, while we claim that apart
from visited states, we should also consider states that can
be predicted from short-term episodic memory.

4.3. Learning World Models with Forward Dynamics
Learning dynamics function from a set of observed data is a
widely-studied topic in reinforcement learning, especially
due to the rapid growth of model-based reinforcement learn-
ing (Wang et al., 2019). Existing work show that an agent’s
world model is implicitly a forward model that predict future
states (Ha & Schmidhuber, 2018a; Freeman et al., 2019).
Recently, people have proposed latent dynamics models
that work well on high-dimensional inputs (Okada et al.,
2020). These latent dynamics models encode image obser-
vations and predict future states in the latent space (Ha &
Schmidhuber, 2018b; Hafner et al., 2019; 2023), outputting
realistic future observations on visually complex domains
including DeepMind Control Suite (Tunyasuvunakool et al.,
2020), VizDoom (Kempka et al., 2016), Atari Games, and
DeepMind Lab (Beattie et al., 2016). The learned dynamics
models can be used to guide exploration by prediction error
(Stadie et al., 2015; Pathak et al., 2017; Burda et al., 2018),
surprise (Achiam & Sastry, 2017), or information gain by
variance of model ensemble means (Sekar et al., 2020). Our
method differ from the previous methods by directly gener-
ating and hashing the predicted states and add them to an
episodic reachable state buffer. With the advanced world
model structures, our method can be extended to diverse
domains with complex observations.

5. Discussions and Future Work

This paper shows an effective way to combine learned world
models with episodic memory to intrinsically guide efficient
exploration. Our method achieves state-of-the-art perfor-
mance on procedurally-generated hard exploration tasks
and also works well on singleton continuous control do-
mains. However, it still has certain limitations. First of all,
the dynamics model we use for the Minigrid experiments
is deterministic, making it possible to generate less accu-
rate predictions and making the performance of our method
worse than using the real dynamics. A possible way to make
improvement on this is to make the prediction model gen-
erative and sample possible future states. Secondly, for the
control tasks with complex visual inputs, we hash the im-
ages with static hashing to make them discrete hash codes.
However, to better capture the semantic similarities between
the image observations, it would be beneficial to learn hash
functions, for example, by using an autoencoder (AE) to
learn meaningful hash codes (Tang et al., 2017). We leave
these investigations as future work.

Figure 9. We make forward predictions for different number of
future steps k using both the real dynamics and the learned dy-
namics model. The plots above show the training performance on
Minigrid MultiRoom-N7-S8.

4. Related Work

4.1. Exploration in Reinforcement Learning
Efficient exploration in reinforcement learning, especially
for sparse-reward reinforcement learning problems is chal-
lenging. A natural and popular solution is to design some
metric to evaluate state novelty and assign high intrinsic re-
ward to novel states. For example, COUNT-based intrinsic
reward (Strehl & Littman, 2008; Kolter & Ng, 2009; Tang
et al., 2017) and curiosity-based intrinsic motivation (Stadie
et al., 2015; Pathak et al., 2017; Burda et al., 2018). An-
other popular way is to do state space entropy maximization
(Hazan et al., 2019; Lee et al., 2019). Recently, nearest
neighbor entropy estimation methods (Yarats et al., 2021a;
Liu & Abbeel, 2021) have shown great performance im-
provements in challenging visual domains. Our method is
compatible with all these successful exploration intrinsic re-
ward designs by using them as rlifelong, but we additionally
encourage the episodic-level reachable space expansion to
achieve large state space coverage within a single episode.

4.2. Episodic Memory
Deriving useful information from episodic buffer have
shown great success in improving the training sample ef-
ficiency in RL on navigation, control, and Atari games.
Episodic memory buffers are applied to mimic hippocampal
episodic control and rapidly assimilate recent experience
(Blundell et al., 2016; Pritzel et al., 2017). As is men-
tioned in the previous sections, (Savinov et al., 2018) keeps
an episodic buffer to store observations and introduce an
episodic curiosity module to determine if a new observation
is reachable from previous observations or not. RAPID (Zha
et al., 2021) proposes a novel way to do behaviour cloning
on episodes with high episodic coverage. NGU (Badia et al.,
2020) combines an episodic novelty module and a lifelong
novelty module to generate intrinsic rewards. However,
in NGU, the episodic novelty is a measurement of differ-
ence between the current observations from the previous
observations, while ours focus on how much the reachable
space is expanded from the new state. RIDE (Raileanu &

8

real dynamics

learned dynamics

n
r
u
t
e
R
e
g
a
r
e
v
A

0.6

0.4

0.2

0.0

0.6

0.4

0.2

0.0

0.0

0.2

0.3

0.5

0.6

0.8

0.0

0.2

0.3

0.5

0.6

0.8

1e7

k=1

k=2

k=3

1e7

 
Go Beyond Imagination: Maximizing Episodic Reachability with World Models

6. Conclusion
In this work, we introduce Go Beyond Imagination- GoBI,
a novel episodic intrinsic reward design that encourages
efficient episodic-level exploration by expanding reachable
space. While most previous episodic intrinsic rewards use a
naive episodic state count or state visitation coverage, our
method exploits learned world models to predict reachable
states and motivates the agent to seek for the states with
more unexplored neighbors. Combined with lifelong intrin-
sic rewards, our method shows great training time sample
efficiency improvement on hard procedurally-generated en-
vironments. At the same time, it can be extended to guide
exploration on continuous control tasks with visual inputs,
both indicating a promising future in this direction.

7. Acknowledgments

This work was supported in part by grants from LG AI Re-
search, NSF IIS 1453651, and NSF FW-HTF-R 2128623.

References

Achiam, J. and Sastry, S. Surprise-based intrinsic moti-
vation for deep reinforcement learning. arXiv preprint
arXiv:1703.01732, 2017.

Andersen, P., Morris, R., Amaral, D., Bliss, T., and O’Keefe,
J. The hippocampus book. Oxford university press, 2006.

Badia, A. P., Sprechmann, P., Vitvitskyi, A., Guo, D., Piot,
B., Kapturowski, S., Tieleman, O., Arjovsky, M., Pritzel,
A., Bolt, A., et al. Never give up: Learning directed
exploration strategies. arXiv preprint arXiv:2002.06038,
2020.

Beattie, C., Leibo, J. Z., Teplyashin, D., Ward, T., Wain-
wright, M., K¨uttler, H., Lefrancq, A., Green, S., Vald´es,
V., Sadik, A., et al. Deepmind lab. arXiv preprint
arXiv:1612.03801, 2016.

Blundell, C., Uria, B., Pritzel, A., Li, Y., Ruderman,
A., Leibo, J. Z., Rae, J., Wierstra, D., and Hass-
abis, D. Model-free episodic control. arXiv preprint
arXiv:1606.04460, 2016.

Burda, Y., Edwards, H., Storkey, A., and Klimov, O. Ex-
ploration by random network distillation. arXiv preprint
arXiv:1810.12894, 2018.

Chevalier-Boisvert, M., Willems, L., and Pal, S. Minimalis-
tic gridworld environment for openai gym. https://
github.com/maximecb/gym-minigrid, 2018.

Chevalier-Boisvert, M., Dai, B., Towers, M., de Lazcano,
R., Willems, L., Lahlou, S., Pal, S., Castro, P. S., and
Terry, J. Minigrid & miniworld: Modular & customizable

reinforcement learning environments for goal-oriented
tasks. CoRR, abs/2306.13831, 2023.

Cobbe, K., Klimov, O., Hesse, C., Kim, T., and Schulman,
J. Quantifying generalization in reinforcement learning.
In International Conference on Machine Learning, pp.
1282–1289. PMLR, 2019.

Cobbe, K., Hesse, C., Hilton, J., and Schulman, J. Lever-
aging procedural generation to benchmark reinforcement
learning. In International conference on machine learn-
ing, pp. 2048–2056. PMLR, 2020.

Eichenbaum, H. The role of the hippocampus in navigation
is memory. Journal of neurophysiology, 117(4):1785–
1796, 2017.

Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih,
V., Ward, T., Doron, Y., Firoiu, V., Harley, T., Dunning,
I., et al. Impala: Scalable distributed deep-rl with im-
portance weighted actor-learner architectures. In Interna-
tional conference on machine learning, pp. 1407–1416.
PMLR, 2018.

Flet-Berliac, Y., Ferret, J., Pietquin, O., Preux, P., and Geist,
M. Adversarially guided actor-critic. arXiv preprint
arXiv:2102.04376, 2021.

Florensa, C., Held, D., Geng, X., and Abbeel, P. Automatic
goal generation for reinforcement learning agents.
In
International conference on machine learning, pp. 1515–
1528. PMLR, 2018.

Freeman, D., Ha, D., and Metz, L. Learning to predict
without looking ahead: World models without forward
prediction. Advances in Neural Information Processing
Systems, 32, 2019.

Ha, D. and Schmidhuber, J. Recurrent world models facil-
itate policy evolution. Advances in neural information
processing systems, 31, 2018a.

Ha, D. and Schmidhuber, J. World models. arXiv preprint

arXiv:1803.10122, 2018b.

Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. Dream to
control: Learning behaviors by latent imagination. arXiv
preprint arXiv:1912.01603, 2019.

Hafner, D., Pasukonis, J., Ba, J., and Lillicrap, T. Mastering
diverse domains through world models. arXiv preprint
arXiv:2301.04104, 2023.

Hazan, E., Kakade, S., Singh, K., and Van Soest, A. Prov-
ably efficient maximum entropy exploration. In Interna-
tional Conference on Machine Learning, pp. 2681–2691.
PMLR, 2019.

9

Go Beyond Imagination: Maximizing Episodic Reachability with World Models

Kempka, M., Wydmuch, M., Runc, G., Toczek, J., and
Ja´skowski, W. Vizdoom: A doom-based ai research plat-
form for visual reinforcement learning. In 2016 IEEE con-
ference on computational intelligence and games (CIG),
pp. 1–8. IEEE, 2016.

Kirk, R., Zhang, A., Grefenstette, E., and Rockt¨aschel, T. A
survey of generalisation in deep reinforcement learning.
arXiv preprint arXiv:2111.09794, 2021.

Kolter, J. Z. and Ng, A. Y. Near-bayesian exploration in
In Proceedings of the 26th annual
polynomial time.
international conference on machine learning, pp. 513–
520, 2009.

Laskin, M., Lee, K., Stooke, A., Pinto, L., Abbeel, P., and
Srinivas, A. Reinforcement learning with augmented data.
Advances in neural information processing systems, 33:
19884–19895, 2020.

Lee, L., Eysenbach, B., Parisotto, E., Xing, E., Levine,
S., and Salakhutdinov, R. Efficient exploration via state
marginal matching. arXiv preprint arXiv:1906.05274,
2019.

Liu, H. and Abbeel, P. Behavior from the void: Unsuper-
vised active pre-training. Advances in Neural Information
Processing Systems, 34:18459–18473, 2021.

Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,
Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing
atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.

Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,
T., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-
chronous methods for deep reinforcement learning. In
International conference on machine learning, pp. 1928–
1937. PMLR, 2016.

Okada, M., Kosaka, N., and Taniguchi, T. Planet of the
bayesians: Reconsidering and improving deep planning
network by incorporating bayesian inference. In 2020
IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), pp. 5611–5618. IEEE, 2020.

Parisi, S., Dean, V., Pathak, D., and Gupta, A. Interesting
object, curious agent: Learning task-agnostic exploration.
Advances in Neural Information Processing Systems, 34,
2021.

Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T.
Curiosity-driven exploration by self-supervised predic-
tion. In International conference on machine learning,
pp. 2778–2787. PMLR, 2017.

Pritzel, A., Uria, B., Srinivasan, S., Badia, A. P., Vinyals,
O., Hassabis, D., Wierstra, D., and Blundell, C. Neural

episodic control. In International Conference on Machine
Learning, pp. 2827–2836. PMLR, 2017.

Raileanu, R. and Rockt¨aschel, T. Ride: Rewarding impact-
driven exploration for procedurally-generated environ-
ments. arXiv preprint arXiv:2002.12292, 2020.

Riedmiller, M., Hafner, R., Lampe, T., Neunert, M., De-
grave, J., Wiele, T., Mnih, V., Heess, N., and Springen-
berg, J. T. Learning by playing solving sparse reward
tasks from scratch. In International conference on ma-
chine learning, pp. 4344–4353. PMLR, 2018.

Savinov, N., Raichuk, A., Marinier, R., Vincent, D., Polle-
feys, M., Lillicrap, T., and Gelly, S. Episodic curiosity
through reachability. arXiv preprint arXiv:1810.02274,
2018.

Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
Klimov, O. Proximal policy optimization algorithms.
arXiv preprint arXiv:1707.06347, 2017.

Sekar, R., Rybkin, O., Daniilidis, K., Abbeel, P., Hafner, D.,
and Pathak, D. Planning to explore via self-supervised
world models. In International Conference on Machine
Learning, pp. 8583–8592. PMLR, 2020.

Seo, Y., Chen, L., Shin, J., Lee, H., Abbeel, P., and Lee,
K. State entropy maximization with random encoders
for efficient exploration.
In Meila, M. and Zhang, T.
(eds.), Proceedings of the 38th International Conference
on Machine Learning, volume 139 of Proceedings of
Machine Learning Research, pp. 9443–9454. PMLR, 18–
24 Jul 2021. URL https://proceedings.mlr.
press/v139/seo21a.html.

Stadie, B. C., Levine, S., and Abbeel, P. Incentivizing ex-
ploration in reinforcement learning with deep predictive
models. arXiv preprint arXiv:1507.00814, 2015.

Strehl, A. L. and Littman, M. L. An analysis of model-
based interval estimation for markov decision processes.
Journal of Computer and System Sciences, 74(8):1309–
1331, 2008.

Tang, H., Houthooft, R., Foote, D., Stooke, A., Xi Chen, O.,
Duan, Y., Schulman, J., DeTurck, F., and Abbeel, P. #
exploration: A study of count-based exploration for deep
reinforcement learning. Advances in neural information
processing systems, 30, 2017.

Tunyasuvunakool, S., Muldal, A., Doron, Y., Liu, S., Bohez,
S., Merel, J., Erez, T., Lillicrap, T., Heess, N., and Tassa,
Y. dm control: Software and tasks for continuous control.
Software Impacts, 6:100022, 2020.
ISSN 2665-9638.
https://doi.org/10.1016/j.simpa.2020.100022.
doi:
https://www.sciencedirect.com/
URL
science/article/pii/S2665963820300099.

10

Go Beyond Imagination: Maximizing Episodic Reachability with World Models

Wang, T., Bao, X., Clavera, I., Hoang, J., Wen, Y., Lan-
glois, E., Zhang, S., Zhang, G., Abbeel, P., and Ba,
J. Benchmarking model-based reinforcement learning.
arXiv preprint arXiv:1907.02057, 2019.

Yarats, D., Fergus, R., Lazaric, A., and Pinto, L. Reinforce-
ment learning with prototypical representations. In Inter-
national Conference on Machine Learning, pp. 11920–
11931. PMLR, 2021a.

Yarats, D., Zhang, A., Kostrikov, I., Amos, B., Pineau, J.,
and Fergus, R. Improving sample efficiency in model-
free reinforcement learning from images. In Proceedings
of the AAAI Conference on Artificial Intelligence, vol-
ume 35, pp. 10674–10681, 2021b.

Zha, D., Ma, W., Yuan, L., Hu, X., and Liu, J. Rank
the episodes: A simple approach for exploration in
procedurally-generated environments. arXiv preprint
arXiv:2101.08152, 2021.

Zhang, T., Xu, H., Wang, X., Wu, Y., Keutzer, K., Gonza-
lez, J. E., and Tian, Y. Noveld: A simple yet effective
exploration criterion. Advances in Neural Information
Processing Systems, 34, 2021.

11

Go Beyond Imagination: Maximizing Episodic Reachability with World Models

A. Implementation Details

A.1. Experiments on Minigrid

Baselines
Implementations of GoBI, NovelD (Zhang
et al., 2021), RIDE (Raileanu & Rockt¨aschel, 2020),
RND (Burda et al., 2018), and EC (Savinov et al., 2018) are
built on the official codebase of NovelD. For fair compar-
isons, only the intrinsic reward rint differs among the meth-
ods and they all use the same base algorithm IMPALA (Es-
peholt et al., 2018). At the same time, all the experiments
are run with the same compute resource with Nvidia TI-
TAN X GPU and 40 CPUs. For NovelD, we rerun their
official code to get the results of Minigrid MultiRoom and
KeyCorridor. For the experiments on ObstructedMaze, we
did not find the proper hyper-parameters to fully reproduce
their results. Therefore, we directly take the results reported
in their paper. For RIDE and RND, we run the code in
the official codebase of NovelD. For EC (Savinov et al.,
2018), their original paper does not include experiments
on Minigrid environments. Therefore, we implement our
own version and tune the hyper-parameters with grid search.
The intrinsic reward functions of GoBI and the baselines
are listed below:

• GoBI: (mt+1 − mt)/(cid:112)N (ot+1), where mt is the size
of the episodic buffer M and N (ot+1) is the lifelong
count of the observation ot+1 starting from the begin-
ning of training.

• RND: ∥ϕ(ot+1) − ˆϕ(ot+1)∥2, which is the difference
between a fixed random network ˆϕ and a trained state
embedding network ϕ. Here, ϕ is trained to minimize
the same error.

• NovelD: max[novelty(ot+1) − α · novelty(ot), 0] ∗
1{N epi(st+1 = 1)}. They apply RND to measure the
novelty of ot, i.e., novelty(ot) = ∥ϕ(ot) − ˆϕ(ot)∥2.
N epi(st+1 = 1) checks if the agent visits state st+1
for the first time in an episode. Notice that they use
the full environment information, i.e. everything in the
grid world instead of only the 7×7 partially-observable
view. Therefore N epi counts st+1 instead of ot+1.
• RIDE: ∥ϕ(ot) − ϕ(ot+1)∥2/(cid:112)N epi(st+1), where ϕ is
the state embedding network trained to minimize the
prediction error of an inverse and a forward dynamics.
N epi indicates the episodic counts. Same as NovelD,
in RIDE, they also use the state information st+1 for
episodic count.

Policy and Value Function Training For fair compar-
isons, the policy network and value function network are
the same for all approaches. The input observations of di-
mension 7 × 7 × 3 are put into a shared feature extraction
network, which includes three convolutional layers of ker-
nel size= 3 × 3, padding= 1, channel=32, 128, 512, and
stride= 1, 2, 2 respectively with ELU activation. The fea-
tures are then flattened and put through 2 linear layers with
1024 units and ReLU activation, and an LSTM layer with
1024 units. This shared feature is passed separately to 2
fully-connected layers with 1024 units to output action dis-
tribution and value estimation.

Dynamics model For our implementation of the dynamics
model, our input is the panorama of the current step. To get
the panorama, we let the agent rotate for 3 times and con-
catenate the 4 observations to get inputs of size 28 × 7 × 3.
It is then passed to a feature extraction module that has the
same structure as our policy and value function networks,
except that the input to the first linear layer is 4 × 1024.
We then concatenate it with actions and put it through a de-
coder with 2 linear layers of sizes 256 and 512, and reshape
back to 7 × 7 × 3 to get a predicted observation. We pre-
train the dynamics model using 1e5 (panot, at, ot+1) pairs
collected by a random policy. RIDE also requires training
dynamics models for the state embedding network ϕ. The
input of their dynamics model is the state embedding and
action. The forward model contains two fully-connected
layers with 256 and 128 units activated by ReLU. The in-
verse dynamics model contains two fully-connected layers
with 256 units and a ReLU activation function. Its input is
the state embeddings of two consecutive steps.

Hash Functions We directly apply the default Python
hashing function to hash the 7 × 7 × 3 observations and
predicted future observations before adding them to the
episodic buffer.

State embedding NovelD, RIDE, and RND all require
training a state embedding network ϕ. The input is the
observation in MiniGrid with dimension 7×7×3. It contains
three convolutional layers with kernel size= 3 × 3, padding
= 1, stride = 1, 2, 2, number of channels = 32, 128, 512
respectively. The activation function is ELU. Following the
convolutional layers are two linear layers of 2048 and 1024
units with ReLU activation.

• EC: β − C(M, ot+1), where C(M, ot+1) is the 90-th
percentile similarity scores between ot+1 and all the
observations in the episodic buffer M. The similar-
ity scores are calculated using a pre-trained episodic
curiosity module. β is a hyper-parameter.

Visitation Count For GoBI, N (o) stores the flattened 7 ×
7 × 3 observations of each step. And for NovelD and RIDE,
they count the full states at episodic level, whose shape
varies from environment to environment. For example, the
shape is 25 × 25 × 3 for MultiRoom environments.

12

Go Beyond Imagination: Maximizing Episodic Reachability with World Models

Hyper-parameters Table 1 shows the values of hyper-
parameters shared across different methods.

Parameter name

Batch Size
Optimizer
Learning Rate
LSTM Steps
Discount Factor γ
Weight of Policy Entropy Loss
Weight of Value Function Loss

Value

32
RMSProp
0.0001
100
0.99
0.0005
0.5

Table 1. Hyper-parameters for experiments on Minigrid. These
hyper-parameters are shared across all the methods

For all of our experiments using GoBI on Minigrid, we set
the intrinsic reward coefficient λ = 0.01 and k = 1, which
means only forwarding the dynamics model by 1 step. At
the same time, as the action space is small and discrete,
instead of randomly sampling some actions, we directly
predict the future observations using all 7 possible actions.
We list the hyper-parameter choices of intrinsic decay factor
ρ in Table 2. The value of ρ is chosen to make the intrinsic
reward large at the beginning of training and near-zero at
the end of the training.

Parameters

Forward Step k
Intrinsic Decay ρ

ˆfϕ Optimizer
ˆfϕ Learning Rate

Value

1
6e−7 for MR-N7S8;
8e−7 for MR-N12S10, MR-N6;
1.5e−6 for KC-S3R3;
5e−7 for KC-S4R3, KC-S5R3
3e−7 for KC-S6R3, OM-2Dlh
2e−7 for OM-1Q, OM-2Dlhb, OM-2Q
5e−8 for OM-Full
Adam
5e−4

Table 2. The hyper-parameters of GoBI for experiments on Mini-
grid.

For NovelD, we set λ = 0.05 for all the environments
as is suggested in their official codebase. For RIDE, we
use λ = 0.1 on KeyCorridor-S3R3 and λ = 0.5 on all
other environments. For RND, we set λ = 0.1 on all the
environments. For EC, we make λ = 0.01 so that the initial
average intrinsic reward of EC is similar to ours.

experiments apply the same base reinforcement learning
algorithm RAD (Laskin et al., 2020). For RE3, we rerun
their official code to get the results on all four environments.
For ICM and RND, we follow the implementation details
listed in RE3 to implement them to be compatible with
DeepMind Control tasks. For a fair comparison, only the
intrinsic reward design differs among the methods. The in-
trinsic reward functions of GoBI and the baselines are listed
below:

• GoBI: (mt+1 −mt)×log(||yi −yk−N N

||2 +1), where
mt is the size of the episodic buffer M. The latter part
is the RE3 intrinsic reward which we introduce below.

i

i

• RE3: log(||yi − yk−N N

||2 + 1), where yi = fθ(si) is
a fixed representation outputs from a randomly initial-
ized encoder and yk−N N
is a set of k-nearest neighbors
of yi among all the collected y’s from the beginning of
training.

i

• ICM: η

2 || ˆϕ(ot+1) − ϕ(ot+1)||2

2, where η is a scaling
factor. ϕ(o) is a feature vector that is jointly optimized
with a forward prediction model and an inverse dynam-
ics model and ˆϕ(o) predicts the feature encoding at
time step t + 1.

• RND: ∥ϕ(ot+1) − ˆϕ(ot+1)∥2, which is the difference
between a fixed random network ˆϕ and a trained state
embedding network ϕ. Here, ϕ is trained to minimize
∥ϕ(ot+1) − ˆϕ(ot+1)∥2.

Architecture The observation size of all the environments
is 84×84×3. The encoder architecture follows the same one
as in (Yarats et al., 2021b), which contains 4 convolutional
layers of 3 × 3 kernels, channel=32, and stride=2, 1, 1, 1
with ReLU activations. The output is then passed to a fully-
connected layer and normalized by LayerNorm.

Dynamics Model For the forward dynamics model that
we use to generate future predictions, we apply the same
world model structure as in Dreamer (Hafner et al., 2019).
The input size of Dreamer is 64 × 64 × 3. We down-sample
the input observations to 64 × 64 instead of tuning the world
model layers. We train the dynamics model together with
the RL policy in an online manner instead of pre-train it
because it takes many episodes for the predictions to be
visually reasonable. Therefore it does not add extra effort to
determine how many data should we collect to pre-train the
dynamics model.

A.2. Experiments on Deepmind Control Suites

Baselines
Implementations of GoBI, RE3 (Seo et al.,
2021), ICM (Pathak et al., 2017), and RND (Burda et al.,
2018) are built on the official codebase of RE3. All the

Image hashing As the observations are images in high-
dimensional space and the predictions are usually not accu-
rate, we hash the images to lower dimension to avoid taking
too much space and to collapse similar observations and

13

Go Beyond Imagination: Maximizing Episodic Reachability with World Models

predictions. Following (Tang et al., 2017), we use the sim-
ple SimHash function to map the images to 50 bits. More
specifically, we project the flattened images to a random
initialized vector and use the signs of output vector values
as the hash code.

Hyper-parameters Table 3 shows the values of hyper-
parameters shared across different methods.

decreases to very small. Therefore sometimes it is hard for
the agent to learn anything useful, resulting in unsatisfac-
tory performance. Meanwhile, if ρ is too small, for example
when ρ = 5e − 7, the intrinsic reward will be too large at
later stage of training and make the agent focus less on the
extrinsic reward. Therefore the policy may converge slower.

Parameter name

Augmentation
Observation Size
Action Repeat
Replay Buffer Size
Initial Random Exploration Steps
Frame Stack
Actor Learning Rate
Critic Learning Rate
Batch Size
# Nearest Neighbors
Critic Target Update Freq

Value

Crop
(84, 84)
2
100000
1000
3
0.0002
0.0002
512
3
2

Table 3. Hyper-parameters for experiments on DeepMind Control
Suites. These hyper-parameters are shared across all the methods

For the intrinsic reward coefficients, we follow the best
choices reported in RE3. For GoBI, we apply the same
intrinsic reward coefficient λ and intrinsic reward decay ρ
as the ones in RE3 for fair comparison. The intrinsic rewards
that are specific to our method is shown in Table 4. For the
number of random actions n, we perform hyper-parameter
search over {3, 5, 10, 20} and find that n = 5 perform well
across all the tasks. For the number of forward steps k, we
perform hyper-parameter search over {1, 2, 3, 5} and report
the ones with the best results.

Parameter name

# Forward Step k

# Random Actions n

Value

3 for pendulum-swingup;
1 for others
5

Table 4. Hyper-parameters for experiments on DeepMind Control
Suites. These hyper-parameters are specific to our method.

B. Hyper-Parameters

B.1. Intrinsic reward decay

Figure 10. Training performance of GoBI on Minigrid Multiroom-
N12-S10 with different intrinsic reward decay ρ.

Figure 11. Training performance of GoBI on Minigrid Multiroom-
N7-S8 with different n randomly sampled actions per step.

B.2. Number of randomly sampled actions

In the Minigrid experiments reported in Section 3, we do
not randomly sample actions because Minigrid has a small
discrete action space with only 7 actions. Therefore we
directly predict future observations of all 7 actions. How-
ever, we also report the results with n = 3, 5, 10, 15 random
actions in Figure 11. To summarize, n = 5, 10, 15 all have
similar performance on Minigrid, while a larger n makes
the wallclock training time longer. n = 3 is slightly slower
at the early stage, but still outperforms the previous state-
of-the-arts. Overall, n = 5 would be a good choice for the
Minigrid environments.

In Figure 10, we show the training performance with differ-
ent intrinsic reward decay ρ. The choice of ρ is to balance
between the relative importance of the extrinsic and intrinsic
reward. If ρ is too large, for example when ρ = 1e − 6,
before the agent finds any goal, the intrinsic reward already

C. Exploration Behaviour Comparison

between GoBI and EC

In Figure 12, we visualize the policy visitation heatmaps
of GoBI and EC (Savinov et al., 2018) on a MultiRoom

14

n
r
u
t
e
R
e
g
a
r
e
v
A

0.6

0.4

0.2

0.0

0.0

MultiRoom-N12-S10

0.3

0.2
0.6
environment steps (1e7)

0.5

 = 1e-6
 = 9e-7

 = 8e-7
 = 7e-7

0.8

 = 6e-7
 = 5e-7

 
MultiRoom-N7-S8

n
r
u
t
e
R
e
g
a
r
e
v
A

0.6

0.4

0.2

0.0

0.0

0.2

0.3

0.5

0.6

0.8

1e7

n=3

n=5

n=10

n=15

 
Go Beyond Imagination: Maximizing Episodic Reachability with World Models

Figure 12. Heatmaps of example trajectories at different training steps in Minigrid-Multiroom-N7-S8 environment.

Algorithm Wallclock Time (hours)

NovelD
GoBI(ours)

5.46(±0.058)
10.65(±0.082)

Table 5. Wallclock training time comparison in hours between
NovelD and GoBI.

environment from Minigrid. Although EC fails to learn an
optimal policy, we can still capture its preference from the
heatmaps. An agent trained with EC prefers going to the
corners of the room, which generally have lower similarity
scores than the states in the middle of the room. However,
if the similarity scores are not low enough for the states
to be added to the episodic buffer, it will continue staying
at the states to maximize its intrinsic reward. We tried to
tune the similarity score threshold using grid search but still
have not find a good hyper-parameter choice for it because
the similarity score at different corners does not share a
consistent value. Unlike EC, we can see from the figure
that GoBI chooses not to visit the border of the room early
on in training, as the information on the border are easily
predictable from the information in the middle of the room.

Figure 14. An illustrative example of why R2 does not work well
to encourage efficient exploration. In this example, our goal is
to include all the states into the episodic buffer quickly. GoBI
can move directly to the center in one step for maximum intrinsic
reward, while R2 may choose to take extra steps for exploration
since it mainly focuses on whether the episodic buffer expands or
not.

Figure 13. Training performance comparison on MultiRoom-N7-
S8 environment among 1) use a fixed pre-trained dynamics model,
2) use a pre-trained dynamics model, and fine-tune it online, 3)
no pre-training, directly train the dynamics model together with
policy training.

D. Wallclock Training Time

Table 5 shows the wallclock time needed to train NovelD
and our method for 10M Minigrid environment steps. GoBI
requires about 2x the wallclock time needed to train NovelD
for the same number of environment steps.

E. Dynamics Training

In the experiment section 3, we report the results of apply-
ing a pre-trained forward dynamics for GoBI on Minigrid.
However, the forward dynamics model can also be trained
together with policy training. In Figure 13, we report the
results of an ablation study on a Minigrid environment of
3 settings: 1) use a pre-trained dynamics model, and keep
it fixed when training the policy, 2) pre-train a dynamics
model, and fine-tune it when training the policy, 3) no pre-
training, directly train the dynamics model in an online
manner. In summary, all 3 versions work similarly, but due
to the fact that training the dynamics model online will add

15

GoBI(Ours)

Training Environment when Episode = 2

Goal

Agent

Minigrid-MultiRoom-N7S8-v0

Episodic 
Curiosity (EC)

s
t
n
u
o
C
d
e
t
i
s
V

i

2.3M

2.1M

4.5M

4.0M

8.9M

Training Steps

5.9M

8.0M

Unseen

Seen

 
MultiRoom-N7-S8

n
r
u
t
e
R
e
g
a
r
e
v
A

0.6

0.4

0.2

0.0

0.0

0.2

0.3

0.5

0.6

0.8

1e7

pre_train+fine_tune

no_pre_train

no_fine_tune

 
Model

t = 0

t = 1

t = 2

t = 3

R2

GoBI
(Ours)

s0

s0

s2

s3 s2

s0

s1

s0

s1

s0

s1

s1

s0

st

Visited State

Reachable State

Non-reachable State

Trajectory (Directed via Temporal Order)

Observed Transition Path (Bidirectional)

Unreachable Transition Path (Bidirectional)

Go Beyond Imagination: Maximizing Episodic Reachability with World Models

extra wall clock training time, we use option 1 in our main
experiments.

F. Ablation Study Illustration

In this section, we provide an illustrative example of why
only considering whether new states are added to the
episodic buffer or not, i.e., R2 in Section 3.3, works way
worse than our method. The example is shown in Figure 14.
If these 9 states are only a small part of the environment, we
want a policy that explore this part as quickly as possible -
mark all the states as reachable as quickly as possible. How-
ever, in order to maximize its step-wise intrinsic reward, an
agent trained with R2 will go along the border to only add a
few new states to the episodic buffer at a time, which wastes
many unnecessary steps so is not beneficial for exploration.

16

","Go Beyond Imagination : Maximizing Episodic Reachability with World Models Yao Fu 1 Run Peng 1 Honglak Lee 1 2 3 2 0 2 g u A 5 2 ] G L . s c [ 1 v 1 6 6 3 1 . 8 0 3 2 : v i X r a Abstract Efficient exploration is a challenging topic in rein- forcement learning , especially for sparse reward tasks . To deal with the reward sparsity , people commonly apply intrinsic rewards to motivate agents to explore the state space efficiently . In this paper , we introduce a new intrinsic reward design called GoBI - Go Beyond Imagination , which combines the traditional lifelong novelty motivation with an episodic intrinsic reward that is designed to maximize the stepwise reachability expansion . More specifically , we apply learned world models to generate predicted future states with random actions . States with more unique predictions that are not in episodic memory are as- signed high intrinsic rewards . Our method greatly outperforms previous state-of-the-art methods on 12 of the most challenging Minigrid navigation tasks and improves the sample efficiency on loco- motion tasks from DeepMind Control Suite . 1 . Introduction Efficient exploration in state space is a fundamental chal- lenge in reinforcement learning ( RL ) ( Hazan et al. , 2019 ; Lee et al. , 2019 ) , especially when the environment rewards are sparse ( Mnih et al. , 2013 ; 2016 ; Schulman et al. , 2017 ) or absent ( Liu & Abbeel , 2021 ; Parisi et al. , 2021 ) . Such reward sparsity makes RL algorithms easy to fail due to the lack of useful signals for policy update ( Riedmiller et al. , 2018 ; Florensa et al. , 2018 ; Sekar et al. , 2020 ) . A com- mon approach for exploration is to introduce self-motivated intrinsic rewards such as state visitation counts ( Strehl & Littman , 2008 ; Kolter & Ng , 2009 ) and prediction er- rors ( Stadie et al. , 2015 ; Pathak et al. , 2017 ; Burda et al. , 2018 ) . Most of these intrinsic reward designs measure life- long state novelty and prioritize visiting states that are less 1University of Michigan 2LG AI . Correspon- Yao Fu < violetfy @ umich.edu > , Run Peng dence to : < roihn @ umich.edu > , Honglak Lee < honglak @ eecs.umich.edu & honglak @ lgresearch.ai > . Proceedings of the 40 th International Conference on Machine Learning , Honolulu , Hawaii , USA . PMLR 202 , 2023 . Copyright 2023 by the author ( s ) . 1 visited starting from the beginning of training . While the above methods achieves great improvement on hard-exploration tasks like Montezumas Revenge ( Burda et al. , 2018 ) , they generally only work well on “ single- ton ” environments , where training and evaluation environ- ments are the same . However , due to the poor generalization performance of reinforcement learning in unseen environ- ments ( Kirk et al. , 2021 ) , nowadays researchers have been paying more attention on procedurally-generated environ- ments ( Cobbe et al. , 2019 ; 2020 ; Flet-Berliac et al. , 2021 ) , where the nature of task remains the same but the environ- ment is randomly constructed for each new episode . For example , a maze-like environment will have different maze structures , making it rare for the agent to encounter the same observations across different episodes . Therefore , lifelong novelty intrinsic motivations usually fail in hard procedurally-generated environments of this kind ( Raileanu & Rockt¨aschel , 2020 ; Zha et al. , 2021 ) because an agent will be trapped around newly-generated states . Inspired by human ’ s frequent use of short-term memory ( An- dersen et al. , 2006 ; Eichenbaum , 2017 ) to avoid repeatedly visiting the same space , recent work propose to derive intrin- sic rewards on episodic level ( Savinov et al. , 2018 ; Badia et al. , 2020 ; Raileanu & Rockt¨aschel , 2020 ; Zha et al. , 2021 ; Zhang et al. , 2021 ) . The episodic intrinsic rewards gener- ally give bonus to large episodic-level state space visitation coverage , therefore encourage visiting as many states as possible in the same episode . However , does visiting more states necessarily mean efficient episodic-level exploration ? We notice that some state visitations are unnecessary and can be avoided if they are predictable from episodic mem- ory . For example , when navigating through a house to find a fridge , if you open a door and find an empty room , you do not need to go into it anymore because you can easily predict what the states are like in the room ( i.e. , intuitively speaking , you would be moving around in an empty room ) . With this inspiration , we propose to design the episodic in- trinsic reward to not only maximize the number of visited states in an episode , but also consider those states that are not visited but can be predicted from episodic memory . More precisely , we maintain an episodic buffer to store all the visited states as well as states reachable from the visited states within a few time steps . To get the reachable states , Go Beyond Imagination : Maximizing Episodic Reachability with World Models Figure 1 . Illustration of how GoBI works on Minigrid . For the environment in the upper-left corner , the red triangle indicates the position and orientation of the agent . It has a 7 × 7 partially-observable view ( highlighted ) . During pre-training ( Stage 1 ) , we collect data using a random policy to train a forward dynamics model ˆfϕ ( panot , at ) = ot+1 , where panot denotes the panoramic view as is defined in Section 3.1 . For policy training ( Stage 2 ) , we apply ˆf to predict observations in the future k time steps with n random actions for each step . We add the new ones to an episodic buffer M and take the change of size of M as the episodic intrinsic reward repi . The lifelong intrinsic reward is COUNT-based . Our intrinsic reward GoBI is rlifelong ∗ repi . we train a world model with forward dynamics function and apply random actions to the learned dynamics model to pre- dict future states . The predictions are added to the episodic buffer if they are not there already . We use the change of size of this episodic buffer as the episodic intrinsic reward . Following many previous work , we weight the episodic in- trinsic reward by a lifelong intrinsic reward ( Badia et al. , 2020 ; Zhang et al. , 2021 ) like the COUNT-based rewards . With this newly proposed intrinsic reward design GoBI - Go Beyond Imagination , the agent is expected to both explore the most of the state space throughout training to discover extrinsic rewards , and learn to act in an efficient manner within a single episode to avoid being trapped by seemingly novel states . The contributions of this work can be highlighted as fol- lows : ( i ) We propose a novel way to combine world models with episodic memory to formulate an effective episodic intrinsic reward design . ( ii ) In sparse-reward procedurally- generated Minigrid environments ( Chevalier-Boisvert et al. , 2023 ) , GoBI greatly improves the training sample efficiency in comparison with prior state-of-the-art intrinsic reward functions . ( iii ) GoBI extends well to DeepMind Control Suite ( Tunyasuvunakool et al. , 2020 ) with high-dimensional visual inputs and shows promising results on sparse-reward continuous control tasks . ( iv ) We analyze the design of GoBI and present extensive ablations to show the contribu- tion of each component . 2 . Method We consider reinforcement learning problems framed as Markov Decision Process ( MDP ) M = ( S , A , T , R , γ ) , where S and A denote the state space and action space . T : S × A × S → [ 0 , 1 ] is the state transition function . R : S × A × S → R is the reward function . γ is the reward discount factor . At each step t , the state of the environment is denoted as st ∈ S. The agent generates an action at ∈ A to interact with the environment . The environment then transits to the next underlying state st+1 ∈ S. Apart from the new state st+1 , the environment also returns an extrinsic reward rext that describes how well the agent reacts to st . In sparse-reward tasks , rext is usually 0 . In this work , we follow the previous work to train RL algorithms with rext +λ∗rint is a self-motivated intrinsic reward and λ is a hyper-parameter that controls the relative importance between intrinsic and extrinsic rewards . , where rint t t 2.1 . Go Beyond Imagination Reachable States and Episodic Buffer Our intrinsic re- ward design aims to exploit the information hidden inside the neighbourhood of states . We define a state A to be k- step reachable from state B if the agent can reach A from B within k time steps . During the training process , for each new episode , we initialize an empty episodic memory buffer M. At time step t , we hash st as well as all the states reachable from st. We denote the set containing all the hash 2 Environment e.g . 🚶 Action Space 1 . Left 2 . Right 3 . Forward 4 . Pickup 5 . Drop 6 . Toggle 7 . Done ⬅ ➡ 🚶 🤏 ✋ 🛠 ✅ Encoder Decoder Policy Net Conv2d U L E R + r a e n L i U L E R + r a e n L i Episodic Memory Expansion = 🚶 next obs = Random Actions k n Already in New state Pre-train forward dynamics model Stage 1 X Count ( ) Episodic Reachability Maximization Stage 2 Go Beyond Imagination : Maximizing Episodic Reachability with World Models Figure 2 . An illustration of how our episodic buffer updates . We consider reachable states that are k = 2 time steps away from st. After the agent moves from s1 to s2 , the episodic buffer M ( shaded in green ) expands by 3 new reachable states ( shaded in yellow ) . More formally , M ← M ∪ MR ( s2 ) , where MR ( s2 ) indicates a set of all the states 2-step reachable from s2 . Notice that all the states in the trajectory , i.e. , s0 , s1 , s2 are also added to the buffer . codes of st and its reachable states as MR ( st ) . Then we update M by M ← M ∪ MR ( st ) . Storing the hash codes instead of directly storing the states may alleviate the poten- tial memory issue of the buffer . We illustrate this process in Figure 2 . When the agent reaches state s2 , we add 3 more states that are reachable from s2 but not in M. Forward Dynamics In real environments , it is common that we do not have access to the neighbourhood relationship between states . However , we can learn a world model by training a forward dynamics model ˆfϕ ( st , at ) = st+1 to predict the states reachable from st . This forward dynamics can be pre-trained using data collected by a random policy or trained online together with policy training . When training the policy , for each time step t , we generate k · n random actions and use the learned dynamics ˆfϕ to predict states in the future k steps . We hash the current state st as well as the t+1 , ... , ˆsn predicted future states ˆs1 t+k and add the hash codes to the episodic buffer M if they are not in the buffer . Apart from alleviating potential memory issue as is mentioned in the last paragraph , using a hash function may also mitigate the noise introduced by ˆf . With a learned dynamics model , the predictions of reachable states are usually not perfect . However , in the experiment section we show that even with imperfect predictions , our method can improve the training sample efficiency a lot . t+k , ... , ˆsn t+1 , ... , ˆs1 Episodic Novelty We aim to design an episodic-level nov- elty reward that guides the agent to extend the frontier of its predicted reachable space efficiently to discover states not visited and not predictable within the same episode . More specifically , we denote the size of the episodic buffer M as mt at time step t and design a reachability-based bonus repi = mt+1 − mt that encourages the agent to find unex- plored regions . For each time step , the agent is expected to reach the state that is reachable to more new states in the current episode . 3 Intrinsic Reward Formulation We further weight our episodic intrinsic reward by a lifelong intrinsic reward to encourage the agent to explore the regions that are not well explored in the past . More formally , the proposed intrinsic reward GoBI is defined as : t = ( mt+1 − mt ) ∗ rlifelong rint t ( 1 ) Here , rlifelong denotes lifelong intrinsic reward . We note t that our framework is compatible with any choice of lifelong intrinsic reward . Specifically , we use the simple COUNT- based reward 1/ ( cid:112 ) N ( st+1 ) for the navigation experiments on Minigrid environments ( Chevalier-Boisvert et al. , 2023 ) , where N denotes the count of st+1 from the start of train- ing.1 For the experiments on DeepMind Control Suite ( Tun- yasuvunakool et al. , 2020 ) we use the state-of-the-art in- trinsic reward RE3 ( Seo et al. , 2021 ) , which estimates state entropy by a random encoder . Intrinsic Decay Intrinsic rewards are expected to be asymptotically consistent so that it will not influence the policy learning at later stage of training and result in a sub-optimal policy . To guarantee that the policy learning focuses more on extrinsic rewards as training proceeds , in RE3 ( Seo et al. , 2021 ) , the authors apply exponential decay schedule for the intrinsic rewards to decrease over time . Al- though COUNT-based reward theoretically converges to 0 with enough exploration , it decreases quite slowly in procedurally-generated environments . Therefore , we also apply intrinsic reward decay when calculating GoBI by de- creasing the intrinsic reward coefficient λ during training . We summarize our method in Algorithm 1 and illustrate the training process on Minigrid navigation tasks in Figure 1 . 1For environments that are partially observable ( e.g. , in Mini- grid , the agent observes a 7×7 pixel local view of the environment ) , we substitute state st with observation ot when calculating the in- trinsic rewards . t = 1 t = 2 Action : Move ( S1 , S2 ) Episodic Buffer Non-reachable Space New Reachable Space Reachable States from Visited State Reachable State Non-reachable State Trajectory Observed Transition Path Unreachable Transition Path Go Beyond Imagination : Maximizing Episodic Reachability with World Models Algorithm 1 Go Beyond Imagination Input : Intrinsic Reward Coefficient λ0 , Forward Pre- diction Step k , Number of Random Actions n , Intrinsic Reward Decay Parameter ρ Initialize policy πθ , dynamics model ˆfϕ , replay buffer B . ( Optional ) Collect episodes with πθ and train ˆfϕ with prediction loss for episode e = 1 , 2 , ... until convergence do Initialize episodic buffer M. λ ← λ0 ∗ ( 1 − ρ ) ( e−1 ) ∗T for t = 1 to T do Execute πθ in the environment to get a transition pair ( st , at , st+1 , rext mt ← size ( M ) M ← M ∪ { hash ( st ) } ) for t′ = 1 to k do ) . t gets a high intrinsic reward if the corresponding similarity scores are low . Only with low enough similarity scores do they add st to the buffer . Although their method and ours are similar at high level , they are different by design . For ex- ample , for an agent standing in front of an empty blind alley with dead end , agent trained with GoBI does not benefit in going deep into the blind alley because everything there can be predicted as reachable and added to the episodic buffer already . However , EC encourages going to the very end of the blind alley to reach the state with low similarity score and high intrinsic reward , even though going into an empty blind alley is not beneficial for exploration and wastes time that can be used to explore other parts of the environment . In Appendix C , we present the visitation heatmaps of poli- cies learned by EC and find that it prefers going to the room corners , which well matches our explanation above . t′ , ... , an t′ generate n random actions a1 M ← M ∪ { hash ( ˆfϕ ( ( cid:98 ) si t+t′ , ai end for t = ( size ( M ) − mt ) ∗ rlifelong rint B ← B ∪ { ( st , at , st+1 , rext t + λ ∗ rint t ) } t′ ) |i = 1 , ... , n } ) end for update πθ with RL objective update ˆfϕ with prediction loss end for 2.2 . Conceptual Advantage of GoBI over Prior Works Previous works including RIDE ( Raileanu & Rockt¨aschel , 2020 ) and NovelD ( Zhang et al. , 2021 ) also combine episodic intrinsic reward with lifelong novelty as we do . However , most of them focus on episodic-level state vis- itation . For example , NovelD only assigns non-zero re- wards to a state when it is visited for the first time in the episode . However , we notice that not all state visitations are necessary . The agent ’ s goal for exploration is to gather information about the states . Therefore for states that are easily predictable from episodic memory , visiting them may not really help to acquire more information about the en- vironment . In Figure 4 , we plot the visitation heatmap of GoBI and NovelD to demonstrate the different exploration behaviours of the two methods . Our method is closely related to another work that mea- sures episodic curiosity ( EC ) ( Savinov et al. , 2018 ) . In EC , the authors train a reachability network that takes in two arbitrary states and outputs a similarity score between 0 and 1 , where 1 indicates the two states are the same and 0 indicates they are totally different . The network is trained using collected episodes by marking temporally close states as positive examples and temporally far ones as negative samples . Meanwhile , they also maintain an episodic buffer . A state st is compared with all the states in the buffer and ( a ) MiniGrid ( b ) Deepmind Control Figure 3 . Rendering of the environments used in this work . Left : 2D grid world navigation tasks that require object interactions . Right : DeepMind Control tasks with visual observations . 3 . Experiments In this section , we evaluate GoBI in two domains : 2D procedurally-generated Minigrid environments ( Chevalier- Boisvert et al. , 2018 ) with hard-exploration tasks and lo- comotion tasks from DeepMind Control Suite ( Tunyasu- vunakool et al. , 2020 ) . The experiments are designed to answer the following research questions : ( 1 ) How does GoBI perform against previous state-of-the-art intrinsic re- ward designs in terms of training-time sample efficiency on challenging procedurally-generated environments ? ( 2 ) Can GoBI successfully extend to complex continuous domains with high-dimensional observations , for example control tasks with visual observations ? ( 3 ) How does each compo- nent of our intrinsic reward contribute to the performance ? ( 4 ) What is the influence of the accuracy of the learned world models to our method ? 3.1 . Minigrid Navigation Tasks Minigrid Environments MiniGrid ( Chevalier-Boisvert et al. , 2018 ) is a set of partially-observable procedurally- 4 Go Beyond Imagination : Maximizing Episodic Reachability with World Models Figure 4 . Visitation heatmaps on KeyCorridorS5R3 at different training stages . This figure compares the policy behaviour of GoBI and NovelD . A dark red color means plentiful visitations , white means the agent has seen the space but did not step on it , and black means space that are not discovered . It is worth noticing that early in the training ( 3.5M and 7M time steps ) , our policy already learns not to go into an empty room , likely because states in an empty room are easily predictable . On the contrary , even after 11M steps , an agent trained with NovelD still goes into an empty room ( bottom-right corner ) for more state visitations . generated grid world navigation tasks . The agent is expected to interact with objects such as keys , balls , doors , and boxes to navigate through rooms and find the goal that is randomly placed in one of the rooms . The tasks only provide one sparse reward at the end of each episode , which indicates if the agent successfully finds the goal or not and how many steps it takes to reach the goal . In this work , we consider 3 types of tasks including MultiRoom , KeyCorridor , and ObstructedMaze . Some environments that we experiment on in this paper are shown in Figure 3a . The upper-right is a KeyCorridor-S4R3 environment , where the agent should learn to open the doors to find a key , use it to open the locked blue door , and pick up the green ball . The bottom-left fig- ure shows an ObstructedMaze-Full environment , which is similar to KeyCorridor but more challenging . The rooms are larger , the doors are blocked by balls , and the keys are hidden in boxes . The upper-left and bottom-right environ- ments are MultiRoom environments , in which the agent has to navigate through connected rooms to reach the goal in the last room . Baselines We compare with state-of-the-art intrinsic re- ward designs that work well on Minigrid including Nov- elD ( Zhang et al. , 2021 ) , RIDE ( Raileanu & Rockt¨aschel , 2020 ) , and RND ( Burda et al. , 2018 ) . For a fair compari- son , we follow the same basic RL algorithm and network architectures used in the official codebase of NovelD and only change the intrinsic rewards rint for all the methods . We also compare our method with EC ( Savinov et al. , 2018 ) because of the similarity of the high-level idea between the two methods . However , the original paper of EC does not include experiments on Minigrid . Therefore , we im- plement our own version to adapt to Minigrid . We follow their implementation suggestions in the paper and tune the hyper-parameters such as novelty threshold by grid search . Dynamics Model Training For each experiment on Min- igrid , we first run a random policy for 1e5 steps to collect data and use them to train a forward dynamics model as the world model . Among the pairs collected , there are about 5e4 different transition pairs . During our experiments , we observe that fine-tuning the pre-trained dynamics model during policy training has no significant influence on the performance . Similar to ( Parisi et al. , 2021 ) , we use the 360◦ panoramic views as the input to predict the future observations . This is a rotation-invariant representation of the observed state . We consider this still a fair compari- son with the previous state-of-the-arts because both NovelD and RIDE rely on using the state information instead of observations for the episodic count calculation . Due to the limited field of view of the agent , we only forward the learned dynamics by k = 1 step when predicting . We predict the next observations produced by all 7 discrete actions in the Minigrid tasks including turn left , turn right , forward , toggle , pick up , drop , and done . We directly apply the default Python hashing function to hash the observations and predicted future observations . We do not expect the hashing function to mitigate the prediction error on Minigrid , but only use it to reduce the dimension of observations and predictions . Training Performance on Minigrid Figure 5 shows the learning curves of GoBI and state-of-the-art explo- ration baselines NovelD , RIDE , RND , and EC on 12 most challenging Minigrid navigation tasks , including Multi- Room , KeyCorridor , and ObstructedMaze . Our curves are shifted towards right by the number of random explo- ration environment steps used to train the world model . In all 12 environments , GoBI significantly outperforms previous methods in terms of sample efficiency . For in- 5 GoBI ( Ours ) One episode of Training Environment same s t n u o C d e t i s V i Unlock 3.5M 3.5M 7M 7M 11M 11M Training Steps 21M 21M Pick Minigrid-KeyCorridor-S5R3-v0 NovelD Unseen Seen Go Beyond Imagination : Maximizing Episodic Reachability with World Models Figure 5 . Training performance of GoBI and the baselines on 12 MiniGrid environments . The x-axis shows the number of environment steps . We shift the training curves towards right by the number of environment steps we use to pre-train the dynamics model , i.e . 1e5 time steps . Results are averaged across 4 seeds . stance , on ObstructedMaze-2Dlhb , GoBI is about three times more sample efficient than NovelD . On the hard- est ObstructedMaze-Full environment , GoBI achieves near- optimal performance within 70M steps . Lastly , although we try to tune the hyper-parameters of EC , our implementation of EC still does not learn well on the Minigrid environments . Qualitative Results To clearly present the exploration be- havior learned by GoBI , we show the visitation heatmaps of GoBI and NovelD on a KeyCorridorS5R3 environment in Figure 4 . Not only does our method converge to an opti- mal policy faster , the exploration behaviour is very different from NovelD . GoBI quickly learns not to visit easily pre- dictable states like an empty room , making it more efficient to explore interesting parts of the environment , for example , the room with a key in it . Dynamics Model Training We follow the world model structure in Dreamer ( Hafner et al. , 2019 ) and directly apply their encoder , transition model , and observation model to predict future observations . However , compared to Mini- grid , it requires way more data to train a decent dynamics model on DeepMind Control to generate visually-reasonable predictions . Therefore , different from the experiments on Minigrid , we do not pre-train the dynamics models . Instead we train the dynamics model together with the policy as is shown in Algorithm 1 . We find that the number of sampled random actions n = 5 works well across all 4 environments . For the number of forward prediction steps k , we set it to be 3 for Pendulum Swingup and 1 for the other 3 environments . For the hashing function , we find that a simple SimHash as is suggested in ( Tang et al. , 2017 ) works well in capturing the similarities between similar observations . We use SimHash to hash the image observations to 50 bits . 3.2 . Experiments on Control Tasks We further test GoBI on DeepMind Control Suite , which are a set of image-based continuous control tasks . These tasks are more challenging than Minigrid because of its high- dimensional observations and stochastic transitions . Notice that these environments are not procedurally-generated . The experiments in this section are to show the generality of our method by experimentally showing that GoBI extends well to sparse-reward tasks with continuous action space and high-dimensional observation space . Training Performance on DeepMind Control We com- pare with the state-of-the-art intrinsic motivation on Deepm- Mind Control tasks - RE3 ( Seo et al. , 2021 ) , which applies a k-nearest neighbor entropy estimator in the low-dimensional representation space of a randomly initialized encoder to maximize state entropy . RE3 is also what we use for the life- long intrinsic reward part rlifelong of GoBI in Eq 1 . Another two intrinsic reward baselines we consider are ICM ( Pathak et al. , 2017 ) and RND ( Burda et al. , 2018 ) . For a fair com- parison , all the experiments use the same basic RL algorithm 6 MultiRoom-N6 MultiRoom-N7-S8 MultiRoom-N12-S10 KeyCorridorS3R3 0.6 0.4 0.2 0.0 0.6 0.4 0.2 0.0 0.8 0.6 0.4 0.2 0.0 0.0 0.2 0.3 0.5 0.6 0.8 0.0 0.1 0.3 0.5 0.6 0.8 0.0 0.2 0.4 0.6 0.8 0.9 0.0 0.2 0.5 0.7 1.0 1.2 KeyCorridorS4R3 KeyCorridorS5R3 KeyCorridorS6R3 ObstructedMaze-2Dlh 1e7 0.8 0.6 0.4 0.2 0.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 0.0 0.2 0.5 0.7 1.0 1.2 0.0 0.3 0.6 0.9 1.2 1.5 0.0 0.4 0.8 1.2 1.6 2.0 0.0 0.4 0.8 1.3 1.7 2.1 ObstructedMaze-2Dlhb ObstructedMaze-1Q ObstructedMaze-2Q ObstructedMaze-Full 1e7 0.8 0.6 0.4 0.2 0.0 0.8 0.6 0.4 0.2 0.0 1.0 0.8 0.6 0.4 0.2 0.0 n r u t e R e g a r e v A 0.6 0.4 0.2 0.0 n r u t e R e g a r e v A 0.8 0.6 0.4 0.2 0.0 n r u t e R e g a r e v A 0.8 0.6 0.4 0.2 0.0 0.0 0.8 1.6 2.4 3.2 4.0 0.0 0.6 1.3 1.9 2.6 3.2 0.0 0.9 1.7 2.6 3.4 4.3 0.0 2.0 4.0 6.0 8.0 10.0 NovelD RIDE EC RND Ours ( GoBI ) 1e7 Go Beyond Imagination : Maximizing Episodic Reachability with World Models episodes than MultiRoom ( all generated rooms are squares with fixed sizes ) , therefore the COUNT-based rewards con- tribute more in such environments than in MultiRoom . At the same time , R2 performs way worse than GoBI . Agents trained with R2 prefer actions that only increase the size of the episodic buffer a bit therefore getting positive score more often . We provide an illustrative example in Appendix F to explain why R2 does not work well compared to GoBI . Using only lifelong intrinsic reward R3 performs the worst and struggles to learn efficiently on large Multiroom , Key Corridor , and Obstructed Maze environments . Real Dynamics vs Learned Dynamics A learned dynam- ics model is generally not perfect , especially for partially- observable environments like Minigrid . In many cases the predictions can never be accurate . For example , when the agent first opens the door of a new room , usually it will not accurately predict everything behind the door . Figure 8 shows the training curves between using the real dynamics model vs a learned dynamics model . Not surprisingly , with the same intrinsic reward function , using the real dynamics converges faster to a near-optimal policy . However , even with imperfect dynamics model , our method still greatly surpasses previous state-of-the-arts . Figure 8 . Comparison between using the real dynamics model of the environments vs using a learned one on Minigrid environments . In both MultiRoom and KeyCorridor , using a real dynamics model to derive intrinsic reward makes the policy converge faster , espe- cially on KeyCorridor . Multi-Step Predictions Figure 9 shows the learning per- formance of GoBI on Minigrid with a varying choices of the number of future steps to do predictions k = 1 , 2 , 3 . For k > 1 , our dynamics model outputs panot+1 instead of ot+1 and we hash and store the observations from panoramas in each future time step . With a real forward dynamics model , a larger k generally accelerates exploration more , because it prioritizes actions that lead to the states that are reachable to more states in the long run . However , due to the limited field of view of the agent and the model inaccuracy , this is not the case if we use a learned model . Forwarding 2 steps is still faster than only 1 step , but more steps than that does not really make exploration faster . Figure 6 . Training curves of GoBI and the baselines on DeepMind Control Suite . The curves are averaged across 5 seeds . RAD ( Laskin et al. , 2020 ) . The results are shown in Figure 6 . The additional episodic-level intrinsic reward term im- proves the sample efficiency a lot compared to only using lifelong intrinsic reward , especially on Hopper Hop and Walker Run Sparse . 3.3 . Ablation Study GoBI Variations In this section , we analyze how each component of our intrinsic reward contributes to the final performance . We ablate each component of GoBI and run experiments on Minigrid environments with the following : • R1 : only episodic intrinsic reward mt+1 − mt • R2 : indicator of whether new states are added to the episodic buffer ( 1 { mt+1 − mt > 0 } ) / ( cid:112 ) N ( ot+1 ) • R3 : only lifelong intrinsic reward 1/ ( cid:112 ) N ( ot+1 ) Figure 7 . Training performance comparison among GoBI , R1 , R2 , and R3 on 3 Minigrid environments . Training performance of GoBI as well as R1 , R2 , and R3 are shown in Figure 7 . Although R1 works on MultiRoom , it suffers on Obstructed Maze and large KeyCorridor environ- ments . The underlying reason may be that in Key Corridor and Obstructed Maze the room structures change less across 7 n r u t e R e d o s p E i 800 600 400 200 0 n r u t e R e d o s p E i 250 200 150 100 50 0 Pendulum Swingup Cartpole Swingup Sparse 800 600 400 200 0 0.0 0.2 0.4 0.7 0.9 1.1 0.0 0.5 1.0 1.6 2.1 2.6 Hopper Hop Walker Run Sparse 1e5 1e5 400 300 200 100 0 0.0 1.0 2.0 3.0 4.0 5.0 0.0 1.0 2.0 3.0 4.0 5.0 RAD+ICM RAD+RND RAD+RE3 RAD 1e5 1e5 Ours MultiRoom-N12-S10 KeyCorridorS6R3 ObstructedMaze-2Dlh n r u t e R e g a r e v A 0.6 0.5 0.4 0.3 0.2 0.1 0.0 0.8 0.6 0.4 0.2 0.0 0.8 0.6 0.4 0.2 0.0 0.0 0.2 0.3 0.5 0.6 0.8 0.0 0.4 0.7 1.1 1.4 1.8 0.0 0.2 0.4 0.6 0.8 1.0 R1 R2 R3 Ours 1e7 MultiRoom-N7-S8 KeyCorridorS3R3 t n r u e R e g a r e v A 0.6 0.4 0.2 0.0 0.8 0.6 0.4 0.2 0.0 0.0 0.1 0.2 0.4 0.5 0.6 0.0 0.1 0.2 0.3 0.4 0.5 real dynamics learned dynamics 1e7 Go Beyond Imagination : Maximizing Episodic Reachability with World Models Rockt¨aschel , 2020 ) and NovelD ( Zhang et al. , 2021 ) both count the episodic state visitations , while we claim that apart from visited states , we should also consider states that can be predicted from short-term episodic memory . 4.3 . Learning World Models with Forward Dynamics Learning dynamics function from a set of observed data is a widely-studied topic in reinforcement learning , especially due to the rapid growth of model-based reinforcement learn- ing ( Wang et al. , 2019 ) . Existing work show that an agent ’ s world model is implicitly a forward model that predict future states ( Ha & Schmidhuber , 2018a ; Freeman et al. , 2019 ) . Recently , people have proposed latent dynamics models that work well on high-dimensional inputs ( Okada et al. , 2020 ) . These latent dynamics models encode image obser- vations and predict future states in the latent space ( Ha & Schmidhuber , 2018b ; Hafner et al. , 2019 ; 2023 ) , outputting realistic future observations on visually complex domains including DeepMind Control Suite ( Tunyasuvunakool et al. , 2020 ) , VizDoom ( Kempka et al. , 2016 ) , Atari Games , and DeepMind Lab ( Beattie et al. , 2016 ) . The learned dynamics models can be used to guide exploration by prediction error ( Stadie et al. , 2015 ; Pathak et al. , 2017 ; Burda et al. , 2018 ) , surprise ( Achiam & Sastry , 2017 ) , or information gain by variance of model ensemble means ( Sekar et al. , 2020 ) . Our method differ from the previous methods by directly gener- ating and hashing the predicted states and add them to an episodic reachable state buffer . With the advanced world model structures , our method can be extended to diverse domains with complex observations . 5 . Discussions and Future Work This paper shows an effective way to combine learned world models with episodic memory to intrinsically guide efficient exploration . Our method achieves state-of-the-art perfor- mance on procedurally-generated hard exploration tasks and also works well on singleton continuous control do- mains . However , it still has certain limitations . First of all , the dynamics model we use for the Minigrid experiments is deterministic , making it possible to generate less accu- rate predictions and making the performance of our method worse than using the real dynamics . A possible way to make improvement on this is to make the prediction model gen- erative and sample possible future states . Secondly , for the control tasks with complex visual inputs , we hash the im- ages with static hashing to make them discrete hash codes . However , to better capture the semantic similarities between the image observations , it would be beneficial to learn hash functions , for example , by using an autoencoder ( AE ) to learn meaningful hash codes ( Tang et al. , 2017 ) . We leave these investigations as future work . Figure 9 . We make forward predictions for different number of future steps k using both the real dynamics and the learned dy- namics model . The plots above show the training performance on Minigrid MultiRoom-N7-S8 . 4 . Related Work 4.1 . Exploration in Reinforcement Learning Efficient exploration in reinforcement learning , especially for sparse-reward reinforcement learning problems is chal- lenging . A natural and popular solution is to design some metric to evaluate state novelty and assign high intrinsic re- ward to novel states . For example , COUNT-based intrinsic reward ( Strehl & Littman , 2008 ; Kolter & Ng , 2009 ; Tang et al. , 2017 ) and curiosity-based intrinsic motivation ( Stadie et al. , 2015 ; Pathak et al. , 2017 ; Burda et al. , 2018 ) . An- other popular way is to do state space entropy maximization ( Hazan et al. , 2019 ; Lee et al. , 2019 ) . Recently , nearest neighbor entropy estimation methods ( Yarats et al. , 2021a ; Liu & Abbeel , 2021 ) have shown great performance im- provements in challenging visual domains . Our method is compatible with all these successful exploration intrinsic re- ward designs by using them as rlifelong , but we additionally encourage the episodic-level reachable space expansion to achieve large state space coverage within a single episode . 4.2 . Episodic Memory Deriving useful information from episodic buffer have shown great success in improving the training sample ef- ficiency in RL on navigation , control , and Atari games . Episodic memory buffers are applied to mimic hippocampal episodic control and rapidly assimilate recent experience ( Blundell et al. , 2016 ; Pritzel et al. , 2017 ) . As is men- tioned in the previous sections , ( Savinov et al. , 2018 ) keeps an episodic buffer to store observations and introduce an episodic curiosity module to determine if a new observation is reachable from previous observations or not . RAPID ( Zha et al. , 2021 ) proposes a novel way to do behaviour cloning on episodes with high episodic coverage . NGU ( Badia et al. , 2020 ) combines an episodic novelty module and a lifelong novelty module to generate intrinsic rewards . However , in NGU , the episodic novelty is a measurement of differ- ence between the current observations from the previous observations , while ours focus on how much the reachable space is expanded from the new state . RIDE ( Raileanu & 8 real dynamics learned dynamics n r u t e R e g a r e v A 0.6 0.4 0.2 0.0 0.6 0.4 0.2 0.0 0.0 0.2 0.3 0.5 0.6 0.8 0.0 0.2 0.3 0.5 0.6 0.8 1e7 k=1 k=2 k=3 1e7 Go Beyond Imagination : Maximizing Episodic Reachability with World Models 6 . Conclusion In this work , we introduce Go Beyond Imagination- GoBI , a novel episodic intrinsic reward design that encourages efficient episodic-level exploration by expanding reachable space . While most previous episodic intrinsic rewards use a naive episodic state count or state visitation coverage , our method exploits learned world models to predict reachable states and motivates the agent to seek for the states with more unexplored neighbors . Combined with lifelong intrin- sic rewards , our method shows great training time sample efficiency improvement on hard procedurally-generated en- vironments . At the same time , it can be extended to guide exploration on continuous control tasks with visual inputs , both indicating a promising future in this direction . 7 . Acknowledgments This work was supported in part by grants from LG AI Re- search , NSF IIS 1453651 , and NSF FW-HTF-R 2128623 . References Achiam , J. and Sastry , S. Surprise-based intrinsic moti- vation for deep reinforcement learning . arXiv preprint arXiv:1703.01732 , 2017 . Andersen , P. , Morris , R. , Amaral , D. , Bliss , T. , and O ’ Keefe , J . The hippocampus book . Oxford university press , 2006 . Badia , A. P. , Sprechmann , P. , Vitvitskyi , A. , Guo , D. , Piot , B. , Kapturowski , S. , Tieleman , O. , Arjovsky , M. , Pritzel , A. , Bolt , A. , et al . Never give up : Learning directed exploration strategies . arXiv preprint arXiv:2002.06038 , 2020 . Beattie , C. , Leibo , J . Z. , Teplyashin , D. , Ward , T. , Wain- wright , M. , K¨uttler , H. , Lefrancq , A. , Green , S. , Vald´es , V. , Sadik , A. , et al . Deepmind lab . arXiv preprint arXiv:1612.03801 , 2016 . Blundell , C. , Uria , B. , Pritzel , A. , Li , Y. , Ruderman , A. , Leibo , J . Z. , Rae , J. , Wierstra , D. , and Hass- abis , D. Model-free episodic control . arXiv preprint arXiv:1606.04460 , 2016 . Burda , Y. , Edwards , H. , Storkey , A. , and Klimov , O. Ex- ploration by random network distillation . arXiv preprint arXiv:1810.12894 , 2018 . Chevalier-Boisvert , M. , Willems , L. , and Pal , S. Minimalis- tic gridworld environment for openai gym . https : // github.com/maximecb/gym-minigrid , 2018 . Chevalier-Boisvert , M. , Dai , B. , Towers , M. , de Lazcano , R. , Willems , L. , Lahlou , S. , Pal , S. , Castro , P. S. , and Terry , J. Minigrid & miniworld : Modular & customizable reinforcement learning environments for goal-oriented tasks . CoRR , abs/2306.13831 , 2023 . Cobbe , K. , Klimov , O. , Hesse , C. , Kim , T. , and Schulman , J. Quantifying generalization in reinforcement learning . In International Conference on Machine Learning , pp . 1282–1289 . PMLR , 2019 . Cobbe , K. , Hesse , C. , Hilton , J. , and Schulman , J. Lever- aging procedural generation to benchmark reinforcement learning . In International conference on machine learn- ing , pp . 2048–2056 . PMLR , 2020 . Eichenbaum , H. The role of the hippocampus in navigation is memory . Journal of neurophysiology , 117 ( 4 ) :1785– 1796 , 2017 . Espeholt , L. , Soyer , H. , Munos , R. , Simonyan , K. , Mnih , V. , Ward , T. , Doron , Y. , Firoiu , V. , Harley , T. , Dunning , I. , et al . Impala : Scalable distributed deep-rl with im- portance weighted actor-learner architectures . In Interna- tional conference on machine learning , pp . 1407–1416 . PMLR , 2018 . Flet-Berliac , Y. , Ferret , J. , Pietquin , O. , Preux , P. , and Geist , M. Adversarially guided actor-critic . arXiv preprint arXiv:2102.04376 , 2021 . Florensa , C. , Held , D. , Geng , X. , and Abbeel , P. Automatic goal generation for reinforcement learning agents . In International conference on machine learning , pp . 1515– 1528 . PMLR , 2018 . Freeman , D. , Ha , D. , and Metz , L. Learning to predict without looking ahead : World models without forward prediction . Advances in Neural Information Processing Systems , 32 , 2019 . Ha , D. and Schmidhuber , J. Recurrent world models facil- itate policy evolution . Advances in neural information processing systems , 31 , 2018a . Ha , D. and Schmidhuber , J . World models . arXiv preprint arXiv:1803.10122 , 2018b . Hafner , D. , Lillicrap , T. , Ba , J. , and Norouzi , M. Dream to control : Learning behaviors by latent imagination . arXiv preprint arXiv:1912.01603 , 2019 . Hafner , D. , Pasukonis , J. , Ba , J. , and Lillicrap , T. Mastering diverse domains through world models . arXiv preprint arXiv:2301.04104 , 2023 . Hazan , E. , Kakade , S. , Singh , K. , and Van Soest , A. Prov- ably efficient maximum entropy exploration . In Interna- tional Conference on Machine Learning , pp . 2681–2691 . PMLR , 2019 . 9 Go Beyond Imagination : Maximizing Episodic Reachability with World Models Kempka , M. , Wydmuch , M. , Runc , G. , Toczek , J. , and Ja´skowski , W. Vizdoom : A doom-based ai research plat- form for visual reinforcement learning . In 2016 IEEE con- ference on computational intelligence and games ( CIG ) , pp . 1–8 . IEEE , 2016 . Kirk , R. , Zhang , A. , Grefenstette , E. , and Rockt¨aschel , T. A survey of generalisation in deep reinforcement learning . arXiv preprint arXiv:2111.09794 , 2021 . Kolter , J . Z. and Ng , A. Y. Near-bayesian exploration in In Proceedings of the 26th annual polynomial time . international conference on machine learning , pp . 513– 520 , 2009 . Laskin , M. , Lee , K. , Stooke , A. , Pinto , L. , Abbeel , P. , and Srinivas , A. Reinforcement learning with augmented data . Advances in neural information processing systems , 33 : 19884–19895 , 2020 . Lee , L. , Eysenbach , B. , Parisotto , E. , Xing , E. , Levine , S. , and Salakhutdinov , R. Efficient exploration via state marginal matching . arXiv preprint arXiv:1906.05274 , 2019 . Liu , H. and Abbeel , P. Behavior from the void : Unsuper- vised active pre-training . Advances in Neural Information Processing Systems , 34:18459–18473 , 2021 . Mnih , V. , Kavukcuoglu , K. , Silver , D. , Graves , A. , Antonoglou , I. , Wierstra , D. , and Riedmiller , M. Playing atari with deep reinforcement learning . arXiv preprint arXiv:1312.5602 , 2013 . Mnih , V. , Badia , A. P. , Mirza , M. , Graves , A. , Lillicrap , T. , Harley , T. , Silver , D. , and Kavukcuoglu , K. Asyn- chronous methods for deep reinforcement learning . In International conference on machine learning , pp . 1928– 1937 . PMLR , 2016 . Okada , M. , Kosaka , N. , and Taniguchi , T. Planet of the bayesians : Reconsidering and improving deep planning network by incorporating bayesian inference . In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems ( IROS ) , pp . 5611–5618 . IEEE , 2020 . Parisi , S. , Dean , V. , Pathak , D. , and Gupta , A . Interesting object , curious agent : Learning task-agnostic exploration . Advances in Neural Information Processing Systems , 34 , 2021 . Pathak , D. , Agrawal , P. , Efros , A . A. , and Darrell , T. Curiosity-driven exploration by self-supervised predic- tion . In International conference on machine learning , pp . 2778–2787 . PMLR , 2017 . Pritzel , A. , Uria , B. , Srinivasan , S. , Badia , A. P. , Vinyals , O. , Hassabis , D. , Wierstra , D. , and Blundell , C. Neural episodic control . In International Conference on Machine Learning , pp . 2827–2836 . PMLR , 2017 . Raileanu , R. and Rockt¨aschel , T. Ride : Rewarding impact- driven exploration for procedurally-generated environ- ments . arXiv preprint arXiv:2002.12292 , 2020 . Riedmiller , M. , Hafner , R. , Lampe , T. , Neunert , M. , De- grave , J. , Wiele , T. , Mnih , V. , Heess , N. , and Springen- berg , J. T. Learning by playing solving sparse reward tasks from scratch . In International conference on ma- chine learning , pp . 4344–4353 . PMLR , 2018 . Savinov , N. , Raichuk , A. , Marinier , R. , Vincent , D. , Polle- feys , M. , Lillicrap , T. , and Gelly , S. Episodic curiosity through reachability . arXiv preprint arXiv:1810.02274 , 2018 . Schulman , J. , Wolski , F. , Dhariwal , P. , Radford , A. , and Klimov , O. Proximal policy optimization algorithms . arXiv preprint arXiv:1707.06347 , 2017 . Sekar , R. , Rybkin , O. , Daniilidis , K. , Abbeel , P. , Hafner , D. , and Pathak , D. Planning to explore via self-supervised world models . In International Conference on Machine Learning , pp . 8583–8592 . PMLR , 2020 . Seo , Y. , Chen , L. , Shin , J. , Lee , H. , Abbeel , P. , and Lee , K. State entropy maximization with random encoders for efficient exploration . In Meila , M. and Zhang , T . ( eds . ) , Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pp . 9443–9454 . PMLR , 18– 24 Jul 2021 . URL https : //proceedings.mlr . press/v139/seo21a.html . Stadie , B. C. , Levine , S. , and Abbeel , P. Incentivizing ex- ploration in reinforcement learning with deep predictive models . arXiv preprint arXiv:1507.00814 , 2015 . Strehl , A. L. and Littman , M. L. An analysis of model- based interval estimation for markov decision processes . Journal of Computer and System Sciences , 74 ( 8 ) :1309– 1331 , 2008 . Tang , H. , Houthooft , R. , Foote , D. , Stooke , A. , Xi Chen , O. , Duan , Y. , Schulman , J. , DeTurck , F. , and Abbeel , P. # exploration : A study of count-based exploration for deep reinforcement learning . Advances in neural information processing systems , 30 , 2017 . Tunyasuvunakool , S. , Muldal , A. , Doron , Y. , Liu , S. , Bohez , S. , Merel , J. , Erez , T. , Lillicrap , T. , Heess , N. , and Tassa , Y. dm control : Software and tasks for continuous control . Software Impacts , 6:100022 , 2020 . ISSN 2665-9638. https : . doi : https : //www.sciencedirect.com/ URL . 10 Go Beyond Imagination : Maximizing Episodic Reachability with World Models Wang , T. , Bao , X. , Clavera , I. , Hoang , J. , Wen , Y. , Lan- glois , E. , Zhang , S. , Zhang , G. , Abbeel , P. , and Ba , J. Benchmarking model-based reinforcement learning . arXiv preprint arXiv:1907.02057 , 2019 . Yarats , D. , Fergus , R. , Lazaric , A. , and Pinto , L. Reinforce- ment learning with prototypical representations . In Inter- national Conference on Machine Learning , pp . 11920– 11931 . PMLR , 2021a . Yarats , D. , Zhang , A. , Kostrikov , I. , Amos , B. , Pineau , J. , and Fergus , R. Improving sample efficiency in model- free reinforcement learning from images . In Proceedings of the AAAI Conference on Artificial Intelligence , vol- ume 35 , pp . 10674–10681 , 2021b . Zha , D. , Ma , W. , Yuan , L. , Hu , X. , and Liu , J . Rank the episodes : A simple approach for exploration in procedurally-generated environments . arXiv preprint arXiv:2101.08152 , 2021 . Zhang , T. , Xu , H. , Wang , X. , Wu , Y. , Keutzer , K. , Gonza- lez , J. E. , and Tian , Y. Noveld : A simple yet effective exploration criterion . Advances in Neural Information Processing Systems , 34 , 2021 . 11 Go Beyond Imagination : Maximizing Episodic Reachability with World Models A . Implementation Details A.1 . Experiments on Minigrid Baselines Implementations of GoBI , NovelD ( Zhang et al. , 2021 ) , RIDE ( Raileanu & Rockt¨aschel , 2020 ) , RND ( Burda et al. , 2018 ) , and EC ( Savinov et al. , 2018 ) are built on the official codebase of NovelD . For fair compar- isons , only the intrinsic reward rint differs among the meth- ods and they all use the same base algorithm IMPALA ( Es- peholt et al. , 2018 ) . At the same time , all the experiments are run with the same compute resource with Nvidia TI- TAN X GPU and 40 CPUs . For NovelD , we rerun their official code to get the results of Minigrid MultiRoom and KeyCorridor . For the experiments on ObstructedMaze , we did not find the proper hyper-parameters to fully reproduce their results . Therefore , we directly take the results reported in their paper . For RIDE and RND , we run the code in the official codebase of NovelD . For EC ( Savinov et al. , 2018 ) , their original paper does not include experiments on Minigrid environments . Therefore , we implement our own version and tune the hyper-parameters with grid search . The intrinsic reward functions of GoBI and the baselines are listed below : • GoBI : ( mt+1 − mt ) / ( cid:112 ) N ( ot+1 ) , where mt is the size of the episodic buffer M and N ( ot+1 ) is the lifelong count of the observation ot+1 starting from the begin- ning of training . • RND : ∥ϕ ( ot+1 ) − ˆϕ ( ot+1 ) ∥2 , which is the difference between a fixed random network ˆϕ and a trained state embedding network ϕ . Here , ϕ is trained to minimize the same error . • NovelD : max [ novelty ( ot+1 ) − α · novelty ( ot ) , 0 ] ∗ 1 { N epi ( st+1 = 1 ) } . They apply RND to measure the novelty of ot , i.e. , novelty ( ot ) = ∥ϕ ( ot ) − ˆϕ ( ot ) ∥2 . N epi ( st+1 = 1 ) checks if the agent visits state st+1 for the first time in an episode . Notice that they use the full environment information , i.e . everything in the grid world instead of only the 7×7 partially-observable view . Therefore N epi counts st+1 instead of ot+1 . • RIDE : ∥ϕ ( ot ) − ϕ ( ot+1 ) ∥2/ ( cid:112 ) N epi ( st+1 ) , where ϕ is the state embedding network trained to minimize the prediction error of an inverse and a forward dynamics . N epi indicates the episodic counts . Same as NovelD , in RIDE , they also use the state information st+1 for episodic count . Policy and Value Function Training For fair compar- isons , the policy network and value function network are the same for all approaches . The input observations of di- mension 7 × 7 × 3 are put into a shared feature extraction network , which includes three convolutional layers of ker- nel size= 3 × 3 , padding= 1 , channel=32 , 128 , 512 , and stride= 1 , 2 , 2 respectively with ELU activation . The fea- tures are then flattened and put through 2 linear layers with 1024 units and ReLU activation , and an LSTM layer with 1024 units . This shared feature is passed separately to 2 fully-connected layers with 1024 units to output action dis- tribution and value estimation . Dynamics model For our implementation of the dynamics model , our input is the panorama of the current step . To get the panorama , we let the agent rotate for 3 times and con- catenate the 4 observations to get inputs of size 28 × 7 × 3 . It is then passed to a feature extraction module that has the same structure as our policy and value function networks , except that the input to the first linear layer is 4 × 1024 . We then concatenate it with actions and put it through a de- coder with 2 linear layers of sizes 256 and 512 , and reshape back to 7 × 7 × 3 to get a predicted observation . We pre- train the dynamics model using 1e5 ( panot , at , ot+1 ) pairs collected by a random policy . RIDE also requires training dynamics models for the state embedding network ϕ . The input of their dynamics model is the state embedding and action . The forward model contains two fully-connected layers with 256 and 128 units activated by ReLU . The in- verse dynamics model contains two fully-connected layers with 256 units and a ReLU activation function . Its input is the state embeddings of two consecutive steps . Hash Functions We directly apply the default Python hashing function to hash the 7 × 7 × 3 observations and predicted future observations before adding them to the episodic buffer . State embedding NovelD , RIDE , and RND all require training a state embedding network ϕ . The input is the observation in MiniGrid with dimension 7×7×3 . It contains three convolutional layers with kernel size= 3 × 3 , padding = 1 , stride = 1 , 2 , 2 , number of channels = 32 , 128 , 512 respectively . The activation function is ELU . Following the convolutional layers are two linear layers of 2048 and 1024 units with ReLU activation . • EC : β − C ( M , ot+1 ) , where C ( M , ot+1 ) is the 90-th percentile similarity scores between ot+1 and all the observations in the episodic buffer M. The similar- ity scores are calculated using a pre-trained episodic curiosity module . β is a hyper-parameter . Visitation Count For GoBI , N ( o ) stores the flattened 7 × 7 × 3 observations of each step . And for NovelD and RIDE , they count the full states at episodic level , whose shape varies from environment to environment . For example , the shape is 25 × 25 × 3 for MultiRoom environments . 12 Go Beyond Imagination : Maximizing Episodic Reachability with World Models Hyper-parameters Table 1 shows the values of hyper- parameters shared across different methods . Parameter name Batch Size Optimizer Learning Rate LSTM Steps Discount Factor γ Weight of Policy Entropy Loss Weight of Value Function Loss Value 32 RMSProp 0.0001 100 0.99 0.0005 0.5 Table 1 . Hyper-parameters for experiments on Minigrid . These hyper-parameters are shared across all the methods For all of our experiments using GoBI on Minigrid , we set the intrinsic reward coefficient λ = 0.01 and k = 1 , which means only forwarding the dynamics model by 1 step . At the same time , as the action space is small and discrete , instead of randomly sampling some actions , we directly predict the future observations using all 7 possible actions . We list the hyper-parameter choices of intrinsic decay factor ρ in Table 2 . The value of ρ is chosen to make the intrinsic reward large at the beginning of training and near-zero at the end of the training . Parameters Forward Step k Intrinsic Decay ρ ˆfϕ Optimizer ˆfϕ Learning Rate Value 1 6e−7 for MR-N7S8 ; 8e−7 for MR-N12S10 , MR-N6 ; 1.5e−6 for KC-S3R3 ; 5e−7 for KC-S4R3 , KC-S5R3 3e−7 for KC-S6R3 , OM-2Dlh 2e−7 for OM-1Q , OM-2Dlhb , OM-2Q 5e−8 for OM-Full Adam 5e−4 Table 2 . The hyper-parameters of GoBI for experiments on Mini- grid . For NovelD , we set λ = 0.05 for all the environments as is suggested in their official codebase . For RIDE , we use λ = 0.1 on KeyCorridor-S3R3 and λ = 0.5 on all other environments . For RND , we set λ = 0.1 on all the environments . For EC , we make λ = 0.01 so that the initial average intrinsic reward of EC is similar to ours . experiments apply the same base reinforcement learning algorithm RAD ( Laskin et al. , 2020 ) . For RE3 , we rerun their official code to get the results on all four environments . For ICM and RND , we follow the implementation details listed in RE3 to implement them to be compatible with DeepMind Control tasks . For a fair comparison , only the intrinsic reward design differs among the methods . The in- trinsic reward functions of GoBI and the baselines are listed below : • GoBI : ( mt+1 −mt ) ×log ( ||yi −yk−N N ||2 +1 ) , where mt is the size of the episodic buffer M. The latter part is the RE3 intrinsic reward which we introduce below . i i • RE3 : log ( ||yi − yk−N N ||2 + 1 ) , where yi = fθ ( si ) is a fixed representation outputs from a randomly initial- ized encoder and yk−N N is a set of k-nearest neighbors of yi among all the collected y ’ s from the beginning of training . i • ICM : η 2 || ˆϕ ( ot+1 ) − ϕ ( ot+1 ) ||2 2 , where η is a scaling factor . ϕ ( o ) is a feature vector that is jointly optimized with a forward prediction model and an inverse dynam- ics model and ˆϕ ( o ) predicts the feature encoding at time step t + 1 . • RND : ∥ϕ ( ot+1 ) − ˆϕ ( ot+1 ) ∥2 , which is the difference between a fixed random network ˆϕ and a trained state embedding network ϕ . Here , ϕ is trained to minimize ∥ϕ ( ot+1 ) − ˆϕ ( ot+1 ) ∥2 . Architecture The observation size of all the environments is 84×84×3 . The encoder architecture follows the same one as in ( Yarats et al. , 2021b ) , which contains 4 convolutional layers of 3 × 3 kernels , channel=32 , and stride=2 , 1 , 1 , 1 with ReLU activations . The output is then passed to a fully- connected layer and normalized by LayerNorm . Dynamics Model For the forward dynamics model that we use to generate future predictions , we apply the same world model structure as in Dreamer ( Hafner et al. , 2019 ) . The input size of Dreamer is 64 × 64 × 3 . We down-sample the input observations to 64 × 64 instead of tuning the world model layers . We train the dynamics model together with the RL policy in an online manner instead of pre-train it because it takes many episodes for the predictions to be visually reasonable . Therefore it does not add extra effort to determine how many data should we collect to pre-train the dynamics model . A.2 . Experiments on Deepmind Control Suites Baselines Implementations of GoBI , RE3 ( Seo et al. , 2021 ) , ICM ( Pathak et al. , 2017 ) , and RND ( Burda et al. , 2018 ) are built on the official codebase of RE3 . All the Image hashing As the observations are images in high- dimensional space and the predictions are usually not accu- rate , we hash the images to lower dimension to avoid taking too much space and to collapse similar observations and 13 Go Beyond Imagination : Maximizing Episodic Reachability with World Models predictions . Following ( Tang et al. , 2017 ) , we use the sim- ple SimHash function to map the images to 50 bits . More specifically , we project the flattened images to a random initialized vector and use the signs of output vector values as the hash code . Hyper-parameters Table 3 shows the values of hyper- parameters shared across different methods . decreases to very small . Therefore sometimes it is hard for the agent to learn anything useful , resulting in unsatisfac- tory performance . Meanwhile , if ρ is too small , for example when ρ = 5e − 7 , the intrinsic reward will be too large at later stage of training and make the agent focus less on the extrinsic reward . Therefore the policy may converge slower . Parameter name Augmentation Observation Size Action Repeat Replay Buffer Size Initial Random Exploration Steps Frame Stack Actor Learning Rate Critic Learning Rate Batch Size # Nearest Neighbors Critic Target Update Freq Value Crop ( 84 , 84 ) 2 100000 1000 3 0.0002 0.0002 512 3 2 Table 3 . Hyper-parameters for experiments on DeepMind Control Suites . These hyper-parameters are shared across all the methods For the intrinsic reward coefficients , we follow the best choices reported in RE3 . For GoBI , we apply the same intrinsic reward coefficient λ and intrinsic reward decay ρ as the ones in RE3 for fair comparison . The intrinsic rewards that are specific to our method is shown in Table 4 . For the number of random actions n , we perform hyper-parameter search over { 3 , 5 , 10 , 20 } and find that n = 5 perform well across all the tasks . For the number of forward steps k , we perform hyper-parameter search over { 1 , 2 , 3 , 5 } and report the ones with the best results . Parameter name # Forward Step k # Random Actions n Value 3 for pendulum-swingup ; 1 for others 5 Table 4 . Hyper-parameters for experiments on DeepMind Control Suites . These hyper-parameters are specific to our method . B. Hyper-Parameters B.1 . Intrinsic reward decay Figure 10 . Training performance of GoBI on Minigrid Multiroom- N12-S10 with different intrinsic reward decay ρ . Figure 11 . Training performance of GoBI on Minigrid Multiroom- N7-S8 with different n randomly sampled actions per step . B.2 . Number of randomly sampled actions In the Minigrid experiments reported in Section 3 , we do not randomly sample actions because Minigrid has a small discrete action space with only 7 actions . Therefore we directly predict future observations of all 7 actions . How- ever , we also report the results with n = 3 , 5 , 10 , 15 random actions in Figure 11 . To summarize , n = 5 , 10 , 15 all have similar performance on Minigrid , while a larger n makes the wallclock training time longer . n = 3 is slightly slower at the early stage , but still outperforms the previous state- of-the-arts . Overall , n = 5 would be a good choice for the Minigrid environments . In Figure 10 , we show the training performance with differ- ent intrinsic reward decay ρ . The choice of ρ is to balance between the relative importance of the extrinsic and intrinsic reward . If ρ is too large , for example when ρ = 1e − 6 , before the agent finds any goal , the intrinsic reward already C. Exploration Behaviour Comparison between GoBI and EC In Figure 12 , we visualize the policy visitation heatmaps of GoBI and EC ( Savinov et al. , 2018 ) on a MultiRoom 14 n r u t e R e g a r e v A 0.6 0.4 0.2 0.0 0.0 MultiRoom-N12-S10 0.3 0.2 0.6 environment steps ( 1e7 ) 0.5 = 1e-6 = 9e-7 = 8e-7 = 7e-7 0.8 = 6e-7 = 5e-7 MultiRoom-N7-S8 n r u t e R e g a r e v A 0.6 0.4 0.2 0.0 0.0 0.2 0.3 0.5 0.6 0.8 1e7 n=3 n=5 n=10 n=15 Go Beyond Imagination : Maximizing Episodic Reachability with World Models Figure 12 . Heatmaps of example trajectories at different training steps in Minigrid-Multiroom-N7-S8 environment . Algorithm Wallclock Time ( hours ) NovelD GoBI ( ours ) 5.46 ( ±0.058 ) 10.65 ( ±0.082 ) Table 5 . Wallclock training time comparison in hours between NovelD and GoBI . environment from Minigrid . Although EC fails to learn an optimal policy , we can still capture its preference from the heatmaps . An agent trained with EC prefers going to the corners of the room , which generally have lower similarity scores than the states in the middle of the room . However , if the similarity scores are not low enough for the states to be added to the episodic buffer , it will continue staying at the states to maximize its intrinsic reward . We tried to tune the similarity score threshold using grid search but still have not find a good hyper-parameter choice for it because the similarity score at different corners does not share a consistent value . Unlike EC , we can see from the figure that GoBI chooses not to visit the border of the room early on in training , as the information on the border are easily predictable from the information in the middle of the room . Figure 14 . An illustrative example of why R2 does not work well to encourage efficient exploration . In this example , our goal is to include all the states into the episodic buffer quickly . GoBI can move directly to the center in one step for maximum intrinsic reward , while R2 may choose to take extra steps for exploration since it mainly focuses on whether the episodic buffer expands or not . Figure 13 . Training performance comparison on MultiRoom-N7- S8 environment among 1 ) use a fixed pre-trained dynamics model , 2 ) use a pre-trained dynamics model , and fine-tune it online , 3 ) no pre-training , directly train the dynamics model together with policy training . D. Wallclock Training Time Table 5 shows the wallclock time needed to train NovelD and our method for 10M Minigrid environment steps . GoBI requires about 2x the wallclock time needed to train NovelD for the same number of environment steps . E. Dynamics Training In the experiment section 3 , we report the results of apply- ing a pre-trained forward dynamics for GoBI on Minigrid . However , the forward dynamics model can also be trained together with policy training . In Figure 13 , we report the results of an ablation study on a Minigrid environment of 3 settings : 1 ) use a pre-trained dynamics model , and keep it fixed when training the policy , 2 ) pre-train a dynamics model , and fine-tune it when training the policy , 3 ) no pre- training , directly train the dynamics model in an online manner . In summary , all 3 versions work similarly , but due to the fact that training the dynamics model online will add 15 GoBI ( Ours ) Training Environment when Episode = 2 Goal Agent Minigrid-MultiRoom-N7S8-v0 Episodic Curiosity ( EC ) s t n u o C d e t i s V i 2.3M 2.1M 4.5M 4.0M 8.9M Training Steps 5.9M 8.0M Unseen Seen MultiRoom-N7-S8 n r u t e R e g a r e v A 0.6 0.4 0.2 0.0 0.0 0.2 0.3 0.5 0.6 0.8 1e7 pre_train+fine_tune no_pre_train no_fine_tune Model t = 0 t = 1 t = 2 t = 3 R2 GoBI ( Ours ) s0 s0 s2 s3 s2 s0 s1 s0 s1 s0 s1 s1 s0 st Visited State Reachable State Non-reachable State Trajectory ( Directed via Temporal Order ) Observed Transition Path ( Bidirectional ) Unreachable Transition Path ( Bidirectional ) Go Beyond Imagination : Maximizing Episodic Reachability with World Models extra wall clock training time , we use option 1 in our main experiments . F. Ablation Study Illustration In this section , we provide an illustrative example of why only considering whether new states are added to the episodic buffer or not , i.e. , R2 in Section 3.3 , works way worse than our method . The example is shown in Figure 14 . If these 9 states are only a small part of the environment , we want a policy that explore this part as quickly as possible - mark all the states as reachable as quickly as possible . How- ever , in order to maximize its step-wise intrinsic reward , an agent trained with R2 will go along the border to only add a few new states to the episodic buffer at a time , which wastes many unnecessary steps so is not beneficial for exploration . 16","['go', 'imagination', 'maximize', 'episodic', 'reachability', 'world', 'model', 'run', 'honglak', 'g', 'u', 'l', 'c', 'r', 'abstract', 'efficient', 'exploration', 'challenging', 'topic', 'rein', 'forcement', 'learning', 'especially', 'sparse', 'reward', 'task', 'deal', 'reward', 'sparsity', 'people', 'commonly', 'apply', 'intrinsic', 'reward', 'motivate', 'agent', 'explore', 'state', 'space', 'efficiently', 'paper', 'introduce', 'new', 'intrinsic', 'reward', 'design', 'call', 'gobi', 'go', 'imagination', 'combine', 'traditional', 'lifelong', 'novelty', 'motivation', 'episodic', 'intrinsic', 'reward', 'design', 'maximize', 'stepwise', 'reachability', 'expansion', 'specifically', 'apply', 'learn', 'world', 'model', 'generate', 'predict', 'future', 'state', 'random', 'action', 'state', 'unique', 'prediction', 'episodic', 'memory', 'sign', 'high', 'intrinsic', 'reward', 'method', 'greatly', 'outperform', 'previous', 'stateoftheart', 'method', 'challenging', 'minigrid', 'navigation', 'task', 'improve', 'sample', 'efficiency', 'loco', 'motion', 'task', 'deepmind', 'control', 'introduction', 'efficient', 'exploration', 'state', 'space', 'fundamental', 'chal', 'lenge', 'reinforcement', 'learning', 'especially', 'environment', 'reward', 'sparse', 'schulman', 'absent', 'abbeel', 'parisi', 'reward', 'sparsity', 'make', 'algorithm', 'easy', 'fail', 'lack', 'useful', 'signal', 'policy', 'update', 'riedmiller', 'florensa', 'com', 'mon', 'approach', 'exploration', 'introduce', 'selfmotivate', 'intrinsic', 'reward', 'state', 'visitation', 'count', 'strehl', 'littman', 'kolter', 'prediction', 'ror', 'stadie', 'pathak', 'intrinsic', 'reward', 'design', 'measure', 'life', 'long', 'state', 'novelty', 'prioritize', 'visit', 'state', 'less', 'ai', 'correspon', 'yao', 'violetfy', 'run', 'peng', 'dence', 'eecsumichedu', 'honglak', 'proceeding', 'th', 'international', 'conference', 'machine', 'learn', 'pmlr', 'copyright', 'author', 'visit', 'start', 'beginning', 'training', 'method', 'achieve', 'great', 'improvement', 'hardexploration', 'task', 'generally', 'work', 'well', 'single', 'ton', 'environment', 'training', 'evaluation', 'environ', 'ment', 'however', 'poor', 'generalization', 'performance', 'reinforcement', 'learning', 'unseen', 'environ', 'ment', 'nowadays', 'researcher', 'pay', 'attention', 'procedurallygenerate', 'environ', 'ment', 'cobbe', 'fletberliac', 'nature', 'task', 'remain', 'environ', 'ment', 'randomly', 'construct', 'new', 'episode', 'example', 'mazelike', 'environment', 'different', 'maze', 'structure', 'make', 'rare', 'agent', 'encounter', 'observation', 'different', 'episode', 'therefore', 'lifelong', 'novelty', 'intrinsic', 'motivation', 'usually', 'fail', 'hard', 'procedurallygenerate', 'environment', 'kind', 'raileanu', 'zha', 'et', 'agent', 'trap', 'newlygenerate', 'state', 'inspire', 'human', 'frequent', 'use', 'shortterm', 'memory', 'dersen', 'eichenbaum', 'avoid', 'repeatedly', 'visit', 'space', 'recent', 'work', 'propose', 'derive', 'intrin', 'sic', 'reward', 'episodic', 'level', 'badia', 'raileanu', 'zha', 'et', 'episodic', 'intrinsic', 'reward', 'give', 'bonus', 'large', 'episodiclevel', 'state', 'space', 'visitation', 'coverage', 'therefore', 'encourage', 'visit', 'many', 'state', 'possible', 'episode', 'however', 'visit', 'state', 'necessarily', 'mean', 'efficient', 'episodiclevel', 'exploration', 'notice', 'state', 'visitation', 'unnecessary', 'avoid', 'predictable', 'episodic', 'ory', 'example', 'navigate', 'house', 'find', 'fridge', 'open', 'door', 'find', 'empty', 'room', 'need', 'go', 'anymore', 'easily', 'predict', 'state', 'room', 'intuitively', 'speak', 'move', 'around', 'empty', 'room', 'inspiration', 'propose', 'design', 'episodic', 'trinsic', 'reward', 'maximize', 'number', 'visit', 'state', 'episode', 'also', 'consider', 'state', 'visit', 'predict', 'episodic', 'memory', 'precisely', 'maintain', 'episodic', 'buffer', 'store', 'visit', 'state', 'well', 'state', 'reachable', 'visit', 'state', 'time', 'step', 'get', 'reachable', 'state', 'go', 'imagination', 'maximize', 'episodic', 'reachability', 'world', 'model', 'figure', 'illustration', 'gobi', 'work', 'minigrid', 'environment', 'upperleft', 'corner', 'red', 'triangle', 'indicate', 'position', 'orientation', 'agent', '×', 'partiallyobservable', 'view', 'highlight', 'pretraine', 'stage', 'collect', 'datum', 'use', 'random', 'policy', 'train', 'forward', 'dynamic', 'model', 'panot', 'ot1', 'panot', 'denote', 'panoramic', 'view', 'define', 'section', 'policy', 'training', 'stage', 'apply', 'ˆf', 'predict', 'observation', 'future', 'time', 'step', 'random', 'action', 'step', 'add', 'new', 'one', 'episodic', 'buffer', 'take', 'change', 'size', 'episodic', 'intrinsic', 'reward', 'repi', 'lifelong', 'intrinsic', 'reward', 'countbase', 'intrinsic', 'reward', 'gobi', '∗', 'repi', 'train', 'world', 'model', 'forward', 'dynamic', 'function', 'apply', 'random', 'action', 'learn', 'dynamic', 'model', 'pre', 'dict', 'future', 'state', 'prediction', 'add', 'episodic', 'buffer', 'already', 'use', 'change', 'size', 'episodic', 'buffer', 'episodic', 'intrinsic', 'reward', 'follow', 'many', 'previous', 'work', 'weight', 'episodic', 'trinsic', 'reward', 'lifelong', 'intrinsic', 'reward', 'badia', 'countbase', 'reward', 'newly', 'propose', 'intrinsic', 'reward', 'design', 'gobi', 'go', 'imagination', 'agent', 'expect', 'explore', 'state', 'space', 'training', 'discover', 'extrinsic', 'reward', 'learn', 'act', 'efficient', 'manner', 'single', 'episode', 'avoid', 'trap', 'seemingly', 'novel', 'state', 'contribution', 'work', 'highlight', 'low', 'propose', 'novel', 'way', 'combine', 'world', 'model', 'episodic', 'memory', 'formulate', 'effective', 'episodic', 'intrinsic', 'reward', 'design', 'procedurally', 'generate', 'minigrid', 'environment', 'gobi', 'greatly', 'improve', 'training', 'sample', 'efficiency', 'comparison', 'prior', 'stateoftheart', 'intrinsic', 'reward', 'function', 'gobi', 'extend', 'well', 'deepmind', 'control', 'suite', 'tunyasuvunakool', 'highdimensional', 'visual', 'input', 'show', 'promise', 'result', 'sparsereward', 'continuous', 'control', 'task', 'analyze', 'design', 'gobi', 'present', 'extensive', 'ablation', 'show', 'contribu', 'tion', 'component', 'method', 'consider', 'reinforcement', 'learning', 'problem', 'frame', 'markov', 'decision', 'process', 'r', 'denote', 'state', 'space', 'action', 'space', '×', 'state', 'transition', 'function', 'r', '×', '×', 'r', 'reward', 'function', 'reward', 'discount', 'factor', 'step', 'state', 'environment', 'denote', 'agent', 'generate', 'action', 'interact', 'environment', 'environment', 'transit', 'next', 'underlying', 'state', 'st1', 'apart', 'new', 'state', 'st1', 'environment', 'also', 'return', 'extrinsic', 'reward', 'rext', 'describe', 'well', 'agent', 'react', 'sparsereward', 'task', 'rext', 'usually', 'work', 'follow', 'previous', 'work', 'train', 'algorithm', 'rext', 'λ∗rint', 'selfmotivated', 'intrinsic', 'reward', 'hyperparameter', 'control', 'relative', 'importance', 'intrinsic', 'extrinsic', 'reward', 'go', 'imagination', 'reachable', 'state', 'episodic', 'buffer', 'intrinsic', 'ward', 'design', 'aim', 'exploit', 'information', 'hide', 'neighbourhood', 'state', 'define', 'state', 'k', 'step', 'reachable', 'agent', 'reach', 'b', 'k', 'time', 'step', 'training', 'process', 'new', 'episode', 'initialize', 'empty', 'episodic', 'memory', 'buffer', 'time', 'step', 'hash', 'well', 'state', 'reachable', 'denote', 'set', 'contain', 'hash', 'environment', '🚶', 'action', 'space', 'leave', 'right', 'forward', 'pickup', 'drop', 'toggle', '⬅', '➡', '🚶', '🤏', '✋', '🛠', '✅', 'encoder', 'decoder', 'policy', 'net', 'r', 'r', 'e', 'r', 'r', 'e', 'episodic', 'memory', 'expansion', '🚶', 'next', 'obs', 'random', 'action', 'n', 'already', 'new', 'state', 'pretrain', 'forward', 'dynamic', 'model', 'stage', 'count', 'episodic', 'reachability', 'maximization', 'stage', 'go', 'imagination', 'maximize', 'episodic', 'reachability', 'world', 'model', 'figure', 'illustration', 'episodic', 'buffer', 'update', 'consider', 'reachable', 'state', 'k', 'time', 'step', 'away', 'agent', 'move', 'episodic', 'buffer', 'shaded', 'green', 'expand', 'new', 'reachable', 'state', 'shade', 'yellow', 'formally', 'indicate', 'set', 'state', 'reachable', 'notice', 'state', 'trajectory', 'also', 'add', 'buffer', 'code', 'reachable', 'state', 'update', 'store', 'hash', 'code', 'instead', 'directly', 'store', 'state', 'alleviate', 'poten', 'tial', 'memory', 'issue', 'buffer', 'illustrate', 'process', 'figure', 'agent', 'reach', 'state', 'add', 'state', 'reachable', 'forward', 'dynamic', 'real', 'environment', 'common', 'access', 'neighbourhood', 'relationship', 'state', 'however', 'learn', 'world', 'model', 'train', 'forward', 'dynamic', 'model', 'st1', 'predict', 'state', 'reachable', 'forward', 'dynamic', 'pretraine', 'use', 'datum', 'collect', 'random', 'policy', 'train', 'online', 'together', 'policy', 'training', 'train', 'policy', 'time', 'step', 'generate', 'k', 'random', 'action', 'use', 'learn', 'dynamic', 'ˆfϕ', 'predict', 'state', 'future', 'step', 'hash', 'current', 'state', 'well', 'ˆsn', 'predict', 'future', 'state', 'add', 'hash', 'code', 'episodic', 'buffer', 'buffer', 'apart', 'alleviate', 'potential', 'memory', 'issue', 'mention', 'last', 'paragraph', 'use', 'hash', 'function', 'also', 'mitigate', 'noise', 'introduce', 'ˆf', 'learn', 'dynamic', 'model', 'prediction', 'reachable', 'state', 'usually', 'perfect', 'however', 'experiment', 'section', 'show', 'even', 'imperfect', 'prediction', 'method', 'improve', 'training', 'sample', 'efficiency', 'lot', 'tk', 'ˆsn', 't1', 'ˆs1', 'episodic', 'novelty', 'aim', 'design', 'episodiclevel', 'elty', 'reward', 'guide', 'agent', 'extend', 'frontier', 'predict', 'reachable', 'space', 'efficiently', 'discover', 'state', 'visit', 'predictable', 'episode', 'specifically', 'denote', 'size', 'episodic', 'buffer', 'time', 'step', 'design', 'reachabilitybased', 'bonus', 'repi', 'encourage', 'agent', 'find', 'unex', 'plored', 'region', 'time', 'step', 'agent', 'expect', 'reach', 'state', 'reachable', 'new', 'state', 'current', 'episode', 'intrinsic', 'reward', 'formulation', 'far', 'weight', 'episodic', 'intrinsic', 'reward', 'lifelong', 'intrinsic', 'reward', 'encourage', 'agent', 'explore', 'region', 'well', 'explore', 'past', 'formally', 'propose', 'intrinsic', 'reward', 'gobi', 'define', '∗', 'denote', 'lifelong', 'intrinsic', 'reward', 'note', 'framework', 'compatible', 'choice', 'lifelong', 'intrinsic', 'reward', 'specifically', 'use', 'simple', 'count', 'base', 'reward', 'cid112', 'st1', 'navigation', 'experiment', 'minigrid', 'environment', 'denote', 'count', 'st1', 'start', 'train', 'ing1', 'experiment', 'deepmind', 'control', 'suite', 'tun', 'yasuvunakool', 'use', 'stateoftheart', 'seo', 'estimate', 'state', 'random', 'encoder', 'intrinsic', 'decay', 'intrinsic', 'reward', 'expect', 'asymptotically', 'consistent', 'influence', 'policy', 'learning', 'later', 'stage', 'training', 'result', 'suboptimal', 'policy', 'guarantee', 'policy', 'learning', 'focus', 'extrinsic', 'reward', 'training', 'proceed', 're3', 'seo', 'author', 'apply', 'exponential', 'decay', 'schedule', 'intrinsic', 'reward', 'decrease', 'time', 'countbase', 'reward', 'theoretically', 'converge', 'enough', 'exploration', 'decrease', 'quite', 'slowly', 'procedurallygenerate', 'environment', 'therefore', 'also', 'apply', 'intrinsic', 'reward', 'decay', 'calculate', 'gobi', 'crease', 'intrinsic', 'reward', 'coefficient', 'training', 'summarize', 'method', 'illustrate', 'training', 'process', 'minigrid', 'navigation', 'task', 'figure', 'environment', 'partially', 'observable', 'eg', 'mini', 'grid', 'agent', 'observe', 'pixel', 'local', 'view', 'environment', 'substitute', 'state', 'observation', 'ot', 'calculate', 'trinsic', 'reward', 'action', 'move', 'episodic', 'buffer', 'nonreachable', 'space', 'new', 'reachable', 'space', 'reachable', 'state', 'visited', 'state', 'reachable', 'state', 'nonreachable', 'state', 'trajectory', 'observe', 'transition', 'path', 'unreachable', 'transition', 'path', 'go', 'imagination', 'maximize', 'episodic', 'reachability', 'world', 'model', 'go', 'imagination', 'input', 'intrinsic', 'reward', 'coefficient', 'λ0', 'forward', 'pre', 'diction', 'step', 'k', 'number', 'random', 'action', 'intrinsic', 'reward', 'decay', 'parameter', 'initialize', 'policy', 'πθ', 'dynamic', 'model', 'replay', 'optional', 'collect', 'episode', 'πθ', 'train', 'ˆfϕ', 'prediction', 'loss', 'episode', 'convergence', 'initialize', 'episodic', 'buffer', 'λ0', '∗', 'execute', 'πθ', 'environment', 'get', 'transition', 'pair', 'st1', 'rext', 'size', '∪', 'hash', 'get', 'high', 'intrinsic', 'reward', 'corresponding', 'similarity', 'score', 'low', 'low', 'enough', 'similarity', 'score', 'add', 'buffer', 'method', 'similar', 'high', 'level', 'different', 'design', 'ex', 'ample', 'agent', 'stand', 'front', 'empty', 'blind', 'alley', 'dead', 'end', 'agent', 'train', 'gobi', 'benefit', 'go', 'deep', 'blind', 'alley', 'predict', 'reachable', 'add', 'episodic', 'buffer', 'already', 'however', 'encourage', 'go', 'end', 'blind', 'alley', 'reach', 'state', 'low', 'similarity', 'score', 'high', 'intrinsic', 'reward', 'even', 'go', 'empty', 'blind', 'alley', 'beneficial', 'exploration', 'waste', 'time', 'use', 'explore', 'part', 'environment', 'c', 'present', 'visitation', 'heatmap', 'poli', 'cie', 'learn', 'find', 'prefer', 'go', 'room', 'corner', 'well', 'match', 'explanation', 'generate', 'random', 'action', '∪', 'hash', 'ˆfϕ', 'tt′', 'ai', 'end', 'size', '∗', '∪', 'st1', 'rext', 'λ', 'rint', 'end', 'update', 'πθ', 'objective', 'update', 'ˆfϕ', 'prediction', 'loss', 'end', 'conceptual', 'advantage', 'gobi', 'prior', 'work', 'previous', 'work', 'include', 'ride', 'raileanu', 'also', 'combine', 'episodic', 'intrinsic', 'reward', 'lifelong', 'novelty', 'however', 'focus', 'episodiclevel', 'state', 'itation', 'example', 'noveld', 'assign', 'nonzero', 'ward', 'state', 'visit', 'first', 'time', 'episode', 'however', 'notice', 'state', 'visitation', 'necessary', 'agent', 'goal', 'exploration', 'gather', 'information', 'state', 'therefore', 'state', 'easily', 'predictable', 'episodic', 'memory', 'visit', 'really', 'help', 'acquire', 'information', 'en', 'vironment', 'figure', 'plot', 'visitation', 'heatmap', 'gobi', 'noveld', 'demonstrate', 'different', 'exploration', 'behaviour', 'method', 'method', 'closely', 'relate', 'work', 'sure', 'episodic', 'curiosity', 'author', 'train', 'reachability', 'network', 'take', 'arbitrary', 'state', 'output', 'similarity', 'score', 'indicate', 'state', 'indicate', 'totally', 'different', 'network', 'train', 'use', 'collect', 'episode', 'mark', 'temporally', 'close', 'state', 'positive', 'example', 'temporally', 'far', 'one', 'negative', 'sample', 'meanwhile', 'also', 'maintain', 'episodic', 'buffer', 'state', 'compare', 'state', 'buffer', 'minigrid', 'deepmind', 'control', 'figure', 'rendering', 'environment', 'use', 'work', 'leave', 'grid', 'world', 'navigation', 'task', 'require', 'object', 'interaction', 'right', 'deepmind', 'control', 'task', 'visual', 'observation', 'experiment', 'section', 'evaluate', 'gobi', 'domain', 'procedurallygenerate', 'minigrid', 'environment', 'chevalier', 'hardexploration', 'task', 'comotion', 'task', 'deepmind', 'control', 'suite', 'vunakool', 'experiment', 'design', 'answer', 'follow', 'research', 'question', 'gobi', 'perform', 'previous', 'stateoftheart', 'intrinsic', 'ward', 'design', 'term', 'trainingtime', 'sample', 'efficiency', 'challenge', 'procedurallygenerate', 'environment', 'gobi', 'successfully', 'extend', 'complex', 'continuous', 'domain', 'highdimensional', 'observation', 'example', 'control', 'task', 'visual', 'observation', 'compo', 'nent', 'intrinsic', 'reward', 'contribute', 'performance', 'influence', 'accuracy', 'learned', 'world', 'model', 'method', 'minigrid', 'navigation', 'task', 'set', 'partiallyobservable', 'procedurally', 'go', 'imagination', 'maximize', 'episodic', 'reachability', 'world', 'model', 'figure', 'visitation', 'heatmap', 'different', 'training', 'stage', 'figure', 'compare', 'policy', 'behaviour', 'gobi', 'noveld', 'dark', 'red', 'color', 'mean', 'plentiful', 'visitation', 'mean', 'agent', 'see', 'space', 'step', 'black', 'mean', 'space', 'discover', 'worth', 'notice', 'early', 'training', 'time', 'step', 'policy', 'already', 'learn', 'go', 'empty', 'room', 'likely', 'state', 'empty', 'room', 'easily', 'predictable', 'contrary', 'even', 'step', 'agent', 'train', 'still', 'go', 'empty', 'room', 'bottomright', 'corner', 'state', 'visitation', 'generate', 'grid', 'world', 'navigation', 'task', 'agent', 'expect', 'interact', 'object', 'key', 'ball', 'door', 'box', 'navigate', 'room', 'find', 'goal', 'randomly', 'place', 'room', 'task', 'provide', 'sparse', 'reward', 'end', 'episode', 'indicate', 'agent', 'successfully', 'find', 'goal', 'many', 'step', 'take', 'reach', 'goal', 'work', 'consider', 'type', 'task', 'include', 'multiroom', 'keycorridor', 'obstructedmaze', 'environment', 'experiment', 'paper', 'show', 'figure', 'upperright', 'environment', 'agent', 'learn', 'open', 'door', 'find', 'key', 'use', 'open', 'locked', 'blue', 'door', 'pick', 'bottomleft', 'fig', 'ure', 'show', 'obstructedmazefull', 'environment', 'similar', 'keycorridor', 'challenging', 'room', 'large', 'door', 'block', 'ball', 'key', 'hide', 'box', 'upperleft', 'bottomright', 'environ', 'ment', 'multiroom', 'environment', 'agent', 'navigate', 'connect', 'room', 'reach', 'goal', 'last', 'room', 'baseline', 'compare', 'stateoftheart', 'intrinsic', 'ward', 'design', 'work', 'well', 'minigrid', 'include', 'ride', 'raileanu', 'fair', 'compari', 'son', 'follow', 'basic', 'network', 'architecture', 'use', 'official', 'codebase', 'change', 'intrinsic', 'reward', 'rint', 'method', 'also', 'compare', 'method', 'similarity', 'highlevel', 'idea', 'method', 'however', 'original', 'paper', 'include', 'experiment', 'minigrid', 'therefore', 'plement', 'version', 'adapt', 'minigrid', 'follow', 'implementation', 'suggestion', 'paper', 'tune', 'hyperparameter', 'novelty', 'threshold', 'grid', 'search', 'dynamic', 'model', 'training', 'experiment', 'min', 'igrid', 'first', 'run', 'random', 'policy', 'step', 'collect', 'datum', 'use', 'train', 'forward', 'dynamic', 'model', 'world', 'model', 'pair', 'collect', '5e4', 'different', 'transition', 'pair', 'experiment', 'observe', 'finetune', 'pretraine', 'dynamic', 'model', 'policy', 'training', 'significant', 'influence', 'performance', 'similar', 'parisi', 'use', '◦', 'panoramic', 'view', 'input', 'predict', 'future', 'observation', 'rotationinvariant', 'representation', 'observed', 'state', 'consider', 'still', 'fair', 'compari', 'son', 'previous', 'stateoftheart', 'noveld', 'ride', 'rely', 'use', 'state', 'information', 'instead', 'observation', 'episodic', 'count', 'calculation', 'limited', 'field', 'view', 'agent', 'forward', 'learned', 'dynamic', 'step', 'predict', 'predict', 'next', 'observation', 'produce', 'discrete', 'action', 'minigrid', 'task', 'include', 'turn', 'leave', 'turn', 'right', 'forward', 'toggle', 'pick', 'drop', 'directly', 'apply', 'default', 'python', 'hash', 'function', 'hash', 'observation', 'predict', 'future', 'observation', 'expect', 'hash', 'function', 'mitigate', 'prediction', 'error', 'minigrid', 'use', 'reduce', 'dimension', 'observation', 'prediction', 'training', 'performance', 'minigrid', 'figure', 'show', 'learn', 'curve', 'gobi', 'stateoftheart', 'explo', 'ration', 'baseline', 'ride', 'challenging', 'minigrid', 'navigation', 'task', 'include', 'multi', 'room', 'keycorridor', 'obstructedmaze', 'curve', 'shift', 'right', 'number', 'random', 'explo', 'ration', 'environment', 'step', 'use', 'train', 'world', 'model', 'environment', 'gobi', 'significantly', 'outperform', 'previous', 'method', 'term', 'sample', 'efficiency', 'gobi', 'episode', 'training', 'environment', 'c', 'e', 'v', 'unlock', 'train', 'step', 'pick', 'unseen', 'see', 'go', 'imagination', 'maximize', 'episodic', 'reachability', 'world', 'model', 'figure', 'training', 'performance', 'gobi', 'baseline', 'minigrid', 'environment', 'show', 'number', 'environment', 'step', 'shift', 'training', 'curve', 'right', 'number', 'environment', 'step', 'use', 'pretrain', 'dynamic', 'model', '1e5', 'time', 'step', 'result', 'average', 'seed', 'stance', 'obstructedmaze2dlhb', 'gobi', 'time', 'sample', 'efficient', 'hard', 'obstructedmazefull', 'environment', 'gobi', 'achieve', 'optimal', 'performance', 'step', 'lastly', 'try', 'tune', 'hyperparameter', 'implementation', 'still', 'learn', 'well', 'minigrid', 'environment', 'qualitative', 'result', 'clearly', 'present', 'exploration', 'havior', 'learn', 'show', 'visitation', 'heatmap', 'gobi', 'noveld', 'keycorridors5r3', 'environment', 'figure', 'method', 'converge', 'mal', 'policy', 'fast', 'exploration', 'behaviour', 'different', 'gobi', 'quickly', 'learn', 'visit', 'easily', 'pre', 'dictable', 'state', 'empty', 'room', 'make', 'efficient', 'explore', 'interesting', 'part', 'environment', 'example', 'room', 'key', 'dynamic', 'model', 'training', 'follow', 'world', 'model', 'structure', 'directly', 'apply', 'encoder', 'transition', 'model', 'observation', 'model', 'predict', 'future', 'observation', 'however', 'compare', 'mini', 'grid', 'require', 'way', 'datum', 'train', 'decent', 'dynamic', 'model', 'deepmind', 'control', 'generate', 'visuallyreasonable', 'prediction', 'therefore', 'different', 'experiment', 'minigrid', 'pretrain', 'dynamic', 'model', 'instead', 'train', 'dynamic', 'model', 'together', 'policy', 'show', 'find', 'number', 'sample', 'random', 'action', 'work', 'well', 'environment', 'number', 'forward', 'prediction', 'step', 'k', 'set', 'pendulum', 'swingup', 'environment', 'hash', 'function', 'find', 'simple', 'simhash', 'suggest', 'work', 'well', 'capture', 'similarity', 'similar', 'observation', 'use', 'simhash', 'hash', 'image', 'observation', 'bit', 'experiment', 'control', 'task', 'far', 'test', 'gobi', 'deepmind', 'control', 'suite', 'set', 'imagebase', 'continuous', 'control', 'task', 'task', 'challenging', 'high', 'dimensional', 'observation', 'stochastic', 'transition', 'notice', 'environment', 'procedurallygenerate', 'experiment', 'section', 'show', 'generality', 'method', 'experimentally', 'show', 'gobi', 'extend', 'well', 'sparsereward', 'task', 'continuous', 'action', 'space', 'highdimensional', 'observation', 'space', 'training', 'performance', 'deepmind', 'control', 'com', 'pare', 'stateoftheart', 'intrinsic', 'motivation', 'mind', 'control', 'task', 'seo', 'apply', 'knear', 'estimator', 'lowdimensional', 'representation', 'space', 'randomly', 'initialize', 'encoder', 'maximize', 'state', 're3', 'also', 'use', 'life', 'long', 'intrinsic', 'reward', 'part', 'rlifelong', 'gobi', 'intrinsic', 'reward', 'baseline', 'consider', 'icm', 'pathak', 'fair', 'com', 'parison', 'experiment', 'use', 'basic', 'multiroomn6', 'multiroomn12s10', 'keycorridors3r3', 'obstructedmaze2q', 'obstructedmazefull', 'r', 'e', 'r', 'e', 'g', 'r', 'e', 'r', 'e', 'r', 'e', 'g', 'r', 'e', 'r', 'e', 'r', 'e', 'g', 'r', 'e', 'ride', 'gobi', '1e7', 'go', 'imagination', 'maximize', 'episodic', 'reachability', 'world', 'model', 'episode', 'multiroom', 'generate', 'room', 'square', 'fix', 'size', 'therefore', 'countbase', 'reward', 'tribute', 'environment', 'multiroom', 'time', 'perform', 'way', 'bad', 'gobi', 'agent', 'train', 'prefer', 'action', 'increase', 'size', 'episodic', 'buffer', 'bit', 'therefore', 'get', 'positive', 'score', 'often', 'provide', 'illustrative', 'example', 'explain', 'work', 'well', 'compare', 'gobi', 'use', 'lifelong', 'intrinsic', 'reward', 'perform', 'bad', 'struggle', 'learn', 'efficiently', 'large', 'multiroom', 'key', 'corridor', 'obstruct', 'maze', 'environment', 'real', 'dynamic', 'learn', 'dynamic', 'learn', 'ic', 'model', 'generally', 'perfect', 'especially', 'partially', 'observable', 'environment', 'minigrid', 'many', 'case', 'prediction', 'never', 'accurate', 'example', 'agent', 'first', 'open', 'door', 'new', 'room', 'usually', 'accurately', 'predict', 'door', 'figure', 'show', 'training', 'curve', 'use', 'real', 'dynamic', 'model', 'learn', 'dynamic', 'model', 'surprisingly', 'intrinsic', 'reward', 'function', 'use', 'real', 'dynamic', 'converge', 'fast', 'nearoptimal', 'policy', 'however', 'even', 'imperfect', 'dynamic', 'model', 'method', 'still', 'greatly', 'surpass', 'previous', 'stateoftheart', 'figure', 'comparison', 'use', 'real', 'dynamic', 'model', 'environment', 'use', 'learn', 'minigrid', 'environment', 'multiroom', 'use', 'real', 'dynamic', 'model', 'derive', 'intrinsic', 'reward', 'make', 'policy', 'converge', 'fast', 'espe', 'cially', 'multistep', 'prediction', 'figure', 'show', 'learning', 'formance', 'gobi', 'minigrid', 'vary', 'choice', 'number', 'future', 'step', 'prediction', 'dynamic', 'model', 'panot1', 'instead', 'hash', 'store', 'observation', 'future', 'time', 'step', 'real', 'forward', 'dynamic', 'model', 'large', 'generally', 'accelerate', 'exploration', 'prioritize', 'action', 'lead', 'state', 'reachable', 'state', 'long', 'run', 'however', 'limited', 'field', 'view', 'agent', 'model', 'inaccuracy', 'case', 'use', 'learned', 'model', 'forward', 'step', 'still', 'fast', 'step', 'step', 'really', 'make', 'exploration', 'fast', 'figure', 'training', 'curve', 'gobi', 'baseline', 'deepmind', 'control', 'suite', 'curve', 'average', 'seed', 'result', 'show', 'figure', 'additional', 'episodiclevel', 'intrinsic', 'reward', 'term', 'prove', 'sample', 'efficiency', 'lot', 'compare', 'use', 'lifelong', 'intrinsic', 'reward', 'especially', 'hopper', 'walker', 'run', 'sparse', 'ablation', 'study', 'gobi', 'variation', 'section', 'analyze', 'component', 'intrinsic', 'reward', 'contribute', 'final', 'performance', 'ablate', 'component', 'gobi', 'run', 'experiment', 'minigrid', 'environment', 'follow', 'r1', 'episodic', 'intrinsic', 'reward', 'indicator', 'new', 'state', 'add', 'episodic', 'buffer', 'ot1', 'lifelong', 'intrinsic', 'reward', 'cid112', 'ot1', 'figure', 'training', 'performance', 'comparison', 'gobi', 'minigrid', 'environment', 'training', 'performance', 'gobi', 'well', 'r1', 'show', 'figure', 'r1', 'work', 'multiroom', 'suffer', 'obstruct', 'maze', 'large', 'keycorridor', 'environ', 'ment', 'underlie', 'reason', 'key', 'corridor', 'obstruct', 'maze', 'room', 'structure', 'change', 'less', 'r', 'u', 'e', 'e', 'p', 'e', 'r', 'e', 'e', 'p', 'e', 'pendulum', 'swingup', 'cartpole', 'sparse', 'hopper', 'run', 'sparse', 'radicm', 'multiroomn12s10', 'e', 'r', 'e', 'g', 'r', 'e', 'r1', '1e7', 'keycorridors3r3', 'e', 'r', 'e', 'real', 'dynamic', 'learn', 'dynamic', '1e7', 'go', 'imagination', 'maximize', 'episodic', 'reachability', 'world', 'model', 'count', 'episodic', 'state', 'visitation', 'claim', 'apart', 'visit', 'state', 'also', 'consider', 'state', 'predict', 'episodic', 'memory', 'learn', 'world', 'model', 'forward', 'dynamic', 'learn', 'dynamic', 'function', 'set', 'observe', 'datum', 'widelystudied', 'topic', 'reinforcement', 'learning', 'especially', 'rapid', 'growth', 'modelbase', 'reinforcement', 'learn', 'exist', 'work', 'show', 'agent', 'world', 'model', 'implicitly', 'forward', 'model', 'predict', 'future', 'state', 'schmidhuber', 'freeman', 'recently', 'people', 'propose', 'latent', 'dynamic', 'model', 'work', 'well', 'highdimensional', 'input', 'latent', 'dynamic', 'model', 'encode', 'image', 'vation', 'predict', 'future', 'state', 'latent', 'space', 'schmidhuber', 'hafner', 'output', 'realistic', 'future', 'observation', 'visually', 'complex', 'domain', 'include', 'deepmind', 'control', 'suite', 'tunyasuvunakool', 'vizdoom', 'atari', 'game', 'deepmind', 'lab', 'beattie', 'learn', 'dynamic', 'model', 'use', 'guide', 'exploration', 'prediction', 'error', 'stadie', 'pathak', 'surprise', 'sastry', 'information', 'gain', 'variance', 'model', 'ensemble', 'mean', 'method', 'differ', 'previous', 'method', 'directly', 'ate', 'hash', 'predict', 'state', 'add', 'episodic', 'reachable', 'state', 'buffer', 'advanced', 'world', 'model', 'structure', 'method', 'extend', 'diverse', 'domain', 'complex', 'observation', 'discussion', 'future', 'work', 'paper', 'show', 'effective', 'way', 'combine', 'learn', 'world', 'model', 'episodic', 'memory', 'intrinsically', 'guide', 'efficient', 'exploration', 'method', 'achieve', 'stateoftheart', 'mance', 'procedurallygenerate', 'hard', 'exploration', 'task', 'also', 'work', 'well', 'singleton', 'continuous', 'control', 'main', 'however', 'still', 'certain', 'limitation', 'first', 'dynamic', 'model', 'use', 'minigrid', 'experiment', 'deterministic', 'make', 'possible', 'generate', 'less', 'accu', 'rate', 'prediction', 'make', 'performance', 'method', 'bad', 'use', 'real', 'dynamic', 'possible', 'way', 'make', 'improvement', 'make', 'prediction', 'model', 'erative', 'sample', 'possible', 'future', 'state', 'secondly', 'control', 'task', 'complex', 'visual', 'input', 'hash', 'age', 'static', 'hash', 'make', 'discrete', 'hash', 'code', 'however', 'well', 'capture', 'semantic', 'similarity', 'image', 'observation', 'beneficial', 'learn', 'hash', 'function', 'example', 'use', 'autoencoder', 'learn', 'meaningful', 'hash', 'code', 'leave', 'investigation', 'future', 'work', 'figure', 'make', 'forward', 'prediction', 'different', 'number', 'future', 'step', 'use', 'real', 'dynamic', 'learn', 'dy', 'namic', 'model', 'plot', 'show', 'training', 'performance', 'relate', 'work', 'exploration', 'reinforcement', 'learning', 'efficient', 'exploration', 'reinforcement', 'learning', 'especially', 'sparsereward', 'reinforcement', 'learning', 'problem', 'chal', 'lenge', 'natural', 'popular', 'solution', 'design', 'metric', 'evaluate', 'state', 'novelty', 'assign', 'high', 'intrinsic', 'ward', 'novel', 'state', 'example', 'countbase', 'intrinsic', 'reward', 'strehl', 'littman', 'kolter', 'curiositybase', 'intrinsic', 'motivation', 'stadie', 'pathak', 'popular', 'way', 'state', 'space', 'recently', 'near', 'estimation', 'method', 'yarat', 'abbeel', 'show', 'great', 'performance', 'provement', 'challenge', 'visual', 'domain', 'method', 'compatible', 'successful', 'exploration', 'intrinsic', 'ward', 'design', 'use', 'additionally', 'encourage', 'episodiclevel', 'reachable', 'space', 'expansion', 'achieve', 'large', 'state', 'space', 'coverage', 'single', 'episode', 'episodic', 'memory', 'derive', 'useful', 'information', 'episodic', 'buffer', 'show', 'great', 'success', 'improve', 'training', 'sample', 'ef', 'ficiency', 'rl', 'navigation', 'control', 'game', 'episodic', 'memory', 'buffer', 'apply', 'mimic', 'hippocampal', 'episodic', 'control', 'rapidly', 'assimilate', 'recent', 'experience', 'blundell', 'pritzel', 'man', 'tione', 'previous', 'section', 'keep', 'episodic', 'buffer', 'store', 'observation', 'introduce', 'episodic', 'curiosity', 'module', 'determine', 'new', 'observation', 'reachable', 'previous', 'observation', 'rapid', 'zha', 'et', 'propose', 'novel', 'way', 'behaviour', 'cloning', 'episode', 'high', 'episodic', 'coverage', 'badia', 'combine', 'episodic', 'novelty', 'module', 'lifelong', 'novelty', 'module', 'generate', 'intrinsic', 'reward', 'however', 'episodic', 'novelty', 'measurement', 'differ', 'ence', 'current', 'observation', 'previous', 'observation', 'focus', 'much', 'reachable', 'space', 'expand', 'new', 'state', 'ride', 'raileanu', 'real', 'dynamic', 'learn', 'dynamic', 'e', 'r', 'e', 'g', 'r', 'e', 'k3', 'go', 'imagination', 'maximize', 'episodic', 'reachability', 'world', 'model', 'conclusion', 'work', 'introduce', 'go', 'imagination', 'gobi', 'novel', 'episodic', 'intrinsic', 'reward', 'design', 'encourage', 'efficient', 'episodiclevel', 'exploration', 'expand', 'reachable', 'space', 'previous', 'episodic', 'intrinsic', 'reward', 'use', 'naive', 'episodic', 'state', 'count', 'state', 'visitation', 'coverage', 'method', 'exploit', 'learn', 'world', 'model', 'predict', 'reachable', 'state', 'motivate', 'agent', 'seek', 'state', 'unexplored', 'neighbor', 'combine', 'lifelong', 'intrin', 'sic', 'reward', 'method', 'show', 'great', 'training', 'time', 'sample', 'efficiency', 'improvement', 'hard', 'procedurallygenerate', 'vironment', 'time', 'extend', 'guide', 'exploration', 'continuous', 'control', 'task', 'visual', 'input', 'indicate', 'promising', 'future', 'direction', 'acknowledgment', 'work', 'support', 'part', 'grant', 'ai', 'search', 'iis', 'fwhtfr', 'reference', 'surprisebase', 'intrinsic', 'moti', 'vation', 'deep', 'reinforcement', 'learning', 'arxiv', 'preprint', 'p', 'r', 'amaral', 'bliss', 'keefe', 'book', 'press', 'badia', 'p', 'vitvitskyi', 'guo', 'piot', 'tieleman', 'arjovsky', 'pritzel', 'bolt', 'et', 'never', 'give', 'learn', 'direct', 'exploration', 'strategy', 'arxiv', 'preprint', 'beattie', 'c', 'leibo', 'z', 'teplyashin', 'ward', 'wain', 'wright', 'h', 'lefrancq', 'green', 'sadik', 'et', 'deepmind', 'lab', 'arxiv', 'preprint', 'arxiv161203801', 'blundell', 'c', 'uria', 'b', 'pritzel', 'li', 'ruderman', 'leibo', 'z', 'wierstra', 'abis', 'modelfree', 'episodic', 'control', 'arxiv', 'preprint', 'h', 'storkey', 'klimov', 'ex', 'ploration', 'random', 'network', 'distillation', 'arxiv', 'preprint', 'willem', 'l', 'pal', 'tic', 'gridworld', 'environment', 'gym', 'https', 'githubcommaximecbgymminigrid', 'tower', 'r', 'willem', 'l', 'pal', 'minigrid', 'miniworld', 'modular', 'customizable', 'reinforcement', 'learning', 'environment', 'goaloriente', 'task', 'hesse', 'schulman', 'j', 'quantify', 'generalization', 'reinforcement', 'learning', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'schulman', 'lever', 'age', 'procedural', 'generation', 'benchmark', 'reinforcement', 'learning', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'eichenbaum', 'h', 'role', 'hippocampus', 'navigation', 'memory', 'journal', 'neurophysiology', 'espeholt', 'l', 'soyer', 'h', 'muno', 'r', 'ward', 'doron', 'firoiu', 'harley', 'dun', 'et', 'impala', 'scalable', 'distribute', 'deeprl', 'weight', 'actorlearner', 'architecture', 'tional', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'fletberliac', 'ferret', 'pietquin', 'preux', 'p', 'adversarially', 'guide', 'actorcritic', 'arxiv', 'preprint', 'florensa', 'c', 'hold', 'geng', 'abbeel', 'p', 'automatic', 'goal', 'generation', 'reinforcement', 'learning', 'agent', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'freeman', 'metz', 'l', 'learning', 'predict', 'look', 'ahead', 'world', 'model', 'forward', 'prediction', 'advance', 'neural', 'information', 'processing', 'system', 'schmidhuber', 'recurrent', 'world', 'model', 'facil', 'itate', 'policy', 'evolution', 'advance', 'neural', 'information', 'processing', 'system', 'schmidhuber', 'world', 'model', 'arxiv', 'preprint', 'hafner', 'lillicrap', 'j', 'dream', 'control', 'learn', 'behavior', 'latent', 'imagination', 'arxiv', 'preprint', 'hafner', 'j', 'master', 'diverse', 'domain', 'world', 'model', 'arxiv', 'preprint', 'hazan', 'e', 'kakade', 'k', 'prov', 'ably', 'efficient', 'maximum', 'entropy', 'exploration', 'tional', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'go', 'imagination', 'maximize', 'episodic', 'reachability', 'world', 'model', 'wydmuch', 'runc', 'g', 'toczek', 'j', 'w', 'vizdoom', 'doombase', 'ai', 'research', 'plat', 'form', 'visual', 'reinforcement', 'learning', 'ieee', 'con', 'ference', 'computational', 'intelligence', 'game', 'pp', 'ieee', 'r', 'grefenstette', 'e', 'survey', 'generalisation', 'deep', 'reinforcement', 'learning', 'arxiv', 'preprint', 'kolter', 'z', 'nearbayesian', 'exploration', 'proceeding', '26th', 'annual', 'polynomial', 'time', 'international', 'conference', 'machine', 'learn', 'pp', 'laskin', 'stooke', 'pinto', 'l', 'abbeel', 'p', 'srinivas', 'reinforcement', 'learning', 'augmented', 'datum', 'advance', 'neural', 'information', 'processing', 'system', 'l', 'eysenbach', 'b', 'e', 'e', 'salakhutdinov', 'r', 'efficient', 'exploration', 'state', 'marginal', 'matching', 'arxiv', 'preprint', 'arxiv190605274', 'h', 'abbeel', 'p', 'behavior', 'void', 'unsuper', 'vise', 'active', 'pretraine', 'advance', 'neural', 'information', 'processing', 'system', 'silver', 'grave', 'antonoglou', 'riedmiller', 'play', 'atari', 'deep', 'reinforcement', 'learning', 'arxiv', 'preprint', 'badia', 'p', 'mirza', 'grave', 'lillicrap', 'harley', 'silver', 'asyn', 'chronous', 'method', 'deep', 'reinforcement', 'learning', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'taniguchi', 'planet', 'bayesian', 'reconsider', 'improve', 'deep', 'planning', 'network', 'incorporate', 'bayesian', 'inference', 'ieeersj', 'international', 'conference', 'intelligent', 'robot', 'system', 'iro', 'pp', 'ieee', 'dean', 'pathak', 'gupta', 'interesting', 'object', 'curious', 'agent', 'learn', 'taskagnostic', 'exploration', 'advance', 'neural', 'information', 'processing', 'system', 'pathak', 'agrawal', 'efro', 'darrell', 'exploration', 'selfsupervise', 'predic', 'tion', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'pritzel', 'uria', 'b', 'srinivasan', 'badia', 'p', 'vinyal', 'wierstra', 'blundell', 'c', 'neural', 'episodic', 'control', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'raileanu', 'r', 'ride', 'rewarding', 'impact', 'drive', 'exploration', 'procedurallygenerate', 'environ', 'ment', 'arxiv', 'preprint', 'riedmiller', 'hafner', 'r', 'lampe', 'neunert', 'heess', 'springen', 'learn', 'play', 'solve', 'sparse', 'reward', 'task', 'scratch', 'international', 'conference', 'pp', 'pmlr', 'raichuk', 'marini', 'r', 'vincent', 'polle', 'fey', 'lillicrap', 'gelly', 'episodic', 'curiosity', 'reachability', 'arxiv', 'preprint', 'arxiv181002274', 'schulman', 'dhariwal', 'p', 'radford', 'klimov', 'proximal', 'policy', 'optimization', 'algorithm', 'arxiv', 'preprint', 'r', 'rybkin', 'abbeel', 'p', 'hafner', 'pathak', 'planning', 'explore', 'selfsupervise', 'world', 'model', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'seo', 'h', 'abbeel', 'p', 'maximization', 'random', 'encoder', 'efficient', 'exploration', 'ed', 'proceeding', '38th', 'international', 'conference', 'machine', 'learn', 'volume', 'proceeding', 'machine', 'learn', 'research', 'pp', 'pmlr', 'jul', 'proceedingsmlr', 'pressv139seo21ahtml', 'stadie', 'c', 'abbeel', 'p', 'incentivize', 'ex', 'ploration', 'reinforcement', 'learning', 'deep', 'predictive', 'model', 'arxiv', 'preprint', 'strehl', 'l', 'littman', 'l', 'analysis', 'model', 'base', 'interval', 'estimation', 'markov', 'decision', 'process', 'journal', 'computer', 'system', 'science', 'h', 'r', 'foote', 'stooke', 'schulman', 'abbeel', 'p', 'exploration', 'study', 'countbase', 'exploration', 'deep', 'reinforcement', 'learning', 'advance', 'neural', 'information', 'processing', 'system', 'tunyasuvunakool', 'muldal', 'doron', 'merel', 'erez', 'lillicrap', 'tassa', 'control', 'software', 'task', 'continuous', 'control', 'software', 'impact', 'issn', 'https', 'go', 'imagination', 'maximize', 'episodic', 'reachability', 'world', 'model', 'bao', 'clavera', 'abbeel', 'p', 'ba', 'j', 'benchmarke', 'modelbase', 'reinforcement', 'learning', 'arxiv', 'preprint', 'yarat', 'fergus', 'r', 'lazaric', 'l', 'reinforce', 'ment', 'learn', 'prototypical', 'representation', 'national', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'yarat', 'kostrikov', 'amos', 'b', 'fergus', 'r', 'improve', 'sample', 'efficiency', 'model', 'free', 'reinforcement', 'learning', 'image', 'proceeding', 'conference', 'artificial', 'intelligence', 'vol', 'ume', 'pp', 'zha', 'l', 'rank', 'episode', 'simple', 'approach', 'exploration', 'procedurallygenerate', 'environment', 'arxiv', 'preprint', 'h', 'simple', 'yet', 'effective', 'exploration', 'criterion', 'advance', 'neural', 'information', 'processing', 'system', 'go', 'imagination', 'maximize', 'episodic', 'reachability', 'world', 'model', 'implementation', 'detail', 'experiment', 'minigrid', 'baseline', 'implementation', 'ride', 'raileanu', 'build', 'official', 'codebase', 'fair', 'compar', 'ison', 'intrinsic', 'reward', 'rint', 'differ', 'use', 'base', 'time', 'experiment', 'run', 'compute', 'resource', 'cpu', 'rerun', 'official', 'code', 'get', 'result', 'minigrid', 'multiroom', 'keycorridor', 'experiment', 'obstructedmaze', 'find', 'proper', 'hyperparameter', 'fully', 'reproduce', 'result', 'therefore', 'directly', 'take', 'result', 'report', 'paper', 'ride', 'rnd', 'run', 'code', 'official', 'codebase', 'original', 'paper', 'include', 'experiment', 'minigrid', 'environment', 'therefore', 'implement', 'version', 'tune', 'hyperparameter', 'grid', 'search', 'intrinsic', 'reward', 'function', 'gobi', 'baseline', 'list', 'gobi', 'ot1', 'size', 'episodic', 'buffer', 'ot1', 'lifelong', 'count', 'observation', 'ot1', 'start', 'begin', 'ning', 'training', 'rnd', 'ot1', '∥2', 'difference', 'fix', 'random', 'network', 'train', 'state', 'embed', 'network', 'train', 'minimize', 'error', 'noveld', 'novelty', 'novelty', '∗', 'epi', 'st1', 'apply', 'measure', 'novelty', 'ie', 'novelty', '∥ϕ', '∥2', 'epi', 'st1', 'check', 'agent', 'visit', 'state', 'st1', 'first', 'time', 'episode', 'notice', 'use', 'full', 'environment', 'information', 'grid', 'world', 'instead', 'partiallyobservable', 'view', 'therefore', 'epi', 'count', 'st1', 'instead', 'ride', 'ot1', '∥2', 'epi', 'st1', 'state', 'embed', 'network', 'train', 'minimize', 'prediction', 'error', 'inverse', 'forward', 'dynamic', 'epi', 'indicate', 'episodic', 'count', 'ride', 'also', 'use', 'state', 'information', 'st1', 'episodic', 'count', 'policy', 'value', 'function', 'training', 'fair', 'compar', 'ison', 'policy', 'network', 'value', 'function', 'network', 'approach', 'input', 'observation', '×', 'put', 'share', 'feature', 'extraction', 'network', 'include', 'convolutional', 'layer', '×', 'padding', 'channel32', 'stride', 'respectively', 'elu', 'activation', 'ture', 'flatten', 'put', 'linear', 'layer', 'unit', 'relu', 'activation', 'lstm', 'layer', 'unit', 'share', 'feature', 'pass', 'separately', 'fullyconnected', 'layer', 'unit', 'output', 'action', 'dis', 'tribution', 'value', 'estimation', 'dynamic', 'model', 'implementation', 'dynamic', 'model', 'input', 'panorama', 'current', 'step', 'get', 'panorama', 'let', 'agent', 'rotate', 'time', 'catenate', 'observation', 'get', 'input', 'size', '×', 'pass', 'feature', 'extraction', 'module', 'structure', 'policy', 'value', 'function', 'network', 'input', 'first', 'linear', 'layer', 'concatenate', 'action', 'put', 'coder', 'linear', 'layer', 'size', 'reshape', 'back', '×', 'get', 'predict', 'observation', 'pre', 'train', 'dynamic', 'model', 'use', 'panot', 'ot1', 'pair', 'collect', 'random', 'policy', 'ride', 'also', 'require', 'training', 'dynamic', 'model', 'state', 'embed', 'network', 'input', 'dynamic', 'model', 'state', 'embed', 'action', 'forward', 'model', 'contain', 'fullyconnected', 'layer', 'unit', 'activate', 'relu', 'verse', 'dynamic', 'model', 'contain', 'fullyconnected', 'layer', 'unit', 'relu', 'activation', 'function', 'input', 'state', 'embedding', 'consecutive', 'step', 'hash', 'function', 'directly', 'apply', 'default', 'python', 'hash', 'function', 'hash', '×', 'observation', 'predict', 'future', 'observation', 'add', 'episodic', 'buffer', 'state', 'embed', 'ride', 'require', 'train', 'state', 'embed', 'network', 'input', 'observation', 'minigrid', 'dimension', 'contain', 'convolutional', 'layer', 'kernel', 'size', 'padding', 'stride', 'number', 'channel', 'respectively', 'activation', 'function', 'elu', 'follow', 'convolutional', 'layer', 'linear', 'layer', 'unit', 'relu', 'activation', 'c', 'ot1', 'c', 'ot1', '90th', 'percentile', 'similarity', 'score', 'observation', 'episodic', 'buffer', 'similar', 'ity', 'score', 'calculate', 'use', 'pretraine', 'episodic', 'curiosity', 'module', 'hyperparameter', 'visitation', 'count', 'gobi', 'store', 'flatten', '×', 'observation', 'step', 'noveld', 'ride', 'count', 'full', 'state', 'episodic', 'level', 'shape', 'vary', 'environment', 'environment', 'example', 'shape', '×', 'multiroom', 'environment', 'go', 'imagination', 'maximize', 'episodic', 'reachability', 'world', 'model', 'hyperparameter', 'table', 'show', 'value', 'hyper', 'parameter', 'share', 'different', 'method', 'parameter', 'name', 'batch', 'size', 'optimizer', 'learning', 'rate', 'step', 'discount', 'factor', 'weight', 'policy', 'entropy', 'loss', 'weight', 'value', 'function', 'loss', 'value', 'rmsprop', 'table', 'hyperparameter', 'experiment', 'minigrid', 'hyperparameter', 'share', 'method', 'experiment', 'use', 'gobi', 'minigrid', 'set', 'intrinsic', 'reward', 'coefficient', 'k', 'mean', 'forward', 'dynamic', 'model', 'step', 'time', 'action', 'space', 'small', 'discrete', 'instead', 'randomly', 'sample', 'action', 'directly', 'predict', 'future', 'observation', 'use', 'possible', 'action', 'list', 'hyperparameter', 'choice', 'intrinsic', 'decay', 'factor', 'ρ', 'table', 'value', 'choose', 'make', 'intrinsic', 'reward', 'large', 'beginning', 'training', 'nearzero', 'end', 'training', 'parameter', 'forward', 'step', 'intrinsic', 'decay', 'ˆfϕ', 'learn', 'rate', 'value', 'mrn7s8', '8e−7', 'mrn12s10', 'kcs5r3', 'om1q', 'om2q', 'omfull', 'table', 'hyperparameter', 'gobi', 'experiment', 'mini', 'grid', 'set', 'environment', 'suggest', 'official', 'codebase', 'ride', 'use', 'λ', 'environment', 'set', 'environment', 'make', 'initial', 'average', 'intrinsic', 'reward', 'similar', 'experiment', 'apply', 'base', 'reinforcement', 're3', 'rerun', 'official', 'code', 'get', 'result', 'environment', 'icm', 'follow', 'implementation', 'detail', 'list', 're3', 'implement', 'compatible', 'deepmind', 'control', 'task', 'fair', 'comparison', 'intrinsic', 'reward', 'design', 'differ', 'method', 'trinsic', 'reward', 'function', 'gobi', 'baseline', 'list', 'gobi', 'size', 'episodic', 'buffer', 'latter', 'part', 're3', 'intrinsic', 'reward', 'introduce', '•', 'log', 'yk−n', 'n', 'fθ', 'fix', 'representation', 'output', 'randomly', 'initial', 'ized', 'encoder', 'n', 'set', 'knear', 'neighbor', 'collect', 'beginning', 'training', '•', 'icm', 'ot1', 'ot1', 'scaling', 'factor', 'feature', 'vector', 'jointly', 'optimize', 'forward', 'prediction', 'model', 'inverse', 'dynam', 'ics', 'model', 'predict', 'feature', 'encoding', 'time', 'step', '•', 'rnd', 'ot1', '∥2', 'difference', 'fix', 'random', 'network', 'train', 'state', 'embed', 'network', 'train', 'minimize', 'ot1', '∥2', 'architecture', 'observation', 'size', 'environment', 'encoder', 'architecture', 'follow', 'one', 'yarat', 'contain', 'convolutional', 'layer', '×', 'kernel', 'channel32', 'stride2', 'relu', 'activation', 'output', 'pass', 'fully', 'connect', 'layer', 'normalize', 'layernorm', 'dynamic', 'model', 'forward', 'dynamic', 'model', 'use', 'generate', 'future', 'prediction', 'apply', 'world', 'model', 'structure', 'input', 'size', 'downsample', 'input', 'observation', 'instead', 'tune', 'world', 'model', 'layer', 'train', 'dynamic', 'model', 'together', 'policy', 'online', 'manner', 'instead', 'pretrain', 'take', 'many', 'episode', 'prediction', 'visually', 'reasonable', 'therefore', 'add', 'extra', 'effort', 'determine', 'many', 'datum', 'collect', 'pretrain', 'dynamic', 'model', 'a2', 'experiment', 'deepmind', 'control', 'suit', 'baseline', 'implementation', 'gobi', 'seo', 'icm', 'build', 'official', 'codebase', 're3', 'image', 'hash', 'observation', 'image', 'high', 'dimensional', 'space', 'prediction', 'usually', 'accu', 'rate', 'hash', 'image', 'low', 'dimension', 'avoid', 'take', 'much', 'space', 'collapse', 'similar', 'observation', 'go', 'imagination', 'maximize', 'episodic', 'reachability', 'world', 'model', 'prediction', 'follow', 'use', 'sim', 'ple', 'simhash', 'function', 'map', 'image', 'bit', 'specifically', 'project', 'flatten', 'image', 'random', 'initialize', 'vector', 'use', 'sign', 'output', 'vector', 'value', 'hash', 'code', 'hyperparameter', 'table', 'show', 'value', 'hyper', 'parameter', 'share', 'different', 'method', 'decrease', 'small', 'therefore', 'sometimes', 'hard', 'agent', 'learn', 'useful', 'result', 'unsatisfac', 'tory', 'performance', 'meanwhile', 'small', 'example', 'ρ', 'intrinsic', 'reward', 'large', 'later', 'stage', 'training', 'make', 'agent', 'focus', 'less', 'extrinsic', 'reward', 'therefore', 'policy', 'converge', 'slow', 'parameter', 'name', 'augmentation', 'observation', 'size', 'action', 'repeat', 'replay', 'buffer', 'size', 'initial', 'random', 'exploration', 'step', 'frame', 'stack', 'actor', 'learn', 'rate', 'critic', 'learn', 'rate', 'batch', 'size', 'near', 'neighbor', 'critic', 'target', 'update', 'value', 'crop', 'table', 'hyperparameter', 'experiment', 'deepmind', 'control', 'suit', 'hyperparameter', 'share', 'method', 'intrinsic', 'reward', 'coefficient', 'follow', 'good', 'choice', 'report', 're3', 'gobi', 'apply', 'intrinsic', 'reward', 'coefficient', 'intrinsic', 'reward', 'decay', 'ρ', 'one', 're3', 'fair', 'comparison', 'intrinsic', 'reward', 'specific', 'method', 'show', 'table', 'number', 'random', 'action', 'perform', 'hyperparameter', 'search', 'find', 'perform', 'well', 'task', 'number', 'forward', 'step', 'k', 'perform', 'hyperparameter', 'search', 'report', 'one', 'good', 'result', 'parameter', 'forward', 'step', 'k', 'random', 'action', 'value', 'pendulumswingup', 'table', 'hyperparameter', 'experiment', 'deepmind', 'control', 'suit', 'hyperparameter', 'specific', 'method', 'hyperparameter', 'intrinsic', 'reward', 'decay', 'figure', 'training', 'performance', 'gobi', 'minigrid', 'multiroom', 'n12s10', 'different', 'intrinsic', 'reward', 'decay', 'ρ', 'figure', 'training', 'performance', 'gobi', 'minigrid', 'multiroom', 'different', 'randomly', 'sample', 'action', 'step', 'b2', 'number', 'randomly', 'sample', 'action', 'minigrid', 'experiment', 'report', 'section', 'randomly', 'sample', 'action', 'minigrid', 'small', 'discrete', 'action', 'space', 'action', 'therefore', 'directly', 'predict', 'future', 'observation', 'action', 'ever', 'also', 'report', 'result', 'n', 'random', 'action', 'figure', 'summarize', 'n', 'similar', 'performance', 'minigrid', 'large', 'n', 'make', 'wallclock', 'training', 'time', 'long', 'slightly', 'slow', 'early', 'stage', 'still', 'outperform', 'previous', 'state', 'oftheart', 'overall', 'n', 'good', 'choice', 'minigrid', 'environment', 'figure', 'show', 'training', 'performance', 'differ', 'ent', 'intrinsic', 'reward', 'decay', 'ρ', 'choice', 'balance', 'relative', 'importance', 'extrinsic', 'intrinsic', 'reward', 'large', 'example', 'agent', 'find', 'goal', 'intrinsic', 'reward', 'already', 'c', 'exploration', 'behaviour', 'comparison', 'gobi', 'figure', 'visualize', 'policy', 'visitation', 'heatmap', 'gobi', 'multiroom', 'r', 'e', 'r', 'e', 'g', 'r', 'e', 'multiroomn12s10', 'environment', 'step', '9e7', 'e', 'r', 'e', 'g', 'r', 'e', 'n3', 'n10', 'n15', 'go', 'imagination', 'maximize', 'episodic', 'reachability', 'world', 'model', 'figure', 'heatmap', 'example', 'trajectory', 'different', 'training', 'step', 'environment', 'time', 'hour', 'noveld', 'gobi', '±0058', 'table', 'wallclock', 'training', 'time', 'comparison', 'hour', 'noveld', 'gobi', 'environment', 'minigrid', 'fail', 'learn', 'optimal', 'policy', 'still', 'capture', 'preference', 'heatmap', 'agent', 'train', 'prefer', 'go', 'corner', 'room', 'generally', 'low', 'similarity', 'score', 'state', 'middle', 'room', 'however', 'similarity', 'score', 'low', 'enough', 'state', 'add', 'episodic', 'buffer', 'continue', 'stay', 'state', 'maximize', 'intrinsic', 'reward', 'try', 'tune', 'similarity', 'score', 'threshold', 'use', 'grid', 'search', 'still', 'find', 'good', 'hyperparameter', 'choice', 'similarity', 'score', 'different', 'corner', 'share', 'consistent', 'value', 'see', 'figure', 'gobi', 'choose', 'visit', 'border', 'room', 'early', 'training', 'information', 'border', 'easily', 'predictable', 'information', 'middle', 'room', 'figure', 'illustrative', 'example', 'work', 'well', 'encourage', 'efficient', 'exploration', 'example', 'goal', 'include', 'state', 'episodic', 'buffer', 'quickly', 'gobi', 'move', 'directly', 'center', 'step', 'maximum', 'intrinsic', 'reward', 'choose', 'take', 'extra', 'step', 'exploration', 'mainly', 'focus', 'episodic', 'buffer', 'expand', 'figure', 'training', 'performance', 'comparison', 'environment', 'use', 'fix', 'pretraine', 'dynamic', 'model', 'use', 'pretraine', 'dynamic', 'model', 'online', 'pretraine', 'directly', 'train', 'dynamic', 'model', 'together', 'policy', 'training', 'wallclock', 'training', 'time', 'table', 'show', 'wallclock', 'time', 'need', 'train', 'method', 'environment', 'step', 'gobi', 'require', 'wallclock', 'time', 'need', 'train', 'noveld', 'number', 'environment', 'step', 'e', 'dynamic', 'training', 'experiment', 'section', 'report', 'result', 'apply', 'pretraine', 'forward', 'dynamic', 'gobi', 'minigrid', 'however', 'forward', 'dynamic', 'model', 'also', 'train', 'together', 'policy', 'training', 'figure', 'report', 'result', 'ablation', 'study', 'minigrid', 'environment', 'setting', 'use', 'pretraine', 'dynamic', 'model', 'keep', 'fix', 'train', 'policy', 'pretrain', 'dynamic', 'model', 'finetune', 'train', 'policy', 'pre', 'training', 'directly', 'train', 'dynamic', 'model', 'online', 'manner', 'summary', 'version', 'work', 'similarly', 'fact', 'train', 'dynamic', 'model', 'online', 'add', 'gobi', 'training', 'environment', 'episode', 'goal', 'agent', 'minigridmultiroomn7s8v0', 'episodic', 'curiosity', 'c', 'e', 'train', 'step', 'unseen', 'see', 'e', 'r', 'e', 'g', 'r', 'e', 'nopretrain', 'model', 'r2', 'gobi', 's0', 's0', 's1', 's0', 's1', 's0', 's1', 's1', 'visit', 'state', 'reachable', 'state', 'nonreachable', 'state', 'trajectory', 'direct', 'temporal', 'order', 'observe', 'transition', 'path', 'bidirectional', 'unreachable', 'transition', 'path', 'bidirectional', 'go', 'imagination', 'maximize', 'episodic', 'reachability', 'world', 'model', 'extra', 'wall', 'clock', 'training', 'time', 'use', 'option', 'main', 'experiment', 'ablation', 'study', 'illustration', 'section', 'provide', 'illustrative', 'example', 'consider', 'new', 'state', 'add', 'episodic', 'buffer', 'section', 'work', 'way', 'bad', 'method', 'example', 'show', 'figure', 'state', 'small', 'part', 'environment', 'want', 'policy', 'explore', 'part', 'quickly', 'possible', 'mark', 'state', 'reachable', 'quickly', 'possible', 'ever', 'order', 'maximize', 'stepwise', 'intrinsic', 'reward', 'agent', 'train', 'go', 'border', 'add', 'new', 'state', 'episodic', 'buffer', 'time', 'waste', 'many', 'unnecessary', 'step', 'beneficial', 'exploration']",
"Field evaluation of a mobile app for assisting blind and visually
  impaired travelers to find bus stops","[{'href': 'http://arxiv.org/abs/2309.10940v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2309.10940v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-09-19 21:39:32,"May 2023 

Classifying Organizations for Food 
System Ontologies using Natural 
Language Processing 

Tianyu JIANG a, Sonia VINOGRADOVA b, Nathan STRINGHAM a, 
E. Louise EARL  b, Allan D. HOLLANDER c, Patrick R. HUBER c, Ellen RILOFF  a,1, 
R. Sandra SCHILLO  b, Giorgio A. UBBIALI  d, Matthew LANGE  e, 
a University of Utah 
b University of Ottawa 
c University of California, Davis 
d University of Milan 
e IC-FOODS 
ORCiD ID: E. Louise Earl https://orcid.org/0000-0002-5646-0132, R. Sandra Schillo 
https://orcid.org/0000-0002-3468-0206, Giorgio A. Ubbiali 
https://orcid.org/0000-0001-7872-1770 , Matthew Lange 
https://orcid.org/0000-0002-6148-7962 

Abstract.  Our  research  explores  the  use  of  natural  language  processing  (NLP) 
methods to automatically classify entities for the purpose of knowledge graph pop- 
ulation and integration with food system ontologies. We have created NLP models 
that can automatically classify organizations with respect to categories associated 
with environmental issues as well as Standard Industrial Classification (SIC) codes, 
which are used by the U.S. government to characterize business activities. As in- 
put, the NLP models are provided with text snippets retrieved by Google’s search 
engine for each organization, which serves as a textual description of the organi- 
zation that is used for learning. Our experimental results show that NLP models 
can achieve reasonably good performance for these two classification tasks, and 
they rely on a general framework that could be applied to many other classification 
problems as well. We believe that NLP models represent a promising approach for 
automatically harvesting information to populate knowledge graphs and aligning 
the information with existing ontologies through shared categories and concepts. 

Keywords. food system ontologies, classification models, natural language processing, 
SIC, sustainability issues, unstructured data 

1.  Introduction 

Food systems include activities and relationships that are involved with the production, 
transport, and consumption of food, and thus are linked to a wide variety of natural and 
human systems (Tomich et al. [13]). Their extensive nature can have dramatic impacts 

1Corresponding Author: Ellen Riloff (riloff@cs.utah.edu). 

 
 
 
 
 
 
 
 
May 2023 

on these natural and human systems (IPBES [9], UNEP [15]). Conversely, food systems 
are vulnerable to changes in these other systems. Food activities also show a pivotal role 
in fostering human social and health-related issues. For instance, inequalities and injus- 
tices exist across the whole global food supply chain, as well as within related sectors 
(D’Odorico et al. [5], Nicole´tis and Termine [10]). To date, hunger, malnutrition, and 
micronutrient deficiencies, just to cite a few, still largely threaten humanity worldwide 
(Fanzo et al. [6], Fanzo and Davis [7], UNICEF [14]). 

As our world becomes more interconnected and interdependent, ontologies become 
useful ways of categorizing and relating information together. This is especially appli- 
cable to food systems, where data access, interoperability, and reusability are essential 
to  deal  with  the  interrelated  issues  arising  from  food  systems  activities.  An  ontology 
is a “formal theory” which provides a “commonly accepted” dictionary of terms, sup- 
ported by a “canonical syntax” and a set of axioms, for a knowledge domain of interest 
(Smith [12]). In doing so, an ontology offers a common semantic framework for that 
domain of knowledge (Smith [12], Hollander et al. [1]). Thus, an ontology fosters data 
access, interoperability, and reusability across several disparate resources, which employ 
that ontology as a common reference semantic standard (Hollander et al. [1]). Nowa- 
days, several ontologies focusing on food, health, and related aspects have been devel- 
oped  (Boulos  et  al.  [2],  Dooley  et  al.  [3]).  Here,  we  just  cite  a  few  noteworthy  ones 
as examples: 1) AGROVOC2 is a three-decade-long well-established “multilingual the- 
saurus”, addressing food and related domains of interest (Boulos et al. [2]). It belongs 
to the United Nations Food and Agriculture Organisation (FAO). 2) The Agronomy On- 
tology AgrO3 was developed within the frame of the CGIAR Platform for Big Data in 
Agriculture, and which proposes a vocabulary of terms, covering “agronomic manage- 
ment practices, implements, and variables used during agronomic experiments” (Dooley 
et al. [3]). 3) The Compositional Dietary Nutrition Ontology CDNO4 presents terms re- 
lated to “nutritional attributes from crops, livestock, and fisheries that contribute to hu- 
man diet and which are referenced in precision food commodity laboratory analytics” 
(Dooley et al. [3]). 4) The Food Ontology FoodOn5 provides a lexicon about “basic raw 
food source ingredients, process terms for packaging, cooking, and preservation, and an 
upper-level variety of product type schemes under which food products can be catego- 
rized”. FoodOn stands out as a fundamental ontology in addressing food-related aspects 
(Dooley et al. [4]). Further, FoodOn aims to state the lingua franca within the domain 
of food, for sharing and reusing food-related information both by humans and machines 
(Dooley et al. [4]). Despite the availability of food-health ontologies, the envisaged data 
access, interoperability and reusability across food industries and other food-related sec- 
tors, as we claimed above, appears to have not been reached yet (Tomich et al. [13]). 

In this paper, we present new research that uses artificial intelligence (AI) technol- 
ogy to automatically categorize organizations, ultimately for the purpose of linking them 
into food system ontologies. Our eventual goal is to automatically populate large knowl- 
edge graphs of information related to agriculture and food systems, where the concepts in 
the graph are aligned with well-established ontologies to ensure that the knowledge will 
be represented consistently and aligned with other resources and systems that rely on the 

2https://www.fao.org/agrovoc/ 
3https://bigdata.cgiar.org/resources/agronomy-ontology/ 
4https://cdno.info/ 
5https://github.com/FoodOntology/foodon/ 

 
 
 
May 2023 

same ontological frameworks. Populating knowledge graphs by hand is time-consuming 
and expensive, so AI technology offers the opportunity to automatically harvest infor- 
mation much more quickly and efficiently. 

Specifically, we focus on two classification tasks related to organizations. We aim 
to categorize organizations based on 1) a set of environmental issues that are relevant to 
environmental planning and food systems, and 2) standard industrial classification (SIC) 
codes6, which the U.S. government assigns to businesses to categorize the nature of their 
activities. SIC codes are analogous to North American Industry Classification System 
(NAICS) codes, which are used across North America. We have designed natural lan- 
guage processing (NLP) models that read text associated with an organization to auto- 
matically assign the organization to categories for these two tasks. In the following sec- 
tions, we describe the classification tasks in more detail, explain how we collect relevant 
texts for the NLP models to use, present the NLP technology underlying the classification 
models, and show experimental results for the two classification tasks. 

2.  Classification Tasks & Datasets 

2.1.  Environmental Issues Classification Task and Dataset 

There is a large amount of information currently available concerning the state of the 
environment. Around the world, many organizations are collecting and analyzing data. 
However, there remains a major gap in our ability to connect these data sources and make 
them “smart”. They typically use different formats and vocabularies, rendering them un- 
able to be used together. The conservation community lacks an informatics backbone 
to begin linking people and data in ways that enhance our capacity for informed deci- 
sion making in our effort to conserve and enhance the Earth’s ecosystems. This need is 
especially crucial as we enter an unprecedented era of rapid environmental change. 

One key question in environmental planning, food systems, and many other contexts 
is “Who is doing what where?”. “Who” can be people or organizations, “what” may be 
projects or other activities, and “where” could refer to many kinds of geographies. To 
help provide machine-readable answers to this question, Hollander et al. [1] developed 
an ontology called “PPOD” (People, Projects, Organizations, and Datasets).7 This ontol- 
ogy formally describes the characteristics of and relationships between these classes of 
information. 

Members of our team have instantiated the PPOD ontology with information con- 
cerning  the  conservation  of  working  landscapes  in  California.  This  knowledge  graph 
(KG) contains over 2,000 organizations which were identified and collected in an ad hoc 
manner through an array of online searches using terms such as “conservation”, “bio- 
diversity”, “grazing”, and “water supply”. Each organization is associated with multi- 
ple attributes that describe its structure and mission, such as hasOrgType, hasOrgActiv- 
ity, and issues. The “issues” attribute describes potential environmental issues associated 
with the organization. PPOD has pre-defined 44 high-level environmental issues called 
“integrated issues”, and 325 more fine-grained environmental issues called “component 
issues”. The ontology provides a detailed textual description for each issue label and 

6https://www.osha.gov/data/sic-manual 
7https://github.com/PPODschema 

 
 
 
 
 
 
 
May 2023 

Category 

Water 

Physical Infrastructure 

Wastes & Pollution 

Biodiversity 

Land & Soil 

Food Production 

Institutions 

Governance 

Protected Areas 

Sociocultural Systems 

Public Health 

Disasters 

Common Pool Resources 

Air & Climate 

Technology 

# of Organizations  % Percentage 

Example Organization 

845 

702 

578 

556 

535 

393 

345 

274 

253 

231 

179 

162 

128 

126 

122 

39.0 

32.4 

26.7 

25.7 

24.7 

18.2 

15.9 

12.7 

11.7 

10.7 

8.3 

7.5 

5.9 

5.8 

5.6 

American Rivers 

Rebuild NorthBay Foundation 

Heal the Ocean 

Feather River Land Trust 

Agricultural Research Service 

American Grassfed Association 

Southern California Edison 

Merced County 

American Forest Resource Council 

Enterprise Rancheria 

California Department of Public Health 

Tahoe Fire & Fuels Team 

Sustainable Conservation 

Irvine Global Warming Group 

CDFW Data and Technology Division 

Table 1.  Examples of environmental issues and how many organizations each issue is associated with. The 
last column shows an example organization labeled with each issue category. 

the relation between integrated issues and component issues. For example, component 
issues “Air Pollution”, “Air Quality”, “Greenhouse Gas Mitigation” and “Greenhouse 
Gas Emissions” are children of the integrated issue “Air & Climate”, which is described 
as “GHG emissions, ozone layer depletion, air quality, climate change influenced and 
extreme weather events, shifts in growing zones for key crops due to climate change.” 
Expert opinion was used to associate each organization with one or more issues. 

We define a task based on the PPOD ontology called environmental issues classi- 
fication. Given an organization, the task requires that the NLP model assign one or mul- 
tiple environmental issue category labels to the organization that describe the organi- 
zation’s activities and/or mission. The NLP model needs to be trained with a reasonable 
number of examples for each category, so we started with the most common categories 
in the existing PPOD data. We mapped all component issues to their parent integrated 
issues (could be more than one), then sorted the issues based on the number of organi- 
zations associated with each issue. Finally, we selected the 15 most common integrated 
issues to be our set of category labels. The resulting dataset contains 1,870 organizations 
that are associated with 15 environmental issue categories: Air & Climate, Biodiversity, 
Common Pool Resources, Disasters, Food Production, Governance, Institutions, Land 
& Soil, Physical Infrastructure, Protected Areas, Public Health, Sociocultural Systems, 
Technology, Wastes & Pollution, and Water. Table 1 shows the categories, the number 
and percentage of organizations that each category is associated with, and an example 
organization for each category. 

2.2.  Standard Industrial Classification (SIC) Task and Dataset 

Standard Industrial Classification (SIC) codes were created by the U.S. government to 
categorize businesses according to the industry that they serve and operate in. We be- 
lieve that incorporating industry classification such as SIC codes, which remain in use al- 
though replaced by the North American Industry Classification System in 1997, into food 
system ontologies is valuable for understanding the nature of an organization’s activi- 

 
 
 
 
 
May 2023 

ties and its economic and logistical relationships with other organizations (e.g., supply 
chain relationships). Although the SIC codes for many organizations can be looked up 
in government or business databases, having an AI model that can automatically assign 
SIC codes to an organization could be used to 1) categorize newly formed organizations 
quickly, without waiting for official databases to be updated, 2) categorize organizations 
outside of the U.S. with respect to these standardized industry codes, and 3) maintain the 
currency of a knowledge graph by automatically reclassifying organizations on a regular 
basis (say, annually) to reflect changes that an organization has made in its activities (e.g., 
expansion of business activities, or retraction). Toward this end, we define a second task 
called SIC code classification, which requires the NLP model to assign one or multiple 
SIC code category labels to an organization. 

The SIC codes are 4 digits long and are hierarchical. These digits represent the Divi- 
sion, Major Group, Industry Group, and Industry of an entity, respectively. For example, 
a company with code 0116 would belong to the “Soybeans” industry within the “Cash 
Grains” industry group and the “Agricultural Production Crops” major group. This sys- 
tem allows us to study companies at different levels of granularity by simply grouping 
companies according to the first 1,2,3 or 4 digits of their SIC codes. 

To train a NLP model for this task, we need examples of organizations and their asso- 
ciated SIC codes. Conveniently, the Securities and Exchange Commission (SEC) main- 
tains a publicly accessible database of U.S. companies called EDGAR. This database 
contains SIC codes for companies as well as their SEC filing reports. Of particular in- 
terest are the 10-K and 20-F forms, which provide a company’s annual report. These 
forms detail a range of information related to the company’s operations in the past year, 
including financial, legal, risk factors, and other information that allows stakeholders to 
assess the state of the business. So we downloaded these reports as well, as a source of 
textual information that the NLP model could potentially use. Specifically, we collected 
the natural language text from the “Item 1: Business” section of a company’s most recent 
10-K filing, as well as the company’s official SIC code. In the EDGAR database there is 
only one SIC code per company, despite the fact that in principle a company could have 
multiple codes associated with it. 

To collect company information, we started with a list of all 816,115 companies in 
the database and their Central Index Key (CIK) number, which is provided by the SEC.8 
Then we queried each CIK number in EDGAR to collect the name, SIC, SIC description 
and their 10-k forms. We focused our research on the 36,715 organizations that had all 
of these fields. We observed that the distribution of the data is highly skewed, with many 
SIC codes containing very few instances. This may be partly due to the fact that organi- 
zations are forced to pick a SIC/NAICS code when incorporating their organization, but 
business models change, and they may be involved in multiple lines of business that can 
be reflected by multiple SIC/NAICS codes. 

For our experiments, we created a balanced subset of the SEC data so that we have 
the same amount of information for each category and can fairly compare the perfor- 
mance of our NLP models across categories. We decided to focus on just the first 2 digits 
of each SIC code as the category labels, which helps to minimize data sparsity (because 
many of the longer 3-digit and 4-digit codes have relatively few organizations associated 
with them) and provides a useful high-level view of each company’s general type of busi- 

8https://www.sec.gov/Archives/edgar/cik-lookup-data.txt 

 
 
 
May 2023 

SIC  Description 

10  Metal Mining 

13 

20 

27 

28 

34 

35 

36 

37 

Oil and Gas Extraction 

Food and Kindred Products 

Printing, Publishing and Allied Industries 

Chemicals and Allied Products 

Fabricated Metal Products 

Industrial and Commercial Machinery ... 

Electronic 

Transportation  Equipment 

38  Measuring, Photographic, Medical, 

48 

49 

Communications 

Electric, Gas and Sanitary Services 

50  Wholesale Trade - Durable Goods 

51  Wholesale Trade - Nondurable Goods 

SIC  Description 

58 

Eating and Drinking Places 

59  Miscellaneous Retail 

60 

61 

62 

63 

65 

67 

70 

73 

79 

80 

87 

Depository Institutions 

Nondepository Credit Institutions 

Security 

Insurance Carriers 

Real Estate 

Holding and Other Investment Offices 

Hotels, Rooming Houses, Camps ... 

Business  Services 

Amusement and Recreation Services 

Health Services 

Engineering, Accounting, Research ... 

Table 2.  Descriptions for the most common SIC codes. 

ness operations. To create the dataset for our experiments, we selected all of the 2-digit 
SIC codes that have at least 200 associated companies, which resulted in the set of 27 
SIC codes shown in Table 2. Finally, we randomly sampled 200 companies for each of 
these codes, which created a balanced dataset containing 5,400 organizations with their 
2-digit SIC codes. 

2.3.  Organization Information Collection via Google Search 

Our goal is to create a classification model that is given a text about an organization as 
input and produces category labels for that organization as output. So a key question is: 
where can we find text that describes an organization? We initially considered using the 
official website for an organization as the input text because websites often contain in- 
formation about an organization’s initiatives, policies and practices. However, 1) some 
organizations do not have an official website, and 2) many websites do not allow auto- 
mated crawling, and in that case we cannot use a web scraper to automatically extract 
the text from the website. Of the 2,165 organizations in the environmental issues dataset, 
we found that only about half of the organizations’ websites could be crawled. 

As an alternative, we decided to use the Google Search Engine to retrieve textual 
information about organizations. For each organization, we give the organization’s name 
as a search query to the Google Search API and extract the first 10 returned results. The 
search results from Google provide several types of information, such as organic results, 
knowledge graph, local results, related questions, etc. Organic results are the algorith- 
mically calculated query results (as opposed to advertisements) that most users typically 
read, which includes the title, link, and a text “snippet” for each retrieved web page. We 
use the text snippet, which is the small block of text that appears underneath the link to 
a website. It is usually around 100-200 characters in length and typically provides users 
with a general description of the content of the website. For each organization, we then 
concatenate the text snippets from the top 10 retrieved websites into a single “pseudo- 
document” (PseudoDoc), which serves as the text data that we use for the organization. 
Figure 1 shows the pipeline for creating the PseudoDoc for an organization using the 
Google Search API. 

 
 
 
 
 
 
 
May 2023 

snippet 

…… 

The cattle at 5 Bar Beef have an 
idyllic life grazing on a 
multigenerational family ranch in 
east Orange County and are helping 
support wildfire mitigation and ... 

5 Bar Angus Ranch raises premium 
quality Angus Bulls for sale as well as 
home grown angus beef for sale. 
Angus cattle are proven in herd 
genetics and meat ... 

5 Bar Beef is a family ranch in 
Southern California dedicated to 
providing the best-quality and best- 
tasting 100% grass-fed and pasture- 
raised beef. 

5BarBeef. @5_beef. 100% Grass-fed 
beef, pasture raised holistically in 
Orange County, California. ... 
Manassero Farms is now carrying 5 
Bar Beef! 

...... 

Top N Search Results 

Collect “Snippets” 

Figure 1.  Download Google snippets for each organization as its textual representation. 

As we will show in Section 4, using these text snippets from retrieved web pages 
produced reasonably good classification models. We observed two things that explain 
why Google Search produced useful textual information about organizations. First, when 
an organization did have an official website, Google typically found it and ranked it as 
the #1 or #2 top hit. So in many cases, the text snippets returned for an organization 
include text from the organization’s own website. Second, the other websites retrieved 
by Google for an organization usually either (a) discussed the organization, or (b) dis- 
cussed similar organizations (i.e., organizations with similar names). Consequently, the 
text snippets from those websites often contained relevant information that could be use- 
ful for inferring the organization’s activities and mission. By using text snippets from 
10 retrieved websites, each pseudo-document contained information about the organiza- 
tion (or similar organizations) that originated from multiple sources, which collectively 
painted a good picture of the nature of the organization. 

3.  Methods 

3.1.  Background: Pre-trained Language Models 

Pretrained language models, such as BERT [8] and GPT-2 [11], have achieved great suc- 
cess in the field of Natural Language Processing because of their ability to absorb a lot 
of information about language from massive amounts of text, without any human su- 
pervision. These large language models are neural network (“deep learning”) architec- 
tures that can be additionally trained for a specific application task using a method called 
fine-tuning, where the model is provided with human-labeled data for the application 
task. During fine-tuning, the model combines the general knowledge about language that 
it previously absorbed during pre-training with the new information in the task-specific 
data. Fine-tuned models can perform very well for many application tasks, even when 
provided with only a small amount of task-specific data. 

For this work, we use a well-known pre-trained language model called BERT [8]. 
BERT has been pre-trained on 3.3 billion English words from Wikipedia and the Google 

 
 
 
 
 
 
 
 
 
 
May 2023 

Air & Climate 

  0.1 

Biodiversity 

  0.6 

... 

Water 

0.1 

BERT 

[CLS] 

Tok 1

...

Tok N 

  [SEP] 

  Tok 1 

... 

  Tok M 

Organization Text 

Label Description 

... 

Figure 2.  OrgModel-2 model architecture and an illustration example for organization 5 Bar Beef. 

Books text collection. We use the base variant of BERT which has 12 layers (transformer 
blocks),  768  hidden  units  (hidden  size),  and  12  self-attention  heads.  The  BERT-base 
model consists of approximately 110 million parameters (learned weights), offering a 
good balance between computational efficiency and performance across a wide range of 
natural language processing tasks. 

3.2.  Classification Models 

We created two different designs for our classification models — one basic fine-tuning 
design and one slightly more complex design. The first model, OrgModel-1, takes the 
text associated with an organization as input and fine-tunes the BERT language model 
with the “gold” training data (labeled examples) for the task. Specifically, we follow the 
common practice of using the embedding vector of the [CLS] token for the classification 
task and stacking a linear classification layer on top of BERT’s last layer, which produces 
an n-dimensional output vector, where n is the number of categories for the task. 

For  the  SIC  code  classification  task,  we  use  cross-entropy  loss  for  training.  The 
environmental issues classification task is slightly different because it is a multi-label 
problem  (an  organization  can  be  associated  with  more  than  one  issue),  so  we  further 
apply a sigmoid function to transform each dimension value to a number between 0 and 
1. If the number is ≥ 0.5, the system predicts Yes (meaning the organization belongs 
to this environmental issue category), otherwise No. We use binary cross-entropy loss 
during training. 

The second model, OrgModel-2, takes advantage of an additional source of infor- 
mation: the model is provided with expert-written descriptions for each category as in- 
put, along with the text associated with an organization. This provides the model with a 
definition for each category so that the model can potentially produce a richer semantic 
representation of the categories to help find the best match with an organization. 

Specifically, suppose the organization text is denoted by oi, and each category de- 
scription is denoted by d j  ( j = 1..n). We created n sequence pairs ⟨oi, d1⟩, ⟨oi, d2⟩, ..., 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
May 2023 

⟨oi, dn⟩ and asked a system to assign a number between 0 and 1 to each pair ⟨oi, d j⟩ 
representing the strength of association between organization oi and category d j. The 
OrgModel-2 model takes the text for an organization and n label descriptions and pre- 
dicts a strength value for each ⟨oi, d j⟩ pair. Figure 2 depicts the full architecture of the 
OrgModel-2 model. 

4.  Evaluation 

4.1.  Evaluation Metrics 

To  evaluate  the  ability  of  our  NLP  models  to  classify  organizations,  we  report  three 
evaluation metrics that are commonly used in the NLP research community: precision, 
recall and f1-score. Intuitively, precision captures the accuracy when predicting a cat- 
egory, while recall captures coverage for recognizing instances of the category. These 
metrics are defined with respect to a specific category C and computed as: precision = 
TP/(TP + FP), recall = TP/(TP + FN), f 1-score = 2/(precision−1 + recall−1), where 
TP (true positive) is the number of true instances of C that were correctly identified by 
the model; FP (false positive) is the number of instances that the model predicted as C 
but do not belong to C; FN (false negative) is the number of true instances of C that were 
not identified by the model. The f1-score is the harmonic mean of precision and recall, 
which is an average over precision and recall that also reflects how balanced they are 
(more balanced is better). 

In addition, we report micro and macro averaged scores for each evaluation met- 
ric, which provides an overall view of performance across the different categories. The 
micro average score aggregates the results for all instances (across all classes) and then 
calculates the metric. This gives more weight to the most frequent classes because they 
have more instances. In contrast, the macro average score calculates the metric for each 
class separately and then averages the results, which gives equal weight to all classes. 
For example, micro and macro averaged precision are defined as: micro  precision = 
∑n  TPi/(∑n    TPi + ∑n    FPi), macro precision = 1 ∑n    TPi/(TPi + FPi), where i in- 

i=1 

i=1 

i=1 

n    i=1 

dicates the i-th class, and n is the total number of class labels. 

4.2.  Results and Analysis 

4.2.1.  Environmental Issues Classification Results 

For our experiments, the 1,870 organizations in the environmental issues dataset were 
split into 1,370 for training and 500 for testing. Table 3 shows the experimental results 
for each environmental issue category. 

OrgModel-1  vs.  OrgModel-2  Our  OrgModel-1  system  achieved  76.8%  micro- 
averaged f1-score and 69.6% macro-averaged f1-score. By adding the environmental is- 
sue category descriptions, the OrgModel-2 achieved better performance with a 80.1% 
micro-averaged  f1  and  74.0%  macro-averaged  f1.  Overall,  we  see  good  precision  for 
most of the categories (87% micro-average), but recall is lower (74% micro-average). 

If we take a closer look at the individual categories, we can see that OrgModel-2 
achieved the best performance (>80% f1-score) for Biodiversity, Governance, Physical 

 
 
 
 
 
 
 
 
May 2023 

OrgModel-1 

OrgModel-2 

Precision 

Recall 

F1-score  

Precision 

Recall 

F1-score 

Air & Climate 

Biodiversity 

Common Pool Resources 

Disasters 

Food Production 

Governance 

Institutions 

Land & Soil 

Physical Infrastructure 

Protected Areas 

Public Health 

Sociocultural  Systems 

Technology 

Wastes & Pollution 

Water 

Micro Average 

Macro Average 

69.2 

80.6 

57.1 

85.7 

66.7 

97.5 

85.9 

74.4 

94.0 

77.2 

72.0 

83.3 

100.0 

83.7 

93.2 

84.5 

81.4 

32.1 

84.5 

40.0 

44.4 

47.3 

88.6 

62.6 

75.6 

83.0 

57.9 

40.9 

56.6 

66.7 

71.1 

79.2 

70.4 

62.0 

43.9 

82.5 

47.1 

58.5 

55.3 

92.9 

72.4 

75.0 

88.1 

66.2 

52.2 

67.4 

80.0 

76.9 

85.6 

76.8 

69.6 

75.0 

85.6 

70.0 

68.4 

81.2 

100.0 

85.5 

77.9 

95.4 

81.7 

58.3 

85.3 

100.0 

87.4 

94.4 

87.2 

83.1 

53.6 

80.4 

46.7 

48.1 

60.2 

94.3 

66.4 

72.5 

88.8 

64.5 

47.7 

54.7 

71.8 

76.3 

81.4 

74.2 

67.2 

62.5 

82.9 

56.0 

56.5 

69.1 

97.1 

74.7 

75.1 

92.0 

72.1 

52.5 

66.7 

83.6 

81.5 

87.4 

80.1 

74.0 

Table 3.  Environmental issues classification: OrgModel-1 and OrgModel-2 performance for each category. 

Infrastructure, Technology, Wastes & Pollution, and Water. The model struggled the most 
(< 60% f1 score) for Common Pool Resources, Disasters, and Public Health. 

Data Sparsity  We investigated whether the number of instances has an impact on the 
model performance. We focused on the 5 least common categories which are associated 
with fewer than 10 percent of all organizations in the dataset: Public Health, Disasters, 
Common Pool Resources, Air & Climate and Technology; and the 5 most common cate- 
gories which are associated with more than 20 percent of all organizations: Water, Phys- 
ical Infrastructure, Wastes & Pollution, Biodiversity, and Land & Soil. Figure 3 shows a 
plot of their f1-scores for both OrgModel-1 and OrgModel-2. Comparing rare categories 
(circles) with common categories (squares), we can see that the latter perform better with 
both OrgModel-1 and OrgModel-2. OrgModel-1’s average f1-score of the common cate- 
gories is 81.6% but the average f1-score of rare categories is only 56.3%. OrgModel-2’s 
average f1-score of the common categories is 83.8% and the average f1-score of rare 
categories is 62.2%. This huge gap indicates that performance on the rare categories will 
likely improve if we can get more human-annotated data. 

Component-Integrated Mapping  During our dataset construction for environmental 
issue  classification,  we  map  component  issues  to  integrated  issues  to  focus  on  a  lim- 
ited set of common labels. However, this many-to-many mapping can create some prob- 
lems. For example, the component issue Land Use is associated with the integrated issue 
Food Production. Yet not all organizations tagged with Land Use should be tagged with 
Food Production, e.g., organizations “Air Force Civil Engineer Center” and “Snowlands 
Network” don’t actually produce any food. For future work, we will consider a better 
alignment between component and integrated issues, or refine the label set by including 
component issues directly. 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
May 2023 

100 

e
r
o
c
s
-
1
F

80 

60 

OrgModel-1 
OrgModel-2 

40 

T ech n olo g y 
Bio diversity 
P h ysical Infrastructure 
L an d  &  S oil 
P u blic H ealth 
D isasters 
m o n P o ol R eso urces 
A ir  &  Cli m ate 
W astes &  P ollutio n 
C o m

W ater 

Figure 3.  F1-score for each category. Circles ⃝ represent low frequency environmental issues and squares □ 
represent high frequency issues. 

4.2.2.  SIC Code Classification Results 

For the SIC code classification task, we split the 5,400 organizations into train, develop- 
ment, and test sets containing 2700, 900, and 1800 examples respectively. 

10-K Filing vs. Google PseudoDoc  Our first experiment explores the use of two dif- 
ferent types of textual data for the SIC code classification task: the Pseudo-documents 
created from Google’s retrieved text snippets, and the 10-K filing forms from the SEC 
database. We trained two different OrgModel-1 systems, one using textual data from the 
pseudo-documents and one using the 10-K filing reports. These NLP models are iden- 
tical except for the textual descriptions used in the training process. Table 4 shows that 
the model which used Google snippets as the text source for an organization strongly 
outperformed the model which used the text from 10-K forms.9 In fact, the model trained 
on Google snippets achieved a higher f1-score across every single category except for 
one (62 - Security And Commodity Brokers, Dealers, Exchanges, And Services). This 
category had the smallest difference in performance between the two models at 1.3%. 
In many cases, the model trained with Google snippets outperformed the model trained 
with 10-K forms by a significant margin leading to a 13% increase in macro-averaged 
f1-score. This approach is also more generalizable because many organizations do not 
have SEC filings. 

OrgModel-1  vs.  OrgModel-2  We  trained  both  OrgModel-1  and  OrgModel-2  with 
Google PseudoDoc texts for the SIC code classification task as well. For OrgModel-2, 
we also need a description for the category labels, and we experimented with 3 different 
types of information: 1) The shortest representation (Short) is simply the name of the 2 
digit Major Group. 2) The second representation (Tree) exploits the hierarchical nature 
of SIC codes by concatenating together the 2-digit name and the names for all the codes 
in its subtree. The motivation here is to include the specific subcategory information to 

9It is worth noting that about 4% of the organizations’ 10-K filings extracted are empty. 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
May 2023 

10-K Filing 

Google PseudoDoc 

Class 

Recall 

Precision 

F1-Score  Recall 

Precision 

F1-Score 

10 
13 
20 
27 

28 
34 
35 

36 
37 
38 

48 
49 
50 

51 
58 
59 

60 
61 
62 

63 
65 
67 

70 
73 
79 

80 
87 

54.8 
67.7 
54.2 
67.1 

71.4 
38.0 
24.2 

28.5 
47.1 
63.3 

66.6 
68.9 
32.2 

28.1 
89.2 
48.4 

81.5 
70.0 
77.2 

55.9 
66.1 
49.3 

71.4 
28.0 
67.3 

73.7 
30.5 

Macro Avg 

56.4 

73.9 
66.6 
83.6 
70.4 

60.8 
45.0 
11.1 

36.6 
57.8 
61.2 

58.1 
80.9 
15.1 

13.4 
75.3 
40.7 

84.1 
71.0 
85.9 

91.0 
66.1 
50.7 

69.4 
26.6 
42.3 

70.3 
41.2 

57.4 

62.9 
67.1 
65.8 
68.8 

65.6 
41.2 
15.2 

32.1 
51.9 
62.2 

62.1 
74.4 
20.6 

18.1 
81.6 
44.2 

82.8 
70.5 
81.3 

69.3 
66.1 
50.0 

70.4 
27.3 
51.9 

72.0 
35.1 

80.2 
82.8 
77.4 
72.5 

71.7 
68.5 
46.3 

43.0 
68.8 
59.4 

73.5 
74.6 
73.6 

47.4 
86.4 
68.1 

90.6 
89.3 
75.9 

91.0 
69.5 
52.1 

85.7 
30.0 
75.7 

81.8 
54.2 

56.8 

70.0 

82.6 
84.1 
90.1 
73.7 

75.6 
52.1 
44.4 

51.6 
73.6 
75.8 

70.9 
84.1 
42.4 

41.7 
83.1 
59.2 

92.0 
85.5 
84.5 

91.0 
70.5 
52.1 

83.3 
35.0 
64.1 

84.3 
60.3 

69.9 

81.4 
83.4 
83.3 
73.1 

73.6 
59.2 
45.3 

46.9 
71.1 
66.6 

72.2 
79.1 
53.8 

44.4 
84.7 
63.3 

91.3 
87.4 
80.0 

91.0 
70.0 
52.1 

84.5 
32.3 
69.4 

83.0 
57.1 

69.9 

Table 4.  Performance of OrgModel-1 trained on different data sources. 

Label Desc. 

Precision 

Recall 

F1 

OrgModel-1 

- 

OrgModel-2 

Short 

Tree 
Long 

69.9 

47.0 

73.6 
73.6 

70.0 

69.9 

49.9 

73.6 
73.5 

46.1 

73.4 
73.1 

Table 5.  Macro average scores for OrgModel-1 and OrgModel-2 with different representations for the label 
description. 

create a richer representation of the parent category. 3) We used the plain text explanation 
(Long) of the Major Group found in the SIC manual. This is a written explanation of 
the criteria for an entity to be included in the specified Major Group. Table 5 shows the 
results. The short representation didn’t contain enough information to perform well. Be- 
tween the tree and long representations, we see similar performance in terms of f1-score. 
Comparing OrgModel-2 with tree description to OrgModel-1, OrgModel-2 achieved a 
3% increase in f1-score making it our best performing model on the SIC task. 

 
 
 
 
 
 
 
 
 
 
May 2023 

Overall,  the  classifier  achieves  reasonable  performance  across  the  different  cate- 
gories; however, there are a few categories (35, 50, 51, 67) where the performance is still 
weak, possibly due to being particularly broad and difficult to distinguish from other cat- 
egories. Yet, our model achieves greater than 70% f1-score on the majority of categories, 
illustrating the promise of using NLP models to automatically classify organizations. 

5.  Conclusion and Future Plans 

Our study has demonstrated the potential of using Natural Language Processing (NLP) 
techniques for the automatic acquisition of structured food system knowledge from un- 
structured  sources.  This  includes  the  classification  of  entities  involved  in  food  activ- 
ities,  and  their  linkage  to  social,  environmental,  and  health-related  issues.  Using  text 
snippets retrieved from Google’s search engine as a descriptive basis, our NLP mod- 
els were able to classify organizations according to environmental issue categories and 
Standard Industrial Classification (SIC) codes. Specifically, we build our models based 
on transformer-based language models. Our experimental results show that using textual 
data from Google Search achieved a better performance than using 10-K filings from 
organizations’ annual reports, which are provided by the Securities and Exchange Com- 
mission (SEC) database. We also show that by incorporating the description text of the 
environmental issue categories or SIC codes, our model can achieve better performance 
for both classification tasks. The promising results underline the applicability of this ap- 
proach to improve food ontologies as well as other intelligent food systems knowledge 
resources  with  little  to  no  human  supervision.  Our  work  illustrates  how  NLP  models 
hold the potential for a wide range of automatic categorization of food system actors and 
their activities into existing food, environment, and health system ontologies. These on- 
tologies instantiated by NLP models represent burgeoning yet powerful instruments for 
quick and efficient population of large food systems knowledge graphs with consistent 
knowledge representation. 

Our  research  opens  the  door  to  providing  a  critical  component  to  the  design  and 
creation of reusable cyberinfrastructure components capable of addressing major social- 
environmental issues like scaling climate actions in food systems. Other components will 
include the development of additional technological, knowledge, and relational infras- 
tructures that will build on recent advances in data, modeling, and management. Linking 
these components together affords the opportunity to make leading edge insights, tools, 
and practical guidance available to action partners across the food system. 

Acknowledgments 

We want to thank the anonymous reviewers for their valuable comments. This research 
was supported in part by the ICICLE project through NSF award OAC 2112606 and the 
Canadian Institutes of Health Research (CIHR) FRN 177412. 

References 

[1]  Hollander AD, Hoy C, Huber PR, Hyder A, Lange MC, Latham A, Quinn JF, Rig- 
gle CM, Tomich TP. Toward smart foodsheds: Using stakeholder engagement to 

 
 
 
 
 
May 2023 

improve informatics frameworks for regional food systems. Annals of the Ameri- 
can Association of Geographers. 2020. DOI: 10.1080/24694452.2019.1662764 
[2]  Boulos MN, Yassine A, Shirmohammadi S, Namahoot CS, Bru¨ckner M. Towards 
an “Internet of Food”: food ontologies for the internet of things. Future Internet. 
2015. 

[3]  Dooley D, Andre´s-Herna´ndez L, Bordea G, Carmody L, Cavalieri D, Chan L, 
Castellano-Escuder P, Lachat C, Mougin F, Vitali F, Yang C. Obo foundry food 
ontology interconnectivity. InCEUR Workshop Proceedings. 2021. 

[4]  Dooley  DM,  Griffiths  EJ,  Gosal  GS,  Buttigieg  PL,  Hoehndorf  R,  Lange  MC, 
Schriml LM, Brinkman FS, Hsiao WW. FoodOn: a harmonized food ontology to 
increase global food traceability, quality control and data integration. npj Science 
of Food. 2018. 

[5]  D’Odorico P, Carr JA, Davis KF, Dell’Angelo J, Seekell DA. Food inequality, 

injustice, and rights. BioScience. 2019. 

[6]  Fanzo J, Arabi M, Burlingame B, Haddad L, Kimenju S, Miller G, Nie F, Recine 
E, Serra-Majem L, Sinha D. Nutrition and food systems. A report by the high level 
panel of experts on food security and nutrition of the committee on world food 
security. 2017. 

[7]  Fanzo J, Davis C. Global food systems, diets, and nutrition. Springer International 

Publishing. 2021. 

[8]  Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-training of deep bidirec- 
tional transformers for language understanding. In Proceedings of the 2019 Con- 
ference of the North American Chapter of the Association for Computational Lin- 
guistics: Human Language Technologies. 2019. 

[9]  Kotiaho  JS,  Halme  P.  The  IPBES  assessment  report  on  land  degradation  and 

restoration. IPBES Secretariat, UN Campus: Bonn, Germany. 2018. 

[10]  Nicole´tis E´ , Termine P. Reducing inequalities for food security and nutrition - 
HLPE  consultation  on  the  report’s  scope.  Global  Forum  on  Food  Security  and 
Nutrition (FSN Forum). 2022. 

[11]  Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I. Language models are 

unsupervised multitask learners. OpenAI blog. 2019. 

[12]  Smith B. Ontology. In Blackwell Guide to the Philosophy of Computing and In- 

formation, edited by Luciano Floridi. Oxford: Blackwell. 2003. 

[13]  Tomich TP, Hoy C, Dimock MR, Hollander AD, Huber PR, Hyder A, Lange MC, 
Riggle CM, Roberts MT, Quinn JF. Why do we need food systems informatics? 
Introduction to this special collection on smart and connected regional food sys- 
tems. Sustainability. 2023. 

[14]  FAO, IFAD, UNICEF, WFP and WHO. The state of food security and nutrition in 
the world 2021. Transforming food systems for food security, improved nutrition 
and affordable healthy diets for all. Rome, FAO. 2021. DOI: 10.4060/cb4474en 

[15]  United Nations Environment Programme. Emissions gap report 2022: The closing 
window - Climate crisis calls for rapid transformation of societies. Nairobi. 2022. 

 
 
","May 2023 Classifying Organizations for Food System Ontologies using Natural Language Processing Tianyu JIANG a , Sonia VINOGRADOVA b , Nathan STRINGHAM a , E. Louise EARL b , Allan D. HOLLANDER c , Patrick R. HUBER c , Ellen RILOFF a,1 , R. Sandra SCHILLO b , Giorgio A. UBBIALI d , Matthew LANGE e , a University of Utah b University of Ottawa c University of California , Davis d University of Milan e IC-FOODS ORCiD ID : E. Louise Earl https : //orcid.org/0000-0002-5646-0132 , R. Sandra Schillo https : //orcid.org/0000-0002-3468-0206 , Giorgio A. Ubbiali https : //orcid.org/0000-0001-7872-1770 , Matthew Lange https : //orcid.org/0000-0002-6148-7962 Abstract . Our research explores the use of natural language processing ( NLP ) methods to automatically classify entities for the purpose of knowledge graph pop- ulation and integration with food system ontologies . We have created NLP models that can automatically classify organizations with respect to categories associated with environmental issues as well as Standard Industrial Classification ( SIC ) codes , which are used by the U.S. government to characterize business activities . As in- put , the NLP models are provided with text snippets retrieved by Google ’ s search engine for each organization , which serves as a textual description of the organi- zation that is used for learning . Our experimental results show that NLP models can achieve reasonably good performance for these two classification tasks , and they rely on a general framework that could be applied to many other classification problems as well . We believe that NLP models represent a promising approach for automatically harvesting information to populate knowledge graphs and aligning the information with existing ontologies through shared categories and concepts . Keywords . food system ontologies , classification models , natural language processing , SIC , sustainability issues , unstructured data 1 . Introduction Food systems include activities and relationships that are involved with the production , transport , and consumption of food , and thus are linked to a wide variety of natural and human systems ( Tomich et al . [ 13 ] ) . Their extensive nature can have dramatic impacts 1Corresponding Author : Ellen Riloff ( riloff @ cs.utah.edu ) . May 2023 on these natural and human systems ( IPBES [ 9 ] , UNEP [ 15 ] ) . Conversely , food systems are vulnerable to changes in these other systems . Food activities also show a pivotal role in fostering human social and health-related issues . For instance , inequalities and injus- tices exist across the whole global food supply chain , as well as within related sectors ( D ’ Odorico et al . [ 5 ] , Nicole´tis and Termine [ 10 ] ) . To date , hunger , malnutrition , and micronutrient deficiencies , just to cite a few , still largely threaten humanity worldwide ( Fanzo et al . [ 6 ] , Fanzo and Davis [ 7 ] , UNICEF [ 14 ] ) . As our world becomes more interconnected and interdependent , ontologies become useful ways of categorizing and relating information together . This is especially appli- cable to food systems , where data access , interoperability , and reusability are essential to deal with the interrelated issues arising from food systems activities . An ontology is a “ formal theory ” which provides a “ commonly accepted ” dictionary of terms , sup- ported by a “ canonical syntax ” and a set of axioms , for a knowledge domain of interest ( Smith [ 12 ] ) . In doing so , an ontology offers a common semantic framework for that domain of knowledge ( Smith [ 12 ] , Hollander et al . [ 1 ] ) . Thus , an ontology fosters data access , interoperability , and reusability across several disparate resources , which employ that ontology as a common reference semantic standard ( Hollander et al . [ 1 ] ) . Nowa- days , several ontologies focusing on food , health , and related aspects have been devel- oped ( Boulos et al . [ 2 ] , Dooley et al . [ 3 ] ) . Here , we just cite a few noteworthy ones as examples : 1 ) AGROVOC2 is a three-decade-long well-established “ multilingual the- saurus ” , addressing food and related domains of interest ( Boulos et al . [ 2 ] ) . It belongs to the United Nations Food and Agriculture Organisation ( FAO ) . 2 ) The Agronomy On- tology AgrO3 was developed within the frame of the CGIAR Platform for Big Data in Agriculture , and which proposes a vocabulary of terms , covering “ agronomic manage- ment practices , implements , and variables used during agronomic experiments ” ( Dooley et al . [ 3 ] ) . 3 ) The Compositional Dietary Nutrition Ontology CDNO4 presents terms re- lated to “ nutritional attributes from crops , livestock , and fisheries that contribute to hu- man diet and which are referenced in precision food commodity laboratory analytics ” ( Dooley et al . [ 3 ] ) . 4 ) The Food Ontology FoodOn5 provides a lexicon about “ basic raw food source ingredients , process terms for packaging , cooking , and preservation , and an upper-level variety of product type schemes under which food products can be catego- rized ” . FoodOn stands out as a fundamental ontology in addressing food-related aspects ( Dooley et al . [ 4 ] ) . Further , FoodOn aims to state the lingua franca within the domain of food , for sharing and reusing food-related information both by humans and machines ( Dooley et al . [ 4 ] ) . Despite the availability of food-health ontologies , the envisaged data access , interoperability and reusability across food industries and other food-related sec- tors , as we claimed above , appears to have not been reached yet ( Tomich et al . [ 13 ] ) . In this paper , we present new research that uses artificial intelligence ( AI ) technol- ogy to automatically categorize organizations , ultimately for the purpose of linking them into food system ontologies . Our eventual goal is to automatically populate large knowl- edge graphs of information related to agriculture and food systems , where the concepts in the graph are aligned with well-established ontologies to ensure that the knowledge will be represented consistently and aligned with other resources and systems that rely on the 2https : //www.fao.org/agrovoc/ 3https : 4https : //cdno.info/ 5https : //github.com/FoodOntology/foodon/ May 2023 same ontological frameworks . Populating knowledge graphs by hand is time-consuming and expensive , so AI technology offers the opportunity to automatically harvest infor- mation much more quickly and efficiently . Specifically , we focus on two classification tasks related to organizations . We aim to categorize organizations based on 1 ) a set of environmental issues that are relevant to environmental planning and food systems , and 2 ) standard industrial classification ( SIC ) codes6 , which the U.S. government assigns to businesses to categorize the nature of their activities . SIC codes are analogous to North American Industry Classification System ( NAICS ) codes , which are used across North America . We have designed natural lan- guage processing ( NLP ) models that read text associated with an organization to auto- matically assign the organization to categories for these two tasks . In the following sec- tions , we describe the classification tasks in more detail , explain how we collect relevant texts for the NLP models to use , present the NLP technology underlying the classification models , and show experimental results for the two classification tasks . 2 . Classification Tasks & Datasets 2.1 . Environmental Issues Classification Task and Dataset There is a large amount of information currently available concerning the state of the environment . Around the world , many organizations are collecting and analyzing data . However , there remains a major gap in our ability to connect these data sources and make them “ smart ” . They typically use different formats and vocabularies , rendering them un- able to be used together . The conservation community lacks an informatics backbone to begin linking people and data in ways that enhance our capacity for informed deci- sion making in our effort to conserve and enhance the Earth ’ s ecosystems . This need is especially crucial as we enter an unprecedented era of rapid environmental change . One key question in environmental planning , food systems , and many other contexts is “ Who is doing what where ? ” . “ Who ” can be people or organizations , “ what ” may be projects or other activities , and “ where ” could refer to many kinds of geographies . To help provide machine-readable answers to this question , Hollander et al . [ 1 ] developed an ontology called “ PPOD ” ( People , Projects , Organizations , and Datasets ) .7 This ontol- ogy formally describes the characteristics of and relationships between these classes of information . Members of our team have instantiated the PPOD ontology with information con- cerning the conservation of working landscapes in California . This knowledge graph ( KG ) contains over 2,000 organizations which were identified and collected in an ad hoc manner through an array of online searches using terms such as “ conservation ” , “ bio- diversity ” , “ grazing ” , and “ water supply ” . Each organization is associated with multi- ple attributes that describe its structure and mission , such as hasOrgType , hasOrgActiv- ity , and issues . The “ issues ” attribute describes potential environmental issues associated with the organization . PPOD has pre-defined 44 high-level environmental issues called “ integrated issues ” , and 325 more fine-grained environmental issues called “ component issues ” . The ontology provides a detailed textual description for each issue label and 6https : //www.osha.gov/data/sic-manual 7https : //github.com/PPODschema May 2023 Category Water Physical Infrastructure Wastes & Pollution Biodiversity Land & Soil Food Production Institutions Governance Protected Areas Sociocultural Systems Public Health Disasters Common Pool Resources Air & Climate Technology # of Organizations % Percentage Example Organization 845 702 578 556 535 393 345 274 253 231 179 162 128 126 122 39.0 32.4 26.7 25.7 24.7 18.2 15.9 12.7 11.7 10.7 8.3 7.5 5.9 5.8 5.6 American Rivers Rebuild NorthBay Foundation Heal the Ocean Feather River Land Trust Agricultural Research Service American Grassfed Association Southern California Edison Merced County American Forest Resource Council Enterprise Rancheria California Department of Public Health Tahoe Fire & Fuels Team Sustainable Conservation Irvine Global Warming Group CDFW Data and Technology Division Table 1 . Examples of environmental issues and how many organizations each issue is associated with . The last column shows an example organization labeled with each issue category . the relation between integrated issues and component issues . For example , component issues “ Air Pollution ” , “ Air Quality ” , “ Greenhouse Gas Mitigation ” and “ Greenhouse Gas Emissions ” are children of the integrated issue “ Air & Climate ” , which is described as “ GHG emissions , ozone layer depletion , air quality , climate change influenced and extreme weather events , shifts in growing zones for key crops due to climate change. ” Expert opinion was used to associate each organization with one or more issues . We define a task based on the PPOD ontology called environmental issues classi- fication . Given an organization , the task requires that the NLP model assign one or mul- tiple environmental issue category labels to the organization that describe the organi- zation ’ s activities and/or mission . The NLP model needs to be trained with a reasonable number of examples for each category , so we started with the most common categories in the existing PPOD data . We mapped all component issues to their parent integrated issues ( could be more than one ) , then sorted the issues based on the number of organi- zations associated with each issue . Finally , we selected the 15 most common integrated issues to be our set of category labels . The resulting dataset contains 1,870 organizations that are associated with 15 environmental issue categories : Air & Climate , Biodiversity , Common Pool Resources , Disasters , Food Production , Governance , Institutions , Land & Soil , Physical Infrastructure , Protected Areas , Public Health , Sociocultural Systems , Technology , Wastes & Pollution , and Water . Table 1 shows the categories , the number and percentage of organizations that each category is associated with , and an example organization for each category . 2.2 . Standard Industrial Classification ( SIC ) Task and Dataset Standard Industrial Classification ( SIC ) codes were created by the U.S. government to categorize businesses according to the industry that they serve and operate in . We be- lieve that incorporating industry classification such as SIC codes , which remain in use al- though replaced by the North American Industry Classification System in 1997 , into food system ontologies is valuable for understanding the nature of an organization ’ s activi- May 2023 ties and its economic and logistical relationships with other organizations ( e.g. , supply chain relationships ) . Although the SIC codes for many organizations can be looked up in government or business databases , having an AI model that can automatically assign SIC codes to an organization could be used to 1 ) categorize newly formed organizations quickly , without waiting for official databases to be updated , 2 ) categorize organizations outside of the U.S. with respect to these standardized industry codes , and 3 ) maintain the currency of a knowledge graph by automatically reclassifying organizations on a regular basis ( say , annually ) to reflect changes that an organization has made in its activities ( e.g. , expansion of business activities , or retraction ) . Toward this end , we define a second task called SIC code classification , which requires the NLP model to assign one or multiple SIC code category labels to an organization . The SIC codes are 4 digits long and are hierarchical . These digits represent the Divi- sion , Major Group , Industry Group , and Industry of an entity , respectively . For example , a company with code 0116 would belong to the “ Soybeans ” industry within the “ Cash Grains ” industry group and the “ Agricultural Production Crops ” major group . This sys- tem allows us to study companies at different levels of granularity by simply grouping companies according to the first 1,2,3 or 4 digits of their SIC codes . To train a NLP model for this task , we need examples of organizations and their asso- ciated SIC codes . Conveniently , the Securities and Exchange Commission ( SEC ) main- tains a publicly accessible database of U.S. companies called EDGAR . This database contains SIC codes for companies as well as their SEC filing reports . Of particular in- terest are the 10-K and 20-F forms , which provide a company ’ s annual report . These forms detail a range of information related to the company ’ s operations in the past year , including financial , legal , risk factors , and other information that allows stakeholders to assess the state of the business . So we downloaded these reports as well , as a source of textual information that the NLP model could potentially use . Specifically , we collected the natural language text from the “ Item 1 : Business ” section of a company ’ s most recent 10-K filing , as well as the company ’ s official SIC code . In the EDGAR database there is only one SIC code per company , despite the fact that in principle a company could have multiple codes associated with it . To collect company information , we started with a list of all 816,115 companies in the database and their Central Index Key ( CIK ) number , which is provided by the SEC.8 Then we queried each CIK number in EDGAR to collect the name , SIC , SIC description and their 10-k forms . We focused our research on the 36,715 organizations that had all of these fields . We observed that the distribution of the data is highly skewed , with many SIC codes containing very few instances . This may be partly due to the fact that organi- zations are forced to pick a SIC/NAICS code when incorporating their organization , but business models change , and they may be involved in multiple lines of business that can be reflected by multiple SIC/NAICS codes . For our experiments , we created a balanced subset of the SEC data so that we have the same amount of information for each category and can fairly compare the perfor- mance of our NLP models across categories . We decided to focus on just the first 2 digits of each SIC code as the category labels , which helps to minimize data sparsity ( because many of the longer 3-digit and 4-digit codes have relatively few organizations associated with them ) and provides a useful high-level view of each company ’ s general type of busi- 8https : May 2023 SIC Description 10 Metal Mining 13 20 27 28 34 35 36 37 Oil and Gas Extraction Food and Kindred Products Printing , Publishing and Allied Industries Chemicals and Allied Products Fabricated Metal Products Industrial and Commercial Machinery ... Electronic Transportation Equipment 38 Measuring , Photographic , Medical , 48 49 Communications Electric , Gas and Sanitary Services 50 Wholesale Trade - Durable Goods 51 Wholesale Trade - Nondurable Goods SIC Description 58 Eating and Drinking Places 59 Miscellaneous Retail 60 61 62 63 65 67 70 73 79 80 87 Depository Institutions Nondepository Credit Institutions Security Insurance Carriers Real Estate Holding and Other Investment Offices Hotels , Rooming Houses , Camps ... Business Services Amusement and Recreation Services Health Services Engineering , Accounting , Research ... Table 2 . Descriptions for the most common SIC codes . ness operations . To create the dataset for our experiments , we selected all of the 2-digit SIC codes that have at least 200 associated companies , which resulted in the set of 27 SIC codes shown in Table 2 . Finally , we randomly sampled 200 companies for each of these codes , which created a balanced dataset containing 5,400 organizations with their 2-digit SIC codes . 2.3 . Organization Information Collection via Google Search Our goal is to create a classification model that is given a text about an organization as input and produces category labels for that organization as output . So a key question is : where can we find text that describes an organization ? We initially considered using the official website for an organization as the input text because websites often contain in- formation about an organization ’ s initiatives , policies and practices . However , 1 ) some organizations do not have an official website , and 2 ) many websites do not allow auto- mated crawling , and in that case we can not use a web scraper to automatically extract the text from the website . Of the 2,165 organizations in the environmental issues dataset , we found that only about half of the organizations ’ websites could be crawled . As an alternative , we decided to use the Google Search Engine to retrieve textual information about organizations . For each organization , we give the organization ’ s name as a search query to the Google Search API and extract the first 10 returned results . The search results from Google provide several types of information , such as organic results , knowledge graph , local results , related questions , etc . Organic results are the algorith- mically calculated query results ( as opposed to advertisements ) that most users typically read , which includes the title , link , and a text “ snippet ” for each retrieved web page . We use the text snippet , which is the small block of text that appears underneath the link to a website . It is usually around 100-200 characters in length and typically provides users with a general description of the content of the website . For each organization , we then concatenate the text snippets from the top 10 retrieved websites into a single “ pseudo- document ” ( PseudoDoc ) , which serves as the text data that we use for the organization . Figure 1 shows the pipeline for creating the PseudoDoc for an organization using the Google Search API . May 2023 snippet …… The cattle at 5 Bar Beef have an idyllic life grazing on a multigenerational family ranch in east Orange County and are helping support wildfire mitigation and ... 5 Bar Angus Ranch raises premium quality Angus Bulls for sale as well as home grown angus beef for sale . Angus cattle are proven in herd genetics and meat ... 5 Bar Beef is a family ranch in Southern California dedicated to providing the best-quality and best- tasting 100 % grass-fed and pasture- raised beef . 5BarBeef . @ 5_beef . 100 % Grass-fed beef , pasture raised holistically in Orange County , California . ... Manassero Farms is now carrying 5 Bar Beef ! ...... Top N Search Results Collect “ Snippets ” Figure 1 . Download Google snippets for each organization as its textual representation . As we will show in Section 4 , using these text snippets from retrieved web pages produced reasonably good classification models . We observed two things that explain why Google Search produced useful textual information about organizations . First , when an organization did have an official website , Google typically found it and ranked it as the # 1 or # 2 top hit . So in many cases , the text snippets returned for an organization include text from the organization ’ s own website . Second , the other websites retrieved by Google for an organization usually either ( a ) discussed the organization , or ( b ) dis- cussed similar organizations ( i.e. , organizations with similar names ) . Consequently , the text snippets from those websites often contained relevant information that could be use- ful for inferring the organization ’ s activities and mission . By using text snippets from 10 retrieved websites , each pseudo-document contained information about the organiza- tion ( or similar organizations ) that originated from multiple sources , which collectively painted a good picture of the nature of the organization . 3 . Methods 3.1 . Background : Pre-trained Language Models Pretrained language models , such as BERT [ 8 ] and GPT-2 [ 11 ] , have achieved great suc- cess in the field of Natural Language Processing because of their ability to absorb a lot of information about language from massive amounts of text , without any human su- pervision . These large language models are neural network ( “ deep learning ” ) architec- tures that can be additionally trained for a specific application task using a method called fine-tuning , where the model is provided with human-labeled data for the application task . During fine-tuning , the model combines the general knowledge about language that it previously absorbed during pre-training with the new information in the task-specific data . Fine-tuned models can perform very well for many application tasks , even when provided with only a small amount of task-specific data . For this work , we use a well-known pre-trained language model called BERT [ 8 ] . BERT has been pre-trained on 3.3 billion English words from Wikipedia and the Google May 2023 Air & Climate 0.1 Biodiversity 0.6 ... Water 0.1 BERT [ CLS ] Tok 1 ... Tok N [ SEP ] Tok 1 ... Tok M Organization Text Label Description ... Figure 2 . OrgModel-2 model architecture and an illustration example for organization 5 Bar Beef . Books text collection . We use the base variant of BERT which has 12 layers ( transformer blocks ) , 768 hidden units ( hidden size ) , and 12 self-attention heads . The BERT-base model consists of approximately 110 million parameters ( learned weights ) , offering a good balance between computational efficiency and performance across a wide range of natural language processing tasks . 3.2 . Classification Models We created two different designs for our classification models — one basic fine-tuning design and one slightly more complex design . The first model , OrgModel-1 , takes the text associated with an organization as input and fine-tunes the BERT language model with the “ gold ” training data ( labeled examples ) for the task . Specifically , we follow the common practice of using the embedding vector of the [ CLS ] token for the classification task and stacking a linear classification layer on top of BERT ’ s last layer , which produces an n-dimensional output vector , where n is the number of categories for the task . For the SIC code classification task , we use cross-entropy loss for training . The environmental issues classification task is slightly different because it is a multi-label problem ( an organization can be associated with more than one issue ) , so we further apply a sigmoid function to transform each dimension value to a number between 0 and 1 . If the number is ≥ 0.5 , the system predicts Yes ( meaning the organization belongs to this environmental issue category ) , otherwise No . We use binary cross-entropy loss during training . The second model , OrgModel-2 , takes advantage of an additional source of infor- mation : the model is provided with expert-written descriptions for each category as in- put , along with the text associated with an organization . This provides the model with a definition for each category so that the model can potentially produce a richer semantic representation of the categories to help find the best match with an organization . Specifically , suppose the organization text is denoted by oi , and each category de- scription is denoted by d j ( j = 1 .. n ) . We created n sequence pairs ⟨oi , d1⟩ , ⟨oi , d2⟩ , ... , May 2023 ⟨oi , dn⟩ and asked a system to assign a number between 0 and 1 to each pair ⟨oi , d j⟩ representing the strength of association between organization oi and category d j . The OrgModel-2 model takes the text for an organization and n label descriptions and pre- dicts a strength value for each ⟨oi , d j⟩ pair . Figure 2 depicts the full architecture of the OrgModel-2 model . 4 . Evaluation 4.1 . Evaluation Metrics To evaluate the ability of our NLP models to classify organizations , we report three evaluation metrics that are commonly used in the NLP research community : precision , recall and f1-score . Intuitively , precision captures the accuracy when predicting a cat- egory , while recall captures coverage for recognizing instances of the category . These metrics are defined with respect to a specific category C and computed as : precision = TP/ ( TP + FP ) , recall = TP/ ( TP + FN ) , f 1-score = 2/ ( precision−1 + recall−1 ) , where TP ( true positive ) is the number of true instances of C that were correctly identified by the model ; FP ( false positive ) is the number of instances that the model predicted as C but do not belong to C ; FN ( false negative ) is the number of true instances of C that were not identified by the model . The f1-score is the harmonic mean of precision and recall , which is an average over precision and recall that also reflects how balanced they are ( more balanced is better ) . In addition , we report micro and macro averaged scores for each evaluation met- ric , which provides an overall view of performance across the different categories . The micro average score aggregates the results for all instances ( across all classes ) and then calculates the metric . This gives more weight to the most frequent classes because they have more instances . In contrast , the macro average score calculates the metric for each class separately and then averages the results , which gives equal weight to all classes . For example , micro and macro averaged precision are defined as : micro precision = ∑n TPi/ ( ∑n TPi + ∑n FPi ) , macro precision = 1 ∑n TPi/ ( TPi + FPi ) , where i in- i=1 i=1 i=1 n i=1 dicates the i-th class , and n is the total number of class labels . 4.2 . Results and Analysis 4.2.1 . Environmental Issues Classification Results For our experiments , the 1,870 organizations in the environmental issues dataset were split into 1,370 for training and 500 for testing . Table 3 shows the experimental results for each environmental issue category . OrgModel-1 vs. OrgModel-2 Our OrgModel-1 system achieved 76.8 % micro- averaged f1-score and 69.6 % macro-averaged f1-score . By adding the environmental is- sue category descriptions , the OrgModel-2 achieved better performance with a 80.1 % micro-averaged f1 and 74.0 % macro-averaged f1 . Overall , we see good precision for most of the categories ( 87 % micro-average ) , but recall is lower ( 74 % micro-average ) . If we take a closer look at the individual categories , we can see that OrgModel-2 achieved the best performance ( > 80 % f1-score ) for Biodiversity , Governance , Physical May 2023 OrgModel-1 OrgModel-2 Precision Recall F1-score Precision Recall F1-score Air & Climate Biodiversity Common Pool Resources Disasters Food Production Governance Institutions Land & Soil Physical Infrastructure Protected Areas Public Health Sociocultural Systems Technology Wastes & Pollution Water Micro Average Macro Average 69.2 80.6 57.1 85.7 66.7 97.5 85.9 74.4 94.0 77.2 72.0 83.3 100.0 83.7 93.2 84.5 81.4 32.1 84.5 40.0 44.4 47.3 88.6 62.6 75.6 83.0 57.9 40.9 56.6 66.7 71.1 79.2 70.4 62.0 43.9 82.5 47.1 58.5 55.3 92.9 72.4 75.0 88.1 66.2 52.2 67.4 80.0 76.9 85.6 76.8 69.6 75.0 85.6 70.0 68.4 81.2 100.0 85.5 77.9 95.4 81.7 58.3 85.3 100.0 87.4 94.4 87.2 83.1 53.6 80.4 46.7 48.1 60.2 94.3 66.4 72.5 88.8 64.5 47.7 54.7 71.8 76.3 81.4 74.2 67.2 62.5 82.9 56.0 56.5 69.1 97.1 74.7 75.1 92.0 72.1 52.5 66.7 83.6 81.5 87.4 80.1 74.0 Table 3 . Environmental issues classification : OrgModel-1 and OrgModel-2 performance for each category . Infrastructure , Technology , Wastes & Pollution , and Water . The model struggled the most ( < 60 % f1 score ) for Common Pool Resources , Disasters , and Public Health . Data Sparsity We investigated whether the number of instances has an impact on the model performance . We focused on the 5 least common categories which are associated with fewer than 10 percent of all organizations in the dataset : Public Health , Disasters , Common Pool Resources , Air & Climate and Technology ; and the 5 most common cate- gories which are associated with more than 20 percent of all organizations : Water , Phys- ical Infrastructure , Wastes & Pollution , Biodiversity , and Land & Soil . Figure 3 shows a plot of their f1-scores for both OrgModel-1 and OrgModel-2 . Comparing rare categories ( circles ) with common categories ( squares ) , we can see that the latter perform better with both OrgModel-1 and OrgModel-2 . OrgModel-1 ’ s average f1-score of the common cate- gories is 81.6 % but the average f1-score of rare categories is only 56.3 % . OrgModel-2 ’ s average f1-score of the common categories is 83.8 % and the average f1-score of rare categories is 62.2 % . This huge gap indicates that performance on the rare categories will likely improve if we can get more human-annotated data . Component-Integrated Mapping During our dataset construction for environmental issue classification , we map component issues to integrated issues to focus on a lim- ited set of common labels . However , this many-to-many mapping can create some prob- lems . For example , the component issue Land Use is associated with the integrated issue Food Production . Yet not all organizations tagged with Land Use should be tagged with Food Production , e.g. , organizations “ Air Force Civil Engineer Center ” and “ Snowlands Network ” don ’ t actually produce any food . For future work , we will consider a better alignment between component and integrated issues , or refine the label set by including component issues directly . May 2023 100 e r o c s - 1 F 80 60 OrgModel-1 OrgModel-2 40 T ech n olo g y Bio diversity P h ysical Infrastructure L an d & S oil P u blic H ealth D isasters m o n P o ol R eso urces A ir & Cli m ate W astes & P ollutio n C o m W ater Figure 3 . F1-score for each category . Circles ⃝ represent low frequency environmental issues and squares □ represent high frequency issues . 4.2.2 . SIC Code Classification Results For the SIC code classification task , we split the 5,400 organizations into train , develop- ment , and test sets containing 2700 , 900 , and 1800 examples respectively . 10-K Filing vs. Google PseudoDoc Our first experiment explores the use of two dif- ferent types of textual data for the SIC code classification task : the Pseudo-documents created from Google ’ s retrieved text snippets , and the 10-K filing forms from the SEC database . We trained two different OrgModel-1 systems , one using textual data from the pseudo-documents and one using the 10-K filing reports . These NLP models are iden- tical except for the textual descriptions used in the training process . Table 4 shows that the model which used Google snippets as the text source for an organization strongly outperformed the model which used the text from 10-K forms.9 In fact , the model trained on Google snippets achieved a higher f1-score across every single category except for one ( 62 - Security And Commodity Brokers , Dealers , Exchanges , And Services ) . This category had the smallest difference in performance between the two models at 1.3 % . In many cases , the model trained with Google snippets outperformed the model trained with 10-K forms by a significant margin leading to a 13 % increase in macro-averaged f1-score . This approach is also more generalizable because many organizations do not have SEC filings . OrgModel-1 vs. OrgModel-2 We trained both OrgModel-1 and OrgModel-2 with Google PseudoDoc texts for the SIC code classification task as well . For OrgModel-2 , we also need a description for the category labels , and we experimented with 3 different types of information : 1 ) The shortest representation ( Short ) is simply the name of the 2 digit Major Group . 2 ) The second representation ( Tree ) exploits the hierarchical nature of SIC codes by concatenating together the 2-digit name and the names for all the codes in its subtree . The motivation here is to include the specific subcategory information to 9It is worth noting that about 4 % of the organizations ’ 10-K filings extracted are empty . May 2023 10-K Filing Google PseudoDoc Class Recall Precision F1-Score Recall Precision F1-Score 10 13 20 27 28 34 35 36 37 38 48 49 50 51 58 59 60 61 62 63 65 67 70 73 79 80 87 54.8 67.7 54.2 67.1 71.4 38.0 24.2 28.5 47.1 63.3 66.6 68.9 32.2 28.1 89.2 48.4 81.5 70.0 77.2 55.9 66.1 49.3 71.4 28.0 67.3 73.7 30.5 Macro Avg 56.4 73.9 66.6 83.6 70.4 60.8 45.0 11.1 36.6 57.8 61.2 58.1 80.9 15.1 13.4 75.3 40.7 84.1 71.0 85.9 91.0 66.1 50.7 69.4 26.6 42.3 70.3 41.2 57.4 62.9 67.1 65.8 68.8 65.6 41.2 15.2 32.1 51.9 62.2 62.1 74.4 20.6 18.1 81.6 44.2 82.8 70.5 81.3 69.3 66.1 50.0 70.4 27.3 51.9 72.0 35.1 80.2 82.8 77.4 72.5 71.7 68.5 46.3 43.0 68.8 59.4 73.5 74.6 73.6 47.4 86.4 68.1 90.6 89.3 75.9 91.0 69.5 52.1 85.7 30.0 75.7 81.8 54.2 56.8 70.0 82.6 84.1 90.1 73.7 75.6 52.1 44.4 51.6 73.6 75.8 70.9 84.1 42.4 41.7 83.1 59.2 92.0 85.5 84.5 91.0 70.5 52.1 83.3 35.0 64.1 84.3 60.3 69.9 81.4 83.4 83.3 73.1 73.6 59.2 45.3 46.9 71.1 66.6 72.2 79.1 53.8 44.4 84.7 63.3 91.3 87.4 80.0 91.0 70.0 52.1 84.5 32.3 69.4 83.0 57.1 69.9 Table 4 . Performance of OrgModel-1 trained on different data sources . Label Desc . Precision Recall F1 OrgModel-1 - OrgModel-2 Short Tree Long 69.9 47.0 73.6 73.6 70.0 69.9 49.9 73.6 73.5 46.1 73.4 73.1 Table 5 . Macro average scores for OrgModel-1 and OrgModel-2 with different representations for the label description . create a richer representation of the parent category . 3 ) We used the plain text explanation ( Long ) of the Major Group found in the SIC manual . This is a written explanation of the criteria for an entity to be included in the specified Major Group . Table 5 shows the results . The short representation didn ’ t contain enough information to perform well . Be- tween the tree and long representations , we see similar performance in terms of f1-score . Comparing OrgModel-2 with tree description to OrgModel-1 , OrgModel-2 achieved a 3 % increase in f1-score making it our best performing model on the SIC task . May 2023 Overall , the classifier achieves reasonable performance across the different cate- gories ; however , there are a few categories ( 35 , 50 , 51 , 67 ) where the performance is still weak , possibly due to being particularly broad and difficult to distinguish from other cat- egories . Yet , our model achieves greater than 70 % f1-score on the majority of categories , illustrating the promise of using NLP models to automatically classify organizations . 5 . Conclusion and Future Plans Our study has demonstrated the potential of using Natural Language Processing ( NLP ) techniques for the automatic acquisition of structured food system knowledge from un- structured sources . This includes the classification of entities involved in food activ- ities , and their linkage to social , environmental , and health-related issues . Using text snippets retrieved from Google ’ s search engine as a descriptive basis , our NLP mod- els were able to classify organizations according to environmental issue categories and Standard Industrial Classification ( SIC ) codes . Specifically , we build our models based on transformer-based language models . Our experimental results show that using textual data from Google Search achieved a better performance than using 10-K filings from organizations ’ annual reports , which are provided by the Securities and Exchange Com- mission ( SEC ) database . We also show that by incorporating the description text of the environmental issue categories or SIC codes , our model can achieve better performance for both classification tasks . The promising results underline the applicability of this ap- proach to improve food ontologies as well as other intelligent food systems knowledge resources with little to no human supervision . Our work illustrates how NLP models hold the potential for a wide range of automatic categorization of food system actors and their activities into existing food , environment , and health system ontologies . These on- tologies instantiated by NLP models represent burgeoning yet powerful instruments for quick and efficient population of large food systems knowledge graphs with consistent knowledge representation . Our research opens the door to providing a critical component to the design and creation of reusable cyberinfrastructure components capable of addressing major social- environmental issues like scaling climate actions in food systems . Other components will include the development of additional technological , knowledge , and relational infras- tructures that will build on recent advances in data , modeling , and management . Linking these components together affords the opportunity to make leading edge insights , tools , and practical guidance available to action partners across the food system . Acknowledgments We want to thank the anonymous reviewers for their valuable comments . This research was supported in part by the ICICLE project through NSF award OAC 2112606 and the Canadian Institutes of Health Research ( CIHR ) FRN 177412 . References [ 1 ] Hollander AD , Hoy C , Huber PR , Hyder A , Lange MC , Latham A , Quinn JF , Rig- gle CM , Tomich TP . Toward smart foodsheds : Using stakeholder engagement to May 2023 improve informatics frameworks for regional food systems . Annals of the Ameri- can Association of Geographers . 2020 . DOI : 10.1080/24694452.2019.1662764 [ 2 ] Boulos MN , Yassine A , Shirmohammadi S , Namahoot CS , Bru¨ckner M. Towards an “ Internet of Food ” : food ontologies for the internet of things . Future Internet . 2015 . [ 3 ] Dooley D , Andre´s-Herna´ndez L , Bordea G , Carmody L , Cavalieri D , Chan L , Castellano-Escuder P , Lachat C , Mougin F , Vitali F , Yang C. Obo foundry food ontology interconnectivity . InCEUR Workshop Proceedings . 2021 . [ 4 ] Dooley DM , Griffiths EJ , Gosal GS , Buttigieg PL , Hoehndorf R , Lange MC , Schriml LM , Brinkman FS , Hsiao WW . FoodOn : a harmonized food ontology to increase global food traceability , quality control and data integration . npj Science of Food . 2018 . [ 5 ] D ’ Odorico P , Carr JA , Davis KF , Dell ’ Angelo J , Seekell DA . Food inequality , injustice , and rights . BioScience . 2019 . [ 6 ] Fanzo J , Arabi M , Burlingame B , Haddad L , Kimenju S , Miller G , Nie F , Recine E , Serra-Majem L , Sinha D. Nutrition and food systems . A report by the high level panel of experts on food security and nutrition of the committee on world food security . 2017 . [ 7 ] Fanzo J , Davis C. Global food systems , diets , and nutrition . Springer International Publishing . 2021 . [ 8 ] Devlin J , Chang MW , Lee K , Toutanova K. BERT : Pre-training of deep bidirec- tional transformers for language understanding . In Proceedings of the 2019 Con- ference of the North American Chapter of the Association for Computational Lin- guistics : Human Language Technologies . 2019 . [ 9 ] Kotiaho JS , Halme P. The IPBES assessment report on land degradation and restoration . IPBES Secretariat , UN Campus : Bonn , Germany . 2018 . [ 10 ] Nicole´tis E´ , Termine P. Reducing inequalities for food security and nutrition - HLPE consultation on the report ’ s scope . Global Forum on Food Security and Nutrition ( FSN Forum ) . 2022 . [ 11 ] Radford A , Wu J , Child R , Luan D , Amodei D , Sutskever I . Language models are unsupervised multitask learners . OpenAI blog . 2019 . [ 12 ] Smith B. Ontology . In Blackwell Guide to the Philosophy of Computing and In- formation , edited by Luciano Floridi . Oxford : Blackwell . 2003 . [ 13 ] Tomich TP , Hoy C , Dimock MR , Hollander AD , Huber PR , Hyder A , Lange MC , Riggle CM , Roberts MT , Quinn JF . Why do we need food systems informatics ? Introduction to this special collection on smart and connected regional food sys- tems . Sustainability . 2023 . [ 14 ] FAO , IFAD , UNICEF , WFP and WHO . The state of food security and nutrition in the world 2021 . Transforming food systems for food security , improved nutrition and affordable healthy diets for all . Rome , FAO . 2021 . DOI : 10.4060/cb4474en [ 15 ] United Nations Environment Programme . Emissions gap report 2022 : The closing window - Climate crisis calls for rapid transformation of societies . Nairobi . 2022 .","['classify', 'organization', 'food', 'system', 'ontology', 'use', 'natural', 'language', 'processing', 'hollander', 'patrick', 'r', 'huber', 'c', 'ellen', 'riloff', 'r', 'giorgio', 'university', 'icfood', 'r', 'giorgio', 'ubbiali', 'https', 'research', 'explore', 'use', 'natural', 'language', 'processing', 'nlp', 'method', 'automatically', 'classify', 'entity', 'purpose', 'knowledge', 'graph', 'pop', 'ulation', 'integration', 'food', 'system', 'ontologie', 'create', 'nlp', 'model', 'automatically', 'classify', 'organization', 'respect', 'category', 'associate', 'environmental', 'issue', 'well', 'standard', 'industrial', 'classification', 'sic', 'code', 'use', 'government', 'characterize', 'business', 'activity', 'put', 'nlp', 'model', 'provide', 'text', 'snippet', 'retrieve', 'search', 'engine', 'organization', 'serve', 'textual', 'description', 'organi', 'zation', 'use', 'learn', 'experimental', 'result', 'show', 'nlp', 'model', 'achieve', 'reasonably', 'good', 'performance', 'classification', 'task', 'rely', 'general', 'framework', 'apply', 'many', 'classification', 'problem', 'well', 'believe', 'model', 'represent', 'promising', 'approach', 'automatically', 'harvesting', 'information', 'populate', 'knowledge', 'graph', 'align', 'information', 'exist', 'ontology', 'share', 'category', 'concept', 'keyword', 'food', 'system', 'ontologie', 'classification', 'model', 'natural', 'language', 'processing', 'sic', 'sustainability', 'issue', 'unstructured', 'datum', 'introduction', 'food', 'system', 'include', 'activity', 'relationship', 'involve', 'production', 'transport', 'consumption', 'food', 'thus', 'link', 'wide', 'variety', 'natural', 'human', 'system', 'extensive', 'nature', 'dramatic', 'impact', 'author', 'ellen', 'riloff', 'riloff', 'csutahedu', 'natural', 'human', 'system', 'ipbe', 'unep', 'conversely', 'food', 'system', 'vulnerable', 'change', 'system', 'food', 'activity', 'also', 'show', 'pivotal', 'role', 'foster', 'human', 'social', 'healthrelate', 'issue', 'instance', 'inequality', 'inju', 'tice', 'exist', 'whole', 'global', 'food', 'supply', 'chain', 'well', 'related', 'sector', 'odorico', 'nicole´tis', 'termine', 'date', 'hunger', 'malnutrition', 'micronutrient', 'deficiency', 'cite', 'still', 'largely', 'threaten', 'humanity', 'worldwide', 'fanzo', 'fanzo', 'unicef', 'world', 'become', 'interconnected', 'interdependent', 'ontology', 'become', 'useful', 'way', 'categorize', 'relate', 'information', 'together', 'especially', 'appli', 'cable', 'food', 'system', 'datum', 'access', 'interoperability', 'reusability', 'essential', 'deal', 'interrelated', 'issue', 'arise', 'food', 'system', 'activity', 'ontology', 'formal', 'theory', 'provide', 'commonly', 'accept', 'dictionary', 'term', 'sup', 'port', 'canonical', 'syntax', 'set', 'axiom', 'knowledge', 'domain', 'interest', 'ontology', 'offer', 'common', 'semantic', 'framework', 'domain', 'knowledge', 'hollander', 'thus', 'ontology', 'foster', 'datum', 'access', 'interoperability', 'reusability', 'several', 'disparate', 'resource', 'employ', 'ontology', 'common', 'reference', 'semantic', 'standard', 'hollander', 'day', 'several', 'ontology', 'focus', 'food', 'health', 'related', 'aspect', 'devel', 'ope', 'dooley', 'cite', 'noteworthy', 'one', 'example', 'agrovoc2', 'threedecadelong', 'wellestablished', 'multilingual', 'saurus', 'address', 'food', 'related', 'domain', 'interest', 'belong', 'agriculture', 'organisation', 'fao', 'agronomy', 'tology', 'agro3', 'develop', 'frame', 'cgiar', 'platform', 'big', 'datum', 'agriculture', 'propose', 'vocabulary', 'term', 'cover', 'agronomic', 'manage', 'ment', 'practice', 'implement', 'variable', 'use', 'agronomic', 'experiment', 'dooley', 'compositional', 'dietary', 'nutrition', 'ontology', 'cdno4', 'present', 'term', 'late', 'nutritional', 'attribute', 'crop', 'livestock', 'fishery', 'contribute', 'diet', 'reference', 'precision', 'food', 'commodity', 'laboratory', 'analytic', 'dooley', 'food', 'ontology', 'foodon5', 'provide', 'lexicon', 'basic', 'raw', 'food', 'source', 'ingredient', 'process', 'term', 'packaging', 'cooking', 'preservation', 'upperlevel', 'variety', 'product', 'type', 'scheme', 'food', 'product', 'catego', 'rize', 'foodon', 'stand', 'fundamental', 'ontology', 'address', 'foodrelated', 'aspect', 'dooley', 'foodon', 'aim', 'state', 'lingua', 'franca', 'domain', 'food', 'share', 'reuse', 'foodrelated', 'information', 'human', 'machine', 'availability', 'foodhealth', 'ontology', 'envisage', 'data', 'access', 'interoperability', 'reusability', 'food', 'industry', 'foodrelated', 'tor', 'claim', 'appear', 'reach', 'yet', 'tomich', 'paper', 'present', 'new', 'research', 'use', 'artificial', 'intelligence', 'ai', 'technol', 'ogy', 'automatically', 'categorize', 'organization', 'ultimately', 'purpose', 'link', 'food', 'system', 'ontologie', 'eventual', 'goal', 'automatically', 'populate', 'large', 'knowl', 'edge', 'graph', 'information', 'relate', 'agriculture', 'food', 'system', 'concept', 'graph', 'align', 'wellestablished', 'ontology', 'ensure', 'knowledge', 'represent', 'consistently', 'align', 'resource', 'system', 'rely', 'wwwfaoorgagrovoc', 'cdnoinfo', 'ontological', 'framework', 'populate', 'knowledge', 'graph', 'hand', 'timeconsuming', 'expensive', 'ai', 'technology', 'offer', 'opportunity', 'automatically', 'harvest', 'infor', 'mation', 'much', 'quickly', 'efficiently', 'specifically', 'focus', 'classification', 'task', 'relate', 'organization', 'aim', 'categorize', 'organization', 'base', 'set', 'environmental', 'issue', 'relevant', 'environmental', 'planning', 'food', 'system', 'standard', 'industrial', 'classification', 'sic', 'codes6', 'government', 'assign', 'business', 'categorize', 'nature', 'activity', 'sic', 'code', 'analogous', 'north', 'american', 'industry', 'classification', 'system', 'naic', 'code', 'use', 'design', 'natural', 'lan', 'guage', 'processing', 'nlp', 'model', 'read', 'text', 'associate', 'organization', 'auto', 'matically', 'assign', 'organization', 'category', 'task', 'follow', 'tion', 'describe', 'classification', 'task', 'detail', 'explain', 'collect', 'relevant', 'text', 'nlp', 'model', 'use', 'present', 'nlp', 'technology', 'underlie', 'classification', 'model', 'show', 'experimental', 'result', 'classification', 'task', 'classification', 'task', 'dataset', 'environmental', 'issue', 'classification', 'task', 'dataset', 'large', 'amount', 'information', 'currently', 'available', 'concern', 'state', 'environment', 'world', 'many', 'organization', 'collect', 'analyze', 'datum', 'however', 'remain', 'major', 'gap', 'ability', 'connect', 'datum', 'source', 'make', 'smart', 'typically', 'use', 'different', 'format', 'vocabulary', 'render', 'able', 'use', 'together', 'conservation', 'community', 'lack', 'informatic', 'backbone', 'begin', 'link', 'people', 'datum', 'way', 'enhance', 'capacity', 'inform', 'deci', 'sion', 'make', 'effort', 'conserve', 'enhance', 'earth', 'ecosystem', 'need', 'especially', 'crucial', 'enter', 'unprecedented', 'era', 'rapid', 'environmental', 'change', 'key', 'question', 'environmental', 'planning', 'food', 'system', 'many', 'context', 'people', 'organization', 'project', 'activity', 'refer', 'many', 'kind', 'geography', 'help', 'provide', 'machinereadable', 'answer', 'question', 'hollander', 'develop', 'ontology', 'call', 'ppod', 'people', 'project', 'organization', 'dataset', 'ontol', 'ogy', 'formally', 'describe', 'characteristic', 'relationship', 'class', 'information', 'member', 'team', 'instantiate', 'ppod', 'ontology', 'information', 'con', 'cerne', 'conservation', 'work', 'landscape', 'knowledge', 'graph', 'contain', 'organization', 'identify', 'collect', 'manner', 'array', 'online', 'search', 'use', 'term', 'conservation', 'diversity', 'graze', 'water', 'supply', 'organization', 'associate', 'attribute', 'describe', 'structure', 'mission', 'hasorgtype', 'ity', 'issue', 'issue', 'attribute', 'describe', 'potential', 'environmental', 'issue', 'associate', 'organization', 'predefine', 'highlevel', 'environmental', 'issue', 'call', 'integrate', 'issue', 'finegrained', 'environmental', 'issue', 'call', 'component', 'issue', 'ontology', 'provide', 'detailed', 'textual', 'description', 'issue', 'label', 'wwwoshagovdatasicmanual', 'githubcomppodschema', 'category', 'water', 'physical', 'infrastructure', 'waste', 'pollution', 'biodiversity', 'land', 'soil', 'food', 'production', 'institution', 'governance', 'protect', 'area', 'sociocultural', 'system', 'public', 'health', 'disaster', 'common', 'pool', 'resource', 'air', 'climate', 'technology', 'organization', 'percentage', 'example', 'organization', 'american', 'river', 'rebuild', 'heal', 'ocean', 'land', 'trust', 'agricultural', 'research', 'merce', 'public', 'health', 'tahoe', 'fire', 'fuel', 'team', 'sustainable', 'conservation', 'irvine', 'global', 'warming', 'group', 'cdfw', 'datum', 'technology', 'division', 'table', 'example', 'environmental', 'issue', 'many', 'organization', 'issue', 'associate', 'last', 'column', 'show', 'example', 'organization', 'label', 'issue', 'category', 'relation', 'integrate', 'issue', 'component', 'issue', 'example', 'component', 'issue', 'air', 'pollution', 'air', 'quality', 'greenhouse', 'gas', 'mitigation', 'greenhouse', 'gas', 'emission', 'child', 'integrate', 'issue', 'air', 'climate', 'describe', 'emission', 'ozone', 'layer', 'depletion', 'air', 'quality', 'climate', 'change', 'influence', 'extreme', 'weather', 'event', 'shift', 'grow', 'zone', 'key', 'crop', 'climate', 'change', 'expert', 'opinion', 'use', 'associate', 'organization', 'issue', 'define', 'task', 'base', 'ppod', 'ontology', 'call', 'environmental', 'issue', 'classi', 'fication', 'give', 'organization', 'task', 'require', 'model', 'assign', 'tiple', 'environmental', 'issue', 'category', 'label', 'organization', 'describe', 'activity', 'andor', 'mission', 'model', 'need', 'train', 'reasonable', 'number', 'example', 'category', 'start', 'common', 'category', 'exist', 'ppod', 'datum', 'map', 'component', 'issue', 'parent', 'integrate', 'issue', 'sort', 'issue', 'base', 'number', 'organi', 'zation', 'associate', 'issue', 'finally', 'select', 'common', 'integrate', 'issue', 'set', 'category', 'label', 'result', 'dataset', 'contain', 'organization', 'associate', 'environmental', 'issue', 'category', 'air', 'climate', 'biodiversity', 'common', 'pool', 'resource', 'disaster', 'food', 'production', 'governance', 'institution', 'land', 'soil', 'physical', 'infrastructure', 'protect', 'area', 'public', 'health', 'sociocultural', 'system', 'technology', 'waste', 'pollution', 'water', 'table', 'show', 'category', 'number', 'percentage', 'organization', 'category', 'associate', 'example', 'organization', 'category', 'standard', 'industrial', 'classification', 'sic', 'task', 'dataset', 'standard', 'industrial', 'classification', 'sic', 'code', 'create', 'government', 'categorize', 'business', 'accord', 'industry', 'serve', 'operate', 'lieve', 'incorporate', 'industry', 'classification', 'sic', 'code', 'remain', 'use', 'replace', 'north', 'american', 'industry', 'classification', 'system', 'food', 'system', 'ontology', 'valuable', 'understand', 'nature', 'organization', 'activi', 'tie', 'economic', 'logistical', 'relationship', 'organization', 'supply', 'chain', 'relationship', 'sic', 'code', 'many', 'organization', 'look', 'government', 'business', 'database', 'ai', 'model', 'automatically', 'assign', 'sic', 'code', 'organization', 'use', 'categorize', 'newly', 'form', 'organization', 'quickly', 'wait', 'official', 'database', 'update', 'categorize', 'organization', 'respect', 'standardized', 'industry', 'code', 'maintain', 'currency', 'knowledge', 'graph', 'automatically', 'reclassify', 'organization', 'regular', 'basis', 'say', 'annually', 'reflect', 'change', 'organization', 'make', 'activity', 'expansion', 'business', 'activity', 'retraction', 'end', 'define', 'second', 'task', 'call', 'sic', 'code', 'classification', 'require', 'nlp', 'model', 'assign', 'multiple', 'sic', 'code', 'category', 'label', 'organization', 'sic', 'code', 'digit', 'long', 'hierarchical', 'digit', 'represent', 'sion', 'major', 'group', 'industry', 'group', 'industry', 'entity', 'respectively', 'example', 'company', 'code', 'belong', 'soybean', 'industry', 'cash', 'grain', 'industry', 'group', 'agricultural', 'production', 'crop', 'major', 'group', 'tem', 'allow', 'study', 'company', 'different', 'level', 'granularity', 'simply', 'group', 'company', 'accord', 'first', 'digit', 'sic', 'code', 'train', 'nlp', 'model', 'task', 'need', 'example', 'organization', 'asso', 'ciate', 'sic', 'code', 'conveniently', 'security', 'main', 'tain', 'publicly', 'accessible', 'database', 'company', 'call', 'edgar', 'database', 'contain', 'sic', 'code', 'company', 'well', 'filing', 'report', 'particular', 'terest', '10k', '20f', 'form', 'provide', 'company', 'annual', 'report', 'form', 'detail', 'range', 'information', 'relate', 'company', 'operation', 'past', 'year', 'include', 'financial', 'legal', 'risk', 'factor', 'information', 'allow', 'stakeholder', 'assess', 'state', 'business', 'download', 'report', 'well', 'source', 'textual', 'information', 'nlp', 'model', 'potentially', 'use', 'specifically', 'collect', 'natural', 'language', 'text', 'item', 'business', 'section', 'company', 'recent', '10k', 'filing', 'well', 'company', 'official', 'sic', 'code', 'edgar', 'database', 'sic', 'code', 'company', 'fact', 'principle', 'company', 'multiple', 'code', 'associate', 'collect', 'company', 'information', 'start', 'list', 'company', 'database', 'central', 'index', 'key', 'number', 'provide', 'query', 'number', 'edgar', 'collect', 'name', 'sic', 'sic', 'description', '10k', 'form', 'focus', 'research', 'organization', 'field', 'observe', 'distribution', 'datum', 'highly', 'skewed', 'many', 'sic', 'code', 'contain', 'instance', 'partly', 'due', 'fact', 'organi', 'zation', 'force', 'pick', 'sicnaic', 'code', 'incorporate', 'organization', 'business', 'model', 'change', 'involve', 'multiple', 'line', 'business', 'reflect', 'multiple', 'sicnaic', 'code', 'experiment', 'create', 'balanced', 'subset', 'datum', 'amount', 'information', 'category', 'fairly', 'compare', 'perfor', 'mance', 'nlp', 'model', 'category', 'decide', 'focus', 'first', 'digit', 'sic', 'code', 'category', 'label', 'help', 'minimize', 'datum', 'sparsity', 'many', 'long', 'code', 'relatively', 'organization', 'associate', 'provide', 'useful', 'highlevel', 'view', 'company', 'general', 'type', 'sic', 'description', 'metal', 'mining', 'oil', 'gas', 'extraction', 'food', 'kindre', 'product', 'print', 'publishing', 'ally', 'industry', 'chemical', 'ally', 'product', 'fabricate', 'metal', 'product', 'industrial', 'commercial', 'machinery', 'electronic', 'transportation', 'equipment', 'measure', 'photographic', 'medical', 'communication', 'electric', 'gas', 'sanitary', 'service', 'wholesale', 'trade', 'durable', 'good', 'wholesale', 'trade', 'nondurable', 'good', 'sic', 'description', 'eat', 'drinking', 'place', 'miscellaneous', 'retail', 'depository', 'institution', 'nondepository', 'credit', 'institution', 'security', 'insurance', 'carrier', 'real', 'estate', 'holding', 'investment', 'office', 'hotel', 'room', 'house', 'camp', 'business', 'service', 'amusement', 'recreation', 'service', 'health', 'service', 'engineering', 'accounting', 'research', 'table', 'description', 'common', 'sic', 'code', 'ness', 'operation', 'create', 'dataset', 'experiment', 'select', 'sic', 'code', 'least', 'associated', 'company', 'result', 'set', 'sic', 'code', 'show', 'table', 'finally', 'randomly', 'sample', 'company', 'code', 'create', 'balanced', 'dataset', 'contain', 'organization', 'sic', 'code', 'organization', 'information', 'collection', 'goal', 'create', 'classification', 'model', 'give', 'text', 'organization', 'input', 'produce', 'category', 'label', 'organization', 'output', 'key', 'question', 'find', 'text', 'describe', 'organization', 'initially', 'consider', 'use', 'official', 'website', 'organization', 'input', 'text', 'website', 'often', 'contain', 'formation', 'organization', 'initiative', 'policy', 'practice', 'however', 'organization', 'official', 'website', 'many', 'website', 'allow', 'auto', 'mate', 'crawl', 'case', 'use', 'web', 'scraper', 'automatically', 'extract', 'text', 'website', 'organization', 'environmental', 'issue', 'dataset', 'find', 'half', 'organization', 'website', 'crawl', 'alternative', 'decide', 'use', 'search', 'engine', 'retrieve', 'textual', 'information', 'organization', 'organization', 'give', 'organization', 'name', 'search', 'query', 'search', 'api', 'extract', 'first', 'return', 'result', 'search', 'result', 'provide', 'several', 'type', 'information', 'organic', 'result', 'knowledge', 'graph', 'local', 'result', 'relate', 'question', 'organic', 'result', 'mically', 'calculate', 'query', 'result', 'oppose', 'advertisement', 'user', 'typically', 'read', 'include', 'title', 'link', 'text', 'snippet', 'retrieve', 'web', 'page', 'use', 'text', 'snippet', 'small', 'block', 'text', 'appear', 'link', 'website', 'usually', 'character', 'length', 'typically', 'provide', 'user', 'general', 'description', 'content', 'website', 'organization', 'concatenate', 'text', 'snippet', 'top', 'retrieve', 'website', 'single', 'pseudo', 'document', 'serve', 'text', 'datum', 'use', 'organization', 'figure', 'show', 'pipeline', 'create', 'organization', 'use', 'search', 'api', 'snippet', 'cattle', 'bar', 'beef', 'idyllic', 'life', 'graze', 'multigenerational', 'family', 'ranch', 'help', 'support', 'wildfire', 'mitigation', 'bar', 'angus', 'ranch', 'raise', 'premium', 'quality', 'angus', 'bull', 'sale', 'well', 'home', 'grow', 'angus', 'beef', 'sale', 'angus', 'cattle', 'prove', 'herd', 'genetic', 'meat', 'bar', 'beef', 'family', 'ranch', 'dedicate', 'provide', 'bestquality', 'well', 'taste', 'grassfe', 'pasture', 'raise', 'beef', 'grassfe', 'beef', 'pasture', 'raise', 'holistically', 'farm', 'carry', 'bar', 'beef', 'top', 'n', 'search', 'result', 'collect', 'snippet', 'figure', 'download', 'snippet', 'organization', 'textual', 'representation', 'show', 'section', 'use', 'text', 'snippet', 'retrieve', 'web', 'page', 'produce', 'reasonably', 'good', 'classification', 'model', 'observe', 'thing', 'explain', 'search', 'produce', 'useful', 'textual', 'information', 'organization', 'first', 'organization', 'official', 'website', 'typically', 'find', 'rank', 'top', 'hit', 'many', 'case', 'text', 'snippet', 'return', 'organization', 'include', 'text', 'organization', 'website', 'second', 'website', 'retrieve', 'organization', 'usually', 'discuss', 'organization', 'b', 'cuss', 'similar', 'organization', 'organization', 'similar', 'name', 'consequently', 'text', 'snippet', 'website', 'often', 'contain', 'relevant', 'information', 'use', 'ful', 'infer', 'organization', 'activity', 'mission', 'use', 'text', 'snippet', 'retrieve', 'website', 'pseudodocument', 'contain', 'information', 'organiza', 'tion', 'similar', 'organization', 'originate', 'multiple', 'source', 'collectively', 'paint', 'good', 'picture', 'nature', 'organization', 'method', 'background', 'pretraine', 'language', 'model', 'pretraine', 'language', 'model', 'gpt2', 'achieve', 'great', 'suc', 'cess', 'field', 'natural', 'language', 'processing', 'ability', 'absorb', 'lot', 'information', 'language', 'massive', 'amount', 'text', 'human', 'pervision', 'large', 'language', 'model', 'neural', 'network', 'deep', 'learn', 'architec', 'ture', 'additionally', 'train', 'specific', 'application', 'task', 'use', 'method', 'call', 'finetune', 'model', 'provide', 'humanlabeled', 'datum', 'application', 'task', 'finetune', 'model', 'combine', 'general', 'knowledge', 'language', 'previously', 'absorb', 'pretraine', 'new', 'information', 'datum', 'finetune', 'model', 'perform', 'well', 'many', 'application', 'task', 'even', 'provide', 'small', 'amount', 'taskspecific', 'datum', 'work', 'use', 'wellknown', 'pretraine', 'language', 'model', 'call', 'bert', 'pretraine', 'english', 'word', 'air', 'climate', 'biodiversity', 'water', 'bert', 'cls', 'tok', 'tok', 'n', 'sep', 'tok', 'tok', 'organization', 'text', 'label', 'description', 'figure', 'orgmodel2', 'model', 'architecture', 'illustration', 'example', 'organization', 'bar', 'beef', 'book', 'text', 'collection', 'use', 'base', 'variant', 'bert', 'layer', 'transformer', 'block', 'hide', 'unit', 'hide', 'size', 'selfattention', 'head', 'bertbase', 'model', 'consist', 'approximately', 'parameter', 'learn', 'weight', 'offer', 'good', 'balance', 'computational', 'efficiency', 'performance', 'wide', 'range', 'natural', 'language', 'processing', 'task', 'classification', 'model', 'create', 'different', 'design', 'classification', 'model', 'basic', 'finetuning', 'design', 'slightly', 'complex', 'design', 'first', 'model', 'orgmodel1', 'take', 'text', 'associate', 'organization', 'input', 'finetune', 'language', 'model', 'gold', 'training', 'datum', 'label', 'example', 'task', 'specifically', 'follow', 'common', 'practice', 'use', 'embed', 'vector', 'cls', 'token', 'classification', 'task', 'stack', 'linear', 'classification', 'layer', 'top', 'last', 'layer', 'produce', 'ndimensional', 'output', 'vector', 'n', 'number', 'category', 'task', 'sic', 'code', 'classification', 'task', 'use', 'crossentropy', 'loss', 'train', 'environmental', 'issue', 'classification', 'task', 'slightly', 'different', 'multilabel', 'problem', 'organization', 'associate', 'issue', 'far', 'apply', 'sigmoid', 'function', 'transform', 'dimension', 'value', 'number', 'number', 'system', 'predict', 'mean', 'organization', 'belong', 'environmental', 'issue', 'category', 'otherwise', 'use', 'binary', 'crossentropy', 'loss', 'train', 'second', 'model', 'orgmodel2', 'take', 'advantage', 'additional', 'source', 'mation', 'model', 'provide', 'expertwritten', 'description', 'category', 'put', 'text', 'associate', 'organization', 'provide', 'model', 'definition', 'category', 'model', 'potentially', 'produce', 'rich', 'semantic', 'representation', 'category', 'help', 'find', 'good', 'match', 'organization', 'specifically', 'suppose', 'organization', 'text', 'denote', 'category', 'de', 'scription', 'denote', 'j', 'create', 'n', 'sequence', 'pair', '⟨oi', 'dn⟩', 'ask', 'system', 'assign', 'number', 'pair', '⟨oi', 'j⟩', 'represent', 'strength', 'association', 'organization', 'oi', 'category', 'j', 'orgmodel2', 'model', 'take', 'text', 'organization', 'n', 'label', 'description', 'pre', 'dict', 'strength', 'value', '⟨oi', 'j⟩', 'pair', 'figure', 'depict', 'full', 'architecture', 'orgmodel2', 'model', 'evaluation', 'evaluation', 'metric', 'evaluate', 'ability', 'nlp', 'model', 'classify', 'organization', 'report', 'evaluation', 'metric', 'commonly', 'use', 'research', 'community', 'precision', 'recall', 'f1score', 'intuitively', 'precision', 'capture', 'accuracy', 'predict', 'cat', 'egory', 'capture', 'coverage', 'recognize', 'instance', 'category', 'metric', 'define', 'respect', 'specific', 'category', 'c', 'compute', 'precision', 'tp', 'recall', 'tp', 'recall−1', 'tp', 'true', 'positive', 'number', 'true', 'instance', 'c', 'correctly', 'identify', 'model', 'false', 'positive', 'number', 'instance', 'model', 'predict', 'c', 'belong', 'fn', 'false', 'negative', 'number', 'true', 'instance', 'c', 'identify', 'model', 'f1score', 'harmonic', 'mean', 'precision', 'recall', 'average', 'precision', 'recall', 'also', 'reflect', 'balanced', 'balanced', 'well', 'addition', 'report', 'average', 'score', 'evaluation', 'meet', 'provide', 'overall', 'view', 'performance', 'different', 'category', 'micro', 'average', 'score', 'aggregate', 'result', 'instance', 'class', 'calculate', 'metric', 'give', 'weight', 'frequent', 'class', 'instance', 'contrast', 'macro', 'average', 'score', 'calculate', 'metric', 'class', 'separately', 'average', 'result', 'give', 'equal', 'weight', 'class', 'example', 'macro', 'average', 'precision', 'define', 'precision', 'fpi', 'macro', 'precision', '∑n', 'tpi', 'fpi', 'dicate', 'class', 'n', 'total', 'number', 'class', 'label', 'result', 'analysis', 'environmental', 'issue', 'classification', 'result', 'experiment', 'organization', 'environmental', 'issue', 'dataset', 'split', 'training', 'testing', 'table', 'show', 'experimental', 'result', 'environmental', 'issue', 'category', 'orgmodel1', 'orgmodel2', 'orgmodel1', 'system', 'achieve', 'average', 'f1score', 'macroaverage', 'f1score', 'add', 'environmental', 'sue', 'category', 'description', 'orgmodel2', 'achieve', 'well', 'performance', 'microaverage', 'f1', 'macroaverage', 'overall', 'see', 'good', 'precision', 'category', 'microaverage', 'low', 'microaverage', 'take', 'close', 'look', 'individual', 'category', 'see', 'orgmodel2', 'achieve', 'good', 'performance', 'f1score', 'biodiversity', 'governance', 'physical', 'orgmodel1', 'orgmodel2', 'precision', 'recall', 'f1score', 'precision', 'recall', 'air', 'climate', 'biodiversity', 'common', 'pool', 'resource', 'disaster', 'food', 'production', 'governance', 'institution', 'land', 'soil', 'physical', 'infrastructure', 'protect', 'area', 'public', 'health', 'sociocultural', 'system', 'technology', 'waste', 'pollution', 'macro', 'average', 'table', 'environmental', 'issue', 'classification', 'orgmodel1', 'orgmodel2', 'performance', 'category', 'infrastructure', 'technology', 'waste', 'pollution', 'water', 'model', 'struggle', 'f1', 'score', 'common', 'pool', 'resource', 'disaster', 'public', 'health', 'datum', 'sparsity', 'investigate', 'number', 'instance', 'impact', 'model', 'performance', 'focus', 'least', 'common', 'category', 'associate', 'percent', 'organization', 'dataset', 'public', 'health', 'disaster', 'common', 'pool', 'resource', 'air', 'climate', 'technology', 'common', 'cate', 'gorie', 'associate', 'percent', 'organization', 'water', 'phy', 'ical', 'infrastructure', 'waste', 'pollution', 'biodiversity', 'land', 'soil', 'figure', 'show', 'plot', 'f1score', 'orgmodel1', 'orgmodel2', 'compare', 'rare', 'category', 'circle', 'common', 'category', 'square', 'see', 'latter', 'perform', 'well', 'orgmodel1', 'orgmodel2', 'average', 'f1score', 'common', 'cate', 'gorie', 'average', 'f1score', 'rare', 'category', 'orgmodel2', 'average', 'f1score', 'common', 'category', 'average', 'f1score', 'rare', 'category', 'huge', 'gap', 'indicate', 'performance', 'rare', 'category', 'likely', 'improve', 'get', 'humanannotated', 'datum', 'componentintegrate', 'mapping', 'dataset', 'construction', 'environmental', 'issue', 'classification', 'map', 'component', 'issue', 'integrate', 'issue', 'focus', 'ite', 'set', 'common', 'label', 'however', 'manytomany', 'mapping', 'create', 'prob', 'lem', 'example', 'component', 'issue', 'land', 'use', 'associate', 'integrate', 'issue', 'food', 'production', 'yet', 'organization', 'tag', 'land', 'use', 'tag', 'food', 'production', 'eg', 'organization', 'air', 'force', 'civil', 'engineer', 'center', 'snowland', 'network', 'actually', 'produce', 'food', 'future', 'work', 'consider', 'well', 'alignment', 'component', 'integrate', 'issue', 'refine', 'label', 'set', 'include', 'component', 'issue', 'directly', 'e', 'r', 'c', 'orgmodel1', 'orgmodel2', 'diversity', 'p', 'ysical', 'infrastructure', 'l', 'oil', 'blic', 'isaster', 'n', 'ol', 'r', 'eso', 'urce', 'cli', 'eat', 'aste', 'p', 'ater', 'figure', 'f1score', 'category', 'circle', 'represent', 'low', 'frequency', 'environmental', 'issue', 'square', 'represent', 'high', 'frequency', 'issue', 'sic', 'code', 'classification', 'result', 'sic', 'code', 'classification', 'task', 'split', 'organization', 'train', 'develop', 'ment', 'test', 'set', 'contain', 'example', 'respectively', '10k', 'filing', 'first', 'experiment', 'explore', 'use', 'dif', 'ferent', 'type', 'textual', 'datum', 'sic', 'code', 'classification', 'task', 'pseudodocument', 'create', 'retrieve', 'text', 'snippet', '10k', 'file', 'form', 'database', 'train', 'different', 'orgmodel1', 'system', 'use', 'textual', 'datum', 'pseudodocument', 'use', '10k', 'filing', 'report', 'nlp', 'model', 'iden', 'tical', 'textual', 'description', 'use', 'training', 'process', 'table', 'show', 'model', 'use', 'google', 'snippet', 'text', 'source', 'organization', 'strongly', 'outperform', 'model', 'use', 'text', '10k', 'forms9', 'fact', 'model', 'train', 'snippet', 'achieve', 'high', 'f1score', 'single', 'category', 'security', 'commodity', 'broker', 'dealer', 'exchange', 'service', 'category', 'small', 'difference', 'performance', 'model', 'many', 'case', 'model', 'train', 'snippet', 'outperform', 'model', 'train', '10k', 'form', 'significant', 'margin', 'lead', 'increase', 'macroaveraged', 'f1score', 'approach', 'also', 'generalizable', 'many', 'organization', 'filing', 'orgmodel1', 'orgmodel2', 'train', 'orgmodel1', 'orgmodel2', 'text', 'sic', 'code', 'classification', 'task', 'well', 'orgmodel2', 'also', 'need', 'description', 'category', 'label', 'experiment', 'different', 'type', 'information', 'short', 'representation', 'short', 'simply', 'name', 'digit', 'major', 'group', 'second', 'representation', 'tree', 'exploit', 'hierarchical', 'nature', 'sic', 'code', 'concatenate', 'together', 'name', 'name', 'code', 'subtree', 'motivation', 'include', 'specific', 'subcategory', 'information', '9it', 'worth', 'note', 'organization', '10k', 'filing', 'extract', 'empty', '10k', 'file', 'class', 'recall', 'f1score', 'recall', 'precision', 'macro', 'avg', 'table', 'performance', 'orgmodel1', 'train', 'different', 'data', 'source', 'label', 'desc', 'precision', 'recall', 'orgmodel1', 'orgmodel2', 'short', 'tree', 'long', 'table', 'macro', 'average', 'score', 'orgmodel1', 'orgmodel2', 'different', 'representation', 'label', 'description', 'create', 'rich', 'representation', 'parent', 'category', 'use', 'plain', 'text', 'explanation', 'long', 'major', 'group', 'find', 'sic', 'manual', 'write', 'explanation', 'criterion', 'entity', 'include', 'specified', 'major', 'group', 'table', 'show', 'result', 'short', 'representation', 'contain', 'enough', 'information', 'perform', 'well', 'tween', 'tree', 'long', 'representation', 'see', 'similar', 'performance', 'term', 'f1score', 'compare', 'orgmodel2', 'tree', 'description', 'orgmodel1', 'orgmodel2', 'achieve', 'increase', 'f1score', 'make', 'well', 'perform', 'model', 'sic', 'task', 'overall', 'classifier', 'achieve', 'reasonable', 'performance', 'different', 'cate', 'gorie', 'however', 'category', 'performance', 'still', 'weak', 'possibly', 'particularly', 'broad', 'difficult', 'distinguish', 'cat', 'egorie', 'yet', 'model', 'achieve', 'great', 'f1score', 'majority', 'category', 'illustrate', 'promise', 'use', 'nlp', 'model', 'automatically', 'classify', 'organization', 'conclusion', 'future', 'plan', 'study', 'demonstrate', 'potential', 'use', 'natural', 'language', 'processing', 'nlp', 'technique', 'automatic', 'acquisition', 'structured', 'food', 'system', 'knowledge', 'source', 'include', 'classification', 'entity', 'involve', 'food', 'activ', 'itie', 'linkage', 'social', 'environmental', 'healthrelate', 'issue', 'use', 'text', 'snippet', 'retrieve', 'search', 'engine', 'descriptive', 'basis', 'el', 'able', 'classify', 'organization', 'accord', 'environmental', 'issue', 'category', 'standard', 'industrial', 'classification', 'sic', 'code', 'specifically', 'build', 'model', 'base', 'transformerbase', 'language', 'model', 'experimental', 'result', 'show', 'use', 'textual', 'datum', 'search', 'achieve', 'well', 'performance', 'use', '10k', 'filing', 'organization', 'annual', 'report', 'provide', 'security', 'exchange', 'com', 'mission', 'database', 'also', 'show', 'incorporate', 'description', 'text', 'environmental', 'issue', 'category', 'sic', 'code', 'model', 'achieve', 'well', 'performance', 'classification', 'task', 'promise', 'result', 'underline', 'applicability', 'proach', 'improve', 'food', 'ontology', 'well', 'intelligent', 'food', 'system', 'knowledge', 'resource', 'little', 'human', 'supervision', 'work', 'illustrate', 'model', 'hold', 'potential', 'wide', 'range', 'automatic', 'categorization', 'food', 'system', 'actor', 'activity', 'exist', 'food', 'environment', 'health', 'system', 'ontologie', 'tologie', 'instantiate', 'nlp', 'model', 'represent', 'burgeon', 'yet', 'powerful', 'instrument', 'quick', 'efficient', 'population', 'large', 'food', 'system', 'knowledge', 'graph', 'consistent', 'knowledge', 'representation', 'research', 'open', 'door', 'provide', 'critical', 'component', 'design', 'creation', 'reusable', 'cyberinfrastructure', 'component', 'capable', 'address', 'major', 'social', 'environmental', 'issue', 'scale', 'climate', 'action', 'food', 'system', 'component', 'include', 'development', 'additional', 'technological', 'knowledge', 'relational', 'infra', 'tructure', 'build', 'recent', 'advance', 'datum', 'modeling', 'management', 'link', 'component', 'together', 'afford', 'opportunity', 'make', 'lead', 'edge', 'insight', 'tool', 'practical', 'guidance', 'available', 'action', 'partner', 'food', 'system', 'acknowledgment', 'want', 'thank', 'anonymous', 'reviewer', 'valuable', 'comment', 'research', 'support', 'part', 'icicle', 'project', 'award', 'oac', 'canadian', 'institute', 'health', 'research', 'cihr', 'reference', 'hollander', 'ad', 'huber', 'hyder', 'lange', 'quinn', 'jf', 'rig', 'tomich', 'tp', 'smart', 'foodshed', 'use', 'stakeholder', 'engagement', 'improve', 'informatic', 'framework', 'regional', 'food', 'system', 'annal', 'geographer', 'doi', 'yassine', 'shirmohammadi', 'namahoot', 'bru¨ckner', 'internet', 'food', 'food', 'ontology', 'internet', 'thing', 'future', 'internet', 'dooley', 'bordea', 'carmody', 'l', 'castellanoescuder', 'c', 'mougin', 'food', 'ontology', 'interconnectivity', 'inceur', 'workshop', 'proceeding', 'dooley', 'griffith', 'gosal', 'gs', 'r', 'lange', 'schriml', 'harmonize', 'food', 'ontology', 'increase', 'global', 'food', 'traceability', 'quality', 'control', 'data', 'integration', 'science', 'food', 'odorico', 'seekell', 'food', 'inequality', 'injustice', 'right', 'bioscience', 'fanzo', 'j', 'arabi', 'recine', 'e', 'l', 'sinha', 'nutrition', 'food', 'system', 'report', 'high', 'level', 'panel', 'expert', 'food', 'security', 'nutrition', 'committee', 'world', 'food', 'security', 'fanzo', 'system', 'diet', 'nutrition', 'springer', 'international', 'publishing', 'pretraining', 'deep', 'bidirec', 'tional', 'transformer', 'language', 'understanding', 'proceeding', 'con', 'ference', 'north', 'american', 'chapter', 'association', 'human', 'language', 'technology', 'kotiaho', 'halme', 'p', 'ipbe', 'assessment', 'report', 'land', 'degradation', 'restoration', 'ipbe', 'nicole´tis', 'e', '´', 'termine', 'p', 'reduce', 'inequality', 'food', 'security', 'nutrition', 'hlpe', 'consultation', 'report', 'scope', 'global', 'forum', 'food', 'security', 'nutrition', 'fsn', 'radford', 'child', 'r', 'sutskever', 'language', 'model', 'unsupervise', 'learner', 'blog', 'ontology', 'blackwell', 'guide', 'philosophy', 'compute', 'formation', 'edit', 'blackwell', 'tomich', 'tp', 'c', 'dimock', 'hollander', 'ad', 'huber', 'hyder', 'lange', 'mc', 'riggle', 'need', 'food', 'system', 'informatic', 'introduction', 'special', 'collection', 'smart', 'connected', 'regional', 'food', 'sys', 'tem', 'sustainability', 'fao', 'ifad', 'unicef', 'wfp', 'state', 'food', 'security', 'nutrition', 'world', 'transform', 'food', 'system', 'food', 'security', 'improve', 'nutrition', 'affordable', 'healthy', 'diet', 'fao', 'doi', 'emission', 'gap', 'report', 'closing', 'window', 'climate', 'crisis', 'call', 'rapid', 'transformation', 'society']",
"Classifying Organizations for Food System Ontologies using Natural
  Language Processing","[{'href': 'http://arxiv.org/abs/2309.10880v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2309.10880v1', 'rel': 'related', 'type': 'application/pdf'}]",2023-09-19 19:07:48,"Why We Don’t Have AGI Yet 

Peter	Voss,	AIGO.ai,	Austin	TX,	USA,	peter@aigo.ai	
Mlađan	Jovanović,	Singidunum	University,	Belgrade	Serbia,	mjovanovic@singidunum.ac.rs	

Abstract	–	The	original	vision	of	AI	was	re-articulated	in	2002	via	the	term	‘Artificial	General	Intelligence’	
or	AGI.	This	vision	is	to	build	‘Thinking	Machines’	–	computer	systems	that	can	learn,	reason,	and	solve	
problems	similar	to	the	way	humans	do.	This	is	in	stark	contrast	to	the	‘Narrow	AI’	approach	practiced	
by	almost	everyone	in	the	field	over	the	many	decades.	While	several	large-scale	efforts	have	nominally	
been	working	on	AGI	(most	notably	DeepMind),	the	field	of	pure	focused	AGI	development	has	not	been	
well	 funded	 or	 promoted.	 This	 is	 surprising	 given	 the	 fantastic	 value	 that	 true	 AGI	 can	 bestow	 on	
humanity.	 In	 addition	 to	 the	 dearth	 of	 effort	 in	 this	 field,	 there	 are	 also	 several	 theoretical	 and	
methodical	 missteps	 that	 are	 hampering	 progress.	 We	 highlight	 why	 purely	 statistical	 approaches	 are	
unlikely	 to	 lead	 to	 AGI,	 and	 identify	 several	 crucial	 cognitive	 abilities	 required	 to	 achieve	 human-like	
adaptability	and	autonomous	learning.	We	conclude	with	a	survey	of	socio-technical	factors	that	have	
undoubtedly	slowed	progress	towards	AGI.	

Keywords:	AGI,	Cognitive	AI,	Adaptive	AI,	Human-Level	AI,	Cognitive	Architecture,	Third	Wave	of	AI.	

A Brief History of ‘AGI’ 
Originally,	the	term	‘Artificial	Intelligence’,	coined	by	John	McCarthy	in	1955	[1],	referred	to	machines	or	
computer	 programs	 that	 can	 think,	 learn,	 and	 reason	 the	 way	 humans	 do	 –	 that	 have	 the	 general	
cognitive	ability	that	we	possess.	At	the	time	he	expected	this	to	be	achieved	within	a	few	months!		

As	 it	 turned	 out	 this	 was	 a	 lot	 harder,	 so	 over	 the	 years	 and	 decades	 the	 field	 of	 ‘AI’	 morphed	 into	
‘Narrow	AI’	–	solving	just	one	(type	of)	problem	at	a	time.	An	often	overlooked	but	crucial	detrimental	
consequence	of	this	shift	was	that	focus	was	now	on	‘achieving	a	particular	goal	via	the	ingenuity	of	the	
programmer	or	data	scientist’	rather	than	‘how	do	we	build	a	system	that	has	general	intelligence’.		

Therefore,	AI’s	focus	had	shifted	from	having	internal	intelligence	to	utilizing	external	intelligence	(the	
programmer’s	 intelligence)	 to	 solve	 particular	 problems.	 This	 preoccupation	 with	 achieving	 specific	
narrow	goals	also	had	the	undesirable	side-effect	of	ignoring	the	importance	of	adaptation	and	agency.	
Intelligent	 entities	 must	 be	 able	 to	 autonomously	 adapt	 to	 changing	 circumstances	 and	 goals.	 The	
entities	are	agents	that	proactively	initiate	actions	in	their	surroundings	[2].	

By	2001	a	number	of	AI	researchers	had	independently	concluded	that	the	time	was	ripe	to	return	to	
the	original	vision	of	‘AI’	and	decided	to	join	forces	to	write	a	book	on	this	subject	[3].	In	2002,	three	of	
the	authors	(Ben	Goertzel,	Shane	Legg,	and	Peter	Voss)	coined	the	term	‘Artificial	General	Intelligence’	
for	the	book	title.	

AGI	 refers	 	 to	 creating	 (semi-)autonomous,	 adaptive	 computer	 systems	 with	 the	 general	 cognitive	
capabilities	 typical	 for	 	 humans.	 The	 ability	 to	 support	 abstraction,	 analogy,	 planning	 and	 problem-
solving.	

1	

	
	
	
	
Figure	1.	Comparison	of	external	and	internal	intelligence.	

The Value of AGI 
Imagine	a	computer	system	that	has	the	general	cognitive	prowess	and	knowledge	of	a	medical	cancer	
expert.	Now	imagine	a	million	copies	of	this	chipping	away	at	this	scourge.	Also	consider	the	following	
advantages	of	artificial	researchers	and	workers:	

!  Massively	lower	cost	than	humans	performing	cognitive	tasks.	
!  Much	better	at	communicating	and	sharing	knowledge	with	each	other	–	no	egos	to	get	in	the	way!	
!  Toiling	away	365/24/7,	much	faster	and	with	better	concentration.	
!  Not	having	the	various	‘distractions’	that	we	have	–	e.g.	family,	vacation,	hobbies,	etc.	
! 
!  A	much-improved	ability	to	think	logically,	and	to	perform	complex	planning	and	reasoning.	

‘Photographic	memory’,	and	instant	access	to	all	online	information.	

Now	 apply	 these	 powers	 to	 the	 many	 challenges	 facing	 humanity:	 energy,	 environment,	 poverty,	
physical	and	mental	wellbeing,	aging,	conflict	resolution,	governance,	etc.		

AGI	promises	to	be	able	to	significantly	increase	human	flourishing.	

GPT (Generative Pretrained Transformer) as an attempt to achieve AGI 
The	 recent	 success	 of	 transformer-based	 systems	 and	 other	 Large	 Language	 Models	 (LLMs)	 has	 given	
rise	 to	 speculations	 that	 these	 powerful	 systems	 may,	 with	 relatively	 minor	 enhancements,	 grow	 into	
AGI.	 This	 is	 extremely	 unlikely	 given	 the	 hard	 requirements	 for	 human-level	 AGI	 such	 as	 reliability	 [4],	
predictability	 [5],	 and	 non-toxicity	 [6];	 real-time,	 life-long	 learning;	 and	 high-level	 reasoning	 and	
metacognition.		

GPT-based	systems	are	by	definition	‘generative’	(they	produce	artificial	content	based	on	probabilities	
of	co-occurrences;	they	make	up	stuff)	and	pre-trained	(almost	all	of	their	knowledge	is	non-adaptive,	
not	 acquired	 interactively	 or	 in	 real-time).	 Due	 to	 their	 correlational	 nature	 LLMs	 fundamentally	 lack	
robust	 reasoning.	 Moreover,	 neural	 networks	 can	 perform	 poorly	 on	 previously	 learned	 tasks	 when	
exposed	to	new	data	and	learning	new	tasks	(known	as	catastrophic	forgetting)	[7].		

Despite	these	limitations,	properly	curated	and	cross-validated	LLM	queries	could	be	extremely	valuable	
for	 helping	 with	 the	 difficult	 task	 of	 training	 AGIs;	 helping	 to	 provide	 the	 massive	 amount	 of	 general	
knowledge	that	they	will	need.	

2	

	
	
	
	
	
	
Human-Like Cognitive Abilities – Cognitive AI 
Cognitive	 AI	 describes	 machine	 systems	 able	 to	 understand	 language,	 use	 commonsense	 knowledge,	
reason,	and	adapt	to	unseen	circumstances,	similar	to	humans	[8].	

AGI	 assistants	 and	 researchers	 that	 can	 help	 us	 solve	 important	 (and	 not	 so	 important)	 problems	 will	
need	 to	 operate	 in	 the	 real	 world,	 to	 have	 a	 deep	 understanding	 of	 life	 and	 science,	 to	 effectively	
communicate	with	us,	to	use	our	tools	and	systems,	and	to	be	able	to	learn	and	innovate.	This	requires	a	
specific	 approach	 to	 building	 AGI,	 one	 that	 focuses	 on	 real-time,	 life-long	 conceptual	 learning	 and	
reasoning.		

While	such	AGIs	will	be	super-human	in	many	ways,	they	will	be	constrained	by	having	to	operate	with	
incomplete	and	often	contradictory	information,	and	limited	time	and	resources	to	perform	their	tasks.	
On	the	other	hand,	they	will	not	require	human-level	sense-acuity	or	dexterity.	One	could	call	this	the	
Helen-Hawking	model	of	AGI	(Helen	Keller/	Stephen	Hawking)	–	AGI	with	human-level	cognition	but	not	
overall	 human-level	 physical	 ability.	 They	 do,	 however,	 need	 to	 have	 some	 means	 of	 capturing	 and	
interacting	 with	 our	 4D	 world.	 This	 could	 be	 accomplished	 via,	 for	 example,	 PC	 screen,	 keyboard,	 and	
mouse	access.	Beyond	that,	AGIs	will	also	be	excellent	tool	users,	just	like	us.	

We	see	that	Cognitive	AI	is	the	clearest,	and	most	definitive	and	direct	path	to	AGI.	

Figure	2.	Dimensions	of	Adaptive	Autonomous	Intelligence.	

Defining the ‘I’ in AGI 
Defining	‘intelligence’	as	such	is	a	thankless	task,	so	instead,	we	will	focus	on	a	practical	description	of	
the	 kind	 of	 cognition/	 intelligence	 required	 to	 achieve	 the	 AGI	 goals	 mentioned	 above.	 Essential	
requirements	include:	

""  The	ability	to	autonomously	learn	multi-dimensional	objects,	actions	and	sequences,	including	their	

appropriate	context.	

""  To	do	this	incrementally	in	real	time,	in	an	integrated	manner	(knowledge	representation).	
""  To	contextually	identify	known	objects,	actions,	and	sequences	given	partial	or	similar		input.	

3	

	
	
	
""  The	ability	to	generalize	and	to	form	new	concepts	and	analogies	autonomously.	
""  To	learn	to	associate	words	with	their	respective	concepts,	and	to	learn	language	by	understanding	

it.	

""  To	 learn	 action	 sequences	 and	 their	 associated	 ‘triggers’	 via	 aping,	 exploration,	 trial-and-error,	
reinforcement,	instructions,	reasoning,	and	by	other	means	(autonomous	and	semi-autonomous).	

""  The	ability	to	adapt	or	unlearn	existing	knowledge	and	skills.	
""  The	ability	to	reason	abstractly,	including	planning,	and	theory-of-mind	reasoning.	
""  Meta-cognitive	reasoning	and	control	(System	2	from	[9]).	
""  Heuristic	search	and	problem	solving	ability.	 
""  To	have	and	use	both	short-term	and	long-term	memory	for	context,	recognition,	and	reasoning.	
""  Means	to	focus	on	and	select	particular	features	available	both	externally	and	internally.	

This	list	is	not	exhaustive	but	does	cover	the	most	essential	cognitive	abilities	required	for	AGI.	The	ideas	
of	 AGI	 have	 been	 around	 for	 decades.	 But	 only	 recently,	 it	 has	 been	 recognized	 as	 an	 emerging	
technological	megatrend	[10].	

The Three Waves of AI 
A	few	years	ago	DARPA	presented	a	simple	chronological	taxonomy	of	AI	called	‘The	Three	Waves	of	AI’	
[11]	broken	up	as	follows:	

1)  Rule-based	 approaches	 also	 referred	 to	 as	 ‘GOFAI’	 (Good	 Old-Fashioned	 AI),	 which	 dominated	 the	
field	 until	 about	 2010	 is	 the	 first	 ‘wave’.	 This	 is	 characterized	 by	 largely	 hand-crafted	 data	 and	
algorithms.	 It	 includes	 expert	 systems,	 sophisticated	 logic	 and	 search	 algorithms,	 planning	 and	
scheduling	systems,	semantic	web	representation,	natural	language	processing	systems	and	the	like.	
Its	most	visible	successes	were	IBM’s	Deep	Blue	chess	champion	in	1997	and	Watson,	their	Jeopardy	
quiz	champion.	

2)  The	second	wave	hit	like	a	tsunami	around	2012	when	researchers	figured	out	how	to	build	neural	
networks	using	massive	amounts	of	data	and	computational	power,	including	GPU/TPUs.	This	led	to	
breakthroughs	in	translation,	image	and	speech	recognition,	mastery	of	many	games	(including	Go)	
and	 ultimately	 powerful	 vision,	 speech,	 and	 text	 generation	 via	 GPTs.	 Currently,	 the	 pinnacle	 of	
these	developments	is	represented	by	various	LLMs	such	as	ChatGPT.	This	wave	is	characterized	by	
statistical	and	reinforcement	learning;	much	of	it	is	un-	or	self-supervised.	

3)  This	final	wave	is	still	in	its	infancy.	Its	focus	fully	aligns	with	the	requirements	of	AGI:	Autonomous,	
real-time	 learning	 and	 adaptation,	 and	 high-level	 reasoning.	 It	 also	 expects	 concepts	 to	 be	 more	
grounded	in	reality	(as	opposed	to	language	statistics),	robust	few-shot	learning,	and	explainability.	
We	 would	 expect	 these	 systems	 to	 elegantly	 integrate	 sub-symbolic	 pattern	 matching	 with	 high-
level	 symbolic	 and	 linguistic	 reasoning.	 An	 obvious	 candidate	 for	 all	 of	 these	 requirements	 is	 the	
‘Cognitive	Architecture’	approach.	

4	

	
	
	
Figure	3.	The	Three	waves	of	AI.	Adapted	from	DARPA	[11].	

Cognitive Architectures 
Cognitive	 Architectures,	 or	 more	 generally	 ‘Cognitive	 AI’,	 are	 founded	 on	 the	 idea	 of	 creating	 systems	
that	 encompass	 and	 embody	 all	 of	 the	 essential	 structures	 required	 for	 a	 (human-level)	 mind.	
Importantly,	it	also	considers	how	these	structures	and	functions	need	to	work	together	in	conjunction	
with	changing	knowledge	and	skills	to	yield	intelligence	in	diverse,	dynamic	environments	[12].	

Various	 cognitive	 architecture	 projects	 have	 been	 active	 for	 a	 few	 decades,	 though	 so	 far	 none	 have	
shown	sufficient	commercial promise	to	be	widely	adopted	or	particularly	well-funded	[8].	The	reasons	
are	manifold	and	complex	(see	next	section),	but	a	common	theme	is	that	they	are	implemented	in	far	
too	modular	and	inefficient	ways	[13],	and	they	lack	a	deep	theory	of	learning	and	cognition	[14]. 

A	novel	Cognitive	Architecture	designed	to	overcome	these	limitations	is	detailed	in	‘Concepts	is	All	You	
Need:	A	More	Direct	Path	to	AGI’	[15].	

Why don’t we have AGI yet? 
The	 short	 answer	may	well	be	that	there	simply	hasn’t	been	a	project	with	the	right	approach/theory	
plus	an	adequate	amount	of	funding.	Recent	success	of	ChatGPT	suggests	that	hardware	limitations	may	
not	 be	 a	 major	 bottleneck	 at	 this	 time,	 making	 feasible	 highly	 sophisticated	 language	 production	 or	
‘inference’.	

A	longer	answer	includes	the	following	considerations:	

#  Undoubtedly	 a	 major	 factor	 is	 that	 in	 spite	 of	 tens	 of	 thousands	 of	 AI	 researchers	 working	 in	 the	
field,	 only	 a	 tiny	 number	 are,	 by	 their	 own	 admission	 or	 by	 objective	 analysis,	 actually	 directly	
working	on	achieving	AGI.	

#  One	objective	measure	is	whether	the	AI	work	done	involves	a	clearly	identified	step	or	aspect	of	
an	 overall	 detailed	 plan	 to	 achieve	 AGI.	 Very	 little	 AI	 work	 matches	 this	 criterion.	 Specifically,	
Generative	AI	research	does	not.	

#  Even	 projects	 dedicated	 to	 developing	 AGI	 are	 seldom	 implemented	 with	 an	 explicit	 theory	 that	
actually	 matches	 the	 requirements	 of	 the	 kind	 of	 autonomous,	 adaptive	 intelligence	 needed	 for	
AGI.	

5	

	
	
	
	
#  Because	 of	 the	 tremendous	 success	 of	 Statistical	 AI	 (as	 opposed	 to	 Cognitive	 AI)	 over	 the	 past	
decade,	 currently	 almost	 all	 of	 the	 leading	 experts	 and	 practitioners	 in	 the	 field	 come	 from	
statistics,	 mathematics,	 or	 formal	 logic.	 This	 makes	 it	 almost	 impossible	 for	 them	 to	 see	 AGI	
requirements	from	a	cognitive	perspective.	

#  Unfortunately,	motivations	and	incentives	for	individuals,	teams,	and	companies	are	poorly	aligned	
to	optimizing	progress	towards	AGI.	Quite	the	contrary.	For	academics	it	is	to	publish	rather	than	to	
develop.	 For	 companies	 it	 is	 to	 produce	 impressive	 demos,	 or	 to	 beat	 humans	 at	 some	 game	 or	
activity	in	order	to	secure	additional	funding.	For	most	it	is	to	beat	existing	benchmarks	and	not	to	
change	them.	

#  Using	 existing	 benchmarks	 for	 AGI	

is	 highly	 problematic:	 Firstly,	 focus	 on	

incremental	
improvements	to	specific	existing	benchmarks	takes	effort	away	from	working	on	other	problems	
that	are	actually	more	fundamental	to	achieving	AGI.	It	is	easier	to	work	on	things	that	you	know	
how	 to	 make	 progress	 on,	 than	 to	 tackle	 difficult	 unknown	 issues.	 Secondly,	 current	 benchmarks	
are	 extremely	 poor	 at	 measuring	 progress	 of	 proto-AGIs.	 Early	 AGI	 systems	 will	 by	 definition	 do	
very	 poorly	 on	 existing	 narrow	 benchmarks,	 as	 well	 as	 on	 high-level	 IQ	 or	 professional	 admission	
tests.	

#  Even	if	all	the	stars	are	aligned	in	favor	of	developing	AGI	–	a	good	theory	and	development	plan,	
great	 cognitive	 team	 and	 funding,	 the	 right	 benchmarks	 –	 there	 still	 lurks	 what	 we	 can	 call	 ‘The	
Narrow	AI	Trap’.	Human	nature	is	such	that	we	instinctively	want	to	show	maximal	progress	in	the	
shortest	 time.	 Unfortunately,	 for	 AGI	 this	 often	 means	 that	 we	 end	 up	 using	 external	 human	
intelligence	 to	 achieve	 a	 specific	 result	 or	 make	 progress	 on	 a	 given	 benchmark	 rather	 than	
implementing	it	in	a	way	that	puts	the	intelligence	(adaptive,	autonomous	problem-solving	ability)	
into	the	system.	It	takes	careful	discipline	to	avoid	this.	Naturally,	AI	efforts	that	are	only	nominally	
AGI	(without	good	theory	or	plan),	will	more	easily	fall	into	this	trap.	

Figure	4.	Pressure	for	near-term	results	and	for	beating	existing	non-AGI	benchmarks	tends	to	deflect	

projects	away	from	AGI	capabilities,	towards	narrower	or	commercial	implementations.	

6	

	
	
	
	
	
	
	
#  Finally,	it	is	worth	mentioning	a	lack	of	clear	vision	by	people	who	could	help	to	make	AGI	happen	
sooner.	 People	 who	 claim	 to	 not	 be	 motivated	 by	 money,	 but	 by	 helping	 mankind	 flourish.	 Yet	
many	put	their	effort,	reputation	and	money	behind	the	latest	fad	or	biggest	potential	short-term	
win.	

Conclusion 
The	 spectacular	 performance	 of	 recent	 GPT	 technology	 teases	 the	 possibility	 that	 we	 may	 at	 last	 be	
close	to	being	able	to	realize	the	original	vision	of	‘AI’	–	to	have	human-level	‘Thinking	Machines’.	The	
term	 ‘AGI’	 was	 coined	 to	 (re)focus	 on	 this	 objective,	 and	 to	 bring	 about	 technology	 that	 can	 help	 us	
solve	the	many	problems	that	humanity	faces,	to	enhance	human	flourishing.	

However,	a	detailed	analysis	of	what	human-level	cognition	requires	shows	that	most	of	the	technical	
approaches,	 motivations	 and	 benchmarks	 currently	 dominating	 the	 field	 of	 AI	 are	 not	 aligned	 with	
achieving	this	objective.	

In	order	to	accelerate	progress	towards	AGI	we	will	need	to	focus	on	the	core	requirements	of	human-
like	 cognition	 –	 on	 items	 like	 autonomous,	 real-time,	 incremental	 learning;	 concept	 formation;	 and	
metacognitive	control.	We	need	to	shift	from	Second	to	Third	Wave	AI,	from	Statistical	or	Generative	AI	
to	Cognitive	AI.	

Acknowledgment 
We	thank	Dr.	Pat	Langley	for	his	constructive	comments.	

References 
[1]	 McCarthy,	 J.,	 Minsky,	 M.	 L.,	 Rochester,	 N.,	 &	 Shannon,	 C.	 E.	 (2006).	 A	 Proposal	 for	 the	 Dartmouth	
Summer	Research	Project	on	Artificial	Intelligence,	August	31,	1955.	AI	Magazine,	27(4),	12.	

[2]	Gallagher,	S.	(2000).	Philosophical	conceptions	of	the	self:	implications	for	cognitive	science.	Trends	
in	cognitive	sciences,	4(1),	14-21.	

[3]	Goertzel,	B.	(2007).	Artificial	general	intelligence	(Vol.	2,	p.	1).	C.	Pennachin	(Ed.).	New	York:	Springer.	

[4]	 Zou,	 A.,	 Wang,	 Z.,	 Kolter,	 J.	 Z.,	 &	 Fredrikson,	 M.	 (2023).	 Universal	 and	 Transferable	 Adversarial	
Attacks	on	Aligned	Language	Models.	arXiv	preprint	arXiv:2307.15043.	

[5]	Chen,	L.,	Zaharia,	M.,	&	Zou,	J.	(2023).	How	is	ChatGPT's	behavior	changing	over	time?.	arXiv	preprint	
arXiv:2307.09009. 

[6]	 Wang,	 B.,	 Chen,	 W.,	 Pei,	 H.,	 Xie,	 et	 al.,	 (2023).	 DecodingTrust:	 A	 Comprehensive	 Assessment	 of	
Trustworthiness	in	GPT	Models.	arXiv	preprint	arXiv:2306.11698.	

[7]	 Kong,	 Y.,	 Liu,	 L.,	 Chen,	 H.,	 Kacprzyk,	 J.,	 &	 Tao,	 D.	 (2023).	 Overcoming	 Catastrophic	 Forgetting	 in	
Continual	 Learning	 by	 Exploring	 Eigenvalues	 of	 Hessian	 Matrix.	 IEEE	 Transactions	 on	 Neural	 Networks	
and	Learning	Systems.	

[8]	Kotseruba,	I.,	&	Tsotsos,	J.	K.	(2020).	40	years	of	cognitive	architectures:	core	cognitive	abilities	and	
practical	applications.	Artificial	Intelligence	Review,	53(1),	17-94.	

[9]	Kahneman,	D.	(2011).	Thinking,	fast	and	slow.	Farrar,	Straus	and	Giroux.	

[10]	 Bash,	 C.,	 Faraboschi,	 P.,	 Frachtenberg,	 E.,	 Laplante,	 P.,	 Milojicic,	 D.,	 &	 Saracco,	 R.	 (2023).	
Megatrends.	IEEE	Computer,	56(07),	93-100.	

[11]	 A	 DARPA	 Perspective	 on	 Artificial	 Intelligence	 (2017).	 https://www.darpa.mil/about-us/darpa-
perspective-on-ai.	(Accessed	07.08.2023).	

[12]	Langley,	P.	(2012).	The	Cognitive	Systems	Paradigm.	Advances	in	Cognitive	Systems,	1(1),	3-13.	

7	

	
	
	
	
[13]	 Langley,	 P.,	 Laird,	 J.	 E.,	 &	 Rogers,	 S.	 (2009).	 Cognitive	 architectures:	 Research	 issues	 and	
challenges.	Cognitive	Systems	Research,	10(2),	141-160.	

[14]	 Wang,	 P.	 (2012).	 Theories	 of	 Artificial	 Intelligence	 -	 Meta-theoretical	 Considerations.	 Theoretical	
Foundations	of	Artificial	General	Intelligence.	Paris:	Atlantis	Press.	

[15]	Voss,	P.,	&	Jovanovic,	M.	(2023).	Concepts	is	All	You	Need:	A	More	Direct	Path	to	AGI.	arXiv	preprint	
arXiv:2309.01622.	

8	

	
","Why We Don ’ t Have AGI Yet Peter Voss , AIGO.ai , Austin TX , USA , peter @ aigo.ai Mlađan Jovanović , Singidunum University , Belgrade Serbia , mjovanovic @ singidunum.ac.rs Abstract – The original vision of AI was re-articulated in 2002 via the term ‘ Artificial General Intelligence ’ or AGI . This vision is to build ‘ Thinking Machines ’ – computer systems that can learn , reason , and solve problems similar to the way humans do . This is in stark contrast to the ‘ Narrow AI ’ approach practiced by almost everyone in the field over the many decades . While several large-scale efforts have nominally been working on AGI ( most notably DeepMind ) , the field of pure focused AGI development has not been well funded or promoted . This is surprising given the fantastic value that true AGI can bestow on humanity . In addition to the dearth of effort in this field , there are also several theoretical and methodical missteps that are hampering progress . We highlight why purely statistical approaches are unlikely to lead to AGI , and identify several crucial cognitive abilities required to achieve human-like adaptability and autonomous learning . We conclude with a survey of socio-technical factors that have undoubtedly slowed progress towards AGI . Keywords : AGI , Cognitive AI , Adaptive AI , Human-Level AI , Cognitive Architecture , Third Wave of AI . A Brief History of ‘ AGI ’ Originally , the term ‘ Artificial Intelligence ’ , coined by John McCarthy in 1955 [ 1 ] , referred to machines or computer programs that can think , learn , and reason the way humans do – that have the general cognitive ability that we possess . At the time he expected this to be achieved within a few months ! As it turned out this was a lot harder , so over the years and decades the field of ‘ AI ’ morphed into ‘ Narrow AI ’ – solving just one ( type of ) problem at a time . An often overlooked but crucial detrimental consequence of this shift was that focus was now on ‘ achieving a particular goal via the ingenuity of the programmer or data scientist ’ rather than ‘ how do we build a system that has general intelligence ’ . Therefore , AI ’ s focus had shifted from having internal intelligence to utilizing external intelligence ( the programmer ’ s intelligence ) to solve particular problems . This preoccupation with achieving specific narrow goals also had the undesirable side-effect of ignoring the importance of adaptation and agency . Intelligent entities must be able to autonomously adapt to changing circumstances and goals . The entities are agents that proactively initiate actions in their surroundings [ 2 ] . By 2001 a number of AI researchers had independently concluded that the time was ripe to return to the original vision of ‘ AI ’ and decided to join forces to write a book on this subject [ 3 ] . In 2002 , three of the authors ( Ben Goertzel , Shane Legg , and Peter Voss ) coined the term ‘ Artificial General Intelligence ’ for the book title . AGI refers to creating ( semi- ) autonomous , adaptive computer systems with the general cognitive capabilities typical for humans . The ability to support abstraction , analogy , planning and problem- solving . 1 Figure 1 . Comparison of external and internal intelligence . The Value of AGI Imagine a computer system that has the general cognitive prowess and knowledge of a medical cancer expert . Now imagine a million copies of this chipping away at this scourge . Also consider the following advantages of artificial researchers and workers : ! Massively lower cost than humans performing cognitive tasks . ! Much better at communicating and sharing knowledge with each other – no egos to get in the way ! ! Toiling away 365/24/7 , much faster and with better concentration . ! Not having the various ‘ distractions ’ that we have – e.g . family , vacation , hobbies , etc . ! ! A much-improved ability to think logically , and to perform complex planning and reasoning . ‘ Photographic memory ’ , and instant access to all online information . Now apply these powers to the many challenges facing humanity : energy , environment , poverty , physical and mental wellbeing , aging , conflict resolution , governance , etc . AGI promises to be able to significantly increase human flourishing . GPT ( Generative Pretrained Transformer ) as an attempt to achieve AGI The recent success of transformer-based systems and other Large Language Models ( LLMs ) has given rise to speculations that these powerful systems may , with relatively minor enhancements , grow into AGI . This is extremely unlikely given the hard requirements for human-level AGI such as reliability [ 4 ] , predictability [ 5 ] , and non-toxicity [ 6 ] ; real-time , life-long learning ; and high-level reasoning and metacognition . GPT-based systems are by definition ‘ generative ’ ( they produce artificial content based on probabilities of co-occurrences ; they make up stuff ) and pre-trained ( almost all of their knowledge is non-adaptive , not acquired interactively or in real-time ) . Due to their correlational nature LLMs fundamentally lack robust reasoning . Moreover , neural networks can perform poorly on previously learned tasks when exposed to new data and learning new tasks ( known as catastrophic forgetting ) [ 7 ] . Despite these limitations , properly curated and cross-validated LLM queries could be extremely valuable for helping with the difficult task of training AGIs ; helping to provide the massive amount of general knowledge that they will need . 2 Human-Like Cognitive Abilities – Cognitive AI Cognitive AI describes machine systems able to understand language , use commonsense knowledge , reason , and adapt to unseen circumstances , similar to humans [ 8 ] . AGI assistants and researchers that can help us solve important ( and not so important ) problems will need to operate in the real world , to have a deep understanding of life and science , to effectively communicate with us , to use our tools and systems , and to be able to learn and innovate . This requires a specific approach to building AGI , one that focuses on real-time , life-long conceptual learning and reasoning . While such AGIs will be super-human in many ways , they will be constrained by having to operate with incomplete and often contradictory information , and limited time and resources to perform their tasks . On the other hand , they will not require human-level sense-acuity or dexterity . One could call this the Helen-Hawking model of AGI ( Helen Keller/ Stephen Hawking ) – AGI with human-level cognition but not overall human-level physical ability . They do , however , need to have some means of capturing and interacting with our 4D world . This could be accomplished via , for example , PC screen , keyboard , and mouse access . Beyond that , AGIs will also be excellent tool users , just like us . We see that Cognitive AI is the clearest , and most definitive and direct path to AGI . Figure 2 . Dimensions of Adaptive Autonomous Intelligence . Defining the ‘ I ’ in AGI Defining ‘ intelligence ’ as such is a thankless task , so instead , we will focus on a practical description of the kind of cognition/ intelligence required to achieve the AGI goals mentioned above . Essential requirements include : `` The ability to autonomously learn multi-dimensional objects , actions and sequences , including their appropriate context. `` To do this incrementally in real time , in an integrated manner ( knowledge representation ) . `` To contextually identify known objects , actions , and sequences given partial or similar input . 3 `` The ability to generalize and to form new concepts and analogies autonomously. `` To learn to associate words with their respective concepts , and to learn language by understanding it. `` To learn action sequences and their associated ‘ triggers ’ via aping , exploration , trial-and-error , reinforcement , instructions , reasoning , and by other means ( autonomous and semi-autonomous ) . `` The ability to adapt or unlearn existing knowledge and skills. `` The ability to reason abstractly , including planning , and theory-of-mind reasoning. `` Meta-cognitive reasoning and control ( System 2 from [ 9 ] ) . `` Heuristic search and problem solving ability. `` To have and use both short-term and long-term memory for context , recognition , and reasoning. `` Means to focus on and select particular features available both externally and internally . This list is not exhaustive but does cover the most essential cognitive abilities required for AGI . The ideas of AGI have been around for decades . But only recently , it has been recognized as an emerging technological megatrend [ 10 ] . The Three Waves of AI A few years ago DARPA presented a simple chronological taxonomy of AI called ‘ The Three Waves of AI ’ [ 11 ] broken up as follows : 1 ) Rule-based approaches also referred to as ‘ GOFAI ’ ( Good Old-Fashioned AI ) , which dominated the field until about 2010 is the first ‘ wave ’ . This is characterized by largely hand-crafted data and algorithms . It includes expert systems , sophisticated logic and search algorithms , planning and scheduling systems , semantic web representation , natural language processing systems and the like . Its most visible successes were IBM ’ s Deep Blue chess champion in 1997 and Watson , their Jeopardy quiz champion . 2 ) The second wave hit like a tsunami around 2012 when researchers figured out how to build neural networks using massive amounts of data and computational power , including GPU/TPUs . This led to breakthroughs in translation , image and speech recognition , mastery of many games ( including Go ) and ultimately powerful vision , speech , and text generation via GPTs . Currently , the pinnacle of these developments is represented by various LLMs such as ChatGPT . This wave is characterized by statistical and reinforcement learning ; much of it is un- or self-supervised . 3 ) This final wave is still in its infancy . Its focus fully aligns with the requirements of AGI : Autonomous , real-time learning and adaptation , and high-level reasoning . It also expects concepts to be more grounded in reality ( as opposed to language statistics ) , robust few-shot learning , and explainability . We would expect these systems to elegantly integrate sub-symbolic pattern matching with high- level symbolic and linguistic reasoning . An obvious candidate for all of these requirements is the ‘ Cognitive Architecture ’ approach . 4 Figure 3 . The Three waves of AI . Adapted from DARPA [ 11 ] . Cognitive Architectures Cognitive Architectures , or more generally ‘ Cognitive AI ’ , are founded on the idea of creating systems that encompass and embody all of the essential structures required for a ( human-level ) mind . Importantly , it also considers how these structures and functions need to work together in conjunction with changing knowledge and skills to yield intelligence in diverse , dynamic environments [ 12 ] . Various cognitive architecture projects have been active for a few decades , though so far none have shown sufficient commercial promise to be widely adopted or particularly well-funded [ 8 ] . The reasons are manifold and complex ( see next section ) , but a common theme is that they are implemented in far too modular and inefficient ways [ 13 ] , and they lack a deep theory of learning and cognition [ 14 ] . A novel Cognitive Architecture designed to overcome these limitations is detailed in ‘ Concepts is All You Need : A More Direct Path to AGI ’ [ 15 ] . Why don ’ t we have AGI yet ? The short answer may well be that there simply hasn ’ t been a project with the right approach/theory plus an adequate amount of funding . Recent success of ChatGPT suggests that hardware limitations may not be a major bottleneck at this time , making feasible highly sophisticated language production or ‘ inference ’ . A longer answer includes the following considerations : # Undoubtedly a major factor is that in spite of tens of thousands of AI researchers working in the field , only a tiny number are , by their own admission or by objective analysis , actually directly working on achieving AGI . # One objective measure is whether the AI work done involves a clearly identified step or aspect of an overall detailed plan to achieve AGI . Very little AI work matches this criterion . Specifically , Generative AI research does not . # Even projects dedicated to developing AGI are seldom implemented with an explicit theory that actually matches the requirements of the kind of autonomous , adaptive intelligence needed for AGI . 5 # Because of the tremendous success of Statistical AI ( as opposed to Cognitive AI ) over the past decade , currently almost all of the leading experts and practitioners in the field come from statistics , mathematics , or formal logic . This makes it almost impossible for them to see AGI requirements from a cognitive perspective . # Unfortunately , motivations and incentives for individuals , teams , and companies are poorly aligned to optimizing progress towards AGI . Quite the contrary . For academics it is to publish rather than to develop . For companies it is to produce impressive demos , or to beat humans at some game or activity in order to secure additional funding . For most it is to beat existing benchmarks and not to change them . # Using existing benchmarks for AGI is highly problematic : Firstly , focus on incremental improvements to specific existing benchmarks takes effort away from working on other problems that are actually more fundamental to achieving AGI . It is easier to work on things that you know how to make progress on , than to tackle difficult unknown issues . Secondly , current benchmarks are extremely poor at measuring progress of proto-AGIs . Early AGI systems will by definition do very poorly on existing narrow benchmarks , as well as on high-level IQ or professional admission tests . # Even if all the stars are aligned in favor of developing AGI – a good theory and development plan , great cognitive team and funding , the right benchmarks – there still lurks what we can call ‘ The Narrow AI Trap ’ . Human nature is such that we instinctively want to show maximal progress in the shortest time . Unfortunately , for AGI this often means that we end up using external human intelligence to achieve a specific result or make progress on a given benchmark rather than implementing it in a way that puts the intelligence ( adaptive , autonomous problem-solving ability ) into the system . It takes careful discipline to avoid this . Naturally , AI efforts that are only nominally AGI ( without good theory or plan ) , will more easily fall into this trap . Figure 4 . Pressure for near-term results and for beating existing non-AGI benchmarks tends to deflect projects away from AGI capabilities , towards narrower or commercial implementations . 6 # Finally , it is worth mentioning a lack of clear vision by people who could help to make AGI happen sooner . People who claim to not be motivated by money , but by helping mankind flourish . Yet many put their effort , reputation and money behind the latest fad or biggest potential short-term win . Conclusion The spectacular performance of recent GPT technology teases the possibility that we may at last be close to being able to realize the original vision of ‘ AI ’ – to have human-level ‘ Thinking Machines ’ . The term ‘ AGI ’ was coined to ( re ) focus on this objective , and to bring about technology that can help us solve the many problems that humanity faces , to enhance human flourishing . However , a detailed analysis of what human-level cognition requires shows that most of the technical approaches , motivations and benchmarks currently dominating the field of AI are not aligned with achieving this objective . In order to accelerate progress towards AGI we will need to focus on the core requirements of human- like cognition – on items like autonomous , real-time , incremental learning ; concept formation ; and metacognitive control . We need to shift from Second to Third Wave AI , from Statistical or Generative AI to Cognitive AI . Acknowledgment We thank Dr. Pat Langley for his constructive comments . References [ 1 ] McCarthy , J. , Minsky , M. L. , Rochester , N. , & Shannon , C. E. ( 2006 ) . A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence , August 31 , 1955 . AI Magazine , 27 ( 4 ) , 12 . [ 2 ] Gallagher , S. ( 2000 ) . Philosophical conceptions of the self : implications for cognitive science . Trends in cognitive sciences , 4 ( 1 ) , 14-21 . [ 3 ] Goertzel , B . ( 2007 ) . Artificial general intelligence ( Vol . 2 , p. 1 ) . C. Pennachin ( Ed. ) . New York : Springer . [ 4 ] Zou , A. , Wang , Z. , Kolter , J . Z. , & Fredrikson , M. ( 2023 ) . Universal and Transferable Adversarial Attacks on Aligned Language Models . arXiv preprint arXiv:2307.15043 . [ 5 ] Chen , L. , Zaharia , M. , & Zou , J . ( 2023 ) . How is ChatGPT 's behavior changing over time ? . arXiv preprint arXiv:2307.09009 . [ 6 ] Wang , B. , Chen , W. , Pei , H. , Xie , et al. , ( 2023 ) . DecodingTrust : A Comprehensive Assessment of Trustworthiness in GPT Models . arXiv preprint arXiv:2306.11698 . [ 7 ] Kong , Y. , Liu , L. , Chen , H. , Kacprzyk , J. , & Tao , D. ( 2023 ) . Overcoming Catastrophic Forgetting in Continual Learning by Exploring Eigenvalues of Hessian Matrix . IEEE Transactions on Neural Networks and Learning Systems . [ 8 ] Kotseruba , I. , & Tsotsos , J. K. ( 2020 ) . 40 years of cognitive architectures : core cognitive abilities and practical applications . Artificial Intelligence Review , 53 ( 1 ) , 17-94 . [ 9 ] Kahneman , D. ( 2011 ) . Thinking , fast and slow . Farrar , Straus and Giroux . [ 10 ] Bash , C. , Faraboschi , P. , Frachtenberg , E. , Laplante , P. , Milojicic , D. , & Saracco , R. ( 2023 ) . Megatrends . IEEE Computer , 56 ( 07 ) , 93-100 . [ 11 ] A DARPA Perspective on Artificial Intelligence ( 2017 ) . https : //www.darpa.mil/about-us/darpa- perspective-on-ai . ( Accessed 07.08.2023 ) . [ 12 ] Langley , P. ( 2012 ) . The Cognitive Systems Paradigm . Advances in Cognitive Systems , 1 ( 1 ) , 3-13 . 7 [ 13 ] Langley , P. , Laird , J. E. , & Rogers , S. ( 2009 ) . Cognitive architectures : Research issues and challenges . Cognitive Systems Research , 10 ( 2 ) , 141-160 . [ 14 ] Wang , P. ( 2012 ) . Theories of Artificial Intelligence - Meta-theoretical Considerations . Theoretical Foundations of Artificial General Intelligence . Paris : Atlantis Press . [ 15 ] Voss , P. , & Jovanovic , M. ( 2023 ) . Concepts is All You Need : A More Direct Path to AGI . arXiv preprint arXiv:2309.01622 . 8","['agi', 'yet', 'singidunumacrs', 'abstract', 'original', 'vision', 'rearticulate', 'term', 'artificial', 'general', 'intelligence', 'agi', 'vision', 'build', 'think', 'machine', 'computer', 'system', 'learn', 'reason', 'solve', 'problem', 'similar', 'way', 'human', 'stark', 'contrast', 'narrow', 'ai', 'approach', 'practice', 'almost', 'field', 'many', 'decade', 'several', 'largescale', 'effort', 'nominally', 'work', 'notably', 'deepmind', 'field', 'pure', 'focused', 'agi', 'development', 'well', 'fund', 'promote', 'surprising', 'give', 'fantastic', 'value', 'true', 'agi', 'bestow', 'humanity', 'addition', 'dearth', 'effort', 'field', 'also', 'several', 'theoretical', 'methodical', 'misstep', 'hamper', 'progress', 'highlight', 'purely', 'statistical', 'approach', 'unlikely', 'lead', 'agi', 'identify', 'several', 'crucial', 'cognitive', 'ability', 'require', 'achieve', 'humanlike', 'adaptability', 'autonomous', 'learning', 'conclude', 'survey', 'sociotechnical', 'factor', 'undoubtedly', 'slow', 'progress', 'agi', 'keyword', 'agi', 'cognitive', 'ai', 'adaptive', 'ai', 'humanlevel', 'ai', 'cognitive', 'architecture', 'third', 'wave', 'brief', 'history', 'agi', 'originally', 'term', 'artificial', 'intelligence', 'coin', 'refer', 'machine', 'computer', 'program', 'think', 'learn', 'reason', 'way', 'human', 'general', 'cognitive', 'ability', 'possess', 'time', 'expect', 'achieve', 'month', 'turn', 'lot', 'hard', 'year', 'decade', 'field', 'morph', 'narrow', 'ai', 'solve', 'type', 'problem', 'time', 'often', 'overlook', 'crucial', 'detrimental', 'consequence', 'shift', 'focus', 'achieve', 'particular', 'goal', 'ingenuity', 'programmer', 'datum', 'scientist', 'rather', 'build', 'system', 'general', 'intelligence', 'therefore', 'ai', 'focus', 'shift', 'internal', 'intelligence', 'utilize', 'external', 'intelligence', 'programmer', 'intelligence', 'solve', 'particular', 'problem', 'preoccupation', 'achieve', 'specific', 'narrow', 'goal', 'also', 'undesirable', 'sideeffect', 'ignore', 'importance', 'adaptation', 'agency', 'intelligent', 'entity', 'able', 'autonomously', 'adapt', 'change', 'circumstance', 'goal', 'entity', 'agent', 'proactively', 'initiate', 'action', 'surrounding', 'number', 'researcher', 'independently', 'conclude', 'time', 'ripe', 'return', 'original', 'vision', 'decide', 'join', 'force', 'write', 'book', 'subject', 'author', 'goertzel', 'shane', 'legg', 'voss', 'coin', 'term', 'artificial', 'general', 'intelligence', 'book', 'title', 'agi', 'refer', 'create', 'semi', 'autonomous', 'adaptive', 'computer', 'system', 'general', 'cognitive', 'capability', 'typical', 'human', 'ability', 'support', 'abstraction', 'analogy', 'planning', 'problem', 'solve', 'figure', 'comparison', 'external', 'internal', 'intelligence', 'value', 'agi', 'imagine', 'computer', 'system', 'general', 'cognitive', 'prowess', 'knowledge', 'medical', 'cancer', 'expert', 'imagine', 'copy', 'chip', 'away', 'scourge', 'also', 'consider', 'follow', 'advantage', 'artificial', 'researcher', 'worker', 'massively', 'low', 'cost', 'human', 'perform', 'cognitive', 'task', 'much', 'well', 'communicate', 'share', 'knowledge', 'egos', 'get', 'way', 'toil', 'away', 'much', 'fast', 'well', 'concentration', 'various', 'distraction', 'eg', 'family', 'vacation', 'hobby', 'muchimproved', 'ability', 'think', 'logically', 'perform', 'complex', 'planning', 'reasoning', 'photographic', 'memory', 'instant', 'access', 'online', 'information', 'apply', 'power', 'many', 'challenge', 'face', 'humanity', 'energy', 'environment', 'poverty', 'physical', 'mental', 'wellbeing', 'age', 'conflict', 'resolution', 'governance', 'agi', 'promise', 'able', 'significantly', 'increase', 'human', 'flourish', 'gpt', 'generative', 'pretraine', 'transformer', 'attempt', 'achieve', 'agi', 'recent', 'success', 'transformerbase', 'system', 'large', 'language', 'model', 'llm', 'give', 'rise', 'speculation', 'powerful', 'system', 'relatively', 'minor', 'enhancement', 'grow', 'agi', 'extremely', 'unlikely', 'give', 'hard', 'requirement', 'humanlevel', 'agi', 'reliability', 'predictability', 'nontoxicity', 'realtime', 'lifelong', 'learning', 'highlevel', 'reasoning', 'metacognition', 'gptbase', 'system', 'definition', 'generative', 'produce', 'artificial', 'content', 'base', 'probability', 'cooccurrence', 'make', 'stuff', 'pretraine', 'almost', 'knowledge', 'nonadaptive', 'acquire', 'interactively', 'realtime', 'correlational', 'nature', 'llm', 'fundamentally', 'lack', 'robust', 'reasoning', 'moreover', 'neural', 'network', 'perform', 'poorly', 'previously', 'learn', 'task', 'expose', 'new', 'datum', 'learn', 'new', 'task', 'know', 'catastrophic', 'forgetting', 'limitation', 'properly', 'curate', 'crossvalidate', 'llm', 'query', 'extremely', 'valuable', 'help', 'difficult', 'task', 'train', 'agis', 'help', 'provide', 'massive', 'amount', 'general', 'knowledge', 'need', 'humanlike', 'cognitive', 'ability', 'cognitive', 'ai', 'cognitive', 'describe', 'machine', 'system', 'able', 'understand', 'language', 'use', 'commonsense', 'knowledge', 'reason', 'adapt', 'unseen', 'circumstance', 'similar', 'human', 'agi', 'assistant', 'researcher', 'help', 'solve', 'important', 'important', 'problem', 'need', 'operate', 'real', 'world', 'deep', 'understanding', 'life', 'science', 'effectively', 'communicate', 'use', 'tool', 'system', 'able', 'learn', 'innovate', 'require', 'specific', 'approach', 'build', 'agi', 'one', 'focus', 'realtime', 'lifelong', 'conceptual', 'learning', 'reasoning', 'agis', 'superhuman', 'many', 'way', 'constrain', 'operate', 'incomplete', 'often', 'contradictory', 'information', 'limited', 'time', 'resource', 'perform', 'task', 'hand', 'require', 'humanlevel', 'senseacuity', 'dexterity', 'call', 'helenhawking', 'model', 'agi', 'hawk', 'agi', 'humanlevel', 'cognition', 'overall', 'humanlevel', 'physical', 'ability', 'however', 'need', 'mean', 'capture', 'interact', 'world', 'accomplish', 'example', 'pc', 'screen', 'keyboard', 'mouse', 'access', 'agis', 'also', 'excellent', 'tool', 'user', 'see', 'cognitive', 'ai', 'clear', 'definitive', 'direct', 'path', 'agi', 'figure', 'dimension', 'adaptive', 'autonomous', 'intelligence', 'define', 'agi', 'define', 'intelligence', 'thankless', 'task', 'instead', 'focus', 'practical', 'description', 'kind', 'cognition', 'intelligence', 'require', 'achieve', 'agi', 'goal', 'mention', 'essential', 'requirement', 'include', 'ability', 'autonomously', 'learn', 'multidimensional', 'object', 'action', 'sequence', 'include', 'appropriate', 'context', 'incrementally', 'real', 'time', 'integrate', 'manner', 'knowledge', 'representation', 'contextually', 'identify', 'know', 'object', 'action', 'sequence', 'give', 'partial', 'similar', 'input', 'ability', 'generalize', 'form', 'new', 'concept', 'analogy', 'autonomously', 'learn', 'associate', 'word', 'respective', 'concept', 'learn', 'language', 'understand', 'learn', 'action', 'sequence', 'associate', 'trigger', 'ape', 'exploration', 'reinforcement', 'instruction', 'reasoning', 'mean', 'autonomous', 'semiautonomous', 'ability', 'adapt', 'unlearn', 'exist', 'knowledge', 'skill', 'ability', 'reason', 'abstractly', 'include', 'planning', 'theoryofmind', 'reasoning', 'metacognitive', 'reasoning', 'control', 'system', 'heuristic', 'search', 'problem', 'solve', 'ability', 'use', 'shortterm', 'longterm', 'memory', 'context', 'recognition', 'reasoning', 'mean', 'focus', 'select', 'particular', 'feature', 'available', 'externally', 'internally', 'list', 'exhaustive', 'cover', 'essential', 'cognitive', 'ability', 'require', 'agi', 'idea', 'agi', 'around', 'decade', 'recently', 'recognize', 'emerge', 'technological', 'megatrend', 'wave', 'ai', 'year', 'ago', 'present', 'simple', 'chronological', 'taxonomy', 'call', 'wave', 'break', 'follow', 'rulebase', 'approach', 'also', 'refer', 'gofai', 'oldfashione', 'dominate', 'field', 'first', 'wave', 'characterize', 'largely', 'handcraft', 'datum', 'algorithm', 'include', 'expert', 'system', 'sophisticated', 'logic', 'search', 'algorithm', 'planning', 'scheduling', 'system', 'semantic', 'web', 'representation', 'natural', 'language', 'processing', 'system', 'visible', 'success', 'deep', 'blue', 'chess', 'champion', 'watson', 'jeopardy', 'quiz', 'champion', 'second', 'wave', 'hit', 'tsunami', 'researcher', 'figure', 'build', 'neural', 'network', 'use', 'massive', 'amount', 'datum', 'computational', 'power', 'include', 'lead', 'breakthrough', 'translation', 'image', 'speech', 'mastery', 'many', 'game', 'include', 'go', 'ultimately', 'powerful', 'vision', 'speech', 'text', 'generation', 'gpt', 'currently', 'pinnacle', 'development', 'represent', 'various', 'llm', 'chatgpt', 'wave', 'characterize', 'statistical', 'reinforcement', 'learning', 'much', 'selfsupervise', 'final', 'wave', 'still', 'infancy', 'focus', 'fully', 'align', 'requirement', 'agi', 'autonomous', 'realtime', 'learning', 'adaptation', 'highlevel', 'reasoning', 'also', 'expect', 'concept', 'ground', 'reality', 'oppose', 'language', 'statistic', 'robust', 'fewshot', 'learning', 'explainability', 'expect', 'system', 'elegantly', 'integrate', 'subsymbolic', 'pattern', 'match', 'high', 'level', 'symbolic', 'linguistic', 'reasoning', 'obvious', 'candidate', 'requirement', 'cognitive', 'architecture', 'approach', 'figure', 'wave', 'adapt', 'cognitive', 'architecture', 'cognitive', 'architecture', 'generally', 'cognitive', 'ai', 'found', 'idea', 'create', 'system', 'encompass', 'embody', 'essential', 'structure', 'require', 'humanlevel', 'mind', 'importantly', 'also', 'consider', 'structure', 'function', 'need', 'work', 'together', 'conjunction', 'change', 'knowledge', 'skill', 'yield', 'intelligence', 'diverse', 'dynamic', 'environment', 'various', 'cognitive', 'architecture', 'project', 'active', 'decade', 'far', 'none', 'show', 'sufficient', 'commercial', 'promise', 'widely', 'adopt', 'particularly', 'wellfunde', 'reason', 'manifold', 'complex', 'see', 'next', 'section', 'common', 'theme', 'implement', 'far', 'modular', 'inefficient', 'way', 'lack', 'deep', 'theory', 'learning', 'cognition', 'novel', 'cognitive', 'architecture', 'design', 'overcome', 'limitation', 'detail', 'concept', 'need', 'direct', 'path', 'agi', 'yet', 'short', 'answer', 'well', 'simply', 'project', 'right', 'approachtheory', 'adequate', 'amount', 'funding', 'recent', 'success', 'chatgpt', 'suggest', 'hardware', 'limitation', 'major', 'bottleneck', 'time', 'make', 'feasible', 'highly', 'sophisticated', 'language', 'production', 'inference', 'long', 'answer', 'include', 'follow', 'consideration', 'undoubtedly', 'major', 'factor', 'spite', 'ten', 'thousand', 'researcher', 'work', 'field', 'tiny', 'number', 'admission', 'objective', 'analysis', 'actually', 'directly', 'work', 'achieve', 'agi', 'objective', 'measure', 'work', 'involve', 'clearly', 'identify', 'step', 'aspect', 'overall', 'detailed', 'plan', 'achieve', 'agi', 'little', 'ai', 'work', 'match', 'criterion', 'specifically', 'generative', 'ai', 'research', 'even', 'project', 'dedicate', 'develop', 'agi', 'seldom', 'implement', 'explicit', 'theory', 'actually', 'match', 'requirement', 'kind', 'autonomous', 'adaptive', 'intelligence', 'need', 'agi', 'tremendous', 'success', 'statistical', 'ai', 'oppose', 'cognitive', 'ai', 'past', 'decade', 'currently', 'almost', 'lead', 'expert', 'practitioner', 'field', 'come', 'statistic', 'mathematic', 'formal', 'logic', 'make', 'almost', 'impossible', 'see', 'agi', 'requirement', 'cognitive', 'perspective', 'unfortunately', 'motivation', 'incentive', 'individual', 'team', 'company', 'poorly', 'align', 'optimize', 'progress', 'agi', 'contrary', 'academic', 'publish', 'rather', 'develop', 'company', 'produce', 'impressive', 'demos', 'beat', 'human', 'game', 'activity', 'order', 'secure', 'additional', 'funding', 'beat', 'exist', 'benchmark', 'change', 'use', 'exist', 'benchmark', 'agi', 'highly', 'problematic', 'firstly', 'focus', 'incremental', 'improvement', 'specific', 'exist', 'benchmark', 'take', 'effort', 'away', 'work', 'problem', 'actually', 'fundamental', 'achieve', 'agi', 'easy', 'work', 'thing', 'know', 'make', 'progress', 'tackle', 'difficult', 'unknown', 'issue', 'secondly', 'current', 'benchmark', 'extremely', 'poor', 'measure', 'progress', 'protoagis', 'early', 'agi', 'system', 'definition', 'poorly', 'exist', 'narrow', 'benchmark', 'well', 'highlevel', 'iq', 'professional', 'admission', 'test', 'even', 'star', 'align', 'favor', 'develop', 'agi', 'good', 'theory', 'development', 'plan', 'great', 'cognitive', 'team', 'fund', 'right', 'benchmark', 'still', 'lurk', 'call', 'narrow', 'trap', 'human', 'nature', 'instinctively', 'want', 'show', 'maximal', 'progress', 'short', 'time', 'unfortunately', 'agi', 'often', 'mean', 'end', 'use', 'external', 'human', 'intelligence', 'achieve', 'specific', 'result', 'make', 'progress', 'give', 'benchmark', 'rather', 'implement', 'way', 'put', 'intelligence', 'adaptive', 'autonomous', 'problemsolving', 'ability', 'system', 'take', 'careful', 'discipline', 'avoid', 'naturally', 'ai', 'effort', 'nominally', 'agi', 'good', 'theory', 'plan', 'easily', 'fall', 'trap', 'figure', 'pressure', 'nearterm', 'result', 'beat', 'exist', 'nonagi', 'benchmark', 'tend', 'deflect', 'project', 'away', 'agi', 'capability', 'narrow', 'commercial', 'implementation', 'finally', 'worth', 'mention', 'lack', 'clear', 'vision', 'people', 'help', 'make', 'happen', 'soon', 'people', 'claim', 'motivate', 'money', 'help', 'mankind', 'flourish', 'yet', 'many', 'put', 'effort', 'reputation', 'money', 'late', 'fad', 'big', 'potential', 'shortterm', 'win', 'conclusion', 'spectacular', 'performance', 'recent', 'gpt', 'technology', 'tease', 'possibility', 'last', 'close', 'able', 'realize', 'original', 'vision', 'humanlevel', 'thinking', 'machine', 'term', 'agi', 'coin', 'focus', 'objective', 'bring', 'technology', 'help', 'solve', 'many', 'problem', 'humanity', 'face', 'enhance', 'human', 'flourish', 'however', 'detailed', 'analysis', 'humanlevel', 'cognition', 'require', 'show', 'technical', 'approach', 'motivation', 'benchmark', 'currently', 'dominate', 'field', 'ai', 'align', 'achieve', 'objective', 'order', 'accelerate', 'progress', 'agi', 'need', 'focus', 'core', 'requirement', 'human', 'cognition', 'item', 'autonomous', 'realtime', 'incremental', 'learning', 'concept', 'formation', 'metacognitive', 'control', 'need', 'shift', 'second', 'third', 'wave', 'ai', 'statistical', 'generative', 'ai', 'cognitive', 'ai', 'acknowledgment', 'thank', 'dr', 'constructive', 'comment', 'reference', 'mccarthy', 'minsky', 'l', 'e', 'proposal', 'dartmouth', 'summer', 'research', 'project', 'artificial', 'intelligence', 'magazine', 'gallagher', 'philosophical', 'conception', 'self', 'implication', 'cognitive', 'science', 'trend', 'cognitive', 'science', 'goertzel', 'b', 'artificial', 'general', 'intelligence', 'vol', 'p', 'c', 'zou', 'kolter', 'fredrikson', 'universal', 'transferable', 'adversarial', 'attack', 'align', 'language', 'model', 'arxiv', 'preprint', 'l', 'j', 'behavior', 'change', 'time', 'arxiv', 'preprint', 'h', 'decodingtrust', 'comprehensive', 'assessment', 'trustworthiness', 'gpt', 'model', 'arxiv', 'preprint', 'arxiv230611698', 'overcome', 'catastrophic', 'forgetting', 'continual', 'learning', 'explore', 'eigenvalue', 'hessian', 'matrix', 'ieee', 'transaction', 'neural', 'network', 'learn', 'system', 'kotseruba', 'tsotsos', 'year', 'cognitive', 'architecture', 'core', 'cognitive', 'ability', 'practical', 'application', 'artificial', 'intelligence', 'review', 'kahneman', 'think', 'fast', 'slow', 'farrar', 'straus', 'giroux', 'bash', 'c', 'faraboschi', 'p', 'frachtenberg', 'e', 'laplante', 'p', 'milojicic', 'saracco', 'r', 'megatrend', 'ieee', 'computer', 'darpa', 'perspective', 'artificial', 'intelligence', 'https', 'perspectiveonai', 'access', 'cognitive', 'system', 'paradigm', 'advance', 'cognitive', 'system', 'roger', 'cognitive', 'architecture', 'research', 'issue', 'challenge', 'cognitive', 'system', 'research', 'theory', 'artificial', 'intelligence', 'metatheoretical', 'consideration', 'theoretical', 'foundation', 'artificial', 'general', 'intelligence', 'press', 'voss', 'p', 'jovanovic', 'concept', 'need', 'direct', 'path', 'agi', 'arxiv', 'preprint']",
From Stagnant to Stunning: Google Transforms Still Images into Photo-Realistic Animations,https://syncedreview.com/2023/09/20/from-stagnant-to-stunning-google-transforms-still-images-into-photo-realistic-animations/,2023-09-20,"Motion is one of the most conspicuous visual cues in the natural world, and humans possess a remarkable sensitivity to it. While humans effortlessly perceive motion, training a model to learn realistic scene motion presents substantial challenges due to the intricacies involved in measuring and capturing physical properties on a large scale. Fortunately, recent advancements in generative models, particularly conditional diffusion models, have ushered in a new era of modeling highly intricate and diverse distributions of real images based on text input. Moreover, recent research indicates that extending this modeling to other domains, such as videos and 3D geometry, holds significant potential for downstream applications. In a paper titled “Generative Image Dynamics,” a Google research team introduces an innovative approach to model natural oscillation dynamics using a single static image. This approach yields photo-realistic animations derived from a lone image, surpassing the performance of previous methods by a substantial margin. Furthermore, it opens doors to various other applications, such as the creation of interactive animations. This model is trained on automatically extracted motion trajectories from a large collection of real video sequences. Conditioned on an input image, the trained model predicts a neural stochastic motion texture: a set of coefficients of a motion basis that characterize each pixel’s trajectory into the future. The core of this model’s training process lies in automatically extracted motion trajectories from an extensive collection of real video sequences. Given an input image, the trained model predicts a neural stochastic motion texture, which consists of coefficients representing the motion basis for each pixel’s trajectory into the future. This prediction occurs via a diffusion model, which generates coefficients one frequency at a time while coordinating these predictions across frequency bands. The resulting frequency-space textures can then be converted into dense, long-range pixel motion trajectories. Together with an image-based rendering diffusion model, these trajectories can be employed to synthesize future frames, effectively transforming static images into lifelike animations. In contrast to prior models relying solely on raw RGB pixel data, this motion-based representation captures a more fundamental, lower-dimensional underlying structure that efficiently accounts for variations in pixel values. Consequently, the proposed approach results in more coherent, long-term generation and affords finer control over animations when compared to earlier methods that perform image animation through raw video synthesis. In their empirical study, the research team rigorously compared their approach against several recent single-image animation and video prediction methods. The results clearly demonstrate that their proposed approach beats prior single-image animation benchmarks in terms of both image and video synthesis quality. Overall, this work represents a highly promising breakthrough, capable of generating photo-realistic animations from a single static image while significantly surpassing the performance of previous baseline methods. The paper Generative Image Dynamics on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Motion is one of the most conspicuous visual cues in the natural world , and humans possess a remarkable sensitivity to it . While humans effortlessly perceive motion , training a model to learn realistic scene motion presents substantial challenges due to the intricacies involved in measuring and capturing physical properties on a large scale . Fortunately , recent advancements in generative models , particularly conditional diffusion models , have ushered in a new era of modeling highly intricate and diverse distributions of real images based on text input . Moreover , recent research indicates that extending this modeling to other domains , such as videos and 3D geometry , holds significant potential for downstream applications . In a paper titled “ Generative Image Dynamics , ” a Google research team introduces an innovative approach to model natural oscillation dynamics using a single static image . This approach yields photo-realistic animations derived from a lone image , surpassing the performance of previous methods by a substantial margin . Furthermore , it opens doors to various other applications , such as the creation of interactive animations . This model is trained on automatically extracted motion trajectories from a large collection of real video sequences . Conditioned on an input image , the trained model predicts a neural stochastic motion texture : a set of coefficients of a motion basis that characterize each pixel ’ s trajectory into the future . The core of this model ’ s training process lies in automatically extracted motion trajectories from an extensive collection of real video sequences . Given an input image , the trained model predicts a neural stochastic motion texture , which consists of coefficients representing the motion basis for each pixel ’ s trajectory into the future . This prediction occurs via a diffusion model , which generates coefficients one frequency at a time while coordinating these predictions across frequency bands . The resulting frequency-space textures can then be converted into dense , long-range pixel motion trajectories . Together with an image-based rendering diffusion model , these trajectories can be employed to synthesize future frames , effectively transforming static images into lifelike animations . In contrast to prior models relying solely on raw RGB pixel data , this motion-based representation captures a more fundamental , lower-dimensional underlying structure that efficiently accounts for variations in pixel values . Consequently , the proposed approach results in more coherent , long-term generation and affords finer control over animations when compared to earlier methods that perform image animation through raw video synthesis . In their empirical study , the research team rigorously compared their approach against several recent single-image animation and video prediction methods . The results clearly demonstrate that their proposed approach beats prior single-image animation benchmarks in terms of both image and video synthesis quality . Overall , this work represents a highly promising breakthrough , capable of generating photo-realistic animations from a single static image while significantly surpassing the performance of previous baseline methods . The paper Generative Image Dynamics on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['motion', 'conspicuous', 'visual', 'cue', 'natural', 'world', 'human', 'possess', 'remarkable', 'sensitivity', 'human', 'effortlessly', 'perceive', 'motion', 'train', 'model', 'learn', 'realistic', 'scene', 'motion', 'present', 'substantial', 'challenge', 'intricacy', 'involve', 'measure', 'capture', 'physical', 'property', 'large', 'scale', 'fortunately', 'recent', 'advancement', 'generative', 'model', 'particularly', 'conditional', 'diffusion', 'model', 'usher', 'new', 'era', 'model', 'highly', 'intricate', 'diverse', 'distribution', 'real', 'image', 'base', 'text', 'input', 'moreover', 'recent', 'research', 'indicate', 'extend', 'modeling', 'domain', 'video', '3d', 'geometry', 'hold', 'significant', 'potential', 'downstream', 'application', 'paper', 'title', 'generative', 'image', 'dynamic', 'research', 'team', 'introduce', 'innovative', 'approach', 'model', 'natural', 'oscillation', 'dynamic', 'use', 'single', 'static', 'image', 'approach', 'yield', 'photorealistic', 'animation', 'derive', 'lone', 'image', 'surpass', 'performance', 'previous', 'method', 'substantial', 'margin', 'furthermore', 'open', 'door', 'various', 'application', 'creation', 'interactive', 'animation', 'model', 'train', 'automatically', 'extract', 'motion', 'trajectory', 'large', 'collection', 'real', 'video', 'sequence', 'condition', 'input', 'image', 'train', 'model', 'predict', 'neural', 'stochastic', 'motion', 'texture', 'set', 'coefficient', 'motion', 'basis', 'trajectory', 'future', 'core', 'model', 'training', 'process', 'lie', 'automatically', 'extract', 'motion', 'trajectory', 'extensive', 'collection', 'real', 'video', 'sequence', 'give', 'input', 'image', 'train', 'model', 'predict', 'neural', 'stochastic', 'motion', 'texture', 'consist', 'coefficient', 'represent', 'motion', 'basis', 'trajectory', 'future', 'prediction', 'occur', 'diffusion', 'model', 'generate', 'coefficient', 'frequency', 'time', 'coordinate', 'prediction', 'frequency', 'band', 'result', 'frequencyspace', 'texture', 'convert', 'dense', 'longrange', 'pixel', 'motion', 'trajectory', 'together', 'imagebase', 'render', 'diffusion', 'model', 'trajectory', 'employ', 'synthesize', 'future', 'frame', 'effectively', 'transform', 'static', 'image', 'lifelike', 'animation', 'contrast', 'prior', 'model', 'rely', 'solely', 'raw', 'motionbased', 'representation', 'capture', 'fundamental', 'lowerdimensional', 'underlying', 'structure', 'efficiently', 'account', 'variation', 'pixel', 'value', 'consequently', 'propose', 'approach', 'result', 'coherent', 'longterm', 'generation', 'afford', 'fine', 'control', 'animation', 'compare', 'early', 'method', 'perform', 'image', 'animation', 'raw', 'video', 'synthesis', 'empirical', 'study', 'research', 'team', 'rigorously', 'compare', 'approach', 'several', 'recent', 'singleimage', 'animation', 'video', 'prediction', 'method', 'result', 'clearly', 'demonstrate', 'propose', 'approach', 'beat', 'prior', 'singleimage', 'animation', 'benchmark', 'term', 'image', 'video', 'synthesis', 'quality', 'overall', 'work', 'represent', 'highly', 'promising', 'breakthrough', 'capable', 'generate', 'photorealistic', 'animation', 'single', 'static', 'image', 'significantly', 'surpass', 'performance', 'previous', 'baseline', 'method', 'paper', 'generative', 'image', 'dynamic', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
In a paper titled “Generative Image Dynamics,” a Google research team introduces an innovative approach to model natural oscillation dynamics using a single static image. This approach yields photo-realistic animations derived from a lone image, surpassing the performance of previous methods by a substantial margin.

 "
Unveiling the Enigma: Meta AI & UPC Decodes the Inner Workings of Large Scale Language Models,https://syncedreview.com/2023/09/20/unveiling-the-enigma-meta-ai-upc-decodes-the-inner-workings-of-large-scale-language-models/,2023-09-20,"Recent developments in large-scale language models have showcased their remarkable ability to tackle a wide array of tasks using a single model. However, a significant challenge lies in comprehending their internal workings, particularly as scaling these models up also raises the bar for interpretability. In a new paper titled “Neurons in Large Language Models: Dead, N-gram, Positional,” a research team from Meta AI and the Universitat Politècnica de Catalunya embarks on a comprehensive analysis of a family of Open Pre-trained Transformer Language Models (OPT) with parameters ranging up to 66 billion. Their goal is to shed light on how the feed-forward network (FFN) layers function within these models. The team places particular emphasis on the neurons housed within the FFNs. They contend that FFN neurons are more likely to represent meaningful features. The elementwise nonlinearity within these neurons disrupts rotational invariance, prompting features to align with the basis dimensions. Essentially, when an FFN neuron is activated, it updates the residual stream by extracting the corresponding row from the second FFN layer. Conversely, when it remains inactive, it has no impact on the residual stream. Armed with these insights, the researchers can decipher the functions of these FFN neurons by understanding when they activate and interpreting the associated updates made to the residual stream. Their initial observations reveal that a significant portion of neurons never activate across diverse datasets. An analysis of neuron activation frequencies underscores the substantial prevalence of dormant neurons. For instance, in the 66 billion parameter model, some layers exhibit a proportion of dead neurons exceeding 70%. Subsequently, delving deeper into the patterns embedded in the lower half of the models, the researchers investigate how neuron activations correlate with input n-grams. Their findings indicate that in larger models, neurons are covered by fewer n-grams, aligning with the hypothesis that the model allocates discreet shallow patterns to specifically designated neurons. Furthermore, the researchers identify certain neurons responsible for encoding positional information, irrespective of textual patterns. This discovery suggests that FFN layers can be employed by the model in ways that extend beyond the conventional key-value memory perspective. Overall, by conducting analysis of whether an FFN neuron is activated or not on the OPT family of models ranging from 125m to 66b parameters, the team summarizes the discoveries of neurons that: The team asserts that this work represents the first instance of mechanisms specialized in removing information from the residual stream. They believe that their findings offer valuable insights into the inner workings of how large language models achieve their impressive capabilities. The paper Neurons in Large Language Models: Dead, N-gram, Positional on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Recent developments in large-scale language models have showcased their remarkable ability to tackle a wide array of tasks using a single model . However , a significant challenge lies in comprehending their internal workings , particularly as scaling these models up also raises the bar for interpretability . In a new paper titled “ Neurons in Large Language Models : Dead , N-gram , Positional , ” a research team from Meta AI and the Universitat Politècnica de Catalunya embarks on a comprehensive analysis of a family of Open Pre-trained Transformer Language Models ( OPT ) with parameters ranging up to 66 billion . Their goal is to shed light on how the feed-forward network ( FFN ) layers function within these models . The team places particular emphasis on the neurons housed within the FFNs . They contend that FFN neurons are more likely to represent meaningful features . The elementwise nonlinearity within these neurons disrupts rotational invariance , prompting features to align with the basis dimensions . Essentially , when an FFN neuron is activated , it updates the residual stream by extracting the corresponding row from the second FFN layer . Conversely , when it remains inactive , it has no impact on the residual stream . Armed with these insights , the researchers can decipher the functions of these FFN neurons by understanding when they activate and interpreting the associated updates made to the residual stream . Their initial observations reveal that a significant portion of neurons never activate across diverse datasets . An analysis of neuron activation frequencies underscores the substantial prevalence of dormant neurons . For instance , in the 66 billion parameter model , some layers exhibit a proportion of dead neurons exceeding 70 % . Subsequently , delving deeper into the patterns embedded in the lower half of the models , the researchers investigate how neuron activations correlate with input n-grams . Their findings indicate that in larger models , neurons are covered by fewer n-grams , aligning with the hypothesis that the model allocates discreet shallow patterns to specifically designated neurons . Furthermore , the researchers identify certain neurons responsible for encoding positional information , irrespective of textual patterns . This discovery suggests that FFN layers can be employed by the model in ways that extend beyond the conventional key-value memory perspective . Overall , by conducting analysis of whether an FFN neuron is activated or not on the OPT family of models ranging from 125m to 66b parameters , the team summarizes the discoveries of neurons that : The team asserts that this work represents the first instance of mechanisms specialized in removing information from the residual stream . They believe that their findings offer valuable insights into the inner workings of how large language models achieve their impressive capabilities . The paper Neurons in Large Language Models : Dead , N-gram , Positional on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['recent', 'development', 'largescale', 'language', 'model', 'showcase', 'remarkable', 'ability', 'tackle', 'wide', 'array', 'task', 'use', 'single', 'model', 'however', 'significant', 'challenge', 'lie', 'comprehend', 'internal', 'working', 'particularly', 'scale', 'model', 'also', 'raise', 'bar', 'interpretability', 'new', 'paper', 'title', 'neuron', 'large', 'language', 'model', 'dead', 'ngram', 'positional', 'research', 'team', 'meta', 'embark', 'comprehensive', 'analysis', 'family', 'pretraine', 'transformer', 'language', 'model', 'opt', 'parameter', 'range', 'goal', 'shed', 'light', 'feedforward', 'network', 'ffn', 'layer', 'function', 'model', 'team', 'place', 'particular', 'emphasis', 'neuron', 'house', 'ffns', 'contend', 'ffn', 'neuron', 'likely', 'represent', 'meaningful', 'feature', 'elementwise', 'nonlinearity', 'neuron', 'disrupt', 'rotational', 'invariance', 'prompt', 'feature', 'align', 'basis', 'dimension', 'essentially', 'ffn', 'activate', 'update', 'residual', 'stream', 'extract', 'correspond', 'row', 'second', 'ffn', 'layer', 'conversely', 'remain', 'inactive', 'impact', 'residual', 'stream', 'armed', 'insight', 'researcher', 'decipher', 'function', 'ffn', 'neuron', 'understand', 'activate', 'interpret', 'associate', 'update', 'make', 'residual', 'stream', 'initial', 'observation', 'reveal', 'significant', 'portion', 'neuron', 'never', 'activate', 'diverse', 'dataset', 'analysis', 'neuron', 'activation', 'frequency', 'underscore', 'substantial', 'prevalence', 'dormant', 'neuron', 'instance', 'parameter', 'model', 'layer', 'exhibit', 'proportion', 'dead', 'neuron', 'exceed', 'subsequently', 'delve', 'deeply', 'pattern', 'embed', 'low', 'half', 'model', 'researcher', 'investigate', 'neuron', 'activation', 'correlate', 'input', 'ngram', 'finding', 'indicate', 'large', 'model', 'neuron', 'cover', 'ngram', 'align', 'hypothesis', 'model', 'allocate', 'discreet', 'shallow', 'pattern', 'specifically', 'designate', 'neuron', 'furthermore', 'researcher', 'identify', 'certain', 'neuron', 'responsible', 'encode', 'positional', 'information', 'irrespective', 'textual', 'pattern', 'discovery', 'suggest', 'ffn', 'layer', 'employ', 'model', 'way', 'extend', 'conventional', 'perspective', 'overall', 'conduct', 'analysis', 'ffn', 'neuron', 'activate', 'family', 'model', 'range', '66b', 'parameter', 'team', 'summarize', 'discovery', 'neuron', 'team', 'assert', 'work', 'represent', 'first', 'instance', 'mechanism', 'specialize', 'remove', 'information', 'residual', 'stream', 'believe', 'finding', 'offer', 'valuable', 'insight', 'inner', 'working', 'large', 'language', 'model', 'achieve', 'impressive', 'capability', 'paper', 'neuron', 'large', 'language', 'model', 'dead', 'ngram', 'positional', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
In a new paper Neurons in Large Language Models: Dead, N-gram, Positional, a research team from Meta AI and Universitat Politècnica de Catalunya  conducts comprehensive analysis of a family of Open Pre-trained Transformer Language Models (OPT) up to 66b parameters to provide insights of how feed-forward network (FFN) layers act.
"
Revolutionizing Autonomous Agents: AGENTS Framework Puts Power in Your Hands,https://syncedreview.com/2023/09/18/revolutionizing-autonomous-agents-agents-framework-puts-power-in-your-hands/,2023-09-18,"In recent years, the rapid progress of Large Language Models (LLMs) has showcased their potential in creating autonomous agents capable of tackling complex tasks and engaging with the world, humans, and fellow agents through a profound understanding of their surroundings. However, despite this promising trajectory, the creation of such agents remains a significant challenge for practitioners, including those with substantial experience in the field. Designing, fine-tuning, and developing new agents demands a considerable amount of effort and expertise. In response to this challenge, a collaborative research team comprising AIWaves Inc., Zhejiang University, and ETH Zürich has introduced “AGENTS,” an open-source framework aimed at empowering individuals, even those without specialized knowledge, to develop and deploy cutting-edge autonomous language agents with minimal coding requirements. The underlying philosophy of the AGENTS framework is to provide user-friendly tools for customizing, fine-tuning, and deploying language agents, making the process accessible even to beginners while retaining flexibility for developers and researchers. The AGENTS framework consists of three primary classes: Agent, Environment, and Standard Operating Procedure (SOP). The former two classes can be initialized using straightforward configuration files filled with plain text. These configuration files serve to define fundamental elements and modularize complex prompts, significantly reducing the effort required from users. The SOP class is pivotal, comprising a graphical representation of an agent’s various states during task execution. It outlines different scenarios an agent may encounter and employs an LLM-based control function to dictate transitions between states and guide the agent’s subsequent actions. SOPs can be generated by an LLM and further customized and fine-tuned by users to suit their specific needs. Additionally, AGENTS offers core features that include tool usage, long-short term memory integration, and multi-agent communication. Notably, it introduces human-agent interaction and controllability for the first time, marking a significant milestone in the development of autonomous agents. In conclusion, the AGENTS framework stands as a testament to its ability to simplify the process of building personalized autonomous language agents for both developers and non-technical users. Its user-friendly approach and robust feature set make it a valuable tool in the advancement of autonomous agent technology. All demos are available at http://www.aiwaves-agents.com/. The paper Agents: An Open-source Framework for Autonomous Language Agents on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","In recent years , the rapid progress of Large Language Models ( LLMs ) has showcased their potential in creating autonomous agents capable of tackling complex tasks and engaging with the world , humans , and fellow agents through a profound understanding of their surroundings . However , despite this promising trajectory , the creation of such agents remains a significant challenge for practitioners , including those with substantial experience in the field . Designing , fine-tuning , and developing new agents demands a considerable amount of effort and expertise . In response to this challenge , a collaborative research team comprising AIWaves Inc. , Zhejiang University , and ETH Zürich has introduced “ AGENTS , ” an open-source framework aimed at empowering individuals , even those without specialized knowledge , to develop and deploy cutting-edge autonomous language agents with minimal coding requirements . The underlying philosophy of the AGENTS framework is to provide user-friendly tools for customizing , fine-tuning , and deploying language agents , making the process accessible even to beginners while retaining flexibility for developers and researchers . The AGENTS framework consists of three primary classes : Agent , Environment , and Standard Operating Procedure ( SOP ) . The former two classes can be initialized using straightforward configuration files filled with plain text . These configuration files serve to define fundamental elements and modularize complex prompts , significantly reducing the effort required from users . The SOP class is pivotal , comprising a graphical representation of an agent ’ s various states during task execution . It outlines different scenarios an agent may encounter and employs an LLM-based control function to dictate transitions between states and guide the agent ’ s subsequent actions . SOPs can be generated by an LLM and further customized and fine-tuned by users to suit their specific needs . Additionally , AGENTS offers core features that include tool usage , long-short term memory integration , and multi-agent communication . Notably , it introduces human-agent interaction and controllability for the first time , marking a significant milestone in the development of autonomous agents . In conclusion , the AGENTS framework stands as a testament to its ability to simplify the process of building personalized autonomous language agents for both developers and non-technical users . Its user-friendly approach and robust feature set make it a valuable tool in the advancement of autonomous agent technology . All demos are available at http : //www.aiwaves-agents.com/ . The paper Agents : An Open-source Framework for Autonomous Language Agents on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['recent', 'year', 'rapid', 'progress', 'large', 'language', 'model', 'llm', 'showcase', 'potential', 'create', 'autonomous', 'agent', 'capable', 'tackle', 'complex', 'task', 'engage', 'world', 'human', 'fellow', 'agent', 'profound', 'understanding', 'surrounding', 'however', 'promising', 'trajectory', 'creation', 'agent', 'remain', 'significant', 'challenge', 'practitioner', 'include', 'substantial', 'experience', 'field', 'design', 'finetune', 'develop', 'new', 'agent', 'demand', 'considerable', 'amount', 'effort', 'expertise', 'response', 'challenge', 'collaborative', 'research', 'team', 'comprise', 'introduce', 'agent', 'opensource', 'framework', 'aim', 'empower', 'individual', 'even', 'specialized', 'knowledge', 'develop', 'deploy', 'cuttingedge', 'autonomous', 'language', 'agent', 'minimal', 'code', 'requirement', 'underlie', 'philosophy', 'agent', 'framework', 'provide', 'userfriendly', 'tool', 'customize', 'finetune', 'deploy', 'language', 'agent', 'make', 'process', 'accessible', 'even', 'beginner', 'retain', 'flexibility', 'developer', 'researcher', 'agent', 'framework', 'consist', 'primary', 'class', 'agent', 'environment', 'standard', 'operate', 'procedure', 'sop', 'former', 'class', 'initialize', 'use', 'straightforward', 'configuration', 'file', 'fill', 'plain', 'text', 'configuration', 'file', 'serve', 'define', 'fundamental', 'element', 'modularize', 'complex', 'prompt', 'significantly', 'reduce', 'effort', 'require', 'user', 'class', 'pivotal', 'comprise', 'graphical', 'representation', 'agent', 'various', 'state', 'task', 'execution', 'outline', 'different', 'scenario', 'agent', 'encounter', 'employ', 'llmbased', 'control', 'function', 'dictate', 'transition', 'state', 'guide', 'agent', 'subsequent', 'action', 'sop', 'generate', 'llm', 'far', 'customize', 'finetune', 'user', 'suit', 'specific', 'need', 'additionally', 'agent', 'offer', 'core', 'feature', 'include', 'tool', 'usage', 'longshort', 'term', 'memory', 'integration', 'multiagent', 'communication', 'notably', 'introduce', 'humanagent', 'interaction', 'controllability', 'first', 'time', 'mark', 'significant', 'milestone', 'development', 'autonomous', 'agent', 'conclusion', 'agent', 'framework', 'stand', 'testament', 'ability', 'simplify', 'process', 'build', 'personalize', 'autonomous', 'language', 'agent', 'developer', 'nontechnical', 'user', 'userfriendly', 'approach', 'robust', 'feature', 'set', 'make', 'valuable', 'tool', 'advancement', 'autonomous', 'agent', 'technology', 'demo', 'available', 'http', 'wwwaiwavesagentscom', 'paper', 'agent', 'opensource', 'framework', 'autonomous', 'language', 'agent', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
In a new paper Agents: An Open-source Framework for Autonomous Language Agents, a research team from AIWaves Inc., Zhejiang University and ETH Zürich releases AGENTS, an open-source framework that enables non-specialists for developing and deploying state-of-the-art autonomous language agents with minimal coding work.
"
DeepMind Decodes the Puzzle of ‘ Grokking ’ In Neural Network Generalization Through Circuit Efficiency,https://syncedreview.com/2023/09/15/deepmind-decodes-the-puzzle-of-grokking-in-neural-network-generalization-through-circuit-efficiency/,2023-09-15,"One of the intriguing puzzles within the realm of neural network generalization is a phenomenon known as “grokking.” It involves a neural network achieving perfect training accuracy but displaying poor generalization capabilities. Interestingly, it has been observed that further training can transform a network experiencing this phenomenon into one that exhibits perfect generalization—a result that challenges our conventional understanding. Typically, we assume that a neural network, once its training loss converges to a low value, should not undergo significant changes. In a recently published paper titled “Explaining Grokking through Circuit Efficiency,” a research team from DeepMind successfully unravels the mystery behind grokking through their circuit efficiency theory. This breakthrough sheds light on why the generalizing solution takes longer to learn compared to memorization. Additionally, the team introduces two novel concepts: “ungrokking” and “semi-grokking,” which contribute to a deeper understanding of neural network generalization. The team summarizes their main contributions as follows: The team’s explanation of grokking revolves around three key factors: the generalizing circuit, efficiency, and the rate of learning. They begin by highlighting the existence of two categories of circuits that yield favorable training outcomes: Building upon this theory and considering behaviors around the critical dataset size, the team makes two groundbreaking predictions, hitherto unreported in previous research: Finally, through rigorous experimentation, the team successfully validates their theory. This achievement not only solves the enigma of grokking but also underscores the potential for a broader comprehension of deep learning by examining it through the lens of circuit efficiency. The paper Explaining grokking through circuit efficiency on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","One of the intriguing puzzles within the realm of neural network generalization is a phenomenon known as “ grokking. ” It involves a neural network achieving perfect training accuracy but displaying poor generalization capabilities . Interestingly , it has been observed that further training can transform a network experiencing this phenomenon into one that exhibits perfect generalization—a result that challenges our conventional understanding . Typically , we assume that a neural network , once its training loss converges to a low value , should not undergo significant changes . In a recently published paper titled “ Explaining Grokking through Circuit Efficiency , ” a research team from DeepMind successfully unravels the mystery behind grokking through their circuit efficiency theory . This breakthrough sheds light on why the generalizing solution takes longer to learn compared to memorization . Additionally , the team introduces two novel concepts : “ ungrokking ” and “ semi-grokking , ” which contribute to a deeper understanding of neural network generalization . The team summarizes their main contributions as follows : The team ’ s explanation of grokking revolves around three key factors : the generalizing circuit , efficiency , and the rate of learning . They begin by highlighting the existence of two categories of circuits that yield favorable training outcomes : Building upon this theory and considering behaviors around the critical dataset size , the team makes two groundbreaking predictions , hitherto unreported in previous research : Finally , through rigorous experimentation , the team successfully validates their theory . This achievement not only solves the enigma of grokking but also underscores the potential for a broader comprehension of deep learning by examining it through the lens of circuit efficiency . The paper Explaining grokking through circuit efficiency on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['intriguing', 'puzzle', 'realm', 'neural', 'network', 'generalization', 'phenomenon', 'know', 'grokke', 'involve', 'neural', 'network', 'achieve', 'perfect', 'training', 'accuracy', 'display', 'poor', 'generalization', 'capability', 'interestingly', 'observe', 'training', 'transform', 'network', 'experience', 'phenomenon', 'exhibit', 'perfect', 'generalization', 'result', 'challenge', 'conventional', 'understanding', 'typically', 'assume', 'neural', 'network', 'training', 'loss', 'converge', 'low', 'value', 'undergo', 'significant', 'change', 'recently', 'publish', 'paper', 'title', 'explain', 'grokke', 'circuit', 'efficiency', 'research', 'team', 'successfully', 'unravel', 'mystery', 'grokke', 'circuit', 'efficiency', 'theory', 'breakthrough', 'shed', 'light', 'generalizing', 'solution', 'take', 'long', 'learn', 'compare', 'memorization', 'additionally', 'team', 'introduce', 'novel', 'concept', 'ungrokke', 'semigrokke', 'contribute', 'deep', 'understanding', 'neural', 'network', 'generalization', 'team', 'summarize', 'main', 'contribution', 'follow', 'team', 'explanation', 'grokking', 'revolve', 'key', 'factor', 'generalize', 'circuit', 'efficiency', 'rate', 'learn', 'begin', 'highlight', 'existence', 'category', 'circuit', 'yield', 'favorable', 'training', 'outcome', 'build', 'theory', 'consider', 'behavior', 'critical', 'dataset', 'size', 'team', 'make', 'groundbreaking', 'prediction', 'hitherto', 'unreporte', 'previous', 'research', 'finally', 'rigorous', 'experimentation', 'team', 'successfully', 'validate', 'theory', 'achievement', 'solve', 'enigma', 'grokking', 'also', 'underscore', 'potential', 'broad', 'comprehension', 'deep', 'learning', 'examine', 'lens', 'circuit', 'efficiency', 'paper', 'explaining', 'grokke', 'circuit', 'efficiency', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
In a new paper Explaining grokking through circuit efficiency, a DeepMind research team solves the puzzle of the grokking through circuit efficiency theory, revealing that the generalizing solution is slower to learn then memorizing.
"
"Microsoft’s phi-1.5 Challenges LLMs Scaling Law, Showcases the Crucial Rule for ‘Textbook Quality’ Dataset",https://syncedreview.com/2023/09/14/microsofts-phi-1-5-challenges-llms-scaling-law-showcases-the-crucial-rule-for-textbook-quality-dataset/,2023-09-14,"Large Language Models (LLMs) have undoubtedly showcased remarkable performance in the field of natural language processing. However, beneath their accomplishments lies a profound and far-reaching impact on the economic landscape, and they have yet to fully redefine the artificial intelligence framework and even cognition itself. On the flip side, the enhancement of LLMs appears to be predominantly driven by their sheer scale. Many of today’s state-of-the-art models approach the staggering realm of trillions of parameters and tokens, demanding substantial resources for training, deployment, and maintenance. This exponential growth in scale inevitably leads to soaring costs. Hence, a pressing question emerges: “How compact can an LLM be while retaining its capabilities?” In a groundbreaking paper titled “Textbooks Are All You Need II: phi-1.5 Technical Report,” a dedicated research team at Microsoft embarks on a quest to explore this inquiry. They introduce phi-1.5, a 1.3 billion parameter model trained on a vast dataset of 30 billion tokens, remarkably delivering performance that rivals models five times its size. Moreover, it outperforms most non-frontier LLMs in tackling intricate reasoning tasks. This endeavor builds upon the foundation laid by phi-1, a pioneering Transformer-based model introduced by Microsoft in June of this year. With a mere 1.3 billion parameters, phi-1 managed to surpass the formidable GPT-3.5, thanks to its utilization of high-quality “textbook” training data. The phi-1.5 model preserves the exact architecture of phi-1, boasting 24 layers, 32 heads, and each head with a dimension of 64. Additional enhancements include the use of rotary embedding, flash-attention for speech training, and the codegen-mono tokenizer. The training data for phi-1.5 comprises a fusion of phi-1’s training data and synthetic “textbook-like” data, meticulously crafted to enhance common-sense reasoning and general knowledge acquisition. The researchers initiated phi-1.5 from random initialization with a constant learning rate of 2e−4, coupled with a weight decay of 0.1. The training process harnessed the Adam optimizer and fp16 with DeepSpeed ZeRO Stage 2. In their empirical investigation, the research team subjected phi-1.5 to rigorous evaluation on benchmark natural language tasks, encompassing common sense reasoning, language skills, and multi-step reasoning. The results unequivocally demonstrate that phi-1.5 achieves performance on par with significantly larger models, even surpassing them in the realm of complex reasoning tasks. Collectively, this groundbreaking work challenges the prevailing belief that the prowess of LLMs hinges primarily on their scale. Instead, it underscores the pivotal role played by data quality, suggesting that it may hold the key to unlocking the true potential of these transformative models. The paper Textbooks Are All You Need II: phi-1.5 technical report on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Large Language Models ( LLMs ) have undoubtedly showcased remarkable performance in the field of natural language processing . However , beneath their accomplishments lies a profound and far-reaching impact on the economic landscape , and they have yet to fully redefine the artificial intelligence framework and even cognition itself . On the flip side , the enhancement of LLMs appears to be predominantly driven by their sheer scale . Many of today ’ s state-of-the-art models approach the staggering realm of trillions of parameters and tokens , demanding substantial resources for training , deployment , and maintenance . This exponential growth in scale inevitably leads to soaring costs . Hence , a pressing question emerges : “ How compact can an LLM be while retaining its capabilities ? ” In a groundbreaking paper titled “ Textbooks Are All You Need II : phi-1.5 Technical Report , ” a dedicated research team at Microsoft embarks on a quest to explore this inquiry . They introduce phi-1.5 , a 1.3 billion parameter model trained on a vast dataset of 30 billion tokens , remarkably delivering performance that rivals models five times its size . Moreover , it outperforms most non-frontier LLMs in tackling intricate reasoning tasks . This endeavor builds upon the foundation laid by phi-1 , a pioneering Transformer-based model introduced by Microsoft in June of this year . With a mere 1.3 billion parameters , phi-1 managed to surpass the formidable GPT-3.5 , thanks to its utilization of high-quality “ textbook ” training data . The phi-1.5 model preserves the exact architecture of phi-1 , boasting 24 layers , 32 heads , and each head with a dimension of 64 . Additional enhancements include the use of rotary embedding , flash-attention for speech training , and the codegen-mono tokenizer . The training data for phi-1.5 comprises a fusion of phi-1 ’ s training data and synthetic “ textbook-like ” data , meticulously crafted to enhance common-sense reasoning and general knowledge acquisition . The researchers initiated phi-1.5 from random initialization with a constant learning rate of 2e−4 , coupled with a weight decay of 0.1 . The training process harnessed the Adam optimizer and fp16 with DeepSpeed ZeRO Stage 2 . In their empirical investigation , the research team subjected phi-1.5 to rigorous evaluation on benchmark natural language tasks , encompassing common sense reasoning , language skills , and multi-step reasoning . The results unequivocally demonstrate that phi-1.5 achieves performance on par with significantly larger models , even surpassing them in the realm of complex reasoning tasks . Collectively , this groundbreaking work challenges the prevailing belief that the prowess of LLMs hinges primarily on their scale . Instead , it underscores the pivotal role played by data quality , suggesting that it may hold the key to unlocking the true potential of these transformative models . The paper Textbooks Are All You Need II : phi-1.5 technical report on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['large', 'language', 'model', 'llm', 'undoubtedly', 'showcase', 'remarkable', 'performance', 'field', 'natural', 'language', 'processing', 'however', 'accomplishment', 'lie', 'profound', 'farreache', 'impact', 'economic', 'landscape', 'yet', 'fully', 'redefine', 'artificial', 'intelligence', 'framework', 'even', 'cognition', 'flip', 'side', 'enhancement', 'llm', 'appear', 'predominantly', 'drive', 'sheer', 'scale', 'many', 'today', 'stateoftheart', 'model', 'approach', 'staggering', 'realm', 'trillion', 'parameter', 'token', 'demand', 'substantial', 'resource', 'training', 'deployment', 'maintenance', 'exponential', 'growth', 'scale', 'inevitably', 'lead', 'soar', 'cost', 'hence', 'press', 'question', 'emerge', 'compact', 'llm', 'retain', 'capability', 'groundbreaking', 'paper', 'title', 'textbook', 'need', 'phi15', 'technical', 'report', 'dedicated', 'research', 'team', 'embark', 'quest', 'explore', 'inquiry', 'introduce', 'phi15', 'parameter', 'model', 'train', 'vast', 'dataset', 'token', 'remarkably', 'deliver', 'performance', 'rival', 'model', 'time', 'size', 'moreover', 'outperform', 'nonfronti', 'llm', 'tackle', 'intricate', 'reasoning', 'task', 'endeavor', 'build', 'foundation', 'lay', 'pioneer', 'transformerbased', 'model', 'introduce', 'year', 'mere', 'parameter', 'manage', 'surpass', 'formidable', 'gpt35', 'thank', 'utilization', 'highquality', 'textbook', 'training', 'datum', 'phi15', 'model', 'preserve', 'exact', 'architecture', 'phi1', 'boast', 'layer', 'head', 'head', 'dimension', 'additional', 'enhancement', 'include', 'use', 'rotary', 'embed', 'flashattention', 'speech', 'training', 'codegenmono', 'tokenizer', 'training', 'datum', 'phi15', 'comprise', 'fusion', 'training', 'datum', 'synthetic', 'textbooklike', 'datum', 'meticulously', 'craft', 'enhance', 'commonsense', 'reasoning', 'general', 'knowledge', 'acquisition', 'researcher', 'initiate', 'phi15', 'random', 'initialization', 'constant', 'learning', 'rate', 'couple', 'weight', 'decay', 'training', 'process', 'harness', 'optimizer', 'deepspeed', 'stage', 'empirical', 'investigation', 'research', 'team', 'subject', 'phi15', 'rigorous', 'evaluation', 'benchmark', 'natural', 'language', 'task', 'encompass', 'common', 'sense', 'reason', 'language', 'skill', 'multistep', 'reason', 'result', 'unequivocally', 'demonstrate', 'achieve', 'performance', 'par', 'significantly', 'large', 'model', 'even', 'surpass', 'realm', 'complex', 'reasoning', 'task', 'collectively', 'groundbreaking', 'work', 'challenge', 'prevail', 'belief', 'prowess', 'llm', 'hinge', 'primarily', 'scale', 'instead', 'underscore', 'pivotal', 'role', 'play', 'data', 'quality', 'suggest', 'hold', 'key', 'unlock', 'true', 'potential', 'transformative', 'model', 'paper', 'textbook', 'need', 'phi15', 'technical', 'report', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
A Microsoft research team introduce phi-1.5, a 1.3 billion parameter model trained on a vast dataset of 30 billion tokens, remarkably delivering performance that rivals models five times its size. Moreover, it outperforms most non-frontier LLMs in tackling intricate reasoning tasks.
"
Unlocking the Power of Visual Modeling: Microsoft’s Sparse MoEs Redefine Efficiency and Excellence,https://syncedreview.com/2023/09/12/unlocking-the-power-of-visual-modeling-microsofts-sparse-moes-redefine-efficiency-and-excellence/,2023-09-12,"In recent years, sparsely-gated Mixture-of-Experts models (sparse MoEs) have garnered substantial attention and acclaim for their remarkable ability to decouple model size from inference efficiency. This enables unprecedented scalability, leading to significant successes across various domains, including natural language processing, computer vision, and speech recognition. Sparse MoEs offer the tantalizing prospect of augmenting model capabilities while simultaneously mitigating computational costs. This makes them an enticing option for integration with Transformers, the prevailing architectural choice for large-scale visual modeling, albeit constrained by their resource-intensive nature. In pursuit of this endeavor, an Apple research team has introduced the concept of sparse Mobile Vision MoEs (V-MoEs) in their paper titled “Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts.” These V-MoEs represent a streamlined and mobile-friendly Mixture-of-Experts architecture that efficiently downscales Vision Transformers (ViTs) while preserving impressive model performance. The team summarizes their main contributions as follows: The core innovation of the proposed sparse Mobile V-MoE lies in its utilization of a single per-image router, as opposed to per-patch routing. The conventional per-patch routing typically necessitates the activation of a larger number of experts for each image. In contrast, the per-image router processes entire images as inputs, thereby reducing the number of activated experts per image. And the architecture comprises ViT layers followed by MoE-ViT layers. It’s important to note that, unlike ViT layers, the MoE-ViT layers feature a distinct Multi-Layer Perceptron (MLP) for each expert, while the remaining portions of the layers are shared across all experts. During the training phase, the researchers adopted a novel approach. They initially trained a dense baseline model and subsequently computed the model’s confusion matrix using a held-out validation set from the training dataset. This confusion matrix served as the foundation for creating a confusion graph, which was further subjected to a graph clustering algorithm. The outcome of this process was a super-class division. This strategy holds promise for enhancing the performance of highly perplexing classes, as different MoE experts specialize in distinct semantic data clusters. The research team applied this training approach in their experiments on ImageNet-1k. The results demonstrated that MoE-ViT offers an improved trade-off between performance and efficiency when compared to dense ViT. This underscores its potential in resource-constrained applications. In summary, sparse Mobile Vision MoEs represent a groundbreaking advancement in the realm of Vision Transformers, promising to revolutionize the field by enabling efficient scalability without sacrificing performance. The innovative training strategy and promising results on ImageNet-1k highlight the considerable potential of MoE-ViT in resource-constrained scenarios. The paper Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","In recent years , sparsely-gated Mixture-of-Experts models ( sparse MoEs ) have garnered substantial attention and acclaim for their remarkable ability to decouple model size from inference efficiency . This enables unprecedented scalability , leading to significant successes across various domains , including natural language processing , computer vision , and speech recognition . Sparse MoEs offer the tantalizing prospect of augmenting model capabilities while simultaneously mitigating computational costs . This makes them an enticing option for integration with Transformers , the prevailing architectural choice for large-scale visual modeling , albeit constrained by their resource-intensive nature . In pursuit of this endeavor , an Apple research team has introduced the concept of sparse Mobile Vision MoEs ( V-MoEs ) in their paper titled “ Mobile V-MoEs : Scaling Down Vision Transformers via Sparse Mixture-of-Experts. ” These V-MoEs represent a streamlined and mobile-friendly Mixture-of-Experts architecture that efficiently downscales Vision Transformers ( ViTs ) while preserving impressive model performance . The team summarizes their main contributions as follows : The core innovation of the proposed sparse Mobile V-MoE lies in its utilization of a single per-image router , as opposed to per-patch routing . The conventional per-patch routing typically necessitates the activation of a larger number of experts for each image . In contrast , the per-image router processes entire images as inputs , thereby reducing the number of activated experts per image . And the architecture comprises ViT layers followed by MoE-ViT layers . It ’ s important to note that , unlike ViT layers , the MoE-ViT layers feature a distinct Multi-Layer Perceptron ( MLP ) for each expert , while the remaining portions of the layers are shared across all experts . During the training phase , the researchers adopted a novel approach . They initially trained a dense baseline model and subsequently computed the model ’ s confusion matrix using a held-out validation set from the training dataset . This confusion matrix served as the foundation for creating a confusion graph , which was further subjected to a graph clustering algorithm . The outcome of this process was a super-class division . This strategy holds promise for enhancing the performance of highly perplexing classes , as different MoE experts specialize in distinct semantic data clusters . The research team applied this training approach in their experiments on ImageNet-1k . The results demonstrated that MoE-ViT offers an improved trade-off between performance and efficiency when compared to dense ViT . This underscores its potential in resource-constrained applications . In summary , sparse Mobile Vision MoEs represent a groundbreaking advancement in the realm of Vision Transformers , promising to revolutionize the field by enabling efficient scalability without sacrificing performance . The innovative training strategy and promising results on ImageNet-1k highlight the considerable potential of MoE-ViT in resource-constrained scenarios . The paper Mobile V-MoEs : Scaling Down Vision Transformers via Sparse Mixture-of-Experts on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['recent', 'year', 'sparselygate', 'mixtureofexpert', 'model', 'sparse', 'moe', 'garner', 'substantial', 'attention', 'acclaim', 'remarkable', 'ability', 'decouple', 'model', 'size', 'inference', 'efficiency', 'enable', 'unprecedented', 'scalability', 'lead', 'significant', 'success', 'various', 'domain', 'include', 'natural', 'language', 'processing', 'computer', 'vision', 'speech', 'sparse', 'moe', 'offer', 'tantalizing', 'prospect', 'augment', 'model', 'capability', 'simultaneously', 'mitigate', 'computational', 'cost', 'make', 'enticing', 'option', 'integration', 'transformer', 'prevail', 'architectural', 'choice', 'largescale', 'visual', 'modeling', 'constrain', 'resourceintensive', 'nature', 'pursuit', 'endeavor', 'apple', 'research', 'team', 'introduce', 'concept', 'sparse', 'mobile', 'vision', 'moe', 'vmoe', 'paper', 'title', 'mobile', 'vmoe', 'scale', 'vision', 'transformer', 'sparse', 'mixtureofexpert', 'vmoe', 'represent', 'streamlined', 'mobilefriendly', 'mixtureofexpert', 'architecture', 'efficiently', 'downscale', 'vision', 'transformer', 'vit', 'preserve', 'impressive', 'model', 'performance', 'team', 'summarize', 'main', 'contribution', 'follow', 'core', 'innovation', 'propose', 'sparse', 'mobile', 'vmoe', 'lie', 'utilization', 'single', 'perimage', 'router', 'oppose', 'perpatch', 'route', 'conventional', 'perpatch', 'route', 'typically', 'necessitate', 'activation', 'large', 'number', 'expert', 'image', 'contrast', 'perimage', 'router', 'process', 'entire', 'image', 'input', 'thereby', 'reduce', 'number', 'activate', 'expert', 'image', 'architecture', 'comprise', 'vit', 'layer', 'follow', 'layer', 'important', 'note', 'layer', 'moevit', 'layer', 'feature', 'distinct', 'multilayer', 'mlp', 'expert', 'remain', 'portion', 'layer', 'share', 'expert', 'training', 'phase', 'researcher', 'adopt', 'novel', 'approach', 'initially', 'train', 'dense', 'baseline', 'model', 'subsequently', 'compute', 'model', 'confusion', 'matrix', 'use', 'heldout', 'validation', 'set', 'training', 'dataset', 'confusion', 'matrix', 'serve', 'foundation', 'create', 'confusion', 'graph', 'far', 'subject', 'graph', 'cluster', 'outcome', 'process', 'superclass', 'division', 'strategy', 'hold', 'promise', 'enhance', 'performance', 'highly', 'perplexing', 'class', 'different', 'moe', 'expert', 'specialize', 'distinct', 'semantic', 'datum', 'cluster', 'research', 'team', 'apply', 'training', 'approach', 'experiment', 'result', 'demonstrate', 'moevit', 'offer', 'improved', 'tradeoff', 'performance', 'efficiency', 'compare', 'dense', 'vit', 'underscore', 'potential', 'resourceconstraine', 'application', 'summary', 'sparse', 'mobile', 'vision', 'moe', 'represent', 'groundbreaking', 'advancement', 'realm', 'vision', 'transformer', 'promise', 'revolutionize', 'field', 'enable', 'efficient', 'scalability', 'sacrifice', 'performance', 'innovative', 'training', 'strategy', 'promise', 'result', 'highlight', 'considerable', 'potential', 'resourceconstraine', 'scenario', 'paper', 'mobile', 'scale', 'vision', 'transformer', 'sparse', 'mixtureofexpert', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
An Apple research team introduces the concept of sparse Mobile Vision MoEs (V-MoEs), which represents a streamlined and mobile-friendly Mixture-of-Experts architecture that efficiently downscales Vision Transformers (ViTs) while preserving impressive model performance.
"
Revolutionizing Optimization: DeepMind Leverages Large Language Models as Intelligent Optimizers,https://syncedreview.com/2023/09/12/revolutionizing-optimization-deepmind-leverages-large-language-models-as-intelligent-optimizers/,2023-09-12,"Optimization plays a pivotal role in a diverse array of real-world applications. Nevertheless, traditional optimization algorithms often demand substantial manual intervention to tailor them to specific tasks, grappling with the intricacies posed by the decision space and performance landscape. To tackle this challenge head-on, a Google DeepMind research team has introduced a groundbreaking approach in their recent paper titled “Large Language Models as Optimizers.” This innovative method, known as Optimization by PROmpting (OPRO), harnesses the power of large language models (LLMs) as optimizers. These LLMs are capable of generating optimization solutions based on the natural language that describes the optimization task. The remarkable ability of LLMs to comprehend natural language opens up exciting possibilities for generating optimization solutions based on a problem’s verbal description. Instead of adhering to traditional approaches, which typically define optimization problems formally and employ programmed solvers to derive update steps, this research takes a distinctive path. Here, the researchers guide the optimization process by instructing the LLM to iteratively generate new solutions based on natural language descriptions and previously discovered solutions. To provide an overview of the OPRO framework, a meta-prompt is employed, containing both the description of the optimization problem and previously evaluated solutions. This meta-prompt serves as input, empowering the LLM to generate candidate solutions based on the provided information. Subsequently, these newly generated solutions are assessed and integrated into the meta-prompt for subsequent optimization iterations. This iterative optimization process persists until the LLM can no longer propose solutions with higher scores or reaches the maximum number of optimization steps. In essence, the ultimate objective is to formulate a prompt that maximizes task accuracy. In their empirical investigation, the research team evaluated the OPRO framework across various LLMs, including text-bison, Palm 2-L, gpt-3.5-turbo, and gpt-4. On small-scale traveling salesman problems, OPRO demonstrated performance on par with hand-crafted heuristic algorithms, surpassing human-designed prompts by a substantial margin on GSM8K and Big-Bench Hard, even achieving over a 50% improvement. The paper Large Language Models as Optimizers on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Optimization plays a pivotal role in a diverse array of real-world applications . Nevertheless , traditional optimization algorithms often demand substantial manual intervention to tailor them to specific tasks , grappling with the intricacies posed by the decision space and performance landscape . To tackle this challenge head-on , a Google DeepMind research team has introduced a groundbreaking approach in their recent paper titled “ Large Language Models as Optimizers. ” This innovative method , known as Optimization by PROmpting ( OPRO ) , harnesses the power of large language models ( LLMs ) as optimizers . These LLMs are capable of generating optimization solutions based on the natural language that describes the optimization task . The remarkable ability of LLMs to comprehend natural language opens up exciting possibilities for generating optimization solutions based on a problem ’ s verbal description . Instead of adhering to traditional approaches , which typically define optimization problems formally and employ programmed solvers to derive update steps , this research takes a distinctive path . Here , the researchers guide the optimization process by instructing the LLM to iteratively generate new solutions based on natural language descriptions and previously discovered solutions . To provide an overview of the OPRO framework , a meta-prompt is employed , containing both the description of the optimization problem and previously evaluated solutions . This meta-prompt serves as input , empowering the LLM to generate candidate solutions based on the provided information . Subsequently , these newly generated solutions are assessed and integrated into the meta-prompt for subsequent optimization iterations . This iterative optimization process persists until the LLM can no longer propose solutions with higher scores or reaches the maximum number of optimization steps . In essence , the ultimate objective is to formulate a prompt that maximizes task accuracy . In their empirical investigation , the research team evaluated the OPRO framework across various LLMs , including text-bison , Palm 2-L , gpt-3.5-turbo , and gpt-4 . On small-scale traveling salesman problems , OPRO demonstrated performance on par with hand-crafted heuristic algorithms , surpassing human-designed prompts by a substantial margin on GSM8K and Big-Bench Hard , even achieving over a 50 % improvement . The paper Large Language Models as Optimizers on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['optimization', 'play', 'pivotal', 'role', 'diverse', 'array', 'realworld', 'application', 'nevertheless', 'traditional', 'optimization', 'algorithm', 'often', 'demand', 'substantial', 'manual', 'intervention', 'tailor', 'specific', 'task', 'grappling', 'intricacy', 'pose', 'decision', 'space', 'performance', 'landscape', 'tackle', 'challenge', 'research', 'team', 'introduce', 'groundbreake', 'approach', 'recent', 'paper', 'title', 'large', 'language', 'model', 'optimizer', 'innovative', 'method', 'know', 'optimization', 'prompt', 'opro', 'harness', 'power', 'large', 'language', 'model', 'llm', 'optimizer', 'llm', 'capable', 'generate', 'optimization', 'solution', 'base', 'natural', 'language', 'describe', 'optimization', 'task', 'remarkable', 'ability', 'llm', 'comprehend', 'natural', 'language', 'open', 'exciting', 'possibility', 'generate', 'optimization', 'solution', 'base', 'problem', 'verbal', 'description', 'instead', 'adhere', 'traditional', 'approach', 'typically', 'define', 'optimization', 'problem', 'formally', 'employ', 'program', 'solver', 'derive', 'update', 'step', 'research', 'take', 'distinctive', 'path', 'researcher', 'guide', 'optimization', 'process', 'instruct', 'llm', 'iteratively', 'generate', 'new', 'solution', 'base', 'natural', 'language', 'description', 'previously', 'discover', 'solution', 'provide', 'overview', 'opro', 'framework', 'metaprompt', 'employ', 'contain', 'description', 'optimization', 'problem', 'previously', 'evaluate', 'solution', 'metaprompt', 'serve', 'input', 'empower', 'llm', 'generate', 'candidate', 'solution', 'base', 'provide', 'information', 'subsequently', 'newly', 'generate', 'solution', 'assess', 'integrate', 'metaprompt', 'subsequent', 'optimization', 'iteration', 'iterative', 'optimization', 'process', 'persist', 'llm', 'long', 'propose', 'solution', 'high', 'score', 'reach', 'maximum', 'number', 'optimization', 'step', 'essence', 'ultimate', 'objective', 'formulate', 'prompt', 'maximize', 'task', 'accuracy', 'empirical', 'investigation', 'research', 'team', 'evaluate', 'opro', 'framework', 'various', 'llm', 'include', 'textbison', 'palm', 'smallscale', 'travel', 'salesman', 'problem', 'opro', 'demonstrate', 'performance', 'par', 'handcraft', 'heuristic', 'algorithm', 'surpass', 'humandesigne', 'prompt', 'substantial', 'margin', 'gsm8k', 'bigbench', 'hard', 'even', 'achieve', 'improvement', 'paper', 'large', 'language', 'model', 'optimizer', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
In a new paper Large Language Models as Optimizers, a Google DeepMind research team introduces Optimization by PROmpting (OPRO), an effective method that leverages large language models (LLMs) as optimizers, which can generate optimization solutions conditioned on the natural language that describes the optimization task. 
"
Equall & Apple’s Revolutionizing Transformers: One Wide Feedforward for Unprecedented Efficiency and Accuracy,https://syncedreview.com/2023/09/08/equall-apples-revolutionizing-transformers-one-wide-feedforward-for-unprecedented-efficiency-and-accuracy/,2023-09-08,"The Transformer architecture has demonstrated remarkable scalability, leading to substantial improvements in accuracy. However, this advancement comes at the cost of exceedingly high computational requirements, which have emerged as a significant obstacle in real-world applications. Although researchers have actively pursued solutions to reduce the dimensions of Transformer components and prune elements like attention heads, another critical component, the Feed Forward Network (FFN), has remained relatively underexplored. In a recent paper titled “One Wide Feedforward is All You Need,” a collaborative research effort from Equall and Apple delves into the role of the FFN and uncovers a surprising revelation: despite consuming a significant portion of the model’s parameters, the FFN exhibits high redundancy. As a result, the researchers propose sharing a single FFN across both the encoder and decoder, thereby reducing the parameter count while causing only a modest drop in accuracy. In the Transformer architecture, two main components reign supreme: attention and the FFN. Typically, FFNs occupy roughly two-thirds of the parameter budget, leaving attention with the remaining third. In their study, the researchers explore parameter sharing between the encoder and decoder FFNs, aiming to assess its impact on model accuracy. The overarching objective is to strike a balance between model size, latency, and accuracy. The research team’s primary focus revolves around answering the following questions: To address these questions, the researchers introduce the “One Wide FFN” model, a novel architectural approach that features a single shared wide FFN in the encoder, complemented by an FFN in the decoder. They also employ Linear Centered Kernel Alignment to assess the similarity between internal representations and Local Neighborhood Similarity to gauge semantic space similarity across different models. The results of their study demonstrate that both model accuracy and the internal representations of the Transformer remain stable when employing the One Wide FFN model architecture. Meanwhile, a significant reduction in the number of parameters has been achieved, offering promise for more efficient and practical implementation of Transformer models. The paper One Wide Feedforward is All You Need on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","The Transformer architecture has demonstrated remarkable scalability , leading to substantial improvements in accuracy . However , this advancement comes at the cost of exceedingly high computational requirements , which have emerged as a significant obstacle in real-world applications . Although researchers have actively pursued solutions to reduce the dimensions of Transformer components and prune elements like attention heads , another critical component , the Feed Forward Network ( FFN ) , has remained relatively underexplored . In a recent paper titled “ One Wide Feedforward is All You Need , ” a collaborative research effort from Equall and Apple delves into the role of the FFN and uncovers a surprising revelation : despite consuming a significant portion of the model ’ s parameters , the FFN exhibits high redundancy . As a result , the researchers propose sharing a single FFN across both the encoder and decoder , thereby reducing the parameter count while causing only a modest drop in accuracy . In the Transformer architecture , two main components reign supreme : attention and the FFN . Typically , FFNs occupy roughly two-thirds of the parameter budget , leaving attention with the remaining third . In their study , the researchers explore parameter sharing between the encoder and decoder FFNs , aiming to assess its impact on model accuracy . The overarching objective is to strike a balance between model size , latency , and accuracy . The research team ’ s primary focus revolves around answering the following questions : To address these questions , the researchers introduce the “ One Wide FFN ” model , a novel architectural approach that features a single shared wide FFN in the encoder , complemented by an FFN in the decoder . They also employ Linear Centered Kernel Alignment to assess the similarity between internal representations and Local Neighborhood Similarity to gauge semantic space similarity across different models . The results of their study demonstrate that both model accuracy and the internal representations of the Transformer remain stable when employing the One Wide FFN model architecture . Meanwhile , a significant reduction in the number of parameters has been achieved , offering promise for more efficient and practical implementation of Transformer models . The paper One Wide Feedforward is All You Need on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['transformer', 'architecture', 'demonstrate', 'remarkable', 'scalability', 'lead', 'substantial', 'improvement', 'accuracy', 'however', 'advancement', 'come', 'cost', 'exceedingly', 'high', 'computational', 'requirement', 'emerge', 'significant', 'obstacle', 'realworld', 'application', 'researcher', 'actively', 'pursue', 'solution', 'reduce', 'dimension', 'transformer', 'component', 'prune', 'element', 'attention', 'head', 'critical', 'component', 'feed', 'forward', 'network', 'ffn', 'remain', 'relatively', 'underexplored', 'recent', 'paper', 'title', 'wide', 'feedforward', 'need', 'collaborative', 'research', 'effort', 'equall', 'apple', 'delf', 'role', 'ffn', 'uncover', 'surprising', 'revelation', 'consume', 'significant', 'portion', 'model', 'parameter', 'ffn', 'exhibit', 'high', 'redundancy', 'result', 'researcher', 'propose', 'share', 'single', 'ffn', 'encoder', 'decoder', 'thereby', 'reduce', 'parameter', 'count', 'cause', 'modest', 'drop', 'accuracy', 'transformer', 'architecture', 'main', 'component', 'reign', 'supreme', 'attention', 'ffn', 'typically', 'ffns', 'occupy', 'roughly', 'twothird', 'parameter', 'budget', 'leave', 'attention', 'remain', 'third', 'study', 'researcher', 'explore', 'parameter', 'sharing', 'encoder', 'decoder', 'ffns', 'aim', 'assess', 'impact', 'model', 'accuracy', 'overarching', 'objective', 'strike', 'balance', 'model', 'size', 'latency', 'accuracy', 'research', 'team', 'primary', 'focus', 'revolve', 'answer', 'follow', 'question', 'address', 'question', 'researcher', 'introduce', 'wide', 'ffn', 'model', 'novel', 'architectural', 'approach', 'feature', 'single', 'share', 'wide', 'ffn', 'encoder', 'complement', 'ffn', 'decoder', 'also', 'employ', 'center', 'kernel', 'alignment', 'assess', 'similarity', 'internal', 'representation', 'local', 'neighborhood', 'similarity', 'gauge', 'semantic', 'space', 'similarity', 'different', 'model', 'result', 'study', 'demonstrate', 'model', 'accuracy', 'internal', 'representation', 'transformer', 'remain', 'stable', 'employ', 'wide', 'ffn', 'model', 'architecture', 'meanwhile', 'significant', 'reduction', 'number', 'parameter', 'achieve', 'offer', 'promise', 'efficient', 'practical', 'implementation', 'transformer', 'model', 'paper', 'wide', 'feedforward', 'need', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
A collaborative research effort from Equall and Apple delves into the role of the FFN and uncovers a surprising revelation: despite consuming a significant portion of the model’s parameters, the FFN exhibits high redundancy. As a result, the researchers propose sharing a single FFN across both the encoder and decoder, thereby reducing the parameter count while causing only a modest drop in accuracy.
"
Unlocking Limitless Retrieval Power: Google’s MEMORY-VQ Revolutionizes LLMs with Remarkable Compression,https://syncedreview.com/2023/09/06/unlocking-limitless-retrieval-power-googles-memory-vq-revolutionizes-llms-with-remarkable-compression/,2023-09-06,"Retrieval augmentation is a commonly employed and effective approach for enhancing the factual knowledge of language models, while simultaneously accelerating model inference times. Nonetheless, this approach comes with considerable computational costs attributed to the substantial storage demands required for storing precomputed representations. To address this pertinent issue, a Google research team has presented a groundbreaking solution in their new paper titled “MEMORY-VQ: Compression for Tractable Internet-Scale Memory.” This innovative method, MEMORY-VQ, significantly diminishes the storage prerequisites associated with memory-based techniques while upholding high performance levels, achieving an impressive 16x compression rate on the KILT benchmark. Remarkably, this endeavor marks a pioneering effort in the realm of compressing pre-encoded token memory representations, as no prior research has explored this avenue. The MEMORY-VQ approach seamlessly blends product quantization with the VQ-VAE method to achieve its primary objective: reducing storage requirements for memory-based methods without compromising quality. The core concept involves employing vector quantization techniques to substitute the original memory vectors with integer codes for memory compression. These codes can then be efficiently transformed back into vectors as needed. By implementing this approach in LUMEN, a potent memory-based technique that pre-computes token representations for retrieved passages to significantly expedite inference, the researchers have developed the LUMEN-VQ model. In their empirical investigation, the research team conducted a comparative analysis, pitting LUMEN-VQ against naïve baselines such as LUMEN-Large and LUMEN-Light, using a subset of knowledge-intensive tasks from the KILT benchmark. Impressively, LUMEN-VQ managed to achieve a remarkable 16x compression rate with only a limited loss in quality. In summary, this research underscores the effectiveness of MEMORY-VQ as a memory augmentation technique and a pragmatic solution for substantially enhancing inference speed when dealing with extensive retrieval corpora.The paper MEMORY-VQ: Compression for Tractable Internet-Scale Memory on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Retrieval augmentation is a commonly employed and effective approach for enhancing the factual knowledge of language models , while simultaneously accelerating model inference times . Nonetheless , this approach comes with considerable computational costs attributed to the substantial storage demands required for storing precomputed representations . To address this pertinent issue , a Google research team has presented a groundbreaking solution in their new paper titled “ MEMORY-VQ : Compression for Tractable Internet-Scale Memory. ” This innovative method , MEMORY-VQ , significantly diminishes the storage prerequisites associated with memory-based techniques while upholding high performance levels , achieving an impressive 16x compression rate on the KILT benchmark . Remarkably , this endeavor marks a pioneering effort in the realm of compressing pre-encoded token memory representations , as no prior research has explored this avenue . The MEMORY-VQ approach seamlessly blends product quantization with the VQ-VAE method to achieve its primary objective : reducing storage requirements for memory-based methods without compromising quality . The core concept involves employing vector quantization techniques to substitute the original memory vectors with integer codes for memory compression . These codes can then be efficiently transformed back into vectors as needed . By implementing this approach in LUMEN , a potent memory-based technique that pre-computes token representations for retrieved passages to significantly expedite inference , the researchers have developed the LUMEN-VQ model . In their empirical investigation , the research team conducted a comparative analysis , pitting LUMEN-VQ against naïve baselines such as LUMEN-Large and LUMEN-Light , using a subset of knowledge-intensive tasks from the KILT benchmark . Impressively , LUMEN-VQ managed to achieve a remarkable 16x compression rate with only a limited loss in quality . In summary , this research underscores the effectiveness of MEMORY-VQ as a memory augmentation technique and a pragmatic solution for substantially enhancing inference speed when dealing with extensive retrieval corpora.The paper MEMORY-VQ : Compression for Tractable Internet-Scale Memory on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['retrieval', 'augmentation', 'commonly', 'employ', 'effective', 'approach', 'enhance', 'factual', 'knowledge', 'language', 'model', 'simultaneously', 'accelerate', 'model', 'inference', 'time', 'nonetheless', 'approach', 'come', 'considerable', 'computational', 'cost', 'attribute', 'substantial', 'storage', 'demand', 'require', 'store', 'precomputed', 'representation', 'address', 'pertinent', 'issue', 'research', 'team', 'present', 'groundbreaking', 'solution', 'new', 'paper', 'title', 'memoryvq', 'compression', 'tractable', 'internetscale', 'memory', 'innovative', 'method', 'memoryvq', 'significantly', 'diminish', 'storage', 'prerequisite', 'associate', 'memorybased', 'technique', 'uphold', 'high', 'performance', 'level', 'achieve', 'impressive', '16x', 'compression', 'rate', 'benchmark', 'remarkably', 'endeavor', 'mark', 'pioneering', 'effort', 'realm', 'compress', 'preencoded', 'token', 'memory', 'representation', 'prior', 'research', 'explore', 'avenue', 'memoryvq', 'approach', 'seamlessly', 'blend', 'product', 'quantization', 'vqvae', 'method', 'achieve', 'primary', 'objective', 'reduce', 'storage', 'requirement', 'memorybased', 'method', 'compromise', 'quality', 'core', 'concept', 'involve', 'employ', 'vector', 'quantization', 'technique', 'substitute', 'original', 'memory', 'vector', 'integer', 'code', 'memory', 'compression', 'code', 'efficiently', 'transform', 'back', 'vector', 'need', 'implement', 'approach', 'luman', 'potent', 'memorybased', 'technique', 'precompute', 'token', 'representation', 'retrieve', 'passage', 'significantly', 'expedite', 'inference', 'researcher', 'develop', 'lumenvq', 'model', 'empirical', 'investigation', 'research', 'team', 'conduct', 'comparative', 'analysis', 'pit', 'lumenvq', 'naïve', 'baseline', 'lumenlarge', 'lumenlight', 'use', 'subset', 'knowledgeintensive', 'task', 'benchmark', 'impressively', 'manage', 'achieve', 'remarkable', '16x', 'compression', 'rate', 'limited', 'loss', 'quality', 'summary', 'research', 'underscore', 'effectiveness', 'memoryvq', 'memory', 'augmentation', 'technique', 'pragmatic', 'solution', 'substantially', 'enhance', 'inference', 'speed', 'deal', 'extensive', 'retrieval', 'paper', 'memoryvq', 'compression', 'tractable', 'internetscale', 'memory', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
In a new paper MEMORY-VQ: Compression for Tractable Internet-Scale Memory, a Google research team introduces MEMORY-VQ, a novel method that significantly reduce storage requirements for memory-based methods while maintaining high performance, achieving 16x compression rate on the KILT benchmark.
"
MIT’s AskIt Provides A Unified Programming Interface for Code Generation with LLMs,https://syncedreview.com/2023/09/05/mits-askit-provides-a-unified-programming-interface-for-code-generation-with-llms/,2023-09-05,"Large Language Models (LLMs) have showcased remarkable capabilities in recent years. One of the most intriguing facets of their capabilities is their adeptness across a wide variety of tasks. Researchers have aptly termed this phenomenon as “emergent abilities,” which serves to distinguish LLMs from other language models. However, the integration of LLMs into software development poses significant challenges. This is primarily due to the complex decision-making process involved in embedding them into applications. Additionally, the effective design of prompts remains a substantial concern. To tackle these challenges head-on, a research team from MIT CSAIL has presented a new paper titled “AskIt: Unified Programming Interface for Programming with Large Language Models.” AskIt is a domain-specific language (DSL) designed specifically for LLMs, with the aim of accommodating a wide array of tasks. This innovative approach substantially reduces the developmental overhead and effort required by practitioners in the field of software development. The team summarizes their main contributions as follows: AskIt offers two essential APIs: “ask” and “define.” It boasts a type system that empowers developers to specify the expected output type of a task via synthesized prompts, eliminating the need for manual prompt engineering. Furthermore, its template-based function definitions enable developers to craft functions for specific computational and linguistic tasks by leveraging LLMs. The code generation features of AskIt ensure seamless transitions between integrating an LLM into software and using it for programming. To bring AskIt to life, the team implemented it for TypeScript and developed a DSL compiler as a TypeScript compiler plugin. In the computational flow, the DSL compiler traverses the Abstract Syntax Tree (AST) of the input code, converting AskIt APIs into specific TypeScript functions. When “define” is called, the DSL compiler generates the corresponding function. For “ask” or functions defined by “define,” the DSL runtime produces a prompt based on the type information. The LLM consumes the prompt to provide a response, which is then received and parsed by the DSL runtime to extract the answer. In their empirical study, the team implemented AskIt in TypeScript and Python across a diverse tasks, through benchmarking, the results show that AskIt significantly speedup the code generation time, demonstrating its operational efficiency and efficacy. The paper AskIt: Unified Programming Interface for Programming with Large Language Models on arXiv. Author: Hecate He | Editor: Chain Zhang We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Large Language Models ( LLMs ) have showcased remarkable capabilities in recent years . One of the most intriguing facets of their capabilities is their adeptness across a wide variety of tasks . Researchers have aptly termed this phenomenon as “ emergent abilities , ” which serves to distinguish LLMs from other language models . However , the integration of LLMs into software development poses significant challenges . This is primarily due to the complex decision-making process involved in embedding them into applications . Additionally , the effective design of prompts remains a substantial concern . To tackle these challenges head-on , a research team from MIT CSAIL has presented a new paper titled “ AskIt : Unified Programming Interface for Programming with Large Language Models. ” AskIt is a domain-specific language ( DSL ) designed specifically for LLMs , with the aim of accommodating a wide array of tasks . This innovative approach substantially reduces the developmental overhead and effort required by practitioners in the field of software development . The team summarizes their main contributions as follows : AskIt offers two essential APIs : “ ask ” and “ define. ” It boasts a type system that empowers developers to specify the expected output type of a task via synthesized prompts , eliminating the need for manual prompt engineering . Furthermore , its template-based function definitions enable developers to craft functions for specific computational and linguistic tasks by leveraging LLMs . The code generation features of AskIt ensure seamless transitions between integrating an LLM into software and using it for programming . To bring AskIt to life , the team implemented it for TypeScript and developed a DSL compiler as a TypeScript compiler plugin . In the computational flow , the DSL compiler traverses the Abstract Syntax Tree ( AST ) of the input code , converting AskIt APIs into specific TypeScript functions . When “ define ” is called , the DSL compiler generates the corresponding function . For “ ask ” or functions defined by “ define , ” the DSL runtime produces a prompt based on the type information . The LLM consumes the prompt to provide a response , which is then received and parsed by the DSL runtime to extract the answer . In their empirical study , the team implemented AskIt in TypeScript and Python across a diverse tasks , through benchmarking , the results show that AskIt significantly speedup the code generation time , demonstrating its operational efficiency and efficacy . The paper AskIt : Unified Programming Interface for Programming with Large Language Models on arXiv . Author : Hecate He | Editor : Chain Zhang We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['large', 'language', 'model', 'llm', 'showcase', 'remarkable', 'capability', 'recent', 'year', 'intriguing', 'facet', 'capability', 'adeptness', 'wide', 'variety', 'task', 'researcher', 'aptly', 'term', 'phenomenon', 'emergent', 'ability', 'serve', 'distinguish', 'llm', 'language', 'model', 'however', 'integration', 'llm', 'software', 'development', 'pose', 'significant', 'challenge', 'primarily', 'due', 'complex', 'decisionmake', 'process', 'involve', 'embed', 'application', 'additionally', 'effective', 'design', 'prompt', 'remain', 'substantial', 'concern', 'tackle', 'challenge', 'headon', 'research', 'team', 'csail', 'present', 'new', 'paper', 'title', 'askit', 'unified', 'programming', 'interface', 'programming', 'large', 'language', 'model', 'askit', 'domainspecific', 'language', 'dsl', 'design', 'specifically', 'llm', 'aim', 'accommodate', 'wide', 'array', 'task', 'innovative', 'approach', 'substantially', 'reduce', 'developmental', 'overhead', 'effort', 'require', 'practitioner', 'field', 'software', 'development', 'team', 'summarize', 'main', 'contribution', 'follow', 'askit', 'offer', 'essential', 'apis', 'ask', 'define', 'boast', 'type', 'system', 'empower', 'developer', 'specify', 'expect', 'output', 'type', 'task', 'synthesize', 'prompt', 'eliminate', 'need', 'manual', 'prompt', 'engineering', 'furthermore', 'templatebased', 'function', 'definition', 'enable', 'developer', 'craft', 'function', 'specific', 'computational', 'linguistic', 'task', 'leverage', 'llm', 'code', 'generation', 'feature', 'askit', 'ensure', 'seamless', 'transition', 'integrate', 'llm', 'software', 'use', 'programming', 'bring', 'askit', 'life', 'team', 'implement', 'typescript', 'develop', 'dsl', 'compiler', 'typescript', 'compiler', 'plugin', 'computational', 'flow', 'dsl', 'compiler', 'traverse', 'abstract', 'syntax', 'tree', 'input', 'code', 'convert', 'askit', 'apis', 'specific', 'typescript', 'function', 'define', 'call', 'compiler', 'generate', 'corresponding', 'function', 'ask', 'function', 'define', 'define', 'runtime', 'produce', 'prompt', 'base', 'type', 'information', 'llm', 'consume', 'prompt', 'provide', 'response', 'receive', 'parse', 'runtime', 'extract', 'answer', 'empirical', 'study', 'team', 'implement', 'askit', 'typescript', 'python', 'diverse', 'task', 'benchmarke', 'result', 'show', 'askit', 'significantly', 'speedup', 'code', 'generation', 'time', 'demonstrate', 'operational', 'efficiency', 'efficacy', 'paper', 'askit', 'unified', 'programming', 'interface', 'programming', 'large', 'language', 'model', 'arxiv', 'author', 'hecate', 'editor', 'chain', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
In a new paper AskIt: Unified Programming Interface for Programming with Large Language Models, a MIT CSAIL research team presents AskIt, a domain-specific language (DSL) tailored for LLMs to accommodate a wide variety of tasks, which substantially reducing practitioners’ developmental overhead and effort for software.
"
70 billion parameter LLaMA2 model training accelerated by 195% with best foundation model practice upgraded,https://syncedreview.com/2023/09/04/70-billion-parameter-llama2-model-training-accelerated-by-195-with-best-foundation-model-practice-upgraded/,2023-09-04,"Initially triggered by ChatGPT, the large model boom is continuing to intensify. Tech giants and star startups are scrambling to contribute models for the competitive and diversified commercial market. Among these models, the LLaMA series has accumulated a vast amount of users and practical applications due to its basic capabilities and open ecology. For countless open-source model latecomers, it has become a benchmark model for imitation and comparison. However, key bottlenecks still exist for AIGC-related enterprises, including questions about how developers can reduce pre-training costs of big models like LLaMA2, as well as how they can build these models practically using continual pre-training and fine-tuning. As the world’s largest and most active community for large model development tools, Colossal-AI provides revolutionary LLaMA2 training efficiency for 8 to 512 GPUs, fine-tuning, and inference solutions. The 70 billion parameter training can be accelerated by 195%, and provides a fully-managed ML cloud platform solution, greatly reducing the cost of large model development and applications. Open source address: https://github.com/hpcaitech/ColossalAI Meta’s open-source large model series, LLaMA, further stimulated the enthusiasm for creating models like ChatGPT, which has inspired the development of many projects and applications. The latest 7B~70B LLaMA2 model further improves the basic capabilities of the language model. However, since most of the pre-training information for LLaMA2 is derived from English generalized knowledge, the domain information and multilingual capabilities that can be enhanced and injected with fine-tuning are relatively limited. Additionally, high-quality datasets and expertise are typically regarded as core assets of companies and kept in a privatized form. Considering the increase of high-quality private business data, pre-training/fine-tuning the LLaMA2 series of big models efficiently but cheaply is an urgent necessity for many industries and companies. However, LLaMA2 big models only release the original model weights and inference scripts which do not support training/fine-tuning or datasets. To address the needs mentioned above, Colossal-AI has open sourced a full-flow solution for LLaMA2 with high scalability. This supports models ranging from 7 to 70 billion parameters, while still maintaining good performance from 8 to 512 GPUs. When training/fine-tuning LLaMA2-7B using 8 GPUs, Colossal-AI is able to achieve an industry-leading hardware utilization (MFU) of about 54%. As for pre-training, when LLaMA2-70B was pre-trained with 512 A100 40GB, the DeepSpeed ZeRO3 strategy could not be activated due to insufficient GPU memory. This strategy could only be activated using ZeRO3-offload with a large speed decay. Colossal-AI, on the other hand, still maintains good performance and a training speedup of 195% due to its excellent system optimization and scalability. The high performance of Colossal-AI’s LLaMA-2 training/fine-tuning comes from system optimizations such as the new heterogeneous memory management system Gemini, and high-performance operators like Flash attention 2. Gemini provides highly scalable, robust, and easily usable interfaces. Its format, Checkpoint, is also fully compatible with HuggingFace, reducing usage and conversion costs. It is more flexible for cuts, offloads, etc., covering more hardware configurations for LLaMA-2 training/fine-tuning tasks. All of these advantages can be used with just a few lines of code:  Although Colossal-AI’s Gemini already performs well for a mainstream hardware model, for some extreme hardware conditions or special models, fine-grained optimizations with multi-dimensional parallelism may be needed. Other existing solutions usually require veterans in distributed systems to manually refactor and tune the code to scale, but Colossal-AI’s ShardFormer provides incredible multi-dimensional parallelism and operators optimization capabilities. This can be utilized with a few lines of code while providing good performance on both a standalone server/large-scale clusters. Colossal-AI’s ShardFormer supports mainstream open source models including LLaMA1/2, BLOOM, OPT, T5, GPT-2, BERT, GLM, etc. Huggingface/transformers models can be imported directly. The Checkpoint format is also fully compatible with HuggingFace, greatly improving the usability compared to Megatron-LM and other projects that require lots of rewritten code. For the parallel strategy, it has supported the following multiple parallel methods: tensor parallelism, pipeline parallelism, sequence parallelism, data parallelism, Zero data parallelism, etc. It can even combine multiple parallel methods to adapt to various complex hardware environments/models with simple configuration commands. Furthermore, it has a variety of built-in high-performance operators, eliminating the need for a tedious compatibility/configuration process. These include: In order to further improve development and deployment efficiency, the Colossal-AI team also combines advantages of the systems above with computational resources to provide the Colossal-AI Cloud Platform. This offers cheap computational power and mainstream AI applications, including dialog big models, multimodal models, biomedicine, etc. The invitation for internal testing is now open. By shielding underlying distributed parallel computing, memory, communication management and optimization of large models, AI developers can focus on the model and algorithm design. This completes the AI model at a lower cost and faster speed, ultimately reducing business costs while increasing efficiency. Users only need to upload relevant data to train personalized private models without code and can deploy the trained models with one click. The application has been carefully optimized by the Colossal-AI team, and thanks to the optimization of algorithms and systems, the cost of model training and deployment can be reduced immensely. Colossal-AI Cloud Platform: platform.colossalai.com Colossal-AI open source address: https://github.com/hpcaitech/ColossalAI https://www.hpc-ai.tech/blog/70b-llama2-training  We know you don’t want to miss any news or research breakthroughs. Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates.","Initially triggered by ChatGPT , the large model boom is continuing to intensify . Tech giants and star startups are scrambling to contribute models for the competitive and diversified commercial market . Among these models , the LLaMA series has accumulated a vast amount of users and practical applications due to its basic capabilities and open ecology . For countless open-source model latecomers , it has become a benchmark model for imitation and comparison . However , key bottlenecks still exist for AIGC-related enterprises , including questions about how developers can reduce pre-training costs of big models like LLaMA2 , as well as how they can build these models practically using continual pre-training and fine-tuning . As the world ’ s largest and most active community for large model development tools , Colossal-AI provides revolutionary LLaMA2 training efficiency for 8 to 512 GPUs , fine-tuning , and inference solutions . The 70 billion parameter training can be accelerated by 195 % , and provides a fully-managed ML cloud platform solution , greatly reducing the cost of large model development and applications . Open source address : https : //github.com/hpcaitech/ColossalAI Meta ’ s open-source large model series , LLaMA , further stimulated the enthusiasm for creating models like ChatGPT , which has inspired the development of many projects and applications . The latest 7B~70B LLaMA2 model further improves the basic capabilities of the language model . However , since most of the pre-training information for LLaMA2 is derived from English generalized knowledge , the domain information and multilingual capabilities that can be enhanced and injected with fine-tuning are relatively limited . Additionally , high-quality datasets and expertise are typically regarded as core assets of companies and kept in a privatized form . Considering the increase of high-quality private business data , pre-training/fine-tuning the LLaMA2 series of big models efficiently but cheaply is an urgent necessity for many industries and companies . However , LLaMA2 big models only release the original model weights and inference scripts which do not support training/fine-tuning or datasets . To address the needs mentioned above , Colossal-AI has open sourced a full-flow solution for LLaMA2 with high scalability . This supports models ranging from 7 to 70 billion parameters , while still maintaining good performance from 8 to 512 GPUs . When training/fine-tuning LLaMA2-7B using 8 GPUs , Colossal-AI is able to achieve an industry-leading hardware utilization ( MFU ) of about 54 % . As for pre-training , when LLaMA2-70B was pre-trained with 512 A100 40GB , the DeepSpeed ZeRO3 strategy could not be activated due to insufficient GPU memory . This strategy could only be activated using ZeRO3-offload with a large speed decay . Colossal-AI , on the other hand , still maintains good performance and a training speedup of 195 % due to its excellent system optimization and scalability . The high performance of Colossal-AI ’ s LLaMA-2 training/fine-tuning comes from system optimizations such as the new heterogeneous memory management system Gemini , and high-performance operators like Flash attention 2 . Gemini provides highly scalable , robust , and easily usable interfaces . Its format , Checkpoint , is also fully compatible with HuggingFace , reducing usage and conversion costs . It is more flexible for cuts , offloads , etc. , covering more hardware configurations for LLaMA-2 training/fine-tuning tasks . All of these advantages can be used with just a few lines of code : Although Colossal-AI ’ s Gemini already performs well for a mainstream hardware model , for some extreme hardware conditions or special models , fine-grained optimizations with multi-dimensional parallelism may be needed . Other existing solutions usually require veterans in distributed systems to manually refactor and tune the code to scale , but Colossal-AI ’ s ShardFormer provides incredible multi-dimensional parallelism and operators optimization capabilities . This can be utilized with a few lines of code while providing good performance on both a standalone server/large-scale clusters . Colossal-AI ’ s ShardFormer supports mainstream open source models including LLaMA1/2 , BLOOM , OPT , T5 , GPT-2 , BERT , GLM , etc . Huggingface/transformers models can be imported directly . The Checkpoint format is also fully compatible with HuggingFace , greatly improving the usability compared to Megatron-LM and other projects that require lots of rewritten code . For the parallel strategy , it has supported the following multiple parallel methods : tensor parallelism , pipeline parallelism , sequence parallelism , data parallelism , Zero data parallelism , etc . It can even combine multiple parallel methods to adapt to various complex hardware environments/models with simple configuration commands . Furthermore , it has a variety of built-in high-performance operators , eliminating the need for a tedious compatibility/configuration process . These include : In order to further improve development and deployment efficiency , the Colossal-AI team also combines advantages of the systems above with computational resources to provide the Colossal-AI Cloud Platform . This offers cheap computational power and mainstream AI applications , including dialog big models , multimodal models , biomedicine , etc . The invitation for internal testing is now open . By shielding underlying distributed parallel computing , memory , communication management and optimization of large models , AI developers can focus on the model and algorithm design . This completes the AI model at a lower cost and faster speed , ultimately reducing business costs while increasing efficiency . Users only need to upload relevant data to train personalized private models without code and can deploy the trained models with one click . The application has been carefully optimized by the Colossal-AI team , and thanks to the optimization of algorithms and systems , the cost of model training and deployment can be reduced immensely . Colossal-AI Cloud Platform : platform.colossalai.com Colossal-AI open source address : https : //github.com/hpcaitech/ColossalAI https : We know you don ’ t want to miss any news or research breakthroughs . Subscribe to our popular newsletter Synced Global AI Weekly to get weekly AI updates .","['initially', 'trigger', 'chatgpt', 'large', 'model', 'boom', 'continue', 'intensify', 'tech', 'giant', 'star', 'startup', 'scramble', 'contribute', 'model', 'competitive', 'diversified', 'commercial', 'market', 'model', 'accumulate', 'vast', 'amount', 'user', 'practical', 'application', 'basic', 'capability', 'open', 'ecology', 'countless', 'opensource', 'model', 'latecomer', 'become', 'benchmark', 'model', 'imitation', 'comparison', 'however', 'key', 'bottleneck', 'still', 'exist', 'aigcrelated', 'enterprise', 'include', 'question', 'developer', 'reduce', 'pretraine', 'cost', 'big', 'model', 'llama2', 'well', 'build', 'model', 'practically', 'use', 'continual', 'pretraining', 'finetune', 'world', 'large', 'active', 'community', 'large', 'model', 'development', 'tool', 'provide', 'revolutionary', 'llama2', 'training', 'efficiency', 'gpu', 'finetune', 'inference', 'solution', 'parameter', 'training', 'accelerate', 'provide', 'fullymanaged', 'cloud', 'platform', 'solution', 'greatly', 'reduce', 'cost', 'large', 'model', 'development', 'application', 'open', 'source', 'address', 'opensource', 'large', 'model', 'far', 'stimulate', 'enthusiasm', 'create', 'model', 'chatgpt', 'inspire', 'development', 'many', 'project', 'application', 'late', 'llama2', 'model', 'far', 'improve', 'basic', 'capability', 'language', 'model', 'however', 'pretraine', 'information', 'llama2', 'derive', 'generalized', 'knowledge', 'domain', 'information', 'multilingual', 'capability', 'enhance', 'inject', 'finetuning', 'relatively', 'limited', 'additionally', 'highquality', 'dataset', 'expertise', 'typically', 'regard', 'core', 'asset', 'company', 'keep', 'privatized', 'form', 'consider', 'increase', 'highquality', 'private', 'business', 'datum', 'pretrainingfinetune', 'series', 'big', 'model', 'efficiently', 'cheaply', 'urgent', 'necessity', 'many', 'industry', 'company', 'however', 'llama2', 'big', 'model', 'release', 'original', 'model', 'weight', 'inference', 'script', 'support', 'trainingfinetune', 'dataset', 'address', 'need', 'mention', 'open', 'source', 'fullflow', 'solution', 'llama2', 'high', 'scalability', 'support', 'model', 'range', 'parameter', 'still', 'maintain', 'good', 'performance', 'gpu', 'trainingfinetune', 'llama27b', 'use', 'gpu', 'able', 'achieve', 'industryleade', 'hardware', 'utilization', 'mfu', 'pretraine', 'pretraine', 'gb', 'deepspeed', 'zero3', 'strategy', 'activate', 'insufficient', 'memory', 'strategy', 'activate', 'use', 'large', 'speed', 'decay', 'hand', 'still', 'maintain', 'good', 'performance', 'training', 'speedup', 'due', 'excellent', 'system', 'optimization', 'scalability', 'high', 'performance', 'llama2', 'trainingfinetuning', 'come', 'system', 'optimization', 'new', 'heterogeneous', 'memory', 'management', 'system', 'gemini', 'highperformance', 'operator', 'flash', 'attention', 'gemini', 'provide', 'highly', 'scalable', 'robust', 'easily', 'usable', 'interface', 'format', 'checkpoint', 'also', 'fully', 'compatible', 'huggingface', 'reduce', 'usage', 'conversion', 'cost', 'flexible', 'cut', 'offload', 'cover', 'hardware', 'configuration', 'llama2', 'trainingfinetune', 'task', 'advantage', 'use', 'line', 'code', 'gemini', 'already', 'perform', 'well', 'mainstream', 'hardware', 'model', 'extreme', 'hardware', 'condition', 'special', 'model', 'finegraine', 'optimization', 'multidimensional', 'parallelism', 'need', 'exist', 'solution', 'usually', 'require', 'veteran', 'distribute', 'system', 'manually', 'refactor', 'tune', 'code', 'scale', 'shardformer', 'provide', 'incredible', 'multidimensional', 'parallelism', 'operator', 'optimization', 'capability', 'utilize', 'line', 'code', 'provide', 'good', 'performance', 'standalone', 'serverlargescale', 'cluster', 'shardformer', 'support', 'mainstream', 'open', 'source', 'model', 'include', 'llama12', 'bloom', 'opt', 'gpt2', 'glm', 'huggingfacetransformer', 'model', 'import', 'directly', 'checkpoint', 'format', 'also', 'fully', 'compatible', 'huggingface', 'greatly', 'improve', 'usability', 'compare', 'megatronlm', 'project', 'require', 'lot', 'rewrite', 'code', 'parallel', 'strategy', 'support', 'follow', 'multiple', 'parallel', 'method', 'tensor', 'parallelism', 'pipeline', 'parallelism', 'sequence', 'parallelism', 'datum', 'parallelism', 'datum', 'parallelism', 'even', 'combine', 'multiple', 'parallel', 'method', 'adapt', 'various', 'complex', 'hardware', 'environmentsmodel', 'simple', 'configuration', 'command', 'furthermore', 'variety', 'builtin', 'highperformance', 'operator', 'eliminate', 'need', 'tedious', 'compatibilityconfiguration', 'process', 'include', 'order', 'far', 'improve', 'development', 'deployment', 'efficiency', 'team', 'also', 'combine', 'advantage', 'system', 'computational', 'resource', 'provide', 'cloud', 'platform', 'offer', 'cheap', 'computational', 'power', 'mainstream', 'application', 'include', 'dialog', 'big', 'model', 'multimodal', 'model', 'biomedicine', 'invitation', 'internal', 'testing', 'open', 'shield', 'underlying', 'distribute', 'parallel', 'computing', 'memory', 'communication', 'management', 'optimization', 'large', 'model', 'ai', 'developer', 'focus', 'model', 'design', 'complete', 'model', 'low', 'cost', 'fast', 'speed', 'ultimately', 'reduce', 'business', 'cost', 'increase', 'efficiency', 'user', 'need', 'upload', 'relevant', 'datum', 'train', 'personalize', 'private', 'model', 'code', 'deploy', 'train', 'model', 'click', 'application', 'carefully', 'optimize', 'team', 'thank', 'optimization', 'algorithm', 'system', 'cost', 'model', 'training', 'deployment', 'reduce', 'immensely', 'source', 'address', 'know', 'want', 'miss', 'news', 'research', 'breakthrough', 'subscribe', 'popular', 'newsletter', 'sync', 'global', 'ai', 'weekly', 'get', 'weekly', 'update']","
Colossal-AI provides revolutionary LLaMA2 training efficiency for 8 to 512 GPUs, fine-tuning, and inference solutions. The 70 billion parameter training can be accelerated by 195%, and provides a fully-managed ML cloud platform solution, greatly reducing the cost of large model development and applications.
"
