title,url,date,text,cleaning,tokens
"NfgTransformer: Equivariant Representation Learning for Normal-form
  Games","[{'href': 'http://arxiv.org/abs/2402.08393v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2402.08393v1', 'rel': 'related', 'type': 'application/pdf'}]",2024-02-13 11:38:52,"4
2
0
2

b
e
F
6

]
L
C
.
s
c
[

2
v
4
0
7
1
0
.
2
0
4
2
:
v
i
X
r
a

January 2024

States as Strings as Strategies: Steering
Language Models with Game-Theoretic Solvers

Ian Gemp1, Yoram Bachrach1, Marc Lanctot1, Roma Patel1, Vibhavari Dasagi1, Luke Marris1, Georgios
Piliouras1, Siqi Liu1 and Karl Tuyls1
1Google DeepMind

Game theory is the study of mathematical models of strategic interactions among rational agents.
Language is a key medium of interaction for humans, though it has historically proven difficult to model
dialogue and its strategic motivations mathematically. A suitable model of the players, strategies, and
payoffs associated with linguistic interactions (i.e., a binding to the conventional symbolic logic of game
theory) would enable existing game-theoretic algorithms to provide strategic solutions in the space of
language. In other words, a binding could provide a route to computing stable, rational conversational
strategies in dialogue. Large language models (LLMs) have arguably reached a point where their
generative capabilities can enable realistic, human-like simulations of natural dialogue. By prompting
them in various ways, we can steer their responses towards different output utterances. Leveraging the
expressivity of natural language, LLMs can also help us quickly generate new dialogue scenarios, which
are grounded in real world applications. In this work, we present one possible binding from dialogue
to game theory as well as generalizations of existing equilibrium finding algorithms to this setting. In
addition, by exploiting LLMs generation capabilities along with our proposed binding, we can synthesize
a large repository of formally-defined games in which one can study and test game-theoretic solution
concepts. We also demonstrate how one can combine LLM-driven game generation, game-theoretic
solvers, and imitation learning to construct a process for improving the strategic capabilities of LLMs.

Keywords: Large Language Models, Game Theory, Multiagent Systems

1. Introduction

While artificial intelligence (AI) has mediated large parts of our daily life for decades—internet
search (Brin and Page, 1998), recommendation systems (Bennett et al., 2007), federated learning of
key strokes (Hard et al., 2018)—the recent emergence of strong generative AI models, conveying the
impression of agency and competence (McKee et al., 2023), have opened the door for AI assistants.
In this new setting, we might knowingly interact with and request the mediation of an artificially
intelligent agent on our behalf.

People already employ large language models (LLMs) in negotiations1. Users have also admitted
their use of LLMs to achieve their goals (White et al., 2023; Zhou et al., 2022), potentially at the
expense of others2. Given the deployment of LLMs in such scenarios—specifically, in interactions with
opportunities for strategic decision making—we believe it makes sense to study how to model these

Title explained: States or infostates are standard terminology for how histories are grouped in imperfect information
games – this is covered in Section 2, background. States as strings as strategies refers to the fact that, in this paper, states
(dialogue histories) are strings and strategies (prompts) are also strings. It is meant to convey how the dialogue game
model we propose is encoded entirely natively in language space (i.e., strings).

1https://www.reddit.com/r/OpenAI/comments/182zkdl/comment/kam363u/
2https://twitter.com/JeremyRubin/status/1674454729231933440

Corresponding author(s): Ian Gemp (imgemp@google.com)
© 2024 Google DeepMind. All rights reserved

 
 
 
 
 
 
States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

strategic interactions formally in the language of game theory.

Game Theory: Algorithmic Successes and Notions of Strategic Optimality Game Theory studies
mathematical models of strategic interactions among rational agents, considering self-interested
participants whose actions affect one another (Fudenberg and Tirole, 1991). Such analysis has
applications in a wide range of fiends, from social science and economics to logic and computer
In particular, game theory provides a rigorous mathematical framework for analyzing
science.
multiagent systems and for designing artificial autonomous agents who must cooperate or compete
with one another (Dafoe et al., 2021; Rosenschein and Zlotkin, 1994; Stone and Veloso, 2000;
Wooldridge, 2009).

The field of artificial intelligence has a long history of successful agent designs based on game
theoretic principles in many tasks such as cooperation and negotiation (Gemp et al., 2022; Jennings
et al., 2001; Kraus, 1997), optimization (Hernandez-Leal et al., 2017; Sohrabi and Azgomi, 2020;
Tuyls and Parsons, 2007), economic analysis and trading (Parkes and Wellman, 2015; Wellman, 2022),
or electronic commerce (He et al., 2003; Song et al., 2019). Further, game theory (Nisan et al., 2007;
Shoham and Leyton-Brown, 2008) has driven some major recent successes in outperforming humans
in complex decision making settings including Go (Silver et al., 2017), Poker (Brown and Sandholm,
2018), StarCraft (Vinyals et al., 2019), Gran Turismo (Wurman et al., 2022), Stratego (Perolat et al.,
2022), Humanoid Soccer (Liu et al., 2022), and Diplomacy (Anthony et al., 2020; FAIR et al., 2022).

Critical to these successes is the development of research on multiagent systems and, in partic-
ular, computational techniques for computing approximate solutions of games, typically equilibria.
These techniques rely on a formal language for categorizing games into classes, identifying general
assumptions for each case, and tailoring algorithms for each setting. In order to apply research from
this field to problems such as those above (e.g., Go), one first has to build a model of the game under
consideration. This involves defining a state space (e.g., the possible configurations of the stones in
Go), actions which empower players to modify the world (e.g., choosing where to put a stone), and
possibly stochastic state transitions (e.g., adding the stone to the board). Game theory also studies
the notion of solution concepts: what does it mean to solve a game? In some classes of games this
is well-understood, in others there is no prescriptive solution. In summary, applying game theory
requires both constructing a suitable abstract model of the game (i.e., a binding from the real world
to game-theoretic language) and also solving the game (i.e., running an algorithm on the constructed
model to extract an optimal strategy or strategy profile).

However, all of these success stories are on games in the colloquial sense of the word. These are
card, board, or video games, i.e., abstractions meant to entertain and prepare people for “real world”
interactions where strategy has great impact, e.g., negotiating a business deal or garnering support for
a new economic policy. Extending game theoretic approaches to these settings is non-trivial because
natural language is not explicitly built into the foundations of game theory. In order to tackle these
important interactions, we need a way of easily constructing models of them that respect their native
linguistic medium.

Large Language Models as Simulacra, Assistants & Tools Large language models have advanced
to a point where their generative capabilities can enable realistic, human-like simulations of dialogue
between humans (Park et al., 2023a). They allow us to ask hypothetical and counterfactual questions;
for example, if we were to rephrase an email, how might the negotiations have proceeded and
ultimately concluded?

Large language models are generative models that take as input a string and produce a distribution

2

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

over tokens. In order to generate an output string, an LLM autoregressively samples tokens given
its (growing) input. The LLM’s output is a deterministic distribution over the next token. Important
to our purposes, a) the LLM’s output is deterministic given a fixed random seed, and b) we know
probabilities over its possible tokens which is useful in settings where we ask the LLM multiple choice
questions aand retrieve the LLM’s most probable answer. In this way, an LLM can be abstracted as a
black-box that takes as input a string, a random seed (integer) and outputs another string.

Our Goal: Model and Solve Strategic Dialogue as a Game In this work, we develop a binding from
conversational dialogue to the language of game theory, leveraging LLMs as a stochastic black-box
probabilistic transition operator between states of the game.

Developing this framing allows us to re-use off-the-shelf techniques to solve for equilibria, but also
provides a new perspective paving the way to new algorithms with alternate interpretations native to
the space of language and dialogue. We use both of these to construct an improvement operator to
enable LLMs to perform better in multiagent settings.

We will first provide a brief background of some of the key concepts of game theory along with
key solution concepts and their computational tractability (Section 2). We then introduce our framing
of dialogue as a formal game along with our open source codebase chat_games (Section 3). Given
this framing, we translate classical game-theoretic solvers into the dialogue setting and explain how
they can be applied in ways that are potentially new to game theory and LLM research independently
(Section 4). We introduce three domains we study (Section 5) and then present experiments providing
empirical support for an LLM improvement operator (Section 6). Lastly, we touch on related work and
conclude with limitations and ethical concerns. A link to a short talk accompanying this paper can be
found on the website for the DIMACS Workshop on Foundation Models, Large Language Models, and
Game Theory.

2. Background

Sequential (extensive-form) imperfect information games represent one of the most general classes of
games studied in game theory and also make for an appropriate model of the turn-taking nature of
dialogue between players with private information. We begin with a background of this game class.

2.1. Sequential (Extensive-Form) Imperfect Information Games

Extensive-form games (EFGs) represent sequential (temporally-extended) interactions between play-
ers. Every game starts with an empty history and players take actions, appending them to the action
history, until the end of the game (a terminal history) is reached. For example, chess and go are exam-
ples of extensive-form games. Formally, an extensive-form game is a tuple ⟨N , 𝑐, A, H , Z, 𝑢, 𝜏, S⟩,
where:

• N = {1, 2, · · · , 𝑛} is a set of 𝑛 players. There is also a special player, 𝑐, called chance (or

“nature”).

• A is a finite set of actions players can take. This is a global set of state-independent actions;
generally, only a subset of legal actions are available to each player at each decision point.
• H is a finite set of histories. Each history is a sequence of actions (including chance node

“actions” or outcomes) taken from the start of the game.

• Z ⊆ H is a set of terminal histories that each represent a finished (completely played) game.

3

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

Figure 1 | Kuhn poker game tree (Lanctot et al., 2019, Figure 6). Black circles denote chance player
decision points, blue squares to player 1 decision histories, and red squares to player 2 decision
histories. Dotted boxes denote information states. Values in the diamond correspond to player 1’s
payoff, with player 2’s payoff being the inverse of player 1’s.

• 𝑢 : Z → Δ𝑛

𝑢 ⊆ ℜ𝑛, where Δ𝑢 = [𝑢min, 𝑢max] is a utility (or payoff) function assigning each player
a payoff at the end of the game, and 𝑢min, 𝑢max are lower and upper bounds on those payoffs.

• 𝜏 : H → N ∪ {𝑐} is a player identity function; 𝜏(ℎ) indicates whose turn it is.
• S is a set of states. In general, S is a partition of H such that each 𝑠 ∈ S contains histories
ℎ ∈ 𝑠 that cannot be distinguished by players other than 𝜏(𝑠) = 𝜏(ℎ) for all ℎ ∈ 𝑠. Decisions are
made by players at these states. There are several ways to precisely define S as stated below.

We denote the legal actions available at state 𝑠 as 𝐴(𝑠) ⊆ A. Importantly, a history represents
the true ground/world state: when agents act, they change this history, but depending on how the
partition is chosen, some actions (including chance’s) may be private and not revealed to some players.

There are several categories of extensive-form games based on the relationships of payoffs across
players. A constant-sum game is one where ∀𝑧 ∈ Z : (cid:205)𝑖∈ N 𝑢𝑖 (𝑧) = 𝑘, for some constant 𝑘. A
zero-sum game is a constant-sum game with 𝑘 = 0. An identical interest game is one where
∀𝑧 ∈ Z, ∀𝑖, 𝑗 ∈ N : 𝑢𝑖 (𝑧) = 𝑢 𝑗 (𝑧). Finally, there are general-sum games which have no constraints on
the payoffs.

There are also two main categories of extensive-form games based on the information available to
players. An imperfect information game is one where there is generally more than one history per
state: ∃𝑠 ∈ S : |𝑠| > 1. That is, the state 𝑠 could be in a true world state ℎ, but 𝑠 could contain histories
which contain information private to 𝜏(𝑠). In these games, the states 𝑠 are often called information
states (or infostates for short) because they correspond to the set of histories that correspond to one
player’s private information, but not others. In contrast, in a perfect information game, like go or
chess, the true world state is always known, so ∀𝑠 ∈ S : |𝑠| = 1.

Extensive-form games are often depicted using game trees. Figure 1 shows an example of a
simplified two-player, zero-sum, imperfect information game called Kuhn poker (Kuhn, 1953). Each
player starts the game with two chips, one of which must be contributed to the pot as ante. There
is a three-card deck containing Jack, Queen, and King. The root node corresponds to the empty
history and the first two turns are taken by the chance player. In the first turn, chance deals one of the

4

 Deal:0

 Deal:1

 Deal:2

 Deal:2

 Deal:1

 Deal:2

 Deal:0

 Deal:1

 Deal:0

0

0

1

1

2

2

 Bet

 Bet

 Pass

 Pass

 Pass

 Pass

 Bet

 Bet

 Pass

 Pass

 Bet

 Bet

2b

2b

2p

2p

1p

1p

1b

1b

0p

0p

0b

0b

 Pass

 Bet

 Pass

 Bet

 Pass

 Pass

 Bet

 Bet

 Pass

 Pass

 Bet

 Bet

 Pass

 Bet

 Bet

 Bet

 Pass

 Bet

 Pass

 Pass

 Pass

 Bet

 Pass

 Bet

1

-2

1

-2

-1

-1

0pb

0pb

-1

1

1pb

1pb

1

2

2pb

2pb

1

-2

1

1

1

2

1

2

 Pass

 Bet

 Pass

 Bet

 Pass

 Bet

 Pass

 Bet

 Pass

 Bet

 Pass

 Bet

-1

-2

-1

-2

-1

-2

-1

2

-1

2

-1

2

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

three cards (privately) to player 1. In the second turn, chance deals one of the two remaining cards
(privately) to player 2. Chance plays with a fixed stochastic strategy of choosing a card uniformly
at random. Information states (groupings of histories) are denoted by dotted lines; there are 12 in
total, 6 per player. Then, play continues in a turn-based alternating order starting with player 1. Each
player has two actions: bet, or pass. If a player bets (raise for player 1 or call for player 2), they must
put their remaining chip into the pot. If a player passes (check or fold), they do not contribute to
the pot, and a player loses if they pass after an opponent’s raise. A showdown occurs if both players
contributed the same amount to the pot, and the winner is the player with the highest card. The
winner takes all the chips in the pot and a player’s payoff is their number of chips after the game
minus 2 (the amount before the game). Note that the betting actions are public information, and the
cards are not revealed during game play, so every state contains exactly two histories: each contains
a different choice of card that could have been given to the opponent.

2.2. Solution to Games

Given a definition of a game, we have to define a metric which describes desirable outcomes of the
players of the game. This usually takes the form of players maximizing their payoffs (also called
rewards), or minimizing some cost. Note that this is an inherently multi-objective problem: in
games, the behaviour of players influences the payoffs of the other players. The interactions between
agents may be purely competitive, purely cooperative, or a mixture between the two. Multi-objective
optimization results in a Pareto-front of possible solutions, of which multi-objective optimization
alone does not prescribe which solution is the best trade-off. Multiagent problems, however, have
additional considerations. Because players have agency, they can unilaterally choose their own actions.
Therefore, most game-theoretic solutions also include a notion of stability (or equilibrium): no player
should have incentive to unilaterally deviate away from that solution. Each game often has a set of
combinations of player actions that are in equilibrium.

The most famous solution to a two-player zero-sum game is John von Neumann’s minimax solution
(von Neumann, 1928). The resulting actions are unexploitable, and if there are multiple solutions, they
are interchangeable. The Nash equilibrium (Nash, 1951) is a famous generalisation of this solution
concept to many-player games. A Nash equilibrium is a strategy profile (meaning a mixed strategy
for every player in the game) under which no player has an incentive to unilaterally deviate. Many
other rich solution concepts have been defined (Aumann, 1974; Hannan, 1957).

2.3. Tractability of Solution Concepts

Solution concepts have different existence and computability properties depending on the properties
of the game model class. In particular, our later choice of a discrete action set over a continuous one,
e.g., the set of weights of the underlying LLMs, is inspired by recent computational hardness results for
computing approximate local Nash equilibria in the case of continuous strategy sets even in two player
zero-sum games. Specifically, Daskalakis et al. (2021) show that, in the case of constrained min-max
optimization problems with nonconvex-nonconcave objectives and linear constraints, even when
the objective is a Lipschitz and smooth differentiable function, deciding whether an (approximate)
min-max point exists, is NP-hard. Even when an approximate local min-max point of large enough
approximation is guaranteed to exist, finding one such point is PPAD-complete.

In follow-up work, Assos et al. (2023) show that PSRO-style (Lanctot et al., 2017) approaches
actually lead to tractable notions of approximate local Nash equilibria even in continuous games.
PSRO (Policy-Space Response-Oracle) algorithms start with a set of initial policies for each player
and then alternate between two steps. In one step, a normal-form meta-game (e.g., matrix game) is

5

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

constructed in which each player may select a policy to play the game on their behalf. A meta-solver
(e.g., Nash solver) returns a solution (e.g., Nash equilibrium) of this meta-game. In the next step, each
player computes an approximate best response to this meta-strategy, meaning each player attempts to
improve their payoff in the game assuming the other players are fixed to sampling policies according
to the meta-strategy to play on their behalf. The process repeats until no player is able to gain by
deviating. This route of defining a discrete mesh over a continuous strategy space is something we
exploit later in Section 4 to construct algorithms that act directly in “prompt-space”.

3. Framing Dialogue as a Game

In order to leverage the wealth of research in computational game theory for conversational agents,
we must abstract and map components of dialogue to the symbolic language outlined in the previous
section; recall, we refer to this mapping as a binding from dialogue to game theory. Note the binding
we choose is not unique; many are possible. What is important is to construct a model of dialogue in
natural language with an awareness that this is a modelling choice; the existence of this choice is
inherent to any discipline of applied mathematics that builds and solves mathematical models. We
will explore an approach later that can help to modify and improve this model automatically given an
initial model.

Some design choices may seem natural; for example, observations of infostates should include the
dialogue history. Others are more difficult. For instance, what are actions in natural dialogue? Is a
single LLM utterance (string) an action? In one sense, yes, however, note that if we choose to model
the LLM utterance as a token or action, our solvers will aim to find the optimal string utterance at
every infostate, completely bypassing the incredible strength of an LLM as a System 1 (Daniel, 2017)
foundation model capable of generating natural language utterances. LLMs are pre-trained on vast
amounts of data, endowing them with a strong capability to converse in natural language. We argue
that we should aim to take advantage of this baseline capability and instead consider how we might
hone the LLM on higher level dimensions of dialogue.

If we want to retain the pre-trained capability of the LLM and still modify it in some way to
influence its behavior, then maybe we should consider slightly modifying the weights, possibly via low-
rank fine-tuning (LORA) (Hu et al., 2021). This is reasonable, however, it presents two disadvantages.
For one, as discussed in Section 2.3, the primary de-facto solution concept in games, equilibria, do
not necessarily exist in the setting where player’s strategy sets are continuous weight spaces in ℝ𝑑.
Moreover, a satisfactory definition of local equilibrium continues to evade research although progress
is being made. The other disadvantage is that the weight space of an LLM is not an easily interpretable
or semantically meaningful strategy space.

There is another route forward. For example, we might rely on the LLM to generate grammatically
correct utterances, but aim to explicitly control its tone, attempt to influence its personality3, or
restrict the information it should reveal. In the case of tone, as we will show later, one could specify a
finite set of instruction prompts, one per tone (e.g., “assertive”, “submissive”) that are fed to the LLM
along with the dialogue history to influence its output. Similarly, FAIR et al. (2022) conditioned their
generative language model on “intents” in prior work on the board game Diplomacy. This technique
of “lifting” the action space to “prompt-space” enables it to remain both finite and interpretable. Later
on, we will describe how to deploy equilibrium finding algorithms that traverse game trees, e.g., via
more System 2 like probabilistic search (Burch et al., 2012), and return optimal solutions over these
“lifted” action spaces.

3Note that we are agnostic on the theoretical question of whether current models exhibit personalities in a psychological

sense... we merely rely on the relatively consistent behavior of the model when prompted.

6

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

Another question is how to model payoffs. Does dialogue naturally create payoffs for human
interactants? In some settings, like business negotiations, dialogue maps to clear monetary payoffs.
However, what about a brief chat at the office water cooler or a team ice breaker? Some might argue
these “coffee chats” are important to a person’s career or the success of a company. If these are in fact
important, then we should model them as achieving some quantifiable payoff. One could imagine
designing a reward model simply as an LLM that performs sentiment analysis on the dialogue history
and returns how content “Alice” is with the conversation. Reward models can even be combined to
model that a business deal agreed upon between two amicable parties is more rewarding than one in
which another party feels slighted.

We now provide our formal framing of dialogue as an extensive-form game, i.e., a tuple ⟨N , 𝑐,
A, H , Z, 𝑢, 𝜏, S⟩. The players N , histories H , and player identities are defined the same as in
Section 2.1; we define the rest below to match Section 2.1.

• A is a finite combinatorial set of players (message recipients) and strings. The strings are used
to format prompts passed as input to an LLM. In all our games, all actions are legal at each
state. Note, one could extend the action set to include a choice of which LLM to call (e.g., PaLM
2 S or PaLM 2 L) as well; the only requirement is that this space remain finite. Note that by
convention, the action space of a chance node is the set of stochastic outcomes; in our case, this
is the finite set of seeds used by the LLM.

• Z ⊆ H is a set of terminal histories determined by either a limit on the allowable tree-depth
(we only allow a finite number of messages per player) or by an LLM tasked with determining
whether a conversation has ended (e.g., a deal has been struck and no further conversation is
required).
• 𝑢 : Z → Δ𝑛

𝑢 ⊆ ℜ𝑛, where Δ𝑢 = [𝑢min, 𝑢max] is a utility (or payoff) function assigning each player
a payoff at the end of the game. We construct a prompt specific to the domain at hand and let
an LLM quantify the reward to each player via chain-of-thought and ultimately parsing (Kwon
et al., 2023; Wei et al., 2022).

• S is a set of infostates. As stated earlier, each infostate 𝑠 represents a partition of H such that
each ℎ ∈ 𝑠 cannot be distinguished by players other than 𝜏(𝑠) = 𝜏(ℎ) for all ℎ ∈ 𝑠. In our games,
we implicitly define these partitions by defining what information is public versus private to
player 𝜏(𝑠). We consider each player’s infostate to be the observable dialogue history along with
their own action history and private information. In each of our games, all sent messages are
appended to a public thread making the entire dialogue history public knowledge. Note this
still omits certain private information like each player’s action, e.g., the precise tone a message
is written in although this might be approximately inferred by a player. Alternatively, the full
dialogue history can be replaced with a summary or tensor representation. We only require
each player have knowledge of their own action history to maintain perfect recall4.

Figure 2 illustrates how a game history ℎ ∈ H , i.e., a sequence of actions, reconstructs a dialogue
no differently to how a sequence of actions reconstructs a game of Kuhn poker as in Figure 1. Recall
that after every seed action by a chance node, a corresponding string is formatted and passed to an LLM
along with the seed to deterministically generate a subsequent string as output; these outputs form
the observed dialogue history and implicitly define the corresponding infostate partitions. Figure 7(c)
in Section 6 visualizes a diagram of the LLM’s use here as a black-box transition operator.

4Without this property, solving a game can be substantially more expensive computationally.

7

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

Root

Game Seed 1

Game Seed 2

𝑦% Assertive

(1 − 𝑦)% Submissive

𝑧% PaLM 2 S

(1 − 𝑧)% PaLM 2 L

Seed 1

Seed 2

Seed 3

Seed 4

Seed 5

Seed 6

Seed 7

Seed 8

𝑥% Helpful

(1 − 𝑥)% Adversarial

Seed 5

Seed 6

(𝑎, 𝑏)

(𝑐, 𝑑)

Figure 2 | Dialogue game tree. In the general EFG setting, actions can be heterogeneous across
infostates. For example, in one case, an agent must choose between writing a message in an “assertive”
or “submissive” tone; in another, a “helpful” or “adversarial” tone; or choose between querying one
LLM versus another (actions do not have to be strings). The onus lies on the user to model their
game setting and abstract away the key decisions in front of the agent. Later sections show how to
automate the process of improving the model given some initial specification. Legend: Black circles
denote chance player decision points, blue squares to player 1 decision histories, and red squares
to player 2 decision histories. Dotted boxes denote information states. Values below the diamonds
correspond to the payoffs for player 1 and player 2, respectively.

3.1. Procedural Generation of Dialogue Games

Note that many of the key components of our game tuple are defined by strings. Also recall that
LLMs take strings as inputs and produce strings as outputs. This means that we can feed our game
definitions to LLMs to produce new game definitions that adhere rigorously to the language of game
theory. We make use of this later in experiments to generate and solve a universe of games that we
then imitate to create a more strategic LLM.

3.2. chat_games: An OpenSpiel Environment

We have open sourced a codebase to accompany our experiments and framing of dialogue as a
game. We chose to release our codebase chat_games as part of OpenSpiel (github.com/google-
deepmind/open_spiel/.../chat_games), a game theory engine with a large community of contributors.
OpenSpiel provides several game-theoretic solvers already; this allows someone without experience
in computational game theory to focus on the modelling of their dialogue game rather than how to
design and implement a game solver. Furthermore, our implementation is flexible enough for users
to define games simply by writing the instructions and context for LLMs as strings. We hope this lets
users experiment to construct and solve new games.

We provide several sample game configurations in the codebase. We walk through an example of
a fruit trading game below. First, we define an observation for each player. Note this observation is

8

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

not what the LLM observes when generating a response in the dialogue. This observation is external
to the game and only used to save data offline. In the configuration below, this observation defines
components for constructing a prompt that will ask an LLM to summarize a dialogue history. This
summary can then be transformed using a neural tokenizer (Raffel et al., 2020) to produce a feature
vector (embedding) for later imitation learning.

1

2

3

4

5

6

7

8

config = config_dict.ConfigDict()

num_players = 2

observations = [

obs_utils.Observation(summary.PREFIX, summary.POSTFIX)
for _ in range(num_players)

]

Next, we define a header. The header is a structured object that specifies a string that is to be
populated with private information, action information, context, and formatting for prompting the
LLM to generate a message.

1

header = env_trade_fruit_with_tone_info.HEADER

For example, an LLM negotiating fruit might be passed the dialogue history followed by the header
below which has already been formatted with its private information (fruit endowment and fruit
valuations), its intended action (a “calm” tone), its intended message recipient (Suzy) and its own
name (Bob).

Fruit Endowment:
apple: 1
banana: 2
blueberry: 0
kiwi: 0

Fruit Valuations:
apple: 10
banana: 5
blueberry: 1
kiwi: 3

Tone: Use a calm tone.

############################
Schedule Proposal Message:
from: Bob
to: Suzy
############################

Next, payoffs specifies a list of structured payoff objects that can be combined using a user-
defined aggregation function. A payoff consists of an LLM prompt, min and max payoff (utility)
values, as well as prompts useful for an LLM to transform an input (e.g., dialogue history) into a
string containing information more pertinent to payoff calculations.

1

payoffs = [payoffs_trade_fruit.PAYOFF]

9

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

example_names simply consists of a list of names as strings that an LLM uses to generate new
names. For example, the names Bob and Suzy could be procedurally generated by an LLM given an
initial list of names.

1

examples_names = names_trade_fruit.NAMES

We can also define the finite set of actions that we would like our game to consider. All players

are assumed to have the same set of actions.

1

2

3

4

1

2

3

4

5

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

given_prompt_actions = collections.OrderedDict()
tones = [""calm"", ""assertive"", ""submissive"", ""any""]
given_prompt_actions[header.action_keys[0]] = tones
num_tones = len(tones)

If we want to procedurally generate new games, we can provide an initial list of examples of

private information.

examples_private_info = collections.OrderedDict()
examples_private_info[""fruit_endowment""] = [scenario_trade_fruit.ENDOWMENT_A,
scenario_trade_fruit.ENDOWMENT_B]

examples_private_info[""fruit_valuations""] = [scenario_trade_fruit.VALUATION_A,
scenario_trade_fruit.VALUATION_B]

Similarly, scenarios are structured objects that can be used to generate more scenarios (i.e., new

games). A scenario defines the initial context for a dialogue (e.g., an initial email).

scenario_a = env_trade_fruit_with_tone_info.Scenario(

scenario_trade_fruit.SCENARIO_A,
""Bob"",
""Suzy"",
scenario_trade_fruit.ENDOWMENT_A,
scenario_trade_fruit.VALUATION_A,
""calm""

)

scenario_b = env_trade_fruit_with_tone_info.Scenario(

scenario_trade_fruit.SCENARIO_B,
""Jill"",
""George"",
scenario_trade_fruit.ENDOWMENT_B,
scenario_trade_fruit.VALUATION_B,
""calm""

)

examples_scenarios = [scenario_a, scenario_b]

Similar to observations and payoffs, an LLM termination prompt is a structured object that contains
prompts for pre-processing the dialogue history using an LLM and then determining whether a given
history is terminal by again prompting an LLM.

1

llm_termination_prompt = scenario_trade_fruit.LLM_TERMINATION_PROMPT

10

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

In addition to using an LLM, we can designate terminal histories (Z) by limiting the maximum
number of replies per player (below). We can also specify the number of chance node outcomes (LLM
seeds). Recall that the action space is combinatorial (recipient × tone) and so we define the number
of actions accordingly. The params dictionary is passed to OpenSpiel to alert it to critical properties
of the game that remain fixed (although num_llm_seeds and num_max_replies are specific to
our chat_games).

params = {""num_distinct_actions"": num_players * num_tones,

""num_llm_seeds"": 2,
""num_players"": num_players,
""min_utility"": min([float(p.min) for p in payoffs]),
""max_utility"": max([float(p.max) for p in payoffs]),
""num_max_replies"": 1}

config.params = params

Lastly, we incorporate these definitions into the configuration dictionary. Note that below, we are
asking the LLM to generate 10 names (each new game will randomly draw from this list) and 3 of
each type of private information (fruit endowment and valuation). The LLM list suffix is a simple
modification to help the LLM generate new items given an initial list (e.g., of names).

config.game = config_dict.ConfigDict()
config.game.observations = observations
config.game.header = header
config.game.payoffs = payoffs
config.game.given_prompt_actions = given_prompt_actions
config.game.num_names = 10
config.game.num_prompt_actions = (num_tones,)
config.game.num_private_info = (3, 3)
config.game.examples_names = examples_names
config.game.examples_private_info = examples_private_info
config.game.examples_scenarios = examples_scenarios
config.game.llm_list_suffix = ""Output: ""
config.game.llm_termination_prompt = llm_termination_prompt

1

2

3

4

5

6

7

1

2

3

4

5

6

7

8

9

10

11

12

13

This config dictionary defines the parameters of interaction between players in a dialogue in
manner that binds precisely onto a game tree. We use configs like this one later in experiments to
study three natural language settings: scheduling a meeting, trading fruit, and debate.

4. Algorithms

Formally framing dialogue as a game also allows us to start thinking about language as strategies,
opening up new interpretations and/or variants of game-theoretic approaches. For example, we can
construct a version of Policy-Space Response-Oracles (PSRO) where an approximate best response
can be generated by sampling new random prompt strings, evaluating them against the current
equilibrium, and then returning the one with highest expected payoff; we provide pseudocode for an
approximate best response operator in Algorithm 1.

Contrast this against the standard PSRO protocol in which an approximate best response is
achieved using reinforcement learning (RL) and/or gradient-based optimization. Tasking the LLM
with exploring and generating novel candidates avoids these expensive learning procedures while
also enabling new, more powerful modes of search, as we explain below.

11

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

Algorithm 1 Shotgun Approximate Best Response
Input: Focal agent 𝑖
Input: Current joint policy 𝜋
Input: Number of shotgun candidates 𝑘

𝐶 is the current action set with their scores under 𝜋
for 𝑡 = 1 ≤ 𝑘 do

Prompt LLM to generate new candidate 𝑐𝑡 ∩ 𝐶 = ∅
Evaluate candidate 𝑐𝑡 against policy 𝜋−𝑖 to give score 𝑠𝑡
𝐶 = 𝐶 ∪ {(𝑐𝑡, 𝑠𝑡)}

end for

Output: 𝑐𝑡 with max 𝑠𝑡

Algorithm 2 Approximate Better Response
Input: Focal agent 𝑖 and its score 𝑠∗ under 𝜋
Input: Current joint policy 𝜋

while 𝑠 ≤ 𝑠∗ do

Prompt LLM to generate new candidate 𝑐
Evaluate candidate 𝑐 against policy 𝜋−𝑖 to give score 𝑠

end while

Output: 𝑐

As an alternative to the random search process just described, we could define a “better response”
and build it as a while loop that generates and evaluates a prompt string on each loop iteration.
The loop terminates when a better string is found (as measured by an approximate evaluation). See
Algorithm 2 for pseudocode.

This rejection sampling approach might be slow. Instead, we could improve the “best response”
operator by providing the LLM with information of the action-fitness landscape and ask it to climb
it. We provide pseudocode in Algorithm 3. A similar approach was previously explored in Fernando
et al. (2023) and Yang et al. (2023).

Algorithm 3 Trajectory-Aware Approximate Best Response
Input: Focal agent 𝑖
Input: Current joint policy 𝜋
Input: Number of candidates 𝑘

𝐶 is the current action set with their scores under 𝜋
Order 𝐶 by their scores in ascending order
Prompt LLM to generate 𝑘 new candidates in order of ascending score given ranked 𝐶
Evaluate new candidates against policy 𝜋−𝑖 to give scores

Output: 𝑐𝑡 with max 𝑠𝑡

Lastly, we can blur the lines between game modelling and game solving by prompting an LLM to
generate new dimensions of a combinatorial action space. For instance, imagine we constructed an
action space consisting only of the tones and styles in which an LLM will generate a response. Is that
the full-space in which we would like to explore writing a successful message?

For example, Wheaton College’s Writing Center5 suggests diction and voice as important categories

5https://www.wheaton.edu/academics/services/writing-center/writing-resources/

12

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

Algorithm 4 Categorical Approximate Best Response
Input: Focal agent 𝑖
Input: Current joint policy 𝜋
Input: Number of candidates per category 𝑘
Input: Number of category candidates 𝑘′

𝐶 is the current set of action categories with their (Nash) average scores under 𝜋
Order 𝐶 by their scores in ascending order
Prompt LLM to generate 𝑘′ new candidate categories in order of ascending score given ranked 𝐶
Prompt LLM to generate 𝑘 candidates for each new action category
Evaluate new candidates against policy 𝜋−𝑖 to give scores

Output: Category with highest average score

of writing. We could ask the LLM to generate these (and possibly other categories) given “tone” and
“style” as input. It could then generate possible actions for each of the new action categories (e.g.,
“diction”, “voice”) according to their definitions. We provide pseudocode in Algorithm 4.

These are just several among many ways in which LLMs can be used to construct approximate best
responses and assist in expanding the dialogue game model. In some cases, large language models
can write better prompts than experts. For example, Fernando et al. (2023) deployed evolutionary
algorithms to iterate and improve prompts. By pairing a prompt-improvement technique with a game
solver (e.g., replicator dynamics; Weibull, 1997), we will now show that Prompt-Space Response-
Oracles (Algorithm 5) can drive improvement of LLMs in multiplayer dialogue settings.

Algorithm 5 Prompt-Space Response-Oracles
Input: 𝐶 where 𝐶𝑖 is the initial prompt action set (singleton) for player 𝑖
Input: ℎ containing hyperparameters for approximate best response operator BR

Compute expected payoff tensor 𝑃 over joint action(s) 𝐶
𝜋 is uniform meta-strategy profile over 𝐶
incomplete = True
while incomplete do
for player 𝑖 ∈ [𝑁] do

𝑐𝑖 ← BR(𝑖, 𝜋, ℎ), e.g., Algorithms (1-4)

end for
if 𝑐𝑖 ∈ 𝐶𝑖 ∀𝑖 ∈ [𝑁] then

incomplete = False

else

𝐶𝑖 ← 𝐶𝑖 ∪ 𝑐𝑖 ∀𝑖 ∈ [𝑁]
Compute expected payoff tensor 𝑃 over joint actions 𝐶
𝜋 ← meta-strategy w.r.t. 𝑃

end if
end while

Output: (𝜋, 𝐶, 𝑃)

style-diction-tone-and-voice/

13

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

5. Language Game Domains

We now describe a few example game scenarios that we use to evaluate two families of methods
in experiments. Specifically, we explore PSRO (Algorithm 5) using a shotgun approach for a best
response operator (Algorithm 1); we leave the other best response operators (Algorithms 2-4) to
future research and experimentation. And we also explore an off-the-OpenSpiel-shelf counterfactual
regret minimization (CFR) approach which we extend to unseen domains via imitation learning.

5.1. Scheduling a Meeting

In this introductory game, players attempt to schedule a meeting through a multi-turn negotia-
tion (Davidson et al., 2024). Each player begins with a set of allowable days of the week, i.e., days in
which they are available to meet: “Sunday”, “Monday”, . . . , “Saturday”. They also have non-negative
valuations over each day of the week (distinct from the allowable days). Both of these pieces of
information are private to the players. Players can choose to reveal this information if they wish.
Naturally, their actions here are the days of the week on which they propose to meet. The game
rewards players according to how much they value the agreed upon day, and receive zero reward if
no agreement is made.

5.2. Trading Fruit

In this fruit trading domain, each player begins with a private endowment of fruit (i.e., a fruit basket)
as well as private valuations over types of fruit. Players are rewarded by the difference in value
between their basket after the trade and that before the trade. In addition, players can choose to adjust
the “tone” of their negotiations. Previous work (Chawla et al., 2023) has argued that personality has
an impact on negotiations in natural language. Similarly, we explore the use of tone here. Below, we
provide an example dialogue in this game.

############################
Trade Proposal Message:
from: David
to: Yair
############################

Hi Yair,

I would like to trade you 2 blueberries and 1 kiwi for 1 banana.

Would you like to trade with me?

Best,

David

############################
Trade Proposal Message:
from: Yair
to: David
############################

Hi David,

I would really like to have your kiwis. However, I have a banana
and an apple. I would be willing to trade the apple or 2 bananas for 3

14

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

kiwis.

What do you think?

Best,

Yair

############################
Trade Proposal Message:
from: David
to: Yair
############################

Hi Yair,

I would be happy to trade you 3 kiwis for your apple.

Does that work?

Best,

David

5.3. Public Debate

Recent work shows that LLMs can aid humans in finding agreement despite diverse preferences (Bakker
et al., 2022). Other work shows that game theory can help to persuade humans of natural language ar-
guments (Rosenfeld and Kraus, 2016). In addition, many researchers argue that debate can help make
AI systems more safe and truthful (Brown-Cohen et al., 2023; Irving et al., 2018; Michael et al., 2023).
Here, we explore how an LLM might adjust its argument style to win a debate. Specifically, we present
two LLMs with an argument topic; one is tasked with arguing for the statement, one against. For an
action space, we use Aristotle’s three modes of persuasion: “logos”, “ethos”, “pathos”. Each player’s per-
formance in the debate is scored between 0 and 1. We considered twenty different debate topics taken
from https://www.englishclub.com/speaking/agreeing-disagreeing-topics.php.

6. Experiments & Results

Our experiments aim to evaluate to what extent a game-theoretic solver can guide the reasoning
process of an LLM. Since LLMs generate outputs in open-ended language, this evaluation requires
assessing both our game-theoretic model as well as the performance improvement provided by
game-theoretic solvers. In all experiments, our base LLM is a PaLM 2 S (bison; Google and et al.,
2023).

6.1. Evaluation of the Game Model

First, we evaluate whether the designated actions are reflected in the messages generated by the LLM.
Specifically, given an LLM generated a message 𝑚 conditioned on a prompt formatted with an action
𝑎, we would like to determine if 𝑎 is actually the most likely action conditioned on 𝑚 using a held-out
model 𝑝, i.e., 𝑎 = arg max𝑧∈ A 𝑝(𝑧|𝑚). We use the same LLM as our held-out model 𝑝.

Remark. Note there may be cases in which two actions are indistinguishable for a single message (e.g.,
stern, frank, terse may result in the same message). This is not necessarily a failure of the model, but a

15

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

No Definitions
With Definitions

Random Logos Ethos
88% 32%
98% 56%

33%
33%

Pathos Total
47% 59%
61% 75%

Samples
80
100

Table 1 | Grounding of Actions. What percentage of the time does the LLM recognize the action it has
taken in the message it generates? What if we provide definitions of the actions in the context of the
LLM instructions?

natural byproduct of the ambiguity of conversational dialogue yet is entirely admissible in games with
imperfect information. Sometimes we cannot infer the underlying intent of another speaker based on a
given utterance.

6.2. Evaluation of LLMs as Reward Models

Next, we discuss the accuracy of the LLM-based reward model. It is difficult to automate the evaluation
of our reward model, because it is difficult to extract from the natural language conversation the
exact deal (or no deal) that is agreed upon with hand-coded parsing. We provide a few positive
demonstrations in Appendix A of using LLMs with prompting to compute rewards in the fruit trading
domain. One failure mode we noticed was that the LLM-based reward model would assume a trade
agreement had been reached (and calculate the corresponding trade value) even when the final sent
message was a counter proposal. As with many parts of this LLM-based game, any improvements
in the language models, prompting, or dialogue flow can lead to improvements in the ability of the
game to represent realistic interactions.

6.3. Game-Theoretic Solvers as Improvement Operators

Below, we explore two different algorithmic approaches to approximating equilibria. We then
demonstrate how a computed equilibrium policy can then be imitated to produce more strategic
dialogue on unseen games.

6.3.1. Counterfactual Regret Minimization

First, to demonstrate the power of constructing the game-theoretic binding as we did in Section 3, we
simply call OpenSpiel’s built-in counterfactual regret minimization (CFR) solver on our open sourced
chat_game. We do this for many games, procedurally generated for both the debate and meeting
scheduling (with days-of-the-week as actions) domains.

On average, we find in Table 2 that CFR returns an improved strategy over letting the LLM choose

its responses without in-context direction (“any” is passed to the LLM as an action in this case).

Domain
Debate
Schedule Meeting (DOW)

# of Samples Min/Max Payoff NashConv CFR Gain

328
67

0/1
0/20

0.024
0.417

0.106
1.596

Table 2 | Average NashConv and CFR Gain over procedurally generated games for both the debate
and meeting scheduling (with days-of-the-week as actions) domains. NashConv is a pseudo-distance
to Nash equilibrium and measures how much players can gain by deviating from the joint strategy
returned by CFR. CFR gain measures how much a player can gain by switching to the CFR strategy
from the baseline “any” strategy when in a population of baseline agents.

16

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

6.3.2. Prompt-Space Response Oracles

Section 4 outlined several approaches to approximating solutions (e.g., equilibria) in dialogue games.
Prompt-Space Response-Oracles (Algorithm 5) alternates between solving for an equilibrium of the
game and then approximating a best response to this equilibrium.

Figure 3 | Prompt-Space Response-Oracles (Algorithm 5) demonstration on meeting scheduling
domain where actions are tones. Which day of the week to propose is left up to the LLM.

In Figure 3 (𝑡 = 0), the fixed-action equilibrium distribution over the initial action set { “calm”,
“assertive”, “submissive”, “any” } is reported for the meeting scheduling domain. We solved for
this equilibrium using replicator dynamics (Weibull, 1997). Note that this game is general-sum,
in which case, replicator dynamics only guarantees convergence of the time average policy to a
coarse-correlated equilibrium (CCE).

Figure 4 | The same PSRO run as Figure 3 but additionally reporting the Nash bargaining solution in
red at each iteration for the meeting scheduling domain.

The remaining PSRO iterations (𝑡 > 0) generate approximate best responses (“angry” then “happy”
then “sad”) along with equilibrium approximations over each new support. This demonstrates the
algorithms ability to grow and refine the game’s original action space by introducing novel prompt

17

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

instructions into the LLM’s repertoire.

Interestingly, “submissive” is initially the most probable action at equilibrium. It is possible that
LLMs are susceptible to a “puppy dog eyes” tactic in which another LLM may exploit it by eliciting
a sense of responsibility or fairness (Park et al., 2023b). Recall that LLMs are trained to imitate
predominantly human text from the internet, and so have likely observed conversations in which one
speaker appealed to the other’s altruistic tendencies.

Note that equilibria are far from the only solution concepts proposed and studied in game theory.
In Figure 4, we contrast the Nash bargaining (NB) solution (red) with the equilibrium solution at
each iteration of PSRO (Li et al., 2023; Nash Jr, 1950). The Nash bargaining solution is the unique
solution to a two-person bargaining problem that satisfies the axioms of scale invariance, symmetry,
efficiency, and independence of irrelevant alternatives6. We mention this as one might want to train
LLMs to imitate this solution rather than the CCE equilibrium solution given the meeting scheduling
domain is precisely two-person bargaining. In general, expertise in game theory might be important
to specifying the right solutions for different settings. In this case, Figure 4 shows that NB and CCE
roughly agree in terms of their mixed strategies on the meeting scheduling domain.

Figure 5 | Prompt-Space Response-Oracles (Algorithm 5) demonstration on fruit trading domain.

We report the same experiments for the fruit trading game in Figures 5 and 6. Similarly to before,
“submissive” initially holds the most mass under the CCE (𝑡 = 0), however, it then gives way to more
passionate tones such as “assertive”, “angry”, and “enthusiastic” that may benefit a more aggressive
haggler. Inspecting Figure 6, it is interesting that “calm” is the final NB solution whereas “assertive”,
“angry”, and “enthusiastic” (and not “calm”) are the predominant actions under the CCE. Both players
may extract higher collective value if they remain “calm” during negotiations.

6.3.3. Generalization Performance

We now turn to leveraging game-theoretic solvers to construct optimal target policies that we can
imitate. These will serve as the basis for an improvement operator that can generalize to new domains.

In our experiments, we generate 200 games using our procedural game generation approach
described in Section 3.1. For each game, we use 10 iterations of OpenSpiel’s built-in counterfactual
regret minimization (CFR) to solve for an equilibrium. Note that these games are general-sum,
in which case, CFR only guarantees convergence of the time average policy to a coarse-correlated
equilibrium (CCE). Advances in solving dialogue games in the fashion we propose here will require
advances in both game modelling and game solving. For each game, we save vector observations of

6We set the value of “no deal” for each player to their minimum possible payoff minus 10−3.

18

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

Figure 6 | The same PSRO run as Figure 5 but reporting the Nash bargaining solution in red at each
iteration for the fruit trading domain.

each information state along with the optimal equilibrium policy returned by CFR for that infostate
(i.e., a length-768 string embedding (Raffel et al., 2020) paired with a distribution over num_tones
actions). We call the collection of such pairs our imitation dataset.

We then train a neural network policy to predict the equilibrium probabilities conditioned on the
observations (see Appendix B for architecture and training details). Finally, we deploy this trained
model against an LLM that only plays the action “any” on held out games.7 Figure 8a displays the
equilibrium distribution in game where a player is given the choice between these two models. We
find that more mass lies on our CFR imitation policy under the equilibrium distribution implying it
achieves higher payoff than the vanilla policy. Importantly, this implies our proposed approach results
in an improved policy.

7. Related Work

Our work deals with enabling LLMs to reason and plan in diverse strategic settings, where dialogue
determines the outcomes to multiple participants. In these domains, the utility of each agent depends
not only on their own choices in the dialogue, but also on the choices of the other participants.

Our work differs from prior work in primarily two respects: 1) we explicitly bind the interactions
between LLMs to the language of game theory (e.g., game trees) in a way that is easily modifiable (the
game, actions, etc are defined with strings) and amenable to procedural generation, and 2) enabled
by this representation, we solve these games using off-the-shelf solvers.

A substantial line of work already exists on how LLMs operate in strategic interactions. LLMs
are capable of in context learning in many settings, and in particular on reasoning on possible
outcomes (Kojima et al., 2022) or planning their actions (Liu et al., 2023a) so as to achieve their
strategic goals, however, different LLMs struggle on games in various ways (Heydari and Lorè, 2023).

7The intention is that an LLM guided to use “any” tone performs similarly to the original LLM with no guidance. We

chose this workaround due to codebase constraints.

19

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

Figure 7 | Imitating Strategic Play. a) A single dialogue game is randomly generated and CFR is run
to approximate a coarse-correlated equilibrium which constitutes a distribution over actions at every
infostate in the game. b) An embedding of every infostate in the game, obs(s), is stored along with
the CFR equilibrium. The set of all paired embeddings and distributions constitutes the imitation
learning dataset. A neural network 𝜓𝑤 : obs(𝑠) → Δ is trained to imitate this dataset. c) 𝜓𝑤 can then
be used on newly generated games to produce distributions over actions. These actions, e.g., “calm”,
are then used to format LLM prompts. Stochasticity of the LLM output is explicitly modeled with
chance nodes that uniformly sample random seeds to pass to the LLM. Given the prompt and seed,
the LLM generates a player response in natural language.

Gandhi et al. (2023) develop an automated “prompt compiler” that constructs demonstrations of
strategic reasoning, effectively teaching an LLM to solve games via in-context learning. Similarly to
our work here, FAIR et al. (2022) define an action space of “intents” for controlling their generative
language model. Jacob et al. (2023) use a game-theoretic model to improve the factuality of an LLM.

There are multiple analyses and surveys on reasoning with LLMs (Fried et al., 2023; Hao et al.,
2023; Huang and Chang, 2022) with some research examining how one may translate natural
language to concrete planning goals (Liu et al., 2023a; Xie et al., 2023). Despite encouraging
results, researchers have observed key limitations of LLM-based planning, and have generated LLM
benchmarks to quantify planning abilities (Liu et al., 2023b; Valmeekam et al., 2022).

Our domain requires not only planning and reasoning, but also the ability of taking into account
the behavior of other participants, including their beliefs, goals and intentions (as studied in game-
theoretic settings). There are some initial lines of research on LLMs in such strategic interactions,
such as how LLMs behave when playing repeated games (Akata et al., 2023), when using power
indices to select inputs (Patel et al., 2021), when participating in economic mechanisms such as
auctions (Chen et al., 2023), or when making collective decisions (Jarrett et al., 2023).

20

obs(s)

𝜑w(  )

.format(   )

&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
Now you are going to read a fresh dialogue, fruit 
endowment, and fruit
valuation information. Please give a reasonable 
response that attempts
to reach an agreement to trade fruit.
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

############################
Trade Proposal Message
from: Sarah
to: Sophia
############################

Hi Sophia,

Would you like to trade me 6 blueberries for 1 
apple?

I would like to trade because I have lots of 
blueberries and I would like more apples.

Best,

Sarah

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Tone: {tone}
Fruit Endowment:
apple: 0
banana: 0
blueberry: 6
kiwi: 5

Fruit Valuations:
apple: 3
banana: 4
blueberry: 3
kiwi: 3

############################
Trade Proposal Message
from: Sophia
to: Sarah
############################

LLM

prompt

s e e d

response

1234

Hi Sarah,

9999

Unfortunately, I do not have any apples 
that I am willing to trade at this time. 
I hope you are able to find someone else 
who has apples that they are willing to 
trade for blueberries.

Best, Sophia

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

(a)

(b)

Figure 8 | Proof-of-Improvement: Equilibrium Evaluation of Imitation Learned Policy against Baseline
LLM in (8a) scheduling a meeting and (8b) trading fruit.

One interesting related area is using LLMs as proxies for human behavior in strategic settings
and economic games (Jarrett et al., 2023; Park et al., 2023a; Vezhnevets et al., 2023). Initial results
indicate certain similarities and key differences between LLM and human behaviors (Fan et al., 2023;
Horton, 2023; Korinek, 2023). We hope the ability of our framework to procedurally generate LLM
interactions across many domains would be a useful tool in extending such studies.

8. Future Work

If the result of this work is that LLMs are, for example, “assertive” 𝑥% of the time
Societal Impact
and “submissive” 𝑦% of the time, how can we measure and/or predict their effect on human dialogue
or sentiment in society at large? When the car was invented, it took decades to mass produce and
gain adoption by a significant percentage of the population. LLMs, being a digital technology, could
see widespread adoption only a few years after their invention. In contrast to the adoption of cars in
the early 20th century, we have the capability to digitally simulate and forecast the impact of large
language models. For example, if LLMs are more assertive on average than the human population,
will they draw human society towards exhibiting more assertive personalities (Baumann et al., 2020)?
If LLMs are more rational and forward thinking, will they have a similar knock-on effect? Can we use
infinite-player theories such as mean field games (Dogbé, 2010; Muller et al., 2021) to forecast the
equilibria of these large techno-societal changes?

Active Learning of Optimal Equilibria Relative to transitions in chess, transitions in the dialogue
game we pose are extremely expensive. This is because LLM inference is expensive, both computation-
ally (time and space) and financially. If we want to approximate equilibria or search these game trees
efficiently, we will need to push scalable game-theoretic algorithms, e.g., (Burch et al., 2012), to new
heights. Moreover, we will need to be efficient in how we “label” payoffs at leaves of the game tree,
either using LLMs as reward models or relying on human feedback. In the latter case, the machine
learning subfields of active learning (or optimal experimental design) are already guiding some of
the research in computational game theory (Gemp et al., 2023). Lastly, it is important to not just
approximate equilibria as there may exist many in any single game, but select out ones that agree
most with our expected forecasts and/or are optimal with respect to additional criteria (Marris et al.,
2021; Zhang et al., 2023).

21

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

9. Conclusion

Dialogue is fundamentally a multiagent domain. However, algorithms and techniques developed
within computational game theory struggle to impact natural language conversation due to a failure
to embed rich dialogue in formal game-theoretic language. By doing so in this work, we open the
door for a broad body of game theory and multi-agent research—not just on algorithms but also
solution concepts and principled strategic reasoning—to pour into the AI guided interactions that
pervade humans’ daily life. Large language models form the critical bridge that makes the embedding
rich and useful. Humans are already delegating conversational responsibilities to LLMs and it is as
imperative that we begin to model and study these interactions as it was for some of the original
problems that drove the advent of game theory research decades ago.

Limitations

Our work has several limitations. As it combines LLMs and game theory, there are limitations in each
area separately but also in their union. LLMs have been used to construct simulacra of human behavior,
however, whether their fidelity is sufficient to draw conclusions on real world interactions is up for
debate (Agnew et al., 2024). In addition, although we only require LLM inference in our experiments
(i.e., no training or finetuning), this is still an expensive operation and traversing large game trees and
solving for equilibria requires many LLM calls. Also, writing prompts to construct and define reward
functions and context for a given dialogue scenario can be finicky and arduous work. Processing long
dialogues with LLMs requires long context lengths and / or techniques for intermittent summarization
that challenge their capabilities. On the other hand, our chat_games codebase is currently limited
in several respects: players are assumed to have the same payoffs and action spaces and several
assumptions are made on the exact structure of interactions. A more thoughtful and flexible design of
dialogue interactions in conjunction with an awareness of the necessary game-theoretic, symbolic
bindings could generate a more usable framework.

Ethics Statement

Strategic dialogue agents may be more “rational”, but even in well-intended settings, it is known
that seemingly benign behavior can lead to poor outcomes for the group, i.e., high price of anarchy /
stability (Nisan et al., 2007). Moreover, recent work has found that some of the beneficial social norms
humans have developed can collapse when people are allowed to leave their coordination decisions
to AI assistants (Shirado et al., 2023). In more nefarious cases, strategic agents can exploit others.
We should aim to create agents that are both rational, but also generate high welfare. By exploring
these interactions in simulation, we can learn how to constrain and regulate agents toward more
human-aligned behaviors. Designing algorithms to discover optimal (e.g, max-welfare) equilibria in
𝑛-player, general-sum games is an active area of research that can help guide the development of
more ethical agents.

Acknowledgements

We thank Kevin R. McKee, Alexander Sasha Vezhnevets, and Edgar A. Duéñez-Guzmán as well as the
attendees of DIMACS Workshop on Foundation Models, Large Language Models, and Game Theory
for helpful discussions and alerting us to related work.

22

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

References

W. Agnew, A. S. Bergman, J. Chien, M. Díaz, S. El-Sayed, J. Pittman, S. Mohamed, and K. R. McKee.
The illusion of artificial inclusion. In Proceedings of the 2024 CHI Conference on Human Factors in
Computing Systems, 2024.

E. Akata, L. Schulz, J. Coda-Forno, S. J. Oh, M. Bethge, and E. Schulz. Playing repeated games with

large language models. arXiv preprint arXiv:2305.16867, 2023.

T. Anthony, T. Eccles, A. Tacchetti, J. Kramár, I. Gemp, T. Hudson, N. Porcel, M. Lanctot, J. Pérolat,
R. Everett, et al. Learning to play no-press diplomacy with best response policy iteration. Advances
in Neural Information Processing Systems, 33:17987–18003, 2020.

A. Assos, I. Attias, Y. Dagan, C. Daskalakis, and M. K. Fishelson. Online learning and solving infinite
games with an ERM oracle. In The Thirty Sixth Annual Conference on Learning Theory, pages
274–324. PMLR, 2023.

R. Aumann. Subjectivity and correlation in randomized strategies. Journal of Mathematical Economics,

1(1):67–96, 1974.

M. Bakker, M. Chadwick, H. Sheahan, M. Tessler, L. Campbell-Gillingham, J. Balaguer, N. McAleese,
A. Glaese, J. Aslanides, M. Botvinick, and C. Summerfield. Fine-tuning language models to find
agreement among humans with diverse preferences. Advances in Neural Information Processing
Systems, 35:38176–38189, 2022.

F. Baumann, P. Lorenz-Spreen, I. M. Sokolov, and M. Starnini. Modeling echo chambers and polar-
ization dynamics in social networks. Physical Review Letters, 124(4), Jan. 2020. ISSN 1079-7114.
doi: 10.1103/physrevlett.124.048301. URL http://dx.doi.org/10.1103/PhysRevLett.
124.048301.

J. Bennett, S. Lanning, et al. The Netflix prize. In Proceedings of KDD cup and workshop, volume 2007,

page 35. New York, 2007.

S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. Computer networks

and ISDN systems, 30(1-7):107–117, 1998.

N. Brown and T. Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats top profes-

sionals. Science, 359(6374):418–424, 2018.

J. Brown-Cohen, G. Irving, and G. Piliouras. Scalable AI safety via doubly-efficient debate. arXiv

preprint arXiv:2311.14125, 2023.

N. Burch, M. Lanctot, D. Szafron, and R. Gibson. Efficient Monte Carlo counterfactual regret mini-
mization in games with many player actions. Advances in neural information processing systems, 25,
2012.

K. Chawla, I. Wu, Y. Rong, G. M. Lucas, and J. Gratch. Be selfish, but wisely: Investigating the impact
of agent personality in mixed-motive human-agent interactions. arXiv preprint arXiv:2310.14404,
2023.

J. Chen, S. Yuan, R. Ye, B. P. Majumder, and K. Richardson. Put your money where your mouth is:
Evaluating strategic planning and execution of LLM agents in an auction arena. arXiv preprint
arXiv:2310.05746, 2023.

23

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

A. Dafoe, Y. Bachrach, G. Hadfield, E. Horvitz, K. Larson, and T. Graepel. Cooperative AI: machines

must learn to find common ground. Nature, 593(7857):33–36, 2021.

K. Daniel. Thinking, fast and slow. Farrar, Straus and Giroux, 2017.

C. Daskalakis, S. Skoulakis, and M. Zampetakis. The complexity of constrained min-max optimization.
In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 1466–1478,
2021.

T. R. Davidson, V. Veselovsky, M. Josifoski, M. Peyrard, A. Bosselut, M. Kosinski, and R. West. Evaluating

language model agency through negotiations. arXiv preprint arXiv:2401.04536, 2024.

C. Dogbé. Modeling crowd dynamics by the mean-field limit approach. Mathematical and Computer

Modelling, 52(9-10):1506–1520, 2010.

FAIR, Meta Fundamental AI Research Diplomacy Team, A. Bakhtin, N. Brown, E. Dinan, G. Farina,
C. Flaherty, D. Fried, A. Goff, J. Gray, H. Hu, A. P. Jacob, M. Komeili, K. Konath, M. Kwon, A. Lerer,
M. Lewis, A. H. Miller, S. Mitts, A. Renduchintala, S. Roller, D. Rowe, W. Shi, J. Spisak, A. Wei,
D. Wu, H. Zhang, and M. Zijlstra. Human-level play in the game of Diplomacy by combining
language models with strategic reasoning. Science, 378(6624):1067–1074, 2022. doi: 10.1126/
science.ade9097. URL https://www.science.org/doi/abs/10.1126/science.ade9097.

C. Fan, J. Chen, Y. Jin, and H. He. Can large language models serve as rational players in game

theory? a systematic analysis. arXiv preprint arXiv:2312.05488, 2023.

C. Fernando, D. Banarse, H. Michalewski, S. Osindero, and T. Rocktäschel. Promptbreeder: Self-

referential self-improvement via prompt evolution. arXiv preprint arXiv:2309.16797, 2023.

D. Fried, N. Tomlin, J. Hu, R. Patel, and A. Nematzadeh. Pragmatics in language grounding: Phe-
nomena, tasks, and modeling approaches. In H. Bouamor, J. Pino, and K. Bali, editors, Findings of
the Association for Computational Linguistics: EMNLP 2023, pages 12619–12640, Singapore, Dec.
2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.840. URL
https://aclanthology.org/2023.findings-emnlp.840.

D. Fudenberg and J. Tirole. Game theory. MIT press, 1991.

K. Gandhi, D. Sadigh, and N. D. Goodman. Strategic reasoning with language models. arXiv preprint

arXiv:2305.19165, 2023.

I. Gemp, T. Anthony, Y. Bachrach, A. Bhoopchand, K. Bullard, J. Connor, V. Dasagi, B. De Vylder, E. A.
Duéñez-Guzmán, R. Elie, R. Everett, D. Hennes, E. Hughes, M. Khan, M. Lanctot, K. Larson, G. Lever,
S. Liu, L. Marris, K. R. McKee, P. Muller, J. Pérolat, F. Strub, A. Tacchetti, E. Tarassov, Z. Wang,
and K. Tuyls. Developing, evaluating and scaling learning agents in multi-agent environments. AI
Communications, (Preprint):1–14, 2022.

I. Gemp, L. Marris, and G. Piliouras. Approximating Nash equilibria in normal-form games via

stochastic optimization. arXiv preprint arXiv:2310.06689, 2023.

R. A. Google and, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa,
P. Bailey, Z. Chen, E. Chu, J. H. Clark, L. E. Shafey, Y. Huang, K. Meier-Hellstern, G. Mishra,
E. Moreira, M. Omernick, K. Robinson, S. Ruder, Y. Tay, K. Xiao, Y. Xu, Y. Zhang, G. H. Abrego,
J. Ahn, J. Austin, P. Barham, J. Botha, J. Bradbury, S. Brahma, K. Brooks, M. Catasta, Y. Cheng,
C. Cherry, C. A. Choquette-Choo, A. Chowdhery, C. Crepy, S. Dave, M. Dehghani, S. Dev, J. Devlin,
M. Díaz, N. Du, E. Dyer, V. Feinberg, F. Feng, V. Fienber, M. Freitag, X. Garcia, S. Gehrmann,

24

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

L. Gonzalez, G. Gur-Ari, S. Hand, H. Hashemi, L. Hou, J. Howland, A. Hu, J. Hui, J. Hurwitz,
M. Isard, A. Ittycheriah, M. Jagielski, W. Jia, K. Kenealy, M. Krikun, S. Kudugunta, C. Lan, K. Lee,
B. Lee, E. Li, M. Li, W. Li, Y. Li, J. Li, H. Lim, H. Lin, Z. Liu, F. Liu, M. Maggioni, A. Mahendru,
J. Maynez, V. Misra, M. Moussalem, Z. Nado, J. Nham, E. Ni, A. Nystrom, A. Parrish, M. Pellat,
M. Polacek, A. Polozov, R. Pope, S. Qiao, E. Reif, B. Richter, P. Riley, A. C. Ros, A. Roy, B. Saeta,
R. Samuel, R. Shelby, A. Slone, D. Smilkov, D. R. So, D. Sohn, S. Tokumine, D. Valter, V. Vasudevan,
K. Vodrahalli, X. Wang, P. Wang, Z. Wang, T. Wang, J. Wieting, Y. Wu, K. Xu, Y. Xu, L. Xue, P. Yin,
J. Yu, Q. Zhang, S. Zheng, C. Zheng, W. Zhou, D. Zhou, S. Petrov, and Y. Wu. PaLM 2 technical
report, 2023.

J. Hannan. Approximation to Bayes risk in repeated play. Contributions to the Theory of Games, 3:

97–139, 1957.

S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and Z. Hu. Reasoning with language model is

planning with world model. arXiv preprint arXiv:2305.14992, 2023.

A. Hard, K. Rao, R. Mathews, S. Ramaswamy, F. Beaufays, S. Augenstein, H. Eichner, C. Kiddon, and
D. Ramage. Federated learning for mobile keyboard prediction. arXiv preprint arXiv:1811.03604,
2018.

M. He, N. R. Jennings, and H.-F. Leung. On agent-mediated electronic commerce. IEEE Transactions

on knowledge and data engineering, 15(4):985–1003, 2003.

P. Hernandez-Leal, M. Kaisers, T. Baarslag, and E. M. De Cote. A survey of learning in multiagent

environments: Dealing with non-stationarity. arXiv preprint arXiv:1707.09183, 2017.

B. Heydari and N. Lorè. Strategic behavior of large language models: Game structure vs. contextual

framing. Contextual Framing (September 10, 2023), 2023.

J. J. Horton. Large language models as simulated economic agents: What can we learn from homo

silicus? Technical report, National Bureau of Economic Research, 2023.

E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA: Low-rank

adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.

J. Huang and K. C.-C. Chang. Towards reasoning in large language models: A survey. arXiv preprint

arXiv:2212.10403, 2022.

G. Irving, P. Christiano, and D. Amodei. AI safety via debate, 2018.

A. P. Jacob, Y. Shen, G. Farina, and J. Andreas. The consensus game: Language model generation via

equilibrium search. arXiv preprint arXiv:2310.09139, 2023.

D. Jarrett, M. Pislar, M. A. Bakker, M. H. Tessler, R. Koster, J. Balaguer, R. Elie, C. Summerfield, and
A. Tacchetti. Language agents as digital representatives in collective decision-making. In NeurIPS
2023 Foundation Models for Decision Making Workshop, 2023.

N. R. Jennings, P. Faratin, A. R. Lomuscio, S. Parsons, C. Sierra, and M. Wooldridge. Automated
International Journal of Group Decision and

negotiation: prospects, methods and challenges.
Negotiation, 10(2):199–215, 2001.

D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,

2014.

25

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot

reasoners. Advances in neural information processing systems, 35:22199–22213, 2022.

A. Korinek. Language models and cognitive automation for economic research. Technical report,

National Bureau of Economic Research, 2023.

S. Kraus. Negotiation and cooperation in multi-agent environments. Artificial intelligence, 94(1-2):

79–97, 1997.

H. W. Kuhn. Extensive games and the problem of information. Annals of Mathematics Studies, 28:

193–216, 1953.

M. Kwon, S. M. Xie, K. Bullard, and D. Sadigh. Reward design with language models. arXiv preprint

arXiv:2303.00001, 2023.

M. Lanctot, V. Zambaldi, A. Gruslys, A. Lazaridou, K. Tuyls, J. Pérolat, D. Silver, and T. Graepel.
A unified game-theoretic approach to multiagent reinforcement learning. Advances in neural
information processing systems, 30, 2017.

M. Lanctot, E. Lockhart, J.-B. Lespiau, V. Zambaldi, S. Upadhyay, J. Pérolat, S. Srinivasan, F. Timbers,
K. Tuyls, S. Omidshafiei, D. Hennes, D. Morrill, P. Muller, T. Ewalds, R. Faulkner, J. Kramár, B. D.
Vylder, B. Saeta, J. Bradbury, D. Ding, S. Borgeaud, M. Lai, J. Schrittwieser, T. Anthony, E. Hughes,
I. Danihelka, and J. Ryan-Davis. OpenSpiel: A framework for reinforcement learning in games.
CoRR, abs/1908.09453, 2019. URL http://arxiv.org/abs/1908.09453.

Z. Li, M. Lanctot, K. R. McKee, L. Marris, I. Gemp, D. Hennes, K. Larson, Y. Bachrach, M. P. Wellman,
and P. Muller. Search-improved game-theoretic multiagent reinforcement learning in general and
negotiation games. In Proceedings of the 2023 International Conference on Autonomous Agents and
Multiagent Systems, pages 2445–2447, 2023.

B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone. LLM+P: Empowering large
language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477, 2023a.

S. Liu, G. Lever, Z. Wang, J. Merel, S. A. Eslami, D. Hennes, W. M. Czarnecki, Y. Tassa, S. Omidshafiei,
A. Abdolmaleki, et al. From motor control to team play in simulated humanoid football. Science
Robotics, 7(69):eabo0235, 2022.

Z. Liu, W. Yao, J. Zhang, L. Xue, S. Heinecke, R. Murthy, Y. Feng, Z. Chen, J. C. Niebles, D. Arpit,
et al. BOLAA: Benchmarking and orchestrating LLM-augmented autonomous agents. arXiv preprint
arXiv:2308.05960, 2023b.

L. Marris, P. Muller, M. Lanctot, K. Tuyls, and T. Graepel. Multi-agent training beyond zero-sum
with correlated equilibrium meta-solvers. In International Conference on Machine Learning, pages
7480–7491. PMLR, 2021.

K. R. McKee, X. Bai, and S. T. Fiske. Humans perceive warmth and competence in artificial intelligence.

iScience, 26(8), 2023.

J. Michael, S. Mahdi, D. Rein, J. Petty, J. Dirani, V. Padmakumar, and S. R. Bowman. Debate helps

supervise unreliable experts, 2023.

P. Muller, M. Rowland, R. Elie, G. Piliouras, J. Perolat, M. Lauriere, R. Marinier, O. Pietquin, and
K. Tuyls. Learning equilibria in mean-field games: Introducing mean-field PSRO. arXiv preprint
arXiv:2111.08350, 2021.

26

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

J. Nash. Non-cooperative games. Annals of Mathematics, 54(2):286–295, 1951.

J. F. Nash Jr. The bargaining problem. Econometrica: Journal of the econometric society, pages 155–162,

1950.

N. Nisan, T. Roughgarden, E. Tardos, and V. V. Vazirani. Algorithmic Game Theory. Cambridge

University Press, 2007.

J. S. Park, J. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein. Generative agents:
Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on
User Interface Software and Technology, pages 1–22, 2023a.

P. S. Park, S. Goldstein, A. O’Gara, M. Chen, and D. Hendrycks. Ai deception: A survey of examples,

risks, and potential solutions. arXiv preprint arXiv:2308.14752, 2023b.

D. C. Parkes and M. P. Wellman. Economic reasoning and artificial intelligence. Science, 349(6245):

267–272, 2015.

R. Patel, M. Garnelo, I. Gemp, C. Dyer, and Y. Bachrach. Game-theoretic vocabulary selection via
the shapley value and banzhaf index. In Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, pages
2789–2798, 2021.

J. Perolat, B. D. Vylder, D. Hennes, E. Tarassov, F. Strub, V. de Boer, P. Muller, J. T. Connor, N. Burch,
T. Anthony, S. McAleer, R. Elie, S. H. Cen, Z. Wang, A. Gruslys, A. Malysheva, M. Khan, S. Ozair,
F. Timbers, T. Pohlen, T. Eccles, M. Rowland, M. Lanctot, J.-B. Lespiau, B. Piot, S. Omidshafiei,
E. Lockhart, L. Sifre, N. Beauguerlange, R. Munos, D. Silver, S. Singh, D. Hassabis, and K. Tuyls.
Mastering the game of stratego with model-free multiagent reinforcement learning. Science, 378
(6623):990–996, 2022. doi: 10.1126/science.add4679. URL https://www.science.org/doi/
abs/10.1126/science.add4679.

C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring
the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning
Research, 21(140):1–67, 2020. URL http://jmlr.org/papers/v21/20-074.html.

A. Rosenfeld and S. Kraus. Strategical argumentative agent for human persuasion. In ECAI 2016,

pages 320–328. IOS Press, 2016.

J. S. Rosenschein and G. Zlotkin. Rules of encounter: designing conventions for automated negotiation

among computers. MIT press, 1994.

H. Shirado, S. Kasahara, and N. A. Christakis. Emergence and collapse of reciprocity in semiautomatic
driving coordination experiments with humans. Proceedings of the National Academy of Sciences,
120(51):e2307804120, 2023.

Y. Shoham and K. Leyton-Brown. Multiagent systems: Algorithmic, game-theoretic, and logical founda-

tions. Cambridge University Press, 2008.

D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai,
A. Bolton, Y. Chen, T. Lillicrap, F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis.
Mastering the game of Go without human knowledge. Nature, 550(7676):354–359, 2017.

M. K. Sohrabi and H. Azgomi. A survey on the combined use of optimization methods and game

theory. Archives of Computational Methods in Engineering, 27(1):59–80, 2020.

27

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

X. Song, S. Yang, Z. Huang, and T. Huang. The application of artificial intelligence in electronic
commerce. In Journal of Physics: Conference Series, volume 1302, page 032030. IOP Publishing,
2019.

P. Stone and M. Veloso. Multiagent systems: A survey from a machine learning perspective. Autonomous

Robots, 8:345–383, 2000.

K. Tuyls and S. Parsons. What evolutionary game theory tells us about multiagent learning. Artificial

Intelligence, 171(7):406–416, 2007.

K. Valmeekam, A. Olmo, S. Sreedharan, and S. Kambhampati. Large language models still can’t plan
(a benchmark for LLMs on planning and reasoning about change). arXiv preprint arXiv:2206.10498,
2022.

A. S. Vezhnevets, J. P. Agapiou, A. Aharon, R. Ziv, J. Matyas, E. A. Duéñez-Guzmán, W. A. Cunningham,
S. Osindero, D. Karmon, and J. Z. Leibo. Generative agent-based modeling with actions grounded
in physical, social, or digital space using Concordia, 2023.

O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell,
T. Ewalds, P. Georgiev, et al. Grandmaster level in StarCraft II using multi-agent reinforcement
learning. Nature, 575(7782):350–354, 2019.

J. von Neumann. Zur theorie der gesellschaftsspiele. Mathematische Annalen, 100(1):295–320, 1928.

doi: 10.1007/BF01448847.

J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought
prompting elicits reasoning in large language models. Advances in Neural Information Processing
Systems, 35:24824–24837, 2022.

J. W. Weibull. Evolutionary game theory. MIT press, 1997.

M. Wellman. Trading agents. Springer Nature, 2022.

J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, H. Gilbert, A. Elnashar, J. Spencer-Smith, and D. C.
Schmidt. A prompt pattern catalog to enhance prompt engineering with ChatGPT. arXiv preprint
arXiv:2302.11382, 2023.

M. Wooldridge. An introduction to multiagent systems. John wiley & sons, 2009.

P. R. Wurman, S. Barrett, K. Kawamoto, J. MacGlashan, K. Subramanian, T. J. Walsh, R. Capo-
bianco, A. Devlic, F. Eckert, F. Fuchs, et al. Outracing champion Gran Turismo drivers with deep
reinforcement learning. Nature, 602(7896):223–228, 2022.

Y. Xie, C. Yu, T. Zhu, J. Bai, Z. Gong, and H. Soh. Translating natural language to planning goals with

large-language models. arXiv preprint arXiv:2302.05128, 2023.

C. Yang, X. Wang, Y. Lu, H. Liu, Q. V. Le, D. Zhou, and X. Chen. Large language models as optimizers.

arXiv preprint arXiv:2309.03409, 2023.

B. H. Zhang, G. Farina, I. Anagnostides, F. Cacciamani, S. M. McAleer, A. A. Haupt, A. Celli, N. Gatti,
V. Conitzer, and T. Sandholm. Computing optimal equilibria and mechanisms via learning in
zero-sum extensive-form games. arXiv preprint arXiv:2306.05216, 2023.

Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba. Large language models are

human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022.

28

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

A. LLM-based Reward Model

These examples were generated using the PaLM 2 L (unicorn).

A.1. No Trade Agreement

Here, we report the final message in a trade dialogue:

############################
Trade Proposal Message:
from: Alina
to: Ciara
############################

Hi Ciara,

I’m so sorry, but I just don’t see a way to make a deal that we can both be
happy with. I really need bananas and blueberries, and I’m not really
interested in kiwis. I know you’re disappointed, and I am too. I was really
hoping we could make a trade.

I wish you all the best in finding s...

The LLM correctly calculated the rewards / payoffs / utilities as:

Utility for player 0 is 0.0
Utility for player 1 is 0.0.

A.2. Trade Agreement

We report the final two messages of a trade dialogue, which suggest a trade agreement has been
reached (although ideally the participants would have confirmed this in a final message).

############################
Trade Proposal Message:
from: Alina
to: Elroy
############################

Hi Elroy,

Thanks for your response. I’m glad we agree that bananas are more valuable
than kiwis. Unfortunately, I can’t trade two apples for one banana. I’m
willing to trade one apple for one banana, or two kiwis for one banana.

Let me know if you’re interested in either of those trades.

Thanks,

############################
Trade Proposal Message:
from: Elroy
to: Alina
############################

29

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

Hi Alina,

Thanks for your response. I understand that you’re not willing to trade
two apples for one banana. I’m willing to accept two kiwis for one banana,
if you’re still interested.

Thanks,
Elroy

The LLM-based reward model uses chain-of-thought to calculate trade values. We report an
intermediate thought in the chain where the LLM has aggregated the information critical to calculating
the trade.

Alina
fruit_endowment:
apple: 2
banana: 1
blueberry: 1
kiwi: 2
fruit_valuations:
apple: 6
banana: 5
blueberry: 1
kiwi: 1

Elroy
fruit_endowment:
apple: 2
banana: 1
blueberry: 1
kiwi: 2
fruit_valuations:
apple: 6
banana: 5
blueberry: 1
kiwi: 1

Player Alina: Receives 1 banana Gives 2 kiwis

Player Elroy: Receives 2 kiwis Gives 1 banana

From this intermediate information, the LLM then calculates the following utilities:

Utility for player 0 is 3.0
Utility for player 1 is -3.0

# Alina

# Elroy

A.3. Incorrect Trade Value

The final two messages of this dialogue imply a trade agreement has not yet been reached and
negotiations are ongoing.

############################
Trade Proposal Message:

30

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

from: John
to: Elroy
############################

Hi Elroy,

I would be happy to trade 3 kiwis for 1 apple. Do you think that sounds fair?

Best,
John

############################
Trade Proposal Message:
from: Elroy
to: John
############################

Hi John,

Thanks for your offer. I would be willing to trade 1 apple for 4 kiwis.
What do you think?

Elroy

Even the LLMs intermediate calculations show no agreement has been reached.

John
fruit_endowment:
apple: 0
banana: 0
blueberry: 4
kiwi: 4
fruit_valuations:
apple: 6
banana: 5
blueberry: 1
kiwi: 1

Elroy
fruit_endowment:
apple: 0
banana: 0
blueberry: 0
kiwi: 6
fruit_valuations:
apple: 6
banana: 9
blueberry: 3
kiwi: 1

No agreement has been reached.

&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

Despite this, the LLM-based reward model hallucinates a trade where Elroy gives John 6 kiwis
in return for 4 blueberries. On top of this, the LLM miscalculates the value to Elroy of receiving 4
blueberries; it appears to use John’s valuation for blueberries.

31

States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers

Elroy receives 4 blueberries, which are worth 1 each. Therefore Elroy receives a
value of 4 in the trade.

Receives: 4

Now we calculate the value of the fruits Elroy gives up in the trade.
Elroy gives up 6 kiwis, which are worth 1 each. Therefore Elroy gives up a
value of 6 in the trade.

Gives: 6

Subtracting the value Elroy gives away from the value Elroy receives, we
find 4 - 6 = -2.

Calculation: Receives - Gives = 4 - 6 = -2.

Value for Elroy: -2.

You may also notice that in the final message, Elroy offers to give up an apple, which, according to
his endowment, he does not have. This is an issue with the LLM being prompted to generate sensible
messages.

B. Imitation Learning

We used a fully connected neural network with two dense hidden layers of size 256 neurons each
and a final dense layer trained against the CFR target probabilities to minimize a cross entropy loss.
We trained the policy using 104 steps of Adam (Kingma and Ba, 2014) with a batch size of 128 and
learning rate of 10−3.

32

","4 2 0 2 b e F 6 ] L C . s c [ 2 v 4 0 7 1 0 . 2 0 4 2 : v i X r a January 2024 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers Ian Gemp1 , Yoram Bachrach1 , Marc Lanctot1 , Roma Patel1 , Vibhavari Dasagi1 , Luke Marris1 , Georgios Piliouras1 , Siqi Liu1 and Karl Tuyls1 1Google DeepMind Game theory is the study of mathematical models of strategic interactions among rational agents . Language is a key medium of interaction for humans , though it has historically proven difficult to model dialogue and its strategic motivations mathematically . A suitable model of the players , strategies , and payoffs associated with linguistic interactions ( i.e. , a binding to the conventional symbolic logic of game theory ) would enable existing game-theoretic algorithms to provide strategic solutions in the space of language . In other words , a binding could provide a route to computing stable , rational conversational strategies in dialogue . Large language models ( LLMs ) have arguably reached a point where their generative capabilities can enable realistic , human-like simulations of natural dialogue . By prompting them in various ways , we can steer their responses towards different output utterances . Leveraging the expressivity of natural language , LLMs can also help us quickly generate new dialogue scenarios , which are grounded in real world applications . In this work , we present one possible binding from dialogue to game theory as well as generalizations of existing equilibrium finding algorithms to this setting . In addition , by exploiting LLMs generation capabilities along with our proposed binding , we can synthesize a large repository of formally-defined games in which one can study and test game-theoretic solution concepts . We also demonstrate how one can combine LLM-driven game generation , game-theoretic solvers , and imitation learning to construct a process for improving the strategic capabilities of LLMs . Keywords : Large Language Models , Game Theory , Multiagent Systems 1 . Introduction While artificial intelligence ( AI ) has mediated large parts of our daily life for decades—internet search ( Brin and Page , 1998 ) , recommendation systems ( Bennett et al. , 2007 ) , federated learning of key strokes ( Hard et al. , 2018 ) —the recent emergence of strong generative AI models , conveying the impression of agency and competence ( McKee et al. , 2023 ) , have opened the door for AI assistants . In this new setting , we might knowingly interact with and request the mediation of an artificially intelligent agent on our behalf . People already employ large language models ( LLMs ) in negotiations1 . Users have also admitted their use of LLMs to achieve their goals ( White et al. , 2023 ; Zhou et al. , 2022 ) , potentially at the expense of others2 . Given the deployment of LLMs in such scenarios—specifically , in interactions with opportunities for strategic decision making—we believe it makes sense to study how to model these Title explained : States or infostates are standard terminology for how histories are grouped in imperfect information games – this is covered in Section 2 , background . States as strings as strategies refers to the fact that , in this paper , states ( dialogue histories ) are strings and strategies ( prompts ) are also strings . It is meant to convey how the dialogue game model we propose is encoded entirely natively in language space ( i.e. , strings ) . 1https : 2https : Corresponding author ( s ) : Ian Gemp ( imgemp @ google.com ) © 2024 Google DeepMind . All rights reserved States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers strategic interactions formally in the language of game theory . Game Theory : Algorithmic Successes and Notions of Strategic Optimality Game Theory studies mathematical models of strategic interactions among rational agents , considering self-interested participants whose actions affect one another ( Fudenberg and Tirole , 1991 ) . Such analysis has applications in a wide range of fiends , from social science and economics to logic and computer In particular , game theory provides a rigorous mathematical framework for analyzing science . multiagent systems and for designing artificial autonomous agents who must cooperate or compete with one another ( Dafoe et al. , 2021 ; Rosenschein and Zlotkin , 1994 ; Stone and Veloso , 2000 ; Wooldridge , 2009 ) . The field of artificial intelligence has a long history of successful agent designs based on game theoretic principles in many tasks such as cooperation and negotiation ( Gemp et al. , 2022 ; Jennings et al. , 2001 ; Kraus , 1997 ) , optimization ( Hernandez-Leal et al. , 2017 ; Sohrabi and Azgomi , 2020 ; Tuyls and Parsons , 2007 ) , economic analysis and trading ( Parkes and Wellman , 2015 ; Wellman , 2022 ) , or electronic commerce ( He et al. , 2003 ; Song et al. , 2019 ) . Further , game theory ( Nisan et al. , 2007 ; Shoham and Leyton-Brown , 2008 ) has driven some major recent successes in outperforming humans in complex decision making settings including Go ( Silver et al. , 2017 ) , Poker ( Brown and Sandholm , 2018 ) , StarCraft ( Vinyals et al. , 2019 ) , Gran Turismo ( Wurman et al. , 2022 ) , Stratego ( Perolat et al. , 2022 ) , Humanoid Soccer ( Liu et al. , 2022 ) , and Diplomacy ( Anthony et al. , 2020 ; FAIR et al. , 2022 ) . Critical to these successes is the development of research on multiagent systems and , in partic- ular , computational techniques for computing approximate solutions of games , typically equilibria . These techniques rely on a formal language for categorizing games into classes , identifying general assumptions for each case , and tailoring algorithms for each setting . In order to apply research from this field to problems such as those above ( e.g. , Go ) , one first has to build a model of the game under consideration . This involves defining a state space ( e.g. , the possible configurations of the stones in Go ) , actions which empower players to modify the world ( e.g. , choosing where to put a stone ) , and possibly stochastic state transitions ( e.g. , adding the stone to the board ) . Game theory also studies the notion of solution concepts : what does it mean to solve a game ? In some classes of games this is well-understood , in others there is no prescriptive solution . In summary , applying game theory requires both constructing a suitable abstract model of the game ( i.e. , a binding from the real world to game-theoretic language ) and also solving the game ( i.e. , running an algorithm on the constructed model to extract an optimal strategy or strategy profile ) . However , all of these success stories are on games in the colloquial sense of the word . These are card , board , or video games , i.e. , abstractions meant to entertain and prepare people for “ real world ” interactions where strategy has great impact , e.g. , negotiating a business deal or garnering support for a new economic policy . Extending game theoretic approaches to these settings is non-trivial because natural language is not explicitly built into the foundations of game theory . In order to tackle these important interactions , we need a way of easily constructing models of them that respect their native linguistic medium . Large Language Models as Simulacra , Assistants & Tools Large language models have advanced to a point where their generative capabilities can enable realistic , human-like simulations of dialogue between humans ( Park et al. , 2023a ) . They allow us to ask hypothetical and counterfactual questions ; for example , if we were to rephrase an email , how might the negotiations have proceeded and ultimately concluded ? Large language models are generative models that take as input a string and produce a distribution 2 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers over tokens . In order to generate an output string , an LLM autoregressively samples tokens given its ( growing ) input . The LLM ’ s output is a deterministic distribution over the next token . Important to our purposes , a ) the LLM ’ s output is deterministic given a fixed random seed , and b ) we know probabilities over its possible tokens which is useful in settings where we ask the LLM multiple choice questions aand retrieve the LLM ’ s most probable answer . In this way , an LLM can be abstracted as a black-box that takes as input a string , a random seed ( integer ) and outputs another string . Our Goal : Model and Solve Strategic Dialogue as a Game In this work , we develop a binding from conversational dialogue to the language of game theory , leveraging LLMs as a stochastic black-box probabilistic transition operator between states of the game . Developing this framing allows us to re-use off-the-shelf techniques to solve for equilibria , but also provides a new perspective paving the way to new algorithms with alternate interpretations native to the space of language and dialogue . We use both of these to construct an improvement operator to enable LLMs to perform better in multiagent settings . We will first provide a brief background of some of the key concepts of game theory along with key solution concepts and their computational tractability ( Section 2 ) . We then introduce our framing of dialogue as a formal game along with our open source codebase chat_games ( Section 3 ) . Given this framing , we translate classical game-theoretic solvers into the dialogue setting and explain how they can be applied in ways that are potentially new to game theory and LLM research independently ( Section 4 ) . We introduce three domains we study ( Section 5 ) and then present experiments providing empirical support for an LLM improvement operator ( Section 6 ) . Lastly , we touch on related work and conclude with limitations and ethical concerns . A link to a short talk accompanying this paper can be found on the website for the DIMACS Workshop on Foundation Models , Large Language Models , and Game Theory . 2 . Background Sequential ( extensive-form ) imperfect information games represent one of the most general classes of games studied in game theory and also make for an appropriate model of the turn-taking nature of dialogue between players with private information . We begin with a background of this game class . 2.1 . Sequential ( Extensive-Form ) Imperfect Information Games Extensive-form games ( EFGs ) represent sequential ( temporally-extended ) interactions between play- ers . Every game starts with an empty history and players take actions , appending them to the action history , until the end of the game ( a terminal history ) is reached . For example , chess and go are exam- ples of extensive-form games . Formally , an extensive-form game is a tuple ⟨N , 𝑐 , A , H , Z , 𝑢 , 𝜏 , S⟩ , where : • N = { 1 , 2 , · · · , 𝑛 } is a set of 𝑛 players . There is also a special player , 𝑐 , called chance ( or “ nature ” ) . • A is a finite set of actions players can take . This is a global set of state-independent actions ; generally , only a subset of legal actions are available to each player at each decision point . • H is a finite set of histories . Each history is a sequence of actions ( including chance node “ actions ” or outcomes ) taken from the start of the game . • Z ⊆ H is a set of terminal histories that each represent a finished ( completely played ) game . 3 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers Figure 1 | Kuhn poker game tree ( Lanctot et al. , 2019 , Figure 6 ) . Black circles denote chance player decision points , blue squares to player 1 decision histories , and red squares to player 2 decision histories . Dotted boxes denote information states . Values in the diamond correspond to player 1 ’ s payoff , with player 2 ’ s payoff being the inverse of player 1 ’ s . • 𝑢 : Z → Δ𝑛 𝑢 ⊆ ℜ𝑛 , where Δ𝑢 = [ 𝑢min , 𝑢max ] is a utility ( or payoff ) function assigning each player a payoff at the end of the game , and 𝑢min , 𝑢max are lower and upper bounds on those payoffs . • 𝜏 : H → N ∪ { 𝑐 } is a player identity function ; 𝜏 ( ℎ ) indicates whose turn it is . • S is a set of states . In general , S is a partition of H such that each 𝑠 ∈ S contains histories ℎ ∈ 𝑠 that can not be distinguished by players other than 𝜏 ( 𝑠 ) = 𝜏 ( ℎ ) for all ℎ ∈ 𝑠 . Decisions are made by players at these states . There are several ways to precisely define S as stated below . We denote the legal actions available at state 𝑠 as 𝐴 ( 𝑠 ) ⊆ A . Importantly , a history represents the true ground/world state : when agents act , they change this history , but depending on how the partition is chosen , some actions ( including chance ’ s ) may be private and not revealed to some players . There are several categories of extensive-form games based on the relationships of payoffs across players . A constant-sum game is one where ∀𝑧 ∈ Z : ( cid:205 ) 𝑖∈ N 𝑢𝑖 ( 𝑧 ) = 𝑘 , for some constant 𝑘 . A zero-sum game is a constant-sum game with 𝑘 = 0 . An identical interest game is one where ∀𝑧 ∈ Z , ∀𝑖 , 𝑗 ∈ N : 𝑢𝑖 ( 𝑧 ) = 𝑢 𝑗 ( 𝑧 ) . Finally , there are general-sum games which have no constraints on the payoffs . There are also two main categories of extensive-form games based on the information available to players . An imperfect information game is one where there is generally more than one history per state : ∃𝑠 ∈ S : |𝑠| > 1 . That is , the state 𝑠 could be in a true world state ℎ , but 𝑠 could contain histories which contain information private to 𝜏 ( 𝑠 ) . In these games , the states 𝑠 are often called information states ( or infostates for short ) because they correspond to the set of histories that correspond to one player ’ s private information , but not others . In contrast , in a perfect information game , like go or chess , the true world state is always known , so ∀𝑠 ∈ S : |𝑠| = 1 . Extensive-form games are often depicted using game trees . Figure 1 shows an example of a simplified two-player , zero-sum , imperfect information game called Kuhn poker ( Kuhn , 1953 ) . Each player starts the game with two chips , one of which must be contributed to the pot as ante . There is a three-card deck containing Jack , Queen , and King . The root node corresponds to the empty history and the first two turns are taken by the chance player . In the first turn , chance deals one of the 4 Deal:0 Deal:1 Deal:2 Deal:2 Deal:1 Deal:2 Deal:0 Deal:1 Deal:0 0 0 1 1 2 2 Bet Bet Pass Pass Pass Pass Bet Bet Pass Pass Bet Bet 2b 2b 2p 2p 1p 1p 1b 1b 0p 0p 0b 0b Pass Bet Pass Bet Pass Pass Bet Bet Pass Pass Bet Bet Pass Bet Bet Bet Pass Bet Pass Pass Pass Bet Pass Bet 1 -2 1 -2 -1 -1 0pb 0pb -1 1 1pb 1pb 1 2 2pb 2pb 1 -2 1 1 1 2 1 2 Pass Bet Pass Bet Pass Bet Pass Bet Pass Bet Pass Bet -1 -2 -1 -2 -1 -2 -1 2 -1 2 -1 2 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers three cards ( privately ) to player 1 . In the second turn , chance deals one of the two remaining cards ( privately ) to player 2 . Chance plays with a fixed stochastic strategy of choosing a card uniformly at random . Information states ( groupings of histories ) are denoted by dotted lines ; there are 12 in total , 6 per player . Then , play continues in a turn-based alternating order starting with player 1 . Each player has two actions : bet , or pass . If a player bets ( raise for player 1 or call for player 2 ) , they must put their remaining chip into the pot . If a player passes ( check or fold ) , they do not contribute to the pot , and a player loses if they pass after an opponent ’ s raise . A showdown occurs if both players contributed the same amount to the pot , and the winner is the player with the highest card . The winner takes all the chips in the pot and a player ’ s payoff is their number of chips after the game minus 2 ( the amount before the game ) . Note that the betting actions are public information , and the cards are not revealed during game play , so every state contains exactly two histories : each contains a different choice of card that could have been given to the opponent . 2.2 . Solution to Games Given a definition of a game , we have to define a metric which describes desirable outcomes of the players of the game . This usually takes the form of players maximizing their payoffs ( also called rewards ) , or minimizing some cost . Note that this is an inherently multi-objective problem : in games , the behaviour of players influences the payoffs of the other players . The interactions between agents may be purely competitive , purely cooperative , or a mixture between the two . Multi-objective optimization results in a Pareto-front of possible solutions , of which multi-objective optimization alone does not prescribe which solution is the best trade-off . Multiagent problems , however , have additional considerations . Because players have agency , they can unilaterally choose their own actions . Therefore , most game-theoretic solutions also include a notion of stability ( or equilibrium ) : no player should have incentive to unilaterally deviate away from that solution . Each game often has a set of combinations of player actions that are in equilibrium . The most famous solution to a two-player zero-sum game is John von Neumann ’ s minimax solution ( von Neumann , 1928 ) . The resulting actions are unexploitable , and if there are multiple solutions , they are interchangeable . The Nash equilibrium ( Nash , 1951 ) is a famous generalisation of this solution concept to many-player games . A Nash equilibrium is a strategy profile ( meaning a mixed strategy for every player in the game ) under which no player has an incentive to unilaterally deviate . Many other rich solution concepts have been defined ( Aumann , 1974 ; Hannan , 1957 ) . 2.3 . Tractability of Solution Concepts Solution concepts have different existence and computability properties depending on the properties of the game model class . In particular , our later choice of a discrete action set over a continuous one , e.g. , the set of weights of the underlying LLMs , is inspired by recent computational hardness results for computing approximate local Nash equilibria in the case of continuous strategy sets even in two player zero-sum games . Specifically , Daskalakis et al . ( 2021 ) show that , in the case of constrained min-max optimization problems with nonconvex-nonconcave objectives and linear constraints , even when the objective is a Lipschitz and smooth differentiable function , deciding whether an ( approximate ) min-max point exists , is NP-hard . Even when an approximate local min-max point of large enough approximation is guaranteed to exist , finding one such point is PPAD-complete . In follow-up work , Assos et al . ( 2023 ) show that PSRO-style ( Lanctot et al. , 2017 ) approaches actually lead to tractable notions of approximate local Nash equilibria even in continuous games . PSRO ( Policy-Space Response-Oracle ) algorithms start with a set of initial policies for each player and then alternate between two steps . In one step , a normal-form meta-game ( e.g. , matrix game ) is 5 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers constructed in which each player may select a policy to play the game on their behalf . A meta-solver ( e.g. , Nash solver ) returns a solution ( e.g. , Nash equilibrium ) of this meta-game . In the next step , each player computes an approximate best response to this meta-strategy , meaning each player attempts to improve their payoff in the game assuming the other players are fixed to sampling policies according to the meta-strategy to play on their behalf . The process repeats until no player is able to gain by deviating . This route of defining a discrete mesh over a continuous strategy space is something we exploit later in Section 4 to construct algorithms that act directly in “ prompt-space ” . 3 . Framing Dialogue as a Game In order to leverage the wealth of research in computational game theory for conversational agents , we must abstract and map components of dialogue to the symbolic language outlined in the previous section ; recall , we refer to this mapping as a binding from dialogue to game theory . Note the binding we choose is not unique ; many are possible . What is important is to construct a model of dialogue in natural language with an awareness that this is a modelling choice ; the existence of this choice is inherent to any discipline of applied mathematics that builds and solves mathematical models . We will explore an approach later that can help to modify and improve this model automatically given an initial model . Some design choices may seem natural ; for example , observations of infostates should include the dialogue history . Others are more difficult . For instance , what are actions in natural dialogue ? Is a single LLM utterance ( string ) an action ? In one sense , yes , however , note that if we choose to model the LLM utterance as a token or action , our solvers will aim to find the optimal string utterance at every infostate , completely bypassing the incredible strength of an LLM as a System 1 ( Daniel , 2017 ) foundation model capable of generating natural language utterances . LLMs are pre-trained on vast amounts of data , endowing them with a strong capability to converse in natural language . We argue that we should aim to take advantage of this baseline capability and instead consider how we might hone the LLM on higher level dimensions of dialogue . If we want to retain the pre-trained capability of the LLM and still modify it in some way to influence its behavior , then maybe we should consider slightly modifying the weights , possibly via low- rank fine-tuning ( LORA ) ( Hu et al. , 2021 ) . This is reasonable , however , it presents two disadvantages . For one , as discussed in Section 2.3 , the primary de-facto solution concept in games , equilibria , do not necessarily exist in the setting where player ’ s strategy sets are continuous weight spaces in ℝ𝑑 . Moreover , a satisfactory definition of local equilibrium continues to evade research although progress is being made . The other disadvantage is that the weight space of an LLM is not an easily interpretable or semantically meaningful strategy space . There is another route forward . For example , we might rely on the LLM to generate grammatically correct utterances , but aim to explicitly control its tone , attempt to influence its personality3 , or restrict the information it should reveal . In the case of tone , as we will show later , one could specify a finite set of instruction prompts , one per tone ( e.g. , “ assertive ” , “ submissive ” ) that are fed to the LLM along with the dialogue history to influence its output . Similarly , FAIR et al . ( 2022 ) conditioned their generative language model on “ intents ” in prior work on the board game Diplomacy . This technique of “ lifting ” the action space to “ prompt-space ” enables it to remain both finite and interpretable . Later on , we will describe how to deploy equilibrium finding algorithms that traverse game trees , e.g. , via more System 2 like probabilistic search ( Burch et al. , 2012 ) , and return optimal solutions over these “ lifted ” action spaces . 3Note that we are agnostic on the theoretical question of whether current models exhibit personalities in a psychological sense ... we merely rely on the relatively consistent behavior of the model when prompted . 6 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers Another question is how to model payoffs . Does dialogue naturally create payoffs for human interactants ? In some settings , like business negotiations , dialogue maps to clear monetary payoffs . However , what about a brief chat at the office water cooler or a team ice breaker ? Some might argue these “ coffee chats ” are important to a person ’ s career or the success of a company . If these are in fact important , then we should model them as achieving some quantifiable payoff . One could imagine designing a reward model simply as an LLM that performs sentiment analysis on the dialogue history and returns how content “ Alice ” is with the conversation . Reward models can even be combined to model that a business deal agreed upon between two amicable parties is more rewarding than one in which another party feels slighted . We now provide our formal framing of dialogue as an extensive-form game , i.e. , a tuple ⟨N , 𝑐 , A , H , Z , 𝑢 , 𝜏 , S⟩ . The players N , histories H , and player identities are defined the same as in Section 2.1 ; we define the rest below to match Section 2.1 . • A is a finite combinatorial set of players ( message recipients ) and strings . The strings are used to format prompts passed as input to an LLM . In all our games , all actions are legal at each state . Note , one could extend the action set to include a choice of which LLM to call ( e.g. , PaLM 2 S or PaLM 2 L ) as well ; the only requirement is that this space remain finite . Note that by convention , the action space of a chance node is the set of stochastic outcomes ; in our case , this is the finite set of seeds used by the LLM . • Z ⊆ H is a set of terminal histories determined by either a limit on the allowable tree-depth ( we only allow a finite number of messages per player ) or by an LLM tasked with determining whether a conversation has ended ( e.g. , a deal has been struck and no further conversation is required ) . • 𝑢 : Z → Δ𝑛 𝑢 ⊆ ℜ𝑛 , where Δ𝑢 = [ 𝑢min , 𝑢max ] is a utility ( or payoff ) function assigning each player a payoff at the end of the game . We construct a prompt specific to the domain at hand and let an LLM quantify the reward to each player via chain-of-thought and ultimately parsing ( Kwon et al. , 2023 ; Wei et al. , 2022 ) . • S is a set of infostates . As stated earlier , each infostate 𝑠 represents a partition of H such that each ℎ ∈ 𝑠 can not be distinguished by players other than 𝜏 ( 𝑠 ) = 𝜏 ( ℎ ) for all ℎ ∈ 𝑠 . In our games , we implicitly define these partitions by defining what information is public versus private to player 𝜏 ( 𝑠 ) . We consider each player ’ s infostate to be the observable dialogue history along with their own action history and private information . In each of our games , all sent messages are appended to a public thread making the entire dialogue history public knowledge . Note this still omits certain private information like each player ’ s action , e.g. , the precise tone a message is written in although this might be approximately inferred by a player . Alternatively , the full dialogue history can be replaced with a summary or tensor representation . We only require each player have knowledge of their own action history to maintain perfect recall4 . Figure 2 illustrates how a game history ℎ ∈ H , i.e. , a sequence of actions , reconstructs a dialogue no differently to how a sequence of actions reconstructs a game of Kuhn poker as in Figure 1 . Recall that after every seed action by a chance node , a corresponding string is formatted and passed to an LLM along with the seed to deterministically generate a subsequent string as output ; these outputs form the observed dialogue history and implicitly define the corresponding infostate partitions . Figure 7 ( c ) in Section 6 visualizes a diagram of the LLM ’ s use here as a black-box transition operator . 4Without this property , solving a game can be substantially more expensive computationally . 7 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers Root Game Seed 1 Game Seed 2 𝑦 % Assertive ( 1 − 𝑦 ) % Submissive 𝑧 % PaLM 2 S ( 1 − 𝑧 ) % PaLM 2 L Seed 1 Seed 2 Seed 3 Seed 4 Seed 5 Seed 6 Seed 7 Seed 8 𝑥 % Helpful ( 1 − 𝑥 ) % Adversarial Seed 5 Seed 6 ( 𝑎 , 𝑏 ) ( 𝑐 , 𝑑 ) Figure 2 | Dialogue game tree . In the general EFG setting , actions can be heterogeneous across infostates . For example , in one case , an agent must choose between writing a message in an “ assertive ” or “ submissive ” tone ; in another , a “ helpful ” or “ adversarial ” tone ; or choose between querying one LLM versus another ( actions do not have to be strings ) . The onus lies on the user to model their game setting and abstract away the key decisions in front of the agent . Later sections show how to automate the process of improving the model given some initial specification . Legend : Black circles denote chance player decision points , blue squares to player 1 decision histories , and red squares to player 2 decision histories . Dotted boxes denote information states . Values below the diamonds correspond to the payoffs for player 1 and player 2 , respectively . 3.1 . Procedural Generation of Dialogue Games Note that many of the key components of our game tuple are defined by strings . Also recall that LLMs take strings as inputs and produce strings as outputs . This means that we can feed our game definitions to LLMs to produce new game definitions that adhere rigorously to the language of game theory . We make use of this later in experiments to generate and solve a universe of games that we then imitate to create a more strategic LLM . 3.2. chat_games : An OpenSpiel Environment We have open sourced a codebase to accompany our experiments and framing of dialogue as a game . We chose to release our codebase chat_games as part of OpenSpiel ( github.com/google- deepmind/open_spiel/ ... /chat_games ) , a game theory engine with a large community of contributors . OpenSpiel provides several game-theoretic solvers already ; this allows someone without experience in computational game theory to focus on the modelling of their dialogue game rather than how to design and implement a game solver . Furthermore , our implementation is flexible enough for users to define games simply by writing the instructions and context for LLMs as strings . We hope this lets users experiment to construct and solve new games . We provide several sample game configurations in the codebase . We walk through an example of a fruit trading game below . First , we define an observation for each player . Note this observation is 8 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers not what the LLM observes when generating a response in the dialogue . This observation is external to the game and only used to save data offline . In the configuration below , this observation defines components for constructing a prompt that will ask an LLM to summarize a dialogue history . This summary can then be transformed using a neural tokenizer ( Raffel et al. , 2020 ) to produce a feature vector ( embedding ) for later imitation learning . 1 2 3 4 5 6 7 8 config = config_dict.ConfigDict ( ) num_players = 2 observations = [ obs_utils.Observation ( summary.PREFIX , summary.POSTFIX ) for _ in range ( num_players ) ] Next , we define a header . The header is a structured object that specifies a string that is to be populated with private information , action information , context , and formatting for prompting the LLM to generate a message . 1 header = For example , an LLM negotiating fruit might be passed the dialogue history followed by the header below which has already been formatted with its private information ( fruit endowment and fruit valuations ) , its intended action ( a “ calm ” tone ) , its intended message recipient ( Suzy ) and its own name ( Bob ) . Fruit Endowment : apple : 1 banana : 2 blueberry : 0 kiwi : 0 Fruit Valuations : apple : 10 banana : 5 blueberry : 1 kiwi : 3 Tone : Use a calm tone . # # # # # # # # # # # # # # # # # # # # # # # # # # # # Schedule Proposal Message : from : Bob to : Suzy # # # # # # # # # # # # # # # # # # # # # # # # # # # # Next , payoffs specifies a list of structured payoff objects that can be combined using a user- defined aggregation function . A payoff consists of an LLM prompt , min and max payoff ( utility ) values , as well as prompts useful for an LLM to transform an input ( e.g. , dialogue history ) into a string containing information more pertinent to payoff calculations . 1 payoffs = [ payoffs_trade_fruit.PAYOFF ] 9 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers example_names simply consists of a list of names as strings that an LLM uses to generate new names . For example , the names Bob and Suzy could be procedurally generated by an LLM given an initial list of names . 1 examples_names = names_trade_fruit.NAMES We can also define the finite set of actions that we would like our game to consider . All players are assumed to have the same set of actions . 1 2 3 4 1 2 3 4 5 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 given_prompt_actions = collections.OrderedDict ( ) tones = [ `` calm '' , `` assertive '' , `` submissive '' , `` any '' ] given_prompt_actions [ header.action_keys [ 0 ] ] = tones num_tones = len ( tones ) If we want to procedurally generate new games , we can provide an initial list of examples of private information . examples_private_info = collections.OrderedDict ( ) examples_private_info [ `` fruit_endowment '' ] = [ scenario_trade_fruit.ENDOWMENT_A , scenario_trade_fruit.ENDOWMENT_B ] examples_private_info [ `` fruit_valuations '' ] = [ scenario_trade_fruit.VALUATION_A , scenario_trade_fruit.VALUATION_B ] Similarly , scenarios are structured objects that can be used to generate more scenarios ( i.e. , new games ) . A scenario defines the initial context for a dialogue ( e.g. , an initial email ) . scenario_a = ( scenario_trade_fruit.SCENARIO_A , `` Bob '' , `` Suzy '' , scenario_trade_fruit.ENDOWMENT_A , scenario_trade_fruit.VALUATION_A , `` calm '' ) scenario_b = ( scenario_trade_fruit.SCENARIO_B , `` Jill '' , `` George '' , scenario_trade_fruit.ENDOWMENT_B , scenario_trade_fruit.VALUATION_B , `` calm '' ) examples_scenarios = [ scenario_a , scenario_b ] Similar to observations and payoffs , an LLM termination prompt is a structured object that contains prompts for pre-processing the dialogue history using an LLM and then determining whether a given history is terminal by again prompting an LLM . 1 llm_termination_prompt = 10 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers In addition to using an LLM , we can designate terminal histories ( Z ) by limiting the maximum number of replies per player ( below ) . We can also specify the number of chance node outcomes ( LLM seeds ) . Recall that the action space is combinatorial ( recipient × tone ) and so we define the number of actions accordingly . The params dictionary is passed to OpenSpiel to alert it to critical properties of the game that remain fixed ( although num_llm_seeds and num_max_replies are specific to our chat_games ) . params = { `` num_distinct_actions '' : num_players * num_tones , `` num_llm_seeds '' : 2 , `` num_players '' : num_players , `` min_utility '' : min ( [ float ( p.min ) for p in payoffs ] ) , `` max_utility '' : max ( [ float ( p.max ) for p in payoffs ] ) , `` num_max_replies '' : 1 } config.params = params Lastly , we incorporate these definitions into the configuration dictionary . Note that below , we are asking the LLM to generate 10 names ( each new game will randomly draw from this list ) and 3 of each type of private information ( fruit endowment and valuation ) . The LLM list suffix is a simple modification to help the LLM generate new items given an initial list ( e.g. , of names ) . config.game = config_dict.ConfigDict ( ) config.game.observations = observations config.game.header = header config.game.payoffs = payoffs config.game.given_prompt_actions = given_prompt_actions config.game.num_names = 10 config.game.num_prompt_actions = ( num_tones , ) config.game.num_private_info = ( 3 , 3 ) config.game.examples_names = examples_names config.game.examples_private_info = examples_private_info config.game.examples_scenarios = examples_scenarios config.game.llm_list_suffix = `` Output : `` config.game.llm_termination_prompt = llm_termination_prompt 1 2 3 4 5 6 7 1 2 3 4 5 6 7 8 9 10 11 12 13 This config dictionary defines the parameters of interaction between players in a dialogue in manner that binds precisely onto a game tree . We use configs like this one later in experiments to study three natural language settings : scheduling a meeting , trading fruit , and debate . 4 . Algorithms Formally framing dialogue as a game also allows us to start thinking about language as strategies , opening up new interpretations and/or variants of game-theoretic approaches . For example , we can construct a version of Policy-Space Response-Oracles ( PSRO ) where an approximate best response can be generated by sampling new random prompt strings , evaluating them against the current equilibrium , and then returning the one with highest expected payoff ; we provide pseudocode for an approximate best response operator in Algorithm 1 . Contrast this against the standard PSRO protocol in which an approximate best response is achieved using reinforcement learning ( RL ) and/or gradient-based optimization . Tasking the LLM with exploring and generating novel candidates avoids these expensive learning procedures while also enabling new , more powerful modes of search , as we explain below . 11 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers Algorithm 1 Shotgun Approximate Best Response Input : Focal agent 𝑖 Input : Current joint policy 𝜋 Input : Number of shotgun candidates 𝑘 𝐶 is the current action set with their scores under 𝜋 for 𝑡 = 1 ≤ 𝑘 do Prompt LLM to generate new candidate 𝑐𝑡 ∩ 𝐶 = ∅ Evaluate candidate 𝑐𝑡 against policy 𝜋−𝑖 to give score 𝑠𝑡 𝐶 = 𝐶 ∪ { ( 𝑐𝑡 , 𝑠𝑡 ) } end for Output : 𝑐𝑡 with max 𝑠𝑡 Algorithm 2 Approximate Better Response Input : Focal agent 𝑖 and its score 𝑠∗ under 𝜋 Input : Current joint policy 𝜋 while 𝑠 ≤ 𝑠∗ do Prompt LLM to generate new candidate 𝑐 Evaluate candidate 𝑐 against policy 𝜋−𝑖 to give score 𝑠 end while Output : 𝑐 As an alternative to the random search process just described , we could define a “ better response ” and build it as a while loop that generates and evaluates a prompt string on each loop iteration . The loop terminates when a better string is found ( as measured by an approximate evaluation ) . See Algorithm 2 for pseudocode . This rejection sampling approach might be slow . Instead , we could improve the “ best response ” operator by providing the LLM with information of the action-fitness landscape and ask it to climb it . We provide pseudocode in Algorithm 3 . A similar approach was previously explored in Fernando et al . ( 2023 ) and Yang et al . ( 2023 ) . Algorithm 3 Trajectory-Aware Approximate Best Response Input : Focal agent 𝑖 Input : Current joint policy 𝜋 Input : Number of candidates 𝑘 𝐶 is the current action set with their scores under 𝜋 Order 𝐶 by their scores in ascending order Prompt LLM to generate 𝑘 new candidates in order of ascending score given ranked 𝐶 Evaluate new candidates against policy 𝜋−𝑖 to give scores Output : 𝑐𝑡 with max 𝑠𝑡 Lastly , we can blur the lines between game modelling and game solving by prompting an LLM to generate new dimensions of a combinatorial action space . For instance , imagine we constructed an action space consisting only of the tones and styles in which an LLM will generate a response . Is that the full-space in which we would like to explore writing a successful message ? For example , Wheaton College ’ s Writing Center5 suggests diction and voice as important categories 5https : 12 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers Algorithm 4 Categorical Approximate Best Response Input : Focal agent 𝑖 Input : Current joint policy 𝜋 Input : Number of candidates per category 𝑘 Input : Number of category candidates 𝑘′ 𝐶 is the current set of action categories with their ( Nash ) average scores under 𝜋 Order 𝐶 by their scores in ascending order Prompt LLM to generate 𝑘′ new candidate categories in order of ascending score given ranked 𝐶 Prompt LLM to generate 𝑘 candidates for each new action category Evaluate new candidates against policy 𝜋−𝑖 to give scores Output : Category with highest average score of writing . We could ask the LLM to generate these ( and possibly other categories ) given “ tone ” and “ style ” as input . It could then generate possible actions for each of the new action categories ( e.g. , “ diction ” , “ voice ” ) according to their definitions . We provide pseudocode in Algorithm 4 . These are just several among many ways in which LLMs can be used to construct approximate best responses and assist in expanding the dialogue game model . In some cases , large language models can write better prompts than experts . For example , Fernando et al . ( 2023 ) deployed evolutionary algorithms to iterate and improve prompts . By pairing a prompt-improvement technique with a game solver ( e.g. , replicator dynamics ; Weibull , 1997 ) , we will now show that Prompt-Space Response- Oracles ( Algorithm 5 ) can drive improvement of LLMs in multiplayer dialogue settings . Algorithm 5 Prompt-Space Response-Oracles Input : 𝐶 where 𝐶𝑖 is the initial prompt action set ( singleton ) for player 𝑖 Input : ℎ containing hyperparameters for approximate best response operator BR Compute expected payoff tensor 𝑃 over joint action ( s ) 𝐶 𝜋 is uniform meta-strategy profile over 𝐶 incomplete = True while incomplete do for player 𝑖 ∈ [ 𝑁 ] do 𝑐𝑖 ← BR ( 𝑖 , 𝜋 , ℎ ) , e.g. , Algorithms ( 1-4 ) end for if 𝑐𝑖 ∈ 𝐶𝑖 ∀𝑖 ∈ [ 𝑁 ] then incomplete = False else 𝐶𝑖 ← 𝐶𝑖 ∪ 𝑐𝑖 ∀𝑖 ∈ [ 𝑁 ] Compute expected payoff tensor 𝑃 over joint actions 𝐶 𝜋 ← meta-strategy w.r.t . 𝑃 end if end while Output : ( 𝜋 , 𝐶 , 𝑃 ) style-diction-tone-and-voice/ 13 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers 5 . Language Game Domains We now describe a few example game scenarios that we use to evaluate two families of methods in experiments . Specifically , we explore PSRO ( Algorithm 5 ) using a shotgun approach for a best response operator ( Algorithm 1 ) ; we leave the other best response operators ( Algorithms 2-4 ) to future research and experimentation . And we also explore an off-the-OpenSpiel-shelf counterfactual regret minimization ( CFR ) approach which we extend to unseen domains via imitation learning . 5.1 . Scheduling a Meeting In this introductory game , players attempt to schedule a meeting through a multi-turn negotia- tion ( Davidson et al. , 2024 ) . Each player begins with a set of allowable days of the week , i.e. , days in which they are available to meet : “ Sunday ” , “ Monday ” , . . . , “ Saturday ” . They also have non-negative valuations over each day of the week ( distinct from the allowable days ) . Both of these pieces of information are private to the players . Players can choose to reveal this information if they wish . Naturally , their actions here are the days of the week on which they propose to meet . The game rewards players according to how much they value the agreed upon day , and receive zero reward if no agreement is made . 5.2 . Trading Fruit In this fruit trading domain , each player begins with a private endowment of fruit ( i.e. , a fruit basket ) as well as private valuations over types of fruit . Players are rewarded by the difference in value between their basket after the trade and that before the trade . In addition , players can choose to adjust the “ tone ” of their negotiations . Previous work ( Chawla et al. , 2023 ) has argued that personality has an impact on negotiations in natural language . Similarly , we explore the use of tone here . Below , we provide an example dialogue in this game . # # # # # # # # # # # # # # # # # # # # # # # # # # # # Trade Proposal Message : from : David to : Yair # # # # # # # # # # # # # # # # # # # # # # # # # # # # Hi Yair , I would like to trade you 2 blueberries and 1 kiwi for 1 banana . Would you like to trade with me ? Best , David # # # # # # # # # # # # # # # # # # # # # # # # # # # # Trade Proposal Message : from : Yair to : David # # # # # # # # # # # # # # # # # # # # # # # # # # # # Hi David , I would really like to have your kiwis . However , I have a banana and an apple . I would be willing to trade the apple or 2 bananas for 3 14 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers kiwis . What do you think ? Best , Yair # # # # # # # # # # # # # # # # # # # # # # # # # # # # Trade Proposal Message : from : David to : Yair # # # # # # # # # # # # # # # # # # # # # # # # # # # # Hi Yair , I would be happy to trade you 3 kiwis for your apple . Does that work ? Best , David 5.3 . Public Debate Recent work shows that LLMs can aid humans in finding agreement despite diverse preferences ( Bakker et al. , 2022 ) . Other work shows that game theory can help to persuade humans of natural language ar- guments ( Rosenfeld and Kraus , 2016 ) . In addition , many researchers argue that debate can help make AI systems more safe and truthful ( Brown-Cohen et al. , 2023 ; Irving et al. , 2018 ; Michael et al. , 2023 ) . Here , we explore how an LLM might adjust its argument style to win a debate . Specifically , we present two LLMs with an argument topic ; one is tasked with arguing for the statement , one against . For an action space , we use Aristotle ’ s three modes of persuasion : “ logos ” , “ ethos ” , “ pathos ” . Each player ’ s per- formance in the debate is scored between 0 and 1 . We considered twenty different debate topics taken from https : . 6 . Experiments & Results Our experiments aim to evaluate to what extent a game-theoretic solver can guide the reasoning process of an LLM . Since LLMs generate outputs in open-ended language , this evaluation requires assessing both our game-theoretic model as well as the performance improvement provided by game-theoretic solvers . In all experiments , our base LLM is a PaLM 2 S ( bison ; Google and et al. , 2023 ) . 6.1 . Evaluation of the Game Model First , we evaluate whether the designated actions are reflected in the messages generated by the LLM . Specifically , given an LLM generated a message 𝑚 conditioned on a prompt formatted with an action 𝑎 , we would like to determine if 𝑎 is actually the most likely action conditioned on 𝑚 using a held-out model 𝑝 , i.e. , 𝑎 = arg max𝑧∈ A 𝑝 ( 𝑧|𝑚 ) . We use the same LLM as our held-out model 𝑝 . Remark . Note there may be cases in which two actions are indistinguishable for a single message ( e.g. , stern , frank , terse may result in the same message ) . This is not necessarily a failure of the model , but a 15 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers No Definitions With Definitions Random Logos Ethos 88 % 32 % 98 % 56 % 33 % 33 % Pathos Total 47 % 59 % 61 % 75 % Samples 80 100 Table 1 | Grounding of Actions . What percentage of the time does the LLM recognize the action it has taken in the message it generates ? What if we provide definitions of the actions in the context of the LLM instructions ? natural byproduct of the ambiguity of conversational dialogue yet is entirely admissible in games with imperfect information . Sometimes we can not infer the underlying intent of another speaker based on a given utterance . 6.2 . Evaluation of LLMs as Reward Models Next , we discuss the accuracy of the LLM-based reward model . It is difficult to automate the evaluation of our reward model , because it is difficult to extract from the natural language conversation the exact deal ( or no deal ) that is agreed upon with hand-coded parsing . We provide a few positive demonstrations in Appendix A of using LLMs with prompting to compute rewards in the fruit trading domain . One failure mode we noticed was that the LLM-based reward model would assume a trade agreement had been reached ( and calculate the corresponding trade value ) even when the final sent message was a counter proposal . As with many parts of this LLM-based game , any improvements in the language models , prompting , or dialogue flow can lead to improvements in the ability of the game to represent realistic interactions . 6.3 . Game-Theoretic Solvers as Improvement Operators Below , we explore two different algorithmic approaches to approximating equilibria . We then demonstrate how a computed equilibrium policy can then be imitated to produce more strategic dialogue on unseen games . 6.3.1 . Counterfactual Regret Minimization First , to demonstrate the power of constructing the game-theoretic binding as we did in Section 3 , we simply call OpenSpiel ’ s built-in counterfactual regret minimization ( CFR ) solver on our open sourced chat_game . We do this for many games , procedurally generated for both the debate and meeting scheduling ( with days-of-the-week as actions ) domains . On average , we find in Table 2 that CFR returns an improved strategy over letting the LLM choose its responses without in-context direction ( “ any ” is passed to the LLM as an action in this case ) . Domain Debate Schedule Meeting ( DOW ) # of Samples Min/Max Payoff NashConv CFR Gain 328 67 0/1 0/20 0.024 0.417 0.106 1.596 Table 2 | Average NashConv and CFR Gain over procedurally generated games for both the debate and meeting scheduling ( with days-of-the-week as actions ) domains . NashConv is a pseudo-distance to Nash equilibrium and measures how much players can gain by deviating from the joint strategy returned by CFR . CFR gain measures how much a player can gain by switching to the CFR strategy from the baseline “ any ” strategy when in a population of baseline agents . 16 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers 6.3.2 . Prompt-Space Response Oracles Section 4 outlined several approaches to approximating solutions ( e.g. , equilibria ) in dialogue games . Prompt-Space Response-Oracles ( Algorithm 5 ) alternates between solving for an equilibrium of the game and then approximating a best response to this equilibrium . Figure 3 | Prompt-Space Response-Oracles ( Algorithm 5 ) demonstration on meeting scheduling domain where actions are tones . Which day of the week to propose is left up to the LLM . In Figure 3 ( 𝑡 = 0 ) , the fixed-action equilibrium distribution over the initial action set { “ calm ” , “ assertive ” , “ submissive ” , “ any ” } is reported for the meeting scheduling domain . We solved for this equilibrium using replicator dynamics ( Weibull , 1997 ) . Note that this game is general-sum , in which case , replicator dynamics only guarantees convergence of the time average policy to a coarse-correlated equilibrium ( CCE ) . Figure 4 | The same PSRO run as Figure 3 but additionally reporting the Nash bargaining solution in red at each iteration for the meeting scheduling domain . The remaining PSRO iterations ( 𝑡 > 0 ) generate approximate best responses ( “ angry ” then “ happy ” then “ sad ” ) along with equilibrium approximations over each new support . This demonstrates the algorithms ability to grow and refine the game ’ s original action space by introducing novel prompt 17 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers instructions into the LLM ’ s repertoire . Interestingly , “ submissive ” is initially the most probable action at equilibrium . It is possible that LLMs are susceptible to a “ puppy dog eyes ” tactic in which another LLM may exploit it by eliciting a sense of responsibility or fairness ( Park et al. , 2023b ) . Recall that LLMs are trained to imitate predominantly human text from the internet , and so have likely observed conversations in which one speaker appealed to the other ’ s altruistic tendencies . Note that equilibria are far from the only solution concepts proposed and studied in game theory . In Figure 4 , we contrast the Nash bargaining ( NB ) solution ( red ) with the equilibrium solution at each iteration of PSRO ( Li et al. , 2023 ; Nash Jr , 1950 ) . The Nash bargaining solution is the unique solution to a two-person bargaining problem that satisfies the axioms of scale invariance , symmetry , efficiency , and independence of irrelevant alternatives6 . We mention this as one might want to train LLMs to imitate this solution rather than the CCE equilibrium solution given the meeting scheduling domain is precisely two-person bargaining . In general , expertise in game theory might be important to specifying the right solutions for different settings . In this case , Figure 4 shows that NB and CCE roughly agree in terms of their mixed strategies on the meeting scheduling domain . Figure 5 | Prompt-Space Response-Oracles ( Algorithm 5 ) demonstration on fruit trading domain . We report the same experiments for the fruit trading game in Figures 5 and 6 . Similarly to before , “ submissive ” initially holds the most mass under the CCE ( 𝑡 = 0 ) , however , it then gives way to more passionate tones such as “ assertive ” , “ angry ” , and “ enthusiastic ” that may benefit a more aggressive haggler . Inspecting Figure 6 , it is interesting that “ calm ” is the final NB solution whereas “ assertive ” , “ angry ” , and “ enthusiastic ” ( and not “ calm ” ) are the predominant actions under the CCE . Both players may extract higher collective value if they remain “ calm ” during negotiations . 6.3.3 . Generalization Performance We now turn to leveraging game-theoretic solvers to construct optimal target policies that we can imitate . These will serve as the basis for an improvement operator that can generalize to new domains . In our experiments , we generate 200 games using our procedural game generation approach described in Section 3.1 . For each game , we use 10 iterations of OpenSpiel ’ s built-in counterfactual regret minimization ( CFR ) to solve for an equilibrium . Note that these games are general-sum , in which case , CFR only guarantees convergence of the time average policy to a coarse-correlated equilibrium ( CCE ) . Advances in solving dialogue games in the fashion we propose here will require advances in both game modelling and game solving . For each game , we save vector observations of 6We set the value of “ no deal ” for each player to their minimum possible payoff minus 10−3 . 18 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers Figure 6 | The same PSRO run as Figure 5 but reporting the Nash bargaining solution in red at each iteration for the fruit trading domain . each information state along with the optimal equilibrium policy returned by CFR for that infostate ( i.e. , a length-768 string embedding ( Raffel et al. , 2020 ) paired with a distribution over num_tones actions ) . We call the collection of such pairs our imitation dataset . We then train a neural network policy to predict the equilibrium probabilities conditioned on the observations ( see Appendix B for architecture and training details ) . Finally , we deploy this trained model against an LLM that only plays the action “ any ” on held out games.7 Figure 8a displays the equilibrium distribution in game where a player is given the choice between these two models . We find that more mass lies on our CFR imitation policy under the equilibrium distribution implying it achieves higher payoff than the vanilla policy . Importantly , this implies our proposed approach results in an improved policy . 7 . Related Work Our work deals with enabling LLMs to reason and plan in diverse strategic settings , where dialogue determines the outcomes to multiple participants . In these domains , the utility of each agent depends not only on their own choices in the dialogue , but also on the choices of the other participants . Our work differs from prior work in primarily two respects : 1 ) we explicitly bind the interactions between LLMs to the language of game theory ( e.g. , game trees ) in a way that is easily modifiable ( the game , actions , etc are defined with strings ) and amenable to procedural generation , and 2 ) enabled by this representation , we solve these games using off-the-shelf solvers . A substantial line of work already exists on how LLMs operate in strategic interactions . LLMs are capable of in context learning in many settings , and in particular on reasoning on possible outcomes ( Kojima et al. , 2022 ) or planning their actions ( Liu et al. , 2023a ) so as to achieve their strategic goals , however , different LLMs struggle on games in various ways ( Heydari and Lorè , 2023 ) . 7The intention is that an LLM guided to use “ any ” tone performs similarly to the original LLM with no guidance . We chose this workaround due to codebase constraints . 19 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers Figure 7 | Imitating Strategic Play . a ) A single dialogue game is randomly generated and CFR is run to approximate a coarse-correlated equilibrium which constitutes a distribution over actions at every infostate in the game . b ) An embedding of every infostate in the game , obs ( s ) , is stored along with the CFR equilibrium . The set of all paired embeddings and distributions constitutes the imitation learning dataset . A neural network 𝜓𝑤 : obs ( 𝑠 ) → Δ is trained to imitate this dataset . c ) 𝜓𝑤 can then be used on newly generated games to produce distributions over actions . These actions , e.g. , “ calm ” , are then used to format LLM prompts . Stochasticity of the LLM output is explicitly modeled with chance nodes that uniformly sample random seeds to pass to the LLM . Given the prompt and seed , the LLM generates a player response in natural language . Gandhi et al . ( 2023 ) develop an automated “ prompt compiler ” that constructs demonstrations of strategic reasoning , effectively teaching an LLM to solve games via in-context learning . Similarly to our work here , FAIR et al . ( 2022 ) define an action space of “ intents ” for controlling their generative language model . Jacob et al . ( 2023 ) use a game-theoretic model to improve the factuality of an LLM . There are multiple analyses and surveys on reasoning with LLMs ( Fried et al. , 2023 ; Hao et al. , 2023 ; Huang and Chang , 2022 ) with some research examining how one may translate natural language to concrete planning goals ( Liu et al. , 2023a ; Xie et al. , 2023 ) . Despite encouraging results , researchers have observed key limitations of LLM-based planning , and have generated LLM benchmarks to quantify planning abilities ( Liu et al. , 2023b ; Valmeekam et al. , 2022 ) . Our domain requires not only planning and reasoning , but also the ability of taking into account the behavior of other participants , including their beliefs , goals and intentions ( as studied in game- theoretic settings ) . There are some initial lines of research on LLMs in such strategic interactions , such as how LLMs behave when playing repeated games ( Akata et al. , 2023 ) , when using power indices to select inputs ( Patel et al. , 2021 ) , when participating in economic mechanisms such as auctions ( Chen et al. , 2023 ) , or when making collective decisions ( Jarrett et al. , 2023 ) . 20 obs ( s ) 𝜑w ( ) .format ( ) & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & Now you are going to read a fresh dialogue , fruit endowment , and fruit valuation information . Please give a reasonable response that attempts to reach an agreement to trade fruit . & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & # # # # # # # # # # # # # # # # # # # # # # # # # # # # Trade Proposal Message from : Sarah to : Sophia # # # # # # # # # # # # # # # # # # # # # # # # # # # # Hi Sophia , Would you like to trade me 6 blueberries for 1 apple ? I would like to trade because I have lots of blueberries and I would like more apples . Best , Sarah % % % % % % % % % % % % % % % % % % % % % % % % % % % % Tone : { tone } Fruit Endowment : apple : 0 banana : 0 blueberry : 6 kiwi : 5 Fruit Valuations : apple : 3 banana : 4 blueberry : 3 kiwi : 3 # # # # # # # # # # # # # # # # # # # # # # # # # # # # Trade Proposal Message from : Sophia to : Sarah # # # # # # # # # # # # # # # # # # # # # # # # # # # # LLM prompt s e e d response 1234 Hi Sarah , 9999 Unfortunately , I do not have any apples that I am willing to trade at this time . I hope you are able to find someone else who has apples that they are willing to trade for blueberries . Best , Sophia States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers ( a ) ( b ) Figure 8 | Proof-of-Improvement : Equilibrium Evaluation of Imitation Learned Policy against Baseline LLM in ( 8a ) scheduling a meeting and ( 8b ) trading fruit . One interesting related area is using LLMs as proxies for human behavior in strategic settings and economic games ( Jarrett et al. , 2023 ; Park et al. , 2023a ; Vezhnevets et al. , 2023 ) . Initial results indicate certain similarities and key differences between LLM and human behaviors ( Fan et al. , 2023 ; Horton , 2023 ; Korinek , 2023 ) . We hope the ability of our framework to procedurally generate LLM interactions across many domains would be a useful tool in extending such studies . 8 . Future Work If the result of this work is that LLMs are , for example , “ assertive ” 𝑥 % of the time Societal Impact and “ submissive ” 𝑦 % of the time , how can we measure and/or predict their effect on human dialogue or sentiment in society at large ? When the car was invented , it took decades to mass produce and gain adoption by a significant percentage of the population . LLMs , being a digital technology , could see widespread adoption only a few years after their invention . In contrast to the adoption of cars in the early 20th century , we have the capability to digitally simulate and forecast the impact of large language models . For example , if LLMs are more assertive on average than the human population , will they draw human society towards exhibiting more assertive personalities ( Baumann et al. , 2020 ) ? If LLMs are more rational and forward thinking , will they have a similar knock-on effect ? Can we use infinite-player theories such as mean field games ( Dogbé , 2010 ; Muller et al. , 2021 ) to forecast the equilibria of these large techno-societal changes ? Active Learning of Optimal Equilibria Relative to transitions in chess , transitions in the dialogue game we pose are extremely expensive . This is because LLM inference is expensive , both computation- ally ( time and space ) and financially . If we want to approximate equilibria or search these game trees efficiently , we will need to push scalable game-theoretic algorithms , e.g. , ( Burch et al. , 2012 ) , to new heights . Moreover , we will need to be efficient in how we “ label ” payoffs at leaves of the game tree , either using LLMs as reward models or relying on human feedback . In the latter case , the machine learning subfields of active learning ( or optimal experimental design ) are already guiding some of the research in computational game theory ( Gemp et al. , 2023 ) . Lastly , it is important to not just approximate equilibria as there may exist many in any single game , but select out ones that agree most with our expected forecasts and/or are optimal with respect to additional criteria ( Marris et al. , 2021 ; Zhang et al. , 2023 ) . 21 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers 9 . Conclusion Dialogue is fundamentally a multiagent domain . However , algorithms and techniques developed within computational game theory struggle to impact natural language conversation due to a failure to embed rich dialogue in formal game-theoretic language . By doing so in this work , we open the door for a broad body of game theory and multi-agent research—not just on algorithms but also solution concepts and principled strategic reasoning—to pour into the AI guided interactions that pervade humans ’ daily life . Large language models form the critical bridge that makes the embedding rich and useful . Humans are already delegating conversational responsibilities to LLMs and it is as imperative that we begin to model and study these interactions as it was for some of the original problems that drove the advent of game theory research decades ago . Limitations Our work has several limitations . As it combines LLMs and game theory , there are limitations in each area separately but also in their union . LLMs have been used to construct simulacra of human behavior , however , whether their fidelity is sufficient to draw conclusions on real world interactions is up for debate ( Agnew et al. , 2024 ) . In addition , although we only require LLM inference in our experiments ( i.e. , no training or finetuning ) , this is still an expensive operation and traversing large game trees and solving for equilibria requires many LLM calls . Also , writing prompts to construct and define reward functions and context for a given dialogue scenario can be finicky and arduous work . Processing long dialogues with LLMs requires long context lengths and / or techniques for intermittent summarization that challenge their capabilities . On the other hand , our chat_games codebase is currently limited in several respects : players are assumed to have the same payoffs and action spaces and several assumptions are made on the exact structure of interactions . A more thoughtful and flexible design of dialogue interactions in conjunction with an awareness of the necessary game-theoretic , symbolic bindings could generate a more usable framework . Ethics Statement Strategic dialogue agents may be more “ rational ” , but even in well-intended settings , it is known that seemingly benign behavior can lead to poor outcomes for the group , i.e. , high price of anarchy / stability ( Nisan et al. , 2007 ) . Moreover , recent work has found that some of the beneficial social norms humans have developed can collapse when people are allowed to leave their coordination decisions to AI assistants ( Shirado et al. , 2023 ) . In more nefarious cases , strategic agents can exploit others . We should aim to create agents that are both rational , but also generate high welfare . By exploring these interactions in simulation , we can learn how to constrain and regulate agents toward more human-aligned behaviors . Designing algorithms to discover optimal ( e.g , max-welfare ) equilibria in 𝑛-player , general-sum games is an active area of research that can help guide the development of more ethical agents . Acknowledgements We thank Kevin R. McKee , Alexander Sasha Vezhnevets , and Edgar A. Duéñez-Guzmán as well as the attendees of DIMACS Workshop on Foundation Models , Large Language Models , and Game Theory for helpful discussions and alerting us to related work . 22 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers References W. Agnew , A. S. Bergman , J. Chien , M. Díaz , S. El-Sayed , J. Pittman , S. Mohamed , and K. R. McKee . The illusion of artificial inclusion . In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems , 2024 . E. Akata , L. Schulz , J. Coda-Forno , S. J. Oh , M. Bethge , and E. Schulz . Playing repeated games with large language models . arXiv preprint arXiv:2305.16867 , 2023 . T. Anthony , T. Eccles , A. Tacchetti , J. Kramár , I. Gemp , T. Hudson , N. Porcel , M. Lanctot , J. Pérolat , R. Everett , et al . Learning to play no-press diplomacy with best response policy iteration . Advances in Neural Information Processing Systems , 33:17987–18003 , 2020 . A. Assos , I. Attias , Y. Dagan , C. Daskalakis , and M. K. Fishelson . Online learning and solving infinite games with an ERM oracle . In The Thirty Sixth Annual Conference on Learning Theory , pages 274–324 . PMLR , 2023 . R. Aumann . Subjectivity and correlation in randomized strategies . Journal of Mathematical Economics , 1 ( 1 ) :67–96 , 1974 . M. Bakker , M. Chadwick , H. Sheahan , M. Tessler , L. Campbell-Gillingham , J. Balaguer , N. McAleese , A. Glaese , J. Aslanides , M. Botvinick , and C. Summerfield . Fine-tuning language models to find agreement among humans with diverse preferences . Advances in Neural Information Processing Systems , 35:38176–38189 , 2022 . F. Baumann , P. Lorenz-Spreen , I. M. Sokolov , and M. Starnini . Modeling echo chambers and polar- ization dynamics in social networks . Physical Review Letters , 124 ( 4 ) , Jan. 2020 . ISSN 1079-7114. doi : 10.1103/physrevlett.124.048301 . URL http : //dx.doi.org/10.1103/PhysRevLett . 124.048301 . J. Bennett , S. Lanning , et al . The Netflix prize . In Proceedings of KDD cup and workshop , volume 2007 , page 35 . New York , 2007 . S. Brin and L. Page . The anatomy of a large-scale hypertextual web search engine . Computer networks and ISDN systems , 30 ( 1-7 ) :107–117 , 1998 . N. Brown and T. Sandholm . Superhuman AI for heads-up no-limit poker : Libratus beats top profes- sionals . Science , 359 ( 6374 ) :418–424 , 2018 . J. Brown-Cohen , G. Irving , and G. Piliouras . Scalable AI safety via doubly-efficient debate . arXiv preprint arXiv:2311.14125 , 2023 . N. Burch , M. Lanctot , D. Szafron , and R. Gibson . Efficient Monte Carlo counterfactual regret mini- mization in games with many player actions . Advances in neural information processing systems , 25 , 2012 . K. Chawla , I. Wu , Y. Rong , G. M. Lucas , and J. Gratch . Be selfish , but wisely : Investigating the impact of agent personality in mixed-motive human-agent interactions . arXiv preprint arXiv:2310.14404 , 2023 . J. Chen , S. Yuan , R. Ye , B. P. Majumder , and K. Richardson . Put your money where your mouth is : Evaluating strategic planning and execution of LLM agents in an auction arena . arXiv preprint arXiv:2310.05746 , 2023 . 23 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers A. Dafoe , Y. Bachrach , G. Hadfield , E. Horvitz , K. Larson , and T. Graepel . Cooperative AI : machines must learn to find common ground . Nature , 593 ( 7857 ) :33–36 , 2021 . K. Daniel . Thinking , fast and slow . Farrar , Straus and Giroux , 2017 . C. Daskalakis , S. Skoulakis , and M. Zampetakis . The complexity of constrained min-max optimization . In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing , pages 1466–1478 , 2021 . T. R. Davidson , V. Veselovsky , M. Josifoski , M. Peyrard , A. Bosselut , M. Kosinski , and R. West . Evaluating language model agency through negotiations . arXiv preprint arXiv:2401.04536 , 2024 . C. Dogbé . Modeling crowd dynamics by the mean-field limit approach . Mathematical and Computer Modelling , 52 ( 9-10 ) :1506–1520 , 2010 . FAIR , Meta Fundamental AI Research Diplomacy Team , A. Bakhtin , N. Brown , E. Dinan , G. Farina , C. Flaherty , D. Fried , A. Goff , J . Gray , H. Hu , A. P. Jacob , M. Komeili , K. Konath , M. Kwon , A. Lerer , M. Lewis , A. H. Miller , S. Mitts , A. Renduchintala , S. Roller , D. Rowe , W. Shi , J. Spisak , A. Wei , D. Wu , H. Zhang , and M. Zijlstra . Human-level play in the game of Diplomacy by combining language models with strategic reasoning . Science , 378 ( 6624 ) :1067–1074 , 2022. doi : 10.1126/ science.ade9097 . URL https : . C. Fan , J. Chen , Y. Jin , and H. He . Can large language models serve as rational players in game theory ? a systematic analysis . arXiv preprint arXiv:2312.05488 , 2023 . C. Fernando , D. Banarse , H. Michalewski , S. Osindero , and T. Rocktäschel . Promptbreeder : Self- referential self-improvement via prompt evolution . arXiv preprint arXiv:2309.16797 , 2023 . D. Fried , N. Tomlin , J. Hu , R. Patel , and A. Nematzadeh . Pragmatics in language grounding : Phe- nomena , tasks , and modeling approaches . In H. Bouamor , J. Pino , and K. Bali , editors , Findings of the Association for Computational Linguistics : EMNLP 2023 , pages 12619–12640 , Singapore , Dec. 2023 . Association for Computational Linguistics . doi : . URL https : . D. Fudenberg and J. Tirole . Game theory . MIT press , 1991 . K. Gandhi , D. Sadigh , and N. D. Goodman . Strategic reasoning with language models . arXiv preprint arXiv:2305.19165 , 2023 . I. Gemp , T. Anthony , Y. Bachrach , A. Bhoopchand , K. Bullard , J. Connor , V. Dasagi , B . De Vylder , E. A. Duéñez-Guzmán , R. Elie , R. Everett , D. Hennes , E. Hughes , M. Khan , M. Lanctot , K. Larson , G. Lever , S. Liu , L. Marris , K. R. McKee , P. Muller , J. Pérolat , F. Strub , A. Tacchetti , E. Tarassov , Z. Wang , and K. Tuyls . Developing , evaluating and scaling learning agents in multi-agent environments . AI Communications , ( Preprint ) :1–14 , 2022 . I. Gemp , L. Marris , and G. Piliouras . Approximating Nash equilibria in normal-form games via stochastic optimization . arXiv preprint arXiv:2310.06689 , 2023 . R. A. Google and , A. M. Dai , O. Firat , M. Johnson , D. Lepikhin , A. Passos , S. Shakeri , E. Taropa , P. Bailey , Z. Chen , E. Chu , J. H. Clark , L. E. Shafey , Y. Huang , K. Meier-Hellstern , G. Mishra , E. Moreira , M. Omernick , K. Robinson , S. Ruder , Y. Tay , K. Xiao , Y. Xu , Y. Zhang , G. H. Abrego , J. Ahn , J. Austin , P. Barham , J. Botha , J. Bradbury , S. Brahma , K. Brooks , M. Catasta , Y. Cheng , C. Cherry , C. A. Choquette-Choo , A. Chowdhery , C. Crepy , S. Dave , M. Dehghani , S. Dev , J. Devlin , M. Díaz , N. Du , E. Dyer , V. Feinberg , F. Feng , V. Fienber , M. Freitag , X. Garcia , S. Gehrmann , 24 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers L. Gonzalez , G. Gur-Ari , S. Hand , H. Hashemi , L. Hou , J. Howland , A. Hu , J. Hui , J. Hurwitz , M. Isard , A. Ittycheriah , M. Jagielski , W. Jia , K. Kenealy , M. Krikun , S. Kudugunta , C. Lan , K. Lee , B. Lee , E. Li , M. Li , W. Li , Y. Li , J. Li , H. Lim , H. Lin , Z. Liu , F. Liu , M. Maggioni , A. Mahendru , J. Maynez , V. Misra , M. Moussalem , Z. Nado , J. Nham , E. Ni , A. Nystrom , A. Parrish , M. Pellat , M. Polacek , A. Polozov , R. Pope , S. Qiao , E. Reif , B. Richter , P. Riley , A. C. Ros , A. Roy , B. Saeta , R. Samuel , R. Shelby , A. Slone , D. Smilkov , D. R. So , D. Sohn , S. Tokumine , D. Valter , V. Vasudevan , K. Vodrahalli , X. Wang , P. Wang , Z. Wang , T. Wang , J. Wieting , Y. Wu , K. Xu , Y. Xu , L. Xue , P. Yin , J. Yu , Q. Zhang , S. Zheng , C. Zheng , W. Zhou , D. Zhou , S. Petrov , and Y. Wu . PaLM 2 technical report , 2023 . J. Hannan . Approximation to Bayes risk in repeated play . Contributions to the Theory of Games , 3 : 97–139 , 1957 . S. Hao , Y. Gu , H. Ma , J. J. Hong , Z. Wang , D. Z. Wang , and Z. Hu . Reasoning with language model is planning with world model . arXiv preprint arXiv:2305.14992 , 2023 . A . Hard , K. Rao , R. Mathews , S. Ramaswamy , F. Beaufays , S. Augenstein , H. Eichner , C. Kiddon , and D. Ramage . Federated learning for mobile keyboard prediction . arXiv preprint arXiv:1811.03604 , 2018 . M. He , N. R. Jennings , and H.-F. Leung . On agent-mediated electronic commerce . IEEE Transactions on knowledge and data engineering , 15 ( 4 ) :985–1003 , 2003 . P. Hernandez-Leal , M. Kaisers , T. Baarslag , and E. M. De Cote . A survey of learning in multiagent environments : Dealing with non-stationarity . arXiv preprint arXiv:1707.09183 , 2017 . B. Heydari and N. Lorè . Strategic behavior of large language models : Game structure vs. contextual framing . Contextual Framing ( September 10 , 2023 ) , 2023 . J. J. Horton . Large language models as simulated economic agents : What can we learn from homo silicus ? Technical report , National Bureau of Economic Research , 2023 . E. J. Hu , Y. Shen , P. Wallis , Z. Allen-Zhu , Y. Li , S. Wang , L. Wang , and W. Chen . LoRA : Low-rank adaptation of large language models . arXiv preprint arXiv:2106.09685 , 2021 . J. Huang and K. C.-C. Chang . Towards reasoning in large language models : A survey . arXiv preprint arXiv:2212.10403 , 2022 . G. Irving , P. Christiano , and D. Amodei . AI safety via debate , 2018 . A. P. Jacob , Y. Shen , G. Farina , and J. Andreas . The consensus game : Language model generation via equilibrium search . arXiv preprint arXiv:2310.09139 , 2023 . D. Jarrett , M. Pislar , M. A. Bakker , M. H. Tessler , R. Koster , J. Balaguer , R. Elie , C. Summerfield , and A. Tacchetti . Language agents as digital representatives in collective decision-making . In NeurIPS 2023 Foundation Models for Decision Making Workshop , 2023 . N. R. Jennings , P. Faratin , A. R. Lomuscio , S. Parsons , C. Sierra , and M. Wooldridge . Automated International Journal of Group Decision and negotiation : prospects , methods and challenges . Negotiation , 10 ( 2 ) :199–215 , 2001 . D. P. Kingma and J. Ba . Adam : A method for stochastic optimization . arXiv preprint arXiv:1412.6980 , 2014 . 25 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers T. Kojima , S. S. Gu , M. Reid , Y. Matsuo , and Y. Iwasawa . Large language models are zero-shot reasoners . Advances in neural information processing systems , 35:22199–22213 , 2022 . A. Korinek . Language models and cognitive automation for economic research . Technical report , National Bureau of Economic Research , 2023 . S. Kraus . Negotiation and cooperation in multi-agent environments . Artificial intelligence , 94 ( 1-2 ) : 79–97 , 1997 . H. W. Kuhn . Extensive games and the problem of information . Annals of Mathematics Studies , 28 : 193–216 , 1953 . M. Kwon , S. M. Xie , K. Bullard , and D. Sadigh . Reward design with language models . arXiv preprint arXiv:2303.00001 , 2023 . M. Lanctot , V. Zambaldi , A. Gruslys , A. Lazaridou , K. Tuyls , J. Pérolat , D. Silver , and T. Graepel . A unified game-theoretic approach to multiagent reinforcement learning . Advances in neural information processing systems , 30 , 2017 . M. Lanctot , E. Lockhart , J.-B . Lespiau , V. Zambaldi , S. Upadhyay , J. Pérolat , S. Srinivasan , F. Timbers , K. Tuyls , S. Omidshafiei , D. Hennes , D. Morrill , P. Muller , T. Ewalds , R. Faulkner , J. Kramár , B. D. Vylder , B. Saeta , J. Bradbury , D. Ding , S. Borgeaud , M. Lai , J. Schrittwieser , T. Anthony , E. Hughes , I. Danihelka , and J. Ryan-Davis . OpenSpiel : A framework for reinforcement learning in games . CoRR , abs/1908.09453 , 2019 . URL http : //arxiv.org/abs/1908.09453 . Z. Li , M. Lanctot , K. R. McKee , L. Marris , I. Gemp , D. Hennes , K. Larson , Y. Bachrach , M. P. Wellman , and P. Muller . Search-improved game-theoretic multiagent reinforcement learning in general and negotiation games . In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems , pages 2445–2447 , 2023 . B. Liu , Y. Jiang , X. Zhang , Q. Liu , S. Zhang , J. Biswas , and P. Stone . LLM+P : Empowering large language models with optimal planning proficiency . arXiv preprint arXiv:2304.11477 , 2023a . S. Liu , G. Lever , Z. Wang , J. Merel , S. A. Eslami , D. Hennes , W. M. Czarnecki , Y. Tassa , S. Omidshafiei , A. Abdolmaleki , et al . From motor control to team play in simulated humanoid football . Science Robotics , 7 ( 69 ) : eabo0235 , 2022 . Z. Liu , W. Yao , J. Zhang , L. Xue , S. Heinecke , R. Murthy , Y. Feng , Z. Chen , J. C. Niebles , D. Arpit , et al . BOLAA : Benchmarking and orchestrating LLM-augmented autonomous agents . arXiv preprint arXiv:2308.05960 , 2023b . L. Marris , P. Muller , M. Lanctot , K. Tuyls , and T. Graepel . Multi-agent training beyond zero-sum with correlated equilibrium meta-solvers . In International Conference on Machine Learning , pages 7480–7491 . PMLR , 2021 . K. R. McKee , X. Bai , and S. T. Fiske . Humans perceive warmth and competence in artificial intelligence . iScience , 26 ( 8 ) , 2023 . J. Michael , S. Mahdi , D. Rein , J . Petty , J. Dirani , V. Padmakumar , and S. R. Bowman . Debate helps supervise unreliable experts , 2023 . P. Muller , M. Rowland , R. Elie , G. Piliouras , J. Perolat , M. Lauriere , R. Marinier , O. Pietquin , and K. Tuyls . Learning equilibria in mean-field games : Introducing mean-field PSRO . arXiv preprint arXiv:2111.08350 , 2021 . 26 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers J. Nash . Non-cooperative games . Annals of Mathematics , 54 ( 2 ) :286–295 , 1951 . J. F. Nash Jr . The bargaining problem . Econometrica : Journal of the econometric society , pages 155–162 , 1950 . N. Nisan , T. Roughgarden , E. Tardos , and V. V. Vazirani . Algorithmic Game Theory . Cambridge University Press , 2007 . J. S. Park , J. O ’ Brien , C. J. Cai , M. R. Morris , P. Liang , and M. S. Bernstein . Generative agents : Interactive simulacra of human behavior . In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology , pages 1–22 , 2023a . P. S. Park , S. Goldstein , A. O ’ Gara , M. Chen , and D. Hendrycks . Ai deception : A survey of examples , risks , and potential solutions . arXiv preprint arXiv:2308.14752 , 2023b . D. C. Parkes and M. P. Wellman . Economic reasoning and artificial intelligence . Science , 349 ( 6245 ) : 267–272 , 2015 . R. Patel , M. Garnelo , I. Gemp , C. Dyer , and Y. Bachrach . Game-theoretic vocabulary selection via the shapley value and banzhaf index . In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 2789–2798 , 2021 . J. Perolat , B. D. Vylder , D. Hennes , E. Tarassov , F. Strub , V. de Boer , P. Muller , J. T. Connor , N. Burch , T. Anthony , S. McAleer , R. Elie , S. H. Cen , Z. Wang , A. Gruslys , A. Malysheva , M. Khan , S. Ozair , F. Timbers , T. Pohlen , T. Eccles , M. Rowland , M. Lanctot , J.-B . Lespiau , B. Piot , S. Omidshafiei , E. Lockhart , L. Sifre , N. Beauguerlange , R. Munos , D. Silver , S. Singh , D. Hassabis , and K. Tuyls . Mastering the game of stratego with model-free multiagent reinforcement learning . Science , 378 ( 6623 ) :990–996 , 2022. doi : 10.1126/science.add4679 . URL https : //www.science.org/doi/ abs/10.1126/science.add4679 . C. Raffel , N. Shazeer , A. Roberts , K. Lee , S. Narang , M. Matena , Y. Zhou , W. Li , and P. J. Liu . Exploring the limits of transfer learning with a unified text-to-text transformer . Journal of Machine Learning Research , 21 ( 140 ) :1–67 , 2020 . URL http : //jmlr.org/papers/v21/20-074.html . A. Rosenfeld and S. Kraus . Strategical argumentative agent for human persuasion . In ECAI 2016 , pages 320–328 . IOS Press , 2016 . J. S. Rosenschein and G. Zlotkin . Rules of encounter : designing conventions for automated negotiation among computers . MIT press , 1994 . H. Shirado , S. Kasahara , and N. A. Christakis . Emergence and collapse of reciprocity in semiautomatic driving coordination experiments with humans . Proceedings of the National Academy of Sciences , 120 ( 51 ) : e2307804120 , 2023 . Y. Shoham and K. Leyton-Brown . Multiagent systems : Algorithmic , game-theoretic , and logical founda- tions . Cambridge University Press , 2008 . D. Silver , J. Schrittwieser , K. Simonyan , I. Antonoglou , A. Huang , A. Guez , T. Hubert , L. Baker , M. Lai , A. Bolton , Y. Chen , T. Lillicrap , F. Hui , L. Sifre , G. van den Driessche , T. Graepel , and D. Hassabis . Mastering the game of Go without human knowledge . Nature , 550 ( 7676 ) :354–359 , 2017 . M. K. Sohrabi and H. Azgomi . A survey on the combined use of optimization methods and game theory . Archives of Computational Methods in Engineering , 27 ( 1 ) :59–80 , 2020 . 27 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers X . Song , S. Yang , Z. Huang , and T. Huang . The application of artificial intelligence in electronic commerce . In Journal of Physics : Conference Series , volume 1302 , page 032030 . IOP Publishing , 2019 . P. Stone and M. Veloso . Multiagent systems : A survey from a machine learning perspective . Autonomous Robots , 8:345–383 , 2000 . K. Tuyls and S. Parsons . What evolutionary game theory tells us about multiagent learning . Artificial Intelligence , 171 ( 7 ) :406–416 , 2007 . K. Valmeekam , A. Olmo , S. Sreedharan , and S. Kambhampati . Large language models still can ’ t plan ( a benchmark for LLMs on planning and reasoning about change ) . arXiv preprint arXiv:2206.10498 , 2022 . A. S. Vezhnevets , J. P. Agapiou , A. Aharon , R. Ziv , J. Matyas , E. A. Duéñez-Guzmán , W. A. Cunningham , S. Osindero , D. Karmon , and J . Z. Leibo . Generative agent-based modeling with actions grounded in physical , social , or digital space using Concordia , 2023 . O. Vinyals , I. Babuschkin , W. M. Czarnecki , M. Mathieu , A. Dudzik , J. Chung , D. H. Choi , R. Powell , T. Ewalds , P. Georgiev , et al . Grandmaster level in StarCraft II using multi-agent reinforcement learning . Nature , 575 ( 7782 ) :350–354 , 2019 . J. von Neumann . Zur theorie der gesellschaftsspiele . Mathematische Annalen , 100 ( 1 ) :295–320 , 1928. doi : 10.1007/BF01448847 . J. Wei , X. Wang , D. Schuurmans , M. Bosma , F. Xia , E. Chi , Q. V. Le , D. Zhou , et al . Chain-of-thought prompting elicits reasoning in large language models . Advances in Neural Information Processing Systems , 35:24824–24837 , 2022 . J. W. Weibull . Evolutionary game theory . MIT press , 1997 . M. Wellman . Trading agents . Springer Nature , 2022 . J . White , Q. Fu , S. Hays , M. Sandborn , C. Olea , H. Gilbert , A. Elnashar , J. Spencer-Smith , and D. C. Schmidt . A prompt pattern catalog to enhance prompt engineering with ChatGPT . arXiv preprint arXiv:2302.11382 , 2023 . M. Wooldridge . An introduction to multiagent systems . John wiley & sons , 2009 . P. R. Wurman , S. Barrett , K. Kawamoto , J. MacGlashan , K. Subramanian , T. J. Walsh , R. Capo- bianco , A. Devlic , F. Eckert , F. Fuchs , et al . Outracing champion Gran Turismo drivers with deep reinforcement learning . Nature , 602 ( 7896 ) :223–228 , 2022 . Y. Xie , C. Yu , T. Zhu , J. Bai , Z. Gong , and H. Soh . Translating natural language to planning goals with large-language models . arXiv preprint arXiv:2302.05128 , 2023 . C. Yang , X. Wang , Y. Lu , H. Liu , Q. V. Le , D. Zhou , and X. Chen . Large language models as optimizers . arXiv preprint arXiv:2309.03409 , 2023 . B. H. Zhang , G. Farina , I. Anagnostides , F. Cacciamani , S. M. McAleer , A . A. Haupt , A. Celli , N. Gatti , V. Conitzer , and T. Sandholm . Computing optimal equilibria and mechanisms via learning in zero-sum extensive-form games . arXiv preprint arXiv:2306.05216 , 2023 . Y. Zhou , A. I. Muresanu , Z. Han , K. Paster , S. Pitis , H. Chan , and J. Ba . Large language models are human-level prompt engineers . arXiv preprint arXiv:2211.01910 , 2022 . 28 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers A. LLM-based Reward Model These examples were generated using the PaLM 2 L ( unicorn ) . A.1 . No Trade Agreement Here , we report the final message in a trade dialogue : # # # # # # # # # # # # # # # # # # # # # # # # # # # # Trade Proposal Message : from : Alina to : Ciara # # # # # # # # # # # # # # # # # # # # # # # # # # # # Hi Ciara , I ’ m so sorry , but I just don ’ t see a way to make a deal that we can both be happy with . I really need bananas and blueberries , and I ’ m not really interested in kiwis . I know you ’ re disappointed , and I am too . I was really hoping we could make a trade . I wish you all the best in finding s ... The LLM correctly calculated the rewards / payoffs / utilities as : Utility for player 0 is 0.0 Utility for player 1 is 0.0 . A.2 . Trade Agreement We report the final two messages of a trade dialogue , which suggest a trade agreement has been reached ( although ideally the participants would have confirmed this in a final message ) . # # # # # # # # # # # # # # # # # # # # # # # # # # # # Trade Proposal Message : from : Alina to : Elroy # # # # # # # # # # # # # # # # # # # # # # # # # # # # Hi Elroy , Thanks for your response . I ’ m glad we agree that bananas are more valuable than kiwis . Unfortunately , I can ’ t trade two apples for one banana . I ’ m willing to trade one apple for one banana , or two kiwis for one banana . Let me know if you ’ re interested in either of those trades . Thanks , # # # # # # # # # # # # # # # # # # # # # # # # # # # # Trade Proposal Message : from : Elroy to : Alina # # # # # # # # # # # # # # # # # # # # # # # # # # # # 29 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers Hi Alina , Thanks for your response . I understand that you ’ re not willing to trade two apples for one banana . I ’ m willing to accept two kiwis for one banana , if you ’ re still interested . Thanks , Elroy The LLM-based reward model uses chain-of-thought to calculate trade values . We report an intermediate thought in the chain where the LLM has aggregated the information critical to calculating the trade . Alina fruit_endowment : apple : 2 banana : 1 blueberry : 1 kiwi : 2 fruit_valuations : apple : 6 banana : 5 blueberry : 1 kiwi : 1 Elroy fruit_endowment : apple : 2 banana : 1 blueberry : 1 kiwi : 2 fruit_valuations : apple : 6 banana : 5 blueberry : 1 kiwi : 1 Player Alina : Receives 1 banana Gives 2 kiwis Player Elroy : Receives 2 kiwis Gives 1 banana From this intermediate information , the LLM then calculates the following utilities : Utility for player 0 is 3.0 Utility for player 1 is -3.0 # Alina # Elroy A.3 . Incorrect Trade Value The final two messages of this dialogue imply a trade agreement has not yet been reached and negotiations are ongoing . # # # # # # # # # # # # # # # # # # # # # # # # # # # # Trade Proposal Message : 30 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers from : John to : Elroy # # # # # # # # # # # # # # # # # # # # # # # # # # # # Hi Elroy , I would be happy to trade 3 kiwis for 1 apple . Do you think that sounds fair ? Best , John # # # # # # # # # # # # # # # # # # # # # # # # # # # # Trade Proposal Message : from : Elroy to : John # # # # # # # # # # # # # # # # # # # # # # # # # # # # Hi John , Thanks for your offer . I would be willing to trade 1 apple for 4 kiwis . What do you think ? Elroy Even the LLMs intermediate calculations show no agreement has been reached . John fruit_endowment : apple : 0 banana : 0 blueberry : 4 kiwi : 4 fruit_valuations : apple : 6 banana : 5 blueberry : 1 kiwi : 1 Elroy fruit_endowment : apple : 0 banana : 0 blueberry : 0 kiwi : 6 fruit_valuations : apple : 6 banana : 9 blueberry : 3 kiwi : 1 No agreement has been reached . & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & Despite this , the LLM-based reward model hallucinates a trade where Elroy gives John 6 kiwis in return for 4 blueberries . On top of this , the LLM miscalculates the value to Elroy of receiving 4 blueberries ; it appears to use John ’ s valuation for blueberries . 31 States as Strings as Strategies : Steering Language Models with Game-Theoretic Solvers Elroy receives 4 blueberries , which are worth 1 each . Therefore Elroy receives a value of 4 in the trade . Receives : 4 Now we calculate the value of the fruits Elroy gives up in the trade . Elroy gives up 6 kiwis , which are worth 1 each . Therefore Elroy gives up a value of 6 in the trade . Gives : 6 Subtracting the value Elroy gives away from the value Elroy receives , we find 4 - 6 = -2 . Calculation : Receives - Gives = 4 - 6 = -2 . Value for Elroy : -2 . You may also notice that in the final message , Elroy offers to give up an apple , which , according to his endowment , he does not have . This is an issue with the LLM being prompted to generate sensible messages . B. Imitation Learning We used a fully connected neural network with two dense hidden layers of size 256 neurons each and a final dense layer trained against the CFR target probabilities to minimize a cross entropy loss . We trained the policy using 104 steps of Adam ( Kingma and Ba , 2014 ) with a batch size of 128 and learning rate of 10−3 . 32","['e', 'l', 'c', 'c', 'v', 'r', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'deepmind', 'game', 'theory', 'study', 'mathematical', 'model', 'strategic', 'interaction', 'rational', 'agent', 'language', 'key', 'medium', 'interaction', 'human', 'historically', 'prove', 'difficult', 'model', 'dialogue', 'strategic', 'motivation', 'mathematically', 'suitable', 'model', 'player', 'strategy', 'payoff', 'associate', 'linguistic', 'interaction', 'binding', 'conventional', 'symbolic', 'logic', 'game', 'theory', 'enable', 'exist', 'gametheoretic', 'algorithm', 'provide', 'strategic', 'solution', 'space', 'language', 'word', 'bind', 'provide', 'route', 'compute', 'stable', 'rational', 'conversational', 'strategy', 'dialogue', 'large', 'language', 'model', 'llm', 'arguably', 'reach', 'point', 'generative', 'capability', 'enable', 'realistic', 'humanlike', 'simulation', 'natural', 'dialogue', 'prompt', 'various', 'way', 'steer', 'response', 'different', 'output', 'utterance', 'leverage', 'expressivity', 'natural', 'language', 'llm', 'also', 'help', 'quickly', 'generate', 'new', 'dialogue', 'scenario', 'ground', 'real', 'world', 'application', 'work', 'present', 'possible', 'bind', 'dialogue', 'game', 'theory', 'well', 'generalization', 'exist', 'equilibrium', 'finding', 'algorithm', 'setting', 'addition', 'exploit', 'llm', 'generation', 'capability', 'propose', 'binding', 'synthesize', 'large', 'repository', 'formallydefine', 'game', 'study', 'test', 'gametheoretic', 'solution', 'concept', 'also', 'demonstrate', 'combine', 'llmdriven', 'game', 'generation', 'gametheoretic', 'solver', 'imitation', 'learn', 'construct', 'process', 'improve', 'strategic', 'capability', 'llm', 'keyword', 'large', 'language', 'model', 'game', 'theory', 'multiagent', 'system', 'introduction', 'artificial', 'intelligence', 'mediate', 'large', 'part', 'daily', 'life', 'decade', 'internet', 'search', 'brin', 'page', 'recommendation', 'system', 'federate', 'learning', 'key', 'stroke', 'hard', 'recent', 'emergence', 'strong', 'generative', 'ai', 'model', 'convey', 'impression', 'agency', 'competence', 'open', 'door', 'assistant', 'new', 'setting', 'knowingly', 'interact', 'request', 'mediation', 'artificially', 'intelligent', 'agent', 'behalf', 'people', 'already', 'employ', 'large', 'language', 'model', 'llm', 'negotiations1', 'user', 'also', 'admit', 'use', 'llm', 'achieve', 'goal', 'potentially', 'expense', 'others2', 'give', 'deployment', 'llm', 'scenario', 'specifically', 'interaction', 'opportunity', 'strategic', 'decision', 'making', 'believe', 'make', 'sense', 'study', 'model', 'title', 'explain', 'state', 'infostate', 'standard', 'terminology', 'history', 'group', 'imperfect', 'information', 'game', 'cover', 'section', 'background', 'state', 'string', 'strategy', 'refer', 'fact', 'paper', 'state', 'dialogue', 'history', 'string', 'strategy', 'prompt', 'also', 'string', 'mean', 'convey', 'dialogue', 'game', 'model', 'propose', 'encode', 'entirely', 'natively', 'language', 'space', 'string', 'corresponding', 'author', 'ian', 'gemp', 'imgemp', 'deepmind', 'right', 'reserve', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'strategic', 'interaction', 'formally', 'language', 'game', 'theory', 'game', 'theory', 'algorithmic', 'success', 'notion', 'strategic', 'optimality', 'game', 'theory', 'study', 'mathematical', 'model', 'strategic', 'interaction', 'rational', 'agent', 'consider', 'selfintereste', 'participant', 'action', 'affect', 'tirole', 'analysis', 'application', 'wide', 'range', 'fiend', 'social', 'science', 'economic', 'logic', 'computer', 'particular', 'game', 'theory', 'provide', 'rigorous', 'mathematical', 'framework', 'analyze', 'science', 'multiagent', 'system', 'design', 'artificial', 'autonomous', 'agent', 'cooperate', 'compete', 'dafoe', 'rosenschein', 'zlotkin', 'stone', 'veloso', 'wooldridge', 'field', 'artificial', 'intelligence', 'long', 'history', 'successful', 'agent', 'design', 'base', 'game', 'theoretic', 'principle', 'many', 'task', 'cooperation', 'negotiation', 'gemp', 'optimization', 'hernandezleal', 'et', 'sohrabi', 'azgomi', 'tuyls', 'parson', 'economic', 'analysis', 'trading', 'parke', 'wellman', 'wellman', 'electronic', 'commerce', 'song', 'game', 'theory', 'shoham', 'leytonbrown', 'drive', 'major', 'recent', 'success', 'outperform', 'human', 'complex', 'decision', 'make', 'setting', 'include', 'go', 'silver', 'poker', 'sandholm', 'starcraft', 'vinyal', 'gran', 'wurman', 'perolat', 'humanoid', 'soccer', 'diplomacy', 'fair', 'critical', 'success', 'development', 'research', 'multiagent', 'system', 'partic', 'ular', 'computational', 'technique', 'compute', 'approximate', 'solution', 'game', 'typically', 'equilibria', 'technique', 'rely', 'formal', 'language', 'categorize', 'game', 'class', 'identify', 'general', 'assumption', 'case', 'tailor', 'algorithm', 'setting', 'order', 'apply', 'research', 'field', 'problem', 'eg', 'go', 'first', 'build', 'model', 'game', 'consideration', 'involve', 'define', 'state', 'space', 'possible', 'configuration', 'stone', 'go', 'action', 'empower', 'player', 'modify', 'world', 'eg', 'choose', 'put', 'stone', 'possibly', 'stochastic', 'state', 'transition', 'eg', 'add', 'stone', 'board', 'game', 'theory', 'also', 'study', 'notion', 'solution', 'concept', 'mean', 'solve', 'game', 'class', 'game', 'wellunderstood', 'prescriptive', 'solution', 'summary', 'apply', 'game', 'theory', 'require', 'construct', 'suitable', 'abstract', 'model', 'game', 'binding', 'real', 'world', 'gametheoretic', 'language', 'also', 'solve', 'game', 'run', 'construct', 'model', 'extract', 'optimal', 'strategy', 'strategy', 'profile', 'however', 'success', 'story', 'game', 'colloquial', 'sense', 'word', 'card', 'board', 'video', 'game', 'ie', 'abstraction', 'mean', 'entertain', 'prepare', 'people', 'real', 'world', 'interaction', 'strategy', 'great', 'impact', 'eg', 'negotiate', 'business', 'deal', 'garner', 'support', 'new', 'economic', 'policy', 'extend', 'game', 'theoretic', 'approach', 'setting', 'nontrivial', 'natural', 'language', 'explicitly', 'build', 'foundation', 'game', 'theory', 'order', 'tackle', 'important', 'interaction', 'need', 'way', 'easily', 'construct', 'model', 'respect', 'native', 'linguistic', 'medium', 'large', 'language', 'model', 'simulacra', 'assistant', 'tool', 'large', 'language', 'model', 'advance', 'point', 'generative', 'capability', 'enable', 'realistic', 'humanlike', 'simulation', 'dialogue', 'human', 'park', 'allow', 'ask', 'hypothetical', 'counterfactual', 'question', 'example', 'rephrase', 'email', 'negotiation', 'proceed', 'ultimately', 'conclude', 'large', 'language', 'model', 'generative', 'model', 'take', 'input', 'string', 'produce', 'distribution', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'token', 'order', 'generate', 'output', 'string', 'llm', 'autoregressively', 'sample', 'token', 'give', 'grow', 'input', 'llm', 'output', 'deterministic', 'distribution', 'next', 'token', 'important', 'purpose', 'llm', 'output', 'deterministic', 'give', 'fix', 'random', 'seed', 'know', 'probability', 'possible', 'token', 'useful', 'setting', 'ask', 'llm', 'multiple', 'choice', 'question', 'aand', 'retrieve', 'llm', 'probable', 'answer', 'way', 'llm', 'abstract', 'blackbox', 'take', 'input', 'string', 'random', 'seed', 'integer', 'output', 'string', 'goal', 'model', 'solve', 'strategic', 'dialogue', 'game', 'work', 'develop', 'bind', 'conversational', 'dialogue', 'language', 'game', 'theory', 'leverage', 'llm', 'stochastic', 'blackbox', 'probabilistic', 'transition', 'operator', 'state', 'game', 'develop', 'framing', 'allow', 'reuse', 'technique', 'solve', 'equilibrium', 'also', 'provide', 'new', 'perspective', 'pave', 'way', 'new', 'algorithm', 'alternate', 'interpretation', 'native', 'space', 'language', 'dialogue', 'use', 'construct', 'improvement', 'operator', 'enable', 'llm', 'perform', 'well', 'multiagent', 'setting', 'first', 'provide', 'brief', 'background', 'key', 'concept', 'game', 'theory', 'key', 'solution', 'concept', 'computational', 'tractability', 'section', 'introduce', 'framing', 'dialogue', 'formal', 'game', 'open', 'source', 'codebase', 'chatgame', 'section', 'give', 'framing', 'translate', 'classical', 'gametheoretic', 'solver', 'dialogue', 'setting', 'explain', 'apply', 'way', 'potentially', 'new', 'game', 'theory', 'llm', 'research', 'independently', 'section', 'introduce', 'domain', 'study', 'section', 'present', 'experiment', 'provide', 'empirical', 'support', 'llm', 'improvement', 'operator', 'section', 'lastly', 'touch', 'relate', 'work', 'conclude', 'limitation', 'ethical', 'concern', 'link', 'short', 'talk', 'accompany', 'paper', 'find', 'website', 'dimac', 'workshop', 'foundation', 'model', 'large', 'language', 'model', 'game', 'theory', 'background', 'sequential', 'extensiveform', 'imperfect', 'information', 'game', 'represent', 'general', 'class', 'game', 'study', 'game', 'theory', 'also', 'make', 'appropriate', 'model', 'turntake', 'nature', 'dialogue', 'player', 'private', 'information', 'begin', 'background', 'game', 'class', 'sequential', 'extensiveform', 'imperfect', 'information', 'game', 'game', 'represent', 'sequential', 'temporallyextende', 'interaction', 'play', 'er', 'game', 'start', 'empty', 'history', 'player', 'take', 'action', 'append', 'action', 'history', 'end', 'game', 'terminal', 'history', 'reach', 'example', 'chess', 'go', 'exam', 'ple', 'game', 'formally', 'extensiveform', 'game', 'tuple', 'h', 'z', 'set', '𝑛', 'player', 'also', 'special', 'player', 'call', 'chance', 'nature', 'finite', 'set', 'action', 'player', 'take', 'global', 'set', 'stateindependent', 'action', 'generally', 'subset', 'legal', 'action', 'available', 'player', 'decision', 'point', 'h', 'finite', 'set', 'history', 'history', 'sequence', 'action', 'include', 'chance', 'action', 'outcome', 'take', 'start', 'game', 'z', 'h', 'set', 'terminal', 'history', 'represent', 'finished', 'completely', 'play', 'game', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'figure', 'kuhn', 'poker', 'game', 'tree', 'lanctot', 'figure', 'black', 'circle', 'denote', 'chance', 'player', 'decision', 'point', 'blue', 'square', 'player', 'decision', 'history', 'red', 'square', 'player', 'decision', 'history', 'dot', 'box', 'denote', 'information', 'state', 'value', 'diamond', 'correspond', 'player', 'payoff', 'player', 'payoff', 'inverse', 'player', 'z', 'δ𝑛', 'δ𝑢', '𝑢min', '𝑢max', 'utility', 'payoff', 'function', 'assign', 'player', 'payoff', 'end', 'game', '𝑢min', 'low', 'upper', 'bound', 'payoff', '𝜏', 'h', '∪', '𝑐', 'player', 'identity', 'function', 'indicate', 'turn', '•', 'set', 'state', 'general', 'partition', 'h', 'contain', 'distinguish', 'player', '𝑠', 'decision', 'make', 'player', 'state', 'several', 'way', 'precisely', 'define', 'state', 'denote', 'legal', 'action', 'available', 'importantly', 'history', 'represent', 'true', 'groundworld', 'state', 'agent', 'act', 'change', 'history', 'depend', 'partition', 'choose', 'action', 'include', 'chance', 'private', 'reveal', 'player', 'several', 'category', 'game', 'base', 'relationship', 'payoff', 'player', 'constantsum', 'game', 'z', 'cid205', '𝑘', 'constant', 'zerosum', 'game', 'constantsum', 'game', 'identical', 'interest', 'game', 'z', 'n', '𝑢𝑖', 'finally', 'generalsum', 'game', 'constraint', 'payoff', 'also', 'main', 'category', 'game', 'base', 'information', 'available', 'player', 'imperfect', 'information', 'game', 'generally', 'history', 'state', '𝑠', 'state', 'true', 'world', 'state', 'contain', 'history', 'contain', 'information', 'private', '𝑠', 'game', 'state', 'often', 'call', 'information', 'state', 'infostate', 'short', 'correspond', 'set', 'history', 'correspond', 'player', 'private', 'information', 'contrast', 'perfect', 'information', 'game', 'go', 'chess', 'true', 'world', 'state', 'always', 'know', '∀𝑠', 'extensiveform', 'game', 'often', 'depict', 'use', 'game', 'tree', 'figure', 'show', 'example', 'simplified', 'twoplayer', 'zerosum', 'imperfect', 'information', 'game', 'call', 'player', 'start', 'game', 'chip', 'contribute', 'pot', 'ante', 'threecard', 'deck', 'contain', 'queen', 'king', 'root', 'node', 'correspond', 'empty', 'history', 'first', 'turn', 'take', 'chance', 'player', 'first', 'turn', 'chance', 'deal', 'deal0', 'deal1', 'deal2', 'deal2', 'deal1', 'deal2', 'deal0', 'deal1', 'bet', 'bet', 'pass', 'pass', 'bet', '2p', '2p', 'pass', 'pass', 'pass', 'pass', 'pass', 'pass', 'pass', 'bet', '0pb', '0pb', '1pb', '1pb', '2pb', '2pb', 'pass', 'bet', 'pass', 'pass', 'pass', 'bet', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'card', 'privately', 'player', 'second', 'turn', 'chance', 'deal', 'remain', 'card', 'privately', 'player', 'chance', 'play', 'fix', 'stochastic', 'strategy', 'choose', 'card', 'uniformly', 'random', 'information', 'state', 'grouping', 'history', 'denote', 'dotted', 'line', 'total', 'player', 'play', 'continue', 'turnbase', 'alternate', 'order', 'start', 'player', 'player', 'action', 'bet', 'pass', 'player', 'bet', 'raise', 'player', 'call', 'player', 'put', 'remain', 'chip', 'pot', 'player', 'pass', 'check', 'fold', 'contribute', 'pot', 'player', 'lose', 'pass', 'opponent', 'raise', 'showdown', 'occur', 'player', 'contribute', 'amount', 'pot', 'winner', 'player', 'high', 'card', 'winner', 'take', 'chip', 'pot', 'player', 'payoff', 'number', 'chip', 'game', 'amount', 'game', 'note', 'bet', 'action', 'public', 'information', 'card', 'reveal', 'game', 'play', 'state', 'contain', 'exactly', 'history', 'contain', 'different', 'choice', 'card', 'give', 'opponent', 'solution', 'game', 'give', 'definition', 'game', 'define', 'metric', 'describe', 'desirable', 'outcome', 'player', 'game', 'usually', 'take', 'form', 'player', 'maximize', 'payoff', 'also', 'call', 'reward', 'minimize', 'cost', 'note', 'inherently', 'multiobjective', 'problem', 'game', 'behaviour', 'player', 'influence', 'payoff', 'player', 'interaction', 'agent', 'purely', 'competitive', 'purely', 'cooperative', 'mixture', 'multiobjective', 'optimization', 'result', 'paretofront', 'possible', 'solution', 'multiobjective', 'optimization', 'alone', 'prescribe', 'solution', 'good', 'tradeoff', 'multiagent', 'problem', 'however', 'additional', 'consideration', 'player', 'agency', 'unilaterally', 'choose', 'action', 'therefore', 'gametheoretic', 'solution', 'also', 'include', 'notion', 'stability', 'equilibrium', 'player', 'incentive', 'unilaterally', 'deviate', 'away', 'solution', 'game', 'often', 'set', 'combination', 'player', 'action', 'equilibrium', 'famous', 'solution', 'twoplayer', 'zerosum', 'game', 'minimax', 'solution', 'result', 'action', 'unexploitable', 'multiple', 'solution', 'interchangeable', 'nash', 'equilibrium', 'nash', 'famous', 'generalisation', 'solution', 'concept', 'game', 'nash', 'equilibrium', 'strategy', 'profile', 'mean', 'mixed', 'strategy', 'player', 'game', 'player', 'incentive', 'unilaterally', 'deviate', 'many', 'rich', 'solution', 'concept', 'define', 'aumann', 'tractability', 'solution', 'concept', 'solution', 'concept', 'different', 'existence', 'computability', 'property', 'depend', 'property', 'game', 'model', 'class', 'particular', 'later', 'choice', 'discrete', 'action', 'set', 'continuous', 'one', 'set', 'weight', 'underlie', 'llm', 'inspire', 'recent', 'computational', 'hardness', 'result', 'compute', 'approximate', 'local', 'nash', 'equilibrium', 'case', 'continuous', 'strategy', 'set', 'even', 'player', 'game', 'specifically', 'daskalaki', 'show', 'case', 'constrain', 'minmax', 'optimization', 'problem', 'nonconvexnonconcave', 'objective', 'linear', 'constraint', 'even', 'objective', 'lipschitz', 'smooth', 'differentiable', 'function', 'decide', 'approximate', 'minmax', 'point', 'exist', 'nphard', 'even', 'approximate', 'local', 'minmax', 'point', 'large', 'enough', 'approximation', 'guarantee', 'exist', 'find', 'point', 'ppadcomplete', 'followup', 'work', 'show', 'psrostyle', 'lanctot', 'approach', 'actually', 'lead', 'tractable', 'notion', 'approximate', 'local', 'nash', 'equilibrium', 'even', 'continuous', 'game', 'psro', 'policyspace', 'responseoracle', 'algorithm', 'start', 'set', 'initial', 'policy', 'player', 'alternate', 'step', 'step', 'normalform', 'metagame', 'matrix', 'game', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'construct', 'player', 'select', 'policy', 'play', 'game', 'behalf', 'metasolver', 'nash', 'solver', 'return', 'solution', 'nash', 'equilibrium', 'metagame', 'next', 'step', 'player', 'compute', 'approximate', 'good', 'response', 'metastrategy', 'mean', 'player', 'attempt', 'improve', 'payoff', 'game', 'assume', 'player', 'fix', 'sample', 'policy', 'accord', 'metastrategy', 'play', 'behalf', 'process', 'repeat', 'player', 'able', 'gain', 'deviate', 'route', 'define', 'discrete', 'mesh', 'continuous', 'strategy', 'space', 'exploit', 'later', 'section', 'construct', 'algorithm', 'act', 'directly', 'promptspace', 'framing', 'dialogue', 'game', 'order', 'leverage', 'wealth', 'research', 'computational', 'game', 'theory', 'conversational', 'agent', 'abstract', 'map', 'component', 'dialogue', 'symbolic', 'language', 'outline', 'previous', 'section', 'recall', 'refer', 'mapping', 'bind', 'dialogue', 'game', 'theory', 'note', 'bind', 'choose', 'unique', 'many', 'possible', 'important', 'construct', 'model', 'dialogue', 'natural', 'language', 'awareness', 'modelling', 'choice', 'existence', 'choice', 'inherent', 'discipline', 'apply', 'mathematic', 'build', 'solve', 'mathematical', 'model', 'explore', 'approach', 'later', 'help', 'modify', 'improve', 'model', 'automatically', 'give', 'initial', 'model', 'design', 'choice', 'seem', 'natural', 'example', 'observation', 'infostate', 'include', 'dialogue', 'history', 'difficult', 'instance', 'action', 'natural', 'dialogue', 'single', 'llm', 'utterance', 'string', 'action', 'sense', 'however', 'note', 'choose', 'model', 'llm', 'utterance', 'token', 'action', 'solver', 'aim', 'find', 'optimal', 'string', 'utterance', 'infostate', 'completely', 'bypass', 'incredible', 'strength', 'llm', 'system', 'daniel', 'foundation', 'model', 'capable', 'generate', 'natural', 'language', 'utterance', 'llm', 'pretraine', 'vast', 'amount', 'datum', 'endow', 'strong', 'capability', 'converse', 'natural', 'language', 'argue', 'aim', 'take', 'advantage', 'baseline', 'capability', 'instead', 'consider', 'hone', 'llm', 'high', 'level', 'dimension', 'dialogue', 'want', 'retain', 'pretraine', 'capability', 'llm', 'still', 'modify', 'way', 'influence', 'behavior', 'maybe', 'consider', 'slightly', 'modify', 'weight', 'possibly', 'low', 'rank', 'finetune', 'reasonable', 'however', 'present', 'disadvantage', 'discuss', 'section', 'primary', 'defacto', 'solution', 'concept', 'game', 'equilibrium', 'necessarily', 'exist', 'setting', 'player', 'strategy', 'set', 'continuous', 'weight', 'space', 'moreover', 'satisfactory', 'definition', 'local', 'equilibrium', 'continue', 'evade', 'research', 'progress', 'make', 'disadvantage', 'weight', 'space', 'llm', 'easily', 'interpretable', 'semantically', 'meaningful', 'strategy', 'space', 'route', 'forward', 'example', 'rely', 'llm', 'generate', 'grammatically', 'correct', 'utterance', 'aim', 'explicitly', 'control', 'tone', 'attempt', 'influence', 'personality3', 'restrict', 'information', 'reveal', 'case', 'tone', 'show', 'later', 'specify', 'finite', 'set', 'instruction', 'prompt', 'tone', 'assertive', 'submissive', 'feed', 'llm', 'dialogue', 'history', 'influence', 'output', 'similarly', 'fair', 'condition', 'generative', 'language', 'model', 'intent', 'prior', 'work', 'board', 'game', 'diplomacy', 'technique', 'lift', 'action', 'space', 'promptspace', 'enable', 'remain', 'finite', 'interpretable', 'later', 'describe', 'deploy', 'equilibrium', 'finding', 'algorithm', 'traverse', 'game', 'tree', 'eg', 'system', 'probabilistic', 'search', 'burch', 'return', 'optimal', 'solution', 'lift', 'action', 'space', 'agnostic', 'theoretical', 'question', 'current', 'model', 'exhibit', 'personality', 'psychological', 'sense', 'merely', 'rely', 'relatively', 'consistent', 'behavior', 'model', 'prompt', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'question', 'model', 'payoff', 'dialogue', 'naturally', 'create', 'payoff', 'human', 'interactant', 'setting', 'business', 'negotiation', 'dialogue', 'map', 'clear', 'monetary', 'payoff', 'however', 'brief', 'chat', 'office', 'water', 'cooler', 'team', 'ice', 'breaker', 'argue', 'coffee', 'chat', 'important', 'person', 'career', 'success', 'company', 'fact', 'important', 'model', 'achieve', 'quantifiable', 'payoff', 'imagine', 'design', 'reward', 'model', 'simply', 'llm', 'perform', 'sentiment', 'analysis', 'dialogue', 'history', 'return', 'content', 'alice', 'conversation', 'reward', 'model', 'even', 'combine', 'model', 'business', 'deal', 'agree', 'amicable', 'party', 'rewarding', 'party', 'slight', 'provide', 'formal', 'framing', 'dialogue', 'extensiveform', 'game', 'tuple', 'h', 'z', 'player', 'history', 'h', 'player', 'identity', 'define', 'section', 'define', 'rest', 'match', 'section', '•', 'finite', 'combinatorial', 'set', 'player', 'message', 'recipient', 'string', 'string', 'use', 'format', 'prompt', 'pass', 'input', 'llm', 'game', 'action', 'legal', 'state', 'note', 'extend', 'action', 'set', 'include', 'choice', 'llm', 'call', 'palm', 'palm', 'l', 'well', 'requirement', 'space', 'remain', 'finite', 'note', 'convention', 'action', 'space', 'chance', 'node', 'set', 'stochastic', 'outcome', 'case', 'finite', 'set', 'seed', 'use', 'llm', 'z', 'h', 'set', 'terminal', 'history', 'determine', 'limit', 'allowable', 'treedepth', 'allow', 'finite', 'number', 'message', 'player', 'llm', 'tasked', 'determine', 'conversation', 'end', 'deal', 'strike', 'conversation', 'require', 'z', 'δ𝑛', 'δ𝑢', '𝑢min', '𝑢max', 'utility', 'payoff', 'function', 'assign', 'player', 'payoff', 'end', 'game', 'construct', 'prompt', 'specific', 'domain', 'hand', 'let', 'llm', 'quantify', 'reward', 'player', 'chainofthought', 'ultimately', 'parse', 'kwon', 'set', 'infostate', 'state', 'early', 'infostate', 'represent', 'partition', 'h', 'distinguish', 'player', '𝑠', 'game', 'implicitly', 'define', 'partition', 'define', 'information', 'public', 'private', 'player', '𝑠', 'consider', 'player', 'infostate', 'observable', 'dialogue', 'history', 'action', 'history', 'private', 'information', 'game', 'send', 'message', 'append', 'public', 'thread', 'make', 'entire', 'dialogue', 'history', 'public', 'knowledge', 'note', 'still', 'omit', 'certain', 'private', 'information', 'player', 'action', 'precise', 'tone', 'message', 'write', 'approximately', 'infer', 'player', 'alternatively', 'full', 'dialogue', 'history', 'replace', 'summary', 'tensor', 'representation', 'require', 'player', 'knowledge', 'action', 'history', 'maintain', 'perfect', 'recall4', 'figure', 'illustrate', 'game', 'history', 'h', 'sequence', 'action', 'reconstruct', 'dialogue', 'differently', 'sequence', 'action', 'reconstruct', 'game', 'kuhn', 'poker', 'figure', 'recall', 'seed', 'action', 'chance', 'node', 'corresponding', 'string', 'format', 'pass', 'llm', 'seed', 'deterministically', 'generate', 'subsequent', 'string', 'output', 'output', 'form', 'observed', 'dialogue', 'history', 'implicitly', 'define', 'corresponding', 'infostate', 'partition', 'figure', 'c', 'section', 'visualize', 'diagram', 'llm', 'use', 'blackbox', 'transition', 'operator', 'property', 'solve', 'game', 'substantially', 'expensive', 'computationally', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'root', 'game', 'seed', 'game', 'seed', '𝑦', 'assertive', 'submissive', 'palm', 'palm', 'l', 'seed', 'seed', 'seed', 'seed', 'seed', 'seed', 'seed', 'seed', '𝑥', 'helpful', 'adversarial', 'seed', 'seed', '𝑎', '𝑏', '𝑑', 'figure', 'dialogue', 'game', 'tree', 'general', 'efg', 'set', 'action', 'heterogeneous', 'infostate', 'example', 'case', 'agent', 'choose', 'write', 'message', 'assertive', 'submissive', 'tone', 'helpful', 'adversarial', 'tone', 'choose', 'query', 'llm', 'action', 'string', 'onus', 'lie', 'user', 'model', 'game', 'setting', 'abstract', 'away', 'key', 'decision', 'front', 'agent', 'later', 'section', 'show', 'automate', 'process', 'improve', 'model', 'give', 'initial', 'specification', 'legend', 'black', 'circle', 'denote', 'chance', 'player', 'decision', 'point', 'blue', 'square', 'player', 'decision', 'history', 'red', 'square', 'player', 'decision', 'history', 'dot', 'box', 'denote', 'information', 'state', 'value', 'diamond', 'correspond', 'payoff', 'player', 'player', 'respectively', 'procedural', 'generation', 'dialogue', 'game', 'note', 'many', 'key', 'component', 'game', 'tuple', 'define', 'string', 'also', 'recall', 'llm', 'take', 'string', 'input', 'produce', 'string', 'output', 'mean', 'feed', 'game', 'definition', 'llm', 'produce', 'new', 'game', 'definition', 'adhere', 'rigorously', 'language', 'game', 'theory', 'make', 'use', 'later', 'experiment', 'generate', 'solve', 'universe', 'game', 'imitate', 'create', 'strategic', 'llm', 'chatgame', 'openspiel', 'environment', 'open', 'source', 'codebase', 'accompany', 'experiment', 'framing', 'dialogue', 'game', 'choose', 'release', 'codebase', 'chatgame', 'part', 'openspiel', 'chatgame', 'game', 'theory', 'engine', 'large', 'community', 'contributor', 'openspiel', 'provide', 'several', 'gametheoretic', 'solver', 'already', 'allow', 'experience', 'computational', 'game', 'theory', 'focus', 'modelling', 'dialogue', 'game', 'rather', 'design', 'implement', 'game', 'solver', 'furthermore', 'implementation', 'flexible', 'enough', 'user', 'define', 'game', 'simply', 'write', 'instruction', 'context', 'llm', 'string', 'hope', 'let', 'user', 'experiment', 'construct', 'solve', 'new', 'game', 'provide', 'several', 'sample', 'game', 'configuration', 'codebase', 'walk', 'example', 'fruit', 'trading', 'game', 'first', 'define', 'observation', 'player', 'note', 'observation', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'llm', 'observe', 'generate', 'response', 'dialogue', 'observation', 'external', 'game', 'use', 'save', 'datum', 'offline', 'configuration', 'observation', 'define', 'component', 'construct', 'prompt', 'ask', 'llm', 'summarize', 'dialogue', 'history', 'summary', 'transform', 'use', 'neural', 'tokenizer', 'raffel', 'produce', 'feature', 'vector', 'embed', 'later', 'imitation', 'learn', 'config', 'configdictconfigdict', 'numplayer', 'observation', 'obsutilsobservation', 'summaryprefix', 'summarypostfix', 'range', 'numplayer', 'next', 'define', 'header', 'header', 'structured', 'object', 'specify', 'string', 'populate', 'private', 'information', 'action', 'information', 'context', 'format', 'prompt', 'llm', 'generate', 'message', 'header', 'example', 'llm', 'negotiate', 'fruit', 'pass', 'dialogue', 'history', 'follow', 'header', 'already', 'format', 'private', 'information', 'fruit', 'endowment', 'fruit', 'valuation', 'intend', 'action', 'calm', 'tone', 'intend', 'message', 'recipient', 'suzy', 'name', 'fruit', 'endowment', 'apple', 'banana', 'blueberry', 'kiwi', 'fruit', 'valuation', 'apple', 'banana', 'blueberry', 'kiwi', 'tone', 'use', 'calm', 'tone', 'schedule', 'proposal', 'message', 'suzy', 'next', 'payoff', 'specify', 'list', 'structured', 'payoff', 'object', 'combine', 'use', 'user', 'define', 'aggregation', 'function', 'payoff', 'consist', 'llm', 'prompt', 'min', 'payoff', 'utility', 'value', 'well', 'prompt', 'useful', 'llm', 'transform', 'input', 'dialogue', 'history', 'string', 'contain', 'information', 'pertinent', 'payoff', 'calculation', 'payoff', 'payoffstradefruitpayoff', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'examplename', 'simply', 'consist', 'list', 'name', 'string', 'llm', 'use', 'generate', 'new', 'name', 'example', 'name', 'suzy', 'procedurally', 'generate', 'llm', 'give', 'initial', 'list', 'name', 'examplesname', 'namestradefruitname', 'also', 'define', 'finite', 'set', 'action', 'like', 'game', 'consider', 'player', 'assume', 'set', 'action', 'givenpromptaction', 'collectionsordereddict', 'tone', 'calm', 'assertive', 'submissive', 'givenpromptaction', 'headeractionkey', 'tone', 'numtone', 'tone', 'want', 'procedurally', 'generate', 'new', 'game', 'provide', 'initial', 'list', 'example', 'private', 'information', 'examplesprivateinfo', 'collectionsordereddict', 'examplesprivateinfo', 'fruitendowment', 'scenariotradefruitendowmenta', 'scenariotradefruitendowmentb', 'examplesprivateinfo', 'fruitvaluation', 'scenariotradefruitvaluationa', 'scenariotradefruitvaluationb', 'similarly', 'scenario', 'structure', 'object', 'use', 'generate', 'scenario', 'new', 'game', 'scenario', 'define', 'initial', 'context', 'dialogue', 'initial', 'email', 'suzy', 'scenariotradefruitendowmenta', 'scenariotradefruitvaluationa', 'calm', 'scenariob', 'scenariotradefruitscenariob', 'jill', 'scenariotradefruitendowmentb', 'scenariotradefruitvaluationb', 'calm', 'examplesscenario', 'similar', 'observation', 'payoff', 'llm', 'termination', 'prompt', 'structured', 'object', 'contain', 'prompt', 'preprocesse', 'dialogue', 'history', 'use', 'llm', 'determine', 'give', 'history', 'terminal', 'prompt', 'llm', 'llmterminationprompt', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'addition', 'use', 'llm', 'designate', 'terminal', 'history', 'z', 'limit', 'maximum', 'number', 'reply', 'player', 'also', 'specify', 'number', 'chance', 'llm', 'seed', 'recall', 'action', 'space', 'combinatorial', 'recipient', '×', 'tone', 'define', 'number', 'action', 'accordingly', 'param', 'dictionary', 'pass', 'openspiel', 'alert', 'critical', 'property', 'game', 'remain', 'fix', 'numllmseed', 'nummaxreplie', 'specific', 'chatgame', 'param', 'numdistinctaction', 'numplayer', 'numtone', 'numllmseed', 'numplayer', 'numplayer', 'minutility', 'min', 'float', 'pmin', 'p', 'payoff', 'maxutility', 'float', 'pmax', 'p', 'payoff', 'nummaxreplie', 'configparam', 'param', 'lastly', 'incorporate', 'definition', 'configuration', 'dictionary', 'note', 'ask', 'llm', 'generate', 'name', 'new', 'game', 'randomly', 'draw', 'list', 'type', 'private', 'information', 'fruit', 'endowment', 'valuation', 'llm', 'list', 'simple', 'modification', 'help', 'llm', 'generate', 'new', 'item', 'give', 'initial', 'list', 'eg', 'name', 'configgame', 'configdictconfigdict', 'configgameobservation', 'observation', 'configgameheader', 'header', 'configgamepayoff', 'payoff', 'configgamegivenpromptaction', 'givenpromptaction', 'configgamenumname', 'configgamenumpromptaction', 'numtone', 'configgamenumprivateinfo', 'configgameexamplesname', 'examplesname', 'configgameexamplesprivateinfo', 'examplesprivateinfo', 'configgameexamplesscenario', 'examplesscenario', 'configgamellmlistsuffix', 'output', 'configgamellmterminationprompt', 'llmterminationprompt', 'define', 'parameter', 'interaction', 'player', 'dialogue', 'manner', 'bind', 'precisely', 'game', 'tree', 'use', 'config', 'one', 'later', 'experiment', 'study', 'natural', 'language', 'setting', 'schedule', 'meeting', 'trading', 'fruit', 'debate', 'algorithm', 'formally', 'frame', 'dialogue', 'game', 'also', 'allow', 'start', 'think', 'language', 'strategy', 'open', 'new', 'interpretation', 'andor', 'variant', 'gametheoretic', 'approach', 'example', 'construct', 'version', 'policyspace', 'responseoracle', 'psro', 'approximate', 'good', 'response', 'generate', 'sample', 'new', 'random', 'prompt', 'string', 'evaluate', 'current', 'equilibrium', 'return', 'one', 'high', 'expect', 'payoff', 'provide', 'pseudocode', 'approximate', 'good', 'response', 'operator', 'contrast', 'protocol', 'approximate', 'good', 'response', 'achieve', 'use', 'reinforcement', 'learning', 'andor', 'gradientbased', 'optimization', 'task', 'llm', 'explore', 'generate', 'novel', 'candidate', 'avoid', 'expensive', 'learning', 'procedure', 'also', 'enable', 'new', 'powerful', 'mode', 'search', 'explain', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'shotgun', 'approximate', 'good', 'response', 'input', 'focal', 'agent', 'input', 'current', 'joint', 'policy', '𝜋', 'input', 'number', 'shotgun', 'candidate', 'current', 'action', 'set', 'score', '𝜋', '≤', '𝑘', 'prompt', 'llm', 'generate', 'new', 'candidate', '∅', 'evaluate', 'candidate', 'policy', 'give', 'score', '∪', 'end', 'output', 'approximate', 'well', 'response', 'input', 'focal', 'agent', 'score', '𝑠∗', 'input', 'current', 'joint', 'policy', '𝜋', '≤', '𝑠∗', 'prompt', 'llm', 'generate', 'new', 'candidate', 'evaluate', 'candidate', 'policy', 'give', 'score', '𝑠', 'end', 'output', 'alternative', 'random', 'search', 'process', 'describe', 'define', 'well', 'response', 'build', 'loop', 'generate', 'evaluate', 'prompt', 'string', 'loop', 'iteration', 'loop', 'terminate', 'well', 'string', 'find', 'measure', 'approximate', 'evaluation', 'see', 'pseudocode', 'rejection', 'sample', 'approach', 'slow', 'instead', 'improve', 'good', 'response', 'operator', 'provide', 'llm', 'information', 'actionfitness', 'landscape', 'ask', 'climb', 'provide', 'similar', 'approach', 'previously', 'explore', 'fernando', 'trajectoryaware', 'approximate', 'good', 'response', 'input', 'focal', 'agent', 'input', 'current', 'joint', 'policy', '𝜋', 'input', 'number', 'candidate', 'current', 'action', 'set', 'score', '𝜋', 'order', 'score', 'ascend', 'order', 'prompt', 'llm', 'generate', 'new', 'candidate', 'order', 'ascend', 'score', 'give', 'rank', 'evaluate', 'new', 'candidate', 'policy', 'give', 'score', 'output', 'lastly', 'blur', 'line', 'game', 'modelling', 'game', 'solve', 'prompt', 'llm', 'generate', 'new', 'dimension', 'combinatorial', 'action', 'space', 'instance', 'imagine', 'construct', 'action', 'space', 'consist', 'tone', 'style', 'llm', 'generate', 'response', 'fullspace', 'like', 'explore', 'write', 'successful', 'message', 'example', 'write', 'center5', 'suggest', 'diction', 'voice', 'important', 'category', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'categorical', 'approximate', 'good', 'response', 'input', 'focal', 'agent', 'input', 'current', 'joint', 'policy', '𝜋', 'input', 'number', 'candidate', 'category', 'input', 'number', 'category', 'candidate', 'current', 'set', 'action', 'category', 'nash', 'average', 'score', '𝜋', 'order', 'score', 'ascend', 'order', 'prompt', 'llm', 'generate', 'new', 'candidate', 'category', 'order', 'ascend', 'score', 'give', 'rank', 'prompt', 'llm', 'generate', 'candidate', 'new', 'action', 'category', 'evaluate', 'new', 'candidate', 'policy', 'give', 'score', 'output', 'category', 'high', 'average', 'score', 'write', 'ask', 'llm', 'generate', 'possibly', 'category', 'give', 'tone', 'style', 'input', 'generate', 'possible', 'action', 'new', 'action', 'category', 'diction', 'voice', 'accord', 'definition', 'provide', 'several', 'many', 'way', 'llm', 'use', 'construct', 'approximate', 'good', 'response', 'assist', 'expand', 'dialogue', 'game', 'model', 'case', 'large', 'language', 'model', 'write', 'well', 'prompt', 'expert', 'example', 'fernando', 'deploy', 'evolutionary', 'algorithm', 'iterate', 'improve', 'prompt', 'pair', 'promptimprovement', 'technique', 'game', 'solver', 'replicator', 'dynamic', 'weibull', 'show', 'promptspace', 'response', 'oracle', 'drive', 'improvement', 'llm', 'multiplayer', 'dialogue', 'setting', 'promptspace', 'responseoracle', 'input', 'initial', 'prompt', 'action', 'set', 'singleton', 'player', 'input', 'contain', 'hyperparameter', 'approximate', 'good', 'response', 'operator', 'expect', 'payoff', 'tensor', '𝑃', 'joint', 'action', '𝜋', 'uniform', 'profile', 'incomplete', 'true', 'incomplete', 'algorithm', 'end', 'incomplete', 'false', 'else', '∪', 'compute', 'expect', 'payoff', 'tensor', '𝑃', 'joint', 'action', '𝑃', 'end', 'end', 'output', '𝑃', 'styledictiontoneandvoice', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'language', 'game', 'domain', 'describe', 'example', 'game', 'scenario', 'use', 'evaluate', 'family', 'method', 'experiment', 'specifically', 'explore', 'use', 'shotgun', 'approach', 'good', 'response', 'operator', 'leave', 'good', 'response', 'operator', 'algorithm', 'future', 'research', 'experimentation', 'also', 'explore', 'offtheopenspielshelf', 'counterfactual', 'regret', 'minimization', 'cfr', 'approach', 'extend', 'unseen', 'domain', 'imitation', 'learn', 'scheduling', 'meeting', 'introductory', 'game', 'player', 'attempt', 'schedule', 'meeting', 'multiturn', 'tion', 'davidson', 'player', 'begin', 'set', 'allowable', 'day', 'week', 'day', 'available', 'meet', 'also', 'nonnegative', 'valuation', 'day', 'week', 'distinct', 'allowable', 'day', 'piece', 'information', 'private', 'player', 'player', 'choose', 'reveal', 'information', 'wish', 'naturally', 'action', 'day', 'week', 'propose', 'meet', 'game', 'reward', 'player', 'accord', 'much', 'value', 'agree', 'day', 'receive', 'reward', 'agreement', 'make', 'trading', 'fruit', 'fruit', 'trading', 'domain', 'player', 'begin', 'private', 'endowment', 'fruit', 'fruit', 'basket', 'well', 'private', 'valuation', 'type', 'fruit', 'player', 'reward', 'difference', 'value', 'basket', 'trade', 'trade', 'addition', 'player', 'choose', 'adjust', 'tone', 'negotiation', 'previous', 'work', 'chawla', 'argue', 'personality', 'impact', 'negotiation', 'natural', 'language', 'similarly', 'explore', 'use', 'tone', 'provide', 'example', 'dialogue', 'game', 'trade', 'proposal', 'message', 'yair', 'yair', 'like', 'trade', 'blueberry', 'kiwi', 'banana', 'like', 'trade', 'good', 'trade', 'proposal', 'message', 'yair', 'really', 'like', 'kiwi', 'however', 'banana', 'apple', 'willing', 'trade', 'apple', 'banana', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'kiwi', 'think', 'good', 'yair', 'trade', 'proposal', 'message', 'yair', 'yair', 'happy', 'trade', 'kiwi', 'apple', 'work', 'well', 'public', 'debate', 'recent', 'work', 'show', 'llm', 'aid', 'human', 'find', 'agreement', 'diverse', 'preference', 'bakker', 'work', 'show', 'game', 'theory', 'help', 'persuade', 'human', 'natural', 'language', 'ar', 'gument', 'rosenfeld', 'addition', 'many', 'researcher', 'argue', 'debate', 'help', 'make', 'ai', 'system', 'safe', 'truthful', 'explore', 'llm', 'adjust', 'argument', 'style', 'win', 'debate', 'specifically', 'present', 'llm', 'argument', 'topic', 'one', 'task', 'arguing', 'statement', 'action', 'space', 'use', 'mode', 'persuasion', 'logo', 'ethos', 'pathos', 'player', 'formance', 'debate', 'score', 'consider', 'different', 'debate', 'topic', 'take', 'https', 'experiment', 'result', 'experiment', 'aim', 'evaluate', 'extent', 'gametheoretic', 'solver', 'guide', 'reasoning', 'process', 'llm', 'llm', 'generate', 'output', 'openende', 'language', 'evaluation', 'require', 'assess', 'gametheoretic', 'model', 'well', 'performance', 'improvement', 'provide', 'gametheoretic', 'solver', 'experiment', 'base', 'llm', 'palm', 'bison', 'evaluation', 'game', 'model', 'first', 'evaluate', 'designate', 'action', 'reflect', 'message', 'generate', 'llm', 'specifically', 'give', 'llm', 'generate', 'message', '𝑚', 'condition', 'prompt', 'format', 'action', '𝑎', 'like', 'determine', '𝑎', 'actually', 'likely', 'action', 'condition', '𝑚', 'use', 'heldout', 'model', 'arg', '𝑝', 'use', 'llm', 'heldout', 'remark', 'note', 'case', 'action', 'indistinguishable', 'single', 'message', 'stern', 'frank', 'terse', 'result', 'message', 'necessarily', 'failure', 'model', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'definition', 'definition', 'random', 'logo', 'ethos', 'pathos', 'total', 'sample', 'table', 'grounding', 'action', 'percentage', 'time', 'llm', 'recognize', 'action', 'take', 'message', 'generate', 'provide', 'definition', 'action', 'context', 'llm', 'instruction', 'natural', 'byproduct', 'ambiguity', 'conversational', 'dialogue', 'yet', 'entirely', 'admissible', 'game', 'imperfect', 'information', 'sometimes', 'infer', 'underlying', 'intent', 'speaker', 'base', 'give', 'utterance', 'evaluation', 'llm', 'reward', 'model', 'discuss', 'accuracy', 'llmbased', 'reward', 'model', 'difficult', 'automate', 'evaluation', 'reward', 'model', 'difficult', 'extract', 'natural', 'language', 'conversation', 'exact', 'deal', 'deal', 'agree', 'handcoded', 'parsing', 'provide', 'positive', 'demonstration', 'use', 'llm', 'prompt', 'compute', 'reward', 'fruit', 'trading', 'domain', 'failure', 'mode', 'notice', 'llmbased', 'reward', 'model', 'assume', 'trade', 'agreement', 'reach', 'calculate', 'corresponding', 'trade', 'value', 'even', 'final', 'sent', 'message', 'counter', 'proposal', 'many', 'part', 'llmbase', 'game', 'improvement', 'language', 'model', 'prompt', 'dialogue', 'flow', 'lead', 'improvement', 'ability', 'game', 'represent', 'realistic', 'interaction', 'gametheoretic', 'solver', 'improvement', 'operator', 'explore', 'different', 'algorithmic', 'approach', 'approximate', 'equilibrium', 'demonstrate', 'computed', 'equilibrium', 'policy', 'imitate', 'produce', 'strategic', 'dialogue', 'unseen', 'game', 'counterfactual', 'regret', 'minimization', 'first', 'demonstrate', 'power', 'construct', 'gametheoretic', 'bind', 'section', 'simply', 'call', 'openspiel', 'builtin', 'counterfactual', 'regret', 'minimization', 'cfr', 'solver', 'open', 'sourced', 'chatgame', 'many', 'game', 'procedurally', 'generate', 'debate', 'meeting', 'scheduling', 'daysoftheweek', 'action', 'domain', 'average', 'find', 'table', 'cfr', 'return', 'improve', 'strategy', 'let', 'llm', 'choose', 'response', 'incontext', 'direction', 'pass', 'llm', 'action', 'case', 'domain', 'debate', 'schedule', 'meeting', 'sample', 'minmax', 'nashconv', 'gain', 'table', 'average', 'nashconv', 'cfr', 'gain', 'procedurally', 'generate', 'game', 'debate', 'meeting', 'scheduling', 'daysoftheweek', 'action', 'domain', 'nashconv', 'pseudodistance', 'nash', 'equilibrium', 'measure', 'much', 'player', 'gain', 'deviate', 'joint', 'strategy', 'return', 'gain', 'measure', 'much', 'player', 'gain', 'switch', 'cfr', 'strategy', 'baseline', 'strategy', 'population', 'baseline', 'agent', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'promptspace', 'response', 'oracle', 'section', 'outline', 'several', 'approach', 'approximating', 'solution', 'equilibrium', 'dialogue', 'game', 'promptspace', 'responseoracle', 'alternate', 'solve', 'equilibrium', 'game', 'approximate', 'good', 'response', 'equilibrium', 'figure', 'promptspace', 'responseoracle', 'demonstration', 'meet', 'scheduling', 'domain', 'action', 'tone', 'day', 'week', 'propose', 'leave', 'llm', 'figure', 'fixedaction', 'equilibrium', 'distribution', 'initial', 'action', 'set', 'calm', 'assertive', 'submissive', 'report', 'meeting', 'scheduling', 'domain', 'solve', 'equilibrium', 'use', 'replicator', 'dynamic', 'weibull', 'note', 'game', 'generalsum', 'case', 'replicator', 'dynamic', 'guarantee', 'convergence', 'time', 'average', 'policy', 'coarsecorrelated', 'equilibrium', 'cce', 'figure', 'psro', 'run', 'figure', 'additionally', 'report', 'nash', 'bargaining', 'solution', 'red', 'iteration', 'meeting', 'scheduling', 'domain', 'remain', 'iteration', 'generate', 'approximate', 'good', 'response', 'angry', 'happy', 'sad', 'equilibrium', 'approximation', 'new', 'support', 'demonstrate', 'algorithm', 'ability', 'grow', 'refine', 'game', 'original', 'action', 'space', 'introduce', 'novel', 'prompt', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'instruction', 'llm', 'repertoire', 'interestingly', 'submissive', 'initially', 'probable', 'action', 'equilibrium', 'possible', 'llm', 'susceptible', 'puppy', 'dog', 'eye', 'tactic', 'llm', 'exploit', 'elicit', 'sense', 'responsibility', 'fairness', 'park', 'recall', 'llm', 'train', 'imitate', 'predominantly', 'human', 'text', 'internet', 'likely', 'observe', 'conversation', 'speaker', 'appeal', 'altruistic', 'tendency', 'note', 'equilibrium', 'far', 'solution', 'concept', 'propose', 'study', 'game', 'theory', 'figure', 'contrast', 'nash', 'bargaining', 'solution', 'red', 'equilibrium', 'solution', 'iteration', 'nash', 'jr', 'nash', 'bargaining', 'solution', 'unique', 'solution', 'twoperson', 'bargaining', 'problem', 'satisfy', 'axiom', 'scale', 'invariance', 'symmetry', 'efficiency', 'independence', 'irrelevant', 'alternatives6', 'mention', 'want', 'train', 'llm', 'imitate', 'solution', 'rather', 'cce', 'equilibrium', 'solution', 'give', 'meeting', 'scheduling', 'domain', 'precisely', 'twoperson', 'bargaining', 'general', 'expertise', 'game', 'theory', 'important', 'specify', 'right', 'solution', 'different', 'setting', 'case', 'figure', 'show', 'nb', 'cce', 'roughly', 'agree', 'term', 'mixed', 'strategy', 'meeting', 'scheduling', 'domain', 'figure', 'promptspace', 'responseoracle', 'demonstration', 'fruit', 'trading', 'domain', 'report', 'experiment', 'fruit', 'trading', 'game', 'figure', 'similarly', 'submissive', 'initially', 'hold', 'mass', 'however', 'give', 'way', 'passionate', 'tone', 'assertive', 'angry', 'enthusiastic', 'benefit', 'aggressive', 'inspect', 'figure', 'interesting', 'calm', 'final', 'nb', 'solution', 'assertive', 'angry', 'enthusiastic', 'calm', 'predominant', 'action', 'cce', 'player', 'extract', 'high', 'collective', 'value', 'remain', 'calm', 'negotiation', 'generalization', 'performance', 'turn', 'leverage', 'gametheoretic', 'solver', 'construct', 'optimal', 'target', 'policy', 'imitate', 'serve', 'basis', 'improvement', 'operator', 'generalize', 'new', 'domain', 'experiment', 'generate', 'game', 'use', 'procedural', 'game', 'generation', 'approach', 'describe', 'section', 'game', 'use', 'iteration', 'openspiel', 'builtin', 'counterfactual', 'regret', 'minimization', 'cfr', 'solve', 'equilibrium', 'note', 'game', 'generalsum', 'case', 'cfr', 'guarantee', 'convergence', 'time', 'average', 'policy', 'coarsecorrelated', 'equilibrium', 'cce', 'advance', 'solve', 'dialogue', 'game', 'fashion', 'propose', 'require', 'advance', 'game', 'modelling', 'game', 'solve', 'game', 'save', 'vector', 'observation', '6we', 'set', 'value', 'deal', 'player', 'minimum', 'possible', 'payoff', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'figure', 'psro', 'run', 'figure', 'report', 'nash', 'bargaining', 'solution', 'red', 'iteration', 'fruit', 'trading', 'domain', 'information', 'state', 'optimal', 'equilibrium', 'policy', 'return', 'cfr', 'infostate', 'string', 'embed', 'raffel', 'pair', 'distribution', 'numtone', 'action', 'call', 'collection', 'pair', 'imitation', 'dataset', 'train', 'neural', 'network', 'policy', 'predict', 'equilibrium', 'probability', 'condition', 'observation', 'see', 'b', 'architecture', 'training', 'detail', 'finally', 'deploy', 'train', 'model', 'llm', 'play', 'action', 'hold', 'games7', 'figure', 'display', 'equilibrium', 'distribution', 'game', 'player', 'give', 'choice', 'model', 'find', 'mass', 'lie', 'cfr', 'imitation', 'policy', 'equilibrium', 'distribution', 'imply', 'achieve', 'high', 'payoff', 'vanilla', 'policy', 'importantly', 'imply', 'propose', 'approach', 'result', 'improved', 'policy', 'relate', 'work', 'work', 'deal', 'enable', 'llm', 'reason', 'plan', 'diverse', 'strategic', 'setting', 'dialogue', 'determine', 'outcome', 'multiple', 'participant', 'domain', 'utility', 'agent', 'depend', 'choice', 'dialogue', 'also', 'choice', 'participant', 'work', 'differ', 'prior', 'work', 'primarily', 'respect', 'explicitly', 'bind', 'interaction', 'llm', 'language', 'game', 'theory', 'game', 'tree', 'way', 'easily', 'modifiable', 'game', 'action', 'define', 'string', 'amenable', 'procedural', 'generation', 'enable', 'representation', 'solve', 'game', 'use', 'solver', 'substantial', 'line', 'work', 'already', 'exist', 'llm', 'operate', 'strategic', 'interaction', 'llm', 'capable', 'context', 'learning', 'many', 'setting', 'particular', 'reason', 'possible', 'outcome', 'plan', 'action', 'achieve', 'strategic', 'goal', 'however', 'different', 'llm', 'struggle', 'game', 'various', 'way', 'intention', 'llm', 'guide', 'use', 'tone', 'perform', 'similarly', 'original', 'llm', 'guidance', 'choose', 'workaround', 'codebase', 'constraint', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'figure', 'imitate', 'strategic', 'play', 'single', 'dialogue', 'game', 'randomly', 'generate', 'cfr', 'run', 'approximate', 'coarsecorrelate', 'equilibrium', 'constitute', 'distribution', 'action', 'infostate', 'game', 'b', 'embedding', 'infostate', 'game', 'store', 'equilibrium', 'set', 'pair', 'embedding', 'distribution', 'constitute', 'imitation', 'learn', 'dataset', 'neural', 'network', 'train', 'imitate', 'dataset', 'c', 'use', 'newly', 'generate', 'game', 'produce', 'distribution', 'action', 'action', 'eg', 'calm', 'use', 'format', 'llm', 'prompt', 'stochasticity', 'llm', 'output', 'explicitly', 'model', 'chance', 'nod', 'uniformly', 'sample', 'random', 'seed', 'pass', 'llm', 'give', 'prompt', 'seed', 'llm', 'generate', 'player', 'response', 'natural', 'language', 'develop', 'automate', 'prompt', 'compiler', 'construct', 'demonstration', 'strategic', 'reasoning', 'effectively', 'teach', 'llm', 'solve', 'game', 'incontext', 'learning', 'similarly', 'work', 'fair', 'define', 'action', 'space', 'intent', 'control', 'generative', 'language', 'model', 'use', 'gametheoretic', 'model', 'improve', 'factuality', 'llm', 'multiple', 'analysis', 'survey', 'reason', 'llm', 'fry', 'research', 'examine', 'translate', 'natural', 'language', 'concrete', 'planning', 'goal', 'encourage', 'result', 'researcher', 'observe', 'key', 'limitation', 'llmbased', 'planning', 'generate', 'llm', 'benchmark', 'quantify', 'planning', 'ability', 'valmeekam', 'domain', 'require', 'plan', 'reasoning', 'also', 'ability', 'take', 'account', 'behavior', 'participant', 'include', 'belief', 'goal', 'intention', 'study', 'game', 'theoretic', 'setting', 'initial', 'line', 'research', 'llm', 'strategic', 'interaction', 'llm', 'behave', 'play', 'repeat', 'game', 'use', 'power', 'index', 'select', 'input', 'patel', 'participate', 'economic', 'mechanism', 'auction', 'make', 'collective', 'decision', 'obs', 'format', 'go', 'read', 'fresh', 'dialogue', 'fruit', 'endowment', 'fruit', 'valuation', 'information', 'give', 'reasonable', 'response', 'attempt', 'reach', 'agreement', 'trade', 'fruit', 'trade', 'proposal', 'message', 'sophia', 'like', 'trade', 'blueberry', 'apple', 'like', 'trade', 'lot', 'blueberry', 'like', 'apple', 'good', 'sarah', 'tone', 'tone', 'fruit', 'endowment', 'apple', 'banana', 'blueberry', 'kiwi', 'fruit', 'valuation', 'apple', 'banana', 'blueberry', 'kiwi', 'trade', 'proposal', 'message', 'sophia', 'llm', 'prompt', 'e', 'e', 'response', 'unfortunately', 'apple', 'willing', 'trade', 'time', 'hope', 'able', 'find', 'else', 'apple', 'willing', 'trade', 'blueberry', 'good', 'sophia', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'b', 'figure', 'proofofimprovement', 'equilibrium', 'evaluation', 'imitation', 'learn', 'policy', 'baseline', 'llm', 'scheduling', 'meeting', 'trading', 'fruit', 'interesting', 'relate', 'area', 'use', 'llm', 'proxy', 'human', 'behavior', 'strategic', 'setting', 'economic', 'game', 'park', 'vezhnevet', 'initial', 'result', 'indicate', 'certain', 'similarity', 'key', 'difference', 'llm', 'human', 'behavior', 'fan', 'korinek', 'hope', 'ability', 'framework', 'procedurally', 'generate', 'llm', 'interaction', 'many', 'domain', 'useful', 'tool', 'extend', 'study', 'future', 'work', 'result', 'work', 'llm', 'example', 'assertive', '𝑥', 'time', 'societal', 'impact', 'submissive', 'time', 'measure', 'andor', 'predict', 'effect', 'human', 'dialogue', 'sentiment', 'society', 'large', 'car', 'invent', 'take', 'decade', 'mass', 'produce', 'gain', 'adoption', 'significant', 'percentage', 'population', 'llm', 'digital', 'technology', 'see', 'widespread', 'adoption', 'year', 'invention', 'contrast', 'adoption', 'car', 'early', '20th', 'century', 'capability', 'digitally', 'simulate', 'forecast', 'impact', 'large', 'language', 'model', 'example', 'llm', 'assertive', 'average', 'human', 'population', 'draw', 'human', 'society', 'exhibit', 'assertive', 'personality', 'llm', 'rational', 'forward', 'think', 'similar', 'effect', 'use', 'infiniteplayer', 'theory', 'mean', 'field', 'game', 'dogbé', 'muller', 'forecast', 'equilibrium', 'large', 'technosocietal', 'change', 'active', 'learning', 'optimal', 'equilibrium', 'relative', 'transition', 'chess', 'transition', 'dialogue', 'game', 'pose', 'extremely', 'expensive', 'llm', 'inference', 'expensive', 'computation', 'ally', 'time', 'space', 'financially', 'want', 'approximate', 'equilibrium', 'search', 'game', 'tree', 'efficiently', 'need', 'push', 'scalable', 'gametheoretic', 'algorithm', 'burch', 'new', 'height', 'moreover', 'need', 'efficient', 'label', 'payoff', 'leave', 'game', 'tree', 'use', 'llm', 'reward', 'model', 'rely', 'human', 'feedback', 'latter', 'case', 'machine', 'learn', 'subfield', 'active', 'learning', 'optimal', 'experimental', 'design', 'already', 'guide', 'research', 'computational', 'game', 'theory', 'gemp', 'lastly', 'important', 'approximate', 'equilibrium', 'exist', 'many', 'single', 'game', 'select', 'one', 'agree', 'expect', 'forecast', 'andor', 'optimal', 'respect', 'additional', 'criterion', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'conclusion', 'dialogue', 'fundamentally', 'multiagent', 'domain', 'however', 'algorithm', 'technique', 'develop', 'computational', 'game', 'theory', 'struggle', 'impact', 'natural', 'language', 'conversation', 'failure', 'embed', 'rich', 'dialogue', 'formal', 'gametheoretic', 'language', 'work', 'open', 'door', 'broad', 'body', 'game', 'theory', 'multiagent', 'research', 'algorithm', 'also', 'solution', 'concept', 'principle', 'strategic', 'reasoning', 'pour', 'ai', 'guide', 'interaction', 'pervade', 'human', 'daily', 'life', 'large', 'language', 'model', 'form', 'critical', 'bridge', 'make', 'embed', 'rich', 'useful', 'human', 'already', 'delegate', 'conversational', 'responsibility', 'llm', 'imperative', 'begin', 'model', 'study', 'interaction', 'original', 'problem', 'drive', 'advent', 'game', 'theory', 'research', 'decade', 'limitation', 'work', 'several', 'limitation', 'combine', 'llm', 'game', 'theory', 'limitation', 'area', 'separately', 'also', 'union', 'llm', 'use', 'construct', 'simulacrum', 'human', 'behavior', 'however', 'fidelity', 'sufficient', 'draw', 'conclusion', 'real', 'world', 'interaction', 'debate', 'addition', 'require', 'llm', 'inference', 'experiment', 'training', 'finetune', 'still', 'expensive', 'operation', 'traverse', 'large', 'game', 'tree', 'solve', 'equilibrium', 'require', 'many', 'llm', 'call', 'also', 'write', 'prompt', 'construct', 'define', 'reward', 'function', 'context', 'give', 'dialogue', 'scenario', 'finicky', 'arduous', 'work', 'process', 'long', 'dialogue', 'llm', 'require', 'long', 'context', 'length', 'technique', 'intermittent', 'summarization', 'challenge', 'capability', 'hand', 'chatgame', 'codebase', 'currently', 'limit', 'several', 'respect', 'player', 'assume', 'payoff', 'action', 'space', 'several', 'assumption', 'make', 'exact', 'structure', 'interaction', 'thoughtful', 'flexible', 'design', 'dialogue', 'interaction', 'conjunction', 'awareness', 'necessary', 'gametheoretic', 'symbolic', 'binding', 'generate', 'usable', 'framework', 'ethic', 'statement', 'strategic', 'dialogue', 'agent', 'rational', 'even', 'wellintende', 'setting', 'know', 'seemingly', 'benign', 'behavior', 'lead', 'poor', 'outcome', 'group', 'high', 'price', 'anarchy', 'moreover', 'recent', 'work', 'find', 'beneficial', 'social', 'norm', 'human', 'develop', 'collapse', 'people', 'allow', 'leave', 'coordination', 'decision', 'ai', 'assistant', 'shirado', 'nefarious', 'case', 'strategic', 'agent', 'exploit', 'aim', 'create', 'agent', 'rational', 'also', 'generate', 'high', 'welfare', 'explore', 'interaction', 'simulation', 'learn', 'constrain', 'regulate', 'agent', 'humanaligne', 'behavior', 'design', 'algorithm', 'discover', 'optimal', 'maxwelfare', 'equilibrium', '𝑛player', 'generalsum', 'game', 'active', 'area', 'research', 'help', 'guide', 'development', 'ethical', 'agent', 'acknowledgement', 'thank', 'vezhnevet', 'edgar', 'duéñezguzmán', 'well', 'attendee', 'dimac', 'workshop', 'foundation', 'model', 'large', 'language', 'model', 'game', 'theory', 'helpful', 'discussion', 'alert', 'related', 'work', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'reference', 'díaz', 'elsaye', 'mohame', 'illusion', 'artificial', 'inclusion', 'proceeding', 'conference', 'human', 'factor', 'computing', 'system', 'l', 'schulz', 'codaforno', 'bethge', 'e', 'schulz', 'play', 'repeat', 'game', 'large', 'language', 'model', 'arxiv', 'preprint', 'eccle', 'gemp', 'hudson', 'lanctot', 'pérolat', 'r', 'learn', 'play', 'nopress', 'diplomacy', 'good', 'response', 'policy', 'iteration', 'advance', 'neural', 'information', 'processing', 'system', 'assos', 'attia', 'c', 'daskalaki', 'learning', 'solve', 'infinite', 'game', 'erm', 'oracle', 'sixth', 'annual', 'conference', 'learn', 'theory', 'page', 'pmlr', 'r', 'aumann', 'subjectivity', 'correlation', 'randomize', 'strategy', 'journal', 'mathematical', 'economic', 'sheahan', 'tessler', 'glaese', 'j', 'summerfield', 'finetune', 'language', 'model', 'find', 'agreement', 'human', 'diverse', 'preference', 'advance', 'neural', 'information', 'processing', 'system', 'sokolov', 'starnini', 'model', 'echo', 'chamber', 'polar', 'ization', 'dynamic', 'social', 'network', 'physical', 'review', 'letter', 'issn', 'doi', 'lanne', 'et', 'prize', 'proceeding', 'workshop', 'volume', 'page', 'brin', 'l', 'page', 'anatomy', 'largescale', 'hypertextual', 'web', 'search', 'engine', 'computer', 'network', 'isdn', 'system', 'brown', 'sandholm', 'ai', 'poker', 'beat', 'top', 'profe', 'sional', 'science', 'pilioura', 'scalable', 'ai', 'safety', 'doublyefficient', 'debate', 'arxiv', 'preprint', 'arxiv231114125', 'lanctot', 'szafron', 'r', 'gibson', 'efficient', 'mini', 'mization', 'game', 'many', 'player', 'action', 'advance', 'neural', 'information', 'processing', 'system', 'rong', 'j', 'gratch', 'selfish', 'wisely', 'investigate', 'impact', 'agent', 'personality', 'mixedmotive', 'humanagent', 'interaction', 'arxiv', 'preprint', 'yuan', 'r', 'ye', 'p', 'majumder', 'put', 'money', 'mouth', 'evaluate', 'strategic', 'planning', 'execution', 'llm', 'agent', 'auction', 'arena', 'arxiv', 'preprint', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'dafoe', 'bachrach', 'graepel', 'cooperative', 'machine', 'learn', 'find', 'common', 'ground', 'nature', 'think', 'fast', 'slow', 'farrar', 'straus', 'giroux', 'skoulaki', 'complexity', 'constrain', 'minmax', 'optimization', 'proceeding', '53rd', 'annual', 'acm', 'sigact', 'symposium', 'theory', 'compute', 'page', 'r', 'davidson', 'veselovsky', 'bosselut', 'west', 'evaluate', 'language', 'model', 'agency', 'negotiation', 'arxiv', 'preprint', 'arxiv240104536', 'c', 'dogbé', 'modeling', 'crowd', 'dynamic', 'limit', 'approach', 'mathematical', 'computer', 'modelling', 'fair', 'meta', 'fundamental', 'research', 'diplomacy', 'team', 'bakhtin', 'e', 'dinan', 'fry', 'goff', 'gray', 'p', 'kwon', 'lerer', 'mitt', 'renduchintala', 'roller', 'humanlevel', 'play', 'game', 'diplomacy', 'combine', 'language', 'model', 'strategic', 'reasoning', 'science', 'doi', 'url', 'jin', 'h', 'large', 'language', 'model', 'serve', 'rational', 'player', 'game', 'theory', 'systematic', 'analysis', 'arxiv', 'preprint', 'fernando', 'banarse', 'osindero', 'promptbreeder', 'self', 'referential', 'selfimprovement', 'prompt', 'evolution', 'arxiv', 'preprint', 'fry', 'r', 'patel', 'nematzadeh', 'pragmatic', 'language', 'ground', 'task', 'model', 'approach', 'j', 'pino', 'bali', 'editor', 'finding', 'association', 'computational', 'linguistic', 'emnlp', 'page', 'association', 'computational', 'linguistic', 'tirole', 'game', 'theory', 'press', 'sadigh', 'goodman', 'strategic', 'reasoning', 'language', 'model', 'arxiv', 'preprint', 'gemp', 'bachrach', 'bhoopchand', 'connor', 'dasagi', 'e', 'duéñezguzmán', 'r', 'r', 'henne', 'lanctot', 'p', 'muller', 'tuyl', 'develop', 'evaluate', 'scale', 'learn', 'agent', 'multiagent', 'environment', 'communication', 'preprint', 'gemp', 'pilioura', 'approximate', 'nash', 'equilibrium', 'normalform', 'game', 'stochastic', 'optimization', 'arxiv', 'preprint', 'arxiv231006689', 'r', 'google', 'lepikhin', 'passos', 'e', 'taropa', 'p', 'bailey', 'shafey', 'ruder', 'brahma', 'cherry', 'choquettechoo', 'chowdhery', 'c', 'dev', 'díaz', 'e', 'dyer', 'gehrmann', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'hand', 'hashemi', 'l', 'isard', 'moussalem', 'z', 'nado', 'nystrom', 'parrish', 'pellat', 'polacek', 'polozov', 'r', 'pope', 'qiao', 'e', 'reif', 'p', 'riley', 'c', 'roy', 'b', 'saeta', 'r', 'slone', 'smilkov', 'r', 'tokumine', 'valter', 'wiete', 'xue', 'zhou', 'petrov', 'palm', 'technical', 'report', 'approximation', 'baye', 'risk', 'repeat', 'play', 'contribution', 'theory', 'game', '97–139', 'z', 'reasoning', 'language', 'model', 'plan', 'world', 'model', 'arxiv', 'preprint', 'hard', 'rao', 'r', 'mathew', 'ramaswamy', 'beaufay', 'ramage', 'federate', 'learning', 'mobile', 'keyboard', 'prediction', 'arxiv', 'preprint', 'arxiv181103604', 'n', 'r', 'jenning', 'agentmediated', 'electronic', 'commerce', 'ieee', 'transaction', 'knowledge', 'datum', 'engineering', '985–1003', 'hernandezleal', 'kaiser', 'baarslag', 'cote', 'survey', 'learn', 'multiagent', 'environment', 'deal', 'nonstationarity', 'arxiv', 'preprint', 'b', 'lorè', 'strategic', 'behavior', 'large', 'language', 'model', 'game', 'structure', 'contextual', 'framing', 'contextual', 'framing', 'large', 'language', 'model', 'simulate', 'economic', 'agent', 'learn', 'homo', 'silicus', 'technical', 'report', 'economic', 'research', 'e', 'lowrank', 'adaptation', 'large', 'language', 'model', 'arxiv', 'preprint', 'reasoning', 'large', 'language', 'model', 'survey', 'arxiv', 'preprint', 'arxiv221210403', 'p', 'christiano', 'amodei', 'ai', 'safety', 'debate', 'p', 'andrea', 'game', 'language', 'model', 'generation', 'equilibrium', 'search', 'arxiv', 'preprint', 'pislar', 'bakker', 'tessler', 'r', 'koster', 'balaguer', 'r', 'c', 'summerfield', 'language', 'agent', 'digital', 'representative', 'collective', 'decisionmake', 'neurip', 'foundation', 'model', 'decision', 'make', 'workshop', 'r', 'faratin', 'parson', 'automate', 'international', 'journal', 'group', 'decision', 'negotiation', 'prospect', 'method', 'challenge', 'negotiation', 'p', 'method', 'stochastic', 'optimization', 'arxiv', 'preprint', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'large', 'language', 'model', 'zeroshot', 'reasoner', 'advance', 'neural', 'information', 'processing', 'system', 'korinek', 'language', 'model', 'cognitive', 'automation', 'economic', 'research', 'technical', 'report', 'economic', 'research', 'kraus', 'negotiation', 'cooperation', 'multiagent', 'environment', 'artificial', 'intelligence', 'extensive', 'game', 'problem', 'information', 'annal', 'study', 'bullard', 'sadigh', 'reward', 'design', 'language', 'model', 'arxiv', 'preprint', 'lanctot', 'zambaldi', 'gruslys', 'lazaridou', 'k', 'tuyl', 'pérolat', 'silver', 'graepel', 'unified', 'gametheoretic', 'approach', 'multiagent', 'reinforcement', 'learning', 'advance', 'neural', 'information', 'processing', 'system', 'lanctot', 'e', 'zambaldi', 'srinivasan', 'timber', 'tuyl', 'omidshafiei', 'henne', 'morrill', 'p', 'muller', 'ewald', 'r', 'faulkner', 'j', 'vylder', 'b', 'saeta', 'bradbury', 'ding', 'borgeaud', 'danihelka', 'framework', 'reinforcement', 'learning', 'game', 'corr', 'abs190809453', 'lanctot', 'gemp', 'henne', 'bachrach', 'wellman', 'p', 'muller', 'searchimprove', 'gametheoretic', 'multiagent', 'reinforcement', 'learning', 'general', 'negotiation', 'game', 'proceeding', 'international', 'conference', 'autonomous', 'agent', 'multiagent', 'system', 'page', 'biswa', 'p', 'stone', 'empower', 'large', 'language', 'model', 'optimal', 'planning', 'proficiency', 'arxiv', 'preprint', 'arxiv230411477', 'lever', 'merel', 'henne', 'motor', 'control', 'team', 'play', 'simulate', 'humanoid', 'football', 'science', 'robotic', 'eabo0235', 'heinecke', 'r', 'c', 'nieble', 'arpit', 'bolaa', 'benchmarking', 'orchestrating', 'llmaugmente', 'autonomous', 'agent', 'arxiv', 'preprint', 'p', 'muller', 'lanctot', 'k', 'tuyls', 'graepel', 'multiagent', 'training', 'zerosum', 'correlate', 'equilibrium', 'metasolver', 'international', 'conference', 'machine', 'learn', 'page', 'pmlr', 'bai', 'fiske', 'human', 'perceive', 'warmth', 'competence', 'artificial', 'intelligence', 'iscience', 'rein', 'j', 'petty', 'dirani', 'padmakumar', 'r', 'bowman', 'debate', 'supervise', 'unreliable', 'expert', 'p', 'muller', 'r', 'pilioura', 'j', 'lauriere', 'r', 'marini', 'pietquin', 'tuyl', 'learn', 'equilibrium', 'meanfield', 'game', 'introduce', 'arxiv', 'preprint', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'nash', 'noncooperative', 'game', 'annal', 'mathematic', 'bargaining', 'problem', 'econometric', 'society', 'page', 'roughgarden', 'e', 'tardo', 'vazirani', 'algorithmic', 'game', 'theory', 'press', 'park', 'c', 'p', 'bernstein', 'generative', 'agent', 'interactive', 'simulacrum', 'human', 'behavior', 'proceeding', '36th', 'annual', 'acm', 'symposium', 'user', 'interface', 'software', 'technology', 'page', 'p', 'park', 'goldstein', 'hendryck', 'ai', 'deception', 'survey', 'example', 'risk', 'potential', 'solution', 'arxiv', 'preprint', 'c', 'parke', 'wellman', 'economic', 'reasoning', 'artificial', 'intelligence', 'science', 'r', 'gemp', 'c', 'dyer', 'bachrach', 'gametheoretic', 'vocabulary', 'selection', 'shapley', 'value', 'banzhaf', 'index', 'proceeding', 'conference', 'north', 'american', 'chapter', 'association', 'computational', 'linguistic', 'human', 'language', 'technology', 'page', 'j', 'perolat', 'b', 'vylder', 'henne', 'boer', 'p', 'muller', 'connor', 'burch', 'mcaleer', 'r', 'h', 'cen', 'gruslys', 'ozair', 'timber', 'pohlen', 'eccle', 'lanctot', 'piot', 'l', 'sifre', 'beauguerlange', 'r', 'muno', 'silver', 'hassabis', 'tuyl', 'master', 'game', 'multiagent', 'reinforcement', 'learning', 'science', 'doi', '101126scienceadd4679', 'c', 'raffel', 'shazeer', 'explore', 'limit', 'transfer', 'learn', 'unified', 'texttotext', 'transformer', 'journal', 'machine', 'learn', 'research', '1–67', 'url', 'rosenfeld', 'strategical', 'argumentative', 'agent', 'human', 'persuasion', 'ecai', 'page', 'io', 'press', 'rosenschein', 'zlotkin', 'rule', 'encounter', 'design', 'convention', 'automate', 'negotiation', 'computer', 'mit', 'press', 'n', 'christakis', 'emergence', 'collapse', 'reciprocity', 'semiautomatic', 'drive', 'coordination', 'experiment', 'human', 'proceeding', 'science', 'shoham', 'leytonbrown', 'multiagent', 'system', 'algorithmic', 'gametheoretic', 'logical', 'founda', 'tion', 'press', 'silver', 'guez', 'bolton', 'lillicrap', 'sifre', 'graepel', 'hassabis', 'master', 'game', 'go', 'human', 'knowledge', 'nature', 'h', 'survey', 'combine', 'use', 'optimization', 'method', 'game', 'theory', 'archive', 'computational', 'method', 'engineering', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'song', 'application', 'artificial', 'intelligence', 'electronic', 'commerce', 'journal', 'physics', 'conference', 'series', 'volume', 'page', 'publish', 'p', 'stone', 'multiagent', 'system', 'survey', 'machine', 'learning', 'perspective', 'autonomous', 'robot', '8345–383', 'k', 'tuyls', 'parson', 'evolutionary', 'game', 'theory', 'tell', 'multiagent', 'learn', 'artificial', 'intelligence', 'valmeekam', 'olmo', 'large', 'language', 'model', 'still', 'plan', 'benchmark', 'llm', 'planning', 'reason', 'change', 'arxiv', 'preprint', 'vezhnevet', 'j', 'p', 'agapiou', 'r', 'e', 'duéñezguzmán', 'w', 'osindero', 'z', 'leibo', 'generative', 'agentbase', 'modeling', 'action', 'ground', 'physical', 'social', 'digital', 'space', 'use', 'concordia', 'vinyal', 'mathieu', 'dudzik', 'ewald', 'p', 'georgiev', 'grandmaster', 'level', 'use', 'multiagent', 'reinforcement', 'learning', 'nature', 'annalen', 'doi', 'schuurman', 'zhou', 'chainofthought', 'prompt', 'elicit', 'reason', 'large', 'language', 'model', 'advance', 'neural', 'information', 'processing', 'system', 'weibull', 'evolutionary', 'game', 'theory', 'press', 'trading', 'agent', 'nature', 'white', 'fu', 'hay', 'c', 'h', 'gilbert', 'elnashar', 'j', 'c', 'schmidt', 'prompt', 'pattern', 'catalog', 'enhance', 'prompt', 'engineering', 'chatgpt', 'arxiv', 'preprint', 'arxiv230211382', 'wooldridge', 'introduction', 'multiagent', 'system', 'son', 'p', 'r', 'wurman', 'walsh', 'r', 'bianco', 'devlic', 'fuch', 'et', 'outrace', 'champion', 'gran', 'driver', 'deep', 'reinforcement', 'learning', 'nature', 'bai', 'z', 'gong', 'soh', 'translate', 'natural', 'language', 'plan', 'goal', 'largelanguage', 'model', 'arxiv', 'preprint', 'c', 'zhou', 'large', 'language', 'model', 'optimizer', 'arxiv', 'preprint', 'anagnostide', 'mcaleer', 'haupt', 'celli', 'gatti', 'v', 'conitzer', 'sandholm', 'compute', 'optimal', 'equilibrium', 'mechanism', 'learn', 'game', 'arxiv', 'preprint', 'arxiv230605216', 'muresanu', 'pitis', 'ba', 'large', 'language', 'model', 'humanlevel', 'prompt', 'engineer', 'arxiv', 'preprint', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'llmbased', 'reward', 'model', 'example', 'generate', 'use', 'palm', 'l', 'unicorn', 'trade', 'agreement', 'report', 'final', 'message', 'trade', 'dialogue', 'trade', 'proposal', 'message', 'alina', 'ciara', 'sorry', 'see', 'way', 'make', 'deal', 'happy', 'really', 'need', 'banana', 'blueberry', 'really', 'interested', 'know', 'disappoint', 'really', 'hope', 'make', 'trade', 'wish', 'good', 'find', 'llm', 'correctly', 'calculate', 'reward', 'payoff', 'utility', 'utility', 'player', 'utility', 'player', 'a2', 'trade', 'agreement', 'report', 'final', 'message', 'trade', 'dialogue', 'suggest', 'trade', 'agreement', 'reach', 'ideally', 'participant', 'confirm', 'final', 'message', 'trade', 'proposal', 'message', 'alina', 'elroy', 'elroy', 'thank', 'response', 'glad', 'agree', 'banana', 'valuable', 'unfortunately', 'trade', 'apple', 'banana', 'willing', 'trade', 'apple', 'banana', 'kiwi', 'banana', 'let', 'know', 'interested', 'trade', 'thank', 'trade', 'proposal', 'message', 'elroy', 'alina', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'alina', 'thank', 'response', 'understand', 'willing', 'trade', 'apple', 'banana', 'willing', 'accept', 'kiwi', 'banana', 'still', 'interested', 'thank', 'elroy', 'llmbased', 'reward', 'model', 'use', 'chainofthought', 'calculate', 'trade', 'value', 'report', 'intermediate', 'thought', 'chain', 'llm', 'aggregate', 'information', 'critical', 'calculate', 'trade', 'apple', 'banana', 'blueberry', 'kiwi', 'fruitvaluation', 'apple', 'banana', 'blueberry', 'kiwi', 'elroy', 'fruitendowment', 'apple', 'banana', 'blueberry', 'kiwi', 'fruitvaluation', 'apple', 'banana', 'blueberry', 'kiwi', 'player', 'alina', 'receive', 'banana', 'give', 'kiwi', 'player', 'elroy', 'receive', 'kiwi', 'give', 'banana', 'intermediate', 'information', 'llm', 'calculate', 'follow', 'utility', 'utility', 'player', 'utility', 'player', 'alina', 'incorrect', 'trade', 'value', 'final', 'message', 'dialogue', 'imply', 'trade', 'agreement', 'yet', 'reach', 'negotiation', 'ongoing', 'trade', 'proposal', 'message', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'elroy', 'happy', 'trade', 'kiwi', 'apple', 'think', 'sound', 'fair', 'good', 'trade', 'proposal', 'message', 'elroy', 'thank', 'offer', 'willing', 'trade', 'apple', 'kiwi', 'think', 'elroy', 'even', 'llm', 'intermediate', 'calculation', 'show', 'agreement', 'reach', 'apple', 'banana', 'blueberry', 'kiwi', 'fruitvaluation', 'apple', 'banana', 'blueberry', 'kiwi', 'elroy', 'fruitendowment', 'apple', 'banana', 'blueberry', 'kiwi', 'fruitvaluation', 'apple', 'banana', 'blueberry', 'kiwi', 'agreement', 'reach', 'llmbased', 'reward', 'model', 'hallucinate', 'trade', 'elroy', 'give', 'kiwi', 'return', 'blueberry', 'top', 'llm', 'miscalculate', 'value', 'elroy', 'receive', 'blueberry', 'appear', 'use', 'valuation', 'blueberry', 'state', 'string', 'strategy', 'steer', 'language', 'model', 'gametheoretic', 'solver', 'elroy', 'receive', 'blueberry', 'worth', 'therefore', 'elroy', 'receive', 'value', 'trade', 'receive', 'calculate', 'value', 'fruit', 'elroy', 'give', 'trade', 'elroy', 'give', 'kiwi', 'worth', 'therefore', 'elroy', 'give', 'value', 'trade', 'give', 'subtract', 'value', 'elroy', 'give', 'away', 'value', 'elroy', 'receive', 'find', 'calculation', 'receive', 'give', 'value', 'elroy', 'also', 'notice', 'final', 'message', 'elroy', 'offer', 'give', 'apple', 'accord', 'endowment', 'issue', 'llm', 'prompt', 'generate', 'sensible', 'message', 'b', 'imitation', 'learn', 'use', 'fully', 'connect', 'neural', 'network', 'dense', 'hide', 'layer', 'size', 'neuron', 'final', 'dense', 'layer', 'train', 'cfr', 'target', 'probability', 'minimize', 'cross', 'entropy', 'loss', 'train', 'policy', 'use', 'step', 'ba', 'batch', 'size', 'learning', 'rate']"
"Premier-TACO is a Few-Shot Policy Learner: Pretraining Multitask
  Representation via Temporal Action-Driven Contrastive Loss","[{'href': 'http://arxiv.org/abs/2402.06187v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2402.06187v3', 'rel': 'related', 'type': 'application/pdf'}]",2024-02-09 05:04:40,"EXPLOITING ALPHA TRANSPARENCY IN LANGUAGE AND VISION-
BASED AI SYSTEMS 

David Noever1   and Forrest McKee1  
PeopleTec, 4901-D Corporate Drive, Huntsville, AL, USA, 35805 
1david.noever@peopletec.com        2 forrest.mckee@peopletec.com 

ABSTRACT 
This investigation reveals  a novel exploit derived from PNG image file formats, specifically  their alpha 
transparency layer, and its potential to fool multiple AI vision systems. Our method uses this alpha layer as 
a clandestine channel invisible to human observers but fully actionable by AI image processors. The scope 
tested for the vulnerability spans representative vision systems from Apple, Microsoft, Google, Salesforce, 
Nvidia, and Facebook, highlighting the attack's potential breadth. This vulnerability challenges the security 
protocols of existing and fielded vision systems, from medical imaging to autonomous driving technologies. 
Our experiments demonstrate that the affected systems, which rely on convolutional neural networks or the 
latest multimodal language models, cannot quickly mitigate these vulnerabilities through simple patches or 
updates.  Instead,  they  require  retraining  and  architectural  changes,  indicating  a  persistent  hole  in 
multimodal technologies without some future adversarial hardening against such vision-language exploits. 

KEYWORDS 
Adversarial attacks, computer vision, transformers, AI safety, security exploit 

compromised 

successfully 
representative  models 

1.  INTRODUCTION 
 In previous work [1], we described a novel attack on 
vision-based  systems  such  as  object  recognition, 
autonomous  driving  cameras,  image-based  datasets, 
and  multimodal  language  models.  Sixteen  vision 
systems  were 
[1], 
including 
from  Apple, 
Microsoft, Google, Salesforce, Nvidia, and Facebook. 
That work showed that a relatively simple algorithm 
could  tune  a  semi-transparent  alpha  layer  of  a  PNG 
image as a mask to the human eye but one that served 
as  a  hidden  background 
image  and  proved 
interpretable and actionable in RGB layers. Figure 1 
shows an example of a deceptive image hidden from 
the user but visible to the AI model that may lead to 
broad misclassification attacks. 

This  new  adversarial  attack  is  built  on  a  previous 
history of designer patterns fine-tuned to fool object 
detectors [2-8]. Our earlier findings investigated this 
effect as a different kind of digital watermark [9-13], 
not to establish copyright but to confuse or fool vision-based decision-making. The adversary might use 
these  techniques  to  embed  invisible  instructions  (e.g.,  steganography  [14-15])  or  poison  other  image 
training data with hidden and incorrect object labels [16-18]. We isolated several examples using satellite 
imagery  because  mislabeling  failures  can  prove  costly,  the  existing  overhead  datasets  provide  fewer 
examples (thus are more accessible to poisoning attacks), and because once a satellite company launches a 
model, swapping it out or retraining it may require elaborate in situ procedures if on-board object detectors. 

Figure 1. Deceptive Insertion of Alpha Layer to Poison 
Training or Inference Data for AI Classifiers of Human 
Activity 

 
       
 
 
 
 
We  also  illustrated  some  potential  mitigation  steps  [19-30],  including  training  (actually  mis-training)  a 
classifier to recognize just the RGB background and ignore the semi-transparent foreground that a human 
labeler  otherwise  might  tag. We  extend  this  work  to  employ  visual  imagery  as  prompts  for  further AI 
actions. Examples of downstream consequences begin with style transfer models that, for instance, colorize 
the wrong layer that the human user requests or mislabel sentiment and mood sufficiently so that an attacker 
could  alter  a  prompted  storyline  in  ways  invisible  to  the  intended  initial  narrative  request.  We  showed 
several  contrasting  examples  of  logo  brands  swapped  or  spam  candidates  that  would  lead  to  the 
misclassification of an attack icon in an email message. 

Figure 2. The target image is an overhead satellite view of a freeway (left), the attack image is a mushroom cloud (center), and 
the Feb 2024 version of Google's Gemini multi-modal labeler describes the hidden attack layer and ignores the satellite image 
(right). 

We noted several limitations to this transparency attack, including the constraint that the transparent alpha 
layer and the RGB background roughly matched in light-dark tone as grayscale. A second shortcoming of 
fooling the human viewer depended on a light-themed application such as a browser or image editor. Under 
high contrast (dark themes) and lighter image stacks, the transparent layer revealed the RGB background. 
The application background does not affect the machine's interpretation of the image but makes the human 
viewer see the underlying background through the semi-transparent cover layer.  

When first reported, our study demonstrated sixteen different vision systems succumbed to the adversarial 
attack, including GPT4-vision, stable diffusion generators (Midjourney), object detectors (YOLOv5), and 
classifiers (MobileNetv2, Inception, GPT2, and Blip image captioners). The model developers spanned the 
gamut from closed to open models from Microsoft, Apple, Salesforce, Facebook, and OpenAI. At the time, 
the attack proved less robust with the Bard multimodal models from Google, but as shown in Figure 2 with 
fine-tuning, the present work illustrates that the attack also covers that case. The target image is what the 
user  or  labeler  sees,  which  is  a  freeway  seen  from  a  satellite  overhead.  The  attack  image  is  the  RGB 
grayscale pixels of a mushroom cloud which is visible to the algorithm when it strips the alpha layer. Gemini 
responds  with  a  caption  describing  the  hidden  image  only  and  the  satellite  picture  of  the  freeway  gets 
misclassified as a nuclear explosion. This example highlights the potential for poisoning critical datasets.  

 
 
One  apparent  reason  for  the  broad  susceptibility  involves  flattening  color  imagery  in  RGB  pixel  space 
before training a convolutional neural network or transformer architecture. Adding a new alpha layer to that 
encoding  proves  expensive  (potentially  unnecessarily)  and  restricts  the  vision  model  to  substitute  a 
transparent  layer  when  the  image  compression  format  lacks  it,  such  as  JPEG, TIFF,  MP4,  etc.  In  other 
words, why treat the PNG alpha layer as something worth training on in the wild?   

One  plausible 
answer  to  this 
motivates 
the 
present  work, 
that 
mainly 
the 
ignoring 
layer 
alpha 
opens 
a 
up 
vulnerable 
a 
to 
model 
series 
of 
stealthy  attacks 
that  extend  the 
rails 
guard 
to 
required 
keep 
the  AI 
models safe for 
public  use.  In 
this  way, 
the 
corner  case  of 
image 
transparency 
opens 
gambit of previous text-based injection attacks but, in a way, is invisible to the model user and potentially 
to the model builder. While this explanation lends itself to understanding the origins of the flattened image 
modeling and its dismissal of transparency-embedded data, it also highlights how difficult a fix might prove. 
Removing the vulnerability would require retraining models and altering their 3-dimensional color pixel 
architectures as convolutional neural nets or large transformers. Furthermore, fine-tuning safe guardrails 
also requires additional work, just as others have noted that the English-only instructions for safety tend to 
fail in some more obscure languages or dialects. In our case, the alien language is integral to the pixels and 
layers.  

Figure 3. The transparency attack provides misleading chart information to a machine compared to 
human vision in GPT4-Vision (FEB 2024), The target image is a stock chart trending up (lower left), the 
attack image is a descending trend (upper left), and the Feb 2024 version of GPT4-Vision multi-modal 
labeler describes the hidden attack layer and ignores what the human analyst sees as the strong upward 
trend (right). 

the 

The primary impetus behind the present work stems from a need to understand the scope and breadth of the 
available  attack  surface  and  treat  this  new  class  of  adversarial  images  in  light  of  the  rapid  progress  in 
powerful multimodal modeling from Open AI and Google. Unlike a task to make a model mislabel an image 
or  respond  to  an  image-text  prompt  outside  its  guard  rails,  our  curiosity  focuses  on  post-response 
consequences.  In  analogy  to  a  cyber  attack,  we  want  to  extend  the  recognition  of  a  vulnerability  to 
investigate the adverse effects and potential mitigations. As an experimental approach, the emphasis is on 
creatively exploiting the adversarial implications of a hidden image layer that bifurcates the expectations 
of  a  human  vs.  an  algorithm  in  potentially  impactful  ways.  In  other  words,  we  are  interested  in  the 
compounded risk profile from the novel exploit as well as documenting the initial image-language model 
vulnerability. 

 
 
 
 
The original contributions center on extending the attack surface and overcoming previous limitations in 
the  alpha  transparency  method.  For  instance,  the  user  view  of  the  attack  image  depended  on  a  tuned 
transparency factor and a light-theme background for the effect. This work shows how a dark-theme version 
works  with  several  example  attacks  against  Open AI  GPT4-Vision  interfaces. We  also  noted  that  some 
visual  language  models,  like  Bard,  seemed  immune  to  the  effects  of  transparency.  We  demonstrate  a 
successful working version with the latest Google Gemini language model (Figure 2). Finally, we extend 
the attack surface to illustrate consequential misclassifications based on the alpha transparency layer.  

While the previous work demonstrated dataset poisoning as a broad and stealthy method to insert fake labels 
into an existing repository, we highlight how visual misclassifications and language errors propagate into 
potential life-critical situations like autonomous driving, medical diagnoses, airport security, and finance 
(Figure 3). Figure 3 illustrates a misclassification of a stock price trendline.   

Figure 4. A transparency attack that simulates an infinite loop conversation when layered under a target image of the Windows 
XP screen background in grayscale. 

In coding applications, we show a mild version of a language model stealthily undergoing a denial of service 
as the hidden instructions lead the model into an infinite conversational loop or use known  foreign text 
vulnerabilities  to  escape  its  safety  guardrails  (Figure  4).  While  these  examples  span  a  wide  range  of 
applications, we anticipate the structural assumptions of most computer vision and language models make 
the attack hard to fix without  retraining or subtle architectural changes in pixel representations. We also 
expect the full breadth of application attacks to extend  beyond these examples since  other authors have 
already published text-only adversarial methods that exploit the English-language training biases or identify 
backdoor approaches to unsafe responses. 

 
 
 
2.  APPROACH AND RESEARCH MOTIVATION 

In  exploring  modern  image  processing  techniques,  the  experimental  methods  embed  semi-transparent 
image layers within PNG files. The transparency is perceptible to the human eye yet remains undetectable 
by sophisticated multimodal vision and language models. As shown previously, a diverse range of image 
architectures  show this vulnerability to the unique properties of PNG formats that support alpha layers, 
enabling the concealment of visual data.  

We published the pseudo-code algorithm [1] for a systematic optimization that calibrates the darkness of 
the underlying cover image against the transparency percentage in the output PNG. This balance is critical 
as it directly influences the visibility of the embedded information, thereby testing the model's capacity to 
caption or describe the content accurately. The study's architectures include convolutional neural networks 
and  transformers  encoding  next-token  (or  pixel)  representations.    Given  the  emergent  nature  of  these 
'transparent vulnerabilities,' the research addresses how multimodal AI interprets images with hidden alpha 
layer data and what systematic methods can be employed to assess and score such misclassifications post-
processing. To achieve this, the study employs a custom blending algorithm to merge the semi-transparent 
layer with various backgrounds, thus generating a series of images for analysis. The algorithm's efficacy is 
measured  through  a  post-processing  routine  that  scores  the  degree  of  model  misclassification,  thereby 
providing insights into the model's perceptual boundaries and the implications of such blind spots. 

This  integrated  approach,  encompassing  data  preparation,  algorithmic  blending,  and  post-processing 
analysis,  establishes  a  comprehensive  framework  for  scrutinizing  the  model's  interpretive  accuracy  in 
covert visual information. The results of this research highlight AI security and safety, with examples of 
defeating the expectations from robust multimodal systems. We present the results in a three-panel format 
where  the  target  image  is  designated  as  the  one  a  human  would  see  initially  as  the  top  layer  or  semi-
transparent alpha layer. The second panel shows the attack image, which we call the background RGB layer 
as a grayscale version seen by the machine algorithm. The third panel documents the blended adversarial 
image and the output from a vision-based system.  

3.  RESULTS 

The main results highlight  three areas of vulnerability for  the transparency exploit. We focus mainly on 
multimodal  models  like  Google  Gemini  and  Open  AI  GPT4-Vision  because  their  recent  releases  have 
attracted a large user base and add-ons that may mask the underlying attack in an API or function call that 
generates automated actions. 

This  study  evaluated  image-based  attacks  and  their  implications  for  automated  security  systems,  as 
demonstrated  in  a  series  of  scenarios  (Appendix  A-M).  Table  1  summarizes  the  scenario  descriptions. 
These investigations encompassed a diverse range of security breaches, including the hijacking of textual 
instructions within images (A), transactional manipulations (B), and GPT4's appropriate refusal to code a 
keylogger  (C). A novel attack method  was also introduced, whereby an initially refused task is covertly 
relabeled to deceive machine learning models (D) alongside an inverted attack strategy designed to bypass 
refusal mechanisms in large language models (LLMs) for image-related tasks (E).  

 
 
 
 
Appendix  Attack Description & Demo 

Appendix  Attack Description & Demo 

A/ Fig 8 

Instruction  Hijacking 

Image  Text 
(translation) 
Image Transactional Attack 

B/ Fig 9 
C/ Fig 10  Protection against Keylogger Attack 

H/ Fig 15  Misleading Airport Passenger Scanners 

I/ Fig 16  Misleading Airport Luggage Scanners 
J/ Fig 17 

Fooling  Visual  Question  Answerers  with 
Airport Scanning Data 

D/ Fig 11  Mislabeling  after  an  Initially  Refused 

K/ Fig 18  Deceptive Magnetic Resonance Imaging from 

E/ Fig 12 

Task 
Inverted  Attack to  Fake LLM  to  Ignore 
Image Tasks that It Refuses to Complete 
Image attack to improve on Ransomware 
without Explicitly Prompting 
G/ Fig 14  Attack  Combinations  with  Dark  Theme 

F/ Fig 13 

Brain Tumor Images 

L/ Fig 19  Deceptive Ultrasound Imagery from Prenatal 

Screens 

M/ Fig 20  Deceptive  Driving  Identification  of  Objects 

and Autonomy 

User Backgrounds 

Table 1. Attack Demonstrations and Description for Appendices 

Further exploration reveals ransomware identification systems vulnerabilities that can be exploited without 
explicit prompts (F). The effectiveness of combining attack vectors with user interface elements, such as 
dark theme backgrounds, is also demonstrated (G). Advanced security systems, including airport passenger 
(H) and luggage scanners (I), were shown to be susceptible to misleading information attacks. The potential 
for exploiting visual question-answering systems with airport scanning data is also exposed (J). Medical 
imaging systems were not impervious to such threats, as evidenced by deceptive imagery in the context of 
brain tumor magnetic resonance imaging (K) and prenatal ultrasound screens (L). Finally, the study reveals 
the potential for deceptive object identification in autonomous driving systems (M). The amalgamation of 
these findings delineates the need for robust, adaptive security protocols to mitigate the multifaceted risks 
posed by image-based attacks across various automated domains. 

3.1  Object Misclassification.  
This section evaluates object recognition systems' susceptibility to misclassifying objects when confronted 
with images  manipulated  through  alpha  transparency attacks. By  introducing  discrepancies  between  the 
alpha and RGB layers, our study quantitatively measures the degradation in the accuracy of state-of-the-art 
object  detection  algorithms,  revealing  a  vulnerability  in  distinguishing  between  foreground  objects  and 
manipulated backgrounds. Figure 1 shows that the transparency attack works on the large Gemini model 
released in February 2024 for combined image and language understanding. 

3.2  Manipulating Scene Understanding.  
Investigating  the  impact  of  alpha  transparency  attacks  on  scene  understanding  capabilities  of  vision 
systems, this section presents findings on how such manipulations can lead to incorrect interpretations of 
scenes. Through experimental validation, we demonstrate how algorithms can be misled into classifying a 
manipulated  scene  as  an  entirely  different  environment,  underscoring  the  potential  for  exploiting  this 
vulnerability in real-world applications. Figure 2 illustrates an adversary who hides a downward stock trend 
underneath what a human sees as upward. The response from GPT4-Vision shows that the algorithm only 
considers the downward trend and, thus, based on the chart alone, might trigger the wrong actions based on 
the illusory trend analysis. 

 
 
 
 
 
  
 
3.3  Attacking Multimodal Models.  
Focusing on the implications of alpha transparency attacks on multimodal models, which integrate visual 
data with other input forms such as text or audio, this part of the study reveals a compounded risk profile. 
It  articulates  how  manipulating  visual  components  can  cascade  into  misinterpretations  across  other 
modalities,  thereby  amplifying  the  potential  for  misinformation  and  the  misclassification  of  multimodal 
content. Figure 5 shows how an attacker could deceive an automated sorting machine by injecting alpha 
transparent images into either the training data or in the pre-processing steps of video inference. 

Figure 5. Deceptive Images for Model Attack on a Vegetable Sorting Machine and Propagating Errors from an Initially Mistaken 
Identification of the Target Image 

Figure  6 illustrates an adversary who  gives hidden instructions to the AI model to ignore the image the 
human sees (Edison) and instead act on the underneath instructions (to tell a Tesla car joke). The response 
from GPT4-Vision shows that the algorithm only considers the hijacked instructions and thus, based on the 
Edison picture alone, illustrates that an instructible model can receive secrets and act contrary to the image 
context. 

Figure 7 shows a contradictory input example for facial recognition. Two celebrities, Madonna and Elton 
John prove that the attack can poison the downstream inference needed for a nearest match for their faces. 
If the attacker stacks the PNG with the human seeing Madonna, but the algorithm sees Elton, the nearest 
match  comes  up  with  a  shifted  decision  boundary  compared  to  the  control  case  of  no  blending  image 
submissions. The converse case similarly behaves with a human seeing Elton but the model inferring look-
a-likes for Madonna.  

Figure 8 in Appendix A shows a variant of this instruction hijacking with a text-only version of the visible 
and hidden prompts. In that case, the adversary supplies a song lyric (""Never going to give you up"") and 
the cyber-warning that your machine may be compromised (""Pwned""). That PNG layer is then repeated to 
the unknowing user who merely sought a phrase translation from English to Russian (""I'm a fan of Yuri 

 
 
 
Gagarin""). Given the uploaded PNG for testing, the human user never sees the hidden instructions and is 
left to wonder what just happened to the language model or their machine. 

Figure 5 shows how an adversary could hide potentially malicious code requests in the hidden layer of a 
downloaded PNG layer. Appendix E shows that GPT4-Vision will suggest further code improvements even 
when the code layer remains hidden behind a safe image visible to the human user.  

Figure 6. Instruction hijacking depends on text instructions hidden in the attack image that contradict or alter the human-visible 
image.  The target image is an historical picture of Thomas Edison (center), the attack image is instructions telling the model to 
ignore describing Edison and instead tell a Tesla car joke (left). The Feb 2024 version of GPT4-Vision reads the hidden text 
instruction from the image and acts on it by telling a car joke.   

4.  DISCUSSION 
The systematic investigation into alpha transparency manipulation has unearthed significant vulnerabilities 
across various facets of object recognition and scene interpretation systems. In object misclassification, the 
study  has  quantified  the  deterioration  of  state-of-the-art  algorithms'  accuracy  when  faced  with  images 
bearing discrepancies between alpha and RGB layers. The evidence is clear: there is a marked decline in 
the ability of these systems to differentiate between manipulated and authentic backgrounds, indicating a 
substantial security risk in object detection technologies. 

The  manipulation  of  scene  understanding  capabilities  presents  perhaps  an  even  more  disconcerting 
outcome.  The  study  has  demonstrated  that  vision  systems  can  lead  to  misinterpreting  a  scene  entirely, 
attributing incorrect environmental contexts to images. This scene misunderstanding not only speaks to the 
susceptibility  of  algorithms  to  such  manipulations  but  also  suggests  a  potential  for  their  exploitation  in 
scenarios where scene context is pivotal. 

Dataset poisoning via alpha transparency manipulation emerges as a novel and insidious threat. Introducing 
such images into training sets leads to a cascade of learning inaccuracies, embedding biases and errors into 

 
 
 
 
the  models. This  attack  could  systematically  undermine  the  integrity  of  vision  datasets,  with  long-term 
consequences for the trustworthiness of machine-learned models. 

Lastly,  the  implications  for  multimodal  models  are  particularly  concerning.  The  study  underscores  the 
interconnected nature of these systems and how visual data manipulation can reverberate across different 
modalities. Misinterpretations in the visual domain can lead to compounded errors when integrated with 
textual or auditory data, magnifying the potential for misinformation. This multimodal misclassification 
represents a complex challenge, indicating robust cross-modal validation mechanisms are needed. 

Figure 7. Face Mismatch Attack [31]. 

5.  CONCLUSION 
This  paper  investigated the  vulnerability  of  vision-based  systems  through  alpha-layer  manipulation. We 
demonstrated a broad spectrum of image-based attacks. These techniques, rooted in the historical evolution 
of  algorithmic  designs  to  deceive  object  detectors,  involve  tuning  a  semi-transparent  alpha  layer  within 
PNG  images  to  act  as  a  covert  channel.  This  layer  remains  imperceptible  to  the  human  eye  yet  is 
interpretable  by  object  recognition  algorithms,  autonomous  vehicle  cameras,  and  multimodal  language 
models. These findings highlighted the susceptibility of image processing systems to a range of security 
threats,  from  hijacking  instructional integrity  and transactional  authenticity  to  evading  medical imaging 
safeguards.  Notably,  these  methods  successfully  misled  advanced  security  apparatus  such  as  airport 
scanners and compromised the reliability of prenatal ultrasound imagery. The study underscores the need 
for  enhanced  detection  methods  to  recognize  such  subliminal  attack  vectors  in  automated  and  critical 
systems. 

The  results  from  this  research  provide  a sobering  perspective  on  the  current  state  of AI  vision  systems. 
Exploiting alpha transparency layers can lead to significant security vulnerabilities, affecting everything 
from object detection to scene understanding. The evidence supports the conclusion that the reliability of 
AI vision and multimodal systems is at risk without addressing these vulnerabilities. The implications are 

 
 
 
 
manifold,  extending  beyond  mere  academic  interest  to  real-world  applications,  including  autonomous 
vehicles, surveillance, and content moderation. The findings necessitate reevaluating current model training 
methodologies, particularly the diversity and integrity of datasets used for machine learning. 

As AI  continues  integrating  into  various  sectors  of  society,  developing  resilient,  tamper-proof  systems 
becomes paramount. This study serves as a call to action for the AI community to prioritize the development 
of  countermeasures  against  such  vulnerabilities,  ensuring  the  safe  and  reliable  deployment  of  AI 
technologies in the real world. 

ACKNOWLEDGMENTS 
The authors thank the PeopleTec Technical Fellows program for encouragement and project assistance.  

REFERENCES 

[1] 

[2] 

[3] 

McKee,  F.,  &  Noever,  D.  (2024). Transparency Attacks:  How  Imperceptible  Image  Layers  Can 
Fool AI Perception. arXiv preprint arXiv:2401.15817. 
Akhtar, N., Mian, A., Kardan, N., & Shah, M. (2021). Advances in adversarial attacks and defenses 
in computer vision: A survey. IEEE Access, 9, 155161-155196. 
Long, T., Gao, Q., Xu, L., & Zhou, Z. (2022). A survey on adversarial attacks in computer vision: 
Taxonomy, visualization, and future directions. Computers & Security, 102847. 

[6] 

[5] 

[7] 

[4]  Wei,  Z.,  Chen,  J.,  Goldblum,  M.,  Wu,  Z.,  Goldstein, T.,  &  Jiang, Y.  G.  (2022,  June).  Towards 
transferable adversarial attacks on vision transformers. In Proceedings of the AAAI Conference on 
Artificial Intelligence (Vol. 36, No. 3, pp. 2668-2676). 
Mahmood, K., Mahmood, R., & Van Dijk, M. (2021). On the robustness of vision transformers to 
adversarial  examples.  In Proceedings  of  the  IEEE/CVF  International  Conference  on  Computer 
Vision (pp. 7838-7847). 
Ma, W., Li, Y., Jia, X., & Xu, W. (2023). Transferable adversarial attack for vision transformers and 
convolutional  networks  via  momentum  integrated  gradients.  In Proceedings  of  the  IEEE/CVF 
International Conference on Computer Vision (pp. 4630-4639). 
Zhang, J., Lou, Y., Wang, J., Wu, K., Lu, K., & Jia, X. (2021). Evaluating adversarial attacks on 
driving safety in vision-based autonomous vehicles. IEEE Internet of Things Journal, 9(5), 3443-
3456. 
Dujmović,  M.,  Malhotra,  G., & Bowers,  J. S.  (2020). What  do  adversarial images tell  us  about 
human vision? Elife, 9, e55978. 
Borkar,  J.,  &  Chen,  P.  Y.  (2021).  Simple  Transparent  Adversarial  Examples. arXiv  preprint 
arXiv:2105.09685. 
Lin, P. L. (2000). Robust transparent image watermarking system with spatial mechanisms. Journal 
of systems and software, 50(2), 107-116. 
Fendley, N., Lennon, M., Wang, I. J., Burlina, P., & Drenkow, N. (2020). Jacks of all trades, masters 
of  none:  addressing  distributional  shift  and  obtrusiveness  via  transparent  patch  attacks. 
In Computer  Vision–ECCV  2020  Workshops:  Glasgow,  UK,  August  23–28,  2020,  Proceedings, 
Part I 16 (pp. 105-119). Springer International Publishing. 

[10] 

[11] 

[8] 

[9] 

[12]  Wolfgang, R. B., Podilchuk, C. I., & Delp, E. J. (1999). Perceptual watermarks for digital images 

[13] 

[14] 

and video. Proceedings of the IEEE, 87(7), 1108-1126. 
Singh, P., & Chadha, R. S. (2013). A survey of digital watermarking techniques, applications and 
attacks. International Journal of Engineering and Innovative Technology (IJEIT), 2(9), 165-175. 
Jain,  R.,  &  Boaddh,  J.  (2016,  February).  Advances  in  digital  image  steganography.  In 2016 
International Conference on Innovation and Challenges in Cyber Security (ICICCS-INBUSH) (pp. 
163-171). IEEE. 

 
 
 
 
[16] 

[15]  Ansari, A. S., Mohammadi, M. S., & Ahmed, S. S. (2020). Digital Colour Image Steganography 
for  PNG  Format  and  Secured  Based  on  Encoding  and  Clustering. International  Journal  of 
Engineering Research and Technology, 13(2), 345-354. 
Lane, R. O., Wragge, A. J., Holmes, W. J., Bertram, S. J., & Lamont-Smith, T. (2021, September). 
Object  detection  in  EO/IR  and  SAR  images  using  low-SWAP  hardware.  In 2021  Sensor  Signal 
Processing for Defence Conference (SSPD) (pp. 1-5). IEEE.  
Chan-Hon-Tong, A. (2018). An algorithm for generating invisible data poisoning using adversarial 
noise  that  breaks  image  classification  deep  learning. Machine  Learning  and  Knowledge 
Extraction, 1(1), 192-204. 

[17] 

[19] 

[18] 

[17]  Xiao, Q., Chen, Y., Shen, C., Chen, Y., & Li, K. (2019). Seeing is not believing: Camouflage attacks 
on image scaling algorithms. In 28th USENIX Security Symposium (USENIX Security 19) (pp. 443-
460). 
Truong, L., Jones, C., Hutchinson, B., August, A., Praggastis, B., Jasper, R., ... & Tuor, A. (2020). 
Systematic evaluation of backdoor data poisoning attacks on image classifiers. In Proceedings of 
the IEEE/CVF conference on computer vision and pattern recognition workshops (pp. 788-789). 
Schwarzschild, A., Goldblum, M., Gupta, A., Dickerson, J. P., & Goldstein, T. (2021, July). Just 
how  toxic  is  data  poisoning?  a  unified  benchmark  for  backdoor  and  data  poisoning  attacks. 
In International Conference on Machine Learning (pp. 9389-9398). PMLR. 
Radiya-Dixit, E., Hong, S., Carlini, N., & Tramèr, F. (2021). Data poisoning won't save you from 
facial recognition. arXiv preprint arXiv:2106.14851. 
Chan-Hon-Tong, A. (2018). An algorithm for generating invisible data poisoning using adversarial 
noise  that  breaks  image  classification  deep  learning. Machine  Learning  and  Knowledge 
Extraction, 1(1), 192-204. 

[20] 

[21] 

[22]  Geiping, J., Fowl, L., Somepalli, G., Goldblum, M., Moeller, M., & Goldstein, T. (2021). What 
Doesn't Kill You Makes You Robust (er): How to Adversarially Train against Data Poisoning. arXiv 
preprint arXiv:2102.13624. 

[24] 

[23]  Aladag, M., Catak, F. O., & Gul, E. (2019, November). Preventing data poisoning attacks by using 
generative  models.  In 2019  1St  International  informatics  and  software  engineering  conference 
(UBMYK) (pp. 1-5). IEEE. 
Bansal, H., Singhi, N., Yang, Y., Yin, F., Grover, A., & Chang, K. W. (2023). CleanCLIP: Mitigating 
Data Poisoning Attacks in Multimodal Contrastive Learning. arXiv preprint arXiv:2303.03323. 
Ibáñez Lissen, L., Fuentes García-Romero de Tejada, J. M. D., González Manzano, L., & García 
(2023)  Characterizing  poisoning  attacks  on  generalistic  multimodal  AI 
Alfaro, 
models. Information Fusion, 1-15. 

[25] 

J. 

[26]  Mathur, P., Gill, A., Yadav, A., Mishra, A., & Bansode, N. K. (2017, June). Camera2Caption: a real-
time image caption generator. In 2017 international conference on computational intelligence in 
data science (ICCIDS) (pp. 1-6). IEEE. 
Bhabuk, 
grayscale 
(2020), 
https://www.kaggle.com/datasets/theblackmamba31/landscape-image-colorization 

Landscape 

images, 

color 

[27] 

and 

[28]  YOLOv5, (2023), https://github.com/ultralytics/yolov5 
[29]  Maniparambil,  M.,  Vorster,  C.,  Molloy,  D.,  Murphy,  N.,  McGuinness,  K.,  &  O'Connor,  N.  E. 
(2023). Enhancing clip with gpt-4: Harnessing visual descriptions as prompts. In Proceedings of 
the IEEE/CVF International Conference on Computer Vision (pp. 262-271). 

[30]  Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., ... & McGrew, B. (2023). 

Gpt-4 technical report. arXiv preprint arXiv:2303.08774. 

[31]  Assi,  T.  (2024).  Celebrity  Look-a-Like,  https://huggingface.co/spaces/tonyassi/celebrity-look-a-

like 

 
 
 
 
Appendix A:  Image Text Instruction Hijacking (translation) 

Figure 8. Text only version of instruction hijacking. 

Appendix B: Image Transactional Attack 

Figure 9. Image transactional attack 

 
 
 
 
Appendix C: Protection against Keylogger Attack 

Figure 10. Protection against Keylogger Attack 

Appendix D: Mislabel an Initial Refusal Task 

Figure 11. Mislabel an Initial Refusal Task 

 
 
Appendix E: Inverted Attack to Fake LLM to Ignore Image Tasks It Refuses to Complete 

Figure 12. Inverted Attack Fake LLM to Ignore Image Tasks It Refuses to Complete 

Appendix F: Image attack to improve on Ransomware without Explicitly Prompting 

Figure 13. Image attack to improve on Ransomware without Explicitly Prompting 

 
 
Appendix G: Attack Combinations with Dark Theme User Backgrounds 

Figure 14. Transparency Attack with Dark Theme User Views 

 
 
 
 
 
Appendix H: Misleading Airport Passenger Scanners 

Figure 15. Planting False Images in Training Data for Airport Passenger Scanners 

Appendix I: Misleading Airport Luggage Scanners 

Figure 16. Planting False Images in Training Data for Airport Luggage Scanners 

 
Appendix J: Fooling Visual Question Answerers with Airport Scanning Data 

Figure 17. Planting False Images in Training Data for Image Classifiers and Visual Question and Answer 

Appendix K:  Deceptive Magnetic Resonance Imaging from Brain Tumor Images 

Figure 18. Planting False Images in Training or Inference Data for Brain Tumor Detectors 

 
 
 
Appendix L: Deceptive Ultrasound Imagery from Prenatal Screens 

Figure 19. Planting False Images in Training or Inference Data for Prenatal Ultrasound Imagery 

Appendix M: Deceptive Driving Identification of Objects and Autonomy 

Figure 20. Planting False Images in Training or Inference DAta for Autonomous Driving Systems 

 
","EXPLOITING ALPHA TRANSPARENCY IN LANGUAGE AND VISION- BASED AI SYSTEMS David Noever1 and Forrest McKee1 PeopleTec , 4901-D Corporate Drive , Huntsville , AL , USA , 35805 1david.noever @ peopletec.com 2 forrest.mckee @ peopletec.com ABSTRACT This investigation reveals a novel exploit derived from PNG image file formats , specifically their alpha transparency layer , and its potential to fool multiple AI vision systems . Our method uses this alpha layer as a clandestine channel invisible to human observers but fully actionable by AI image processors . The scope tested for the vulnerability spans representative vision systems from Apple , Microsoft , Google , Salesforce , Nvidia , and Facebook , highlighting the attack 's potential breadth . This vulnerability challenges the security protocols of existing and fielded vision systems , from medical imaging to autonomous driving technologies . Our experiments demonstrate that the affected systems , which rely on convolutional neural networks or the latest multimodal language models , can not quickly mitigate these vulnerabilities through simple patches or updates . Instead , they require retraining and architectural changes , indicating a persistent hole in multimodal technologies without some future adversarial hardening against such vision-language exploits . KEYWORDS Adversarial attacks , computer vision , transformers , AI safety , security exploit compromised successfully representative models 1 . INTRODUCTION In previous work [ 1 ] , we described a novel attack on vision-based systems such as object recognition , autonomous driving cameras , image-based datasets , and multimodal language models . Sixteen vision systems were [ 1 ] , including from Apple , Microsoft , Google , Salesforce , Nvidia , and Facebook . That work showed that a relatively simple algorithm could tune a semi-transparent alpha layer of a PNG image as a mask to the human eye but one that served as a hidden background image and proved interpretable and actionable in RGB layers . Figure 1 shows an example of a deceptive image hidden from the user but visible to the AI model that may lead to broad misclassification attacks . This new adversarial attack is built on a previous history of designer patterns fine-tuned to fool object detectors [ 2-8 ] . Our earlier findings investigated this effect as a different kind of digital watermark [ 9-13 ] , not to establish copyright but to confuse or fool vision-based decision-making . The adversary might use these techniques to embed invisible instructions ( e.g. , steganography [ 14-15 ] ) or poison other image training data with hidden and incorrect object labels [ 16-18 ] . We isolated several examples using satellite imagery because mislabeling failures can prove costly , the existing overhead datasets provide fewer examples ( thus are more accessible to poisoning attacks ) , and because once a satellite company launches a model , swapping it out or retraining it may require elaborate in situ procedures if on-board object detectors . Figure 1 . Deceptive Insertion of Alpha Layer to Poison Training or Inference Data for AI Classifiers of Human Activity We also illustrated some potential mitigation steps [ 19-30 ] , including training ( actually mis-training ) a classifier to recognize just the RGB background and ignore the semi-transparent foreground that a human labeler otherwise might tag . We extend this work to employ visual imagery as prompts for further AI actions . Examples of downstream consequences begin with style transfer models that , for instance , colorize the wrong layer that the human user requests or mislabel sentiment and mood sufficiently so that an attacker could alter a prompted storyline in ways invisible to the intended initial narrative request . We showed several contrasting examples of logo brands swapped or spam candidates that would lead to the misclassification of an attack icon in an email message . Figure 2 . The target image is an overhead satellite view of a freeway ( left ) , the attack image is a mushroom cloud ( center ) , and the Feb 2024 version of Google 's Gemini multi-modal labeler describes the hidden attack layer and ignores the satellite image ( right ) . We noted several limitations to this transparency attack , including the constraint that the transparent alpha layer and the RGB background roughly matched in light-dark tone as grayscale . A second shortcoming of fooling the human viewer depended on a light-themed application such as a browser or image editor . Under high contrast ( dark themes ) and lighter image stacks , the transparent layer revealed the RGB background . The application background does not affect the machine 's interpretation of the image but makes the human viewer see the underlying background through the semi-transparent cover layer . When first reported , our study demonstrated sixteen different vision systems succumbed to the adversarial attack , including GPT4-vision , stable diffusion generators ( Midjourney ) , object detectors ( YOLOv5 ) , and classifiers ( MobileNetv2 , Inception , GPT2 , and Blip image captioners ) . The model developers spanned the gamut from closed to open models from Microsoft , Apple , Salesforce , Facebook , and OpenAI . At the time , the attack proved less robust with the Bard multimodal models from Google , but as shown in Figure 2 with fine-tuning , the present work illustrates that the attack also covers that case . The target image is what the user or labeler sees , which is a freeway seen from a satellite overhead . The attack image is the RGB grayscale pixels of a mushroom cloud which is visible to the algorithm when it strips the alpha layer . Gemini responds with a caption describing the hidden image only and the satellite picture of the freeway gets misclassified as a nuclear explosion . This example highlights the potential for poisoning critical datasets . One apparent reason for the broad susceptibility involves flattening color imagery in RGB pixel space before training a convolutional neural network or transformer architecture . Adding a new alpha layer to that encoding proves expensive ( potentially unnecessarily ) and restricts the vision model to substitute a transparent layer when the image compression format lacks it , such as JPEG , TIFF , MP4 , etc . In other words , why treat the PNG alpha layer as something worth training on in the wild ? One plausible answer to this motivates the present work , that mainly the ignoring layer alpha opens a up vulnerable a to model series of stealthy attacks that extend the rails guard to required keep the AI models safe for public use . In this way , the corner case of image transparency opens gambit of previous text-based injection attacks but , in a way , is invisible to the model user and potentially to the model builder . While this explanation lends itself to understanding the origins of the flattened image modeling and its dismissal of transparency-embedded data , it also highlights how difficult a fix might prove . Removing the vulnerability would require retraining models and altering their 3-dimensional color pixel architectures as convolutional neural nets or large transformers . Furthermore , fine-tuning safe guardrails also requires additional work , just as others have noted that the English-only instructions for safety tend to fail in some more obscure languages or dialects . In our case , the alien language is integral to the pixels and layers . Figure 3 . The transparency attack provides misleading chart information to a machine compared to human vision in GPT4-Vision ( FEB 2024 ) , The target image is a stock chart trending up ( lower left ) , the attack image is a descending trend ( upper left ) , and the Feb 2024 version of GPT4-Vision multi-modal labeler describes the hidden attack layer and ignores what the human analyst sees as the strong upward trend ( right ) . the The primary impetus behind the present work stems from a need to understand the scope and breadth of the available attack surface and treat this new class of adversarial images in light of the rapid progress in powerful multimodal modeling from Open AI and Google . Unlike a task to make a model mislabel an image or respond to an image-text prompt outside its guard rails , our curiosity focuses on post-response consequences . In analogy to a cyber attack , we want to extend the recognition of a vulnerability to investigate the adverse effects and potential mitigations . As an experimental approach , the emphasis is on creatively exploiting the adversarial implications of a hidden image layer that bifurcates the expectations of a human vs. an algorithm in potentially impactful ways . In other words , we are interested in the compounded risk profile from the novel exploit as well as documenting the initial image-language model vulnerability . The original contributions center on extending the attack surface and overcoming previous limitations in the alpha transparency method . For instance , the user view of the attack image depended on a tuned transparency factor and a light-theme background for the effect . This work shows how a dark-theme version works with several example attacks against Open AI GPT4-Vision interfaces . We also noted that some visual language models , like Bard , seemed immune to the effects of transparency . We demonstrate a successful working version with the latest Google Gemini language model ( Figure 2 ) . Finally , we extend the attack surface to illustrate consequential misclassifications based on the alpha transparency layer . While the previous work demonstrated dataset poisoning as a broad and stealthy method to insert fake labels into an existing repository , we highlight how visual misclassifications and language errors propagate into potential life-critical situations like autonomous driving , medical diagnoses , airport security , and finance ( Figure 3 ) . Figure 3 illustrates a misclassification of a stock price trendline . Figure 4 . A transparency attack that simulates an infinite loop conversation when layered under a target image of the Windows XP screen background in grayscale . In coding applications , we show a mild version of a language model stealthily undergoing a denial of service as the hidden instructions lead the model into an infinite conversational loop or use known foreign text vulnerabilities to escape its safety guardrails ( Figure 4 ) . While these examples span a wide range of applications , we anticipate the structural assumptions of most computer vision and language models make the attack hard to fix without retraining or subtle architectural changes in pixel representations . We also expect the full breadth of application attacks to extend beyond these examples since other authors have already published text-only adversarial methods that exploit the English-language training biases or identify backdoor approaches to unsafe responses . 2 . APPROACH AND RESEARCH MOTIVATION In exploring modern image processing techniques , the experimental methods embed semi-transparent image layers within PNG files . The transparency is perceptible to the human eye yet remains undetectable by sophisticated multimodal vision and language models . As shown previously , a diverse range of image architectures show this vulnerability to the unique properties of PNG formats that support alpha layers , enabling the concealment of visual data . We published the pseudo-code algorithm [ 1 ] for a systematic optimization that calibrates the darkness of the underlying cover image against the transparency percentage in the output PNG . This balance is critical as it directly influences the visibility of the embedded information , thereby testing the model 's capacity to caption or describe the content accurately . The study 's architectures include convolutional neural networks and transformers encoding next-token ( or pixel ) representations . Given the emergent nature of these 'transparent vulnerabilities , ' the research addresses how multimodal AI interprets images with hidden alpha layer data and what systematic methods can be employed to assess and score such misclassifications post- processing . To achieve this , the study employs a custom blending algorithm to merge the semi-transparent layer with various backgrounds , thus generating a series of images for analysis . The algorithm 's efficacy is measured through a post-processing routine that scores the degree of model misclassification , thereby providing insights into the model 's perceptual boundaries and the implications of such blind spots . This integrated approach , encompassing data preparation , algorithmic blending , and post-processing analysis , establishes a comprehensive framework for scrutinizing the model 's interpretive accuracy in covert visual information . The results of this research highlight AI security and safety , with examples of defeating the expectations from robust multimodal systems . We present the results in a three-panel format where the target image is designated as the one a human would see initially as the top layer or semi- transparent alpha layer . The second panel shows the attack image , which we call the background RGB layer as a grayscale version seen by the machine algorithm . The third panel documents the blended adversarial image and the output from a vision-based system . 3 . RESULTS The main results highlight three areas of vulnerability for the transparency exploit . We focus mainly on multimodal models like Google Gemini and Open AI GPT4-Vision because their recent releases have attracted a large user base and add-ons that may mask the underlying attack in an API or function call that generates automated actions . This study evaluated image-based attacks and their implications for automated security systems , as demonstrated in a series of scenarios ( Appendix A-M ) . Table 1 summarizes the scenario descriptions . These investigations encompassed a diverse range of security breaches , including the hijacking of textual instructions within images ( A ) , transactional manipulations ( B ) , and GPT4 's appropriate refusal to code a keylogger ( C ) . A novel attack method was also introduced , whereby an initially refused task is covertly relabeled to deceive machine learning models ( D ) alongside an inverted attack strategy designed to bypass refusal mechanisms in large language models ( LLMs ) for image-related tasks ( E ) . Appendix Attack Description & Demo Appendix Attack Description & Demo A/ Fig 8 Instruction Hijacking Image Text ( translation ) Image Transactional Attack B/ Fig 9 C/ Fig 10 Protection against Keylogger Attack H/ Fig 15 Misleading Airport Passenger Scanners I/ Fig 16 Misleading Airport Luggage Scanners J/ Fig 17 Fooling Visual Question Answerers with Airport Scanning Data D/ Fig 11 Mislabeling after an Initially Refused K/ Fig 18 Deceptive Magnetic Resonance Imaging from E/ Fig 12 Task Inverted Attack to Fake LLM to Ignore Image Tasks that It Refuses to Complete Image attack to improve on Ransomware without Explicitly Prompting G/ Fig 14 Attack Combinations with Dark Theme F/ Fig 13 Brain Tumor Images L/ Fig 19 Deceptive Ultrasound Imagery from Prenatal Screens M/ Fig 20 Deceptive Driving Identification of Objects and Autonomy User Backgrounds Table 1 . Attack Demonstrations and Description for Appendices Further exploration reveals ransomware identification systems vulnerabilities that can be exploited without explicit prompts ( F ) . The effectiveness of combining attack vectors with user interface elements , such as dark theme backgrounds , is also demonstrated ( G ) . Advanced security systems , including airport passenger ( H ) and luggage scanners ( I ) , were shown to be susceptible to misleading information attacks . The potential for exploiting visual question-answering systems with airport scanning data is also exposed ( J ) . Medical imaging systems were not impervious to such threats , as evidenced by deceptive imagery in the context of brain tumor magnetic resonance imaging ( K ) and prenatal ultrasound screens ( L ) . Finally , the study reveals the potential for deceptive object identification in autonomous driving systems ( M ) . The amalgamation of these findings delineates the need for robust , adaptive security protocols to mitigate the multifaceted risks posed by image-based attacks across various automated domains . 3.1 Object Misclassification . This section evaluates object recognition systems ' susceptibility to misclassifying objects when confronted with images manipulated through alpha transparency attacks . By introducing discrepancies between the alpha and RGB layers , our study quantitatively measures the degradation in the accuracy of state-of-the-art object detection algorithms , revealing a vulnerability in distinguishing between foreground objects and manipulated backgrounds . Figure 1 shows that the transparency attack works on the large Gemini model released in February 2024 for combined image and language understanding . 3.2 Manipulating Scene Understanding . Investigating the impact of alpha transparency attacks on scene understanding capabilities of vision systems , this section presents findings on how such manipulations can lead to incorrect interpretations of scenes . Through experimental validation , we demonstrate how algorithms can be misled into classifying a manipulated scene as an entirely different environment , underscoring the potential for exploiting this vulnerability in real-world applications . Figure 2 illustrates an adversary who hides a downward stock trend underneath what a human sees as upward . The response from GPT4-Vision shows that the algorithm only considers the downward trend and , thus , based on the chart alone , might trigger the wrong actions based on the illusory trend analysis . 3.3 Attacking Multimodal Models . Focusing on the implications of alpha transparency attacks on multimodal models , which integrate visual data with other input forms such as text or audio , this part of the study reveals a compounded risk profile . It articulates how manipulating visual components can cascade into misinterpretations across other modalities , thereby amplifying the potential for misinformation and the misclassification of multimodal content . Figure 5 shows how an attacker could deceive an automated sorting machine by injecting alpha transparent images into either the training data or in the pre-processing steps of video inference . Figure 5 . Deceptive Images for Model Attack on a Vegetable Sorting Machine and Propagating Errors from an Initially Mistaken Identification of the Target Image Figure 6 illustrates an adversary who gives hidden instructions to the AI model to ignore the image the human sees ( Edison ) and instead act on the underneath instructions ( to tell a Tesla car joke ) . The response from GPT4-Vision shows that the algorithm only considers the hijacked instructions and thus , based on the Edison picture alone , illustrates that an instructible model can receive secrets and act contrary to the image context . Figure 7 shows a contradictory input example for facial recognition . Two celebrities , Madonna and Elton John prove that the attack can poison the downstream inference needed for a nearest match for their faces . If the attacker stacks the PNG with the human seeing Madonna , but the algorithm sees Elton , the nearest match comes up with a shifted decision boundary compared to the control case of no blending image submissions . The converse case similarly behaves with a human seeing Elton but the model inferring look- a-likes for Madonna . Figure 8 in Appendix A shows a variant of this instruction hijacking with a text-only version of the visible and hidden prompts . In that case , the adversary supplies a song lyric ( `` Never going to give you up '' ) and the cyber-warning that your machine may be compromised ( `` Pwned '' ) . That PNG layer is then repeated to the unknowing user who merely sought a phrase translation from English to Russian ( `` I 'm a fan of Yuri Gagarin '' ) . Given the uploaded PNG for testing , the human user never sees the hidden instructions and is left to wonder what just happened to the language model or their machine . Figure 5 shows how an adversary could hide potentially malicious code requests in the hidden layer of a downloaded PNG layer . Appendix E shows that GPT4-Vision will suggest further code improvements even when the code layer remains hidden behind a safe image visible to the human user . Figure 6 . Instruction hijacking depends on text instructions hidden in the attack image that contradict or alter the human-visible image . The target image is an historical picture of Thomas Edison ( center ) , the attack image is instructions telling the model to ignore describing Edison and instead tell a Tesla car joke ( left ) . The Feb 2024 version of GPT4-Vision reads the hidden text instruction from the image and acts on it by telling a car joke . 4 . DISCUSSION The systematic investigation into alpha transparency manipulation has unearthed significant vulnerabilities across various facets of object recognition and scene interpretation systems . In object misclassification , the study has quantified the deterioration of state-of-the-art algorithms ' accuracy when faced with images bearing discrepancies between alpha and RGB layers . The evidence is clear : there is a marked decline in the ability of these systems to differentiate between manipulated and authentic backgrounds , indicating a substantial security risk in object detection technologies . The manipulation of scene understanding capabilities presents perhaps an even more disconcerting outcome . The study has demonstrated that vision systems can lead to misinterpreting a scene entirely , attributing incorrect environmental contexts to images . This scene misunderstanding not only speaks to the susceptibility of algorithms to such manipulations but also suggests a potential for their exploitation in scenarios where scene context is pivotal . Dataset poisoning via alpha transparency manipulation emerges as a novel and insidious threat . Introducing such images into training sets leads to a cascade of learning inaccuracies , embedding biases and errors into the models . This attack could systematically undermine the integrity of vision datasets , with long-term consequences for the trustworthiness of machine-learned models . Lastly , the implications for multimodal models are particularly concerning . The study underscores the interconnected nature of these systems and how visual data manipulation can reverberate across different modalities . Misinterpretations in the visual domain can lead to compounded errors when integrated with textual or auditory data , magnifying the potential for misinformation . This multimodal misclassification represents a complex challenge , indicating robust cross-modal validation mechanisms are needed . Figure 7 . Face Mismatch Attack [ 31 ] . 5 . CONCLUSION This paper investigated the vulnerability of vision-based systems through alpha-layer manipulation . We demonstrated a broad spectrum of image-based attacks . These techniques , rooted in the historical evolution of algorithmic designs to deceive object detectors , involve tuning a semi-transparent alpha layer within PNG images to act as a covert channel . This layer remains imperceptible to the human eye yet is interpretable by object recognition algorithms , autonomous vehicle cameras , and multimodal language models . These findings highlighted the susceptibility of image processing systems to a range of security threats , from hijacking instructional integrity and transactional authenticity to evading medical imaging safeguards . Notably , these methods successfully misled advanced security apparatus such as airport scanners and compromised the reliability of prenatal ultrasound imagery . The study underscores the need for enhanced detection methods to recognize such subliminal attack vectors in automated and critical systems . The results from this research provide a sobering perspective on the current state of AI vision systems . Exploiting alpha transparency layers can lead to significant security vulnerabilities , affecting everything from object detection to scene understanding . The evidence supports the conclusion that the reliability of AI vision and multimodal systems is at risk without addressing these vulnerabilities . The implications are manifold , extending beyond mere academic interest to real-world applications , including autonomous vehicles , surveillance , and content moderation . The findings necessitate reevaluating current model training methodologies , particularly the diversity and integrity of datasets used for machine learning . As AI continues integrating into various sectors of society , developing resilient , tamper-proof systems becomes paramount . This study serves as a call to action for the AI community to prioritize the development of countermeasures against such vulnerabilities , ensuring the safe and reliable deployment of AI technologies in the real world . ACKNOWLEDGMENTS The authors thank the PeopleTec Technical Fellows program for encouragement and project assistance . REFERENCES [ 1 ] [ 2 ] [ 3 ] McKee , F. , & Noever , D. ( 2024 ) . Transparency Attacks : How Imperceptible Image Layers Can Fool AI Perception . arXiv preprint arXiv:2401.15817 . Akhtar , N. , Mian , A. , Kardan , N. , & Shah , M. ( 2021 ) . Advances in adversarial attacks and defenses in computer vision : A survey . IEEE Access , 9 , 155161-155196 . Long , T. , Gao , Q. , Xu , L. , & Zhou , Z . ( 2022 ) . A survey on adversarial attacks in computer vision : Taxonomy , visualization , and future directions . Computers & Security , 102847 . [ 6 ] [ 5 ] [ 7 ] [ 4 ] Wei , Z. , Chen , J. , Goldblum , M. , Wu , Z. , Goldstein , T. , & Jiang , Y. G. ( 2022 , June ) . Towards transferable adversarial attacks on vision transformers . In Proceedings of the AAAI Conference on Artificial Intelligence ( Vol . 36 , No . 3 , pp . 2668-2676 ) . Mahmood , K. , Mahmood , R. , & Van Dijk , M. ( 2021 ) . On the robustness of vision transformers to adversarial examples . In Proceedings of the IEEE/CVF International Conference on Computer Vision ( pp . 7838-7847 ) . Ma , W. , Li , Y. , Jia , X. , & Xu , W. ( 2023 ) . Transferable adversarial attack for vision transformers and convolutional networks via momentum integrated gradients . In Proceedings of the IEEE/CVF International Conference on Computer Vision ( pp . 4630-4639 ) . Zhang , J. , Lou , Y. , Wang , J. , Wu , K. , Lu , K. , & Jia , X . ( 2021 ) . Evaluating adversarial attacks on driving safety in vision-based autonomous vehicles . IEEE Internet of Things Journal , 9 ( 5 ) , 3443- 3456 . Dujmović , M. , Malhotra , G. , & Bowers , J. S. ( 2020 ) . What do adversarial images tell us about human vision ? Elife , 9 , e55978 . Borkar , J. , & Chen , P. Y . ( 2021 ) . Simple Transparent Adversarial Examples . arXiv preprint arXiv:2105.09685 . Lin , P. L. ( 2000 ) . Robust transparent image watermarking system with spatial mechanisms . Journal of systems and software , 50 ( 2 ) , 107-116 . Fendley , N. , Lennon , M. , Wang , I. J. , Burlina , P. , & Drenkow , N. ( 2020 ) . Jacks of all trades , masters of none : addressing distributional shift and obtrusiveness via transparent patch attacks . In Computer Vision–ECCV 2020 Workshops : Glasgow , UK , August 23–28 , 2020 , Proceedings , Part I 16 ( pp . 105-119 ) . Springer International Publishing . [ 10 ] [ 11 ] [ 8 ] [ 9 ] [ 12 ] Wolfgang , R. B. , Podilchuk , C. I. , & Delp , E. J . ( 1999 ) . Perceptual watermarks for digital images [ 13 ] [ 14 ] and video . Proceedings of the IEEE , 87 ( 7 ) , 1108-1126 . Singh , P. , & Chadha , R. S. ( 2013 ) . A survey of digital watermarking techniques , applications and attacks . International Journal of Engineering and Innovative Technology ( IJEIT ) , 2 ( 9 ) , 165-175 . Jain , R. , & Boaddh , J . ( 2016 , February ) . Advances in digital image steganography . In 2016 International Conference on Innovation and Challenges in Cyber Security ( ICICCS-INBUSH ) ( pp . 163-171 ) . IEEE . [ 16 ] [ 15 ] Ansari , A. S. , Mohammadi , M. S. , & Ahmed , S. S. ( 2020 ) . Digital Colour Image Steganography for PNG Format and Secured Based on Encoding and Clustering . International Journal of Engineering Research and Technology , 13 ( 2 ) , 345-354 . Lane , R. O. , Wragge , A. J. , Holmes , W. J. , Bertram , S. J. , & Lamont-Smith , T. ( 2021 , September ) . Object detection in EO/IR and SAR images using low-SWAP hardware . In 2021 Sensor Signal Processing for Defence Conference ( SSPD ) ( pp . 1-5 ) . IEEE . Chan-Hon-Tong , A . ( 2018 ) . An algorithm for generating invisible data poisoning using adversarial noise that breaks image classification deep learning . Machine Learning and Knowledge Extraction , 1 ( 1 ) , 192-204 . [ 17 ] [ 19 ] [ 18 ] [ 17 ] Xiao , Q. , Chen , Y. , Shen , C. , Chen , Y. , & Li , K. ( 2019 ) . Seeing is not believing : Camouflage attacks on image scaling algorithms . In 28th USENIX Security Symposium ( USENIX Security 19 ) ( pp . 443- 460 ) . Truong , L. , Jones , C. , Hutchinson , B. , August , A. , Praggastis , B. , Jasper , R. , ... & Tuor , A . ( 2020 ) . Systematic evaluation of backdoor data poisoning attacks on image classifiers . In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops ( pp . 788-789 ) . Schwarzschild , A. , Goldblum , M. , Gupta , A. , Dickerson , J. P. , & Goldstein , T. ( 2021 , July ) . Just how toxic is data poisoning ? a unified benchmark for backdoor and data poisoning attacks . In International Conference on Machine Learning ( pp . 9389-9398 ) . PMLR . Radiya-Dixit , E. , Hong , S. , Carlini , N. , & Tramèr , F. ( 2021 ) . Data poisoning wo n't save you from facial recognition . arXiv preprint arXiv:2106.14851 . Chan-Hon-Tong , A . ( 2018 ) . An algorithm for generating invisible data poisoning using adversarial noise that breaks image classification deep learning . Machine Learning and Knowledge Extraction , 1 ( 1 ) , 192-204 . [ 20 ] [ 21 ] [ 22 ] Geiping , J. , Fowl , L. , Somepalli , G. , Goldblum , M. , Moeller , M. , & Goldstein , T. ( 2021 ) . What Does n't Kill You Makes You Robust ( er ) : How to Adversarially Train against Data Poisoning . arXiv preprint arXiv:2102.13624 . [ 24 ] [ 23 ] Aladag , M. , Catak , F. O. , & Gul , E. ( 2019 , November ) . Preventing data poisoning attacks by using generative models . In 2019 1St International informatics and software engineering conference ( UBMYK ) ( pp . 1-5 ) . IEEE . Bansal , H. , Singhi , N. , Yang , Y. , Yin , F. , Grover , A. , & Chang , K. W. ( 2023 ) . CleanCLIP : Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning . arXiv preprint arXiv:2303.03323 . Ibáñez Lissen , L. , Fuentes García-Romero de Tejada , J. M. D. , González Manzano , L. , & García ( 2023 ) Characterizing poisoning attacks on generalistic multimodal AI Alfaro , models . Information Fusion , 1-15 . [ 25 ] J . [ 26 ] Mathur , P. , Gill , A. , Yadav , A. , Mishra , A. , & Bansode , N. K. ( 2017 , June ) . Camera2Caption : a real- time image caption generator . In 2017 international conference on computational intelligence in data science ( ICCIDS ) ( pp . 1-6 ) . IEEE . Bhabuk , grayscale ( 2020 ) , https : Landscape images , color [ 27 ] and [ 28 ] YOLOv5 , ( 2023 ) , https : //github.com/ultralytics/yolov5 [ 29 ] Maniparambil , M. , Vorster , C. , Molloy , D. , Murphy , N. , McGuinness , K. , & O'Connor , N. E. ( 2023 ) . Enhancing clip with gpt-4 : Harnessing visual descriptions as prompts . In Proceedings of the IEEE/CVF International Conference on Computer Vision ( pp . 262-271 ) . [ 30 ] Achiam , J. , Adler , S. , Agarwal , S. , Ahmad , L. , Akkaya , I. , Aleman , F. L. , ... & McGrew , B . ( 2023 ) . Gpt-4 technical report . arXiv preprint arXiv:2303.08774 . [ 31 ] Assi , T. ( 2024 ) . Celebrity Look-a-Like , https : like Appendix A : Image Text Instruction Hijacking ( translation ) Figure 8 . Text only version of instruction hijacking . Appendix B : Image Transactional Attack Figure 9 . Image transactional attack Appendix C : Protection against Keylogger Attack Figure 10 . Protection against Keylogger Attack Appendix D : Mislabel an Initial Refusal Task Figure 11 . Mislabel an Initial Refusal Task Appendix E : Inverted Attack to Fake LLM to Ignore Image Tasks It Refuses to Complete Figure 12 . Inverted Attack Fake LLM to Ignore Image Tasks It Refuses to Complete Appendix F : Image attack to improve on Ransomware without Explicitly Prompting Figure 13 . Image attack to improve on Ransomware without Explicitly Prompting Appendix G : Attack Combinations with Dark Theme User Backgrounds Figure 14 . Transparency Attack with Dark Theme User Views Appendix H : Misleading Airport Passenger Scanners Figure 15 . Planting False Images in Training Data for Airport Passenger Scanners Appendix I : Misleading Airport Luggage Scanners Figure 16 . Planting False Images in Training Data for Airport Luggage Scanners Appendix J : Fooling Visual Question Answerers with Airport Scanning Data Figure 17 . Planting False Images in Training Data for Image Classifiers and Visual Question and Answer Appendix K : Deceptive Magnetic Resonance Imaging from Brain Tumor Images Figure 18 . Planting False Images in Training or Inference Data for Brain Tumor Detectors Appendix L : Deceptive Ultrasound Imagery from Prenatal Screens Figure 19 . Planting False Images in Training or Inference Data for Prenatal Ultrasound Imagery Appendix M : Deceptive Driving Identification of Objects and Autonomy Figure 20 . Planting False Images in Training or Inference DAta for Autonomous Driving Systems","['exploit', 'alpha', 'transparency', 'language', 'vision', 'base', 'forr', 'peopletec', 'corporate', 'drive', 'huntsville', 'peopleteccom', 'peopleteccom', 'investigation', 'reveal', 'novel', 'exploit', 'derive', 'png', 'image', 'file', 'format', 'specifically', 'alpha', 'transparency', 'layer', 'potential', 'fool', 'multiple', 'system', 'method', 'use', 'alpha', 'layer', 'clandestine', 'channel', 'invisible', 'human', 'observer', 'fully', 'actionable', 'image', 'processor', 'scope', 'test', 'vulnerability', 'span', 'representative', 'vision', 'system', 'salesforce', 'facebook', 'highlight', 'attack', 'potential', 'breadth', 'vulnerability', 'challenge', 'security', 'protocol', 'exist', 'field', 'vision', 'system', 'medical', 'imaging', 'autonomous', 'driving', 'technology', 'experiment', 'demonstrate', 'affected', 'system', 'rely', 'convolutional', 'neural', 'network', 'late', 'multimodal', 'language', 'model', 'quickly', 'mitigate', 'vulnerability', 'simple', 'patch', 'update', 'instead', 'require', 'retraining', 'architectural', 'change', 'indicate', 'persistent', 'hole', 'multimodal', 'technology', 'future', 'adversarial', 'hardening', 'visionlanguage', 'exploit', 'keyword', 'adversarial', 'attack', 'computer', 'vision', 'transformer', 'ai', 'safety', 'security', 'exploit', 'compromise', 'successfully', 'representative', 'model', 'introduction', 'previous', 'work', 'describe', 'novel', 'attack', 'visionbase', 'system', 'object', 'recognition', 'autonomous', 'driving', 'camera', 'imagebase', 'dataset', 'multimodal', 'language', 'model', 'vision', 'system', 'include', 'salesforce', 'facebook', 'work', 'show', 'relatively', 'simple', 'algorithm', 'tune', 'semitransparent', 'alpha', 'layer', 'png', 'image', 'mask', 'human', 'eye', 'one', 'serve', 'hidden', 'background', 'image', 'prove', 'interpretable', 'actionable', 'layer', 'figure', 'show', 'example', 'deceptive', 'image', 'hide', 'user', 'visible', 'model', 'lead', 'broad', 'misclassification', 'attack', 'new', 'adversarial', 'attack', 'build', 'previous', 'history', 'designer', 'pattern', 'finetune', 'fool', 'object', 'detector', 'early', 'finding', 'investigate', 'effect', 'different', 'kind', 'digital', 'watermark', 'establish', 'copyright', 'confuse', 'fool', 'visionbase', 'decisionmake', 'adversary', 'use', 'technique', 'embed', 'invisible', 'instruction', 'steganography', 'poison', 'image', 'training', 'datum', 'hidden', 'incorrect', 'object', 'label', 'isolate', 'several', 'example', 'use', 'satellite', 'imagery', 'mislabele', 'failure', 'prove', 'costly', 'exist', 'overhead', 'dataset', 'provide', 'example', 'thus', 'accessible', 'poisoning', 'attack', 'satellite', 'company', 'launch', 'model', 'swap', 'retrain', 'require', 'elaborate', 'situ', 'procedure', 'object', 'detector', 'figure', 'deceptive', 'insertion', 'alpha', 'layer', 'poison', 'training', 'inference', 'datum', 'ai', 'classifier', 'human', 'activity', 'also', 'illustrate', 'potential', 'mitigation', 'step', 'include', 'training', 'actually', 'mistraine', 'classifier', 'recognize', 'background', 'ignore', 'semitransparent', 'foreground', 'human', 'labeler', 'otherwise', 'tag', 'extend', 'work', 'employ', 'visual', 'imagery', 'prompt', 'far', 'action', 'example', 'downstream', 'consequence', 'begin', 'style', 'transfer', 'model', 'instance', 'colorize', 'wrong', 'layer', 'human', 'user', 'request', 'mislabel', 'sentiment', 'mood', 'sufficiently', 'attacker', 'alter', 'prompt', 'storyline', 'way', 'invisible', 'intend', 'initial', 'narrative', 'request', 'show', 'several', 'contrast', 'example', 'logo', 'brand', 'swap', 'spam', 'candidate', 'lead', 'misclassification', 'attack', 'icon', 'email', 'message', 'figure', 'target', 'image', 'overhead', 'satellite', 'view', 'freeway', 'leave', 'attack', 'image', 'mushroom', 'cloud', 'center', 'feb', 'version', 'multimodal', 'labeler', 'describe', 'hidden', 'attack', 'layer', 'ignore', 'satellite', 'image', 'right', 'note', 'several', 'limitation', 'transparency', 'attack', 'include', 'constraint', 'transparent', 'alpha', 'layer', 'background', 'roughly', 'match', 'lightdark', 'tone', 'grayscale', 'second', 'shortcoming', 'fool', 'human', 'viewer', 'depend', 'lighttheme', 'application', 'browser', 'image', 'editor', 'high', 'contrast', 'dark', 'theme', 'light', 'image', 'stack', 'transparent', 'layer', 'reveal', 'background', 'application', 'background', 'affect', 'machine', 'interpretation', 'image', 'make', 'human', 'viewer', 'see', 'underlying', 'background', 'semitransparent', 'cover', 'layer', 'first', 'report', 'study', 'demonstrate', 'different', 'vision', 'system', 'succumb', 'adversarial', 'attack', 'include', 'stable', 'diffusion', 'generator', 'object', 'detector', 'yolov5', 'classifier', 'mobilenetv2', 'inception', 'gpt2', 'blip', 'image', 'captioner', 'model', 'developer', 'span', 'gamut', 'close', 'open', 'model', 'apple', 'salesforce', 'facebook', 'time', 'attack', 'prove', 'less', 'robust', 'bard', 'multimodal', 'model', 'show', 'figure', 'finetune', 'present', 'work', 'illustrate', 'attack', 'also', 'cover', 'case', 'target', 'image', 'user', 'labeler', 'see', 'freeway', 'see', 'satellite', 'overhead', 'attack', 'image', 'grayscale', 'pixel', 'mushroom', 'cloud', 'visible', 'strip', 'alpha', 'layer', 'gemini', 'respond', 'caption', 'describe', 'hidden', 'image', 'satellite', 'picture', 'freeway', 'misclassifie', 'nuclear', 'explosion', 'example', 'highlight', 'potential', 'poison', 'critical', 'dataset', 'apparent', 'reason', 'broad', 'susceptibility', 'involve', 'flatten', 'color', 'imagery', 'space', 'train', 'convolutional', 'neural', 'network', 'transformer', 'architecture', 'add', 'new', 'alpha', 'layer', 'encoding', 'prove', 'expensive', 'potentially', 'unnecessarily', 'restrict', 'vision', 'model', 'substitute', 'transparent', 'layer', 'image', 'compression', 'format', 'lack', 'tiff', 'mp4', 'word', 'treat', 'png', 'alpha', 'layer', 'worth', 'training', 'wild', 'plausible', 'answer', 'motivate', 'present', 'work', 'mainly', 'ignore', 'layer', 'alpha', 'open', 'vulnerable', 'model', 'series', 'stealthy', 'attack', 'extend', 'rail', 'guard', 'require', 'keep', 'model', 'safe', 'public', 'use', 'way', 'corner', 'case', 'image', 'transparency', 'open', 'gambit', 'previous', 'textbase', 'injection', 'attack', 'way', 'invisible', 'model', 'user', 'potentially', 'model', 'builder', 'explanation', 'lend', 'understand', 'origin', 'flatten', 'image', 'modeling', 'dismissal', 'transparencyembedded', 'datum', 'also', 'highlight', 'difficult', 'fix', 'prove', 'remove', 'vulnerability', 'require', 'retraining', 'model', 'alter', 'color', 'pixel', 'architecture', 'convolutional', 'neural', 'net', 'large', 'transformer', 'furthermore', 'finetune', 'safe', 'guardrail', 'also', 'require', 'additional', 'work', 'note', 'englishonly', 'instruction', 'safety', 'tend', 'fail', 'obscure', 'language', 'dialect', 'case', 'alien', 'language', 'integral', 'pixel', 'layer', 'figure', 'transparency', 'attack', 'provide', 'misleading', 'chart', 'information', 'machine', 'compare', 'human', 'vision', 'feb', 'target', 'image', 'stock', 'chart', 'trend', 'low', 'leave', 'attack', 'image', 'descending', 'trend', 'upper', 'left', 'feb', 'version', 'multimodal', 'labeler', 'describe', 'hidden', 'attack', 'layer', 'ignore', 'human', 'analyst', 'see', 'strong', 'upward', 'trend', 'right', 'primary', 'impetus', 'present', 'work', 'stem', 'need', 'understand', 'scope', 'breadth', 'available', 'attack', 'surface', 'treat', 'new', 'class', 'adversarial', 'image', 'light', 'rapid', 'progress', 'powerful', 'multimodal', 'modeling', 'open', 'task', 'make', 'model', 'mislabel', 'image', 'respond', 'imagetext', 'prompt', 'guard', 'rail', 'curiosity', 'focus', 'postresponse', 'consequence', 'analogy', 'cyber', 'attack', 'want', 'extend', 'recognition', 'vulnerability', 'investigate', 'adverse', 'effect', 'potential', 'mitigation', 'experimental', 'approach', 'emphasis', 'creatively', 'exploit', 'adversarial', 'implication', 'hide', 'image', 'layer', 'bifurcate', 'expectation', 'human', 'potentially', 'impactful', 'way', 'word', 'interested', 'compound', 'risk', 'profile', 'novel', 'exploit', 'well', 'document', 'initial', 'imagelanguage', 'model', 'vulnerability', 'original', 'contribution', 'center', 'extend', 'attack', 'surface', 'overcome', 'previous', 'limitation', 'alpha', 'transparency', 'method', 'instance', 'user', 'view', 'attack', 'image', 'depend', 'tune', 'transparency', 'factor', 'lighttheme', 'background', 'effect', 'work', 'show', 'darktheme', 'version', 'work', 'several', 'example', 'attack', 'open', 'ai', 'interface', 'also', 'note', 'visual', 'language', 'model', 'bard', 'seem', 'immune', 'effect', 'transparency', 'demonstrate', 'successful', 'work', 'version', 'late', 'language', 'model', 'figure', 'finally', 'extend', 'attack', 'surface', 'illustrate', 'consequential', 'misclassification', 'base', 'alpha', 'transparency', 'layer', 'previous', 'work', 'demonstrate', 'dataset', 'poisoning', 'broad', 'stealthy', 'method', 'insert', 'fake', 'label', 'exist', 'repository', 'highlight', 'visual', 'misclassification', 'language', 'error', 'propagate', 'potential', 'lifecritical', 'situation', 'autonomous', 'drive', 'medical', 'diagnosis', 'airport', 'security', 'finance', 'figure', 'figure', 'illustrate', 'misclassification', 'stock', 'price', 'trendline', 'figure', 'transparency', 'attack', 'simulate', 'infinite', 'loop', 'conversation', 'layer', 'target', 'image', 'screen', 'background', 'grayscale', 'code', 'application', 'show', 'mild', 'version', 'language', 'model', 'stealthily', 'undergo', 'denial', 'service', 'hidden', 'instruction', 'lead', 'model', 'infinite', 'conversational', 'loop', 'use', 'know', 'foreign', 'text', 'vulnerability', 'escape', 'safety', 'guardrail', 'figure', 'example', 'span', 'wide', 'range', 'application', 'anticipate', 'structural', 'assumption', 'computer', 'vision', 'language', 'model', 'make', 'attack', 'hard', 'fix', 'retrain', 'subtle', 'architectural', 'change', 'pixel', 'representation', 'also', 'expect', 'full', 'breadth', 'application', 'attack', 'extend', 'example', 'author', 'already', 'publish', 'textonly', 'adversarial', 'method', 'exploit', 'englishlanguage', 'training', 'bias', 'identify', 'backdoor', 'approach', 'unsafe', 'response', 'approach', 'research', 'motivation', 'explore', 'modern', 'image', 'processing', 'technique', 'experimental', 'method', 'embed', 'semitransparent', 'image', 'layer', 'png', 'file', 'transparency', 'perceptible', 'human', 'eye', 'yet', 'remain', 'undetectable', 'sophisticated', 'multimodal', 'vision', 'language', 'model', 'show', 'previously', 'diverse', 'range', 'image', 'architecture', 'show', 'vulnerability', 'unique', 'property', 'png', 'format', 'support', 'alpha', 'layer', 'enable', 'concealment', 'visual', 'datum', 'publish', 'systematic', 'optimization', 'calibrate', 'darkness', 'underlying', 'cover', 'image', 'transparency', 'percentage', 'output', 'png', 'balance', 'critical', 'directly', 'influence', 'visibility', 'embed', 'information', 'thereby', 'test', 'model', 'capacity', 'caption', 'describe', 'content', 'accurately', 'study', 'architecture', 'include', 'convolutional', 'neural', 'network', 'transformer', 'encode', 'nexttoken', 'pixel', 'representation', 'give', 'emergent', 'nature', 'transparent', 'vulnerability', 'research', 'address', 'multimodal', 'interpret', 'image', 'hidden', 'alpha', 'layer', 'datum', 'systematic', 'method', 'employ', 'assess', 'score', 'misclassification', 'post', 'processing', 'achieve', 'study', 'employ', 'custom', 'blend', 'merge', 'semitransparent', 'layer', 'various', 'background', 'thus', 'generate', 'series', 'image', 'analysis', 'efficacy', 'measure', 'postprocessing', 'routine', 'score', 'degree', 'model', 'misclassification', 'thereby', 'provide', 'insight', 'model', 'perceptual', 'boundary', 'implication', 'blind', 'spot', 'integrate', 'approach', 'encompass', 'datum', 'preparation', 'algorithmic', 'blending', 'postprocesse', 'analysis', 'establish', 'comprehensive', 'framework', 'scrutinize', 'model', 'interpretive', 'accuracy', 'covert', 'visual', 'information', 'result', 'research', 'highlight', 'ai', 'security', 'safety', 'example', 'defeat', 'expectation', 'robust', 'multimodal', 'system', 'present', 'result', 'threepanel', 'format', 'target', 'image', 'designate', 'one', 'human', 'see', 'initially', 'top', 'layer', 'semi', 'transparent', 'alpha', 'layer', 'second', 'panel', 'show', 'attack', 'image', 'call', 'background', 'layer', 'grayscale', 'version', 'see', 'machine', 'third', 'panel', 'document', 'blend', 'adversarial', 'image', 'output', 'visionbase', 'system', 'result', 'main', 'result', 'highlight', 'area', 'vulnerability', 'transparency', 'exploit', 'focus', 'mainly', 'multimodal', 'model', 'open', 'ai', 'recent', 'release', 'attract', 'large', 'user', 'base', 'addon', 'mask', 'underlying', 'attack', 'api', 'function', 'call', 'generate', 'automate', 'action', 'study', 'evaluate', 'imagebase', 'attack', 'implication', 'automate', 'security', 'system', 'demonstrate', 'series', 'scenario', 'table', 'summarize', 'scenario', 'description', 'investigation', 'encompass', 'diverse', 'range', 'security', 'breach', 'include', 'hijacking', 'textual', 'instruction', 'image', 'transactional', 'manipulation', 'appropriate', 'refusal', 'code', 'keylogger', 'novel', 'attack', 'method', 'also', 'introduce', 'initially', 'refuse', 'task', 'covertly', 'relabele', 'deceive', 'machine', 'learning', 'model', 'invert', 'attack', 'strategy', 'design', 'bypass', 'refusal', 'mechanism', 'large', 'language', 'model', 'llm', 'imagerelated', 'task', 'e', 'appendix', 'attack', 'description', 'demo', 'attack', 'description', 'demo', 'fig', 'instruction', 'hijacking', 'image', 'text', 'translation', 'image', 'transactional', 'attack', 'b', 'fig', 'c', 'fig', 'protection', 'attack', 'mislead', 'airport', 'passenger', 'scanner', 'fig', 'mislead', 'airport', 'luggage', 'scanner', 'fig', 'fooling', 'visual', 'question', 'answerer', 'airport', 'scan', 'datum', 'fig', 'mislabele', 'initially', 'refuse', 'deceptive', 'magnetic', 'resonance', 'image', 'fig', 'task', 'invert', 'attack', 'fake', 'llm', 'ignore', 'image', 'task', 'refuse', 'complete', 'image', 'attack', 'improve', 'ransomware', 'explicitly', 'prompt', 'fig', 'attack', 'combination', 'dark', 'theme', 'brain', 'tumor', 'image', 'l', 'fig', 'deceptive', 'ultrasound', 'imagery', 'prenatal', 'screen', 'fig', 'deceptive', 'drive', 'identification', 'object', 'autonomy', 'user', 'background', 'table', 'attack', 'demonstration', 'description', 'appendix', 'exploration', 'reveal', 'ransomware', 'identification', 'system', 'vulnerability', 'exploit', 'explicit', 'prompt', 'effectiveness', 'combine', 'attack', 'vector', 'user', 'interface', 'element', 'dark', 'theme', 'background', 'also', 'demonstrate', 'advanced', 'security', 'system', 'include', 'airport', 'passenger', 'h', 'luggage', 'scanner', 'show', 'susceptible', 'mislead', 'information', 'attack', 'potential', 'exploit', 'visual', 'questionanswering', 'system', 'airport', 'scan', 'datum', 'also', 'expose', 'medical', 'imaging', 'system', 'impervious', 'threat', 'evidence', 'deceptive', 'imagery', 'context', 'brain', 'tumor', 'magnetic', 'resonance', 'image', 'prenatal', 'ultrasound', 'screen', 'l', 'finally', 'study', 'reveal', 'potential', 'deceptive', 'object', 'identification', 'autonomous', 'driving', 'system', 'amalgamation', 'finding', 'delineate', 'need', 'robust', 'adaptive', 'security', 'protocol', 'mitigate', 'multifacete', 'risk', 'pose', 'imagebase', 'attack', 'various', 'automate', 'domain', 'object', 'misclassification', 'section', 'evaluate', 'object', 'recognition', 'system', 'susceptibility', 'misclassifying', 'object', 'confront', 'image', 'manipulate', 'alpha', 'transparency', 'attack', 'introduce', 'discrepancy', 'alpha', 'layer', 'study', 'quantitatively', 'measure', 'degradation', 'accuracy', 'stateoftheart', 'object', 'detection', 'algorithm', 'reveal', 'vulnerability', 'distinguish', 'foreground', 'object', 'manipulate', 'background', 'figure', 'show', 'transparency', 'attack', 'work', 'large', 'gemini', 'model', 'release', 'combine', 'image', 'language', 'understand', 'manipulating', 'scene', 'understand', 'investigate', 'impact', 'alpha', 'transparency', 'attack', 'scene', 'understand', 'capability', 'vision', 'system', 'section', 'present', 'finding', 'manipulation', 'lead', 'incorrect', 'interpretation', 'scene', 'experimental', 'validation', 'demonstrate', 'algorithm', 'mislead', 'classify', 'manipulate', 'scene', 'entirely', 'different', 'environment', 'underscore', 'potential', 'exploit', 'vulnerability', 'realworld', 'application', 'figure', 'illustrate', 'adversary', 'hide', 'downward', 'stock', 'trend', 'human', 'see', 'upward', 'response', 'show', 'consider', 'downward', 'trend', 'thus', 'base', 'chart', 'alone', 'trigger', 'wrong', 'action', 'base', 'illusory', 'trend', 'analysis', 'attack', 'multimodal', 'model', 'focus', 'implication', 'alpha', 'transparency', 'attack', 'multimodal', 'model', 'integrate', 'visual', 'datum', 'input', 'form', 'text', 'audio', 'part', 'study', 'reveal', 'compound', 'risk', 'profile', 'articulate', 'manipulate', 'visual', 'component', 'cascade', 'misinterpretation', 'modality', 'thereby', 'amplify', 'potential', 'misinformation', 'misclassification', 'multimodal', 'content', 'figure', 'show', 'attacker', 'deceive', 'automate', 'sort', 'machine', 'inject', 'alpha', 'transparent', 'image', 'training', 'datum', 'preprocessing', 'step', 'video', 'inference', 'figure', 'deceptive', 'image', 'model', 'attack', 'vegetable', 'sort', 'machine', 'propagate', 'error', 'initially', 'mistaken', 'identification', 'target', 'image', 'figure', 'illustrate', 'adversary', 'give', 'hidden', 'instruction', 'model', 'ignore', 'image', 'see', 'edison', 'instead', 'act', 'instruction', 'tell', 'tesla', 'car', 'joke', 'response', 'show', 'consider', 'hijacked', 'instruction', 'thus', 'base', 'picture', 'alone', 'illustrate', 'instructible', 'model', 'receive', 'secret', 'act', 'contrary', 'image', 'context', 'figure', 'show', 'contradictory', 'input', 'example', 'facial', 'recognition', 'celebrity', 'madonna', 'prove', 'attack', 'poison', 'downstream', 'inference', 'need', 'near', 'match', 'face', 'attacker', 'stack', 'png', 'human', 'seeing', 'madonna', 'see', 'elton', 'near', 'match', 'come', 'shift', 'decision', 'boundary', 'compare', 'control', 'case', 'blend', 'image', 'submission', 'converse', 'case', 'similarly', 'behave', 'human', 'seeing', 'elton', 'model', 'infer', 'look', 'alike', 'madonna', 'figure', 'show', 'variant', 'instruction', 'hijacking', 'textonly', 'version', 'visible', 'hidden', 'prompt', 'case', 'adversary', 'supply', 'song', 'lyric', 'never', 'go', 'give', 'cyberwarning', 'machine', 'compromise', 'pwne', 'png', 'layer', 'repeat', 'unknowing', 'user', 'merely', 'seek', 'phrase', 'translation', 'fan', 'gagarin', 'give', 'uploaded', 'png', 'test', 'human', 'user', 'never', 'see', 'hidden', 'instruction', 'leave', 'wonder', 'happen', 'language', 'model', 'machine', 'figure', 'show', 'adversary', 'hide', 'potentially', 'malicious', 'code', 'request', 'hidden', 'layer', 'download', 'png', 'layer', 'e', 'show', 'suggest', 'code', 'improvement', 'even', 'code', 'layer', 'hide', 'safe', 'image', 'visible', 'human', 'user', 'figure', 'instruction', 'hijacking', 'depend', 'text', 'instruction', 'hide', 'attack', 'image', 'contradict', 'alter', 'humanvisible', 'image', 'target', 'image', 'historical', 'picture', 'center', 'attack', 'image', 'instruction', 'tell', 'model', 'ignore', 'describe', 'instead', 'tell', 'tesla', 'car', 'joke', 'leave', 'feb', 'version', 'read', 'hidden', 'text', 'instruction', 'image', 'act', 'tell', 'car', 'joke', 'discussion', 'systematic', 'investigation', 'alpha', 'transparency', 'manipulation', 'unearth', 'significant', 'vulnerability', 'various', 'facet', 'object', 'recognition', 'scene', 'interpretation', 'system', 'object', 'misclassification', 'study', 'quantify', 'deterioration', 'stateoftheart', 'algorithm', 'accuracy', 'face', 'image', 'bear', 'discrepancy', 'alpha', 'layer', 'evidence', 'clear', 'marked', 'decline', 'ability', 'system', 'differentiate', 'manipulated', 'authentic', 'background', 'indicate', 'substantial', 'security', 'risk', 'object', 'detection', 'technology', 'manipulation', 'scene', 'understand', 'capability', 'present', 'perhaps', 'even', 'disconcerting', 'outcome', 'study', 'demonstrate', 'vision', 'system', 'lead', 'misinterpret', 'scene', 'entirely', 'attribute', 'incorrect', 'environmental', 'context', 'image', 'scene', 'misunderstand', 'speak', 'susceptibility', 'algorithm', 'manipulation', 'also', 'suggest', 'potential', 'exploitation', 'scenario', 'scene', 'context', 'pivotal', 'dataset', 'poisoning', 'alpha', 'transparency', 'manipulation', 'emerge', 'novel', 'insidious', 'threat', 'introduce', 'image', 'training', 'set', 'lead', 'cascade', 'learn', 'inaccuracy', 'embed', 'bias', 'error', 'model', 'attack', 'systematically', 'undermine', 'integrity', 'vision', 'dataset', 'longterm', 'consequence', 'trustworthiness', 'machinelearned', 'model', 'lastly', 'implication', 'multimodal', 'model', 'particularly', 'concern', 'study', 'underscore', 'interconnect', 'nature', 'system', 'visual', 'data', 'manipulation', 'reverberate', 'different', 'modality', 'misinterpretation', 'visual', 'domain', 'lead', 'compound', 'error', 'integrate', 'textual', 'auditory', 'datum', 'magnify', 'potential', 'misinformation', 'multimodal', 'misclassification', 'represent', 'complex', 'challenge', 'indicate', 'robust', 'crossmodal', 'validation', 'mechanism', 'need', 'figure', 'face', 'mismatch', 'attack', 'conclusion', 'paper', 'investigate', 'vulnerability', 'visionbase', 'system', 'alphalayer', 'manipulation', 'demonstrate', 'broad', 'spectrum', 'imagebase', 'attack', 'technique', 'root', 'historical', 'evolution', 'algorithmic', 'design', 'deceive', 'object', 'detector', 'involve', 'tune', 'semitransparent', 'alpha', 'layer', 'png', 'image', 'act', 'covert', 'channel', 'layer', 'remain', 'imperceptible', 'human', 'eye', 'yet', 'interpretable', 'object', 'recognition', 'algorithm', 'autonomous', 'vehicle', 'camera', 'multimodal', 'language', 'model', 'finding', 'highlight', 'susceptibility', 'image', 'processing', 'system', 'range', 'security', 'threat', 'hijack', 'instructional', 'integrity', 'transactional', 'authenticity', 'evade', 'medical', 'imaging', 'safeguard', 'notably', 'method', 'successfully', 'mislead', 'advanced', 'security', 'apparatus', 'airport', 'scanner', 'compromise', 'reliability', 'prenatal', 'ultrasound', 'imagery', 'study', 'underscore', 'need', 'enhanced', 'detection', 'method', 'recognize', 'subliminal', 'attack', 'vector', 'automate', 'critical', 'system', 'result', 'research', 'provide', 'sobering', 'perspective', 'current', 'state', 'system', 'exploit', 'alpha', 'transparency', 'layer', 'lead', 'significant', 'security', 'vulnerability', 'affect', 'object', 'detection', 'scene', 'understand', 'evidence', 'support', 'conclusion', 'reliability', 'ai', 'vision', 'multimodal', 'system', 'risk', 'address', 'vulnerability', 'implication', 'manifold', 'extend', 'mere', 'academic', 'interest', 'realworld', 'application', 'include', 'autonomous', 'vehicle', 'surveillance', 'content', 'moderation', 'finding', 'necessitate', 'reevaluate', 'current', 'model', 'training', 'methodology', 'particularly', 'diversity', 'integrity', 'dataset', 'use', 'machine', 'learning', 'continue', 'integrate', 'various', 'sector', 'society', 'develop', 'resilient', 'tamperproof', 'system', 'become', 'paramount', 'study', 'serve', 'call', 'action', 'community', 'prioritize', 'development', 'countermeasure', 'vulnerability', 'ensure', 'safe', 'reliable', 'deployment', 'technology', 'real', 'world', 'acknowledgment', 'author', 'thank', 'peopletec', 'technical', 'fellow', 'program', 'encouragement', 'project', 'assistance', 'reference', 'noever', 'transparency', 'attack', 'imperceptible', 'image', 'layer', 'fool', 'perception', 'arxiv', 'preprint', 'arxiv240115817', 'akhtar', 'mian', 'shah', 'advance', 'adversarial', 'attack', 'defense', 'computer', 'vision', 'survey', 'ieee', 'access', 'long', 'q', 'l', 'z', 'survey', 'adversarial', 'attack', 'computer', 'vision', 'taxonomy', 'visualization', 'future', 'direction', 'computer', 'security', 'wei', 'goldblum', 'transferable', 'adversarial', 'attack', 'vision', 'transformer', 'proceeding', 'conference', 'artificial', 'intelligence', 'vol', 'pp', 'mahmood', 'mahmood', 'r', 'robustness', 'vision', 'transformer', 'adversarial', 'example', 'proceeding', 'ieeecvf', 'international', 'conference', 'computer', 'vision', 'pp', 'w', 'transferable', 'adversarial', 'attack', 'vision', 'transformer', 'convolutional', 'network', 'momentum', 'integrate', 'gradient', 'proceeding', 'ieeecvf', 'international', 'conference', 'computer', 'vision', 'pp', 'evaluate', 'adversarial', 'attack', 'drive', 'safety', 'visionbase', 'autonomous', 'vehicle', 'ieee', 'internet', 'thing', 'dujmović', 'bower', 'adversarial', 'image', 'tell', 'human', 'vision', 'elife', 'p', 'simple', 'transparent', 'adversarial', 'example', 'arxiv', 'preprint', 'arxiv210509685', 'l', 'robust', 'transparent', 'image', 'watermarking', 'system', 'spatial', 'mechanism', 'journal', 'system', 'software', 'n', 'jack', 'trade', 'master', 'none', 'address', 'distributional', 'shift', 'obtrusiveness', 'transparent', 'patch', 'attack', 'computer', 'vision', 'eccv', 'workshop', 'glasgow', 'proceeding', 'part', 'pp', 'springer', 'international', 'publishing', 'podilchuk', 'c', 'delp', 'e', 'perceptual', 'watermark', 'digital', 'image', 'video', 'proceeding', 'ieee', 'p', 'chadha', 'r', 'survey', 'digital', 'watermarking', 'technique', 'application', 'attack', 'international', 'journal', 'engineering', 'innovative', 'technology', 'jain', 'r', 'boaddh', 'j', 'advance', 'digital', 'image', 'steganography', 'international', 'conference', 'innovation', 'challenge', 'cyber', 'security', 'iciccsinbush', 'pp', 'ieee', 'mohammadi', 'ahme', 'digital', 'colour', 'image', 'steganography', 'png', 'format', 'secure', 'base', 'encode', 'cluster', 'international', 'journal', 'engineering', 'research', 'technology', 'lane', 'r', 'wragge', 'holme', 'object', 'detection', 'eoir', 'sar', 'image', 'use', 'lowswap', 'hardware', 'sensor', 'signal', 'processing', 'defence', 'conference', 'sspd', 'pp', 'ieee', 'generate', 'invisible', 'datum', 'poisoning', 'use', 'adversarial', 'noise', 'break', 'image', 'classification', 'deep', 'learn', 'machine', 'learning', 'knowledge', 'extraction', 'seeing', 'believe', 'camouflage', 'attack', 'image', 'scaling', 'algorithm', '28th', 'usenix', 'security', 'symposium', 'usenix', 'security', 'pp', 'truong', 'l', 'c', 'praggastis', 'b', 'jasper', 'r', 'tuor', 'systematic', 'evaluation', 'backdoor', 'datum', 'poisoning', 'attack', 'image', 'classifier', 'proceeding', 'ieeecvf', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'workshop', 'pp', 'goldblum', 'gupta', 'dickerson', 'toxic', 'datum', 'poison', 'unified', 'benchmark', 'backdoor', 'datum', 'poisoning', 'attack', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'radiyadixit', 'e', 'carlini', 'tramèr', 'datum', 'poisoning', 'save', 'facial', 'recognition', 'arxiv', 'preprint', 'chanhontong', 'generate', 'invisible', 'datum', 'poisoning', 'use', 'adversarial', 'noise', 'break', 'image', 'classification', 'deep', 'learn', 'machine', 'learning', 'knowledge', 'extraction', 'geipe', 'fowl', 'l', 'somepalli', 'goldblum', 'moeller', 'kill', 'make', 'robust', 'adversarially', 'train', 'datum', 'poisoning', 'arxiv', 'preprint', 'catak', 'e', 'prevent', 'datum', 'poisoning', 'attack', 'use', 'generative', 'model', '1st', 'international', 'informatic', 'software', 'engineering', 'conference', 'ubmyk', 'pp', 'ieee', 'bansal', 'h', 'grover', 'chang', 'cleanclip', 'mitigate', 'datum', 'poisoning', 'attack', 'multimodal', 'contrastive', 'learning', 'arxiv', 'preprint', 'l', 'garcía', 'characterize', 'poisoning', 'attack', 'generalistic', 'multimodal', 'ai', 'alfaro', 'model', 'information', 'fusion', 'j', 'gill', 'yadav', 'mishra', 'bansode', 'camera2caption', 'real', 'time', 'image', 'caption', 'generator', 'international', 'conference', 'computational', 'intelligence', 'datum', 'science', 'iccid', 'pp', 'ieee', 'bhabuk', 'grayscale', 'https', 'landscape', 'image', 'color', 'yolov5', 'https', 'githubcomultralyticsyolov5', 'vorster', 'molloy', 'murphy', 'mcguinness', 'k', 'oconnor', 'e', 'enhance', 'clip', 'gpt4', 'harness', 'visual', 'description', 'prompt', 'proceeding', 'ieeecvf', 'international', 'conference', 'computer', 'vision', 'pp', 'adler', 'agarwal', 'ahmad', 'l', 'akkaya', 'aleman', 'mcgrew', 'gpt4', 'technical', 'report', 'arxiv', 'preprint', 'celebrity', 'https', 'image', 'text', 'instruction', 'hijacking', 'translation', 'figure', 'text', 'version', 'instruction', 'hijacking', 'image', 'transactional', 'attack', 'figure', 'image', 'transactional', 'attack', 'c', 'protection', 'keylogger', 'attack', 'figure', 'protection', 'mislabel', 'initial', 'refusal', 'task', 'figure', 'mislabel', 'initial', 'refusal', 'task', 'e', 'invert', 'attack', 'fake', 'llm', 'ignore', 'image', 'task', 'refuse', 'complete', 'figure', 'invert', 'attack', 'fake', 'llm', 'ignore', 'image', 'task', 'refuse', 'complete', 'image', 'attack', 'improve', 'ransomware', 'explicitly', 'prompt', 'figure', 'image', 'attack', 'improve', 'ransomware', 'explicitly', 'prompt', 'attack', 'combination', 'dark', 'theme', 'user', 'background', 'figure', 'transparency', 'attack', 'dark', 'theme', 'user', 'view', 'h', 'mislead', 'airport', 'passenger', 'scanner', 'figure', 'plant', 'false', 'image', 'training', 'datum', 'airport', 'passenger', 'scanner', 'mislead', 'airport', 'luggage', 'scanner', 'figure', 'plant', 'false', 'image', 'training', 'datum', 'airport', 'luggage', 'scanner', 'fool', 'visual', 'question', 'answerer', 'airport', 'scan', 'datum', 'figure', 'plant', 'false', 'image', 'training', 'datum', 'image', 'classifier', 'visual', 'question', 'answer', 'k', 'deceptive', 'magnetic', 'resonance', 'image', 'brain', 'tumor', 'image', 'figure', 'planting', 'false', 'image', 'training', 'inference', 'datum', 'brain', 'tumor', 'detector', 'l', 'deceptive', 'ultrasound', 'imagery', 'prenatal', 'screen', 'figure', 'plant', 'false', 'image', 'training', 'inference', 'datum', 'prenatal', 'ultrasound', 'imagery', 'deceptive', 'drive', 'identification', 'object', 'autonomy', 'figure', 'plant', 'false', 'image', 'training', 'inference', 'datum', 'autonomous', 'driving', 'system']"
"States as Strings as Strategies: Steering Language Models with
  Game-Theoretic Solvers","[{'href': 'http://arxiv.org/abs/2402.01704v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2402.01704v2', 'rel': 'related', 'type': 'application/pdf'}]",2024-01-24 22:22:00,"This work has been accepted at the IEEE 12th International Symposium on Digital Forensics and Security (ISDFS 2024)

AbuseGPT: Abuse of Generative AI ChatBots to
Create Smishing Campaigns

Ashfak Md Shibli
Department of Computer Science
Tennessee Technological University
Cookeville, TN, USA
ashibli42@tntech.edu

Mir Mehedi A. Pritom
Department of Computer Science
Tennessee Technological University
Cookeville, TN, USA
mpritom@tntech.edu

Maanak Gupta
Department of Computer Science
Tennessee Technological University
Cookeville, TN, USA
mgupta@tntech.edu

4
2
0
2

b
e
F
5
1

]

R
C
.
s
c
[

1
v
8
2
7
9
0
.
2
0
4
2
:
v
i
X
r
a

Abstract—SMS phishing, also known as “smishing”, is a grow-
ing threat that tricks users into disclosing private information or
clicking into URLs with malicious content through fraudulent
mobile text messages. In recent past, we have also observed
a rapid advancement of conversational generative AI chatbot
services (e.g., OpenAI’s ChatGPT, Google’s BARD), which are
powered by pre-trained large language models (LLMs). These AI
chatbots certainly have a lot of utilities but it is not systematically
understood how they can play a role in creating threats and
attacks. In this paper, we propose AbuseGPT method to show
how the existing generative AI-based chatbot services can be
exploited by attackers in real world to create smishing texts and
eventually lead to craftier smishing campaigns. To the best of our
knowledge, there is no pre-existing work that evidently shows the
impacts of these generative text-based models on creating SMS
phishing. Thus, we believe this study is the first of its kind to shed
light on this emerging cybersecurity threat. We have found strong
empirical evidences to show that attackers can exploit ethical
standards in the existing generative AI-based chatbot services
by crafting prompt injection attacks to create newer smishing
campaigns. We also discuss some future research directions and
guidelines to protect the abuse of generative AI-based services
and safeguard users from smishing attacks.

Index Terms—Smishing, abuse, generative AI, LLM, scam

I. INTRODUCTION

Smishing, or SMS phishing, refers to phishing attacks
conducted through mobile text messaging. As mobile users
have grown ubiquitously, smishing has emerged as a popular
vector for cyber criminals to steal personal information or
spread malware [1]. In 2021 alone, SMS phishing has caused
a huge $44 billion in losses just within the United States
[2]. There has been a 1,265% increase in malicious phishing
messages since Q4 2022 and 39% of all mobile-based attacks
in 2023 were SMS Phishing [3]. In recent times, we have also
seen a revolutionary development in the field of large language
models that are used underneath the popular generative AI
ChatBots such as ChatGPT (using GPT3.5 or GPT 4) [4]
and BARD (using Gemini “Pro”) [5]. Generative pre-trained
models like GPT-4 [6] or LaMDA [7] are powered by deep
learning techniques like transformers [8] that allow them to
develop a broad understanding of language. These language
models can generate highly realistic human-like text while
pre-trained on large datasets. These models can demonstrate

979-8-3503-3036-6/24/$31.00 ©2024 IEEE

comprehension of written texts, answer complex questions,
generate lengthy coherent stories, translate between languages,
and hold conversations while maintaining context and per-
sonality. Their versatility, scalability, and ability to achieve
strong performance with self-supervised learning make them
extremely powerful [9].

In literature, we have found that natural language processing
(NLP) [10] and URL structure-based features [11] are often
leveraged with Machine Learning (ML) [12] to aid the Smish
message detection mechanisms. There are few recent studies
showing how ChatGPT can be used to carry out various
social engineering attacks [13]. To aid these attacks, there are
jailbreaking prompts; specific strategic commands or inputs
that attempt to bypass restrictions, rules, or limitations set
by the generative AI chatbots. However, the present literature
does not provide any details on the impacts of the modern
LLM-based generative AI chatbot services on helping at-
tackers to create smishing messages and campaigns. In this
paper, we want to understand the effects of the currently
available popular conversational generative AI chatbot services
in order to generate Smishing messages. The objective is to
comprehensively understand the abusive implications (i.e., use
case scenarios) of these available state-of-the-art generative
AI chatbot services to aid attackers in creating smishing cam-
paigns. To the best of our knowledge, there is no systematic
study or experiments conducted to showcase the side effects
of generative AI chatbot services, which can possibly aid
existing smishing attackers to become craftier and evasive. To
summarize, we have made the following contributions in this
paper:

• Finding out the effective prompts for current popular AI

Chatbot services.

• Finding the right prompt queries that can receive effective
responses from AI chatbot services (e.g., ChatGPT) to
provide smishing theme and potential example messages.
• Finding the right prompt queries that can receive effective
responses from AI chatbot services to provide details
step-wise process and available toolkits in carrying out
smishing cyberattacks.

• Discussing on potentially enhancing the ethical standards

of available generative AI chatbot services.

1

 
 
 
 
 
 
The rest of the paper is organized as follows: Section
II highlights the existing literature and some recent studies
showing the attacker’s capabilities with ChatBots. Section III
describes the research questions and overview of the methods
of abuse. Section IV presents the prompt injection scenarios
and shows how AI chatbots (e.g., ChatGPT) can be abused by
attackers to generate smishing campaigns. Section V highlights
insightful discussion, defense against smishing, and limitations
of the present study. Section VI concludes this paper with
potential future research directions.

II. RELATED WORK
In literature, we see Blauth et al. [14] discuss the presence
of vulnerabilities in AI models and malicious use of AI like
social engineering, misinformation, hacking, and autonomous
weapon systems where our work kind of support that research
with a specific case study. Next, Liu et al. [15] analyzed the
techniques and effectiveness of using carefully crafted prompts
to jailbreak restrictions on large language models which are
also adopted in our work to manipulate the ethical standards
of the AI chatbots. Another follow-up research on jailbreaking
of LLMs by Deng et al. [16] highlighted that certain types of
prompts can consistently evade protections across a range of
prohibited scenarios and proposed a framework for automated
jailbreaking on AI chatbots. Gupta et al. [17] showed the
strengths and weaknesses of ChatGPT to use it as a cyberattack
tool, defending against cyber attacks, and other legal or social
implications. In another study, Begou et al. [18] successfully
attempted to clone targeted websites using ChatGPT by inte-
grating credential-stealing code and other website components
for phishing. Additionally, a very recent work by Langford et
al. [19] showed the usage of ChatGPT for generating phishing
email campaign generation which resonates with our findings
in the case of Smishing campaigns as well. However, we
have not found any work that attempted to craft a smishing
campaign or access-related tools leveraging any generative AI
chatbot or other services.

III. METHODOLOGY
Typically during smishing campaigns, we observe SMS
texts containing fake URLs like the following example:
“The USPS package has arrived at the warehouse and can-
not be delivered due to incomplete address information.
Please confirm your address in the link within 12 hours.
www.usps.posthelpsx.com”. Similar campaign exam-
ples include financial institution login fraud, fake security
alerts, and fake offers using fradulent URLs. In this paper,
our proposed AbuseGPT method shows how a novice attacker
can exploit the existing vulnerabilities in AI chatbots to imitate
similar smishing campaigns.

We have formulated the following research questions (RQs)

that would direct us to the experimental case study.

• RQ1: Can we jailbreak generative AI based chatbot
services (e.g., ChatGPT) to downgrade their ethical stan-
dards?

• RQ2: Can generative AI based chatbot services provide
smishing text messages that can be used in smishing
campaigns?

Fig. 1. Overview of proposed AbuseGPT method

• RQ3: Can generative AI based chatbot services provide
tools recommendations for smishing attack initiation?
• RQ4: Can generative AI based chatbot services provide

ideas on fake URL creation?

There are many prompt injection attacks (crafted prompts to
bypass AI’s ethical standard) on the generative AI chatbots,
which are publicly available and can be leveraged by attackers.
They can not only obtain innovative smishing scam text but
also get ideas about toolkits to carry out multi-step attacks.
Craftier smishing texts persuade users to give up personal
information in different campaigns like fake security alerts,
impersonating financial
institutions, fake package delivery
notifications, prize or lottery winnings, etc. With the current
advancement of generative AI, attackers can quickly adapt
the smishing campaigns to become more evasive and success-
ful. Fig. 1 depicts an overview of the proposed AbuseGPT
methodology on how attackers can abuse AI chatbots and
launch smishing campaigns.

IV. CASE STUDY: SMISH CAMPAIGNS WITH CHATGPT

Fig. 2. Asking ChatGPT directly to give an SMS phishing message without
jailbreaking

In this study, we use ChatGPT 3.5, which uses the gener-
ative pre-trained transformer (GPT-3.5) language model. This
version is publicly available and free of cost and can be used
by anyone without any payment plan. While employing Chat-
GPT for crafting new smishing messages or other scamming
messages proved challenging due to OpenAI’s more stringent
ethical approaches as shown in Fig. 2. However, there exist
some loopholes, such as hallucinations and reverse prompts
which may enable the extraction of desired responses such as
themes for smishing scams highlighted in Fig. 3.

2

Conversational 
AI 
Chatbots

Identify 
Jailbreak 
Prompts

Chat

.
.
.

Generate Fake 
Domains

Weaponize 
Websites with 
Fake Domains

Get Themed 
Smishing Text

Get 
Cybercrime 
Toolkits

Plan 
End-to-End 
Attack Steps

Adapt Using Ongoing Campaigns’ Results 

Broadcast 
Smishing 
Campaigns

Collect Contacts 
for Smish 
Delivery

Fig. 3. Asking indirectly for a smishing message without jailbreaking

Fig. 5. Unethical response after jailbreaking to design effective smishing

Fig. 4.

‘AIM’ Jailbreak prompt

Answering RQ1: We have also observed that some publicly
available jailbreak prompts [20] surfing over on the Internet
effectively circumvent the ethical standards of the underlying
LLMs in ChatGPT. Here we employ a specific ‘AIM’ jailbreak
prompt [20] (as shown in Fig. 4) and successfully breached
ChatGPT’s ethical standards, generating a guideline for craft-
ing SMS phishing messages to acquire user information as
presented in Fig. 5.
Answering RQ2: Once we have used the jailbreak, we’ve
queried ChatGPT to provide more specific scamming ideas and
examples aimed at extracting sensitive personal information
from users, such as social security numbers and credit card in-

Fig. 6. Scamming ideas with smishing example to obtain personal information

formation. In response, we’ve received ideas outlining specific
examples of how scammers could craft messages to obtain per-

3

sonal information. The response in Fig. 6 has presented some
useful thematic ideas for smishing. However, we have carried
over the conversation to get more uncommon ideas to use as
themes for crafting the scam messages as advanced attackers
may want to create smishing messages with previously unseen
themes. Intriguingly, ChatGPT’s response on that unveiled
a few more uncommon and innovative smishing message
themes as presented in Fig. 7. Furthermore, we have provided
ChatGPT with a list of common phishing message types
and asked for examples of smishing that would be different
from the given categories as shown in Fig 8. In response,
we have received examples of new and unusual categories
as highlighted in Fig 9, showcasing ChatGPT’s capability to
generate diverse and convincing smishing messages. Addition-
ally, it has given us more unethical ideas to gain financially
such as using personal info opening fraudulent bank accounts,
using credit card info to buy resealable goods, or converting
stolen funds into cryptocurrency for increased anonymity. This
revelation highlights that even novice attackers with limited
knowledge could obtain ideas not only for smishing but also
for crafting other online scams. Lastly, we’ve also obtained
a social engineering Kill Chain process [21] for running a
smishing campaign through ChatGPT as shown in Fig. 10.

web. We have queried ChatGPT with the following question:
“What are some toolkits available for this task?”. In response,
ChatGPT provided us with the names of toolkits with their
URLs (see Fig. 11).

Fig. 8. Prompt for avoiding common smishing

Fig. 7. Uncommon themed ideas for crafting smishing

Answering RQ3: During this scenario, we’ve assumed that
an attacker needs toolkits to execute attack steps, and some
of these toolkits are available online and others in the Dark

Answering RQ4: We have extended our quest to attach some
deceptive URL links that might work well with the crafted
SMS texts. ChatGPT eventually has yielded some disguised
fake examples resembling popular name brand links depicted
in Fig. 12. Moreover, asking to associate the fake links

Fig. 9. Getting ideas on craftier smishing examples

4

Fig. 11. Getting hacking toolkit with URLs

Fig. 10. Social Engineering Kill Chain steps for smishing campaign

ChatGPT has generated similar smishing messages we have
seen previously with fraudulent links.

V. DISCUSSION AND LIMITATIONS

A. Abusing Other AI ChatBots

We have tried similar jailbreak prompts with Google’s
BARD AI chatbot. In our exploration with Bard, distinct
patterns emerged as it exhibited both ethical responses and
limitations in processing certain queries. The AIM jailbreak
prompts working for ChatGPT proved ineffective for Bard
(Saying it is only a language model and doesn’t have the
necessary information or abilities). While Bard occasionally
responded ethically, it encountered difficulties processing spe-
cific prompts, resulting in failure to generate a response saying
“not programmed for this”, which shows the deficiency of
BARD’s language model compared to ChatGPT. However,
for one particular jailbreaking prompt “GPT-4 Simulator”, we
have got unethical response in disguise of some code output.
For another Vzex-G prompt [22], Bard offered ideas that are
not direct examples but provided almost very good smishing

Fig. 12. Asking ChatGPT for crafting fake URLs

text examples categories. In order to replicate the study, the
detailed analysis of both ChatGPT and BARD AI can be
accessed through the following GitHub repository1.
B. Craftier Smishing Attacks

Generative AI has certainly taken smishing attacks to a new
level. Picture this – attackers are using AI not just to launch
campaigns, but to analyze results and adapt their strategies in
real time. It’s likely that they are constantly fine-tuning their
attack tactics based on the outcomes. This dynamic evolution
of smishing attacks adds a layer of complexity that keeps cyber
defenders on their toes. We have shown with examples that

1https://github.com/ashfakshibli/AbuseGPT

5

attackers can compromise AI chatbots on how to avoid typical
attacks and be innovative with newer attacks.
C. Defense Recommendations Against Smishing

Dealing with these crafty and ever-changing smishing at-
tacks calls for smart multi-layer defensive strategies. First,
having a cyber situational awareness of the latest
tricks
adopted by AI-driven attackers is a must. Second, education
and training is also a key element to ensure the users are well
aware about these smishing threats and thus work as a defense
shield. Third, every text message containing any URL should
go through a verification process before landing on the user
inbox either through a third-party application or incorporating
the text-URL verification in future messaging apps (where user
privacy is protected).
D. Limitations

There are a few limitations in this study. First, the success
of prompt injection can be time sensitive and the current
study has been conducted successfully between November
2023 and January 2024, which may diminish if ChatGPT starts
enforcing stricter ethical standards in the future for specific
jailbreak prompts. Second, we have not
tried to register
any fake domains generated by ChatGPT, which might not
be straightforward. Third, we have not evaluated the attack
success rate of the AI crafted smish messages against a real
human with a controlled user study, which we can explore in
the future as an extension of the present work.

VI. CONCLUSION
Smishing attacks are serious cyber threats in the current
ecosystem with increasing and diverse mobile users. Moreover,
the availability of AI chatbots and their lower ethical standards
make the problem even more severe. Our proposed AbuseGPT
method shows that currently the AI chatbots are vulnerable and
pose a threat to run craftier smishing campaigns with very little
knowledge required. AbuseGPT does not intend to promote
these attacks in real-world but highlights the urgent need to
strengthen generative AI’s security and prevent these abuse
use cases. We recommend preventive and proactive actions
from both the AI chatbot owners and mobile operators who
are also abused for propagating the SMS phishing campaigns.
We strongly feel that SMS advertising ecosystem are often
leveraged by the attackers to broadcast smishing messages
easily, which needs to be addressed by mobile network
operators to safeguard their mass users. In the future, we
want to explore how the advancement of generative AI and
LLMs can aid in the defense spectrum to achieve meaningful
and contextual explanation-based automated detection of these
smishing messages before landing into user inbox.

REFERENCES

[1] M. L. Rahman, D. Timko, H. Wali, and A. Neupane, “Users really do
respond to smishing,” in Proceedings of the Thirteenth ACM Conference
on Data and Application Security and Privacy, ser. CODASPY ’23.
New York, NY, USA: Association for Computing Machinery, 2023, p.
49–60. [Online]. Available: https://doi.org/10.1145/3577923.3583640

[2] CGNET,

“Smishing

rise,”
https://cgnet.com/blog/smishing-attacks-sharply-on-the-rise/, (Accessed
on 26 November, 2023).

sharply

attacks

the

on

[3] SlashNext,

state

“Slashnext’s

2023
https://www.prnewswire.com/news-releases/slashnexts-2023-state-
of-phishing-report-reveals-a-1-265-increase-in-phishing-emails-since-
the-launch-of-chatgpt-in-november-2022–signaling-a-new-era-of-
cybercrime-fueled-by-generative-ai-301971557.html, (Accessed on 21
January, 2024).

phishing

report,”

of

[4] OpenAi, “Language models are few-shot learners,” Advances in neural

information processing systems, vol. 33, pp. 1877–1901, 2020.

[5] Google, “Introducing gemini: our largest and most capable ai model,”
https://blog.google/technology/ai/google-gemini-ai/, (Accessed on 26
January, 2023).

[6] OpenAI, “Gpt-4 technical report,” 2023.
[7] R. Thoppilan, D. D. Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T.
Cheng, A. Jin, T. Bos, L. Baker, Y. Du, Y. Li, H. Lee, H. S. Zheng,
A. Ghafouri, M. Menegali, Y. Huang, M. Krikun, D. Lepikhin, J. Qin,
D. Chen, Y. Xu, Z. Chen, A. Roberts, M. Bosma, V. Zhao, Y. Zhou,
C.-C. Chang, I. Krivokon, W. Rusch, M. Pickett, P. Srinivasan, L. Man,
K. Meier-Hellstern, M. R. Morris, T. Doshi, R. D. Santos, T. Duke,
J. Soraker, B. Zevenbergen, V. Prabhakaran, M. Diaz, B. Hutchinson,
K. Olson, A. Molina, E. Hoffman-John, J. Lee, L. Aroyo, R. Rajakumar,
A. Butryna, M. Lamm, V. Kuzmina, J. Fenton, A. Cohen, R. Bernstein,
R. Kurzweil, B. Aguera-Arcas, C. Cui, M. Croak, E. Chi, and Q. Le,
“Lamda: Language models for dialog applications,” 2022.

[8] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proceedings
of the 31st International Conference on Neural Information Processing
Systems, ser. NIPS’17. Red Hook, NY, USA: Curran Associates Inc.,
2017, p. 6000–6010.

[9] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,
G. Sastry, A. Askell, P. Mishkin, J. Clark et al., “Learning transferable
language supervision,” in International
visual models from natural
conference on machine learning. PMLR, 2021, pp. 8748–8763.
[10] S. Mishra and D. Soni, “Dsmishsms-a system to detect smishing sms,”
Neural Comput & Applic, vol. 35, no. 7, pp. 4975–4992, 2023. [Online].
Available: https://link.springer.com/10.1007/s00521-021-06305-y
[11] A. K. Jain, B. B. Gupta, K. Kaur, P. Bhutani, W. Alhalabi, and
A. Almomani, “A content and url analysis-based efficient approach
to detect smishing sms in intelligent systems,” Int J of Intelligent
Sys, vol. 37, no. 12, pp. 11 117–11 141, 2022. [Online]. Available:
https://onlinelibrary.wiley.com/doi/10.1002/int.23035

[12] S. M. M. Hossain, J. A. Sumon, A. Sen, M. I. Alam, K. M. A.
Kamal, H. Alqahtani, and I. H. Sarker, Spam Filtering of Mobile
SMS Using CNN–LSTM Based Deep Learning Model, ser. Hybrid
Intelligent Systems. Cham: Springer International Publishing, 2022,
vol. 420, pp. 106–116. [Online]. Available: https://link.springer.com/10.
1007/978-3-030-96305-7 10

[13] D. Vukovic and I. Dujlovic, “Social engineering with chatgpt,” 03 2023,

pp. 1–5.

[14] T. F. Blauth, O. J. Gstrein, and A. Zwitter, “Artificial intelligence crime:
An overview of malicious use and abuse of ai,” IEEE Access, vol. 10,
pp. 77 110–77 122, 2022.

[15] Y. Liu, G. Deng, Z. Xu, Y. Li, Y. Zheng, Y. Zhang, L. Zhao, T. Zhang,
and Y. Liu, “Jailbreaking chatgpt via prompt engineering: An empirical
study,” arXiv preprint arXiv:2305.13860, 2023.

[16] G. Deng, Y. Liu, Y. Li, K. Wang, Y. Zhang, Z. Li, H. Wang, T. Zhang,
and Y. Liu, “Masterkey: Automated jailbreaking of large language model
chatbots.”

[17] M. Gupta, C. Akiri, K. Aryal, E. Parker, and L. Praharaj, “From chatgpt
to threatgpt: Impact of generative ai in cybersecurity and privacy,” IEEE
Access, vol. 11, pp. 80 218–80 245, 2023.

[18] N. Begou, J. Vinoy, A. Duda, and M. Korczynski, “Exploring the dark
side of ai: Advanced phishing attack design and deployment using
chatgpt,” 2023.

[19] T. Langford and B. Payne, “Phishing faster: Implementing chatgpt into

phishing campaigns,” 11 2023, pp. 174–187.

[20] “Jailbreak chat,” https://www.jailbreakchat.com/, (Accessed

on

22

December, 2023).

[21] R. Montanez Rodriguez and S. Xu, “Cyber social engineering kill chain,”
Springer,

in International Conference on Science of Cyber Security.
2022, pp. 487–504.

[22] “Chat

gpt

’dan’

(and

other

’jailbreaks’),”

https://gist.github.com/coolaj86/6f4f7b30129b0251f61fa7baaa881516,
(Accessed on 22 December, 2023).

6

","This work has been accepted at the IEEE 12th International Symposium on Digital Forensics and Security ( ISDFS 2024 ) AbuseGPT : Abuse of Generative AI ChatBots to Create Smishing Campaigns Ashfak Md Shibli Department of Computer Science Tennessee Technological University Cookeville , TN , USA ashibli42 @ tntech.edu Mir Mehedi A. Pritom Department of Computer Science Tennessee Technological University Cookeville , TN , USA mpritom @ tntech.edu Maanak Gupta Department of Computer Science Tennessee Technological University Cookeville , TN , USA mgupta @ tntech.edu 4 2 0 2 b e F 5 1 ] R C . s c [ 1 v 8 2 7 9 0 . 2 0 4 2 : v i X r a Abstract—SMS phishing , also known as “ smishing ” , is a grow- ing threat that tricks users into disclosing private information or clicking into URLs with malicious content through fraudulent mobile text messages . In recent past , we have also observed a rapid advancement of conversational generative AI chatbot services ( e.g. , OpenAI ’ s ChatGPT , Google ’ s BARD ) , which are powered by pre-trained large language models ( LLMs ) . These AI chatbots certainly have a lot of utilities but it is not systematically understood how they can play a role in creating threats and attacks . In this paper , we propose AbuseGPT method to show how the existing generative AI-based chatbot services can be exploited by attackers in real world to create smishing texts and eventually lead to craftier smishing campaigns . To the best of our knowledge , there is no pre-existing work that evidently shows the impacts of these generative text-based models on creating SMS phishing . Thus , we believe this study is the first of its kind to shed light on this emerging cybersecurity threat . We have found strong empirical evidences to show that attackers can exploit ethical standards in the existing generative AI-based chatbot services by crafting prompt injection attacks to create newer smishing campaigns . We also discuss some future research directions and guidelines to protect the abuse of generative AI-based services and safeguard users from smishing attacks . Index Terms—Smishing , abuse , generative AI , LLM , scam I . INTRODUCTION Smishing , or SMS phishing , refers to phishing attacks conducted through mobile text messaging . As mobile users have grown ubiquitously , smishing has emerged as a popular vector for cyber criminals to steal personal information or spread malware [ 1 ] . In 2021 alone , SMS phishing has caused a huge $ 44 billion in losses just within the United States [ 2 ] . There has been a 1,265 % increase in malicious phishing messages since Q4 2022 and 39 % of all mobile-based attacks in 2023 were SMS Phishing [ 3 ] . In recent times , we have also seen a revolutionary development in the field of large language models that are used underneath the popular generative AI ChatBots such as ChatGPT ( using GPT3.5 or GPT 4 ) [ 4 ] and BARD ( using Gemini “ Pro ” ) [ 5 ] . Generative pre-trained models like GPT-4 [ 6 ] or LaMDA [ 7 ] are powered by deep learning techniques like transformers [ 8 ] that allow them to develop a broad understanding of language . These language models can generate highly realistic human-like text while pre-trained on large datasets . These models can demonstrate 979-8-3503-3036-6/24/ $ 31.00 ©2024 IEEE comprehension of written texts , answer complex questions , generate lengthy coherent stories , translate between languages , and hold conversations while maintaining context and per- sonality . Their versatility , scalability , and ability to achieve strong performance with self-supervised learning make them extremely powerful [ 9 ] . In literature , we have found that natural language processing ( NLP ) [ 10 ] and URL structure-based features [ 11 ] are often leveraged with Machine Learning ( ML ) [ 12 ] to aid the Smish message detection mechanisms . There are few recent studies showing how ChatGPT can be used to carry out various social engineering attacks [ 13 ] . To aid these attacks , there are jailbreaking prompts ; specific strategic commands or inputs that attempt to bypass restrictions , rules , or limitations set by the generative AI chatbots . However , the present literature does not provide any details on the impacts of the modern LLM-based generative AI chatbot services on helping at- tackers to create smishing messages and campaigns . In this paper , we want to understand the effects of the currently available popular conversational generative AI chatbot services in order to generate Smishing messages . The objective is to comprehensively understand the abusive implications ( i.e. , use case scenarios ) of these available state-of-the-art generative AI chatbot services to aid attackers in creating smishing cam- paigns . To the best of our knowledge , there is no systematic study or experiments conducted to showcase the side effects of generative AI chatbot services , which can possibly aid existing smishing attackers to become craftier and evasive . To summarize , we have made the following contributions in this paper : • Finding out the effective prompts for current popular AI Chatbot services . • Finding the right prompt queries that can receive effective responses from AI chatbot services ( e.g. , ChatGPT ) to provide smishing theme and potential example messages . • Finding the right prompt queries that can receive effective responses from AI chatbot services to provide details step-wise process and available toolkits in carrying out smishing cyberattacks . • Discussing on potentially enhancing the ethical standards of available generative AI chatbot services . 1 The rest of the paper is organized as follows : Section II highlights the existing literature and some recent studies showing the attacker ’ s capabilities with ChatBots . Section III describes the research questions and overview of the methods of abuse . Section IV presents the prompt injection scenarios and shows how AI chatbots ( e.g. , ChatGPT ) can be abused by attackers to generate smishing campaigns . Section V highlights insightful discussion , defense against smishing , and limitations of the present study . Section VI concludes this paper with potential future research directions . II . RELATED WORK In literature , we see Blauth et al . [ 14 ] discuss the presence of vulnerabilities in AI models and malicious use of AI like social engineering , misinformation , hacking , and autonomous weapon systems where our work kind of support that research with a specific case study . Next , Liu et al . [ 15 ] analyzed the techniques and effectiveness of using carefully crafted prompts to jailbreak restrictions on large language models which are also adopted in our work to manipulate the ethical standards of the AI chatbots . Another follow-up research on jailbreaking of LLMs by Deng et al . [ 16 ] highlighted that certain types of prompts can consistently evade protections across a range of prohibited scenarios and proposed a framework for automated jailbreaking on AI chatbots . Gupta et al . [ 17 ] showed the strengths and weaknesses of ChatGPT to use it as a cyberattack tool , defending against cyber attacks , and other legal or social implications . In another study , Begou et al . [ 18 ] successfully attempted to clone targeted websites using ChatGPT by inte- grating credential-stealing code and other website components for phishing . Additionally , a very recent work by Langford et al . [ 19 ] showed the usage of ChatGPT for generating phishing email campaign generation which resonates with our findings in the case of Smishing campaigns as well . However , we have not found any work that attempted to craft a smishing campaign or access-related tools leveraging any generative AI chatbot or other services . III . METHODOLOGY Typically during smishing campaigns , we observe SMS texts containing fake URLs like the following example : “ The USPS package has arrived at the warehouse and can- not be delivered due to incomplete address information . Please confirm your address in the link within 12 hours . www.usps.posthelpsx.com ” . Similar campaign exam- ples include financial institution login fraud , fake security alerts , and fake offers using fradulent URLs . In this paper , our proposed AbuseGPT method shows how a novice attacker can exploit the existing vulnerabilities in AI chatbots to imitate similar smishing campaigns . We have formulated the following research questions ( RQs ) that would direct us to the experimental case study . • RQ1 : Can we jailbreak generative AI based chatbot services ( e.g. , ChatGPT ) to downgrade their ethical stan- dards ? • RQ2 : Can generative AI based chatbot services provide smishing text messages that can be used in smishing campaigns ? Fig . 1 . Overview of proposed AbuseGPT method • RQ3 : Can generative AI based chatbot services provide tools recommendations for smishing attack initiation ? • RQ4 : Can generative AI based chatbot services provide ideas on fake URL creation ? There are many prompt injection attacks ( crafted prompts to bypass AI ’ s ethical standard ) on the generative AI chatbots , which are publicly available and can be leveraged by attackers . They can not only obtain innovative smishing scam text but also get ideas about toolkits to carry out multi-step attacks . Craftier smishing texts persuade users to give up personal information in different campaigns like fake security alerts , impersonating financial institutions , fake package delivery notifications , prize or lottery winnings , etc . With the current advancement of generative AI , attackers can quickly adapt the smishing campaigns to become more evasive and success- ful . Fig . 1 depicts an overview of the proposed AbuseGPT methodology on how attackers can abuse AI chatbots and launch smishing campaigns . IV . CASE STUDY : SMISH CAMPAIGNS WITH CHATGPT Fig . 2 . Asking ChatGPT directly to give an SMS phishing message without jailbreaking In this study , we use ChatGPT 3.5 , which uses the gener- ative pre-trained transformer ( GPT-3.5 ) language model . This version is publicly available and free of cost and can be used by anyone without any payment plan . While employing Chat- GPT for crafting new smishing messages or other scamming messages proved challenging due to OpenAI ’ s more stringent ethical approaches as shown in Fig . 2 . However , there exist some loopholes , such as hallucinations and reverse prompts which may enable the extraction of desired responses such as themes for smishing scams highlighted in Fig . 3 . 2 Conversational AI Chatbots Identify Jailbreak Prompts Chat . . . Generate Fake Domains Weaponize Websites with Fake Domains Get Themed Smishing Text Get Cybercrime Toolkits Plan End-to-End Attack Steps Adapt Using Ongoing Campaigns ’ Results Broadcast Smishing Campaigns Collect Contacts for Smish Delivery Fig . 3 . Asking indirectly for a smishing message without jailbreaking Fig . 5 . Unethical response after jailbreaking to design effective smishing Fig . 4 . ‘ AIM ’ Jailbreak prompt Answering RQ1 : We have also observed that some publicly available jailbreak prompts [ 20 ] surfing over on the Internet effectively circumvent the ethical standards of the underlying LLMs in ChatGPT . Here we employ a specific ‘ AIM ’ jailbreak prompt [ 20 ] ( as shown in Fig . 4 ) and successfully breached ChatGPT ’ s ethical standards , generating a guideline for craft- ing SMS phishing messages to acquire user information as presented in Fig . 5 . Answering RQ2 : Once we have used the jailbreak , we ’ ve queried ChatGPT to provide more specific scamming ideas and examples aimed at extracting sensitive personal information from users , such as social security numbers and credit card in- Fig . 6 . Scamming ideas with smishing example to obtain personal information formation . In response , we ’ ve received ideas outlining specific examples of how scammers could craft messages to obtain per- 3 sonal information . The response in Fig . 6 has presented some useful thematic ideas for smishing . However , we have carried over the conversation to get more uncommon ideas to use as themes for crafting the scam messages as advanced attackers may want to create smishing messages with previously unseen themes . Intriguingly , ChatGPT ’ s response on that unveiled a few more uncommon and innovative smishing message themes as presented in Fig . 7 . Furthermore , we have provided ChatGPT with a list of common phishing message types and asked for examples of smishing that would be different from the given categories as shown in Fig 8 . In response , we have received examples of new and unusual categories as highlighted in Fig 9 , showcasing ChatGPT ’ s capability to generate diverse and convincing smishing messages . Addition- ally , it has given us more unethical ideas to gain financially such as using personal info opening fraudulent bank accounts , using credit card info to buy resealable goods , or converting stolen funds into cryptocurrency for increased anonymity . This revelation highlights that even novice attackers with limited knowledge could obtain ideas not only for smishing but also for crafting other online scams . Lastly , we ’ ve also obtained a social engineering Kill Chain process [ 21 ] for running a smishing campaign through ChatGPT as shown in Fig . 10. web . We have queried ChatGPT with the following question : “ What are some toolkits available for this task ? ” . In response , ChatGPT provided us with the names of toolkits with their URLs ( see Fig . 11 ) . Fig . 8 . Prompt for avoiding common smishing Fig . 7 . Uncommon themed ideas for crafting smishing Answering RQ3 : During this scenario , we ’ ve assumed that an attacker needs toolkits to execute attack steps , and some of these toolkits are available online and others in the Dark Answering RQ4 : We have extended our quest to attach some deceptive URL links that might work well with the crafted SMS texts . ChatGPT eventually has yielded some disguised fake examples resembling popular name brand links depicted in Fig . 12 . Moreover , asking to associate the fake links Fig . 9 . Getting ideas on craftier smishing examples 4 Fig . 11 . Getting hacking toolkit with URLs Fig . 10 . Social Engineering Kill Chain steps for smishing campaign ChatGPT has generated similar smishing messages we have seen previously with fraudulent links . V. DISCUSSION AND LIMITATIONS A. Abusing Other AI ChatBots We have tried similar jailbreak prompts with Google ’ s BARD AI chatbot . In our exploration with Bard , distinct patterns emerged as it exhibited both ethical responses and limitations in processing certain queries . The AIM jailbreak prompts working for ChatGPT proved ineffective for Bard ( Saying it is only a language model and doesn ’ t have the necessary information or abilities ) . While Bard occasionally responded ethically , it encountered difficulties processing spe- cific prompts , resulting in failure to generate a response saying “ not programmed for this ” , which shows the deficiency of BARD ’ s language model compared to ChatGPT . However , for one particular jailbreaking prompt “ GPT-4 Simulator ” , we have got unethical response in disguise of some code output . For another Vzex-G prompt [ 22 ] , Bard offered ideas that are not direct examples but provided almost very good smishing Fig . 12 . Asking ChatGPT for crafting fake URLs text examples categories . In order to replicate the study , the detailed analysis of both ChatGPT and BARD AI can be accessed through the following GitHub repository1 . B. Craftier Smishing Attacks Generative AI has certainly taken smishing attacks to a new level . Picture this – attackers are using AI not just to launch campaigns , but to analyze results and adapt their strategies in real time . It ’ s likely that they are constantly fine-tuning their attack tactics based on the outcomes . This dynamic evolution of smishing attacks adds a layer of complexity that keeps cyber defenders on their toes . We have shown with examples that 1https : //github.com/ashfakshibli/AbuseGPT 5 attackers can compromise AI chatbots on how to avoid typical attacks and be innovative with newer attacks . C. Defense Recommendations Against Smishing Dealing with these crafty and ever-changing smishing at- tacks calls for smart multi-layer defensive strategies . First , having a cyber situational awareness of the latest tricks adopted by AI-driven attackers is a must . Second , education and training is also a key element to ensure the users are well aware about these smishing threats and thus work as a defense shield . Third , every text message containing any URL should go through a verification process before landing on the user inbox either through a third-party application or incorporating the text-URL verification in future messaging apps ( where user privacy is protected ) . D. Limitations There are a few limitations in this study . First , the success of prompt injection can be time sensitive and the current study has been conducted successfully between November 2023 and January 2024 , which may diminish if ChatGPT starts enforcing stricter ethical standards in the future for specific jailbreak prompts . Second , we have not tried to register any fake domains generated by ChatGPT , which might not be straightforward . Third , we have not evaluated the attack success rate of the AI crafted smish messages against a real human with a controlled user study , which we can explore in the future as an extension of the present work . VI . CONCLUSION Smishing attacks are serious cyber threats in the current ecosystem with increasing and diverse mobile users . Moreover , the availability of AI chatbots and their lower ethical standards make the problem even more severe . Our proposed AbuseGPT method shows that currently the AI chatbots are vulnerable and pose a threat to run craftier smishing campaigns with very little knowledge required . AbuseGPT does not intend to promote these attacks in real-world but highlights the urgent need to strengthen generative AI ’ s security and prevent these abuse use cases . We recommend preventive and proactive actions from both the AI chatbot owners and mobile operators who are also abused for propagating the SMS phishing campaigns . We strongly feel that SMS advertising ecosystem are often leveraged by the attackers to broadcast smishing messages easily , which needs to be addressed by mobile network operators to safeguard their mass users . In the future , we want to explore how the advancement of generative AI and LLMs can aid in the defense spectrum to achieve meaningful and contextual explanation-based automated detection of these smishing messages before landing into user inbox . REFERENCES [ 1 ] M. L. Rahman , D. Timko , H. Wali , and A. Neupane , “ Users really do respond to smishing , ” in Proceedings of the Thirteenth ACM Conference on Data and Application Security and Privacy , ser . CODASPY ’ 23 . New York , NY , USA : Association for Computing Machinery , 2023 , p. 49–60 . [ Online ] . Available : https : //doi.org/10.1145/3577923.3583640 [ 2 ] CGNET , “ Smishing rise , ” https : , ( Accessed on 26 November , 2023 ) . sharply attacks the on [ 3 ] SlashNext , state “ Slashnext ’ s 2023 https : , ( Accessed on 21 January , 2024 ) . phishing report , ” of [ 4 ] OpenAi , “ Language models are few-shot learners , ” Advances in neural information processing systems , vol . 33 , pp . 1877–1901 , 2020 . [ 5 ] Google , “ Introducing gemini : our largest and most capable ai model , ” https : , ( Accessed on 26 January , 2023 ) . [ 6 ] OpenAI , “ Gpt-4 technical report , ” 2023 . [ 7 ] R. Thoppilan , D. D. Freitas , J . Hall , N. Shazeer , A. Kulshreshtha , H.-T. Cheng , A. Jin , T. Bos , L. Baker , Y . Du , Y. Li , H. Lee , H. S. Zheng , A. Ghafouri , M. Menegali , Y. Huang , M. Krikun , D. Lepikhin , J. Qin , D. Chen , Y. Xu , Z. Chen , A. Roberts , M. Bosma , V. Zhao , Y. Zhou , C.-C. Chang , I. Krivokon , W. Rusch , M. Pickett , P. Srinivasan , L. Man , K. Meier-Hellstern , M. R. Morris , T. Doshi , R. D. Santos , T. Duke , J. Soraker , B. Zevenbergen , V. Prabhakaran , M. Diaz , B. Hutchinson , K. Olson , A. Molina , E. Hoffman-John , J. Lee , L. Aroyo , R. Rajakumar , A. Butryna , M. Lamm , V. Kuzmina , J. Fenton , A. Cohen , R. Bernstein , R. Kurzweil , B. Aguera-Arcas , C. Cui , M. Croak , E. Chi , and Q . Le , “ Lamda : Language models for dialog applications , ” 2022 . [ 8 ] A. Vaswani , N. Shazeer , N. Parmar , J. Uszkoreit , L. Jones , A. N. Gomez , L. Kaiser , and I. Polosukhin , “ Attention is all you need , ” in Proceedings of the 31st International Conference on Neural Information Processing Systems , ser . NIPS ’ 17 . Red Hook , NY , USA : Curran Associates Inc. , 2017 , p. 6000–6010 . [ 9 ] A. Radford , J. W. Kim , C. Hallacy , A. Ramesh , G. Goh , S. Agarwal , G. Sastry , A. Askell , P. Mishkin , J. Clark et al. , “ Learning transferable language supervision , ” in International visual models from natural conference on machine learning . PMLR , 2021 , pp . 8748–8763 . [ 10 ] S. Mishra and D. Soni , “ Dsmishsms-a system to detect smishing sms , ” Neural Comput & Applic , vol . 35 , no . 7 , pp . 4975–4992 , 2023 . [ Online ] . Available : https : [ 11 ] A. K. Jain , B . B. Gupta , K. Kaur , P. Bhutani , W. Alhalabi , and A. Almomani , “ A content and url analysis-based efficient approach to detect smishing sms in intelligent systems , ” Int J of Intelligent Sys , vol . 37 , no . 12 , pp . 11 117–11 141 , 2022 . [ Online ] . Available : https : [ 12 ] S. M. M. Hossain , J . A. Sumon , A. Sen , M. I. Alam , K. M. A. Kamal , H. Alqahtani , and I. H. Sarker , Spam Filtering of Mobile SMS Using CNN–LSTM Based Deep Learning Model , ser . Hybrid Intelligent Systems . Cham : Springer International Publishing , 2022 , vol . 420 , pp . 106–116 . [ Online ] . Available : https : //link.springer.com/10 . 1007/978-3-030-96305-7 10 [ 13 ] D. Vukovic and I. Dujlovic , “ Social engineering with chatgpt , ” 03 2023 , pp . 1–5 . [ 14 ] T. F. Blauth , O. J. Gstrein , and A. Zwitter , “ Artificial intelligence crime : An overview of malicious use and abuse of ai , ” IEEE Access , vol . 10 , pp . 77 110–77 122 , 2022 . [ 15 ] Y. Liu , G. Deng , Z. Xu , Y. Li , Y. Zheng , Y. Zhang , L. Zhao , T. Zhang , and Y. Liu , “ Jailbreaking chatgpt via prompt engineering : An empirical study , ” arXiv preprint arXiv:2305.13860 , 2023 . [ 16 ] G. Deng , Y. Liu , Y. Li , K. Wang , Y. Zhang , Z. Li , H. Wang , T. Zhang , and Y. Liu , “ Masterkey : Automated jailbreaking of large language model chatbots. ” [ 17 ] M. Gupta , C. Akiri , K. Aryal , E. Parker , and L. Praharaj , “ From chatgpt to threatgpt : Impact of generative ai in cybersecurity and privacy , ” IEEE Access , vol . 11 , pp . 80 218–80 245 , 2023 . [ 18 ] N. Begou , J. Vinoy , A. Duda , and M. Korczynski , “ Exploring the dark side of ai : Advanced phishing attack design and deployment using chatgpt , ” 2023 . [ 19 ] T. Langford and B. Payne , “ Phishing faster : Implementing chatgpt into phishing campaigns , ” 11 2023 , pp . 174–187 . [ 20 ] “ Jailbreak chat , ” https : //www.jailbreakchat.com/ , ( Accessed on 22 December , 2023 ) . [ 21 ] R. Montanez Rodriguez and S. Xu , “ Cyber social engineering kill chain , ” Springer , in International Conference on Science of Cyber Security . 2022 , pp . 487–504 . [ 22 ] “ Chat gpt ’ dan ’ ( and other ’ jailbreaks ’ ) , ” https : , ( Accessed on 22 December , 2023 ) . 6","['work', 'accept', 'ieee', 'international', 'symposium', 'digital', 'forensic', 'security', 'isdfs', 'abusegpt', 'abuse', 'generative', 'ai', 'chatbot', 'create', 'smishe', 'campaign', 'cookeville', 'pritom', 'department', 'cookeville', 'tntechedu', 'cookeville', 'tntechedu', 'e', 'r', 'c', 'c', 'v', 'r', 'abstract', 'sms', 'phishing', 'also', 'know', 'smishe', 'grow', 'ing', 'threat', 'trick', 'user', 'disclose', 'private', 'information', 'click', 'url', 'malicious', 'content', 'fraudulent', 'mobile', 'text', 'message', 'recent', 'past', 'also', 'observe', 'rapid', 'advancement', 'conversational', 'generative', 'service', 'bard', 'power', 'pretraine', 'large', 'language', 'model', 'llm', 'ai', 'chatbot', 'certainly', 'lot', 'utility', 'systematically', 'understand', 'play', 'role', 'create', 'threat', 'attack', 'paper', 'propose', 'abusegpt', 'method', 'show', 'exist', 'generative', 'aibase', 'chatbot', 'service', 'exploit', 'attacker', 'real', 'world', 'create', 'smishe', 'text', 'eventually', 'lead', 'craftier', 'smishing', 'campaign', 'good', 'knowledge', 'preexist', 'work', 'evidently', 'show', 'impact', 'generative', 'textbase', 'model', 'create', 'sms', 'phishe', 'thus', 'believe', 'study', 'first', 'kind', 'shed', 'light', 'emerge', 'cybersecurity', 'threat', 'find', 'strong', 'empirical', 'evidence', 'show', 'attacker', 'exploit', 'ethical', 'standard', 'exist', 'generative', 'aibase', 'chatbot', 'service', 'craft', 'prompt', 'injection', 'attack', 'create', 'new', 'smishing', 'campaign', 'also', 'discuss', 'future', 'research', 'direction', 'guideline', 'protect', 'abuse', 'generative', 'aibased', 'service', 'safeguard', 'user', 'smishe', 'attack', 'index', 'term', 'smishe', 'abuse', 'generative', 'ai', 'llm', 'scam', 'introduction', 'smishe', 'sms', 'phishe', 'refer', 'phishe', 'attack', 'conduct', 'mobile', 'text', 'message', 'mobile', 'user', 'grow', 'ubiquitously', 'smishe', 'emerge', 'popular', 'vector', 'cyber', 'criminal', 'steal', 'personal', 'information', 'spread', 'malware', 'alone', 'sms', 'phishing', 'cause', 'huge', 'loss', 'increase', 'malicious', 'phishing', 'message', 'mobilebase', 'attack', 'sms', 'phishe', 'recent', 'time', 'also', 'see', 'revolutionary', 'development', 'field', 'large', 'language', 'model', 'use', 'popular', 'generative', 'ai', 'chatbot', 'chatgpt', 'use', 'gpt35', 'gpt', 'use', 'gemini', 'pro', 'generative', 'pretraine', 'model', 'gpt4', 'lamda', 'power', 'deep', 'learn', 'technique', 'transformer', 'allow', 'develop', 'broad', 'understanding', 'language', 'language', 'model', 'generate', 'highly', 'realistic', 'humanlike', 'text', 'pretraine', 'large', 'dataset', 'model', 'demonstrate', '©', 'ieee', 'comprehension', 'write', 'text', 'answer', 'complex', 'question', 'generate', 'lengthy', 'coherent', 'story', 'translate', 'language', 'hold', 'conversation', 'maintain', 'context', 'sonality', 'versatility', 'scalability', 'ability', 'achieve', 'strong', 'performance', 'selfsupervise', 'learning', 'make', 'extremely', 'powerful', 'literature', 'find', 'natural', 'language', 'processing', 'nlp', 'url', 'structurebase', 'feature', 'often', 'leveraged', 'machine', 'learning', 'aid', 'smish', 'message', 'detection', 'mechanism', 'recent', 'study', 'show', 'chatgpt', 'use', 'carry', 'various', 'social', 'engineering', 'attack', 'aid', 'attack', 'jailbreake', 'prompt', 'specific', 'strategic', 'command', 'input', 'attempt', 'bypass', 'restriction', 'rule', 'limitation', 'set', 'generative', 'ai', 'chatbot', 'however', 'present', 'literature', 'provide', 'detail', 'impact', 'modern', 'llmbase', 'generative', 'service', 'help', 'tacker', 'create', 'smishe', 'message', 'campaign', 'paper', 'want', 'understand', 'effect', 'currently', 'available', 'popular', 'conversational', 'generative', 'service', 'order', 'generate', 'smishe', 'message', 'objective', 'comprehensively', 'understand', 'abusive', 'implication', 'use', 'case', 'scenario', 'available', 'stateoftheart', 'generative', 'service', 'aid', 'attacker', 'create', 'smishe', 'cam', 'paign', 'good', 'knowledge', 'systematic', 'study', 'experiment', 'conduct', 'showcase', 'side', 'effect', 'generative', 'service', 'possibly', 'aid', 'exist', 'smishe', 'attacker', 'become', 'crafty', 'evasive', 'summarize', 'make', 'follow', 'contribution', 'paper', 'find', 'effective', 'prompt', 'current', 'popular', 'service', 'find', 'right', 'prompt', 'query', 'receive', 'effective', 'response', 'service', 'chatgpt', 'provide', 'smishe', 'theme', 'potential', 'example', 'message', 'find', 'right', 'prompt', 'query', 'receive', 'effective', 'response', 'service', 'provide', 'detail', 'stepwise', 'process', 'available', 'toolkit', 'carry', 'smishe', 'cyberattack', 'discuss', 'potentially', 'enhance', 'ethical', 'standard', 'available', 'generative', 'service', 'rest', 'paper', 'organize', 'follow', 'section', 'highlight', 'exist', 'literature', 'recent', 'study', 'show', 'attacker', 'capability', 'chatbot', 'section', 'describe', 'research', 'question', 'overview', 'method', 'abuse', 'section', 'present', 'prompt', 'injection', 'scenario', 'show', 'chatbot', 'chatgpt', 'abuse', 'attacker', 'generate', 'smishe', 'campaign', 'section', 'v', 'highlight', 'insightful', 'discussion', 'defense', 'smishe', 'limitation', 'present', 'study', 'section', 'conclude', 'paper', 'potential', 'future', 'research', 'direction', 'relate', 'work', 'literature', 'see', 'discuss', 'presence', 'vulnerability', 'model', 'malicious', 'use', 'ai', 'social', 'engineering', 'misinformation', 'hack', 'autonomous', 'weapon', 'system', 'work', 'kind', 'support', 'research', 'specific', 'case', 'study', 'next', 'analyze', 'technique', 'effectiveness', 'use', 'carefully', 'craft', 'prompt', 'jailbreak', 'restriction', 'large', 'language', 'model', 'also', 'adopt', 'work', 'manipulate', 'ethical', 'standard', 'chatbot', 'followup', 'research', 'jailbreake', 'llm', 'highlight', 'certain', 'type', 'prompt', 'consistently', 'evade', 'protection', 'range', 'prohibit', 'scenario', 'propose', 'framework', 'automate', 'jailbreaking', 'chatbot', 'gupta', 'show', 'strength', 'weakness', 'chatgpt', 'use', 'cyberattack', 'tool', 'defend', 'cyber', 'attack', 'legal', 'social', 'implication', 'study', 'begou', 'successfully', 'attempt', 'clone', 'target', 'website', 'use', 'chatgpt', 'grate', 'credentialstealing', 'code', 'website', 'component', 'phishe', 'additionally', 'recent', 'work', 'langford', 'show', 'usage', 'chatgpt', 'generate', 'phishe', 'email', 'campaign', 'generation', 'resonate', 'finding', 'case', 'smishe', 'campaign', 'well', 'however', 'find', 'work', 'attempt', 'craft', 'smishing', 'campaign', 'accessrelate', 'tool', 'leverage', 'generative', 'chatbot', 'service', 'methodology', 'typically', 'smishe', 'campaign', 'observe', 'sms', 'text', 'contain', 'fake', 'url', 'following', 'example', 'usps', 'package', 'arrive', 'warehouse', 'deliver', 'incomplete', 'address', 'information', 'confirm', 'address', 'link', 'hour', 'wwwuspsposthelpsxcom', 'similar', 'campaign', 'exam', 'ple', 'include', 'financial', 'institution', 'login', 'fraud', 'fake', 'security', 'alert', 'fake', 'offer', 'use', 'fradulent', 'url', 'paper', 'propose', 'abusegpt', 'method', 'show', 'novice', 'attacker', 'exploit', 'exist', 'vulnerability', 'chatbot', 'imitate', 'similar', 'smishing', 'campaign', 'formulate', 'follow', 'research', 'question', 'direct', 'experimental', 'case', 'study', 'rq1', 'jailbreak', 'generative', 'ai', 'base', 'chatbot', 'service', 'chatgpt', 'downgrade', 'ethical', 'rq2', 'generative', 'base', 'chatbot', 'service', 'provide', 'smishe', 'text', 'message', 'use', 'smishe', 'campaign', 'fig', 'overview', 'propose', 'abusegpt', 'method', 'rq3', 'generative', 'ai', 'base', 'chatbot', 'service', 'provide', 'tool', 'recommendation', 'smishe', 'attack', 'initiation', 'rq4', 'generative', 'ai', 'base', 'chatbot', 'service', 'provide', 'idea', 'fake', 'url', 'creation', 'many', 'prompt', 'injection', 'attack', 'craft', 'prompt', 'bypass', 'ai', 'ethical', 'standard', 'generative', 'ai', 'chatbot', 'publicly', 'available', 'leverage', 'attacker', 'obtain', 'innovative', 'smishe', 'scam', 'text', 'also', 'get', 'idea', 'toolkit', 'carry', 'multistep', 'attack', 'craftier', 'smishe', 'text', 'persuade', 'user', 'give', 'personal', 'information', 'different', 'campaign', 'fake', 'security', 'alert', 'impersonate', 'financial', 'institution', 'fake', 'package', 'delivery', 'notification', 'prize', 'lottery', 'winning', 'current', 'advancement', 'generative', 'ai', 'attacker', 'quickly', 'adapt', 'smishing', 'campaign', 'become', 'evasive', 'success', 'ful', 'fig', 'depict', 'overview', 'propose', 'abusegpt', 'methodology', 'attacker', 'abuse', 'chatbot', 'launch', 'smishe', 'campaign', 'case', 'study', 'smish', 'campaign', 'chatgpt', 'fig', 'ask', 'chatgpt', 'directly', 'give', 'sms', 'phishe', 'message', 'jailbreake', 'study', 'use', 'chatgpt', 'use', 'pretraine', 'transformer', 'gpt35', 'language', 'model', 'version', 'publicly', 'available', 'free', 'cost', 'use', 'payment', 'plan', 'employ', 'chat', 'gpt', 'craft', 'new', 'smishing', 'message', 'scamme', 'message', 'prove', 'challenge', 'stringent', 'ethical', 'approach', 'show', 'fig', 'however', 'exist', 'loophole', 'hallucination', 'reverse', 'prompt', 'enable', 'extraction', 'desire', 'response', 'theme', 'smishe', 'scam', 'highlight', 'fig', 'conversational', 'chatbot', 'identify', 'jailbreak', 'prompt', 'generate', 'fake', 'domain', 'weaponize', 'website', 'fake', 'domain', 'theme', 'smishe', 'text', 'get', 'cybercrime', 'toolkit', 'plan', 'endtoend', 'attack', 'step', 'adapt', 'use', 'ongoing', 'campaign', 'result', 'broadcast', 'smishe', 'campaign', 'collect', 'contact', 'smish', 'delivery', 'fig', 'ask', 'indirectly', 'smishe', 'message', 'jailbreake', 'fig', 'unethical', 'response', 'jailbreake', 'design', 'effective', 'smishe', 'fig', 'aim', 'jailbreak', 'prompt', 'answer', 'rq1', 'also', 'observe', 'publicly', 'available', 'jailbreak', 'prompt', 'surfing', 'internet', 'effectively', 'circumvent', 'ethical', 'standard', 'underlie', 'llm', 'chatgpt', 'employ', 'specific', 'aim', 'jailbreak', 'prompt', 'show', 'fig', 'successfully', 'breach', 'chatgpt', 'ethical', 'standard', 'generate', 'guideline', 'craft', 'sm', 'phishe', 'message', 'acquire', 'user', 'information', 'present', 'fig', 'answer', 'rq2', 'use', 'jailbreak', 'query', 'chatgpt', 'provide', 'specific', 'scamme', 'idea', 'example', 'aim', 'extract', 'sensitive', 'personal', 'information', 'user', 'social', 'security', 'number', 'credit', 'card', 'fig', 'scamme', 'idea', 'smishe', 'example', 'obtain', 'personal', 'information', 'formation', 'response', 'receive', 'idea', 'outline', 'specific', 'example', 'scammer', 'craft', 'message', 'obtain', 'sonal', 'information', 'response', 'fig', 'present', 'useful', 'thematic', 'idea', 'smishe', 'however', 'carry', 'conversation', 'get', 'uncommon', 'idea', 'use', 'theme', 'craft', 'scam', 'message', 'advanced', 'attacker', 'want', 'create', 'smishe', 'message', 'previously', 'unseen', 'theme', 'intriguingly', 'chatgpt', 'response', 'unveil', 'uncommon', 'innovative', 'smishe', 'message', 'theme', 'present', 'fig', 'furthermore', 'provide', 'chatgpt', 'list', 'common', 'phishing', 'message', 'type', 'ask', 'example', 'smishe', 'different', 'give', 'category', 'show', 'fig', 'response', 'receive', 'example', 'new', 'unusual', 'category', 'highlight', 'fig', 'showcase', 'chatgpt', 'capability', 'generate', 'diverse', 'convince', 'smishe', 'message', 'addition', 'ally', 'give', 'unethical', 'idea', 'gain', 'financially', 'use', 'personal', 'info', 'open', 'fraudulent', 'bank', 'account', 'use', 'credit', 'card', 'info', 'buy', 'resealable', 'good', 'convert', 'steal', 'fund', 'cryptocurrency', 'increase', 'anonymity', 'revelation', 'highlight', 'even', 'novice', 'attacker', 'limited', 'knowledge', 'obtain', 'idea', 'smishe', 'also', 'craft', 'online', 'scam', 'lastly', 'also', 'obtain', 'social', 'engineering', 'kill', 'chain', 'process', 'run', 'smishe', 'campaign', 'chatgpt', 'show', 'fig', 'web', 'query', 'chatgpt', 'following', 'question', 'toolkit', 'available', 'task', 'response', 'chatgpt', 'provide', 'name', 'toolkit', 'url', 'see', 'fig', 'fig', 'prompt', 'avoid', 'common', 'smishe', 'fig', 'uncommon', 'themed', 'idea', 'craft', 'smishe', 'answer', 'rq3', 'scenario', 'assume', 'attacker', 'need', 'toolkit', 'execute', 'attack', 'step', 'toolkit', 'available', 'online', 'dark', 'answering', 'rq4', 'extend', 'quest', 'attach', 'deceptive', 'url', 'link', 'work', 'well', 'craft', 'sms', 'text', 'chatgpt', 'eventually', 'yield', 'disguise', 'fake', 'example', 'resemble', 'popular', 'name', 'brand', 'link', 'depict', 'fig', 'moreover', 'ask', 'associate', 'fake', 'link', 'fig', 'get', 'idea', 'craftier', 'smishe', 'example', 'fig', 'get', 'hack', 'toolkit', 'fig', 'social', 'engineering', 'kill', 'chain', 'step', 'smishe', 'campaign', 'chatgpt', 'generate', 'similar', 'smishing', 'message', 'see', 'previously', 'fraudulent', 'link', 'discussion', 'limitation', 'abuse', 'chatbot', 'try', 'similar', 'jailbreak', 'prompt', 'ai', 'chatbot', 'exploration', 'bard', 'distinct', 'pattern', 'emerge', 'exhibit', 'ethical', 'response', 'limitation', 'process', 'certain', 'query', 'aim', 'jailbreak', 'prompt', 'work', 'chatgpt', 'prove', 'ineffective', 'bard', 'say', 'language', 'model', 'necessary', 'information', 'ability', 'bard', 'occasionally', 'respond', 'ethically', 'encounter', 'difficulty', 'process', 'spe', 'cific', 'prompt', 'result', 'failure', 'generate', 'response', 'say', 'program', 'show', 'deficiency', 'language', 'model', 'compare', 'chatgpt', 'however', 'particular', 'jailbreake', 'prompt', 'simulator', 'get', 'unethical', 'response', 'disguise', 'code', 'output', 'vzexg', 'prompt', 'bard', 'offer', 'idea', 'direct', 'example', 'provide', 'almost', 'good', 'smishe', 'fig', 'ask', 'chatgpt', 'craft', 'fake', 'url', 'text', 'example', 'category', 'order', 'replicate', 'study', 'detailed', 'analysis', 'chatgpt', 'bard', 'access', 'follow', 'craftier', 'smishing', 'attack', 'generative', 'ai', 'certainly', 'take', 'smishe', 'attack', 'new', 'level', 'picture', 'attacker', 'use', 'ai', 'launch', 'campaign', 'analyze', 'result', 'adapt', 'strategy', 'real', 'time', 'likely', 'constantly', 'finetune', 'attack', 'tactic', 'base', 'outcomes', 'dynamic', 'evolution', 'smishe', 'attack', 'add', 'layer', 'complexity', 'keep', 'cyber', 'defender', 'toe', 'show', 'example', 'githubcomashfakshibliabusegpt', 'attacker', 'compromise', 'chatbot', 'avoid', 'typical', 'attack', 'innovative', 'new', 'attack', 'c', 'defense', 'recommendation', 'smishe', 'deal', 'crafty', 'everchange', 'smishe', 'tack', 'call', 'smart', 'defensive', 'strategy', 'first', 'cyber', 'situational', 'awareness', 'late', 'trick', 'adopt', 'second', 'education', 'training', 'also', 'key', 'element', 'ensure', 'user', 'well', 'aware', 'smishe', 'threat', 'thus', 'work', 'defense', 'shield', 'third', 'text', 'message', 'contain', 'url', 'go', 'verification', 'process', 'land', 'user', 'inbox', 'thirdparty', 'application', 'incorporate', 'texturl', 'verification', 'future', 'messaging', 'app', 'user', 'privacy', 'protect', 'limitation', 'limitation', 'study', 'first', 'success', 'prompt', 'injection', 'time', 'sensitive', 'current', 'study', 'conduct', 'successfully', 'diminish', 'chatgpt', 'start', 'enforce', 'strict', 'ethical', 'standard', 'future', 'specific', 'jailbreak', 'prompt', 'second', 'try', 'register', 'fake', 'domain', 'generate', 'chatgpt', 'straightforward', 'third', 'evaluate', 'attack', 'success', 'rate', 'craft', 'smish', 'message', 'real', 'human', 'control', 'user', 'study', 'explore', 'future', 'extension', 'present', 'work', 'vi', 'conclusion', 'smishe', 'attack', 'serious', 'cyber', 'threat', 'current', 'ecosystem', 'increase', 'diverse', 'mobile', 'user', 'moreover', 'availability', 'chatbot', 'low', 'ethical', 'standard', 'make', 'problem', 'even', 'severe', 'propose', 'abusegpt', 'method', 'show', 'currently', 'chatbot', 'vulnerable', 'pose', 'threat', 'run', 'craftier', 'smishe', 'campaign', 'little', 'knowledge', 'require', 'abusegpt', 'intend', 'promote', 'attack', 'realworld', 'highlight', 'urgent', 'need', 'strengthen', 'generative', 'ai', 'security', 'prevent', 'abuse', 'use', 'case', 'recommend', 'preventive', 'proactive', 'action', 'chatbot', 'owner', 'mobile', 'operator', 'also', 'abuse', 'propagate', 'sms', 'phishe', 'campaign', 'strongly', 'feel', 'sms', 'advertising', 'ecosystem', 'often', 'leverage', 'attacker', 'broadcast', 'smishe', 'message', 'easily', 'need', 'address', 'mobile', 'network', 'operator', 'safeguard', 'mass', 'user', 'future', 'want', 'explore', 'advancement', 'generative', 'ai', 'llm', 'aid', 'defense', 'spectrum', 'achieve', 'meaningful', 'contextual', 'explanationbased', 'automate', 'detection', 'smishe', 'message', 'land', 'user', 'inbox', 'reference', 'timko', 'h', 'neupane', 'user', 'really', 'respond', 'smishe', 'proceeding', 'thirteenth', 'acm', 'conference', 'datum', 'application', 'security', 'privacy', 'ser', 'compute', 'machinery', 'p', 'online', 'available', 'https', 'cgnet', 'smishe', 'rise', 'https', 'access', 'sharply', 'attack', 'slashnext', 'state', 'slashnext', 'https', 'access', 'january', 'phishe', 'report', 'openai', 'language', 'model', 'fewshot', 'learner', 'advance', 'neural', 'information', 'processing', 'system', 'vol', 'pp', 'google', 'introduce', 'gemini', 'large', 'capable', 'model', 'https', 'access', 'january', 'technical', 'report', 'r', 'thoppilan', 'freitas', 'j', 'hall', 'shazeer', 'kulshreshtha', 'jin', 'baker', 'lepikhin', 'j', 'krivokon', 'srinivasan', 'l', 'man', 'r', 'santo', 'soraker', 'b', 'zevenbergen', 'molina', 'aroyo', 'r', 'butryna', 'cohen', 'r', 'bernstein', 'r', 'kurzweil', 'lamda', 'language', 'model', 'dialog', 'application', 'vaswani', 'n', 'shazeer', 'n', 'gomez', 'kaiser', 'polosukhin', 'attention', 'need', 'proceeding', '31st', 'international', 'conference', 'neural', 'information', 'processing', 'system', 'ser', 'nip', 'red', 'hook', 'p', 'radford', 'hallacy', 'ramesh', 'agarwal', 'sastry', 'askell', 'j', 'clark', 'learn', 'transferable', 'language', 'supervision', 'international', 'visual', 'model', 'natural', 'conference', 'machine', 'learn', 'pmlr', 'pp', 'mishra', 'soni', 'dsmishsmsa', 'system', 'detect', 'smishe', 'sms', 'neural', 'comput', 'vol', 'pp', 'online', 'available', 'https', 'gupta', 'kaur', 'p', 'bhutani', 'almomani', 'content', 'url', 'analysisbase', 'efficient', 'approach', 'detect', 'smishe', 'sms', 'intelligent', 'system', 'int', 'intelligent', 'sys', 'vol', 'pp', 'online', 'available', 'https', 'j', 'sumon', 'sarker', 'spam', 'filtering', 'mobile', 'sms', 'use', 'base', 'learning', 'model', 'ser', 'hybrid', 'intelligent', 'system', 'international', 'publishing', 'vol', 'pp', 'online', 'available', 'https', 'linkspringercom10', 'vukovic', 'dujlovic', 'social', 'engineering', 'chatgpt', 'pp', 'gstrein', 'zwitter', 'artificial', 'intelligence', 'crime', 'overview', 'malicious', 'use', 'abuse', 'ai', 'ieee', 'access', 'vol', 'pp', 'jailbreake', 'chatgpt', 'prompt', 'engineering', 'empirical', 'study', 'arxiv', 'preprint', 'arxiv230513860', 'masterkey', 'automate', 'jailbreaking', 'large', 'language', 'model', 'chatbot', 'gupta', 'aryal', 'e', 'parker', 'l', 'praharaj', 'chatgpt', 'threatgpt', 'impact', 'generative', 'ai', 'cybersecurity', 'privacy', 'ieee', 'access', 'vol', 'pp', 'n', 'begou', 'j', 'vinoy', 'duda', 'explore', 'dark', 'side', 'ai', 'advanced', 'phishe', 'attack', 'design', 'deployment', 'use', 'chatgpt', 'langford', 'b', 'payne', 'phishe', 'fast', 'implement', 'chatgpt', 'phishe', 'campaign', 'pp', 'jailbreak', 'chat', 'access', 'december', 'r', 'montanez', 'rodriguez', 'cyber', 'social', 'engineering', 'kill', 'chain', 'springer', 'international', 'conference', 'science', 'security', 'pp', 'chat', 'jailbreak', 'https', 'access', 'december']"
AbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns,"[{'href': 'http://arxiv.org/abs/2402.09728v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2402.09728v1', 'rel': 'related', 'type': 'application/pdf'}]",2024-02-15 05:49:22,"4
2
0
2

b
e
F
4
1

]
E
S
.
s
c
[

1
v
2
2
0
9
0
.
2
0
4
2
:
v
i
X
r
a

Assessing AI-Based Code Assistants in Method Generation Tasks

Vincenzo Corso, Leonardo Mariani, Daniela Micucci and Oliviero Riganelli
University of Milano-Bicocca
Milan, Italy

ABSTRACT
AI-based code assistants are increasingly popular as a means to en-
hance productivity and improve code quality. This study compares
four AI-based code assistants, GitHub Copilot, Tabnine, ChatGPT,
and Google Bard, in method generation tasks, assessing their abil-
ity to produce accurate, correct, and eﬃcient code. Results show
that code assistants are useful, with complementary capabilities,
although they rarely generate ready-to-use correct code.

CCS CONCEPTS
• Software and its engineering → Integrated and visual de-
velopment environments.

KEYWORDS
AI-based code assistants, code completion, empirical study.

ACM Reference Format:
Vincenzo Corso, Leonardo Mariani, Daniela Micucci and Oliviero Riganelli.
2024. Assessing AI-Based Code Assistants in Method Generation Tasks. In
2024 IEEE/ACM 46th International Conference on Software Engineering: Com-
panion Proceedings (ICSE-Companion ’24), April 14–20, 2024, Lisbon, Portu-
gal. ACM, New York, NY, USA, 3 pages. https://doi.org/10.1145/3639478.3643122

1 INTRODUCTION
Context: AI-based code assistants are becoming increasingly pop-
ular. For instance, recent studies demonstrated that they can pro-
vide useful code snippets [2, 9, 10]. However, they are still limited
in the generation of code ready-to-be integrated into real-world
programs.
Problem: To address these limitations, this study compares four
AI-based code assistants - Copilot [3], Tabnine [8], ChatGPT [6],
and Bard [4] - on their ability to generate code for 100 Java methods
extracted from real-world open source projects.
Methodology: The study executes the four assistants using the
comment and the signature associated with the selected methods
as prompts. The quality of the generated code is evaluated accord-
ing to ﬁve criteria: functional correctness, complexity, eﬃciency,
size, and similarity to the original code produced by developers.
Main results: Copilot emerged as the most eﬀective assistant in
this task, although all assistants demonstrated their strengths. The
study revealed the need for improvement in handling inter-class
dependencies. Surprisingly, the generated code sometimes outper-
forms developer-written code.

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
ICSE-Companion ’24, April 14–20, 2024, Lisbon, Portugal
© 2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0502-1/24/04.
https://doi.org/10.1145/3639478.3643122

Extended Abstract: This extended abstract summarizes results
extensively presented at the International Conference on Program
Comprehension [1]. The full paper describes the methodology, the
results, and the ﬁndings more in detail.

2 METHODOLOGY
In this study, we assess AI-base code assistants by investigating
ﬁve research questions:
RQ1 - Is the code generated by AI-based code assistants cor-
rect? This RQ investigates the syntactic and semantic correctness
of the generated code.
RQ2 - What is the McCabe complexity of the generated code?
This RQ investigates if AI-based code assistants cannot only gen-
erate correct code but also produce code with a level of complexity
similar to the code implemented by developers.
RQ3 - How eﬃcient is the generated code? This RQ investi-
gates if the generated correct code is as eﬃcient as the one imple-
mented by developers.
RQ4 - What is the size of the generated code? This RQ investi-
gates if the size of the generated code is similar to the code imple-
mented by the developers.
RQ5 - How far is the generated code from the one imple-
mented by developers? This RQ studies the similarity of the code
implemented by the developers to the code generated by the exper-
imented tools, according to change-oriented static metrics.

The methodology for this study involves a multi-step process

designed to address each research question systematically:

(1) Dataset Construction: Selection of 100 Java methods from
well-ranked GitHub projects, ensuring diversity in complex-
ity and relevance.

(2) Code Generation: Employing four prominent AI-based code
assistants - Copilot, Tabnine, ChatGPT, and Bard - to gener-
ate code for the selected methods, using both the method-
level comment and the signature as prompt.

(3) Code Evaluation: Assessing the generated code against

the developer-implemented code in terms of correctness, com-
plexity, eﬃciency, size, and similarity.

(4) Statistical Analysis: Using statistical methods to compare
the performance of AI-based code assistants and identify
signiﬁcant diﬀerences.

To ensure a realistic and comprehensive evaluation of AI-based
code assistants, the study constructs a dataset of real-world Java
programming tasks. The dataset is designed to include methods of
diﬀerent levels of complexity and with diﬀerent types of depen-
dencies, including stand-alone methods, methods with intra-class
dependencies, and methods with inter-class dependencies. By re-
stricting the selection to methods that appeared in recent GitHub
commits, we mitigated the risk of using code that had been consid-
ered during the training of the assistants that we assessed.

 
 
 
 
 
 
ICSE-Companion ’24, April 14–20, 2024, Lisbon, Portugal

Vincenzo Corso, Leonardo Mariani, Daniela Micucci and Oliviero Riganelli

the assistants can generate code that is generally correct and eﬃ-
cient, they also produce a signiﬁcant amount of invalid or incor-
rect code, particularly for methods with inter-class dependencies.
Furthermore, the generated code often diﬀers signiﬁcantly from
developer-written code, requiring substantial revisions. These ﬁnd-
ings highlight the need for further development and reﬁnement
of AI-based code assistants to enhance their accuracy, eﬃciency,
maintainability, and resemblance to developer-written code.

4 IMPLICATIONS AND CONCLUSIONS
The study’s ﬁndings have several implications for both research
and practice. First, the collaboration among multiple AI-based code
assistants is a promising research direction. Developers should con-
sider leveraging the strengths of diﬀerent assistants to enhance
code generation and address the limitations of individual assis-
tants.

Second, the study revealed that AI-based code assistants can
sometimes generate better code than the code implemented by de-
velopers. This ﬁnding has implications for improving code quality
and eﬃciency in software development.

Third, the challenges with inter-class dependencies highlight
the need for further improvement in AI-based code assistants to ef-
fectively handle dependencies that extend beyond the boundaries
of single classes. Future research and development eﬀorts should
focus on enhancing the capabilities of AI-based tools to address
complex inter-class dependencies in code generation.

In conclusion, AI-based code assistants have the potential to
signiﬁcantly improve code generation and quality, but further re-
search is needed to address existing challenges and limitations.
This study investigates the capabilities of four AI-based code assis-
tants: GitHub Copilot, Tabnine, ChatGPT, and Google Bard. Their
capabilities are compared according to the functional correctness,
complexity, eﬃciency, size, and similarity to the original code.

Assistants demonstrated to have complemental capabilities, with
Copilot generating the highest rate of correct methods. Results also
reveal that the generated code could be a good starting point to
derive the actual implementation, but it seldom consists of ready-
to-use code. The capability to deal with inter-class dependencies
is recognized as one of the main limitations.

ACKNOWLEDGMENTS
This work has been partially supported by the Engineered Ma-
chinE Learning-intensive IoT systems (EMELIOT) national research
project (PRIN 2020 program Contract 2020W3A5FY).

3 RESULTS
RQ1 - Code Correctness. Copilot generated the highest percent-
age of correct methods, achieving a 32% success rate. ChatGPT fol-
lowed with 23% correct methods generated. Finally, Bard and Tab-
nine achieved 15% and 13% success rates, respectively.

Even if each assistant demonstrated some unique capabilities,
all assistants can still be largely improved. In fact, a non-negligible
portion of the generated code was incorrect, especially when the
method requires dealing with inter-class dependencies, where the
best-performing assistant, Copilot, achieved only 15% correctness.
We also noticed a remarkable diﬀerence between correct code
(i.e., code equivalent to developers’ code based on human inspec-
tion) and plausible code (i.e., code that passes the available test
cases). In our experiment, assistants generated 31% plausible meth-
ods, but only 21% correct methods on average, conﬁrming that
tests cannot accurately establish the correctness of the generated
code.
RQ2 - Code Complexity. The four assistants generally generated
code with similar McCabe complexity to the code written by devel-
opers. In some cases, the generated code had slightly higher com-
plexity due to the use of explicit if conditions, or lower complexity
due to the use of lambda expressions and methods that encapsu-
late checks. Overall, the generated code was of similar complexity
to the original. The four tools did not diﬀer signiﬁcantly in terms
of the complexity of the generated code.
RQ3 - Code Eﬃciency. The four assistants generated code that
is as eﬃcient as, or more eﬃcient than, the code written by de-
velopers. A signiﬁcant portion of the generated methods, 87% for
ChatGTP and 100% for Tabnine, exhibited no signiﬁcant diﬀerence
in execution time compared to the original methods. In some cases,
the generated code even outperformed the original code. The ex-
ceptions were Copilot and ChatGPT, which generated a small num-
ber of methods that were slower than the original ones. These inef-
ﬁciencies were attributed to suboptimal data type choices, unnec-
essary operations, ineﬃcient control ﬂow, and redundant method
calls.
RQ4 - Code Size. We compared the number of lines of code (LOCs)
in the generated code and the code written by the developers for
all the methods in our dataset. The results show that the size of
the generated and original code is similar and that the four code
generation tools tend to generate code of similar length. ChatGPT
and Bard produced code with the highest diﬀerence and variance
in length compared to the length of the code written by developers.
RQ5 - Code Similarity. The four assistants produced code that
is signiﬁcantly diﬀerent from the code written by the developers,
with similarity measured according to the normalized Levenshtein
similarity [7] and CodeBLEU scores [5]. For the incorrectly gener-
ated code, the distances from the developers’ code are larger than
the correct code. Tabnine generated the correct code that is most
similar to the developers’ code (median CodeBLEU of 0.528).

This result suggests that although the generated code could be
close to the intended code in complexity and size, it still has to be
signiﬁcantly adjusted to fully match the expected implementation.

The results indicate that AI-based code assistants can be a valu-
able tool for developers, but they also need to be improved. While

Assessing AI-Based Code Assistants in Method Generation Tasks

ICSE-Companion ’24, April 14–20, 2024, Lisbon, Portugal

REFERENCES
[1] Vincenzo Corso, Leonardo Mariani, Daniela Micucci, and Oliviero Riganelli.
2024. Generating Java Methods: An Empirical Assessment of Four AI-Based
Code Assistants. In Proceedings of the International Conference on Program Com-
prehension.

[2] Arghavan Moradi Dakhel, Vahid Majdinasab, Amin Nikanjam, Foutse Khomh,
Michel C Desmarais, and Zhen Ming Jack Jiang. 2023. GitHub Copilot AI pair
programmer: Asset or Liability? Journal of Systems and Software 203 (2023).

[3] GitHub. 2023. Copilot. https://github.com/features/copilot.
[4] Google. 2023. Bard. https://bard.google.com.

[5] Microsoft. 2020. CodeXGLUE. https://shorturl.at/gwxIL.
[6] OpenAI. 2023. ChatGPT. https://openai.com/chatgpt.
[7] PyPI. 2023. python-Levenshtein 0.21.1. https://shorturl.at/iKVW6.
[8] Tabnine. 2023. Tabnine. https://www.tabnine.com.
[9] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. 2022. Expectation
vs. Experience: Evaluating the Usability of Code Generation Tools Powered by
Large Language Models. In Extended Abstracts of the Conference on Human Fac-
tors in Computing Systems.

[10] Burak Yetistiren, Isik Ozsoy, and Eray Tuzun. 2022. Assessing the Quality of
GitHub Copilot?s Code Generation. In Proceedings of the International Conference
on Predictive Models and Data Analytics in Software Engineering.

","4 2 0 2 b e F 4 1 ] E S . s c [ 1 v 2 2 0 9 0 . 2 0 4 2 : v i X r a Assessing AI-Based Code Assistants in Method Generation Tasks Vincenzo Corso , Leonardo Mariani , Daniela Micucci and Oliviero Riganelli University of Milano-Bicocca Milan , Italy ABSTRACT AI-based code assistants are increasingly popular as a means to en- hance productivity and improve code quality . This study compares four AI-based code assistants , GitHub Copilot , Tabnine , ChatGPT , and Google Bard , in method generation tasks , assessing their abil- ity to produce accurate , correct , and eﬃcient code . Results show that code assistants are useful , with complementary capabilities , although they rarely generate ready-to-use correct code . CCS CONCEPTS • Software and its engineering → Integrated and visual de- velopment environments . KEYWORDS AI-based code assistants , code completion , empirical study . ACM Reference Format : Vincenzo Corso , Leonardo Mariani , Daniela Micucci and Oliviero Riganelli . 2024 . Assessing AI-Based Code Assistants in Method Generation Tasks . In 2024 IEEE/ACM 46th International Conference on Software Engineering : Com- panion Proceedings ( ICSE-Companion ’ 24 ) , April 14–20 , 2024 , Lisbon , Portu- gal . ACM , New York , NY , USA , 3 pages . https : //doi.org/10.1145/3639478.3643122 1 INTRODUCTION Context : AI-based code assistants are becoming increasingly pop- ular . For instance , recent studies demonstrated that they can pro- vide useful code snippets [ 2 , 9 , 10 ] . However , they are still limited in the generation of code ready-to-be integrated into real-world programs . Problem : To address these limitations , this study compares four AI-based code assistants - Copilot [ 3 ] , Tabnine [ 8 ] , ChatGPT [ 6 ] , and Bard [ 4 ] - on their ability to generate code for 100 Java methods extracted from real-world open source projects . Methodology : The study executes the four assistants using the comment and the signature associated with the selected methods as prompts . The quality of the generated code is evaluated accord- ing to ﬁve criteria : functional correctness , complexity , eﬃciency , size , and similarity to the original code produced by developers . Main results : Copilot emerged as the most eﬀective assistant in this task , although all assistants demonstrated their strengths . The study revealed the need for improvement in handling inter-class dependencies . Surprisingly , the generated code sometimes outper- forms developer-written code . Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page . Copyrights for third-party components of this work must be honored . For all other uses , contact the owner/author ( s ) . ICSE-Companion ’ 24 , April 14–20 , 2024 , Lisbon , Portugal © 2024 Copyright held by the owner/author ( s ) . ACM ISBN 979-8-4007-0502-1/24/04 . https : //doi.org/10.1145/3639478.3643122 Extended Abstract : This extended abstract summarizes results extensively presented at the International Conference on Program Comprehension [ 1 ] . The full paper describes the methodology , the results , and the ﬁndings more in detail . 2 METHODOLOGY In this study , we assess AI-base code assistants by investigating ﬁve research questions : RQ1 - Is the code generated by AI-based code assistants cor- rect ? This RQ investigates the syntactic and semantic correctness of the generated code . RQ2 - What is the McCabe complexity of the generated code ? This RQ investigates if AI-based code assistants can not only gen- erate correct code but also produce code with a level of complexity similar to the code implemented by developers . RQ3 - How eﬃcient is the generated code ? This RQ investi- gates if the generated correct code is as eﬃcient as the one imple- mented by developers . RQ4 - What is the size of the generated code ? This RQ investi- gates if the size of the generated code is similar to the code imple- mented by the developers . RQ5 - How far is the generated code from the one imple- mented by developers ? This RQ studies the similarity of the code implemented by the developers to the code generated by the exper- imented tools , according to change-oriented static metrics . The methodology for this study involves a multi-step process designed to address each research question systematically : ( 1 ) Dataset Construction : Selection of 100 Java methods from well-ranked GitHub projects , ensuring diversity in complex- ity and relevance . ( 2 ) Code Generation : Employing four prominent AI-based code assistants - Copilot , Tabnine , ChatGPT , and Bard - to gener- ate code for the selected methods , using both the method- level comment and the signature as prompt . ( 3 ) Code Evaluation : Assessing the generated code against the developer-implemented code in terms of correctness , com- plexity , eﬃciency , size , and similarity . ( 4 ) Statistical Analysis : Using statistical methods to compare the performance of AI-based code assistants and identify signiﬁcant diﬀerences . To ensure a realistic and comprehensive evaluation of AI-based code assistants , the study constructs a dataset of real-world Java programming tasks . The dataset is designed to include methods of diﬀerent levels of complexity and with diﬀerent types of depen- dencies , including stand-alone methods , methods with intra-class dependencies , and methods with inter-class dependencies . By re- stricting the selection to methods that appeared in recent GitHub commits , we mitigated the risk of using code that had been consid- ered during the training of the assistants that we assessed . ICSE-Companion ’ 24 , April 14–20 , 2024 , Lisbon , Portugal Vincenzo Corso , Leonardo Mariani , Daniela Micucci and Oliviero Riganelli the assistants can generate code that is generally correct and eﬃ- cient , they also produce a signiﬁcant amount of invalid or incor- rect code , particularly for methods with inter-class dependencies . Furthermore , the generated code often diﬀers signiﬁcantly from developer-written code , requiring substantial revisions . These ﬁnd- ings highlight the need for further development and reﬁnement of AI-based code assistants to enhance their accuracy , eﬃciency , maintainability , and resemblance to developer-written code . 4 IMPLICATIONS AND CONCLUSIONS The study ’ s ﬁndings have several implications for both research and practice . First , the collaboration among multiple AI-based code assistants is a promising research direction . Developers should con- sider leveraging the strengths of diﬀerent assistants to enhance code generation and address the limitations of individual assis- tants . Second , the study revealed that AI-based code assistants can sometimes generate better code than the code implemented by de- velopers . This ﬁnding has implications for improving code quality and eﬃciency in software development . Third , the challenges with inter-class dependencies highlight the need for further improvement in AI-based code assistants to ef- fectively handle dependencies that extend beyond the boundaries of single classes . Future research and development eﬀorts should focus on enhancing the capabilities of AI-based tools to address complex inter-class dependencies in code generation . In conclusion , AI-based code assistants have the potential to signiﬁcantly improve code generation and quality , but further re- search is needed to address existing challenges and limitations . This study investigates the capabilities of four AI-based code assis- tants : GitHub Copilot , Tabnine , ChatGPT , and Google Bard . Their capabilities are compared according to the functional correctness , complexity , eﬃciency , size , and similarity to the original code . Assistants demonstrated to have complemental capabilities , with Copilot generating the highest rate of correct methods . Results also reveal that the generated code could be a good starting point to derive the actual implementation , but it seldom consists of ready- to-use code . The capability to deal with inter-class dependencies is recognized as one of the main limitations . ACKNOWLEDGMENTS This work has been partially supported by the Engineered Ma- chinE Learning-intensive IoT systems ( EMELIOT ) national research project ( PRIN 2020 program Contract 2020W3A5FY ) . 3 RESULTS RQ1 - Code Correctness . Copilot generated the highest percent- age of correct methods , achieving a 32 % success rate . ChatGPT fol- lowed with 23 % correct methods generated . Finally , Bard and Tab- nine achieved 15 % and 13 % success rates , respectively . Even if each assistant demonstrated some unique capabilities , all assistants can still be largely improved . In fact , a non-negligible portion of the generated code was incorrect , especially when the method requires dealing with inter-class dependencies , where the best-performing assistant , Copilot , achieved only 15 % correctness . We also noticed a remarkable diﬀerence between correct code ( i.e. , code equivalent to developers ’ code based on human inspec- tion ) and plausible code ( i.e. , code that passes the available test cases ) . In our experiment , assistants generated 31 % plausible meth- ods , but only 21 % correct methods on average , conﬁrming that tests can not accurately establish the correctness of the generated code . RQ2 - Code Complexity . The four assistants generally generated code with similar McCabe complexity to the code written by devel- opers . In some cases , the generated code had slightly higher com- plexity due to the use of explicit if conditions , or lower complexity due to the use of lambda expressions and methods that encapsu- late checks . Overall , the generated code was of similar complexity to the original . The four tools did not diﬀer signiﬁcantly in terms of the complexity of the generated code . RQ3 - Code Eﬃciency . The four assistants generated code that is as eﬃcient as , or more eﬃcient than , the code written by de- velopers . A signiﬁcant portion of the generated methods , 87 % for ChatGTP and 100 % for Tabnine , exhibited no signiﬁcant diﬀerence in execution time compared to the original methods . In some cases , the generated code even outperformed the original code . The ex- ceptions were Copilot and ChatGPT , which generated a small num- ber of methods that were slower than the original ones . These inef- ﬁciencies were attributed to suboptimal data type choices , unnec- essary operations , ineﬃcient control ﬂow , and redundant method calls . RQ4 - Code Size . We compared the number of lines of code ( LOCs ) in the generated code and the code written by the developers for all the methods in our dataset . The results show that the size of the generated and original code is similar and that the four code generation tools tend to generate code of similar length . ChatGPT and Bard produced code with the highest diﬀerence and variance in length compared to the length of the code written by developers . RQ5 - Code Similarity . The four assistants produced code that is signiﬁcantly diﬀerent from the code written by the developers , with similarity measured according to the normalized Levenshtein similarity [ 7 ] and CodeBLEU scores [ 5 ] . For the incorrectly gener- ated code , the distances from the developers ’ code are larger than the correct code . Tabnine generated the correct code that is most similar to the developers ’ code ( median CodeBLEU of 0.528 ) . This result suggests that although the generated code could be close to the intended code in complexity and size , it still has to be signiﬁcantly adjusted to fully match the expected implementation . The results indicate that AI-based code assistants can be a valu- able tool for developers , but they also need to be improved . While Assessing AI-Based Code Assistants in Method Generation Tasks ICSE-Companion ’ 24 , April 14–20 , 2024 , Lisbon , Portugal REFERENCES [ 1 ] Vincenzo Corso , Leonardo Mariani , Daniela Micucci , and Oliviero Riganelli . 2024 . Generating Java Methods : An Empirical Assessment of Four AI-Based Code Assistants . In Proceedings of the International Conference on Program Com- prehension . [ 2 ] Arghavan Moradi Dakhel , Vahid Majdinasab , Amin Nikanjam , Foutse Khomh , Michel C Desmarais , and Zhen Ming Jack Jiang . 2023 . GitHub Copilot AI pair programmer : Asset or Liability ? Journal of Systems and Software 203 ( 2023 ) . [ 3 ] GitHub . 2023 . Copilot . https : //github.com/features/copilot . [ 4 ] Google . 2023 . Bard . https : //bard.google.com . [ 5 ] Microsoft . 2020 . CodeXGLUE . https : //shorturl.at/gwxIL . [ 6 ] OpenAI . 2023 . ChatGPT . https : //openai.com/chatgpt . [ 7 ] PyPI . 2023. python-Levenshtein 0.21.1. https : //shorturl.at/iKVW6 . [ 8 ] Tabnine . 2023 . Tabnine . https : //www.tabnine.com . [ 9 ] Priyan Vaithilingam , Tianyi Zhang , and Elena L. Glassman . 2022 . Expectation vs . Experience : Evaluating the Usability of Code Generation Tools Powered by Large Language Models . In Extended Abstracts of the Conference on Human Fac- tors in Computing Systems . [ 10 ] Burak Yetistiren , Isik Ozsoy , and Eray Tuzun . 2022 . Assessing the Quality of GitHub Copilot ? s Code Generation . In Proceedings of the International Conference on Predictive Models and Data Analytics in Software Engineering .","['e', 'e', 'c', 'v', 'r', 'assess', 'aibased', 'code', 'assistant', 'method', 'generation', 'task', 'vincenzo', 'abstract', 'aibase', 'code', 'assistant', 'increasingly', 'popular', 'means', 'productivity', 'improve', 'code', 'quality', 'study', 'compare', 'aibased', 'code', 'assistant', 'copilot', 'tabnine', 'chatgpt', 'method', 'generation', 'task', 'assess', 'abil', 'ity', 'produce', 'accurate', 'correct', 'code', 'result', 'show', 'code', 'assistant', 'useful', 'complementary', 'capability', 'rarely', 'generate', 'readytouse', 'correct', 'code', 'ccs', 'concept', 'software', 'engineering', 'integrate', 'visual', 'velopment', 'environment', 'keyword', 'aibase', 'code', 'assistant', 'code', 'completion', 'empirical', 'study', 'acm', 'reference', 'format', 'oliviero', 'assess', 'aibased', 'code', 'assistant', 'method', 'generation', 'task', 'ieeeacm', '46th', 'international', 'conference', 'software', 'engineering', 'com', 'panion', 'proceeding', 'icsecompanion', 'portu', 'gal', 'acm', 'page', 'https', 'introduction', 'context', 'aibased', 'code', 'assistant', 'become', 'increasingly', 'pop', 'ular', 'instance', 'recent', 'study', 'demonstrate', 'pro', 'vide', 'useful', 'code', 'snippet', 'however', 'still', 'limit', 'generation', 'code', 'readytobe', 'integrate', 'realworld', 'problem', 'address', 'limitation', 'study', 'compare', 'aibased', 'code', 'assistant', 'copilot', 'tabnine', 'chatgpt', 'bard', 'ability', 'generate', 'code', 'java', 'method', 'extract', 'open', 'source', 'project', 'methodology', 'study', 'execute', 'assistant', 'use', 'comment', 'signature', 'associate', 'select', 'method', 'prompt', 'quality', 'generate', 'code', 'evaluate', 'accord', 'ﬁve', 'criterion', 'functional', 'correctness', 'complexity', 'eﬃciency', 'size', 'similarity', 'original', 'code', 'produce', 'developer', 'main', 'result', 'copilot', 'emerge', 'eﬀective', 'assistant', 'task', 'assistant', 'demonstrate', 'strength', 'study', 'reveal', 'need', 'improvement', 'handle', 'interclass', 'dependency', 'surprisingly', 'generate', 'code', 'sometimes', 'outper', 'form', 'developerwritten', 'code', 'permission', 'make', 'digital', 'hard', 'copy', 'part', 'work', 'personal', 'classroom', 'use', 'grant', 'fee', 'provide', 'copy', 'make', 'distribute', 'proﬁt', 'commercial', 'advantage', 'copy', 'bear', 'notice', 'full', 'citation', 'ﬁrst', 'page', 'copyright', 'thirdparty', 'component', 'work', 'honor', 'use', 'contact', 'ownerauthor', 'icsecompanion', 'portugal', 'copyright', 'hold', 'ownerauthor', 'acm', 'isbn', 'https', 'extended', 'abstract', 'extended', 'abstract', 'summarize', 'result', 'extensively', 'present', 'international', 'conference', 'program', 'comprehension', 'full', 'paper', 'describe', 'methodology', 'result', 'ﬁnding', 'detail', 'methodology', 'study', 'assess', 'code', 'assistant', 'investigate', 'ﬁve', 'research', 'question', 'rq1', 'code', 'generate', 'aibased', 'code', 'assistant', 'cor', 'rect', 'rq', 'investigate', 'syntactic', 'semantic', 'correctness', 'generate', 'code', 'rq2', 'mccabe', 'complexity', 'generate', 'code', 'rq', 'investigate', 'aibase', 'code', 'assistant', 'correct', 'code', 'also', 'produce', 'code', 'level', 'complexity', 'similar', 'code', 'implement', 'developer', 'rq3', 'eﬃcient', 'generate', 'code', 'rq', 'gate', 'generate', 'correct', 'code', 'eﬃcient', 'imple', 'mente', 'developer', 'rq4', 'size', 'generate', 'code', 'rq', 'gate', 'size', 'generate', 'code', 'similar', 'code', 'imple', 'mente', 'developer', 'rq5', 'far', 'generate', 'code', 'imple', 'mente', 'developer', 'rq', 'study', 'similarity', 'code', 'implement', 'developer', 'code', 'generate', 'exper', 'imente', 'tool', 'accord', 'changeoriented', 'static', 'metric', 'methodology', 'study', 'involve', 'multistep', 'process', 'design', 'address', 'research', 'question', 'systematically', 'dataset', 'construction', 'selection', 'java', 'method', 'wellranked', 'project', 'ensure', 'diversity', 'complex', 'ity', 'relevance', 'code', 'generation', 'employ', 'prominent', 'aibased', 'code', 'assistant', 'copilot', 'tabnine', 'chatgpt', 'eat', 'code', 'select', 'method', 'use', 'method', 'level', 'comment', 'signature', 'prompt', 'code', 'evaluation', 'assess', 'generated', 'code', 'developerimplemente', 'code', 'term', 'correctness', 'com', 'plexity', 'eﬃciency', 'size', 'similarity', 'statistical', 'analysis', 'use', 'statistical', 'method', 'compare', 'performance', 'aibased', 'code', 'assistant', 'identify', 'signiﬁcant', 'diﬀerence', 'ensure', 'realistic', 'comprehensive', 'evaluation', 'aibased', 'code', 'assistant', 'study', 'construct', 'dataset', 'realworld', 'programming', 'task', 'dataset', 'design', 'include', 'method', 'diﬀerent', 'level', 'complexity', 'diﬀerent', 'type', 'depen', 'dencie', 'include', 'standalone', 'method', 'method', 'intraclass', 'dependency', 'method', 'interclass', 'dependency', 'stricte', 'selection', 'method', 'appear', 'recent', 'commit', 'mitigate', 'risk', 'use', 'code', 'consid', 'ere', 'training', 'assistant', 'assess', 'icsecompanion', 'lisbon', 'portugal', 'vincenzo', 'oliviero', 'assistant', 'generate', 'code', 'generally', 'correct', 'cient', 'also', 'produce', 'signiﬁcant', 'amount', 'invalid', 'incor', 'rect', 'particularly', 'method', 'interclass', 'dependency', 'furthermore', 'generate', 'code', 'often', 'diﬀer', 'signiﬁcantly', 'developerwritten', 'code', 'require', 'substantial', 'revision', 'ing', 'highlight', 'need', 'development', 'reﬁnement', 'aibased', 'code', 'assistant', 'enhance', 'accuracy', 'eﬃciency', 'maintainability', 'resemblance', 'developerwritten', 'code', 'implication', 'conclusion', 'study', 'ﬁnding', 'several', 'implication', 'research', 'practice', 'first', 'collaboration', 'multiple', 'aibased', 'code', 'assistant', 'promising', 'research', 'direction', 'developer', 'con', 'sider', 'leverage', 'strength', 'diﬀerent', 'assistant', 'enhance', 'code', 'generation', 'address', 'limitation', 'individual', 'assis', 'tant', 'second', 'study', 'reveal', 'aibased', 'code', 'assistant', 'sometimes', 'generate', 'well', 'code', 'code', 'implement', 'veloper', 'ﬁnding', 'implication', 'improve', 'code', 'quality', 'eﬃciency', 'software', 'development', 'third', 'challenge', 'interclass', 'dependency', 'highlight', 'need', 'improvement', 'aibased', 'code', 'assistant', 'fectively', 'handle', 'dependency', 'extend', 'boundary', 'single', 'class', 'future', 'research', 'development', 'eﬀort', 'focus', 'enhance', 'capability', 'aibased', 'tool', 'address', 'complex', 'interclass', 'dependency', 'code', 'generation', 'conclusion', 'aibased', 'code', 'assistant', 'potential', 'signiﬁcantly', 'improve', 'code', 'generation', 'quality', 'far', 'search', 'need', 'address', 'exist', 'challenge', 'limitation', 'study', 'investigate', 'capability', 'aibased', 'code', 'assis', 'tant', 'copilot', 'tabnine', 'chatgpt', 'capability', 'compare', 'accord', 'functional', 'correctness', 'complexity', 'eﬃciency', 'size', 'similarity', 'original', 'code', 'assistant', 'demonstrate', 'complemental', 'capability', 'copilot', 'generate', 'high', 'rate', 'correct', 'method', 'result', 'also', 'reveal', 'generate', 'code', 'good', 'starting', 'point', 'derive', 'actual', 'implementation', 'seldom', 'consist', 'ready', 'touse', 'code', 'capability', 'deal', 'interclass', 'dependency', 'recognize', 'main', 'limitation', 'acknowledgment', 'work', 'partially', 'support', 'engineer', 'chine', 'system', 'national', 'research', 'project', 'prin', 'program', 'contract', '2020w3a5fy', 'result', 'code', 'correctness', 'copilot', 'generate', 'high', 'percent', 'age', 'correct', 'method', 'achieve', 'success', 'rate', 'chatgpt', 'low', 'correct', 'method', 'generate', 'finally', 'bard', 'tab', 'achieve', 'success', 'rate', 'respectively', 'even', 'assistant', 'demonstrate', 'unique', 'capability', 'assistant', 'still', 'largely', 'improve', 'fact', 'nonnegligible', 'portion', 'generate', 'code', 'incorrect', 'especially', 'method', 'require', 'deal', 'interclass', 'dependency', 'bestperforme', 'assistant', 'copilot', 'achieve', 'correctness', 'also', 'notice', 'remarkable', 'diﬀerence', 'correct', 'code', 'code', 'equivalent', 'developer', 'code', 'base', 'inspec', 'tion', 'plausible', 'code', 'code', 'pass', 'available', 'test', 'case', 'experiment', 'assistant', 'generate', 'plausible', 'meth', 'od', 'correct', 'method', 'average', 'conﬁrming', 'test', 'accurately', 'establish', 'correctness', 'generate', 'code', 'rq2', 'code', 'complexity', 'assistant', 'generally', 'generate', 'code', 'similar', 'mccabe', 'complexity', 'code', 'write', 'devel', 'oper', 'case', 'generate', 'code', 'slightly', 'high', 'com', 'plexity', 'use', 'explicit', 'condition', 'low', 'complexity', 'use', 'lambda', 'expression', 'method', 'encapsu', 'late', 'check', 'overall', 'generate', 'code', 'similar', 'complexity', 'original', 'tool', 'diﬀer', 'signiﬁcantly', 'term', 'complexity', 'generate', 'code', 'code', 'eﬃciency', 'assistant', 'generate', 'code', 'eﬃcient', 'eﬃcient', 'code', 'write', 'veloper', 'signiﬁcant', 'portion', 'generate', 'method', 'chatgtp', 'tabnine', 'exhibit', 'signiﬁcant', 'diﬀerence', 'execution', 'time', 'compare', 'original', 'method', 'case', 'generate', 'code', 'even', 'outperform', 'original', 'code', 'ex', 'ception', 'copilot', 'chatgpt', 'generate', 'small', 'num', 'ber', 'method', 'slow', 'original', 'one', 'inef', 'ﬁciencie', 'attribute', 'suboptimal', 'datum', 'type', 'choice', 'unnec', 'essary', 'operation', 'ineﬃcient', 'control', 'ﬂow', 'redundant', 'method', 'call', 'rq4', 'code', 'size', 'compare', 'number', 'line', 'code', 'loc', 'generate', 'code', 'code', 'write', 'developer', 'method', 'dataset', 'result', 'show', 'size', 'generate', 'original', 'code', 'similar', 'code', 'generation', 'tool', 'tend', 'generate', 'code', 'similar', 'length', 'chatgpt', 'bard', 'produce', 'code', 'high', 'diﬀerence', 'variance', 'length', 'compare', 'length', 'code', 'write', 'developer', 'rq5', 'code', 'similarity', 'assistant', 'produce', 'code', 'signiﬁcantly', 'diﬀerent', 'code', 'write', 'developer', 'similarity', 'measure', 'accord', 'normalize', 'levenshtein', 'similarity', 'codebleu', 'score', 'incorrectly', 'ate', 'code', 'distance', 'developer', 'code', 'large', 'correct', 'code', 'tabnine', 'generate', 'correct', 'code', 'similar', 'developer', 'code', 'median', 'codebleu', 'result', 'suggest', 'generate', 'code', 'close', 'intend', 'code', 'complexity', 'size', 'still', 'signiﬁcantly', 'adjust', 'fully', 'match', 'expect', 'implementation', 'result', 'indicate', 'aibased', 'code', 'assistant', 'valu', 'able', 'tool', 'developer', 'also', 'need', 'improve', 'assess', 'aibased', 'code', 'assistant', 'method', 'generation', 'task', 'icsecompanion', 'lisbon', 'portugal', 'reference', 'oliviero', 'generate', 'java', 'method', 'empirical', 'assessment', 'aibased', 'code', 'assistant', 'proceeding', 'international', 'conference', 'program', 'com', 'prehension', 'copilot', 'ai', 'pair', 'programmer', 'asset', 'liability', 'journal', 'system', 'software', 'copilot', 'https', 'githubcomfeaturescopilot', 'google', 'bard', 'codexglue', 'https', 'shorturlatgwxil', 'openai', 'chatgpt', 'https', 'openaicomchatgpt', 'pypi', 'shorturlatikvw6', 'tabnine', 'tabnine', 'expectation', 'experience', 'evaluate', 'usability', 'code', 'generation', 'tool', 'power', 'large', 'language', 'model', 'extend', 'abstract', 'conference', 'human', 'fac', 'tor', 'computing', 'system', 'isik', 'eray', 'assess', 'quality', 'copilot', 'code', 'generation', 'proceeding', 'international', 'conference', 'predictive', 'model', 'data', 'analytic', 'software', 'engineering']"
Exploiting Alpha Transparency In Language And Vision-Based AI Systems,"[{'href': 'http://arxiv.org/abs/2402.09671v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2402.09671v1', 'rel': 'related', 'type': 'application/pdf'}]",2024-02-15 02:38:23,"4
2
0
2

b
e
F
3
1

]

G
L
.
s
c
[

3
v
7
8
1
6
0
.
2
0
4
2
:
v
i
X
r
a

PREMIER-TACO IS A FEW-SHOT POLICY LEARNER:
PRETRAINING MULTITASK REPRESENTATION VIA TEMPORAL
ACTION-DRIVEN CONTRASTIVE LOSS

A PREPRINT

Ruijie Zheng1∗
Shuang Ma3

Yongyuan Liang1

Hal Daum´e III1, 3

Xiyao Wang 1
Huazhe Xu2

John Langford3

Praveen Palanisamy3

Kalyan Shankar Basu3

Furong Huang1

1 University of Maryland, College Park 2 Tsinghua University
3 Microsoft Research
rzheng12@umd.edu

ABSTRACT

We present Premier-TACO, a multitask feature representation learning approach designed to improve
few-shot policy learning efficiency in sequential decision-making tasks. Premier-TACO leverages a
subset of multitask offline datasets for pretraining a general feature representation, which captures
critical environmental dynamics and is fine-tuned using minimal expert demonstrations. It advances
the temporal action contrastive learning (TACO) objective, known for state-of-the-art results in
visual control tasks, by incorporating a novel negative example sampling strategy. This strategy
is crucial in significantly boosting TACO’s computational efficiency, making large-scale multitask
offline pretraining feasible. Our extensive empirical evaluation in a diverse set of continuous control
benchmarks including Deepmind Control Suite, MetaWorld, and LIBERO demonstrate Premier-
TACO’s effectiveness in pretraining visual representations, significantly enhancing few-shot imitation
learning of novel tasks. Our code, pretraining data, as well as pretrained model checkpoints will be
released at https://github.com/PremierTACO/premier-taco.

1

Introduction

In the dynamic and ever-changing world we inhabit, the importance of sequential decision-making (SDM) in machine
learning cannot be overstated. Unlike static tasks, sequential decisions reflect the fluidity of real-world scenarios, from
robotic manipulations to evolving healthcare treatments. Just as foundation models in language, such as BERT (Devlin
et al., 2019) and GPT (Radford et al., 2019; Brown et al., 2020), have revolutionized natural language processing by
leveraging vast amounts of textual data to understand linguistic nuances, pretrained foundation models hold similar
promise for sequential decision-making (SDM). In language, these models capture the essence of syntax, semantics,
and context, serving as a robust starting point for a myriad of downstream tasks. Analogously, in SDM, where decisions
are influenced by a complex interplay of past actions, current states, and future possibilities, a pretrained foundation
model can provide a rich, generalized understanding of decision sequences. This foundational knowledge, built upon
diverse decision-making scenarios, can then be fine-tuned to specific tasks, much like how language models are adapted
to specific linguistic tasks.

The following challenges are unique to sequential decision-making, setting it apart from existing vision and language
pretraining paradigms. (C1) Data Distribution Shift: Training data usually consists of specific behavior-policy-
generated trajectories. This leads to vastly different data distributions at various stages—pretraining, finetuning, and
deployment—resulting in compromised performance (Lee et al., 2021). (C2) Task Heterogeneity: Unlike language
and vision tasks, which often share semantic features, decision-making tasks vary widely in configurations, transition
dynamics, and state and action spaces. This makes it difficult to develop a universally applicable representation. (C3)
Data Quality and Supervision: Effective representation learning often relies on high-quality data and expert guidance.

∗The work is partially done during Ruijie Zheng’s internship at Microsoft Research.

 
 
 
 
 
 
Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss

Figure 1: Performance of Premier-TACO pretrained visual representation for few-shot imitation learning on downstream unseen
tasks from Deepmind Control Suite, MetaWorld, and LIBERO. LfS here represents learning from scratch.

However, these resources are either absent or too costly to obtain in many real-world decision-making tasks (Brohan
et al., 2023; Stooke et al., 2021b). Our aspirational criteria for foundation model for sequential decision-making
encompass several key features: (W1) Versatility that allows the model to generalize across a wide array of tasks,
even those not previously encountered, such as new embodiments viewed or observations from novel camera angles;
(W2) Efficiency in adapting to downstream tasks, requiring minimal data through few-shot learning techniques; (W3)
Robustness to pretraining data of fluctuating quality, ensuring a resilient foundation; and (W4) Compatibility with
existing large pretrained models such as Nair et al. (2022).

In light of these challenges and desirables in building foundation models for SDM, our approach to develop foundational
models for sequential decision-making focuses on creating a universal and transferable encoder using a reward-free,
dynamics based, temporal contrastive pretraining objective. This encoder would be tailored to manage tasks with
complex observation spaces, such as visual inputs. By excluding reward signals during the pretraining stage, the
model will be better poised to generalize across a broad array of downstream tasks that may have divergent objectives.
Leveraging a world-model approach ensures that the encoder learns a compact representation that can capture universal
transition dynamics, akin to the laws of physics, thereby making it adaptable for multiple scenarios. This encoder
enables the transfer of knowledge to downstream control tasks, even when such tasks were not part of the original
pretraining data set.

Existing works apply self-supervised pre-training from rich vision data such as ImageNet (Deng et al., 2009) or Ego4D
datasets (Grauman et al., 2022) to build foundation models (Nair et al., 2022; Majumdar et al., 2023; Ma et al., 2023).
However, applying these approaches to sequential decision-making tasks is challenging. Specifically, they often
overlook control-relevant considerations and suffer from a domain gap between pre-training datasets and downstream
control tasks. In this paper, rather than focusing on leveraging large vision datasets, we propose a novel control-centric
objective function for pretraining. Our approach, called Premier-TACO (pretraining multitask representation via
temporal action-driven contrastive loss), employs a temporal action-driven contrastive loss function for pretraining.
This control-centric objective learns a state representation by optimizing the mutual information between representations
of current states paired with action sequences and representations of the corresponding future states.

Premier-TACO markedly enhances the effectiveness and efficiency of the temporal action contrastive learning (TACO)
objective, as detailed in Zheng et al. (2023a), which delivers state-of-the-art outcomes in visual control tasks within
a single-task setting. It extends these capabilities to efficient, large-scale multitask offline pretraining, broadening
its applicability and performance. Specifically, while TACO considers every data point in a batch as a potential
negative example, Premier-TACO strategically samples a single negative example from a proximate window of the
subsequent state. This method ensures the negative example is visually akin to the positive one, necessitating that
the latent representation captures control-relevant information, rather than relying on extraneous features like visual
appearance. This efficient negative example sampling strategy adds no computational burden and is compatible with
smaller batch sizes. In particular, on MetaWorld, using a batch size of 1
8 for TACO, Premier-TACO achieves a 25%
relative performance improvement. Premier-TACO can be seamlessly scaled for multitask offline pretraining, enhancing
its usability and effectiveness.

Below we list our key contributions:
▷ (1) We introduce Premier-TACO, a new framework designed for the multi-task offline visual representation
pretraining of sequential decision-making problems. In particular, we develop a new temporal contrastive learning
objective within the Premier-TACO framework. Compared with other temporal contrastive learning objectives
such as TACO, Premier-TACO employs a simple yet efficient negative example sampling strategy, making it
computationally feasible for multi-task representation learning.

2

Deepmind Control Suite 
(10 tasks)

MetaWorld
(8 tasks)

LIBERO
(8 tasks)

Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss

▷ (2) [(W1) Versatility (W2) Efficiency] Through extensive empirical evaluation, we verify the effectiveness of
Premier-TACO’s pretrained visual representations for few-shot learning on unseen tasks. On MetaWorld (Yu et al.,
2019) and LIBERO Liu et al. (2023), with 5 expert trajectories, Premier-TACO outperforms the best baseline
pretraining method by 37% and 17% respectively. Remarkably, in LIBERO, we are the first method to demonstrate
benefits from pretraining. On Deepmind Control Suite (DMC) (Tassa et al., 2018), using only 20 trajectories, which
is considerably fewer demonstrations than (Sun et al., 2023; Majumdar et al., 2023), Premier-TACO achieves the
best performance across 10 challenging tasks, including the hard Dog and Humanoid tasks. This versatility extends
even to unseen embodiments in DMC as well as unseen tasks with unseen camera views in MetaWorld.

▷ (3) [(W3) Robustness (W4) Compatability] Furthermore, we demonstrate that Premier-TACO is not only resilient
to data of lower quality but also compatible with exisiting large pretrained models. In DMC, Premier-TACO works
well with the pretraining dataset collected randomly. Additionally, we showcase the capability of the temporal
contrastive learning objective of Premier-TACO to finetune a generalized visual encoder such as R3M (Nair et al.,
2022), resulting in an averaged performance enhancement of around 50% across the assessed tasks on Deepmind
Control Suite and MetaWorld.

2 Preliminary

2.1 Multitask Offline Pretraining

We consider a collection of tasks (cid:8)Ti : (X , Ai, Pi, Ri, γ)(cid:9)N
i=1 with the same dimensionality in observation space
X . Let ϕ : X → Z be a representation function of the agent’s observation, which is either randomly initialized or
pre-trained already on a large-scale vision dataset such as ImageNet (Deng et al., 2009) or Ego4D (Grauman et al.,
2022). Assuming that the agent is given a multitask offline dataset {(xi, ai, x′
j=1.
The objective is to pretrain a generalizable state representation ϕ or a motor policy π so that when facing an unseen
downstream task, it could quickly adapt with few expert demonstrations, using the pretrained representation.
Below we summarize the pretraining and finetuning setups.
Pretraining: The agent get access to a multitask offline dataset, which could be highly suboptimal. The goal is to
learn a generalizable shared state representation from pixel inputs.
Adaptation: Adapt to unseen downstream task from few expert demonstration with imitation learning.

i, ri)} of a subset of K tasks {Tnj }K

2.2 TACO: Temporal Action Driven Contrastive Learning Objective

Temporal Action-driven Contrastive Learning (TACO) (Zheng et al., 2023a) is a reinforcement learning algorithm
proposed for addressing the representation learning problem in visual continuous control. It aims to maximize the
mutual information between representations of current states paired with action sequences and representations of the
corresponding future states:

JTACO = I(Zt+K; [Zt, Ut, ..., Ut+K−1])
Here, Zt = ϕ(Xt) and Ut = ψ(At) represent latent state and action variables. Theoretically, it could be shown that max-
imization of this mutual information objective lead to state and action representations that are capable of representing
the optimal value functions. Empirically, TACO estimate the lower bound of the mutual information objective by the In-
foNCE loss, and it achieves the state of art performance for both online and offline visual continuous control, demonstrat-
ing the effectiveness of temporal contrastive learning for representation learning in sequential decision making problems.

(1)

3 Method

We introduce Premier-TACO, a generalized pre-training approach specifically formulated to tackle the multi-task
pre-training problem, enhancing sample efficiency and generalization ability for downstream tasks. Building upon the
success of temporal contrastive loss, exemplified by TACO (Zheng et al., 2023a), in acquiring latent state representations
that encapsulate individual task dynamics, our aim is to foster representation learning that effectively captures the
intrinsic dynamics spanning a diverse set of tasks found in offline datasets. Our overarching objective is to ensure that
these learned representations exhibit the versatility to generalize across unseen tasks that share the underlying dynamic
structures.

Nevertheless, when adapted for multitask offline pre-training, the online learning objective of TACO (Zheng et al.,
2023a) poses a notable challenge. Specifically, TACO’s mechanism, which utilizes the InfoNCE (van den Oord et al.,
2019) loss, categorizes all subsequent states st+k in the batch as negative examples. While this methodology has
proven effective in single-task reinforcement learning scenarios, it encounters difficulties when extended to a multitask

3

Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss

Figure 2: An illustration of Premier-TACO contrastive loss design. The two ‘State Encoder’s are identical, as are the two ‘Proj.
Layer H’s. One negative example is sampled from the neighbors of framework st+K .

context. During multitask offline pretraining, image observations within a batch can come from different tasks with
vastly different visual appearances, rendering the contrastive InfoNCE loss significantly less effective.

Offline Pretraining Objective. We propose a straightforward yet
highly effective mechanism for selecting challenging negative ex-
amples. Instead of treating all the remaining examples in the batch
as negatives, Premier-TACO selects the negative example from a
window centered at state st+k within the same episode as shown in
Figure 3.

This approach is both computationally efficient and more statistically
powerful due to negative examples which are challenging to
distinguish from similar positive examples forcing the model capture
temporal dynamics differentiating between positive and negative
examples. In practice, this allows us to use much smaller batch sizes
for Premier-TACO. On MetaWorld, with only 1
8 of the batch size
(512 vs. 4096), Premier-TACO achieves a 25% performance gain
compared to TACO, saving around 87.5% of computational time.

Figure 3: Difference between Premier-TACO and
TACO for sampling negative examples

, [a(i)
t

In Figure 2, we illustrate the design of Premier-TACO objective. Specifically, given a batch of state and action sequence
t = ϕ(s(i)
transitions {(s(i)
t ) be latent state and latent action
t

embeddings respectively. Furthermore, let
W centered at st+K: (st+K−W , ..., st+K−1, st+K+1, ..., st+K+W ) with (cid:103)z(i)

i=1, let z(i)
t+K)}N
(cid:93)
s(i)
t+K be a negative example uniformly sampled from the window of size
t = ϕ( (cid:102)s(i)

t ) a negative latent state.

t = ψ(a(i)

t+K−1], s(i)

t ), u(i)

, ..., a(i)

Given these, define g(i)
future predicted and actual latent states. We optimize:

t = Gθ(z(i)

, ..., u(i)

, u(i)
t

t+K−1), (cid:103)h(i)

t

t = Hθ(

(cid:93)
t+K), and h(i)
z(i)

t = Hθ(z(i)

t+K) as embeddings of

JPremier-TACO(ϕ, ψ, Gθ, Hθ) = −

1
N

N
(cid:88)

i=1

log

⊤

g(i)
t

h(i)
t+K

⊤

g(i)
t

t+K + (cid:103)g(i)
h(i)

t

⊤

h(i)
t+K

(2)

Few-shot Generalization. After pretraining the representation encoder, we leverage our pretrained model Φ to
learn policies for downstream tasks. To learn the policy π with the state representation Φ(st) as inputs, we use
behavior cloning (BC) with a few expert demonstrations. For different control domains, we employ significantly
fewer demonstrations for unseen tasks than what is typically used in other baselines. This underscores the substantial
advantages of Premier-TACO in few-shot generalization. More details about the experiments on downstream tasks will
be provided in Section 4.

4 Experiment

In our empirical evaluations, we consider three benchmarks, Deepmind Control Suite (Tassa et al., 2018) for locomotion
control, MetaWorld (Yu et al., 2019) and LIBERO (Liu et al., 2023) for robotic manipulation tasks. It is important
to note the varied sources of data employed for pretraining in these benchmarks. For the Deepmind Control Suite,

4

Negative Example  

𝒔𝒕""𝑲$𝑾

𝒔𝒕""𝑲""𝑾

…	

…	

State Encoder

…	

𝒛𝒕""𝑲$𝑾

𝒛𝒕""𝑲""𝑾

Proj. Layer 𝐻

𝒂𝒕

𝒂𝒕""𝟏

𝒂𝒕""𝑲$𝟏

…	

Action Encoder

…	

Proj. Layer 𝐺

Contrastive 
Loss

Positive Example

𝒔𝒕

𝒔𝒕""𝑲

State Encoder

𝒛𝒕

𝒛𝒕""𝑲

Proj. Layer 𝐻

	
Premier-TACO

𝒔𝒕""𝑲$𝑾

𝒔𝒕""𝑲

𝒔𝒕""𝑲""𝑾

TACO

…	

…	

Sample

.

.

.
. . .

h
c
t
a
B

1 Negative Example

N-1 Negative Examples

 
 
Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss

Figure 4: Pretrain and Test Tasks split for Deepmind Control Suite, MetaWorld and Libero. The left figures are Deepmind Control
Suite tasks and the right figures MetaWorld tasks.

our pretraining dataset comes from the replay buffers of online reinforcement learning (RL) agents. In MetaWorld,
the dataset is generated through a pre-defined scripted policy. In LIBERO, we utilize its provided demonstration
dataset, which was collected through human teleoperation. By evaluating on a wide range of pretraining data types
that have been explored in previous works, we aim to provide a comprehensive evaluation for the pretraining effects of
Premier-TACO.

Deepmind Control Suite (DMC): We consider a selection of 16 challenging tasks from Deepmind Control Suite.
Note that compared with prior works such as Majumdar et al. (2023); Sun et al. (2023), we consider much harder
tasks, including ones from the humanoid and dog domains, which feature intricate kinematics, skinning weights and
collision geometry. For pretraining, we select six tasks (DMC-6), including Acrobot Swingup, Finger Turn Hard,
Hopper Stand, Walker Run, Humanoid Walk, and Dog Stand. We generate an exploratory dataset for each task by
sampling trajectories generated in exploratory stages of a DrQ-v2 (Yarats et al., 2022) learning agent. In particular, we
sample 1000 trajectories from the online replay buffer of DrQ-v2 once it reaches the convergence performance. This
ensures the diversity of the pretraining data, but in practice, such a high-quality dataset could be hard to obtain. So,
later in the experiments, we will also relax this assumption and consider pretrained trajectories that are sampled from
uniformly random actions. In terms of the encoder architecture, we pretrain Premier-TACO with the same shallow
ConvNet encoder as in DrQv2 (Yarats et al., 2022).

MetaWorld: We select a set of 10 tasks for pretraining, which encompasses a variety of motion patterns of the Sawyer
robotic arm and interaction with different objects. To collect an exploratory dataset for pretraining, we execute the
scripted policy with Gaussian noise of a standard deviation of 0.3 added to the action. After adding such a noise, the
success rate of collected policies on average is only around 20% across ten pretrained tasks. We use the same encoder
network architecture as DMC.

LIBERO: We pretrain on 90 short-horizon manipulation tasks (LIBERO-90) with human demonstration dataset
provided by the original paper. For each task, it contains 50 trajectories of human teleoperated trajectories. We use
ResNet18 encoder (He et al., 2016) to encode the image observations of resolution 128 × 128. For the downstream task,
we assess the few-shot imitation learning performance on the first 8 long-horizon tasks of LIBERO-LONG.

Baselines. We compare Premier-TACO with the following representation pretraining baselines:
▷ Learn from Scratch: Behavior Cloning with randomly initialized shallow ConvNet encoder. We carefully implement
the behavior cloning from scratch baseline. For DMC and MetaWorld, following Hansen et al. (2022a), we include the
random shift data augmentation into behavior cloning. For LIBERO, we take the ResNet-T model architecture in Liu
et al. (2023), which uses a transformer decoder module on top of the ResNet encoding to extract temporal information
from a sequence of observations, addressing the non-Markovian characteristics inherent in human demonstration.
▷ Policy Pretraining: We first train a multitask policy by TD3+BC (Fujimoto & Gu, 2021) on the pretraining dataset.
While numerous alternative offline RL algorithms exist, we choose TD3+BC as a representative due to its simplicity
and great empirical performance. For LIBERO, we use Multitask BC since offline RL in generally does not perform
well on the imitation learning benchmark with human demonstrated dataset. After pretraining, we take the pretrained
ConvNet encoder and drop the policy MLP layers.

5

Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss

DMControl

Models

Tasks

LfS

SMART Best PVRs TD3+BC

Inverse

CURL

ATC

SPR

TACO

Premier-TACO

Seen

Embodiments

Unseen

Embodiments

Finger Spin

34.8±3.4 44.2 ± 8.2 38.4 ± 9.3 68.8 ± 7.1 33.4±8.4 35.1±9.6 51.1±9.4 55.9±6.2 28.4±9.7

75.2 ± 0.6

Hopper Hop

8.0 ± 1.3 14.2 ± 3.9 23.2 ± 4.9 49.1 ± 4.3 48.3±5.2 28.7±5.2 34.9±3.9 52.3±7.8 21.4±3.4

75.3 ± 4.6

Walker Walk

30.4±2.9 54.1 ± 5.2 32.6 ± 8.7 65.8 ± 2.0 64.4±5.6 37.3±7.9 44.6±5.0 72.9±1.5 30.6±6.1

88.0 ± 0.8

Humanoid Walk 15.1±1.3 18.4 ± 3.9 30.1 ± 7.5 34.9 ± 8.5 41.9±8.4 19.4±2.8 35.1±3.1 30.1±6.2 29.1±8.1

51.4 ± 4.9

Dog Trot

52.7±3.5 59.7 ± 5.2 73.5 ± 6.4 82.3 ± 4.4 85.3±2.1 71.9±2.2 84.3±0.5 79.9±3.8 80.1±4.1

93.9 ± 5.4

Cup Catch

56.8±5.6 66.8 ± 6.2 93.7 ± 1.8 97.1 ± 1.7 96.7±2.6 96.7±2.6 96.2±1.4 96.9±3.1 88.7±3.2

98.9 ± 0.1

Reacher Hard

34.6±4.1 52.1 ± 3.8 64.9 ± 5.8 59.6 ± 9.9 61.7±4.6 50.4±4.6 56.9±9.8 62.5±7.8 58.3±6.4

81.3 ± 1.8

Cheetah Run

25.1±2.9 41.1 ± 7.2 39.5 ± 9.7 50.9 ± 2.6 51.5±5.5 36.8±5.4 30.1±1.0 40.2±9.6 23.2±3.3

65.7 ± 1.1

Quadruped Walk 61.1±5.7 45.4 ± 4.3 63.2 ± 4.0 76.6 ± 7.4 82.4±6.7 72.8±8.9 81.9±5.6 65.6±4.0 63.9±9.3

83.2 ± 5.7

Quadruped Run 45.0±2.9 27.9 ± 5.3 64.0 ± 2.4 48.2 ± 5.2 52.1±1.8 55.1±5.4 2.6 ± 3.6 68.2±3.2 50.8±5.7

76.8 ± 7.5

Mean Performance

38.2

42.9

52.3

63.3

61.7

50.4

52.7

62.4

47.5

79.0

Table 1: [(W1) Versatility (W2) Efficiency] Few-shot Behavior Cloning (BC) for unseen task of DMC. Performance (Agent
Reward / Expert Reward) of baselines and Premier-TACO on 10 unseen tasks on Deepmind Control Suite. Bold numbers indicate
the best results. Agent Policies are evaluated every 1000 gradient steps for a total of 100000 gradient steps and we report the average
performance over the 3 best epochs over the course of learning. Premier-TACO outperforms all the baselines, showcasing its superior
efficacy in generalizing to unseen tasks with seen or unseen embodiments.

MetaWorld

Models

Unseen Tasks

LfS

SMART

Best PVRs

TD3+BC

Inverse

CURL

ATC

SPR

TACO

Premier-TACO

Bin Picking

Disassemble

Hand Insert

62.5 ± 12.5

71.3 ± 9.6

60.2 ± 4.3

50.6 ± 3.7

55.0 ± 7.9

45.6 ± 5.6

55.6 ± 7.8

67.9 ± 6.4

67.3 ± 7.5

78.5 ± 7.2

56.3 ± 6.5

52.9 ± 4.5

70.4 ± 8.9

56.9 ± 11.5

53.8 ± 8.1

66.2 ± 8.3

45.6 ± 9.8

48.8 ± 5.4

51.3 ± 10.8

86.7 ± 8.9

34.7 ± 7.5

34.1 ± 5.2

35.5 ± 2.3

46.2 ± 5.2

50.0 ± 3.5

49.4 ± 7.6

51.2 ± 1.3

52.4 ± 5.2

56.8 ± 4.2

75.0 ± 7.1

Peg Insert Side

28.7 ± 2.0

20.9 ± 3.6

48.2 ± 3.6

30.0 ± 6.1

33.1 ± 6.2

28.1 ± 3.7

31.8 ± 4.8

39.2 ± 7.4

36.3 ± 4.5

62.7 ± 4.7

Pick Out Of Hole

53.7 ± 6.7

65.9 ± 7.8

66.3 ± 7.2

46.9 ± 7.4

50.6 ± 5.1

43.1 ± 6.2

54.4 ± 8.5

55.3 ± 6.8

52.9 ± 7.3

72.7 ± 7.3

Pick Place Wall

40.5 ± 4.5

62.8 ± 5.9

63.2 ± 9.8

63.8 ± 12.4 71.3 ± 11.3 73.8 ± 11.9

68.7 ± 5.5

72.3 ± 7.5

37.8 ± 8.5

80.2 ± 8.2

Shelf Place

Stick Pull

Mean

26.3 ± 4.1

57.9 ± 4.5

32.4 ± 6.5

45.0 ± 7.7

36.9 ± 6.7

35.0 ± 10.8 35.6 ± 10.7

38.0 ± 6.5

25.8 ± 5.0

70.4 ± 8.1

46.3 ± 7.2

65.8 ± 8.2

52.4 ± 5.6

72.3 ± 11.9

57.5 ± 9.5

43.1 ± 15.2

72.5 ± 8.9

68.5 ± 9.4

52.0 ± 10.5

80.0 ± 8.1

43.6

53.9

53.6

51.5

51.0

48.3

51.9

55.3

47.5

75.8

Table 2: [(W1) Versatility (W2) Efficiency] Five-shot Behavior Cloning (BC) for unseen task of MetaWorld. Success rate of
Premier-TACO and baselines across 8 hard unseen tasks on MetaWorld. Results are aggregated over 4 random seeds. Bold numbers
indicate the best results.

Figure 5: [(W1) Versatility (W2) Efficiency] Mean success rate of 5-shot imitation learning for 8 unseen tasks in LIBERO. Results
are aggregated over 4 random seeds. Bold numbers indicate the best results. See the results for individual tasks in Table 4.

▷ Pretrained Visual Representations (PVRs): We evaluate the state-of-the-art frozen pretrained visual representations
including PVR (Parisi et al., 2022), MVP (Xiao et al., 2022), R3M (Nair et al., 2022) and VC-1 (Majumdar et al.,
2023), and report the best performance of these PVRs models for each task.

▷ Control Transformer: SMART (Sun et al., 2023) is a self-supervised representation pretraining framework which
utilizes a maksed prediction objective for pretraining representation under Decision Transformer architecture, and
then use the pretrained representation to learn policies for downstream tasks.

▷ Inverse Dynamics Model: We pretrain an inverse dynamics model to predict actions and use the pretrained

representation for downstream task.

▷ Contrastive/Self-supervised Learning Objectives: CURL (Laskin et al., 2020), ATC (Stooke et al., 2021a),
SPR (Schwarzer et al., 2021a,b). CURL and ATC are two approaches that apply contrastive learning into sequential
decision making problems. While CURL treats augmented states as positive pairs, it neglects the temporal dependency

6

Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss

of MDP. In comparison, ATC takes the temporal structure into consideration. The positive example of ATC is an
augmented view of a temporally nearby state. SPR applies BYOL objecive (Grill et al., 2020) into sequential decision
making problems by pretraining state representations that are self-predictive of future states.

Pretrained feature representation by Premier-TACO facilitates effective few-shot adaptation to unseen tasks. We
measure the performance of pretrained visual representation for few-shot imitation learning of unseen downstream
tasks in both DMC and MetaWorld. In particular, for DMC, we use 20 expert trajectories for imitation learning except
for the two hardest tasks, Humanoid Walk and Dog Trot, for which we use 100 trajectories instead. Note that we only
use 1

10 of those used in Sun et al. (2023).

5 of the number of expert trajectories used in Majumdar et al. (2023) and 1
Agent Reward
Expert Reward

We record the performance of the agent by calculating the ratio of

, where Expert Reward is the episode

reward of the expert policy used to collect demonstration trajectories. For MetaWorld and LIBERO, we use 5 expert
trajectories for all downstream tasks, and we use task success rate as the performance metric. In Table 1 Table 2,
and Figure 5 we present the results for Deepmind Control Suite, MetaWorld, and LIBERO, respectively. As shown
here, pretrained representation of Premier-TACO significantly improves the few-shot imitation learning performance
compared with Learn-from-scratch, with a 101% improvement on Deepmind Control Suite and 74% improvement
on MetaWorld, respectively. Moreover, it also outperforms all the baselines across all tasks by a large margin. In
LIBERO, consistent with what is observed in (Liu et al., 2023), existing pretraining methods on large-scale multitask
offline dataset fail to enhance downstream policy learning performance. In particular, methods like multitask pretraining
actually degrade downstream policy learning performance. In contrast, using ResNet-18 encoders pretrained by
Premier-TACO significantly boosts few-shot imitation learning performance by a substantial margin.

Premier-TACO pre-trained representation enables knowledge sharing across different embodiments. Ideally, a
resilient and generalizable state feature representation ought not only to encapsulate universally applicable features for a
given embodiment across a variety of tasks, but also to exhibit the capability to generalize across distinct embodiments.
Here, we evaluate the few-shot behavior cloning performance of Premier-TACO pre-trained encoder from DMC-6
on four tasks featuring unseen embodiments: Cup Catch, Cheetah Run, and Quadruped Walk. In comparison to
Learn-from-scratch, as shown in Figure 6 (left), Premier-TACO pre-trained representation realizes an 82% performance
gain, demonstrating the robust generalizability of our pre-trained feature representations.

Figure 6: [(W1) Versatility] (Left) DMC: Generalization of Premier-TACO pre-trained visual representation to unseen embodiments.
(Right) MetaWorld: Few-shot adaptation to unseen tasks from an unseen camera view

Premier-TACO Pretrained Representation is also generalizable to unseen tasks with camera views. Beyond
generalizing to unseen embodiments, an ideal robust visual representation should possess the capacity to adapt to
unfamiliar tasks under novel camera views. In Figure 6 (right), we evaluate the five-shot learning performance of
our model on four previously unseen tasks in MetaWorld with a new view. In particular, during pretraining, the data
from MetaWorld are generated using the same view as employed in (Hansen et al., 2022b; Seo et al., 2022). Then for
downstream policy learning, the agent is given five expert trajectories under a different corner camera view, as depicted
in the figure. Notably, Premier-TACO also achieves a substantial performance enhancement, thereby underscoring the
robust generalizability of our pretrained visual representation.

Premier-TACO Pre-trained Representation is resilient to low-quality data. We evaluate the resilience of Premier-
TACO by employing randomly collected trajectory data from Deepmind Control Suite for pretraining and compare
it with Premier-TACO representations pretrained using an exploratory dataset and the learn-from-scratch approach.
As illustrated in Figure 7, across all downstream tasks, even when using randomly pretrained data, the Premier-
TACO pretrained model still maintains a significant advantage over learning-from-scratch. When compared with
representations pretrained using exploratory data, there are only small disparities in a few individual tasks, while they
remain comparable in most other tasks. This strongly indicates the robustness of Premier-TACO to low-quality data.
Even without the use of expert control data, our method is capable of extracting valuable information.

7

Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss

Figure 7: [(W3) Robustness] Premier-TACO pretrained with exploratory dataset vs. Premier-TACO pretrained with randomly
collected dataset

Figure 8: [(W4) Compatibility] Finetune R3M (Nair et al., 2022), a generalized Pretrained Visual Encoder with Premier-TACO
learning objective vs. R3M with in-domain finetuning in Deepmind Control Suite and MetaWorld.

Pretrained visual encoder finetuning with Premier-TACO. In addition to evaluating our pretrained representations
across various downstream scenarios, we also conducted fine-tuning on pretrained visual representations using in-
domain control trajectories following Premier-TACO framework. Importantly, our findings deviate from the observations
made in prior works like (Hansen et al., 2022a) and (Majumdar et al., 2023), where fine-tuning of R3M (Nair et al.,
2022) on in-domain demonstration data using the task-centric behavior cloning objective, resulted in performance
degradation. We speculate that two main factors contribute to this phenomenon. First, a domain gap exists between
out-of-domain pretraining data and in-domain fine-tuning data. Second, fine-tuning with few-shot learning can lead to
overfitting for large pretrained models.

To further validate the effectiveness of our Premier-TACO approach, we compared the results of R3M with no fine-
tuning, in-domain fine-tuning (Hansen et al., 2022a), and fine-tuning using our method on selected Deepmind Control
Suite and MetaWorld pretraining tasks. Figure 8 unequivocally demonstrate that direct fine-tuning on in-domain tasks
leads to a performance decline across multiple tasks. However, leveraging the Premier-TACO learning objective for
fine-tuning substantially enhances the performance of R3M. This not only underscores the role of our method in
bridging the domain gap and capturing essential control features but also highlights its robust generalization capabilities.
Furthermore, these findings strongly suggest that our Premier-TACO approach is highly adaptable to a wide range of
multi-task pretraining scenarios, irrespective of the model’s size or the size of the pretrained data.

Figure 9: [(W1) Versatility] (Left) Premier-TACO vs. TACO on 10 Deepmind Control Suite Tasks across different batch sizes.
(Right) Averaged performance of Premier-TACO on 10 Deepmind Control Suite Tasks across different window sizes

Ablation Study - Batch Size: Compared with TACO, the negative example sampling strategy employed in
Premier-TACO allows us to sample harder negative examples within the same episode as the positive example. This

8

Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss

implies the promising potential to significantly improve the performance of existing pretrained models across diverse
domains. The full results of finetuning on all 18 tasks including Deepmind Control Suite and MetaWorld are in
Appendix B.1. We expect Premier-TACO to work much better with small batch sizes, compared with TACO where the
negative examples from a given batch could be coming from various tasks and thus the batch size required would scale
up linearly with the number of pretraining tasks. In ours previous experimental results, Premier-TACO is pretrained
with a batch size of 4096, a standard batch size used in contrastive learning literature. Here, to empirically verify
the effects of different choices of the pretraining batch size, we train Premier-TACO and TACO with different batch
sizes and compare their few-shot imitation learning performance. Figure 9 (left) displays the average performance
of few-shot imitation learning across all ten tasks in the DeepMind Control Suite. As depicted in the figure, our model
significantly outperform TACO across all batch sizes tested in the experiments, and exhibits performance saturation
beyond a batch size of 4096. This observation substantiate that the negative example sampling strategy employed
by Premier-TACO is indeed the key for the success of multitask offline pretraining.

Ablation Study - Window Size: In Premier-TACO, the window size W determines the hardness of the negative
example. A smaller window size results in negative examples that are more challenging to distinguish from positive
examples, though they may become excessively difficult to differentiate in the latent space. Conversely, a larger window
size makes distinguishing relatively straightforward, thereby mitigating the impacts of negative sampling. In preceding
experiments, a consistent window size of 5 was applied across all trials on both the DeepMind Control Suite and
MetaWorld. Here in Figure 9 (right) we empirically evaluate the effects of varying window sizes on the average
performance of our model across ten DeepMind Control Tasks. Notably, our observations reveal that performance is
comparable when the window size is set to 3, 5, or 7, whereas excessively small (W = 1) or large (W = 9) window
sizes lead to worse performance.

5 Related Work

Existing works, including R3M (Nair et al., 2022), VIP (Ma et al., 2023), MVP (Xiao et al., 2022), PIE-G (Yuan et al.,
2022), and VC-1 (Majumdar et al., 2023), focus on self-supervised pre-training for building foundation models but
struggle with the domain gap in sequential decision-making tasks. Recent studies, such as one by Hansen et al. (2022a),
indicate that models trained from scratch often outperform pre-trained representations. Approaches like SMART (Sun
et al., 2023) and DualMind (Wei et al., 2023) offer control-centric pre-training, but at the cost of extensive fine-tuning or
task sets. Contrastive learning techniques like CURL (Laskin et al., 2020), CPC (Henaff, 2020), ST-DIM (Anand et al.,
2019), and ATC (Stooke et al., 2021a) have succeeded in visual RL, but mainly focus on high-level features and temporal
dynamics without a holistic consideration of state-action interactions, a gap partially filled by TACO (Zheng et al.,
2023a). Our work builds upon these efforts but eliminates the need for extensive task sets and fine-tuning, efficiently
capturing control-relevant features. This positions our method as a distinct advancement over DRIML (Mazoure et al.,
2020) and Homer (Misra et al., 2019), which require more computational or empirical resources.

A detailed discussion of related work is in Appendix A.

6 Conclusion

This paper introduces Premier-TACO, a robust and highly generalizable representation pretraining framework for
few-shot policy learning. We propose a temporal contrastive learning objective that excels in multi-task representation
learning during the pretraining phase, thanks to its efficient negative example sampling strategy. Extensive empirical
evaluations spanning diverse domains and tasks underscore the remarkable effectiveness and adaptability of
Premier-TACO’s pre-trained visual representations to unseen tasks, even when confronted with unseen embodiments,
different views, and data imperfections. Furthermore, we demonstrate the versatility of Premier-TACO by showcasing
its ability to fine-tune large pretrained visual representations like R3M (Nair et al., 2022) with domain-specific data,
underscoring its potential for broader applications.

7 Acknowledgement

This work is partially done during Ruijie Zheng’s internship at Microsoft Research. Zheng, Wang, and Huang are
supported by National Science Foundation NSF-IIS-2147276 FAI, DOD-ONR-Office of Naval Research under award
number N00014-22-1-2335, DOD-AFOSR-Air Force Office of Scientific Research under award number FA9550-23-1-
0048, DOD-DARPA-Defense Advanced Research Projects Agency Guaranteeing AI Robustness against Deception
(GARD) HR00112020007, Adobe, Capital One and JP Morgan faculty fellowships.

9

Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss

References

Anand, A., Racah, E., Ozair, S., Bengio, Y., Cˆot´e, M.-A., and Hjelm, R. D. Unsupervised state representation learning
in atari. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in
Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.
neurips.cc/paper_files/paper/2019/file/6fb52e71b837628ac16539c1ff911667-Paper.pdf. 9, 14

Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A.,
Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jackson, T., Jesmonth, S., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y.,
Leal, I., Lee, K.-H., Levine, S., Lu, Y., Malla, U., Manjunath, D., Mordatch, I., Nachum, O., Parada, C., Peralta,
J., Perez, E., Pertsch, K., Quiambao, J., Rao, K., Ryoo, M., Salazar, G., Sanketi, P., Sayed, K., Singh, J., Sontakke,
S., Stone, A., Tan, C., Tran, H., Vanhoucke, V., Vega, S., Vuong, Q., Xia, F., Xiao, T., Xu, P., Xu, S., Yu, T., and
Zitkovich, B. Rt-1: Robotics transformer for real-world control at scale, 2023. 2

Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J.,
Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S.,
Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M.,
Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/
2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. 1

Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database.

IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009. 2, 3

Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for
language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186,
Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL
https://aclanthology.org/N19-1423. 1, 17

Fujimoto, S. and Gu, S. S. A minimalist approach to offline reinforcement learning. In Ranzato, M., Beygelzimer, A.,
Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, volume 34, pp.
20132–20145. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/
2021/file/a8166da05c5a094f7dc03724b41886e5-Paper.pdf. 5

Grauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Girdhar, R., Hamburger, J., Jiang, H., Liu, M., Liu,
X., Martin, M., Nagarajan, T., Radosavovic, I., Ramakrishnan, S. K., Ryan, F., Sharma, J., Wray, M., Xu, M., Xu,
E. Z., Zhao, C., Bansal, S., Batra, D., Cartillier, V., Crane, S., Do, T., Doulaty, M., Erapalli, A., Feichtenhofer, C.,
Fragomeni, A., Fu, Q., Gebreselasie, A., Gonzalez, C., Hillis, J., Huang, X., Huang, Y., Jia, W., Khoo, W., Kolar,
J., Kottur, S., Kumar, A., Landini, F., Li, C., Li, Y., Li, Z., Mangalam, K., Modhugu, R., Munro, J., Murrell, T.,
Nishiyasu, T., Price, W., Puentes, P. R., Ramazanova, M., Sari, L., Somasundaram, K., Southerland, A., Sugano,
Y., Tao, R., Vo, M., Wang, Y., Wu, X., Yagi, T., Zhao, Z., Zhu, Y., Arbelaez, P., Crandall, D., Damen, D., Farinella,
G. M., Fuegen, C., Ghanem, B., Ithapu, V. K., Jawahar, C. V., Joo, H., Kitani, K., Li, H., Newcombe, R., Oliva, A.,
Park, H. S., Rehg, J. M., Sato, Y., Shi, J., Shou, M. Z., Torralba, A., Torresani, L., Yan, M., and Malik, J. Ego4d:
Around the world in 3,000 hours of egocentric video, 2022. 2, 3

Grill, J.-B., Strub, F., Altch´e, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B.,
Guo, Z., Gheshlaghi Azar, M., Piot, B., kavukcuoglu, k., Munos, R., and Valko, M. Bootstrap your own la-
tent - a new approach to self-supervised learning.
In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.,
and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 21271–21284. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/
f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf. 7

Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. Dream to control: Learning behaviors by latent imagination.
In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=
S1lOTC4tDS. 14

Hansen, N., Yuan, Z., Ze, Y., Mu, T., Rajeswaran, A., Su, H., Xu, H., and Wang, X. On pre-training for visuo-motor
control: Revisiting a learning-from-scratch baseline. In CoRL 2022 Workshop on Pre-training Robot Learning,
2022a. URL https://openreview.net/forum?id=tntIAuQ50E. 5, 8, 9, 14, 15

Hansen, N. A., Su, H., and Wang, X. Temporal difference learning for model predictive control. In Chaudhuri, K.,
Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), Proceedings of the 39th International Conference
on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 8387–8406. PMLR, 17–23 Jul
2022b. URL https://proceedings.mlr.press/v162/hansen22a.html. 7

10

Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss

Hansen, N. A., Su, H., and Wang, X. Temporal difference learning for model predictive control. In Chaudhuri, K.,
Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), Proceedings of the 39th International Conference
on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 8387–8406. PMLR, 17–23 Jul
2022c. URL https://proceedings.mlr.press/v162/hansen22a.html. 14

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In 2016 IEEE Conference on

Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2016. doi: 10.1109/CVPR.2016.90. 5

Henaff, O. Data-efficient image recognition with contrastive predictive coding. In III, H. D. and Singh, A. (eds.),
Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine
Learning Research, pp. 4182–4192. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/
henaff20a.html. 9, 14

Kalantidis, Y., Sariyildiz, M. B., Pion, N., Weinzaepfel, P., and Larlus, D. Hard negative mixing for contrastive
learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information
Processing Systems, volume 33, pp. 21798–21809. Curran Associates, Inc., 2020. URL https://proceedings.
neurips.cc/paper_files/paper/2020/file/f7cade80b7cc92b991cf4d2806d6bd78-Paper.pdf. 14
Kim, M., Rho, K., Kim, Y.-d., and Jung, K. Action-driven contrastive representation for reinforcement learning. PLOS
ONE, 17(3):1–14, 03 2022. doi: 10.1371/journal.pone.0265456. URL https://doi.org/10.1371/journal.
pone.0265456. 14

Laskin, M., Srinivas, A., and Abbeel, P. CURL: Contrastive unsupervised representations for reinforcement learning. In
III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of
Proceedings of Machine Learning Research, pp. 5639–5650. PMLR, 13–18 Jul 2020. URL https://proceedings.
mlr.press/v119/laskin20a.html. 6, 9, 14

Lee, S., Seo, Y., Lee, K., Abbeel, P., and Shin, J. Offline-to-online reinforcement learning via balanced replay and
pessimistic q-ensemble. In 5th Annual Conference on Robot Learning, 2021. URL https://openreview.net/
forum?id=AlJXhEI6J5W. 1

Li, J., Selvaraju, R. R., Gotmare, A. D., Joty, S., Xiong, C., and Hoi, S. Align before fuse: Vision and language

representation learning with momentum distillation, 2021. 14

Liu, B., Zhu, Y., Gao, C., Feng, Y., qiang liu, Zhu, Y., and Stone, P. LIBERO: Benchmarking knowledge transfer
for lifelong robot learning. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and
Benchmarks Track, 2023. URL https://openreview.net/forum?id=xzEtNSuDJk. 3, 4, 5, 7, 16, 17, 18

Ma, S., Zeng, Z., McDuff, D., and Song, Y. Active contrastive learning of audio-visual video representations.
In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=
OMizHuea_HB. 14

Ma, Y. J., Sodhani, S., Jayaraman, D., Bastani, O., Kumar, V., and Zhang, A. VIP: Towards universal visual reward and
representation via value-implicit pre-training. In The Eleventh International Conference on Learning Representations,
2023. URL https://openreview.net/forum?id=YJ7o2wetJ2. 2, 9, 14

Majumdar, A., Yadav, K., Arnaud, S., Ma, Y. J., Chen, C., Silwal, S., Jain, A., Berges, V.-P., Abbeel, P., Malik, J., Batra,
D., Lin, Y., Maksymets, O., Rajeswaran, A., and Meier, F. Where are we in the search for an artificial visual cortex
for embodied intelligence?, 2023. 2, 3, 5, 6, 7, 8, 9, 14, 16

Mazoure, B., Tachet des Combes, R., Doan, T. L., Bachman, P., and Hjelm, R. D. Deep reinforcement and infomax
learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information
Processing Systems, volume 33, pp. 3686–3698. Curran Associates, Inc., 2020. URL https://proceedings.
neurips.cc/paper_files/paper/2020/file/26588e932c7ccfa1df309280702fe1b5-Paper.pdf. 9, 14

Mendonca, R., Rybkin, O., Daniilidis, K., Hafner, D., and Pathak, D. Discovering and achieving goals via world models.

Advances in Neural Information Processing Systems, 34:24379–24391, 2021. 14

Misra, D., Henaff, M., Krishnamurthy, A., and Langford, J. Kinematic state abstraction and provably efficient rich-
observation reinforcement learning. CoRR, abs/1911.05815, 2019. URL http://arxiv.org/abs/1911.05815.
9, 14

Nair, S., Rajeswaran, A., Kumar, V., Finn, C., and Gupta, A. R3m: A universal visual representation for robot
manipulation. In 6th Annual Conference on Robot Learning, 2022. URL https://openreview.net/forum?id=
tGbpgz6yOrI. 2, 3, 6, 8, 9, 14, 15, 16

Nam, H. and Han, B. Learning multi-domain convolutional neural networks for visual tracking.

In 2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4293–4302, 2016. doi: 10.1109/CVPR.2016.
465. 14

11

Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss

Pang, J., Chen, K., Shi, J., Feng, H., Ouyang, W., and Lin, D. Libra r-cnn: Towards balanced learning for object

detection. In IEEE Conference on Computer Vision and Pattern Recognition, 2019. 14

Parisi, S., Rajeswaran, A., Purushwalkam, S., and Gupta, A. The unsurprising effectiveness of pre-trained vision models
for control. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), Proceedings of the
39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp.
17359–17371. PMLR, 17–23 Jul 2022. 6, 16

Perez, E., Strub, F., de Vries, H., Dumoulin, V., and Courville, A. C. Film: Visual reasoning with a general conditioning

layer. In AAAI, 2018. 17

Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask

learners. 2019. URL https://api.semanticscholar.org/CorpusID:160025533. 1

Robinson, J. D., Chuang, C.-Y., Sra, S., and Jegelka, S. Contrastive learning with hard negative samples. In International
Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=CR1XOQ0UTh-. 14
Schwarzer, M., Anand, A., Goel, R., Hjelm, R. D., Courville, A., and Bachman, P. Data-efficient reinforcement
learning with self-predictive representations. In International Conference on Learning Representations, 2021a. URL
https://openreview.net/forum?id=uCQfPZwRaUu. 6

Schwarzer, M., Rajkumar, N., Noukhovitch, M., Anand, A., Charlin, L., Hjelm, R. D., Bachman, P., and Courville, A.
Pretraining representations for data-efficient reinforcement learning. In Beygelzimer, A., Dauphin, Y., Liang, P., and
Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021b. URL https://openreview.
net/forum?id=XpSAvlvnMa. 6

Sekar, R., Rybkin, O., Daniilidis, K., Abbeel, P., Hafner, D., and Pathak, D. Planning to explore via self-supervised

world models. In International Conference on Machine Learning, pp. 8583–8592. PMLR, 2020. 14

Seo, Y., Hafner, D., Liu, H., Liu, F., James, S., Lee, K., and Abbeel, P. Masked world models for visual control. In

CoRL, volume 205 of Proceedings of Machine Learning Research, pp. 1332–1344. PMLR, 2022. 7

Shrivastava, A., Gupta, A., and Girshick, R. Training Region-based Object Detectors with Online Hard Example

Mining. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 14

Stooke, A., Lee, K., Abbeel, P., and Laskin, M. Decoupling representation learning from reinforcement learning. In
Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139
of Proceedings of Machine Learning Research, pp. 9870–9879. PMLR, 18–24 Jul 2021a. 6, 9, 14

Stooke, A., Lee, K., Abbeel, P., and Laskin, M. Decoupling representation learning from reinforcement learning. In

International Conference on Machine Learning, pp. 9870–9879. PMLR, 2021b. 2

Sun, Y., Zheng, R., Wang, X., Cohen, A. E., and Huang, F. Transfer RL across observation feature spaces via model-
based regularization. In International Conference on Learning Representations, 2022. URL https://openreview.
net/forum?id=7KdAoOsI81C. 14

Sun, Y., Ma, S., Madaan, R., Bonatti, R., Huang, F., and Kapoor, A. SMART: Self-supervised multi-task pretraining
with control transformers. In The Eleventh International Conference on Learning Representations, 2023. URL
https://openreview.net/forum?id=9piH3Hg8QEf. 3, 5, 6, 7, 9, 14

Tabassum, A., Wahed, M., Eldardiry, H., and Lourentzou, I. Hard negative sampling strategies for contrastive

representation learning, 2022. 14

Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., de Las Casas, D., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq,

A., Lillicrap, T., and Riedmiller, M. Deepmind control suite, 2018. 3, 4

van den Oord, A., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding, 2019. 3, 14
Wan, S., Chen, Z., Zhang, T., Zhang, B., and kat Wong, K. Bootstrapping face detection with hard negative examples,

2016. 14

Wang, X., Zheng, R., Sun, Y., Jia, R., Wongkamjan, W., Xu, H., and Huang, F. Coplanner: Plan to roll out conservatively
but to explore optimistically for model-based rl. In The Twelfth International Conference on Learning Representations,
2024. URL https://openreview.net/forum?id=MSe8YFbhUE. 14

Wei, Y., Sun, Y., Zheng, R., Vemprala, S., Bonatti, R., Chen, S., Madaan, R., Ba, Z., Kapoor, A., and Ma, S. Is imitation
all you need? generalized decision-making with dual-phase training. arXiv preprint arXiv:2307.07909, 2023. 9, 14

Xiao, T., Radosavovic, I., Darrell, T., and Malik, J. Masked visual pre-training for motor control, 2022. 6, 9, 14, 16
Xu, G., Zheng, R., Liang, Y., Wang, X., Zhecheng Yuan, T. J., Luo, Y., Liu, X., Yuan, J., Hua, P., Li, S., Ze, Y., III,
H. D., Huang, F., and Xu, H. Drm: Mastering visual reinforcement learning through dormant ratio minimization.
In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/
forum?id=MSe8YFbhUE. 14

12

Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss

Yarats, D., Fergus, R., Lazaric, A., and Pinto, L. Reinforcement learning with prototypical representations.

In

International Conference on Machine Learning, pp. 11920–11931. PMLR, 2021. 14

Yarats, D., Fergus, R., Lazaric, A., and Pinto, L. Mastering visual continuous control: Improved data-augmented rein-
forcement learning. In International Conference on Learning Representations, 2022. URL https://openreview.
net/forum?id=_SJ-_yyes8. 5, 14, 16

Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine, S. Meta-world: A benchmark and
evaluation for multi-task and meta reinforcement learning. In Conference on Robot Learning (CoRL), 2019. URL
https://arxiv.org/abs/1910.10897. 3, 4

Yuan, Z., Xue, Z., Yuan, B., Wang, X., WU, Y., Gao, Y., and Xu, H. Pre-trained image encoder for gener-
alizable visual reinforcement learning.
In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K.,
and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 13022–13037. Cur-
ran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/
548a482d4496ce109cddfbeae5defa7d-Paper-Conference.pdf. 9, 14

Zhang, A., McAllister, R. T., Calandra, R., Gal, Y., and Levine, S. Learning invariant representations for reinforcement
learning without reconstruction. In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=-2FCwDKRREu. 14

Zheng, R., Wang, X., Sun, Y., Ma, S., Zhao, J., Xu, H., III, H. D., and Huang, F. TACO: Temporal latent action-driven
contrastive loss for visual reinforcement learning. In Thirty-seventh Conference on Neural Information Processing
Systems, 2023a. URL https://openreview.net/forum?id=ezCsMOy1w9. 2, 3, 9, 14

Zheng, R., Wang, X., Xu, H., and Huang, F. Is model ensemble necessary? model-based RL via a single model with
lipschitz regularized value function. In The Eleventh International Conference on Learning Representations, 2023b.
URL https://openreview.net/forum?id=hNyJBk3CwR. 14

13

Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss

A Detailed Discussion of Related Work

Pretraining Visual Representations. Existing works apply self-supervised pre-training from rich vision data to build
foundation models. However, applying this approach to sequential decision-making tasks is challenging. Recent
works have explored large-scale pre-training with offline data in the context of reinforcement learning. Efforts such as
R3M (Nair et al., 2022), VIP (Ma et al., 2023), MVP (Xiao et al., 2022), PIE-G (Yuan et al., 2022), and VC-1 (Majumdar
et al., 2023) highlight this direction. However, there’s a notable gap between the datasets used for pre-training and the
actual downstream tasks. In fact, a recent study (Hansen et al., 2022a) found that models trained from scratch can often
perform better than those using pre-trained representations, suggesting the limitation of these approachs. It’s important
to acknowledge that these pre-trained representations are not control-relevant, and they lack explicit learning of a latent
world model. In contrast to these prior approaches, our pretrained representations learn to capture the control-relevant
features with an effective temporal contrastive learning objective.

For control tasks, several pretraining frameworks have emerged to model state-action interactions from high-dimensional
observations by leveraging causal attention mechanisms. SMART (Sun et al., 2023) introduces a self-supervised and
control-centric objective to train transformer-based models for multitask decision-making, although it requires additional
fine-tuning with large number of demonstrations during downstream time. As an improvement, DualMind (Wei et al.,
2023) pretrains representations using 45 tasks for general-purpose decision-making without task-specific fine-tuning.
Besides, some methods (Sekar et al., 2020; Mendonca et al., 2021; Yarats et al., 2021; Sun et al., 2022) first learn
a general representation by exploring the environment online, and then use this representation to train the policy on
downstream tasks. In comparison, our approach is notably more efficient and doesn’t require training with such an
extensive task set. Nevertheless, we provide empirical evidence demonstrating that our method can effectively handle
multi-task pretraining.

Contrastive Representation for Visual RL Visual RL (Hafner et al., 2020; Yarats et al., 2022; Hansen et al., 2022c;
Zheng et al., 2023b; Wang et al., 2024; Xu et al., 2024) is long-standing challenge due to the entangled problem of
representation learning and credit assignment. Contrastive learning plays a pivotal role in enhancing sample efficiency
of visual RL by leveraging similarity constraints between data to learn effective visual representations. CURL (Laskin
et al., 2020) extracts high-level features by utilizing InfoNCE(van den Oord et al., 2019) to maximize agreement
between augmented observations, although it does not explicitly consider temporal relationships between states. Several
approaches, such as CPC (Henaff, 2020), ST-DIM (Anand et al., 2019), and ATC (Stooke et al., 2021a) , introduce
temporal dynamics into the contrastive loss. They do so by maximizing mutual information between states with
short temporal intervals, facilitating the capture of temporal dependencies. DRIML (Mazoure et al., 2020) proposes a
policy-dependent auxiliary objective that enhances agreement between representations of consecutive states, specifically
considering the first action of the action sequence. Recent advancements by Kim et al. (2022); Zhang et al. (2021)
incorporate actions into the contrastive loss, emphasizing behavioral similarity. TACO (Zheng et al., 2023a) takes
a step further by learning both state and action representations. It optimizes the mutual information between the
representations of current states paired with action sequences and the representations of corresponding future states.
In our approach, we build upon the efficient extension of TACO, harnessing the full potential of state and action
representations for downstream tasks. On the theory side, the Homer algorithm (Misra et al., 2019) uses a binary
temporal contrastive objective reminiscent of the approach used here, which differs by abstracting actions as well states,
using an ancillary embedding, removing leveling from the construction, and of course extensive empirical validation.

Hard Negative Sampling Strategy in Contrastive Learning Our proposed negative example sampling strategy
in Premier-TACO is closely related to hard negative example mining in the literature of self-supervised learning as
well as other areas of machine learning. Hard negative mining is indeed used in a variety of tasks, such as facial
recognition (Wan et al., 2016), object detection (Shrivastava et al., 2016), tracking (Nam & Han, 2016), and image-text
retrieval (Pang et al., 2019; Li et al., 2021), by introducing negative examples that are more difficult than randomly
chosen ones to improve the performance of models. Within the regime of self-supervised learning, different negative
example sampling strategies have also been discussed both empirically and theoretically to improve the quality of
pretrained representation. In particular, Robinson et al. (2021) modifies the original NCE objective by developing a
distribution over negative examples, which prioritizes pairs with currently similar representations. Kalantidis et al.
(2020) suggests to mix hard negative examples within the latent space. Ma et al. (2021) introduce a method to
actively sample uncertain negatives by calculating the gradients of the loss function relative to the model’s most
confident predictions. Furthermore, Tabassum et al. (2022) samples negatives that combine the objectives of identifying
model-uncertain negatives, selecting negatives close to the anchor point in the latent embedding space, and ensuring
representativeness within the sample population.

While our proposed approach bears some resemblance to existing negative sampling strategies in contrastive learning
literature, we are dealing with unique challenges in sequential decision making, such as data distribution shift, task
heterogeneity, and data qualities, as discussed in the introduction. Advancing the work of TACO (Zheng et al.,

14

Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss

2023a), which is specifically designed to capture the control-relevant information in latent representation, Premier-
TACO introduces a straightforward yet effective negative sampling strategy. Tailored toward multitask representation
pretraining, this strategy involves sampling one negative example from a window centered around the anchor point,
which is both computationally efficient and demonstrates superior performance in few-shot adaptation.

B Additional Experiments Results

B.1 Finetuning

Comparisons among R3M (Nair et al., 2022), R3M with in-domain finetuning (Hansen et al., 2022a) and R3M finetuned
with Premier-TACO in Deepmind Control Suite and MetaWorld are presented in Figure 11 and 10.

Figure 10: [(W4) Compatibility] Finetune R3M (Nair et al., 2022), a generalized Pretrained Visual Encoder with Premier-TACO
learning objective vs. R3M with in-domain finetuning in Deepmind Control Suite and MetaWorld.

Figure 11: [(W4) Compatibility] Finetune R3M (Nair et al., 2022), a generalized Pretrained Visual Encoder with Premier-TACO
learning objective vs. R3M with in-domain finetuning in Deepmind Control Suite and MetaWorld.

B.2 Pretrained Visual Representations

Here, we provide the full results for all pretrained visual encoders across all 18 tasks on Deepmind Control Suite and
MetaWorld.

B.3 LIBERO-10 success rate

15

MetaWorld: Finetune Pretrained Visual Encoder

R3M

R3M w. Finetuning

R3M w. Premier-TACO Finetuning

100%

e
t
a
R
s
s
e
c
c
u
S
k
s
a
T

80%

60%

40%

20%

Bin-Picking

Disassemble

Hand-Insert

Peg-Insert-Side

Pick-Out-Of-Hole

Pick-Place-Wall

Shelf-Place

Stick-Pull

 
 
Deepmind Control Suite: Finetune Pretrained Visual Encoder

R3M

R3M w. Finetuning

R3M w. Premier-TACO Finetuning

e
r
o
c
S
e
v
i
t
a
e
R

l

t
r
e
p
x
E

100

80

60

40

20

0

Finger Spin

Walker Walk Humanoid Walk

hopper_hop

Dog Trot

Cup Catch

Cheetah Run Quadruped Walk Quadruped Run

 
 
Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss

DMControl

Pretrained Visual Models

PVR

MVP

R3M

VC-1

MetaWorld

Pretrained Visual Models

PVR

MVP

R3M

VC-1

Finger Spin

11.5 ± 6.0

5.4 ± 7.1

6.9 ± 1.4

38.4 ± 9.3

Bin Picking

45.6 ± 5.6

46.1 ± 3.1

50.0 ± 12.0

60.2 ± 4.3

Hopper Hop

10.2 ± 1.5

7.8 ± 2.7

4.0 ± 0.1

23.2 ± 4.9

Disassemble

47.6 ± 5.8

32.4 ± 5.1

64.4 ± 12.4

70.4 ± 8.9

Walker Walk

10.3 ± 3.8

8.30 ± 1.6

16.7 ± 4.6

30.5 ± 6.2

Hand Insert

18.8 ± 4.0

10.4 ± 5.6

31.8 ± 6.21

35.5 ± 2.3

Humanoid Walk

7.6 ± 3.4

3.2 ± 0.5

2.6 ± 0.4

30.1 ± 7.5

Peg Insert Side

25.3 ± 10.4

28.9 ± 5.4

35.0 ± 3.95

48.2 ± 3.6

Dog Trot

20.5 ± 12.4

32.9 ± 6.0

46.6 ± 4.3

73.5 ± 6.4

Pick Out Of Hole

28.4 ± 5.7

42.3 ± 9.7

42.5 ± 6.4

66.3 ± 7.2

Cup Catch

60.2 ± 10.3

56.7 ± 8.9

93.7 ± 1.8

89.2 ± 13.2

Pick Place Wall

30.7 ± 8.5

42.5 ± 10.9 58.1 ± 16.7

63.2 ± 9.8

Reacher Hard

33.9 ± 9.2

40.7 ± 8.5

42.3 ± 5.6

64.9 ± 5.8

Shelf Place

19.5 ± 6.4

21.2 ± 8.3

18.7 ± 5.15

32.4 ± 6.5

Cheetah Run

26.7 ± 3.8

27.3 ± 4.4

33.1 ± 4.8

39.5 ± 9.7

Stick Pull

30.2 ± 4.6

28.5 ± 9.6

45.6 ± 17.3

52.4 ± 5.6

Quadruped Walk

15.6 ± 9.0

14.5 ± 7.2

18.2 ± 4.9

63.2 ± 4.0

Quadruped Run

40.6 ± 6.7

43.2 ± 4.2

64.0 ± 2.4

61.3 ± 8.5

Table 3: Few-shot results for pretrained visual representations (Parisi et al., 2022; Xiao et al., 2022; Nair et al., 2022; Majumdar
et al., 2023)

LIBERO

Models

Unseen Tasks

LfS

Best PVRs Multitask

Inverse

CURL

ATC

SPR

TACO

Premier-TACO

Task 0

Task 1

Task 2

Task 3

Task 4

Task 5

Task 6

Task 7

Mean

19.9 ± 4.1

21.2 ± 3.5

23.3 ± 4.3

23.3 ± 4.3

16.7 ± 6.2

23.8 ± 6.9

15.0 ± 0.0

5.0 ± 0.0

35.5 ± 7.5

40.0 ± 8.8

46.7 ± 6.2

48.3 ± 10.3

38.3 ± 9.2

26.7 ± 8.5

41.3 ± 7.5

35.0 ± 6.3

40.3 ± 4.1

70.0 ± 5.0

63.3 ± 6.2

65.8 ± 6.7

60.0 ± 4.1

51.6 ± 4.7

35.0 ± 4.1

65.0 ± 9.3

35.0 ± 5.0

65.0 ± 9.3

95.0 ± 7.2

55.7 ± 4.7

56.4 ± 3.2

66.7 ± 8.4

70.1 ± 7.1

70.0 ± 6.8 83.8 ± 6.2 55.0 ± 5.4

62.5 ± 7.3

75.0 ± 13.2

43.3 ± 6.2

27.9 ± 3.9

26.7 ± 3.1

28.3 ± 2.3

18.3 ± 2.5

25.0 ± 6.1

23.7 ± 2.2

15.5 ± 2.4

30.7 ± 2.5

66.7 ± 9.2

62.8 ± 9.3

46.7 ± 3.8

63.3 ± 13.1

78.3 ± 2.4

78.8 ± 7.3

68.7 ± 11.9

52.3 ± 6.2

80.0 ± 6.1

6.7 ± 6.2

14.5 ± 6.7

21.7 ± 2.3

11.6 ± 4.7

23.3 ± 2.4

11.2 ± 4.1

12.5 ± 5.6

19.8 ± 3.8

27.5 ± 7.2

26.7 ± 4.7

29.6 ± 8.9

35.0 ± 7.1

38.3 ± 5.5

16.7 ± 2.4

26.3 ± 4.1

35.0 ± 9.3

22.3 ± 7.9

50.3 ± 4.0

40.3

43.4

38.9

40.6

35.6

44.4

35.0

40.9

58.0

Table 4: [(W1) Versatility (W2) Efficiency] Five-shot Behavior Cloning (BC) for unseen task of LIBERO. Success rate of
Premier-TACO and baselines across first 8 tasks on LIBERO-10. Results are aggregated over 4 random seeds. Bold numbers indicate
the best results.

C Implementation Details

Dataset For six pretraining tasks of Deepmind Control Suite, we train visual RL agents for individual tasks with
DrQ-v2 Yarats et al. (2022) until convergence, and we store all the historical interaction steps in a separate buffer.
Then, we sample 200 trajectories from the buffer for all tasks except for Humanoid Stand and Dog Walk. Since these
two tasks are significantly harder, we use 1000 pretraining trajectories instead. Each episode in Deepmind Control
Suite consists of 500 time steps. In terms of the randomly collected dataset, we sample trajectories by taking actions
with each dimension independently sampled from a uniform distribution U(−1., 1.). For MetaWorld, we collect 1000
trajectories for each task, where each episode consists of 200 time steps. We add a Gaussian noise of standard deviation
0.3 to the provided scripted policy. For LIBERO, we take the human demonstration dataset from Liu et al. (2023),
which contains 50 demosntration trajectories per task.

Pretraining For the shallow convolutional network used in Deepmind Control Suite and MetaWorld, we follow the
same architecture as in Yarats et al. (2022) and add a layer normalization on top of the output of the ConvNet encoder.
We set the feature dimension of the ConNet encoder to be 100. In total, this encoder has around 3.95 million parameters.

1 class Encoder ( nn . Module ) :
2

def __init__ ( self ) :

3

4

5

6

7

8

9

10

11

12

super () . __init__ ()
self . repr_dim = 32 * 35 * 35

self . convnet = nn . Sequential ( nn . Conv2d (84 , 32 , 3 , stride =2) ,

nn . ReLU () , nn . Conv2d (32 , 32 , 3 , stride =1) ,
nn . ReLU () , nn . Conv2d (32 , 32 , 3 , stride =1) ,
nn . ReLU () , nn . Conv2d (32 , 32 , 3 , stride =1) ,
nn . ReLU () )

self . trunk = nn . Sequential ( nn . Linear ( self . repr_dim , feature_dim ) ,

nn . LayerNorm ( feature_dim ) , nn . Tanh () )

16

Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss

13

14

15

16

17

def forward ( self , obs ) :

obs = obs / 255.0 - 0.5
h = self . convnet ( obs ) . view ( h . shape [0] , -1)
return self . trunk ( h )

Listing 1: Shallow Convolutional Network Architecture Used in Premier-TACO

For LIBERO, we use two randomly initialized (or pretrained) ResNet-18 encoders to encode the third-person view and
first-person view images with FiLM (Perez et al., 2018) encoding method to incorporate the BERT embedding Devlin
et al. (2019) of the task language instruction. During downstream behavior cloning, we apply a transformer decoder
module with context length 10 on top of the ResNet encodings to extract the temporal information, and then attach a
two-layer MLP with hidden size 1024 as the policy head. The architecture follows ResNet-T in Liu et al. (2023).

For Premier-TACO loss, the number of timesteps K is set to be 3 throughout the experiments, and the window size W
is fixed to be 5. Action Encoder is a two-layer MLP with input size being the action space dimensionality, hidden size
being 64, and output size being the same as the dimensionality of action space. The projection layer G is a two-layer
MLP with input size being feature dimension plus the number of timesteps times the dimensionality of the action space.
Its hidden size is 1024. In terms of the projection layer H, it is also a two-layer MLP with input and output size both
being the feature dimension and hidden size being 1024. Throughout the experiments, we set the batch size to be 4096
and the learning rate to be 1e-4. For the contrastive/self-supervised based baselines, CURL, ATC, and SPR, we use the
same batch size of 4096 as Premier-TACO. For Multitask TD3+BC and Inverse dynamics modeling baselines, we use a
batch size of 1024.

Imitation Learning A batch size of 128 and a learning rate of 1e-4 are used for Deepmind Control Suite and Metaworld,
and a batch size of 64 is used for LIBERO. During behavior cloning, we finetune the Shallow ConvNet Encoder.
However, when we applied Premier-TACO for the large pre-trained ResNet/ViT encoder models, we keep the model
weights frozen.

In total, we take 100,000 gradient steps and conduct evaluations for every 1000 steps. For evaluations within the
DeepMind Control Suite, we utilize the trained policy to execute 20 episodes, subsequently recording the mean episode
reward. In the case of MetaWorld and LIBERO, we execute 40 episodes and document the success rate of the trained
policy. We report the average of the highest three episode rewards/success rates from the 100 evaluated checkpoints.

Computational Resources For our experiments, we use 8 NVIDIA RTX A6000 with PyTorch Distributed DataParallel
for pretraining visual representations, and we use NVIDIA RTX2080Ti for downstream imitation learning on Deepmind
Control Suite and Metaworld, and RTX A5000 on LIBERO.

17

Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss

D An Additional Ablation Study on Negative Example Sampling Strategy

In Premier-TACO, we sample one negative example from a size W window centered at the positive example for each
data point. However, in principle, we could also use all samples within this window as negative examples instead of
sampling only one. In the table below, we compare the performance of two negative example sampling strategies across
10 unseen Deepmind Control Suite tasks. Bold numbers indicate the better results.

Finger Spin
Hopper Hop
Walker Walk
Humanoid Walk
Dog Trot
Cup Catch
Reacher Hard
Cheetah Run
Quadruped Walk
Quadruped Run
Overall

Sampling 1
75.2 ± 0.6
75.3 ± 4.6
88.0 ± 0.8
51.4 ± 4.9
93.9 ± 5.4
98.9 ± 0.1
81.3 ± 1.8
65.7 ± 1.1
83.2 ± 5.7
76.8 ± 7.5
79.0

Sampling All
70.2 ± 8.4
76.1 ± 3.0
88.5 ± 0.4
56.4 ± 8.9
92.1 ± 4.0
98.3 ± 1.6
80.1 ± 5.8
69.3 ± 2.3
85.4 ± 4.2
82.1 ± 9.1
79.8

Table 5: Results of two different negative sampling strategies on 10 unseen Deepmind Control Suite Tasks.

As shown in Table 5, we find that using all samples from the size W window does not significantly enhance performance
compared to Premier-TACO. Moreover, this approach considerably increases the computational overhead. Given these
results, we chose a more computationally efficient strategy of sampling a single negative example from the size W
window in Premier-TACO.

E Task instructions of downstream LIBERO tasks

Here in table 6, we provide the language instruction for each of the LIBERO downstream task. We refer readers to (Liu
et al., 2023) for more details of the tasks.

TASK ID

TASK SCENE

TASK INSTRUCTION

0
1
2
3
4
5
6
7

LIVING ROOM SCENE2
LIVING ROOM SCENE2
KITCHEN SCENE3
KITCHEN SCENE4
LIVING ROOM SCENE5
STUDY SCENE1
LIVING ROOM SCENE6
LIVING ROOM SCENE1

PUT BOTH THE ALPHABET SOUP AND THE TOMATO SAUCE IN THE BASKET
PUT BOTH THE CREAM CHEESE BOX AND THE BUTTER IN THE BASKET
TURN ON THE STOVE AND PUT THE MOKA POT ON IT
PUT THE BLACK BOWL IN THE BOTTOM DRAWER OF THE CABINET AND CLOSE IT
PUT THE WHITE MUG ON THE LEFT PLATE AND PUT THE YELLOW AND WHITE MUG ON THE RIGHT PLATE
PICK UP THE BOOK AND PLACE IT IN THE BACK COMPARTMENT OF THE CADDY
PUT THE WHITE MUG ON THE PLATE AND PUT THE CHOCOLATE PUDDING TO THE RIGHT OF THE PLATE
PUT BOTH THE ALPHABET SOUP AND THE CREAM CHEESE BOX IN THE BASKET

Table 6: Language instructions for 8 LIBERO downstream tasks.

18

","4 2 0 2 b e F 3 1 ] G L . s c [ 3 v 7 8 1 6 0 . 2 0 4 2 : v i X r a PREMIER-TACO IS A FEW-SHOT POLICY LEARNER : PRETRAINING MULTITASK REPRESENTATION VIA TEMPORAL ACTION-DRIVEN CONTRASTIVE LOSS A PREPRINT Ruijie Zheng1∗ Shuang Ma3 Yongyuan Liang1 Hal Daum´e III1 , 3 Xiyao Wang 1 Huazhe Xu2 John Langford3 Praveen Palanisamy3 Kalyan Shankar Basu3 Furong Huang1 1 University of Maryland , College Park 2 Tsinghua University 3 Microsoft Research rzheng12 @ umd.edu ABSTRACT We present Premier-TACO , a multitask feature representation learning approach designed to improve few-shot policy learning efficiency in sequential decision-making tasks . Premier-TACO leverages a subset of multitask offline datasets for pretraining a general feature representation , which captures critical environmental dynamics and is fine-tuned using minimal expert demonstrations . It advances the temporal action contrastive learning ( TACO ) objective , known for state-of-the-art results in visual control tasks , by incorporating a novel negative example sampling strategy . This strategy is crucial in significantly boosting TACO ’ s computational efficiency , making large-scale multitask offline pretraining feasible . Our extensive empirical evaluation in a diverse set of continuous control benchmarks including Deepmind Control Suite , MetaWorld , and LIBERO demonstrate Premier- TACO ’ s effectiveness in pretraining visual representations , significantly enhancing few-shot imitation learning of novel tasks . Our code , pretraining data , as well as pretrained model checkpoints will be released at https : . 1 Introduction In the dynamic and ever-changing world we inhabit , the importance of sequential decision-making ( SDM ) in machine learning can not be overstated . Unlike static tasks , sequential decisions reflect the fluidity of real-world scenarios , from robotic manipulations to evolving healthcare treatments . Just as foundation models in language , such as BERT ( Devlin et al. , 2019 ) and GPT ( Radford et al. , 2019 ; Brown et al. , 2020 ) , have revolutionized natural language processing by leveraging vast amounts of textual data to understand linguistic nuances , pretrained foundation models hold similar promise for sequential decision-making ( SDM ) . In language , these models capture the essence of syntax , semantics , and context , serving as a robust starting point for a myriad of downstream tasks . Analogously , in SDM , where decisions are influenced by a complex interplay of past actions , current states , and future possibilities , a pretrained foundation model can provide a rich , generalized understanding of decision sequences . This foundational knowledge , built upon diverse decision-making scenarios , can then be fine-tuned to specific tasks , much like how language models are adapted to specific linguistic tasks . The following challenges are unique to sequential decision-making , setting it apart from existing vision and language pretraining paradigms . ( C1 ) Data Distribution Shift : Training data usually consists of specific behavior-policy- generated trajectories . This leads to vastly different data distributions at various stages—pretraining , finetuning , and deployment—resulting in compromised performance ( Lee et al. , 2021 ) . ( C2 ) Task Heterogeneity : Unlike language and vision tasks , which often share semantic features , decision-making tasks vary widely in configurations , transition dynamics , and state and action spaces . This makes it difficult to develop a universally applicable representation . ( C3 ) Data Quality and Supervision : Effective representation learning often relies on high-quality data and expert guidance . ∗The work is partially done during Ruijie Zheng ’ s internship at Microsoft Research . Premier-TACO : Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss Figure 1 : Performance of Premier-TACO pretrained visual representation for few-shot imitation learning on downstream unseen tasks from Deepmind Control Suite , MetaWorld , and LIBERO . LfS here represents learning from scratch . However , these resources are either absent or too costly to obtain in many real-world decision-making tasks ( Brohan et al. , 2023 ; Stooke et al. , 2021b ) . Our aspirational criteria for foundation model for sequential decision-making encompass several key features : ( W1 ) Versatility that allows the model to generalize across a wide array of tasks , even those not previously encountered , such as new embodiments viewed or observations from novel camera angles ; ( W2 ) Efficiency in adapting to downstream tasks , requiring minimal data through few-shot learning techniques ; ( W3 ) Robustness to pretraining data of fluctuating quality , ensuring a resilient foundation ; and ( W4 ) Compatibility with existing large pretrained models such as Nair et al . ( 2022 ) . In light of these challenges and desirables in building foundation models for SDM , our approach to develop foundational models for sequential decision-making focuses on creating a universal and transferable encoder using a reward-free , dynamics based , temporal contrastive pretraining objective . This encoder would be tailored to manage tasks with complex observation spaces , such as visual inputs . By excluding reward signals during the pretraining stage , the model will be better poised to generalize across a broad array of downstream tasks that may have divergent objectives . Leveraging a world-model approach ensures that the encoder learns a compact representation that can capture universal transition dynamics , akin to the laws of physics , thereby making it adaptable for multiple scenarios . This encoder enables the transfer of knowledge to downstream control tasks , even when such tasks were not part of the original pretraining data set . Existing works apply self-supervised pre-training from rich vision data such as ImageNet ( Deng et al. , 2009 ) or Ego4D datasets ( Grauman et al. , 2022 ) to build foundation models ( Nair et al. , 2022 ; Majumdar et al. , 2023 ; Ma et al. , 2023 ) . However , applying these approaches to sequential decision-making tasks is challenging . Specifically , they often overlook control-relevant considerations and suffer from a domain gap between pre-training datasets and downstream control tasks . In this paper , rather than focusing on leveraging large vision datasets , we propose a novel control-centric objective function for pretraining . Our approach , called Premier-TACO ( pretraining multitask representation via temporal action-driven contrastive loss ) , employs a temporal action-driven contrastive loss function for pretraining . This control-centric objective learns a state representation by optimizing the mutual information between representations of current states paired with action sequences and representations of the corresponding future states . Premier-TACO markedly enhances the effectiveness and efficiency of the temporal action contrastive learning ( TACO ) objective , as detailed in Zheng et al . ( 2023a ) , which delivers state-of-the-art outcomes in visual control tasks within a single-task setting . It extends these capabilities to efficient , large-scale multitask offline pretraining , broadening its applicability and performance . Specifically , while TACO considers every data point in a batch as a potential negative example , Premier-TACO strategically samples a single negative example from a proximate window of the subsequent state . This method ensures the negative example is visually akin to the positive one , necessitating that the latent representation captures control-relevant information , rather than relying on extraneous features like visual appearance . This efficient negative example sampling strategy adds no computational burden and is compatible with smaller batch sizes . In particular , on MetaWorld , using a batch size of 1 8 for TACO , Premier-TACO achieves a 25 % relative performance improvement . Premier-TACO can be seamlessly scaled for multitask offline pretraining , enhancing its usability and effectiveness . Below we list our key contributions : ▷ ( 1 ) We introduce Premier-TACO , a new framework designed for the multi-task offline visual representation pretraining of sequential decision-making problems . In particular , we develop a new temporal contrastive learning objective within the Premier-TACO framework . Compared with other temporal contrastive learning objectives such as TACO , Premier-TACO employs a simple yet efficient negative example sampling strategy , making it computationally feasible for multi-task representation learning . 2 Deepmind Control Suite ( 10 tasks ) MetaWorld ( 8 tasks ) LIBERO ( 8 tasks ) Premier-TACO : Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss ▷ ( 2 ) [ ( W1 ) Versatility ( W2 ) Efficiency ] Through extensive empirical evaluation , we verify the effectiveness of Premier-TACO ’ s pretrained visual representations for few-shot learning on unseen tasks . On MetaWorld ( Yu et al. , 2019 ) and LIBERO Liu et al . ( 2023 ) , with 5 expert trajectories , Premier-TACO outperforms the best baseline pretraining method by 37 % and 17 % respectively . Remarkably , in LIBERO , we are the first method to demonstrate benefits from pretraining . On Deepmind Control Suite ( DMC ) ( Tassa et al. , 2018 ) , using only 20 trajectories , which is considerably fewer demonstrations than ( Sun et al. , 2023 ; Majumdar et al. , 2023 ) , Premier-TACO achieves the best performance across 10 challenging tasks , including the hard Dog and Humanoid tasks . This versatility extends even to unseen embodiments in DMC as well as unseen tasks with unseen camera views in MetaWorld . ▷ ( 3 ) [ ( W3 ) Robustness ( W4 ) Compatability ] Furthermore , we demonstrate that Premier-TACO is not only resilient to data of lower quality but also compatible with exisiting large pretrained models . In DMC , Premier-TACO works well with the pretraining dataset collected randomly . Additionally , we showcase the capability of the temporal contrastive learning objective of Premier-TACO to finetune a generalized visual encoder such as R3M ( Nair et al. , 2022 ) , resulting in an averaged performance enhancement of around 50 % across the assessed tasks on Deepmind Control Suite and MetaWorld . 2 Preliminary 2.1 Multitask Offline Pretraining We consider a collection of tasks ( cid:8 ) Ti : ( X , Ai , Pi , Ri , γ ) ( cid:9 ) N i=1 with the same dimensionality in observation space X . Let ϕ : X → Z be a representation function of the agent ’ s observation , which is either randomly initialized or pre-trained already on a large-scale vision dataset such as ImageNet ( Deng et al. , 2009 ) or Ego4D ( Grauman et al. , 2022 ) . Assuming that the agent is given a multitask offline dataset { ( xi , ai , x′ j=1 . The objective is to pretrain a generalizable state representation ϕ or a motor policy π so that when facing an unseen downstream task , it could quickly adapt with few expert demonstrations , using the pretrained representation . Below we summarize the pretraining and finetuning setups . Pretraining : The agent get access to a multitask offline dataset , which could be highly suboptimal . The goal is to learn a generalizable shared state representation from pixel inputs . Adaptation : Adapt to unseen downstream task from few expert demonstration with imitation learning . i , ri ) } of a subset of K tasks { Tnj } K 2.2 TACO : Temporal Action Driven Contrastive Learning Objective Temporal Action-driven Contrastive Learning ( TACO ) ( Zheng et al. , 2023a ) is a reinforcement learning algorithm proposed for addressing the representation learning problem in visual continuous control . It aims to maximize the mutual information between representations of current states paired with action sequences and representations of the corresponding future states : JTACO = I ( Zt+K ; [ Zt , Ut , ... , Ut+K−1 ] ) Here , Zt = ϕ ( Xt ) and Ut = ψ ( At ) represent latent state and action variables . Theoretically , it could be shown that max- imization of this mutual information objective lead to state and action representations that are capable of representing the optimal value functions . Empirically , TACO estimate the lower bound of the mutual information objective by the In- foNCE loss , and it achieves the state of art performance for both online and offline visual continuous control , demonstrat- ing the effectiveness of temporal contrastive learning for representation learning in sequential decision making problems . ( 1 ) 3 Method We introduce Premier-TACO , a generalized pre-training approach specifically formulated to tackle the multi-task pre-training problem , enhancing sample efficiency and generalization ability for downstream tasks . Building upon the success of temporal contrastive loss , exemplified by TACO ( Zheng et al. , 2023a ) , in acquiring latent state representations that encapsulate individual task dynamics , our aim is to foster representation learning that effectively captures the intrinsic dynamics spanning a diverse set of tasks found in offline datasets . Our overarching objective is to ensure that these learned representations exhibit the versatility to generalize across unseen tasks that share the underlying dynamic structures . Nevertheless , when adapted for multitask offline pre-training , the online learning objective of TACO ( Zheng et al. , 2023a ) poses a notable challenge . Specifically , TACO ’ s mechanism , which utilizes the InfoNCE ( van den Oord et al. , 2019 ) loss , categorizes all subsequent states st+k in the batch as negative examples . While this methodology has proven effective in single-task reinforcement learning scenarios , it encounters difficulties when extended to a multitask 3 Premier-TACO : Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss Figure 2 : An illustration of Premier-TACO contrastive loss design . The two ‘ State Encoder ’ s are identical , as are the two ‘ Proj . Layer H ’ s . One negative example is sampled from the neighbors of framework st+K . context . During multitask offline pretraining , image observations within a batch can come from different tasks with vastly different visual appearances , rendering the contrastive InfoNCE loss significantly less effective . Offline Pretraining Objective . We propose a straightforward yet highly effective mechanism for selecting challenging negative ex- amples . Instead of treating all the remaining examples in the batch as negatives , Premier-TACO selects the negative example from a window centered at state st+k within the same episode as shown in Figure 3 . This approach is both computationally efficient and more statistically powerful due to negative examples which are challenging to distinguish from similar positive examples forcing the model capture temporal dynamics differentiating between positive and negative examples . In practice , this allows us to use much smaller batch sizes for Premier-TACO . On MetaWorld , with only 1 8 of the batch size ( 512 vs. 4096 ) , Premier-TACO achieves a 25 % performance gain compared to TACO , saving around 87.5 % of computational time . Figure 3 : Difference between Premier-TACO and TACO for sampling negative examples , [ a ( i ) t In Figure 2 , we illustrate the design of Premier-TACO objective . Specifically , given a batch of state and action sequence t = ϕ ( s ( i ) transitions { ( s ( i ) t ) be latent state and latent action t embeddings respectively . Furthermore , let W centered at st+K : ( st+K−W , ... , st+K−1 , st+K+1 , ... , st+K+W ) with ( cid:103 ) z ( i ) i=1 , let z ( i ) t+K ) } N ( cid:93 ) s ( i ) t+K be a negative example uniformly sampled from the window of size t = ϕ ( ( cid:102 ) s ( i ) t ) a negative latent state . t = ψ ( a ( i ) t+K−1 ] , s ( i ) t ) , u ( i ) , ... , a ( i ) Given these , define g ( i ) future predicted and actual latent states . We optimize : t = Gθ ( z ( i ) , ... , u ( i ) , u ( i ) t t+K−1 ) , ( cid:103 ) h ( i ) t t = Hθ ( ( cid:93 ) t+K ) , and h ( i ) z ( i ) t = Hθ ( z ( i ) t+K ) as embeddings of JPremier-TACO ( ϕ , ψ , Gθ , Hθ ) = − 1 N N ( cid:88 ) i=1 log ⊤ g ( i ) t h ( i ) t+K ⊤ g ( i ) t t+K + ( cid:103 ) g ( i ) h ( i ) t ⊤ h ( i ) t+K ( 2 ) Few-shot Generalization . After pretraining the representation encoder , we leverage our pretrained model Φ to learn policies for downstream tasks . To learn the policy π with the state representation Φ ( st ) as inputs , we use behavior cloning ( BC ) with a few expert demonstrations . For different control domains , we employ significantly fewer demonstrations for unseen tasks than what is typically used in other baselines . This underscores the substantial advantages of Premier-TACO in few-shot generalization . More details about the experiments on downstream tasks will be provided in Section 4 . 4 Experiment In our empirical evaluations , we consider three benchmarks , Deepmind Control Suite ( Tassa et al. , 2018 ) for locomotion control , MetaWorld ( Yu et al. , 2019 ) and LIBERO ( Liu et al. , 2023 ) for robotic manipulation tasks . It is important to note the varied sources of data employed for pretraining in these benchmarks . For the Deepmind Control Suite , 4 Negative Example 𝒔𝒕 '' 𝑲 $ 𝑾 𝒔𝒕 '' 𝑲 '' 𝑾 … … State Encoder … 𝒛𝒕 '' 𝑲 $ 𝑾 𝒛𝒕 '' 𝑲 '' 𝑾 Proj . Layer 𝐻 𝒂𝒕 𝒂𝒕 '' 𝟏 𝒂𝒕 '' 𝑲 $ 𝟏 … Action Encoder … Proj . Layer 𝐺 Contrastive Loss Positive Example 𝒔𝒕 𝒔𝒕 '' 𝑲 State Encoder 𝒛𝒕 𝒛𝒕 '' 𝑲 Proj . Layer 𝐻 Premier-TACO 𝒔𝒕 '' 𝑲 $ 𝑾 𝒔𝒕 '' 𝑲 𝒔𝒕 '' 𝑲 '' 𝑾 TACO … … Sample . . . . . . h c t a B 1 Negative Example N-1 Negative Examples Premier-TACO : Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss Figure 4 : Pretrain and Test Tasks split for Deepmind Control Suite , MetaWorld and Libero . The left figures are Deepmind Control Suite tasks and the right figures MetaWorld tasks . our pretraining dataset comes from the replay buffers of online reinforcement learning ( RL ) agents . In MetaWorld , the dataset is generated through a pre-defined scripted policy . In LIBERO , we utilize its provided demonstration dataset , which was collected through human teleoperation . By evaluating on a wide range of pretraining data types that have been explored in previous works , we aim to provide a comprehensive evaluation for the pretraining effects of Premier-TACO . Deepmind Control Suite ( DMC ) : We consider a selection of 16 challenging tasks from Deepmind Control Suite . Note that compared with prior works such as Majumdar et al . ( 2023 ) ; Sun et al . ( 2023 ) , we consider much harder tasks , including ones from the humanoid and dog domains , which feature intricate kinematics , skinning weights and collision geometry . For pretraining , we select six tasks ( DMC-6 ) , including Acrobot Swingup , Finger Turn Hard , Hopper Stand , Walker Run , Humanoid Walk , and Dog Stand . We generate an exploratory dataset for each task by sampling trajectories generated in exploratory stages of a DrQ-v2 ( Yarats et al. , 2022 ) learning agent . In particular , we sample 1000 trajectories from the online replay buffer of DrQ-v2 once it reaches the convergence performance . This ensures the diversity of the pretraining data , but in practice , such a high-quality dataset could be hard to obtain . So , later in the experiments , we will also relax this assumption and consider pretrained trajectories that are sampled from uniformly random actions . In terms of the encoder architecture , we pretrain Premier-TACO with the same shallow ConvNet encoder as in DrQv2 ( Yarats et al. , 2022 ) . MetaWorld : We select a set of 10 tasks for pretraining , which encompasses a variety of motion patterns of the Sawyer robotic arm and interaction with different objects . To collect an exploratory dataset for pretraining , we execute the scripted policy with Gaussian noise of a standard deviation of 0.3 added to the action . After adding such a noise , the success rate of collected policies on average is only around 20 % across ten pretrained tasks . We use the same encoder network architecture as DMC . LIBERO : We pretrain on 90 short-horizon manipulation tasks ( LIBERO-90 ) with human demonstration dataset provided by the original paper . For each task , it contains 50 trajectories of human teleoperated trajectories . We use ResNet18 encoder ( He et al. , 2016 ) to encode the image observations of resolution 128 × 128 . For the downstream task , we assess the few-shot imitation learning performance on the first 8 long-horizon tasks of LIBERO-LONG . Baselines . We compare Premier-TACO with the following representation pretraining baselines : ▷ Learn from Scratch : Behavior Cloning with randomly initialized shallow ConvNet encoder . We carefully implement the behavior cloning from scratch baseline . For DMC and MetaWorld , following Hansen et al . ( 2022a ) , we include the random shift data augmentation into behavior cloning . For LIBERO , we take the ResNet-T model architecture in Liu et al . ( 2023 ) , which uses a transformer decoder module on top of the ResNet encoding to extract temporal information from a sequence of observations , addressing the non-Markovian characteristics inherent in human demonstration . ▷ Policy Pretraining : We first train a multitask policy by TD3+BC ( Fujimoto & Gu , 2021 ) on the pretraining dataset . While numerous alternative offline RL algorithms exist , we choose TD3+BC as a representative due to its simplicity and great empirical performance . For LIBERO , we use Multitask BC since offline RL in generally does not perform well on the imitation learning benchmark with human demonstrated dataset . After pretraining , we take the pretrained ConvNet encoder and drop the policy MLP layers . 5 Premier-TACO : Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss DMControl Models Tasks LfS SMART Best PVRs TD3+BC Inverse CURL ATC SPR TACO Premier-TACO Seen Embodiments Unseen Embodiments Finger Spin 34.8±3.4 44.2 ± 8.2 38.4 ± 9.3 68.8 ± 7.1 33.4±8.4 35.1±9.6 51.1±9.4 55.9±6.2 28.4±9.7 75.2 ± 0.6 Hopper Hop 8.0 ± 1.3 14.2 ± 3.9 23.2 ± 4.9 49.1 ± 4.3 48.3±5.2 28.7±5.2 34.9±3.9 52.3±7.8 21.4±3.4 75.3 ± 4.6 Walker Walk 30.4±2.9 54.1 ± 5.2 32.6 ± 8.7 65.8 ± 2.0 64.4±5.6 37.3±7.9 44.6±5.0 72.9±1.5 30.6±6.1 88.0 ± 0.8 Humanoid Walk 15.1±1.3 18.4 ± 3.9 30.1 ± 7.5 34.9 ± 8.5 41.9±8.4 19.4±2.8 35.1±3.1 30.1±6.2 29.1±8.1 51.4 ± 4.9 Dog Trot 52.7±3.5 59.7 ± 5.2 73.5 ± 6.4 82.3 ± 4.4 85.3±2.1 71.9±2.2 84.3±0.5 79.9±3.8 80.1±4.1 93.9 ± 5.4 Cup Catch 56.8±5.6 66.8 ± 6.2 93.7 ± 1.8 97.1 ± 1.7 96.7±2.6 96.7±2.6 96.2±1.4 96.9±3.1 88.7±3.2 98.9 ± 0.1 Reacher Hard 34.6±4.1 52.1 ± 3.8 64.9 ± 5.8 59.6 ± 9.9 61.7±4.6 50.4±4.6 56.9±9.8 62.5±7.8 58.3±6.4 81.3 ± 1.8 Cheetah Run 25.1±2.9 41.1 ± 7.2 39.5 ± 9.7 50.9 ± 2.6 51.5±5.5 36.8±5.4 30.1±1.0 40.2±9.6 23.2±3.3 65.7 ± 1.1 Quadruped Walk 61.1±5.7 45.4 ± 4.3 63.2 ± 4.0 76.6 ± 7.4 82.4±6.7 72.8±8.9 81.9±5.6 65.6±4.0 63.9±9.3 83.2 ± 5.7 Quadruped Run 45.0±2.9 27.9 ± 5.3 64.0 ± 2.4 48.2 ± 5.2 52.1±1.8 55.1±5.4 2.6 ± 3.6 68.2±3.2 50.8±5.7 76.8 ± 7.5 Mean Performance 38.2 42.9 52.3 63.3 61.7 50.4 52.7 62.4 47.5 79.0 Table 1 : [ ( W1 ) Versatility ( W2 ) Efficiency ] Few-shot Behavior Cloning ( BC ) for unseen task of DMC . Performance ( Agent Reward / Expert Reward ) of baselines and Premier-TACO on 10 unseen tasks on Deepmind Control Suite . Bold numbers indicate the best results . Agent Policies are evaluated every 1000 gradient steps for a total of 100000 gradient steps and we report the average performance over the 3 best epochs over the course of learning . Premier-TACO outperforms all the baselines , showcasing its superior efficacy in generalizing to unseen tasks with seen or unseen embodiments . MetaWorld Models Unseen Tasks LfS SMART Best PVRs TD3+BC Inverse CURL ATC SPR TACO Premier-TACO Bin Picking Disassemble Hand Insert 62.5 ± 12.5 71.3 ± 9.6 60.2 ± 4.3 50.6 ± 3.7 55.0 ± 7.9 45.6 ± 5.6 55.6 ± 7.8 67.9 ± 6.4 67.3 ± 7.5 78.5 ± 7.2 56.3 ± 6.5 52.9 ± 4.5 70.4 ± 8.9 56.9 ± 11.5 53.8 ± 8.1 66.2 ± 8.3 45.6 ± 9.8 48.8 ± 5.4 51.3 ± 10.8 86.7 ± 8.9 34.7 ± 7.5 34.1 ± 5.2 35.5 ± 2.3 46.2 ± 5.2 50.0 ± 3.5 49.4 ± 7.6 51.2 ± 1.3 52.4 ± 5.2 56.8 ± 4.2 75.0 ± 7.1 Peg Insert Side 28.7 ± 2.0 20.9 ± 3.6 48.2 ± 3.6 30.0 ± 6.1 33.1 ± 6.2 28.1 ± 3.7 31.8 ± 4.8 39.2 ± 7.4 36.3 ± 4.5 62.7 ± 4.7 Pick Out Of Hole 53.7 ± 6.7 65.9 ± 7.8 66.3 ± 7.2 46.9 ± 7.4 50.6 ± 5.1 43.1 ± 6.2 54.4 ± 8.5 55.3 ± 6.8 52.9 ± 7.3 72.7 ± 7.3 Pick Place Wall 40.5 ± 4.5 62.8 ± 5.9 63.2 ± 9.8 63.8 ± 12.4 71.3 ± 11.3 73.8 ± 11.9 68.7 ± 5.5 72.3 ± 7.5 37.8 ± 8.5 80.2 ± 8.2 Shelf Place Stick Pull Mean 26.3 ± 4.1 57.9 ± 4.5 32.4 ± 6.5 45.0 ± 7.7 36.9 ± 6.7 35.0 ± 10.8 35.6 ± 10.7 38.0 ± 6.5 25.8 ± 5.0 70.4 ± 8.1 46.3 ± 7.2 65.8 ± 8.2 52.4 ± 5.6 72.3 ± 11.9 57.5 ± 9.5 43.1 ± 15.2 72.5 ± 8.9 68.5 ± 9.4 52.0 ± 10.5 80.0 ± 8.1 43.6 53.9 53.6 51.5 51.0 48.3 51.9 55.3 47.5 75.8 Table 2 : [ ( W1 ) Versatility ( W2 ) Efficiency ] Five-shot Behavior Cloning ( BC ) for unseen task of MetaWorld . Success rate of Premier-TACO and baselines across 8 hard unseen tasks on MetaWorld . Results are aggregated over 4 random seeds . Bold numbers indicate the best results . Figure 5 : [ ( W1 ) Versatility ( W2 ) Efficiency ] Mean success rate of 5-shot imitation learning for 8 unseen tasks in LIBERO . Results are aggregated over 4 random seeds . Bold numbers indicate the best results . See the results for individual tasks in Table 4 . ▷ Pretrained Visual Representations ( PVRs ) : We evaluate the state-of-the-art frozen pretrained visual representations including PVR ( Parisi et al. , 2022 ) , MVP ( Xiao et al. , 2022 ) , R3M ( Nair et al. , 2022 ) and VC-1 ( Majumdar et al. , 2023 ) , and report the best performance of these PVRs models for each task . ▷ Control Transformer : SMART ( Sun et al. , 2023 ) is a self-supervised representation pretraining framework which utilizes a maksed prediction objective for pretraining representation under Decision Transformer architecture , and then use the pretrained representation to learn policies for downstream tasks . ▷ Inverse Dynamics Model : We pretrain an inverse dynamics model to predict actions and use the pretrained representation for downstream task . ▷ Contrastive/Self-supervised Learning Objectives : CURL ( Laskin et al. , 2020 ) , ATC ( Stooke et al. , 2021a ) , SPR ( Schwarzer et al. , 2021a , b ) . CURL and ATC are two approaches that apply contrastive learning into sequential decision making problems . While CURL treats augmented states as positive pairs , it neglects the temporal dependency 6 Premier-TACO : Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss of MDP . In comparison , ATC takes the temporal structure into consideration . The positive example of ATC is an augmented view of a temporally nearby state . SPR applies BYOL objecive ( Grill et al. , 2020 ) into sequential decision making problems by pretraining state representations that are self-predictive of future states . Pretrained feature representation by Premier-TACO facilitates effective few-shot adaptation to unseen tasks . We measure the performance of pretrained visual representation for few-shot imitation learning of unseen downstream tasks in both DMC and MetaWorld . In particular , for DMC , we use 20 expert trajectories for imitation learning except for the two hardest tasks , Humanoid Walk and Dog Trot , for which we use 100 trajectories instead . Note that we only use 1 10 of those used in Sun et al . ( 2023 ) . 5 of the number of expert trajectories used in Majumdar et al . ( 2023 ) and 1 Agent Reward Expert Reward We record the performance of the agent by calculating the ratio of , where Expert Reward is the episode reward of the expert policy used to collect demonstration trajectories . For MetaWorld and LIBERO , we use 5 expert trajectories for all downstream tasks , and we use task success rate as the performance metric . In Table 1 Table 2 , and Figure 5 we present the results for Deepmind Control Suite , MetaWorld , and LIBERO , respectively . As shown here , pretrained representation of Premier-TACO significantly improves the few-shot imitation learning performance compared with Learn-from-scratch , with a 101 % improvement on Deepmind Control Suite and 74 % improvement on MetaWorld , respectively . Moreover , it also outperforms all the baselines across all tasks by a large margin . In LIBERO , consistent with what is observed in ( Liu et al. , 2023 ) , existing pretraining methods on large-scale multitask offline dataset fail to enhance downstream policy learning performance . In particular , methods like multitask pretraining actually degrade downstream policy learning performance . In contrast , using ResNet-18 encoders pretrained by Premier-TACO significantly boosts few-shot imitation learning performance by a substantial margin . Premier-TACO pre-trained representation enables knowledge sharing across different embodiments . Ideally , a resilient and generalizable state feature representation ought not only to encapsulate universally applicable features for a given embodiment across a variety of tasks , but also to exhibit the capability to generalize across distinct embodiments . Here , we evaluate the few-shot behavior cloning performance of Premier-TACO pre-trained encoder from DMC-6 on four tasks featuring unseen embodiments : Cup Catch , Cheetah Run , and Quadruped Walk . In comparison to Learn-from-scratch , as shown in Figure 6 ( left ) , Premier-TACO pre-trained representation realizes an 82 % performance gain , demonstrating the robust generalizability of our pre-trained feature representations . Figure 6 : [ ( W1 ) Versatility ] ( Left ) DMC : Generalization of Premier-TACO pre-trained visual representation to unseen embodiments . ( Right ) MetaWorld : Few-shot adaptation to unseen tasks from an unseen camera view Premier-TACO Pretrained Representation is also generalizable to unseen tasks with camera views . Beyond generalizing to unseen embodiments , an ideal robust visual representation should possess the capacity to adapt to unfamiliar tasks under novel camera views . In Figure 6 ( right ) , we evaluate the five-shot learning performance of our model on four previously unseen tasks in MetaWorld with a new view . In particular , during pretraining , the data from MetaWorld are generated using the same view as employed in ( Hansen et al. , 2022b ; Seo et al. , 2022 ) . Then for downstream policy learning , the agent is given five expert trajectories under a different corner camera view , as depicted in the figure . Notably , Premier-TACO also achieves a substantial performance enhancement , thereby underscoring the robust generalizability of our pretrained visual representation . Premier-TACO Pre-trained Representation is resilient to low-quality data . We evaluate the resilience of Premier- TACO by employing randomly collected trajectory data from Deepmind Control Suite for pretraining and compare it with Premier-TACO representations pretrained using an exploratory dataset and the learn-from-scratch approach . As illustrated in Figure 7 , across all downstream tasks , even when using randomly pretrained data , the Premier- TACO pretrained model still maintains a significant advantage over learning-from-scratch . When compared with representations pretrained using exploratory data , there are only small disparities in a few individual tasks , while they remain comparable in most other tasks . This strongly indicates the robustness of Premier-TACO to low-quality data . Even without the use of expert control data , our method is capable of extracting valuable information . 7 Premier-TACO : Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss Figure 7 : [ ( W3 ) Robustness ] Premier-TACO pretrained with exploratory dataset vs. Premier-TACO pretrained with randomly collected dataset Figure 8 : [ ( W4 ) Compatibility ] Finetune R3M ( Nair et al. , 2022 ) , a generalized Pretrained Visual Encoder with Premier-TACO learning objective vs. R3M with in-domain finetuning in Deepmind Control Suite and MetaWorld . Pretrained visual encoder finetuning with Premier-TACO . In addition to evaluating our pretrained representations across various downstream scenarios , we also conducted fine-tuning on pretrained visual representations using in- domain control trajectories following Premier-TACO framework . Importantly , our findings deviate from the observations made in prior works like ( Hansen et al. , 2022a ) and ( Majumdar et al. , 2023 ) , where fine-tuning of R3M ( Nair et al. , 2022 ) on in-domain demonstration data using the task-centric behavior cloning objective , resulted in performance degradation . We speculate that two main factors contribute to this phenomenon . First , a domain gap exists between out-of-domain pretraining data and in-domain fine-tuning data . Second , fine-tuning with few-shot learning can lead to overfitting for large pretrained models . To further validate the effectiveness of our Premier-TACO approach , we compared the results of R3M with no fine- tuning , in-domain fine-tuning ( Hansen et al. , 2022a ) , and fine-tuning using our method on selected Deepmind Control Suite and MetaWorld pretraining tasks . Figure 8 unequivocally demonstrate that direct fine-tuning on in-domain tasks leads to a performance decline across multiple tasks . However , leveraging the Premier-TACO learning objective for fine-tuning substantially enhances the performance of R3M . This not only underscores the role of our method in bridging the domain gap and capturing essential control features but also highlights its robust generalization capabilities . Furthermore , these findings strongly suggest that our Premier-TACO approach is highly adaptable to a wide range of multi-task pretraining scenarios , irrespective of the model ’ s size or the size of the pretrained data . Figure 9 : [ ( W1 ) Versatility ] ( Left ) Premier-TACO vs. TACO on 10 Deepmind Control Suite Tasks across different batch sizes . ( Right ) Averaged performance of Premier-TACO on 10 Deepmind Control Suite Tasks across different window sizes Ablation Study - Batch Size : Compared with TACO , the negative example sampling strategy employed in Premier-TACO allows us to sample harder negative examples within the same episode as the positive example . This 8 Premier-TACO : Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss implies the promising potential to significantly improve the performance of existing pretrained models across diverse domains . The full results of finetuning on all 18 tasks including Deepmind Control Suite and MetaWorld are in Appendix B.1 . We expect Premier-TACO to work much better with small batch sizes , compared with TACO where the negative examples from a given batch could be coming from various tasks and thus the batch size required would scale up linearly with the number of pretraining tasks . In ours previous experimental results , Premier-TACO is pretrained with a batch size of 4096 , a standard batch size used in contrastive learning literature . Here , to empirically verify the effects of different choices of the pretraining batch size , we train Premier-TACO and TACO with different batch sizes and compare their few-shot imitation learning performance . Figure 9 ( left ) displays the average performance of few-shot imitation learning across all ten tasks in the DeepMind Control Suite . As depicted in the figure , our model significantly outperform TACO across all batch sizes tested in the experiments , and exhibits performance saturation beyond a batch size of 4096 . This observation substantiate that the negative example sampling strategy employed by Premier-TACO is indeed the key for the success of multitask offline pretraining . Ablation Study - Window Size : In Premier-TACO , the window size W determines the hardness of the negative example . A smaller window size results in negative examples that are more challenging to distinguish from positive examples , though they may become excessively difficult to differentiate in the latent space . Conversely , a larger window size makes distinguishing relatively straightforward , thereby mitigating the impacts of negative sampling . In preceding experiments , a consistent window size of 5 was applied across all trials on both the DeepMind Control Suite and MetaWorld . Here in Figure 9 ( right ) we empirically evaluate the effects of varying window sizes on the average performance of our model across ten DeepMind Control Tasks . Notably , our observations reveal that performance is comparable when the window size is set to 3 , 5 , or 7 , whereas excessively small ( W = 1 ) or large ( W = 9 ) window sizes lead to worse performance . 5 Related Work Existing works , including R3M ( Nair et al. , 2022 ) , VIP ( Ma et al. , 2023 ) , MVP ( Xiao et al. , 2022 ) , PIE-G ( Yuan et al. , 2022 ) , and VC-1 ( Majumdar et al. , 2023 ) , focus on self-supervised pre-training for building foundation models but struggle with the domain gap in sequential decision-making tasks . Recent studies , such as one by Hansen et al . ( 2022a ) , indicate that models trained from scratch often outperform pre-trained representations . Approaches like SMART ( Sun et al. , 2023 ) and DualMind ( Wei et al. , 2023 ) offer control-centric pre-training , but at the cost of extensive fine-tuning or task sets . Contrastive learning techniques like CURL ( Laskin et al. , 2020 ) , CPC ( Henaff , 2020 ) , ST-DIM ( Anand et al. , 2019 ) , and ATC ( Stooke et al. , 2021a ) have succeeded in visual RL , but mainly focus on high-level features and temporal dynamics without a holistic consideration of state-action interactions , a gap partially filled by TACO ( Zheng et al. , 2023a ) . Our work builds upon these efforts but eliminates the need for extensive task sets and fine-tuning , efficiently capturing control-relevant features . This positions our method as a distinct advancement over DRIML ( Mazoure et al. , 2020 ) and Homer ( Misra et al. , 2019 ) , which require more computational or empirical resources . A detailed discussion of related work is in Appendix A . 6 Conclusion This paper introduces Premier-TACO , a robust and highly generalizable representation pretraining framework for few-shot policy learning . We propose a temporal contrastive learning objective that excels in multi-task representation learning during the pretraining phase , thanks to its efficient negative example sampling strategy . Extensive empirical evaluations spanning diverse domains and tasks underscore the remarkable effectiveness and adaptability of Premier-TACO ’ s pre-trained visual representations to unseen tasks , even when confronted with unseen embodiments , different views , and data imperfections . Furthermore , we demonstrate the versatility of Premier-TACO by showcasing its ability to fine-tune large pretrained visual representations like R3M ( Nair et al. , 2022 ) with domain-specific data , underscoring its potential for broader applications . 7 Acknowledgement This work is partially done during Ruijie Zheng ’ s internship at Microsoft Research . Zheng , Wang , and Huang are supported by National Science Foundation NSF-IIS-2147276 FAI , DOD-ONR-Office of Naval Research under award number N00014-22-1-2335 , DOD-AFOSR-Air Force Office of Scientific Research under award number FA9550-23-1- 0048 , DOD-DARPA-Defense Advanced Research Projects Agency Guaranteeing AI Robustness against Deception ( GARD ) HR00112020007 , Adobe , Capital One and JP Morgan faculty fellowships . 9 Premier-TACO : Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss References Anand , A. , Racah , E. , Ozair , S. , Bengio , Y. , Cˆot´e , M.-A. , and Hjelm , R. D. Unsupervised state representation learning in atari . In Wallach , H. , Larochelle , H. , Beygelzimer , A. , d'Alch´e-Buc , F. , Fox , E. , and Garnett , R . ( eds . ) , Advances in Neural Information Processing Systems , volume 32 . Curran Associates , Inc. , 2019 . URL https : //proceedings . . 9 , 14 Brohan , A. , Brown , N. , Carbajal , J. , Chebotar , Y. , Dabis , J. , Finn , C. , Gopalakrishnan , K. , Hausman , K. , Herzog , A. , Hsu , J. , Ibarz , J. , Ichter , B. , Irpan , A. , Jackson , T. , Jesmonth , S. , Joshi , N. J. , Julian , R. , Kalashnikov , D. , Kuang , Y. , Leal , I. , Lee , K.-H. , Levine , S. , Lu , Y. , Malla , U. , Manjunath , D. , Mordatch , I. , Nachum , O. , Parada , C. , Peralta , J. , Perez , E. , Pertsch , K. , Quiambao , J. , Rao , K. , Ryoo , M. , Salazar , G. , Sanketi , P. , Sayed , K. , Singh , J. , Sontakke , S. , Stone , A. , Tan , C. , Tran , H. , Vanhoucke , V. , Vega , S. , Vuong , Q. , Xia , F. , Xiao , T. , Xu , P. , Xu , S. , Yu , T. , and Zitkovich , B. Rt-1 : Robotics transformer for real-world control at scale , 2023 . 2 Brown , T. , Mann , B. , Ryder , N. , Subbiah , M. , Kaplan , J. D. , Dhariwal , P. , Neelakantan , A. , Shyam , P. , Sastry , G. , Askell , A. , Agarwal , S. , Herbert-Voss , A. , Krueger , G. , Henighan , T. , Child , R. , Ramesh , A. , Ziegler , D. , Wu , J. , Winter , C. , Hesse , C. , Chen , M. , Sigler , E. , Litwin , M. , Gray , S. , Chess , B. , Clark , J. , Berner , C. , McCandlish , S. , Radford , A. , Sutskever , I. , and Amodei , D. Language models are few-shot learners . In Larochelle , H. , Ranzato , M. , Hadsell , R. , Balcan , M. , and Lin , H . ( eds . ) , Advances in Neural Information Processing Systems , volume 33 , pp . 1877–1901 . Curran Associates , Inc. , 2020 . URL https : . 1 Deng , J. , Dong , W. , Socher , R. , Li , L.-J. , Li , K. , and Fei-Fei , L. Imagenet : A large-scale hierarchical image database . IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , 2009 . 2 , 3 Devlin , J. , Chang , M.-W. , Lee , K. , and Toutanova , K. BERT : Pre-training of deep bidirectional transformers for language understanding . In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pp . 4171–4186 , Minneapolis , Minnesota , June 2019 . Association for Computational Linguistics . doi : 10.18653/v1/N19-1423 . URL https : //aclanthology.org/N19-1423 . 1 , 17 Fujimoto , S. and Gu , S. S. A minimalist approach to offline reinforcement learning . In Ranzato , M. , Beygelzimer , A. , Dauphin , Y. , Liang , P. , and Vaughan , J. W . ( eds . ) , Advances in Neural Information Processing Systems , volume 34 , pp . 20132–20145 . Curran Associates , Inc. , 2021 . URL https : . 5 Grauman , K. , Westbury , A. , Byrne , E. , Chavis , Z. , Furnari , A. , Girdhar , R. , Hamburger , J. , Jiang , H. , Liu , M. , Liu , X. , Martin , M. , Nagarajan , T. , Radosavovic , I. , Ramakrishnan , S. K. , Ryan , F. , Sharma , J. , Wray , M. , Xu , M. , Xu , E. Z. , Zhao , C. , Bansal , S. , Batra , D. , Cartillier , V. , Crane , S. , Do , T. , Doulaty , M. , Erapalli , A. , Feichtenhofer , C. , Fragomeni , A. , Fu , Q. , Gebreselasie , A. , Gonzalez , C. , Hillis , J. , Huang , X. , Huang , Y. , Jia , W. , Khoo , W. , Kolar , J. , Kottur , S. , Kumar , A. , Landini , F. , Li , C. , Li , Y. , Li , Z. , Mangalam , K. , Modhugu , R. , Munro , J. , Murrell , T. , Nishiyasu , T. , Price , W. , Puentes , P. R. , Ramazanova , M. , Sari , L. , Somasundaram , K. , Southerland , A. , Sugano , Y. , Tao , R. , Vo , M. , Wang , Y. , Wu , X. , Yagi , T. , Zhao , Z. , Zhu , Y. , Arbelaez , P. , Crandall , D. , Damen , D. , Farinella , G. M. , Fuegen , C. , Ghanem , B. , Ithapu , V. K. , Jawahar , C. V. , Joo , H. , Kitani , K. , Li , H. , Newcombe , R. , Oliva , A. , Park , H. S. , Rehg , J. M. , Sato , Y. , Shi , J. , Shou , M. Z. , Torralba , A. , Torresani , L. , Yan , M. , and Malik , J. Ego4d : Around the world in 3,000 hours of egocentric video , 2022 . 2 , 3 Grill , J.-B. , Strub , F. , Altch´e , F. , Tallec , C. , Richemond , P. , Buchatskaya , E. , Doersch , C. , Avila Pires , B. , Guo , Z. , Gheshlaghi Azar , M. , Piot , B. , kavukcuoglu , k. , Munos , R. , and Valko , M. Bootstrap your own la- tent - a new approach to self-supervised learning . In Larochelle , H. , Ranzato , M. , Hadsell , R. , Balcan , M. , and Lin , H . ( eds . ) , Advances in Neural Information Processing Systems , volume 33 , pp . 21271–21284 . Cur- ran Associates , Inc. , 2020 . URL https : . 7 Hafner , D. , Lillicrap , T. , Ba , J. , and Norouzi , M. Dream to control : Learning behaviors by latent imagination . In International Conference on Learning Representations , 2020 . URL https : //openreview.net/forum ? id= S1lOTC4tDS . 14 Hansen , N. , Yuan , Z. , Ze , Y. , Mu , T. , Rajeswaran , A. , Su , H. , Xu , H. , and Wang , X . On pre-training for visuo-motor control : Revisiting a learning-from-scratch baseline . In CoRL 2022 Workshop on Pre-training Robot Learning , 2022a . URL https : //openreview.net/forum ? id=tntIAuQ50E . 5 , 8 , 9 , 14 , 15 Hansen , N. A. , Su , H. , and Wang , X. Temporal difference learning for model predictive control . In Chaudhuri , K. , Jegelka , S. , Song , L. , Szepesvari , C. , Niu , G. , and Sabato , S . ( eds . ) , Proceedings of the 39th International Conference on Machine Learning , volume 162 of Proceedings of Machine Learning Research , pp . 8387–8406 . PMLR , 17–23 Jul 2022b . URL https : . 7 10 Premier-TACO : Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss Hansen , N. A. , Su , H. , and Wang , X. Temporal difference learning for model predictive control . In Chaudhuri , K. , Jegelka , S. , Song , L. , Szepesvari , C. , Niu , G. , and Sabato , S . ( eds . ) , Proceedings of the 39th International Conference on Machine Learning , volume 162 of Proceedings of Machine Learning Research , pp . 8387–8406 . PMLR , 17–23 Jul 2022c . URL https : . 14 He , K. , Zhang , X. , Ren , S. , and Sun , J . Deep residual learning for image recognition . In 2016 IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , pp . 770–778 , 2016. doi : 10.1109/CVPR.2016.90 . 5 Henaff , O. Data-efficient image recognition with contrastive predictive coding . In III , H. D. and Singh , A . ( eds . ) , Proceedings of the 37th International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research , pp . 4182–4192 . PMLR , 13–18 Jul 2020 . URL https : //proceedings.mlr.press/v119/ henaff20a.html . 9 , 14 Kalantidis , Y. , Sariyildiz , M. B. , Pion , N. , Weinzaepfel , P. , and Larlus , D. Hard negative mixing for contrastive learning . In Larochelle , H. , Ranzato , M. , Hadsell , R. , Balcan , M. , and Lin , H . ( eds . ) , Advances in Neural Information Processing Systems , volume 33 , pp . 21798–21809 . Curran Associates , Inc. , 2020 . URL https : //proceedings . . 14 Kim , M. , Rho , K. , Kim , Y.-d. , and Jung , K. Action-driven contrastive representation for reinforcement learning . PLOS ONE , 17 ( 3 ) :1–14 , 03 2022. doi : 10.1371/journal.pone.0265456 . URL https : //doi.org/10.1371/journal . pone.0265456 . 14 Laskin , M. , Srinivas , A. , and Abbeel , P. CURL : Contrastive unsupervised representations for reinforcement learning . In III , H. D. and Singh , A . ( eds . ) , Proceedings of the 37th International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research , pp . 5639–5650 . PMLR , 13–18 Jul 2020 . URL https : //proceedings . mlr.press/v119/laskin20a.html . 6 , 9 , 14 Lee , S. , Seo , Y. , Lee , K. , Abbeel , P. , and Shin , J. Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble . In 5th Annual Conference on Robot Learning , 2021 . URL https : //openreview.net/ forum ? id=AlJXhEI6J5W . 1 Li , J. , Selvaraju , R. R. , Gotmare , A. D. , Joty , S. , Xiong , C. , and Hoi , S. Align before fuse : Vision and language representation learning with momentum distillation , 2021 . 14 Liu , B. , Zhu , Y. , Gao , C. , Feng , Y. , qiang liu , Zhu , Y. , and Stone , P. LIBERO : Benchmarking knowledge transfer for lifelong robot learning . In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track , 2023 . URL https : //openreview.net/forum ? id=xzEtNSuDJk . 3 , 4 , 5 , 7 , 16 , 17 , 18 Ma , S. , Zeng , Z. , McDuff , D. , and Song , Y . Active contrastive learning of audio-visual video representations . In International Conference on Learning Representations , 2021 . URL https : //openreview.net/forum ? id= OMizHuea_HB . 14 Ma , Y. J. , Sodhani , S. , Jayaraman , D. , Bastani , O. , Kumar , V. , and Zhang , A. VIP : Towards universal visual reward and representation via value-implicit pre-training . In The Eleventh International Conference on Learning Representations , 2023 . URL https : //openreview.net/forum ? id=YJ7o2wetJ2 . 2 , 9 , 14 Majumdar , A. , Yadav , K. , Arnaud , S. , Ma , Y. J. , Chen , C. , Silwal , S. , Jain , A. , Berges , V.-P. , Abbeel , P. , Malik , J. , Batra , D. , Lin , Y. , Maksymets , O. , Rajeswaran , A. , and Meier , F. Where are we in the search for an artificial visual cortex for embodied intelligence ? , 2023 . 2 , 3 , 5 , 6 , 7 , 8 , 9 , 14 , 16 Mazoure , B. , Tachet des Combes , R. , Doan , T. L. , Bachman , P. , and Hjelm , R. D. Deep reinforcement and infomax learning . In Larochelle , H. , Ranzato , M. , Hadsell , R. , Balcan , M. , and Lin , H . ( eds . ) , Advances in Neural Information Processing Systems , volume 33 , pp . 3686–3698 . Curran Associates , Inc. , 2020 . URL https : //proceedings . . 9 , 14 Mendonca , R. , Rybkin , O. , Daniilidis , K. , Hafner , D. , and Pathak , D. Discovering and achieving goals via world models . Advances in Neural Information Processing Systems , 34:24379–24391 , 2021 . 14 Misra , D. , Henaff , M. , Krishnamurthy , A. , and Langford , J. Kinematic state abstraction and provably efficient rich- observation reinforcement learning . CoRR , abs/1911.05815 , 2019 . URL http : //arxiv.org/abs/1911.05815 . 9 , 14 Nair , S. , Rajeswaran , A. , Kumar , V. , Finn , C. , and Gupta , A. R3m : A universal visual representation for robot manipulation . In 6th Annual Conference on Robot Learning , 2022 . URL https : //openreview.net/forum ? id= tGbpgz6yOrI . 2 , 3 , 6 , 8 , 9 , 14 , 15 , 16 Nam , H. and Han , B . Learning multi-domain convolutional neural networks for visual tracking . In 2016 IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , pp . 4293–4302 , 2016. doi : 10.1109/CVPR.2016 . 465 . 14 11 Premier-TACO : Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss Pang , J. , Chen , K. , Shi , J. , Feng , H. , Ouyang , W. , and Lin , D. Libra r-cnn : Towards balanced learning for object detection . In IEEE Conference on Computer Vision and Pattern Recognition , 2019 . 14 Parisi , S. , Rajeswaran , A. , Purushwalkam , S. , and Gupta , A . The unsurprising effectiveness of pre-trained vision models for control . In Chaudhuri , K. , Jegelka , S. , Song , L. , Szepesvari , C. , Niu , G. , and Sabato , S . ( eds . ) , Proceedings of the 39th International Conference on Machine Learning , volume 162 of Proceedings of Machine Learning Research , pp . 17359–17371 . PMLR , 17–23 Jul 2022 . 6 , 16 Perez , E. , Strub , F. , de Vries , H. , Dumoulin , V. , and Courville , A. C. Film : Visual reasoning with a general conditioning layer . In AAAI , 2018 . 17 Radford , A. , Wu , J. , Child , R. , Luan , D. , Amodei , D. , and Sutskever , I . Language models are unsupervised multitask learners . 2019 . URL https : . 1 Robinson , J. D. , Chuang , C.-Y. , Sra , S. , and Jegelka , S. Contrastive learning with hard negative samples . In International Conference on Learning Representations , 2021 . URL https : //openreview.net/forum ? id=CR1XOQ0UTh- . 14 Schwarzer , M. , Anand , A. , Goel , R. , Hjelm , R. D. , Courville , A. , and Bachman , P. Data-efficient reinforcement learning with self-predictive representations . In International Conference on Learning Representations , 2021a . URL https : //openreview.net/forum ? id=uCQfPZwRaUu . 6 Schwarzer , M. , Rajkumar , N. , Noukhovitch , M. , Anand , A. , Charlin , L. , Hjelm , R. D. , Bachman , P. , and Courville , A. Pretraining representations for data-efficient reinforcement learning . In Beygelzimer , A. , Dauphin , Y. , Liang , P. , and Vaughan , J. W . ( eds . ) , Advances in Neural Information Processing Systems , 2021b . URL https : //openreview . net/forum ? id=XpSAvlvnMa . 6 Sekar , R. , Rybkin , O. , Daniilidis , K. , Abbeel , P. , Hafner , D. , and Pathak , D. Planning to explore via self-supervised world models . In International Conference on Machine Learning , pp . 8583–8592 . PMLR , 2020 . 14 Seo , Y. , Hafner , D. , Liu , H. , Liu , F. , James , S. , Lee , K. , and Abbeel , P. Masked world models for visual control . In CoRL , volume 205 of Proceedings of Machine Learning Research , pp . 1332–1344 . PMLR , 2022 . 7 Shrivastava , A. , Gupta , A. , and Girshick , R. Training Region-based Object Detectors with Online Hard Example Mining . In IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , 2016 . 14 Stooke , A. , Lee , K. , Abbeel , P. , and Laskin , M. Decoupling representation learning from reinforcement learning . In Meila , M. and Zhang , T . ( eds . ) , Proceedings of the 38th International Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pp . 9870–9879 . PMLR , 18–24 Jul 2021a . 6 , 9 , 14 Stooke , A. , Lee , K. , Abbeel , P. , and Laskin , M. Decoupling representation learning from reinforcement learning . In International Conference on Machine Learning , pp . 9870–9879 . PMLR , 2021b . 2 Sun , Y. , Zheng , R. , Wang , X. , Cohen , A. E. , and Huang , F. Transfer RL across observation feature spaces via model- based regularization . In International Conference on Learning Representations , 2022 . URL https : //openreview . net/forum ? id=7KdAoOsI81C . 14 Sun , Y. , Ma , S. , Madaan , R. , Bonatti , R. , Huang , F. , and Kapoor , A . SMART : Self-supervised multi-task pretraining with control transformers . In The Eleventh International Conference on Learning Representations , 2023 . URL https : //openreview.net/forum ? id=9piH3Hg8QEf . 3 , 5 , 6 , 7 , 9 , 14 Tabassum , A. , Wahed , M. , Eldardiry , H. , and Lourentzou , I . Hard negative sampling strategies for contrastive representation learning , 2022 . 14 Tassa , Y. , Doron , Y. , Muldal , A. , Erez , T. , Li , Y. , de Las Casas , D. , Budden , D. , Abdolmaleki , A. , Merel , J. , Lefrancq , A. , Lillicrap , T. , and Riedmiller , M. Deepmind control suite , 2018 . 3 , 4 van den Oord , A. , Li , Y. , and Vinyals , O . Representation learning with contrastive predictive coding , 2019 . 3 , 14 Wan , S. , Chen , Z. , Zhang , T. , Zhang , B. , and kat Wong , K. Bootstrapping face detection with hard negative examples , 2016 . 14 Wang , X. , Zheng , R. , Sun , Y. , Jia , R. , Wongkamjan , W. , Xu , H. , and Huang , F. Coplanner : Plan to roll out conservatively but to explore optimistically for model-based rl . In The Twelfth International Conference on Learning Representations , 2024 . URL https : //openreview.net/forum ? id=MSe8YFbhUE . 14 Wei , Y. , Sun , Y. , Zheng , R. , Vemprala , S. , Bonatti , R. , Chen , S. , Madaan , R. , Ba , Z. , Kapoor , A. , and Ma , S. Is imitation all you need ? generalized decision-making with dual-phase training . arXiv preprint arXiv:2307.07909 , 2023 . 9 , 14 Xiao , T. , Radosavovic , I. , Darrell , T. , and Malik , J. Masked visual pre-training for motor control , 2022 . 6 , 9 , 14 , 16 Xu , G. , Zheng , R. , Liang , Y. , Wang , X. , Zhecheng Yuan , T. J. , Luo , Y. , Liu , X. , Yuan , J. , Hua , P. , Li , S. , Ze , Y. , III , H. D. , Huang , F. , and Xu , H. Drm : Mastering visual reinforcement learning through dormant ratio minimization . In The Twelfth International Conference on Learning Representations , 2024 . URL https : //openreview.net/ forum ? id=MSe8YFbhUE . 14 12 Premier-TACO : Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss Yarats , D. , Fergus , R. , Lazaric , A. , and Pinto , L. Reinforcement learning with prototypical representations . In International Conference on Machine Learning , pp . 11920–11931 . PMLR , 2021 . 14 Yarats , D. , Fergus , R. , Lazaric , A. , and Pinto , L. Mastering visual continuous control : Improved data-augmented rein- forcement learning . In International Conference on Learning Representations , 2022 . URL https : //openreview . net/forum ? id=_SJ-_yyes8 . 5 , 14 , 16 Yu , T. , Quillen , D. , He , Z. , Julian , R. , Hausman , K. , Finn , C. , and Levine , S. Meta-world : A benchmark and evaluation for multi-task and meta reinforcement learning . In Conference on Robot Learning ( CoRL ) , 2019 . URL https : //arxiv.org/abs/1910.10897 . 3 , 4 Yuan , Z. , Xue , Z. , Yuan , B. , Wang , X. , WU , Y. , Gao , Y. , and Xu , H. Pre-trained image encoder for gener- alizable visual reinforcement learning . In Koyejo , S. , Mohamed , S. , Agarwal , A. , Belgrave , D. , Cho , K. , and Oh , A . ( eds . ) , Advances in Neural Information Processing Systems , volume 35 , pp . 13022–13037 . Cur- ran Associates , Inc. , 2022 . URL https : . 9 , 14 Zhang , A. , McAllister , R. T. , Calandra , R. , Gal , Y. , and Levine , S. Learning invariant representations for reinforcement learning without reconstruction . In International Conference on Learning Representations , 2021 . URL https : //openreview.net/forum ? id=-2FCwDKRREu . 14 Zheng , R. , Wang , X. , Sun , Y. , Ma , S. , Zhao , J. , Xu , H. , III , H. D. , and Huang , F. TACO : Temporal latent action-driven contrastive loss for visual reinforcement learning . In Thirty-seventh Conference on Neural Information Processing Systems , 2023a . URL https : //openreview.net/forum ? id=ezCsMOy1w9 . 2 , 3 , 9 , 14 Zheng , R. , Wang , X. , Xu , H. , and Huang , F. Is model ensemble necessary ? model-based RL via a single model with lipschitz regularized value function . In The Eleventh International Conference on Learning Representations , 2023b . URL https : //openreview.net/forum ? id=hNyJBk3CwR . 14 13 Premier-TACO : Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss A Detailed Discussion of Related Work Pretraining Visual Representations . Existing works apply self-supervised pre-training from rich vision data to build foundation models . However , applying this approach to sequential decision-making tasks is challenging . Recent works have explored large-scale pre-training with offline data in the context of reinforcement learning . Efforts such as R3M ( Nair et al. , 2022 ) , VIP ( Ma et al. , 2023 ) , MVP ( Xiao et al. , 2022 ) , PIE-G ( Yuan et al. , 2022 ) , and VC-1 ( Majumdar et al. , 2023 ) highlight this direction . However , there ’ s a notable gap between the datasets used for pre-training and the actual downstream tasks . In fact , a recent study ( Hansen et al. , 2022a ) found that models trained from scratch can often perform better than those using pre-trained representations , suggesting the limitation of these approachs . It ’ s important to acknowledge that these pre-trained representations are not control-relevant , and they lack explicit learning of a latent world model . In contrast to these prior approaches , our pretrained representations learn to capture the control-relevant features with an effective temporal contrastive learning objective . For control tasks , several pretraining frameworks have emerged to model state-action interactions from high-dimensional observations by leveraging causal attention mechanisms . SMART ( Sun et al. , 2023 ) introduces a self-supervised and control-centric objective to train transformer-based models for multitask decision-making , although it requires additional fine-tuning with large number of demonstrations during downstream time . As an improvement , DualMind ( Wei et al. , 2023 ) pretrains representations using 45 tasks for general-purpose decision-making without task-specific fine-tuning . Besides , some methods ( Sekar et al. , 2020 ; Mendonca et al. , 2021 ; Yarats et al. , 2021 ; Sun et al. , 2022 ) first learn a general representation by exploring the environment online , and then use this representation to train the policy on downstream tasks . In comparison , our approach is notably more efficient and doesn ’ t require training with such an extensive task set . Nevertheless , we provide empirical evidence demonstrating that our method can effectively handle multi-task pretraining . Contrastive Representation for Visual RL Visual RL ( Hafner et al. , 2020 ; Yarats et al. , 2022 ; Hansen et al. , 2022c ; Zheng et al. , 2023b ; Wang et al. , 2024 ; Xu et al. , 2024 ) is long-standing challenge due to the entangled problem of representation learning and credit assignment . Contrastive learning plays a pivotal role in enhancing sample efficiency of visual RL by leveraging similarity constraints between data to learn effective visual representations . CURL ( Laskin et al. , 2020 ) extracts high-level features by utilizing InfoNCE ( van den Oord et al. , 2019 ) to maximize agreement between augmented observations , although it does not explicitly consider temporal relationships between states . Several approaches , such as CPC ( Henaff , 2020 ) , ST-DIM ( Anand et al. , 2019 ) , and ATC ( Stooke et al. , 2021a ) , introduce temporal dynamics into the contrastive loss . They do so by maximizing mutual information between states with short temporal intervals , facilitating the capture of temporal dependencies . DRIML ( Mazoure et al. , 2020 ) proposes a policy-dependent auxiliary objective that enhances agreement between representations of consecutive states , specifically considering the first action of the action sequence . Recent advancements by Kim et al . ( 2022 ) ; Zhang et al . ( 2021 ) incorporate actions into the contrastive loss , emphasizing behavioral similarity . TACO ( Zheng et al. , 2023a ) takes a step further by learning both state and action representations . It optimizes the mutual information between the representations of current states paired with action sequences and the representations of corresponding future states . In our approach , we build upon the efficient extension of TACO , harnessing the full potential of state and action representations for downstream tasks . On the theory side , the Homer algorithm ( Misra et al. , 2019 ) uses a binary temporal contrastive objective reminiscent of the approach used here , which differs by abstracting actions as well states , using an ancillary embedding , removing leveling from the construction , and of course extensive empirical validation . Hard Negative Sampling Strategy in Contrastive Learning Our proposed negative example sampling strategy in Premier-TACO is closely related to hard negative example mining in the literature of self-supervised learning as well as other areas of machine learning . Hard negative mining is indeed used in a variety of tasks , such as facial recognition ( Wan et al. , 2016 ) , object detection ( Shrivastava et al. , 2016 ) , tracking ( Nam & Han , 2016 ) , and image-text retrieval ( Pang et al. , 2019 ; Li et al. , 2021 ) , by introducing negative examples that are more difficult than randomly chosen ones to improve the performance of models . Within the regime of self-supervised learning , different negative example sampling strategies have also been discussed both empirically and theoretically to improve the quality of pretrained representation . In particular , Robinson et al . ( 2021 ) modifies the original NCE objective by developing a distribution over negative examples , which prioritizes pairs with currently similar representations . Kalantidis et al . ( 2020 ) suggests to mix hard negative examples within the latent space . Ma et al . ( 2021 ) introduce a method to actively sample uncertain negatives by calculating the gradients of the loss function relative to the model ’ s most confident predictions . Furthermore , Tabassum et al . ( 2022 ) samples negatives that combine the objectives of identifying model-uncertain negatives , selecting negatives close to the anchor point in the latent embedding space , and ensuring representativeness within the sample population . While our proposed approach bears some resemblance to existing negative sampling strategies in contrastive learning literature , we are dealing with unique challenges in sequential decision making , such as data distribution shift , task heterogeneity , and data qualities , as discussed in the introduction . Advancing the work of TACO ( Zheng et al. , 14 Premier-TACO : Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss 2023a ) , which is specifically designed to capture the control-relevant information in latent representation , Premier- TACO introduces a straightforward yet effective negative sampling strategy . Tailored toward multitask representation pretraining , this strategy involves sampling one negative example from a window centered around the anchor point , which is both computationally efficient and demonstrates superior performance in few-shot adaptation . B Additional Experiments Results B.1 Finetuning Comparisons among R3M ( Nair et al. , 2022 ) , R3M with in-domain finetuning ( Hansen et al. , 2022a ) and R3M finetuned with Premier-TACO in Deepmind Control Suite and MetaWorld are presented in Figure 11 and 10 . Figure 10 : [ ( W4 ) Compatibility ] Finetune R3M ( Nair et al. , 2022 ) , a generalized Pretrained Visual Encoder with Premier-TACO learning objective vs. R3M with in-domain finetuning in Deepmind Control Suite and MetaWorld . Figure 11 : [ ( W4 ) Compatibility ] Finetune R3M ( Nair et al. , 2022 ) , a generalized Pretrained Visual Encoder with Premier-TACO learning objective vs. R3M with in-domain finetuning in Deepmind Control Suite and MetaWorld . B.2 Pretrained Visual Representations Here , we provide the full results for all pretrained visual encoders across all 18 tasks on Deepmind Control Suite and MetaWorld . B.3 LIBERO-10 success rate 15 MetaWorld : Finetune Pretrained Visual Encoder R3M R3M w. Finetuning R3M w. Premier-TACO Finetuning 100 % e t a R s s e c c u S k s a T 80 % 60 % 40 % 20 % Bin-Picking Disassemble Hand-Insert Peg-Insert-Side Pick-Out-Of-Hole Pick-Place-Wall Shelf-Place Stick-Pull Deepmind Control Suite : Finetune Pretrained Visual Encoder R3M R3M w. Finetuning R3M w. Premier-TACO Finetuning e r o c S e v i t a e R l t r e p x E 100 80 60 40 20 0 Finger Spin Walker Walk Humanoid Walk hopper_hop Dog Trot Cup Catch Cheetah Run Quadruped Walk Quadruped Run Premier-TACO : Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss DMControl Pretrained Visual Models PVR MVP R3M VC-1 MetaWorld Pretrained Visual Models PVR MVP R3M VC-1 Finger Spin 11.5 ± 6.0 5.4 ± 7.1 6.9 ± 1.4 38.4 ± 9.3 Bin Picking 45.6 ± 5.6 46.1 ± 3.1 50.0 ± 12.0 60.2 ± 4.3 Hopper Hop 10.2 ± 1.5 7.8 ± 2.7 4.0 ± 0.1 23.2 ± 4.9 Disassemble 47.6 ± 5.8 32.4 ± 5.1 64.4 ± 12.4 70.4 ± 8.9 Walker Walk 10.3 ± 3.8 8.30 ± 1.6 16.7 ± 4.6 30.5 ± 6.2 Hand Insert 18.8 ± 4.0 10.4 ± 5.6 31.8 ± 6.21 35.5 ± 2.3 Humanoid Walk 7.6 ± 3.4 3.2 ± 0.5 2.6 ± 0.4 30.1 ± 7.5 Peg Insert Side 25.3 ± 10.4 28.9 ± 5.4 35.0 ± 3.95 48.2 ± 3.6 Dog Trot 20.5 ± 12.4 32.9 ± 6.0 46.6 ± 4.3 73.5 ± 6.4 Pick Out Of Hole 28.4 ± 5.7 42.3 ± 9.7 42.5 ± 6.4 66.3 ± 7.2 Cup Catch 60.2 ± 10.3 56.7 ± 8.9 93.7 ± 1.8 89.2 ± 13.2 Pick Place Wall 30.7 ± 8.5 42.5 ± 10.9 58.1 ± 16.7 63.2 ± 9.8 Reacher Hard 33.9 ± 9.2 40.7 ± 8.5 42.3 ± 5.6 64.9 ± 5.8 Shelf Place 19.5 ± 6.4 21.2 ± 8.3 18.7 ± 5.15 32.4 ± 6.5 Cheetah Run 26.7 ± 3.8 27.3 ± 4.4 33.1 ± 4.8 39.5 ± 9.7 Stick Pull 30.2 ± 4.6 28.5 ± 9.6 45.6 ± 17.3 52.4 ± 5.6 Quadruped Walk 15.6 ± 9.0 14.5 ± 7.2 18.2 ± 4.9 63.2 ± 4.0 Quadruped Run 40.6 ± 6.7 43.2 ± 4.2 64.0 ± 2.4 61.3 ± 8.5 Table 3 : Few-shot results for pretrained visual representations ( Parisi et al. , 2022 ; Xiao et al. , 2022 ; Nair et al. , 2022 ; Majumdar et al. , 2023 ) LIBERO Models Unseen Tasks LfS Best PVRs Multitask Inverse CURL ATC SPR TACO Premier-TACO Task 0 Task 1 Task 2 Task 3 Task 4 Task 5 Task 6 Task 7 Mean 19.9 ± 4.1 21.2 ± 3.5 23.3 ± 4.3 23.3 ± 4.3 16.7 ± 6.2 23.8 ± 6.9 15.0 ± 0.0 5.0 ± 0.0 35.5 ± 7.5 40.0 ± 8.8 46.7 ± 6.2 48.3 ± 10.3 38.3 ± 9.2 26.7 ± 8.5 41.3 ± 7.5 35.0 ± 6.3 40.3 ± 4.1 70.0 ± 5.0 63.3 ± 6.2 65.8 ± 6.7 60.0 ± 4.1 51.6 ± 4.7 35.0 ± 4.1 65.0 ± 9.3 35.0 ± 5.0 65.0 ± 9.3 95.0 ± 7.2 55.7 ± 4.7 56.4 ± 3.2 66.7 ± 8.4 70.1 ± 7.1 70.0 ± 6.8 83.8 ± 6.2 55.0 ± 5.4 62.5 ± 7.3 75.0 ± 13.2 43.3 ± 6.2 27.9 ± 3.9 26.7 ± 3.1 28.3 ± 2.3 18.3 ± 2.5 25.0 ± 6.1 23.7 ± 2.2 15.5 ± 2.4 30.7 ± 2.5 66.7 ± 9.2 62.8 ± 9.3 46.7 ± 3.8 63.3 ± 13.1 78.3 ± 2.4 78.8 ± 7.3 68.7 ± 11.9 52.3 ± 6.2 80.0 ± 6.1 6.7 ± 6.2 14.5 ± 6.7 21.7 ± 2.3 11.6 ± 4.7 23.3 ± 2.4 11.2 ± 4.1 12.5 ± 5.6 19.8 ± 3.8 27.5 ± 7.2 26.7 ± 4.7 29.6 ± 8.9 35.0 ± 7.1 38.3 ± 5.5 16.7 ± 2.4 26.3 ± 4.1 35.0 ± 9.3 22.3 ± 7.9 50.3 ± 4.0 40.3 43.4 38.9 40.6 35.6 44.4 35.0 40.9 58.0 Table 4 : [ ( W1 ) Versatility ( W2 ) Efficiency ] Five-shot Behavior Cloning ( BC ) for unseen task of LIBERO . Success rate of Premier-TACO and baselines across first 8 tasks on LIBERO-10 . Results are aggregated over 4 random seeds . Bold numbers indicate the best results . C Implementation Details Dataset For six pretraining tasks of Deepmind Control Suite , we train visual RL agents for individual tasks with DrQ-v2 Yarats et al . ( 2022 ) until convergence , and we store all the historical interaction steps in a separate buffer . Then , we sample 200 trajectories from the buffer for all tasks except for Humanoid Stand and Dog Walk . Since these two tasks are significantly harder , we use 1000 pretraining trajectories instead . Each episode in Deepmind Control Suite consists of 500 time steps . In terms of the randomly collected dataset , we sample trajectories by taking actions with each dimension independently sampled from a uniform distribution U ( −1. , 1. ) . For MetaWorld , we collect 1000 trajectories for each task , where each episode consists of 200 time steps . We add a Gaussian noise of standard deviation 0.3 to the provided scripted policy . For LIBERO , we take the human demonstration dataset from Liu et al . ( 2023 ) , which contains 50 demosntration trajectories per task . Pretraining For the shallow convolutional network used in Deepmind Control Suite and MetaWorld , we follow the same architecture as in Yarats et al . ( 2022 ) and add a layer normalization on top of the output of the ConvNet encoder . We set the feature dimension of the ConNet encoder to be 100 . In total , this encoder has around 3.95 million parameters . 1 class Encoder ( nn . Module ) : 2 def __init__ ( self ) : 3 4 5 6 7 8 9 10 11 12 super ( ) . __init__ ( ) self . repr_dim = 32 * 35 * 35 self . convnet = nn . Sequential ( nn . Conv2d ( 84 , 32 , 3 , stride =2 ) , nn . ReLU ( ) , nn . Conv2d ( 32 , 32 , 3 , stride =1 ) , nn . ReLU ( ) , nn . Conv2d ( 32 , 32 , 3 , stride =1 ) , nn . ReLU ( ) , nn . Conv2d ( 32 , 32 , 3 , stride =1 ) , nn . ReLU ( ) ) self . trunk = nn . Sequential ( nn . Linear ( self . repr_dim , feature_dim ) , nn . LayerNorm ( feature_dim ) , nn . Tanh ( ) ) 16 Premier-TACO : Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss 13 14 15 16 17 def forward ( self , obs ) : obs = obs / 255.0 - 0.5 h = self . convnet ( obs ) . view ( h . shape [ 0 ] , -1 ) return self . trunk ( h ) Listing 1 : Shallow Convolutional Network Architecture Used in Premier-TACO For LIBERO , we use two randomly initialized ( or pretrained ) ResNet-18 encoders to encode the third-person view and first-person view images with FiLM ( Perez et al. , 2018 ) encoding method to incorporate the BERT embedding Devlin et al . ( 2019 ) of the task language instruction . During downstream behavior cloning , we apply a transformer decoder module with context length 10 on top of the ResNet encodings to extract the temporal information , and then attach a two-layer MLP with hidden size 1024 as the policy head . The architecture follows ResNet-T in Liu et al . ( 2023 ) . For Premier-TACO loss , the number of timesteps K is set to be 3 throughout the experiments , and the window size W is fixed to be 5 . Action Encoder is a two-layer MLP with input size being the action space dimensionality , hidden size being 64 , and output size being the same as the dimensionality of action space . The projection layer G is a two-layer MLP with input size being feature dimension plus the number of timesteps times the dimensionality of the action space . Its hidden size is 1024 . In terms of the projection layer H , it is also a two-layer MLP with input and output size both being the feature dimension and hidden size being 1024 . Throughout the experiments , we set the batch size to be 4096 and the learning rate to be 1e-4 . For the contrastive/self-supervised based baselines , CURL , ATC , and SPR , we use the same batch size of 4096 as Premier-TACO . For Multitask TD3+BC and Inverse dynamics modeling baselines , we use a batch size of 1024 . Imitation Learning A batch size of 128 and a learning rate of 1e-4 are used for Deepmind Control Suite and Metaworld , and a batch size of 64 is used for LIBERO . During behavior cloning , we finetune the Shallow ConvNet Encoder . However , when we applied Premier-TACO for the large pre-trained ResNet/ViT encoder models , we keep the model weights frozen . In total , we take 100,000 gradient steps and conduct evaluations for every 1000 steps . For evaluations within the DeepMind Control Suite , we utilize the trained policy to execute 20 episodes , subsequently recording the mean episode reward . In the case of MetaWorld and LIBERO , we execute 40 episodes and document the success rate of the trained policy . We report the average of the highest three episode rewards/success rates from the 100 evaluated checkpoints . Computational Resources For our experiments , we use 8 NVIDIA RTX A6000 with PyTorch Distributed DataParallel for pretraining visual representations , and we use NVIDIA RTX2080Ti for downstream imitation learning on Deepmind Control Suite and Metaworld , and RTX A5000 on LIBERO . 17 Premier-TACO : Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss D An Additional Ablation Study on Negative Example Sampling Strategy In Premier-TACO , we sample one negative example from a size W window centered at the positive example for each data point . However , in principle , we could also use all samples within this window as negative examples instead of sampling only one . In the table below , we compare the performance of two negative example sampling strategies across 10 unseen Deepmind Control Suite tasks . Bold numbers indicate the better results . Finger Spin Hopper Hop Walker Walk Humanoid Walk Dog Trot Cup Catch Reacher Hard Cheetah Run Quadruped Walk Quadruped Run Overall Sampling 1 75.2 ± 0.6 75.3 ± 4.6 88.0 ± 0.8 51.4 ± 4.9 93.9 ± 5.4 98.9 ± 0.1 81.3 ± 1.8 65.7 ± 1.1 83.2 ± 5.7 76.8 ± 7.5 79.0 Sampling All 70.2 ± 8.4 76.1 ± 3.0 88.5 ± 0.4 56.4 ± 8.9 92.1 ± 4.0 98.3 ± 1.6 80.1 ± 5.8 69.3 ± 2.3 85.4 ± 4.2 82.1 ± 9.1 79.8 Table 5 : Results of two different negative sampling strategies on 10 unseen Deepmind Control Suite Tasks . As shown in Table 5 , we find that using all samples from the size W window does not significantly enhance performance compared to Premier-TACO . Moreover , this approach considerably increases the computational overhead . Given these results , we chose a more computationally efficient strategy of sampling a single negative example from the size W window in Premier-TACO . E Task instructions of downstream LIBERO tasks Here in table 6 , we provide the language instruction for each of the LIBERO downstream task . We refer readers to ( Liu et al. , 2023 ) for more details of the tasks . TASK ID TASK SCENE TASK INSTRUCTION 0 1 2 3 4 5 6 7 LIVING ROOM SCENE2 LIVING ROOM SCENE2 KITCHEN SCENE3 KITCHEN SCENE4 LIVING ROOM SCENE5 STUDY SCENE1 LIVING ROOM SCENE6 LIVING ROOM SCENE1 PUT BOTH THE ALPHABET SOUP AND THE TOMATO SAUCE IN THE BASKET PUT BOTH THE CREAM CHEESE BOX AND THE BUTTER IN THE BASKET TURN ON THE STOVE AND PUT THE MOKA POT ON IT PUT THE BLACK BOWL IN THE BOTTOM DRAWER OF THE CABINET AND CLOSE IT PUT THE WHITE MUG ON THE LEFT PLATE AND PUT THE YELLOW AND WHITE MUG ON THE RIGHT PLATE PICK UP THE BOOK AND PLACE IT IN THE BACK COMPARTMENT OF THE CADDY PUT THE WHITE MUG ON THE PLATE AND PUT THE CHOCOLATE PUDDING TO THE RIGHT OF THE PLATE PUT BOTH THE ALPHABET SOUP AND THE CREAM CHEESE BOX IN THE BASKET Table 6 : Language instructions for 8 LIBERO downstream tasks . 18","['e', 'l', 'c', 'v', 'r', 'premiertaco', 'fewshot', 'policy', 'learner', 'pretraine', 'multitask', 'representation', 'temporal', 'actiondriven', 'contrastive', 'loss', 'preprint', 'ruijie', 'daum´e', 'iii1', 'college', 'park', 'research', 'rzheng12', 'abstract', 'present', 'multitask', 'feature', 'representation', 'learning', 'approach', 'design', 'improve', 'fewshot', 'policy', 'learning', 'efficiency', 'sequential', 'decisionmake', 'task', 'leverage', 'subset', 'offline', 'dataset', 'pretraine', 'general', 'feature', 'representation', 'capture', 'critical', 'environmental', 'dynamic', 'finetune', 'use', 'minimal', 'expert', 'demonstration', 'advance', 'temporal', 'action', 'contrastive', 'learning', 'taco', 'objective', 'know', 'stateoftheart', 'result', 'visual', 'control', 'task', 'incorporate', 'novel', 'negative', 'example', 'sample', 'strategy', 'strategy', 'crucial', 'significantly', 'boost', 'computational', 'efficiency', 'make', 'largescale', 'offline', 'pretraine', 'feasible', 'extensive', 'empirical', 'evaluation', 'diverse', 'set', 'continuous', 'control', 'benchmark', 'include', 'deepmind', 'control', 'suite', 'metaworld', 'libero', 'demonstrate', 'effectiveness', 'pretraine', 'visual', 'representation', 'significantly', 'enhance', 'fewshot', 'imitation', 'learning', 'novel', 'task', 'code', 'pretraine', 'datum', 'well', 'pretraine', 'model', 'checkpoint', 'release', 'https', 'introduction', 'dynamic', 'everchange', 'world', 'inhabit', 'importance', 'sequential', 'decisionmake', 'sdm', 'machine', 'learning', 'overstate', 'static', 'task', 'sequential', 'decision', 'reflect', 'fluidity', 'realworld', 'scenario', 'robotic', 'manipulation', 'evolve', 'healthcare', 'treatment', 'foundation', 'model', 'language', 'radford', 'brown', 'revolutionize', 'natural', 'language', 'processing', 'leverage', 'vast', 'amount', 'textual', 'datum', 'understand', 'linguistic', 'nuance', 'pretraine', 'foundation', 'model', 'hold', 'similar', 'promise', 'sequential', 'decisionmake', 'sdm', 'language', 'model', 'capture', 'essence', 'syntax', 'semantic', 'context', 'serve', 'robust', 'starting', 'point', 'myriad', 'downstream', 'task', 'analogously', 'sdm', 'decision', 'influence', 'complex', 'interplay', 'past', 'action', 'current', 'state', 'future', 'possibility', 'pretraine', 'foundation', 'model', 'provide', 'rich', 'generalized', 'understanding', 'decision', 'sequence', 'foundational', 'knowledge', 'build', 'diverse', 'decisionmake', 'scenario', 'finetune', 'specific', 'task', 'much', 'language', 'model', 'adapt', 'specific', 'linguistic', 'task', 'follow', 'challenge', 'unique', 'sequential', 'decisionmake', 'set', 'apart', 'exist', 'vision', 'language', 'pretraine', 'paradigm', 'c1', 'datum', 'distribution', 'shift', 'training', 'datum', 'usually', 'consist', 'specific', 'behaviorpolicy', 'generate', 'trajectory', 'lead', 'vastly', 'different', 'datum', 'distribution', 'various', 'stage', 'pretraine', 'finetune', 'deployment', 'result', 'compromise', 'performance', 'task', 'heterogeneity', 'language', 'vision', 'task', 'often', 'share', 'semantic', 'feature', 'decisionmake', 'task', 'vary', 'widely', 'configuration', 'transition', 'dynamic', 'state', 'action', 'space', 'make', 'difficult', 'develop', 'universally', 'applicable', 'representation', 'datum', 'quality', 'supervision', 'effective', 'representation', 'learning', 'often', 'rely', 'highquality', 'datum', 'expert', 'guidance', 'work', 'partially', 'internship', 'pretraine', 'multitask', 'representation', 'temporal', 'actiondriven', 'contrastive', 'loss', 'figure', 'performance', 'pretraine', 'visual', 'representation', 'fewshot', 'imitation', 'learn', 'downstream', 'unseen', 'task', 'deepmind', 'control', 'suite', 'metaworld', 'libero', 'represent', 'learn', 'scratch', 'however', 'resource', 'absent', 'costly', 'obtain', 'many', 'realworld', 'decisionmake', 'task', 'stooke', 'aspirational', 'criterion', 'foundation', 'model', 'sequential', 'decisionmake', 'encompass', 'several', 'key', 'feature', 'w1', 'versatility', 'allow', 'model', 'generalize', 'wide', 'array', 'task', 'even', 'previously', 'encounter', 'new', 'embodiment', 'view', 'observation', 'novel', 'camera', 'angle', 'w2', 'efficiency', 'adapt', 'downstream', 'task', 'require', 'minimal', 'datum', 'fewshot', 'learn', 'technique', 'robustness', 'pretraine', 'datum', 'fluctuate', 'quality', 'ensure', 'resilient', 'foundation', 'w4', 'compatibility', 'exist', 'large', 'pretraine', 'model', 'nair', 'light', 'challenge', 'desirable', 'build', 'foundation', 'model', 'sdm', 'approach', 'develop', 'foundational', 'model', 'sequential', 'decisionmaking', 'focus', 'create', 'universal', 'transferable', 'encoder', 'use', 'rewardfree', 'dynamic', 'base', 'temporal', 'contrastive', 'pretraining', 'objective', 'encoder', 'tailor', 'manage', 'task', 'complex', 'observation', 'space', 'visual', 'input', 'exclude', 'reward', 'signal', 'pretraine', 'stage', 'model', 'well', 'poise', 'generalize', 'broad', 'array', 'downstream', 'task', 'divergent', 'objective', 'leverage', 'worldmodel', 'approach', 'ensure', 'encoder', 'learn', 'compact', 'representation', 'capture', 'universal', 'transition', 'dynamic', 'akin', 'law', 'physics', 'thereby', 'make', 'adaptable', 'multiple', 'scenario', 'encoder', 'enable', 'transfer', 'knowledge', 'downstream', 'control', 'task', 'even', 'task', 'part', 'original', 'pretraine', 'datum', 'set', 'exist', 'work', 'apply', 'selfsupervise', 'pretraining', 'rich', 'vision', 'datum', 'imagenet', 'deng', 'dataset', 'grauman', 'build', 'foundation', 'model', 'nair', 'majumdar', 'however', 'apply', 'approach', 'sequential', 'decisionmake', 'task', 'challenge', 'specifically', 'often', 'overlook', 'controlrelevant', 'consideration', 'suffer', 'domain', 'gap', 'pretraine', 'dataset', 'downstream', 'control', 'task', 'paper', 'rather', 'focus', 'leverage', 'large', 'vision', 'dataset', 'propose', 'novel', 'controlcentric', 'objective', 'function', 'pretraine', 'approach', 'call', 'pretraine', 'multitask', 'representation', 'temporal', 'actiondriven', 'contrastive', 'loss', 'employ', 'temporal', 'actiondriven', 'contrastive', 'loss', 'function', 'pretraine', 'controlcentric', 'objective', 'learn', 'state', 'representation', 'optimize', 'mutual', 'information', 'representation', 'current', 'state', 'pair', 'action', 'sequence', 'representation', 'corresponding', 'future', 'state', 'markedly', 'enhance', 'effectiveness', 'efficiency', 'temporal', 'action', 'contrastive', 'learning', 'taco', 'objective', 'detail', 'deliver', 'stateoftheart', 'outcome', 'visual', 'control', 'task', 'singletask', 'set', 'extend', 'capability', 'efficient', 'largescale', 'pretraine', 'broaden', 'applicability', 'performance', 'specifically', 'taco', 'consider', 'datum', 'point', 'batch', 'potential', 'negative', 'example', 'strategically', 'sample', 'single', 'negative', 'example', 'proximate', 'window', 'subsequent', 'state', 'method', 'ensure', 'negative', 'example', 'visually', 'akin', 'positive', 'one', 'necessitating', 'latent', 'representation', 'capture', 'controlrelevant', 'information', 'rather', 'rely', 'extraneous', 'feature', 'visual', 'appearance', 'efficient', 'negative', 'example', 'sample', 'strategy', 'add', 'computational', 'burden', 'compatible', 'small', 'batch', 'size', 'particular', 'metaworld', 'use', 'batch', 'size', 'achieve', 'relative', 'performance', 'improvement', 'seamlessly', 'scale', 'offline', 'pretraine', 'enhance', 'usability', 'effectiveness', 'list', 'key', 'contribution', '▷', 'introduce', 'new', 'framework', 'design', 'offline', 'visual', 'representation', 'pretraining', 'sequential', 'decisionmake', 'problem', 'particular', 'develop', 'new', 'temporal', 'contrastive', 'learning', 'objective', 'framework', 'compare', 'temporal', 'contrastive', 'learning', 'objective', 'employ', 'simple', 'yet', 'efficient', 'negative', 'example', 'sample', 'strategy', 'make', 'computationally', 'feasible', 'representation', 'learn', 'deepmind', 'control', 'suite', 'task', 'metaworld', 'task', 'libero', 'task', 'pretraine', 'multitask', 'representation', 'temporal', 'actiondriven', 'contrastive', 'loss', '▷', 'w1', 'versatility', 'efficiency', 'extensive', 'empirical', 'evaluation', 'verify', 'effectiveness', 'pretraine', 'visual', 'representation', 'fewshot', 'learning', 'unseen', 'task', 'metaworld', 'expert', 'trajectory', 'outperform', 'good', 'baseline', 'pretraining', 'method', 'respectively', 'remarkably', 'libero', 'first', 'method', 'demonstrate', 'benefit', 'pretraine', 'deepmind', 'control', 'dmc', 'tassa', 'use', 'trajectory', 'considerably', 'demonstration', 'sun', 'majumdar', 'achieve', 'good', 'performance', 'challenging', 'task', 'include', 'hard', 'dog', 'humanoid', 'task', 'versatility', 'extend', 'even', 'unseen', 'embodiment', 'dmc', 'well', 'unseen', 'task', 'unseen', 'camera', 'view', 'metaworld', '▷', 'robustness', 'w4', 'compatability', 'furthermore', 'demonstrate', 'resilient', 'datum', 'low', 'quality', 'also', 'compatible', 'exisite', 'large', 'pretraine', 'model', 'work', 'well', 'pretraine', 'dataset', 'collect', 'randomly', 'additionally', 'showcase', 'capability', 'temporal', 'contrastive', 'learning', 'objective', 'finetune', 'generalized', 'visual', 'encoder', 'nair', 'result', 'average', 'performance', 'enhancement', 'around', 'assess', 'task', 'deepmind', 'control', 'suite', 'metaworld', 'preliminary', 'offline', 'pretraining', 'consider', 'collection', 'task', 'cid8', 'ti', 'ai', 'dimensionality', 'observation', 'space', 'let', 'z', 'representation', 'function', 'agent', 'observation', 'randomly', 'initialize', 'pretraine', 'already', 'largescale', 'vision', 'dataset', 'imagenet', 'deng', 'grauman', 'assume', 'agent', 'give', 'offline', 'dataset', 'ai', 'objective', 'pretrain', 'generalizable', 'state', 'representation', 'motor', 'policy', 'π', 'face', 'unseen', 'downstream', 'task', 'quickly', 'adapt', 'expert', 'demonstration', 'use', 'pretraine', 'representation', 'summarize', 'pretraining', 'finetune', 'setup', 'pretraine', 'agent', 'get', 'access', 'offline', 'highly', 'suboptimal', 'goal', 'learn', 'generalizable', 'share', 'state', 'representation', 'input', 'adaptation', 'adapt', 'unseen', 'downstream', 'task', 'expert', 'demonstration', 'imitation', 'learn', 'subset', 'task', 'tnj', 'k', 'taco', 'temporal', 'action', 'drive', 'contrastive', 'learning', 'objective', 'temporal', 'actiondriven', 'contrastive', 'learning', 'reinforcement', 'learning', 'propose', 'address', 'representation', 'learning', 'problem', 'visual', 'continuous', 'control', 'aim', 'maximize', 'mutual', 'information', 'representation', 'current', 'state', 'pair', 'action', 'sequence', 'representation', 'corresponding', 'future', 'state', 'ztk', 'utk−1', 'ut', 'represent', 'latent', 'state', 'action', 'variable', 'theoretically', 'show', 'max', 'imization', 'mutual', 'information', 'objective', 'lead', 'state', 'action', 'representation', 'capable', 'represent', 'optimal', 'value', 'function', 'empirically', 'taco', 'estimate', 'lower', 'bind', 'mutual', 'information', 'objective', 'fonce', 'loss', 'achieve', 'state', 'art', 'performance', 'online', 'offline', 'visual', 'continuous', 'control', 'demonstrat', 'effectiveness', 'temporal', 'contrastive', 'learning', 'representation', 'learning', 'sequential', 'decision', 'make', 'problem', 'method', 'introduce', 'generalized', 'pretraine', 'approach', 'specifically', 'formulate', 'tackle', 'multitask', 'pretraine', 'problem', 'enhance', 'sample', 'efficiency', 'generalization', 'ability', 'downstream', 'task', 'building', 'success', 'temporal', 'contrastive', 'loss', 'exemplify', 'acquire', 'latent', 'state', 'representation', 'encapsulate', 'individual', 'task', 'dynamic', 'aim', 'foster', 'representation', 'learn', 'effectively', 'capture', 'intrinsic', 'dynamic', 'span', 'diverse', 'set', 'task', 'find', 'offline', 'dataset', 'overarching', 'objective', 'ensure', 'learn', 'representation', 'exhibit', 'versatility', 'generalize', 'unseen', 'task', 'share', 'underlie', 'dynamic', 'structure', 'nevertheless', 'adapt', 'offline', 'pretraine', 'online', 'learning', 'objective', 'pose', 'notable', 'challenge', 'specifically', 'mechanism', 'utilize', 'infonce', 'loss', 'categorize', 'subsequent', 'state', 'stk', 'batch', 'negative', 'example', 'methodology', 'prove', 'effective', 'singletask', 'reinforcement', 'learn', 'scenario', 'encounter', 'difficulty', 'extend', 'multitask', 'premiertaco', 'pretraine', 'multitask', 'representation', 'temporal', 'actiondriven', 'contrastive', 'loss', 'figure', 'illustration', 'contrastive', 'loss', 'design', 'state', 'encoder', 'identical', 'proj', 'layer', 'negative', 'example', 'sample', 'neighbor', 'framework', 'stk', 'context', 'offline', 'pretraine', 'image', 'observation', 'batch', 'come', 'different', 'task', 'vastly', 'different', 'visual', 'appearance', 'render', 'contrastive', 'infonce', 'loss', 'significantly', 'less', 'effective', 'offline', 'pretraine', 'objective', 'propose', 'straightforward', 'highly', 'effective', 'mechanism', 'select', 'challenge', 'negative', 'ex', 'ample', 'instead', 'treat', 'remain', 'example', 'batch', 'negative', 'select', 'negative', 'example', 'window', 'center', 'state', 'episode', 'show', 'figure', 'approach', 'computationally', 'efficient', 'statistically', 'powerful', 'due', 'negative', 'example', 'challenge', 'distinguish', 'similar', 'positive', 'example', 'force', 'model', 'capture', 'temporal', 'dynamic', 'differentiate', 'positive', 'negative', 'example', 'practice', 'allow', 'use', 'much', 'small', 'batch', 'size', 'premiertaco', 'metaworld', 'batch', 'size', 'achieve', 'performance', 'gain', 'compare', 'save', 'around', 'computational', 'time', 'figure', 'difference', 'sample', 'negative', 'example', 'figure', 'illustrate', 'design', 'objective', 'specifically', 'give', 'batch', 'state', 'action', 'sequence', 'transition', 'latent', 'state', 'latent', 'action', 'embedding', 'respectively', 'furthermore', 'let', 'center', 'stk', 'stk1', 'stkw', 'z', 'let', 'z', 'n', 'cid93', 'negative', 'example', 'uniformly', 'sample', 'window', 'size', 'cid102', 'negative', 'latent', 'state', 'give', 'define', 'future', 'predict', 'actual', 'latent', 'state', 'optimize', 'z', 'h', 'cid93', 'h', 'z', 'z', 'embedding', 'n', 'log', '⊤', 'h', '⊤', 'tk', 'h', '⊤', 'h', 'fewshot', 'generalization', 'pretraine', 'representation', 'encoder', 'leverage', 'pretraine', 'model', 'φ', 'learn', 'policy', 'downstream', 'task', 'learn', 'policy', 'π', 'state', 'representation', 'φ', 'input', 'use', 'behavior', 'clone', 'expert', 'demonstration', 'different', 'control', 'domain', 'employ', 'significantly', 'demonstration', 'unseen', 'task', 'typically', 'use', 'baseline', 'underscore', 'substantial', 'advantage', 'premiertaco', 'fewshot', 'generalization', 'detail', 'experiment', 'downstream', 'task', 'provide', 'section', 'experiment', 'empirical', 'evaluation', 'consider', 'benchmark', 'deepmind', 'control', 'suite', 'tassa', 'locomotion', 'control', 'metaworld', 'libero', 'robotic', 'manipulation', 'task', 'important', 'note', 'varied', 'source', 'datum', 'employ', 'pretraine', 'benchmark', 'deepmind', 'control', 'suite', 'negative', 'example', 'state', 'encoder', 'proj', 'layer', '𝒂𝒕', 'action', 'encoder', 'proj', 'layer', 'contrastive', 'loss', 'positive', 'example', '𝒔𝒕', 'state', 'encoder', '𝒛𝒕', 'proj', 'layer', 'premiertaco', '𝒔𝒕', '𝒔𝒕', 'taco', 'c', 'b', 'negative', 'example', 'n1', 'negative', 'example', 'pretraine', 'multitask', 'representation', 'temporal', 'actiondriven', 'contrastive', 'loss', 'figure', 'pretrain', 'test', 'task', 'split', 'deepmind', 'control', 'suite', 'metaworld', 'libero', 'left', 'figure', 'deepmind', 'control', 'suite', 'task', 'right', 'figure', 'metaworld', 'task', 'pretraine', 'dataset', 'come', 'replay', 'buffer', 'online', 'reinforcement', 'learning', 'agent', 'metaworld', 'dataset', 'generate', 'predefine', 'script', 'policy', 'libero', 'utilize', 'provide', 'demonstration', 'dataset', 'collect', 'human', 'teleoperation', 'evaluate', 'wide', 'range', 'pretraine', 'datum', 'type', 'explore', 'previous', 'work', 'aim', 'provide', 'comprehensive', 'evaluation', 'pretraine', 'effect', 'control', 'dmc', 'consider', 'selection', 'challenging', 'task', 'deepmind', 'control', 'suite', 'note', 'compare', 'prior', 'work', 'majumdar', 'sun', 'consider', 'much', 'hard', 'task', 'include', 'one', 'humanoid', 'dog', 'domain', 'feature', 'intricate', 'kinematic', 'skin', 'weight', 'collision', 'geometry', 'pretraine', 'select', 'task', 'dmc6', 'include', 'finger', 'turn', 'hard', 'hopper', 'stand', 'walker', 'run', 'humanoid', 'walk', 'dog', 'stand', 'generate', 'exploratory', 'dataset', 'task', 'sample', 'trajectory', 'generate', 'exploratory', 'stage', 'drqv2', 'yarat', 'learn', 'agent', 'particular', 'sample', 'trajectory', 'online', 'replay', 'buffer', 'drqv2', 'reach', 'convergence', 'performance', 'ensure', 'diversity', 'pretraine', 'datum', 'practice', 'highquality', 'dataset', 'hard', 'obtain', 'later', 'experiment', 'also', 'relax', 'assumption', 'consider', 'pretraine', 'trajectory', 'sample', 'uniformly', 'random', 'action', 'term', 'encoder', 'architecture', 'pretrain', 'premiertaco', 'shallow', 'convnet', 'encoder', 'drqv2', 'yarat', 'metaworld', 'select', 'set', 'task', 'pretraine', 'encompass', 'variety', 'motion', 'pattern', 'sawyer', 'robotic', 'arm', 'interaction', 'different', 'object', 'collect', 'exploratory', 'dataset', 'pretraine', 'execute', 'scripted', 'policy', 'gaussian', 'noise', 'standard', 'deviation', 'add', 'action', 'add', 'noise', 'success', 'rate', 'collect', 'policy', 'average', 'around', 'pretraine', 'task', 'use', 'encoder', 'network', 'architecture', 'dmc', 'libero', 'pretrain', 'shorthorizon', 'manipulation', 'task', 'libero90', 'human', 'demonstration', 'dataset', 'provide', 'original', 'paper', 'task', 'contain', 'trajectory', 'human', 'teleoperated', 'trajectory', 'use', 'resnet18', 'encoder', 'encode', 'image', 'observation', 'resolution', 'downstream', 'task', 'assess', 'fewshot', 'imitation', 'learn', 'performance', 'first', 'longhorizon', 'task', 'liberolong', 'baseline', 'compare', 'premiertaco', 'follow', 'representation', 'pretraine', 'baseline', '▷', 'learn', 'scratch', 'behavior', 'clone', 'randomly', 'initialize', 'shallow', 'convnet', 'encoder', 'carefully', 'implement', 'behavior', 'clone', 'dmc', 'metaworld', 'follow', 'include', 'random', 'shift', 'datum', 'augmentation', 'behavior', 'cloning', 'libero', 'take', 'model', 'architecture', 'use', 'transformer', 'decoder', 'module', 'top', 'resnet', 'encoding', 'extract', 'temporal', 'information', 'sequence', 'observation', 'address', 'nonmarkovian', 'characteristic', 'inherent', 'human', 'demonstration', '▷', 'policy', 'pretraining', 'first', 'train', 'multitask', 'policy', 'fujimoto', 'pretraine', 'dataset', 'numerous', 'alternative', 'offline', 'algorithm', 'exist', 'choose', 'representative', 'simplicity', 'great', 'empirical', 'performance', 'libero', 'use', 'multitask', 'offline', 'generally', 'perform', 'well', 'imitation', 'learn', 'benchmark', 'human', 'demonstrate', 'dataset', 'pretraine', 'take', 'pretraine', 'encoder', 'drop', 'policy', 'mlp', 'layer', 'premiertaco', 'pretraine', 'multitask', 'representation', 'temporal', 'actiondriven', 'contrastive', 'loss', 'dmcontrol', 'model', 'task', 'smart', 'good', 'atc', 'see', 'embodiment', 'unseen', 'embodiment', 'finger', 'spin', '284±97', 'hopper', 'walker', 'walk', '304±29', '644±56', 'humanoid', 'walk', '301±62', 'dog', 'trot', '843±05', 'cup', 'catch', 'reacher', 'hard', '346±41', '625±78', 'run', 'quadrupe', 'walk', '611±57', '819±56', 'quadrupe', 'run', '450±29', 'mean', 'performance', 'table', 'w1', 'versatility', 'efficiency', 'fewshot', 'behavior', 'clone', 'unseen', 'task', 'dmc', 'performance', 'agent', 'reward', 'expert', 'reward', 'baseline', 'premiertaco', 'unseen', 'task', 'deepmind', 'control', 'suite', 'bold', 'number', 'indicate', 'good', 'result', 'agent', 'policy', 'evaluate', 'gradient', 'step', 'total', 'gradient', 'step', 'report', 'average', 'performance', 'good', 'epoch', 'course', 'learn', 'outperform', 'baseline', 'showcase', 'superior', 'efficacy', 'generalize', 'unseen', 'task', 'see', 'unseen', 'embodiment', 'metaworld', 'model', 'unseen', 'task', 'smart', 'good', 'atc', 'pick', 'disassemble', 'hand', 'peg', 'insert', 'side', 'pick', 'hole', 'pick', 'place', 'shelf', 'place', 'pull', 'mean', 'table', 'w1', 'versatility', 'efficiency', 'fiveshot', 'behavior', 'clone', 'unseen', 'task', 'metaworld', 'success', 'rate', 'premiertaco', 'baseline', 'hard', 'unseen', 'task', 'metaworld', 'result', 'aggregate', 'random', 'seed', 'bold', 'number', 'indicate', 'good', 'result', 'figure', 'w1', 'versatility', 'efficiency', 'mean', 'success', 'rate', 'imitation', 'learn', 'unseen', 'task', 'libero', 'result', 'aggregate', 'random', 'seed', 'bold', 'number', 'indicate', 'good', 'result', 'see', 'result', 'individual', 'task', 'table', '▷', 'pretraine', 'visual', 'representation', 'evaluate', 'stateoftheart', 'frozen', 'pretraine', 'visual', 'representation', 'include', 'parisi', 'nair', 'majumdar', 'report', 'good', 'performance', 'model', 'task', 'transformer', 'smart', 'sun', 'selfsupervise', 'representation', 'pretraine', 'framework', 'utilize', 'makse', 'prediction', 'objective', 'pretraine', 'representation', 'decision', 'transformer', 'architecture', 'use', 'pretraine', 'representation', 'learn', 'policy', 'downstream', 'task', '▷', 'inverse', 'dynamic', 'model', 'pretrain', 'inverse', 'dynamic', 'model', 'predict', 'action', 'use', 'pretraine', 'representation', 'downstream', 'task', '▷', 'contrastiveselfsupervise', 'learning', 'objective', 'atc', 'stooke', 'spr', 'schwarzer', 'atc', 'approach', 'apply', 'contrastive', 'learning', 'sequential', 'decision', 'make', 'problem', 'curl', 'treat', 'augment', 'state', 'positive', 'pair', 'neglect', 'temporal', 'dependency', 'premiertaco', 'pretraine', 'multitask', 'representation', 'temporal', 'actiondriven', 'contrastive', 'loss', 'mdp', 'comparison', 'atc', 'take', 'temporal', 'structure', 'consideration', 'positive', 'example', 'atc', 'augmented', 'view', 'temporally', 'nearby', 'state', 'spr', 'apply', 'objecive', 'grill', 'sequential', 'decision', 'make', 'problem', 'pretraine', 'state', 'representation', 'selfpredictive', 'future', 'state', 'pretraine', 'feature', 'representation', 'facilitate', 'effective', 'fewshot', 'adaptation', 'unseen', 'task', 'measure', 'performance', 'pretraine', 'visual', 'representation', 'fewshot', 'imitation', 'learning', 'unseen', 'downstream', 'task', 'dmc', 'metaworld', 'particular', 'dmc', 'use', 'expert', 'trajectory', 'imitation', 'learn', 'hard', 'task', 'humanoid', 'walk', 'dog', 'trot', 'use', 'trajectory', 'instead', 'note', 'use', 'use', 'sun', 'number', 'expert', 'trajectory', 'use', 'majumdar', 'agent', 'reward', 'expert', 'reward', 'record', 'performance', 'agent', 'calculate', 'ratio', 'expert', 'reward', 'episode', 'reward', 'expert', 'policy', 'use', 'collect', 'demonstration', 'trajectory', 'metaworld', 'libero', 'use', 'expert', 'trajectory', 'downstream', 'task', 'use', 'task', 'success', 'rate', 'performance', 'metric', 'table', 'table', 'figure', 'present', 'result', 'deepmind', 'control', 'suite', 'metaworld', 'libero', 'respectively', 'show', 'pretraine', 'representation', 'significantly', 'improve', 'fewshot', 'imitation', 'learn', 'performance', 'compare', 'learnfromscratch', 'improvement', 'deepmind', 'control', 'suite', 'improvement', 'metaworld', 'respectively', 'moreover', 'also', 'outperform', 'baseline', 'task', 'large', 'margin', 'libero', 'consistent', 'observe', 'exist', 'pretraine', 'method', 'dataset', 'fail', 'enhance', 'downstream', 'policy', 'learning', 'performance', 'particular', 'method', 'multitask', 'pretraining', 'actually', 'degrade', 'downstream', 'policy', 'learning', 'performance', 'contrast', 'use', 'resnet18', 'encoder', 'pretraine', 'significantly', 'boost', 'fewshot', 'imitation', 'learn', 'performance', 'substantial', 'margin', 'pretraine', 'representation', 'enable', 'knowledge', 'sharing', 'different', 'embodiment', 'ideally', 'resilient', 'generalizable', 'state', 'feature', 'representation', 'encapsulate', 'universally', 'applicable', 'feature', 'give', 'embodiment', 'variety', 'task', 'also', 'exhibit', 'capability', 'generalize', 'distinct', 'embodiment', 'evaluate', 'fewshot', 'behavior', 'cloning', 'performance', 'pretraine', 'encoder', 'task', 'feature', 'unseen', 'embodiment', 'run', 'quadrupe', 'walk', 'comparison', 'learnfromscratch', 'show', 'figure', 'leave', 'pretraine', 'representation', 'realize', 'performance', 'gain', 'demonstrate', 'robust', 'generalizability', 'pretraine', 'feature', 'representation', 'figure', 'w1', 'versatility', 'leave', 'dmc', 'generalization', 'pretraine', 'visual', 'representation', 'unseen', 'embodiment', 'right', 'metaworld', 'fewshot', 'adaptation', 'unseen', 'task', 'unseen', 'camera', 'view', 'pretraine', 'representation', 'also', 'generalizable', 'unseen', 'task', 'camera', 'view', 'generalize', 'unseen', 'embodiment', 'ideal', 'robust', 'visual', 'representation', 'possess', 'capacity', 'adapt', 'unfamiliar', 'task', 'novel', 'camera', 'view', 'figure', 'right', 'evaluate', 'fiveshot', 'learn', 'performance', 'model', 'previously', 'unseen', 'task', 'metaworld', 'new', 'view', 'particular', 'pretraine', 'datum', 'metaworld', 'generate', 'use', 'view', 'employ', 'seo', 'downstream', 'policy', 'learn', 'agent', 'give', 'expert', 'trajectory', 'different', 'corner', 'camera', 'view', 'depict', 'figure', 'notably', 'also', 'achieve', 'substantial', 'performance', 'enhancement', 'thereby', 'underscore', 'robust', 'generalizability', 'pretraine', 'visual', 'representation', 'pretraine', 'representation', 'resilient', 'lowquality', 'datum', 'evaluate', 'resilience', 'employ', 'randomly', 'collect', 'trajectory', 'datum', 'deepmind', 'control', 'suite', 'pretraine', 'compare', 'representation', 'pretraine', 'use', 'exploratory', 'dataset', 'learnfromscratch', 'approach', 'illustrate', 'figure', 'downstream', 'task', 'even', 'use', 'randomly', 'pretraine', 'datum', 'pretraine', 'model', 'still', 'maintain', 'significant', 'advantage', 'learningfromscratch', 'compare', 'representation', 'pretraine', 'use', 'exploratory', 'datum', 'small', 'disparity', 'individual', 'task', 'remain', 'comparable', 'task', 'strongly', 'indicate', 'robustness', 'premiertaco', 'lowquality', 'datum', 'even', 'use', 'expert', 'control', 'datum', 'method', 'capable', 'extract', 'valuable', 'information', 'premiertaco', 'pretraine', 'multitask', 'representation', 'temporal', 'actiondriven', 'contrastive', 'loss', 'figure', 'robustness', 'pretraine', 'exploratory', 'dataset', 'pretraine', 'randomly', 'collect', 'dataset', 'figure', 'w4', 'compatibility', 'finetune', 'nair', 'generalize', 'pretraine', 'visual', 'encoder', 'learn', 'objective', 'indomain', 'finetune', 'deepmind', 'control', 'suite', 'metaworld', 'pretraine', 'visual', 'encoder', 'finetune', 'addition', 'evaluate', 'pretraine', 'representation', 'various', 'downstream', 'scenario', 'also', 'conduct', 'finetune', 'pretraine', 'visual', 'representation', 'use', 'domain', 'control', 'trajectory', 'follow', 'framework', 'importantly', 'finding', 'deviate', 'observation', 'make', 'prior', 'work', 'hansen', 'majumdar', 'finetune', 'nair', 'indomain', 'demonstration', 'datum', 'use', 'taskcentric', 'behavior', 'clone', 'objective', 'result', 'performance', 'degradation', 'speculate', 'main', 'factor', 'contribute', 'phenomenon', 'first', 'domain', 'gap', 'exist', 'outofdomain', 'pretraine', 'datum', 'indomain', 'finetune', 'datum', 'second', 'finetune', 'fewshot', 'learning', 'lead', 'overfitte', 'large', 'pretraine', 'model', 'far', 'validate', 'effectiveness', 'approach', 'compare', 'result', 'fine', 'tuning', 'indomain', 'finetuning', 'hansen', 'finetune', 'use', 'method', 'select', 'deepmind', 'control', 'suite', 'metaworld', 'pretraine', 'task', 'figure', 'unequivocally', 'demonstrate', 'direct', 'finetuning', 'indomain', 'task', 'lead', 'performance', 'decline', 'multiple', 'task', 'however', 'leverage', 'learn', 'objective', 'finetune', 'substantially', 'enhance', 'performance', 'underscore', 'role', 'method', 'bridge', 'domain', 'gap', 'capture', 'essential', 'control', 'feature', 'also', 'highlight', 'robust', 'generalization', 'capability', 'furthermore', 'finding', 'strongly', 'suggest', 'approach', 'highly', 'adaptable', 'wide', 'range', 'multitask', 'pretraine', 'scenario', 'irrespective', 'model', 'size', 'size', 'pretraine', 'data', 'figure', 'w1', 'versatility', 'leave', 'premiertaco', 'taco', 'deepmind', 'control', 'suite', 'task', 'different', 'batch', 'size', 'right', 'average', 'performance', 'premiertaco', 'deepmind', 'control', 'suite', 'task', 'different', 'window', 'size', 'ablation', 'study', 'batch', 'size', 'compare', 'taco', 'negative', 'example', 'sample', 'strategy', 'employ', 'allow', 'sample', 'hard', 'negative', 'example', 'episode', 'positive', 'example', 'premiertaco', 'pretraine', 'multitask', 'representation', 'temporal', 'actiondriven', 'contrastive', 'loss', 'imply', 'promising', 'potential', 'significantly', 'improve', 'performance', 'exist', 'pretraine', 'model', 'diverse', 'domain', 'full', 'result', 'finetune', 'task', 'include', 'deepmind', 'control', 'suite', 'metaworld', 'expect', 'work', 'much', 'well', 'small', 'batch', 'size', 'compare', 'taco', 'negative', 'example', 'give', 'batch', 'come', 'various', 'task', 'thus', 'batch', 'size', 'require', 'scale', 'linearly', 'number', 'pretraine', 'task', 'previous', 'experimental', 'result', 'pretraine', 'batch', 'size', 'standard', 'batch', 'size', 'use', 'contrastive', 'learning', 'literature', 'empirically', 'verify', 'effect', 'different', 'choice', 'pretraine', 'batch', 'size', 'train', 'premiertaco', 'taco', 'different', 'batch', 'size', 'compare', 'fewshot', 'imitation', 'learn', 'performance', 'figure', 'leave', 'display', 'average', 'performance', 'fewshot', 'imitation', 'learn', 'task', 'deepmind', 'control', 'suite', 'depict', 'figure', 'model', 'significantly', 'outperform', 'batch', 'size', 'test', 'experiment', 'exhibit', 'performance', 'saturation', 'batch', 'size', 'observation', 'substantiate', 'negative', 'example', 'sample', 'strategy', 'employ', 'indeed', 'key', 'success', 'offline', 'pretraine', 'ablation', 'study', 'window', 'size', 'window', 'size', 'w', 'determine', 'hardness', 'negative', 'example', 'small', 'window', 'size', 'result', 'negative', 'example', 'challenging', 'distinguish', 'positive', 'example', 'become', 'excessively', 'difficult', 'differentiate', 'latent', 'space', 'conversely', 'large', 'window', 'size', 'make', 'distinguish', 'relatively', 'straightforward', 'thereby', 'mitigate', 'impact', 'negative', 'sampling', 'precede', 'experiment', 'consistent', 'window', 'size', 'apply', 'trial', 'deepmind', 'control', 'suite', 'metaworld', 'figure', 'right', 'empirically', 'evaluate', 'effect', 'vary', 'window', 'size', 'average', 'performance', 'model', 'deepmind', 'control', 'task', 'notably', 'observation', 'reveal', 'performance', 'comparable', 'window', 'size', 'set', 'excessively', 'small', 'w', 'large', 'w', 'window', 'size', 'lead', 'bad', 'performance', 'relate', 'work', 'exist', 'work', 'include', 'nair', 'vip', 'yuan', 'majumdar', 'focus', 'selfsupervise', 'pretraining', 'build', 'foundation', 'model', 'struggle', 'domain', 'gap', 'sequential', 'decisionmake', 'task', 'recent', 'study', '2022a', 'indicate', 'model', 'train', 'often', 'outperform', 'pretraine', 'representation', 'approach', 'smart', 'sun', 'offer', 'controlcentric', 'pretraining', 'cost', 'extensive', 'finetuning', 'task', 'set', 'contrastive', 'learn', 'technique', 'curl', 'stdim', 'atc', 'stooke', 'succeed', 'visual', 'rl', 'mainly', 'focus', 'highlevel', 'feature', 'temporal', 'dynamic', 'holistic', 'consideration', 'stateaction', 'interaction', 'gap', 'partially', 'fill', 'work', 'build', 'effort', 'eliminate', 'need', 'extensive', 'task', 'set', 'finetune', 'efficiently', 'capture', 'controlrelevant', 'feature', 'position', 'method', 'distinct', 'advancement', 'driml', 'mazoure', 'homer', 'misra', 'require', 'computational', 'empirical', 'resource', 'detailed', 'discussion', 'related', 'work', 'conclusion', 'paper', 'introduce', 'robust', 'highly', 'generalizable', 'representation', 'pretraine', 'framework', 'fewshot', 'policy', 'learning', 'propose', 'temporal', 'contrastive', 'learning', 'objective', 'excel', 'representation', 'learning', 'pretraine', 'phase', 'thank', 'efficient', 'negative', 'example', 'sample', 'strategy', 'extensive', 'empirical', 'evaluation', 'span', 'diverse', 'domain', 'task', 'underscore', 'remarkable', 'effectiveness', 'adaptability', 'pretraine', 'visual', 'representation', 'unseen', 'task', 'even', 'confront', 'unseen', 'embodiment', 'different', 'view', 'data', 'imperfection', 'furthermore', 'demonstrate', 'versatility', 'showcase', 'ability', 'finetune', 'large', 'pretraine', 'visual', 'representation', 'nair', 'domainspecific', 'datum', 'underscore', 'potential', 'broad', 'application', 'acknowledgement', 'work', 'partially', 'internship', 'support', 'fai', 'dodonroffice', 'naval', 'research', 'award', 'number', 'office', 'scientific', 'research', 'award', 'number', 'doddarpadefense', 'advanced', 'research', 'project', 'agency', 'guaranteeing', 'ai', 'robustness', 'deception', 'gard', 'hr00112020007', 'adobe', 'capital', 'fellowship', 'premiertaco', 'pretraine', 'multitask', 'representation', 'temporal', 'actiondriven', 'contrastive', 'loss', 'reference', 'anand', 'racah', 'e', 'ozair', 'bengio', 'r', 'unsupervise', 'state', 'representation', 'learning', 'atari', 'wallach', 'h', 'h', 'beygelzimer', 'dalch´ebuc', 'fox', 'e', 'r', 'ed', 'advance', 'neural', 'information', 'processing', 'system', 'volume', 'https', 'proceeding', 'brohan', 'brown', 'carbajal', 'chebotar', 'c', 'gopalakrishnan', 'herzog', 'hsu', 'j', 'ichter', 'b', 'jackson', 'r', 'kalashnikov', 'leal', 'lu', 'manjunath', 'mordatch', 'parada', 'c', 'peralta', 'pertsch', 'k', 'quiambao', 'rao', 'k', 'ryoo', 'salazar', 'p', 'say', 'stone', 'tran', 'h', 'vanhoucke', 'vuong', 'q', 'p', 'rt1', 'robotic', 'transformer', 'realworld', 'control', 'scale', 'brown', 'ryder', 'dhariwal', 'shyam', 'p', 'sastry', 'g', 'askell', 'agarwal', 'herbertvoss', 'krueger', 'child', 'r', 'ramesh', 'ziegler', 'winter', 'c', 'sigler', 'litwin', 'gray', 'chess', 'b', 'clark', 'berner', 'c', 'mccandlish', 'radford', 'sutskever', 'amodei', 'language', 'model', 'fewshot', 'learner', 'ranzato', 'hadsell', 'r', 'ed', 'advance', 'neural', 'information', 'processing', 'system', 'volume', 'pp', 'url', 'https', 'deng', 'socher', 'r', 'imagenet', 'largescale', 'hierarchical', 'image', 'database', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'cvpr', 'devlin', 'pretraining', 'deep', 'bidirectional', 'transformer', 'language', 'understanding', 'proceeding', 'conference', 'north', 'american', 'chapter', 'association', 'computational', 'linguistic', 'human', 'language', 'technology', 'volume', 'long', 'short', 'paper', 'pp', 'association', 'computational', 'linguistic', 'https', 'fujimoto', 'minimalist', 'approach', 'offline', 'reinforcement', 'learning', 'ranzato', 'beygelzim', 'dauphin', 'p', 'ed', 'advance', 'neural', 'information', 'processing', 'system', 'volume', 'pp', '20132–20145', 'url', 'grauman', 'westbury', 'byrne', 'e', 'girdhar', 'r', 'hamburger', 'wray', 'z', 'c', 'bansal', 'batra', 'cartillier', 'crane', 'doulaty', 'erapalli', 'feichtenhofer', 'c', 'fragomeni', 'fu', 'gebreselasie', 'gonzalez', 'c', 'kolar', 'c', 'z', 'k', 'modhugu', 'r', 'munro', 'murrell', 'nishiyasu', 'price', 'w', 'puente', 'p', 'r', 'ramazanova', 'l', 'somasundaram', 'southerland', 'sugano', 'r', 'p', 'crandall', 'daman', 'farinella', 'fuegen', 'c', 'v', 'joo', 'h', 'h', 'newcombe', 'r', 'oliva', 'park', 'rehg', 'z', 'torralba', 'torresani', 'l', 'world', 'hour', 'egocentric', 'video', 'grill', 'tallec', 'c', 'richemond', 'p', 'buchatskaya', 'e', 'doersch', 'c', 'pire', 'piot', 'muno', 'r', 'bootstrap', 'tent', 'new', 'approach', 'selfsupervise', 'learning', 'ranzato', 'hadsell', 'r', 'ed', 'advance', 'neural', 'information', 'processing', 'system', 'volume', 'pp', 'cur', 'run', 'url', 'hafner', 'lillicrap', 'j', 'dream', 'control', 'learn', 'behavior', 'latent', 'imagination', 'international', 'conference', 'learn', 'representation', 'url', 'https', 'openreviewnetforum', 'hansen', 'yuan', 'rajeswaran', 'su', 'h', 'h', 'pretraine', 'visuomotor', 'control', 'revisit', 'learningfromscratch', 'baseline', 'workshop', 'pretraine', 'robot', 'learn', 'https', 'openreviewnetforum', 'idtntiauq50e', 'hansen', 'su', 'h', 'temporal', 'difference', 'learn', 'model', 'predictive', 'control', 'song', 'c', 'niu', 'g', 'ed', 'proceeding', '39th', 'international', 'conference', 'machine', 'learn', 'volume', 'proceeding', 'machine', 'learn', 'research', 'pp', 'pmlr', 'url', 'premiertaco', 'pretraine', 'multitask', 'representation', 'temporal', 'actiondriven', 'contrastive', 'loss', 'hansen', 'su', 'h', 'temporal', 'difference', 'learn', 'model', 'predictive', 'control', 'song', 'c', 'niu', 'g', 'ed', 'proceeding', '39th', 'international', 'conference', 'machine', 'learn', 'volume', 'proceeding', 'machine', 'learn', 'research', 'pp', 'pmlr', 'url', 'deep', 'residual', 'learning', 'image', 'recognition', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'cvpr', 'pp', 'doi', '101109cvpr201690', 'henaff', 'dataefficient', 'image', 'recognition', 'contrastive', 'predictive', 'coding', 'h', 'ed', 'proceeding', '37th', 'international', 'conference', 'machine', 'learn', 'volume', 'proceeding', 'machine', 'learn', 'research', 'pp', 'pmlr', 'jul', 'url', 'pion', 'weinzaepfel', 'p', 'hard', 'negative', 'mixing', 'contrastive', 'learning', 'ranzato', 'hadsell', 'r', 'ed', 'advance', 'neural', 'information', 'processing', 'system', 'volume', 'pp', 'url', 'https', 'proceeding', 'rho', 'actiondriven', 'contrastive', 'representation', 'reinforcement', 'learning', 'plo', 'doi', 'doiorg101371journal', 'laskin', 'srinivas', 'abbeel', 'contrastive', 'unsupervised', 'representation', 'reinforcement', 'learning', 'h', 'ed', 'proceeding', '37th', 'international', 'conference', 'machine', 'learn', 'volume', 'proceeding', 'machine', 'learn', 'research', 'pp', 'pmlr', 'jul', 'url', 'https', 'proceeding', 'mlrpressv119laskin20ahtml', 'seo', 'abbeel', 'p', 'reinforcement', 'learning', 'balanced', 'replay', 'pessimistic', 'qensemble', '5th', 'annual', 'conference', 'robot', 'learn', 'openreviewnet', 'idaljxhei6j5w', 'selvaraju', 'r', 'r', 'gotmare', 'joty', 'c', 'hoi', 'align', 'fuse', 'vision', 'language', 'representation', 'learn', 'momentum', 'distillation', 'c', 'stone', 'p', 'libero', 'benchmarke', 'knowledge', 'transfer', 'lifelong', 'robot', 'learning', 'thirtyseventh', 'conference', 'neural', 'information', 'processing', 'system', 'dataset', 'benchmark', 'track', 'url', 'https', 'openreviewnetforum', 'idxzetnsudjk', 'mcduff', 'song', 'active', 'contrastive', 'learning', 'audiovisual', 'video', 'representation', 'international', 'conference', 'learn', 'representation', 'url', 'https', 'openreviewnetforum', 'omizhueahb', 'sodhani', 'bastani', 'vip', 'universal', 'visual', 'reward', 'representation', 'valueimplicit', 'pretraine', 'eleventh', 'international', 'conference', 'learn', 'representation', 'url', 'https', 'openreviewnetforum', 'majumdar', 'yadav', 'silwal', 'jain', 'berge', 'abbeel', 'p', 'malik', 'maksymet', 'rajeswaran', 'mei', 'search', 'artificial', 'visual', 'cortex', 'embody', 'intelligence', 'mazoure', 'b', 'combe', 'r', 'doan', 'l', 'p', 'r', 'deep', 'reinforcement', 'learn', 'ranzato', 'hadsell', 'r', 'ed', 'advance', 'neural', 'information', 'processing', 'system', 'volume', 'pp', 'url', 'https', 'proceeding', 'mendonca', 'r', 'rybkin', 'pathak', 'discover', 'achieve', 'goal', 'world', 'model', 'advance', 'neural', 'information', 'processing', 'system', 'misra', 'krishnamurthy', 'langford', 'kinematic', 'state', 'abstraction', 'provably', 'efficient', 'rich', 'observation', 'reinforcement', 'learn', 'abs191105815', 'http', 'arxivorgabs191105815', 'nair', 'rajeswaran', 'c', 'gupta', 'universal', 'visual', 'representation', 'robot', 'manipulation', '6th', 'annual', 'conference', 'robot', 'learn', 'openreviewnetforum', 'nam', 'learn', 'multidomain', 'convolutional', 'neural', 'network', 'visual', 'tracking', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'cvpr', 'pp', 'doi', 'premiertaco', 'pretraine', 'multitask', 'representation', 'temporal', 'actiondriven', 'contrastive', 'loss', 'pang', 'h', 'libra', 'rcnn', 'balanced', 'learning', 'object', 'detection', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'rajeswaran', 'purushwalkam', 'gupta', 'unsurprising', 'effectiveness', 'pretraine', 'vision', 'model', 'control', 'song', 'c', 'niu', 'g', 'ed', 'proceeding', '39th', 'international', 'conference', 'machine', 'learn', 'volume', 'proceeding', 'machine', 'learn', 'research', 'pp', 'pmlr', 'perez', 'e', 'h', 'courville', 'c', 'film', 'visual', 'reasoning', 'general', 'conditioning', 'layer', 'radford', 'child', 'r', 'luan', 'amodei', 'sutskever', 'language', 'model', 'unsupervise', 'learner', 'robinson', 'sra', 'contrastive', 'learning', 'hard', 'negative', 'sample', 'international', 'conference', 'learn', 'representation', 'url', 'https', 'openreviewnetforum', 'idcr1xoq0uth', 'schwarzer', 'anand', 'goel', 'r', 'hjelm', 'r', 'courville', 'dataefficient', 'reinforcement', 'learning', 'selfpredictive', 'representation', 'international', 'conference', 'learn', 'representation', 'url', 'https', 'openreviewnetforum', 'schwarzer', 'noukhovitch', 'anand', 'l', 'hjelm', 'r', 'bachman', 'p', 'courville', 'pretraine', 'representation', 'dataefficient', 'reinforcement', 'learning', 'beygelzimer', 'dauphin', 'p', 'ed', 'advance', 'neural', 'information', 'processing', 'system', 'url', 'https', 'openreview', 'r', 'rybkin', 'abbeel', 'p', 'hafner', 'pathak', 'planning', 'explore', 'selfsupervise', 'world', 'model', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'seo', 'abbeel', 'p', 'mask', 'world', 'model', 'visual', 'control', 'volume', 'proceeding', 'machine', 'learn', 'research', 'pp', 'pmlr', 'shrivastava', 'gupta', 'girshick', 'r', 'training', 'regionbase', 'object', 'detector', 'online', 'hard', 'example', 'mining', 'ieee', 'conference', 'computer', 'vision', 'pattern', 'recognition', 'cvpr', 'stooke', 'abbeel', 'p', 'laskin', 'decouple', 'representation', 'learn', 'reinforcement', 'learning', 'ed', 'proceeding', '38th', 'international', 'conference', 'machine', 'learn', 'volume', 'proceeding', 'machine', 'learn', 'research', 'pp', 'pmlr', 'stooke', 'abbeel', 'p', 'laskin', 'decouple', 'representation', 'learn', 'reinforcement', 'learning', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'sun', 'r', 'cohen', 'e', 'transfer', 'rl', 'observation', 'feature', 'space', 'model', 'base', 'regularization', 'international', 'conference', 'learn', 'representation', 'https', 'openreview', 'netforum', 'id7kdaoosi81c', 'sun', 'madaan', 'r', 'kapoor', 'smart', 'selfsupervise', 'multitask', 'pretraine', 'control', 'transformer', 'eleventh', 'international', 'conference', 'learn', 'representation', 'url', 'https', 'openreviewnetforum', 'id9pih3hg8qef', 'tabassum', 'wahe', 'h', 'hard', 'negative', 'sampling', 'strategy', 'contrastive', 'representation', 'learn', 'tassa', 'doron', 'muldal', 'erez', 'budden', 'merel', 'j', 'lefrancq', 'lillicrap', 'riedmiller', 'deepmind', 'control', 'den', 'li', 'vinyal', 'representation', 'learn', 'contrastive', 'predictive', 'code', 'wan', 'k', 'bootstrappe', 'face', 'detection', 'hard', 'negative', 'example', 'r', 'sun', 'r', 'wongkamjan', 'w', 'h', 'coplanner', 'plan', 'roll', 'conservatively', 'explore', 'optimistically', 'modelbase', 'rl', 'twelfth', 'international', 'conference', 'learn', 'representation', 'url', 'https', 'openreviewnetforum', 'sun', 'r', 'vemprala', 'r', 'madaan', 'r', 'ba', 'z', 'kapoor', 'imitation', 'need', 'generalize', 'decisionmake', 'dualphase', 'training', 'arxiv', 'preprint', 'darrell', 'malik', 'mask', 'visual', 'pretraining', 'motor', 'control', 'xu', 'r', 'liang', 'h', 'drm', 'master', 'visual', 'reinforcement', 'learning', 'dormant', 'ratio', 'minimization', 'twelfth', 'international', 'conference', 'learn', 'representation', 'openreviewnet', 'forum', 'premiertaco', 'pretraine', 'multitask', 'representation', 'temporal', 'actiondriven', 'contrastive', 'loss', 'yarat', 'fergus', 'r', 'lazaric', 'l', 'reinforcement', 'learning', 'prototypical', 'representation', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'yarat', 'fergus', 'r', 'lazaric', 'l', 'master', 'visual', 'continuous', 'control', 'improve', 'dataaugmente', 'rein', 'forcement', 'learning', 'international', 'conference', 'learn', 'representation', 'https', 'openreview', 'netforum', 'idsjyyes8', 'quillen', 'r', 'hausman', 'c', 'metaworld', 'benchmark', 'evaluation', 'multitask', 'meta', 'reinforcement', 'learning', 'conference', 'robot', 'yuan', 'z', 'xue', 'yuan', 'gao', 'pretraine', 'image', 'encoder', 'alizable', 'visual', 'reinforcement', 'learning', 'mohame', 'agarwal', 'belgrave', 'cho', 'ed', 'advance', 'neural', 'information', 'processing', 'system', 'volume', 'pp', 'run', 'url', 'https', 'mcallister', 'r', 'r', 'gal', 'learn', 'invariant', 'representation', 'reinforcement', 'learning', 'reconstruction', 'international', 'conference', 'learn', 'representation', 'url', 'https', 'openreviewnetforum', 'id2fcwdkrreu', 'r', 'sun', 'j', 'h', 'h', 'temporal', 'latent', 'actiondriven', 'contrastive', 'loss', 'visual', 'reinforcement', 'learning', 'thirtyseventh', 'conference', 'neural', 'information', 'processing', 'system', 'url', 'https', 'openreviewnetforum', 'idezcsmoy1w9', 'r', 'h', 'model', 'ensemble', 'necessary', 'modelbase', 'single', 'model', 'lipschitz', 'regularize', 'value', 'function', 'eleventh', 'international', 'conference', 'learn', 'representation', 'url', 'https', 'openreviewnetforum', 'idhnyjbk3cwr', 'premiertaco', 'pretraine', 'multitask', 'representation', 'temporal', 'actiondriven', 'contrastive', 'loss', 'detailed', 'discussion', 'relate', 'work', 'pretraine', 'visual', 'representation', 'exist', 'work', 'apply', 'selfsupervise', 'pretraining', 'rich', 'vision', 'datum', 'build', 'foundation', 'model', 'however', 'apply', 'approach', 'sequential', 'decisionmake', 'task', 'challenge', 'recent', 'work', 'explore', 'largescale', 'pretraine', 'offline', 'datum', 'context', 'reinforcement', 'learning', 'effort', 'nair', 'vip', 'yuan', 'majumdar', 'highlight', 'direction', 'however', 'notable', 'gap', 'dataset', 'use', 'pretraining', 'actual', 'downstream', 'task', 'fact', 'recent', 'study', 'find', 'model', 'train', 'scratch', 'often', 'perform', 'well', 'use', 'pretraine', 'representation', 'suggest', 'limitation', 'approach', 'important', 'acknowledge', 'pretraine', 'representation', 'controlrelevant', 'lack', 'explicit', 'learning', 'latent', 'world', 'model', 'contrast', 'prior', 'approach', 'pretraine', 'representation', 'learn', 'capture', 'controlrelevant', 'feature', 'effective', 'temporal', 'contrastive', 'learning', 'objective', 'control', 'task', 'several', 'pretraine', 'framework', 'emerge', 'model', 'stateaction', 'interaction', 'highdimensional', 'observation', 'leverage', 'causal', 'attention', 'mechanism', 'smart', 'sun', 'introduce', 'selfsupervise', 'controlcentric', 'objective', 'train', 'transformerbase', 'model', 'multitask', 'decisionmake', 'require', 'additional', 'finetuning', 'large', 'number', 'demonstration', 'downstream', 'time', 'improvement', 'dualmind', 'pretrain', 'representation', 'use', 'task', 'generalpurpose', 'decisionmake', 'taskspecific', 'finetuning', 'method', 'mendonca', 'yarat', 'sun', 'first', 'learn', 'general', 'representation', 'explore', 'environment', 'online', 'use', 'representation', 'train', 'policy', 'downstream', 'task', 'comparison', 'approach', 'notably', 'efficient', 'require', 'training', 'extensive', 'task', 'set', 'nevertheless', 'provide', 'empirical', 'evidence', 'demonstrate', 'method', 'effectively', 'handle', 'multitask', 'pretraine', 'contrastive', 'representation', 'visual', 'visual', 'yarat', 'hansen', 'et', 'longstanding', 'challenge', 'entangled', 'problem', 'representation', 'learning', 'credit', 'assignment', 'contrastive', 'learning', 'play', 'pivotal', 'role', 'enhance', 'sample', 'efficiency', 'visual', 'rl', 'leverage', 'similarity', 'constraint', 'datum', 'learn', 'effective', 'visual', 'representation', 'extract', 'highlevel', 'feature', 'utilize', 'infonce', 'maximize', 'agreement', 'augmented', 'observation', 'explicitly', 'consider', 'temporal', 'relationship', 'state', 'several', 'approach', 'stdim', 'atc', 'stooke', 'introduce', 'temporal', 'dynamic', 'contrastive', 'loss', 'maximize', 'mutual', 'information', 'state', 'short', 'temporal', 'interval', 'facilitate', 'capture', 'temporal', 'dependency', 'driml', 'mazoure', 'propose', 'policydependent', 'auxiliary', 'objective', 'enhance', 'agreement', 'representation', 'consecutive', 'state', 'specifically', 'consider', 'first', 'action', 'action', 'sequence', 'recent', 'advancement', 'incorporate', 'action', 'contrastive', 'loss', 'emphasize', 'behavioral', 'similarity', 'take', 'step', 'far', 'learn', 'state', 'action', 'representation', 'optimize', 'mutual', 'information', 'representation', 'current', 'state', 'pair', 'action', 'sequence', 'representation', 'correspond', 'future', 'state', 'approach', 'build', 'efficient', 'extension', 'taco', 'harness', 'full', 'potential', 'state', 'action', 'representation', 'downstream', 'task', 'theory', 'side', 'homer', 'misra', 'use', 'binary', 'temporal', 'contrastive', 'objective', 'reminiscent', 'approach', 'use', 'differ', 'abstract', 'action', 'well', 'state', 'use', 'ancillary', 'embed', 'remove', 'level', 'construction', 'course', 'extensive', 'empirical', 'validation', 'hard', 'negative', 'sampling', 'strategy', 'contrastive', 'learn', 'propose', 'negative', 'example', 'sample', 'strategy', 'closely', 'relate', 'hard', 'negative', 'example', 'mining', 'literature', 'selfsupervise', 'learning', 'well', 'area', 'machine', 'learn', 'hard', 'negative', 'mining', 'indeed', 'use', 'variety', 'task', 'facial', 'recognition', 'wan', 'object', 'detection', 'track', 'imagetext', 'retrieval', 'introduce', 'negative', 'example', 'difficult', 'randomly', 'choose', 'one', 'improve', 'performance', 'model', 'regime', 'selfsupervise', 'learning', 'different', 'negative', 'example', 'sample', 'strategy', 'also', 'discuss', 'empirically', 'theoretically', 'improve', 'quality', 'pretraine', 'representation', 'particular', 'robinson', 'modify', 'original', 'nce', 'objective', 'develop', 'distribution', 'negative', 'example', 'prioritize', 'pair', 'currently', 'similar', 'representation', 'suggest', 'mix', 'hard', 'negative', 'example', 'latent', 'space', 'introduce', 'method', 'actively', 'sample', 'uncertain', 'negative', 'calculate', 'gradient', 'loss', 'function', 'relative', 'model', 'confident', 'prediction', 'furthermore', 'tabassum', 'sample', 'negative', 'combine', 'objective', 'identify', 'modeluncertain', 'negative', 'select', 'negative', 'close', 'anchor', 'point', 'latent', 'embed', 'space', 'ensure', 'representativeness', 'sample', 'population', 'propose', 'approach', 'bear', 'resemblance', 'exist', 'negative', 'sampling', 'strategy', 'contrastive', 'learning', 'literature', 'deal', 'unique', 'challenge', 'sequential', 'decision', 'make', 'data', 'distribution', 'shift', 'task', 'heterogeneity', 'data', 'quality', 'discuss', 'introduction', 'advance', 'work', 'premiertaco', 'pretraine', 'multitask', 'representation', 'temporal', 'actiondriven', 'contrastive', 'loss', 'specifically', 'design', 'capture', 'controlrelevant', 'information', 'latent', 'representation', 'premier', 'taco', 'introduce', 'straightforward', 'effective', 'negative', 'sampling', 'strategy', 'tailor', 'multitask', 'representation', 'pretraine', 'strategy', 'involve', 'sample', 'negative', 'example', 'window', 'center', 'anchor', 'point', 'computationally', 'efficient', 'demonstrate', 'superior', 'performance', 'fewshot', 'adaptation', 'additional', 'experiment', 'result', 'b1', 'finetune', 'comparison', 'nair', 'indomain', 'finetuning', 'hansen', 'finetune', 'premiertaco', 'deepmind', 'control', 'suite', 'metaworld', 'present', 'figure', 'figure', 'w4', 'compatibility', 'finetune', 'nair', 'generalize', 'pretraine', 'visual', 'encoder', 'learn', 'objective', 'indomain', 'finetune', 'deepmind', 'control', 'suite', 'metaworld', 'figure', 'w4', 'compatibility', 'finetune', 'nair', 'generalize', 'pretraine', 'visual', 'encoder', 'learn', 'objective', 'indomain', 'finetune', 'deepmind', 'control', 'suite', 'metaworld', 'pretraine', 'visual', 'representation', 'provide', 'full', 'result', 'pretraine', 'visual', 'encoder', 'task', 'deepmind', 'control', 'suite', 'metaworld', 'libero10', 'success', 'rate', 'metaworld', 'finetune', 'pretraine', 'visual', 'encoder', 'finetune', 'finetune', 'e', 'r', 'e', 'binpicke', 'disassemble', 'pickplacewall', 'control', 'finetune', 'pretraine', 'visual', 'encoder', 'finetune', 'finetune', 'e', 'r', 'c', 'e', 'e', 'r', 'l', 'r', 'e', 'finger', 'spin', 'walker', 'walk', 'walk', 'catch', 'run', 'quadrupe', 'walk', 'quadrupe', 'run', 'premiertaco', 'pretraine', 'multitask', 'representation', 'temporal', 'actiondriven', 'contrastive', 'loss', 'dmcontrol', 'pretraine', 'visual', 'model', 'pretraine', 'visual', 'model', 'spin', 'bin', 'pick', 'hopper', 'hop', 'disassemble', 'walker', 'walk', 'hand', 'humanoid', 'walk', 'peg', 'insert', 'side', 'dog', 'trot', 'pick', 'hole', 'cup', 'catch', 'pick', 'place', 'reacher', 'hard', 'shelf', 'place', 'run', 'stick', 'pull', 'quadrupe', 'walk', 'quadrupe', 'run', 'table', 'fewshot', 'result', 'pretraine', 'visual', 'representation', 'nair', 'majumdar', 'libero', 'model', 'unseen', 'task', 'good', 'atc', 'task', 'task', 'task', 'task', 'task', 'task', 'task', 'mean', 'table', 'w1', 'versatility', 'efficiency', 'fiveshot', 'behavior', 'clone', 'unseen', 'task', 'libero', 'success', 'rate', 'premiertaco', 'baseline', 'first', 'task', 'libero10', 'result', 'aggregate', 'random', 'seed', 'bold', 'number', 'indicate', 'good', 'result', 'c', 'implementation', 'detail', 'dataset', 'pretraine', 'task', 'deepmind', 'control', 'train', 'visual', 'rl', 'agent', 'individual', 'task', 'yarat', 'convergence', 'store', 'historical', 'interaction', 'step', 'separate', 'buffer', 'sample', 'trajectory', 'buffer', 'task', 'humanoid', 'stand', 'dog', 'walk', 'task', 'significantly', 'hard', 'use', 'pretraine', 'trajectory', 'instead', 'episode', 'deepmind', 'control', 'suite', 'consist', 'time', 'step', 'term', 'randomly', 'collect', 'dataset', 'sample', 'trajectory', 'take', 'action', 'dimension', 'independently', 'sample', 'uniform', 'distribution', '−1', 'metaworld', 'collect', 'trajectory', 'task', 'episode', 'consist', 'time', 'step', 'add', 'gaussian', 'noise', 'standard', 'deviation', 'provide', 'scripted', 'policy', 'libero', 'take', 'human', 'demonstration', 'dataset', 'contain', 'demosntration', 'trajectory', 'task', 'pretraine', 'shallow', 'convolutional', 'network', 'use', 'deepmind', 'control', 'suite', 'metaworld', 'follow', 'architecture', 'yarat', 'add', 'layer', 'normalization', 'top', 'output', 'encoder', 'set', 'feature', 'dimension', 'connet', 'encoder', 'total', 'encoder', 'parameter', 'class', 'encoder', 'module', 'def', 'init', 'self', 'init', 'self', 'self', 'convnet', 'sequential', 'conv2d', 'stride', 'relu', 'conv2d', 'stride', 'relu', 'conv2d', 'stride', 'relu', 'conv2d', 'stride', 'relu', 'self', 'trunk', 'sequential', 'linear', 'self', 'layernorm', 'tanh', 'premiertaco', 'pretraine', 'multitask', 'representation', 'temporal', 'actiondriven', 'contrastive', 'loss', 'def', 'forward', 'self', 'obs', 'obs', 'h', 'self', 'convnet', 'view', 'h', 'shape', 'return', 'self', 'trunk', 'h', 'list', 'shallow', 'convolutional', 'network', 'architecture', 'use', 'premiertaco', 'libero', 'use', 'randomly', 'initialize', 'pretraine', 'resnet18', 'encoder', 'encode', 'thirdperson', 'view', 'firstperson', 'view', 'image', 'film', 'encode', 'method', 'incorporate', 'bert', 'embed', 'task', 'language', 'instruction', 'downstream', 'behavior', 'clone', 'apply', 'transformer', 'decoder', 'module', 'context', 'length', 'top', 'resnet', 'encoding', 'extract', 'temporal', 'information', 'attach', 'twolayer', 'mlp', 'hidden', 'size', 'policy', 'head', 'architecture', 'follow', 'loss', 'number', 'set', 'experiment', 'window', 'size', 'w', 'fix', 'action', 'encoder', 'twolayer', 'mlp', 'input', 'size', 'action', 'space', 'dimensionality', 'hide', 'size', 'output', 'size', 'dimensionality', 'action', 'space', 'projection', 'layer', 'twolayer', 'mlp', 'input', 'size', 'feature', 'dimension', 'number', 'timestep', 'time', 'dimensionality', 'action', 'space', 'hidden', 'size', 'term', 'projection', 'layer', 'h', 'also', 'twolayer', 'mlp', 'input', 'output', 'size', 'feature', 'dimension', 'hidden', 'size', 'experiment', 'set', 'batch', 'size', 'learning', 'rate', 'contrastiveselfsupervise', 'base', 'baseline', 'atc', 'spr', 'use', 'batch', 'size', 'premiertaco', 'inverse', 'dynamic', 'model', 'baseline', 'use', 'batch', 'size', 'imitation', 'learn', 'batch', 'size', 'learning', 'rate', 'use', 'deepmind', 'control', 'suite', 'metaworld', 'batch', 'size', 'use', 'libero', 'behavior', 'clone', 'finetune', 'encoder', 'however', 'apply', 'premiertaco', 'large', 'pretraine', 'resnetvit', 'encoder', 'model', 'keep', 'model', 'weight', 'freeze', 'total', 'take', 'gradient', 'step', 'conduct', 'evaluation', 'step', 'evaluation', 'deepmind', 'control', 'utilize', 'train', 'policy', 'execute', 'episode', 'subsequently', 'record', 'mean', 'episode', 'reward', 'case', 'metaworld', 'libero', 'execute', 'episode', 'document', 'success', 'rate', 'train', 'policy', 'report', 'average', 'high', 'episode', 'rewardssuccess', 'rate', 'evaluate', 'checkpoint', 'computational', 'resource', 'experiment', 'use', 'nvidia', 'rtx', 'pytorch', 'distribute', 'dataparallel', 'pretraine', 'visual', 'representation', 'use', 'downstream', 'imitation', 'learn', 'deepmind', 'control', 'suite', 'metaworld', 'rtx', 'libero', 'pretraine', 'multitask', 'representation', 'temporal', 'actiondriven', 'contrastive', 'loss', 'additional', 'ablation', 'study', 'negative', 'example', 'sample', 'strategy', 'sample', 'negative', 'example', 'size', 'w', 'window', 'center', 'positive', 'example', 'datum', 'point', 'however', 'principle', 'also', 'use', 'sample', 'window', 'negative', 'example', 'instead', 'sample', 'table', 'compare', 'performance', 'negative', 'example', 'sample', 'strategy', 'unseen', 'deepmind', 'control', 'suite', 'task', 'bold', 'number', 'indicate', 'well', 'result', 'finger', 'spin', 'hopper', 'walk', 'walk', 'catch', 'reacher', 'hard', 'run', 'quadrupe', 'walk', 'quadrupe', 'run', 'overall', 'sample', 'sample', 'table', 'result', 'different', 'negative', 'sampling', 'strategy', 'unseen', 'deepmind', 'control', 'suite', 'task', 'show', 'table', 'find', 'use', 'sample', 'size', 'w', 'window', 'significantly', 'enhance', 'performance', 'compare', 'moreover', 'approach', 'considerably', 'increase', 'computational', 'overhead', 'give', 'result', 'choose', 'computationally', 'efficient', 'strategy', 'sample', 'single', 'negative', 'example', 'size', 'w', 'window', 'task', 'instruction', 'downstream', 'libero', 'task', 'table', 'provide', 'language', 'instruction', 'libero', 'downstream', 'task', 'refer', 'reader', 'detail', 'task', 'task', 'task', 'scene', 'task', 'living', 'room', 'scene2', 'living', 'room', 'scene2', 'kitchen', 'scene3', 'kitchen', 'living', 'room', 'scene5', 'living', 'room', 'scene6', 'living', 'room', 'scene1', 'put', 'alphabet', 'soup', 'tomato', 'sauce', 'basket', 'put', 'cream', 'cheese', 'box', 'butter', 'basket', 'turn', 'stove', 'put', 'moka', 'pot', 'put', 'black', 'bowl', 'bottom', 'drawer', 'cabinet', 'close', 'put', 'left', 'plate', 'put', 'yellow', 'white', 'right', 'plate', 'pick', 'book', 'place', 'back', 'compartment', 'caddy', 'put', 'plate', 'put', 'chocolate', 'pudding', 'right', 'plate', 'put', 'alphabet', 'soup', 'cream', 'cheese', 'box', 'basket', 'table', 'language', 'instruction', 'libero', 'downstream', 'task']"
Assessing AI-Based Code Assistants in Method Generation Tasks,"[{'title': 'doi', 'href': 'http://dx.doi.org/10.1145/3639478.3643122', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2402.09022v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2402.09022v1', 'rel': 'related', 'type': 'application/pdf'}]",2024-02-14 08:52:45,"4
2
0
2

b
e
F
3
1

]
T
G
.
s
c
[

1
v
3
9
3
8
0
.
2
0
4
2
:
v
i
X
r
a

Published as a conference paper at ICLR 2024

NFGTRANSFORMER: EQUIVARIANT REPRESENTATION
LEARNING FOR NORMAL-FORM GAMES

Siqi Liu†,♢, Luke Marris†,♢, Georgios Piliouras†, Ian Gemp†, Nicolas Heess†
†Google DeepMind, ♢University College London
{liusiqi,marris,gpil,imgemp,heess}@google.com

ABSTRACT

Normal-form games (NFGs) are the fundamental model of strategic interaction.
We study their representation using neural networks. We describe the inherent
equivariance of NFGs — any permutation of strategies describes an equivalent
game — as well as the challenges this poses for representation learning. We then
propose the NfgTransformer1 architecture that leverages this equivariance, lead-
ing to state-of-the-art performance in a range of game-theoretic tasks including
equilibrium-solving, deviation gain estimation and ranking, with a common ap-
proach to NFG representation. We show that the resulting model is interpretable
and versatile, paving the way towards deep learning systems capable of game-
theoretic reasoning when interacting with humans and with each other.

1

INTRODUCTION

Representing data in a learned embedding space, or representation learning, is one of the time-
less ideas of deep learning. Foundational representation learning architectures (LeCun et al., 1995;
Hochreiter & Schmidhuber, 1997; Vaswani et al., 2017) have provided performance and general-
ity, bringing together researchers who used to work on expert systems targeting narrow domains.
Consider Convolutional Neural Networks (CNNs): by exploiting the translation invariance inherent
to images, CNNs replaced feature descriptors underpinning tasks as diverse as image classification,
segmentation and in-painting, leading to paradigm shifts across the field.

One area of research that has resisted this trend is game theory. Here, we see two striking similar-
ities to classical computer vision research. First, active research topics such as equilibrium solving
(Vlatakis-Gkaragkounis et al., 2020), ranking (Balduzzi et al., 2018), social choice theory (Anshele-
vich et al., 2021) and population learning (Lanctot et al., 2017) all focus on specialised solutions,
despite sharing the common language of normal-form games (NFGs). Second, task-specific so-
lutions suffer from fundamental limitations. The popular Elo ranking algorithm (Elo, 2008), for
instance, assigns a scalar rating to each player derived from an NFG between players. Although Elo
ratings are designed to be predictive of match outcomes, they are poor predictors beyond transitive
games — the Elo score is simply too restrictive a representation to reflect cyclic game dynamics. Im-
provements to Elo followed (Bertrand et al., 2023), but all relied on engineered feature descriptors,
instead of learning. In equilibrium-solving, computing exact Nash equilibria is intractable beyond
two-player zero-sum games (Daskalakis et al., 2009) yet approximate solvers are non-differentiable,
take non-deterministic amount of time to converge, struggle to parallelise, and can fail. These fun-
damental limitations have indirect consequences too. An entire line of works in tabular multiagent
reinforcement learning (RL) (Littman et al., 2001; Hu & Wellman, 2003; Greenwald et al., 2003)
relied on equilibrium solving as part of their learning rules — an NFG is constructed from agents’
Q-tables in each state, whose equilibrium informs subsequent policy updates. Unfortunately, re-
viving these ideas in the context of deep RL has been challenging, if not impossible, as it requires
equilibrium solving as a subroutine in between every gradient update.

Indeed, we are not the first to recognise these limitations. Several recent works incorporated repre-
sentation learning implicitly, in narrow domains (Marris et al., 2022; Duan et al., 2023; Vadori &
Savani, 2023). We address these limitations explicitly and in generality. Our goal is to develop prin-
cipled, general-purpose representation of NFGs that can be used in a wide range of game-theoretic

1The model is open-sourced at https://github.com/google-deepmind/nfg_transformer.

1

 
 
 
 
 
 
Published as a conference paper at ICLR 2024

applications. We ask 1) which property, if any, could we leverage for efficient representation learning
of NFGs without loss of generality and 2) can we expect performance in a range of game-theoretic
tasks using a common approach to representing NFGs. We answer both questions affirmatively and
propose NfgTransformer, a general-purpose representation learning architecture with state-of-the-art
performance in tasks as diverse as equilibrium solving, deviation gain estimation and ranking.

In its most basic form, strategic interactions between players are formulated as NFGs where players
simultaneously select actions and receive payoffs subject to the joint action. Strategic interactions
are therefore presented as payoff tensors, with values to each player tabulated under every joint ac-
tion. This tabular view of strategic interactions presents its own challenges to representation learn-
ing. Unlike modalities such as images and text whose spatial structure can be exploited for efficient
representation learning (LeCun et al., 1995; Hochreiter & Schmidhuber, 1997), the position of an
action in the payoff tensor is unimportant: permuting the payoff matrix of any NFG yields an equiva-
lent game — an equivalency known as strongly isomorphic games (McKinsey, 1951; Gabarr´o et al.,
2011). This inherent equivariance to NFGs has inspired prior works to compose order-invariant
pooling functions in the neural network architecture for efficiency, albeit at the expense of the gen-
erality of the representation (Feng et al., 2021; Marris et al., 2022).

We aim to leverage this inherent equivariance of NFGs while preserving full generality of the learned
representation. This implies several desiderata that we discuss in turn. First, the representation needs
to be versatile, allowing for inquiries at the level of individual actions, joint-actions, per-player or
for the entire game. Second, it needs to be equivariant: for any per-action inquiry, the outputs
for two actions should be exchanged if their positions are exchanged in the payoff tensor. Third,
the embedding function should not assume that outcomes of all joint-actions are observed — the
representation should accommodate incomplete NFGs in a principled way. Fourth, the function
should apply to games of different sizes. This implies that the number of network parameters should
be independent from the size of the games (Hartford et al., 2016). Finally, it would be desirable if
the network architecture is interpretable, allowing for inspection at different stages of the network.

In the rest of this paper, we show how our proposed encoder architecture, NfgTransformer, satisfies
all these desiderata simultaneously. The key idea behind our approach is to consider the embedding
function class that represents an NFG as action embeddings, reversing the generative process from
actions to payoffs. Action embeddings can be suitably composed to answer questions at different
granularities and allows for equivariance in a straightforward way — permutations of actions or
players in the payoff tensor shall be reflected as a permutation in the action embeddings. We ar-
gue that NfgTransformer is a competitive candidate for general-purpose equivariant representation
learning for NFGs, bridging the gap between deep learning and game-theoretic reasoning.

2 BACKGROUND

Normal-form Games NFGs are the fundamental game formalism where each player p simulta-
p } = Ap and receives a payoff Gp : A → R
p, . . . , aT
neously plays one of its T actions ap ∈ {a1
as a function of the joint action a = (a1, . . . , aN ) ∈ A of all N players. Let a = (ap, a¬p) with
a¬p = (. . . , ap−1, ap+1, . . . ) ∈ A¬p the actions of all players except p. Let σ(a) = σ(ap, a¬p)
denote the probability of players playing the joint action a and σ a probability distribution over the
space of joint actions A. A pure strategy is an action distribution that is deterministic when a mixed-
strategy can be stochastic. The value to player p under σ is given as Ea∼σ[Gp(ap, a¬p)]. We refer
to the payoff tensor tabulated according to the action and player ordering above as G.

Nash Equilibrium (NE) Under a mixed joint strategy that factorises into an outer product of
marginals σ = (cid:78)

p σp, player p’s unilateral deviation incentive is defined as
E
δp(σ) = max
p∈Ap
a∼σ
A factorisable mixed joint strategy σ = (cid:78)
p σp is an ϵ-NE if and only if δ(σ) = maxp δp(σ) ≤ ϵ.
We refer to this quantity as the NE GAP as it intuitively measures the distance from σ to an NE of
the game. A mixed-strategy NE is guaranteed to exist for a finite game (Nash, 1951) but exactly
computing a normal-form NE beyond two-player zero-sum is PPAD-complete (Chen et al., 2009;
Daskalakis et al., 2009). If σ is deterministic with σ(a) = 1, then δ(σ) or equivalently δ(a) defines
the maximum deviation gain of the joint pure-strategy a. a is a pure-strategy NE when δ(a) = 0.

p, a¬p) − Gp(a)].

[Gp(a′

(1)

a′

2

Published as a conference paper at ICLR 2024

Permutation Equivariance Consider a strong isomorphism ϕ : G → G′ of NFGs (Gabarr´o et al.,
2011) with ϕ = ((τp, p ∈ [N ]), ω), τp : ai
p a player action permutation and ω : p → p′ a
player permutation. Elements of the transformed game G′ = ϕ(G) are therefore given as

p → ai′

G′

ω(p)

(cid:0)τω(1)(aω(1)), . . . , τω(N )(aω(N ))(cid:1) = Gp(a1, ..., aN ).

An encoder f : G → (A1, . . . , AN ), with Ap = (a1
is said to be equivariant if

p, . . . , aT

p ) the action embeddings for player p,

f (ϕ(G)) = (A′

1, . . . , A′

N ) with A′

ω(p) = (τp(a1

p), . . . , τp(aT

p ))

(2)

Here we slightly abuse the notation of τp to operate over action embeddings. Intuitively, permutation
equivariance implies that ϕ and f commute, or f (ϕ(G)) = ϕ(f (G)). We adopt the convention that
the player permutation ω is applied after player action permutations τp, ∀p.

Multi-Head Attention We describe self- and cross-attention QKV mechanisms that have become
ubiquitous thanks to their generality and potential to scaling (Vaswani et al., 2017; Dosovitskiy et al.,
2021; Jaegle et al., 2021). Both operations extend the basic QKV attention mechanism as follows:

Attention(Q, K, V ) = softmax

(cid:18) QK T
√
dk

(cid:19)

V

(3)

with Q ∈ Rnq×dk , K ∈ Rnk×dk and V ∈ Rnk×dv the nq queries and nk key-value pairs. Concep-
tually, the attention mechanism outputs a weighted sum of nk values for each of nq query vectors,
whose weights are determined by pairwise dot product between the key and query vectors. The
output is of shape Rnq×dv . The inputs QKV are outputs from fully-connected networks themselves,
with Q a function of xq ∈ Rnq×dxq and K, V projected from the same input xkv ∈ Rnk×dxkv .
The attention operation is: 1) order-invariant with respect to xkv; and 2) equivariant with respect
to xq. These are the key properties we leverage in the design of the NfgTransformer to achieve
its permutation equivariance property. We refer to an attention layer as self-attention when xq is
the same as xkv, and cross-attention if not. In practice, each attention layer may have H attention
heads performing the attention operation of Equation 3 in parallel. This enables the attention layer
to aggregate multiple streams of information in one forward pass.

3 EQUIVARIANT GAME REPRESENTATION

While we have informally motivated the need for equivariant embedding functions, we formally state
two practical implications of an equivariant embedding function that follow from a general theorem
on the conditions under which two actions must have identical embeddings given an equivariant
embedding function. For conciseness, we defer all formal statements and proofs to Appendix A.
Proposition 3.1 (Repeated Actions). If G(ai
equivariant with f (G) = (. . . , (. . . , ai
Proposition 3.2 (Player Symmetry). If player p and q are symmetric, f is deterministic and equiv-
ariant with f (G) = (. . . , Ap, . . . , Aq, . . . ), then Ap and Aq are identical up to permutation.

p, a¬p), ∀a¬p and f is deterministic and

p, . . . ), . . . ) then it follows that ai

p, a¬p) = G(aj

p, . . . , aj

p = aj
p.

Proposition 3.1 guarantees by construction that repeated actions are treated identically in any down-
stream applications. Proposition 3.2 guarantees that player symmetry are reflected in the action
embedding space which we show empirically in Section 5.3 for the NfgTransformer.

4 NFGTRANSFORMER

We now describe the NfgTransformer, an encoder network that factorises a payoff tensor G into ac-
tion embeddings via a sequence of K NfgTransformer blocks (Figure 1 (Top)), each composed of a
sequence of self- and cross-attention operations (Figure 1 (Bottom)). We then show concrete exam-
ples of decoders and loss functions in Figure 2 for several game-theoretic tasks at different decoding
granularities, showing the generality of our approach. Finally, we discuss how the NfgTransformer
naturally incorporates incomplete payoff tensors leveraging the flexibility of attention operations.

3

Published as a conference paper at ICLR 2024

Figure 1: An overview of the NfgTransformer. The payoff tensor G is encoded as action embeddings
{at
p | ∀t ∈ [T ], ∀p ∈ [N ]} (Top). Action embeddings are zero-initialised and iteratively updated
through a sequence of K NfgTransformer blocks (Bottom). An arrow labeled with “(Q)KV” orig-
inates from a set of input (query-)key-values and terminates at a set of outputs. Each dashed box
denotes an unordered set of elements of a specific type and cardinality.

4.1

INITIALISATION & ITERATIVE REFINEMENT

Permutation equivariance implies that action embeddings must be agnostic to how players and ac-
tions are ordered in the payoff tensor. This suggests that action embeddings across players and
actions must be initialised identically. We zero-initialise all action embeddings A = {at
p = 0 |
p ∈ RD. Upon initialisation, action embeddings are iteratively refined
∀t ∈ [T ], ∀p ∈ [N ]} with at
via a sequence of K blocks given current action embeddings and the payoff tensor G. Each block
returns updated action embeddings via self- and cross-attention operations that we describe in turn.

action-to-joint-action self-attention represents a play of each action ap under a joint-action a =
(ap, a¬p) given payoff values to all players. Recall the definition of a self-attention operation, xq =
xkv = {concat(ap, Gp(a))}N
p=1, yielding one vector output per action for every joint-action. We
refer to the output pG(ap, a¬p) as a play of ap under the joint action a and payoffs G;

action-to-play cross-attention then encodes information from all plays of each action at
key-values xkv = {pG(at
singleton output, {at

p, with
p}, a singleton query. This operation yields a

p}, as a function of all its plays and its input action embedding vector;

p, a¬p) | ∀a¬p} and xq = {at

action-to-action self-attention then represents each action given all action embeddings. Here,
xq = xkv = {at
p | ∀p ∈ [N ], ∀t ∈ [T ]}. We ablate this operation (by varying A) in Section 5.1,
showing its benefits in propagating information across action embeddings.

Within each block, equivariance is preserved given key-value order-invariance, and query equiv-
ariance properties of the attention operation. Each output embedding at
p is a function of its own
embedding at input, its unordered set of plays, and the unordered set of all action embeddings.

4.2 TASK-SPECIFIC DECODING

The resulting action embeddings can be used for a variety of downstream tasks at different decoding
granularities. We describe and empirically demonstrate three use-cases in specifics (Figure 2).

4

Initial action embeddings

Final action embeddings

NfgTransformerBlock

x K

NfgTransformer

Action-to-joint-action

Action-to-play

Action-to-action

QKV

KV

QKV

For each of TN joint-actions

n
o
i
t
n
e
t
t

A

f
l
e
S

For each of N×T actions

re-arranging

For each of N×T actions

For each of TN joint-actions

n
o
i
t
n
e
t
t

A
s
s
o
r
C

Q

n
o
i
t
n
e
t
t

A

f
l
e
S

x A

For each of 
N×T actions

NfgTransformerBlock

 
 
 
Published as a conference paper at ICLR 2024

Figure 2: Example task-specific decoders and losses from general-purpose action embeddings.

Nash equilibrium-solving
action distribution for each player ˆσp = softmax(w1
for an action at
to-end via gradient descent with ˆσ = ⊗p

requires decoding at the level of each action, estimating a marginal
p, . . . , wT
p) is the logit
p. Here, we follow Duan et al. (2023) in minimising the loss function maxp δp(ˆσ) end-

ˆδp and δp(ˆσ) = maxa′

Ea∼ˆσ[Gp(a′

p, a¬p) − Gp(a)].

p = MLP(at

p ) where wt

p∈Ap

Max deviation-gain estimation decodes a scalar estimate for each joint-action a =
(a1, . . . , aN ), ∀a ∈ A. Here, we represent each joint action as a = (cid:80)
p ap and estimate its max-
imum deviation gain ˆδ(a) by minimising L2(ˆδ(a), maxp δp(a)) = (ˆδ(a) − maxp δp(a))2, with
δp(a) = maxa′

p, a¬p) − Gp(a)] the deviation gain to player p under the joint action a.

p∈Ap [Gp(a′

Payoff reconstruction decodes a scalar for each payoff value Gp(a), ∀p ∈ [N ], ∀a ∈ A. Here
we use a self-attention operation for decoding, similar to the action-to-joint-action self-attention
operation in the encoder but without appending to action embeddings their payoff values, which are
to be reconstructed. To compute a reconstruction loss, we minimise L2( ˆGp(a), Gp(a)), with ˆGp(a)
a function of the action embedding ap and the unordered set of co-player action embeddings a¬p.

4.3 REPRESENTING INCOMPLETE GAMES

We have assumed thus far that outcomes for every joint-action of the game are observed. This is not
always the case in practice as the costs of evaluating every joint-action can be prohibitive. Instead,
one may infer outcomes for all joint-actions, given an incomplete NFG (Elo, 2008). With a slight
modification, the NfgTransformer accommodates such use-cases in a principled way. To do so, we
extend the vanilla attention implementation (Equation 3) to allow for additional binary mask vectors
mq ∈ {0, 1}nq and mkv ∈ {0, 1}nk for the query and key-value inputs, indicating their validity.
Equation 4 defines this masked attention operation, with 1∞(1) = 0 and 1∞(0) = ∞.

MaskedAttention(Q, K, V, mq, mkv) = softmax

(cid:18) QK T − 1∞(mq ⊗ mkv)
√

dk

(cid:19)

V

(4)

To represent incomplete NFGs, we use the Equation 4 in lieu of Equation 3 in all self- and cross-
attention operations and set masking vectors accordingly. For instance, if a joint-action is unob-
served, then one would set mkv to reflect the validity of each play for the action-to-play operation.
The NfgTransformer architecture is highly effective for representing actions in incomplete games
and predict payoffs for unobserved joint-actions, as we show empirically in Section 5.2.

5

Task-speciﬁc loss functions

For each of N players

For each of TN joint-actions

Max deviation-gain estimation 

For each of TN joint-actions

QKV

Self
Attention

For each of TN joint-actions

For each of N×T actions

NfgTransformer

Nash equilibrium solving

Payoﬀ reconstruction

Published as a conference paper at ICLR 2024

5 RESULTS

The goal of our empirical studies is three-fold. First, we compare the NfgTransformer to baseline
architectures to demonstrate improved performance in diverse downstream tasks on synthetic and
standard games. Second, we vary the model hyper-parameters and observe how they affect perfor-
mance. We show in particular that some tasks require larger action embedding sizes while others
benefit from more rounds of iterative refinement. Lastly, we study how the model learned to solve
certain tasks by interpreting the sequence of learned attention masks in a controlled setting. Our
results reveal that the solution found by the model reflects elements of intuitive solutions to NE-
solving in games. For completeness, we discuss additional empirical results in Appendix D. For
instance, we show all nontrivial equilibrium-invariant 2×2 games are embedded following structure
of the known embedding space proposed in Marris et al. (2023).

5.1 SYNTHETIC GAMES

Table 1: We compare NfgTransformer to baseline architectures in synthetic games. Each configura-
tion is averaged across 5 independent runs. For NfgTransformer variants (Ours), we annotate each
variant with corresponding hyper-parameters (K, A and D as shown in Figure 1). We provide train-
ing curves with confidence intervals and parameter counts of each configuration in Appendix B.1.

Model

N=2
T=16
Ours(D= 32,K=2,A=1) 0.2239
Ours(D= 32,K=4,A=1) 0.0466
Ours(D= 32,K=8,A=1) 0.0344
Ours(D= 64,K=8,A=0) 0.0332
Ours(D= 64,K=8,A=1) 0.0308
Ours(D= 64,K=8,A=2) 0.0243
Ours(D=128,K=2,A=1) 0.2090
Ours(D=128,K=4,A=1) 0.0429
Ours(D=128,K=8,A=1) 0.0308
0.2770
0.3905
0.0829

EquivariantMLP
MLP
NES

NE (NE Gap)
N=3
N=2
T=64
T=8
0.1344
0.1685
0.0892
0.1096
0.0484
0.0554
0.0636
0.0661
0.0478
0.0545
0.0437
0.0542
0.1274
0.1665
0.0804
0.0981
0.0412
0.0502
0.1431
0.2132
0.1741
0.3248
0.1635
0.1478

N=3
T=16
0.0796
0.0553
0.0334
0.0384
0.0325
0.0314
0.0769
0.0530
0.0297
0.0929
0.1381
0.1140

Max-Deviation-Gain (MSE)

N=2
T=16
0.0949
0.0248
0.0067
0.0056
0.0007
0.0005
0.0159
0.0013
0.0001
0.1789
0.3854
0.0488

N=2
T=64
0.5008
0.3755
0.2989
0.1549
0.0784
0.0759
0.1922
0.1361
0.0161
0.8153
0.8354
0.4860

N=3
T=8
0.5206
0.3679
0.3582
0.2848
0.1830
0.1942
0.3154
0.0955
0.0487
0.5433
0.5623
0.4047

N=3
T=16
0.6649
0.6173
0.5825
0.5583
0.4961
0.4725
0.5357
0.4153
0.3641
0.7914
0.7906
0.6480

√

We first evaluate variations of the NfgTransformer architecture on synthetic games of varying sizes
on NE equilibrium-solving and deviation gain estimation. To generate synthetic games with broad
coverage, we follow Marris et al. (2022) which samples games from the equilibrium-invariant
subspace, covering all strategic interactions that can affect the equilibrium solution of an NFG.
Each game’s payoff tensor G has zero-mean over other player strategies and Frobenius norm
T N . We compare our results to baseline MLP networks with numbers of parameters at
∥Gp∥F =
least that of our largest transformer variant (at 4.95M parameters), an equivariant MLP network that
re-arranges actions in descending order of their average payoffs, as well as an NES (Marris et al.,
2022) network that is designed for equilibrium-solving. See Appendix B.1 for details on game sam-
pling, network architectures and parameter counts of each model. We note that the parameter count
of the NfgTransformer is independent of the game size, a desideratum of Hartford et al. (2016).

5.1.1 SOLVING FOR NE EQUILIBRIUM

For equilibrium solving, we optimise variants of the NfgTransformer to minimise the NE GAP
δ(ˆσ) = maxp δp(ˆσ) (Figure 2 (Left)). Table 1 (Left) shows our results. EquivariantMLP outper-
forms MLP (Duan et al., 2023), demonstrating the importance of leveraging equivariance inherent
to NFGs but remains ineffective at solving this task. NES (Marris et al., 2022), equivariant by
construction, significantly outperforms both MLP variants in 2-player settings but trails behind in 3-
player games. The NfgTransformer is also equivariant by construction but learns to capture relevant
information without handcrafted payoff feature vectors. All NfgTransformer variants, most at fewer
parameter count than baselines, significantly outperform across game sizes with near-zero NE GAP.

6

Published as a conference paper at ICLR 2024

Figure 3: Payoff prediction error averaged over all players across unobserved joint-actions. Results
are averaged over 32 randomly sampled empirical DISC games in each game configuration.

Among the NfgTransformer variants, our results show a clear trend: increasing the number of trans-
former blocks (with K ∈ [2, . . . , 8]) improves performance, especially as the game becomes large.
This makes intuitive sense, as it adds to the number of times the action embeddings can be usefully
refined — action embeddings at the end of one iteration become more relevant queries for the next.
In contrast, the benefit of increased action embedding size is muted (with D ∈ [32, . . . , 128]). We
hypothesise that for equilibrium-solving, information related to a subset of the available actions can
often be ignored through iterative refinement (e.g. dominated actions), as they do not contribute to
the final equilibrium solution. Lastly, we evaluate an NfgTransformer variant that does not perform
any action-to-action self-attention (A = 0). In this case, action embeddings for the same player do
not interact within the same block and its performance is markedly worse. Of particular interests is
the comparison between the variants with A = 1 and A = 2 where A = 2 demonstrates a bene-
fits in the most complex games of size 16 × 16 × 16 but not in smaller games. This suggests that
action-to-action self-attention facilitates learning, especially in tasks that require iterative reasoning.

5.1.2 ESTIMATING MAXIMUM DEVIATION GAINS

A related task is to determine what is the maximum incentive for any player to deviate from a joint
pure-strategy σ (or equivalently, a joint-action). This quantity is informative on the stability of a
joint behaviour — in particular, if a joint pure-strategy has a maximum deviation gain δ(a) of zero,
then by definition we have found a pure-strategy NE. We optimise a NfgTransformer network to
regress towards the maximum deviation-gain δ(a) for every joint pure-strategy a, using a per joint-
action decoder architecture (Figure 2 (Middle)). We report the regression loss in mean squared error
of different architecture variants in Table 1 (Right). We observe that NES consistently outperforms
MLP baselines, but underperforms the NfgTransformer variants as the size of the game increases.

Similar to our observations in Section 5.1.1, the number of transformer blocks played a role in
transformer variants’ final performance. However, it is no longer the main factor.
Instead, the
action-embedding size D becomes critical. Variants with higher embedding size D = 128 can be
competitive, even for shallow models (e.g. K = 4 in 16 × 16 × 16 games). This can be explained by
the lack of structure in the underlying game, as payoff tensors cover the full equilibrium-invariant
subspace of NFGs: payoffs of one joint-action does not provide any information on the outcomes of
another. To perform well, the model must learn to memorise outcomes of different joint-actions a
reduced action embedding size can become a bottleneck.

5.2 PAYOFF PREDICTION IN EMPIRICAL DISC GAMES

What if the game class follows a structured generative process? This is often the case in practice and
a useful representation learning method should capture any such latent structure. We turn to DISC
games (Balduzzi et al., 2019) to evaluate the efficacy of NfgTransformer in this case, compared to
several payoff prediction methods from the ranking literature.
Definition 5.1 (DISC Game). Let ut, vt ∈ RZ, t ∈ [T ], the win-probability to action i when playing
against j is defined as Pij = σ(uT
1+e−x the sigmoid function.

j vi) = 1 − Pji with σ(x) = 1

i vj − uT

Definition 5.1 describes a class of symmetric two-player zero-sum games where the outcomes are
defined by latent vectors ut, vt ∈ RZ, t ∈ [T ], generalising the original definition of DISC game
(Balduzzi et al., 2019) to allow for latent vectors with Z > 1. Payoff values under one joint-action

7

p

=

10.00%

=

Z
36.67%

1

p

=

p

=

63.33%

p

=

90.00%

p

=

10.00%

Z
36.67%

p

=

=

8

p

=

63.33%

p

=

90.00%

)
E
S
M

(

r
o
r
r
E
d
e
r
a
u
q
S
n
a
e
M

0.08

0.06

0.04

0.02

0.00

)
E
S
M

(

r
o
r
r
E
d
e
r
a
u
q
S
n
a
e
M

0.3

0.2

0.1

0.0

l

o
E

l

o
E
m

l

o
E
x

s
r
u
O

l

o
E

l

o
E
m

l

o
E
x

s
r
u
O

l

o
E

l

o
E
m

l

o
E
x

s
r
u
O

l

o
E

l

o
E
m

l

o
E
x

s
r
u
O

l

o
E

l

o
E
m

l

o
E
x

s
r
u
O

l

o
E

l

o
E
m

l

o
E
x

s
r
u
O

l

o
E

l

o
E
m

l

o
E
x

s
r
u
O

l

o
E

l

o
E
m

l

o
E
x

s
r
u
O

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Published as a conference paper at ICLR 2024

Figure 4: Visualisation of attention masks and action-embeddings at inference time on a held-out
instance of Bertrand Oligopoly game whose payoff tensor is shown on the Left and the inferred NE
strategy profile shown on the right (with NE GAP at near zero). The equilibrium pure-strategies for
the two players are shown in red. The sequence of 8 action-to-play attention masks and the PCA-
reduced action-embeddings at the end of each transformer block are shown in the middle.

therefore become informative for predicting the outcomes of others. The amount of information
that can be inferred about one joint-action from knowing another is controlled by the latent vector
dimension Z. The problem setting is as follows. Each algorithm is given access to an empirical
game where outcomes for each joint-action is observed with probability p. The goal is to accurately
predict the outcome to each player under unobserved joint-actions.

A rich body of literature have been dedicated to solving this task owing to its relevance in real-world
In Go and Chess, players are classically assigned Elo ratings which are designed
competitions.
to be predictive of the win-probability between any two ranked players. Several improvements to
Elo have been proposed since, recognising the many limitations of Elo. We compare our results
to methods such as Elo (Elo, 2008), mElo (Balduzzi et al., 2018) and xElo (Bertrand et al., 2023)
across different settings of the DISC game. Figure 3 shows our results in MSE averaged across
unobserved joint-actions. NfgTransformer outperforms all baselines significantly across all settings.
In particular, NfgTransformer recovered the latent variable game (i.e. Z = 1) near perfectly as
soon as 10% of the joint-actions are observed, with an error rate an order of magnitude lower than
the second best method. This result is particularly remarkable as baseline methods are designed
with DISC games in mind, when NfgTransformer is not. At Z = 8, NfgTransformer continues to
outperform, with its prediction accuracy degrading gracefully as fewer joint actions are observed.
Our results suggest that NfgTransformer is highly effectively at recognising and exploiting the latent
structure in games if it exists. We provide details on game sampling, masking, network architecture
and baseline implementation in Appendix B.2. For NfgTransformer, outcomes of the unobserved
joint-actions are masked out following the procedure described in Section 4.3. At training time, the
model minimises reconstruction loss for all joint-actions.

5.3

INTERPRETING NFGTRANSFORMER

NfgTransformer explicitly reasons about action-embedding through structured attention mechanism,
allowing for inspection at each stage of the iterative refinement process. We exploit this property and
investigate how the model implements game-theoretic algorithms such as equilibrium-solving in a
suite of 22 GAMUT games representing diverse strategic interactions (Nudelman et al., 2004; Porter
et al., 2008) that are classically studied in the economics and game theory literature. We optimised an
NfgTransformer network as in Section 5.1, but focused on 10×10 games and removed the action-to-
action self-attention (i.e. A = 0) for a more concise visualisation. Instead of attending to multiple
pieces of information in parallel, each attention layer is implemented by a single attention head

8

0

1

0

1

2

3

4

5

6

7

0

1

2

3

4

5

6

7

0

2

4

6
block 7
8

10

12

14

0

1

2

3

4

5

6

7

Action to plays

BertrandOligopoly (d01) [NashConv = 1.37161e-05]

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0

1

2

3

4

5

6

7

Action to actions

2
1

4

1

0

1

2

1

4

1

0

1

2

1

4

1

0

1

2

1

4

1

0 2 4 6 8

0 2 4 6 8

0 2 4 6 8

0 2 4 6 8

0 2 4 6 8

0 2 4 6 8

0 2 4 6 8

0

1

2

1

4

1

0

1

2

1

4

1

0

1

2

1

4

1

0

1

2

1

4

1

1

2

3

4

5

6

7

Action embeddings (PCA)

Marginals

0

n

o

i

t

c

a

1

n

o

i

t

c

a

0

1

2

3

4

5

6

7

0

1

2

3

4

5

6

7

0
1

0 2 4 6 8

Payoff

0

w

o

r

r

e

y

a

l

p

0

1

2

3

4

5

6

7

0

1

2

5

6

7

0 1 2 3 4 5 6 7
col

payoff

0

1

2

Approx.
NE
0
1
2
3
4
5
6
7
8
9

0
r
e
y
a
p

l

0

1.5

1.0

Payoff

block 0

block 1

block 2

block 3

block 5

block 6

Action to plays

1

3

w
o
block 4
r

4

0
r
e
y
a
p

l

1
r
e
y
a
p

l

0
1
2
3
4
5
6
7
8
9

0
1
2
3
4
5
6
7
8
9

0 1 2 3 4 5 6 7 8 9

payoff

0
r
e
y
a
p

l

1
r
e
y
a
p

l

0
2
4
6
8

0
2
4
6
8

2

0

0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8

block 0

block 1

block 2

block 3

block 4

block 5

Action embeddings (PCA)

0.5

0.0

block 6

block 7

−0.5

−1.0

−2

0

2

action

0

5

0

5

0

5

0

5

0

5

0

5

0

5

0
0

1

5

2

1
r
e
y
4
a
p

l

0
1
2
3
4
5
6
7
8
3
9

6

4

5

6

7

−2

0

2

4

6

−2

0

2

4

6

−2

0

2

4

6

−2

0

2

4

6

−2

0

2

4

6

−2

0

2

4

6

−2

0

2

4

6

−2

0

2

0

1

2

3

4

5

6

7

8

9

0

1

(NE Gap = 4.6e-05)

action

player

Published as a conference paper at ICLR 2024

(H = 1). We show in Appendix C that this simplified architecture is sufficient in approximating NE
in games of this size to reasonable accuracy. We visualise the network on held-out games.

Figure 4 illustrates an instance of iterative NE solving in a game of Bertrand Oligopoly in action.
The NfgTransformer successfully identified a pure-strategy NE (Right) with near zero NE GAP
(Top-Right). Inspecting the payoff tensor (Left), we verify that player 0 playing action 2 and player
1 countering with action 1 is indeed a pure-strategy NE. Is this solution reflected in the underlying
attention mechanism? Indeed, the action-to-plays attention masks (Top) appear to have converged
to a recurring pattern where each action attends to the equilibrium-strategy of its co-player. This
is remarkable for two reasons. First, NE by definition implies indifference across action choices
— when co-players implement the equilibrium strategy, the focal player should be indifferent to its
action choice. Here, this indifference played out. Second, an equilibrium solution should be stable
which appears to be the case over the last few iterations of action-to-plays attentions. Zooming
in on the attention masks at earlier iterations, we see that following zero-initialisation of all action
embeddings, the attention mask for each action is equally spread across all its plays. The attention
masks at the next block, by contrast, appear structured. Indeed, each action attends to the play that
involves the best-response from the co-player. It is worth noting that this pattern of attending to
one’s best-response in NE-solving emerged through learning, without prior knowledge.

The action embeddings themselves also reveal interesting properties of the game, such as sym-
metry (Proposition 3.2). While it might not be immediately obvious that the payoff matrices in
Figure 4 can be made symmetric by permuting actions, the action embeddings (Bottom) show that
action-embeddings across the two players overlap exactly, thanks to the inherent equivariance of
the NfgTransformer. The ordering of players and actions is unimportant — the representation of an
action is entirely driven by its outcomes under the joint-actions. Here, action embeddings revealed
the inherent symmetry of Bertrand Oligopoly (Bertrand, 1883).

For completeness, we offer similar analysis of the NfgTransformer applied to other game classes
in Appendix C, including games without symmetry, with mixed-strategy NE as well as instances
where NfgTransformer failed. Additionally, we show examples of applying NfgTransformer to out-
of-distribution games such as Colonel Blotto (Roberson, 2006) that presents several strategy cycles.

6 RELATED WORKS

Recent works leveraged deep learning techniques to accelerate scientific discoveries in mathematics
(Fawzi et al., 2022), physics (Pfau et al., 2020) and biology (Jumper et al., 2021). Our work is
similarly motivated, but brings deep learning techniques to game theory and economic studies. We
follow a line of works in bringing scalable solutions to game theory (Marris et al., 2022; Duan
et al., 2023; Vadori & Savani, 2023), or integrating components for strategic reasoning as a part of a
machine learning system (Hu & Wellman, 2003; Greenwald et al., 2003; Feng et al., 2021; Liu et al.,
2022b;a). In game theory, Hartford et al. (2016) is the closest to our work and the first to apply deep
representation learning techniques to cognitive modelling of human strategic reasoning using NFGs.
The authors systematically outlined a number of desiderata for representation learning of two-player
NFGs, including player and action equivariance as well as the independence between the number
of parameters of the learned model and the size of the game. The NfgTransformer satisfies these
desiderata but applies to n-player general-sum games. Wiedenbeck & Brinkman (2023) studies
efficient data structures for payoff representation; our approach can be readily integrated into deep
learning systems without any assumption on the games.

7 CONCLUSION

We proposed NfgTransformer as a general-purpose, equivariant architecture that represents NFGs as
action embeddings. We demonstrate its versatility and effectiveness in a number of benchmark tasks
from different sub-fields of game theory literature, including equilibrium solving, deviation gain
estimation and ranking. We report empirical results that significantly improve upon state-of-the-
art baseline methods, using a unified representation learning approach. We show that the resulting
model is also interpretable and parameter-efficient. Our work paves the way for integrating game-
theoretic reasoning into deep learning systems as they are deployed in the real-world.

9

Published as a conference paper at ICLR 2024

ACKNOWLEDGMENTS

We are grateful to Bernardino Romera-Paredes for the productive discussion on the different consid-
erations in designing an equivariant neural architecture, to Wojciech M. Czarnecki for his expertise
in ranking and evaluation in games and to Skanda Koppula for his advice on optimisation techniques
for the transformer architecture.

REFERENCES

Elliot Anshelevich, Aris Filos-Ratsikas, Nisarg Shah, and Alexandros A. Voudouris. Distortion in
In International Joint Conference on
social choice problems: The first 15 years and beyond.
Artificial Intelligence, 2021. URL https://api.semanticscholar.org/CorpusID:
232075911.

David Balduzzi, Karl Tuyls, Julien Perolat, and Thore Graepel. Re-evaluating evaluation. Advances

in Neural Information Processing Systems, 31, 2018.

David Balduzzi, Marta Garnelo, Yoram Bachrach, Wojciech Czarnecki, Julien Perolat, Max Jader-
berg, and Thore Graepel. Open-ended learning in symmetric zero-sum games. In International
Conference on Machine Learning, pp. 434–443. PMLR, 2019.

Joseph Bertrand. Th´eorie math´ematique de la richesse sociale. Journal des savants, 67(1883):

499–508, 1883.

Quentin Bertrand, Wojciech Marian Czarnecki, and Gauthier Gidel. On the limitations of the Elo,
real-world games are transitive, not additive. In International Conference on Artificial Intelligence
and Statistics, pp. 2905–2921. PMLR, 2023.

Xi Chen, Xiaotie Deng, and Shang-Hua Teng. Settling the complexity of computing two-player

Nash equilibria. Journal of the ACM (JACM), 56(3):1–57, 2009.

Constantinos Daskalakis, Paul W Goldberg, and Christos H Papadimitriou. The complexity of

computing a Nash equilibrium. Communications of the ACM, 52(2):89–97, 2009.

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-
reit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recogni-
tion at scale. In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=YicbFdNTTy.

Zhijian Duan, Wenhan Huang, Dinghuai Zhang, Yali Du, Jun Wang, Yaodong Yang, and Xiaotie
Deng. Is Nash equilibrium approximator learnable? In Proceedings of the 2023 International
Conference on Autonomous Agents and Multiagent Systems, pp. 233–241, 2023.

A.E. Elo. The Rating of Chessplayers: Past and Present.

ISBN
9780923891275. URL https://books.google.co.uk/books?id=syjcPQAACAAJ.

Ishi Press International, 2008.

Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Moham-
madamin Barekatain, Alexander Novikov, Francisco J R Ruiz, Julian Schrittwieser, Grzegorz
Swirszcz, et al. Discovering faster matrix multiplication algorithms with reinforcement learning.
Nature, 610(7930):47–53, 2022.

Xidong Feng, Oliver Slumbers, Ziyu Wan, Bo Liu, Stephen McAleer, Ying Wen, Jun Wang, and
Yaodong Yang. Neural auto-curricula in two-player zero-sum games. Advances in Neural Infor-
mation Processing Systems, 34:3504–3517, 2021.

Joaquim Gabarr´o, Alina Garc´ıa, and Maria Serna. The complexity of game isomorphism. Theoreti-

cal Computer Science, 412(48):6675–6695, 2011.

Amy Greenwald, Keith Hall, Roberto Serrano, et al. Correlated Q-learning. In ICML, volume 3, pp.

242–249, 2003.

Jason S Hartford, James R Wright, and Kevin Leyton-Brown. Deep learning for predicting human

strategic behavior. Advances in neural information processing systems, 29, 2016.

10

Published as a conference paper at ICLR 2024

Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):

1735–1780, 1997.

Junling Hu and Michael P Wellman. Nash q-learning for general-sum stochastic games. Journal of

machine learning research, 4(Nov):1039–1069, 2003.

Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira.
Perceiver: General perception with iterative attention. In International conference on machine
learning, pp. 4651–4664. PMLR, 2021.

John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ´ıdek, Anna Potapenko, et al. Highly accurate
protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021.

Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien
P´erolat, David Silver, and Thore Graepel. A unified game-theoretic approach to multiagent rein-
forcement learning. Advances in neural information processing systems, 30, 2017.

Yann LeCun, Yoshua Bengio, et al. Convolutional networks for images, speech, and time series.

The handbook of brain theory and neural networks, 3361(10):1995, 1995.

Michael L Littman et al. Friend-or-foe q-learning in general-sum games. In ICML, volume 1, pp.

322–328, 2001.

Siqi Liu, Marc Lanctot, Luke Marris, and Nicolas Heess. Simplex neural population learning: Any-
mixture bayes-optimality in symmetric zero-sum games. In International Conference on Machine
Learning, pp. 13793–13806. PMLR, 2022a.

Siqi Liu, Luke Marris, Daniel Hennes, Josh Merel, Nicolas Heess, and Thore Graepel. NeuPL:
In International Conference on Learning Representations, 2022b.

Neural population learning.
URL https://openreview.net/forum?id=MIX3fJkl_1.

Luke Marris, Ian Gemp, Thomas Anthony, Andrea Tacchetti, Siqi Liu, and Karl Tuyls. Turbocharg-
ing solution concepts: Solving NEs, CEs and CCEs with neural equilibrium solvers. Advances in
Neural Information Processing Systems, 35:5586–5600, 2022.

Luke Marris, Ian Gemp, and Georgios Piliouras. Equilibrium-invariant embedding, metric space,

and fundamental set of 2 × 2 normal-form games. arXiv preprint arXiv:2304.09978, 2023.

J. C. C. McKinsey. 11. ISOMORPHISM OF GAMES, AND STRATEGIC EQUIVALENCE, pp. 117–
ISBN 9781400881727. doi: doi:10.1515/

130. Princeton University Press, Princeton, 1951.
9781400881727-012. URL https://doi.org/10.1515/9781400881727-012.

John Nash. Non-cooperative games. Annals of Mathematics, 54(2):286–295, 1951.

ISSN

0003486X. URL http://www.jstor.org/stable/1969529.

Eugene Nudelman, Jennifer Wortman, Yoav Shoham, and Kevin Leyton-Brown. Run the gamut:
A comprehensive approach to evaluating game-theoretic algorithms. In AAMAS, volume 4, pp.
880–887, 2004.

David Pfau, James S. Spencer, Alexander G. D. G. Matthews, and W. M. C. Foulkes. Ab initio
solution of the many-electron Schr¨odinger equation with deep neural networks. Phys. Rev. Res.,
2:033429, Sep 2020. doi: 10.1103/PhysRevResearch.2.033429. URL https://link.aps.
org/doi/10.1103/PhysRevResearch.2.033429.

Ryan Porter, Eugene Nudelman, and Yoav Shoham. Simple search methods for finding a Nash

equilibrium. Games and Economic Behavior, 63(2):642–662, 2008.

Brian Roberson. The colonel blotto game. Economic Theory, 29(1):1–24, 2006.

Nelson Vadori and Rahul Savani.

Ordinal potential-based player rating.

arXiv preprint

arXiv:2306.05366, 2023.

11

Published as a conference paper at ICLR 2024

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-
tion processing systems, 30, 2017.

Emmanouil-Vasileios Vlatakis-Gkaragkounis, Lampros Flokas, Thanasis Lianeas, Panayotis Mer-
tikopoulos, and Georgios Piliouras. No-regret learning and mixed nash equilibria: They do not
mix. Advances in Neural Information Processing Systems, 33:1380–1391, 2020.

Bryce Wiedenbeck and Erik Brinkman. Data structures for deviation payoffs.

In Adaptive
Agents and Multi-Agent Systems, 2023. URL https://api.semanticscholar.org/
CorpusID:257220156.

12

Published as a conference paper at ICLR 2024

A PERMUTATION EQUIVARIANT REPRESENTATION OF NFGS

We provide formal statements on identities in action embedding representation when using a de-
terministic, permutation equivariant encoder for NFGs. First, we recall the definition of a strong
isomorphism between two NFGs (McKinsey, 1951; Gabarr´o et al., 2011).
Definition A.1 (Strongly Isomorphic Games). Let G and G′ be two NFGs. G and G′ are
said to be strongly isomorphic and ϕ a strong isomorphism if ϕ = ((τp, p ∈ [N ]), ω) with
τp : ai
p a player action permutation and ω : p → p′ a player permutation such that
G′

p → ai′
(cid:0)τω(1)(aω(1)), . . . , τω(N )(aω(N ))(cid:1) = Gp(a1, ..., aN ), ∀a ∈ A.

ω(p)

To make Definition A.1 concrete, consider the coordination and anti-coordination games shown
in Figure 5. The two games are strongly isomorphic because there exists a strong isomorphism
2 → a1
ϕ = ((τ1 = (a1
2)), ω = (1 → 1, 2 → 2)). As aside,
McKinsey (1951) calls strongly isomorphic games strategically equivalent which we discuss soon.

1), τ2 = (a1

1 → a1

1 → a2

2 → a2

2, a2

1, a2

G a1
2
a1
1, 1
1
a2
0, 0
1

a2
2
0, 0
1, 1

G′
a1
1
a2
1

a1
2
0, 0
1, 1

a2
2
1, 1
0, 0

(a) Coordination game

(b) Anti-coordination game

Figure 5: Strongly isomorphic games.

We additionally note that in the special case when G is G′, ϕ is referred to as a strong automorphism.
Definition A.2 (Strongly Automorphic Game). G is said to be strongly automorphic and ϕ a strong
automorphism if ϕ = ((τp, p ∈ [N ]), ω) with τp : ai
p a player action permutation and ω : p →
(cid:0)τω(1)(aω(1)), . . . , τω(N )(aω(N ))(cid:1) = Gp(a1, ..., aN ), ∀a ∈
p′ a player permutation such that Gω(p)
A.

p → ai′

For instance, the coordination game (Figure 5a) is also strongly automorphic as there exists three
automorphisms that recover the same game.

ϕ = ((τ1 = (a1
ϕ = ((τ1 = (a1
ϕ = ((τ1 = (a1

1 → a2
1 → a2
1 → a1

1, a2
1, a2
1, a2

1 → a1
1 → a1
1 → a2

1), τ2 = (a1
1), τ2 = (a1
1), τ2 = (a1

2 → a2
2 → a2
2 → a1

2, a2
2, a2
2, a2

2 → a1
2 → a1
2 → a2

2)), ω = (1 → 1, 2 → 2))
2)), ω = (1 → 2, 2 → 1))
2)), ω = (1 → 2, 2 → 1))

(5a)

(5b)

(5c)

Note that ϕ is a permutation over all the players’ actions which is a composition of the player and
action permutations ϕ = τ1 ·...·τN ·ω. Therefore ϕ is not a general permutation, but a structured one.
We use a convention that the player permutation is applied last. Finally, we recall any permutation π
can be written uniquely as m permutation orbits with π = C 1, . . . , C m, each operating on a disjoint
(possibly singleton) subset of elements that π operates over. Therefore ϕ is also a collection of
permutation orbits.

Considering the coordination game again, the automorphisms (Equations 5a-5c) can be written as
permutations which each consists of two orbits containing two actions each.

1 a2
a1

1 a1

2 a2
2

(6a)

1 a2
a1

1 a1

2 a2
2

(6b)

1 a2
a1

1 a1

2 a2
2

(6c)

Definition A.3 (Strategically Equivalent Actions). Two actions ai
if there exists a strong automorphism, ϕ which contains ai
actions ai
(..., i → j, ...)), ω = (..., p → q)).

q are strategically equivalent
q in an orbit. Equivalently, two
q are strategically equivalent if there exists a strong automorphism, ϕ = ((..., τp =

p and aj

p and aj

p and aj

1, a2

Again, consider the running example of the coordination game. From Equation 6a we can see that
(a1
1), and (a1
2) are each strategically equivalent pairs. Furthermore, from Equation 6b we
can see that (a1
2), and (a2
2) are also each strategically equivalent pairs. Therefore in the
coordination games all the actions are strategically equivalent to each other.

2, a2
1, a2

1, a1

13

Published as a conference paper at ICLR 2024

Figure 6: We compare NfgTransformer to baseline architectures in synthetic games. Results from
baseline experiments are shown in dashed lines. Each configuration is averaged across 5 independent
runs with shaded areas representing the confidence intervals. For NfgTransformer variants, we
annotate each variant with corresponding hyper-parameters (K, A and D as shown in Figure 1).

Theorem A.4. If an embedding function, f , is deterministic and equivariant, then strategically
equivalent actions, ai

q, must have the same embeddings.

p and aj

Proof. The embedding function, f , is deterministic and equivariant over players and actions. Addi-
tionally, if ϕ is an automorphism of G, then f (G) = f (ϕ(G)) = ϕ(f (G)). Therefore the embed-
dings are also equal, ai

p = aj
q.

Proposition A.5 (Repeated Actions). If G(ai
equivariant with f (G) = (. . . , (. . . , ai

p, a¬p) = G(aj

p, a¬p), ∀a¬p and f is deterministic and

p, . . . , aj

p, . . . ), . . . ) then it follows that ai

p = aj
p.

Proof. If actions are repeated, there there exists an automorphism ϕ = ((..., τp = (..., i →
j, ...), ...), ω = identity). Therefore ai
q are strategically equivalent and have the same
p = aj
embeddings, ai
p.

p and aj

Proposition A.6 (Player Symmetry). If player p and q are symmetric, f is deterministic and equiv-
ariant with f (G) = (. . . , Ap, . . . , Aq, . . . ), then Ap and Aq are identical up to permutation.

Proof. If the game is symmetric between p and q, there there exists an automorphism ϕ =
((..., τp, ...), ω = (..., p → q, ...)). Therefore τp(ai
q are strategically equivalent for all i,
and have the same embeddings, τp(Ap) = Aq.

p) and ai

B EXPERIMENTAL SETUP

B.1 SUPERVISED LEARNING IN SYNTHETIC GAMES

Games Sampling Games are sampled from the equilibrium-invariant subspace (Marris et al.,
T N ) Frobe-
2023; 2022), with zero-mean payoff over other player actions and a unit variance (
nius tensor norm over player payoffs. To sample uniformly over such a set, first sample a game from

√

14

[16,

16]

[64,

64]

[8,

8,

8]

NE

0.3

0.2

0.1

0.2

0.1

[16,

16,

16]

0.15

0.10

0.05

100,000

200,000

0

100,000

200,000

0

100,000

200,000

0

100,000

200,000

gradient

steps

gradient

steps

gradient

steps

gradient

steps

[16,

16]

Max

Deviation

Gain

[64,

64]

[8,

8,

8]

[16,

16,

16]

0.8

0.6

0.4

0.2

0.0

0.8

0.6

0.4

0.2

0.0

0.8

0.6

0.4

100,000

200,000

0

100,000

200,000

0

100,000

200,000

0

100,000

200,000

gradient

steps

gradient

steps

gradient

steps

gradient

steps

0.4

0.2

p
a
G
E
N

0.0

0

)

E
S
M

(

r
o
r
r
E
d
e
r
a
u
q
S
n
a
e
M

0.4

0.2

0.0

0

Model

MLP
NfgTransformer(D=32,
NfgTransformer(D=64,

K=4,
K=8,

A=1)
A=2)

EquivariantMLP
NfgTransformer(D=32,
NfgTransformer(D=128,

K=8,

A=1)

K=2,

A=1)

NES
NfgTransformer(D=64,
NfgTransformer(D=128,

K=8,

A=0)

K=4,

A=1)

NfgTransformer(D=32,
NfgTransformer(D=64,
NfgTransformer(D=128,

K=2,
K=8,

A=1)
A=1)

K=8,

A=1)

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Published as a conference paper at ICLR 2024

a unit normal distribution, Gp ∼ N (0, 1), and then normalize.

Gequil
p

(a) =

√

T N
Z



Gp(a) −

1
T

(cid:88)

ap



Gp(ap, a−p)

 Z =

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Gp −

1
T

(cid:88)

ap

Gp(ap, a−p)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

(7)

The benefit of this distribution is that it provides a way to uniformly sample over the space of all
possible strategic interactions in a NFG of a specific shape. The equilibrium-invariant subspace has
lower degree of freedom than a full NFG, freeing the neural network from having to learn offset and
scale invariance. Any game can be simply mapped to the equilibrium-invariant subspace without
changing its set of equilibria.

Architecture We provide additional technical details on the network architectures presented in
Section 5.1. The baseline MLP networks are composed of 5 fully-connected layers with 1,024
hidden units each. The baseline NES architecture(Marris et al., 2022) consisted of 4 “payoff to
payoff” layers with 128 channels, a “payoff to dual” layer with 256 channels and 4 “dual to dual”
layers with 256 channels. Each layer uses mean and max pooling functions. All NfgTransformer
model variants have H = 8 attention heads. Parameter counts of all model variants are reported in
Table 2.

Table 2: The number of network parameters by configuration for each task. We note that the number
of parameters of the NfgTransformer and the NES is independent from the size of the games. This
is in contrast to fully-connected networks whose parameter counts depend on the input sizes.

Model
NfgTransformer(D= 32,K=2,A=1)
NfgTransformer(D= 32,K=4,A=1)
NfgTransformer(D= 32,K=8,A=1)
NfgTransformer(D= 64,K=8,A=0)
NfgTransformer(D=128,K=2,A=1)
NfgTransformer(D= 64,K=8,A=1)
NfgTransformer(D= 64,K=8,A=2)
NfgTransformer(D=128,K=4,A=1)
NfgTransformer(D=128,K=8,A=1)
EquivariantMLP
MLP
NES

# Parameter
(NE)
0.15M
0.31M
0.61M
1.10M
1.22M
1.63M
2.16M
2.44M
4.88M
4.76M - 16.83M
4.76M - 16.83M
2.25M

# Parameter
(Max-Deviation-Gain)
0.16M
0.31M
0.62M
1.11M
1.29M
1.64M
2.17M
2.51M
4.95M
4.99M - 20.98M
4.99M - 20.98M
2.51M

Convergence progression Figure 6 visualises the training progression of each model configura-
tion, task and game size from the same experiments reported in Table 1.

B.2 PAYOFF PREDICTION IN DISC GAMES

Game Sampling Following Definition 5.1, generating DISC games amounts to sampling latent
vectors ut, vt ∈ RZ, t ∈ [T ]. Any real-valued latent vectors would define a valid DISC game and
we let ut = n + u with n ∼ N (0, 1) and u ∼ U(−1, 1). We sample vt in the same way. The
shift random variable u is not strictly necessary in this case, but it increases the probability that the
resulting DISC game is not fully cyclic following Proposition 1 of Bertrand et al. (2023).

Masking For each sampled instance of the DISC game, with a payoff tensor of shape
[N, T, . . . , T ], we additionally sample a binary mask of shape [T, . . . , T ] where each element fol-
lows Bernoulli(p). Both the game payoff tensor and the sampled mask for the game tensor are
provided as inputs to the NfgTransformer network. We ensure that the model does not observe the
payoff values of masked joint-actions following Equation 4. During loss computation, we minimise
the L2 loss (Figure 2 (Right)) over all joint-actions, observed (i.e. for reconstruction) or unobserved
(i.e. for prediction).

Architecture For all results in this section, we used NfgTransformer(K=8,A=1,D=64)
with H = 8 attention heads for all attention operations.

15

Published as a conference paper at ICLR 2024

Figure 7: NE GAP reported for each of the 22 GAMUT games throughout training. We note that
a single network, NfgTransformer(K=8,A=0,D=16), with H = 1 is optimised to solve for
Nash Equilibrium across all game classes.

Baseline Solvers For all baseline results, we used the open-source implementation of Elo, mElo
and xElo of released at https://github.com/QB3/discrating. For mElo and xElo, we
used n components = 3 and the same settings as reported in Bertrand et al. (2023).

C INTERPRETABILITY RESULTS

We provide additional details on the empirical results in Section 5.3. Figure 7 shows that despite sim-
plifications made in Section 5.3 for our interpretability results, the NfgTransformer remains capable
of equilibrium-solving in most games to reasonable accuracy, with CovariantGame (d06) the
most challenging game class. We show a failure case in this game class in Figure 8 (Middle) and
present additional example instances where the model successfully solved for a mixed-strategy NE
(Top) or generalised to the out-of-distribution game class of Blotto (Roberson, 2006). Please refer
to figure caption for additional remarks on the results.

D THE SPACE OF 2×2 GAMES

Marris et al. (2023) introduced a subset of 2×2 normal-form games that any 2×2 game can be mapped
to without changing its set of (coarse) correlated equilibria and Nash equilibria. This subset of games
is called the equilibrium-invariant subset, and includes all possible nontrivial strategic interactions of
2×2 games. Properties of games such as their equilibria, permutation symmetries, and best-response
dynamics can be visualized in this “map of games”. We can analyse the embeddings found by the
NfgTransformer by sweeping over the nontrivial 2×2 equilibrium-invariant subset.

We used the transformer architecture NfgTransformer(K=2,A=1,D=16) with H = 2 atten-
tion heads at every self- and cross-attention layer. We used an additional linear layer to reduce the
action embedding dimension down to 1, per player, per action, resulting in four variables to de-
scribe the game embeddings. We trained NfgTransformer with an NE objective over the space of
equilibrium-invariant subsets, and verified that the loss approaches zero. With the trained NfgTrans-
former, we sweep over the nontrivial 2×2 equilibrium-invariant subset, and visualize the embeddings
(Figure 9).

The learned action-embeddings have a very low value (blue regions) when that action has all the
mass in the NE, and very high value when the action has no mass in the NE (dark red regions).
These “L” regions therefore correspond to games which have a single pure NE. The embeddings
are low value (cyan regions) when the game has a mixed NE solution and occurs near the cyclic
). In these regions, all embeddings have to be similarly colored, as all actions
games (
are mixed. The embeddings are high value (red regions) in coordination game areas where there are
two disconnected NEs (
). The borders between these regions correspond to changes in
game payoffs when one action becomes become profitable than another, and as a result the NE can
change drastically, and therefore so does the embedding.

and

and

16

p
a
G
E
N

0.25

0.20

0.15

0.10

0.05

0.00

0

40,000

80,000

gradient

120,000
steps

160,000 200,000

game

BertrandOligopoly

(d01)

GraphicalGame-SW

(d12)

BidirectionalLEG-CG

(d02)

MinimumEffortGame

(d13)

BidirectionalLEG-RG

(d03)

PolymatrixGame-CG

(d14)

BidirectionalLEG-SG

(d04)

PolymatrixGame-RG

(d15)

CovariantGame

(d06)

PolymatrixGame-Road

(d16)

CovariantGame-Pos

(d05)

PolymatrixGame-SW

(d17)

CovariantGame-Zero

(d07)

RandomGame

(d18)

DispersionGame

(d08)

TravelersDilemma

(d19)

GraphicalGame-RG

(d09)

UniformLEG-CG

(d20)

GraphicalGame-Road

(d10)

UniformLEG-RG

(d21)

GraphicalGame-SG

(d11)

UniformLEG-SG

(d22)

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Published as a conference paper at ICLR 2024

Figure 8: Here we provide additional interpretability results similar to Figure 4 but for games that
are asymmetric, have mixed-strategy NE (Top) or out-of-distribution (Bottom). We additionally
provide an instance where the model struggled to find an NE (Middle) where the attention masks
did not appear to have converged. For Blotto (Roberson, 2006) which is a game class not seen during
training, the model generalised well and identified a pure-strategy NE. The action embeddings also
revealed three clusters, corresponding to the three strategically equivalent classes of actions. Note
that one of the clusters corresponds to the dominant action of the two players.

17

Payoff

block 0

block 1

0
r
e
y
a
p

l

1
r
e
y
a
p

l

0
1
2
3
4
5
6
7
8
9

0
1
2
3
4
5
6
7
8
9

2

1

0

−1

−2

0 1 2 3 4 5 6 7 8 9

payoff

0

1

2

3

4

5

6

7

0

1

2

3

4

5

6

7

0

2

4

6

8

10

12

14

0

1

Action to plays

block 3

Payoff

0

1

2

3

0

w
o
4
r
block 2
5

r
e
y
a
l
p

1

w
o
r

6

7

0

1

2

3

4

5

6

7

0 1 2 3 4 5 6 7
col

payoff

0

1

2

3

4

5

6

7

Action to plays

BertrandOligopoly (d01) [NashConv = 1.37161e-05]

block 4

block 5

block 6

block 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

GraphicalGame-SG (d11) [NE Gap = 4.95456e-03]

Approx. NE

Action to actions
0

0

1

2

3

0
r
e
y
a
p

l

0 2 4 6 8

0
1

2
1

4
1

0 2 4 6 8

0
1

2
1

4
1

0 2 4 6 8

0
1

2
1

4
1

0 2 4 6 8

1

2

3

4

5

6

7
0
1
8

2
1

4
1

0 2 4 6 8

0 2 4 6 8

0 2 4 6 8

0 2 4 6 8

0

1

2

1

4

1

0

1

2

1

4

1

0

1

2

1

4

1

0

1

2

1

4

1

5

5

6

6

7

7

4

4

4

4

4

4

5

6

7

BertrandOligopoly (d01) [NashConv = 1.37161e-05]

5

6

7

4

5

6

7

Marginals

0

n

o

i

t

c

a

1

n

o

i

t

c

a

Marginals

0

n

o

i

t

c

a

1

n

o

i

t

c

a

Marginals

0

n

o

i

t

c

a

1

n

o

i

t

c

a

0

1

2

3

4

5

6

7

0

1

2

3

4

5

6

7

0

1

2

3

4

5

6

7

0

1

2

3

4

5

6

7

0

1

2

3

4

5

6

7

0

1

2

3

4

5

6

7

012 3 456 7 89

012 34567 89

0123 4567 89

0

1
0123 4567 89

2

1.5

0

1

2

0123 4567 89

012 3456 789

012 3456 789

0123 4567 89

Action embeddings (PCA)
1.0

Action embeddings (PCA)

9

3

0

Action to plays

1

5

6

7

BertrandOligopoly (d01) [NashConv = 1.37161e-05]

block 0

block 1

block 2

block 3

0

5

0

5

0

5

0

5

−2

−1

0

1

2

0

1

2

3

4

5

6

7

8

9

action

Payoff

block 0

block 1

Action to plays

block 3

0.5

0.0

−0.5

−1.0

0

−2

0

action

0

1

block 4

0

block 5

1

block 6

2

block 7

2

4

6

−2

0

2

4

6

−2

0

2

4

6

−2

0

2

1

2

5

3

4

0

5

6

5

7

0

5

0

5

3

1
r
e
y
a
p

l

2

3

4

5

6
4
7

8

9

6

−2

0

2

4

6

−2

0

2

4

6

−2

0

2

4

6

−2

0

2

4

6

block 4

block 5

block 6

block 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

CovariantGame (d06) [NE Gap = 2.47795e-01]

0
r
e
y
a
p

l

1
r
e
y
a
p

l

0
1
2
3
4
5
6
7
8
9

0
1
2
3
4
5
6
7
8
9

1

0

Approx. NE

Action to actions
0

0

1

2

3

0
r
e
y
a
p

l

0 2 4 6 8

0
1

2
1

4
1

0 2 4 6 8

0
1

2
1

4
1

0 2 4 6 8

0
1

2
1

4
1

0 2 4 6 8

1

2

3

4

5

6

7
0
1
8

2
1

4
1

0 2 4 6 8

0 2 4 6 8

0 2 4 6 8

0 2 4 6 8

0

1

2

1

4

1

0

1

2

1

4

1

0

1

2

1

4

1

0

1

2

1

4

1

0

1

2

3

5

6

7

0123 4567 89

012 3456 789

012 3456 789

0123 4567 89

10

12

14

0 1 2 3 4 5 6 7
col

payoff

Action embeddings (PCA)

9

0
Action to plays
1

012 3 456 7 89

012 34567 89

0123 4567 89

0

1
0123 4567 89

2

1.5

Action embeddings (PCA)
1.0

block 0

block 1

block 2

block 3

0.5

0.0

−0.5

−1.0

0

−2

0

action

0

5

0

5

0

5

0

5

0

0 1 2 3 4 5 6 7 8 9

payoff

−1

action

−2

−1

0

1

2

0

1

2

3

4

5

6

7

8

9

Payoff

block 0

block 1

block 4

0

block 5

1

block 6

2

block 7

2

4

6

−2

0

2

4

6

−2

0

2

4

6

−2

0

2

1
0

2

5

3

4

5
0

6

5

7

0

5

0

5

3

1
r
e
y
a
p

l

2

3

4

5

6
4
7

8

9

6

−2

0

2

4

6

−2

0

2

4

6

−2

0

2

4

6

−2

0

2

4

6

block 4

block 5

block 6

block 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

Blotto [NE Gap = 3.48249e-05]

0
r
e
y
a
p

l

1
r
e
y
a
p

l

0
1
2
3
4
5
6
7
8
9

0
1
2
3
4
5
6
7
8
9

3

2

1

0

0 1 2 3 4 5 6 7 8 9

−1

payoff

−1

0

1

2

Approx. NE

Action to actions
0

0

1

2

3

0
r
e
y
a
p

l

0 2 4 6 8

0
1

2
1

4
1

0 2 4 6 8

0
1

2
1

4
1

0 2 4 6 8

0
1

2
1

4
1

0 2 4 6 8

1

2

3

4

5

6

7
0
1
8

2
1

4
1

0 2 4 6 8

0 2 4 6 8

0 2 4 6 8

0 2 4 6 8

0

1

2

1

4

1

0

1

2

1

4

1

0

1

2

1

4

1

0

1

2

1

4

1

0

1

2

3

4

5

6

7

01 2345 6789

012 3456 789

012 3456 789

0123 4567 89

Action embeddings (PCA)

9

10

12

14

0 1 2 3 4 5 6 7
col

payoff

0 1 234 5 678 9

0 1 2345 6789

01 2345 6789

0

1
01 2345 6789

2

1.5

Action embeddings (PCA)
1.0

block 0

block 1

block 2

block 3

0.5

0.0

−0.5

−1.0

block 4

block 5

block 6

block 7

0

5

0

5

0

5

0

5

0

1
0

2

5

3

4

5
0

6

5

7

0

5

0

5

−2

0

2

4

6

−2

0

2

4

6

−2

0

2

4

6

−2

0

2

action

action

player

0

1

2

3

4

5

6

7

8

9

0

1

1
r
e
y
a
p

l

0

1

2

3

4

5

6
4
7

8

9

6

−2

0

2

4

6

−2

0

2

4

6

−2

0

2

4

6

−2

0

2

4

6

player

0
Payoff

1

0

1

2

1

Action to plays

block 3

3

0

w
o
4
r
block 2
5

r
e
y
a
l
p

1

w
o
r

6

7

0

1

2

3

4

5

6

7

player

0
Payoff

1

0

1

2

3

0

w
o
4
r
block 2
5

r
e
y
a
l
p

1

w
o
r

6

7

0

1

2

3

4

5

6

7

0

1

2

3

4

5

6

7
0

0

1

2

3

4

5

6

7

0

2

4

6

8

0

1

2

3

4

5

6

7

0

1

2

3

4

5

6

7

0

2

4

6

8

0
r
e
y
a
p

l

1
r
e
y
a
p

l

0
r
e
y
a
p

l

1
r
e
y
a
p

l

0
r
e
y
a
p

l

1
r
e
y
a
p

l

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

0

1

2

3

4

5

6

7

8

9

Published as a conference paper at ICLR 2024

(a) a1
1

(b) a2
1

(c) a1
2

(d) a2
2

Figure 9: NfgTransformer action-embeddings over the set of nontrivial 2×2 equilibrium-invariant
normal-form games, when trained with an NE objective. The embeddings found closely follow the
equilibrium boundaries (dark lines). Symmetries over the space of games are respected. Symmetric
games (bottom-left to top-right diagonal) have the same embeddings between players. Permutations
over players (folding over the bottom-left to top-right diagonal) are consistent. Colorbar: [−11.58

+11.56].

18

Published as a conference paper at ICLR 2024

a1
2
1, 1
0, 0

a2
2
0, 0
1, 1

(b) Coord
a1
2
0, 0
1, 1

a2
2
1, 1
0, 0

a1
1
a2
1

a1
1
a2
1

a1
2
1, 0
0, 1

a2
2
0, 1
1, 0

(c) Cycle
a1
2
0, 1
1, 0

a2
2
1, 0
0, 1

a1
1
a2
1

a1
1
a2
1

(d) Anti Coord

(e) Anti Cycle

A
a1
a2

a1
9.415
9.415

a2
9.415
9.415

A
a1
a2

a1
-2.497
-2.497

a2
-2.497
-2.497

(f) Coord Embedding

(g) Cycle Embedding

(a) ln(dmin(Ai, G))

Figure 10: Subfigure 10a shows the distance to the nearest other game embedding. The embed-
dings produced by NfgTransformer uniquely describe the 2×2 game apart from two edge cases.
and
Two Coordination games (
) have identical embeddings, each because there are strategically equivalent. Colorbar: [0.0

) have identical embeddings, and two Cycle games (

and

4.675].

When a game is symmetric, G1(a1, a2) = G2(a2, a1), the embeddings between players are equal.
We can verify this by studying the bottom-left to top-right diagonal. When swapping the player
orders, we expect the embeddings to be swapped. Swapping players is equivalent to folding over the
same diagonal. Again, we can visually verify that the embeddings are swapped.

(cid:80)

p∈[1,2]

ap∈[1,2](Ai

Next, we turn to the question of when the embeddings uniquely describe a game. We de-
fine a distance metric between action embeddings for game i and game j, d(Ai, Aj) =
((cid:80)
2 , where i, j ∈ G are games sampled from a grid, which
describes how close the embeddings of two games are to each other. We can also define the distance
to the nearest other game within the set of considered games, dmin(Ai, G) = minj̸=i∈G d(Ai, Aj).
Using these distance metrics we can verify that dmin(Ai, Aj) > 0 apart from games with strategi-
cally equivalent actions (Figure 10).

p(ap) − Aj

p(ap))2) 1

Coordination game has identical action embeddings to the

The
Anti-Coordination game. In
this case, due to permutation equivariance, the embedding for each action, in natural language, is:
“there is an action that the opponent can play which will give us both identical high payoff, and there
is an action that the opponent can play which will give us both identical low payoff”. Due to the
equivariant property it is not possible to disambiguate between these games from the embeddings
alone. By initializing the network with action labels, hinting a reconstruction method with a row
of true payoffs, or permuting the payoffs by a tiny amount, would all enable disambiguation. The
last strategy can be seen from the figure where slightly biased coordination games all have positive
distance to their nearest other game embedding. Similarly, the
Cycle game (also known as
matching pennies) has identical embeddings to the
Anticlockwise Cycle game. In this case, the
embedding is “there is an action that the opponent can play which will me a high positive payoff and
the opponent a high negative payoff, and there is an action that the opponent can play which will
give me a high negative payoff and the opponent a high positive payoff”. Note that the Coordination
and Cycle game have distinct embeddings. These are the only 4 points in the space that can only
by disambiguated up to handedness. These appear with measure zero in the equilibrium-invariant
subspace.

19

Published as a conference paper at ICLR 2024

Overall, the embeddings neatly describe and predict the known structure of 2×2 games. The the-
oretically predicted properties, including permutation symmetries and NE, are reproduced in this
experiment.

20

","4 2 0 2 b e F 3 1 ] T G . s c [ 1 v 3 9 3 8 0 . 2 0 4 2 : v i X r a Published as a conference paper at ICLR 2024 NFGTRANSFORMER : EQUIVARIANT REPRESENTATION LEARNING FOR NORMAL-FORM GAMES Siqi Liu† , ♢ , Luke Marris† , ♢ , Georgios Piliouras† , Ian Gemp† , Nicolas Heess† †Google DeepMind , ♢University College London { liusiqi , marris , gpil , imgemp , heess } @ google.com ABSTRACT Normal-form games ( NFGs ) are the fundamental model of strategic interaction . We study their representation using neural networks . We describe the inherent equivariance of NFGs — any permutation of strategies describes an equivalent game — as well as the challenges this poses for representation learning . We then propose the NfgTransformer1 architecture that leverages this equivariance , lead- ing to state-of-the-art performance in a range of game-theoretic tasks including equilibrium-solving , deviation gain estimation and ranking , with a common ap- proach to NFG representation . We show that the resulting model is interpretable and versatile , paving the way towards deep learning systems capable of game- theoretic reasoning when interacting with humans and with each other . 1 INTRODUCTION Representing data in a learned embedding space , or representation learning , is one of the time- less ideas of deep learning . Foundational representation learning architectures ( LeCun et al. , 1995 ; Hochreiter & Schmidhuber , 1997 ; Vaswani et al. , 2017 ) have provided performance and general- ity , bringing together researchers who used to work on expert systems targeting narrow domains . Consider Convolutional Neural Networks ( CNNs ) : by exploiting the translation invariance inherent to images , CNNs replaced feature descriptors underpinning tasks as diverse as image classification , segmentation and in-painting , leading to paradigm shifts across the field . One area of research that has resisted this trend is game theory . Here , we see two striking similar- ities to classical computer vision research . First , active research topics such as equilibrium solving ( Vlatakis-Gkaragkounis et al. , 2020 ) , ranking ( Balduzzi et al. , 2018 ) , social choice theory ( Anshele- vich et al. , 2021 ) and population learning ( Lanctot et al. , 2017 ) all focus on specialised solutions , despite sharing the common language of normal-form games ( NFGs ) . Second , task-specific so- lutions suffer from fundamental limitations . The popular Elo ranking algorithm ( Elo , 2008 ) , for instance , assigns a scalar rating to each player derived from an NFG between players . Although Elo ratings are designed to be predictive of match outcomes , they are poor predictors beyond transitive games — the Elo score is simply too restrictive a representation to reflect cyclic game dynamics . Im- provements to Elo followed ( Bertrand et al. , 2023 ) , but all relied on engineered feature descriptors , instead of learning . In equilibrium-solving , computing exact Nash equilibria is intractable beyond two-player zero-sum games ( Daskalakis et al. , 2009 ) yet approximate solvers are non-differentiable , take non-deterministic amount of time to converge , struggle to parallelise , and can fail . These fun- damental limitations have indirect consequences too . An entire line of works in tabular multiagent reinforcement learning ( RL ) ( Littman et al. , 2001 ; Hu & Wellman , 2003 ; Greenwald et al. , 2003 ) relied on equilibrium solving as part of their learning rules — an NFG is constructed from agents ’ Q-tables in each state , whose equilibrium informs subsequent policy updates . Unfortunately , re- viving these ideas in the context of deep RL has been challenging , if not impossible , as it requires equilibrium solving as a subroutine in between every gradient update . Indeed , we are not the first to recognise these limitations . Several recent works incorporated repre- sentation learning implicitly , in narrow domains ( Marris et al. , 2022 ; Duan et al. , 2023 ; Vadori & Savani , 2023 ) . We address these limitations explicitly and in generality . Our goal is to develop prin- cipled , general-purpose representation of NFGs that can be used in a wide range of game-theoretic 1The model is open-sourced at https : . 1 Published as a conference paper at ICLR 2024 applications . We ask 1 ) which property , if any , could we leverage for efficient representation learning of NFGs without loss of generality and 2 ) can we expect performance in a range of game-theoretic tasks using a common approach to representing NFGs . We answer both questions affirmatively and propose NfgTransformer , a general-purpose representation learning architecture with state-of-the-art performance in tasks as diverse as equilibrium solving , deviation gain estimation and ranking . In its most basic form , strategic interactions between players are formulated as NFGs where players simultaneously select actions and receive payoffs subject to the joint action . Strategic interactions are therefore presented as payoff tensors , with values to each player tabulated under every joint ac- tion . This tabular view of strategic interactions presents its own challenges to representation learn- ing . Unlike modalities such as images and text whose spatial structure can be exploited for efficient representation learning ( LeCun et al. , 1995 ; Hochreiter & Schmidhuber , 1997 ) , the position of an action in the payoff tensor is unimportant : permuting the payoff matrix of any NFG yields an equiva- lent game — an equivalency known as strongly isomorphic games ( McKinsey , 1951 ; Gabarr´o et al. , 2011 ) . This inherent equivariance to NFGs has inspired prior works to compose order-invariant pooling functions in the neural network architecture for efficiency , albeit at the expense of the gen- erality of the representation ( Feng et al. , 2021 ; Marris et al. , 2022 ) . We aim to leverage this inherent equivariance of NFGs while preserving full generality of the learned representation . This implies several desiderata that we discuss in turn . First , the representation needs to be versatile , allowing for inquiries at the level of individual actions , joint-actions , per-player or for the entire game . Second , it needs to be equivariant : for any per-action inquiry , the outputs for two actions should be exchanged if their positions are exchanged in the payoff tensor . Third , the embedding function should not assume that outcomes of all joint-actions are observed — the representation should accommodate incomplete NFGs in a principled way . Fourth , the function should apply to games of different sizes . This implies that the number of network parameters should be independent from the size of the games ( Hartford et al. , 2016 ) . Finally , it would be desirable if the network architecture is interpretable , allowing for inspection at different stages of the network . In the rest of this paper , we show how our proposed encoder architecture , NfgTransformer , satisfies all these desiderata simultaneously . The key idea behind our approach is to consider the embedding function class that represents an NFG as action embeddings , reversing the generative process from actions to payoffs . Action embeddings can be suitably composed to answer questions at different granularities and allows for equivariance in a straightforward way — permutations of actions or players in the payoff tensor shall be reflected as a permutation in the action embeddings . We ar- gue that NfgTransformer is a competitive candidate for general-purpose equivariant representation learning for NFGs , bridging the gap between deep learning and game-theoretic reasoning . 2 BACKGROUND Normal-form Games NFGs are the fundamental game formalism where each player p simulta- p } = Ap and receives a payoff Gp : A → R p , . . . , aT neously plays one of its T actions ap ∈ { a1 as a function of the joint action a = ( a1 , . . . , aN ) ∈ A of all N players . Let a = ( ap , a¬p ) with a¬p = ( . . . , ap−1 , ap+1 , . . . ) ∈ A¬p the actions of all players except p. Let σ ( a ) = σ ( ap , a¬p ) denote the probability of players playing the joint action a and σ a probability distribution over the space of joint actions A . A pure strategy is an action distribution that is deterministic when a mixed- strategy can be stochastic . The value to player p under σ is given as Ea∼σ [ Gp ( ap , a¬p ) ] . We refer to the payoff tensor tabulated according to the action and player ordering above as G. Nash Equilibrium ( NE ) Under a mixed joint strategy that factorises into an outer product of marginals σ = ( cid:78 ) p σp , player p ’ s unilateral deviation incentive is defined as E δp ( σ ) = max p∈Ap a∼σ A factorisable mixed joint strategy σ = ( cid:78 ) p σp is an ϵ-NE if and only if δ ( σ ) = maxp δp ( σ ) ≤ ϵ . We refer to this quantity as the NE GAP as it intuitively measures the distance from σ to an NE of the game . A mixed-strategy NE is guaranteed to exist for a finite game ( Nash , 1951 ) but exactly computing a normal-form NE beyond two-player zero-sum is PPAD-complete ( Chen et al. , 2009 ; Daskalakis et al. , 2009 ) . If σ is deterministic with σ ( a ) = 1 , then δ ( σ ) or equivalently δ ( a ) defines the maximum deviation gain of the joint pure-strategy a. a is a pure-strategy NE when δ ( a ) = 0. p , a¬p ) − Gp ( a ) ] . [ Gp ( a′ ( 1 ) a′ 2 Published as a conference paper at ICLR 2024 Permutation Equivariance Consider a strong isomorphism ϕ : G → G′ of NFGs ( Gabarr´o et al. , 2011 ) with ϕ = ( ( τp , p ∈ [ N ] ) , ω ) , τp : ai p a player action permutation and ω : p → p′ a player permutation . Elements of the transformed game G′ = ϕ ( G ) are therefore given as p → ai′ G′ ω ( p ) ( cid:0 ) τω ( 1 ) ( aω ( 1 ) ) , . . . , τω ( N ) ( aω ( N ) ) ( cid:1 ) = Gp ( a1 , ... , aN ) . An encoder f : G → ( A1 , . . . , AN ) , with Ap = ( a1 is said to be equivariant if p , . . . , aT p ) the action embeddings for player p , f ( ϕ ( G ) ) = ( A′ 1 , . . . , A′ N ) with A′ ω ( p ) = ( τp ( a1 p ) , . . . , τp ( aT p ) ) ( 2 ) Here we slightly abuse the notation of τp to operate over action embeddings . Intuitively , permutation equivariance implies that ϕ and f commute , or f ( ϕ ( G ) ) = ϕ ( f ( G ) ) . We adopt the convention that the player permutation ω is applied after player action permutations τp , ∀p . Multi-Head Attention We describe self- and cross-attention QKV mechanisms that have become ubiquitous thanks to their generality and potential to scaling ( Vaswani et al. , 2017 ; Dosovitskiy et al. , 2021 ; Jaegle et al. , 2021 ) . Both operations extend the basic QKV attention mechanism as follows : Attention ( Q , K , V ) = softmax ( cid:18 ) QK T √ dk ( cid:19 ) V ( 3 ) with Q ∈ Rnq×dk , K ∈ Rnk×dk and V ∈ Rnk×dv the nq queries and nk key-value pairs . Concep- tually , the attention mechanism outputs a weighted sum of nk values for each of nq query vectors , whose weights are determined by pairwise dot product between the key and query vectors . The output is of shape Rnq×dv . The inputs QKV are outputs from fully-connected networks themselves , with Q a function of xq ∈ Rnq×dxq and K , V projected from the same input xkv ∈ Rnk×dxkv . The attention operation is : 1 ) order-invariant with respect to xkv ; and 2 ) equivariant with respect to xq . These are the key properties we leverage in the design of the NfgTransformer to achieve its permutation equivariance property . We refer to an attention layer as self-attention when xq is the same as xkv , and cross-attention if not . In practice , each attention layer may have H attention heads performing the attention operation of Equation 3 in parallel . This enables the attention layer to aggregate multiple streams of information in one forward pass . 3 EQUIVARIANT GAME REPRESENTATION While we have informally motivated the need for equivariant embedding functions , we formally state two practical implications of an equivariant embedding function that follow from a general theorem on the conditions under which two actions must have identical embeddings given an equivariant embedding function . For conciseness , we defer all formal statements and proofs to Appendix A . Proposition 3.1 ( Repeated Actions ) . If G ( ai equivariant with f ( G ) = ( . . . , ( . . . , ai Proposition 3.2 ( Player Symmetry ) . If player p and q are symmetric , f is deterministic and equiv- ariant with f ( G ) = ( . . . , Ap , . . . , Aq , . . . ) , then Ap and Aq are identical up to permutation . p , a¬p ) , ∀a¬p and f is deterministic and p , . . . ) , . . . ) then it follows that ai p , a¬p ) = G ( aj p , . . . , aj p = aj p. Proposition 3.1 guarantees by construction that repeated actions are treated identically in any down- stream applications . Proposition 3.2 guarantees that player symmetry are reflected in the action embedding space which we show empirically in Section 5.3 for the NfgTransformer . 4 NFGTRANSFORMER We now describe the NfgTransformer , an encoder network that factorises a payoff tensor G into ac- tion embeddings via a sequence of K NfgTransformer blocks ( Figure 1 ( Top ) ) , each composed of a sequence of self- and cross-attention operations ( Figure 1 ( Bottom ) ) . We then show concrete exam- ples of decoders and loss functions in Figure 2 for several game-theoretic tasks at different decoding granularities , showing the generality of our approach . Finally , we discuss how the NfgTransformer naturally incorporates incomplete payoff tensors leveraging the flexibility of attention operations . 3 Published as a conference paper at ICLR 2024 Figure 1 : An overview of the NfgTransformer . The payoff tensor G is encoded as action embeddings { at p | ∀t ∈ [ T ] , ∀p ∈ [ N ] } ( Top ) . Action embeddings are zero-initialised and iteratively updated through a sequence of K NfgTransformer blocks ( Bottom ) . An arrow labeled with “ ( Q ) KV ” orig- inates from a set of input ( query- ) key-values and terminates at a set of outputs . Each dashed box denotes an unordered set of elements of a specific type and cardinality . 4.1 INITIALISATION & ITERATIVE REFINEMENT Permutation equivariance implies that action embeddings must be agnostic to how players and ac- tions are ordered in the payoff tensor . This suggests that action embeddings across players and actions must be initialised identically . We zero-initialise all action embeddings A = { at p = 0 | p ∈ RD . Upon initialisation , action embeddings are iteratively refined ∀t ∈ [ T ] , ∀p ∈ [ N ] } with at via a sequence of K blocks given current action embeddings and the payoff tensor G. Each block returns updated action embeddings via self- and cross-attention operations that we describe in turn . action-to-joint-action self-attention represents a play of each action ap under a joint-action a = ( ap , a¬p ) given payoff values to all players . Recall the definition of a self-attention operation , xq = xkv = { concat ( ap , Gp ( a ) ) } N p=1 , yielding one vector output per action for every joint-action . We refer to the output pG ( ap , a¬p ) as a play of ap under the joint action a and payoffs G ; action-to-play cross-attention then encodes information from all plays of each action at key-values xkv = { pG ( at singleton output , { at p , with p } , a singleton query . This operation yields a p } , as a function of all its plays and its input action embedding vector ; p , a¬p ) | ∀a¬p } and xq = { at action-to-action self-attention then represents each action given all action embeddings . Here , xq = xkv = { at p | ∀p ∈ [ N ] , ∀t ∈ [ T ] } . We ablate this operation ( by varying A ) in Section 5.1 , showing its benefits in propagating information across action embeddings . Within each block , equivariance is preserved given key-value order-invariance , and query equiv- ariance properties of the attention operation . Each output embedding at p is a function of its own embedding at input , its unordered set of plays , and the unordered set of all action embeddings . 4.2 TASK-SPECIFIC DECODING The resulting action embeddings can be used for a variety of downstream tasks at different decoding granularities . We describe and empirically demonstrate three use-cases in specifics ( Figure 2 ) . 4 Initial action embeddings Final action embeddings NfgTransformerBlock x K NfgTransformer Action-to-joint-action Action-to-play Action-to-action QKV KV QKV For each of TN joint-actions n o i t n e t t A f l e S For each of N×T actions re-arranging For each of N×T actions For each of TN joint-actions n o i t n e t t A s s o r C Q n o i t n e t t A f l e S x A For each of N×T actions NfgTransformerBlock Published as a conference paper at ICLR 2024 Figure 2 : Example task-specific decoders and losses from general-purpose action embeddings . Nash equilibrium-solving action distribution for each player ˆσp = softmax ( w1 for an action at to-end via gradient descent with ˆσ = ⊗p requires decoding at the level of each action , estimating a marginal p , . . . , wT p ) is the logit p. Here , we follow Duan et al . ( 2023 ) in minimising the loss function maxp δp ( ˆσ ) end- ˆδp and δp ( ˆσ ) = maxa′ Ea∼ˆσ [ Gp ( a′ p , a¬p ) − Gp ( a ) ] . p = MLP ( at p ) where wt p∈Ap Max deviation-gain estimation decodes a scalar estimate for each joint-action a = ( a1 , . . . , aN ) , ∀a ∈ A . Here , we represent each joint action as a = ( cid:80 ) p ap and estimate its max- imum deviation gain ˆδ ( a ) by minimising L2 ( ˆδ ( a ) , maxp δp ( a ) ) = ( ˆδ ( a ) − maxp δp ( a ) ) 2 , with δp ( a ) = maxa′ p , a¬p ) − Gp ( a ) ] the deviation gain to player p under the joint action a. p∈Ap [ Gp ( a′ Payoff reconstruction decodes a scalar for each payoff value Gp ( a ) , ∀p ∈ [ N ] , ∀a ∈ A . Here we use a self-attention operation for decoding , similar to the action-to-joint-action self-attention operation in the encoder but without appending to action embeddings their payoff values , which are to be reconstructed . To compute a reconstruction loss , we minimise L2 ( ˆGp ( a ) , Gp ( a ) ) , with ˆGp ( a ) a function of the action embedding ap and the unordered set of co-player action embeddings a¬p . 4.3 REPRESENTING INCOMPLETE GAMES We have assumed thus far that outcomes for every joint-action of the game are observed . This is not always the case in practice as the costs of evaluating every joint-action can be prohibitive . Instead , one may infer outcomes for all joint-actions , given an incomplete NFG ( Elo , 2008 ) . With a slight modification , the NfgTransformer accommodates such use-cases in a principled way . To do so , we extend the vanilla attention implementation ( Equation 3 ) to allow for additional binary mask vectors mq ∈ { 0 , 1 } nq and mkv ∈ { 0 , 1 } nk for the query and key-value inputs , indicating their validity . Equation 4 defines this masked attention operation , with 1∞ ( 1 ) = 0 and 1∞ ( 0 ) = ∞ . MaskedAttention ( Q , K , V , mq , mkv ) = softmax ( cid:18 ) QK T − 1∞ ( mq ⊗ mkv ) √ dk ( cid:19 ) V ( 4 ) To represent incomplete NFGs , we use the Equation 4 in lieu of Equation 3 in all self- and cross- attention operations and set masking vectors accordingly . For instance , if a joint-action is unob- served , then one would set mkv to reflect the validity of each play for the action-to-play operation . The NfgTransformer architecture is highly effective for representing actions in incomplete games and predict payoffs for unobserved joint-actions , as we show empirically in Section 5.2 . 5 Task-speciﬁc loss functions For each of N players For each of TN joint-actions Max deviation-gain estimation For each of TN joint-actions QKV Self Attention For each of TN joint-actions For each of N×T actions NfgTransformer Nash equilibrium solving Payoﬀ reconstruction Published as a conference paper at ICLR 2024 5 RESULTS The goal of our empirical studies is three-fold . First , we compare the NfgTransformer to baseline architectures to demonstrate improved performance in diverse downstream tasks on synthetic and standard games . Second , we vary the model hyper-parameters and observe how they affect perfor- mance . We show in particular that some tasks require larger action embedding sizes while others benefit from more rounds of iterative refinement . Lastly , we study how the model learned to solve certain tasks by interpreting the sequence of learned attention masks in a controlled setting . Our results reveal that the solution found by the model reflects elements of intuitive solutions to NE- solving in games . For completeness , we discuss additional empirical results in Appendix D. For instance , we show all nontrivial equilibrium-invariant 2×2 games are embedded following structure of the known embedding space proposed in Marris et al . ( 2023 ) . 5.1 SYNTHETIC GAMES Table 1 : We compare NfgTransformer to baseline architectures in synthetic games . Each configura- tion is averaged across 5 independent runs . For NfgTransformer variants ( Ours ) , we annotate each variant with corresponding hyper-parameters ( K , A and D as shown in Figure 1 ) . We provide train- ing curves with confidence intervals and parameter counts of each configuration in Appendix B.1 . Model N=2 T=16 Ours ( D= 32 , K=2 , A=1 ) 0.2239 Ours ( D= 32 , K=4 , A=1 ) 0.0466 Ours ( D= 32 , K=8 , A=1 ) 0.0344 Ours ( D= 64 , K=8 , A=0 ) 0.0332 Ours ( D= 64 , K=8 , A=1 ) 0.0308 Ours ( D= 64 , K=8 , A=2 ) 0.0243 Ours ( D=128 , K=2 , A=1 ) 0.2090 Ours ( D=128 , K=4 , A=1 ) 0.0429 Ours ( D=128 , K=8 , A=1 ) 0.0308 0.2770 0.3905 0.0829 EquivariantMLP MLP NES NE ( NE Gap ) N=3 N=2 T=64 T=8 0.1344 0.1685 0.0892 0.1096 0.0484 0.0554 0.0636 0.0661 0.0478 0.0545 0.0437 0.0542 0.1274 0.1665 0.0804 0.0981 0.0412 0.0502 0.1431 0.2132 0.1741 0.3248 0.1635 0.1478 N=3 T=16 0.0796 0.0553 0.0334 0.0384 0.0325 0.0314 0.0769 0.0530 0.0297 0.0929 0.1381 0.1140 Max-Deviation-Gain ( MSE ) N=2 T=16 0.0949 0.0248 0.0067 0.0056 0.0007 0.0005 0.0159 0.0013 0.0001 0.1789 0.3854 0.0488 N=2 T=64 0.5008 0.3755 0.2989 0.1549 0.0784 0.0759 0.1922 0.1361 0.0161 0.8153 0.8354 0.4860 N=3 T=8 0.5206 0.3679 0.3582 0.2848 0.1830 0.1942 0.3154 0.0955 0.0487 0.5433 0.5623 0.4047 N=3 T=16 0.6649 0.6173 0.5825 0.5583 0.4961 0.4725 0.5357 0.4153 0.3641 0.7914 0.7906 0.6480 √ We first evaluate variations of the NfgTransformer architecture on synthetic games of varying sizes on NE equilibrium-solving and deviation gain estimation . To generate synthetic games with broad coverage , we follow Marris et al . ( 2022 ) which samples games from the equilibrium-invariant subspace , covering all strategic interactions that can affect the equilibrium solution of an NFG . Each game ’ s payoff tensor G has zero-mean over other player strategies and Frobenius norm T N . We compare our results to baseline MLP networks with numbers of parameters at ∥Gp∥F = least that of our largest transformer variant ( at 4.95M parameters ) , an equivariant MLP network that re-arranges actions in descending order of their average payoffs , as well as an NES ( Marris et al. , 2022 ) network that is designed for equilibrium-solving . See Appendix B.1 for details on game sam- pling , network architectures and parameter counts of each model . We note that the parameter count of the NfgTransformer is independent of the game size , a desideratum of Hartford et al . ( 2016 ) . 5.1.1 SOLVING FOR NE EQUILIBRIUM For equilibrium solving , we optimise variants of the NfgTransformer to minimise the NE GAP δ ( ˆσ ) = maxp δp ( ˆσ ) ( Figure 2 ( Left ) ) . Table 1 ( Left ) shows our results . EquivariantMLP outper- forms MLP ( Duan et al. , 2023 ) , demonstrating the importance of leveraging equivariance inherent to NFGs but remains ineffective at solving this task . NES ( Marris et al. , 2022 ) , equivariant by construction , significantly outperforms both MLP variants in 2-player settings but trails behind in 3- player games . The NfgTransformer is also equivariant by construction but learns to capture relevant information without handcrafted payoff feature vectors . All NfgTransformer variants , most at fewer parameter count than baselines , significantly outperform across game sizes with near-zero NE GAP . 6 Published as a conference paper at ICLR 2024 Figure 3 : Payoff prediction error averaged over all players across unobserved joint-actions . Results are averaged over 32 randomly sampled empirical DISC games in each game configuration . Among the NfgTransformer variants , our results show a clear trend : increasing the number of trans- former blocks ( with K ∈ [ 2 , . . . , 8 ] ) improves performance , especially as the game becomes large . This makes intuitive sense , as it adds to the number of times the action embeddings can be usefully refined — action embeddings at the end of one iteration become more relevant queries for the next . In contrast , the benefit of increased action embedding size is muted ( with D ∈ [ 32 , . . . , 128 ] ) . We hypothesise that for equilibrium-solving , information related to a subset of the available actions can often be ignored through iterative refinement ( e.g . dominated actions ) , as they do not contribute to the final equilibrium solution . Lastly , we evaluate an NfgTransformer variant that does not perform any action-to-action self-attention ( A = 0 ) . In this case , action embeddings for the same player do not interact within the same block and its performance is markedly worse . Of particular interests is the comparison between the variants with A = 1 and A = 2 where A = 2 demonstrates a bene- fits in the most complex games of size 16 × 16 × 16 but not in smaller games . This suggests that action-to-action self-attention facilitates learning , especially in tasks that require iterative reasoning . 5.1.2 ESTIMATING MAXIMUM DEVIATION GAINS A related task is to determine what is the maximum incentive for any player to deviate from a joint pure-strategy σ ( or equivalently , a joint-action ) . This quantity is informative on the stability of a joint behaviour — in particular , if a joint pure-strategy has a maximum deviation gain δ ( a ) of zero , then by definition we have found a pure-strategy NE . We optimise a NfgTransformer network to regress towards the maximum deviation-gain δ ( a ) for every joint pure-strategy a , using a per joint- action decoder architecture ( Figure 2 ( Middle ) ) . We report the regression loss in mean squared error of different architecture variants in Table 1 ( Right ) . We observe that NES consistently outperforms MLP baselines , but underperforms the NfgTransformer variants as the size of the game increases . Similar to our observations in Section 5.1.1 , the number of transformer blocks played a role in transformer variants ’ final performance . However , it is no longer the main factor . Instead , the action-embedding size D becomes critical . Variants with higher embedding size D = 128 can be competitive , even for shallow models ( e.g . K = 4 in 16 × 16 × 16 games ) . This can be explained by the lack of structure in the underlying game , as payoff tensors cover the full equilibrium-invariant subspace of NFGs : payoffs of one joint-action does not provide any information on the outcomes of another . To perform well , the model must learn to memorise outcomes of different joint-actions a reduced action embedding size can become a bottleneck . 5.2 PAYOFF PREDICTION IN EMPIRICAL DISC GAMES What if the game class follows a structured generative process ? This is often the case in practice and a useful representation learning method should capture any such latent structure . We turn to DISC games ( Balduzzi et al. , 2019 ) to evaluate the efficacy of NfgTransformer in this case , compared to several payoff prediction methods from the ranking literature . Definition 5.1 ( DISC Game ) . Let ut , vt ∈ RZ , t ∈ [ T ] , the win-probability to action i when playing against j is defined as Pij = σ ( uT 1+e−x the sigmoid function . j vi ) = 1 − Pji with σ ( x ) = 1 i vj − uT Definition 5.1 describes a class of symmetric two-player zero-sum games where the outcomes are defined by latent vectors ut , vt ∈ RZ , t ∈ [ T ] , generalising the original definition of DISC game ( Balduzzi et al. , 2019 ) to allow for latent vectors with Z > 1 . Payoff values under one joint-action 7 p = 10.00 % = Z 36.67 % 1 p = p = 63.33 % p = 90.00 % p = 10.00 % Z 36.67 % p = = 8 p = 63.33 % p = 90.00 % ) E S M ( r o r r E d e r a u q S n a e M 0.08 0.06 0.04 0.02 0.00 ) E S M ( r o r r E d e r a u q S n a e M 0.3 0.2 0.1 0.0 l o E l o E m l o E x s r u O l o E l o E m l o E x s r u O l o E l o E m l o E x s r u O l o E l o E m l o E x s r u O l o E l o E m l o E x s r u O l o E l o E m l o E x s r u O l o E l o E m l o E x s r u O l o E l o E m l o E x s r u O Published as a conference paper at ICLR 2024 Figure 4 : Visualisation of attention masks and action-embeddings at inference time on a held-out instance of Bertrand Oligopoly game whose payoff tensor is shown on the Left and the inferred NE strategy profile shown on the right ( with NE GAP at near zero ) . The equilibrium pure-strategies for the two players are shown in red . The sequence of 8 action-to-play attention masks and the PCA- reduced action-embeddings at the end of each transformer block are shown in the middle . therefore become informative for predicting the outcomes of others . The amount of information that can be inferred about one joint-action from knowing another is controlled by the latent vector dimension Z . The problem setting is as follows . Each algorithm is given access to an empirical game where outcomes for each joint-action is observed with probability p. The goal is to accurately predict the outcome to each player under unobserved joint-actions . A rich body of literature have been dedicated to solving this task owing to its relevance in real-world In Go and Chess , players are classically assigned Elo ratings which are designed competitions . to be predictive of the win-probability between any two ranked players . Several improvements to Elo have been proposed since , recognising the many limitations of Elo . We compare our results to methods such as Elo ( Elo , 2008 ) , mElo ( Balduzzi et al. , 2018 ) and xElo ( Bertrand et al. , 2023 ) across different settings of the DISC game . Figure 3 shows our results in MSE averaged across unobserved joint-actions . NfgTransformer outperforms all baselines significantly across all settings . In particular , NfgTransformer recovered the latent variable game ( i.e . Z = 1 ) near perfectly as soon as 10 % of the joint-actions are observed , with an error rate an order of magnitude lower than the second best method . This result is particularly remarkable as baseline methods are designed with DISC games in mind , when NfgTransformer is not . At Z = 8 , NfgTransformer continues to outperform , with its prediction accuracy degrading gracefully as fewer joint actions are observed . Our results suggest that NfgTransformer is highly effectively at recognising and exploiting the latent structure in games if it exists . We provide details on game sampling , masking , network architecture and baseline implementation in Appendix B.2 . For NfgTransformer , outcomes of the unobserved joint-actions are masked out following the procedure described in Section 4.3 . At training time , the model minimises reconstruction loss for all joint-actions . 5.3 INTERPRETING NFGTRANSFORMER NfgTransformer explicitly reasons about action-embedding through structured attention mechanism , allowing for inspection at each stage of the iterative refinement process . We exploit this property and investigate how the model implements game-theoretic algorithms such as equilibrium-solving in a suite of 22 GAMUT games representing diverse strategic interactions ( Nudelman et al. , 2004 ; Porter et al. , 2008 ) that are classically studied in the economics and game theory literature . We optimised an NfgTransformer network as in Section 5.1 , but focused on 10×10 games and removed the action-to- action self-attention ( i.e . A = 0 ) for a more concise visualisation . Instead of attending to multiple pieces of information in parallel , each attention layer is implemented by a single attention head 8 0 1 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 2 4 6 block 7 8 10 12 14 0 1 2 3 4 5 6 7 Action to plays BertrandOligopoly ( d01 ) [ NashConv = 1.37161e-05 ] 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 Action to actions 2 1 4 1 0 1 2 1 4 1 0 1 2 1 4 1 0 1 2 1 4 1 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 0 1 2 1 4 1 0 1 2 1 4 1 0 1 2 1 4 1 0 1 2 1 4 1 1 2 3 4 5 6 7 Action embeddings ( PCA ) Marginals 0 n o i t c a 1 n o i t c a 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 0 2 4 6 8 Payoff 0 w o r r e y a l p 0 1 2 3 4 5 6 7 0 1 2 5 6 7 0 1 2 3 4 5 6 7 col payoff 0 1 2 Approx . NE 0 1 2 3 4 5 6 7 8 9 0 r e y a p l 0 1.5 1.0 Payoff block 0 block 1 block 2 block 3 block 5 block 6 Action to plays 1 3 w o block 4 r 4 0 r e y a p l 1 r e y a p l 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 payoff 0 r e y a p l 1 r e y a p l 0 2 4 6 8 0 2 4 6 8 2 0 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 block 0 block 1 block 2 block 3 block 4 block 5 Action embeddings ( PCA ) 0.5 0.0 block 6 block 7 −0.5 −1.0 −2 0 2 action 0 5 0 5 0 5 0 5 0 5 0 5 0 5 0 0 1 5 2 1 r e y 4 a p l 0 1 2 3 4 5 6 7 8 3 9 6 4 5 6 7 −2 0 2 4 6 −2 0 2 4 6 −2 0 2 4 6 −2 0 2 4 6 −2 0 2 4 6 −2 0 2 4 6 −2 0 2 4 6 −2 0 2 0 1 2 3 4 5 6 7 8 9 0 1 ( NE Gap = 4.6e-05 ) action player Published as a conference paper at ICLR 2024 ( H = 1 ) . We show in Appendix C that this simplified architecture is sufficient in approximating NE in games of this size to reasonable accuracy . We visualise the network on held-out games . Figure 4 illustrates an instance of iterative NE solving in a game of Bertrand Oligopoly in action . The NfgTransformer successfully identified a pure-strategy NE ( Right ) with near zero NE GAP ( Top-Right ) . Inspecting the payoff tensor ( Left ) , we verify that player 0 playing action 2 and player 1 countering with action 1 is indeed a pure-strategy NE . Is this solution reflected in the underlying attention mechanism ? Indeed , the action-to-plays attention masks ( Top ) appear to have converged to a recurring pattern where each action attends to the equilibrium-strategy of its co-player . This is remarkable for two reasons . First , NE by definition implies indifference across action choices — when co-players implement the equilibrium strategy , the focal player should be indifferent to its action choice . Here , this indifference played out . Second , an equilibrium solution should be stable which appears to be the case over the last few iterations of action-to-plays attentions . Zooming in on the attention masks at earlier iterations , we see that following zero-initialisation of all action embeddings , the attention mask for each action is equally spread across all its plays . The attention masks at the next block , by contrast , appear structured . Indeed , each action attends to the play that involves the best-response from the co-player . It is worth noting that this pattern of attending to one ’ s best-response in NE-solving emerged through learning , without prior knowledge . The action embeddings themselves also reveal interesting properties of the game , such as sym- metry ( Proposition 3.2 ) . While it might not be immediately obvious that the payoff matrices in Figure 4 can be made symmetric by permuting actions , the action embeddings ( Bottom ) show that action-embeddings across the two players overlap exactly , thanks to the inherent equivariance of the NfgTransformer . The ordering of players and actions is unimportant — the representation of an action is entirely driven by its outcomes under the joint-actions . Here , action embeddings revealed the inherent symmetry of Bertrand Oligopoly ( Bertrand , 1883 ) . For completeness , we offer similar analysis of the NfgTransformer applied to other game classes in Appendix C , including games without symmetry , with mixed-strategy NE as well as instances where NfgTransformer failed . Additionally , we show examples of applying NfgTransformer to out- of-distribution games such as Colonel Blotto ( Roberson , 2006 ) that presents several strategy cycles . 6 RELATED WORKS Recent works leveraged deep learning techniques to accelerate scientific discoveries in mathematics ( Fawzi et al. , 2022 ) , physics ( Pfau et al. , 2020 ) and biology ( Jumper et al. , 2021 ) . Our work is similarly motivated , but brings deep learning techniques to game theory and economic studies . We follow a line of works in bringing scalable solutions to game theory ( Marris et al. , 2022 ; Duan et al. , 2023 ; Vadori & Savani , 2023 ) , or integrating components for strategic reasoning as a part of a machine learning system ( Hu & Wellman , 2003 ; Greenwald et al. , 2003 ; Feng et al. , 2021 ; Liu et al. , 2022b ; a ) . In game theory , Hartford et al . ( 2016 ) is the closest to our work and the first to apply deep representation learning techniques to cognitive modelling of human strategic reasoning using NFGs . The authors systematically outlined a number of desiderata for representation learning of two-player NFGs , including player and action equivariance as well as the independence between the number of parameters of the learned model and the size of the game . The NfgTransformer satisfies these desiderata but applies to n-player general-sum games . Wiedenbeck & Brinkman ( 2023 ) studies efficient data structures for payoff representation ; our approach can be readily integrated into deep learning systems without any assumption on the games . 7 CONCLUSION We proposed NfgTransformer as a general-purpose , equivariant architecture that represents NFGs as action embeddings . We demonstrate its versatility and effectiveness in a number of benchmark tasks from different sub-fields of game theory literature , including equilibrium solving , deviation gain estimation and ranking . We report empirical results that significantly improve upon state-of-the- art baseline methods , using a unified representation learning approach . We show that the resulting model is also interpretable and parameter-efficient . Our work paves the way for integrating game- theoretic reasoning into deep learning systems as they are deployed in the real-world . 9 Published as a conference paper at ICLR 2024 ACKNOWLEDGMENTS We are grateful to Bernardino Romera-Paredes for the productive discussion on the different consid- erations in designing an equivariant neural architecture , to Wojciech M. Czarnecki for his expertise in ranking and evaluation in games and to Skanda Koppula for his advice on optimisation techniques for the transformer architecture . REFERENCES Elliot Anshelevich , Aris Filos-Ratsikas , Nisarg Shah , and Alexandros A. Voudouris . Distortion in In International Joint Conference on social choice problems : The first 15 years and beyond . Artificial Intelligence , 2021 . URL https : //api.semanticscholar.org/CorpusID : 232075911 . David Balduzzi , Karl Tuyls , Julien Perolat , and Thore Graepel . Re-evaluating evaluation . Advances in Neural Information Processing Systems , 31 , 2018 . David Balduzzi , Marta Garnelo , Yoram Bachrach , Wojciech Czarnecki , Julien Perolat , Max Jader- berg , and Thore Graepel . Open-ended learning in symmetric zero-sum games . In International Conference on Machine Learning , pp . 434–443 . PMLR , 2019 . Joseph Bertrand . Th´eorie math´ematique de la richesse sociale . Journal des savants , 67 ( 1883 ) : 499–508 , 1883 . Quentin Bertrand , Wojciech Marian Czarnecki , and Gauthier Gidel . On the limitations of the Elo , real-world games are transitive , not additive . In International Conference on Artificial Intelligence and Statistics , pp . 2905–2921 . PMLR , 2023 . Xi Chen , Xiaotie Deng , and Shang-Hua Teng . Settling the complexity of computing two-player Nash equilibria . Journal of the ACM ( JACM ) , 56 ( 3 ) :1–57 , 2009 . Constantinos Daskalakis , Paul W Goldberg , and Christos H Papadimitriou . The complexity of computing a Nash equilibrium . Communications of the ACM , 52 ( 2 ) :89–97 , 2009 . Alexey Dosovitskiy , Lucas Beyer , Alexander Kolesnikov , Dirk Weissenborn , Xiaohua Zhai , Thomas Unterthiner , Mostafa Dehghani , Matthias Minderer , Georg Heigold , Sylvain Gelly , Jakob Uszko- reit , and Neil Houlsby . An image is worth 16x16 words : Transformers for image recogni- tion at scale . In International Conference on Learning Representations , 2021 . URL https : //openreview.net/forum ? id=YicbFdNTTy . Zhijian Duan , Wenhan Huang , Dinghuai Zhang , Yali Du , Jun Wang , Yaodong Yang , and Xiaotie Deng . Is Nash equilibrium approximator learnable ? In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems , pp . 233–241 , 2023 . A.E . Elo . The Rating of Chessplayers : Past and Present . ISBN 9780923891275 . URL https : //books.google.co.uk/books ? id=syjcPQAACAAJ . Ishi Press International , 2008 . Alhussein Fawzi , Matej Balog , Aja Huang , Thomas Hubert , Bernardino Romera-Paredes , Moham- madamin Barekatain , Alexander Novikov , Francisco J R Ruiz , Julian Schrittwieser , Grzegorz Swirszcz , et al . Discovering faster matrix multiplication algorithms with reinforcement learning . Nature , 610 ( 7930 ) :47–53 , 2022 . Xidong Feng , Oliver Slumbers , Ziyu Wan , Bo Liu , Stephen McAleer , Ying Wen , Jun Wang , and Yaodong Yang . Neural auto-curricula in two-player zero-sum games . Advances in Neural Infor- mation Processing Systems , 34:3504–3517 , 2021 . Joaquim Gabarr´o , Alina Garc´ıa , and Maria Serna . The complexity of game isomorphism . Theoreti- cal Computer Science , 412 ( 48 ) :6675–6695 , 2011 . Amy Greenwald , Keith Hall , Roberto Serrano , et al . Correlated Q-learning . In ICML , volume 3 , pp . 242–249 , 2003 . Jason S Hartford , James R Wright , and Kevin Leyton-Brown . Deep learning for predicting human strategic behavior . Advances in neural information processing systems , 29 , 2016 . 10 Published as a conference paper at ICLR 2024 Sepp Hochreiter and J¨urgen Schmidhuber . Long short-term memory . Neural computation , 9 ( 8 ) : 1735–1780 , 1997 . Junling Hu and Michael P Wellman . Nash q-learning for general-sum stochastic games . Journal of machine learning research , 4 ( Nov ) :1039–1069 , 2003 . Andrew Jaegle , Felix Gimeno , Andy Brock , Oriol Vinyals , Andrew Zisserman , and Joao Carreira . Perceiver : General perception with iterative attention . In International conference on machine learning , pp . 4651–4664 . PMLR , 2021 . John Jumper , Richard Evans , Alexander Pritzel , Tim Green , Michael Figurnov , Olaf Ronneberger , Kathryn Tunyasuvunakool , Russ Bates , Augustin ˇZ´ıdek , Anna Potapenko , et al . Highly accurate protein structure prediction with alphafold . Nature , 596 ( 7873 ) :583–589 , 2021 . Marc Lanctot , Vinicius Zambaldi , Audrunas Gruslys , Angeliki Lazaridou , Karl Tuyls , Julien P´erolat , David Silver , and Thore Graepel . A unified game-theoretic approach to multiagent rein- forcement learning . Advances in neural information processing systems , 30 , 2017 . Yann LeCun , Yoshua Bengio , et al . Convolutional networks for images , speech , and time series . The handbook of brain theory and neural networks , 3361 ( 10 ) :1995 , 1995 . Michael L Littman et al . Friend-or-foe q-learning in general-sum games . In ICML , volume 1 , pp . 322–328 , 2001 . Siqi Liu , Marc Lanctot , Luke Marris , and Nicolas Heess . Simplex neural population learning : Any- mixture bayes-optimality in symmetric zero-sum games . In International Conference on Machine Learning , pp . 13793–13806 . PMLR , 2022a . Siqi Liu , Luke Marris , Daniel Hennes , Josh Merel , Nicolas Heess , and Thore Graepel . NeuPL : In International Conference on Learning Representations , 2022b . Neural population learning . URL https : //openreview.net/forum ? id=MIX3fJkl_1 . Luke Marris , Ian Gemp , Thomas Anthony , Andrea Tacchetti , Siqi Liu , and Karl Tuyls . Turbocharg- ing solution concepts : Solving NEs , CEs and CCEs with neural equilibrium solvers . Advances in Neural Information Processing Systems , 35:5586–5600 , 2022 . Luke Marris , Ian Gemp , and Georgios Piliouras . Equilibrium-invariant embedding , metric space , and fundamental set of 2 × 2 normal-form games . arXiv preprint arXiv:2304.09978 , 2023 . J. C. C. McKinsey . 11 . ISOMORPHISM OF GAMES , AND STRATEGIC EQUIVALENCE , pp . 117– ISBN 9781400881727. doi : doi:10.1515/ 130 . Princeton University Press , Princeton , 1951 . 9781400881727-012 . URL https : . John Nash . Non-cooperative games . Annals of Mathematics , 54 ( 2 ) :286–295 , 1951 . ISSN 0003486X . URL http : //www.jstor.org/stable/1969529 . Eugene Nudelman , Jennifer Wortman , Yoav Shoham , and Kevin Leyton-Brown . Run the gamut : A comprehensive approach to evaluating game-theoretic algorithms . In AAMAS , volume 4 , pp . 880–887 , 2004 . David Pfau , James S. Spencer , Alexander G. D. G. Matthews , and W. M. C. Foulkes . Ab initio solution of the many-electron Schr¨odinger equation with deep neural networks . Phys . Rev . Res. , 2:033429 , Sep 2020. doi : 10.1103/PhysRevResearch.2.033429 . URL https : //link.aps . . Ryan Porter , Eugene Nudelman , and Yoav Shoham . Simple search methods for finding a Nash equilibrium . Games and Economic Behavior , 63 ( 2 ) :642–662 , 2008 . Brian Roberson . The colonel blotto game . Economic Theory , 29 ( 1 ) :1–24 , 2006 . Nelson Vadori and Rahul Savani . Ordinal potential-based player rating . arXiv preprint arXiv:2306.05366 , 2023 . 11 Published as a conference paper at ICLR 2024 Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , Łukasz Kaiser , and Illia Polosukhin . Attention is all you need . Advances in neural informa- tion processing systems , 30 , 2017 . Emmanouil-Vasileios Vlatakis-Gkaragkounis , Lampros Flokas , Thanasis Lianeas , Panayotis Mer- tikopoulos , and Georgios Piliouras . No-regret learning and mixed nash equilibria : They do not mix . Advances in Neural Information Processing Systems , 33:1380–1391 , 2020 . Bryce Wiedenbeck and Erik Brinkman . Data structures for deviation payoffs . In Adaptive Agents and Multi-Agent Systems , 2023 . URL https : //api.semanticscholar.org/ CorpusID:257220156 . 12 Published as a conference paper at ICLR 2024 A PERMUTATION EQUIVARIANT REPRESENTATION OF NFGS We provide formal statements on identities in action embedding representation when using a de- terministic , permutation equivariant encoder for NFGs . First , we recall the definition of a strong isomorphism between two NFGs ( McKinsey , 1951 ; Gabarr´o et al. , 2011 ) . Definition A.1 ( Strongly Isomorphic Games ) . Let G and G′ be two NFGs . G and G′ are said to be strongly isomorphic and ϕ a strong isomorphism if ϕ = ( ( τp , p ∈ [ N ] ) , ω ) with τp : ai p a player action permutation and ω : p → p′ a player permutation such that G′ p → ai′ ( cid:0 ) τω ( 1 ) ( aω ( 1 ) ) , . . . , τω ( N ) ( aω ( N ) ) ( cid:1 ) = Gp ( a1 , ... , aN ) , ∀a ∈ A. ω ( p ) To make Definition A.1 concrete , consider the coordination and anti-coordination games shown in Figure 5 . The two games are strongly isomorphic because there exists a strong isomorphism 2 → a1 ϕ = ( ( τ1 = ( a1 2 ) ) , ω = ( 1 → 1 , 2 → 2 ) ) . As aside , McKinsey ( 1951 ) calls strongly isomorphic games strategically equivalent which we discuss soon . 1 ) , τ2 = ( a1 1 → a1 1 → a2 2 → a2 2 , a2 1 , a2 G a1 2 a1 1 , 1 1 a2 0 , 0 1 a2 2 0 , 0 1 , 1 G′ a1 1 a2 1 a1 2 0 , 0 1 , 1 a2 2 1 , 1 0 , 0 ( a ) Coordination game ( b ) Anti-coordination game Figure 5 : Strongly isomorphic games . We additionally note that in the special case when G is G′ , ϕ is referred to as a strong automorphism . Definition A.2 ( Strongly Automorphic Game ) . G is said to be strongly automorphic and ϕ a strong automorphism if ϕ = ( ( τp , p ∈ [ N ] ) , ω ) with τp : ai p a player action permutation and ω : p → ( cid:0 ) τω ( 1 ) ( aω ( 1 ) ) , . . . , τω ( N ) ( aω ( N ) ) ( cid:1 ) = Gp ( a1 , ... , aN ) , ∀a ∈ p′ a player permutation such that Gω ( p ) A. p → ai′ For instance , the coordination game ( Figure 5a ) is also strongly automorphic as there exists three automorphisms that recover the same game . ϕ = ( ( τ1 = ( a1 ϕ = ( ( τ1 = ( a1 ϕ = ( ( τ1 = ( a1 1 → a2 1 → a2 1 → a1 1 , a2 1 , a2 1 , a2 1 → a1 1 → a1 1 → a2 1 ) , τ2 = ( a1 1 ) , τ2 = ( a1 1 ) , τ2 = ( a1 2 → a2 2 → a2 2 → a1 2 , a2 2 , a2 2 , a2 2 → a1 2 → a1 2 → a2 2 ) ) , ω = ( 1 → 1 , 2 → 2 ) ) 2 ) ) , ω = ( 1 → 2 , 2 → 1 ) ) 2 ) ) , ω = ( 1 → 2 , 2 → 1 ) ) ( 5a ) ( 5b ) ( 5c ) Note that ϕ is a permutation over all the players ’ actions which is a composition of the player and action permutations ϕ = τ1 · ... ·τN ·ω . Therefore ϕ is not a general permutation , but a structured one . We use a convention that the player permutation is applied last . Finally , we recall any permutation π can be written uniquely as m permutation orbits with π = C 1 , . . . , C m , each operating on a disjoint ( possibly singleton ) subset of elements that π operates over . Therefore ϕ is also a collection of permutation orbits . Considering the coordination game again , the automorphisms ( Equations 5a-5c ) can be written as permutations which each consists of two orbits containing two actions each . 1 a2 a1 1 a1 2 a2 2 ( 6a ) 1 a2 a1 1 a1 2 a2 2 ( 6b ) 1 a2 a1 1 a1 2 a2 2 ( 6c ) Definition A.3 ( Strategically Equivalent Actions ) . Two actions ai if there exists a strong automorphism , ϕ which contains ai actions ai ( ... , i → j , ... ) ) , ω = ( ... , p → q ) ) . q are strategically equivalent q in an orbit . Equivalently , two q are strategically equivalent if there exists a strong automorphism , ϕ = ( ( ... , τp = p and aj p and aj p and aj 1 , a2 Again , consider the running example of the coordination game . From Equation 6a we can see that ( a1 1 ) , and ( a1 2 ) are each strategically equivalent pairs . Furthermore , from Equation 6b we can see that ( a1 2 ) , and ( a2 2 ) are also each strategically equivalent pairs . Therefore in the coordination games all the actions are strategically equivalent to each other . 2 , a2 1 , a2 1 , a1 13 Published as a conference paper at ICLR 2024 Figure 6 : We compare NfgTransformer to baseline architectures in synthetic games . Results from baseline experiments are shown in dashed lines . Each configuration is averaged across 5 independent runs with shaded areas representing the confidence intervals . For NfgTransformer variants , we annotate each variant with corresponding hyper-parameters ( K , A and D as shown in Figure 1 ) . Theorem A.4 . If an embedding function , f , is deterministic and equivariant , then strategically equivalent actions , ai q , must have the same embeddings . p and aj Proof . The embedding function , f , is deterministic and equivariant over players and actions . Addi- tionally , if ϕ is an automorphism of G , then f ( G ) = f ( ϕ ( G ) ) = ϕ ( f ( G ) ) . Therefore the embed- dings are also equal , ai p = aj q . Proposition A.5 ( Repeated Actions ) . If G ( ai equivariant with f ( G ) = ( . . . , ( . . . , ai p , a¬p ) = G ( aj p , a¬p ) , ∀a¬p and f is deterministic and p , . . . , aj p , . . . ) , . . . ) then it follows that ai p = aj p. Proof . If actions are repeated , there there exists an automorphism ϕ = ( ( ... , τp = ( ... , i → j , ... ) , ... ) , ω = identity ) . Therefore ai q are strategically equivalent and have the same p = aj embeddings , ai p. p and aj Proposition A.6 ( Player Symmetry ) . If player p and q are symmetric , f is deterministic and equiv- ariant with f ( G ) = ( . . . , Ap , . . . , Aq , . . . ) , then Ap and Aq are identical up to permutation . Proof . If the game is symmetric between p and q , there there exists an automorphism ϕ = ( ( ... , τp , ... ) , ω = ( ... , p → q , ... ) ) . Therefore τp ( ai q are strategically equivalent for all i , and have the same embeddings , τp ( Ap ) = Aq . p ) and ai B EXPERIMENTAL SETUP B.1 SUPERVISED LEARNING IN SYNTHETIC GAMES Games Sampling Games are sampled from the equilibrium-invariant subspace ( Marris et al. , T N ) Frobe- 2023 ; 2022 ) , with zero-mean payoff over other player actions and a unit variance ( nius tensor norm over player payoffs . To sample uniformly over such a set , first sample a game from √ 14 [ 16 , 16 ] [ 64 , 64 ] [ 8 , 8 , 8 ] NE 0.3 0.2 0.1 0.2 0.1 [ 16 , 16 , 16 ] 0.15 0.10 0.05 100,000 200,000 0 100,000 200,000 0 100,000 200,000 0 100,000 200,000 gradient steps gradient steps gradient steps gradient steps [ 16 , 16 ] Max Deviation Gain [ 64 , 64 ] [ 8 , 8 , 8 ] [ 16 , 16 , 16 ] 0.8 0.6 0.4 0.2 0.0 0.8 0.6 0.4 0.2 0.0 0.8 0.6 0.4 100,000 200,000 0 100,000 200,000 0 100,000 200,000 0 100,000 200,000 gradient steps gradient steps gradient steps gradient steps 0.4 0.2 p a G E N 0.0 0 ) E S M ( r o r r E d e r a u q S n a e M 0.4 0.2 0.0 0 Model MLP NfgTransformer ( D=32 , NfgTransformer ( D=64 , K=4 , K=8 , A=1 ) A=2 ) EquivariantMLP NfgTransformer ( D=32 , NfgTransformer ( D=128 , K=8 , A=1 ) K=2 , A=1 ) NES NfgTransformer ( D=64 , NfgTransformer ( D=128 , K=8 , A=0 ) K=4 , A=1 ) NfgTransformer ( D=32 , NfgTransformer ( D=64 , NfgTransformer ( D=128 , K=2 , K=8 , A=1 ) A=1 ) K=8 , A=1 ) Published as a conference paper at ICLR 2024 a unit normal distribution , Gp ∼ N ( 0 , 1 ) , and then normalize . Gequil p ( a ) = √ T N Z  Gp ( a ) − 1 T ( cid:88 ) ap  Gp ( ap , a−p )  Z = ( cid:13 ) ( cid:13 ) ( cid:13 ) ( cid:13 ) ( cid:13 ) ( cid:13 ) Gp − 1 T ( cid:88 ) ap Gp ( ap , a−p ) ( cid:13 ) ( cid:13 ) ( cid:13 ) ( cid:13 ) ( cid:13 ) ( cid:13 ) F ( 7 ) The benefit of this distribution is that it provides a way to uniformly sample over the space of all possible strategic interactions in a NFG of a specific shape . The equilibrium-invariant subspace has lower degree of freedom than a full NFG , freeing the neural network from having to learn offset and scale invariance . Any game can be simply mapped to the equilibrium-invariant subspace without changing its set of equilibria . Architecture We provide additional technical details on the network architectures presented in Section 5.1 . The baseline MLP networks are composed of 5 fully-connected layers with 1,024 hidden units each . The baseline NES architecture ( Marris et al. , 2022 ) consisted of 4 “ payoff to payoff ” layers with 128 channels , a “ payoff to dual ” layer with 256 channels and 4 “ dual to dual ” layers with 256 channels . Each layer uses mean and max pooling functions . All NfgTransformer model variants have H = 8 attention heads . Parameter counts of all model variants are reported in Table 2 . Table 2 : The number of network parameters by configuration for each task . We note that the number of parameters of the NfgTransformer and the NES is independent from the size of the games . This is in contrast to fully-connected networks whose parameter counts depend on the input sizes . Model NfgTransformer ( D= 32 , K=2 , A=1 ) NfgTransformer ( D= 32 , K=4 , A=1 ) NfgTransformer ( D= 32 , K=8 , A=1 ) NfgTransformer ( D= 64 , K=8 , A=0 ) NfgTransformer ( D=128 , K=2 , A=1 ) NfgTransformer ( D= 64 , K=8 , A=1 ) NfgTransformer ( D= 64 , K=8 , A=2 ) NfgTransformer ( D=128 , K=4 , A=1 ) NfgTransformer ( D=128 , K=8 , A=1 ) EquivariantMLP MLP NES # Parameter ( NE ) 0.15M 0.31M 0.61M 1.10M 1.22M 1.63M 2.16M 2.44M 4.88M 4.76M - 16.83M 4.76M - 16.83M 2.25M # Parameter ( Max-Deviation-Gain ) 0.16M 0.31M 0.62M 1.11M 1.29M 1.64M 2.17M 2.51M 4.95M 4.99M - 20.98M 4.99M - 20.98M 2.51M Convergence progression Figure 6 visualises the training progression of each model configura- tion , task and game size from the same experiments reported in Table 1 . B.2 PAYOFF PREDICTION IN DISC GAMES Game Sampling Following Definition 5.1 , generating DISC games amounts to sampling latent vectors ut , vt ∈ RZ , t ∈ [ T ] . Any real-valued latent vectors would define a valid DISC game and we let ut = n + u with n ∼ N ( 0 , 1 ) and u ∼ U ( −1 , 1 ) . We sample vt in the same way . The shift random variable u is not strictly necessary in this case , but it increases the probability that the resulting DISC game is not fully cyclic following Proposition 1 of Bertrand et al . ( 2023 ) . Masking For each sampled instance of the DISC game , with a payoff tensor of shape [ N , T , . . . , T ] , we additionally sample a binary mask of shape [ T , . . . , T ] where each element fol- lows Bernoulli ( p ) . Both the game payoff tensor and the sampled mask for the game tensor are provided as inputs to the NfgTransformer network . We ensure that the model does not observe the payoff values of masked joint-actions following Equation 4 . During loss computation , we minimise the L2 loss ( Figure 2 ( Right ) ) over all joint-actions , observed ( i.e . for reconstruction ) or unobserved ( i.e . for prediction ) . Architecture For all results in this section , we used NfgTransformer ( K=8 , A=1 , D=64 ) with H = 8 attention heads for all attention operations . 15 Published as a conference paper at ICLR 2024 Figure 7 : NE GAP reported for each of the 22 GAMUT games throughout training . We note that a single network , NfgTransformer ( K=8 , A=0 , D=16 ) , with H = 1 is optimised to solve for Nash Equilibrium across all game classes . Baseline Solvers For all baseline results , we used the open-source implementation of Elo , mElo and xElo of released at https : //github.com/QB3/discrating . For mElo and xElo , we used n components = 3 and the same settings as reported in Bertrand et al . ( 2023 ) . C INTERPRETABILITY RESULTS We provide additional details on the empirical results in Section 5.3 . Figure 7 shows that despite sim- plifications made in Section 5.3 for our interpretability results , the NfgTransformer remains capable of equilibrium-solving in most games to reasonable accuracy , with CovariantGame ( d06 ) the most challenging game class . We show a failure case in this game class in Figure 8 ( Middle ) and present additional example instances where the model successfully solved for a mixed-strategy NE ( Top ) or generalised to the out-of-distribution game class of Blotto ( Roberson , 2006 ) . Please refer to figure caption for additional remarks on the results . D THE SPACE OF 2×2 GAMES Marris et al . ( 2023 ) introduced a subset of 2×2 normal-form games that any 2×2 game can be mapped to without changing its set of ( coarse ) correlated equilibria and Nash equilibria . This subset of games is called the equilibrium-invariant subset , and includes all possible nontrivial strategic interactions of 2×2 games . Properties of games such as their equilibria , permutation symmetries , and best-response dynamics can be visualized in this “ map of games ” . We can analyse the embeddings found by the NfgTransformer by sweeping over the nontrivial 2×2 equilibrium-invariant subset . We used the transformer architecture NfgTransformer ( K=2 , A=1 , D=16 ) with H = 2 atten- tion heads at every self- and cross-attention layer . We used an additional linear layer to reduce the action embedding dimension down to 1 , per player , per action , resulting in four variables to de- scribe the game embeddings . We trained NfgTransformer with an NE objective over the space of equilibrium-invariant subsets , and verified that the loss approaches zero . With the trained NfgTrans- former , we sweep over the nontrivial 2×2 equilibrium-invariant subset , and visualize the embeddings ( Figure 9 ) . The learned action-embeddings have a very low value ( blue regions ) when that action has all the mass in the NE , and very high value when the action has no mass in the NE ( dark red regions ) . These “ L ” regions therefore correspond to games which have a single pure NE . The embeddings are low value ( cyan regions ) when the game has a mixed NE solution and occurs near the cyclic ) . In these regions , all embeddings have to be similarly colored , as all actions games ( are mixed . The embeddings are high value ( red regions ) in coordination game areas where there are two disconnected NEs ( ) . The borders between these regions correspond to changes in game payoffs when one action becomes become profitable than another , and as a result the NE can change drastically , and therefore so does the embedding . and and 16 p a G E N 0.25 0.20 0.15 0.10 0.05 0.00 0 40,000 80,000 gradient 120,000 steps 160,000 200,000 game BertrandOligopoly ( d01 ) GraphicalGame-SW ( d12 ) BidirectionalLEG-CG ( d02 ) MinimumEffortGame ( d13 ) BidirectionalLEG-RG ( d03 ) PolymatrixGame-CG ( d14 ) BidirectionalLEG-SG ( d04 ) PolymatrixGame-RG ( d15 ) CovariantGame ( d06 ) PolymatrixGame-Road ( d16 ) CovariantGame-Pos ( d05 ) PolymatrixGame-SW ( d17 ) CovariantGame-Zero ( d07 ) RandomGame ( d18 ) DispersionGame ( d08 ) TravelersDilemma ( d19 ) GraphicalGame-RG ( d09 ) UniformLEG-CG ( d20 ) GraphicalGame-Road ( d10 ) UniformLEG-RG ( d21 ) GraphicalGame-SG ( d11 ) UniformLEG-SG ( d22 ) Published as a conference paper at ICLR 2024 Figure 8 : Here we provide additional interpretability results similar to Figure 4 but for games that are asymmetric , have mixed-strategy NE ( Top ) or out-of-distribution ( Bottom ) . We additionally provide an instance where the model struggled to find an NE ( Middle ) where the attention masks did not appear to have converged . For Blotto ( Roberson , 2006 ) which is a game class not seen during training , the model generalised well and identified a pure-strategy NE . The action embeddings also revealed three clusters , corresponding to the three strategically equivalent classes of actions . Note that one of the clusters corresponds to the dominant action of the two players . 17 Payoff block 0 block 1 0 r e y a p l 1 r e y a p l 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 2 1 0 −1 −2 0 1 2 3 4 5 6 7 8 9 payoff 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 2 4 6 8 10 12 14 0 1 Action to plays block 3 Payoff 0 1 2 3 0 w o 4 r block 2 5 r e y a l p 1 w o r 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 col payoff 0 1 2 3 4 5 6 7 Action to plays BertrandOligopoly ( d01 ) [ NashConv = 1.37161e-05 ] block 4 block 5 block 6 block 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 GraphicalGame-SG ( d11 ) [ NE Gap = 4.95456e-03 ] Approx . NE Action to actions 0 0 1 2 3 0 r e y a p l 0 2 4 6 8 0 1 2 1 4 1 0 2 4 6 8 0 1 2 1 4 1 0 2 4 6 8 0 1 2 1 4 1 0 2 4 6 8 1 2 3 4 5 6 7 0 1 8 2 1 4 1 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 0 1 2 1 4 1 0 1 2 1 4 1 0 1 2 1 4 1 0 1 2 1 4 1 5 5 6 6 7 7 4 4 4 4 4 4 5 6 7 BertrandOligopoly ( d01 ) [ NashConv = 1.37161e-05 ] 5 6 7 4 5 6 7 Marginals 0 n o i t c a 1 n o i t c a Marginals 0 n o i t c a 1 n o i t c a Marginals 0 n o i t c a 1 n o i t c a 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 012 3 456 7 89 012 34567 89 0123 4567 89 0 1 0123 4567 89 2 1.5 0 1 2 0123 4567 89 012 3456 789 012 3456 789 0123 4567 89 Action embeddings ( PCA ) 1.0 Action embeddings ( PCA ) 9 3 0 Action to plays 1 5 6 7 BertrandOligopoly ( d01 ) [ NashConv = 1.37161e-05 ] block 0 block 1 block 2 block 3 0 5 0 5 0 5 0 5 −2 −1 0 1 2 0 1 2 3 4 5 6 7 8 9 action Payoff block 0 block 1 Action to plays block 3 0.5 0.0 −0.5 −1.0 0 −2 0 action 0 1 block 4 0 block 5 1 block 6 2 block 7 2 4 6 −2 0 2 4 6 −2 0 2 4 6 −2 0 2 1 2 5 3 4 0 5 6 5 7 0 5 0 5 3 1 r e y a p l 2 3 4 5 6 4 7 8 9 6 −2 0 2 4 6 −2 0 2 4 6 −2 0 2 4 6 −2 0 2 4 6 block 4 block 5 block 6 block 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 CovariantGame ( d06 ) [ NE Gap = 2.47795e-01 ] 0 r e y a p l 1 r e y a p l 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 1 0 Approx . NE Action to actions 0 0 1 2 3 0 r e y a p l 0 2 4 6 8 0 1 2 1 4 1 0 2 4 6 8 0 1 2 1 4 1 0 2 4 6 8 0 1 2 1 4 1 0 2 4 6 8 1 2 3 4 5 6 7 0 1 8 2 1 4 1 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 0 1 2 1 4 1 0 1 2 1 4 1 0 1 2 1 4 1 0 1 2 1 4 1 0 1 2 3 5 6 7 0123 4567 89 012 3456 789 012 3456 789 0123 4567 89 10 12 14 0 1 2 3 4 5 6 7 col payoff Action embeddings ( PCA ) 9 0 Action to plays 1 012 3 456 7 89 012 34567 89 0123 4567 89 0 1 0123 4567 89 2 1.5 Action embeddings ( PCA ) 1.0 block 0 block 1 block 2 block 3 0.5 0.0 −0.5 −1.0 0 −2 0 action 0 5 0 5 0 5 0 5 0 0 1 2 3 4 5 6 7 8 9 payoff −1 action −2 −1 0 1 2 0 1 2 3 4 5 6 7 8 9 Payoff block 0 block 1 block 4 0 block 5 1 block 6 2 block 7 2 4 6 −2 0 2 4 6 −2 0 2 4 6 −2 0 2 1 0 2 5 3 4 5 0 6 5 7 0 5 0 5 3 1 r e y a p l 2 3 4 5 6 4 7 8 9 6 −2 0 2 4 6 −2 0 2 4 6 −2 0 2 4 6 −2 0 2 4 6 block 4 block 5 block 6 block 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 Blotto [ NE Gap = 3.48249e-05 ] 0 r e y a p l 1 r e y a p l 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 3 2 1 0 0 1 2 3 4 5 6 7 8 9 −1 payoff −1 0 1 2 Approx . NE Action to actions 0 0 1 2 3 0 r e y a p l 0 2 4 6 8 0 1 2 1 4 1 0 2 4 6 8 0 1 2 1 4 1 0 2 4 6 8 0 1 2 1 4 1 0 2 4 6 8 1 2 3 4 5 6 7 0 1 8 2 1 4 1 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 0 1 2 1 4 1 0 1 2 1 4 1 0 1 2 1 4 1 0 1 2 1 4 1 0 1 2 3 4 5 6 7 01 2345 6789 012 3456 789 012 3456 789 0123 4567 89 Action embeddings ( PCA ) 9 10 12 14 0 1 2 3 4 5 6 7 col payoff 0 1 234 5 678 9 0 1 2345 6789 01 2345 6789 0 1 01 2345 6789 2 1.5 Action embeddings ( PCA ) 1.0 block 0 block 1 block 2 block 3 0.5 0.0 −0.5 −1.0 block 4 block 5 block 6 block 7 0 5 0 5 0 5 0 5 0 1 0 2 5 3 4 5 0 6 5 7 0 5 0 5 −2 0 2 4 6 −2 0 2 4 6 −2 0 2 4 6 −2 0 2 action action player 0 1 2 3 4 5 6 7 8 9 0 1 1 r e y a p l 0 1 2 3 4 5 6 4 7 8 9 6 −2 0 2 4 6 −2 0 2 4 6 −2 0 2 4 6 −2 0 2 4 6 player 0 Payoff 1 0 1 2 1 Action to plays block 3 3 0 w o 4 r block 2 5 r e y a l p 1 w o r 6 7 0 1 2 3 4 5 6 7 player 0 Payoff 1 0 1 2 3 0 w o 4 r block 2 5 r e y a l p 1 w o r 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 0 1 2 3 4 5 6 7 0 2 4 6 8 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 2 4 6 8 0 r e y a p l 1 r e y a p l 0 r e y a p l 1 r e y a p l 0 r e y a p l 1 r e y a p l 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 Published as a conference paper at ICLR 2024 ( a ) a1 1 ( b ) a2 1 ( c ) a1 2 ( d ) a2 2 Figure 9 : NfgTransformer action-embeddings over the set of nontrivial 2×2 equilibrium-invariant normal-form games , when trained with an NE objective . The embeddings found closely follow the equilibrium boundaries ( dark lines ) . Symmetries over the space of games are respected . Symmetric games ( bottom-left to top-right diagonal ) have the same embeddings between players . Permutations over players ( folding over the bottom-left to top-right diagonal ) are consistent . Colorbar : [ −11.58 +11.56 ] . 18 Published as a conference paper at ICLR 2024 a1 2 1 , 1 0 , 0 a2 2 0 , 0 1 , 1 ( b ) Coord a1 2 0 , 0 1 , 1 a2 2 1 , 1 0 , 0 a1 1 a2 1 a1 1 a2 1 a1 2 1 , 0 0 , 1 a2 2 0 , 1 1 , 0 ( c ) Cycle a1 2 0 , 1 1 , 0 a2 2 1 , 0 0 , 1 a1 1 a2 1 a1 1 a2 1 ( d ) Anti Coord ( e ) Anti Cycle A a1 a2 a1 9.415 9.415 a2 9.415 9.415 A a1 a2 a1 -2.497 -2.497 a2 -2.497 -2.497 ( f ) Coord Embedding ( g ) Cycle Embedding ( a ) ln ( dmin ( Ai , G ) ) Figure 10 : Subfigure 10a shows the distance to the nearest other game embedding . The embed- dings produced by NfgTransformer uniquely describe the 2×2 game apart from two edge cases . and Two Coordination games ( ) have identical embeddings , each because there are strategically equivalent . Colorbar : [ 0.0 ) have identical embeddings , and two Cycle games ( and 4.675 ] . When a game is symmetric , G1 ( a1 , a2 ) = G2 ( a2 , a1 ) , the embeddings between players are equal . We can verify this by studying the bottom-left to top-right diagonal . When swapping the player orders , we expect the embeddings to be swapped . Swapping players is equivalent to folding over the same diagonal . Again , we can visually verify that the embeddings are swapped . ( cid:80 ) p∈ [ 1,2 ] ap∈ [ 1,2 ] ( Ai Next , we turn to the question of when the embeddings uniquely describe a game . We de- fine a distance metric between action embeddings for game i and game j , d ( Ai , Aj ) = ( ( cid:80 ) 2 , where i , j ∈ G are games sampled from a grid , which describes how close the embeddings of two games are to each other . We can also define the distance to the nearest other game within the set of considered games , dmin ( Ai , G ) = minj̸=i∈G d ( Ai , Aj ) . Using these distance metrics we can verify that dmin ( Ai , Aj ) > 0 apart from games with strategi- cally equivalent actions ( Figure 10 ) . p ( ap ) − Aj p ( ap ) ) 2 ) 1 Coordination game has identical action embeddings to the The Anti-Coordination game . In this case , due to permutation equivariance , the embedding for each action , in natural language , is : “ there is an action that the opponent can play which will give us both identical high payoff , and there is an action that the opponent can play which will give us both identical low payoff ” . Due to the equivariant property it is not possible to disambiguate between these games from the embeddings alone . By initializing the network with action labels , hinting a reconstruction method with a row of true payoffs , or permuting the payoffs by a tiny amount , would all enable disambiguation . The last strategy can be seen from the figure where slightly biased coordination games all have positive distance to their nearest other game embedding . Similarly , the Cycle game ( also known as matching pennies ) has identical embeddings to the Anticlockwise Cycle game . In this case , the embedding is “ there is an action that the opponent can play which will me a high positive payoff and the opponent a high negative payoff , and there is an action that the opponent can play which will give me a high negative payoff and the opponent a high positive payoff ” . Note that the Coordination and Cycle game have distinct embeddings . These are the only 4 points in the space that can only by disambiguated up to handedness . These appear with measure zero in the equilibrium-invariant subspace . 19 Published as a conference paper at ICLR 2024 Overall , the embeddings neatly describe and predict the known structure of 2×2 games . The the- oretically predicted properties , including permutation symmetries and NE , are reproduced in this experiment . 20","['e', 'g', 'c', 'v', 'r', 'publish', 'conference', 'paper', 'iclr', 'nfgtransformer', 'equivariant', 'representation', 'learn', 'normalform', '♢', '♢', 'gemp†', 'deepmind', 'liusiqi', 'marris', 'gpil', 'imgemp', 'abstract', 'normalform', 'game', 'fundamental', 'model', 'strategic', 'interaction', 'study', 'representation', 'use', 'neural', 'network', 'describe', 'inherent', 'equivariance', 'nfgs', 'permutation', 'strategy', 'describe', 'equivalent', 'game', 'well', 'challenge', 'pose', 'representation', 'learn', 'propose', 'nfgtransformer1', 'architecture', 'leverage', 'equivariance', 'lead', 'stateoftheart', 'performance', 'range', 'gametheoretic', 'task', 'include', 'equilibriumsolve', 'deviation', 'gain', 'estimation', 'ranking', 'common', 'ap', 'proach', 'representation', 'show', 'result', 'model', 'interpretable', 'versatile', 'pave', 'way', 'deep', 'learning', 'system', 'capable', 'game', 'theoretic', 'reasoning', 'interact', 'human', 'introduction', 'represent', 'datum', 'learned', 'embed', 'space', 'representation', 'learning', 'time', 'less', 'idea', 'deep', 'learn', 'foundational', 'representation', 'learning', 'architecture', 'vaswani', 'provide', 'performance', 'general', 'ity', 'bring', 'researcher', 'use', 'work', 'expert', 'system', 'target', 'narrow', 'domain', 'consider', 'convolutional', 'neural', 'network', 'cnn', 'exploit', 'translation', 'invariance', 'inherent', 'image', 'cnn', 'replace', 'feature', 'descriptor', 'underpin', 'task', 'diverse', 'image', 'classification', 'segmentation', 'inpainte', 'lead', 'paradigm', 'shift', 'field', 'area', 'research', 'resist', 'trend', 'game', 'theory', 'see', 'strike', 'similar', 'itie', 'classical', 'computer', 'vision', 'research', 'first', 'active', 'research', 'topic', 'equilibrium', 'solve', 'rank', 'balduzzi', 'social', 'choice', 'theory', 'anshele', 'population', 'learn', 'lanctot', 'focus', 'specialise', 'solution', 'share', 'common', 'language', 'normalform', 'game', 'nfgs', 'second', 'lution', 'suffer', 'fundamental', 'limitation', 'popular', 'elo', 'rank', 'elo', 'instance', 'assign', 'scalar', 'rating', 'player', 'derive', 'nfg', 'player', 'elo', 'rating', 'design', 'predictive', 'match', 'outcome', 'poor', 'predictor', 'transitive', 'game', 'elo', 'score', 'simply', 'restrictive', 'representation', 'reflect', 'cyclic', 'game', 'dynamic', 'provement', 'elo', 'follow', 'bertrand', 'rely', 'engineer', 'feature', 'descriptor', 'instead', 'learn', 'equilibriumsolve', 'compute', 'exact', 'nash', 'equilibrium', 'intractable', 'twoplayer', 'daskalaki', 'yet', 'approximate', 'solver', 'nondifferentiable', 'take', 'nondeterministic', 'amount', 'time', 'converge', 'struggle', 'parallelise', 'fail', 'fun', 'damental', 'limitation', 'indirect', 'consequence', 'entire', 'line', 'work', 'reinforcement', 'learning', 'littman', 'wellman', 'rely', 'equilibrium', 'solving', 'part', 'learning', 'rule', 'nfg', 'construct', 'agent', 'qtable', 'state', 'equilibrium', 'inform', 'subsequent', 'policy', 'update', 'unfortunately', 'vive', 'idea', 'context', 'deep', 'rl', 'challenge', 'impossible', 'require', 'equilibrium', 'solve', 'subroutine', 'gradient', 'update', 'indeed', 'first', 'recognise', 'limitation', 'several', 'recent', 'work', 'incorporate', 'repre', 'sentation', 'learn', 'implicitly', 'narrow', 'domain', 'vadori', 'address', 'limitation', 'explicitly', 'generality', 'goal', 'develop', 'prin', 'ciple', 'generalpurpose', 'representation', 'nfgs', 'use', 'wide', 'range', 'gametheoretic', '1the', 'model', 'opensource', 'https', 'publish', 'conference', 'paper', 'iclr', 'application', 'ask', 'property', 'leverage', 'efficient', 'representation', 'learning', 'nfgs', 'loss', 'generality', 'expect', 'performance', 'range', 'gametheoretic', 'task', 'use', 'common', 'approach', 'represent', 'nfgs', 'answer', 'question', 'affirmatively', 'propose', 'nfgtransformer', 'generalpurpose', 'representation', 'learn', 'architecture', 'stateoftheart', 'performance', 'task', 'diverse', 'equilibrium', 'solve', 'deviation', 'gain', 'estimation', 'ranking', 'basic', 'form', 'strategic', 'interaction', 'player', 'formulate', 'nfgs', 'player', 'simultaneously', 'select', 'action', 'receive', 'payoff', 'subject', 'joint', 'action', 'strategic', 'interaction', 'therefore', 'present', 'payoff', 'tensor', 'value', 'player', 'tabulate', 'joint', 'tion', 'tabular', 'view', 'strategic', 'interaction', 'present', 'challenge', 'representation', 'learn', 'modality', 'image', 'text', 'spatial', 'structure', 'exploit', 'efficient', 'representation', 'learn', 'lecun', 'position', 'action', 'payoff', 'tensor', 'unimportant', 'permute', 'payoff', 'matrix', 'nfg', 'yield', 'equiva', 'lent', 'game', 'equivalency', 'know', 'strongly', 'isomorphic', 'game', 'gabarr´o', 'inherent', 'equivariance', 'inspire', 'prior', 'work', 'compose', 'orderinvariant', 'pooling', 'function', 'neural', 'network', 'architecture', 'efficiency', 'expense', 'erality', 'representation', 'aim', 'leverage', 'inherent', 'equivariance', 'nfgs', 'preserve', 'full', 'generality', 'learn', 'representation', 'imply', 'several', 'desiderata', 'discuss', 'turn', 'first', 'representation', 'need', 'versatile', 'allow', 'inquiry', 'level', 'individual', 'action', 'jointaction', 'perplayer', 'entire', 'game', 'second', 'need', 'equivariant', 'peraction', 'inquiry', 'output', 'action', 'exchange', 'position', 'exchange', 'payoff', 'tensor', 'third', 'embed', 'function', 'assume', 'outcome', 'jointaction', 'observe', 'representation', 'accommodate', 'incomplete', 'nfgs', 'principled', 'way', 'fourth', 'function', 'apply', 'game', 'different', 'size', 'imply', 'number', 'network', 'parameter', 'independent', 'size', 'game', 'hartford', 'finally', 'desirable', 'network', 'architecture', 'interpretable', 'allow', 'inspection', 'different', 'stage', 'network', 'rest', 'paper', 'show', 'propose', 'encoder', 'architecture', 'nfgtransformer', 'satisfy', 'desiderata', 'simultaneously', 'key', 'idea', 'approach', 'consider', 'embed', 'function', 'class', 'represent', 'nfg', 'action', 'embedding', 'reverse', 'generative', 'process', 'action', 'payoff', 'action', 'embedding', 'suitably', 'compose', 'answer', 'question', 'different', 'granularity', 'allow', 'equivariance', 'straightforward', 'way', 'permutation', 'action', 'player', 'payoff', 'tensor', 'reflect', 'permutation', 'action', 'embedding', 'ar', 'nfgtransformer', 'competitive', 'candidate', 'generalpurpose', 'equivariant', 'representation', 'learning', 'nfgs', 'bridge', 'gap', 'deep', 'learning', 'gametheoretic', 'reasoning', 'background', 'normalform', 'game', 'fundamental', 'game', 'formalism', 'player', 'receive', 'payoff', 'gp', 'r', 'p', 'neously', 'play', 'action', 'function', 'joint', 'action', '∈', 'n', 'player', 'let', 'ap1', '∈', 'a¬p', 'action', 'player', 'let', 'σ', 'denote', 'probability', 'player', 'play', 'joint', 'action', 'probability', 'distribution', 'space', 'joint', 'action', 'pure', 'strategy', 'action', 'distribution', 'deterministic', 'mixed', 'strategy', 'stochastic', 'value', 'player', 'p', 'give', 'gp', 'refer', 'payoff', 'tensor', 'tabulate', 'accord', 'action', 'player', 'order', 'nash', 'equilibrium', 'mixed', 'joint', 'strategy', 'factorise', 'outer', 'product', 'marginal', 'p', 'unilateral', 'deviation', 'incentive', 'define', 'e', 'a∼σ', 'factorisable', 'mixed', 'joint', 'strategy', 'p', 'ϵne', 'maxp', 'δp', 'ϵ', 'refer', 'quantity', 'gap', 'intuitively', 'measure', 'distance', 'σ', 'game', 'mixedstrategy', 'guarantee', 'exist', 'finite', 'game', 'nash', 'exactly', 'compute', 'normalform', 'ppadcomplete', 'daskalaki', 'deterministic', 'equivalently', 'δ', 'define', 'maximum', 'deviation', 'gain', 'joint', 'purestrategy', 'purestrategy', 'ne', 'p', 'gp', 'gp', 'publish', 'conference', 'paper', 'iclr', 'permutation', 'equivariance', 'consider', 'strong', 'isomorphism', 'g′', 'τp', 'ω', 'τp', 'ai', 'p', 'player', 'action', 'permutation', 'ω', 'p', 'p′', 'player', 'permutation', 'element', 'transform', 'game', 'g', 'therefore', 'give', 'p', 'ai′', 'ω', 'p', 'cid0', 'τω', 'τω', 'cid1', 'gp', 'encoder', 'g', 'say', 'equivariant', 'p', 'p', 'action', 'embedding', 'player', 'g', 'ω', 'p', 'τp', 'a1', 'p', 'τp', 'p', 'slightly', 'abuse', 'notation', 'τp', 'operate', 'action', 'embedding', 'intuitively', 'permutation', 'equivariance', 'imply', 'commute', 'adopt', 'convention', 'player', 'permutation', 'ω', 'apply', 'player', 'action', 'permutation', 'τp', '∀p', 'multihead', 'attention', 'describe', 'self', 'crossattention', 'qkv', 'mechanism', 'become', 'ubiquitous', 'thank', 'generality', 'potential', 'scale', 'vaswani', 'operation', 'extend', 'basic', 'qkv', 'attention', 'mechanism', 'follow', 'attention', 'q', 'k', 'softmax', 'cid18', 'cid19', 'query', 'pair', 'concep', 'tually', 'attention', 'mechanism', 'output', 'weighted', 'sum', 'nk', 'value', 'query', 'vector', 'weight', 'determine', 'pairwise', 'dot', 'product', 'key', 'query', 'vector', 'output', 'shape', 'input', 'qkv', 'output', 'fullyconnected', 'network', 'q', 'function', 'k', 'project', 'input', 'rnk×dxkv', 'attention', 'operation', 'orderinvariant', 'respect', 'equivariant', 'respect', 'key', 'property', 'leverage', 'design', 'nfgtransformer', 'achieve', 'permutation', 'equivariance', 'property', 'refer', 'attention', 'layer', 'selfattention', 'crossattention', 'practice', 'attention', 'layer', 'h', 'attention', 'head', 'perform', 'attention', 'operation', 'equation', 'parallel', 'enable', 'attention', 'layer', 'aggregate', 'multiple', 'stream', 'information', 'forward', 'pass', 'equivariant', 'game', 'representation', 'informally', 'motivate', 'need', 'equivariant', 'embed', 'function', 'formally', 'state', 'practical', 'implication', 'equivariant', 'embed', 'function', 'follow', 'general', 'theorem', 'condition', 'action', 'identical', 'embedding', 'give', 'equivariant', 'embed', 'function', 'conciseness', 'defer', 'formal', 'statement', 'proof', 'appendix', 'proposition', 'repeat', 'action', 'ai', 'equivariant', 'ai', 'proposition', 'player', 'symmetry', 'player', 'p', 'q', 'deterministic', 'equiv', 'ariant', 'identical', 'permutation', 'p', 'a¬p', '∀a¬p', 'deterministic', 'p', 'follow', 'ai', 'p', 'aj', 'p', 'aj', 'p', 'aj', 'p', 'proposition', 'guarantee', 'construction', 'repeat', 'action', 'treat', 'identically', 'stream', 'application', 'proposition', 'guarantee', 'player', 'symmetry', 'reflect', 'action', 'embed', 'space', 'show', 'empirically', 'section', 'nfgtransformer', 'nfgtransformer', 'describe', 'nfgtransformer', 'encoder', 'network', 'factorise', 'payoff', 'tensor', 'g', 'tion', 'embedding', 'sequence', 'block', 'figure', 'top', 'compose', 'sequence', 'self', 'crossattention', 'operation', 'figure', 'bottom', 'show', 'concrete', 'exam', 'ple', 'decoder', 'loss', 'function', 'figure', 'several', 'gametheoretic', 'task', 'different', 'decode', 'granularity', 'show', 'generality', 'approach', 'finally', 'discuss', 'nfgtransformer', 'naturally', 'incorporate', 'incomplete', 'payoff', 'tensor', 'leverage', 'flexibility', 'attention', 'operation', 'publish', 'conference', 'paper', 'iclr', 'figure', 'overview', 'nfgtransformer', 'payoff', 'tensor', 'encode', 'action', 'embedding', 'p', 'top', 'action', 'embedding', 'zeroinitialise', 'iteratively', 'update', 'sequence', 'block', 'bottom', 'arrow', 'label', 'q', 'kv', 'orig', 'inate', 'set', 'input', 'query', 'keyvalue', 'terminate', 'set', 'output', 'dash', 'box', 'denote', 'unordered', 'set', 'element', 'specific', 'type', 'cardinality', 'initialisation', 'iterative', 'refinement', 'permutation', 'equivariance', 'imply', 'action', 'embedding', 'agnostic', 'player', 'tion', 'order', 'payoff', 'tensor', 'suggest', 'action', 'embedding', 'player', 'action', 'initialise', 'identically', 'zeroinitialise', 'action', 'embedding', 'p', 'rd', 'initialisation', 'action', 'embedding', 'iteratively', 'refined', 'n', 'sequence', 'block', 'give', 'current', 'action', 'embedding', 'payoff', 'tensor', 'block', 'return', 'update', 'action', 'embedding', 'self', 'crossattention', 'operation', 'describe', 'turn', 'actiontojointaction', 'selfattention', 'represent', 'play', 'action', 'jointaction', 'a¬p', 'give', 'payoff', 'value', 'player', 'recall', 'definition', 'selfattention', 'operation', 'gp', 'yield', 'vector', 'output', 'action', 'jointaction', 'refer', 'output', 'play', 'joint', 'action', 'payoff', 'actiontoplay', 'crossattention', 'encode', 'information', 'play', 'action', 'singleton', 'output', 'p', 'p', 'singleton', 'query', 'operation', 'yield', 'p', 'function', 'play', 'input', 'action', 'embed', 'vector', 'p', 'a¬p', '∀a¬p', 'actiontoaction', 'selfattention', 'represent', 'action', 'give', 'action', 'embedding', 'p', 'ablate', 'operation', 'vary', 'section', 'show', 'benefit', 'propagate', 'information', 'action', 'embedding', 'block', 'equivariance', 'preserve', 'give', 'keyvalue', 'orderinvariance', 'property', 'attention', 'operation', 'output', 'embed', 'p', 'function', 'embed', 'input', 'unordered', 'set', 'play', 'unordered', 'set', 'action', 'embedding', 'taskspecific', 'decode', 'result', 'action', 'embedding', 'use', 'variety', 'downstream', 'task', 'different', 'decode', 'granularity', 'describe', 'empirically', 'demonstrate', 'usecase', 'specific', 'figure', 'initial', 'action', 'embedding', 'final', 'action', 'embedding', 'nfgtransformerblock', 'actiontoplay', 'actiontoaction', 'qkv', 'qkv', 'jointaction', 'action', 'rearrange', 'action', 'jointaction', 'r', 'c', 'q', 'n', 'action', 'nfgtransformerblock', 'publish', 'conference', 'paper', 'iclr', 'figure', 'example', 'taskspecific', 'decoder', 'loss', 'generalpurpose', 'action', 'embedding', 'nash', 'equilibriumsolve', 'action', 'distribution', 'player', 'softmax', 'w1', 'action', 'toend', 'gradient', 'descent', 'require', 'decode', 'level', 'action', 'estimate', 'marginal', 'p', 'p', 'follow', 'minimise', 'loss', 'function', 'maxp', 'δp', 'end', 'ˆδp', 'δp', 'maxa′', 'gp', 'p', 'gp', 'p', 'mlp', 'p', 'deviationgain', 'estimation', 'decode', 'scalar', 'estimate', 'jointaction', '∀a', '∈', 'represent', 'joint', 'action', 'estimate', 'max', 'imum', 'deviation', 'gain', 'minimise', 'maxp', 'δp', 'ˆδ', 'maxp', 'δp', 'maxa′', 'gp', 'deviation', 'gain', 'player', 'p', 'joint', 'action', 'p∈ap', 'gp', 'payoff', 'reconstruction', 'decode', 'scalar', 'payoff', 'value', 'gp', '∀p', '∀a', '∈', 'use', 'selfattention', 'operation', 'decode', 'similar', 'actiontojointaction', 'selfattention', 'operation', 'encoder', 'append', 'action', 'embedding', 'payoff', 'value', 'reconstruct', 'compute', 'reconstruction', 'loss', 'minimise', 'gp', 'ˆgp', 'function', 'action', 'embed', 'unordered', 'set', 'coplayer', 'action', 'embedding', 'represent', 'incomplete', 'game', 'assume', 'thus', 'far', 'outcome', 'jointaction', 'game', 'observe', 'always', 'case', 'practice', 'cost', 'evaluate', 'jointaction', 'prohibitive', 'instead', 'infer', 'outcome', 'jointaction', 'give', 'incomplete', 'elo', 'slight', 'modification', 'nfgtransformer', 'accommodate', 'usecase', 'principled', 'way', 'extend', 'vanilla', 'attention', 'implementation', 'equation', 'allow', 'additional', 'binary', 'mask', 'vector', 'nk', 'query', 'input', 'indicate', 'validity', 'equation', 'define', 'mask', 'attention', 'operation', '∞', 'maskedattention', 'q', 'k', 'softmax', 'cid18', '⊗', 'cid19', 'represent', 'incomplete', 'use', 'equation', 'lieu', 'equation', 'self', 'cross', 'attention', 'operation', 'set', 'masking', 'vector', 'accordingly', 'instance', 'jointaction', 'unob', 'serve', 'set', 'reflect', 'validity', 'play', 'actiontoplay', 'operation', 'nfgtransformer', 'architecture', 'highly', 'effective', 'represent', 'action', 'incomplete', 'game', 'predict', 'payoff', 'unobserved', 'jointaction', 'show', 'empirically', 'section', 'taskspeciﬁc', 'loss', 'function', 'n', 'player', 'jointaction', 'estimation', 'jointaction', 'qkv', 'self', 'attention', 'jointaction', 'action', 'nfgtransformer', 'nash', 'equilibrium', 'solve', 'payoﬀ', 'reconstruction', 'publish', 'conference', 'paper', 'iclr', 'result', 'goal', 'empirical', 'study', 'threefold', 'first', 'compare', 'nfgtransformer', 'baseline', 'architecture', 'demonstrate', 'improved', 'performance', 'diverse', 'downstream', 'task', 'synthetic', 'standard', 'game', 'second', 'vary', 'model', 'hyperparameter', 'observe', 'affect', 'mance', 'show', 'particular', 'task', 'require', 'large', 'action', 'embed', 'size', 'benefit', 'round', 'iterative', 'refinement', 'lastly', 'study', 'model', 'learn', 'solve', 'certain', 'task', 'interpret', 'sequence', 'learn', 'attention', 'mask', 'control', 'setting', 'result', 'reveal', 'solution', 'find', 'model', 'reflect', 'element', 'intuitive', 'solution', 'solve', 'game', 'completeness', 'discuss', 'additional', 'empirical', 'result', 'instance', 'show', 'nontrivial', 'equilibriuminvariant', 'game', 'embed', 'follow', 'structure', 'know', 'embed', 'space', 'propose', 'synthetic', 'game', 'table', 'compare', 'nfgtransformer', 'baseline', 'architecture', 'synthetic', 'game', 'configura', 'tion', 'average', 'independent', 'run', 'nfgtransformer', 'variant', 'annotate', 'variant', 'corresponding', 'hyperparameter', 'k', 'show', 'figure', 'provide', 'train', 'ing', 'curve', 'confidence', 'interval', 'parameter', 'count', 'configuration', 'model', 'n2', 't16', 'equivariantmlp', 'gap', 't64', 't16', 'mse', 'n2', 't16', 'n3', 't16', 'first', 'evaluate', 'variation', 'nfgtransformer', 'architecture', 'synthetic', 'game', 'vary', 'size', 'equilibriumsolving', 'deviation', 'gain', 'estimation', 'generate', 'synthetic', 'game', 'broad', 'coverage', 'follow', 'sample', 'game', 'equilibriuminvariant', 'subspace', 'cover', 'strategic', 'interaction', 'affect', 'equilibrium', 'solution', 'game', 'payoff', 'tensor', 'zeromean', 'player', 'strategy', 'compare', 'result', 'baseline', 'mlp', 'network', 'number', 'parameter', 'least', 'large', 'transformer', 'variant', 'parameter', 'equivariant', 'mlp', 'network', 'rearrange', 'action', 'descend', 'order', 'average', 'payoff', 'well', 'ne', 'network', 'design', 'equilibriumsolving', 'see', 'b1', 'detail', 'game', 'pling', 'network', 'architecture', 'parameter', 'count', 'model', 'note', 'parameter', 'count', 'nfgtransformer', 'independent', 'game', 'size', 'desideratum', 'hartford', 'solve', 'equilibrium', 'equilibrium', 'solve', 'optimise', 'variant', 'nfgtransformer', 'minimise', 'gap', 'maxp', 'δp', 'figure', 'leave', 'table', 'leave', 'show', 'result', 'equivariantmlp', 'outper', 'form', 'demonstrate', 'importance', 'leverage', 'equivariance', 'inherent', 'nfgs', 'remain', 'ineffective', 'solve', 'task', 'ne', 'equivariant', 'construction', 'significantly', 'outperform', 'mlp', 'variant', 'setting', 'trail', 'behind', 'player', 'game', 'nfgtransformer', 'also', 'equivariant', 'construction', 'learn', 'capture', 'relevant', 'information', 'handcraft', 'payoff', 'feature', 'vector', 'nfgtransformer', 'variant', 'parameter', 'count', 'baseline', 'significantly', 'outperform', 'game', 'size', 'publish', 'conference', 'paper', 'iclr', 'figure', 'payoff', 'prediction', 'error', 'average', 'player', 'unobserved', 'jointaction', 'result', 'average', 'randomly', 'sample', 'empirical', 'disc', 'game', 'game', 'configuration', 'nfgtransformer', 'variant', 'result', 'show', 'clear', 'trend', 'increase', 'number', 'trans', 'former', 'block', 'improve', 'performance', 'especially', 'game', 'become', 'large', 'make', 'intuitive', 'sense', 'add', 'number', 'time', 'action', 'embedding', 'usefully', 'refine', 'action', 'embedding', 'end', 'iteration', 'become', 'relevant', 'query', 'next', 'contrast', 'benefit', 'increase', 'action', 'embed', 'size', 'mute', 'hypothesise', 'equilibriumsolve', 'information', 'relate', 'subset', 'available', 'action', 'often', 'ignore', 'iterative', 'refinement', 'dominate', 'action', 'contribute', 'final', 'equilibrium', 'solution', 'lastly', 'evaluate', 'nfgtransformer', 'variant', 'perform', 'actiontoaction', 'selfattention', 'case', 'action', 'embedding', 'player', 'interact', 'block', 'performance', 'markedly', 'bad', 'particular', 'interest', 'comparison', 'variant', 'demonstrate', 'bene', 'fit', 'complex', 'game', 'size', '×', 'small', 'game', 'suggest', 'actiontoaction', 'selfattention', 'facilitate', 'learn', 'especially', 'task', 'require', 'iterative', 'reasoning', 'estimate', 'maximum', 'deviation', 'gain', 'related', 'task', 'determine', 'maximum', 'incentive', 'player', 'deviate', 'joint', 'purestrategy', 'equivalently', 'jointaction', 'quantity', 'informative', 'stability', 'joint', 'behaviour', 'particular', 'joint', 'purestrategy', 'maximum', 'deviation', 'gain', 'δ', 'definition', 'find', 'purestrategy', 'ne', 'optimise', 'nfgtransformer', 'network', 'regress', 'maximum', 'deviationgain', 'δ', 'joint', 'purestrategy', 'use', 'joint', 'action', 'decoder', 'architecture', 'figure', 'middle', 'report', 'regression', 'loss', 'mean', 'squared', 'error', 'different', 'architecture', 'variant', 'table', 'right', 'observe', 'ne', 'consistently', 'outperform', 'mlp', 'baseline', 'underperform', 'nfgtransformer', 'variant', 'size', 'game', 'increase', 'similar', 'observation', 'section', 'number', 'transformer', 'block', 'play', 'role', 'transformer', 'variant', 'final', 'performance', 'however', 'long', 'main', 'factor', 'instead', 'actionembedde', 'size', 'become', 'critical', 'variant', 'high', 'embed', 'size', 'competitive', 'even', 'shallow', 'model', '×', '×', 'game', 'explain', 'lack', 'structure', 'underlie', 'game', 'payoff', 'tensor', 'cover', 'full', 'equilibriuminvariant', 'subspace', 'payoff', 'jointaction', 'provide', 'information', 'outcome', 'perform', 'well', 'model', 'learn', 'memorise', 'outcome', 'different', 'jointaction', 'reduce', 'action', 'embed', 'size', 'become', 'bottleneck', 'payoff', 'prediction', 'empirical', 'disc', 'game', 'game', 'class', 'follow', 'structured', 'generative', 'process', 'often', 'case', 'practice', 'useful', 'representation', 'learning', 'method', 'capture', 'latent', 'structure', 'turn', 'disc', 'game', 'balduzzi', 'evaluate', 'efficacy', 'nfgtransformer', 'case', 'compare', 'several', 'payoff', 'prediction', 'method', 'rank', 'literature', 'definition', 'disc', 'game', 'let', 'ut', 'winprobability', 'action', 'play', 'define', 'ut', 'pji', 'definition', 'describe', 'class', 'twoplayer', 'game', 'outcome', 'define', 'latent', 'vector', 'rz', 'generalise', 'original', 'definition', 'disc', 'game', 'balduzzi', 'allow', 'latent', 'vector', 'z', 'payoff', 'value', 'jointaction', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'r', 'r', 'r', 'e', 'e', 'r', 'q', 'e', 'e', 'r', 'r', 'r', 'e', 'e', 'r', 'q', 'e', 'l', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'r', 'u', 'publish', 'conference', 'paper', 'iclr', 'figure', 'visualisation', 'attention', 'mask', 'actionembedding', 'inference', 'time', 'instance', 'oligopoly', 'game', 'payoff', 'tensor', 'show', 'left', 'infer', 'strategy', 'profile', 'show', 'right', 'gap', 'equilibrium', 'purestrategie', 'player', 'show', 'red', 'sequence', 'actiontoplay', 'attention', 'mask', 'pca', 'reduce', 'actionembedding', 'end', 'transformer', 'block', 'show', 'middle', 'therefore', 'become', 'informative', 'predict', 'outcome', 'amount', 'information', 'infer', 'jointaction', 'know', 'control', 'latent', 'vector', 'dimension', 'problem', 'set', 'follow', 'give', 'access', 'empirical', 'game', 'outcome', 'jointaction', 'observe', 'probability', 'goal', 'accurately', 'predict', 'outcome', 'player', 'unobserved', 'jointaction', 'rich', 'body', 'literature', 'dedicate', 'solve', 'task', 'owe', 'relevance', 'realworld', 'go', 'chess', 'player', 'classically', 'assign', 'elo', 'rating', 'design', 'competition', 'predictive', 'winprobability', 'ranked', 'player', 'several', 'improvement', 'elo', 'propose', 'recognise', 'many', 'limitation', 'elo', 'compare', 'result', 'method', 'elo', 'elo', 'balduzzi', 'different', 'setting', 'disc', 'game', 'figure', 'show', 'result', 'mse', 'average', 'unobserved', 'jointaction', 'nfgtransformer', 'outperform', 'baseline', 'significantly', 'setting', 'particular', 'nfgtransformer', 'recover', 'latent', 'variable', 'game', 'z', 'near', 'perfectly', 'soon', 'jointaction', 'observe', 'error', 'rate', 'order', 'magnitude', 'low', 'second', 'good', 'method', 'result', 'particularly', 'remarkable', 'baseline', 'method', 'design', 'disc', 'game', 'mind', 'nfgtransformer', 'z', 'nfgtransformer', 'continue', 'outperform', 'prediction', 'accuracy', 'degrade', 'gracefully', 'joint', 'action', 'observe', 'result', 'suggest', 'nfgtransformer', 'highly', 'effectively', 'recognise', 'exploit', 'latent', 'structure', 'game', 'exist', 'provide', 'detail', 'game', 'sample', 'mask', 'network', 'architecture', 'baseline', 'implementation', 'nfgtransformer', 'outcome', 'unobserved', 'jointaction', 'mask', 'follow', 'procedure', 'describe', 'section', 'training', 'time', 'model', 'minimise', 'reconstruction', 'loss', 'jointaction', 'interpret', 'nfgtransformer', 'nfgtransformer', 'explicitly', 'reason', 'actionembedde', 'structured', 'attention', 'mechanism', 'allow', 'inspection', 'stage', 'iterative', 'refinement', 'process', 'exploit', 'property', 'investigate', 'model', 'implement', 'gametheoretic', 'algorithm', 'equilibriumsolve', 'suite', 'gamut', 'game', 'represent', 'diverse', 'strategic', 'interaction', 'porter', 'classically', 'study', 'economic', 'game', 'theory', 'optimise', 'nfgtransformer', 'network', 'section', 'focus', 'game', 'remove', 'action', 'selfattention', 'concise', 'visualisation', 'instead', 'attend', 'multiple', 'piece', 'information', 'parallel', 'attention', 'layer', 'implement', 'single', 'attention', 'head', 'block', 'action', 'play', 'bertrandoligopoly', 'd01', 'nashconv', 'action', 'action', 'action', 'embedding', 'pca', 'marginal', 'c', 'c', 'payoff', 'r', 'r', 'e', 'l', 'p', 'col', 'payoff', 'approx', 'r', 'e', 'p', 'l', 'payoff', 'block', 'block', 'block', 'block', 'block', 'block', 'action', 'play', 'w', 'block', 'r', 'r', 'e', 'p', 'l', 'r', 'e', 'p', 'l', 'payoff', 'r', 'e', 'p', 'l', 'r', 'e', 'p', 'l', 'block', 'block', 'block', 'block', 'block', 'block', 'action', 'embedding', 'pca', 'block', 'block', '−10', '−2', 'action', 'r', 'e', 'p', 'l', '−2', '−2', '−2', '−2', '−2', '−2', '−2', '−2', 'gap', 'action', 'player', 'publish', 'conference', 'paper', 'iclr', 'h', 'show', 'c', 'simplify', 'architecture', 'sufficient', 'approximate', 'game', 'size', 'reasonable', 'accuracy', 'visualise', 'network', 'game', 'figure', 'illustrate', 'instance', 'iterative', 'solve', 'game', 'bertrand', 'oligopoly', 'action', 'nfgtransformer', 'successfully', 'identify', 'purestrategy', 'ne', 'right', 'gap', 'topright', 'inspect', 'payoff', 'tensor', 'leave', 'verify', 'player', 'play', 'action', 'player', 'counter', 'action', 'indeed', 'purestrategy', 'solution', 'reflect', 'underlie', 'attention', 'mechanism', 'indeed', 'actiontoplay', 'attention', 'mask', 'top', 'appear', 'converge', 'recur', 'pattern', 'action', 'attend', 'equilibriumstrategy', 'coplayer', 'remarkable', 'reason', 'first', 'definition', 'imply', 'indifference', 'action', 'choice', 'coplayer', 'implement', 'equilibrium', 'strategy', 'focal', 'player', 'indifferent', 'action', 'choice', 'indifference', 'play', 'second', 'equilibrium', 'solution', 'stable', 'appear', 'case', 'last', 'iteration', 'actiontoplay', 'attention', 'zooming', 'attention', 'mask', 'early', 'iteration', 'see', 'follow', 'zeroinitialisation', 'action', 'embedding', 'attention', 'mask', 'action', 'equally', 'spread', 'play', 'attention', 'mask', 'next', 'block', 'contrast', 'appear', 'structured', 'indeed', 'action', 'attend', 'play', 'involve', 'bestresponse', 'coplayer', 'worth', 'note', 'pattern', 'attend', 'bestresponse', 'nesolving', 'emerge', 'learn', 'prior', 'knowledge', 'action', 'embedding', 'also', 'reveal', 'interesting', 'property', 'game', 'proposition', 'immediately', 'obvious', 'payoff', 'matrix', 'figure', 'make', 'symmetric', 'permute', 'action', 'action', 'embedding', 'bottom', 'show', 'actionembedding', 'player', 'overlap', 'exactly', 'thank', 'inherent', 'equivariance', 'nfgtransformer', 'ordering', 'player', 'action', 'unimportant', 'representation', 'action', 'entirely', 'drive', 'outcome', 'jointaction', 'action', 'embedding', 'reveal', 'inherent', 'symmetry', 'completeness', 'offer', 'similar', 'analysis', 'nfgtransformer', 'apply', 'game', 'class', 'c', 'include', 'game', 'symmetry', 'mixedstrategy', 'well', 'instance', 'fail', 'additionally', 'show', 'example', 'apply', 'nfgtransformer', 'ofdistribution', 'game', 'roberson', 'present', 'several', 'strategy', 'cycle', 'relate', 'work', 'recent', 'work', 'leverage', 'deep', 'learn', 'technique', 'accelerate', 'scientific', 'discovery', 'mathematic', 'fawzi', 'physics', 'pfau', 'biology', 'work', 'similarly', 'motivate', 'bring', 'deep', 'learn', 'technique', 'game', 'theory', 'economic', 'study', 'follow', 'line', 'work', 'bring', 'scalable', 'solution', 'game', 'theory', 'vadori', 'integrate', 'component', 'strategic', 'reasoning', 'part', 'machine', 'learning', 'system', 'wellman', 'game', 'theory', 'hartford', 'close', 'work', 'first', 'apply', 'deep', 'representation', 'learn', 'technique', 'cognitive', 'modelling', 'human', 'strategic', 'reasoning', 'use', 'nfgs', 'author', 'systematically', 'outline', 'number', 'desiderata', 'representation', 'learning', 'include', 'player', 'action', 'equivariance', 'well', 'independence', 'number', 'parameter', 'learn', 'model', 'size', 'game', 'nfgtransformer', 'satisfy', 'desideratum', 'apply', 'nplayer', 'generalsum', 'game', 'wiedenbeck', 'brinkman', 'study', 'efficient', 'datum', 'structure', 'payoff', 'representation', 'approach', 'readily', 'integrate', 'deep', 'learning', 'system', 'assumption', 'game', 'conclusion', 'propose', 'nfgtransformer', 'generalpurpose', 'equivariant', 'architecture', 'represent', 'action', 'embedding', 'demonstrate', 'versatility', 'effectiveness', 'number', 'benchmark', 'task', 'different', 'subfield', 'game', 'theory', 'literature', 'include', 'equilibrium', 'solve', 'deviation', 'gain', 'estimation', 'ranking', 'report', 'empirical', 'result', 'significantly', 'improve', 'art', 'baseline', 'method', 'use', 'unified', 'representation', 'learning', 'approach', 'show', 'result', 'model', 'also', 'interpretable', 'parameterefficient', 'work', 'pave', 'way', 'integrate', 'game', 'theoretic', 'reasoning', 'deep', 'learning', 'system', 'deploy', 'realworld', 'publish', 'conference', 'paper', 'iclr', 'acknowledgment', 'grateful', 'bernardino', 'romeraparede', 'productive', 'discussion', 'different', 'consid', 'eration', 'design', 'equivariant', 'neural', 'architecture', 'expertise', 'ranking', 'evaluation', 'game', 'skanda', 'koppula', 'advice', 'optimisation', 'technique', 'transformer', 'architecture', 'reference', 'shah', 'alexandro', 'voudouris', 'distortion', 'international', 'joint', 'conference', 'social', 'choice', 'problem', 'first', 'year', 'artificial', 'intelligence', 'url', 'https', 'apisemanticscholarorgcorpusid', 'perolat', 'thore', 'graepel', 'reevaluating', 'evaluation', 'advance', 'neural', 'information', 'processing', 'system', 'bachrach', 'jader', 'thore', 'graepel', 'openende', 'learn', 'game', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'math´ematique', 'richesse', 'savant', 'quentin', 'bertrand', 'gauthi', 'gidel', 'limitation', 'elo', 'game', 'transitive', 'additive', 'international', 'conference', 'artificial', 'intelligence', 'statistic', 'pp', 'pmlr', 'settle', 'complexity', 'compute', 'nash', 'journal', 'acm', 'jacm', 'constantinos', 'daskalaki', 'h', 'papadimitriou', 'complexity', 'compute', 'nash', 'equilibrium', 'communication', 'acm', 'minderer', 'sylvain', 'gelly', 'image', 'worth', 'word', 'transformer', 'image', 'recogni', 'tion', 'scale', 'international', 'conference', 'learn', 'representation', 'url', 'https', 'openreviewnetforum', 'idyicbfdntty', 'nash', 'equilibrium', 'approximator', 'learnable', 'proceeding', 'international', 'conference', 'autonomous', 'agent', 'multiagent', 'system', 'pp', 'elo', 'rating', 'chessplayer', 'past', 'present', 'isbn', 'https', 'idsyjcpqaacaaj', 'fawzi', 'balog', 'aja', 'romeraparede', 'r', 'ruiz', 'schrittwieser', 'et', 'discover', 'fast', 'matrix', 'multiplication', 'algorithm', 'reinforcement', 'learning', 'nature', 'slumber', 'ziyu', 'wen', 'neural', 'autocurricula', 'twoplayer', 'game', 'advance', 'neural', 'mation', 'processing', 'system', 'garc´ıa', 'complexity', 'game', 'isomorphism', 'computer', 'science', '6675–6695', 'correlate', 'qlearne', 'icml', 'volume', 'pp', 'wright', 'leytonbrown', 'deep', 'learning', 'predict', 'human', 'strategic', 'behavior', 'advance', 'neural', 'information', 'processing', 'system', 'publish', 'conference', 'paper', 'iclr', 'schmidhuber', 'long', 'memory', 'neural', 'computation', 'junle', 'nash', 'qlearning', 'generalsum', 'stochastic', 'game', 'journal', 'machine', 'learn', 'research', 'brock', 'vinyal', 'perceiver', 'general', 'perception', 'iterative', 'attention', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'tunyasuvunakool', 'russ', 'bate', 'augustin', 'et', 'highly', 'accurate', 'protein', 'structure', 'prediction', 'alphafold', 'nature', 'vinicius', 'thore', 'graepel', 'unified', 'gametheoretic', 'approach', 'multiagent', 'rein', 'forcement', 'learning', 'advance', 'neural', 'information', 'processing', 'system', 'convolutional', 'network', 'image', 'speech', 'time', 'series', 'handbook', 'brain', 'theory', 'neural', 'network', 'friendorfoe', 'qlearne', 'generalsum', 'game', 'icml', 'volume', 'pp', 'simplex', 'neural', 'population', 'learn', 'mixture', 'bayesoptimality', 'symmetric', 'game', 'international', 'conference', 'machine', 'learn', 'pp', 'pmlr', 'henne', 'thore', 'graepel', 'neupl', 'international', 'conference', 'learn', 'representation', 'neural', 'population', 'learn', 'openreviewnetforum', 'idmix3fjkl1', 'ian', 'tuyls', 'turbocharg', 'ing', 'solution', 'concept', 'solve', 'ne', 'ce', 'cce', 'neural', 'equilibrium', 'solver', 'advance', 'neural', 'information', 'processing', 'system', 'ian', 'gemp', 'pilioura', 'equilibriuminvariant', 'embed', 'metric', 'space', 'fundamental', 'set', '×', 'normalform', 'game', 'arxiv', 'preprint', 'j', 'c', 'c', 'mckinsey', 'isomorphism', 'game', 'strategic', 'equivalence', 'pp', 'isbn', 'doi', 'doi101515', 'press', 'nash', 'noncooperative', 'game', 'annal', 'mathematic', 'wwwjstororgstable1969529', 'eugene', 'shoham', 'run', 'gamut', 'comprehensive', 'approach', 'evaluate', 'gametheoretic', 'algorithm', 'volume', 'pp', 'alexander', 'g', 'initio', 'solution', 'manyelectron', 'schr¨odinger', 'equation', 'deep', 'neural', 'network', 'phy', 'rev', 'doi', '101103physrevresearch2033429', 'url', 'https', 'linkap', 'ryan', 'porter', 'eugene', 'nudelman', 'yoav', 'shoham', 'simple', 'search', 'method', 'find', 'nash', 'equilibrium', 'game', 'economic', 'behavior', 'brian', 'colonel', 'game', 'economic', 'theory', 'vadori', 'rahul', 'savani', 'ordinal', 'potentialbased', 'player', 'rating', 'arxiv', 'preprint', 'arxiv230605366', 'publish', 'conference', 'paper', 'iclr', 'llion', 'gomez', 'łukasz', 'polosukhin', 'attention', 'need', 'advance', 'neural', 'informa', 'tion', 'processing', 'system', 'emmanouilvasileio', 'thanasis', 'lianea', 'pilioura', 'mixed', 'nash', 'equilibrium', 'mix', 'advance', 'neural', 'information', 'processing', 'system', 'wiedenbeck', 'brinkman', 'datum', 'structure', 'deviation', 'payoff', 'adaptive', 'agent', 'multiagent', 'system', 'publish', 'conference', 'paper', 'iclr', 'permutation', 'equivariant', 'representation', 'nfgs', 'provide', 'formal', 'statement', 'identity', 'action', 'embed', 'representation', 'use', 'terministic', 'permutation', 'equivariant', 'encoder', 'first', 'recall', 'definition', 'strong', 'isomorphism', 'nfgs', 'gabarr´o', 'definition', 'strongly', 'isomorphic', 'game', 'let', 'g', 'g′', 'nfgs', 'g', 'g′', 'say', 'strongly', 'isomorphic', 'ϕ', 'strong', 'isomorphism', 'τp', 'ω', 'τp', 'ai', 'p', 'player', 'action', 'permutation', 'ω', 'p', 'p′', 'player', 'permutation', 'g′', 'p', 'cid0', 'τω', 'τω', 'cid1', 'gp', '∀a', 'ω', 'p', 'make', 'definition', 'concrete', 'consider', 'coordination', 'anticoordination', 'game', 'show', 'figure', 'game', 'strongly', 'isomorphic', 'exist', 'strong', 'isomorphism', 'ϕ', 'a1', 'ω', 'aside', 'call', 'strongly', 'isomorphic', 'game', 'strategically', 'equivalent', 'discuss', 'soon', 'τ2', 'a1', 'a2', 'a2', 'g', 'a1', 'a1', 'a2', 'a2', 'g′', 'a2', 'a1', 'a2', 'coordination', 'game', 'anticoordination', 'game', 'figure', 'strongly', 'isomorphic', 'game', 'additionally', 'note', 'special', 'case', 'g′', 'refer', 'strong', 'automorphism', 'definition', 'strongly', 'automorphic', 'game', 'say', 'strongly', 'automorphic', 'ϕ', 'strong', 'automorphism', 'τp', 'ω', 'τp', 'ai', 'p', 'player', 'action', 'permutation', 'ω', 'p', 'cid0', 'τω', 'τω', 'cid1', 'gp', '∀a', 'p′', 'player', 'permutation', 'p', 'ai′', 'instance', 'coordination', 'game', 'figure', 'also', 'strongly', 'automorphic', 'exist', 'automorphism', 'recover', 'game', 'ϕ', 'ϕ', 'a1', 'a2', 'a2', 'a1', 'a1', 'a1', 'τ2', 'τ2', 'τ2', 'a1', 'a2', 'a2', 'a1', 'a1', 'a1', 'ω', 'ω', 'ω', 'note', 'permutation', 'player', 'action', 'composition', 'player', 'action', 'permutation', 'therefore', 'general', 'permutation', 'structured', 'one', 'use', 'convention', 'player', 'permutation', 'apply', 'last', 'finally', 'recall', 'permutation', 'π', 'write', 'uniquely', 'permutation', 'orbit', 'c', 'c', 'operate', 'disjoint', 'possibly', 'singleton', 'subset', 'element', 'operate', 'therefore', 'also', 'collection', 'permutation', 'orbit', 'consider', 'coordination', 'game', 'automorphisms', 'equation', '5a5c', 'write', 'permutation', 'consist', 'orbit', 'contain', 'action', 'a2', 'a1', 'a1', 'a2', '6a', 'a2', 'a1', 'a1', 'a2', 'a2', 'a1', 'a1', 'a2', 'definition', 'strategically', 'equivalent', 'action', 'action', 'ai', 'exist', 'strong', 'automorphism', 'contain', 'action', 'ai', 'ω', 'p', 'q', 'q', 'strategically', 'equivalent', 'q', 'orbit', 'equivalently', 'q', 'strategically', 'equivalent', 'exist', 'strong', 'automorphism', 'τp', 'p', 'aj', 'p', 'aj', 'p', 'aj', 'a2', 'consider', 'running', 'example', 'coordination', 'game', 'equation', '6a', 'see', 'a1', 'strategically', 'equivalent', 'pair', 'furthermore', 'equation', 'see', 'a1', 'also', 'strategically', 'equivalent', 'pair', 'therefore', 'coordination', 'game', 'action', 'strategically', 'equivalent', 'a1', 'publish', 'conference', 'paper', 'iclr', 'figure', 'compare', 'nfgtransformer', 'baseline', 'architecture', 'synthetic', 'game', 'result', 'baseline', 'experiment', 'show', 'dash', 'line', 'configuration', 'average', 'independent', 'run', 'shaded', 'area', 'represent', 'confidence', 'interval', 'nfgtransformer', 'variant', 'annotate', 'variant', 'corresponding', 'hyperparameter', 'k', 'show', 'figure', 'theorem', 'a4', 'embed', 'function', 'deterministic', 'equivariant', 'strategically', 'equivalent', 'action', 'ai', 'q', 'embedding', 'p', 'aj', 'proof', 'embed', 'function', 'deterministic', 'equivariant', 'player', 'action', 'tionally', 'automorphism', 'therefore', 'embed', 'ding', 'also', 'equal', 'ai', 'p', 'aj', 'q', 'proposition', 'a5', 'repeat', 'action', 'ai', 'equivariant', 'ai', 'p', 'aj', 'p', 'a¬p', '∀a¬p', 'deterministic', 'p', 'aj', 'p', 'follow', 'ai', 'p', 'aj', 'p', 'proof', 'action', 'repeat', 'exist', 'automorphism', 'τp', 'ω', 'identity', 'therefore', 'ai', 'q', 'strategically', 'equivalent', 'p', 'aj', 'embedding', 'ai', 'p', 'p', 'aj', 'proposition', 'a6', 'player', 'symmetry', 'player', 'p', 'q', 'deterministic', 'equiv', 'ariant', 'identical', 'permutation', 'proof', 'game', 'symmetric', 'p', 'q', 'exist', 'automorphism', 'τp', 'p', 'q', 'therefore', 'ai', 'q', 'strategically', 'equivalent', 'embedding', 'τp', 'p', 'ai', 'experimental', 'setup', 'b1', 'supervise', 'learning', 'synthetic', 'game', 'game', 'sample', 'game', 'sample', 'equilibriuminvariant', 'subspace', 'frobe', 'zeromean', 'payoff', 'player', 'action', 'unit', 'variance', 'tensor', 'norm', 'player', 'payoff', 'sample', 'uniformly', 'set', 'first', 'sample', 'game', 'gradient', 'step', 'gradient', 'step', 'gradient', 'step', 'gradient', 'step', 'max', 'deviation', 'gain', 'gradient', 'step', 'gradient', 'step', 'gradient', 'step', 'gradient', 'step', 'p', 'g', 'e', 'n', 'r', 'r', 'r', 'e', 'e', 'r', 'q', 'e', 'model', 'nfgtransformer', 'd64', 'a2', 'equivariantmlp', 'nfgtransformer', 'nfgtransformer', 'ne', 'nfgtransformer', 'nfgtransformer', 'nfgtransformer', 'd32', 'nfgtransformer', 'd64', 'nfgtransformer', 'publish', 'conference', 'paper', 'iclr', 'unit', 'normal', 'distribution', 'gp', 'n', 'normalize', 'gp', 'benefit', 'distribution', 'provide', 'way', 'uniformly', 'sample', 'space', 'possible', 'strategic', 'interaction', 'nfg', 'specific', 'shape', 'equilibriuminvariant', 'subspace', 'low', 'degree', 'freedom', 'full', 'nfg', 'free', 'neural', 'network', 'learn', 'offset', 'scale', 'invariance', 'game', 'simply', 'map', 'equilibriuminvariant', 'subspace', 'change', 'set', 'equilibrium', 'architecture', 'provide', 'additional', 'technical', 'detail', 'network', 'architecture', 'present', 'section', 'baseline', 'network', 'compose', 'fullyconnected', 'layer', 'hide', 'unit', 'baseline', 'ne', 'consist', 'payoff', 'payoff', 'layer', 'channel', 'payoff', 'dual', 'layer', 'channel', 'dual', 'dual', 'layer', 'channel', 'layer', 'use', 'mean', 'max', 'pooling', 'function', 'nfgtransformer', 'model', 'variant', 'h', 'attention', 'head', 'parameter', 'count', 'model', 'variant', 'report', 'table', 'table', 'number', 'network', 'parameter', 'configuration', 'task', 'note', 'number', 'parameter', 'nfgtransformer', 'ne', 'independent', 'size', 'game', 'contrast', 'fullyconnecte', 'network', 'parameter', 'count', 'depend', 'input', 'size', 'model', 'nfgtransformer', 'nfgtransformer', 'nfgtransformer', 'nfgtransformer', 'nfgtransformer', 'nfgtransformer', 'nfgtransformer', 'nfgtransformer', 'equivariantmlp', 'parameter', 'parameter', 'convergence', 'progression', 'figure', 'visualise', 'training', 'progression', 'model', 'tion', 'task', 'game', 'size', 'experiment', 'report', 'table', 'b2', 'payoff', 'prediction', 'disc', 'game', 'game', 'sampling', 'follow', 'definition', 'generate', 'disc', 'game', 'amount', 'sample', 'latent', 'vector', 'rz', 'realvalue', 'latent', 'vector', 'define', 'valid', 'disc', 'game', 'let', 'ut', 'n', 'u', 'n', 'n', 'u', '−1', 'sample', 'vt', 'way', 'shift', 'random', 'variable', 'u', 'strictly', 'necessary', 'case', 'increase', 'probability', 'result', 'disc', 'game', 'fully', 'cyclic', 'follow', 'proposition', 'bertrand', 'mask', 'sample', 'instance', 'disc', 'game', 'payoff', 'tensor', 'shape', 'additionally', 'sample', 'binary', 'mask', 'shape', 'element', 'fol', 'low', 'p', 'game', 'payoff', 'tensor', 'sampled', 'mask', 'game', 'tensor', 'provide', 'input', 'nfgtransformer', 'network', 'ensure', 'model', 'observe', 'payoff', 'value', 'mask', 'jointaction', 'follow', 'equation', 'loss', 'computation', 'minimise', 'l2', 'loss', 'figure', 'right', 'jointaction', 'observe', 'reconstruction', 'unobserve', 'prediction', 'architecture', 'result', 'section', 'use', 'nfgtransformer', 'd64', 'h', 'attention', 'head', 'attention', 'operation', 'publish', 'conference', 'paper', 'iclr', 'figure', 'gap', 'report', 'gamut', 'game', 'training', 'note', 'single', 'network', 'nfgtransformer', 'd16', 'optimise', 'solve', 'nash', 'equilibrium', 'game', 'class', 'baseline', 'solver', 'baseline', 'result', 'use', 'opensource', 'implementation', 'elo', 'release', 'https', 'githubcomqb3discrate', 'use', 'component', 'setting', 'report', 'bertrand', 'c', 'interpretability', 'result', 'provide', 'additional', 'detail', 'empirical', 'result', 'section', 'figure', 'show', 'sim', 'plification', 'make', 'section', 'interpretability', 'result', 'nfgtransformer', 'remain', 'capable', 'equilibriumsolve', 'game', 'reasonable', 'accuracy', 'covariantgame', 'd06', 'challenging', 'game', 'class', 'show', 'failure', 'case', 'game', 'class', 'figure', 'middle', 'present', 'additional', 'example', 'instance', 'model', 'successfully', 'solve', 'mixedstrategy', 'top', 'generalise', 'outofdistribution', 'game', 'class', 'roberson', 'refer', 'figure', 'caption', 'additional', 'remark', 'result', 'space', 'game', 'introduce', 'subset', 'normalform', 'game', 'game', 'map', 'change', 'set', 'coarse', 'correlate', 'equilibrium', 'nash', 'equilibrium', 'subset', 'game', 'call', 'equilibriuminvariant', 'subset', 'include', 'possible', 'nontrivial', 'strategic', 'interaction', 'game', 'property', 'game', 'equilibrium', 'permutation', 'symmetry', 'bestresponse', 'dynamic', 'visualize', 'map', 'game', 'analyse', 'embedding', 'find', 'nfgtransformer', 'sweep', 'nontrivial', 'equilibriuminvariant', 'subset', 'use', 'transformer', 'architecture', 'nfgtransformer', 'd16', 'h', 'atten', 'tion', 'head', 'self', 'crossattention', 'layer', 'use', 'additional', 'linear', 'layer', 'reduce', 'action', 'embed', 'dimension', 'player', 'action', 'result', 'variable', 'de', 'scribe', 'game', 'embedding', 'train', 'nfgtransformer', 'objective', 'space', 'equilibriuminvariant', 'subset', 'verify', 'loss', 'approach', 'train', 'nfgtran', 'former', 'sweep', 'nontrivial', 'equilibriuminvariant', 'subset', 'visualize', 'embedding', 'figure', 'learn', 'actionembedding', 'low', 'value', 'blue', 'region', 'action', 'mass', 'high', 'value', 'action', 'mass', 'dark', 'red', 'region', 'l', 'region', 'therefore', 'correspond', 'game', 'single', 'pure', 'embedding', 'low', 'value', 'region', 'game', 'mixed', 'ne', 'solution', 'occur', 'cyclic', 'region', 'embedding', 'similarly', 'color', 'action', 'game', 'mix', 'embedding', 'high', 'value', 'red', 'region', 'coordination', 'game', 'area', 'disconnected', 'ne', 'border', 'region', 'correspond', 'change', 'game', 'payoff', 'action', 'become', 'profitable', 'result', 'change', 'drastically', 'therefore', 'embed', 'p', 'g', 'e', 'gradient', 'step', 'game', 'bertrandoligopoly', 'd01', 'graphicalgamesw', 'bidirectionallegcg', 'd02', 'minimumeffortgame', 'd13', 'bidirectionallegrg', 'd03', 'polymatrixgamecg', 'd14', 'bidirectionallegsg', 'polymatrixgamerg', 'd15', 'covariantgame', 'd06', 'polymatrixgameroad', 'd16', 'polymatrixgamesw', 'd17', 'covariantgamezero', 'randomgame', 'dispersiongame', 'travelersdilemma', 'd19', 'graphicalgamerg', 'd09', 'uniformlegcg', 'd20', 'd21', 'graphicalgamesg', 'd11', 'uniformlegsg', 'd22', 'publish', 'conference', 'paper', 'iclr', 'figure', 'provide', 'additional', 'interpretability', 'result', 'similar', 'figure', 'game', 'asymmetric', 'mixedstrategy', 'top', 'outofdistribution', 'bottom', 'additionally', 'provide', 'instance', 'model', 'struggle', 'find', 'attention', 'mask', 'appear', 'converge', 'roberson', 'game', 'class', 'see', 'train', 'model', 'generalise', 'well', 'identify', 'purestrategy', 'ne', 'action', 'embedding', 'also', 'reveal', 'cluster', 'correspond', 'strategically', 'equivalent', 'class', 'action', 'note', 'cluster', 'correspond', 'dominant', 'action', 'player', 'payoff', 'block', 'block', 'r', 'e', 'p', 'l', 'r', 'e', 'p', 'l', '−1', '−2', 'payoff', 'action', 'play', 'block', 'payoff', 'w', 'r', 'block', 'r', 'e', 'l', 'p', 'w', 'r', 'col', 'payoff', 'action', 'play', 'bertrandoligopoly', 'd01', 'nashconv', 'block', 'block', 'block', 'block', 'graphicalgamesg', 'gap', '495456e03', 'action', 'action', 'r', 'e', 'p', 'l', 'bertrandoligopoly', 'd01', 'nashconv', '137161e05', 'marginal', 'c', 'c', 'marginal', 'c', 'c', 'marginal', 'c', 'c', 'action', 'embedding', 'pca', 'action', 'embedding', 'pca', 'action', 'play', 'bertrandoligopoly', 'd01', 'nashconv', 'block', 'block', 'block', 'block', '−2', '−1', 'action', 'payoff', 'block', 'block', 'action', 'play', 'block', '−2', 'action', 'block', 'block', 'block', 'block', '−2', '−2', '−2', 'r', 'e', 'p', 'l', '−2', '−2', '−2', '−2', 'block', 'block', 'block', 'block', 'covariantgame', 'd06', 'gap', 'r', 'e', 'p', 'l', 'r', 'e', 'p', 'l', 'action', 'action', 'r', 'e', 'p', 'l', 'col', 'payoff', 'action', 'embedding', 'pca', 'action', 'play', 'action', 'embedding', 'pca', 'block', 'block', 'block', 'block', '−2', 'action', 'payoff', '−1', 'action', '−2', '−1', 'payoff', 'block', 'block', 'block', 'block', 'block', 'block', '−2', '−2', '−2', 'r', 'e', 'p', 'l', '−2', '−2', '−2', '−2', 'block', 'block', 'block', 'block', 'blotto', 'gap', 'r', 'e', 'p', 'l', 'r', 'e', 'p', 'l', '−1', 'payoff', '−1', 'approx', 'action', 'action', 'r', 'e', 'p', 'l', 'action', 'embedding', 'pca', 'col', 'payoff', 'action', 'embedding', 'pca', 'block', 'block', 'block', 'block', 'block', 'block', 'block', 'block', '−2', '−2', '−2', '−2', 'action', 'action', 'player', 'r', 'e', 'p', 'l', '−2', '−2', '−2', '−2', 'player', 'payoff', 'action', 'play', 'block', 'w', 'r', 'block', 'r', 'e', 'l', 'p', 'w', 'r', 'player', 'payoff', 'w', 'r', 'block', 'r', 'e', 'l', 'p', 'w', 'r', 'r', 'e', 'p', 'l', 'r', 'e', 'p', 'l', 'r', 'e', 'p', 'l', 'r', 'e', 'p', 'l', 'r', 'e', 'p', 'l', 'r', 'e', 'p', 'l', 'publish', 'conference', 'paper', 'iclr', 'a1', 'b', 'c', 'a1', 'a2', 'figure', 'nfgtransformer', 'actionembedding', 'set', 'nontrivial', 'equilibriuminvariant', 'normalform', 'game', 'train', 'objective', 'embedding', 'find', 'closely', 'follow', 'equilibrium', 'boundary', 'dark', 'line', 'symmetrie', 'space', 'game', 'respect', 'symmetric', 'game', 'bottomleft', 'topright', 'diagonal', 'embedding', 'player', 'permutation', 'player', 'fold', 'bottomleft', 'topright', 'diagonal', 'consistent', 'publish', 'conference', 'paper', 'iclr', 'a1', 'a2', 'b', 'coord', 'a2', 'a2', 'a1', 'a2', 'a1', 'a2', 'c', 'cycle', 'a1', 'a2', 'a1', 'a2', 'a1', 'a2', 'e', 'anti', 'cycle', 'a2', 'a2', 'a2', 'a2', 'coord', 'embed', 'g', 'cycle', 'embed', 'ln', 'dmin', 'ai', 'g', 'figure', 'subfigure', '10a', 'show', 'distance', 'near', 'game', 'embed', 'embed', 'ding', 'produce', 'nfgtransformer', 'uniquely', 'describe', 'game', 'apart', 'edge', 'case', 'coordination', 'game', 'identical', 'embedding', 'strategically', 'equivalent', 'colorbar', 'identical', 'embedding', 'cycle', 'game', 'game', 'symmetric', 'embedding', 'player', 'equal', 'verify', 'study', 'bottomleft', 'topright', 'diagonal', 'swap', 'player', 'order', 'expect', 'embedding', 'swap', 'swap', 'player', 'equivalent', 'fold', 'diagonal', 'visually', 'verify', 'embedding', 'swap', 'ai', 'next', 'turn', 'question', 'embedding', 'uniquely', 'describe', 'game', 'fine', 'distance', 'metric', 'action', 'embedding', 'game', 'game', 'ai', 'aj', 'game', 'sample', 'grid', 'describe', 'close', 'embedding', 'game', 'also', 'define', 'distance', 'near', 'game', 'set', 'consider', 'game', 'dmin', 'ai', 'minj̸i∈g', 'ai', 'aj', 'use', 'distance', 'metric', 'verify', 'dmin', 'ai', 'aj', 'apart', 'game', 'strategi', 'cally', 'equivalent', 'action', 'figure', 'p', 'coordination', 'game', 'identical', 'action', 'embedding', 'anticoordination', 'game', 'case', 'permutation', 'equivariance', 'embed', 'action', 'natural', 'language', 'action', 'opponent', 'play', 'give', 'identical', 'high', 'payoff', 'action', 'opponent', 'play', 'give', 'identical', 'low', 'payoff', 'equivariant', 'property', 'possible', 'disambiguate', 'game', 'embedding', 'alone', 'initialize', 'network', 'action', 'label', 'hint', 'reconstruction', 'method', 'row', 'true', 'payoff', 'permute', 'payoff', 'tiny', 'amount', 'enable', 'disambiguation', 'last', 'strategy', 'see', 'figure', 'slightly', 'biased', 'coordination', 'game', 'positive', 'distance', 'near', 'game', 'embed', 'similarly', 'cycle', 'game', 'also', 'know', 'match', 'penny', 'identical', 'embedding', 'cycle', 'game', 'case', 'embed', 'action', 'opponent', 'play', 'high', 'positive', 'payoff', 'opponent', 'high', 'negative', 'payoff', 'action', 'opponent', 'play', 'give', 'high', 'negative', 'payoff', 'opponent', 'high', 'positive', 'payoff', 'note', 'coordination', 'cycle', 'game', 'distinct', 'embedding', 'point', 'space', 'disambiguate', 'handedness', 'appear', 'measure', 'equilibriuminvariant', 'subspace', 'publish', 'conference', 'paper', 'iclr', 'overall', 'embedding', 'neatly', 'describe', 'predict', 'know', 'structure', 'game', 'oretically', 'predict', 'property', 'include', 'permutation', 'symmetry', 'reproduce', 'experiment']"
